arXiv:1611.05896v1 [cs.CV] 17 Nov 2016

Answering Image Riddles using Vision and Reasoning through
Probabilistic Soft Logic
Somak Aditya
Yezhou Yang
Chitta Baral
Arizona State University, Tempe, AZ

Yiannis Aloimonos
University of Maryland, College Park

{saditya1,yz.yang,chitta}@asu.edu

yiannis@cs.umd.edu

Abstract

(described in words) that is invoked by all the images in that
set. Often the common concept is not something that even
a human can observe in her first glance but can come up
with after some thought about the images. Hence the word
“riddle” in the phrase “image riddles”. Figure 1 shows an
example of an image riddle. The images individually connect to multiple concepts such as: outdoors, nature, trees,
road, forest, rainfall, waterfall, statue, rope, mosque etc.
On further thought, the common concept that emerges for
this example is “fall”. Here, the first image represents the
fall season (concept). There is a “waterfall” (region) in the
second image. In the third image, it shows “rainfall” (concept) and the fourth image depicts that a statue is “fall”ing
(action/event). The word “Fall” is invoked by all the images
as it shows logical connections to objects, regions, actions
or concepts specific to each image.
In addition, the answer also connects the most significant1 aspects of the images. Other possible answers like
“nature” or “outdoors” do not demonstrate such properties.
They are too general. In essence, image riddles is a challenging task that not only tests our ability to detect visual
items in a set of images, but also tests our knowledge and
our ability to think and reason.
Based on the above analysis, we argue that a system
should have the following capabilities to answer Image Riddles appropriately: i) the ability to detect and locate the objects, regions, and their properties; ii) the ability to recognize actions; iii) the ability to infer concepts from the detected words; and iv) the ability to rank a concept (described
in words) based on its relative appropriateness; in other
words, the ability to reason with and process background or
commonsense knowledge about the semantic similarity and
relations between words and phrases. These capabilities, in
fact, are also desired of any automated system that aims to
understand a scene and answer questions about it. For example, in VQA dataset [1], “Does this man have children”,
“Is this a vegetarian Pizza?” are some such examples, where
one needs explicit commonsense knowledge.

In this work, we explore a genre of puzzles (“image riddles”) which involves a set of images and a question. Answering these puzzles require both capabilities involving visual detection (including object, activity recognition) and,
knowledge-based or commonsense reasoning. We compile
a dataset of over 3k riddles where each riddle consists of 4
images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an
automatic evaluation metric to track future progress. Our
task bears similarity with the commonly known IQ tasks
such as analogy solving, sequence filling that are often used
to test intelligence.
We develop a Probabilistic Reasoning-based approach
that utilizes probabilistic commonsense knowledge to answer these riddles with a reasonable accuracy. We demonstrate the results of our approach using both automatic and
human evaluations. Our approach achieves some promising
results for these riddles and provides a strong baseline for
future attempts. We make the entire dataset and related materials publicly available to the community in ImageRiddle
Website (http://bit.ly/22f9Ala).

1. Introduction

Figure 1. An Image Riddle Example. Question: “What word connects these images?” .

A key component of computer vision is understanding of
images and it comes up in various tasks such as image captioning and visual question answering (VQA). In this paper,
we propose a new task of “image riddles” which requires
deeper and conceptual understanding of images. In this task
a set of images are provided and one needs to find a concept

1 Formally, an aspect is as significant as the specificity of the information it contains.

1

These riddles can be thought of as a visual counterpart to IQ test question types such as sequence filling
(x1 , x2 , x3 , ?) and analogy solving (x1 : y1 :: x2 : ?)2
where one needs to find commonalities between items. This
task is different from traditional VQA, as in VQA the
queries provide some clues regarding what to look for in the
image in question. Most riddles in this task require both superior detection and reasoning capabilities, whereas a large
percentage (of questions) of the traditional VQA dataset
tests system’s detection capabilities. This task differs from
both VQA and Captioning in that this task requires analysis
of multiple images. While video analysis may require analysis of multiple images, this task of “image riddles” focuses
on analysis of seemingly different images.
Hence, this task of Image Riddles is simple to explain;
shares similarities with well-known and pre-defined types
of IQ questions and it requires a combination of vision and
reasoning capabilities. In this paper, we introduce a promising approach in tackling the problem.
In our approach, we first use state-of-the-art Image Classification techniques [21] to get the top identified classlabels from each image. Given these probabilistic detections, we use the knowledge of connections and relations
of these words to infer a set of most probable words (or
phrases). We use ConceptNet 5 [15] as the source of commonsense and background knowledge that encodes the relations between words and short phrases using a structured
graph. Note, the possible range of candidates are the entire vocabulary of ConceptNet 5 (roughly 0.2 million). For
representation and reasoning with this huge probabilistic
knowledge we use the Probabilistic Soft Logic (PSL) [10, 2]
framework3 . Given the inferred words for each image, we
then infer the final set of answers for each riddle.
Our contributions are threefold: i) we introduce the 3K
Image Riddles Dataset; ii) we present a probabilistic reasoning approach to solve the riddles with reasonable accuracy;
iii) our reasoning module inputs detected words (a closed
set of class-labels) and logically infers all relevant concepts
(belonging to a much larger vocabulary).

to directly adopt topic model-based or Zero-shot learningbased approaches.
Our work is also related to the field of Visual Question
Answering. Very recently, researchers spent a significant
amount of efforts on both creating datasets and proposing
new models [1, 18, 6, 17]. Interestingly both [1] and [6]
adapted MS-COCO [14] images and created an open domain dataset with human generated questions and answers.
Both [18] and [6] use recurrent networks to encode the sentence and output the answer.
Even though some questions from [1] and [6] are very
challenging which actually require logical reasoning in order to answer correctly, popular approaches are still hoping
to learn the direct signal-to-signal mapping from image and
question to its answer, given a large enough annotated data.
The necessity of common-sense reasoning could be easily
neglected. Here we introduce the new Image Riddle problem which is 1) a well-defined cognitively challenging task
that requires both vision and reasoning capability, 2) it is
impossible to model the problem as direct signal-to-signal
mapping, due to the data sparsity and 3) system’s performance could still be bench-marked automatically for comparison. All these qualities make our Image Riddle dataset
a good testbed for vision and reasoning research.

3. Background
In this Section, we briefly introduce the different techniques and Knowledge Sources used in our system.

3.1. Probabilistic Soft Logic (PSL)
PSL is a recently proposed framework for Probabilistic Logic [10, 2]. A PSL model is defined using a set of
weighted if-then rules in first-order logic.
Let C = (C1 , ..., Cm ) be such a collection where each
Cj is a disjunction of literals, where each literal is a variable
yi or its negation ¬yi , where yi ∈ y. Let Ij+ (resp. Ij− ) be
the set of indices of the variables that are not negated (resp.
negated) in Cj . Each Cj can be written as:
wj : ∧i∈I − yi → ∨i∈I + yi
(1)
j
W j
or equivalently, wj : ∨i∈I − (¬yi ) ∨i∈I + yi . Each rule Cj
j
j
is associated with a non-negative weight wj . PSL relaxes
the boolean truth values of each ground atom a (constant
term or predicate with all variables replaced by constants)
to the the interval [0, 1], denoted by I(a). To compute soft
truth values for logical formulas, Lukasiewiczs relaxation
[11] of conjunctions (∧), disjunctions (∨) and negations (¬)
is used :
I(l1 ∧ l2 ) = max{0, I(l1 ) + I(l2 ) − 1}

2. Related Work
The problem of Image Riddles has some similarities to
the genre of topic modeling [3] and Zero-shot Learning
[13]. However, this dataset imposes a few unique challenges: i) the possible set of target labels is the entire Natural Language vocabulary; ii) each image, when grouped
with different set of images can map to a different label; iii)
almost all the target labels in the dataset are unique (3k examples with 3k class-labels). These challenges make it hard

I(l1 ∨ l2 ) = min{1, I(l1 ) + I(l2 )}
2 Examples are:

word analogy tasks (male : female :: king : ?); numeric
sequence filling tasks: (1, 2, 3, 5, ?).
3 PSL is shown to be a powerful framework for high-level Computer
Vision tasks like Activity Detection [16].

(2)

I(¬l1 ) = 1 − I(l1 )
In PSL, the ground atoms are considered as random variables and the distribution is modeled using Hinge-Loss
2

Markov Random Field, which is defined as follows:

NELL [23, 20]; ConceptNet has a more extensive coverage of English language words and phrases. These properties make this Knowledge Graph a perfect source for the
required probabilistic commonsense knowledge.

Definition 3.1. Let y and x be two vectors of n and
n0 random variables respectively, over the domain D =
0
[0, 1]n+n . The feasible set D̃ is a subset of D, defined as:

3.3. Word2vec

 (y,x)=0,∀k∈E 	

D̃ = (y, x) ∈ Dcckk (y,x)≤0,∀k∈I

Word2vec uses the theory of distributional semantics4
to capture word meanings and produce word embeddings
(vectors). The pre-trained word-embeddings have been successfully used in numerous Natural Language Processing
applications and the induced vector-space is known to capture the graded similarities between words with reasonable
accuracy [19]. Throughout the paper, for word2vec-based
similarities, we use the 3 Million word-vectors trained on
Google-News corpus [19].

where c = (c1 , ..., cr ) are linear constraint functions associated with the index sets E and I denoting equality and
inequality constraints. A Hinge-Loss Markov Random Field
P is a probability density, defined as: if (y, x) ∈
/ D̃, then
P(y|x) = 0; if (y, x) ∈ D̃, then:
P(y|x) =

1
exp(−fw (y, x))
Z(w, x)

(3)

R
where Z(w, x) = y|(y,x)∈D̃ exp(−fw (y, x))dy.
The hinge-loss energy function fw is defined as:
m
P
fw (y, x) =
wj (max{lj (y, x), 0})pj , where wj ’s are

4. Approach
Given a set of images (in our case four: {I1 , I2 , I3 , I4 }),
the objective is to determine a set of ranked words (T ) based
on how well the word semantically connects these image.
In this work, we present an approach that uses Probabilistic
Reasoning on top of a probabilistic Knowledge Base (ConceptNet). It also uses additional semantic knowledge of
words from Word2vec. Using these knowledge sources, we
predict the answers to the riddles.

j=1

non-negative free parameters and lj (y, x) are linear constraints over y, x and pj = {1, 2}.
The final inference objective of HL-MRF is:
P(y|x) ≡ arg min

m
X

wj (max{lj (y, x), 0})pj

(4)

y∈[0,1]n j=1

In PSL, each logical rule Cj in the database C is used to
define lj (y, x) i.e. the linear constraints over (y, x). Given
a set of weighted logical formulas, PSL builds a graphical
model defining a probability distribution over the continuous space of values of the random variables in the model.
The final optimization problem is defined in terms of
“distance to satisfaction”. For each rule Cj ∈ C this
distance
satisfaction P
is measured using	 the term wj ×
 toP
max 1 − i∈I + yi − i∈I − (1 − yi ), 0 . This encodes
j
j
the penalty to the system if a rule is not satisfied. The final
optimization problem becomes:
arg min
y∈[0,1]n

X

X
X

	
wj max 1 −
yi −
(1 − yi ), 0

Cj ∈C

i∈Ij+

4.1. Outline of our Framework
Algorithm 1. Solving Riddles
1: procedure U N R IDDLER(I = {I1 , I2 , I3 , I4 }, Kcnet )
2:
for Ik ∈ I do
3:
P̃ (Sk |Ik ) = getClassLabelsNeuralNetwork(Ik ).
4:
for s ∈ Sk do
5:
Ts , Wm (s, Ts ) = retrieveTargets(s, Kcnet );
Wm (s, tj ) = sim(s, tj )∀tj ∈ Ts
6:
end for
7:
Tk = rankTopTargets(P̃ (Sk |Ik ), TSk , Wm );
8:
I(T̂k ) = inferConfidenceStageI(Tk , P̃ (Sk |Ik )).
9:
end for
10:
I(T ) = inferConfidenceStageII([T̂k ]4k=1 , [P̃ (Sk |Ik )]4k=1 ).
11: end procedure

(5)

i∈Ij−

.

As outlined in algorithm 1, for each image Ik (here,
k ∈ {1, ..., 4}), we follow three stages to infer related words
and phrases: i) Image Classification: we get top class labels
and the confidence from Image Classifier (Sk , P̃ (Sk |Ik )),
ii) Rank and Retrieve: using these labels and confidence
scores, we rank and retrieve top related words from ConceptNet (Kcnet ), iii) Probabilistic Reasoning and Inference
(Stage I): using the labels (Sk ) and the top related words
(Tk ), we design an inference model to logically infer final
set of words (T̂k ) for each image. Lastly, we use another
probabilistic reasoning model (Stage II) on the combined
set of inferred words (targets) from all images in a riddle.

3.2. ConceptNet
ConceptNet [22], is a multilingual Knowledge Graph,
that encodes commonsense knowledge about the world and
is built primarily to assist systems that attempts to understand natural language text. The knowledge in ConceptNet
is semi-curated. The nodes (called concepts) in the graph
are words or short phrases written in natural language. The
nodes are connected by edges (called assertions) which are
labeled with meaningful relations (selected from a welldefined closed set of relation-labels). For example: (reptile,
IsA, animal), (reptile, HasProperty, cold blood) are some
edges. Each edge has an associated confidence score. Also,
compared to other knowledge-bases like WordNet, YAGO,

4 The

3

central idea is: “a word is known by the company it keeps”.

4.3.1

This model assigns the final confidence scores on the combined set of targets (T ). The pipeline followed for each
image is depicted with an example in Figure 2.

Retrieve Related Words For a Seed

Visual Similarity: We observe that, for objects, the
ConceptNet-similarity gives a poor result (See Table 1). So,
we define a metric called visual similarity. Let us call the
similar words as targets. In this metric, we represent the
seed and the target as vectors. To define the dimensions,
for each seed, we use a set of relations (HasA, HasProperty,
PartOf and MemberOf). We query ConceptNet to get the related words (say, W1,W2,W3...) under such relations for the
seed-word and its superclasses. Each of these relation-word
pairs (i.e. HasA-W1,HasA-W2,PartOf-W3,...) becomes a
separate dimension. The values for the seed-vector are the
weights assigned to the assertions. For each target, we
query ConceptNet and populate the target-vector using the
edge-weights for the dimensions defined by the seed-vector.
To get the top words using visual similarity, we use the
cosine similarity of the seed-vector and the target-vector to
re-rank the top 10000 retrieved similar target-words using
ConceptNet-similarity. For abstract seed-words, we do not
get any such relations and we use the ConceptNet similarity directly. Table 1 shows the top similar words using

Figure 2. An overview of the framework followed for each Image;
demonstrated using an example image of an aardvark (resembles
animals such as tapir, ant-eater). We run a similar pipeline for
each image and then infer final results using a final Probabilistic
Inference Stage (Stage II).

4.2. Image Classification
Neural Networks trained on ample source of images and
numerous image classes has been very effective. Studies have found that convolutional neural networks (CNN)
can produce near human level image classification accuracy [12], and related work has been used in various visual recognition tasks such as scene labeling [5] and object
recognition [7]. To exploit these advances, we use the stateof-the-art class detections provided by the Clarifai API [21]
and the Deep Residual Network Architecture by [8] (using
the trained ResNet-200 model). For each image (Ik ) we
use top 20 detections (Sk ). Let us call these detections as
seeds. An example is provided in the Figure 2. Each detection is accompanied with the classifier’s confidence score
(P̃ (Sk |Ik )).

ConceptNet
man, merby, misandrous,
philandry, male human,
dirty pig, mantyhose,
date woman,guyliner,manslut

Visual Similarity
priest, uncle, guy,
geezer, bloke, pope,
bouncer, ecologist,
cupid, fella

word2vec
women, men, males,
mens, boys, man, female,
teenagers,girls,ladies

Table 1. Top 10 similar Words for “Men”. More in appendix.

ConceptNet, word2vec and visual-similarity for the word
“men”. Moreover, the ranked list based on visual-similarity
ranks boy, chap, husband, godfather, male person, male in
the ranks 16 to 22.
Formulation: For each seed (s), we get the top words
(Ts ) from ConceptNet using the visual similarity metric
and the similarity vector Wm (s, Ts ). Together for an
image, these constitute TSk and the matrix Wm , where
Wm (si , tj ) = simvis (si , tj )∀si ∈ Sk , tj ∈ TSk . Next
we describe the defined similarity metric.
A large percentage of the error in Image Classifiers are
due to visually similar (or semantically similar) objects or
objects from the same category [9]. In such cases, we use
this visual similarity metric to predict the possible visually
similar objects and then use an inference model to infer the
actual object.

4.3. Rank and Retrieve Related Words
Our goal is to logically infer words or phrases that represent (higher or lower-level) concepts that can best explain
the co-existence of the seeds in a scene. Say, for “hand” and
“care”, implied words could be “massage”, “ill”, “ache” etc.
For “transportation” and “sit”, implied words/phrases could
be “sit in bus”, “sit in plane” etc. The reader might be inclined to infer other concepts. However, to “infer” is to derive “logical” conclusions. Hence, we prefer the concepts
which shares strong explainable connections with the seedwords.
A logical choice would be traversing a knowledge-graph
like ConceptNet and find the common reachable nodes from
these seeds. As this is computationally quite infeasible,
we use the association-space matrix representation of ConceptNet, where the words are represented as vectors. The
similarity between two words approximately embodies the
strength of the connection over all paths connecting the two
words in the graph. We get the top similar words for each
seed, approximating the reachable nodes.

4.3.2

Rank Targets

We use P̃ (Sk |Ik ) as an approximate vector representation
for the image, in which the seed-words are the dimensions.
The columns of Wm provides vector representations for the
target words (t ∈ TSk ) in the space. We calculate cosine
similarities for each target with such a image-vector and
then re-rank the targets. We consider the top θ#t targets
and we call it Tk .
4

4.4. Probabilistic Reasoning and Inference
4.4.1

indicates higher connectivity of a word in the graph. This
yields a higher similarity score to many words and might
give an unfair bias to this target in the inference model.
Hence, the higher the C(.), the word provides less specific
information for an image. Hence, the weight becomes

PSL Inference Stage I

Given a set of candidate targets Tk and a set of weighted
seeds (Sk , P̃ (Sk |Ik )), we build an inference model to infer
a set of most probable targets (T̂k ). We model the joint
distribution using PSL as this formalism adopts Markov
Random Field which obeys the properties of Gibbs Distribution. In addition, a PSL model is declared using rules.
Given the final answer from the system, the set of satisfied
(grounded) rules show the logical connections between the
detected words and the final answer, which demonstrates
the system’s explainability.
The PSL model can be best depicted as an Undirected
Graphical Model involving seeds and targets, as given in
Figure 3.

wtij = θα1 ∗ simcn (sik , tjk )+
θα2 ∗ simw2v (sik , tjk ) + 1/C(tjk ),

(6)

where simcn (., .) is the normalized ConceptNet-based similarity. simw2v (., .) is the normalized word2vec similarity
of two words and C(.) is the eigenvector-centrality score of
the argument in the ConceptNet matrix.
ii) To model dependencies among the targets, we observe
that if two concepts t1 and t2 are very similar in meaning, then a system that infer t1 should infer t2 too, given
the same set of observed words. Therefore, the two rules
wtjm : tjk → tmk and wtjm : tmk → tjk are designed
to force the confidence values of tjk and tmk to be as close
to each other as possible. wtjm is the same as Equation 6
without the penalty for popularity.
The combined PSL model inference objective becomes:
arg min

Figure 3. Joint Modeling of seeds and targets, depicted as a Undirected Graphical Model. We define the seed-target and targettarget potentials using PSL rules. We connect each seed to each
target and the potential depends on their similarity and the target’s
popularity bias. We connect each target to θt-t (1 or 2) maximally
similar targets. The potential depends on their similarity.

I(Tk

)∈[0,1]|Tk |

X

X

X

	

wtij max I(sik ) − I(tjk ), 0 +

sik ∈Sk tjk ∈Tk

X

n

	
wtjm max I(tmk ) − I(tjk ), 0 +

tjk ∈Tk tmk ∈Tjmax


	o
max I(tjk ) − I(tmk ), 0 .

To let the targets compete against each other, we add a constraint
P on the sum of the confidence scores of the targets
i.e. j:tjk ∈Tk I(tjk ) ≤ θsum1 . Here θsum1 ∈ {1, 2} and
I(tjk ) ∈ [0, 1]. As a result of this model, we get an inferred
reduced set of targets [T̂k ]4k=1 .

Formulation: Using PSL, we add two sets of rules: i) to
define seed-target potentials, we add rules of the form wtij :
sik → tjk for each word sik ∈ Sk and target tjk ∈ Tk ; ii)
to define target-target potentials, for each target tjk , we take
the most similar θt-t targets (Tjmax ). For each target tjk and
each tmk ∈ Tjmax , we add two rules wtjm : tjk → tmk and
wtjm : tmk → tjk . Next, we describe the choices in detail.
i) From the perspective of optimization, the rule wtij :
sik → tjk adds the term wtij ∗ max{I(sik ) − I(tjk ), 0}
to the objective. This means that if confidence score of the
target tjk is not greater than I(sik ) (i.e. P̃ (Sk |Ik )), then
the rule is not satisfied and we penalize the model by wtij
times the difference between the confidence scores. We add
the above rule for seeds and targets for which the combined
weighted similarity exceeds certain threshold θsim,psl1 .
We encode the commonsense knowledge of words and
phrases obtained from different knowledge sources into the
weights of these rules wtij . Both the knowledge sources are
considered because ConceptNet embodies commonsense
knowledge and word2vec encodes word-meanings. It is
also important that the inference model is not biased towards more popular targets (i.e. abstract words or words too
commonly used/detected in corpus). We compute eigenvector centrality score (C(.)) for each word in the context of
ConceptNet (a network of words and phrases). Higher C(.)

4.4.2

PSL Inference Stage II

To learn the most probable set of common targets jointly,
we consider the targets and the seeds from all images together. Assume that the seeds and the targets are nodes
in a knowledge-graph. Then, the most appropriate targetnodes should observe similar properties as an appropriate
answer to the riddle: i) a target-node should be connected
to the high-weight seeds in an image i.e. should relate to the
important aspects of the image; ii) a target-node should be
connected to seeds from all images.
Formulation: Here, we use the rules wtij : sik → tjk
for each word sik ∈ Sk and target tjk ∈ T̂k for all
k ∈ {1, 2.., 4}. To let the set of targets compete against each
P4 P
other, we add the constraint
k=1
j:tjk ∈T̂k I(tjk ) ≤
θsum2 . Here θsum2 = 1 and I(tjk ) ∈ [0, 1].
To minimize the penalty for each rule, the optimal solution will try to maximize the confidence score of tjk . To
5

5.2.1

minimize the overall penalty, it should maximize the confidence scores of those targets which will satisfy most of the
rules (or rules with maximum total weight). As the summation of confidence scores is bounded, only a few top inferred
targets should have non-zero confidence.

Systems

We propose several variations of the proposed approach and
compare them with a simple vision-only baseline (hypothesis I). We introduce an additional Bias-Correction stage after the Image Classification, which aims to re-weight the
detected seeds using additional information from other images. The variations then, are created to test the effects of
varying the Bias-Correction stage and the effects of the individual stages of the framework on the final accuracy (hypothesis II). We also vary the initial Image Classification
Method (Clarifai, Deep Residual Network).
Bias-Correction: We experimented with two variations:
i) greedy bias-correction and ii) no bias-correction. We follow the intuition that the re-weighting of the seeds of one
image can be influenced by the others7 . To this end, we develop the “GreedyUnRiddler” (GUR) approach. In this approach, we consider all of the images together to dictate the
new weight of each seed. Take image Ik for example. To reweight seeds in Sk , we calculate
P the weights using the folsimcosine (Vsk ,j ,Vj )
. Vj
lowing equation: W̃ (sk ) = j∈1,..4
4.0
is vector of the weights assigned P̃ (Sj |Ij ) i.e. confidence
scores of each seed in the image. Each element of Vsk ,j [i]
is the ConceptNet-similarity score between the seed sk and
si,j i.e. the ith seed of the j th image. The re-weighted seeds
(Sk , W̃ (Sk )) of an image are then passed through the rest
of the pipeline to infer the final answers.
In the original pipeline (“UnRiddler”,in short UR), we
just normalize the weights of the seeds and pass on to the
next stage. We experiment with another variation (called
BiasedUnRiddler or BUR), the results of which are included in appendix, as GUR achieves the best results.
Effect of Stages: We observe the accuracy after each
stage in the pipeline (VB: Upto Bias Correction, RR: Upto
Rank and Retrieve stage, All: The entire Pipeline). For VB,
we use the normalized weighted seeds, get the weighted
centroid vector over the word2vec embeddings of the seeds
for each image. Then we obtain the mean vector over these
centroids. The top similar words from the word2vec vocabulary to this mean vector, constitutes the final answers.
For RR, we get the mean vector over the top predicted targets for all images. Again, the most similar words from the
word2vec vocabulary constitutes the answers.
Baseline: We create Vision-only Baselines. We directly
use the class-labels and the confidence scores predicted using a Neural Network-based Classifier. For each image,
we calculate the weighted centroid of the word2vec embeddings of these labels and the mean of these centroids for the
4 images. For the automatic evaluation we use this centroid
and for the human evaluation, we use the most similar word
to this vector, from the word2vec vocabulary. The Baseline

5. Experiments and Results
In this section, we provide the results of the validation
experiments of the newly introduced Image Riddle dataset,
followed by empirical evaluation of the proposed approach
against vision-only baselines.

5.1. Dataset Validation and Analysis
We have collected a set of 3333 riddles from the internet (puzzle websites). Each riddle has 4 images (66 × 66,
6KB in size) and a groundtruth label associated with it.
To verify the groundtruth answers, we define the metrics:
i) “correctness” - how correct and appropriate the answers
are, and ii) “difficulty” - how difficult are the riddles. We
conduct an Amazon Mechanical Turker-based evaluation.
We ask them to rate the correctness from 1-65 . The “difficulty” is rated from 1-76 . According to the Turkers, the
mean correctness rating is 4.4 (with Standard Deviation
1.5). The “difficulty” ratings show the following distribution: toddler (0.27%), younger child (8.96%), older child
(30.3%), teenager (36.7%), adult (19%), linguist (3.6%),
no-one (0.64%). In short, the average age to answer the
riddles seems to be closer to 13-17yrs. Also, few of these
(4.2%) riddles seem to be incredibly hard. Interestingly, the
average age perceived reported for the recently proposed
VQA dataset [1] is 8.92 yrs. Although, this experiment
measures “the turkers’ perception of the required age”, one
can conclude that the riddles are comparably harder.

5.2. System Evaluation
The presented approach suggests the following hypotheses that requires empirical tests: I) the proposed approach
(and their variants) attain reasonable accuracy in solving the
riddles; II) the individual stages of the framework improves
the final inference accuracy of the answers. In addition,
we also experiment to observe the effect of using commercial classification methods like Clarifai against a published
state-of-the-art Image Classification method.
5 1: Completely gibberish, incorrect, 2: relates to one image, 3 and 4:
connects two and three images respectively, 5: connects all 4 images, but
could be a better answer, 6: connects all images and an appropriate answer.
6 These gradings are adopted from VQA AMT instructions [1]. 1: A
toddler can solve it (ages:3-4), 2: A younger child can solve it (ages:58), 3: A older child can solve it (ages:9-12), 4: A teenager can solve it
(ages:13-17), 5: An adult can solve it (ages:18+), 6: Only a Linguist (one
who has above-average knowledge about English words and the language
in general) can solve it, 7: No-one can solve it.

7 A person would often skim through all the images at one go and will
try to come up with the aspects that needs more attention.

6

performances are listed in Table 2 in the VB+UR cells.
5.2.2

with a scenario: We have three separate robots that attempted to answer this riddle. You have to rate the answer
based on the correctness and the degree of intelligence (explainability) shown through the answer.. The correctness is
defined as before. In addition, turkers are asked to rate intelligence in a scale of 1-48 . We plot the the percentage of
total riddles per each value of correctness and intelligence
in Figure 4. In these histograms plots, we expect a increase
in the rightmost buckets for the more “correct” and “intelligent” systems.

Experiment I: Automatic Evaluation

We evaluate the performance of the proposed approach on
the 3333 Image Riddles dataset using both automatic and
Amazon Mechanical Turker (AMT)-based evaluations.
As an evaluation metric, we use word2vec similarity measure. An answer to a riddle may have several semantically similar answers. Hence it is reasonable to use such a metric. For each riddle, we calculate the maximum similarity between the groundtruth
and top 10 detections from an approach. To calculate
phrase similarities, we use n similarity method of the
gensim.models.word2vec package. The average of
such maximum similarities is reported in percentage form.

Clarifai

ResNet

VB
RR
All
VB
RR
All

GUR
3.3k
2.8k
65.3
65.36
65.9
65.73
68.8*
68.7
66.8
66.4
66.3
66.2
68.2
68.2

UR
3.3k
65
65.9
68.5
68.3
67
68.53

2.8k
65.3†
65.7
68.57
68†
66.7
68.2

Table 2. Accuracy on the Image Riddle Dataset. Pipeline variants
(VB, RR and All) are combined with Bias-Correction stage variants (GUR, UR). All values are in percentage form. (*- Best, † Baselines).
θ#t
θα1
θα2
θt-t
θsim,psl1
θsum1

Number of Targets
ConceptNet-similarity Weight
word2vec-similarity weight
Number of maximum similar Targets
Seed-target similarity Threshold
Sum of confidence scores in Stage I

2500
1
4
1
0.8
2

Figure 4. AMT Results of The GUR+All
(our), Clarifai (baseline
.
1) and ResidualNet (baseline 2) approaches. Correctness Means
are: 2.6 ± 1.4, 2.4 ± 1.45, 2.3 ± 1.4. For Intelligence: 2.2 ±
0.87, 2 ± 0.87, 1.8 ± 0.8

Table 3. A List of parameters θ used in the approach

5.2.4 Analysis
Experiment I shows that the GUR variant (GUR+All in Table 2) achieves the best results in terms of word2vec-based
accuracy. Similar trend is reflected in the AMT-based evaluations (Figure 4). Our system has increased the percentage
of puzzles for the rightmost bins i.e. produces more “correct” and “intelligent” answers for more number of puzzles. The word2vec-based accuracy puts the performance
of ResNet baseline close to that of the GUR variant. However, as evident from Figure 4, the AMT evaluation of the
correctness shows clearly that the ResNet baseline lags in
predicting meaningful answers. Experiment II also includes
what the turkers think about the intelligence of the systems
that tried to solve the puzzles. This also puts the GUR variant at the top. The above two experiments empirically show

To select the parameters in the parameter vector θ, We
employed a random search on the parameter-space over first
500 riddles over 500 combinations. The final set of parameters used and their values are tabulated in Table 3.
Each of the stage-variants (VB, RR and All) are combined with different variations of the Bias-Correction stage
(GUR and UR respectively). The accuracies on all are listed
in Table 2. We provide our experimental results on this 3333
riddles and 2833 riddles (barring 500 riddles we used for the
parameter search).
5.2.3

Experiment II: Human Evaluation

We conduct an AMT-based comparative evaluation of the
results of the proposed approach (GUR+All using Clarifai)
and two vision-only baselines. We define two metrics: i)
“correctness” and ii) “intelligence”. Turkers are presented

8 1: Not intelligent, 2: Moderately Intelligent, 3: Intelligent, 4: Very
Intelligent.

7

Figure 5. Positive and Negative (in red) results of the “GUR” approach (GUR+All variant) on some of the riddles. The groudtruth labels,
closest label among top 10 from GUR and the Clarifai baseline are provided for all images. For more results, check Appendix and the
ImageRiddle website (http://bit.ly/1Rj4tFc).

that our approach achieves a reasonable accuracy in solving
the riddles (Hypothesis I). In table 2, we observe how the
accuracy varies after each stage of the pipeline (hypothesis
II). The table shows a jump in the accuracy after the RR
stage, which leads us to believe the primary improvement
of our approach is attributed to the Probabilistic Reasoning
model. We also provide our detailed results for the “GUR”
approach using a few riddles in Figure 5.

improves on vision-only baselines and provides a stronger
baseline for future attempts.
The task of “Image Riddles” is equivalent to conventional IQ test questions such as analogy solving, sequence
filling; which are often used to test human intelligence. This
task of “Image Riddles” is also in line with the current trend
of VQA datasets which require visual recognition and reasoning capabilities. However, it focuses more on the combination of both vision and reasoning capabilities. In addition to the task, the proposed approach introduces a novel
inference model to infer related words (from a large vocabulary) given class labels (from a smaller set), using semantic
knowledge of words. This method is general in terms of
its applications. Systems such as [24], which use a collection of high-level concepts to boost VQA performance; can
benefit from this approach.

6. Conclusion and Future Works
In this work, we presented a Probabilistic Reasoning
based approach to solve a new class of image puzzles, called
“Image Riddles”. We have collected over 3k such riddles.
Crowd-sourced evaluation of the dataset demonstrates the
validity of the annotations and the nature of the difficulty
of the riddles. We empirically show that our approach
8

References

[14] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014, pages 740–755. Springer,
2014. 2

[1] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,
C. L. Zitnick, and D. Parikh. Vqa: Visual question
answering. In International Conference on Computer
Vision (ICCV), 2015. 1, 2, 6

[15] H. Liu and P. Singh. Conceptnet - a practical commonsense reasoning tool-kit. BT Technology Journal,
22(4):211–226, Oct. 2004. 2

[2] S. Bach, B. Huang, B. London, and L. Getoor.
Hinge-loss markov random fields: Convex inference for structured prediction.
arXiv preprint
arXiv:1309.6813, 2013. 2

[16] B. London, S. Khamis, S. Bach, B. Huang, L. Getoor,
and L. Davis. Collective activity detection using
hinge-loss markov random fields. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pages 566–571, 2013. 2

[3] D. M. Blei. Probabilistic topic models. Commun.
ACM, 55(4):77–84, Apr. 2012. 2
[4] M. Brysbaert, A. B. Warriner, and V. Kuperman. Concreteness ratings for 40 thousand generally known
english word lemmas. Behavior research methods,
46(3):904–911, 2014. 10

[17] L. Ma, Z. Lu, and H. Li. Learning to answer questions
from image using convolutional neural network. arXiv
preprint arXiv:1506.00333, 2015. 2

[5] C. Farabet, C. Couprie, L. Najman, and Y. LeCun.
Learning hierarchical features for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1915–1929, 2013. 4

[18] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your
neurons: A neural-based approach to answering questions about images. arXiv preprint arXiv:1505.01121,
2015. 2
[19] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector
space. arXiv preprint arXiv:1301.3781, 2013. 3

[6] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and
W. Xu. Are you talking to a machine? dataset and
methods for multilingual image question answering.
arXiv preprint arXiv:1505.05612, 2015. 2

[20] T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar,
J. Betteridge, A. Carlson, B. Dalvi, M. Gardner,
B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis,
T. Mohamed, N. Nakashole, E. Platanios, A. Ritter, M. Samadi, B. Settles, R. Wang, D. Wijaya,
A. Gupta, X. Chen, A. Saparov, M. Greaves, and
J. Welling. Never-ending learning. In Proceedings
of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15), 2015. 3

[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich
feature hierarchies for accurate object detection and
semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on,
pages 580–587. IEEE, 2014. 4
[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint
arXiv:1512.03385, 2015. 4

[21] G. Sood. clarifai: R Client for the Clarifai API, 2015.
R package version 0.2. 2, 4

[9] D. Hoiem, Y. Chodpathumwan, and Q. Dai. Diagnosing error in object detectors. In European conference
on computer vision, pages 340–353. Springer, 2012. 4

[22] R. Speer and C. Havasi. Representing general relational knowledge in conceptnet 5. 2012. 3

[10] A. Kimmig, S. Bach, M. Broecheler, B. Huang, and
L. Getoor. A short introduction to probabilistic soft
logic. In Proceedings of the NIPS Workshop on Probabilistic Programming: Foundations and Applications,
pages 1–4, 2012. 2

[23] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago:
A core of semantic knowledge. In Proceedings of the
16th International Conference on World Wide Web,
WWW ’07, pages 697–706, New York, NY, USA,
2007. ACM. 3

[11] G. Klir and B. Yuan. Fuzzy sets and fuzzy logic: theory and applications. 1995. 2

[24] Q. Wu, C. Shen, L. Liu, A. Dick, and A. v. d. Hengel. What value do explicit high level concepts have
in vision to language problems?
arXiv preprint
arXiv:1506.01144, 2015. 8

[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing systems, pages 1097–1105, 2012. 4
[13] H. Larochelle, D. Erhan, Y. Bengio, U. D. Montral,
and M. Qubec. Zero-data learning of new tasks. In In
AAAI, 2008. 2
9

Appendices

word v provides similar but more specific information than
word u. Each node has a resource P̃ (u|Ik ), the confidence
assigned by the Neural Network. If there is an edge from the
node, some of this resource should be sent along this edge
until for all edges (u, v) ∈ G, wv becomes greater than wu .
We formulate the problem as a Linear Optimization problem:

A. BiasedUnRiddler (BUR): A Variation of the
BiasCorrection Stage

minimize

w=(w1 ,...w|S | )
k

subject to

X
X
s∈Sk

X

P̃ (sk |Ik )

sk ∈Sk

wu ≥ 0.5P̃ (u|Ik ), ∀u ∈ G

In Figure 6: dinosaur, animal and reptile all provide evidence that the image has an animal. Only the word dinosaur
indicates what kind of animal is in the image. The other
words do not add any additional information. Some highconfidence detections also provide erroneous abstract information. Here, the labels monstrous, monster are some such
detections. Hence, the objective is to re-weight the seeds
so that: i) the more specific seed-words should have higher
weight than the ones which provide similar but more general information; ii) the seeds that are too frequently used
or detected in corpus, should be given lower weights.
Specificity and Popularity: We compute eigenvector
centrality score (ECS) for each word in the context of ConceptNet. Higher ECS indicates higher connectivity and
yields a higher similarity score to many words and might
give an unfair bias to this seed (and words implied by this
seed) in the inference model. Hence, the higher the ECS,
the word provides less specific information for an image.
Additionally, we use the concreteness rating (CR) from
[4]. In this paper, the top 39955 frequent English words
are rated from the scale of 1 (very abstract) to 5 (very concrete). For example, the mean ratings for monster, animal
and dinosaur are 3.72, 4.61 and 4.87 respectively.
Problem Formulation: We formulate the problem as a
resource flow problem on a graph. The directed graph G is
constructed in the following way: we order the seeds based
on decreasing centrality scores (CS). We compute CS as:

To limit the resource a node u can send, we limit the final
minimum value by 0.5 P̃ (u|Ik ). The solution provides us
with the necessary weights for the set of seeds Sk in Ik . We
normalize these weights and get W̃ (Sk ).

B. Intermediate Results for the “Aardvark”
Riddle

Figure 7. The four different Images for the “aardvark” riddle.

From the four figures in Figure 7, we get the top 20 Clarifai detections as given in the Table 4.
Based on the GUR approach (GUR+All in paper), our
PSL Stage I outputs probable concepts (words or phrases)
depending on the initial set of detected class-labels (seeds).
They are provided in Table 5. Note that, these are the
top targets detected from almost 0.2 million possible candidates. Observe the following:
i) the highlighted detected animals have a few visual features in common, such as four short legs, a visible tail, short
height etc.
ii) the detections from the third image does not at all lead
us to an animal and the PSL Stage I still thinks that its a
cartoon of sort.
iii) the detections from second gets affected because of
its close relation to the detections from third image and it
infers that the image just depicts cartoon.
In the final PSL Stage II however, the model figures out
that there is an animal that is common to all these images.
This is mainly because seeds from the three images confidently predict that some animal is present in the images.

(7)

where we normalize ECS and −CR to the scale of 0 to 1.
For each seed u, we check the immediate next node v and
add an edge (u, v) if the (ConceptNet-based) similarity between u and v is greater than θsim,ss 9 . If in this iteration, a
node v is not added in G, we get the most recent predecessor u for which the similarity exceed θsim,ss and add (u, v).
The idea is that if a word u is more abstract than v and if
they are quite similar in terms of conceptual similarity, then
9θ

ws =

wu = P̃ (u|Ik ), u ∈
/G

Figure 6. Clarifai detections and results from different stages for
the aardvark image (for BUR variant).

CS = (ECS + (−CR))/2,

max{wu − wv , 0}

(u,v)∈G

denotes the set of parameters used in the model.

10

Image1
monster
jurassic
monstrous
primitive
lizard
paleontology
vertebrate
dinosaur
creature
wildlife
nature
evolution
reptile
wild
horizontal
illustration
animal
side view
panoramic
mammal

Image2
food
small
vector
dinosaur
wildlife
cartoon
nature
evolution
reptile
outline
cute
sketch
painting
silhouette
horizontal
art
illustration
graphic
animal
panoramic

Image3
fun
retro
clip
halloween
set
border
messy
ink
design
ornate
decoration
ornament
vector
contour
cartoon
cute
silhouette
art
illustration
graphic

Image1
panda
dolphin
african forest elephant
placental mammal
otter
gorilla
wildebeest
chimaera
african savannah elephant
florida panther
liger
rabbit
aardvark
iguana
hippopotamus
hadrosaur
mountain goat
panda bear
velociraptor
whale

Image4
rock
nobody
travel
water
sea
aquatic
outdoors
sand
beach
bird
wildlife
biology
zoology
carnivora
nature
horizontal
animal
side view
panoramic
mammal

Image2
graph toughness
cartography
color paint
graph
spectrograph
revue
linear functional
simulacrum
pen and ink
luck of draw
cartoon
camera lucida
explode view
micrographics
hamiltonian graph
crowd art
depiction
echocardiogram
scenography
linear perspective

Image3
decorative
graph toughness
graph
artwork
spectrograph
kesho mawashi
tapestry
map
arabesque
sgraffito
linear functional
hamiltonian graph
emblazon
pretty as picture
art deco
dazzle camouflage
ecce homo
pointillist
pyrography
echocardiogram

Image3
hamiltonian graph
graph toughness
lacquer
figuration
war paint
graph
spectrograph
map
arabesque
fall off analysis
art collection
statue
delineate
jack o lantern
gussie up
ecce homo
pointillist
art deco
pyrography
scenography

Image4
giraffe
waterbuck
sandy beach
moose
wildebeest
skunk
anteater
echidna
bobcat
mule deer
bison
pygmy marmoset
mongoose
sea otter
squirrel monkey
wolverine
okapi
cane rat
whale
american bison

Table 6. Top 20 detections per each image from PSL Stage I (IUR).

They are provided in the Table 6. Observe that the individual detections are better compared to GUR10 .
Final output from PSL Stage II (for BUR) is comparable to that of the GUR approach. The top detections
are: hadrosaur, sea otter, diagrammatic, panda, iguana, pyrography, mule deer, placental mammal, liger, panda bear,
art deco, squirrel monkey, giraffe, echidna, otter, anteater,
pygmy marmoset, hippopotamus.
Here, the set of output mainly contains the concepts
(words or phrases) that either represents “animals with
some similar visual characteristics to aardvark” or it pertains to “cartoon or art”.

Table 4. Top 20 detections from Calrifai API. The detections that
are completely noisy is colored using red. It can be observed that
the third image does not give any evidence of an animal present.
Image1
dolphin
rhinoceros
komodo dragon
african elephant
lizard
gorilla
crocodile
indian elephant
wildebeest
elephant
echidna
chimaera
chimpanzee
liger
gecko
rabbit
iguana
hippopotamus
mountain goat
loch ness monster

Image2
like paint
projective geometry
diagram
line of sight
venn diagram
hippocratic face
real number line
sight draft
x axis
simulacrum
cartoon
diagrammatic
camera lucida
explode view
crowd art
lottery
depiction
conecept design
infinity symbol
scenography

Image4
bison
american bison
marsupial
gibbon
monotreme
moose
mole
wildebeest
echidna
turtle
mule deer
mongoose
tamarin
chimpanzee
wolverine
prairie dog
western gorilla
anteater
okapi
skunk

C. Detailed Accuracy Histograms For Different Variants
In this section, we plot the accuracy histograms for the
entire dataset for all the variants (using Clarifai API) of our
approach (listed in Table 2 of the paper). We also add the
accuracy histograms for variants using BUR approach. The
plots are shown in the Figure 8. From the plots, the shift towards greater accuracy is evident as we go along the stages
of our pipeline.

D. Visual Similarity: Additional Results

Table 5. Top 20 detections per each image from PSL Stage I
(GUR).

Additional results for Visual Similarity are provided in
Tables 7 and 8.
ConceptNet
man, merby, misandrous,
philandry, male human,
dirty pig, mantyhose,
date woman,guyliner,manslut

That is why most of the top detections correspond to animals and animals having certain characteristics in common.
The top detections from PSL Stage II (GUR)
are: monotreme, gecko, hippopotamus, pyrography,
anteater, lizard, mule deer, chimaera, liger, iguana, komodo dragon, echidna, turtle, art deco, sgraffito, gorilla,
loch ness monster, prairie dog.
BUR: For BUR, PSL Stage I outputs probable concepts
(words or phrases) depending on the current set of seeds.

Visual Similarity
priest, uncle, guy,
geezer, bloke, pope,
bouncer, ecologist,
cupid, fella

word2vec
women, men, males,
mens, boys, man, female,
teenagers,girls,ladies

Table 7. Similar Words for “Men”
10 The

output from the PSL Stage I for BUR, is completely independent
of the other images. In essence, for each image, we are predicting all
relevant concepts from a large vocabulary given a few detections from a
small set of class-labels.

11

(0.029), plate rack, throne, wall clock, face powder,
binder, hair slide,velvet,puck, redbone.
3. hog (0.48), wallaby (0.19), wild boar (0.10), Mexican hairless (0.045), gazelle (0.023), wombat (0.017),
dhole (0.016), hyena (0.015), armadillo (0.009), ibex,
hartebeest, water buffalo, bighorn, kit fox, mongoose,
hare, wood rabbit, warthog, mink, polecat.
These predictions show that for the first and fourth image,
there are some animals detected with some distant visual
similarities. The second and third image has almost no animal mentions. This also shows some very confident detections (such as triceratops for the first image) is quite noisy.
In many cases, due to these high-confidence noisy detections, the PSL-based inference system gets biased towards
them. Compared to that, Clarifiai detections provide quite
a few (abstract but) correct detections about different aspects of the image (for example, for 2nd Image, predicts labels related to “cartoon/art” and “animal” both). This seems
to be one of the reasons, for which the current framework
provide better results for Clarifai Detections. Using Residual Network, the final output from the GUR system for the
“aardvark” riddle is: antelope, prairie dog, volcano rabbit,
marsupial lion, peccary, raccoon, pouch mammal, rabbit,
otter, monotreme, jackrabbit, hippopotamus, moose, tapir,
echidna, gorilla.

Figure 8. The accuracy histograms of the BUR, GUR and UR approaches (combined with the VB, RR and All stage variants).
ConceptNet
saurischian, ornithischian,
protobird, elephant bird,
sauropsid, cassowary,
ibis, nightingale, ceratosaurian,
auk, vulture

Visual Similarity
lambeosaurid, lambeosaur,
bird, allosauroid, therapod, stegosaur,
triceratops, tyrannosaurus rex,
deinonychosaur,dromaeosaur,
brontosaurus

word2vec
dinosaurs, dino, T. rex,
Tyrannosaurus Rex, T rex,
fossil, triceratops, dinosaur species,
tyrannosaurus,dinos,
Tyrannosaurus rex

Table 8. Similar Words for “Dinosaur”

E. More Positive and Negative Results
We provide positive and Negative results in Figures 9
and 10 of the ”GUR+All” variant of the pipeline. We obtain better results with Clarifai detections rather than Residual Network detections. Based on our observations, one of
the key property of the ResidualNetwork confidence score
distribution is that there are few detections (1-3) which are
given the strongest confidence scores and the other detections have very negligible confidence scores. These top detections are often quite noisy.
For example, for the aardvark image 1, the ResidualNetwork detections are: triceratops, wallaby, armadillo, hog,
fox squirrel, wild boar, kit fox, grey fox, Indian elephant, red
fox, mongoose, Egyptian cat, wombat, tusker, mink, Arctic
fox, toy terrier, dugong, lion. Only the first detection has
0.84 score and the rest of the scores are very negligible. For
the second, third and fourth images, the top detections are
respectively:
1. pick (0.236), ocarina (0.114), maraca (0.091), chain
saw (0.06), whistle (0.03), can opener (0.03), triceratops (0.02), muzzle, spatula, loupe, hatchet, letter
opener, thresher, rock beauty, electric ray, tick, gong,
Windsor tie, cleaver, electric guitar
2. jersey (0.137), fire screen (0.129), sweatshirt
(0.037), pick (0.035), comic book (0.030), book jacket
12

Figure 9. More Positive results from the “GUR” approach on some of the riddles. The groudtruth labels, closest label among top 10 from
GUR and the Clarifai baseline are provided for all images. For more results, check http://bit.ly/1Rj4tFc.

13

Figure 10. Some Negative results from the “GUR” approach on some of the riddles. The groudtruth labels, closest label among top 10 from
GUR and the Clarifai baseline are provided for all images. For more results, check http://bit.ly/1Rj4tFc.

14

“Add Another Blue Stack of the Same Height!”:
ASP Based Planning and Plan Failure Analysis
Chitta Baral1 and Tran Cao Son2
1

Department of Computer Science and Engineering, Arizona State University, Tempe, AZ
2
Department of Computer Science, New Mexico State University, Las Cruces, NM

Abstract. We discuss a challenge in developing intelligent agents (robots) that
can collaborate with human in problem solving. Specifically, we consider situations in which a robot must use natural language in communicating with human
and responding to the human’s communication appropriately. In the process, we
identify three main tasks. The first task requires the development of planners
capable of dealing with descriptive goals. The second task, called plan failure
analysis, demands the ability to analyze and determine the reason(s) why the
planning system does not success. The third task focuses on the ability to understand communications via natural language. We show how the first two tasks can
be accomplished in answer set programming.

1

Introduction

Human-Robot interaction is an important field where humans and robots collaborate to
achieve tasks. Such interaction is needed, for example, in search and rescue scenarios
where the human may direct the robot to do certain tasks and at times the robot may have
to make its own plan. Although there has been many works on this topic, there has not
been much research on interactive planning where the human and the robot collaborate
in making plans. For such interactive planning, the human may communicate to the
robot about some goals and the robot may make the plan, or when it is unable it may
explain why it is unable and the human may make further suggestions to overcome the
robot’s problem and this interaction may continue until a plan is made and the robot
executes it. The communication between the robot and the human happens in natural
language as ordinary Search and Rescue officials may not be able to master the Robot’s
formal language to instruct it in situations of duress. Following is an abstract example
of such an interactive planning using natural language communication.
Consider the block world domain in Figure 1 (see, [1]). The robot has its own blocks
and the human has some blocks as well. The two share a table and some other blocks
as well. Suppose that the human communicates to the robot the sentence “Add another
blue stack of the same height!.” Even if we assume that the robot is able to recognize the
color of the blocks, create, and execute plans for constructions of stack of blocks, such
a communication presents several challenges to a robot. Specifically, it requires that the
robot is capable of understanding natural language, i.e., it requires that the robot is able
to identify that
– the human refers to stacks of only blue blocks (blue stack);
– the human refers to the height of a stack as the number of blocks on the stack;

Fig. 1: A Simplified Version of The Blocks World Example from the BAA

– there is a blue stack of the height 2 on the table; and
– it should use its two blue blocks to build a new stack of two blue blocks.
It is easy to see that among the above points, the first three are clearly related to the
research in natural language processing (NLP) and the last one to planning, i.e., an
integration of NLP and planning is needed for solving this problem. Ideally, given the
configuration in Figure 1 and the reasoning above, the robot should devise a plan to
create a new stack using its two blue blocks and execute the plan. On the other hand, if
the robot has only one blue block, the robot should realize that it cannot satisfy the goal
indicated by the human and it needs to respond to the human differently, for example, by
informing the human that it does not have enough blue blocks or by asking the human
for permission to use his blue blocks.
As planning in various settings has been considered in answer set programming and
there have been attempts to translate NLP into ASP, we would like to explore the use
of ASP and related tools in solving this type of problems. Our focus in this paper is the
interactive planning problem that the robot needs to solve.

2

ASP Planning for “Add Another Blue Stack of the Same Height!”

What are the possible responses to the communication from the human “Add another
blue stack of the same height!” for the robot? Assume that the robot can recognize
that it needs to create and execute a plan in responding to the communication. We next
describe an ASP implementation of an ASP planning module for the robot. Following
the literature (e.g., [2]), this module should consist of the planning domain and the
planning problem.

2.1

The Planning Domain

The block world domain Db can be typically encoded by an ASP-program as follows:
– blocks are specified by the predicate blk(.);
– actions such as putontable(X) or put(X,Y), where X and Y denote blocks, are
defined using ASP-rules;
– properties of the world are described by fluents and encoded using predicates—
similar to the use of predicates in Situation Calculus—whose last parameter represents the situation term or an time step in ASP. For simplicity, we use two fluents
on(X,Y,T) and ontable(X,T) which denote Y is on X or X is on the table, respectively, at the time point T;

– Rules encoding executability conditions and effects of actions are also included, for
example, to encode the executable condition and effects of the action putontable(X),
we use the the rules
executable(putontable(X),T):-time(T),action(putontable(X)),clear(X,T).
ontable(X,T+1):-time(T),occ(putontable(X),T).
ontable(X,T+1):-time(T),ontable(X,T),#count{Z:occ(put(Z,X),T)}==0.

Db can be used for planning [2]. To use Db in planning, we need the following
components: (i) rules for generating action occurrences; (ii) rules encoding the initial
state; and (iii) rules checking for goal satisfaction.

2.2

What Are We Planning For? and How Do We Plan For It?

Having defined Db , we now need to specify the planning problem. The initial state can
be easily seen from the figure. The question is then what is the goal of the planning
problem. Instead of specifying a formula that should be satisfied in the goal state, the
sentence describes a desirable state. This desirable state has two properties: (i) it needs
to have the second (or a new) blue stack; and (ii) the new blue stack has the same height
as the existing blue stack. In other words, the goal is descriptive. We will call a planning
problem whose goal is descriptive as a descriptive planning problem.
In the rest of this subsection, we will present an encoding of the planning problem
for “Add another blue stack of the same height!.” First, we need to represent the goal.
Considering that a stack can be represented by the block at the top, the goal for the
problem can be represented by a set of goal conditions in ASP as follows.
g_cond(S,is,stack):-blk(S).
g_cond(S,type,another):-blk(S).
g_cond(S,color,blue):-blk(S). g_cond(S,height,same):-blk(S).

These rules state that the goal is to build a stack represented by S, the stack is blue,
has the same height, and it is a new stack. Note that these rules are still vague in the
sense that they contain unspecified concepts, e.g., “what is a stack?,” “what does it
mean to be the same height?,” etc.
The problem can be solved by adding rules to the block program Db to define when
a goal condition is satisfied. Afterwards, we need to make sure that all goal conditions
are satisfied. As we only need to obtain one new stack, the following rules do that:
not_sat_goal(S,T):-blk(S),g_cond(X,Y,Z), not satisfied(X,Y,Z,T).
sat_goal(S, T)
:-not not_sat_goal(T).
:- X = #count {S : sat_goal(S, n)}, X ==0.

The first two rules define blocks that are considered to satisfy a goal condition. The
last rule ensures that at least one of the blocks satisfies all four goal conditions.
We now need rules defining when different kinds of goal conditions are satisfied.
For simplicity, let us consider stacks to have at least one block. In that case we have the
following rule defining when a block is a stack.
satisfied(S,is,stack,T):-blk(S),time(T),clear(S,T).

We next define that the color of a stack at time point T is blue if all blocks in that
stack at that time point are blue. The next rule simply says that a stack is blue if its top
is blue and all blocks below the top are also blue.

satisfied(S,color,blue,T):-blk(S),time(T),color(S,blue),clear(S,T),
#count{U:above(U,S,T), not color(U,blue)}==0.

Defining satisfied(S,height,same,T) is less straightforward. First, we define
a predicate same height(S,U,T) meaning that in the context of time step T, S and U
have the same height (or two stacks represented by S and U have the same height). We
then use this predicate to define satisfied(S,height,same,T). Note that the definition of the predicate satisfied(S,height,same,T) also enforces the constraint
that the blue stack that is compared to the stack represented by S does not change.
same_height(S,U,T):-blk(S),blk(U),S!=U,ontable(S,T),ontable(U,T).
same_height(S,U,T):-blk(S),blk(U),S!=U,on(S1,S,T),on(U1,U,T),
same_height(S1, U1, T).
satisfied(S,height,same,T):- satisfied(S,is,stack,T), S!=U,
satisfied(U,is,stack,T),satisfied(U,color,blue,T),
unchanged(U,T),clear(S,T),clear(U,T),same_height(S,U,T).

Similarly, defining satisfied(S, type, another,T) is also not straightforward. Intuitively, it means that there is another stack U different from S and U has not
changed over time. We define it using a new predicate unchanged(U,T) which means
that the stack with U at the top has not changed over time (from step 0 to step T).
satisfied(S,type,another,T):-blk(U),unchanged(U,T),same_height(S,U,T).
unchanged(U,T):- time(T),blk(U),not changed(U,T).
changed(U,T) :- time(T),T>0,blk(U),above(V,U,0), not above(V,U,T).
changed(U,T) :- time(T), T > 0, blk(U),ontable(U,0), not ontable(U,T).
changed(U,T) :- time(T), T > 0, blk(U),not ontable(U,0), ontable(U,T).
changed(U,T) :- time(T), T > 0, blk(U),not ontable(U,0), on(X,U,T).

Let us denote with Gb the collection of rules developed in this section. To compute
a plan, we will only need to add the initial state and the rules described above for
enforcing that all goal conditions must be satisfied. Initializing the initial situation as
0 (Figure 1), the description of who has which blocks and what is on the table can be
expressed through the following facts, denoted with Ib :
has(human,b1,0). color(b1,red). has(human,b2,0). color(b2,brown).
has(human,b3,0). color(b3,brown).has(human,b4,0). color(b4,green).
has(human,b5,0). color(b5,blue). has(robot,b6,0). color(b6,red).
has(robot,b7,0). color(b7,red). has(robot,b8,0). color(b8,blue).
has(robot,b9,0). color(b9,blue). has(robot,b10,0).color(b10,green).
has(robot,b11,0).color(b11,green).ontable(b12,0). color(b12,red).
ontable(b13,0). color(b13,blue).on(b13,b14,0). color(b14,blue).

It is easy to see that the program Db ∪ Ib ∪ Gb with n=2 has two answer sets,
one contains the atoms occ(putontable(b8),0), occ(put(b8,b9),1) that corresponds to a possible plan putontable(b8),put(b8,b9) for the robot; the other
one corresponds to the plan putontable(b9),put(b9,b8).

2.3

What If Planning Fails?

The previous subsection presents an initial configuration in which the robot can generate
a plan satisfying the request from the human. Obviously, there are several configurations

of the block world in which the planning generation phase can fail. What should be the
robot’s response? What is the ground for such response? Let us consider one of such
situations.
Assuming a different initial configuration in which the robot has only one blue
block, say b8 (or b9). Let Ibf be the new initial situation. Furthermore, we require that
the robot cannot use the block belonging to the human if it does not get the human’s
permission. In this case, the robot will not be able to add another blue stack of the same
height to the table because there is no plan that can satisfy this goal. More specifically,
it is because the robot does not have enough blue blocks for the task at hand. This is
what the robot should respond back to the human user. We next discuss a possible way
for the robot to arrive at this conclusion.
First, the encoding in the previous subsection must be extended to cover the requirement that the robot can only use its own blocks or the blocks on the table in the
construction of the new stack. This can be achieved with the following rules:
available(X, T) :- time(T),blk(X),has(robot,X,T).
available(X, T) :- time(T),blk(X),ontable(X, T),clear(X,T).
available(X, T) :- time(T),blk(X),above(Y,X,T),clear(X,T).
:-occ(putontable(X),T),not available(X,T).
:-occ(put(Y,X),T),not available(X,T).

Let Dbf be Db unions with the above rules. It is easy to see that, for any constant
n, the program Dbf ∪ Ibf ∪ Gb does not have an answer set. When the planning fails,
the robot should analyze the situation and come up with an appropriate response. A
first reasonable step for the robot is to identify why the planning fails. As it turns out,
the program described in the previous subsection only needs only minor changes to
accomplish this task. First, we need to modify the goals as follows.
g_cond(S,is,stack):-blk(S),ok(1). g_cond(S,height,same):-blk(S),ok(3).
g_cond(S,color,blue):- blk(S),ok(2). g_cond(S,type,another):-blk(S),ok(4).
{ok(1..4)}4. ngoals(X):-X=#count{I:ok(I)}. #maximize {X:ngoals(X)}.

Let Gbf be the new goal, obtained from Gb by replacing the rules defining goal cond(.)
with the above rules. It is easy to check that every answer set of the program Dbf ∪
Ibf ∪ Gbf does not contain the atom ok(2) which indicates that the goal condition
goal cond(S,color,blue) cannot be satisfied. As such, the robot should use the
missing goal condition as a ground for its response. However, the robot could use this
information in different ways. For example, it can tell the human that it does not have
enough blue blocks or it could ask the human for permission to use the human’s blue
blocks to complete the goal. It means that the robot needs the ability to make assumptions and reason with them. This can be defined formally as follows.
Definition 1. A planning problem with assumptions is a tuple P = (D, I, G, AP, AF )
where (D, I, G) is a planning problem, AP is a set of actions, and AF is a set of
fluents. We say that P needs a plan failure analysis if (D, I, G) has no solution.
Intuitively, AP is the set of actions that the robot could execute and AF is a set of
assumptions that the robot could assume. For example, for our running example, AP

could contain the logic program encoding of an action ask(blue) whose effect is that
the robot can use the blue block from the human; AF could be {has(robot,blx),
color(blx,blue)}. So, a planning problem with assumption for the robot is Pbf =
(Dbf , Ibf , Gb , {ask(blue)}, {has(robot, blx), color(blx, blue)}).
Definition 2. A plan failure analysis for a planning problem with assumptions P =
(D, I, G, AP, AF ) is a pair (A, F ) such that A ⊆ AP , F ⊆ AF , and the planning
problem (D ∪A, I ∪F, G) is solvable. (A, F ) is a preferred plan failure analysis if there
exists no analysis (A0 , F 0 ) such that A0 ( A or F 0 ( F .
It is easy to see that Pbf = (Dbf , Ibf , Gb , {ask(blue)}, {has(robot, blx), color(blx, blue)})
has two preferred plan failure analyses; one, (∅, {has(robot,blx),color(blx,blue)}),
tells the robot that it does not have enough blue blocks; another one ({ask(blue)}, ∅)
tells the robot to ask for permission to use the human’s blue blocks.
To compute a preferred plan failure analysis, we can apply the same method used
in identifying a minimal set of satisfying goal conditions. We assume that for each
action act in AP , AP consists of the rules defines its effects and a declaration of the
form is ap(act). For each fluent l in AF , we assume that AF contains a declaration
is af(l) as well as the rules for describing how l changes when actions are executed.
Let Da be the following set of rules:
{action(A) : is_ap(A)}.
a_assume(A):- action(A),is_ap(A).
{assume(L) : is_af(L)}.
L@0:-assume(L).
nl_assume(F):-F=#count{L:assume(L)}.
na_assume(Y):-Y=#coun {A:a_assume(A)}.
#minimize {1@1,F:nl_assume(F)}. #minimize {1@1,A:na_assume(A)}.

The first rule says that the robot can assume any action in AP . Any action that is
assumed will be characterized by the predicate a assume(.) (the second rule). The
third rule says that the robot can assume any fluent in AF . L@0 represents the fact that
L is true at the time 0. The rest of the rules minimizes the number of actions and the
number of assumed fluents, independent from each other. We can show that the new
program returns possible preferred plan failure analyses of the problem.

3

Conclusions and Future Work

We describe two problems related to planning that need to be addressed for an effective
collaboration between an intelligent system and human when communication via natural language is necessary. We show that ASP based approaches can be employed to deal
with these two problems. Our discussion shows that ASP can play an important role
in the development of intelligent systems that can interact with human via natural language. Our future goal is to develop further applications that integrate various aspects
of AI including NLP, Ontologies and Reasoning by using KPARSER that has been used
to address the Winograd Schema Challenge [3].

References
[1] DARPA. Communicating with Computers (CwC), 2015.
[2] V. Lifschitz. Answer set programming and plan generation. AIJ, 138(1–2):39–54, 2002.
[3] A. Sharma, S. Aditya, V. Nguyen, and C. Baral. Towards Addressing the Winograd Schema
Challenge - Building and Using Needed Tools. In Proceedings of IJCAI, 2015.

Pathway Specification and Comparative Queries: A High Level Language with
Petri Net Semantics
Saadat Anwar and Chitta Baral
CIDSE, Arizona State University, Tempe, AZ, USA

Abstract
Understanding biological pathways is an important activity in the biological domain for drug development.
Due to the parallelism and complexity inherent in pathways, computer models that can answer queries about
pathways are needed. A researcher may ask ‘what-if’
questions comparing alternate scenarios, that require
deeper understanding of the underlying model. In this
paper, we present overview of such a system we developed and an English-like high level language to express
pathways and queries. Our language is inspired by high
level action and query languages and it uses Petri Net
execution semantics.

Introduction
Biological pathways are highly interconnected networks
of biochemical processes. These processes execute autonomously in parallel, driven by natural constraints of ingredient supply and demand, i.e., a process can execute
as soon as its preconditions are satisfied. For example, a
metabolic reaction can occur as soon as sufficient quantities of its ingredients become available. Many processes are
also governed by additional preconditions, including availability of additional substances that are not ingredients, substance gradients, inhibition, and stimulation. Collectively,
these preconditions regulate the reactions through feedback
loops and feed-forwards in the network. Reactions execute at
different speeds, generating products on completion, which
become ingredients for the down-stream reactions. The state
of a pathway is defined by the available substance quantities, and the chain of reactions between a starting state and
an ending state of a pathway define a trajectory of pathway’s state evolution. The interplay between limited quantity of ingredients and other preconditions can present multiple choices for alternate trajectory evolutions at each pathway state.
Understanding these pathways is of fundamental importance in biological research for disease diagnosis and drug
development. However, the aforementioned complexities
make it difficult for one person to retain all aspects of the
pathway. As a result, computer based systems are needed to
c 2014, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

represent these pathways, allowing biologists to pose queries
against them. An important class of questions in this regard
are the so called ‘what-if’ questions, which compare alternate scenarios of a pathway. We find such questions in college level books that a professor may ask his students to
gauge their understanding of a pathway. For example the
following question from (Reece et al. 2010) appeared in a
recent knowledge representation and reasoning challenge 1 :
Question 1. “At one point in the process of glycolysis, both
DHAP and G3P are produced. Isomerase catalyzes the reversible conversion between these two isomers. The conversion of DHAP to G3P never reaches equilibrium and G3P is
used in the next step of glycolysis. What would happen to the
rate of glycolysis if DHAP were removed from the process of
glycolysis as quickly as it was produced?”
Answering such questions require simulating different
scenarios and reasoning with the results. For example, question 1 asks for comparison of the rate of glycolysis between
the nominal pathway and an alternate pathway in which
DHAP is removed as quickly as it is produced.
In this paper we describe an English-like high level language to express pathways and queries. We highlight important features of its syntax and semantics and give an
overview of an implementation that understands this language and answer questions about them. Our language is
inspired by high level action and query languages such
as (Gelfond and Lifschitz 1993; Giunchiglia and Lifschitz
1998; Lee, Lifschitz, and Yang 2013). However, compared
to most existing action languages, which describe transition systems (Gelfond and Lifschitz 1998) 2 , our language describes trajectories. Our language is geared towards modeling natural systems, in which actions occur
autonomously (Reiter 1996) when their pre-conditions are
satisfied; and substance quantities do not become negative.
Compared to most other action languages, substance quantities produced and consumed are additive 3 . Our system also
supports a richer query component, which is missing from
most query languages that accompany action languages.
1

https://sites.google.com/site/2nddeepkrchallenge/
Some languages like C+ (Giunchiglia et al. 2004) allow autonomous actions, but their query languages lack expressiveness.
3
Although some languages like C+ have been extended to allow
additive fluents (Lee and Lifschitz 2003).
2

Some aspects of our system are similar to (Baral et al. 2004),
but their work is limited to signaling pathways, does not allow numeric quantities or has provision of loops inherent in
biological pathways.
We use Petri Nets (Peterson 1977) for representing pathways as they mimic biological pathway diagrams and can
represent parallel systems. For ease of reasoning and extensibility, we use Answer Set Programming (ASP) (Baral
2003) for simulating the pathway using the approach
by (Anwar, Baral, and Inoue 2013a; 2013b).
The rest of the paper is organized as follows: we present
important aspects of our high-level language syntax and semantics. Then we illustrate the use of our language with an
example. After that, we briefly describe implementation of
our system that understands this language and conclude with
main contributions.

f change value by e

(6)

A guard cond clause can take the forms:

Description of our system
We present the key elements and discriminating factors of
our high level language below.

f has value w or higher
f has value lower than w

(7)
(8)

The domain description contains statements of the forms:

Pathway Specification Language
The alphabet of pathway specification language P consists
of disjoint nonempty domain-dependent sets A, F , representing actions, and fluents, respectively; a fixed set S of
firing styles; a fixed set K of keywords as syntactic sugar
(shown in bold face); a fixed set of punctuations {‘, ’}; and a
fixed set of special constants {‘1’, ‘∗’, ‘max’}; and integers.
Each fluent f ∈ F has a domain dom(f ) which is either
integer or binary and specifies the values f can take. A state
s is an interpretation of F that maps fluents to their values.
We write s(f ) = v to represent “f has the value v in state s”.
States are indexed, such that consecutive states si and si+1
represent an evolution over one time step from i to i + 1
due to firing of an action set Ti in si . We illustrate the role
of various constructs using a hypothetical example from the
biological domain 4 :
domain of sug is integer, f ac is integer,
acoa is integer, h2o is integer
box may execute causing f ac change value by -1,
acoa change value by +1
if h2o has value 1 or higher
inhibit box if sug has value 1 or higher
initially sug has value 3, f ac has value 4,
acoa has value 0, h2o has value 0

1 unit of fatty-acids available. Line (3) describes an explicit
precondition (or guard) of beta oxidation, i.e. it cannot occur
unless there is at least 1 unit of water available. The water is,
however, not consumed during beta oxidation. Line (4) explicitly inhibits beta-oxidation when there is any sugar available; and line (5) sets up the initial conditions of the pathway, s.t. initial quantities of sugar, fatty-acids, acetyl-CoA,
and water are 3,4,0,0, respectively.
Now we introduce various statements and clauses, give
their intuitive definitions, and show how they are combined
to construct a pathway specification (or domain description).
In the following description, f is a fluent, a is an action, w ∈
N+ ∪ {0}, d ∈ N+ , S ∈ {1, ∗, max}, e ∈ (N \ {0}) ∪ {∗}
for integer or e ∈ {1, −1, ∗} for binary fluents.
An effect clause has the form:

(1)
(2)
(3)
(4)
(5)

Above query is about a process called beta-oxidation represented by ‘box’, the effect of which is captured in lines
(2)-(3). Line (1) declares fluents for the substances used in
the pathway and their domain, i.e. sugar (‘sug’), fatty-acids
(‘f ac’), acetyl-CoA (‘acoa’), and water (‘h2o’) are represented by numeric fluents. Line (2) describes the effect of
beta oxidation on its inputs and outputs, i.e. when betaoxidation occurs, it consumes 1 unit of fatty-acids and produces 1 unit of acetyl-CoA. It implicitly defines a precondition that beta-oxidation cannot occur unless there is at least
4
Multiple domain and initially statements have been written
in a comma-separated compact form.

domain of f is ‘integer0 |‘binary 0
a may execute causing effect 1 , . . . , effect m
if guard cond1 , . . . , guard condn
inhibit a if guard cond1 , . . . , guard condn
initially f has value w
a executes in d time units
firing style S

(9)
(10)
(11)
(12)
(13)
(14)

where m > 0 and n ≥ 0.
A fluent domain declaration statement (9) declares the
values a fluent f can take. While binary domain is commonly used for representing substances in a signaling pathway, metabolic pathways use positive numeric values. Since
the domain is for a physical entity, we disallow fluents with
negative values. A may-execute statement (10) captures the
pre-conditions of an action a and its impact. A single mayexecute statement must not have effect i , effect j with ei < 0
and ej < 0; or ei > 0 and ej > 0 for the same fluent.
An inhibit statement (11) captures explicit inhibition conditions for an action a. An initial condition statement (12)
captures the initial state of pathway (e.g. as substance distribution). A duration statement (13) represents the duration
d an action a takes to execute. A firing style statement (14)
specifies how many actions may execute in parallel, where,
S is either “1”, “∗”, or “max” for serial execution, arbitrary
amount of parallelism, and maximum parallelism. Actions
execute automatically when fireable, subject to the available
fluent quantities.
Definition 1 (Pathway Specification). A pathway specification is composed of one or more domain, may-execute,
inhibit, initially, duration statements, and one firing style
statement.
A pathway specification is consistent if (i) there is at most
one firing style statement (ii) at most one duration statement
for an action a; (iii) the guard cond1 , . . . , guard condn
from a may-execute are disjoint from any other may-execute

for the same action 5 ; (iv) domain of fluents, effects, conditions and numeric values are consistent, i.e., effects and conditions on binary fluents must be binary; and (v) the pathway
specification does not cause it to violate fluent domains by
producing non-binary values for binary fluents.
When missing, duration of an action is assumed to be 1;
and initial fluent quantity for a fluent is assumed to be 0.
Intuitively, a pathway specification D represents a set of
trajectories of the form: σ = s0 , T0 , s1 , . . . , sk−1 , Tk−1 , sk .
Each trajectory encodes an evolution of the pathway. Starting from an initial state s0 , each si , si+1 pair represents the
state evolution in one time step due to execution of the action set Ti in state si producing si+1 . An action set Ti is
only executable in state si , if the total decrease of fluent values due to ei < 0 and ei = ∗ will not result in any of the
fluents becoming negative. Changes to fluents due to ei > 0
for the action set Ti occur over subsequent time-steps depending upon the durations of actions involved. Thus, the
state si (fi ) is the sum of ei > 0 for actions of duration d
that occurred d time steps before (current time step) i, i.e.
a ∈ Ti−d .

Query Specification Language
The alphabet of query language Q consists of the same sets
A, F from P representing actions, and fluents, respectively;
a fixed set of reserved keywords K shown in bold in syntax below; a fixed set {‘ : ’, ‘; ’, ‘, ’, ‘0 ’} of punctuations;
a fixed set of {<, >, =} of directions; and constants. Our
query language asks questions about biological entities and
processes in a biological pathway described by a pathway
specification (or domain description). A query statement is
composed of a query description (the quantity, or property
being sought by the question), interventions (changes to the
pathway), observations (about states and actions of the pathway), and initial setup conditions. For example, the following query indirectly determines the direction of change in
the rate of glycolysis by comparing the rate of production of
‘bpg13’ w.r.t. the glycolysis pathway given in (Reece et al.
2010, Figure 9.9); and corresponds to question (1).
direction of change in average
rate of production of bpg13 is d
when observed between time step 0 and time step k;
comparing nominal with modified pathway obtained
due to interventions :
remove dhap as soon as produced;
using initial setup :
continuously supply f 16bp in quantity 1;
(15)

Intuitively, a query statement is evaluated against the trajectories of a pathway (or domain description). The pathway is
first modified by first applying the initial setup conditions,
and the interventions. The modified pathway is them simulated and its trajectories are filtered to retain only those
which satisfy the observations specified in the query statement. Next we define the syntax of the query language and

its elements 6 , give their intuitive meaning, and how these
components fit together to form a query statement. In the
following description, a’s are actions, f ’s are fluents, n’s are
numbers, q’s are positive integer numbers, d is one of the
directions from {<, >, =}.
hpointi ::= time step ts
(16)
hintervali ::= hpointi and hpointi
(17)
haggopi ::= minimum | maximum | average
(18)
hquant intf i ::= rate of production of f is n
(19)
| rate of firing of a is n
(20)
hquant ptf i ::= value of f is higher than n
(21)
| value of f is n
(22)
hqual intf i ::= f is accumulating
(23)
hqual ptf i ::= a occurs
(24)
| a does not occur
(25)
| a1 switches to a2
(26)
hquant agg intf i ::= haggopi rate of firing of a is n (27)
| haggopi rate of production of f is n
(28)
hquant agg ptf i ::= haggopi value of f is n
(29)
hquant cagg intf i ::= direction of change in
haggopi rate of production of f is d
(30)
| direction of change in
haggopi rate of firing of a is d
(31)
hquant cagg ptf i ::= direction of change in
haggopi value of f is d
(32)
hsimp intf i ::= hquant intfi | hqual intfi
(33)
hsimp ptf i ::= hquant ptfi | hqual ptfi
(34)
hint obsi
::= hsimp ptfi | hsimp ptfi at hpointi | hsimp intfi
(35)
| hsimp intfi when observed between hintervali (36)
hqdesci ::= hint obsi
(37)
| hint obsi in all trajectories
(38)
| hquant agg intfi
when observed between hintervali
(39)
| hquant agg ptfi when observed at hpointi
(40)
hcqdesci ::= hquant cagg intfi
when observed between hintervali
(41)
| hquant cagg ptfi when observed at hpointi
(42)
hintervi ::= remove f as soon as produced
(43)
| disable a
(44)
| continuously supply f in quantity q
(45)
| add delay of q time units in availability of f (46)
| set value of f to q
(47)
hicondi ::= (45) | (47)
(48)
hqstmti ::= hqdesci;
due to interventions : hintervi1 , . . . , hinterviN 1 ;
due to observations : hint obsi1 , . . . , hint obsiN 2 ;
using initial setup : hicondi1 , . . . , hicondiN 3 ;
(49)
6

5

Note that ‘f has value 5 or higher ’
‘f has value 7 or higher ’.

overlaps

with

Although some of our single-trajectory queries can be represented as LTL formulas, we have chosen to keep the current representation as it is more intuitive for our biological domain.

| hcqdesci; comparing nominal pathway with
modified pathway obtained
due to interventions :hintervi1 , . . . , hinterviN 1 ;
due to observations :hint obsi1 , . . . , hint obsiN 2 ;
using initial setup :hicondi1 , . . . , hicondiN 3 ;
(50)

While, a comparative query statement asks whether a query
description holds when a nominal pathway is compared to a
modified pathway, subject to same initial setup, but interventions and observations only applied to the modified pathway.

Given a pathway specification (domain description) P with
trajectories of the form σ = s0 , T0 , s1 , . . . , sk−1 , Tk−1 , sk ,
where each si , Ti , si+1 is an evolution from state si to state
si+1 due to firing of Ti . Intuitively, a point is a time-step and
interval defines a continuous range of time-steps on a trajectory, then point formulas (represented by ptf ) are evaluated
w.r.t. a point on the trajectory, while interval formulas (represented by intf ) are evaluated w.r.t. an interval on a trajectory, e.g. rate of production of f over interval [i, j] is given
by (sj (f ) − si (f ))/(j − i). Quantitative formulas (represented by quant) are evaluated for some quantity n, while
qualitative formulas (represented by qual) are evaluated for
some qualitative attribute of a state / trajectory. Aggregate
quantitative formulas (represented by quant agg) are evaluated for some quantity n, which is an aggregate of quantities
n1 , . . . , nm for trajectories σ1 , . . . , σm , e.g. the average aggregate is computed as n = (n1 + · · · + nm )/m. Comparative quantitative formulas (represented by quant cagg)
are evaluated for some direction of change between aggregate quantities n (over trajectories σ1 , . . . , σm ) and n0 (over
0
trajectories σ10 , . . . , σm
0 ), e.g. the change direction is ‘>’ if
0
n > n.
Intuitively, an internal observation (represented by
int obs) is a simple point formula that holds at any point on
a trajectory, a simple point formula that holds at a specific
point, a simple interval formula that holds over any interval
on a trajectory, or a simple interval formula that holds over
a specific interval. Intuitively, an internal observation filters
the set of trajectories produced by a pathway specification.
Intuitively, a query description (represented by qdesc) is
one of the possible point formulas that holds at a specific
point, an interval formula that holds over a specific range,
an internal observation, an internal observation over all trajectories in a set of given trajectories. A comparative query
description (represented by cqdesc) is one of the quantitative comparative aggregate point formula at a specific point
on two sets of trajectories, or a quantitative comparative aggregate interval formula over a specific interval on two sets
of trajectories. Intuitively, a query description or a comparative query description specifies the property that we want to
have hold true over the trajectories of the domain pathway.
Intuitively, an intervention (represented by interv) specifies a modification to the pathway specification, e.g. intervention (43) modifies the pathway such that all quantity of
f is removed as soon as it is produced.
Intuitively, a query statement (represented by qstmt) is
a comparative query statement if it contains a comparative
query description, and non-comparative otherwise. A query
statement is composed of a query statement, interventions to
the pathway, internal observations to filter the trajectories,
and initial conditions. Intuitively, a query statement asks
whether a query description holds in a pathway, after modifying it with initial setup, interventions and observations.

The semantics of our pathway are given by a Guarded-Arc
Petri Net, which allows choice between different effects
(arc-sets) of a transition, such that only one effect (arc-set)
is active for a transition in a given state determined by its
arc-guard 7 .

Pathway Semantics

Definition 2 (Guard). A guard condition takes one of the following forms: (f < v), (f ≤ v), (f > v), (f ≥ v), or (f =
v), where f is a fluent; and v is a fluent or a numeric constant. A guard is a propositional formula of guard conditions, with each guard condition treated as a proposition.
An interpretation of a guard G is a possible assignment
of a value to each fleuent f ∈ G from the domain of f . A
guard G is satisfied w.r.t. a state s, written s |= G iff G has
an interpretation in which each of its fluents f has the value
s(f ) and G is true.
Definition 3 (Guarded-Arc Petri Net). A Guarded-Arc Petri
Net is a tuple P N G = (P, T, G, E, R, W, D, T G, L):
P is a finite set of places
T is a finite set of transitions
G is a set of guards as defined in definition (2)
T G : T → G are the transition guards
E ⊆ (T × P × G) ∪ (P × T × G) are the guarded arcs
R ⊆ P × T × G are the guarded reset arcs
W : E → N+ are arc weights
D : T → N+ are the transition durations
L : P → N+ specifies maximum tokens for each place
subject to constraints: (i) P ∩ T = ∅ (ii) R ∩ E = ∅ (iii) Let
t ∈ T be a transition, and ggt = {g : (t, p, g) ∈ E} ∪
{g : (p, t, g) ∈ E} ∪ {g : (p, t, g) ∈ R} be the set of arcguards for normal and reset arcs connected to it, then any
two distinct guards g1 ∈ ggt , g2 ∈ ggt must not have an
interpretation that makes both g1 and g2 true.
We make a simplifying assumption that all places are
readable by using their place names. Execution of the P N G
occurs in discrete time steps. The marking (or state) of a
Guarded-Arc Petri Net P N G is the token assignment of
each place pi ∈ P . Initial marking is given by M0 : P →
N0 , while the token assignment at step k is written as Mk .
Next we define the execution semantics of P N G . We
start with terminology used below. Let (i) s0 = M0 represent the the initial marking (or state), sk = Mk represent the marking (or state) at time step k, (ii) sk (p) represent the marking of place p at time step k, such that
sk = [sk (p0 ), . . . , sk (pn )], where P = {p0 , . . . , pn } (iii) Tk
7

Our model is similar to the model in (Jensen, Kristensen, and
Wells 2007) in many aspects with differences in certain key semantics related to biological modeling, such as reset arts.

be the firing-set that fired in step k, (iv) enk be the set of enabled transitions in state sk , (v) delk (p, {t1 , . . . , tn }) be the
sum of tokens that will be consumed from place p if transitions t1 , . . . , tn fired in state sk , (vi) overck ({t1 , . . . , tn })
be the set of places that will have over-consumption of tokens if transitions t1 , . . . , tn were to fire simultaneously
in state sk , (vii) selk (f s) be the set of possible firing-set
choices in state sk using fs firing style (viii) addk (p) be
the total production of tokens in place p (in state sk ) due
to actions terminating in state sk , (ix) sk+1 be the next state
computed from state sk due to firing transition-set Tk Then,
the execution semantics of the Guarded-Arc Petri Net P N G
starting from state s0 using firing-style fs is given as follows:
enk = {t : t ∈ T, sk |= T G(t), ∀(p, t, g) ∈ E,
(sk |= g, sk (p) ≥ W (p, t, g))}
delk (p, {t1 , . . . , tn }) =
X
W (p, ti , g) : (p, ti , g) ∈ E, sk |= g
X

(52)

|=
average rate of production of f is r

if ∃[r1 , . . . , rm ] : (hs1i , σ1 i, j) |= rate of production
of f is r1 . . . (hsm
i , σm i, j) |= rate of production
of f is rm and r = (r1 + · · · + rm )/m
(53)
n
o 
1
m̄
{hs1i , σ1 i, . . . , hsm
i , σm i}, {hs̄i , σ̄1 i, . . . , hs̄i , σ̄m̄ i} , j

sk (p) : (p, ti , g) ∈ R, sk |= g

i=1,...,n

overck ({t1 , . . . , tn }) = {p : p ∈ P,
sk (p) < delk (p, {t1 , . . . , tn })}
selk (1) = {{ss} : ss ∈ enk , overck ({ss}) = ∅}
selk (∗) = {ss : ss ∈ 2enk , overck (ss) = ∅}
selk (max) = {ss : ss ∈ 2enk , overck (p, ss) = ∅,

|= direction of change in average rate of
production of f is d
if ∃ n1 : ({hs1i , σ1 i, . . . , hsm
i , σm i}, j) |= average rate
of production of f is n1 and
∃ n2 : ({hs̄1i , σ̄1 i, . . . , hs̄m̄
i , σ̄m̄ i}, j) |= average rate
of production of f is n2 and n2 d n1
(54)

(@ss0 ∈ 2enk : ss ⊂ ss0 , overck (ss0 ) = ∅)}
Tk ∈ selk (fs)
X
addk (p) =
W (ti , p, g)

n
o
1
m̄
{hs10 , σ1 i, . . . , hsm
0 , σm i}, {hs̄0 , σ̄1 i, . . . , hs̄0 , σ̄m̄ i}

j=0,...,k

: (ti , p, g) ∈ E, ti ∈ Tj , D(ti ) + j = k + 1
sk+1 (p) = min(sk (p) − delk (p, Tk ) + addk (p), L(p))

(hsi , σi, j) |= rate of production of f is n
if n = (sj (f ) − si (f ))/(j − i)
({hs1i , σ1 i, . . . , hsm
i , σm i}, j)

i=1,...,n

+


i in trajectories σ1 , . . . ,	σm ; {hs1i , σ1 i, . . . , hsm
i , σm i},
1
m̄
{hs̄i , σ̄1 i, . . . , hs̄i , σ̄m̄ i} |= F represent F holds at
point i in both sets {σ1 , . . . , σm } and {σ̄1 , . . . , σ̄m̄ };
(hsi , σi, j) |= F represent F holds over interval [i, j] in
σ; ({hs1i , σ1 i, . . . , hsm
i , σm i}, j) |= F represent
 1 F holds
over interval [i, j] in σ1 , . . . , σm ; and
(
{hsi , σ1 i, . . . ,
	
1
m̄
hsm
,
σ
i},
{hs̄
,
σ̄
i,
.
.
.
,
hs̄
,
σ̄
i}
,
j)
|=
F represent
m
1
m̄
i
i
i
F holds over interval [i, j] over two sets {σ1 , . . . , σm } and
{σ̄1 , . . . , σ̄m̄ }. Then the semantics of a rate query are given
as follows:

(51)

Definition 4 (Trajectory). σ = s0 , T0 , s1 , . . . , sk−1 , Tk−1 ,
sk is a trajectory of P N G iff given s0 = M0 , each Ti is
a possible firing-set in si whose firing produces si+1 per
P N G ’s execution semantics in (51).

Query Semantics
First we give the semantics of domain modification due to
an intervention by examples. Consider intervention (43),
we create the modified domain description D0 = D 
(remove f as soon as produced) as follows:
D0 = D + {tr may execute causing f change value by ∗}

Applying intervention (45) D0 = D  (continuously
supply f in quantity q) results in the following
changes:
D0 = D + {tf may execute causing
f change value by + q}

Next we define the semantics of some common formulas and observation using LTL-style. Let σ = s0 , T0 , s1 ,
. . . , Tk−1 , sk represent a trajectory of domain D with initial
marking s0 . Let actions Ti firing in state si be observable in
si such that Ti ⊆ si .
Let hsi , σi |= F represent F holds at point i in σ;
{hs1i , σ1 i, . . . , hsm
i , σm i} |= F represent F holds at point

|= direction of change in average rate of
production of f is d when observed between
time step i and time step j
n
o 
1
m̄
if
{hs1i , σ1 i, . . . , hsm
i , σm i}, {hs̄i , σ̄1 i, . . . , hs̄i , σ̄m̄ i} , j
|= direction of change in haverage rate of
production of f is d

(55)

Definition 5. Let D be a domain description and Q be a
query statement (49) with query description U , interventions
V1 , . . . , V|V | , internal observations O1 , . . . , O|O| , and initial conditions I1 , . . . , I|I| . Let D1 ≡ D  I1  · · ·  I|I| 
V1 · · ·V|V | be the modified domain description constructed
by applying the initial conditions and interventions from Q
and σ1 , . . . , σm be its trajectories that satisfy O1 , . . . , O|O| .
Then, D satisfies Q (written D |= Q) if {σ1 , . . . , σm } |= U .
Definition 6. Let D be a domain description and Q be
a comparative query statement (50) with query description U , interventions V1 , . . . , V|V | , internal observations
O1 , . . . , O|O| , and initial conditions I1 , . . . , I|I| . Let D0 ≡
D  I1  · · ·  I|I| be the nominal domain description
constructed by applying the initial conditions from Q and
σ1 , . . . , σm be its trajectories. Let D1 ≡ D  I1  · · · 
I|I|  V1  · · ·  V|V | be the alternate domain description
constructed by applying the initial conditions and interventions from Q and σ̄1 , . . . , σ̄m̄ be its trajectories that sat-

isfy
Q (written D |= Q)
 O1 , . . . , O|O| . Then, D satisfies

if {σ1 , . . . , σm }, {σ̄1 , . . . , σ̄m̄ } |= U .
The following propositions follow from the above description.
Proposition 1. Let D be a domain description and Q be
a non-comparative query statement. Then, D |= Q iff
D0 |= Q0 where D0 ≡ D  I1 · · ·  I|I|  V1 · · ·  V|V |
and Q0 ≡ Q with I1 , . . . , I|I| , V1 , . . . , V|V | removed, where
I1 , . . . , I|I| are the initial conditions in Q and V1 , . . . , V|V |
are the interventions in Q.
Proposition 2. Let D be a domain description and Q be
a non-comparative query statement. Then, D |= Q iff
{σ1 , . . . , σn } |= Q0 , where σ1 , . . . , σn are trajectories of D0
that satisfy O1 , . . . , O|O| , D0 ≡ DI1 · · ·I|I| V1 · · ·V|V |
and Q0 ≡ Q with I1 , . . . , I|I| , V1 , . . . , V|V | , O1 , . . . , O|O|
removed; I1 , . . . , I|I| are the initial conditions in Q,
V1 , . . . , V|V | are the interventions in Q, and O1 , . . . , O|O|
are internal observations in Q.
Proposition 3. Let D be a domain description and Q
be a comparative query statement. Then, D |= Q iff
({σ1 , . . . , σn }, {σ10 , . . . , σn0 0 }) |= Q0 , where σ1 , . . . , σn
are trajectories of D0 , and σ10 , . . . , σn0 0 are trajectories
of D00 that satisfy O1 , . . . , O|O| , D0 ≡ D  I1 · · · 
I|I| , D00 ≡ D  I1 · · ·  I|I|  V1 · · ·  V|V | , Q0 ≡
Q with I1 , . . . , I|I| , V1 , . . . , V|V | , O1 , . . . , O|O| removed;
I1 , . . . , I|I| are the initial conditions in Q, V1 , . . . , V|V | are
the interventions in Q, and O1 , . . . , O|O| are internal observations in Q.
Intuitively, proposition 1 states that a domain description
D satisfies a (non-comparative) query statement Q whenever the modified domain description D0 constructed by applying interventions and initial conditions from Q to D satisfies the modified query statement Q0 constructed by removing interventions and initial conditions from Q. Proposition 2 states that a domain description D satisfies a (noncomparative) query statement Q whenever the trajectories
of the modified domain description D0 constructed by applying interventions and initial conditions from Q to D that
satisfy observations in Q, satisfy the modified query statement Q0 constructed by removing interventions, initial conditions, and observations from Q. Proposition 3 states that
a domain description D satisfies a comparative query statement Q whenever the comparison between trajectories of
the nominal domain description D0 constructed by applying
initial conditions from Q and the trajectories of the alternate domain description D00 constructed by applying interventions and initial conditions from Q that satisfy the observations in Q satisfy the query statement Q0 constructed by
removing interventions, initial conditions and observations
from Q.
Proof of the above propositions are based on the definitions 5,6 of satisfiability of a query statement Q by domain
D; the semantics of the query description U in the query
statement; and the structure of the query statements. The
forward direction is proven by showing that the trajectories
of the modified domain descriptions (constructed by apply-

ing initial conditions and interventions) filtered by the internal observations (as appropriate) satisfy the query description U in the query statement Q. The reverse direction is
proven by showing that one can select a domain description
D that, when modified through selected initial setup conditions and interventions represents the same trajectories as
D0 , and one can select observations that filter these trajectories to only those satisfied by the query description U . The
selected initial setup conditions, interventions, and the observations when added to Q0 give us Q.

Illustrative Example
We illustrate our high level language by applying it to question (1) and the relevant glycolysis pathway.
tr
dhap
t3

f16bp

t4
g3p

t5a

t5b

t6

2

bpg13

Figure 1: Petri Net for question 1.
The following pathway specification encodes the domain
description D for question (1) and produces the PN in Fig. 1
minus the tr, t3 transitions:
domain of f 16bp is integer, dhap is integer,
g3p is integer, bpg13 is integer
t4 may execute causing f 16bp change value by -1,
dhap change value by +1, g3p change value by +1
t5a may execute causing dhap change value by -1,
g3p change value by +1
t5b may execute causing g3p change value by -1,
dhap change value by +1
t6 may execute causing g3p change value by -1,
bpg13 change value by +2
initially f 16bp has value 0, dhap has value 0,
g3p has value 0, bpg13 has value 0
firing style max

The question is asking for the direction of change in the rate
of glycolysis when the nominal pathway is compared against
a modified pathway in which DHAP is removed as soon as
it is produced. Since this rate can vary with the trajectory of
the world evolution, we consider the average change in rate.
Using domain knowledge (Reece et al. 2010, Figure 9.9), we
measure the rate of glycolysis indirectly by measuring the
rate of production of bpg13, a downstream product, which
is converted into equivalent quantity of the end product of
glycolysis. We also add continuous supply of source ingredient f 16bp at the rate of 1 unit per time step to prevent
starvation. This results in a query statement (15) Q for some
simulation length k. We evaluate it for the direction d using

our deep reasoning system, which gives us d =0 <0 , suggesting that the rate of glycolysis will be lowered when DHAP
is removed as soon as it is produced.

Implementation
We implemented a system 8 that understands a subset of
our high level language using Python as the driver as well
as the high level language parser; and Clingo (Gebser et
al. 2011) as the ASP solver. The system takes a pathway
specification, a query specification, and simulation parameters, such as simulation length and maximum possible tokens at any place for query evaluation. Our system uses
Petri Net semantics for modeling the biological pathway
and ASP for simulating it. Interventions in questions are
modeled as Petri Net extensions and translated into ASP
using the approach in (Anwar, Baral, and Inoue 2013a;
2013b). This gave us the translations of extensions such as
reset arcs, inhibit arcs (Peterson 1977), read arcs (Christensen and Hansen 1993), colored tokens (Peterson and others 1980), and timed transitions (Ramchandani 1974) etc.
We augmented the encoding further to include additional
extensions, such as conditional effects of actions, and more
generalized inhibitions supported by our language.
Next, we briefly summarize how our system processes
a non-comparative aggregate quantitative query statement,
then describe how the comparative quantitative query from
previous section is evaluated.
To evaluate a non-comparative query, the system builds a
Petri Net model from the pathway specification. It then applies the initial conditions and interventions from the query
statement to this model. The model is then translated into
ASP. The resulting ASP code is augmented with constraints
for (internal) observations. Answer sets of the augmented
code provide the filtered trajectories. Atoms needed to compute the quantity specified in the query (e.g. rate of production) are extracted from the answer sets and quantity values
aggregated across answer sets. The aggregated value is compared against any aggregate value provided for query statement satisfaction or the computed value is returned.
To evaluate comparative quantitative query statement
from the previous section is decomposed into two subqueries, Q0 for nominal case and Q1 for the modified case:
Q0 ≡ average rate of production of bpg13 is navg
when observed between time step 0 and time step k;
using initial setup :
continuously supply f 16bp in quantity 1;
Q1 ≡ average rate of production of bpg13 is n0avg
when observed between time step 0 and time step k;
due to interventions :
remove dhap as soon as produced;
using initial setup :
continuously supply f 16bp in quantity 1;

The queries are evaluated w.r.t. the same initial conditions
for average rates navg , n0avg that satisfy direction d in the
original query statement. The Petri Net model in Figure 1
shows nominal case (D0 ) in solid lines and the interventions
8

https://sites.google.com/site/deepqa2014/

added for the alternate case (D1 ) as dotted lines. The average results are compared using d to determine if they specify
the direction given in the comparative query statement.

Conclusion
We have presented a high level (English like) language for
specifying pathways and asking queries against them. Our
pathway specification language uses Petri Net semantics as
the simulation model and our query language is inspired by
action languages. Our query language is one of the main
contributions of this paper. It allows aggregate queries over
a set of trajectories, comparative aggregate queries over two
sets of trajectories, and interventions that are more general
than actions which can be used to modify the pathway as
specified in a query statement. Our approach improves on
the query languages associated with action languages, by
implementing comparative queries.
Though some aspects of our language may appear cumbersome to a person with background in action languages,
we retained the syntax to allow coherent description of a variety of complex interventions and queries.
We illustrated how an example question comparing alternate scenarios of a biological pathway is encoded in our
high level language; and summarized how our implementation performs query evaluation. We will elaborate on these
aspects in companion papers.
Our approach fits into a larger problem of representing
and reasoning about biological pathways gaining interest
lately, geared towards developing an end-to-end system that
extracts biological knowledge from texts, assembles it into
pathways, determines the right level of abstraction eliminating irrelevant details, and answers such questions about them
that require understanding of the inner workings of these
pathways; with the ability to pose questions in natural language and present results in a natural language or a visual
representation such as graphs. These aspects serve as guidelines for future extension to our work, and indeed there is
existing work on many of these, including from our group,
e.g., see (Tari et al. 2009; 2010) for pathway construction
from text extraction and translating natural language questions to formal queries that we will extend.
The importance of determining the right level of abstraction by eliminating irrelevant details is two fold. On one
hand it can improve performance by reducing problem size
and computational resources needed, while on the other, it
presents cleaner results that are easier to interpret by eliminating extraneous information. In this regard, techniques
from multi-scale modeling (see (Dada and Mendes 2011;
Heiner and Gilbert 2013)) could be applicable.
Additional areas of improvements include encapsulating
time information in the queries such that it is hidden from
the user, as well as the ability to apply interventions at arbitrary points during the simulation. Our current implementation uses Clingo, which handles discrete quantities only. We
intend to extend this work to include continuous real numbers based on work by (Lee and Meng 2013).

References
Anwar, S.; Baral, C.; and Inoue, K. 2013a. Encoding higher
level extensions of Petri nets in answer set programming.
In Cabalar, P., and Son, T., eds., Logic Programming and
Nonmonotonic Reasoning, volume 8148 of Lecture Notes in
Computer Science. Springer Berlin Heidelberg. 116–121.
Anwar, S.; Baral, C.; and Inoue, K. 2013b. Encoding Petri
nets in answer set programming for simulation based reasoning. TPLP 13(4-5-Online-Supplement).
Baral, C.; Chancellor, K.; Tran, N.; Tran, N.; Joy, A.; and
Berens, M. 2004. A knowledge based approach for representing and reasoning about signaling networks. Bioinformatics 20(suppl 1):i15–i22.
Baral, C. 2003. Knowledge Representation, Reasoning and
Declarative Problem Solving. Cambridge University Press.
Christensen, S., and Hansen, N. D. 1993. Coloured Petri
nets extended with place capacities, test arcs and inhibitor
arcs. Springer.
Dada, J. O., and Mendes, P. 2011. Multi-scale modelling and
simulation in systems biology. Integrative Biology 3(2):86–
96.
Gebser, M.; Kaminski, R.; Kaufmann, B.; Ostrowski, M.;
Schaub, T.; and Schneider, M. 2011. Potassco: The Potsdam
answer set solving collection. AICom 24(2):105–124.
Gelfond, M., and Lifschitz, V. 1993. Representing action
and change by logic programs. The Journal of Logic Programming 17(2):301–321.
Gelfond, M., and Lifschitz, V. 1998. Action languages.
Electronic Transactions on AI 3(16).
Giunchiglia, E., and Lifschitz, V. 1998. An action language based on causal explanation: Preliminary report. In
AAAI/IAAI, 623–630. Citeseer.
Giunchiglia, E.; Lee, J.; Lifschitz, V.; McCain, N.; and
Turner, H. 2004. Nonmonotonic causal theories. Artificial
Intelligence 153(1):49–104.
Heiner, M., and Gilbert, D. 2013. Biomodel engineering
for multiscale systems biology. Progress in biophysics and
molecular biology 111(2):119–128.
Jensen, K.; Kristensen, L. M.; and Wells, L. 2007. Coloured
Petri nets and CPN Tools for modelling and validation of
concurrent systems. International Journal on Software Tools
for Technology Transfer 9(3-4):213–254.
Lee, J., and Lifschitz, V. 2003. Describing additive fluents
in action language C+. In Proc. of IJCAI 2003.
Lee, J., and Meng, Y. 2013. Answer set programming modulo theories and reasoning about continuous changes. IJCAI’13.
Lee, J.; Lifschitz, V.; and Yang, F. 2013. Action language
BC: Preliminary report. In Proceedings of International
Joint Conference on Artificial Intelligence (IJCAI).
Peterson, J., et al. 1980. A note on colored Petri nets. Information Processing Letters 11(1):40–43.
Peterson, J. L. 1977. Petri nets. Computing Surveys
9(3):223–252.

Ramchandani, C. 1974. Analysis of asynchronous concurrent systems by Petri nets. Technical report, DTIC Document.
Reece, J.; Cain, M.; Urry, L.; Minorsky, P.; and Wasserman,
S. 2010. Campbell Biology. Pearson Benjamin Cummings.
Reiter, R. 1996. Natural actions, concurrency and continuous time in the situation calculus. KR 96:2–13.
Tari, L.; Hakenberg, J.; Gonzalez, G.; and Baral, C. 2009.
Querying a parse tree database of medline text to synthesize
user-specific biomolecular networks. Proc Pac Symp Biocomput 14:87–98.
Tari, L.; Baral, C.; Anwar, S.; Liang, S.; and Hakenberg,
J. 2010. Synthesis of pharmacokinetic pathways through
knowledge acquisition and automated reasoning. Pacific
Symposium on Biocomputing 15:465–476.

Exploring the KD45n Property of a Kripke Model After the Execution
of an Action Sequence
Tran Cao Son and Enrico Pontelli

Chitta Baral and Gregory Gelfond

Computer Science Department
New Mexico State University

Department of Computer Science and Engineering
Arizona State University

Abstract
The paper proposes a condition for preserving the KD45n
property of a Kripke model when a sequence of update models is applied to it. The paper defines the notions of a primitive
update model and a semi-reflexive KD45n (or sr-KD45n )
Kripke model. It proves that updating a sr-KD45n Kripke
model using a primitive update model results in a sr-KD45n
Kripke model, i.e., a primitive update model preserves the
properties of a sr-KD45n Kripke model. It shows that several update models for modeling well-known actions found
in the literature are primitive. This result provides guarantees that can be useful in presence of multiple applications of
actions in multi-agent system (e.g., multi-agent planning).

Introduction and Motivations
In a multi-agent action setting, agents need to not just
reason about the properties of the world, but also about
their knowledge and beliefs about the world and about other
agents’ own knowledge and beliefs. Among the various
formalizations of multi-agent actions and their impact on a
physical and mental world, the action models introduced in
(Baltag and Moss 2004; Baltag, Moss, and Solecki 1998)
and later extended to update models in (van Benthem, van
Eijck, and Kooi 2006; van Ditmarsch, van der Hoek, and
Kooi 2007) are the most widely accepted. For example,
these action models and update models1 have been employed in the study of epistemic planning problems in MAS
(Bolander and Andersen 2011; Löwe, Pacuit, and Witzel
2011; van der Hoek and Wooldridge 2002; Baral et al. 2013).
However, there is something fundamental missing in these
formulations. Let us consider a variant of the coin example
from (Baltag and Moss 2004): there are two agents, 1 and
2, and a coin in a box. The coin lies heads up, but the two
agents are unaware of this fact. Let us assume that agent
1 alone learns that the coin lies heads up (e.g., by peeking into the box while 2 is looking away). Intuitively, in
the resulting state, we should be able to conclude that the
following formulae are true (Ki ϕ and Bi ϕ represent that i
c 2014, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.
1
For the sake of simplicity, we will use the generic term “update model” to denote both “action models” and “update models.”
Furthermore, we will frequently use the terminology “execution of
an action” to indicate the result of applying an update model.

knows that ϕ is true (or i knows ϕ, for short) and i believes
that ϕ is true (i believes ϕ); h denotes the coin being heads
up): K1 h (1 knows h); ¬K2 h∧¬K2 ¬h (2 does not know
h); K1 (¬K2 h∧¬K2 ¬h) (1 knows that 2 does not know h);
B2 (¬K1 h∧¬K1 ¬h) (2 believes that 1 does not know h);
K1 (B2 (¬K1 h∧¬K1 ¬h)) (1 knows that 2 believes that 1
does not know h); etc. Intuitively, and as well accepted, the
distinction between knowledge and beliefs is that knowledge
must be true in the actual world while beliefs could be false.
But when one looks at the formulations in (Baltag and
Moss 2004; Baltag, Moss, and Solecki 1998) the Kripke
models have only one accessibility relation and only one
modality is defined; in (Baltag and Moss 2004) it is referred to as “knowledge”. As one can notice, replacing
B2 (¬K1 h ∧ ¬K1 ¬h) by K2 (¬K1 h ∧ ¬K1 ¬h) would not
be intuitive. Also, the action and update model papers do
not analyze the properties of the resulting Kripke models.
In this paper, we propose a method for drawing conclusions regarding both knowledge and beliefs of agents
(e.g., the above mentioned formulae) after the execution of
a sequence of actions, in the spirit of belief updates (and
update models) rather than the belief revision based approaches2 used in (Segerberg 1998; Baltag and Smets 2008;
van Benthem 2007; van Ditmarsch 2005). We achieve this
goal by noting that knowledge can be reduced to true belief
in certain logics (Halpern, Samet, and Segev 2009). To see
how this works, let us revisit the introductory example. The
initial state of beliefs of agents 1 and 2 can be represented by
the pointed Kripke model on the top-left of Fig. 1 (the actual
world has a double circle). The logic of this model, together
with the axiom Ki ϕ ↔ Bi ϕ ∧ ϕ (indicating knowledge is
true belief), has ¬Ki h∧¬Ki ¬h (i = 1, 2) as a consequence.
The update model representing “1 peeks into the box while 2
is looking away” is given in the bottom-left of Fig. 1 which
encodes the view of the action occurrence of each agent. Intuitively, it says that 1 sees the action occurs (denoted by the
event σ) while 2 does not (τ ). The result of the update is
the pointed Kripke model on the right. It is easy to check
that the logic specified by the result of the update (with the
axiom Ki ϕ ↔ Bi ϕ ∧ ϕ), entails the formulae in our initial
example (e.g., 1 knows h, 1 knows that 2 believes that 1 does
2
The belief revision based approach needs an additional ordering relation and is orthogonal to our approach.

not knows h, etc.).
1,2

s

h

1

1

(s,σ)

¬h

2

τ

2

1,2

1,2

(u,τ)
2

h

pre: true 1,2

pre: h

σ

1,2

u
1,2

¬h
1,2

h
(s,τ)

Figure 1: Knowledge as true belief
With our emphasis on distinguishing belief and knowledge, we also focus on what properties these modalities
have and how they are preserved when a sequence of actions is executed. A limited attention to this has been paid
in the literature—(Herzig, Lang, and Marquis 2005) being
an exception. The focus has been mostly on properties of
knowledge or beliefs of the agents after a single update—
e.g., (Baltag and Moss 2004) shows that all agents have the
correct knowledge about a formula ϕ after the execution of a
public announcement. On the other hand, the more general
question of whether the fundamental properties of beliefs
(i.e., KD45n ) and knowledge (i.e., S4.4n ) are preserved
by interesting classes of actions has been rarely touched.
In this paper, we provide a feasible approach for drawing conclusions about the knowledge and beliefs of agents
after the execution of an arbitrary sequence of actions, i.e.,
after updating a Kripke model by a sequence of update models. This is particularly important in applications such as in
security (e.g., exchanging a key between a group of agents
without revealing it to outsiders) and multi-agent planning
(e.g., executing an attack plan while making the enemy believe otherwise). To this end, we present a sufficient syntactic condition on update models and Kripke models under
which the KD45n property of the Kripke models is preserved after application of an update model. We observe
that this result could be viewed as an extension of the result
in (Halpern, Samet, and Segev 2009) to a dynamic setting.
We demonstrate that update models found in the literature
for representing several well-known actions satisfy the proposed sufficient condition. In the process, we identify some
critical issues that need to be taken into consideration when
update models are used.

Background
Logics of Knowledge and Belief. We consider the standard logics of knowledge and belief with a set of modalities L1 , . . . , Lk and use the notation from (Halpern, Samet,
and Segev 2009). We use a collection P of propositions to describe the properties that characterize the world.
LAG (L1 , . . . , Lk ) is the set of formulae defined as follows.
Each p ∈ P is a formula. If ϕ and ψ are formulae then so
are ¬ϕ, ϕ → ψ, and Li ϕ (i = 1, . . . , k). The connectives
∨, ∧, ↔ can be defined in terms of ¬ and →. An atomic
formula is a formula that does not contain a modal operator.
A logic Λ is a set of formulae in LAG (L1 , . . . , Lk ) that
(i) contains all propositional tautologies; (ii) is closed under
modus ponens; and (iii) is closed under substitution. A logic
is normal if it contains the axioms Li (ϕ → ψ) → (Li ϕ →

Li ψ), referred to as (KLi ) for i = 1, . . . , k, and is closed
under generalization, i.e., if it contains ϕ, then it will also
contain Li ϕ. A logic generated by a set A of formulae (axioms) is the smallest normal logic containing A. For two sets
of axioms Λ1 and Λ2 , Λ1 + Λ2 is the smallest normal logic
containing Λ1 and Λ2 .
Let AG = {1, 2, . . . , n} be a set of n agents. Each
agent i is associated with a belief operator Bi and a knowledge operator Ki . Our focus is the logic of belief, called
KD45n , over the language LAG (B1 , . . . , Bn ) that is generated by the following axioms: (D) Bi ϕ→¬Bi ¬ϕ, (4)
Bi ϕ→Bi Bi ϕ, and (5) ¬Bi ϕ→Bi ¬Bi ϕ where i ∈ AG and
ϕ ∈ LAG (B1 , . . . , Bn ).
It is shown in (Halpern, Samet, and Segev 2009) that
knowledge can be reduced to true belief in KD45n . More
specifically, the knowledge modality Ki is reducible to Bi in
KD45n by the axiom Ki ϕ ↔ (Bi ϕ ∧ ϕ). This also means
that we can remove the knowledge modal operator from the
language yet still be able to derive conclusions about knowledge of agents.
A Kripke frame F is a tuple hS, K1 , . . . , Kn i where S is a
set of worlds (or points) and Ki ⊆ S × S for i ∈ AG, called
the accessibility relation for i. A Kripke model M based on
the frame F is a pair (F, π), where π : F[S] → 2P is a
function that associates an interpretation of P to each world
in F. For M = (F, π), M[π] denotes π and M[S] and
M[i] denote the set of worlds S and Ki of F, respectively.
A pointed Kripke model (or p-model) is a pair (M, s),
where M is a Kripke model and s ∈ M[S], called the actual world. We will often represent a Kripke model M by
a directed graph whose nodes are the worlds in M[S] and
the labeled edges are the members of M[i]. The name of a
world is drawn next to the node and the interpretation associated to it is given by the label of the node. Entailment of
formulae in LAG (L1 , . . . , Ln ) in a p-model is defined next.
Definition 1 Given a formula ϕ and a p-model (M, s):
• (M, s) |= ϕ if M[π](s) |= ϕ and ϕ is an atomic formula;
• (M, s) |= Li ϕ if, for each t s.t. (s, t) ∈ Ki , (M, t) |= ϕ;
• (M, s) |= ¬ϕ if (M, s) 6|= ϕ;
• (M, s) |= ϕ1 → ϕ2 if (M, s) |= ¬ϕ1 or (M, s) |= ϕ2 .
M |= ϕ denotes that (M, s) |= ϕ for each s ∈ M[S]. ϕ
is said to be valid in a frame F if M |= ϕ for every Kripke
model M based on F. The set of valid formulae in F is
denoted with T h(F). For a class of frames S, T h(S) is the
set of formulae valid in each frame in S. A logic Λ is sound
for S if Λ ⊆ T h(S); it is complete for S if T h(S) ⊆ Λ. For
a logic Λ, a frame F is said to be a Λ frame if Λ ⊆ T h(F).
A relation R ⊆ S × S is reflexive iff (u, u) ∈ R for
every u ∈ S; serial iff for every u ∈ S there exists some
v ∈ S such that (u, v) ∈ R; transitive iff (u, v) ∈ R and
(v, z) ∈ R imply that (u, z) ∈ R; Euclidean iff (u, v) ∈ R
and (u, z) ∈ R imply that (v, z) ∈ R.
Frames can be characterized by the properties of their
accessibility relations. It is known that a frame F =
(S, K1 , . . . , Kn ) is a KD45n frame iff for every i =
1, . . . , n, Ki is serial, transitive, and Euclidean. A Kripke
model M = (F, π) is said to be a KD45n model if its
frame F is a KD45n frame.

Update Models. Update models describe transformations
of (pointed) Kripke models according to a predetermined
pattern. An update model uses structures similar to pointed
Kripke models and they describe the effects of a transformation on p-models using an update operator (Baltag and Moss
2004; van Benthem, van Eijck, and Kooi 2006).
A set {p → ϕ | p ∈ P, ϕ ∈ LAG (B1 , . . . , Bn )}
is called an LAG (B1 , . . . , Bn )-substitution (or substitution,
for short). For each substitution sub and each p ∈ P, we
assume that sub contains exactly one formula p → ϕ. For
simplicity of the presentation, we often omit p → p in a
substitution. SU BLAG denotes the set of all substitutions.
A substitution is used to encode changes caused by an action occurrence. A formula p → ϕ in a substitution states
the condition (ϕ) under which p will become true. For example, the action of flipping a coin can be represented by
the substitution {h → ¬h} which says that h (the coin lies
head up) is true iff ¬h (the coin lies head down) were true.
Definition 2 (Update Model) An update model Σ is a tuple
hΣ, R1 , . . . , Rn , pre, subi where
• Σ is a set, whose elements are called events;
• each Ri is a binary relation on Σ;
• pre : Σ → LAG (B1 , . . . , Bn ) is a function mapping
each event a ∈ Σ to a formula in LAG (B1 , . . . , Bn ); and
• sub : Σ → SU BLAG .
An update instance ω is a pair (Σ, e) where Σ is an update
model and e ∈ Σ (or a designated event). An update template is a pair (Σ, Γ) where Σ is an update model with the
set of events Σ and Γ ⊆ Σ.
Intuitively, an update model represents different views of an
action occurrence, associated to the observability of agents.
Each view is represented by an event in Σ. The designated
event in an update instance is the one that agents who are
aware of the action occurrence will observe. Templates extend the notion of instance to capture non-deterministic actions and other non-simple actions. The relation Ri describes agent i’s uncertainty about an action occurrence—
i.e., if (σ, τ ) ∈ Ri and event σ is performed, then agent i
may believe that event τ is executed instead. pre defines the
action precondition and sub specifies the changes of fluent
values after the execution of an action. An update model
is serial (resp., reflexive, transitive, Euclidean) if, for every
i ∈ AG, Ri is serial (resp., reflexive, transitive, Euclidean).
The update model in Fig. 1 is given by Σ0 =
h{σ, τ }, {(σ, σ), (τ, τ )}, {(σ, τ ), (τ, τ )}, pre, subi where
pre(σ) = h, pref (τ ) = true, and sub(σ) = sub(τ )=∅. It
says that if the event σ occurs then 1 is certain that σ occurs
while 2 thinks that τ occurs.
Definition 3 (Updates by an Update Model) Let
Σ = hΣ, R1 , . . . , Rn , pre, subi be an update model
and M=(F, π) be a Kripke model. The update operator
induced by Σ defines a Kripke model M0 =M⊗Σ, where:
• M0 [S] = {(s, τ ) | s ∈ M[S], τ ∈ Σ, (M, s) |= pre(τ )};
• ((s, τ ), (s0 , τ 0 )) ∈ M0 [i] iff (s, τ ), (s0 , τ 0 ) ∈ M0 [S],
(s, s0 ) ∈ M[i] and (τ, τ 0 ) ∈ Ri ;
• ∀f ∈ P.[M0 [π]((s, τ ))|=f iff f →ϕ∈sub(τ ), (M, s)|=ϕ].
The update of a p-model (M, s) given an update template

(Σ, Γ) is a set of p-models, denoted by (M, s) ⊗ (Σ, Γ),
where (M0 , s0 )∈(M, s) ⊗ (Σ, Γ) iff it holds that M0 =M ⊗
Σ and s0 = (s, τ ) where τ ∈ Γ and s0 ∈ M0 [S].
Intuitively, the set (M, s) ⊗ (Σ, Γ) is the set of p-models
encoding the result of the execution of the action, which is
represented by the update template (Σ, Γ), in the p-model
(M, s). It is easy to see that the p-model on the right of
Fig. 1 is the unique element of (M0 , s) ⊗ (Σ, {σ}) where
M0 = (h{s, u}, K1 , K2 i, π) with K1 = K2 = {(x, y) |
x, y ∈ {s, u}}, π(s)(h) = true, and π(u)(h) = f alse.
We will often depict an update instance via a graph with
rectangles representing events (double rectangles for designated events), and labeled edges representing each Ri .

Maintaining KD45n via Update Models
In this section, we present a sufficient condition for the
maintenance of the KD45n property of a Kripke model
M after the application of a sequence of update models
Σ1 , . . . , Σk . The next lemma shows that updating a p-model
by an update model does preserve reflexivity, transitivity,
and Euclidicity of a Kripke model if the update model also
satisfies the corresponding property. Observe that, since we
are exploring structural properties of the frames underlying updated models, we will not emphasize the designated
events and the actual world whenever it is unnecessary.
Lemma 1 Let M = (F, π) be a Kripke model and Σ =
hΣ, R1 , . . . , Rn , pre, subi be an update model. Then, the
following properties hold:
• if M and Σ are reflexive then M ⊗ Σ is reflexive;
• if M and Σ are transitive then M ⊗ Σ is transitive; and
• if M and Σ are Euclidean then M ⊗ Σ is Euclidean.
The lemma shows that reflexivity, transitivity, and Euclidicity of a Kripke model can be easily maintained if the update
model possesses the same property. As reflexivity implies
seriality, the properties of KD45n would be maintained
if all update models used are reflexive, transitive, and Euclidean. This is, however, not always the case, as we will
see later. Furthermore, seriality of M ⊗ Σ is not guaranteed
even when M and Σ are serial, as shown next.
Example 1 Consider the domain with two agents 1 and 2
and the set of propositions {h}. Let us consider the update
model Σ1 = h{σ}, R1 , R2 , pre, subi, where R1 = R2 =
{(σ, σ)}, pre(σ) = h, and sub(σ) = ∅ (Fig. 2, top left).
Let M1 be the Kripke model (F1 , π1 ), where F1 =
h{s, u}, {(s, s), (u, u)}, {(s, u), (u, u)}i, π1 (s) = {h}, and
π1 (u) = ∅ (Fig. 2, bottom left). As we will see later, the update model represents a public announcement that h is true.
We can see that Σ1 and M1 are serial, transitive, and
Euclidean. How(s,σ)
1,2
σ
ever, M1 ⊗ Σ1
⊗
1
h
pre: h
(right, Fig. 2), is
not serial, since
2
1,2
h
¬h
there is no suc- 1
cessor of (s, σ)
s
u
with respect to
Figure 2: Lost of seriality
the agent 2.

The above example also highlights that the update models of epistemic actions might not work properly for logics
different from S5. We will return to this issue in Discussion
Section at the end of the paper. The main reason for the loss
of the seriality in M1 ⊗ Σ1 lies in the fact that the precondition of σ in Σ1 is h and both R1 and R2 are reflexive. This
indicates that for M1 ⊗ Σ1 to maintain its seriality, in each
world that satisfies the precondition of σ, the accessibility
relation of any agent must contain a successor with the same
interpretation. Intuitively, this means that the agent cannot
have wrong beliefs. We will refer to this property as semireflexivity and formalize it as follows.
Let M be a Kripke model and let u, v ∈ M[S]. Let us
define u ∼ v iff M[π](u) ≡ M[π](v). Intuitively, u ∼ v
means that u and v are associated to the same interpretation
over P by the function M[π].
Definition 4 A Kripke model M is semi-reflexive if for every i ∈ AG and u ∈ M[S], there exists some v such that
(u, v) ∈ M [i] and u ∼ v.
It is easy to see that a Kripke model is semi-reflexive then
it is serial. Furthermore, semi-reflexivity is a property of a
Kripke model and differs seriality, transitivity, or reflexivity
which are properties of the frame of the Kripke model. Although semi-reflexive Kripke models can avoid the problem
mentioned in Example 1, updating a semi-reflexive Kripke
model using an arbitrary update model can result in a nonsemi-reflexive Kripke model as seen in the next example.
Example 2 Consider again the domain in Example 1. Let
Σ2 and M2 be the update model and Kripke model depicted
in Fig. 3, top left and bottom left, respectively. The substitution function in Σ2 is given by sub(σ) = {h → f alse}
and sub(τ ) = {h → true}. It is easy to see that M2 ⊗ Σ2
(Fig. 3, right) is not semi-reflexive ((s, σ) wrt. agent 2).
(s,σ)

1

σ

2

h
s

1,2

pre: true

pre: h

1,2

τ

2

¬h
u

1

⊗
1,2

¬h

(s,!)

2
2

1,2

h
2

h

1,2

(u,!)

Figure 3: Loss of semi-reflexivity
In Example 2, M2 is reflexive. However, the accessibility
relation for 2 in Σ2 is not. The key issue lies in that the
substitution function in Σ2 assigns different interpretations
for σ and τ . To maintain the semi-reflexivity of M2 , there
are two possibilities: the update model should be reflexive
or there must be a link in the update model that allows the
preservation of the semi-reflexivity of M2 . This leads us to
define a notion of a primitive update model:
Definition 5 A serial, transitive, and Euclidean update
model is primitive if the precondition pre(σ) at every event
σ is an atomic formula and for every i ∈ AG and (σ, τ ) ∈
Ri such that σ 6= τ then either (i) (σ, σ) ∈ Ri ; or (ii)
pre(τ ) = true and sub(τ ) = sub(σ) = ∅.
A primitive update model Σ essentially guarantees that an
agent cannot have false beliefs if she does not have false

beliefs prior to the occurrence of the action represented by
Σ. This is proved in the next lemma.
Lemma 2 If M is semi-reflexive and Σ is primitive then
M ⊗ Σ is semi-reflexive.
We are now ready to formalize a theorem that characterizes
a sufficient condition for maintaining the belief structures of
a Kripke model after the application of an update model.
Definition 6 A Kripke model is a sr-KD45n Kripke model
if it is semi-reflexive, transitive, and Euclidean.
Theorem 1 For a sr-KD45n Kripke model M and a primitive update model Σ, M⊗Σ is a sr-KD45n Kripke model.
The proof of the theorem follows directly from Lemmas 1
and 2. This leads to the following consequence.
Corollary 1 The result of applying a sequence of primitive update models to a sr-KD45n Kripke model is a srKD45n Kripke model.
The corollary, together with the result in (Halpern, Samet,
and Segev 2009), implies that we can assert the state-ofknowledge of the agents after the execution of a sequence
of actions in terms of update models if the initial p-model
is given by a sr-KD45n Kripke model. We observe that
the initial p-model in the majority of examples from the
literature (e.g., the muddy children, the card game, the
number game, etc.) are S5 Kripke models (and, thus, srKD45n ). This result is also significant to multi-agent
planning approaches that employ update models (Bolander and Andersen 2011; Löwe, Pacuit, and Witzel 2011;
van der Hoek and Wooldridge 2002) as the initial p-model in
a multi-agent planning problem is often a S5 Kripke model.
To support our claims, we need to show next that several
classes of update models found in the literature (Baltag and
Moss 2004; Baltag, Moss, and Solecki 1998; van Benthem,
van Eijck, and Kooi 2006; van Ditmarsch, van der Hoek, and
Kooi 2007) are indeed primitive.
Truthful Public Announcements.
A truthful public
announcement of a formula is an action that communicates to all agents that
pre: !
the formula is true. For
example, in the muddy
children example, the father
informs his children that at 1..n
least one of them is muddy.
After the execution of a
public announcement, all
Figure 4: ΣAnn (AG; ∅; ϕ)
agents will know that the
announced formula is true.
A truthful public announcement of a formula ϕ to the agents
in AG is often given in the literature by an update model
ΣAnn (AG; ∅; ϕ) = h{σ}, R1 , . . . , Rn , pre, subi where
R1 = . . . = Rn = {(σ, σ)}, pre(σ) = ϕ, and sub(σ) = ∅
(Fig. 4). It is easy to see that if ϕ is an atomic formula
then the update model ΣAnn (AG; ∅; ϕ) for truthful public
announcement is a primitive update model.
Private Announcements. A private announcement of
a formula ϕ to a group A ⊆ AG while other agents
are unaware of its occurrence (e.g., the father whis-

σ

pering to one of his children that at least one of them
is muddy) can be represented by an update model
ΣAnn (A; ∅; ϕ) = h{σ, τ }, R1 , . . . , Rn , pre, subi where
Ri
=
{(σ, σ), (τ, τ )}
pre: true
pre: true
for i ∈ A, Ri =
B
A,B
!
σ
{(σ, τ ), (τ, τ )}
for A
i 6∈ A, pre(σ) = ϕ,
pre(τ ) = true, and
Figure 5: ΣAnn (A; ∅; ϕ)
sub(σ) = sub(τ ) = ∅
(Fig. 5). We can easily check that if ϕ is an atomic
formula then the update model ΣAnn (A; ∅; ϕ) for private
announcement is a primitive update model.
Semi-Private Announcements. The occurrence of a
private announcement action can be observed by some
agents. In this situation, the agents—who observe the action
occurrence—will be aware of the fact that the agents—to
whom the announcement is made—have knowledge about
the formula. However, they do not know the truth value
of the formula. The action is referred to as a semi-private
announcement. Let A be the set of agents to whom the anpre: "
pre: ¬"
nouncement is made; let
B
B the set of agents who A B
A
B
σ
!
observe the announcement, where A ∩ B = ∅;
C
C
let C = AG \ (A ∪ B).
A
B
C
A semi-private anε
nouncement to A with
pre: true
B observing can be forAnn
Figure
6:
Σ
(A; B; ϕ)
mulated by the update
Ann
model Σ
(A; B; ϕ)=h{σ, τ, }, R1 , . . . , Rn , pre, subi
where Ri ={(σ, σ), (τ, τ ), (, )} for i
∈
A,
Ri ={(σ, σ), (τ, τ ), (, ), (σ, τ ), (τ, σ)} for i∈B, Ri =
{(σ, ), (τ, ), (, )} for i ∈ C, pre(σ) = ϕ, pre(τ ) = ¬ϕ,
pre() = true, and sub(σ) = sub(τ ) = sub() = ∅
(Fig. 6). We can easily check that if ϕ is an atomic formula
then the update model ΣAnn (A; B; ϕ) for a semi-private
announcement is a primitive update model.
Sensing Actions. A sensing action helps an agent to
determine the truth value of a formula ϕ. In a multiagent environment, the occurrence of a sensing action
that determines the value of ϕ has a similar effect on the
pre: "
pre: ¬"
agents in the environB
ment as a semi-private A B
A
B
σ
!
announcement.
Indeed, the update model
C
C
ΣSensing (A; B; ϕ) for
A
B
C
a sensing action of a
ε
formula ϕ is identical to
pre: true
that of ΣAnn (A; B; ϕ)
Sensing
(A; B; ϕ)
where A and B are sets Figure 7: Σ
of agents that are fully aware and partially aware of the
action occurrence (Fig. 7). The main distinction between a
semi-private announcement and a sensing action lies in that
the former will have only one designated event (σ) while
the latter will have two (σ and τ ). Again, we can check that
ΣSensing (A; B; ϕ) is a primitive update model.
A subtle difference between the occurrence of an announcement action and a sensing action lies in the non-determinism

of the state-of-knowledge of agents who are fully aware of
the action occurrence.
Fully Observable Ontic Actions. An ontic action is different from announcement and sensing actions, in that it
changes the “real” state of the world, and thereby changing
the state of knowledge of the agents. For instance, the action
of flipping a switch changes the position of the switch from
on to ¬on (or off) and from ¬on to on. In general, the effects
of an ontic action act can be given by the set C of formulae
of the form p → ψ where ψ is an atomic formula characterizing the p-model of the world in which p would become
true after the execution of act, i.e., by a substitution.
An occurrence of an ontic action that is fully observable by all agents can be modeled by an update model
ΣOn (AG; ϕ; C) = ({σ}, R1 , . . . , Rn , pre, sub) where
R1 = . . . = Rn = {(σ, σ)}, pre(σ) = ϕ, and sub(σ) = C
where ϕ is the precondition of the action and C is the set of
its effects. Again, we can see that if ϕ is an atomic formula
then ΣOn (AG; ϕ; C) is a primitive update model.

Discussions
Our work in this paper is closely related to (Herzig, Lang,
and Marquis 2005). Indeed, in our terminology, Proposition
1 in (Herzig, Lang, and Marquis 2005) states that the result of an update on a KD45n Kripke model is a KD45n
Kripke model. As it turns out, the key difference between
the two formalizations lies in the assumption made about
the actions and the type of actions that are considered. We
observe that the work in (Herzig, Lang, and Marquis 2005)
considers only two types of actions, ontic actions and observation actions; furthermore, the latter type of actions is considered only in S5 models. Our result, as shown in the previous section, is on the other hand applicable to all different
types of actions. More importantly, it is assumed in (Herzig,
Lang, and Marquis 2005) that the execution of an action in
a given p-model of the world always results in some other
p-model—this assumption means that actions can always be
executed. This is, in our view, not a realistic assumption;
for example, one cannot open a locked door without having a key for it. In our notation, this implies that for every
update model Σ = hΣ, R1 , . . . , Rn , pre, subi, we have that
pre(σ) = true for every σ ∈ Σ. It is easy to see that, under this assumption, the update M ⊗ Σ is a KD45n Kripke
model if M and Σ are serial, transitive, and Euclidean.
Observe that the result of our work is also applicable for
single agent domains. To the best of our knowledge, it is
the first to distinguish between knowledge and beliefs of an
agent in a single-agent domains. Previous works on reasoning about knowledge of an agent in the presence of incomplete information and sensing actions in single-agent
domains (e.g., (Scherl and Levesque 2003; Son and Baral
2001)) do not investigate this distinction as they considered
only the knowledge modal operator.
We conclude this section with an example that shows that
the proposed sufficient condition does not cover the most
general case of ontic actions and highlights a weakness of
the approach using update models. We then discuss a possible solution for the issue raised by this example.

Example 3 Consider a variation of Example 2 from (Baral
et al. 2013): a domain with 3 agents AG = {1, 2, 3} and
two propositions: o (opened) and h (head). The agents are
in a room with a box that is not open (¬o). The box contains
a coin that lies head up (h), but none of the agent knows this.
The initial p-model (M0 , s) encoding the knowledge of the
agents and the actual world is given in Fig. 8 (left), where
the double circle represents the actual world. The interpretation associated to each world is given by the label of the
node. The names of the world (s and u) are given as text
next to the node. On the right of Fig. 8 is an update template
(Σf lip , {σ}), encoding the occurrence of the action flip that
makes o true; agents 1 and 2 are aware of the action occurrence, while 3 is oblivious of it. The substitution in Σf lip is
given by sub(σ) = {o → true} and sub(τ ) = ∅. Observe
that Σf lip is not a primitive update model.
s

u
1,2,3

¬o
h

1,2,3

pre: true

¬o
¬h

σ

1,2,3

1,2

1,2,3

δ

3

!

1,2,3

(s,σ,δ)
1,2

o
h

(s,!,δ)
1,2

1,2,3

1,2,3

3

1,2,3

(s,!)

(u,!)

¬o
h

3

o
h

o
¬h

1,2

Figure 10: (ΣAnn (AG; ∅; o), {δ}) (left) and its result (right)
The above example shows that even when the initial p-model
is S5, the execution of a sequence of non-primitive uppre: true
date models can result in the
loss of the KD45n property.
1,2,3
η
pre: true
3
This means that the proposed
σ
3
condition is one of the weakest
sufficient conditions that guar3
!
1,2,3
antees the maintenance of the
1,2
KD45n property of the initial
pre: true
p-model. Observe that the is- Figure 11: (Σ2 , {σ})
f lip
sue displayed in this example
can be attributed to the fact that agent 3 is oblivious of
the occurrence of flip. If 3 was suspicious that the action might occur, then the update template would have been

o
¬h

3

1,2,3

3

(s,σ)

(u,η)

¬o
¬h

1,2,3

3
3

3

1,2

(u,σ)

3

3

3

o
¬h

3

o
h

(s,η)

1,2
1,2

3

pre: true

Figure 8: (M0 , s) (left) and (Σf lip , {σ}) (right)
After some inspection, we can see that updating (M0 , s)
by (Σf lip , {σ}) results in the p-model (M1 , (s, σ)) in
1,2,3
1,2,3
Fig. 9. M1 is no longer a
(u,!)
semi-reflexive Kripke model, (s,!) ¬o
1,2,3
¬o
h
¬h
but it is still a serial one.
Let us now update the
3
3
3
3
p-model
(M1 , (s, σ))
1,2
o
o
with an update template,
h
¬h
(s,σ)
(u,σ)
Ann
(Σ
(AG; ∅; o), {δ}),
shown in Fig. 10 (left),
1,2
1,2
representing the public announcement of o. The result of Figure 9: (M1 , (s, σ))
this update is shown on the right of Fig. 10. This resulting
p-model is no longer serial. Even worse, the result renders
that agent 3 becomes confused as his belief is inconsistent.
pre: o

(Σ2f lip , {σ}) which is obtained from (Σf lip , {σ}) by adding
an additional event η as in (Fig. 11) where sub(η) = sub(σ).
The result of updating (M0 , s) using (Σ2f lip , {σ}),
(M2 , (s, σ)) (Fig. 12), remains a sr-KD45n pmodel. We can verify that updating (M2 , (s, σ)) with
(ΣAnn (AG; ∅; o), {δ}) results in a sr-KD45n p-model.

1,2

Figure 12: (M2 , (s, σ))
The above example also shows that action models might
yield counterintuitive results if applied to non-S5 Kripke
models. This is a legitimate concern, as the initial p-model
(M0 , s) is a S5 model. Thus, the question of whether previously developed action models are suitable for applications such as multi-agent planning (Bolander and Andersen
2011; Löwe, Pacuit, and Witzel 2011; van der Hoek and
Wooldridge 2002) should be investigated. The above discussion also highlights the question of whether or not an
agent should be considered as oblivious with respect to occurrences of ontic actions if he is not aware of the action occurrence. We believe that both of these issues are important
for reasoning about effects of actions in multi-agent systems.
They will be our main concerns in the near future.

Conclusions
Our main goal in this paper is to extend the result
of (Halpern, Samet, and Segev 2009)—which states that
knowledge is reducible to true beliefs in KD45n —to reasoning about knowledge and beliefs after the execution of
an action sequence. To achieve this goal, we presented one
of the weakest sufficient conditions on Kripke models and
update models which preserves the KD45n property of a
Kripke model after it is updated by a sequence of update
models. This provides a simple way for drawing conclusions regarding the knowledge of agents after the execution
of an action sequence (via updates by update models) since
knowledge is reducible to true belief in KD45n models.
On the one hand, the result opens the door for multi-agent
planning systems proposed in the literature to attack planning problems which require the manipulation of not only
knowledge but also beliefs of agents. On the other hand,
this result can also be used as a guideline for the development of update models in practical applications. We related
our work to others, elaborated an issue faced by our formalization, and identified problems that need to be investigated
further. In addition to the mentioned issues, we intend to
extend the result of this paper to other types of actions (e.g.,
lying, misleading, announcement of knowledge formula).

References
Baltag, A., and Moss, L. 2004. Logics for epistemic programs. Synthese.
Baltag, A., and Smets, S. 2008. A qualitative theory of
dynamic interactive belief revision. In Proc. of 7th LOFT,
Texts in Logic and Games 3, 13–60. Amsterdam University
Press.
Baltag, A.; Moss, L.; and Solecki, S. 1998. The logic of public announcements, common knowledge, and private suspicions. In 7th TARK, 43–56.
Baral, C.; Gelfond, G.; Pontelli, E.; and Son, T. C. 2013.
Reasoning about the beliefs of agents in multi-agent domains in the presence of state constraints: The action language mal. In Leite, J.; Son, T. C.; Torroni, P.; van deer
Torre, L.; and Woltran, S., eds., Proceedings of the 14th International Workshop, Computational Logic in Multi-Agent
Systems, CLIMA VIX, Coruna, Spain, September 16-18,
2013, volume 8143 of Lecture Notes in Computer Science,
290–306. Springer.
Bolander, T., and Andersen, M. 2011. Epistemic Planning
for Single and Multi-Agent Systems. Journal of Applied
Non-Classical Logics 21(1).
Halpern, J.; Samet, D.; and Segev, E. 2009. Defining knowledge in terms of belief: The modal logic perspective. Reviews of Symbolic Logic 2(3):469–486.
Herzig, A.; Lang, J.; and Marquis, P. 2005. Action Progression and Revision in Multiagent Belief Structures. In
Sixth Workshop on Nonmonotonic Reasoning, Action, and
Change (NRAC).
Löwe, B.; Pacuit, E.; and Witzel, A. 2011. Del planning
and some tractable cases. In van Ditmarsch, H.; Lang, J.;
and Ju, S., eds., Logic, Rationality, and Interaction, volume
6953 of Lecture Notes in Computer Science. Springer Berlin
/ Heidelberg. 179–192.
Scherl, R., and Levesque, H. 2003. Knowledge, action, and
the frame problem. Artificial Intelligence 144(1-2).
Segerberg, K. 1998. Irrevocable belief revision in dynamic doxastic logic. Notre Dame Journal of Formal Logic
39(3):287–306.
Son, T. C., and Baral, C. 2001. Formalizing sensing actions
- a transition function based approach. Artificial Intelligence
125(1-2):19–91.
van Benthem, J.; van Eijck, J.; and Kooi, B. P. 2006. Logics
of communication and change. Inf. Comput. 204(11):1620–
1662.
van Benthem, J. 2007. Dynamic logic of belief revision.
Journal of Applied Non-Classical Logics 17(2):129–155.
van der Hoek, W., and Wooldridge, M. 2002. Tractable
multiagent planning for epistemic goals. In The First International Joint Conference on Autonomous Agents & Multiagent Systems, AAMAS 2002, July 15-19, 2002, Bologna,
Italy, Proceedings, 1167–1174. ACM.
van Ditmarsch, H.; van der Hoek, W.; and Kooi, B. 2007.
Dynamic Epistemic Logic. Springer.

van Ditmarsch, H. 2005. Prolegomena to dynamic logic for
belief revision. Synthese (Knowledge, Rationality & Action)
147:229–275.

Proceedings of the 1st Workshop on Natural
Language Processing and Automated Reasoning
(NLPAR) 2013
Editors: Chitta Baral1 and Peter Schüller2
1

Arizona State University, USA
Sabanci University, Turkey

2

Preface
This volume contains the papers presented at NLPAR2013: Workshop on Natural
Language Processing and Automated Reasoning 2013 held on September 15,
2013 in A Corunna, Spain.
The NLPAR Workshop received 6 submissions from different international
institutions and research communities. Each submission was reviewed by 3 program committee members. The committee decided to accept 5 papers. The program also includes 2 invited talks.
The organizing committee wants to thank the Workshop Chair of LPNMR,
Marcello Balduccini, and the Program Chairs of LPNMR, Pedro Cabalar and
Tran Cao Son, for their support in embedding this workshop into the LPNMR
conference organization.
Peter Schüller acknowledges support from Sabanci University. This workshop
was managed using EasyChair.

September 15, 2013
A Corunna, Spain

Chitta Baral
Peter Schüller

Table of Contents
Some Recent Advances in Answer Set Programming (from the
Perspective of NLP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Marcello Balduccini

1

Three Lessons in Creating a Knowledge Base to Enable Reasoning,
Explanation and Dialog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Vinay Chaudhri, Nikhil Dinesh and Daniela Inclezan

7

Qualitative Analysis of Contemporary Urdu Machine Translation Systems
Asad Abdul Malik and Asad Habib

27

The NL2KR System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Chitta Baral, Juraj Dzifcak, Kanchan Kumbhare and Nguyen Vo

37

A Default Inference Rule Operating Internally to the Grammar Devices . .
Christophe Onambélé Manga

48

BOEMIE: Reasoning-based Information Extraction . . . . . . . . . . . . . . . . . . . .
Georgios Petasis, Ralf Möller and Vangelis Karkaletsis

60

Recognizing Implicit Discourse Relations through Abductive Reasoning
with Large-scale Lexical Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Jun Sugiura, Naoya Inoue and Kentaro Inui

ii

76

Program Committee
Marcello Balduccini
Chitta Baral
Johan Bos
Vinay Chaudhri
Esra Erdem
Christian Fermüller
Michael Gelfond
Yuliya Lierler
Peter Schüller

Drexel University
Arizona State University
University of Groningen
SRI International
Sabanci University
TU Wien
Texas Tech University
University of Nebraska at Omaha
Sabanci University

iii

Some Recent Advances in Answer Set
Programming (from the Perspective of NLP)
Marcello Balduccini
College of Computing and Informatics
Drexel University
marcello.balduccini@gmail.com

1

Introduction

Answer Set Programming (ASP) [12, 13, 15, 8] is a logical language for knowledge
representation and reasoning that combines a non-monotonic nature, strong theoretical foundations, an intuitive semantics, and substantial expressive power.
The language has been successfully used for modeling a number of very diverse
domains (e.g. [7, 14]) and for capturing key reasoning tasks such as planning, diagnostics, learning and scheduling (e.g. [10, 5, 1]). All of this makes ASP a prime
candidate for use in the sophisticated knowledge representation and reasoning
tasks involved in Natural Language Processing (NLP).
In this note I will give an overview of some of my recent work on ASP that
I believe may be useful in the context of NLP.

2

Motivation

Reasoning about natural language involves various knowledge-intensive tasks.
Particularly challenging from this perspective are the extraction of semantic content from phrases (e.g. anaphora resolution) and the disambiguation of phrases
using world knowledge and commonsense knowledge. These two tasks are not
only challenging, but also heavily interconnected.
Consider the following collection of passages and proposed corresponding
reasoning:
– “John was walking his dog. He said hi.”
To conclude that “he” refers John, we can use knowledge about grammar,
stating that “he” normally refers to a human male. Additionally, the fact
that saying hi is a capability proper of humans, confirms the correctness of
the association.
– “John was walking his dog. He ran away after a rabbit”
This type of sentence is quite common especially in spoken English. Commonsense tells us that “he” here refers to John’s dog. A justification for this
is the everyday knowledge that running away after a rabbit is a behavior
common of dogs and other animals with hunting habits. Humans typically

do not run away after rabbits (although carefully capturing this last statement appears to be a rather interesting and intricate modeling task in itself).
Although according to grammar rules “he” should be associated with John,
it is also typical for people, and especially pet owners, to refer to their pets
by “he” or “she.”
– “John and Frank entered the room. Frank left right away. He came out two
minutes later.”
In this case the difficulty in finding which object “he” refers to derives from
the fact that two human males are mentioned in approximately the same
locations of the passage. To properly link this occurrence of “he” to John,
one needs to follow the evolution of the domain described by the passage.
The phrase “came out two minutes later” appears to refer to the room that
John and Frank had initially entered. The second sentence states that Frank
has already left the room. So, John is the only other person of interest in
the passage who is left in the room, and thus it is reasonable to assume that
“he” refers to him.
– “Andrea and Frank entered the room, but he left empty-handed.”
To reason about this sentence, it is useful to recall that, in English, Andrea
is both a male and a female name. Reasoning by cases, one can observe
that, if Andrea is a man, then the occurrence of “he” in the sentence is
ambiguous. On the other hand, if Andrea is a female, then it can be concluded
without ambiguity that “he” refers to Frank. Under the assumption that the
speaker or writer crafted the sentence in such a way as to convey the relevant
information in an unambiguous way, then it is reasonable to assume that “he”
refers to Frank. Moreover, one can conclude that Andrea is a woman. This
information can be stored and used later in reasoning about other parts of
the passage.
– “Andrea cannot be the one who took the computer from that room. Andrea
and Frank did enter the room, but he left empty-handed.”
Let us suppose that this passage is in the context of an investigation aimed
at determining who stole a computer from a room. The first sentence focuses
the discussion on Andrea and on Andrea’s innocence. Let us reason again by
cases on the possible associations of “he” – Andrea and Frank. Under both
possible associations, no grammar rules are violated, as long as Andrea is
a man. If “he” refers to Frank, however, the first and the second sentences
appear to have no logical connection, while their construction suggests that
indeed some link exists. On the other hand, if “he” refers to Andrea, then
the link is clear: the first sentence claims Andrea’s innocence, and the second
sentence offers evidence in support of the claim. Similarly to the previous example, the second case appears to be preferred based on the commonsensical
assumption that the speaker or writer crafted the passage in such a way as
to convey the relevant information in an unambiguous and economical way.
Whereas carefully crafted, written-language passages may require relatively
limited reasoning, everyday, colloquial language such as the one exemplified here
requires substantial reasoning for a proper understanding. For the success of

practical systems with natural language interfaces, I argue that everyday, colloquial language must be supported.
Overall, it appears that successfully reasoning about the semantic content of
sentences such as the ones shown above requires a sophisticated combination
of world knowledge, commonsense, and (commonsensical) information about
speaker’s/writer’s behavior and intentions. It is my belief that ASP and its
extensions can be useful in tackling such a task.
In the rest of this note I describe some extensions of ASP I authored or coauthored, and which may be useful in capturing certain aspects of the reasoning
about natural language. For a thorough discussion on ASP and on its use for
knowledge representation, the reader is referred to the existing literature (e.g.
[8]).

3

CR-Prolog

CR-Prolog [6] is an extension of ASP that adds to the language constructs,
called consistency-restoring rules (cr-rules), designed to capture certain advanced
aspects of non-monotonic reasoning.
A central, well-known feature of languages for non-monotonic reasoning such
as ASP is that the the programmer can write “defeasible statements,” which
are normally true, but may not apply to certain cases, called exceptions. A
well-known example is that of the statement “birds normally fly.” While true
for most birds, this statement has exceptions, such as penguins and birds with
broken wings, and hence the use of the word “normally.”
In most languages for non-monotonic reasoning the exceptions must be explicitly listed. In the example above, if a new type of bird is discovered that does
not fly, suitable statements must be added to the system, saying that that type
of bird is an exception. If the exceptions are not added, the systems will apply
the default statement and conclude that the birds of the new type fly. From a
practical perspective, having to know in advance all the exceptions may be a
limiting factor in the development of autonomous systems, since there may not
be sufficient understanding of the problem domain for such a complete list. It
is worth observing that, in everyday reasoning, humans are typically capable of
postulating exceptions to defaults, especially when they observe phenomena that
contradict such defaults.
Cr-rules are an attempt to capture this capability, allowing a reasoner to
postulate exceptions to default statements, but only when strictly necessary.
Making such assumptions is considered strictly necessary when the reasoning
process is otherwise inconsistent. This is the case, for example, of a system given
observations that contradict its knowledge base. For instance, the cr-rule:
+

exception(H, A) ← human(H), animal(A), small(A).
can be used in combination with a default “normally, humans do not chase small
animals” to state that, under exceptional, unknown, circumstances, the default

can be violated. When inconsistencies arise in the knowledge base, the system
can then use the cr-rule to postulate exceptions to the default.
My co-authors and I demonstrated that cr-rules allow for elegantly capturing
types of non-monotonic reasoning that are otherwise difficult or impossible to
capture. We also showed that they can be used to formalize concisely diagnostic
reasoning and certain types of planning.

4

EZCSP

EZCSP [2] is an extension of ASP aimed at increasing performance and scalability in certain – rather large – application domains.
To see how the ability to reason about numbers is important in reasoning
about natural language, consider the following passage: “The train left at 10.
A couple of hours later, we were having lunch in Paris.” To determine if “10”
refers to 10am or 10pm, one may reason as follows. Normally, “a couple of hours”
means two hours. Moreover, let us assume that it is common knowledge that in
Paris people have lunch between noon and 2pm. Hence, if we reason by cases, the
interpretation that “10” refers to “10am” sets the time of the speaker’s lunch in
Paris to noon. The interpretation that “10” refers to “10pm” sets the time of the
lunch to midnight. The second interpretation contradicts the custom of having
lunch between noon and 2pm, and thus the former interpretation is preferred.
This reasoning process relies on the ability to process effectively numerical
information. Although ASP allows in principle for natural and concise formalizations of many kinds of knowledge, in practical applications efficiency often
degrades quickly when dealing with numerical information and variables with
large domains. To overcome this limitation, I designed EZCSP to allow for the
use of constructs from constraint programming within ASP programs. For example, the rule:
required(hour(T ) ≥ 12) ← lunchtime(T ).
states that, if timestamp T refers to lunch time, then the hour of T must be
greater than or equal to 12.
The new language makes it possible to represent and reason about numerical
information efficiently, while at the same time keeping the representations elegant, concise and elaboration tolerant, as usual in ASP. Differently from other
languages that combine ASP and constraint programming (e.g. [11]), EZCSP
includes support for global constraints (a powerful type of construct from constraint programming), and both language and solver are designed to be independent from the underlying ASP and constraint solvers chosen for the computation.

5

ASP{f}

One shortcoming of EZCSP is that it does not allow one to perform full-fledged
non-monotonic reasoning on numerical quantities. For example, one cannot easily

state that “in Paris, people normally have lunch between noon and 2pm” and
reason with evidence that “John had lunch at 3pm.”
This is due to the monotonic nature of the underlying constraint programming constructs. A further shortcoming of EZCSP is that performance in the
presence of variables with large non-numerical domains still tends to be limited,
because constraint programming constructs mainly apply to numerical quantities. In fact, these limitations are shared by all the research attempts aimed at
hybridizing ASP and constraint programming. To overcome these issues, I have
developed a new language, called ASP{f} [3, 4], which adds to ASP the ability
to represent, and reason about, arbitrary (non-Herbrand) functions, including
but not limited to numerical functions. With ASP{f}, the default about lunch
time in Paris can be captured in a simple way by statements such as:
hour(T ) < 14 ← lunchtime(T ), not hour(T ) ≥ 14.
which intuitively states that “lunch ends at 2pm unless otherwise specified.” The
observation about John’s lunch time can be encoded by {lunchtime(t1 ), hour(t1 ) =
15}. In ASP{f}, this observation is sufficient to defeat the default.
In ASP{f} it is also possible to capture rather complex numerical calculations,
such as:
f inancially sound(C) ←
revenue(C) > sum[employee(E, C) = salary(E)] + investments(C).
This rule states that company C is financially sound if its revenue is greater
than the sum of the salaries paid to the employees and of the investments made
by the company. Another example, demonstrating ASP{f}’s ability to deal with
non-numerical information, is the rule:
← siblings(P 1, P 2), f irst name(P 1) = f irst name(P 2), not exception(P 1, P 2).
which captures the commonsensical statement that two siblings shouldn’t have
the same first name. Rather than relying on constraint programming, the new
language includes “native” support for functions, and, differently from other
attempts in this direction (e.g. [9]), is crafted in such a way that state-of-the-art
inference engines for ASP can be extended to support ASP{f} with relatively
simple modifications.

6

Conclusions

In this note I have described some challenges of the task of reasoning about natural language that are relevant to ASP and to commonsense and non-monotonic
reasoning in general. I have also discussed recent extensions of ASP that I have
developed, and which I believe may be useful in tackling these tasks. Of course,
many other extensions of ASP exist, which can be useful for this endeavour. For
the reader’s convenience, a small selection of relevant works was cited in this
note.

References
1. Balduccini, M.: Learning Action Descriptions with A-Prolog: Action Language C.
In: Amir, E., Lifschitz, V., Miller, R. (eds.) Procs of Logical Formalizations of
Commonsense Reasoning, 2007 AAAI Spring Symposium (Mar 2007)
2. Balduccini, M.: Representing Constraint Satisfaction Problems in Answer Set Programming. In: ICLP09 Workshop on Answer Set Programming and Other Computing Paradigms (ASPOCP09) (Jul 2009)
3. Balduccini, M.: Correct Reasoning: Essays on Logic-Based AI in Honour of
Vladimir Lifschitz, chap. 3. A “Conservative” Approach to Extending Answer Set
Programming with Non-Herbrand Functions, pp. 23–39. Lecture Notes in Artificial
Intelligence (LNCS), Springer Verlag, Berlin (Jun 2012)
4. Balduccini, M.: ASP with non-Herbrand Partial Functions: a Language and System
for Practical Use. Journal of Theory and Practice of Logic Programming (TPLP)
(2013)
5. Balduccini, M., Gelfond, M.: Diagnostic reasoning with A-Prolog. Journal of Theory and Practice of Logic Programming (TPLP) 3(4–5), 425–461 (Jul 2003)
6. Balduccini, M., Gelfond, M.: Logic Programs with Consistency-Restoring Rules.
In: Doherty, P., McCarthy, J., Williams, M.A. (eds.) International Symposium on
Logical Formalization of Commonsense Reasoning. pp. 9–18. AAAI 2003 Spring
Symposium Series (Mar 2003)
7. Balduccini, M., Gelfond, M., Nogueira, M.: Answer Set Based Design of Knowledge
Systems. Annals of Mathematics and Artificial Intelligence 47(1–2), 183–219 (2006)
8. Baral, C.: Knowledge Representation, Reasoning, and Declarative Problem Solving. Cambridge University Press (Jan 2003)
9. Cabalar, P.: Functional Answer Set Programming. Journal of Theory and Practice
of Logic Programming (TPLP) 11, 203–234 (2011)
10. Erdem, E.: Application of Logic Programming to Planning: Computational Experiments. In: Proceedings of the 5th International Conference on Logic Programming
and Non-monotonic Reasoning (LPNMR-99). No. 1730 in Lecture Notes in Artificial Intelligence (LNCS), Springer Verlag, Berlin (1999)
11. Gebser, M., Ostrowski, M., Schaub, T.: Constraint Answer Set Solving. In: 25th
International Conference on Logic Programming (ICLP09). vol. 5649 (2009)
12. Gelfond, M., Lifschitz, V.: The stable model semantics for logic programming. In:
Proceedings of ICLP-88. pp. 1070–1080 (1988)
13. Gelfond, M., Lifschitz, V.: Classical Negation in Logic Programs and Disjunctive
Databases. New Generation Computing 9, 365–385 (1991)
14. Grasso, G., Leone, N., Manna, M., Ricca, F.: ASP at Work: Spin-off and Applications of the DLV System. In: Balduccini, M., Son, T.C. (eds.) Symposium on
Constructive Mathematics in Computer Science (Oct 2010)
15. Marek, V.W., Truszczynski, M.: The Logic Programming Paradigm: a 25-Year Perspective, chap. Stable Models and an Alternative Logic Programming Paradigm,
pp. 375–398. Springer Verlag, Berlin (1999)

Three Lessons in Creating a Knowledge Base to Enable
Reasoning, Explanation and Dialog
Vinay K. Chaudhri, Nikhil Dinesh, and Daniela Inclezan
Artificial Intelligence Center,
SRI International, Menlo Park, CA, 94025

Abstract. Our work is driven by the hypothesis that for a program to answer
questions, explain the answers, and engage in a dialog just like a human does, it
must have an explicit representation of knowledge. Such explicit representations
occur naturally in many situations such as engineering designs created by engineers, a software requirement created in unified modeling language or a process
flow diagram for a manufacturing process. Automated approaches based on natural language processing have progressed on tasks such as named entity recognition, fact extraction and relation learning. Use of automated methods can be
problematic in situations where the conceptual distinctions used by humans for
reasoning are not directly expressed in natural language or when the representation must be used to drive a high fidelity simulation.
In this paper, we report on our effort to systematically curate a knowledge base
for substantial fraction of text in a biology textbook [26]. While this experience
and the process is interesting on its own, three aspects can be especially instructive for future development of knowledge bases by both manual and automatic
methods: (1) Consider imposing a simplifying abstract structure on natural language sentences so that the surface form is closer to the target logical form to be
extracted. (2) Adopt an upper ontology that is strongly motivated and influenced
by natural language. (3) Develop a set of guidelines that captures how the conceptual distinctions in the ontology may be realized in natural language. Since
the representation created by this process has been quite effective for answering
questions and producing explanations, it gives a concrete target for what information should be extracted by the automated methods.
Keywords: knowledge representation, ontologies, automated reasoning, conceptual models, knowledge acquisition from text

1

Introduction

Classical approach to achieving intelligent behavior has been driven by the knowledge
representation hypothesis proposed by Smith [27]: Any mechanically embodied intelligent process will be comprised of structural ingredients that (a) we as external observers
naturally take to represent a propositional account of the knowledge that the overall process exhibits, and (b) independent of such external semantic attribution, play a formal
but causal and essential role in engendering the behavior that manifests that knowledge.
In the context of this framework, an intelligent program requires a formal representation of knowledge that can be manipulated by an automated reasoner with the goal that

2

Three Lessons in Creating a Knowledge Base

it will enable a variety of tasks including answering questions, producing explanations
and engaging in a dialog.
There are some domains such as engineering, manufacturing, and finance where
structured representations are routinely created and are a part and parcel of a routine
workflow. Automated methods based on natural language processing (NLP) techniques
are quite effective at creating some limited forms of structured representations such as
named entity extraction [21] and relation extraction [7].
We have recently completed a substantial knowledge engineering effort that has
resulted in a knowledge base called KB Bio 101 that represents a significant fraction
of an introductory college-level biology textbook [11, 10]. We have used KB Bio 101
as part of a prototype of intelligent textbook called Inquire that is designed to help
students in learning better [8]. Inquire answers questions [10], gives explanations and
engages in dialog through natural language generation [1].
In this paper, we describe three specific aspects of the knowledge engineering process and discuss the lessons that can be drawn from this effort which can inspire the
development of a new breed of manual as well as automated knowledge acquisition
methods. These lessons are: (1) re-formulating sentences as universal truths so that the
surface form of knowledge is closer to the knowledge to be extracted (2) using a linguistically motivated ontology into which the knowledge is extracted (3) using a set of
guidelines that define how various conceptual distinctions are expressed in natural language. These three techniques were instrumental in creating KB Bio 101 that enabled
Inquire to answer students questions and led to learning gains as have been reported in a
previous paper [8]. We have organized the paper by first discussing the techniques that
we used in creating the knowledge representation followed by a discussion on how these
can be instructive for future manual, automated as well as semi-automated knowledge
acquisition methods.

2 Reformulating Input Sentences
A textbook is written for pedagogical purposes. Therefore, the authors adopt a style
of writing which is varied, interesting, and that tells a story. This invariably involves
first introducing concepts at an abstract level, and later adding more details, and in
some cases, contradicting and/or overriding the information that has been previously
introduced.
In contrast, an automated reasoning system needs to encode knowledge only once,
and in a succinct manner, using sentences in a formal language. While the axioms can
be arbitrarily complex, in practice, there are frequently occurring axiom patterns, for
example, axioms to represent necessary and sufficient properties of a concept, cardinality constrains, subclass and disjointness statements, etc. For the purpose of the current
discussion, we will work with one such axiom pattern known as universal truth: a set of
facts that are true for all instances of a concept.
To determine what should be represented from a textbook, a knowledge encoder
must gather all the sentences that describe that concept. In general, a sentence will
mention more than one concept. To determine which concept a sentence actually refers
to, the encoder reformulates that sentence as a universal truth. A sentence may result in

Three Lessons in Creating a Knowledge Base

3

more than one universal truth. In our current process, the encoders work at the level of
a single chapter. Once the sentences in a chapter have been reformulated as universal
truths, they can be sorted on the concept so that we now have available all the sentences
that describe a particular concept which can then be used for representation. This process deals with the pedagogical style of the textbook by collecting information about a
concept in one place in a similar surface syntax.
Let us now illustrate this process by taking two example sentences (numbered I and
II) in Table 1.
Textbook Sentence
I. A chemical
signal is detected
when the signaling
molecule binds to
a receptor protein
located at the cells
surface or inside
the cell.
II. The binding
of the signaling
molecule changes
the receptor protein
in
some
way,
initiating
the process of
transduction.

Universal Truth
Concept
During signal re- Signal-Reception
ception, the signaling molecule binds
to a receptor protein located at the
cells surface or inside the cell.

Plan
Signal-Reception − subevent →
Attach
Attach
−
base
→
Receptor-Protein
Attach − object → Molecule
...

During signal re- Signal-Reception Signal-Reception − subevent →
ception, the bindBind
ing of the signal
Attach
−
base
→
molecule changes
Receptor-Protein1
the receptor protein
Attach − result
→
in some way.
Receptor-Protein2
Receptor-Protein1
−
has-state → Receptor-Protein2
During cell signal- Cell-Signaling
Cell-Signaling − subevent →
ing, the binding
Signal-Reception
of the signaling
Cell-Signaling − subevent →
molecule inititates
Signal-Transduction
the process of
Signal-Reception
−
transduction.
next-event
→
Signal-Transduction

Table 1: Procedure for creating KB content from sentences
2.1

From Sentences to Universal Truths

Syntactically, a universal truth (or a UT) is a statement of the form: (a) Every X Y (b)
In X, Y (c) During X, Y. In these statements, X is a noun phrase denoting a concept
and Y is a clause or verb phrase denoting information that is true about the concept.
The concept (X) may not be directly mentioned in the sentence and it might be inferred
from the context and the teacher’s understanding of biology.
The universal truth associated with sentence I has the form – “During X, Y”, where
the concept “X” is “signal reception”. The phrase “signal reception” is not directly mentioned in the sentence, but is inferred from the phrase “a chemical signal is detected”
based on the context in which the sentence appears in the textbook.

4

Three Lessons in Creating a Knowledge Base

2.2

From Universal Truths to Knowledge Representation Plans

When formalized in logic, each universal truth leads to an existential rule, ie, a rule
whose antecedent has one variable that is universally quantified, and whose consequent
has one or more variables which are existentially quantified. Each universal truth is
converted to a plan: which is a set of literals that would appear in the consequent of
the existential rule suggested above. The plan for a universal truth is made by taking
into account the plans for all its superclasses and dependent concepts. For example, the
plan for Cell-Signaling would take into account the plan for Signal-Reception, which is
a step of Cell-Signaling.
Consider the first universal truth in Table 1 – “During signal reception, the signaling
molecule binds to a receptor protein located at the cell’s surface or inside the cell”. A
portion of the plan for this universal truth is shown in the fourth column and this can be
understood as follows:
– Signal-Reception − subevent → Attach – One of the steps of signal reception is
an “attach” or “bind” event.
– Attach−object → Molecule – The object (ie, the entity that undergoes attachment)
of the attach event is a molecule.
– Attach − base → Receptor-Protein – The base (ie, the entity that the object attached to) is a receptor protein.
– We omit the remaining literals, which show the “signaling” role of the molecule
and the location of the protein.
Taken together, these literals can be understood as – “one of the steps of signal
reception is the attachment of a molecule to a receptor protein”. The event Attach and
the relations object and base are provided by the upper ontology called the Component
Library (CLIB) which we will discuss in more detail in the next section.
The plans for a knowledge base are similar to design specification or a pseudo code
for a program. Writing the plans first helps an encoder to think through the overall
design of the representation before entering it into the knowledge base.
2.3

From Plans to Knowledge Representation

The plans are entered into the KB using a graphical interface called concept maps [12].
Figure 1 shows the concept map for Signal-Reception; the white color denotes that it is
universally quantified, while all other concepts are existentially quantified. The concept
map can be read as the following existential rule: “Every signal reception event has a
subevent in which a molecule attaches to a receptor protein, resulting in a change in the
state of the protein”.
There are several side-benefits of reformulating these sentences as universal truths:
(1) The sentence form is closer to the actual logical form that will be represented in the
knowledge base, making the task of creating the concept graphs much easier (2) universal truths aid in developing a consensus understanding of the content of the textbook
(3) They help the encoder in thinking through which concepts should the knowledge be
associated with.

Three Lessons in Creating a Knowledge Base

5

Fig. 1: Concept Map for Signal Reception

3

Linguistically Motivated Upper Ontology

Wordnet is by far the most commonly used resource in natural language processing
for reasoning about entailments [22]. One of the reasons for the success of Wordnet is
that it is linguistically motivated and it encodes knowledge at the level of words. This
ensures good coverage and makes it easy for people to understand what it should or
should not contain. Wordnet is, however, not an ontology and has several limitations
when it comes to supporting automated reasoning [16].
Component Library (or CLIB) is a linguistically motivated ontology designed to
support representation of knowledge for automated reasoning [3]. CLIB adopts four
simple upper level distinctions: entities (things that are), events (things that happen),
relations (associations between things) and roles ways in which entities participate in
events.
For the purpose of this discussion, we will focus on the taxonomy of physical actions where action is a subclass of Event. The reason for focusing on actions is to illustrate how the library of actions is grounded in language and helps us assess coverage
in a manner similar to assessing coverage for Wordnet, and yet, defines the actions to
support automated reasoning, explanation generation and dialog.
In the original version of CLIB [3], the Action has 42 direct subclasses and a total of
147 subclasses in all. Examples of direct subclasses include Attach, Impair, Move, and
Store. Other subclasses include Move-Through which is a subclass of Move, and Break
which is a subclass of Damage which is a subclass of Impair. These subclasses were
developed by consulting lexical resources, such as Wordnet [22], Longman Dictionary
of Contemporary English [30] and Roget’s thesaurus [20].
We will now discuss how this linguistic grounding of the ontology helped us address
the following two problems in our recent effort to represent knowledge from a biology
textbook: (a) ensuring that we have an adequate coverage of actions that occur in the

6

Three Lessons in Creating a Knowledge Base

textbook (b) developing guidelines that inform an encoder which action from the library
should be used to model a verb appearing in a sentence.
3.1

Ensuring Coverage

To check whether CLIB had adequate coverage to support all the process representations that we will need to create for the textbook, we analyzed the verbs appearing in the
textbook. We investigated whether and how their meaning could be represented using
CLIB actions and determined what new action classes should be added to CLIB when
no pre-existing classes matching its meaning was found.
The main body of the biology textbook Campbell Biology consists of 30,346 sentences. We extracted all the verbs appearing in these sentences which gave us a list of
2,870 verbs. The actual number of verbs is smaller, as some of the identified verbs are
in fact just different forms of the same verb (e.g., is and were, two forms of the verb to
be, were counted as different verbs). Next, we stemmed verbs based on their frequency,
which ranged from 1 to 18,407. The sixteen verbs with a frequency higher than 400 can
be seen in Table 2. There were 800 verbs with a frequency greater or equal to ten.
Verb
18,407
3,805
1,433
936

Frequency
to be
to have
to call
to use

Verb
860
708
658
646

Frequency
to produce
to include
to form
to occur

Verb
629
528
499
488

Frequency
to make
to cause
to develop
to do

Verb
460
451
429
413

Frequency
to increase
to grow
to become
to help

Table 2: Textbook Verbs with a Frequency Higher than 400
We analyzed all the verbs with frequency greater than 10 to check whether their
meaning was adequately represented using some action in CLIB. As a result of this
exercise, we identified whether a new action class should be added or we should extend
the meaning of an existing action class.
We identified 21 new action classes that should be added to CLIB. While adding
these classes, we used the principle of correspondence, ie, in many cases pairs of actions
go together and both should be present in the action library. For example, the initial
version of CLIB contained a class called Attach referring to an asymmetric attachment
of one entity to another, but there was no class for a symmetric attachment between two
entities. We remedied this problem by introducing the class Bind, which corresponds
to Attach. We introduced the class Expel as a counterpart of Take-In, where Expel and
Take-In are the subclasses of Move-Out-Of and Move-Into, respectively. Other newly
introduced classes (e.g., Kill) refine the range of one of the relations in their superclasses
(e.g., Kill is a subclass of Destroying a living entity).
The remaining proposed action classes specify the manner in which an action is
performed. For instance, Fly, Run, Swim, Crawl, Hop, and Climb were added as new
subclasses of Locomotion. Alternatively, manner could be described via one or more
relations defined on action classes. This second option would avoid possible problems
related to an increased size of the CLIB action hierarchy and the need to re-organize it.
Finally, one example of an existing action class whose meaning should be extended
is Support. Initially, this action class was defined as “to prevent from falling,” whereas

Three Lessons in Creating a Knowledge Base

7

for use in the domain of biology it is useful to extend its meaning by adding the expression “or provides some other kind of structural support.”
The discussion in this section illustrates how grounding the ontology in natural
language text helped assess its coverage in relation to the knowledge that needs to be
modeled, and informed us how the library should be extended.
3.2

Choosing an Action Class

When a knowledge encoder is representing a sentence that describes some process
knowledge, a choice needs to be made on which action class to use. This choice needs
to be systematic so that it is consistent across the representation of different processes
across the book as well as consistent across multiple encoders. We approached this
problem by systematically analyzing how different verbs should be mapped to actions
in CLIB.
For the purpose of this analysis, we limited ourselves to the 800 verbs that had a
frequency greater than or equal to ten. We analyzed these verbs based on their usage
in the textbook, starting with the most frequent ones. For each verb, we selected a
maximum of 30 sentences that contained it drawn from different parts of the textbook
to ensure that we were considering representative usage. Two challenges we faced in
this exercise are as follows.
1. A large number of verbs have (obviously) multiple meanings, depending on the
context in which they were used. So, we must deal with different senses when
choosing an appropriate CLIB action.
2. The specification of CLIB actions contains definitions and examples related to common sense domains, which are not always helpful when dealing with specialized
knowledge from the domain of biology. For instance, the CLIB action Support is
defined as “to put an object in a state that prevents it from falling;” the use of this
CLIB event is illustrated by the sentence:
(1)

Tom supported the roof with a heavy beam.

However, the use of the verb support in biological descriptions can also refer to a
state that prevents something from changing its shape:
(2)

Intermediate filaments support cell shape.

To address the above challenges we first developed a procedure for identifying an
action class by considering one fourth of the selected verbs, and then tested the procedure on the remaining verbs. We expressed this procedure as a set of guidelines for encoding verbs using CLIB actions. In this process, we realized that frequently-occurring
verbs, especially those with a frequency greater than 400, tended not to describe an actual action taking place and therefore did not require an event to capture their meaning.
This was generally not the case with lower frequency verbs. We have extensive set of
guidelines to handle verbs with frequency greater than 10. For the present discussion,
we illustrate the procedure by considering several examples.

8

Three Lessons in Creating a Knowledge Base

Example 1. Textbook Sentence: The groove is the part of the protein that recognizes
and binds to the target molecules on bacterial walls.
Corresponding Universal Truth(s): The protein binds at the groove with the target
molecules, which are situated on the bacterial walls.
Encoding: The encoder needs to choose a CLIB action class to represent the verb binds.
CLIB contains an action class, Attach, for asymmetrical attachments. We check that the
sentence describes an asymmetrical attachment by verifying that the reverse sentence
– “The target molecules on the bacterial walls attach to the protein” – does not make
sense. To represent this process, we will use the action class Attach and assign values
to the participant relations for it as follows: object = protein, site = groove, and base
= target molecules on bacterial walls. We will discuss the procedure for choosing the
relations in the next section.
Example 2 (Guidelines for the Verb to cross). When analyzing sentences containing the
verb to cross, we first determined that such sentences normally translate into UTs of one
of the following two types:
(a) Entity X is crossed (interbred) with entity Y.
(b) Entity X crossed entity Y.
For UTs of type (a), whether the usage is in the context of an experiment in which an
action class corresponding to that experiment should be used. In this case, conducting
a cross breeding experiment is a domain-specific class to be created and maintained by
the domain experts.
For UTs of type (b), the relevant CLIB class is Move-Through with participant relations
having the values: object = X, base = Y.
We have developed systematic guidelines to help the encoders in identifying a suitable action class from CLIB. Normally, the CLIB action selected to encode a biological
process is designated as its superclass. However, there are two exceptions: sometimes
the identified CLIB action describes a subevent of the biological process, not its superclass; other times, there is a more specific action in the KB that should be made the
superclass. We illustrate this using examples.
(3)

Most often these existing proteins are modified by phosphorylation, the addition of a phosphate group onto the protein.

In the above sentence, should Add be one of the subevents of Phosphorylation, or
the superclass of Phosphorylation, or neither?
We address the subevent possibility first. Let us assume that we have a biological
process P and we have identified a CLIB action A that could be used to model it. We
use the following test to determine whether A should be a step of P or its superclass: If
it is appropriate to say “During P , A happens”, and P is already known to have other
substeps of P , then A should be a sub-step. If we apply these guidelines to (3), we
notice that it is appropriate to say that “during phosphorylation, addition happens,” but
the textbook does not describe any other subevent of phosphorylation. So, Add should
not be modeled as a substep of Phosphorylation.

Three Lessons in Creating a Knowledge Base

9

Next, we consider the superclass possibility. If P is a complex biological process
and A describes just the overall outcome of P but does not capture its intricacies, then
A should not be the superclass of P ; this is especially valid if P has multiple steps.
In this situation, a more specific biological process from the KB should be selected as
the superclass of P . The reason behind this approach is that, in such cases, the CLIB
actions tends to abstract away too many of the relevant details of the biological process.
The CLIB action is useful, though, in expressing the common sense definition of the
process. For instance, although Phosphorylation is described as an addition of a phosphate group to a protein in (3), encoding this process as a specialization of the CLIB
action Add is not a good choice as it would result in an overly simplified model. We
prefer to make Phosphorylation a subclass of Synthesis-Reaction, which is a subclass
of Chemical-Reaction and is better suited for capturing the complexity of this process.
The discussion above illustrates the kind of procedures we needed to develop to
identify suitable actions classes that should be used when modeling a process verb in a
textbook sentence.

4 Guidelines for Choosing Semantic Relations
CLIB provides two types of relations between events and entities, motivated by “case
roles” in linguistics [c.f. 2] :
– Participant relations – agent, base, instrument, raw-material, result, object
– Spatial relations – destination, origin, path, site.
CLIB provides a semantic definition of each relation, together with common sense
examples as shown in Table 3. In the examples, the event in boldface is related to the
entity in italics.
Relation Definition
The entity that initiates, performs, or causes an
agent
event.
Event references something as a major or relabase
tively fixed thing
The specific place of some effect of an event, as
site
opposed to the locale of the event itself

Example
John swatted the fly
Vlad attached the sign to the
post
The nurse stabbed the needle in
my arm at the hospital

Table 3: Definition of relations in CLIB with examples

After a CLIB action is selected for modeling some biological process described by
a sentence, the next step is to identify the semantic relationships between the action
class and its various participants. It is well known that semantic distinctions are not
always directly expressed in language [19] making it difficult to apply the definitions of
the relations as shown above. The following pairs of relations are especially difficult to
distinguish.
– agent and instrument;
– raw-material and instrument;
– base and path.

10

Three Lessons in Creating a Knowledge Base

If the choice between these relationships is not made consistently and correctly,
it significantly interferes with the system’s ability to generate good natural language
sentences to support explanation generation. To further make this point, we consider
two specific problems caused by lack of proper usage.
1. The same entity is assigned to two or more semantic relations of the same event.
With such encoding, the translation into English of events is unnatural, as shown
by the following automatically produced sentence:
(4)

A gated channel is closed by a stimulus with a stimulus.

The above sentence results from an action Close with object = gated channel and
agent = instrument = stimulus.
2. A required relation is assigned an overly general entity such as Physical-Object or
Tangible-Entity. Such process models are only partially useful in answering questions. Furthermore, their translations into natural language are difficult for endusers to understand.
(5)

A gene is moved into an object.

The above sentence resulted from an action Move-Into with object = gene and base
= a tangible entity.
To address this issue, we developed a more detailed characterization of how the
semantic relations might be expressed in language and how an encoder could be better supported in choosing the most appropriate relation. Such characterization involves
specifying syntactic clues and examples from the domain of biology. Syntactic definitions are usually easier to follow, as they are more precise. There is however one semantic relationship, base, that has an irregular syntactic definition, which varies across
CLIB events. Additionally, there are some prepositions that are associated with more
than one semantic relationship (e.g., from may indicate either a donor or an origin). For
these reasons, a combined approach based on both semantic and syntactic definitions, as
summarized in Table 4, works the best. Such an approach benefits from the advantages
of both methods while diminishing their disadvantages.
For the pairs of relations that were particularly difficult to distinguish, we performed
a deeper comparative analysis and provided additional guidelines, as described in Subsection 4.1.
We tested these guidelines and our definitions by asking the domain experts to convert sample encodings created into English sentences and then assessing whether the
resulting sentences were of good quality. We consider a few representative examples of
this evaluation in Subsection 4.2, together with suggestions for correcting them.
4.1

Distinguishing between Problematic Pairs of Relations

In this section, we discuss examples of relations that were too difficult to distinguish for
encoders as originally defined in CLIB, and our approach for developing a procedure to
better distinguish them.

Three Lessons in Creating a Knowledge Base

11

Distinguishing between agent and instrument. In natural language, entities denoting the agent or the instrument of an event can both be realized as the grammatical
subject of a sentence, which makes it difficult to distinguish between the two:
(6)

Birds eat small seeds.

(7)

Intermediate filaments support cell shape.

The subjects of sentences (6) and (7) are mapped into the agent and instrument relations, respectively, based on the original semantic definitions of these relations, which
requires the agent to be sentient, but the instrument need not be sentient:
– An agent is active, while an instrument is passive, being used by the agent if there
is one.
– An agent is typically considered sentient, if only metaphorically, while an instrument
need not be.
Applying these definitions and distinctions is not always straightforward because
different people have different understandings of what sentient means. This is illustrated
by the following example sentence:
(8)

A biomembrane blocks hydrophilic compounds.

A biomembrane is part of a living thing, so it is not clear whether by itself, it is
sentient or not. To solve this problem, we complemented the specifications of the two
slots by adding some syntactic tests for disambiguation:
– Transform a sentence written in the active voice into an equivalent sentence in the
passive voice. The agent is the entity preceded by the preposition by, if such an
entity exists. (e.g., By transforming (6) into an equivalent sentence in the passive
voice, we obtain: “Small seeds are eaten by birds.” The noun birds is preceded by
the preposition by, hence it must indicate the agent.)
– If the subject of a sentence can be replaced by a phrase containing the preposition
with or using when the sentence is transformed into its passive voice equivalent,
then that entity is an instrument. (e.g., The sentence “Cell shape is supported
using intermediate filaments” sounds natural, so the intermediate filaments are the
instrument in sentence (7).)
By performing these syntactic tests on sentence (8), and using the semantic definitions above, we can determine that the biomembrane should be the agent of the
described event.
Distinguishing between raw-material and instrument. Consider the following sentences:
(9)
(10)

A planarian detects light using a pair of eyespots.
The Calvin cycle produces sugar using ATP and NADPH.

12

Three Lessons in Creating a Knowledge Base

Here, the preposition using, normally associated with the instrument relation, appears in both of the sentences. However, only (9) specifies an instrument; (10) specifies
a raw-material.
To determine what sets the two cases apart, we analyzed several sentences which
contained verbs such as to use, to produce, to form, to consume, etc. We determined that
the following distinctions capture how these two relations are expressed in language:
– A raw-material is an entity that is used up in an event and does not come out of it
the same way it entered the process.
– An instrument is an entity that facilitates the occurrence of the event, but it is not
consumed by the process.
This new definition clarifies why (10) is an example of a raw-material: ATP and
NADPH are used up by the Calvin cycle.
Distinguishing between base and path. Consider the sentence:
(11)

A molecule moves through the cell membrane.

which describes a Move-Through action. According to the original CLIB guidelines
for Move-Through, the cell membrane should be mapped into the base relation. This
conflicts with the syntactic guidelines in Table 4, which indicate that the cell membrane
should be the path, because it is preceded by the preposition through. However, opting
for either of the two relations seems to cause problems as we discuss below.
Let us assume that we opt for using the slot base in (11), and let us consider the
sentence:
(12) A molecule moves into the cell.
According to the CLIB guidelines for action Move-Into, the cell in (12) should be the
base of a Move-Into event. This leads to conflicting definitions for the slot base: in
the parent class Move-Through it must be the Barrier that is crossed; in the subclass
Move-Into it must be a Container into which an object is moved.
If we opt for using the slot path in (11), then we run into a different problem. In the
sentence:
(13)

A molecule moves through a pore of the cell membrane.

there would be no relation to assign to the pore, given that the slot path—the most
natural choice—is already assigned the value the cell membrane. This is an even bigger
issue than the first option.
To remedy this problem, we decided to allow the slot base to have different definitions for different action classes, even if these action classes are connected by subclass
relationships in the CLIB ontology. The new general definition of base says that it must
be “a major or relatively fixed thing that the event references” and that cannot be associated with other slots. More specific definitions are given in relation to each action
class for which this relation is relevant.

Three Lessons in Creating a Knowledge Base

4.2

13

Testing Our Definitions and Guidelines

To test the guidelines that we have described above, we asked the encoders to apply
them to encode a few representative actions, and then manually convert them into English. Such a task is in direct support of our goals to enable explanation and dialog.
In most cases the guidelines were effective, ie, when they were followed, the resulting representations led to good natural language sentences. In this section, we will
discuss only those cases where the guidelines were not effective and suggest solutions
for improving them.
(14)

Liquid is transported by a eukaryotic cell to cytoplasm inside a vesicle through
a plasma membrane using an organic molecule. (Pinocytosis)

In (14), the vesicle is mapped into the instrument slot. From a syntactic point of
view, the preposition inside normally indicates association with the base slot. However,
in the process of pinocytosis, the vesicle functions more like a carrier that transports
the liquid. Thus semantically it is closer to an instrument. Note that instruments are
indicated by the expression using, which is also associated with raw-material. We believe that the encoder used the preposition inside for the instrument because the using
relationship had already been used to capture the raw-material in this sentence. One
suggestion would be to use the expression consuming for the raw-material, and the
preposition using for the instrument, resulting in a new sentence:
Liquid is transported by a eukaryotic cell to cytoplasm using a vesicle through a
plasma membrane consuming an organic molecule.
Next, consider the following sentence:
(15)

An image is produced using a radioactive tracer by a PET scanner.

In (15), the radioactive tracer is assigned to slot agent and the PET scanner to the
slot instrument, but the prepositions associated with the two expressions indicate a reversed assignment to slots. What happens in reality is that the image is produced by the
PET device based on the computer analysis of concentrations of the tracer. Therefore,
both syntactically and semantically the tracer should be the instrument and the PET
scanner should be the agent.
(16)

A cell recognizes another cell (a target cell) at a plasma membrane.

In (16), the plasma membrane is assigned the role base, while the preposition at is
normally related to the slot site. Semantically, what this means is that Cell-Cell-Recognition
is a function of the plasma membrane. According to the guidelines for modeling of
Functions [9], this information would be modeled by making the has-function slot of
the plasma membrane point to Cell-Cell-Recognition. Then, the plasma membrane can
be assigned the role of site in this event, as it specifies a particular place on the agent
cell where the effect of recognition occurs.
(17)

Transferring by an electron from a chemical (a reducing agent) to another
chemical (an electron recipient). (Reduction)

14

Three Lessons in Creating a Knowledge Base

In (17), the electron is assigned the role of donor, although it is preceded by the preposition by usually associated to agent. Reduction is defined as “a reaction in which the
atoms in an element accept electrons.” Hence, semantically, electrons are not a donor
(nor an agent), but rather the object of this transfer. To fix this case, we replace the
preposition by with the preposition of as in:
Transferring of an electron from a chemical (a reducing agent) to another chemical
(an electron recipient).
(18)

A cell receives a signal at a receptor protein carried by a chemical.

In (18), the receptor protein is assigned to slot instrument, and the chemical to slot
object. Syntactically, the preposition at is used to denote the site. If we look at the
definition of this process, we see that it uses a different verb than receives: “The target
cell’s detection of a signaling molecule coming from outside the cell.” Moreover, in the
encoding of this process, the chemical plays the role of a signal. Hence, this sentence
could be reformulated as
A chemical entity playing the role of a signal is detected by a cell using a receptor
protein.
As a result, the following assignment of values to slots would be appropriate, according to the information in Table 4: object = chemical with plays = signal, base = cell,
instrument = receptor protein.

5 Discussion and Lessons Learned
Let us now step back and draw some higher level conclusions from the techniques we
have presented here.
Reformulating a sentence as a UT can be more generally viewed as a way to arrive at
a surface structure of a sentence which is more closely aligned with the ultimate logical
form that needs to be created. Of course, the idea of UT needs to be generalized to a
broader set of axiom templates to support sufficient properties, constraints, disjointness
etc. A closely related notion was first introduced under the name of abstract syntax trees
(ASTs) [15]. UTs can be viewed as a specific instance of an AST. The use of ASTs is
more broadly applicable to manual knowledge curation efforts in which the acquisition
process starts from text, and an AST generation provides a graceful migration from the
informal textual knowledge to a more formal logical form. In the context of automated
knowledge acquisition using natural language processing methods, availability of ASTs
can make the task of logical form generation substantially more tractable. The sentences
in the textbook are so complex that unless one uses some form of AST, the task of
getting a reasonable logical form is almost impossible. Therefore, the use of ASTs as
a technique to add knowledge capture is the first major lesson or take away from the
proces described here.
CLIB was originally created to be a linguistically motivated upper ontology. The action names are grounded in language and the semantic relationships based on research

Three Lessons in Creating a Knowledge Base

15

in linguistics. As we saw, the linguistic grounding of CLIB was quite effective in achieving coverage of core concepts that were needed for modeling knowledge in the biology
textbook. Even though CLIB defines semantic relationships and a few key axioms for
each of the action in the library, it is far from clear how to argue the completeness of
those axioms. There are several concepts in CLIB that capture distinctions that are not
usually expressed in language. One such example is the concept of Tangible-Entity.
As we saw during the discussion, such concepts were problematic for natural language
generation, because if such concepts appear in the output, the end-users will fail to naturally understand their meaning. Ideally speaking, the usage of such concept names in
an ontology should be minimized, and preferably, avoided. We expect CLIB to have
special strength for natural language processing application because of its linguistically
motivated concepts and semantic relationships. While we cannot claim that CLIB has
yet proven its value in being an inferentially valuable knowledge resource in the same
way that Wordnet is a lexical resource, continuing to develop CLIB in that direction is
still a sensible direction for future work. Accordingly, we encourage and advocate other
researchers to make their ontologies as linguistically grounded as possible.
Use of a combination of syntactic and semantic guidelines was essential in ensuring
a systematic encoding of knowledge. We developed guidelines that helped encoders determine which semantic relationship is most appropriate for use in a process description.
The linguistically motivated semantic relationships have the strength of being general
across multiple domains. But, as the complexity of the guidelines indicates, they can
also be difficult for humans to use and apply in a consistent manner. We hope that
developing the guidelines that we presented in this paper will provide a foundation
for automated and semi-automated tools that could either acquire such relationships
from text automatically, or provide much better support to encoders as they make their
choices. The basic idea of using a combination of syntactic and semantic guidelines is
quite general and can be adopted by a broad range of applications.

6

Related Work

Several well-known upper ontologies exist today that have been used to create knowledge bases and overlap in their goals and coverage with CLIB. One of them is DOLCE
[6], which is a higher-level ontology than CLIB. It contains approximately 100 concepts in total, whereas CLIB contains more than 1000, 147 of which are action classes.
In DOLCE, events are called occurrents. Entity-event relations are denoted by the expression participation. DOLCE distinguishes between temporary and constant participation (and other types of participation as well), distinctions that are not present in
CLIB. Similarly to CLIB, DOLCE was used in domain-specific applications. Borgo
and Leitão, for instance, used DOLCE to model a manufacturing domain [5].
Other commonly used upper ontologies are: Basic Formal Ontology (BFO) [17, 29]
containing 36 classes in total; General Formal Ontology (GFO) [18] containing 79
classes; or Suggested Upper Merged Ontology (SUMO) [23] containing 20,000 terms.
As far as we know, there is no published research on guidelines for encoding knowledge described by natural language sentences, for any of these ontologies. However, we

16

Three Lessons in Creating a Knowledge Base

believe that the method we describe in this paper is general enough to be applicable to
these upper ontologies as well.
There are several specialized biological or biomedical ontologies currently in use.
They generally tend to have a large number of concepts. Systems Biology Ontology
(SBO) [14] is an ontology dedicated to a specific branch of biology. It incorporates
the concept of interaction, which roughly corresponds to events in CLIB. The Gene
Ontology (GO) [13] is designed to facilitate the description of gene products. The Systematized Nomenclature of Medicine Clinical Terms (SNOMED-CT) [31] is a much
larger biomedical ontology, containing over 400,000 concepts. It is currently in use
in different countries. There has been substantial research in revising and auditing this
large ontology [25, 32, 28]. In contrast with the issues we discussed in relation to CLIB,
the problems identified by this body of work concerned the taxonomy of SNOMED-CT.
Some similarities with our approach are present however, such as a close collaboration
between knowledge engineers and domain experts, and a need to address the mismatch
between a common sense meaning of words and their usage in the ontology.
A different type of research with converging goals to ours is Proposition Bank
(PropBank) [24] — “a corpus of text annotated with information about basic semantic propositions.” The goal of PropBank is to define a methodology for mapping nouns
in a sentence into arguments of the verb in that sentence. PropBank arguments correspond loosely to relations of CLIB, but a PropBank argument may reflect the meaning
of one or more CLIB relations (e.g., Arg0 denotes both agents and experiencers). As a
result, the task we address is much more difficult than the one of PropBank.
One of the resources used by annotators of PropBank texts is a database describing
the arguments associated to each verb in a selected vocabulary. For instance, the arguments specified for the verb to move are: (a) Arg0: mover (b) Arg1: moved (c) Arg2:
destination. If the same noun (entity) plays more than one role in a sentence, only the
argument with the highest rank is assigned. This solution could be used in our application as well, in order to prevent awkward translations into natural language when the
same entity appears several times in a sentence.
A second resource used by annotators is a detailed set of guidelines provided by [4]
for the mapping of nouns into arguments, with specific instructions for sentences with
different syntactic structures (e.g., declarative sentences, questions, etc.). Our work also
focuses on developing guidelines for a consistent assignment of entities to participant
relations of events, but we operate at a higher level of abstraction. We do not look at
sentences expressed in natural language directly; rather we assume that sentences are
transformed into Universal Truths first.

7 Summary and Conclusions
The work reported in this paper has been driven by the assumption that an explicit
representation of knowledge is critical for a system to support reasoning, explanation
and dialog. We described some key aspects of creating a knowledge base from a biology textbook. Even though we used specific examples from our project, there are
three broad lessons that are of interest to other projects using both manual and automated techniques for knowledge acquisition. These lessons are: (1) reformulating the

Three Lessons in Creating a Knowledge Base

17

sentences so that their abstract structure is closer to the logical form to be acquired (2)
use of a linguistically motivated upper ontology (3) use of a combination of syntactic
and semantic guidelines to specify how ontological distinctions are expressed in language. We further hope that the three lessons at a general level, and the specifics of
the guidelines that we presented, will inspire a new breed of manual, semi-automatic
and fully automatic tools for creating knowledge representations that are well-suited for
reasoning, explanation and dialog.

8

Acknowledgment

This work has been funded by Vulcan Inc. and SRI International.

18

Three Lessons in Creating a Knowledge Base

Relation

agent

object

Semantic Definition

Syntactic Definition
Biology Examples
• the grammatical subject of a
sentence in active voice
• preposition: by (sentence in pasA virus enters a cell.
The entity that initiates, performs,
sive voice)
A cell is penetrated by a virus.
or causes an event.
(Assume that biological entities
like protein, bacteria, etc., can be
agents too.)
A virus enters a cell.
The entity that is acted upon by • the grammatical object of a sen- A cell is penetrated by a virus.
an event; the main passive partic- tence in active voice
... the penetration of a cell by a
ipant in the event.
• preposition: of
virus.

The entity that is used (by the
• preposition: with / preceded by:
An animal walks using its legs.
agent if there is one) to perform
using
an event.
The Calvin cycle uses the ATP
• the grammatical object of verbs and NADPH to produce sugar.
The entity/ material used as input
like to use, to consume, etc.
Water is converted to hydrogen.
raw-material
for an event.
• preceded by: using
Chemicals are transported, using energy.
• the grammatical object of verbs
Plants produce their own sugars
The entity that comes into exis- like to produce, to create, etc.
by photosynthesis.
result
• preposition: to / preceded by:
tence as a result of an event.
Water is converted to hydrogen.
producing
The entity that releases the object
Heat is transfered from the
of an event (possibly unintention- • preposition: from
donor
warmer body to the cooler body.
ally).
The entity that receives (takes
Heat is transferred from the
recipient
possession of) the object of an • preposition: to
warmer body to the cooler body.
event.
Water moves into a cell.
An entity that the event references
Water moves out of a cell.
base
as something major or relatively Irregular – depends on the verb.
A signal molecule attaches to a
fixed.
receptor protein.
The entity that benefits from an
• preposition: for
beneficiary
event.
For a sentence containing a verb
describing an emotional or psyPlants sense gravity and the dichological action:
rection of light.
The entity that experiences an
experiencer
• the sentence subject (sentence
Gravity and the direction of light
event.
in active voice)
are sensed by plants.
• preposition: by (sentence in passive voice)
The place where an event (typiWater moves from a hypotonic so• preposition: from
origin
cally a movement) begins.
lution to a hypertonic solution.
The place where an event (typiWater moves from a hypotonic
• preposition: to
destination
cally a movement) ends.
solution to a hypertonic solution.
The place away from which an
The plasma membrane pulls
away-from event transpires, but not necessar- • preposition: away from
away from the wall.
ily where the event starts.
The place toward which an event
Daughter chromosomes move totoward
transpires, but not necessarily • preposition: toward
ward opposite ends of the cell.
where the event ends.
The place (or other entity) along • preposition: across, along, A protein moves into a cell
path
or through which an entity moves. through
through a pore.
The specific place of some effect
The protein binds at the groove
site
of an event, as opposed to the lo- • preposition: at
with the target molecules of baccale of the event itself.
terial walls.
instrument

Table 4: Summary of guidelines for mapping entities into slots.

Bibliography

[1] Eva Banik, Eric Kow, and Vinay K. Chaudhri. User-controlled, robust natural
language generation from an evolving knowledge base. In ENLG 2013 : 14th
European Workshop on Natural Language Generation, 2013.
[2] K. Barker, T. Copeck, S. Delisle, and S. Szpakowicz. Systematic construction of
a versatile case system. Journal of Natural Language Engineering, 3(4):279–315,
1997.
[3] K. Barker, B. Porter, and P. Clark. A library of generic concepts for composing
knowledge bases. In First International Conference on Knowledge Capture, 2001.
[4] Claire Bonial, Jena Hwang, Julia Bonn, Kathryn Conger, Olga Babko-Malaya,
and Martha Palmer. English PropBank annotation guidelines, 2012.
[5] Stefano Borgo and Paulo Leitão. The role of foundational ontologies in manufacturing domain applications. LNCS, 2004.
[6] Stefano Borgo and Claudio Masolo. Foundational choices in DOLCE. In Steffen Staab and Ruder Studer, editors, Handbook on Ontologies. Springer, second
edition, 2009.
[7] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. Toward an architecture for never-ending language learning. In AAAI, 2010.
[8] Vinay K Chaudhri, Britte Cheng, Adam Overholtzer, Jeremy Roschelle, Aaron
Spaulding, Peter Clark, Mark Greaves, and Dave Gunning. Inquire Biology: A
textbook that answers questions. AI Magazine, 34(3), September 2013.
[9] Vinay K. Chaudhri, Nikhil Dinesh, and Craig Heller. Conceptual models of structure and function. Technical report, SRI International, 2013.
[10] Vinay K. Chaudhri, Stijn Heymans, Michael Wessel, and Son Cao Tran. Query
answering in object oriented knowledge bases in logic programming. In Workshop
on ASP and Other Computing Paradigms, 2013.
[11] Vinay K. Chaudhri, Michael A. Wessel, and Stijn Heymans. KB Bio 101: A
challenge for OWL Reasoners. In The OWL Reasoner Evaluation Workshop,
2013.
[12] P. Clark, J. Thompson, K. Barker, B. Porter, V. Chaudhri, A. Rodriguez,
J. Thomere, S. Mishra, Y. Gil, P. Hayes, and T. Reichherzer. Knowledge entry
as the graphical assembly of components. In First International Conference on
Knowledge Capture, 2001.
[13] The Gene Ontology Consortium. Gene ontology: tool for the unification of biology. Nat Genet, 25:25–29, 2000.
[14] M. Courtot, N. Juty, C. Knpfer, D. Waltemath, A. Zhukova, A. Drger, M. Dumontier, A. Finney, M. Golebiewski, J. Hastings, S. Hoops, S. Keating, D.B. Kell,
S. Kerrien, J. Lawson, A. Lister, J. Lu, R. Machne, P. Mendes, M. Pocock, N. Rodriguez, A. Villeger, D.J. Wilkinson, S. Wimalaratne, C. Laibe, M. Hucka, and
N. Le Novre. Model storage, exchange and integration. Molecular Systems Biology, 7(543), 2011.

20

Three Lessons in Creating a Knowledge Base

[15] Nikhil Dinesh, Aravind K. Joshi, Insup Lee, and Oleg Sokolsky. Logic-based
regulatory conformance checking. In Monterey Workshop, pages 147–160, 2007.
[16] Aldo Gangemi, Nicola Guarino, Claudio Masolo, and Alessandro Oltramari.
Sweetening Wordnet with DOLCE. AI Magazine, 24(3):13–24, 2003.
[17] P. Grenon, B. Smith, and L. Goldberg. Applying BFO in the biomedical domain.
Health Technology and Informatics, 102:20–38, 2004.
[18] Heinrich Herre, Barbara Heller, Patryk Burek, Robert Hoehndorf, Frank Loebe,
and Hannes Michalek. General formal ontology (GFO): A foundational ontology integrating objects and processes. http://www.onto-med.de/
ontologies/gfo/, 2013.
[19] Graeme Hirst. Ontology and the lexicon. In Handbook on ontologies, pages 269–
292. Springer, 2009.
[20] S. M. Lloyd, editor. Roget’s Thesaurus. Longman, 1982.
[21] Andrew McCallum and Wei Li. Early results for named entity recognition with
conditional random fields, feature induction and web-enhanced lexicons. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL
2003-Volume 4, pages 188–191. Association for Computational Linguistics, 2003.
[22] George A. Miller and Christiane Fellbaum. Wordnet then and now. Language
Resources and Evaluation, 41(2):209–214, 2007.
[23] Ian Niles and Adam Pease. Towards a standard upper ontology. In Proceedings of
the international conference on Formal Ontology in Information Systems - Volume
2001, FOIS ’01, pages 2–9, New York, NY, USA, 2001. ACM.
[24] Martha Palmer, Dan Gildea, and Paul Kingsbury. The Proposition Bank: A corpus
annotated with semantic roles. Computational Linguistics Journal, 31(1), 2005.
[25] Alan Rector, Luigi Iannone, and Robert Stevens. Quality assurance of the content
of a large DL-based terminology using mixed lexical and semantic criteria: experience with SNOMED CT. In Proceedings of the sixth international conference on
Knowledge capture, K-CAP ’11, pages 57–64, New York, NY, USA, 2011. ACM.
[26] Jane B. Reece, Lisa A. Urry, Michael L. Cain, Steven A. Wasserman, Peter V. Minorsky, and Robert B. Jackson. Campbell biology. Benjamin Cummings imprint
of Pearson, Boston, 2011.
[27] Brian C. Smith. Reflection and Semantics in a Procedural Language. PhD thesis,
Massachusetts Institute of Technology, 1982.
[28] Kent A. Spackman and Guillermo Reynoso. Examining SNOMED from the perspective of formal ontological principles: Some preliminary analysis and observations. In KR-MED, pages 72–80, 2004.
[29] A. D. Spear. Ontology for the twenty first century: An introduction with recommendations. http://www.ifomis.org/bfo/documents/manual.
pdf, 2006.
[30] D. Summers, editor. Longman Dictionary of Contemporary English. Longman,
1987.
[31] Amy Y Wang, Jeremiah H Sable, and Kent A Spackman. The SNOMED clinical
terms development process: refinement and analysis of content. Proc AMIA Symp,
pages 845–9, 2002.
[32] Yue Wang, Michael Halper, Hua Min, Yehoshua Perl, Yan Chen, Kent A. Spackman, All Communications To, and Yehoshua Perl. Structural methodologies for
auditing SNOMED.

Qualitative Analysis of Cotemporary Urdu Machine
Translation Systems
Asad Abdul Malik, Asad Habib
Kohat University of Science and Technology, Kohat, Pakistan

asad_12204@yahoo.com, asadhabib@kust.edu.pk

Abstract. The diversity in source and target languages coupled with source
language ambiguity makes Machine Translation (MT) an exceptionally hard
problem. The highly information intensive corpus based MT leads the MT research field today, with Example Based MT and Statistical MT representing
two dissimilar frameworks in the data-driven paradigm. Example Based MT is
another approach that involves matching of examples from large amount of
training data followed by adaptation and re-combination.
Urdu MT is still in its infancy due to nominal availability of required data
and computational resources. This paper provides a detailed survey of the
aforementioned contemporary MT techniques and reports findings based on qualitative analysis with some quantitative BLEU metric quantitative results.
Strengths and weaknesses of each technique have been brought to surface
through special focus and discussion on examples from Urdu language. The paper concludes with proposal of future directions for research in Urdu machine
translation.
Keywords: Urdu Machine Translation, Qualitative Comparison, Rule Based
MT, Statistical MT, Example Based MT

1

Introduction

Representing text in one natural language, the source language (SL) into another, the
target language (TL) is as old as the written literature [1]. At present, the need of
translation is continuously growing in business, economy, medical and many other
fields. The growth in science and technology in general and computer based solutions
in particular have paved the way to the concept of automatic translation called the
Machine Translation (MT) [2].
1.1

Urdu

Urdu ranks 19th among the 7,105 languages spoken in the world1. It is one the mostspoken languages in South Asia [3]. It is also spreading in the West due to the large
1

http://www.ethnologue.com/statistics/size

Diaspora of Indo-Pak Subcontinent citizens. Urdu is the national language of Pakistan
and it is used i) as medium of teaching in most of the public schools ii) for junior to
mid level administration and iii) in the mass print and electronic media. It is not only
spoken in Pakistan but also in India, Bangladesh, Afghanistan and Nepal. Also it has
become the culture language and lingua franca of the South Asian Muslim Diaspora
outside the Indo-Pak subcontinent, mainly in the Middle East, Europe, Canada and the
United States [4].
1.2

Urdu Machine Translation (UMT)

In spite of the large number of speakers around the world, there are very few computational natural language tools available for Urdu. It is a morphologically rich language having many other distinct linguistic characteristics. On the contrary it is still
an under-resourced language from the point of view of computational research. We
could not find any public domain machine translation tool(s) developed specifically
for Urdu. However some trace of basic MT techniques has been discovered [5-9]. In
the current work we presented a detailed survey on the contemporary research in
UMT. We identified the weaknesses and strengths of each technique and proposed the
guidelines for future directions in UMT research.

2

Literature Survey

Some traces of basic UMT research are presented in this section. Naila et al [5] presented a Rule Based English to Urdu Machine Translation (RBMT) technique primarily based on the transfer approach that tries to handle the case phrases and verb postpositions using Paninian grammar. Statistical Machine Translation (SMT) between
languages with word order differences was discussed by Bushra et al [6]. Example
Based Machine Translation (EBMT) approach was introduced by Maryam and Asif
that translates text form English to Urdu that supports idioms and homographs [7].
Parallel corpus for statistical machine translation for English to Urdu text was presented by Aasim et al [8]. Word-Order Issues in English-to-Urdu have been investigated by Bushra and Zeman [9]. In addition, SMT systems such as Google2 and Bing3
are already available online. However these systems offer poor translation quality and
limited accuracy due to issues related to Urdu syntax and other intrinsic linguistic
features.

Fig. 1. Paradigms for Machine Translation

2
3

http:// translate.google.com
http://www.bing.com/translator

Contemporary Machine Translation techniques can be broadly categorized into
three paradigms as shown in Figure 1.
2.1

Rule Based Machine Translation (RBMT)

To provide suitable rules for translation, the RBMT needs linguistic knowledge of
source as well as the target language. Translation depends on formalized linguistic
knowledge represented in lexicon along with grammars [10]. RBMT is described by
several characteristics; it has firm set of well fashioned rules, several rules rely on
present linguistic theories and the grammatical errors are prohibited. The major advantage of RBMT is that if the required knowledge is not found in available literature
then ad-hoc heuristic rules are applied [5]. This system contains input sentence analyzer (morphological, syntactic and semantic analysis) and procedures for producing
output (structural transfers and inherent Inter-lingua structures).
2.2

Statistical Machine Translation (SMT)

Two models are built in SMT; i) Translation model and ii) Language model. A translation model gives probability of a target sentence given source sentence P(T/S) whereas the language model determines the probability P(S) of the string of target language actually occurring in that language. By using the language model and conditional probabilities of translation model, P(S/T) is calculated using the following formula:
𝑃 𝑆 𝑃
𝑆
𝑃
=
𝑇
𝑃 𝑇

𝑇
𝑆

Probability based analysis of MT is part of SMT. It has numerous diverse applications such as those in word sense disambiguation or structural disambiguation etc.
[11]. The SMT techniques do not need explicit encoding of the linguistic information.
It highly depends upon availability of fine and very large amount of bilingual data
that presently does not exist for Urdu and other languages spoken in the Indo-Pak
Subcontinent region.
2.3

Example Based Machine Translation (EBMT)

Somers referred to EBMT as a hybrid approach of RBMT and SMT [12]. Like SMT,
it is depended upon a corpus of available translations. That is why it is similar to (often confused with) translator‟s aid known as Translation Memory (TM). EBMT and
TM both involve comparison of input text with the database of real examples and then
find out the nearest match. In TM, a translator selects the candidate target text whereas EBMT makes use of automated procedures that identify the translation fragments. Recombination of these fragments produces the target text [10].
Thus the process is split into three phases [10]. i) “Matching” fragments against the
available database of real examples (that are common between EBMT and TM), ii)

“Alignment” identifying corresponding translation fragments and finally iii) “Recombination” that gives the target text. EBMT needs a database of parallel translations
that are searched for source language phrases or sentences and their nearest matching
target language components are generated as output [11].
EBMT saves the translation examples in different manners. In simple case, examples are saved as pairs of strings with no extra information related to them.

3

Methodology

In this section we discuss the methodologies of three major Machine Translation
techniques. English is considered as source language and Urdu as the target language.
We compare the strengths and weaknesses of these techniques in Section 4.
3.1

Rule Based Machine Translation

There are three stages in RBMT; i) Analysis, ii) Transfer and iii) Synthesis

Fig. 2. RBMT Model

Analysis.
The source text is analyzed based upon lexicon and grammar rules of source language. Word segmentation is done and each word is annotated by appropriate POS
tag and parse tree of input text is created. A parse tree for the input text “I called you
several times” is created as shown in figure 3.
Transfer.
In this stage, parse tree of source language text is „transferred‟ into parse tree of desired target language according to the lexicon and structural rules of the target language. English is SVO (Subject, Verb, Object) language whereas Urdu is SOV language. Re-ordering of words is inevitable in order to generate the output parse tree as
shown in Figure 4.

English to Urdu Translation Rules.
Some coarse grained rules for translation from English to Urdu are mentioned in
the following.
1. NP in both languages follows the same rule. So swapping in not required.
2. If NP is having NP and PP, then transform it as in Urdu PP comes before NP.
English
𝑁𝑃 → 𝑁𝑃 + 𝑃𝑃
Urdu
𝑁𝑃 → 𝑃𝑃 + 𝑁𝑃
3. If adverb phrase (AP) appears before verb then swapping is not needed. AP in English can appear in different order depending on the type of AP, however Urdu prefers AP before verb.
Urdu
AP +V
4. In Urdu, Verb phrase (VP) is inflected according to gender, number and person
(GNP) of the head noun while NP depends upon tense, aspect and modality of the
verb phase (VP). Urdu adjectives are also modified by GNP of the head noun.

Fig. 3. English Parse Tree

Fig. 4. Parse Tree (transferred in SOV)

Synthesis.
Finally, the target language lexicon and grammar is used to convert the parse tree of
target language to the target language surface form. It requires two independent monolingual dictionaries so that appropriate surface form of target language can be generated.
As shown in figure 5 the source text “I called you several times” is translated into
“‫ ”میں کئی مرتبہ آپ کو بالیا‬using RBMT.

Fig. 5. Urdu parse tree

3.2

Statistical Machine Translation (SMT)

SMT makes use of i) Translation Model, ii) Language Model and iii) Decoder Algorithm.

Fig. 6. SMT Model

Translation Model.
Words and phrases in the source text are matched against the target language strings.
If the strings are matched the model assigns a probability value P(T/S) to it. This
probability shows that what are the chances that the input text string is present in the
output or target language. These probability values are pre-assigned in a parallel corpus through human translation. Subsequently machine learning techniques are used to
improve the system depending upon the human translated text.
Language Model.
Language model determines the probability P(S) of output text string. It does not
require a parallel corpus. It requires text in only one language. We can calculate the
value by using N-gram model. In this the probability of occurrence of sentence of
length N is the product of probability of each kth word given the occurrence of previous words k-1 and k-2.
Decoder Algorithm.
After finding the product of translation and language model the decoder algorithm
selects the string of output text language with the highest probability value based on
the stochastic formula mentioned in Section 2.2.
3.3

Example Based Machine Translation (EBMT)

English to Urdu EBMT is divided into four phases; i) Sentence Fragmentation, ii)
Search in Corpus, iii) N-ary Product based Retrieval and iv) Ordering of Translated
Text.

Sentence Fragmentation.
For better handling of input sentence by translator, it is better to break the sentence
into phrases. On the other hand same results are achieved by storing sentence in the
corpus and by gaining a broad coverage by fragmenting and combining using genetic
algorithm at run time for obtain new sentences. Fragmentation of a sentence into
phrases is handled by using concept of idioms, cutter points and connecting words.

Fig. 7. EBMT Model

Searching in Corpus.
Bilingual corpus is searched for finding whether the input phrase is accessible or not.
If the system is unable to locate exact match, then in that situation it will look for the
nearest match. Closeness is calculated by threshold at two stages; i) for exact match
and ii) for nearest match. This is done by two algorithms “Levenshtein Algorithm”
and “Semantic Distance Algorithm”.
N-ary Product Based Retrieval.
The translation for an input sentence is extracted in this stage. And there is possibility
that input can have many translations. So the possibilities are collected and the idea of
n-ary product is used to record all the feasible sentences.
Ordering of Translated Phrases.
If a single input sentence is divided into pieces and translated into output language
phrase, then ordering of these translated phrases are done in this phase.

4

Comparison

4.1

Rule Based Machine Translation

The quality of translation in Rule Based Machine Translation (RBMT) depends upon
large number of rules. Therefore its computational cost is very high. Rules are based
on both source and target languages, their respective morphological, syntactical and

semantic structures. With a large set of large and fine grained linguistic rules, RBMT
generates translation with acceptable quality, but developing system like this needs
more time and man hours because this type of linguistic recourses should be hand
crafted (Knowledge Acquisition Problem). As RBMT works with exact matches, it is
unable to translate text when system does not have enough knowledge about the input. It is also difficult to add more rules for generating high quality output.
4.2

Statistical Machine Translation

The knowledge about translation is acquired automatically from the example data.
This is the main reason why SMT is developed fast as compared to RBMT. In a situation where large corpus is available but linguistic knowledge is not readily available
then SMT is a preferred method. When input and output languages are not complex
morphologically then SMT techniques generate better results. SMT based approaches
do not need Bilingual dictionaries. They depend upon the quality of bilingual corpus.
4.3

Example Based Machine Translation

It requires Bilingual dictionary. It translates text by adapting to examples. The computational cost is less than RBMT. By storing proper examples in the DB the system
can be upgraded. It works on best matching reasoning, so therefore when the corresponding example is not available in corpus, the translation process becomes complicated. It translates in fail-safe way. Quality of translation depends upon the difference
between input text and lookup results for similar examples. EBMT can also notify us
that when its translation is improper.
Table 1. Comparison of RBMT, SMT and EBMT

5

Findings

The qualitative findings are tabulated in table 1, and the quantitative findings are
mentioned in table 2. The BLEU metric is used for the evaluation of the machine
translated text, five reference sentences were used for calculating the BLEU value.
From the value of the BLEU it is clearly shown that EBMT performs better than the
rest of the three systems. RBMT was found to be better than both the SMT systems.
Out of the two SMT (Google and Bing), Bing translator gave better results than the
Google translator.
Table 2. BLEU value of RBMT, EBMT and SMT

BLEU Value

6

RBMT

EBMT

0.8

0.8421

SMT
Google
0.6268

Bing
0.709

Discussion

After detailed literature study and investigation of the above mentioned three MT
systems, we can conclude that for languages with similar lexical and syntactic structure e.g. Urdu and Hindi, the Rule based MT technique gives better results. The SMT
systems perform better if necessary resources such as annotated corpora etc. are available. At present, most of the systems translate text from source to target language on
the basis of single sentence whereas in real life text for translation is much larger than
one sentence. Nonetheless, the continuous process of repetitive translation and improvements by human annotators contribute significantly to any MT system.

7

Conclusion and Future Directions

In this paper we explained three main techniques of machine translation; Rule Based
Machine Translation, Statistical Machine Translation and Example Based Machine
Translation. We explained the methodology of each of these systems and found their
comparison based on their respective outputs using carefully selected text. Our current work is preliminary in nature. However it reports significant results based on
qualitative analysis.
In order to contribute a significant role to UMT research, at present we are in the
process of building the required corpora. We intend to use our corpora to conduct
larger scale automated experiments and report quantitative results that are comparable
to human translators. Based on our qualitative and quantitative results, we aim at proposing a new model that minimizes flaws in the existing Urdu MT systems. Ideally,
we would like to implement our proposed system with fewer requirements of computational and human resources.

8

REFERENCES

1. Abdullah, H., Homiedan.: Machine translation. J. King Saud Uni. Lang. & Trans. 10, 1-21
(1998)
2. Hutchins, J.: Latest Development in Machine Translation Technology: Beginning a New
Era in MT RESEARCH. MT Summit IV, 11-34, Kobe, Japan (1993)
3. Lewis, Paul, M., Simons, G.F., Fennig, C.D.: Ethnologue: Language of the World. Seventeenth edition. Dallas, Texas: SIL International (2013)
4. Schmidt, R.L.: Urdu An Essential Grammar. Rutledge Taylor & Francis Group. London
and New York (2004)
5. Ata, N., Jawaid, B., Kamran, A.: Rule Based English to Urdu Machine Translation. Proceedings of Conference on Language and Technology (CLT‟07). (2007)
6. Jawaid, B., Zeman, D., Bojar, O.: Statistical Machine Translation between Languages with
Significant Word Order Difference. Prague (2010)
7. Zafar, M., Masood, A.: Interactive English to Urdu Machine Translation using ExampleBased Approach. IJCSE 1 (3), 276-283 (2009)
8. Ali, A., Siddiq, S., Malik, M.K.: Development of Parallel Corpus and English to Urdu Statistical Machine Translation. IJET-IJENS 10(5), 31-33 (2010)
9. Jawaid, B., Zeman, D.: Word-Order Issues in English-to-Urdu Statistical Machine Translation. Prague Bull. Math. Linguistics. 87-106 (2011)
10. Survey of Machine Translation Evaluation. EuroMatrix. (2007)
11. Samantaray, S.D.: Example based machine translation approach for Indian languages.
ICCS. 1-10 (2004)
12. Somers, H. :Machine translation and Welsh: The way forward. A Report for the Welsh
Language Board, Centre for Computational Linguistics, UMIST, Manchester (2004)

The NL2KR system
Chitta Baral, Juraj Dzifcak, Kanchan Kumbhare, and Nguyen H. Vo
School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University, Tempe, Arizona, USA
chitta@asu.edu, Juraj.Dzifcak@asu.edu, krkumbha@asu.edu,
Nguyen.H.Vo@asu.edu

Abstract. In this paper we will describe the NL2KR system that translates natural language sentences to a targeted knowledge representation
formalism. The system starts with an initial lexicon and learns meaning of new words from a given set of examples of sentences and their
translations. We will describe the first release of our system with several
examples.
Keywords: Natural Language Understanding, Lambda Calculus, Knowledge Representation

1

Introduction and Motivation

Our approach to understanding natural language involves translating natural
language text to formal statements in an appropriate knowledge representation
language so that a reasoning engine can reason with the translated knowledge
and give a response, be it an answer, a clarifying question or an action. To
translate natural language text to a formal statement we propose to use the
compositional method of Montague [1] where the translation (or meaning) of
words are given as lambda calculus formulas and the meaning of phrases and
sentences are obtained by composing the meaning of the constituent words.
The challenge in doing this is in coming up with appropriate lambda calculus
expressions for each word. The challenging aspects in this are: (a) the number
of words may be huge, (b) the lambda calculus expression (or meaning) of some
words are too complicated for humans to come up with it, and (c) the lambda
calculus expressions for the words are target language specific; so it is not a one
time affair like compiling traditional dictionaries. To address these challenges we
use an inverse lambda algorithm [2] that computes the meaning of a word/phrase
G when the meaning of the word/phrase H and the phrase GH (or HG) is known.
The NL2KR system uses an initial lexicon containing some words and their
meanings and a set of training corpus containing sentences in natural language
and their translations to learn new meanings of words. The system then uses the
new learned lexicon to translate new sentences. In this paper, we would like to
give an overview of the NL2KR system and examples of using it.

2

2

Chitta Baral, Juraj Dzifcak, Kanchan Kumbhare, and Nguyen H. Vo

Overview

Shown below in Fig. 1 is the architecture of the NL2KR system. It has two subparts which depend on each other (1) NL2KR-L for learning and (2) NL2KR-T
for translating.

Fig. 1. Architecture of the NL2KR system: NL2KR-T on the left and NL2KR-L on
the right

The NL2KR-L sub-part takes an initial lexicon consisting of some words and
their meanings in terms of λ-calculus expressions & a set of training sentences
and their target formal representations as input. It then uses a Combinatorial Categorical Grammar (CCG) parser to construct the parse trees. Next, the
learning sub-part of the system uses Inverse-λ and Generalization algorithms to
learn meanings of newly encountered words, which are not present in the initial
lexicon, and adds them to the lexicon. A parameter learning method is then
used to estimate a weight for each lexicon entry (word, its syntactic category
and meaning) such that the joint probability of the sentences in the training
set getting translated to their given formal representation is maximized. The result of NL2KR-L is the final lexicon, which contains a larger set of words, their
meanings and their weights.
Once the training component finishes its job, the translation sub-part (NL2KRT) uses this updated lexicon and translates sentences using the CCG parser.
Since words can have multiple meanings and their associated λ-calculus expressions, weights assigned to each lexical entry in the lexicon helps in deciding the
more likely meaning of a word in the context of a sentence.

3

Using NL2KR

The latest version of NL2KR system can be downloaded from
http://nl2kr.engineering.asu.edu. It can be run on Linux (64 bit) and OS-X.

The NL2KR system

3.1

3

Third Party Software Used by NL2KR

The current version of NL2KR uses the following libraries and tools developed
by others:
–
–
–
–
–
3.2

Scripting language Python (version ≥ 2.6)
AspCcgTk version 0.3 1
Stanford Log-linear Part-Of-Speech Tagger 2 (version 3.1.5)
Oracle Java (version ≥ 1.6)
ASP grounder and solver: gringo (version 3.x), clasp (version 2.x) and clingo
(version 3.x) 3
Installation guide

The NL2KR package contains a readme file and a zipped file which contains
–
–
–
–

AspCcgTk
Jar file and models of Stanford Log-linear Part-Of-Speech Tagger
Jar file and configurations for NL2KR
Gringo, clasp and clingo

After unzipping the package, the instruction in the readme file directs how to
install NL2KR.
3.3

Files/Folders in the package

Following is a brief list of important files/folders in the package:
–
–
–
–
–
–
–
–
–
–
–
–
–
1
2
3

README: Installation instruction and examples of how to use NL2KR.
NL2KR.jar: NL2KR’s classes packed in a jar file.
config.properties: Default configuration of the NL2KR system.
install.sh: Script to install NL2KR.
ccgParser.sh: Script to get a CCG parse tree of a given sentence.
Generalization.sh: Script that gives generalized meanings of a word.
Inverse.sh: Script to compute the inverse using the inverse lambda algorithms.
Lambda.sh: Script to do application operation, given a function and an
argument in λ-calculus.
NL2KR-L.sh: Script to run the NL2KR-L sub-part.
NL2KR-T.sh: Script to run the NL2KR-T sub-part.
RunConfiguration: Example inputs of the preceding scripts.
resources: Folder containing AspCcgTk package, gringo, clasp and clingo.
examples: Folder containing examples in various domains for NL2KR-L and
NL2KR-T.

http://www.kr.tuwien.ac.at/staff/former staff/ps/aspccgtk
http://nlp.stanford.edu/software/tagger.shtml
http://potassco.sourceforge.net/

4

Chitta Baral, Juraj Dzifcak, Kanchan Kumbhare, and Nguyen H. Vo

3.4

Executing the scripts

To execute any of the scripts one needs to go to the NL2KR’s root directory and
run
./ < script name > < R unC onfi g urat ion file > [ Optional params ]

where script name is the name of one of the six scripts (e.g. ./Lambda.sh),
RunConfiguration file contains corresponding parameters for the script, and
optional parameters are for the Java Virtual Machine (JVM) to execute the
module that corresponds to the script. For learning and testing large dataset
with NL2KR-L or NL2KR-T, it is recommended to provide more memory for
the JVM and enable garbage collection if needed (i.e. Use -Xmx and -XX:UseGCOverheadLimit).
3.5

Lambda application

To use the lambda application script, the function and the argument of the
lambda application need to be provided. For example, the following snippet in
the RunConfiguration file specifies that we need to apply the argument #x.x@mia
to the function #y.#x.loves(x, y) (note: # is for λ).
function =# y .# x . loves (x , y )
argument =# x . x@mia

The output of the lambda application is f unction@argument. According to
lambda application, the y variable is replaced by #x.x@mia and the result is
#x.loves(x, #x0.x0@mia), where x in the argument is renamed to x0. Running
the lambda application script with the preceding configuration yields the output:
Function =# y .# x . loves (x , y )
Argument =# x . x@mia
Result = # x . loves (x ,# x0 . x0 @ mia )

However, in the following configuration, the argument cannot be applied to
the function since there is no free variable in the function.
function = loves (x , y )
argument = # x . x @ mia

The output thus would be
Function = loves (x , y )
Argument = # x . x @ mia
Cannot apply the argument # x . x @ mia to the function loves (x , y )

3.6

Inverse application

Given two lambda expressions g and h, the lambda application gives us f = g@h
or f = h@g. But sometime, we have only f and g and we need to find h. The
inverse application allow us to calculate the lambda expression h so that f = g@h
or f = h@g. More details about the inverse application can be found in [2]. For
inverse application, we need to provide the parent (lambda expression f ), the
right child (r) or the left child (l). Given one child, the module will calculate the
other child so that f = l@r.

The NL2KR system

5

In the first example below, since there does not exist a lambda expression
h (right child) so that mia@h = #x.loves(x, mia), the inverse algorithm returns right child expression = null. However, when mia is the right child, inverse
lambda returns Lef tchild = #x1.#x.loves(x, x1) because #x1.#x.loves(x, x1)@mia
= #x.loves(x, mia). In case the CCG parse tree specifies that the meaning of
“Mia” must be in the left child, using left child as #x.x@mia instead of mia
will do the trick and let us have the same right child: #x1.#x.loves(x, x1).
Example 1. Input:
parent =# x . loves (x , mia )
left_child = mia

Output:
Parent = # x . loves (x , mia )
Left child = mia
Right child = null

Example 2. Input:
parent =# x . loves (x , mia )
right_child = mia

Output:
Parent = # x . loves (x , mia )
Right child = mia
Left child = # x1 .# x . loves (x , x1 )

Example 3. Input:
parent =# x . loves (x , mia )
left_child =# x . x @ mia

Output:
Parent = # x . loves (x , mia )
Left child =# x . x @ mia
Right child = # x1 .# x . loves (x , x1 )

3.7

Generalization

The inverse lambda module is not always adequate to learn new meanings of
words when we lack meaning of words that will allow us to use the inverse
lambda module. To address that we have developed a generalization module in
NL2KR system, where the generalization technique described in [2] is implemented. For example, if we want to find the meaning of the word plays with the
category (S\N P )/N P using generalization, we can use the lexical entry (eats,
(S\N P )/N P , λy.λx.eats(x, y)) in the lexicon, where the category of the word
eats is same as that of the word plays. The generalization module will add a new
lexical entry (plays, (S\N P )/N P , λy.λx.plays(x, y)) to the lexicon. The input
of the generalization module is the file path for lexicon (existing dictionary) and
a new word, which we want to generalize. Following is an illustration of the use
of generalization with the RunConfiguration file having the following:

6

Chitta Baral, Juraj Dzifcak, Kanchan Kumbhare, and Nguyen H. Vo

lexicon =./ examples / sample / dictionary . txt
word = Mia
ccg = N

where dictionary.txt contains

5

Vincent
N
vincent
Vincent
N
# x . vincent ( x )
takes ( S \ NP ) / NP
# w . # z . ( w@ # x . takes (z , x ) )
plane N
# x . plane ( x )
boxer N
# x . boxer ( x )
fights
S \ NP # x . fight ( x )

In this case, the word Mia can be obtained by generalization from the words
Vincent, plane and boxers, each of category N (meaning noun); and the output
is
New lexical items learned through Generalization :
Mia
[N]
# x . mia ( x )
Mia
[N]
mia

We can restrict generalization by modifying the config.properties file. For
example, adding the following snippet to config.properties will skip the generalization process for NP and N categories, and generalization for the words: on,
of, by and in.
G E N E R A L I Z A T I O N _ D _ E X C L I S T =[ NP ] ,[ N ]
G E N E R A L I Z A T I O N _ D _ P R E P O S I T I O N L I S T = on , of , by , in

3.8

CCG parser

The input of the CCG parser module is the sentence we want to parse and an
optional path of the file containing the words and their additional categories
we want to use. The parser will parse the input sentence and output its CCG
parse tree. Our CCG parser is based on ASP ccgT k [3] with some modifications
such as our use of the Stanford Part-Of-Speech tagger [4] instead of the C&C
Part-Of-Speech tagger [5] to improve accuracy. Following is an example snippet
of the RunConfiguration file.
sentence = ’ Every boxer walks ’
syntaxFile =./ examples / sample / syntax . txt

where syntax.txt contains

5

takes ( S \ NP ) / NP
Every ( S /( S \ NP ) ) / NP
Some ( S /( S \ NP ) ) / NP
walks S \ NP
fights
S \ NP
loves ( S \ NP ) / NP

The output of the CCG parser is a parse tree of the sentence “Every boxer
walks” in ASP format as follows

5

10

nl2kr_token ( t1 , " Every " , "( S /( S \ NP ) ) / NP " , 1) .
nl2kr_token ( t2 , " boxer " , " NP " , 2) .
nl2kr_token ( t3 , " walks " , " S \ NP " , 3) .
nl2kr_token ( t4 , " Every boxer " , " S /( S \ NP ) " ,1) .
nl2kr_token ( t5 , " Every boxer walks " , " S " , 1) .
nl2 k r_ c hi ld _ le ft ( t4 , t1 ) .
n l 2 k r _ c hi l d_ r ig h t ( t4 , t2 ) .
nl2 k r_ c hi ld _ le ft ( t5 , t4 ) .
n l 2 k r _ c hi l d_ r ig h t ( t5 , t3 ) .
n l 2 k r _ v a l i d _ r o o t N o d e ( t5 ) .

The NL2KR system

7

The predicate nl2kr valid rootN ode is used to specify the root node of the parse
tree (corresponds to the whole sentence). nl2kr token is used to specify nodes
in the tree and nl2kr child lef t, nl2kr child right are for the left child and the
right child of a node. The four atoms of nl2kr token are respectively the node
ID, the corresponding phrase, its CCG category and its starting position in the
sentence.
3.9

NL2KR-L: the learning module of NL2KR

To run the learning module NL2KR-L we need to set the initial lexicon file
path, the override file for syntax categories (optional), the training data file and
the output dictionary file path (optional) in the RunConfiguration file of the
NL2KR-L. Following is an example snippet of the RunConfiguration file.
Ldata =./ examples / sample / train . txt
Ldictionary =./ examples / sample / dictionary . txt
Lsyntax =./ examples / sample / syntax . txt
Loutput =./ examples / sample / dic tion ary_t rai n . out

For example, the preceding snippet specifies that: the training data is in ./examples/train.txt, the initial lexicon is in ./examples/sample/dictionary.txt,
and the override syntax categories are in ./examples/sample/syntax.txt.
The override syntax categories will be used in CCG parsing step as showed in
the previous subsection. If it is not specified, the output dictionary is saved as
dictionary train.out in ./output folder.
The training data file contains the training sentences and their formal representation such as:
Some boxer walks EX . ( boxer ( X ) ^ walk ( X ) )
John takes a plane
EX . ( plane ( X ) ^ takes ( john , X ) )
John walks walk ( john )

In the above, EX denotes ∃X.
The initial lexicon contains words and the their meanings that we already
know:

5

John N
john
takes ( S \ NP ) / NP
# w . # z . ( w@ # x . takes (z , x ) )
plane N
# x . plane ( x )
boxer N
# x . boxer ( x )
fights
S \ NP # x . fight ( x )

The NL2KR-L sub-part learns the new meanings of words in multiple iterations. It stops when it cannot learn any new word. Below we give the output of
the script with example inputs.
We start with some snippets of the running output in the following. From
line 5 to 9, NL2KR-L was checking if it has the meaning of Some boxer and
walks. It then learned the meaning of walks by generalization.
From line 15 to 19, NL2KR-L was trying to learn the meaning of Some
boxer given the meaning of walks and the meaning of the whole sentence Some
boxer walks from the training data. Using inverse lambda, it figured out that the
meaning of Some boxer is #x1.EX.boxer(X) ∧ x1@X.
NL2KR-L did not go further to the meaning of “some” because the meaning
“boxer” of boxer was not helpful.

8

Chitta Baral, Juraj Dzifcak, Kanchan Kumbhare, and Nguyen H. Vo

However, using the second parse tree where the meaning #x.boxer(X) of
boxer is used, NL2KR-L can get the meaning of some (line 42-49) : #x2.#x1.EX.x2@X∧
x1@X.

5

10

****** Learning lexicon ...
...
Processing sentence number 1
Processing Parse Tree 1
Word : Some boxer walks sem :: null
Both children do not have current lambda expression : Some boxer , walks
Generalizing for leafs with no expected lambda : walks
New lexical item Learned by Expansion : walks ////[ S \ NP ]////# x . walk ( x )
...
Processing sentence number 1
Processing Parse Tree 1

15

20

25

30

Word : Some boxer walks sem :: null
Applying inverse : EX . boxer ( X ) ^ walk ( X ) # x . walk ( x )
INVERSE_L Tried :
Some boxer walks ( H ) = EX . boxer ( X ) ^ walk ( X )
walks ( G ) = # x . walk ( x )
Some boxer ( F ) = # x1 . EX . boxer ( X ) ^ x1 @ X
Word : walks sem ::# x . walk ( x )
Word : Some boxer sem :: null
Applying inverse : # x1 . EX . boxer ( X ) ^ x1 @ X boxer
INVERSE_L Tried :
Some boxer ( H ) = # x1 . EX . boxer ( X ) ^ x1 @ X
boxer ( G ) = boxer
Some ( F ) = null
Generalizing for leafs with no expected lambda : Some
Generalizing for leafs with no expected lambda : boxer
Word : boxer sem :: boxer
Word : Some sem :: null
Processing Parse Tree 2

35

40

45

Word : Some boxer walks sem :: null
Applying inverse : EX . boxer ( X ) ^ walk ( X ) # x . walk ( x )
INVERSE_L Tried :
Some boxer walks ( H ) = EX . boxer ( X ) ^ walk ( X )
walks ( G ) = # x . walk ( x )
Some boxer ( F ) = # x1 . EX . boxer ( X ) ^ x1 @ X
Word : walks sem ::# x . walk ( x )
Word : Some boxer sem :: null
Applying inverse : # x1 . EX . boxer ( X ) ^ x1 @ X # x . boxer ( x )
INVERSE_L Tried :
Some boxer ( H ) = # x1 . EX . boxer ( X ) ^ x1 @ X
boxer ( G ) = # x . boxer ( x )
Some ( F ) = # x2 .# x1 . EX . x2 @ X ^ x1 @ X
Word : boxer sem ::# x . boxer ( x )
Word : Some sem :: null
New lexicon Learned : Some ////[( S /( S \ NP ) ) / NP ]////# x2 .# x1 . EX . x2 @ X ^ x1 @ X

At the end of the learning phase, parameter estimation is run to assign the
weights for each meaning of words. NL2KR-L then uses those meanings to check
if they are enough to translate the training sentences correctly.
****** Evaluation on training set ...

5

Processing training sentence : Some boxer walks
Predicted Result : EX . boxer ( X ) ^ walk ( X )
Correct Prediction
Processing training sentence : John takes a plane
Predicted Result : EX . plane ( X ) ^ takes ( john , X )
Correct Prediction

The NL2KR system

9

10
...

Following is the output lexicon learned with the example inputs mentioned
earlier. Each row contains a word, its CCG category, its meaning and the associated weight. Compared to the initial dictionary, we can see that NL2KR-L
learned 14 more word meanings. Note that some words such as likes and eats
are in the “syntax.txt”.

5

10

15

Some [( S /( S \ NP ) ) / NP ]
# x2 .# x1 . EX . x2 @ X ^ x1 @ X
0.0074364278
fight [ S \ NP ]
# x . fight ( x ) 0.01
boxer [ N ]
# x . boxer ( x ) 0.060000002
boxer [ N ]
boxer -0.041248113
a
[ NP / N ]
# x4 .# x2 . EX . x4 @ X ^ x2 @ X
0.009887816
John [ NP ] john 0.0073314905
John [ N ]
john 0.01
eats [( S \ NP ) / NP ] # w .# z . w @ # x . eats (z , x ) 0.01
fights
[ S \ NP ]
# x . fights ( x )
0.01
fights
[ S \ NP ]
# x . fight ( x ) 0.01
takes [( S \ NP ) / NP ] # w .# z . w @ # x . takes (z , x ) 0.01
walks [ S \ NP ]
# x . walks ( x ) -0.08722576
walks [ S \ NP ]
# x . walk ( x ) 0.10615976
plane [ N ]
# x . plane ( x ) 0.059950046
plane [ N ]
plane -0.03995005
likes [( S \ NP ) / NP ] # w .# z . w @ # x . likes (z , x ) 0.01
flies [ S \ NP ]
# x . fly ( x )
0.01
flies [ S \ NP ]
# x . flies ( x ) 0.01
loves [( S \ NP ) / NP ] # w .# z . w @ # x . loves (z , x ) 0.01

3.10

NL2KR-L in the Geoquery domain

In this subsection, we present an example of using NL2KR-L for the GEOQUERY4 domain. GEOQUERY uses a Prolog based language to query a database
with geographical information about the U.S. The input of NL2KR-L is specified
in the RunConfiguration file as:
Ldata =./ examples / geoquery / train . txt
Ldictionary =./ examples / geoquery / dictionary . txt
Lsyntax =./ examples / geoquery / syntax . txt
Loutput =

where ./examples/geoquery/train.txt contains

5

How large is texas
answer ( X ) ^ size (B , X ) ^ const (B , sid , texas )
How high is mountmckinley
answer ( X ) ^ elevation (B , X ) ^ const (B ,
pid , mountmckinley )
How big is massachusetts
answer ( X ) ^ size (B , X ) ^ const (B ,
sid , massachusetts )
How long is riogrande
answer ( X ) ^ len (B , X ) ^ const (B , rid , riogrande )
How tall is mountmckinley
answer ( X ) ^ elevation (B , X ) ^ const (B ,
pid , mountmckinley )

./examples/geoquery/dictionary.txt contains

5

How
S/S
# x . answer ( X ) ^ x@X
texas NP
# x . const (x , sid , texas )
mountmckinley
NP
# x . const (x , pid , mountmckinley )
massachusetts
NP
# x . const (x , sid , massachusetts )
riogrande
NP
# x . const (x , rid , riogrande )
is
( S \ NP ) / NP
#y. #x.x @ y

and ./examples/geoquery/syntax.txt contains
4

http://www.cs.utexas.edu/users/ml/geo.html

10

5

10

15

Chitta Baral, Juraj Dzifcak, Kanchan Kumbhare, and Nguyen H. Vo

How
S/S
texas NP
mountmckinley
massachusetts
riogrande
NP
is
( S \ NP ) / NP
rivers
NP
large NP
is
( S \ NP ) / NP
high NP
big
NP
long NP
the
NP / NP
How
S /( S \ NP )
long S \ NP
tall NP
colorado
NP
arizona
NP

NP
NP

After the learning module is executed, 15 more word meanings were learned
by NL2KR-L and the result is:

5

10

15

20

is
[( S \ NP ) / NP ] # y .# x . x @ y -0.0015125279
texas [ NP ] # x . const (x , sid , texas )
0.07666666
texas [ NP ] # x . const (x , rid , texas )
-0.023256822
texas [ NP ] # x . const (x , pid , texas )
-0.023256822
riogrande
[ NP ] # x . const (x , pid , riogrande )
-0.02315781
riogrande
[ NP ] # x . const (x , rid , riogrande )
0.07646726
riogrande
[ NP ] # x . const (x , sid , riogrande )
-0.02315781
mountmckinley
[ NP ] # x . const (x , sid , mountmckinley ) -0.055226557
mountmckinley
[ NP ] # x . const (x , pid , mountmckinley ) 0.14075616
mountmckinley
[ NP ] # x . const (x , rid , mountmckinley ) -0.055226557
massachusetts
[ NP ] # x . const (x , rid , massachusetts ) -0.023190754
massachusetts
[ NP ] # x . const (x , sid , massachusetts ) 0.0765336
massachusetts
[ NP ] # x . const (x , pid , massachusetts ) -0.023190754
long [ NP ] # x3 .# x1 . len (B , x1 ) ^ x3 @ B
0.010111484
How
[ S / S ] # x . answer ( X ) ^ x @ X
0.009999999
high [ NP ] # x3 .# x1 . elevation (B , x1 ) ^ x3 @ B
0.010112147
big
[ NP ] # x3 .# x1 . size (B , x1 ) ^ x3 @ B
0.010111821
tall [ NP ] # x . const (x , rid , tall )
-0.014923776
tall [ NP ] # x . const (x , pid , tall )
-0.014923776
tall [ NP ] # x3 .# x1 . elevation (B , x1 ) ^ x3 @ B
0.084677815
tall [ NP ] # x . const (x , sid , tall )
-0.014923776
large [ NP ] # x3 .# x1 . size (B , x1 ) ^ x3 @ B
0.010112498

3.11

NL2KR-T: the translation sub-part of NL2KR

Similar to NL2KR-L, in the RunConfiguration file of NL2KR-T, we need to set
the lexicon file path, the override file for syntax categories(optional), and the
testing data file as given below:
Tdata =./ examples / sample / test . txt
Tdictionary =./ output / d ic tio na ry_t rai n . out
Tsyntax =./ examples / sample / syntax . txt

For example, the preceding snippet specifies that: the testing data is in ./examples/sample/test.txt, the lexicon is in ./output/dictionary train.out,
and the override syntax categories are in ./examples/sample/syntax.txt.
The lexicon should be the lexicon learned by NL2KR-L.
The content of ./examples/sample/test.txt is
Mia sleeps sleep ( mia )
John catches a bus

EX . ( bus ( X ) ^ catches ( john , X ) )

The NL2KR system

11

Running the NL2KR-T script with the inputs specified above, we have the
following output where John catches a bus is translated to EX. (bus(X) ∧
catches(john, X)) as expected.

5

****** Parsing Sentences ...
...
Parsing test sentence : John catches a bus
Expected Representat ion : EX . ( bus ( X ) ^ catches ( john , X ) )
Generalizing bus = [ bus : [ N ] : # x . bus ( x ) , bus : [ N ] : bus ]
Generalizing catches = [ catches : [( S \ NP ) / NP ] : # w .# z . w @ # x . catches (z , x ) ]
Predicted Result : EX . bus ( X ) ^ catches ( john , X )
Correct Prediction
...

Note that the expected translation in “test.txt” is optional. Without it, the
evaluation is not correct but NL2KR-T still gives its results.

4

Conclusion and Future Work

In this work, we presented the NL2KR system, which is used for translating
natural language to a formal representation. The input of the NL2KR system
are training sentences and their formal representation; and an initial lexicon of
some known meanings of words. NL2KR system will try to learn the meaning of
others words from the training data. We presented six scripts to execute several
modules of NL2KR and show how to use them through examples.
In the future, we plan to make NL2KR more scalable and add more features
to the NL2KR system such as (1) automatically constructing the initial lexicon
and (2) using more knowledge such as word sense to select the correct meaning
of words.

References
1. Montague, R.: English as a Formal Language. In Thomason, R.H., ed.: Formal
Philosophy: Selected Papers of Richard Montague. Yale University Press, New
Haven, London (1974) 188–222
2. Baral, C., Dzifcak, J., Gonzalez, M.A., Zhou, J.: Using Inverse lambda and Generalization to Translate English to Formal Languages. CoRR abs/1108.3843 (2011)
3. Lierler, Y., Schüller, P.: Parsing Combinatory Categorial Grammar via Planning
in Answer Set Programming. In Erdem, E., Lee, J., Lierler, Y., Pearce, D., eds.:
Correct Reasoning. Volume 7265 of Lecture Notes in Computer Science., Springer
(2012) 436–453
4. Toutanova, K., Klein, D., Manning, C.D., Singer, Y.: Feature-rich part-of-speech
tagging with a cyclic dependency network. In: NAACL ’03: Proceedings of the 2003
Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology, Morristown, NJ, USA, Association for
Computational Linguistics (2003) 173–180
5. Clark, S., Curran, J.R.: Parsing the WSJ using CCG and log-linear models. In:
ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational
Linguistics, Morristown, NJ, USA, Association for Computational Linguistics (2004)
103

A Default Inference Rule Operating Internally to
the Grammar Devices
Christophe Onambélé Manga
UMR SFL, CNRS/Université Paris 8, France
onambelemanga@yahoo.fr

Abstract. Minimalist Grammars (MG) are viewed as a resource consuming system where syntactic operations are triggered when a positive
form of a feature matches with its negative form. But a problem arises
when a feature lacks a positive/negative value. For the latter case, we
introduce a default inference rule in order to account for the underspecification of the feature in a lexical entry.
Keywords: minimalist grammars, feature underspecification, default
logic.

1

Introduction

In Bantu syntax, the computational system only needs a noun class formal feature to proceed analysis. Noun classes are sets of words that trigger the same
agreement schema. Ewondo (Bantu, A72a)1 has 14 noun classes2 . The choice
Table 1. Agreement Marker & Gender in Ewondo
Gender
Gender
Gender
Gender
Gender
Gender
Gender
Gender
Gender
Gender
Gender

1

2

Classes Class morpheme Agr marker on verb Agr marker on adj
I
II
III
IV
V
VI
VII
VIII
IX
X

1,2
3,4
5,6
7,8
9,10
9,2
9,6
11,5
11,6
19

ǹ- ∼ø- / b@ǹ- / mìà- / m`@è- / bìn- / nn- / b@n- / m`@ò- / àò- / m`@vì-

á-(À) / b´@ó- / míá- *ĺ´@- *d´@- / m´@é- / bíé-(À) / éé-(À) / b´@é-(À) / m´@ó- / á- *ĺ´@- *d´@ó- / m´@ó-

á-(À) / b´@ó- / míá- *ĺ´@- *d´@- / m´@é- / bíé-(À) / éé-(À) / b´@é-(À) / m´@ó- / á- *ĺ´@- *d´@ó- / m´@ó-

[Gu1] alphanumeric coding (of bantu languages) is mainly geographic. Nevertheless,
the distribution (of languages) is done in zones A, B, ... whether the language has
kept a tone model closed to Proto-bantu.
More information on classes pairing can be found in [On1]. Two locatives noun classes
are to be added to Table 1 , namely cl16 (v-, à-) and cl17 (ò-).

of a noun class prefix indicates whether the noun is viewed as a unit or a set
of units. Except for locatives (cl16, cl17), even-numbered noun classes indicate
augmented (AUG) and odd-numbered are for minimal (MIN)3 . As it can be seen
in Table 1, each class has a different class morpheme that triggers a different
agreement morpheme feature; except for nouns of classes 9, 10 that share the
same class feature.
(1) 1. mÓngÓ
áku
ám̀boo
m-ÓngÓ
á-a-ku
á-m̀boo
1min-child agr1-past1-fall down agr1-lay flat
‘the child falled down and laid flat’
2. bÓngÓ
b´@ku
b´@m̀boo
b-ÓngÓ
b´@-a-ku
b´@-m̀boo
2aug-child agr2-past1-fall down agr2-lay flat
‘the children falled down and laid flat’
(2) 1. ñag
yàdì
bílÒg
ñag
y`@-à-dì
bí-lÒg
9min.cow agr9-Pres-eat 8aug-grass
‘The cow grazes’
bílÒg
2. ñag
yâdì
bí-lÒg
y´@-à-dì
ñag
10aug.cow agr10-Pres-eat 8aug-grass
‘The cows graze’
In (1), the agreement class feature of the head noun (mÓngÓ, bÓngÓ) spreads on
the verbs. In (2) we have the same form of the noun for both the minimal and the
augmented. In fact, when standing alone, one can’t tell whether ñag is minimal
(i.e class 9) or augmented (i.e class 10). It’s rather the agreement it
triggers that helps to distinguish one form to another. As already mentionned,
Bantu agreement phenomenon is characterized by the spreading of class feature
of the head noun all over its dependents including the verb. Structure building
rules (merge, move) in MG are defined in a directional process with a feature
checking system that is a mechanism of resource consumption i.e each selector
feature must match a selectee and each licensor match a licensee. [On1]
proposed to formalize bantu multiple agreement in MG by Head Movement with
Copying, the idea being that a selector is not end-consumed as the items that
select it still exist in the derivations. The aim of this paper is to see how to
deal with the balancing of ambiguity versus underspecification in the feature
(2) in a resource consumption system like MG. Underspecification has being
addressed in type-based grammars [Cr1,Dn1], in Type-Logical Grammars [He1],
but never in MG. Here, we propose to associate a defeasible inference rule
(σ) to lexical items with underspecified class feature. σ is based on Prototypical
3

Ewondo grammatical number has been redefined as Minimal (Min) and Augmented
(Aug), thus we have one single feature [±aug] [On1]

Reasoning [Re1,An1]. Section 2 proposes three ways that languages can have
(or not have) noun classes. Section 3 presents the indeterminacy of class feature
of nouns of class 9/10 in Bantu syntax. In Sect. 4 we show how the building of
syntactic operations works in MG, Sect. 5 provides a new solution that could
help to account for underspecification in MG after showing the limits of the first
proposal made in [On1]. The paper ends with a conclusion.

2

Inherent vs Flexible Gender Features

Given examples (3, 4, 5) that show agreement phenomenon encountered in
French, English and Ewondo. Imagine one removes maisons (houses) from
(3a), then if a French speaker is asked to give the masculine form of the adjective belles (beautifulfem,pl ), he would say beaux (beautifulmasc,pl ) because gender is inherent in adjective in French. On the other hand in English
(4), the adjective stays unchanged, gender (or number) feature is not inherent
in adjective.
(3) 1. toutes
ces
belles
allfem,pl thisfem,pl beautifulfem,pl
’All these beautiful houses’
2. tous
ces
trois
allmasc,pl thismasc,pl threemasc,pl
’All these three days’
(4) 1. all
allø
2. the
theø

maisons
housefem,pl
jours
daymasc,pl

the beautiful
houses
theø beautifulø housepl
desperate
housewives
desperateø housewifepl

For a Ewondo4 speaker, if he is asked to give the gender class of a determinative5 ,
he will be unable to give one. He needs to know the syntactic context in which
this determinative appears to tell what its class marker is.
(5) 1. m@-mǒs
m´@-t¯@
m´@-s@
m´@-lá
6aug-day agr6-this agr6-all agr6-three
‘All these three days’
4
5

Unless specified, all the examples that aren’t French or English are from Ewondo
language
The class marker allows to distinguish between substantives and determinatives.
Substantives are the set of nouns that [Gr1, p. 7] called inherent gender because
this category triggers agreement. The second one he called derived gender is made
of words that agree with the first one. In Ewondo (as in most Bantu languages), there
are two nominal categories that share the fact to have the same nominal prefix. We
term this second one as "determinative"

2. bi-soá
bi-t¯@
bi-s@
bi-lá
8aug-plate agr8-this agr8-all agr8-three
‘All these three plates’
The following observations6 can be made: (i) adjective in French is an unmarked
form that potentially agrees with the noun; (ii) in Ewondo, we can’t indicate the
class marker of a determinative except it appears in a construction, that means
we need the presence of a substantive that bears a specified noun class marker
to tell what are the class markers of the others items. Determinatives don’t have
pre-specified class marker, they inherit the class marker of the head noun; (iii)
adjective in English is invariable. French and Ewondo speakers differ in whether
they are able to produce a particular inflected form of an adjective in isolation.
This is an experimental finding, and can be explained in many ways. One possible
explanation is simply that speakers of any gendered language, when faced with
such a task, think of an appropriate context and report the form the adjective
takes in that context. The different behaviour of the French and Ewondo speakers
is a result of there being only two genders in French, and thus that it is much
easier to think of an appropriate context.

3

The Problem

3.1

Ambiguity in the Feature

In Ewondo, nouns of classes 9, 10 are problematic if one wants to determine their
respective noun class. In (6), the DPs subjects aren’t different as can be found
(chicken vs chickens) in English. It’s rather the agreement class marker the
@ and agr10 y´
@) that differentiates kúb in (6a, b) [Ow1,
noun triggers (agr9 y`
p. 65] is 9min and 10aug respectively.
yàkOn.
(6) 1. kúb
y`@-à-kOn
kúb
9min.chicken agr9-Pres-be sick
’The chicken is sick’
2. kúb
yâkOn.
kúb
y´@-à-kOn
10aug.chicken agr10-Pres-be sick
’The chickens are sick’
As in most Bantu languages, it’s assumed their nominal class morphemes are
originally homophones n- (see Table 1). It’s also difficult to say whether a given
noun has a root /NCVC(V)/ or /CVC(V)/ with a class morpheme n-. Linguists
usually argue by analogy to others noun classes: if most nouns of classes 9,
10 begin with a nasal7 , and if there are less nouns in others classes with that
6
7

My thanks to Greg Kobele for valuable comments after my aviva.
and there is a high percentage of initials [nD] and [nT] (where [D] is a voiced
occlusives and [T] is a non voiced occlusives).

structure, then people assume that roots can’t generally begin with NC; therefore
nouns in classes 9, 10 that always have a NC initial must actually have a prefix
/n-/. An answer to this argument is the possibility to mark a contrast between
roots with NC and C initials in noun classes 9, 10. In Ewondo, we have nouns with
[nD] but also words with [T] and [z]; such thematic roots are ambiguous. Two
explanations are possible : (i) there is a phonological deletion of /n/ in front of
voiceless consonants and fricatives, and (ii) there is a real contrast between NC
and C initials in theses noun classes. In Ewondo, the prefix n- is deleted when
it’s followed by another nasal (7), an unvoiced consonant (8) or by a voiced
consonant /z/ (9):
(7) n+Nàk → Nag: cow

(8) n+tsit → tsíd:
animal

(9) n+z@k → z@g:
pineapple

In short, nouns of classes 9, 10 are morphologically invariable et neutral for
class distinction. One may think there is no difference between minimal and
augmented number.
3.2

Distinction between Class 9, 10 Nouns

As noted, for those nouns that don’t change in minimal/augmented form, the
distinction is made by the agreement they trigger (10, 11).
@
(10) 1. kúb
é-n`
o-nǑn
9min.chicken agr9-pres.be 11min-bird
’The chicken is a bird’
2. kúb
é-nˆ@
a-nǑn
10aug.chicken agr10-pres.be 5aug-bird
’Chickens are birds’
3. *kúb
é-nˇ@
a-nǑn
10aug.chicken agr10-pres.be 5aug-bird
’Chicken are birds’
@
(11) 1. z@g
é-b@d`
á t@b@l@
9min.pineapple agr9-pres.put down on 1min.table
’The pineapple is on the table’
2. z@g
é-b@dˆ@
á t@b@l@
10aug.pineapple agr10-pres.put down on 1min.table
’Pineapples are on the table’
In (10, 11), tone doesn’t help to distinguish the two noun classes. Originally,
the augmented form is obtained by adjoining a suprasegmental High tone |´ |
to the noun of class 9. This floating High tone8 attaches either to the noun or
8

Regarding the architecture of tonal representations, floating tones (not associated)
are usually represented in a circle : spreading high tone , spreading low tone .

to the verb. Nevertheless, it seems that the verb, each time it’s present, bears
the floating High tone. The association of the High tone is done from left to
right. Nouns and verbs that bear a Low tone on the last syllable (10a) yield,
when a High tone is added to them, a High-Low tone on the verb (10b). If the
association is made from the right to the left, then we get a Low-High tone on
the verb, thus the ungrammatical (10c). Let’s take the subject and the verb in
(10b), the High tone of kúb (chickens) spreads on the right :
(12)

(13)

h
u
b
k

h
u
b
k

h
n
l@e

h
n
l@e

In (12), as there is already a High tone
on the verbal prefix (é), there is no difference.
And nothing stops this High tone to
spread on its right up to the verb root
yielding a High-Low tone (13). That’s
the way we get sentence (10b).

With nouns originally with a Low tone, the difference is made at phonological
level with a raising pitch on the first syllable of the verb. This syllable should
bear the Low or High tone to indicate whether the noun is minimal or augmented
and also specify the class agreement feature. But, as the verb already has a High
tone on its first syllable, the original tone of nouns of class 9, 10 spreads to the
last vowel of the verb (11). Let’s take an example with a noun bearing a High
tone9
(14) kúb
é-n`@
9min.chicken agr9-pres.be
‘The chicken is’
We have a High tone on kúb (chicken), a Low tone of class 9 on the verbal
prefix è and a Low tone on the verbal root n`
@ that are shown below (15):
(15)

(16)

9

h
u
b
k

h
u
b
k

n
l@e

n
l@e

The floating High tone of kúb
(chicken) attaches on the right yielding (16):
This floating High tone of kúb
(chicken) pushes the Low tone of the
verbal prefix to the right, and we get
(17):

It’s important to note that tone isn’t a distinctive feature as the word already has
a high tone. The main point to look at is the (minimal) agreement on the verb
comparing to (10b) that is an augmented form. That means the proposed analysis
is the same for N with Low tone.

(17)

h
u
b
k

h
n
l@e

The floating Low tone blocks the High
tone of kúb (chicken) so that it can’t
spread up to the verb root.

And the Low tone goes on this verb root, as the latter already bears a Low
tone, nothing changes. We can conclude that tonal distinction on the agreement
feature can be useful to distinguish the covert class feature of nouns of class
9/10.

4

Minimalist Grammars

MG [St1] attempt to implement the so-called minimalist principles introduced
by [Ch1]. A MG is a quadruplet (V,Cat,Lex,F): V = {P ∪ I}, set of non syntactic features (vocabulary) where P represents the phonetic features and I
the semantics features; Cat = {base ∪ selector ∪ licensor ∪ licensee},
finite set of non syntactic features (categories) which are partitioned into four
kinds (x : base (c, t, v, d, n, ...), =x : selector/probe, -x : licensee,
+x : licensor (feature that trigger move)); Lex = finite set of expressions built
from V and Cat (lexicon); F = {merge ∪ move} : set of generating functions.
Merge and Move are built with trees where : (i) internal nodes are labelled with
direction arrows (< or >) indicating where the head of the structure is, (ii) leaves
are pairs hα, βi with α = vocabulary item and β = set of features. Merge (or external merge) is a binary operation that takes two trees and puts them together.
The tree whose first feature is =x merges with a tree whose category feature is
x to built a new tree. Features =x and x are deleted after merging.
x
(18) merge (t=x
1 , t2 ) =

x
merge (t=x
1 , t2 ) =

<

if t1 is a lexical item

t1 t2
>
if t1 is not a lexical item
t1 t2

Move (or internal merge) is a unary operation that targets (some part of) an
expression to remerge it higher in the structure. Move is applied to a subtree with
+X
a feature -x. Given a subtree with -x written t−x
2 that appears in a tree t1 , we
−x +X
+X
−x +X
write t1 [t2 ] . t1 is the maximal projection of t1 [t2 ]
i.e the largest
subtree with -x as its head. After extraction, the subtree t−x
2 merges as specifier
of the head of the tree, features served for Move operation are removed from the
tree. The shortest move contraint (SMC) that applies to Move requires there
+X
should be exactly one maximal projection t1 [t−x
displaying a subtree t−x
2 ]
2 .
The original place of t−x
is
then
filled
by
an
empty
tree
 i.e a single featureless
2
node.
+X
(19) move (t1 [t−x
)=
2 ]

>
t2 t1 []

There are few syntactic operations implemented in MG (Scrambling & Adjunction
[FG1], Head Movement [St2], Copying [Ko1], Head Movement with Copying
[On1]). In MG, Merge and Move, need a selecting feature matching a selected
feature (both being of the same category) to drive derivations. Now, what’s
happened if the selecting feature is un(der)specified?

5

On Underspecification in Minimalist Grammars

5.1

Unspecified Class Feature

Given the examples below where nouns of classes 9, 10 are in subject position
(20) and in object position (21), the analysis developed in [On1] for (20) is based
on the theoretical claim that nouns of classes 9, 10 are not lexically specified for
their class.
bílÒg
yàdì
(20) ñag
y`@-à-dì
bí-lÒg
ñag
9min.cow agr9-Pres-eat 8aug-grass
‘The cow grazes’
ñag
áwé
(21) ńsOmO
ñag
á-a-wé
ń-sOmO
1min-huntsman agr1-past1-kill 9min/10aug.cow
’The huntsman has killed the cow/cows.’
These nouns enter the derivation with an uninstantiated variable x that will be
valued through postsyntactic insertion of the class morpheme of the agreement
→ on TP. Variable x instantiation means to copy on the subject DP
feature +−
agr
the value of the agreement feature on T head.
(22)

>

<

<

−
→ /ñag(x) / d -q (n(x) ) λ
// -−
agr

(22) is built in 3 steps:
(a)
merge(n(x) <= !cl -k //, n(x)
−
→
-agr /ñag/),
(b) merge(=>+ cl +k d -q //,a),
(c) move(b).

Postsyntactic insertion means the agreement class feature on the verb is
substituting for the variable x yielding the corresponding noun class feature
on the noun. As we said it’s the agreement feature on the verb that give the
information about the nominal class morpheme of the DP. The substitution
process is made in two steps : (i) covert movement then (ii) agree (for detailed
step-by-step justification see [On1]). If (20) is appropriately treated in [On1],
the solution provided is still problematic for (21) where noun of classes 9, 10 are
in object position. To solve this problem, let’s try another approach. Following
[Ro1], we distinguish three features: φ-features are specified class feature for
inherent noun classes; θ-features are underspecified class feature for noun of

Table 2. Syntactic Class Features
Inherent noun classes Neutral noun classes Derived noun
cl1
cl3
cl5
cl7
cl11
cl16
cl17
cl19

cl2
cl4
cl6
cl8

cl9

cl10

flexible

class 9/10; α-features are flexible and inherited class agreement feature found on
derived nouns (i.e determinatives) and verbs. If we think of noun class feature
as Attribute-Value feature system, we could say, noun of class 9, 10 has an
Attribute specification "n" without a Value (i.e without a class number). That
means, a word like ñag(cow) is represented with the feature nθ. The difference
being that a noun with a specified noun class (say m-ÓngÓ: 1min-child) will
be represented with a specified class feature n1. α-features’ transmission is done
through HMC. The question now is how to formalize θ-feature in MG?
5.2

Default Inference Rule

A default rule will be used to model feature underspecification through prototypical reasoning, the latter is used when most instances of a concept have some
property10 . Default Logic [Re1,An1] is a nonmonotonic reasoning approach allowing to rely on incomplete information about problem. A default theory T is a
pair (z, Γ ) where z is a set of FOL sentences representing the background information, Γ represents the defeasible information (i.e a countable set of defaults
rule).
Definition 1. A default rule (say σ) is an inference rule of the form:

δ
= prerequisite, pre(σ)
δ : ρ1 , . . . , ρ n 
ρ1 , . . . , ρn = justifications, just(σ)or simply(σ)

ξ

ξ
= consequent ofσ, cons(σ).

(23)

interpreted as: given δ and as there is no information that ¬ρi , conclude ξ by
default. A default rule is called normal if and only if it has the form:
δ:ξ
(24)
ξ
10

That means for us the case when most instances of noun class feature have AttributeValue property.

A semantics for Default Logic is provided through the notion of extension
[Re1,AS1,An1]. An extension for a default theory T (T = (z, Γ )) is a set of FOL
sentences E where: (a) z ⊆ E; (b) E = ∆(E) where ∆ denotes the deductive
closure; (c) E should be closed under the application of defaults from Γ i.e if
δ:ρ1 ,...,ρn
, δ ∈ E and ¬ρ1 ∈
/ E, . . . , ¬ρn ∈
/ E then ξ ∈ E.
ξ
Definition 2. For T = (z, Γ ), let Π = (σ0 , σ1 , . . .) be a finite or infinite sequence of default rules from Γ without multiple occurrences. Π is viewed as possible order in which default rules from Γ are applied, so a default rule doesn’t need
to be applied more than once in such a reasoning. The initial segment of Π with
length k is denoted Π[k]. Sets of first-order formulae, In(Π) and Out(Π) are
associated to such sequence as Π: (a) In(Π) = ∆(z∪{cons(σ)|σ occurs in Π}),
In(Π) collects the information gained by the application of the default in Π and
represents the current knowledge base after the default in Π have been applied; (b) Out(Π) = {¬ρ|ρ ∈ just(σ) for some σ occuring in Π}, Out(Π)
collects formulae that should not turn out to be true i.e that should not become
part of the current knowledge base even after subsequent application of the other
default rules.
Definition 3. Π is called a process of T iff σk is applicable to In(Π[k]), for
every k such that σk occurs in Π.
Definition 4. For a given process Π of T: (a) Π is a successful process
iff In(Π) ∩ Out(Π) = ∅, otherwise it is a failed process; (b) Π is a closed
process iff every σ ∈ Γ that is applicable to In(Π) already occurs in Π. Closed
processes correspond to the desired property of an extension E being closed under
application of default rules from Γ .
Definition 5. For the application of a default rule, a consistency condition
should be satisfied.
δ : ρ1 , . . . , ρ n
(25)
σ=
ξ
is applicable to a deductively closed set of formulae E iff σ ∈ E and ¬ρ1 ∈
/
E, . . . , ¬ρn ∈
/ E.
Proposition 1. The rule of thumb when treating nouns of class 9, 10 is to say
these nouns are of class 10 unless stated by the grammarian they are of class 911 .
If the latter is done, the default rule (i.e the rule of thumb) already mentionned
isn’t rejected, it’s simply no more applicable as the missing information is now
known.In a classical logic setting12 , we need to say what is the Value of Attribute
n for classes 9, 10 nouns. As we don’t know, no decision could be taken in such
a system. But in default reasoning, the previous rule of thumb can be applied.
11

12

That means, only the augmented is grammatically marked, minimal will have a zero
grammatical marker that is realised as a low tone by default. In fact, that’s the
strategy generally used in natural language that the minimal has a zero morpheme.
As well as in Stablerian MG

Proof. We write Attribute with indices to differentiate between n1 (that stands
for class 10 nouns) and n0 (that
for 	class 9 nouns). So, for T = (z,
 stands
:¬9 n0 :¬10
Γ ), let z = {n1 , n0 } and Γ = n110
, 9
. The default theory T is repre :¬9 n0 :¬10 	
:¬9
, 9
. Let also assume that σ1 = n110
,
sented as : T = {n1 , n0 } , n110
n0 :¬10
σ0 =
and
Π
=
(σ
,
σ
).
As
Γ
contains
only
(a
finite
number
of)
two
de1
0
9
fault rules, closedness doesn’t matter. We apply default rules as long as they
are applicable, and then we get a closed process. So, we apply the first default
σ1 and check default σ0 with respect to the knowledge collected after the application of σ1 . For Π[σ1 ] we have: In(Π[σ1 ]) = ∆ ({n1 , n0 , 10}), Out(Π[σ1 ]) = {9},
In(Π[σ1 ]) ∩ Out(Π[σ1 ]) = ∅, so, we say Π[σ1 ] is closed and successful process.For
Π[σ0 ] we have: In(Π[σ0 ]) = ∆ ({n1 , n0 , 9}), Out(Π[σ0 ]) = {10}. In fact σ0
can’t be applied as 10 ∈ ∆ ({n1 , n0 , 10}) which is our current knowledge base
before we apply σ0 . We know In(Π[k + 1]) = ∆ (Π[k]) ∪ ∆ (Π[k + 1]) and
Out(Π[k + 1]) = Out(Π[k]) ∪ Out (Π[k + 1]) so In(Π[σ0 ]) = ∆ ({n1 , n0 , 10, 9}),
Out(Π[σ0 ]) = {10, 9}, In(Π[σ0 ]) ∩ Out(Π[σ0 ]) = {10, 9}, thus, we say Π[σ0 ]
is failed process.We could have stopped the proof earlier as application of σ1
blocks application of σ0 and vice versa, so there are no more extension of T.
From the application of the first default rule σ1 , we know Attribute n has Value
10, so it is not consistent to assume 9. Thus ∆ ({n1 , n0 , 10}) is the only extension
of T.
t
u
We associate a defeasible inference rule σ to lexical items with feature ambiguity.
A default rule on an underspecified Attribute n is marked using an arrow ↑σ
indicating to map Attribute n to the result of σ. Once the Value of n is calculated,
then the MG derivation can proceed (and not the inverse). The idea being that
the default rule blocks the derivation. So the derivation tree for a word like ñag
(cow) is:
(26)

<
[n ↑σ ] ⇐!cl -k // nx / ñag/

where x is an anonymous variable that
match with any value collected after
the application of [n ↑σ ].

MG are by definition encapsulated, which means that they make reference only
to their own internalised system, and not to any external formal system, such as
a logic for general reasoning. This is intrinsic to the claim of the language faculty
being prior, feeding into more general reasoning devices but separate from them.
If the current proposal is in line with Stabler’s formalization, then we think MG
clearly differentiate a Stabler form of minimalism with others. That might mean
that the notion of encapsulation may be rather different for a Stabler form of
grammar than others.

6

Conclusion

In this paper we attempt to introduce a rule in a Stablerian MG that could
help to account for feature underspecification in a resource consuming system.

The proposal rests on default reasoning that allows to deal with incomplete
information about a problem.
Acknowledgments
We thank Stan Dubinsky, Ruth Kempson and two anonymous reviewers for their
constructive comments, which helped us to improve the manuscript.

References
[AS1]

Antoniou, G., Sperschneider, V.: Operational concepts of nonmonotonic logics.
Part 1. Default logic. Artificial Intelligence Review 8 (1994) 3–16
[An1] Antoniou, G.: A tutorial on default logics. ACM Computing Surveys. Vol.31.
No 3 (1999) 337–359
[Ch1] Chomsky, N.: The Minimalist Program. Cambridge The MIT Press (1995)
[Cr1] Crysmann, B.: Underspecification and neutrality: a unified approach to syncretism. In Proceedings of the Joint Conference on Formal Grammar and Mathematics of Language. CSLI publications (2009) 1–12
[Dn1] Daniels, M.: On a type-based analysis of feature neutrality and the coordination
of unlikes. In Proceedings of the 8th International Conference on Head-Driven
Phrase Structure Grammar. CSLI Publications (2001) 137–147
[FG1] Frey, W., Gärtner, H.-M.: On the treatment of scrambling and adjunction in
minimalist grammars. In Proceedings of the Conference on Formal Grammar
(2002) 41–52
[Gr1] Gregersen, E., A.: Prefix and pronoun in Bantu. International journal of American linguistics. Vol. 33. No 3. Part II. Indiana University Press (1967)
[Gu1] Guthrie, M.: The classification of the Bantu languages. Oxford University Press
for the International African Institute (1948)
[He1] Heylen, D.: Underspecification in Type-Logical Grammars. In Logical Aspects
of Computational Linguistics. LNCS 1582 (1999) 180–199
[Ko1] Kobele, G.: Generating Copies: An Investigation into Structural Identity in
Language and Grammar. Ph.D. thesis. UCLA (2006)
[On1] Onambélé, C.: Vers une grammaire minimaliste de certains aspects syntaxiques
de la langue Ewondo. Ph.D. thesis. Université Paris 8 (2012)
[Ow1] Owona, A.: L’orthographe harmonisée de l’ewondo. Magister Thesis. Université
de Yaoundé 1 (2004)
[Re1] Reiter, R.: A logic for default reasoning. Artificial Intelligence 13. (1980) 81–
132
[Ro1] Rooryck, J.: On two types of underspecification: Towards a feature theory
shared by syntax and phonology. Probus 6 (1994) 207–233
[St1]
Stabler, E.: Derivational minimalism. In Logical Aspects of Computational
Linguistics. LNCS 1328 (1997) 68–95
[St2]
Stabler, E.: Recognizing Head Movement. In Logical Aspects of Computational
Linguistics. LNAI-LNCS 2099 (2001) 245–260

BOEMIE: Reasoning-based Information
Extraction
Georgios Petasis1 , Ralf Möller2 , and Vangelis Karkaletsis1
1

Software and Knowledge Engineering Laboratory
Institute of Informatics and Telecommunications
National Centre for Scientific Research (N.C.S.R.) “Demokritos”
GR-153 10, P.O. BOX 60228, Aghia Paraskevi, Athens, Greece
{petasis,vangelis}@iit.demokritos.gr
2
Institute for Software Systems (STS)
Hamburg University of Technology
Schwarzenbergstr. 95, 21073, Hamburg, Germany
moeller@tu-harburg.de

Abstract. This paper presents a novel approach for exploiting an ontology in an ontology-based information extraction system, which substitutes part of the extraction process with reasoning, guided by a set of
automatically acquired rules.

1

Introduction

Information extraction (IE) is the task of automatically extracting structured
information from unstructured documents, mainly natural language texts. Due
to the ambiguity of the term “structured information”, information extraction
covers a broad range of research, from simple data extraction from Web pages
using patterns and regular grammars, to the semantic analysis of language for
extracting meaning, such as the research areas of word sense disambiguation
or sentiment analysis. The basic idea behind information extraction (the concentration of important information from a document into a structured format,
mainly in the form of a table) is fairly old, with early approaches appearing in
the 1950s, where the applicability of information extraction was proposed by
the Zellig Harris for sub-languages, with the first practical systems appearing at
the end of the 1970s, such as Roger Schank’s systems [27, 28], which exported
“scripts” from newspaper articles. The ease of evaluation of information extraction systems in comparison to other natural language processing technologies
such as machine translation or summarisation, where evaluation is still an open
research issue, made IE systems quite popular and led to the Message Understanding Conferences (MUC) [21] that redefined this research field.
Ontology-Based Information Extraction (OBIE) has recently emerged as a
subfield of information extraction. This synergy between IE and ontologies aims
at alleviating some of the shortcomings of traditional IE systems, such as efficient
representation of domain knowledge, portability into new thematic domains, and

2

Georgios Petasis, Ralf Möller, Vangelis Karkaletsis

interoperability in the era of Semantic Web [14]. Ontologies are a means for sharing and re-using knowledge, a container for capturing semantic information of a
particular domain. A widely accepted definition of ontology in information technology and AI community is that of “a formal, explicit specification of a shared
conceptualization” [29, 10], where “formal implies that the ontology should be
machine-readable and shared that it is accepted by a group or community” [4].
According to [31], an ontology-based information extraction system is a system
that “processes unstructured or semi-structured natural language text through
a mechanism guided by ontologies to extract certain types of information and
presents the output using ontologies”. This definition suggests that the main differences between traditional IE systems and OBIEs are: a) OBIEs present their
output using ontologies, and b) OBIEs use an information extraction process
that is “guided” by an ontology. In all OBIE systems the extraction process is
guided or driven by the ontology to extract things such as classes, properties
and instances [31], in a process known as ontology population [23].
However, the way the extraction process is guided by an ontology in all OBIEs
has not changed much with respect to traditional information extraction systems.
According to a fairly recent survey [31], OBIEs do not employ new extraction
methods, but they rather employ existing methods to identify the components
of an ontology. Current research on the field investigates the development of
“reusable extraction components” that are tied to ontology portions that are
able to identify and populate [30, 11]. In this paper we propose an alternative
approach that tries to minimise the use of traditional information extraction
components, and substitute their effect with reasoning. The motivation behind
the work presented in this paper is to propose a new “kind” of ontology-based information extraction system, which integrates further ontologies and traditional
information extraction approaches, through the use of reasoning for “guiding”
the extraction process, instead of heuristics, rules, or machine learning. The proposed approach splits a traditional OBIE in two parts, the first part of which
deals with the gathering of evidence from documents (in the form of ontology
property instances and relation instances among them), while the second part
employs reasoning to interpret the extracted evidence, driven by plausible explanations for the observed relations. Thus, the innovative aspects of the presented
approach include a) the use of an ontology through reasoning as a substitute for
the embedded knowledge usually found in the extraction components of OBIEs,
b) a proposal of how reasoning can be applied for extracting information from
documents, and c) an approach for inferring the required interpretation rules
even when the ontology evolves with the addition of new concepts and relations.
The rest of this paper is organised as follows: In section 2 related work is
presented in order to place our approach within the current state-of-the-art. In
section 3 the proposed approach is presented, detailing both the interpretation
process and the automatic reasoning rule acquisition. Finally, section 4 concludes
this paper and outlines interesting directions for further research.

BOEMIE: Reasoning-based Information Extraction

2

3

Related Work

Ontology-based information extraction has recently emerged as a subfield of
information extraction that tries to bring together traditional information extraction and ontologies, which provide formal and explicit specifications of conceptualizations, and acquire a crucial role in the information extraction process.
A set of recent surveys have been presented that analyse the state-of-art in the
research fields of OBIEs [13, 14, 31] and ontology learning/evolution [24, 23], a
relevant research field since many OBIE systems also perform ontology evolution/learning. OBIE systems can be classified according to the way they acquire
the ontology to be used for information extraction. One approach is to consider
the ontology as an input to the system: The OBIE is guided by a manually
constructed ontology or from an “off-the-shelf” ontology. Most OBIE systems
appear to adopt this approach [31]. Such systems include SOBA [5, 3], KIM [25,
26] the implementation by Li and Bontcheva [18] and PANKOW [7], Artequact
[15, 2, 1]. The other approach is to construct an ontology as a part of the information extraction process, either starting from scratch or by evolving an initial,
seed ontology. Such systems include Text-To-Onto [19], the implementation by
Hwang [12], Kylin [32], the work by Maedche et al. [20], the work of Dung and
Kameyama [8]. However, all the aforementioned systems employ traditional information extraction methods to identify elements of the ontology, and none
attempts to employ reasoning, as the work presented in this paper suggests.

3

The BOEMIE approach

The work presented in this paper has been developed in the context of the
BOEMIE project. It advocates an ontology-driven multimedia content analysis,
i.e. semantics extraction from images, video, text, audio/speech, through a novel
synergistic method that combines multimedia extraction and ontology evolution
in a bootstrapping fashion. This method involves, on one hand, the continuous
extraction of knowledge from multimedia content sources in order to populate
and enrich the ontologies and, on the other hand, the deployment of these ontologies to enhance the robustness of the multimedia information extraction system.
More details about BOEMIE can be found in [6, 23].
As already mentioned, the proposed approach splits a traditional OBIE in
two parts, the first part of which deals with the gathering of evidence from
documents (in the form of ontology property instances and relation instances
among them), while the second part employs reasoning to interpret the extracted
evidence, driven by plausible explanations for the observed relations. As a result,
the typical extraction process in also split in two phases: “low-level analysis”
(where traditional extraction techniques such as machine learning are used) and
“semantic interpretation”, where analysis’ results are explained, according to the
ontology, through reasoning. Each of the two phases identifies different elements
of the ontology, whose elements are also split in two groups, the “mid-level
concepts” (MLCs - identified by low-level analysis), and the “high-level concepts”
(HLCs), which are identified through semantic interpretation.

4

Georgios Petasis, Ralf Möller, Vangelis Karkaletsis

The implications of this separation are significant: the low-level analysis cannot assume that a Person/Athlete/Journalist has been found in a multimedia
document, just because a name has been identified. Instead the low-level analysis
reports that a name, an age, a nationality, a performance, etc. has been found,
and reports how all these are related through binary relations, extracted from
modality-specific information (i.e. linguistic events for texts, spatial relations for
images/videos, etc.). The identification of Person/Athlete/Journalist instances
is done through reasoning, using the ontology and the reasoning (interpretation)
rules, as low-level analysis cannot know how the Person or Athlete concepts are
defined in the ontology (i.e. what their properties/axioms/restrictions are). In
essence, BOEMIE proposes a novel approach for constructing an OBIE, by keeping the named-entity extraction phase from traditional IE systems, modifying
relation extraction to reflect modality-specific relations at the ontological level,
and implementing the remaining phases of traditional IE systems through reasoning. For example, low-level analysis of an image is responsible for reporting
only that a few tenths of faces have been detected (i.e. the faces of athletes and
the audience – represented as MLC instances), along with a human body (i.e.
the body of an athlete – represented as an MLC instance), a pole, a mattress,
two vertical bars, a horizontal bar, etc. (all these are instances of MLCs). After
MLC instances have been identified, the low-level analysis is expected to identify relational information about these MLC instances. For example, the low-level
analysis is expected to identify that a specific face is adjacent to a human body
and both are adjacent to the pole and the horizontal bar. The low-level analysis
is expected to report the extracted relational information through suitable binary relations between each pair of related MLC instances. On the other hand,
the low-level is not expected to interpret its findings and hypothesise instances
of HLC instances, such as the existence of athletes and their number. It is up
to the second phase, the semantic interpretation, to identify how many athletes
are involved (each one represented as instance of the “Athlete” HLC), and to
interpret the scene shown in the image as an instance of the “Pole Vault” HLC
concept, effectively explaining the image.
3.1

Definitions

The approach presented in this paper organises the ontology into four main ontological modules, the “low-level features”, the “mid-level concepts”, the “highlevel concepts”, and the “interpretation rules”, which are employed through
reasoning in order to provide one or more “interpretations” of a multimedia
document.
Definition 1 (low-level features). Low-level features are concepts related to
the decomposition of a multimedia document (i.e. the description of an HTML
page into text, images or other objects), and concepts that describe surface forms
on single modality documents, such as segments in text and audio documents,
polygons in image and video frames, etc.

BOEMIE: Reasoning-based Information Extraction

5

Definition 2 (Mid-Level Concept (MLC)). Mid-level concepts are concepts
that can be materialised (i.e. have surface forms) on documents of a single modality. Anything that can be extracted by an OBIE that has a surface form on a
document, is an MLC.
For example, the names of persons, locations, etc. in texts, the faces, bodies of
persons in images and the sound events (i.e. applauses) in audio tracks are all
MLC concepts. The BOEMIE OBIE extracts only instances of MLCs (MLCIs)
and relations (i.e. spatial) among them.
Definition 3 (High-Level Concept (HLC)). High-level concepts are compound concepts formed from MLCs. HLCs cannot be directly identified in a multimedia document, as they cannot be associated with a single surface form (i.e.
segment).
For example, the concept “Person” is an HLC, that groups several MLCs (properties), such as “PersonName”, “Age”, “Nationality”, “PersonFace”, “PersonBody”,
etc. Instances of HLCs (HLCIs) in the BOEMIE OBIE are identified through
reasoning over MLC instances (MLCIs) in the ontology, guided by a set of rules,
in a process known as “interpretation”.
Definition 4 (interpretation). Interpretation is the identification of one or
more HLC instances (HLCIs) in a multimedia document.
An OBIE can have identified several MLC instances (MLCIs) and relations between them in a multimedia document. If these MLC instances satisfy the axioms
of the ontology and the interpretation rules are able to generate one or more HLC
instances (HLCIs), then this multimedia document is considered as interpeted
(or explained ) by the ontology, with the HLC instances (HLCIs) constituting the
interpretation of the document. If the same MLC instances (MLCIs) are involved
in more than one HLC instances (HLCIs) of the same HLC, then the document
is considered to have multiple interpretations, usually due to ambiguity.
3.2

Semantic Extraction

The extraction engine is responsible for extracting instances of concept descriptions that can be directly identified in corpora of different modalities. These
concept descriptions are mid-level concepts (MLCs). For example, in the textual
modality the name or the age of a person is an MLC, as instances of these concepts are associated directly with relevant text segments. On the other hand, the
concept person is not an MLC, as it is a “compound”, or “aggregate” concept in
such a way that instances of this concept are related to instances of name, age,
gender or maybe instances of other compound concepts. Compound concepts are
referred to as high-level concepts (HLCs), and instances of such concepts cannot be directly identified in a multimedia document, and thus associated with
a content segment. Thus, such instances and also relationships between these
instances have to be hypothesized. In particular, this engine implements a modular approach [13] that comprises the following three level of abstraction: 1. The

6

Georgios Petasis, Ralf Möller, Vangelis Karkaletsis

low-level analysis, which includes a set of modality-specific (image, text, video,
audio) content analysis tools. 2. A modality-specific semantic interpretation engine. 3. A fusion engine, which combines interpretations from each modality3 .
The first two levels implement ontology-driven, modality-specific information
extraction, while the last one fuses the information obtained from the previous
levels of analysis. The first level involves the identification of “primitive” concepts (MLCs), as well as instances of binary relations amongst them. The second
level involves the semantic interpretation engine, responsible for hypothesizing
instances of high-level concepts (HLCs) representing the interpretation of (parts
of) a document. Semantic interpretation operates on the instances of MLCs
and relations between them extracted by the information extraction engine. The
goal of semantic interpretation is to explain why certain instances of MLCs are
observed in certain relations according to the background knowledge (domain
ontology and a set of interpretation rules) [9], by creating instances of highlevel concepts and relating these instances. Semantic interpretation is performed
through calls to a non-standard reasoning service, known as explanation derivation via abduction. The semantic interpretation is performed on the extracted
information (MLC/relation instances) from a single modality in order to form
modality-specific HLC instances. The fact that content analysis is separated from
semantic interpretation, along with the fact that semantic interpretation is performed through reasoning using rules from the ontology, allows single-modality
extraction to be adaptable to changes in the ontology.
Once a multimedia document has been decomposed into single-modality elements and each element has been analysed and semantically interpreted separately, the various interpretations must be fused into one or more alternative
interpretations of the multimedia document as a whole. This process is performed at a third level, where the modality-specific HLC instances are fused
in order to produce HLC instances that are not modality-specific, and contain
information extracted from all involved modalities. Fusion is also formalized as
explanation generation via abductive reasoning.
Example: the OBIE for the text modality The low-level analysis system
implemented in the context of BOEMIE exploits the infrastructure offered by
the Ellogon4 platform [22], and the Conditional Random Fields [17] machine
learning algorithm, in order to build an adaptable named-entity recognition and
classification (NERC) system, able to identify MLC instances (MLCIs) and relations between MLCIs. Both NERC and relation extraction components operate
in a supervised manner, using MLC instances that populate the (seed or evolved)
ontology as training material (whose surface forms are available through their
low-level features). The fact that both components use the populated ontology
as training source, allows them to adapt to ontology changes, and improve their
extraction performance over time, as the ontology evolves. The performance of
3

4

The fusion engine will not be described in this paper, as it is similar to the semantic
interpretation engine. More information can be found at [6, 23].
http://www.ellogon.org

BOEMIE: Reasoning-based Information Extraction

7

the NERC and relation extraction components has been measured to about 85%
and 70% (F-measure), in the thematic domain of athletics, involving news items
and biographies from official sites like IAAF5 (International Association of Athletics Federations). More details about the low-level analysis system for the text
modality can be found in [13].
The modality specific interpretation engine (not only for text, but for all
modalities) is a process for generating instances of HLCs, by combining instances
of MLCs, through reasoning over instances. Abduction is used for this task, a
type of reasoning where the goal is to derive explanations (causes) for observations (effects). In the framework of this work we regard as explanations the
high-level semantics of a document, given the middle-level semantics, that is, we
use the extracted MLCIs in order to find HLCIs [9]. The reasoning process is
guided by a set of rules, which belong into two kinds, deductive and abductive.
Assuming a knowledge base, Σ = (T, A) (i.e. an ontology), and a set of assertions
Γ , (i.e. the assertions of the semantic interpretation of a document), abduction
tries to derive all sets of assertions (interpretations) ∆ such as Σ ∪ ∆ |= Γ ,
while the following conditions must be satisfied: (a) Σ ∪ ∆ is satisfiable, and
(b) ∆ is a minimal explanation for Γ , i.e. there exists no other explanation ∆0
(∆0 ⊆ ∆) that Σ ∪ ∆0 |= ∆ holds. For example, assuming the following ontology
Σ (containing both a “terminological component” – TBox, and a set of rules):
Jumper v Human

P ole v SportsEquipment
Bar v SportsEquipment

P ole u Bar v ⊥

P ole u Jumper v ⊥
Jumper u Bar v ⊥

JumpingEvent v ∃≤1 hasP articipant.Jumper

P oleV ault v JumpingEvent u ∃hasP art.P ole u ∃hasP art.Bar
HighJump v JumpingEvent u ∃hasP art.Bar
near (Y, Z) ← P oleV ault (X) , hasP art (X, Y ) , Bar (Y ) ,
hasP art (X, W ) , P ole (W ) ,

hasP articipant (X, Z) , Jumper (Z)
near (Y, Z) ← HighJump (X) , hasP art (X, Y ) , Bar (Y ) ,
hasP articipant (X, Z) , Jumper (Z)

And a document (i.e. an image) describing a pole vault event, whose analysis
results Γ contain instances of the MLCs “Pole”, “Human”, “Bar” and a relation
that the human is near the bar:
pole1 : P ole
human1 : Human
5

http://www.iaaf.org/

8

Georgios Petasis, Ralf Möller, Vangelis Karkaletsis

bar1 : Bar
(bar1 , human1 ) : near
The interpretation process splits the set of analysis assertions Γ into two subsets: (a) Γ1 (bona fide assertions): {pole1 : P ole, human1 : Human, bar1 : Bar},
which are assumed to be true by default, and (b) Γ2 (fiat assertions):
{(bar1 , human1 : near)}, containing the assertions aimed to be explained. Since
Γ1 is always true, Σ ∪∆ |= Γ can be expressed as Σ ∪Γ1 ∪∆ |= Γ2 . Then, a query
Q1 is formed from each fiat assertion (Γ2 ), such as Q1 := {()|near (bar1 , human1 )}.
Executing the query, a set of possible explanations (interpretations) is retrieved:
∆1 = {N ewInd1 : P oleV ault, (N ewInd1 , bar1 ) : hasP art,
(N ewInd1 , N ewInd2 ) : hasP art, N ewInd2 : P ole,

(N ewInd1 , human1 ) : hasP articipant, human1 : Jumper}
∆2 = {N ewInd1 : P oleV ault, (N ewInd1 , bar1 ) : hasP art,

(N ewInd1 , pole1 ) : hasP art, (N ewInd1 , human1 ) : hasP articipant,
human1 : Jumper}

∆3 = {N ewInd1 : HighJump, (N ewInd1 , bar1 ) : hasP art,

(N ewInd1 , human1 ) : hasP articipant, human1 : Jumper}

Each interpretation is scored, according to a heuristic based on the number of
hypothesized entities and the number of involved Γ1 assertions used, and the best
scoring interpretations are kept. For the example interpretation shown above,
∆2 is the best scoring explanation, as ∆1 has an excessive hypothesized entity
(N ewInd2 ), and ∆3 does not use the “Pole” instance from Γ1 . More details about
interpretation through abduction can be found in [6] and [9]. The language of
the rules is SROIQV , and they can be written in OWL using nominal schemas
[16]. For instance the first rule of the TBox shown in the previous section can
be written as follows:
Bar u {Y } u ∃hasP art− .(P oleV ault u {X} u (∃hasP art.(P ole u {W })) u

(∃hasP articipant.(Jumper u {Z}))) v {Y } u ∃near.{Z}

where {Z} is a nominal variable [16]. However, we are going to use a more
“comfortable” notation for rules through out this paper.
3.3

The Role of Interpretation Rules

Rules are considered part of the ontology TBox and their role is to provide
guidance to the interpretation process. Their main responsibility is to provide
additional knowledge on how analysis results (specified through MLCIs and relations between MLCIs) can be mapped into HLCIs within a single modality, and
how HLCIs from various modalities can be fused. As a result, rules can be split
in two categories: rules for semantic interpretation, and rules for fusion. Both
categories follow the same design pattern for rules: each rule is built around a

BOEMIE: Reasoning-based Information Extraction

9

specific instance or a relation between two instances in the “head” of the rule,
followed by a set of statements or restrictions in the “body” of the rule. When
a rule is applied by the semantic interpretation engine, instances can be created
to satisfy the rule, either for concepts/relations of the head (forward rules) or
for concepts on the head (backward rules).
Forward rules Forward rules perform an action (usually the addition of a relation between two instances) described in the head of the rule, if the restrictions
contained in the body have been satisfied. For example, consider the following
ABox fragment:
(personN ame1 , “JaroslavRybakov”) : hasV alue
(ranking1 , “1”) : hasV alue
(person1 , personN ame1 ) : hasP ersonN ame
personN ame1 : P ersonN ame
person1 : P erson
ranking1 : Ranking
(personN ame1 , ranking1 ) : personN ameT oRanking

This ABox fragment describes the situation where the semantics extraction engine has identified two MLCIs, a person name (“Jaroslav Rybakov”) and a ranking (“1”), connected with the “personNameToRanking” relation. Also, a “Person”
instance exists that relates only to the “PersonName” instance, but not to the
“Ranking” instance. Despite the fact that the personN ame1 MLCI is related to
the ranking1 MLCI, the person1 HLCI that aggregates personN ame1 is not related to the “Ranking” instance. In order for the “Person” instance to be related
to the “Ranking” instance, a forward rule like the following one must be present
during interpretation:
personT oRanking(X, Z) ← P erson(X), P ersonN ame(Y ),
hasP ersonN ame(X, Y ),
personN ameT oRanking(Y, Z)
This rule can be interpreted as follows: if a “Person” instance X and a “PersonName” instance Y are found connected with a hasP ersonN ame(X, Y ) relation,
and a relation “personNameToRanking” exists between the “PersonName” Y and
any instance Z, then add a relation between the “Person” instance X and the
instance Z. The fact that the rule is applied in a forward way, suggests that all
restrictions in the body have to be met, for the relation “personToRanking” on
the head to be added in an ABox.
Backward rules Backward rules on the other hand assume that the restriction described by the head is already satisfied by the ABox (i.e. instances and

10

Georgios Petasis, Ralf Möller, Vangelis Karkaletsis

relations exist in the ABox), and that the action involves the addition of (one or
more) missing instances or relations to satisfy the body. Consider for example
the following ABox fragment from the image modality:
personBody1 : P ersonBody
personF ace1 : P ersonF ace
(personBody1 , personF ace1 ) : isAdjacent
This ABox fragment describes two MLCIs (a person face and a person body)
that are found adjacent inside an image. Also, suppose that the TBox contains
a backward rule like the following one:
isAdjacent(Y, Z) ← P erson(X), P ersonBody(Y ), P ersonF ace(Z),
hasP art(X, Y ), hasP art(X, Z)
This rule roughly suggests that if a person face and a person body instances
are aggregated by a person instance (and thus both body parts are related to
the person instance with the “hasPart” relation), then the two body parts must
be adjacent to each other. However, since the relation isAdjacent(personBody1 ,
personF ace1 ) already exists in the ABox and the rule is a backward one, it will
try to hypothesise a “Person” instance X, and aggregate the two body parts.
3.4

Rules for Semantic Interpretation

One domain of rules application is the semantic interpretation of the results obtained from the low level analysis, performed on multimedia resources. During
this interpretation process, the MLCIs and the relations among MLCIs are examined, in order to aggregate the MLCIs into HLCIs. Then, relations that hold
between MLCIs are promoted to the HLCIs that aggregate the corresponding
MLCIs. Finally, an iterative process starts, which tries to aggregate the HLCIs
into other HLCIs and again promote the relations, until no other instances can
be added to an ABox. As a result, two types of rules are required during interpretation: rules that aggregate concept instances (either MLCIs or HLCIs) into
instances of HLCs, and rules that promote relations. However, not all relations
must be promoted: only relations that hold between a property instance of an
HLCI and an instance that is not a property of the HLCI should be promoted
to the HLCI. The aggregation of instances into HLCIs is performed with the
help of backward rules 6 , while the promotion of relations from properties to the
aggregating HLCI is performed with forward rules.
3.5

Acquiring Rules

When the ontology changes (i.e. through the addition of a new concept) the
interpretation rules must be modified accordingly. We tried to automate this
6

Backward rules imply the use of abduction to hypothetize instances not contained
in the original ABox.

BOEMIE: Reasoning-based Information Extraction

11

task by monitoring ontological changes: the actions performed by an ontology
expert to the ontology are monitored and reflected to the interpretation rules,
following a transformation based approach. Considering as input what an ABox
can contain without the current concept definition available, and as output the
instances that can be generated from the concept if defined, the rule generation
approach tries to find a set of rules that can transform the input into the desired
output. In order to perform this transformation, the transformation is split into
a set of more primitive “operations” that can be easily transformed into rules.
Assuming the set of all possible concepts C, the set of all possible relations R, a set of predefined operations O on a single concept c ∈ C, and a
N
modification M over c, where M = {mi (c, ci , ri )}1 , mi ∈ O, ci ∈ C, ci 6= c,
N
ri ∈ R, the target is to calculate a rule set S = {ri }1 , ri = Tmi (c, ci , ri ), that
corresponds to the modification M . Tmi is a function that transforms a hypothesized initial state (c0 , ci , ri0 ) to the desired state (c, ci , ri ) for modification mi ,
Tmi : (c0 , ci , ri0 ) → (c, ci , ri ), c0 ∈ C, r0 ∈ R. Each function Tmi depends not only
on mi and the two states, but also on the interpretation engine. Since the objective of rules generation was to eliminate manual supervision, a pattern based
approach was selected for representing each Tmi . Each pattern is responsible for
generating the required rules from transforming the initial state (c0 , ci , ri0 ) to a
final state (c, ci , ri ) for each operation in O, possibly biased towards the specific
interpretation model.
Operations over a Single Concept A set of predefined operations O has
been defined that captures all modifications that can be performed on a concept
c within the BOEMIE system. This set contains the following operations:
– Definition of a new MLC c: This operation reflects the addition of a new MLC
to the ontology TBox, an action that is not associated with the modification
of the rules associated with the TBox. For this operation, T = {}.
– Definition of a new HLC c that aggregates a single concept c0 : This operation
describes the action of the definition of an HLC based on the presence of
either an MLC or an HLC. Typical usage of this operation is when a new HLC
c has been defined that aggregates another concept c0 , and c0 is enough to
define this concept c. In such a case, it is assumed that during interpretation
an instance of c should be created for every instance of c0 found in an ABox.
Thus the set of rules T should create an instance of c for every instance of c0 .
Example of this operation is the definition of “Person” (c) that aggregates
either “PersonName” or “PersonFace” (c0 ).
– Addition of a single concept c0 to an existing HLC c: This operation deals
with the extension of an existing HLC c with a concept c0 , i.e. when adding a
new property to an existing HLC. In such a case, T should contain rules that
aggregate instances of concept c0 with instances of concept c, and promote
all relations between the instance of c0 and instances not aggregated by the
instance of c to the c instance. Examples include the extension of “Person”
with properties like “Age”, “Gender”, or “PersonBody” and the “SportsEvent”
with “Date”, or “Location”.

12

Georgios Petasis, Ralf Möller, Vangelis Karkaletsis

– Removal of a single concept c0 from an HLC c: This operation handles property removals from HLCs. The rule set T is identical to the operation of
adding a property to an HLC, with the difference that each rule in T is
located and removed from the TBox rules, instead of extending it.
– Removal of HLC c that aggregates a single concept c0 : Again, this operation
is the negation of creating a new HLC that aggregates a single concept
operation. Thus, the rule set T is identical between the two operations, but
this operation causes the removal of all rules in T from the TBox.
– Removal of an MLC c: Similar to the addition of a new MLC operation, this
operation has no effect on the TBox rule set, i.e. no rules are removed.
Rule templates for concept definition operations In this subsection the
templates for generating rules are described, for the operators that do not have
an empty set T , and are not related to removals, which share the same T with
the corresponding addition operations.
Definition of a new HLC c that aggregates a single concept c0 The rule set T
during the definition of a new HLC c from a concept c0 should contain rules
that create instances of c from instances of c0 found in the ABox of a multimedia
resource. In the interpretation model used in BOEMIE, this can be accomplished
by a single backward rule, which can be described with the following pattern:
hc0 i (X) ← hci (Y ), has hc0 i (Y, X)
For example, if c is “Person” and c0 is “PersonName”, the following rule can be
generated from this pattern:
P ersonN ame(X) ← P erson(Y ), hasP ersonN ame(Y, X)
Addition of a single concept c0 to an existing HLC c The rule set T during the
addition of a property c0 to an HLC c should contain rules that relate instances
of c with instances of c0 found in the ABox of a multimedia resource. In addition,
it should contain rules that promote the relations of a c0 instance with all instances not aggregated by c onto the c instance. This operation reflects an action
performed on the definition of concept c, from which the “final” state (c, c0 , r) is
known. The state (c, c0 , r) is the part of the concept definition that relates to how
c aggregates c0 . For example, if “Person” in the image modality is defined as having only a single property (hasP ersonF ace : P ersonF ace), and the operation
is to extend it also with “PersonBody” through the role “hasPersonBody”, then
(c, c0 , r) = (P erson, P ersonBody, hasP ersonBody). According to the adopted
interpretation model, c0 can be aggregated with c only if c0 is related with any
property of c. If c00 , c00 6= c0 is an aggregated by c concept, then an “initial” state
(c00 , c0 , r00 ) is hypothesized, relating c0 with c00 through the relation r00 . Continuing
the example, since “Person” has a single aggregated concept, only one initial state
can be hypothesized, i.e. (c00 , c0 , r00 ) = (P ersonF ace, P ersonBody, isAdjacent).
Once both initial and final states have been decided, then a rule pattern can be

BOEMIE: Reasoning-based Information Extraction

13

defined to transform the initial into the final state. In the interpretation model
used within BOEMIE, this can be accomplished by a single backward rule, which
can be described with the following pattern:
hr00 i (Y, Z) ← hci (X) , has hc00 i (X, Y ) , hc00 i (Y ) ,
hri (X, Z) , hc0 i (Z)
Applied to our example, this pattern will lead to the following rule:
isAdjacent (Y, Z) ← P erson (X) , hasP ersonF ace (X, Y ) , P ersonF ace (Y ) ,
hasP ersonBody (X, Z) , P ersonBody (Z)
This rule can relate instances of “PersonBody” to instances of “Person”, already
related to instances of “PersonFace”. The same process should be repeated for
all possible initial states that can be found for concept c.
However these are not the only rules that should be added in set T . Each
relation w defined in the TBox that can have as subject concepts c and c0 , must
be promoted from c0 to c. This can be accomplished with forward rules that can
be generated by the following pattern:
hwi (X, Z) ← hci (X) , hri (X, Y ) , hc0 i (Y ) , hwi (Y, Z)
Please note that in this pattern no type is specified for variable Z, allowing Z
to take as value instances of any concept that is in the range of the relation hwi.
Assuming w = isN ear, this pattern can lead to the following rule:
isN ear (X, Z) ← P erson (X) , hasP ersonBody (X, Y ) , P ersonBody (Y ) ,
isN ear (Y, Z)
The rule set T must be extended with a single rule of the above form for each
w that can be found in the ontology TBox.

4

Conclusions

In this paper we have presented a novel approach for exploiting an ontology in an
ontology-based information extraction system, which substitutes part of the extraction process with reasoning, guided by a set of automatically acquired rules.
Innovative aspects of the presented framework include the use of reasoning in the
construction of an ontology-based information extraction system that can adapt
to changes in the ontology and the clear distinction between concepts of the lowlevel analysis (MLCs), and the semantic interpretation (HLCs). An interesting
future direction is the investigation of how reasoning can be better applied on
modalities involving the dimension of time, such as video. In BOEMIE a simple approach has been followed regarding the handling of time sequences, where
extracted real objects or events were grounded to timestamps, and artificial relations like “before” and “after” were added. Nevertheless, an enhancement that
maintains the temporal semantics from the perspective of reasoning will be an
interesting addition.

14

Georgios Petasis, Ralf Möller, Vangelis Karkaletsis

Acknowledgments.
This work has been partially funded by the BOEMIE Project, FP6-027538, 6th
EU Framework Programme.

References
1. Alani, H., Kim, S., Millard, D.E., Weal, M.J., Hall, W., Lewis, P.H., Shadbolt,
N.R.: Automatic ontology-based knowledge extraction from web documents. IEEE
Intelligent Systems 18(1), 14–21 (Jan 2003), http://dx.doi.org/10.1109/MIS.
2003.1179189
2. Alani, H., Kim, S., Millard, D.E., Weal, M.J., Lewis, P.H., Hall, W., Shadbolt, N.:
Automatic extraction of knowledge from web documents. In: Workshop on Human
Language Technology for the Semantic Web and Web Services, 2 nd Int. Semantic
Web Conf. Sanibel Island (2003)
3. Buitelaar, P., Cimiano, P., Frank, A., Hartung, M., Racioppa, S.: Ontology-based
information extraction and integration from heterogeneous data sources. Int. J.
Hum.-Comput. Stud. 66(11), 759–788 (Nov 2008), http://dx.doi.org/10.1016/
j.ijhcs.2008.07.007
4. Buitelaar, P., Cimiano, P., Magnini, B.: Ontology Learning from Text: Methods,
Evaluation and Applications, Frontiers in Artificial Intelligence and Applications
Series, vol. 123. IOS Press, Amsterdam (7 2005)
5. Buitelaar, P., Siegel, M.: Ontology-based information extraction with soba. In:
In: Proc. of the International Conference on Language Resources and Evaluation
(LREC). pp. 2321–2324 (2006)
6. Castano, S., Peraldi, I.S.E., Ferrara, A., Karkaletsis, V., Kaya, A., Möller, R.,
Montanelli, S., Petasis, G., Wessel, M.: Multimedia Interpretation for Dynamic
Ontology Evolution. Journal of Logic and Computation 19(5), 859–897 (2009)
7. Cimiano, P., Handschuh, S., Staab, S.: Towards the self-annotating web. In: Proceedings of the 13th international conference on World Wide Web. pp. 462–471.
WWW ’04, ACM, New York, NY, USA (2004), http://doi.acm.org/10.1145/
988672.988735
8. Dung, T.Q., Kameyama, W.: Ontology-based information extraction and information retrieval in health care domain. In: Proceedings of the 9th international conference on Data Warehousing and Knowledge Discovery. pp. 323–333. DaWaK’07,
Springer-Verlag, Berlin, Heidelberg (2007), http://dl.acm.org/citation.cfm?
id=2391952.2391991
9. Espinosa, S., Kaya, A., Melzer, S., Möller, R.: On ontology based abduction for
text interpretation. In: Gelbukh, A. (ed.) Proc. of 9th International Conference
on Intelligent Text Processing and Computational Linguistics (CICLing-2008). pp.
194–205. No. 4919 in LNCS, Springer (2008)
10. Gruber, T.R.: Toward principles for the design of ontologies used for knowledge
sharing. Int. J. Hum.-Comput. Stud. 43(5-6), 907–928 (Dec 1995), http://dx.doi.
org/10.1006/ijhc.1995.1081
11. Gutierrez, F., Wimalasuriya, D.C., Dou, D.: Using information extractors with
the neural electromagnetic ontologies. In: Meersman, R., Dillon, T.S., Herrero, P.
(eds.) OTM Workshops. Lecture Notes in Computer Science, vol. 7046, pp. 31–32.
Springer (2011)

BOEMIE: Reasoning-based Information Extraction

15

12. Hwang, C.H.: Incompletely and imprecisely speaking: Using dynamic ontologies for
representing and retrieving information. In: Franconi, E., Kifer, M. (eds.) KRDB.
CEUR Workshop Proceedings, vol. 21, pp. 14–20. CEUR-WS.org (1999)
13. Iosif, E., Petasis, G., Karkaletsis, V.: Ontology-Based Information Extraction under
a Bootstrapping Approach. In: Pazienza, M.T., Stellato, A. (eds.) Semi-Automatic
Ontology Development: Processes and Resources, chap. 1, pp. 1–21. IGI Global,
Hershey, PA, USA (April 2012)
14. Karkaletsis, V., Fragkou, P., Petasis, G., Iosif, E.: Ontology based information
extraction from text. In: Paliouras, G., Spyropoulos, C.D., Tsatsaronis, G. (eds.)
Knowledge-Driven Multimedia Information Extraction and Ontology Evolution.
Lecture Notes in Computer Science, vol. 6050, pp. 89–109. Springer (2011)
15. Kim, S., Alani, H., Hall, W., Lewis, P., Millard, D., Shadbolt, N., Weal, M.: Artequakt: Generating tailored biographies from automatically annotated fragments
from the web. In: Workshop on Semantic Authoring, Annotation & Knowledge
Markup (SAAKM?02), the 15th European Conference on Artificial Intelligence,
(ECAI?02). vol. -, pp. 1–6 (2002), http://eprints.soton.ac.uk/256913/, event
Dates: July 21-26
16. Krötzsch, M., Maier, F., Krisnadhi, A.A., Hitzler, P.: Nominal schemas for integrating rules and description logics. In: Rosati, R., Rudolph, S., Zakharyaschev, M.
(eds.) Description Logics. CEUR Workshop Proceedings, vol. 745. CEUR-WS.org
(2011)
17. Lafferty, J.D., McCallum, A., Pereira, F.C.N.: Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In: Proceedings of the
Eighteenth International Conference on Machine Learning. pp. 282–289. ICML ’01,
Morgan Kaufmann Publishers Inc., San Francisco, CA, USA (2001)
18. Li, Y., Bontcheva, K.: Hierarchical, perceptron-like learning for ontology-based
information extraction. In: Proceedings of the 16th international conference on
World Wide Web. pp. 777–786. WWW ’07, ACM, New York, NY, USA (2007),
http://doi.acm.org/10.1145/1242572.1242677
19. Maedche, A., Maedche, E., Staab, S.: The text-to-onto ontology learning environment. In: Software Demonstration at ICCS-2000 - Eight International Conference
on Conceptual Structures (2000)
20. Maedche, A., Neumann, G., Staab, S.: Intelligent exploration of the web. chap.
Bootstrapping an ontology-based information extraction system, pp. 345–359.
Physica-Verlag GmbH, Heidelberg, Germany, Germany (2003), http://dl.acm.
org/citation.cfm?id=941713.941736
21. Marsh, E., Perzanowski, D.: Muc-7 evaluation of ie technology: Overview of results. In: Proceedings of the Seventh Message Understanding Conference (MUC7). http://www.itl.nist.gov/iaui/894.02/related projects/muc/index.html
(1998)
22. Petasis, G., Karkaletsis, V., Paliouras, G., Androutsopoulos, I., Spyropoulos, C.D.:
Ellogon: A New Text Engineering Platform. In: Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC 2002). pp. 72–
78. European Language Resources Association, Las Palmas, Canary Islands, Spain
(May 29–31 2002)
23. Petasis, G., Karkaletsis, V., Paliouras, G., Krithara, A., Zavitsanos, E.: Ontology Population and Enrichment: State of the Art. In: Paliouras, G., Spyropoulos, C.D., Tsatsaronis, G. (eds.) Knowledge-Driven Multimedia Information Extraction and Ontology Evolution, Lecture Notes in Computer Science, vol. 6050,
pp. 134–166. Springer Berlin / Heidelberg (2011), http://dx.doi.org/10.1007/
978-3-642-20795-2\_6

16

Georgios Petasis, Ralf Möller, Vangelis Karkaletsis

24. Petasis, G., Karkaletsis, V., Paliouras, G., Krithara, A., Zavitsanos, E.: Ontology
Population and Enrichment: State of the Art. In: Paliouras, G., Spyropoulos, C.D.,
Tsatsaronis, G. (eds.) Knowledge-Driven Multimedia Information Extraction and
Ontology Evolution, Lecture Notes in Computer Science, vol. 6050, pp. 134–166.
Springer Berlin / Heidelberg (2011)
25. Popov, B., Kiryakov, A., Kirilov, A., Manov, D., Ognyanoff, D., Goranov, M.: Kim
- semantic annotation platform. In: Fensel, D., Sycara, K.P., Mylopoulos, J. (eds.)
International Semantic Web Conference. Lecture Notes in Computer Science, vol.
2870, pp. 834–849. Springer (2003)
26. Popov, B., Kiryakov, A., Ognyanoff, D., Manov, D., Kirilov, A.: Kim - a semantic
platform for information extraction and retrieval. Natural Language Engineering
10(3-4), 375–392 (2004)
27. Schank, R.C., Abelson, R.P.: Scripts, Plans, Goals and Understanding: an Inquiry
into Human Knowledge Structures. L. Erlbaum, Hillsdale, NJ (1977)
28. Schank, R.C., Kolodner, J.L., DeJong, G.: Conceptual information retrieval. In:
Proceedings of the 3rd annual ACM conference on Research and development in
information retrieval (SIGIR ’80). pp. 94–116. Cambridge, UK (1980)
29. Studer, R., Benjamins, R., Fensel, D.: Knowledge engineering: Principles and methods. Data & Knowledge Engineering 25(1-2), 161–198 (März 1998)
30. Wimalasuriya, D.C., Dou, D.: Components for information extraction: ontologybased information extractors and generic platforms. In: Proceedings of the 19th
ACM international conference on Information and knowledge management. pp. 9–
18. CIKM ’10, ACM, New York, NY, USA (2010), http://doi.acm.org/10.1145/
1871437.1871444
31. Wimalasuriya, D.C., Dou, D.: Ontology-based information extraction: An introduction and a survey of current approaches. J. Inf. Sci. 36(3), 306–323 (Jun 2010),
http://dx.doi.org/10.1177/0165551509360123
32. Wu, F., Weld, D.S.: Automatically refining the wikipedia infobox ontology. In:
Proceedings of the 17th international conference on World Wide Web. pp. 635–
644. WWW ’08, ACM, New York, NY, USA (2008), http://doi.acm.org/10.
1145/1367497.1367583

Recognizing Implicit Discourse Relations
through Abductive Reasoning
with Large-scale Lexical Knowledge
Jun Sugiura, Naoya Inoue, and Kentaro Inui
Tohoku University, 6-3-09 Aoba, Aramaki, Aoba-ku, Sendai, 980-8579, Japan
{jun-s,naoya-i,inui}@ecei.tohoku.ac.jp
Abstract. Discourse relation recognition is the task of identifying the
semantic relationships between textual units. Conventional approaches to
discourse relation recognition exploit surface information and syntactic
information as machine learning features. However, the performance of
these models is severely limited for implicit discourse relation recognition.
In this paper, we propose an abductive theorem proving (ATP) approach
for implicit discourse relation recognition. The contribution of this paper
is that we give a detailed discussion of an ATP-based discourse relation
recognition model with open-domain web texts.
Keywords: Discourse Relation, Abductive Reasoning, Lexical Knowledge, Association Information

1

Introduction

Discourse relation recognition is the task of identifying the semantic relationship between textual units. For example, given the sentence John pushed Paul
towards a hole.(S1) Paul didn’t get hurt.(S2) , we identify a contrast relationship
between textual units (S1) and (S2). Discourse relation recognition is useful for
many NLP tasks such as summarization, question answering, and coreference
resolution.
The traditional studies on discourse relation recognition divided discourse relations into two distinct types according to the presence of discourse connectives
between textual units: (i) an explicit discourse relation, or discourse relation with
discourse connectives (e.g. John hit Tom because he is angry.). (ii) an implicit
discourse relation, or discourse relation without discourse connectives (e.g. John
hit Tom. He got angry.). Identifying an implicit discourse relation is much more
diﬃcult than identifying an explicit discourse relation. In this paper, we focus
on the task of implicit discourse relation recognition.
Conventional approaches to implicit discourse relation recognition exploit
surface information (e.g. bag of words) and syntactic information (e.g. syntactic
dependencies between words) to identify discourse relations [1, 2, etc.]. However,
the performance of these models is severely limited as mentioned in Sec. 2.2. We
believe that the problem of these approaches is two fold: (i) they do not capture causality between the events mentioned in each textual units, and (ii) they

2

Jun Sugiura, Naoya Inoue, Kentaro Inui

do not capture the factuality of these events. We believe that this information
plays a key role for implicit discourse relation recognition. Suppose we want to
recognize a contrast relation between S1 and S2 in the first example. To recognize the discourse relation, we need to at least know the following information:
(i) commonsense knowledge: pushing into a hole usually causes getting hurt;
(ii) factuality: Paul did not get hurt. Finally, combining (i) and (ii), we need
to recognize the unusualness of discourse: something against our commonsense
knowledge happened in S2 . As described in Sec. 3, our investigation revealed
that we have several patterns of reasoning and need to combine several kinds of
reasoning to identify a discourse relation.
Motivated by this observation, we propose an abductive theorem proving
(ATP) approach for implicit discourse relation recognition. The key advantage of
using ATP is that the declarative nature of ATP abstracts the flow of information
away in a modeling phase: we do not have to explicitly specify when and where
to use a particular reasoning. Once we give several primitive inference rules to
an ATP system, the system automatically returns the best answer to a problem,
combining the inference rules.
In this paper, we attempt to answer the following open issues of ATP-based
discourse relation recognition: (i) does it really work on real-life texts?; (ii) does
it work with a large knowledge base which is not customized for solving target
texts? The contribution of this paper is as follows. We give a detailed discussion
of an ATP-based discourse relation recognition model with open-domain web
texts. In addition, we show that our ATP-based model is computationally feasible
with a large knowledge base.
The structure of this paper is as follows. In Sec. 2, we describe abduction and
give an overview of previous eﬀorts on discourse relation recognition. In Sec. 3,
we describe our ATP-based discourse relation recognition model. In Sec. 4, we
report the results of pilot evaluation of our model with large lexical knowledge.

2
2.1

Background
Weighted abduction

Abduction is inference to the best explanation. Formally, logical abduction is
defined as follows:
– Given: Background knowledge B, and observations O, where both B and
O are sets of first-order logical formulas.
– Find: A hypothesis (or explanation, abductive proof ) H such that H ∪ B |=
O, H ∪ B 6|=⊥, where H is a conjunction of literals. We say that p is hypothesized if H ∪ B |= p, and that p is explained if (∃q) q → p ∈ B and
H ∪ B |= q.
Typically, several hypotheses H explaining O exist. We call each of them a
candidate hypothesis and each literal in a hypothesis an elemental hypothesis.
The goal of abduction is to find the best hypothesis among candidate hypotheses

Large-scale Abductive Discourse Relation Recognition

3

by a specific measure. In the literature, several kinds of evaluation measure have
been proposed, including cost-based and probability-based [3, 4, etc.].
In this paper, we adopt the evaluation measure of weighted abduction, which
is proposed by Hobbs et al. [4]. In principle, the evaluation measure gives a
penalty for assuming specific and unreliable information but rewards for inferring
the same information from diﬀerent observations. We summarize the primary
feature of this measure as follows (see [4] for more detail):
– Each elemental hypothesis has a positive real-valued cost;
– The cost of each candidate hypothesis is defined as the sum of costs of the
elemental hypotheses;
– The best hypothesis is defined as the minimum-cost candidate hypothesis;
– If an elemental hypothesis is explained by other elemental hypothesis, the
cost becomes zero.
2.2

Related work

Discourse relation recognition is a prominent research area in NLP. Most researchers have primarily focused on explicit discourse relation recognition, employing statistical machine learning-based models [5, 6, etc.] with superficial and
syntactic information. The performance of explicit discourse relation recognition
is comparatively high; for instance, Lin et al. [7] achieved an 80.6% F-score.
The performance of implicit discourse relation recognition is, however, relatively low (25.5% F-score). Most existing work on implicit discourse relation
recognition [1, 2, etc.] extend the feature set of [5] with richer lexico-syntactic
information. For example, Pitler et al. [2] exploit a syntactic parse tree and sentiment polarity information of words contained in textual units to generate a
feature set. However, the performance is not as high as a practical level.
An abductive discourse relation recognition model is originally presented in
Hobbs et al. [4]. However, Hobbs et al. [4] reported the results in a fairly closed
setting: they tested their model on two test texts with manually encoded background knowledge which is required to solve the discourse relation recognition
problems that appear in two texts. Therefore, it is an open question whether the
abductive discourse relation recognition model works in an open setting where
the wider range of real-life texts and large knowledge base are considered.

3

Abductive theorem proving for discourse relation
recognition

In this section, we describe our discourse relation recognition model. We employ ATP to recognize a discourse relation. Given target discourse segments,
we abductively prove that there exists a coherence relation (i.e. some discourse
relation) between the discourse segments using background knowledge. We axiomatize (i) definition of discourse relations and (ii) lexical knowledge (e.g. causal
knowledge of events) in the background knowledge, which serve as a proof of the
existence of a coherence relation.

4

Jun Sugiura, Naoya Inoue, Kentaro Inui

The motivation of using abductive theorem proving is that we can assume a
proposition with the cost even if we fail to find a complete proof of a coherence
relation between discourse segments, as mentioned in Sec. 2. By choosing the
minimum-cost abductive proof, we can identify the most likely discourse relation.
We first show how to axiomatize the definition of discourse relations (Sec.
3.1). We then conduct an example-driven investigation of lexical knowledge
which is required to solve a few real-life discourse relation recognition problems
in order to identify a type of lexical knowledge needed for an ATP-based recognition model (Sec. 3.2). In Sec. 3.2, we make sure that our developed theory works
on a general-purpose inference engine as we expected. We use the lifted first-order
abductive inference engine Henry, which is developed by one of the authors.[8]
To perform deeper analysis of the inference results, we also improved the existing
visualization module provided by Henry. The inference engine and visualization
tool are publicly available at https://github.com/naoya-i/henry-n700/.
3.1

Axiomatizing definitions of discourse relations

We follow the definitions of discourse relations provided by Penn Discourse TreeBank (PDTB) 2.0 [9], a widely used and large-scale corpus annotated with discourse relations.1 The PDTB defines four coarse-grained discourse relations, but
it is still rather diﬃcult to identify all discourse relations. Therefore, we adopt
two-way classification: whether it is adversative (Comparison in PDTB) or resultative (Temporal, Contingency, Expansion in PDTB). Because a resultative
relation can be regarded as relations other than an adversative relation, we first
axiomatize the definition of adversative and then consider the other relation.
According to the PDTB Annotation Manual [12], an adversative relation
consists of two subtypes: Concession and Contrast. These subtypes are defined
below, respectively.
Concession One of the arguments describes a situation A which causes C, while
the other asserts (or implies) ￢ C. One argument denotes a fact that triggers
a set of potential consequences, while the other denies one or more of them.
Contrast Arg1 and Arg2 share a predicate or a property and the diﬀerence is
highlighted with respect to the values assigned to this property.
The condition of Concession can be described as the following axiom:
event(e1 , type1, F, x1 , x2 , x3 , s1 ) ∧ event(e2 , type2, N F, y1 , y2 , y3 , s2 )

∧ cause(e1 , F, e2 , F ) ∧ event(eu , type2, EF, y1 , y2 , y3 , su ) ⇒ Adversative(s1 , s2 ).

This axiom says that if event e1 occurs in segment s1 (roughly corresponding to
Arg1 in PDTB) and that event is expected to cause an event of type2 while such
an event of type2 does actually not occur in segment s2 (roughly corresponding
to Arg2 in PDTB), then the discourse relation between s1 and s2 is Adversative. The examples using this type of axiom to recognize discourse relations will
1

For other corpora, see [10, 9, 11, etc.].

Large-scale Abductive Discourse Relation Recognition

5

be mentioned later. On the other hand, a typical pattern where the Contrast
relation holds can be described, for example, as follows:
value(e1 , P os, s1 ) ∧ value(e2 , N eg, s2 ) ⇒ Adversative(s1 , s2 ).
This axiom says that when the sentiment polarity of e1 in segment s1 is Positive
and the sentiment polarity of e2 in segment s2 is Negative, the discourse relation
between s1 and s2 is Adversative. The examples of axioms described here represent formation conditions of Adversative and take some variation due to their
value of factuality or sentiment.
Furthermore, these axioms can represent conditions of Resultative. For instance, if the sentiment polarity of e2 in segment s2 is the same as that of segment
s1 , then the discourse relation between s1 and s2 is Resultative as below:
value(e1 , P os, s1 ) ∧ value(e2 , P os, s2 ) ⇒ Resultative(s1 , s2 ).
In total, we created 21 axioms for the definition of discourse relations.
Finally, we add the following axioms to connect the definition of discourse
relations with the existence of a coherence relation between discourse segments:
Adversative(s1 , s2 ) ⇒ CoRel(s1 , s2 )
Resultative(s1 , s2 ) ⇒ CoRel(s1 , s2 ),

(1)
(2)

where CoRel(s1 , s2 ) indicates that there exists a coherence relation between segments s1 and s2 . Given target discourse segments S1 , S2 , we prove CoRel(S1 , S2 )
using the axioms described above and lexical knowledge which is described in
the next section.
We formally describe our meaning representation. First, we use cause(ea , fa , ec , fc )
to represent that event ea with factuality fa causes event ec with factuality fc .
Second, we represent an event by using event(e, t, f, x1 , x2 , x3 , s), where e is the
event variable, t is the event type of e, f is the factuality of event e, x1 , x2 , x3
are arguments of event and s is the segment which event e belongs to. Factuality
of event e can take one of the following four values: F (Fact; e occurred), N F
(NonFact; e did not occur), EF (Expected-Fact; e is expected to occur), and
EN F (Expected-NonFact; e is expected not to occur). In addition, the value
(sentiment polarity) of event e is represented as value(e, v, s). v is either Pos
(Positive) or Neg (Negative).
3.2

Example-driven investigation of lexical knowledge

Next, we manually analyzed a small number of samples for each discourse relation
to investigate what types of knowledge are required to explain those samples and
how to axiomatize them. In this paper, we manually convert each sample text
into the logical forms, extracting main verbs in its matrix clauses as predicates.2
2

In future work, we will exploit the oﬀ-the-shelf semantic parser (e.g. Boxer [13]) to
automatically get the logical forms. Using automatic semantic parsers brings some
challenges to us, e.g. how to represent the verbs in embedded clauses. We do not
address these issues in this work, because we want to focus on the investigation of
types of world knowledge that are required to identify discourse relations.

6

Jun Sugiura, Naoya Inoue, Kentaro Inui

cause
(X8, F, _7~_0, ENF)
$0.00/4
_6=X8, _7=_0
event
(_7, Use-vb~_10, ENF, S1)
$0.36/14
inhibit

event
(X8~_6, Close-vb~_8, F, S2)
$0.36/12

_7=_0, _10=Use-vb, S1=_1

ADVcause9-2

cause
(X8~_6, F, _7, ENF)
$0.36/11

ADVcause9-2 ADVcause9-2

event
(_7~_0, Use-vb, ENF, S1~_1)
$1.20/3
inhibit

event
(X6~_9, Use-vb~_10, F, S1)
$0.36/13
ADVcause9-2

^

_6=X8, _8=Close-vb

_9=X6, _10=Use-vb

Adv
(S1, S2)
$1.20/5

^

?
event
(X8, Close-vb, F, S2)
$1.00/0

CoRel
(S1, S2)
$1.00/2

event
(X6, Use-vb, F, S1)
$1.00/1

Fig. 1. Example of the abductive proof automatically produced by our system. The
black directed edges: backward-chainings. The red undirected edges: unification. The
labels of undirected red edges: unifier. The terms starting with a capital letter: constant;
otherwise, variable. “X ∼ Y ”: Y is unified with X. The grayed nodes: explained literals.
The red nodes: hypothesized literals.

The dataset consists of text which we collect from the Web.3 This website
provides English texts for adult English learners as a second language. We collect
16 discourse segment pairs in which half of them can be regarded as Adversative
and the others can be regarded as Resultative.
Let us take one of the simplest samples, example (2).
(2) S1: A lot of traﬃc once used Folsom Dam Road.
S2: Right now, the road is closed.
(Topic=Working, StoryID=174)
In this example, S1 and S2 are in the Adversative relation. While the Folsom Dam
Road was once used by a lot of traﬃc, it is not usable now because it is closed.
Something was once used but it is now unusable; therefore, the Adversative
relation holds. This can be described as the following axiom:
Condition of Adversative
event(e1 , type1, F, x1 , x2 , x3 , s1 ) ∧ event(e2 , type2, F, y1 , y2 , y3 , s2 )

∧ cause(e2 , F, eu , EN F ) ∧ event(eu , type1, EN F, x1 , x2 , x3 , s1 ) ⇒ Adversative(s1 , s2 )

The causality relation between the “closed” event and the “unusable” event can
be described as:
3

http://www.cdlponline.org/

Large-scale Abductive Discourse Relation Recognition

7

Relation between events
event(e1 , U se, EN F, x1 , x2 , x3 , s1 ) ∧ cause(e2 , F, e1 , EN F )

⇒ event(e2 , Close, F, y1 , x2 , y3 , s2 )

Note that we have cause(e2 , F, e1 , N F ) in the left-hand side of the axiom. We
use this literal to accumulate the type of reasoning. In an abductive proof, we
expect this literal to unify with an elemental hypothesis generated by the axioms
of discourse relation.
Fig. 1 shows the result of applying the proposed model to example (2).4 In
Fig. 1, the observations consists of three literals; occurrence of event whose type
is Use in segment S1 , occurrence of event whose type is Close in segment S2 , and
CoRel(S1 , S2 ) which is a symbol of existence of some discourse relation between
the segments.
To see how our model combines multiple pieces of knowledge, let us take
another example.
(3) S1: Right now, the road is closed.
S2: Most of the people who used the road every day are angry.
(Topic=Working, StoryID=174)
The “closed” event causes the “unusable” event and the “unusable” event than
further causes the “angry” event, which can be explained by combining the
knowledge that being “unusable” is negative in sentiment polarity and the knowledge that a negative event may cause someone to be angry. These pieces of
knowledge can be axiomatized as follows. The proof graph is shown in Fig. 2.
Condition of Resultative
event(e1 , type1, F, x1 , x2 , x3 , s1 ) ∧ event(e2 , type2, F, y1 , y2 , y3 , s2 )
∧ cause(e1 , F, eu , EF ) ∧ event(eu , type2, EF, y1 , y2 , y3 , s2 ) ⇒ Resultative(s1 , s2 )

Relation between events

event(e1 , U se, EN F, x1 , x2 , x3 , s1 ) ∧ cause(e2 , F, e1 , EN F )
⇒ event(e2 , Close, F, y1 , x2 , y3 , s2 )

event(e1 , Angry, EF, x1 , x2 , x3 , s1 ) ∧ cause(e2 , f, e1 , EF )
⇒ value(e2 , N eg, s2 ) ∧ event(e2 , type, f, y1 , y2 , y3 , s2 )

Transitivity

cause(e1 , f1 , e2 , f2 ) ∧ cause(e2 , f2 , e3 , f3 ) ⇒ cause(e1 , f1 , e3 , f3 )

Polarity

value(e1 , N eg, s1 ) ⇒ event(e1 , U se, EN F, x1 , x2 , x3 , s1 )

Through the investigation as illustrated above, we reached the conclusion
that the axioms in Table 1 can recognize discourse relations for most examples.
4

Throughout this paper, we omit the arguments of events from our representation
in a proof graph for readability. Since we do not have a lexical knowledge between
nouns and verbs, this simplification does not aﬀect to the result of inference.

8

Jun Sugiura, Naoya Inoue, Kentaro Inui

event
(_18, Angry-adj~_21, EF, S2)
$0.36/23
_26=_18, _21=Angry-adj, S2=_27
event
(_18~_26, Angry-adj, EF, S2~_27)
$3.17/26
cause

cause

cause
(_0, ENF, _18~_26, EF)
$0.00/27
cause

^

cause

^

cause
(X1, F, _0, ENF)
$0.00/4

_0=_30, _31=ENF, _26=_18

X1=_17, _0=_30, _31=ENF

cause
(_0~_30, ENF~_31, _18, EF)
$0.22/46

cause
(X1~_17, F, _0~_30, ENF~_31)
$0.22/45
Transitive

value
(_0, Neg, _1)
$1.44/7

inhibit

Transitive

REScause6-2

^

polar
event
(_0, Use-vb, ENF, _1)
$1.20/3

event
(X1~_17, Close-vb~_19, F, S1)
$0.36/21

inhibit

cause
(X1~_17, F, _18, EF)
$0.36/20
REScause6-2 REScause6-2

^

event
(E16~_20, Angry-adj~_21, F, S2)
$0.36/22
REScause6-2

^
X1=_17, _19=Close-vb

_20=E16, _21=Angry-adj

Res
(S1, S2)
$1.20/6
?
event
(X1, Close-vb, F, S1)
$1.00/0

CoRel
(S1, S2)
$1.00/2

event
(E16, Angry-adj, F, S2)
$1.00/1

Fig. 2. Example of the abductive proof automatically produced by our system. See the
description of Fig. 1.

4

Pilot large-scale evaluation

As mentioned in the previous section, our model assumes that axioms encoding
lexical knowledge are automatically extracted from a large lexical resources (see
“To be automatically acquired”axioms in Sec. 3.2.) In this section, we extract the
axioms of causal relations and synonym/hyperonym relations from WordNet [14]
and FrameNet [15], both popular and large lexical resources, and then apply our
model to the example texts presented in Sec. 3. Regarding sentiment polarity,
we plan to extract the axioms from a large-scale sentiment polarity lexicon such
as [16] in future work.
We clarify that our primary focus here is the feasibility of our ATP-based
discourse relation recognition model with a large knowledge base. The quantitative evaluation of our model (e.g. the predictive accuracy of discourse relations) is future work. Therefore, we first report how to incorporate WordNet and
FrameNet axioms into our knowledge base (Sec. 4.1) and then preliminarily report the computational time of inference required to solve the example problems,
showing some interesting output (Sec. 4.2).

Large-scale Abductive Discourse Relation Recognition

9

Table 1. Set of axiom type.
Scale

Type of knowledge Examples of axiom
event(e1 , type1, F, x1 , x2 , x3 s1 )
∧ event(e2 , type2, F, y1 , y2 , y3 , s2 )
∧ cause(e2 , F, eu , EN F )
∧ event(eu , type1, EN F, x1 , x2 , x3 , s1 )
⇒ Adversative(s1 , s2 );
value(e1 , P os, s1 ) ∧ value(e2 , P os, s2 )
Definitions of
Small-scale discourse relations ⇒ Resultative(s1 , s2 )
(Manually
cause(e1 , f1 , e2 , f2 ) ∧ cause(e2 , f2 , e3 , f3 )
written)
⇒ cause(e1 , f1 , e3 , f3 )
Transitivity
event(e1 , U se, EN F, x1 , x2 , x3 , s1 ) ∧ cause(e2 , F, e1 , EN F )
⇒ event(e2 , Close, F, y1 , x2 , y3 , s2 );
event(e1 , Angry, EF, x1 , x2 , x3 , s1 ) ∧ cause(e2 , f, e1 , EF )
Large-scale Causal relations
⇒ value(e2 , N eg, s2 ) ∧ event(e2 , type, f, y1 , y2 , y3 , s2 )
(Automatically
event(e1 , Attack, F, x1 , x2 , x3 , s1 )
acquired)
Synonym/Hyponym ⇒ event(e1 , Destroy, F, x1 , x2 , x3 , s1 )
value(e1 , N eg, s1 ) ⇒ event(e1 , Die, F, x1 , x2 , x3 , s1 );
Sentiment polarity value(e1 , P os, s1 ) ⇒ event(e1 , Die, N F, x1 , x2 , x3 , s1 )

4.1

Automatic axiom extraction from linguistic resources

We summarize the axioms extracted from WordNet and FrameNet in Table 2.
For each resource, we extract two kinds of axioms. First, we generate axioms that
map a word to the corresponding WordNet synset or FrameNet frame (WordSynset, or Word-Frame types). The example WordNet axiom in Table 2 enables
us to hypothesize that a “die”-typed event can be mapped to WordNet synset
200358431. Second, we also encode a semantic relation between synsets or frames.
For example, the causal relation between Getting frame and Giving frame is
encoded as the axiom in the Table. 5
4.2

Results and discussion

We have tested our large-scale discourse relation recognition model on the randomly selected 7 texts as those presented in Sec. 3. We restricted the maximum
5

Note that the mapping axioms have bi-directional implications. By using the bidirectional axioms, we can combine the knowledge from FrameNet and WordNet to
perform a robust inference. For instance, we can do an inference like: pass away →
synsetA → die → FNDeath if we do not have a direct mapping from pass away to
FNDeath. Since the framework is declarative, we do not have to specify when and
where to use a particular type of knowledge, which results in a robust reasoning.

10

Jun Sugiura, Naoya Inoue, Kentaro Inui
Table 2. Axioms automatically extracted from WordNet and FrameNet.

Type
Word-Synset

Resources
WordNet

Word-Frame

FrameNet

Causal relations

Synonym/
Hyponym

WordNet
relationsA∗1
FrameNet
relations∗2
WordNet
relationsB∗3

Example
# axioms
event(e, Die, f, x1 , x2 , x3 , s)
169,362
⇔ event(e, WNSynset200358431, f, x1 , x2 , x3 , s)
event(e, Shoot, f, x1 , x2 , x3 , s)
10,358
⇔ event(e, FNUseFireArm, f, x1 , x2 , x3 , s)
event(e1 , WNSynset20036712, EF, x1 , x2 , x3 , s1 )
∧ cause(e1 , EF, e2 , F )
⇒ event(e2 , WNSynset200358431, F, y1 , y2 , y3 , s2 )
35,440
event(e1 , FNGiving, EF, x1 , x2 , x3 , s1 )
∧ cause(e1 , EF, e2 , F )
⇒ event(e2 , FNGetting, F, y1 , y2 , y3 , s2 )
6,584
event(e, WNSynset200060063, f, x1 , x2 , x3 , s)
⇒ event(e, WNSynset200358431, f, x1 , x2 , x3 , s)
177,837

*1: Causality, Entailment, Antonym. *2: IsCausativeOf, InheritsFrom, PerspectiveOn,
Precedes, SeeAlso, SubFrameOf, Uses. *3: Meronym, Hyperonym.

number of backward-chaining steps to 2 due to the computational feasibility.
For each problem, on average, the number of potential elemental hypotheses
was 13,034 and the (typed) number of axioms that were used to generate candidate abductive proofs was 142. The time of inference required to solve each
problem was 7.00 seconds on average.
Now let us show one of the proof graphs automatically produced by our system.6 In Figure 3, we show the abductive proof graph for the following discourse:
S1 : Only 56 people died from the explosion,
S2 : but many other problems have been caused because of it.
(Topic=Activity, StoryID=241)
Although we suﬀer from the insuﬃciency of lexical knowledge, the abductive
engine gave us the best proof where two segments are tied with a resultative
relation. In the proof graph, Die and CauseProblem events are used to prove
“event” literals hypothesized by the axiom of discourse relation. Note that the
causal relation between these events is not proven but assumed with $0.36.
The overall results indicate that we now have a good environment to develop
ATP-based discourse processing.

5

Conclusions

We have explored an abductive theorem proving (ATP)-based approach for implicit discourse relation recognition. We have investigated the type of axioms required for an ATP-based discourse relation recognition and identified five types
of axioms. Our result is based on real-life Web texts, and no previous work has
6

For the simplicity, we used only one axiom for the axiom of discourse relation.

Large-scale Abductive Discourse Relation Recognition

11
cause
(X0~_6, F, _12, _13)
$0.22/52

best proof	

cause
(_12, _13, X5~_7, F)
$0.22/53

Transitive
event
(_10, Die-vb~_8, EF, _11)
$0.36/51

Uses
(X0~_6, F, X5~_7, F)
$0.43/55

Precedes
(X0~_6, F, X5~_7, F)
$0.43/56

_0=_10, _8=Synset201323958, _1=_11
event
(_0, Synset201323958, EF, _1)
$1.22/23
_6=X0, _8=Synset200359806
event
Synset200359806, F, S1)
$1.22/29
wnhyp

_6=X0, _8=Synset200360501

event
(X0, Synset200360501, F, S1)
$1.22/30
wnrev

_6=X0, _8=Pop-o -vb

event
(X0, Pop-o -vb, F, S1)
$1.22/21
wnrev

_6=X0, _8=Snu -it-vb

event
(X0, Snu -it-vb, F, S1)
$1.22/22
wnrev

_6=X0, _8=Decease-vb

event
(X0, Decease-vb, F, S1)
$1.22/12

_6=X0, _8=Drop-dead-vb

event
(X0, Drop-dead-vb, F, S1)
$1.22/14

wnrev

_6=X0, _8=Die-vb

event
(X0, Die-vb, F, S1)
$1.22/13

cause
(CAG_0, X0, F, _0, F)
$0.00/24

event
(_4, Synset200360932, ENF, _5)
$1.22/35

wncsrev wncsrev

_6=X0, _8=Die-vb

wnrev

wnant

^

^

?
cause
(CAG_2, X0, F, _4, NF)
$0.00/36

wnant

REScause

REScause

Is_Causative_of
(X0~_6, F, X5~_7, F)
$0.43/57

?

?

cause
(X0~_6, F, X5~_7, F)
$0.36/48
REScause

See_also
(X0~_6, F, X5~_7, F)
$0.43/58

?

^

?

event
(X5~_7, Cause_problem-vb~_9, F, S2)
$0.36/50

REScause

^

_6=X0, _8=Die-vb
event
(X0, Die-vb, F, S1)
$2.39/38

_6=X0, _8=Die-vb

_6=X0, _8=Synset202109818
event
(X0, Die-vb, F, S1)
$2.39/46

_6=X0, _8=Die-vb

_7=X5, _9=Cause_problem-vb

wnrev
event
(X0, Synset201784953, F, S1)
$1.99/6

wn

event
(X0, Die-vb, F, S1)
$2.39/47
wnrev

wn

event
(X0, Synset202109818, F, S1)
$1.99/7
wn
event
(X0, Die-vb, F, S1)
$1.00/2

?
CoRel
(S1, S2)
$1.00/3

Res
(S1, S2)
$1.20/9

Adv
(S1, S2)
$1.20/8

?

event
(X5, Cause_problem-vb, F, S2)
$1.00/1

only-rb
(X0)
$1.00/0

Fig. 3. Abductive proof with potential elemental hypotheses. The grayed-out nodes are
those which are potentially included in the best proof, but not actually included in the
best proof. Similarly, the dotted edges are potential explainer-explainee relationships
between elemental hypotheses.

done an investigation in the same setting. Also, we have preliminarily evaluated
our model with a large knowledge base. We have automatically constructed axioms of lexical knowledge from WordNet and FrameNet, which results in around
four hundred thousand inference rules. Our experiments showed the great potential of our ATP-based model and that we are ready to develop ATP-based
discourse processing in a real-life setting.
Our future work includes three directions. First, we will create a larger knowledge base, exploiting the linguistic resources which have recently become available. As a first step, we plan to axiomatize Narrative Chain [17], ConceptNet,7
and Semantic Orientations of Words [16] to extend our axioms with the large
knowledge resources. Second, we plan on applying the technique of automatic
parameter tuning for weighted abduction [18] to our model. Third, we plan to
create a dataset for abductive discourse processing, where we annotate simple
English texts with some discourse phenomena including discourse relations and
coreference etc. As the source text, we will use materials for ESL (English as
Second Language) learners,8 a set of syntactically and lexically simple texts, so
that we can trace the detailed behavior of abductive reasoning process. About
the semantic representation, we plan to use the Boxer semantic parser [13] to
automatically get the event arguments.
7
8

http://conceptnet5.media.mit.edu/
http://www.cdlponline.org/

Transitive
Inherits_from
(X0~_6, F, X5~_7, F)
$0.43/54

12

Jun Sugiura, Naoya Inoue, Kentaro Inui

Acknowledgments. This work was supported by JSPS KAKENHI Grant
Number 23240018.

References
1. Lin, Z., Kan, M., Ng, H.: Recognizing implicit discourse relations in the penn
discourse treebank. In: Proceedings of the 2009 Conference on Empirical Methods
in Natural Language Processing. (2009) 343–351
2. Pitler, E., Nenkova, A.: Using syntax to disambiguate explicit discourse connectives
in text. In: Proceedings of the ACL-IJCNLP 2009. (2009) 13–16
3. Charniak, E., Goldman, R.: Probabilistic abduction for plan recognition. Brown
University, Department of Computer Science (1991)
4. Hobbs, J., Stickel, M., Appelt, D., Martin, P.: Interpretation as Abduction. Artificial Intelligence 63(1-2) (1993) 69–142
5. Marcu, D., Echihabi, A.: An unsupervised approach to recognizing discourse relations. In: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. (2002) 368–375
6. Subba, R., Eugenio, B.D.: An eﬀective discourse parser that uses rich linguistic
information. In: NAACL. (2009) 566–574
7. Lin, Z., Ng, H., Kan, M.: A PDTB-Styled End-to-End Discourse Parser. Arxiv
preprint arXiv:1011.0835 (2010)
8. Inoue, N., Inui, K.: Large-scale cost-based abduction in full-fledged first-order
predicate logic with cutting plane inference. In: Proceedings of the 13th European
Conference on Logics in Artificial Intelligence. (2012) 281–293
9. Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo, L., Joshi, A., Webber,
B.: The Penn Discourse TreeBank 2.0. In: Proceedings of the 6th International
Conference on Language Resources and Evaluation (LREC). (2008) 2961–2968
10. Carlson, Lynn and Marcu, Daniel and Okurowski, Mary Ellen: RST Discourse Treebank, LDC2002T07. Number LDC2002T07, Linguistic Data Consortium (2002)
11. Wolf, F., Gibson, E., Fisher, A., Knight, M.: The Discourse GraphBank: A
database of texts annotated with coherence relations. LDC (2005)
12. Prasad, R., Miltsakaki, E., Dinesh, N., Lee, A., Joshi, A., Robaldo, L., Webber,
B.: The Penn Discourse Treebank 2.0 Annotation Manual. IRCS Technical Report
(2007)
13. Bos, J.: Wide-Coverage Semantic Analysis with Boxer. In Bos, J., Delmonte, R.,
eds.: Proceedings of STEP. Research in Computational Semantics, College Publications (2008) 277–286
14. Fellbaum, C., ed.: WordNet: an electronic lexical database. MIT Press (1998)
15. Ruppenhofer, J., Ellsworth, M., Petruck, M., Johnson, C., Scheﬀczyk, J.: FrameNet
II: Extended Theory and Practice. Technical report, Berkeley, USA (2010)
16. Takamura, H., Inui, T., Okumura, M.: Extracting semantic orientations of words
using spin model. In: ACL. (2005) 133–140
17. Chambers, N., Jurafsky, D.: Unsupervised learning of narrative schemas and their
participants. In: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language
Processing of the AFNLP. (2009) 602–610
18. Yamamoto, K., Inoue, N., Watanabe, Y., Okazaki, N., Inui, K.: Discriminative
learning of first-order weighted abduction from partial discourse explanations. In:
CICLing (1). (2013) 545–558

The NL2KR Platform for building Natural Language Translation Systems
Nguyen H. Vo, Arindam Mitra and Chitta Baral
School of Computing, Informatics and Decision Systems Engineering
Arizona State University
{nguyen.h.vo, amitra7, chitta }@asu.edu

Abstract

Our approach to translate natural language text
to formal representation is inspired by Montague’s
work (Montague, 1974) where the meanings of
words and phrases are expressed as λ-calculus expressions and the meaning of a sentence is built
from semantics of constituent words through appropriate λ-calculus (Church, 1936) applications.
A major challenge in using this approach has been
the difficulty of coming up with the λ-calculus
representation of words.

This paper presents the NL2KR platform
to build systems that can translate text to
different formal languages. It is freelyavailable1 , customizable, and comes with
an Interactive GUI support that is useful in the development of a translation
system. Our key contribution is a userfriendly system based on an interactive
multistage learning algorithm. This effective algorithm employs Inverse-λ, Generalization and user provided dictionary to
learn new meanings of words from sentences and their representations. Using
the learned meanings, and the Generalization approach, it is able to translate new
sentences. NL2KR is evaluated on two
standard corpora, Jobs and GeoQuery and
it exhibits state-of-the-art performance on
both of them.

1

Montague’s approach has been widely used in
(Zettlemoyer and Collins, 2005; Kwiatkowski et
al., 2010) to translate natural language to formal
languages. In ZC05 (Zettlemoyer and Collins,
2005) the learning algorithm requires the user to
provide the semantic templates for all words. A
semantic template is a λ-expression (e.g. λx.p(x)
for an arity one predicate), which describes a particular pattern of representation in that formal language. With all these possible templates, the
learning algorithm extracts the semantic representation of the words from the formal representation of a sentence. It then associates the extracted
meanings to the words of the sentence in all possible ways and ranks the associations according to
some goodness measure. While manually coming up with semantic templates for one target language is perhaps reasonable, manually doing it for
different target languages corresponding to different applications may not be a good idea as manual
creation of semantic templates requires deep understanding of translation to the target language.
This calls for automating this process. In UBL
(Kwiatkowski et al., 2010) this process is automated by restricting the choices of formal representation and learning the meanings in a brute
force manner. Given, a sentence S and its representation M in the restricted formal language,

Introduction and Related Work

For natural language interaction with systems one
needs to translate natural language text to the input
language of that system. Since different systems
(such as a robot or database system) may have different input language, we need a way to translate
natural language to different formal languages as
needed by the application. We have developed a
user friendly platform, NL2KR, that takes examples of sentences and their translations (in a desired target language that varies with the application), and some bootstrap information (an initial
lexicon), and constructs a translation system from
text to that desired target language.
1

http://nl2kr.engineering.asu.edu/

899

Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 899–908,
c
Beijing, China, July 26-31, 2015. 
2015
Association for Computational Linguistics

user. The user can then provide meanings of some
unknown words and resumes the learning process.
Another distinguishing feature of NL2KR is its
user-friendly interface that helps users in creating
their own translation system. The closest system
to NL2KR is the UW Semantic Parsing Framework (UW SPF) (Artzi and Zettlemoyer, 2013)
which incorporates the algorithms in (Zettlemoyer
and Collins, 2005; Kwiatkowski et al., 2010) .
However, to use UW SPF for the development of
a new system, the user needs to learn their coding
guidelines and needs to write new code in their
system. NL2KR does not require the users to
write new code and guides the development process with its rich user interface.
We have evaluated NL2KR on two standard
datasets: GeoQuery (Tang and Mooney, 2001) and
Jobs (Tang and Mooney, 2001). GeoQuery is a
database of geographical questions and Jobs contains sentences with job related query. Experiments demonstrate that NL2KR can exhibit stateof-the-art performance with fairly small initial dictionary. The rest of the paper is organized as follows: we first present the algorithms and architecture of the NL2KR platform in section 2; we
discuss about the experiments in section 3; and finally, we conclude in section 4.

it breaks the sentence into two smaller substrings
S1, S2 and uses higher-order unification to compute two λ-terms M 1, M 2 which combines to produce M . It then recursively learns the meanings
of the words, from the sub-instance < S1, M 1 >
and < S2, M 2 >. Since, there are many ways
to split the input sentence S and the choice of
M 1, M 2 can be numerous, it needs to consider all
possible splittings and their combinations; which
produces many spurious meanings. Most importantly, their higher-order unification algorithm imposes various restrictions (such as limited number of conjunctions in a sentence, limited forms of
functional application) on the meaning representation language which severely limits its applicability to new applications. Another common drawback of these two algorithms is that they both suffer when the test sentence contains words that are
not part of the training corpus.
Our platform NL2KR uses a different automated approach based on Inverse-λ (section 2.1)
and Generalization (section 2.2) which does not
impose such restrictions enforced by their higherorder unification algorithm. Also, Generalization algorithm along with Combinatory Categorical Grammar (Steedman, 2000) parser, allows
NL2KR to go beyond the training dictionary and
translate sentences which contain previously unseen words. The main aspect of our approach is as
follows: given a sentence, its semantic representation and an initial dictionary containing the meaning of some words, NL2KR first obtains several
derivation of the input sentence in Combinatory
Categorical Grammar (CCG). Each CCG derivation tree describes the rules of functional application through which constituents combine with
each other. With the user provided initial dictionary, NL2KR then traverses the tree in a bottomup fashion to compute the semantic expressions
of intermediate nodes. It then traverses the augmented tree in a top-down manner to learn the
meaning of missing words using Inverse-λ (section 2.1). If Inverse-λ is not sufficient to learn the
meaning of all unknown words, it employs Generalization (section 2.2) to guess the meanings of
unknown words with the meaning of known similar words. It then restarts the learning process
with the updated knowledge. The learning process stops if it learns the meanings of all words or
fails to learn any new meaning in an iteration. In
the latter case, it shows the augmented tree to the

2

Algorithms and Architecture

The NL2KR architecture (Figure 1) has two subparts which depend on each other (1) NL2KRL for learning and (2) NL2KR-T for translation.
The NL2KR-L sub-part takes the following as input: (1) a set of training sentences and their target formal representations, and (2) an initial lexicon or dictionary consisting of some words, their
CCG categories, and their meanings in terms of λcalculus expressions. It then constructs the CCG
parse trees and uses them for learning of word
meanings.
Learning of word meanings is done by using
Inverse-λ and Generalization (Baral et al., 2012;
Baral et al., 2011) and ambiguity is addressed
by a Parameter Learning module that learns the
weights of the meanings. The learned meanings
update the lexicon. The translation sub-part uses
this updated lexicon to get the meaning of all the
words in a new sentence, and combines them to get
the meaning of the new sentence. Details of each
module will be presented in the following subsections.
900

Figure 1: Architecture of NL2KR

words are considered as similar if they have the
exact same CCG category. As an example, if
we want to generalize the meaning of the word
“plays” with CCG category (S\N P )/N P ) and
the lexicon already contains an entry for “eats”
with the same CCG category, and the meaning λy.λx.eats(x, y), the algorithm will extract the template λy.λx.W ORD(x, y) and apply the template to plays to get the meaning
λy.λx.plays(x, y).

The NL2KR platform provides a GUI (Figure 2)
with six features: λ-application, Inverse-λ, Generalization, CCG-Parser, NL2KR-L and NL2KRT. The fourth feature is a stand-alone CCG parser
and the first four features can help on user with
constructing the initial lexicon. The user can then
use NL2KR-L to update the lexicon using training data and the NL2KR-T button then works as a
translation system.
2.1

Inverse-λ

2.3

Inverse-λ plays a key role in the learning process. Formally, given two λ-expressions H and
G with H = F @G or H = G@F , the
Inverse-λ operation computes the λ expression
F . For example, given the meaning of “is texas”
as λx2.x2@stateid(texas) and the meaning of
“texas” as stateid(texas), with the additional
information that “is” acts as the function while
“texas” is the argument, the Inverse-λ algorithm
computes the meaning of “is” as λx3.λx2.x2@x3
(Figure 4). NL2KR implements the Inverse-λ algorithm specified in (Baral et al., 2012). The
Inverse-λ module is separately accessible through
the main GUI (Figure 2).
2.2

Combinatory Categorial Grammar

Derivation of a sentence in Combinatory Categorial Grammar (CCG) determines the way the constituents combine together to establish the meaning of the whole. CCG is a type of phrase structure grammar and clearly describes the predicateargument structure of constituents.
Figure 3 shows an example output of NL2KR’s
CCG parser. In the figure, “John” and “home”
have the category [N] (means noun) and can
change to [NP] (means noun phrase).
The
phrase“walk home” has the category [S\NP],
which means that it can combine with a constituent with category [NP] (“John” in this case)
from left with the backward application to form
category [S] (sentence). The word “walk” has
the category [(S\NP)/NP], which means it can
combine with a constituent with category [NP]
(“home”) from right through the forward application combinator to form category [S\NP] (of
“walk home”).
A detailed description on CCG goes beyond the
scope of this paper (see (Steedman, 2000) for more
details). Since, natural language sentences can
have various CCG parse trees, each expressing a
different meaning of the sentence, a key challenge

Generalization

Generalization (Baral et al., 2012; Baral et al.,
2011) is used when Inverse-λ is not sufficient to
learn new semantic representation of words. In
contrast to Inverse-λ which learns the exact meaning of a word in a particular context, Generalization learns the meanings of a word from similar words with existing representations. Thus,
Generalization helps NL2KR to learn meanings
of words that are not even present in the training data set. In the current implementation, two
901

Figure 2: NL2KR’s main GUI, Version 1.7.0001

(Curran et al., 2007) and constraints from the Stanford parser (Socher et al., 2013; Toutanova et al.,
2003) to guide the derivation of a sentence. The
output of the CCG parser is a set of k weighted
parse trees, where the parameter k is provided by
the user.
NL2KR system allows one to use the CCG
parser independently through the interactive GUI.
The output graphs look like the one in Figure 3. It
can be zoomed in/out and its nodes can be moved
around, making it easier to work with complex
sentences.
2.4

Multistage learning approach

Learning meanings of words is the major component of our system. The inputs to the learning
module are a list of training sentences, their target
formal representations and an initial lexicon consisting of triplets of the form <word, CCG category, meaning>, where meanings are represented
in terms of λ-calculus expressions. The output
of the algorithm is a final dictionary containing
a set of 4-tuples (word, CCG category, meaning,
weight).

Figure 3: CCG parse tree of ”John walked home”.

in the learning and the translation process is to find
a suitable CCG parse tree for a sentence in natural language. We overcome this impediment by
allowing our learning and translation subsystem
to work with multiple weighted parse trees for a
given sentence and determining on the fly, the one
that is most suitable. We discuss more on this in
sections 2.4-2.6.

Interactive Multistage Learning Algorithm
(IMLA) NL2KR employs an Interactive Multistage Learning Algorithm (Algorithm 1) that runs
many iterations on the input sentences. In each
iteration, it goes through one or more of the following stages:

Existing CCG parsers (Curran et al., 2007; Lierler and Schüller, 2012) either return a single best
parse tree for a given sentence or parse it in all
possible ways with no preferential ordering among
them. In order to overcome this shortcoming and
generate more than one weighted candidate parse
trees, we have developed a new parser using beam
search with Cocke-Younger-Kasami(CYK) algorithm. NL2KRs CCG parser uses the C&C model

Stage 1 In Stage 1, it gets all the unfinished
sentences. It then employs Bottom Up-Top Down
algorithm (Algorithm 2) to learn new meanings
(by Inverse-λ). For a sentence, if it has computed the meanings of all its constituents, which
can be combined to produce the given representation, that sentence is considered as learned. Each
902

Algorithm 1 IMLA algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:

after exiting stage 1, it directly goes to stage 3.

function
IMLA(initLexicon,sentences,
sentsM eanings)
regW ords ← ∅
generalize ← false
lexicon ← initLexicon
repeat
repeat
repeat
for all s ∈ sentences do
newM eanings
←
BT(s,lexicon,sentsM eanings)
lexicon ← lexicon ∪ newM eanings
for all n ∈ newM eanings do
ms ← G ENERALIZE (regW ords, n)
lexicon ← lexicon ∪ ms
end for
end for
until newM eanings = ∅
if generalize=false then
generalize ← true
for all t ∈ unf inishedSents do
words ← G ETA LLW ORDS(t)
ms ← G ENERALIZE(words)
lexicon ← lexicon ∪ ms
regW ords ← regW ords ∪ words
end for
end if
until newM eanings = ∅
I NTERATIVE L EARNING
until unf inishedSents = ∅ OR userBreak
lexicon
←
PARAMETER E STIMA TION (lexicon,sentences)
return lexicon
end function

Stage 3 When both aforementioned stages
can not learn all the sentences, the Interactive
Learning process is invoked and all the unfinished
sentences are shown on the interactive GUI (Figure 4). Users can either skip or provide more information on the GUI and the learning process is
continued.
After finishing all stages, IMLA (Algorithm 1)
calls Parameter Estimation (section 2.5) algorithm
to compute the weight of each lexicon tuple.
Bottom Up-Top Down learning For a given
sentence, the CCG parser is used for the CCG
parse trees like the one of how big is texas in Figure 4. For each parse tree, two main processes
are called, namely “bottom up” and “top down”.
In the first process, all the meanings of the words
in the sentences are retrieved from the lexicon.
These meanings are populated in the leaf nodes
of a parse tree (see Figure 4), which are combined
in a bottom-up manner to compute the meanings
of phrases and full sentences. We call these meanings, the current meanings.
In the “top down” process, using Inverse-λ algorithm, the given meaning of the whole sentence
(called the expected meaning of the sentence) and
the current meanings of the phrases, we calculate the expected meanings of each of the phrases
from the root of the tree to the leaves. For example, given the expected meaning of how big is
texas and the current meaning of how big, we use
Inverse-λ algorithm to get the meaning (expected)
of is texas. This expected meaning is used together
with current meanings of is (texas) to calculate
the expected meanings of texas (is). The expected
meanings of the leaf nodes we have just learned
will be saved to the lexicon and will be used in the
other sentences and in subsequent learning iteration. The “top down” process is stopped when the
expected meanings are same as the current meanings. And in both “bottom up” and “top-down”
processes, the beam search algorithm is used to
speed-up the learning process.

new meaning learned by this process is used to
generalize the words in a waiting list. Initially,
this waiting list is empty and is updated in stage
2. When no more new meaning can be learned
by Bottom Up-Top Down algorithm, IMLA (Algorithm 1) enters stage 2.
Stage 2 In this stage, it takes all the sentences
for which the learning is not yet finished and applies Generalization process on all the words of
those sentences. At the same time, it populates
those words into the waiting list, so that from now
on, Bottom Up-Top Down will try to generalize
new meanings for them when it learns some new
meanings. It then goes back to stage 1. Next time,

Interactive learning In the interactive learning
process it opens a GUI which shows the unfinished
sentences. Users can see the current and expected
meanings for the unfinished sentences. When the
user gives additional meanings of word(s), the λapplication or Inverse-λ operation is automatically
performed to update the new meaning(s) to related
903

Figure 4: Interactive learning GUI. The box under each node show: the corresponding phrases [CCG category], the expected
meanings and the current meanings. Click on the red node will show the window to change the current meaning (CLE)

Algorithm 2 BottomUp-TopDown (BT) algorithm
1: function BT(
sentence, lexicon, sentsM eanings)
2: parseT rees ← C CG PARSER (sentence)
3: for all tree ∈ parseT rees do
4:
t ← B OTTOM U P(tree,lexicon)
5:
T OP D OWN(t,sentsM eanings)
6: end for
7: end function

it knows the expected meaning of “How big is
texas” := S : answer(size(stateid(texas)))
and the current meaning of “how big”. Using
them in the Inverse-λ algorithm, it then compute the meaning of “is texas” := S\N P :
λx1.x1@stateid(texas). Using this expected
meaning and current meaning of “texas” := N P :
stateid(texas), it then calculates the expected
meaning of “is” as “is” := (S\N P )/N P :
λx2.λx1.x1@x2. This newly learned expected
meaning is then saved into the lexicon.

word(s). Once satisfied, the user can switch back
to the automated learning mode.

Since the meaning of all the words in the question are known, the learning algorithm stops here
and the Interactive Learning is never called.

Example Let
us
consider
the
question “How big is texas?”
with meaning
answer(size(stateid(texas))) (see Figure
4).
Let us assume that the initial dictionary has
the following entries: how := N P/(N/N ) :
λx.λy.answer(x@y), big := N/N : λx.size(x)
and texas := N P : stateid(texas). The algorithm
then proceeds as follows.
First, the meanings of “how” and “big” are combined to compute the current meaning of “how
big” := N P : λx.answer(size(x)) in the “bottom up” process. Since the meaning of “is” is unknown, the current meaning of “is texas” still remains unknown.
It then starts the “top down” process where

If initially, the dictionary contains only two
meanings: “big” := N/N : λx.size(x) and
“texas” := N P : stateid(texas), NL2KR tries
to first learn the sentence but fails to learn
the complete sentence and switches to Interactive Learning which shows the interactive
GUI (see Figure 4).
If the user specifies
that “how” means λx.λy.answer(x@y), NL2KR
combines its meaning with the meaning of “big”
to get the meaning “how big” := N P :
λx.answer(size(x)). It will then use Inverseλ to figure out the meaning of “is texas” and
then the meaning of “is”. Now all the meanings are combined to compute the current meaning answer(size(stateid(texas))) of “How big
is texas”. This meaning is same as the expected
904

meaning, so we know that the sentence is successfully learned. Now, the user can press Retry
Learning to switch back to automated learning.
2.5

We report the performance in terms of precision
(percentage of returned logical-forms that are correct), recall (percentage of sentences for which the
correct logical-form was returned), F1-measure
(harmonic mean of precision and recall) and the
size of the initial dictionary.
We compare the performance of our system with recently published, directly-comparable
works, namely, FUBL (Kwiatkowski et al.,
2011), UBL (Kwiatkowski et al., 2010), λ-WASP
(Wong and Mooney, 2007), ZC07 (Zettlemoyer
and Collins, 2007) and ZC05 (Zettlemoyer and
Collins, 2005) systems.

Parameter Estimation

The Parameter Estimation module estimates a
weight for each word-meaning pair such that the
joint probability of the training sentences getting
translated to their given representation is maximized. It implements the algorithm described in
Zettlemoyer et. al.(2005).
2.6

Translation

3.1

The goal of this module is to convert input sentences into the target formalism using the lexicon previously learned. The algorithm used in
Translation module (Algorithm 3) is similar to the
bottom-up process in the learning algorithm. It
first obtains several weighted CCG parse trees of
the input sentence. It then computes a formal representation for each of the parse trees using the
learned dictionary. Finally, it ranks the translations according to the weights of word-meaning
pairs and the weights of the CCG parse trees.
However, test sentences may contain words which
were not present in the training set. In such cases,
Generalization is used to guess the meanings of
those unknown words from the meanings of the
similar words present in the dictionary.

GeoQuery GeoQuery (Tang and Mooney, 2001)
is a corpus containing questions on geographical
facts about the United States. It contains a total of
880 sentences written in natural language, paired
with their meanings in a formal query language,
which can be executed against a database of the
geographical information of the United States.
We follow the standard training/testing split of
600/280. An example sentence meaning pair is
shown below.
Sentence: How long is the Colorado river?
Meaning: answer(A,(len(B,A),const(B,
riverid(colorado)), river(B)))
Jobs The Jobs (Tang and Mooney, 2001) dataset
contains a total of 640 job related queries written
in natural language. The Prolog programming
language has been used to represent the meaning
of a query. Each query specifies a list of job
criteria and can be directly executed against a
database of job listings. An example sentence
meaning pair from the corpus is shown below.

Algorithm 3 Translation algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

3

Corpora

function TRANSLATE(sentence, lexicon)
candidates ← ∅
parseT rees ← C CG PARSER(sentence)
for all tree ∈ parseT rees do
G ENERALIZE(tree);
t ← B OTTOM U P(tree)
candidates ← candidates ∪ t
end for
output ← V ERIFY-R ANK(candidates)
return output
end function

Question: What jobs are there for programmers that know assembly?
Meaning: answer(J,(job(J),title(J,T),
const(T,’Programmer’),language(J,L),
const(L,’assembly’))))
The dataset contains a training split of 500 sentences and a test split of 140 sentences.

Experimental Evaluation

3.2

We have evaluated NL2KR on two standard corpora: GeoQuery and Jobs. For both the corpus, the
output generated by the learned system has been
considered correct if it is an exact replica of the
logical formula described in the corpus.

Initial Dictionary Formulation

GeoQuery For GeoQuery corpus, we manually
selected a set of 100 structurally different sentences from the training set and initiated the learning process with a dictionary containing the repre905

] <word, category >
] <word, category, meaning>
] meaning

GUI Driven
31
36
30

Initial Dictionary
118
127
89

Learned Dictionary
401
1572
819

Table 1: Comparison of Initial and Learned dictionary for GeoQuery corpus on the basis of the number of entries in the
dictionary, number of unique <word, CCG category> pairs and the number of unique meanings across all the entries. “GUI
Driven” denotes the amount of the total meanings given through interactive GUI and is a subset of the Initial dictionary.

] <word, category>
] <word, category, meaning>
] meaning

GUI Driven
58
74
57

Initial Dictionary
103
119
71

Learned Dictionary
226
1793
940

Table 2: Comparison of Initial and Learned dictionary for Jobs corpus.

Jobs For the Jobs dataset, we followed a similar
process as in the GeoQuery dataset. A set of 120
structurally different sentences were selected and a
dictionary was created which contained the representation of the nouns and the question words from
the training corpus. A total of 127 word meanings
were provided in the process. Table 2 compares
the initial and learned dictionary for Jobs. Again,
we can see that the amount of initial effort is substantially less in comparison to the return.

sentation of the nouns and question words. These
meanings were easy to obtain as they follow simple patterns. We then trained the translation system on those selected sentences. The output of
this process was used as the initial dictionary for
training step. Further meanings were provided on
demand through interactive learning. A total of
119 word meanings tuples (Table 1, ] <word, category, meaning >) were provided from which the
NL2KR system learned 1793 tuples. 45 of the 119
were representation of nouns and question words
that were obtained using simple patterns. The remaining 74 were obtained by a human using the
NL2KR GUI. These numbers illustrate the usefulness of the NL2KR GUI as well as the NL2KR
learning component. One of our future goals is to
further automate the process and reduce the GUI
interaction part.
Table 1 compares the initial and learned dictionary for GeoQuery on the basis of number
of unique <word, category, meaning> entries in
dictionary, number of unique <word, category>
pairs and the number of unique meanings across
all the entries in the dictionary. Since each unique
<word, CCG category> pair must have at least
one meaning, the total number of unique <word,
category> pairs in the training corpus provides a
lower bound on the size of the ideal output dictionary. However, one <word, category> pair may
have multiple meanings, so the ideal dictionary
can be much bigger than the number of unique
<word, category> pairs. Indeed, there were many
words such as “of”, “in” that had multiple meanings for the same CCG category. Table 1 clearly
describes that the amount of initial effort is substantially less compared to the return.

3.3

Precision, Recall and F1-measure

Figure 5: Comparison of Precision, Recall and F1-measure
on GeoQuery and Jobs dataset.

Table 3, Table 4 and Figure 5 present the comparison of the performance of NL2KR on the GeoQuery and Jobs domain with other recent works.
NL2KR obtained 91.1% precision value, 92.1%
906

System
ZC05
ZC07
λ-WASP
UBL
FUBL
NL2KR

Precision
0.963
0.916
0.9195
0.885
0.886
0.911

Recall
0.793
0.861
0.8659
0.879
0.886
0.921

F1
0.87
0.888
0.8919
0.882
0.886
0.916

System
ZC05
COCKTAIL
NL2KR

Precision
0.9736
0.9325
0.9543

Recall
0.7929
0.7984
0.9403

F1
0.8740
0.8603
0.9472

Table 4: Comparison of Precision, Recall and F1-measure on
Jobs dataset.

Table 3: Comparison of Precision, Recall and F1-measure on
GeoQuery dataset.

sentence could have had in the target formal language. Among the sentences for which NL2KR
returned a translation, there were very few instances where it did not discover the correct meaning in the set of possible meanings.
It may be noted that even though our precision is lower than ZC05 and very close to ZC07
and WASP; we have achieved significantly higher
F1 measure than all the related systems. In
fact, ZC05, which achieves the best precision for
both the datasets, is better by a margin of only
0.019 on the Jobs dataset and 0.052 on the GeoQuery dataset. We think one of the main reasons is that it uses manually predefined lambdatemplates, which we try to automate as much as
possible.

recall value and a F1-measure of 91.6% on GeoQuery (Figure 5, Geo880) dataset. For Jobs corpus, the precision, recall and F1-measure were
95.43%, 94.03% and 94.72% respectively. In
all cases, NL2KR achieved state-of-the-art recall
and F1 measures and it significantly outperformed
FUBL (the latest work on translation systems) on
GeoQuery.
For both GeoQuery and Jobs corpus, our recall
is significantly higher than existing systems because meanings discovered by NL2KRs learning
algorithm is more general and reusable. In other
words, meanings learned from a particular sentence are highly likely to be applied again in the
context of other sentences. It may be noted that,
larger lexicons do not necessarily imply higher recall as lambda expressions for two phrases may
not be suitable for functional application, thus
failing to generate any translation for the whole.
Moreover, the use of a CCG parser maximizes the
recall by exhibiting consistency and providing a
set of weighted parse trees. By consistency, we
mean that the order of the weighted parse tree remains same over multiple parses of the same sentence and the sentences having similar syntactic
structures have identical ordering of the derivations, thus making Generalization to be more effective in the process of translation.
The sentences for which NL2KR did not have
a translation are the ones having structural difference with the sentences present in the training dataset. More precisely, their structure was
not identical with any of the sentences present in
the training dataset or could not be constructed by
combining the structures observed in the training
sentences.
We analyzed the sentences for which the translated meaning did not match the correct one and
observed that the translation algorithm selected
the wrong meaning, even though it discovered the
correct one as one of the possible meanings the

4

Conclusion

NL2KR is a freely available2 , user friendly, rich
graphical platform for building translation systems
to convert sentences from natural language to their
equivalent formal representations in a wide variety of domains. We have described the system algorithms and architecture and its performance on
the GeoQuery and Jobs datasets. As mentioned
earlier, the NL2KR GUI and the NL2KR learning
module help in starting from a small initial lexicon (for example, 119 in Table 2) and learning
a much larger lexicon (1793 in Table 2). One of
our future goals is to reduce the initial lexicon to
be even smaller by further automating the NL2KR
GUI interaction component .

Acknowledgements
We thank NSF for the DataNet Federation Consortium grant OCI-0940841 and ONR for their grant
N00014-13-1-0334 for partially supporting this research.

2
More examples and a tutorial to use NL2KR are available
in the download package.

907

References

Lappoon R Tang and Raymond J Mooney. 2001. Using multiple clause constructors in inductive logic
programming for semantic parsing. In Machine
Learning: ECML 2001, pages 466–477. Springer.

Yoav Artzi and Luke Zettlemoyer. 2013. UW SPF:
The University of Washington Semantic Parsing
Framework. arXiv preprint arXiv:1311.3011.
Chitta Baral, Juraj Dzifcak, Marcos Alvarez Gonzalez,
and Jiayu Zhou. 2011. Using inverse λ and generalization to translate english to formal languages. In
Proceedings of the Ninth International Conference
on Computational Semantics, pages 35–44. Association for Computational Linguistics.

Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computational Linguistics on Human Language Technology
- Volume 1.

Chitta Baral, Juraj Dzifcak, Marcos Alvarez Gonzalez,
and Aaron Gottesman. 2012. Typed answer set programming lambda calculus theories and correctness
of inverse lambda algorithms with respect to them.
TPLP, 12(4-5):775–791.

Yuk Wah Wong and Raymond J Mooney. 2007.
Learning synchronous grammars for semantic parsing with lambda calculus. In Annual MeetingAssociation for computational Linguistics, volume 45, page 960. Citeseer.

Alonzo Church. 1936. An Unsolvable Problem of
Elementary Number Theory. American Journal of
Mathematics, 58(2):345–363, April.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial
Grammars. In UAI, pages 658–666. AUAI Press.

James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically Motivated Large-Scale NLP with
C&C and Boxer. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 33–36, Prague,
Czech Republic, June. Association for Computational Linguistics.

Luke S Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing
to logical form. In In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-2007).

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higherorder unification. In Proceedings of the 2010 conference on empirical methods in natural language
processing, pages 1223–1233. Association for Computational Linguistics.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in ccg grammar induction for semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
1512–1523. Association for Computational Linguistics.
Yuliya Lierler and Peter Schüller. 2012. Parsing combinatory categorial grammar via planning in answer
set programming. In Correct Reasoning, pages 436–
453. Springer.
Richard Montague. 1974. English as a Formal Language. In Richmond H. Thomason, editor, Formal
Philosophy: Selected Papers of Richard Montague,
pages 188–222. Yale University Press, New Haven,
London.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with Compositional Vector Grammars. In ACL (1), pages 455–
465.
Mark Steedman. 2000. The syntactic process, volume 35. MIT Press.

908

Learning To Use Formulas To Solve Simple Arithmetic Problems
Arindam Mitra
Arizona State University
amitra7@asu.edu

Chitta Baral
Arizona State University
chitta@asu.edu

Abstract

Story Comprehension Challenge (Richardson et
al., 2013), Facebook bAbl task (Weston et al.,
2015), Semantic Textual Similarity (Agirre et al.,
2012) and Textual Entailment (Bowman et al.,
2015; Dagan et al., 2010). The study of word math
problems is also an important problem as quantitative reasoning is inextricably related to human life.
Clark & Etzioni (Clark, 2015; Clark and Etzioni,
2016) discuss various properties of math word
(and science) problems emphasizing elementary
school science and math tests as a driver for AI.
Researchers at Allen AI Institute have published
two standard datasets as part of the Project Euclid1
for future endeavors in this regard. One of them
contains simple addition-subtraction arithmetic
problems (Hosseini et al., 2014) and the other
contains general arithmetic problems (KoncelKedziorski et al., 2015). In this research, we focus
on the former one, namely the AddSub dataset.

Solving simple arithmetic word problems
is one of the challenges in Natural Language Understanding. This paper presents
a novel method to learn to use formulas
to solve simple arithmetic word problems.
Our system, analyzes each of the sentences to identify the variables and their
attributes; and automatically maps this information into a higher level representation. It then uses that representation to
recognize the presence of a formula along
with its associated variables. An equation is then generated from the formal description of the formula. In the training
phase, it learns to score the <formula,
variables> pair from the systematically
generated higher level representation. It is
able to solve 86.07% of the problems in
a corpus of standard primary school test
questions and beats the state-of-the-art by
a margin of 8.07%.

1

Dan grew 42 turnips and 38 cantelopes . Jessica grew 47 turnips . How many turnips did
they grow in total ?
Formula
Associated variables
part-whole
whole: x, parts: {42, 47}
Equation
x = 42 + 47

Introduction

Developing algorithms to solve math word problems (Table 1) has been an interest of NLP researchers for a long time (Feigenbaum and Feldman, 1963). It is an interesting topic of study from
the point of view of natural language understanding and reasoning for several reasons. First, it incorporates rigorous standards of accurate comprehension. Second, we know of a good representation to solve the word problems, namely algebraic
equations. Finally, the evaluation is straightforward and the problems can be collected easily.
In the recent years several challenges have
been proposed for natural language understanding.
This includes the Winograd Schema challenge
for commonsense reasoning (Levesque, 2011),

Table 1: Solving a word problem using part-whole
Broadly speaking, common to the existing approaches (Kushman et al., 2014; Hosseini et al.,
2014; Zhou et al., 2015; Shi et al., 2015; Roy and
Roth, 2015) is the task of grounding, that takes as
input a word problem in the natural language and
represents it in a formal language, such as, a system of equations, expression trees or states (Hosseini et al., 2014), from which the answer can be
easily computed. In this work, we divide this task
of grounding into two parts as follows:
1

http://allenai.org/euclid.html

2144
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2144–2153,
c
Berlin, Germany, August 7-12, 2016. 
2016
Association for Computational Linguistics

In the first step, the system learns to connect the
assertions in a word problem to abstract mathematical concepts or formulas. In the second step,
it maps that formula into an algebraic equation.
Examples of such formulas in the arithmetic domain includes part whole which says, ‘the whole
is equal to the sum of its parts’, or the Unitary
Method that is used to solve problems like ‘A man
walks seven miles in two hours. What is his average speed?’.

Change
R ESULT U NKNOWN
Mary had 18 baseball cards , and 8 were torn .
Fred gave Mary 26 new baseball cards . Mary
bought 40 baseball cards . How many baseball
cards does Mary have now ?
C HANGE U NKNOWN
There were 28 bales of hay in the barn . Tim
stacked bales in the barn today . There are now
54 bales of hay in the barn . How many bales
did he store in the barn ?
S TART U NKNOWN
Sam ’s dog had puppies and 8 had spots . He
gave 2 to his friends . He now has 6 puppies .
How many puppies did he have to start with?
Part Whole
T OTAL S ET U NKNOWN
Tom went to 4 hockey games this year , but
missed 7 . He went to 9 games last year . How
many hockey games did Tom go to in all ?
PART U NKNOWN
Sara ’s high school played 12 basketball games
this year . The team won most of their games
. They were defeated during 4 games . How
many games did they win ?
Comparision
D IFFERENCE U NKNOWN
Last year , egg producers in Douglas County
produced 1416 eggs . This year , those same
farms produced 4636 eggs . How many more
eggs did the farms produce this year ?
L ARGE Q UANTITY U NKNOWN
Bill has 9 marbles. Jim has 7 more marbles than
Bill. How many marbles does Jim have?
S MALL Q UANTITY U NKNOWN
Bill has 9 marbles. He has 7 more marbles than
Jim. How many marbles does Jim have?

Consider the problem in Table 1. If the system
can determine it is a ‘part whole’ problem where
the unknown quantity X plays the role of whole
and its parts are 42 and 47, it can easily express
the relation as X = 42 + 47. The translation of
a formula to an equation requires only the knowledge of the formula and can be formally encoded.
Thus, we are interested in the question, ‘how can
an agent learn to apply the formulas for the word
problems?’ Solving a word problem in general,
requires several such applications in series or parallel, generating multiple equations. However, in
this research, we restrict the problems to be of a
single equation which requires only one application.
Our system currently considers three mathematical concepts: 1) the concept of part whole, 2) the
concept of change and 3) the concept of comparison. These concepts are sufficient to solve the
arithmetic word problems in AddSub. Table 2 illustrates each of these three concepts with examples. The part whole problems deal with the part
whole relationships and ask for either the part or
the whole. The change problems make use of the
relationship between the new value of a quantity
and its original value after the occurrence of a series of increase or decrease. The question then
asks for either the initial value of the quantity or
the final value of the quantity or the change. In
case of comparison problems, the equation can be
visualized as a comparison between two quantities and the question typically looks for either the
larger quantity or the smaller quantity or the difference. While the equations are simple, the problems describe a wide variety of scenarios and the
system needs to make sense of multiple sentences
without a priori restrictions on the syntax or the
vocabulary to solve the problem.
Training has been done in a supervised fashion. For each example problem, we specify the
formula that should be applied to generate the ap-

Table 2: Examples of Add-Sub Word Problems
propriate equation and the relevant variables. The
system then learns to apply the formulas for new
problems. It achieves an accuracy of 86.07% on
the AddSub corpus containing 395 word arithmetic
problems with a margin of 8.07% with the current
state-of-the-art (Roy and Roth, 2015).
Our contributions are three-fold: (a) We model
the application of a formula and present a novel
method to learn to apply a formula; (b) We annotate the publicly available AddSub corpus with the

2145

correct formula and its associated variables; and
(c) We make the code publicly available. 2
The rest of the paper is organized as follows. In
section 2, we formally define the problem and describe our learning algorithm. In section 3, we define our feature function. In section 4, we discuss
related works. Section 5 provides a detailed description of the experimental evaluation. Finally,
we conclude the paper in section 6.

2

Problem Formulation

A single equation word arithmetic problem P is
a sequence of k words hw1 , ..., wk i and contains
a set of variables VP = {v0 , v1 , ..., vn−1 , x}
where v0 , v1 , ..., vn−1 are numbers in P and x is
the unknown whose value is the answer we seek
(Koncel-Kedziorski et al., 2015). Let Paddsub be
the set of all such problems, where each problem P ∈ Paddsub can be solved by a evaluating
a valid mathematical equation E formed by combining the elements of VP and the binary operators
from O = {+, −}.
We assume that each target equation E of
P ∈ Paddsub is generated by applying one
of the possible mathematical formulas from
C = {Cpartwhole , Cchange , Ccomparision }. Let
1
⊆ Paddsub be the set of all problems
Paddsub
where the target equation E can be generated by a
single application of one of the possible formulas
from C. The goal is then to find the correct appli1
.
cation of a formula for the problem P ∈ Paddsub
2.1

Modelling Formulas And their
Applications

We model each formula as a template that has predefined slots and can be mapped to an equation
when the slots are filled with variables. Application of a formula C ∈ C to the problem P , is then
defined as the instantiation of the template by a
subset of VP that contains the unknown.
Part Whole The concept of part whole has
two slots, one for the whole that accepts a single
variable and the other for its parts that accepts a
set of variables of size at least two. If the value
of the whole is w and the value of the parts are
p1 , p2 , ..., pm , then that application is mapped to
the equation, w = p1 + p2 + ... + pm , denoting
that whole is equal to the sum of its parts.
2
The code and data is publicly
https://github.com/ari9dam/MathStudent.

available

Change The change concept has four slots,
namely start, end, gains, losses which respectively
denote the original value of a variable, the final
value of that variable, and the set of increments
and decrements that happen to the original value
of the variable. The start slot can be empty; in
that case it is assumed to be 0. For example, consider the problem, ‘Joan found 70 seashells on the
beach . she gave Sam some of her seashells. She
has 27 seashell . How many seashells did she give
to Sam?’. In this case, our assumption is that before finding the 70 seashells Joan had an empty
hand. Given an instantiation of change concept
the equation is generated as follows:
valstart +

X
g∈gains

valg =

X

vall + valend

l∈losses

Comparision The comparision concept has
three slots namely the large quantity, the small
quantity and their difference. An instantiation of
the comparision concept is mapped to the following equation: large = small + dif f erence.
2.2

The Space of Possible Applications

Consider the problem in Table 1. Even though the
correct application is an instance of part whole
formula with whole = x and the parts being
{42, 47}, there are many other possible applications, such as, partWhole(whole=47, parts=x,42),
change(start=47, losses={x}, gains={}, end
= 42), comparison(large=47, small=x, difference=42).
Note that, comparison(large=47,
small=38, difference=42) is not a valid application since none of the associated variables is an
unknown. Let AP be the set of all possible applications to the problem P . The following lemma
characterizes the size of AP as a function of the
number of variables in P .
1
Lemma 2.2.1. Let P ∈ Paddsub
be an arithmetic
word problem with n variables (|VP | = n), then
the following are true:

at

2146

1. The number of possible applications of part
whole formula to the problem P , Npartwhole
is (n + 1)2n−2 + 1.
2. The number of possible applications of
change formula to the problem P , Nchange
is 3n−3 (2n2 + 6n + 1) − 2n + 1.
3. The number of possible applications of
comparison formula to the problem P ,
Ncomparison is 3(n − 1)(n − 2).

4. The number of all possible applications to
the problem P is Npartwhole + Nchange +
Ncomparison .
Proof of lemma 2.2.1 is provided in the Appendix. The total number of applications for problems having 3, 6, 7, 8 number of variables are 47,
3, 105, 11, 755, 43, 699 respectively. AdditionSubtraction arithmetic problems hardly contain
more than 6 variables. So, the number of possible applications is not intractable in practice.
The total number of applications increases
rapidly mainly due to the change concept. Since,
the template involves two sets, there is a 3n−3 factor present in the formula of Nchange . However,
any application of change concept with gains and
losses slots containing a collection of variables can
be broken down into multiple instances of change
concept where the gains and losses slots accepts
only a single variable by introducing more intermediate unknown variables. Since, for any formula that does not have a slot that accepts a set,
the number of applications is polynomial in the
number of variables, there is a possibility to reduce the application space. We plan to explore
this possibility in our future work. For the part
whole concept, even though there is a exponential term involved, it is practically tractable (for
n = 10, Npartwhole = 2, 817 ). In practice, we
believe that there will hardly be any part whole
application involving more than 10 variables. For
formulas that are used for other categories of word
math problems (algebraic or arithmetic), such as
the unitary method, formulas for ratio, percentage,
time-distance and rate of interest, none of them
have any slot that accepts sets of variables. Thus,
further increase in the space of possible applications will be polynomial.
2.3

1
{(P, y) : P ∈ Paddsub
∧ y ∈ AP }, to accommodate the dependency of the possible applications
on the problem instance. Given the definition of
the feature function φ and the parameter vector θ,
the probability of an application y given a problem
P is defined as,

eθ.φ(P,y)
θ.φ(P,y 0 )
y 0 ∈AP e

p(y|P ; θ) = P

Here, . denotes dot product. Section 3 defines
the feature function. Assuming that the parameter θ is known, the function f that computes the
correct application is defined as,
f (P ) = arg max p(y|P ; θ)
y∈AP

2.4

To learn the function f , we need to estimate the
parameter vector θ. For that, we assume access to
n training examples, {Pi , yi∗ : i = 1 . . . n}, each
containing a word problem Pi and the correct application yi∗ for the problem Pi . We estimate θ
by minimizing the negative of the conditional loglikelihood of the data:
O(θ) = −

n
X

log p(yi∗ |Pi ; θ)

i=1

=−

n
X

[θ.φ(Pi , yi∗ ) − log

i=1

X

eθ.φ(Pi ,y) ]

y∈APi

We use stochastic gradient descent to optimize
the parameters. The gradient of the objective function is given by:
n

X
∇O
=−
[φ(Pi , yi∗ ) −
∇θ
i=1
(1)
X
p(y|Pi ; θ) × φ(Pi , y)]

Probabilistic Model

For each problem P there are different possible
applications y ∈ AP , however not all of them are
meaningful. To capture the semantics of the word
problem to discriminate between competing applications we use the log-linear model, which has a
feature function φ and parameter vector θ ∈ Rd .
The feature function φ : H → Rd takes as input a problem P and a possible application y and
maps it to a d-dimensional real vector (feature
vector) that aims to capture the important information required to discriminate between competing applications. Here, the set H is defined as

Parameter Estimation

y∈APi

Note that, even though the space of possible applications vary with the problem Pi , the gradient
for the example containing the problem Pi can be
easily computed.

3

Feature Function φ

A formula captures the relationship between variables in a compact way which is sufficient to generate an appropriate equation. In a word problem, those relations are hidden in the assertions

2147

of the story. The goal of the feature function is
thus to gather enough information from the story
so that underlying mathematical relation between
the variables can be discovered. The feature function thus needs to be aware of the mathematical relations so that it knows what information it
needs to find. It should also be “familiar” with
the word problem language so that it can extract
the information from the text. In this research,
the feature function has access to machine readable dictionaries such as WordNet (Miller, 1995),
ConceptNet (Liu and Singh, 2004) which captures
inter word relationships such as hypernymy, synonymy, antonymy etc, and syntactic and dependency parsers that help to extract the subject, verb,
object, preposition and temporal information from
the sentences in the text. Given these resources,
the feature function first computes a list of attributes for each variable. Then, for each application y it uses that information, to compute if some
aspects of the expected relationship described in y
is satisfied by the variables in y.
Let the first b dimensions of the feature vector
contain part whole related features, the next c dimensions are for change related features and the
remaining d features are for comparison concept.
Then the feature vector for a problem P and an
application of a formula y is computed in the following way:
Data: A word problem P , an application y
Result: d-dimensional feature vector, f v
Initialize f v := 0
if y is instance of part whole then
compute f v[1 : b]
end
if y is instance of change then
compute f v[b + 1 : b + c]
end
if y is instance of comparision then
compute f v[b + c + 1 : b + c + d]
end
Algorithm 1: Skeleton of the feature function φ

3.1

For each occurrence of a number in the text a variable is created with the attribute value referring
to that numeric value. An unknown variable is
created corresponding to the question. A special
attribute type denotes the kind of object the variable refers to. Table 3 shows several examples
of the type attribute. It plays an important role
in identifying irrelevant numbers while answering
the question.
Text
John had 70 seashells
70 seashells and 8 were broken
61 male and 78 female salmon
35 pears and 27 apples

Type
seashells
seashells
male, salmon
pear

Table 3: Example of type for highlighted variables.
The other attributes of a variable captures its
linguistic context to surrogate the meaning of the
variable. This includes the verb attribute i.e.
the verb attached to the variable, and attributes
corresponding to Stanford dependency relations
(De Marneffe and Manning, 2008), such as nsubj,
tmod, prep in, that spans from either the words in
associated verb or words in the type. These attributes were computed using Stanford Core NLP
(Manning et al., 2014). For the sentence, “John
found 70 seashells on the beach.” the attributes of
the variable are the following: { value : {70},
verb : {found} , nsubj : {John}, prep on :
{beach }}.
3.2

The rest of the section is organized as follows.
We first describe the attributes of the variables that
are computed from the text. Then, we define a list
of boolean variables which computes semantic relations between the attributes of each pair of variables. Finally, we present the complete definition
of the feature function using the description of the
attributes and the boolean variables.

Attributes of Variables

Cross Attribute Relations

Once the variables are created and their attributes
are extracted, our system computes a set of
boolean variables, each denoting whether the attribute a1 of the variable v1 has the same value
as the attribute a2 of the variable v2 . The value
of each attribute is a set of words, consequently
set equality is used to calculate attribute equality.
Two words are considered equal if their lemma
matches.
Four more boolean variables are computed for
each pair of variables based on the attribute type
and they are defined as follows:
subType: Variable v1 is a subT ype of variable v2 if v2 .type ⊂ v1 .type or their type consists
of a single word and there exists the IsA relation
between them in ConceptNet (Speer and Havasi,
2013; Liu and Singh, 2004).

2148

T
disjointType is true if v1 .type v2 .type = φ
intersectingType is true if v1 is neither a
subT ype of v2 nor is disjointT ype nor equal.
We further compute some more variables by utilizing several relations that exist between words:
antonym: For every pair of variables v1 and
v2 , we compute an antonym variable that
S is true if
there exists
a
pair
of
word
in
(v
.verb
v1.adj)×
1
S
(v2 .verb v2.adj) that are antonym to each other
in WordNet irrespective of their part of speech tag.
relatedVerbs: The verbs of two variables are
related if there exists a RelatedTo relations in ConceptNet between them.
subjConsume: The nsubj of v1 consumes the
nsubj of v2 if the formers refers to a group and the
latter is a part of that group. For example, in the
problem, ‘Joan grew 29 carrots and 14 watermelons . Jessica grew 11 carrots . How many carrots
did they grow in all ?’, the nsubj of the unknown
variable consumes others. This is computed using
Stanford co-reference resolution. For the situation
where there is a variable with nsubj as ‘they’ and
it does not refer to any entity, the subjConsume
variable is assumed to be implicitly true for any
variable having a nsubj of type person.
3.3

Features: Part Whole

The part whole features look for some combinations of the boolean variables and the presence
of some cue words (e.g. ‘all’) in the attribute
list. These features capture the underlying reasonings that can affect the decision of applying a part
whole concept. We describe the conditions which
when satisfied activate the features. If active, the
value of a feature is the number of variables associated with the application y and 0 otherwise. This
is also true for change and comparision features
also. Part whole features are computed only when
the y is an instance of the formula part whole. The
same applies for change and comparision features.
Generic Word Cue This feature is activated
if y.whole has a word in its attributes that belongs
to the “total words set” containing the followings
words “all”, “total”, “overall”, “altogether”, “together” and “combine”; and none of the variables
in parts are marked with these words.
ISA Type Cue is active if all the part variables
are subType of the whole.

Type-Verb Cue is active if the type and verb
attributes of vwhole matches that of all the variables
in the part slot of y.
Type-Individual Group Cue is active if the
variable vwhole subjConsume each part variable vp
in y and their type matches.
Type-Verb-Tmod Cue is active if the variable in the slot whole is the unknown and for each
part variable vp their verb, type and tmod (time
modifier of the verb) attributes match.
Type-SubType-Verb Cue is active if the variable in the slot whole is either the unknown or
marked with a word in “total words set” and for
all parts vp , their verb matches and one of the type
or subType boolean variable is true.
Type-SubType-Related Verb Cue is similar
to Type-SubType-Verb Cue however relaxes the
verb match conditions to related verb match. This
is helpful in problems like ‘Mary went to the mall.
She spent $ 13.04 on a shirt and $ 12.27 on a
jacket . She went to 2 shops . In total , how much
money did Mary spend on clothing ? ’.
Type-Loose Verb Cue ConceptNet does not
contain all relations between verbs. For example,
according to ConceptNet ‘buy’ and ‘spend’ are related however there is no relation in ConceptNet
between ‘purchase’ and ‘spend’. To handle these
situations, we use this feature which is similar to
the previous one. The difference is that it assumes
that the verbs of part-whole variable pairs are related if all verbs associated with the parts are same,
even though there is no relation in ConceptNet.
Type-Verb-Prep Cue is active if type and
verb matches. The whole does not have a “preposition” but parts have and they are different.
Other Cues There are also features that add
nsubj match criteria to the above ones. The prior
feature for part whole is that the whole if not unknown, is smaller than the sum of the parts. There
is one more feature that is active if the two part
variables are antonym to each other; one of type
or subType should be true.
3.4

Features: Change

The change features are computed from a set of 10
simple indicator variables, which are computed in
the following way:

2149

Start Cue is active if the verb associated with
the variable in start slot has one of the following
possessive verbs : {‘call for’, ‘be’, ‘contain’, ‘remain’, ‘want’, ‘has’, ‘have’, ‘hold’, ...}; the type
and nsubj of start variable match with the end variable and the tense of the end does not precede the
start. The list of ‘possessive verbs’ is automatically constructed by adding all the verbs associated with the start and the end slot variables in
annotated corpus.

bad = badgains ∨ badlosses . Then the change features are computed from these boolean indicators
using logical operators and, or, not. Table4 shows
some of the change features.
!bad ∧ gaincue ∧ startdef ault ∧ endcue
!bad∧!gaincue ∧losscue ∧startdef ault ∧endcue
!bad
∧
(gaincue
∨
losscue )
∧
startcue ∧!startdef ault ∧ endcue
!bad
∧
(gaincue
∨
losscue )
∧
startexplicit ∧!startdef ault ∧ endcue
!bad ∧ (gaincue ∨ losscue ) ∧ startprior ∧
(endcue ||endprior )
!bad ∧ (gaincue ∨ losscue ) ∧ (startprior ∨
startcue )∧!startdef ault ∧ endprior

Start Explicit Cue is active if one of following words, “started with”, “initially”, “begining”,
“originally” appear in the context of the start variable and the type of start and end variables match.
Start prior is active if the verb associated
with the variable in start slot is a member of the
set ‘possessive verbs’ and the variable appears in
first sentence.

Table 4: Activation criteria of some change related
features.

Start Default Cue is active if the start variable has a “possessive verb” with past tense.

The features for the “compare” concept are relatively straight forward.

End Cue is active if the verb associated with
the variable in slot end has a possessive verb with
the tense of the verb not preceding the tense of
the start, in case the start is not missing. The type
and nsubj should match with either the start or the
gains in case the start is missing.

Difference Unknown Que If the application
y states that the unknown quantity is the difference between the larger and smaller quantity, it is
natural to see if the variable in the difference slot is
marked with a comparative adjective or comparative adverb. The prior is that the value of the larger
quantity must be bigger than the small one. Another two features add the type and subject matching criteria along with the previous ones.

End Prior is true if vend has a possessive verb
and an unknown quantity and at least one of vend
or vstart does not have a nsubj attribute.
Gain Cue is active if for all variables in the
gains slot, the type matches with either vend or
vstart and one of the following is true: 1) the nsubj
of the variable matches with vend or vstart and the
verb implies gain (such as ‘find’) and 2) the nsubj
of the variable does not match with vend or vstart
and the verb implies losing (e.g. spend). The set
of gain and loss verbs are collected from the annotated corpus by following the above procedure.
Gain Prior is true if the problem contains
only three variables, with vstart < vend and the
only variable in the gain slot, associated with nonpossessive verb is the unknown.
Loss Cue & Loss prior are designed in a
fashion similar to the Gain cue and Gain Prior.
Let us say badgains denotes that none of the gain
prior or gain cue is active even though the gain slot
is not empty. badlosses is defined similarly and let

3.5

Features: Comparison

Large & Small Unknown Que These features can be active only when the variable in the
large or small slot is unknown. To detect if the referent is bigger or smaller, it is important to know
the meaning of the comparative words such as
‘less’ and ‘longer’. Since, the corpus contains only
33 comparison problems we collect these comparative words from web which are then divided into
two categories. With these categories, the features
are designed in a fashion similar to change features that looks for type, subject matches.
3.6

Handling Arbitrary Number of Variables

This approach can handle arbitrary number of
variables. To see that consider the problem, ‘Sally
found 9 seashells , Tom found 7 seashells , and
Jessica found 5 seashells on the beach . How
many seashells did they find together ?’. Let us
say that feature vector contains only the ‘TypeIndividual Group Cue’ feature and the weight

2150

of that feature is 1. Consider the two following applications: y1 = partWhole(x,{9,7}) and
y2 = partWhole(x,{9,7, 5}). For both y1 and y2
the ‘Type-Individual Group Cue’ feature is active
since the subject of the unknown x refers to a
group that contains the subject of all part variables
in y1 and y2 and their types match. However, as
mentioned in section 3.3, when active, the value
of a feature is the number of variables associated
4
p(y2 ;P,θ)
with the application. Thus p(y
= ee3 = e.
1 ;P,θ)
Thus, y2 is more probable than y1 .

the property that they give some explanation behind the equation they create. However, the verb
categorization approach of A RIS can only solve a
subset of addition-subtraction problems (see error
analysis in (Hosseini et al., 2014)); whereas the usage of formulas to model the word problem world,
gives our system the ability to accommodate other
math word problems as well.

4

The AddSub dataset consist of a total of 395
addition-subtraction arithmetic problems for third,
fourth, and fifth graders. The dataset is divided
into three diverse set MA1, MA2, IXL containing
134, 140 and 121 problems respectively. As mentioned in (Hosseini et al., 2014), the problems in
MA2 have more irrelevant information compared
to the other two datasets, and IXL includes more
information gaps.

Related Works

Researchers in early years have studied math word
problems in a constrained domain by either limiting the input sentences to a fixed set of patterns (Bobrow, 1964b; Bobrow, 1964a; Hinsley et
al., 1977) or by directly operating on a propositional representation instead of a natural language
text (Kintsch and Greeno, 1985; Fletcher, 1985).
Mukherjee and Garain (2008) survey these works.
Among the recent algorithms, the most general
ones are the work in (Kushman et al., 2014; Zhou
et al., 2015) . Both algorithms try to map a word
math problem to a ‘system template’ that contains
a set of ‘equation templates’ such as ax + by =
c. These ‘system templates’ are collected from
the training data. They implicitly assume that
these templates will reoccur in the new examples
which is a major drawback of these algorithms.
Also, Koncel-Kedziorski et al. (2015) show that
the work of Kushman et al. (2014) heavily relies on the overlap between train and test data and
when this overlap is reduced the system performs
poorly.
Work of (Koncel-Kedziorski et al., 2015; Roy
and Roth, 2015) on the other hand try to map the
math word problem to an expression tree. Even
though, these algorithms can handle all the four
arithmetic operators they cannot solve problems
that require more than one equation. Moreover,
experiments show that our system is much more
robust to diversity in the problem types between
training and test data for the problems it handles.
The system A RIS in (Hosseini et al., 2014)
solves the addition-subtraction problems by categorizing the verbs into seven categories such as
‘positive transfer’, ‘loss’ etc. It represents the information in a problem as a state and then updates
the state according to the category of a verb as the
story progresses. Both A RIS and our system share

5

Experimental Evaluation

5.1

5.2

Dataset

Result

Hosseini et al. (2014) evaluate their system using
3-fold cross validation. We follow that same procedure. Table 5 shows the accuracy of our system on each dataset (when trained on the other
two datasets). Table 6 shows the distribution of
the part whole, change, comparison problems and
the accuracy on recognizing the correct formula.
ARIS
KAZB
ALGES
Roy & Roth
Majority
Our System

MA1
83.6
89.6
45.5
96.27

IXL
75.0
51.1
71.4
82.14

MA2
74.4
51.2
23.7
79.33

Avg
77.7
64.0
77.0
78.0
48.9
86.07

Table 5: Comparision with ARIS, KAZB (Kushman et al., 2014), ALGES (Koncel-Kedziorski et
al., 2015) and the state of the art Roy & Roth on
the accuracy of solving arithmetic problems.
As we can see in Table 6 only IXL contains
problems of type ‘comparison’. So, to study the
accuracy in detecting the compare formula we
uniformly distribute the 33 examples over the 3
datasets. Doing that results in only two errors in
the recognition of a compare formula and also increases the overall accuracy of solving arithmetic
problems to 90.38%.

2151

5.3

Type

Error Analysis

An equation that can be generated from a change
or comparision formula can also be generated by
a part whole formula. Four such errors happened
for the change problems and out of the 33 compare problems, 18 were solved by part whole.
Also, there are 3 problems that require two applications. One example of such problem is, “There
are 48 erasers in the drawer and 30 erasers on the
desk. Alyssa placed 39 erasers and 45 rulers on
the desk. How many erasers are now there in total ?”. To solve this we need to first combine the
two numbers 48 and 30 to find the total number of
erasers she initially had. This requires the knowledge of ‘part-whole’. Now, that sum of 48 and
30, 39 and x can be connected together using the
‘change’ formula. With respect to ‘solving’ arithmetic problems, we find the following categories
as the major source of errors:
Problem Representation: Solving problems
in this category requires involved representation.
Consider the problem, ‘Sally paid $ 12.32 total for
peaches , after a ‘3 dollar’ coupon , and $ 11.54
for cherries . In total , how much money did Sally
spend?’. Since the associated verb for the variable
3 dollar is ‘pay’, our system incorrectly thinks that
Sally did spend it.
Information Gap: Often, information that is
critical to solve a problem is not present in the text.
E.g. Last year , 90171 people were born in a country , and 16320 people immigrated to it . How
many new people began living in the country last
year ?. To correctly solve this problem, it is important to know that both the event ‘born’ and ‘immigration’ imply the ‘began living’ event, however
that information is missing in the text. Another
example is the problem, “Keith spent $6.51 on a
rabbit toy , $5.79 on pet food , and a cage cost
him $12.51 . He found a dollar bill on the ground.
What was the total cost of Keith ’s purchases? ”. It
is important to know here that if a cage cost Keith
$12.51 then Keith has spent $12.51 for cage.
Modals: Consider the question ‘Jason went to
11 football games this month . He went to 17
games last month , and plans to go to 16 games
next month . How many games will he attend in
all?’ To solve this question one needs to understand the meanings of the verb “plan” and “will”.
If we replace “will” in the question by “did” the
answer will be different. Currently our algorithm

part whole
change
compare

Total
correct
Total
correct
Total
correct

MA1
59
59
74
70
0
0

IXL
89
81
18
15
33
0

MA2
51
40
68
56
0
0

Table 6: Accuracy on recognizing the correct application. None of the MA1 and MA2 dataset contains “compare” problems so the cross validation
accuracy on “IXL” for “compare” problems is 0.
cannot solve this problem and we need to either
use a better representation or a more powerful
learning algorithm to be able to answer correctly.
Another interesting example of this kind is the
following: “For his car , Mike spent $118.54 on
speakers and $106.33 on new tires . Mike wanted
3 CD ’s for $4.58 but decided not to . In total ,
how much did Mike spend on car parts?”
Incomplete IsA Knowledge: For the problem “Tom bought a skateboard for $ 9.46 , and
spent $ 9.56 on marbles . Tom also spent $ 14.50
on shorts . In total , how much did Tom spend
on toys ? ”, it is important to know that ‘skateboard’ and ‘marbles’ are toys but ‘shorts’ are not.
However, such knowledge is not always present in
ConceptNet which results in error.
Parser Issue: Error in dependency parsing is
another source of error. Since the attribute values
are computed from the dependency parse tree, a
wrong assignment (mostly for verbs) often makes
the entity irrelevant to the computation.

6

Conclusion

Solving math word problems often requires explicit modeling of the word. In this research, we
use well-known math formulas to model the word
problem and develop an algorithm that learns to
map the assertions in the story to the correct formula. Our future plan is to apply this model to
general arithmetic problems which require multiple applications of formulas.

7

Acknowledgement

We thank NSF for the DataNet Federation Consortium grant OCI-0940841 and ONR for their grant
N00014-13-1-0334 for partially supporting this research.

2152

References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the
main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation, pages 385–393. Association for Computational Linguistics.
Daniel G Bobrow. 1964a. Natural language input for a
computer problem solving system.
Daniel G. Bobrow. 1964b. A question-answering
system for high school algebra word problems. In
Proceedings of the October 27-29, 1964, Fall Joint
Computer Conference, Part I, AFIPS ’64 (Fall, part
I), pages 591–614, New York, NY, USA. ACM.

Walter Kintsch and James G Greeno. 1985. Understanding and solving word arithmetic problems.
Psychological review, 92(1):109.
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish
Sabharwal, Oren Etzioni, and Siena Dumas Ang.
2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585–597.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. Association for Computational Linguistics.
Hector J Levesque. 2011. The winograd schema challenge.
Hugo Liu and Push Singh. 2004. Conceptneta practical commonsense reasoning tool-kit. BT technology
journal, 22(4):211–226.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David McClosky. 2014. The stanford corenlp natural language processing toolkit. In ACL (System Demonstrations), pages 55–60.

Peter Clark and Oren Etzioni. 2016. My computer
is an honor student but how intelligent is it? standardized tests as a measure of ai. AI Magazine.(To
appear).

George A Miller.
1995.
Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.

Peter Clark. 2015. Elementary school science and
math tests as a driver for ai: Take the aristo challenge! In AAAI, pages 4019–4021.

Anirban Mukherjee and Utpal Garain. 2008. A review
of methods for automatic understanding of natural
language mathematical problems. Artificial Intelligence Review, 29(2):93–122.

Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Rational, evaluation and approaches–erratum. Natural
Language Engineering, 16(01):105–105.

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
EMNLP, volume 1, page 2.

Marie-Catherine De Marneffe and Christopher D Manning. 2008. Stanford typed dependencies manual.
Technical report, Technical report, Stanford University.
Edward A Feigenbaum and Julian Feldman.
Computers and thought.

1963.

Charles R Fletcher. 1985. Understanding and solving
arithmetic word problems: A computer simulation.
Behavior Research Methods, Instruments, & Computers, 17(5):565–571.
Dan A Hinsley, John R Hayes, and Herbert A Simon.
1977. From words to equations: Meaning and representation in algebra word problems. Cognitive processes in comprehension, 329.
Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 523–533.

Subhro Roy and Dan Roth. 2015. Solving general
arithmetic word problems. EMNLP.
Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xiaojiang
Liu, and Yong Rui. 2015. Automatically solving number word problems by semantic parsing and
reasoning. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Lisbon, Portugal.
Robert Speer and Catherine Havasi. 2013. Conceptnet
5: A large semantic network for relational knowledge. In The Peoples Web Meets NLP, pages 161–
176. Springer.
Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks.
arXiv preprint arXiv:1502.05698.
Lipu Zhou, Shuaixiang Dai, and Liwei Chen. 2015.
Learn to solve algebra word problems using
quadratic programming. In Proceedings of the 2015
Conference on Empirical Methods in Natural Language Processing, pages 817–822.

2153

(Presented in 6th Multidisciplinary Workshop on Advances in Preference Handling, at ECAI 2012, Aug 2012)

A Non-Monotonic Goal Specification Language for
Planning with Preferences
Tran Cao Son⇤ and Enrico Pontelli⇤ and Chitta Baral+ 1
Abstract. This paper introduces a default logic based approach to
defining goal specification languages that can be non-monotonic and
allow for the specification of inconsistencies and priorities among
goals. The paper starts by presenting a basic goal specification language for planning with preferences. It then defines goal default theories (resp. with priorities) by embedding goal formulae into default
logic (resp. prioritizing default logic). It is possible to show that the
new language is general, as it can express several features of previously developed goal specification languages. The paper discusses
how several other features can be subsumed by extending the basic
goal specification language. Finally , we identify features that might
be important in goal specification that cannot be expressed by our
language.

1

Introduction

An important component of autonomous agent design is goal specification. In classical planning, goals deal with reaching one of a
particular set of states. Nevertheless, often goals of agents are not
just about reaching a particular state; goals are often about satisfying desirable conditions imposed on the trajectory. For example, a
person can have the following desire in preparing travel plans to conferences:
(*) I prefer to fly to the conference site (since it is usually too
far to drive).
The user’s preference restricts the means that can be used in achieving her goal of reaching the conference site, which leads to the selection of a plan that reaches the conference site by airplane, whenever
possible. Ultimately, this affects what actions the person should take
in order to achieve the goal.
These observations led to the development of languages for the
specification of soft goals in planning, e.g., PP introduced in [14]
and modified in [6]. In PP, a basic desire is a temporal formula
describing desirable properties of a plan. Atomic and general preferences are particular classes of formulae built over basic desires. A
preference formula defines a preference order
among the trajectories that achieve the hard goal of the problem, i.e., for every pair
of trajectories ↵ and , ↵
indicates that ↵ is preferable to .
is often a partial order and its definition relies on the notion of
satisfaction between trajectories and a preference specification. Similar ideas have been considered in the planning community and led
to extensions of the planning domain description language PDDL,
with features for representing classes of preferences over plans using
temporal extended preferences (e.g., [10]).
1 ⇤ Department

of Computer Science New Mexico State University, Las
Cruces, New Mexico, USA, email: tson|epontell@cs.nmsu.edu
and + Department of Computer Science and Engineering, Arizona State
University, Tempe, Arizona, USA, email: chitta@asu.edu

In [4], the authors argue that a goal specification language should
be non-monotonic for various reasons, such as elaboration tolerance
and simplicity of goal specification. For example, the same traveler
with the preference (*) would probably not mind driving at most
three hours to the conference site if the only flight to the destination requires to travel the day before the conference starts. In this
case, her preference becomes:
(**) Normally, I prefer to fly to the conference site (since it is usually too far to drive). However, if there are no flights on the same day
of the conference and the driving time is at most three hours, then I
will drive.
To address this issue, an extension of LTL [11], called N-LTL, has
been proposed, allowing weak and strong exceptions to certain rules.
A weakness of this language is that it requires the classification of
weak and strong exceptions when a goal is specified. In [5], the language ER-LTL is introduced to address this limitation of N-LTL.
Similarly to PP, the semantics of N-LTL and ER-LTL relies on the
notion of satisfaction between plans and N-LTL or ER-LTL specifications. Observe that the issue of non-monotonicity is dealt within
PP and in the extensions of PDDL by revising the soft goals, which
is an approach that N-LTL specifically tries to avoid.
We observe that the focus of the work in [1, 4, 5, 6, 10] is on
classical planning, i.e., the planning domains are deterministic and
the initial state is complete, while the work in [14] considers nondeterministic domains and only discusses preferences among weak
plans. In [2], it is argued that plans for non-deterministic domains
should be policies (i.e., a partial function from the set of states to
the set of actions) and the language ⇡-CTL⇤ is developed for specifying goals in non-deterministic domains. ⇡-CTL⇤ is an extension
of CTL⇤ [9] with two modalities A⇡ and E⇡ for considering all or
some trajectories w.r.t. a given policy. In [3], the language ⇡-CTL⇤ is
extended with quantifiers over policies to increase its expressiveness.
Policies satisfying a goal specification are viewed as the solutions of
a planning problem.
In this paper, we explore an approach based on prioritizing default
logic for defining a goal specification language. The new language,
called goal default theories with priorities, is a variation of prioritizing default logic, in which formulae occurring within a default can
be temporal extended preference formulae. We show that the core of
the new language subsumes several features from existing goal languages and can be extended to subsume several other features from
other goal languages. Finally, we discuss the possible applications
of the new language in the study of existing goal languages and the
development of new ones.

2

Background

In this section, we briefly review the basic definitions of planning,
linear temporal logic (LTL) and its extension for specifying prefer-

ences in planning.

2.1

LTL and Temporal Extended Preferences

Let L be a propositional language. By hpi we denote a propositional
formula from L. LTL-formulae are defined by the following syntax
hf i

::=

hpi | hf i ^ hf i | hf i _ hf i |
¬hf i | hf i | 2hf i | 3hf i | hf iUhf i

(1)

The semantics of LTL-formulae is defined with respect to sequences
of interpretations of L. For later use, we will refer to an interpretation
of L as a state and a possibly infinite sequence of interpretations of
L, s0 , s1 , . . ., as a trajectory. For a trajectory = s0 , s1 , . . . , by i
we denote the suffix si , si+1 , . . . of . A trajectory = s0 , s1 , . . .
satisfies an LTL-formula f , denoted by |= f , if 0 |= f where
• j |= p iff sj |= p
• j |= ¬f iff j 6|= f
• j |= f1 ^ f2 iff j |= f1 and j |= f2
• j |= f1 _ f2 iff j |= f1 or j |= f2
• j |= f iff j+1 |= f
• j |= 2f iff k |= f , for all k j
• j |= 3f iff i |= f for some i j
• j |= f1 U f2 iff there exists k j such that
k |= f2 and for all i, j  i < k, i |= f1 .
A finite trajectory s0 , . . . , sn satisfies an LTL-formula f if its extension s0 , . . . , sn , sn+1 , . . . satisfies f , where sk = sn for k > n.
In order to deal with planning problems, LTL is extended with the
following constructs
at end hpi | hpi sometime before hpi |
hpi sometime after hpi

(2)

Formulae of the extended LTL are referred to as Temporal Extended
Preferences (TEP). Note that the last two are syntactic sugars for LTL
formulae. Temporal extended preferences are interpreted over finite
trajectories. The notion of satisfaction for standard LTL-formulae is
defined as above, while satisfaction of TEP formulae is as follows:
given a finite trajectory = s0 , . . . , sn :
• |= at end p iff sn |= p;
•
|= p1 sometime before p2 iff for every i, 0  i  n, if
i |= p1 then j |= p2 for some i  j  n; and
• j |= p1 sometime after p2 iff for every i, 0  i  n, if
i |= p1 then j |= p2 for some 0  j < i  n.

2.2

Planning

In this paper, we describe a dynamic domain as a labeled transition
system T = (F, A, S, L), where:
• F is a set of fluents (or propositions),
• A is a set of actions,
• S is a set of interpretations (or states) of F , and
• L ✓ S ⇥ A ⇥ S.
Each triple hs1 , a, s2 i 2 L indicates that the execution of the action
a in the state s1 might result in the state s2 . T is deterministic if for
each state s and action a, L contains at most one triple hs, a, s2 i;
otherwise, T is non-deterministic.
Given a transition system T , a finite or infinite sequence
s0 a0 s1 a1 . . . sn an sn+1 . . . of alternate states and actions is called
a run if hsi , ai , si+1 i 2 L for every i = 0, . . . A policy ⇡ in a transition system T is a partial function ⇡ : S ! A from the set of states

to the set of actions. A run s0 a0 s1 a1 . . . sk ak sk+1 . . . is said to be
induced by a policy ⇡ if ai = ⇡(si ) for every i = 0, . . . , k, . . .
Definition 1. A planning problem is a triple hT, Si , Sf i where T =
(F, A, S, L) is a transition system, Si ✓ S is the set of initial states,
and Sf ✓ S is the set of final states.
Intuitively, a planning problem asks for a plan which transforms
the transition system from any state belonging to Si to some state in
Sf . In the rest of the discussion, we assume Si and Sf to be finite
sets. We distinguish two classes of planning problems:
Deterministic planning: in this case, T is deterministic and a solution (or plan) of hT, Si , Sf i is an action sequence [a0 ; . . . ; an ]
such that, for every s0 2 Si , s0 a0 s1 a1 . . . an sn+1 is a run in T
and sn+1 2 Sf ;
Non-deterministic planning: in this case, T is non-deterministic
and a solution (or plan) of hT, Si , Sf i is a policy ⇡ such that, for
every s0 2 Si and every run induced by ⇡ in T , ⇡ is finite and is
of the form s0 a0 s1 a1 . . . sk ak sk+1 where sk+1 2 Sf .
In the following, whenever we refer to a possible plan in a transition
system T , we mean a sequence of actions (resp. a policy) if T is deterministic (resp. non-deterministic) that can generate a correct run.
Let us illustrate these basic definitions using the following simple
example.
Example 1. Consider a transportation robot. There are different locations, say l1 , . . . , lk , whose connectivity is given by a graph and
there might be different objects at each location. Let O be a set of objects. The robot can travel between two directly connected locations.
It can pick up objects at a location, hold them, drop them, and carry
them between locations. We assume that, for each pair of connected
locations li and lj , the robot has an action ai,j for traveling from li
to lj . The robot can hold only one object at a time. The domain can
be represented by a transition system T1 = (F, A, S, L):2
• F contains the following types of propositions:
at(i) denotes that the robot is at the location li ;
o at(o, i) denotes that the object o is at the location li ;
h(o) denotes that the robot is holding the object o.
• A contains of the following types of actions:
ai,j the robot moves from li to lj ;
release(o) the robot drops the object o;
pickup(o) the robot picks up the object o.
• S contains the interpretations of F which satisfy the basic constraints, such as the robot is at one location at a time, it holds only
one object, etc.
• L contains transitions of the form hs, a, s0 i such that s0 is the
result of the execution of a in s; for example, if a = ai,j and
at(i) 2 s then s0 = s \ {at(i)} [ {at(j)}.
T1 is a deterministic transition system. We will also refer to T2 as
the non-deterministic version of T1 by defining T2 = (F, A, S, L0 )
where L0 = L[{hsi , ai,j , si i | ai,j 2 A} and at(i) 2 s. Intuitively,
T2 encodes the fact that the action ai,j might fail and, when it does,
the robot will stay where it was after the execution of ai,j .
A planning problem P in this domain is given by specifying the
initial location of the robot and of the objects and the final location of the robot and of the objects. It is deterministic (resp. nondeterministic) if T1 (resp. T2 ) is considered.
For example, Pi = hTi , {{at(1)}}, Sf i where for each s 2 Sf ,
at(k) 2 s is a planning problem for Ti . A solution for P1 is a sequence [a1,2 ; . . . ; ak 1,k ]. On the other hand, a solution for P2 is a
policy ⇡ defined by ⇡(s) = at,t+1 iff at(t) 2 s for t < k.
2

We simplify the definitions of S and L for readability.

3

A Basic Goal Specification Language for
Planning with Preferences

In the literature, a planning problem with preferences is defined
as a pair (P, ) of a planning problem P = hT, Si , Sf i, where
T = (F, A, S, L), and a preference formula in a goal specification language. A plan of P is called a preferred plan if it is a plan
for P and satisfies , where the notion of satisfaction of a preference
formula by a plan is language dependent.
In general, we can characterize a goal specification language G
over a transition system T by a set of preference formulae F and a
satisfaction relation |=G between the set of possible plans of T and
formulae in F . We will write |=G
to denote that the plan
satisfies the formula under the language G.
For later use, we will define a basic goal specification language for
a transition system T = (F, A, S, L), written as Gb = (Fb , |=Gb ), as
follows:
• the set of preference formulae Fb is the set of TEP-formulae over
F [ A, and
• for a planning problem P = hT, Si , Sf i, |=Gb is defined as follows:
if T is deterministic, a plan = [a0 , . . . , an ] for a planning
problem P is said to satisfy a formula
in Fb if for every s0 2 Si , s0 a0 s1 a1 . . . an sn+1 is a run in T and (s0 [
{a0 }), . . . , (sn [ {an }), sn+1 is a trajectory satisfying (in
the TEP-language over F [ A);
if T is non-deterministic, a solution (policy) ⇡ for P is said
to satisfy a formula
in Fb if for every s0 2 Si and every run s0 a0 s1 a1 . . . sk ak sk+1 in T induced by ⇡, (s0 [
{a0 }), . . . , (sn [ {an }), sn+1 is a trajectory satisfying (in
the TEP-language over F [ A).
In the following, we will assume that any goal specification language
G is a conservative extension of Gb , i.e., (i) G contains all formulae
in Gb ; and (ii) for every planning problem P and a formula in G, if
2 Gb and |=Gb with respect to Gb then |=G with respect
to G.
Example 2. Some preference formulae in Gb for the transition systems in Ex. 1 are:
• 3at(2): the robot should visit the location l2 during the execution of the plan;
• at(1)^3at(2): the robot must (i) start in a state satisfying at(1)
(or the robot is at the location l1 initially); and (ii) visit the location l2 at some point during the execution of the plan;
W
• 2[at(2) ) ( i6=2 a2i )]: whenever the robot visits l2 , it should
leave that location immediately by executing an action going to
one of its neighbors;
• h(o) )
¬h(o): if the robot holds an object o in the initial
state then it should release o after the execution of one action;
• 2[h(o) )
¬h(o)]: whenever the robot holds an object o
it should release o after the execution of an action;
• h(o) sometime before at(5): whenever the robot holds the
object o, it must visit the location l5 thereafter before reaching
the goal;
V
• at end [ o2O ¬h(o)]: at the end, the robot should not hold any
object.
2
With a slight abuse of notation, let us view a state s as a formula

V

s |= f

f^

V

s |= ¬f

¬f . Let Si and Sf be two sets of states and

2 _

6
=4

s2Si

s ^ at end [

| {z }
1

|

_

s2Sf

{z

2

s]
}

3
7
5

It is easy to see that any plan satisfying requires its execution to
start from a state satisfying 1 , which is one of the states in Si , and
end in a state satisfying 2 , which is one of the states in Sf . For this
reason, the description of the initial and final states can be folded into
a preference formula. We will therefore define planning problems as
follows.
Definition 2. Given a transition system T and a goal specification
language G = (F , |=G ) over T , a goal formula in F is called a
planning problem. A solution of is a plan in T such that |=G .
By Def. 2, a goal formula represents a planning problem. The literature is quite diversified when a user faces two or more goal formulae which are contradictory with each other. ForV
example, the formula 3at(2) is contradictory with 2¬at(2); 2¬( o2O h(o)) conflicts with 3h(o1 ); etc. A possibility is to consider a possible plan
as solution if it satisfies some goal formulae. Another possibility is
to rank the goal formulae and identify solutions as plans that satisfy
the formula with the highest possible ranking. In the following, we
will show that a uniform framework for dealing with conflicting goal
formulae can be obtained by embedding goal formulae into Reiter’s
default logic.

4

Goal Default Theories

In this section, we will introduce a new goal specification language,
called goal default theory. A goal default theory is a variation of Reiter’s default theory [12], whose defaults can contain preference formulae. Goal default theories provide a possible treatment of planning
with multiple goal formulae.
A goal default theory is defined over a transition system T =
(F, A, S, L) and a goal specification language G = (F , |=G ).
Given a goal specification language (F , |=G ), we say that two formulae ', in F are equivalent w.r.t. |=G if, for each plan of T ,
we have that |=G ' , .3 We can easily extend this notion to
define the notion of logical consequence w.r.t. |=G —if S is a set of
formulae from F and f is another formula
in F , then S |=G f if for
V
each plan of T we have that |=G '2S ' implies |=G f . Given
a set of formulae S, we define Decl(S) = {' | ' 2 F , S |=G '}.
A preference default (or p-default) d over G is of the following
form
↵ :
(3)
where ↵, , and are formulae in F . We call ↵ the precondition,
the justification, and the consequence of d, and we denote them
with prec(d), just(d), and cons(d), respectively. A default d is said
to be
• Normal if its justification is equivalent to its conclusion;
• Prerequisite-free if its precondition is equivalent to true; and
• Supernormal if it is normal and prerequisite-free.
Given a set of formulae S from F , a default d is said to be defeated
in S if S |= ¬just(d). Some preferences and their representation as
p-defaults over Gb for the domain from Example 1 are given next.
Example 3. In these examples, o denotes a particular object in the
domain.
3

',

is a shorthand for (' ^ ) _ (¬' ^ ¬ ).

• If there is no evidence that the robot is initially at the location l2 ,
then it should go to l2 :
> : ¬at(2)
3at(2)

(4)

• Assume that objects might be defective, represented by the proposition def ective. We can write
> : 2[¬defective(o)]
2[at(2) ) h(o)]

(5)

to indicate that normally, we would like that the robot holds the
object o whenever it is at the location l2 . An exception to this rule
is possible if the object o is defective.
• If the robot is not required to hold the object o in the final state
and there is no evidence that it initially holds o, then it should not
execute the action of picking up the object o:
>

:

at end (¬h(o)) ^ ¬h(o)
2[¬pickup(o)]

(6)

• If there is no evidence that the object o is initially in the wrong
place then the robot should not start by executing the action of
picking up the object o:
V
at end (o at(o, i)) :
i6=j ¬o at(o, j)
(7)
¬pickup(o)
• A stronger version of (7) is
at end (o at(o, i)) :

V

i6=j

2¬pickup(o)

¬o at(o, j)

(8)

indicates that the robot should never pick up the object o if o could
already be in the desired final location.
• If there is the possibility that the robot might reach location l2 ,
then it must leave the location immediately after its arrival at l2 .
> : 3[at(2)]
W
2[at(2) )
i6=2 a2,i ]

(9)

satisfies either or . The following result generalizes this observation.
Proposition 1. Let T = (F, A, S, L) be a transition system, G =
(F , |=G ) be a goal specification language, and = { 1 , . . . , n }
be a set of preference formulae in F . Furthermore, let
⌃

• If there is no evidence that an object o will ever appear in location i then the robot should never go there.
> : 2[¬o at(o, i)]
W
2[ j6=i ¬aj,i ]

F [A is a p-default, and any Reiter’s default theory over the language
F [ A is a goal default theory.
Definition 5. Given a transition system T = (F, A, S, L) and a goal
specification language G = (F , |=G ) over T , a planning problem
over T and G is a goal default theory ⌃ = (D, W ) over G and T .
The notion of a solution to a planning problem is modified as follows.
Definition 6. Given a transition system T = (F, A, S, L), a goal
specification language G = (F , |=G ) over T , and a planning problem ⌃ over T and G, a solution of ⌃ is a plan in T such that
|=G E for some extension E of ⌃.
Some planning problems over the transition systems in Exp. 1 and
the language Gb are given in the next example.
Example 4 (Continuation of Example 3). • Let
⌃1
=
({p1 }, {at(1), at end at(5)}) where p1 is the default (4).
Intuitively, we have that ⌃1 identifies plans where the robot starts
at location l1 , goes through the location l2 , and ends in location
l5 .
• Let ⌃2 = ({p6 }, {at(1), at end at(5)}) where p6 is the default
(9). This identifies plans where the robot starts at location l1 , ends
in location l5 , and either (i) never goes through the location l2 ; or
(ii) never stays in the location l2 within two consecutive steps. 2
The planning problems in Example 4 are simple, in that they are
specified by goal default theories whose set of defaults is a singleton.
Let us consider a more complicated example. Assume that we have
two temporal formulae and such that there exists no plan that can
satisfy both and . In this case, the use of goal default theory as a
goal formula comes in handy. Indeed, every solution of the planning
problem expressed by the goal default theory
✓⇢
◆
>:¬
>:¬
⌃ , =
,
,;
(11)

(10)

In the following, we will refer to the p-defaults in (4)-(9) by
p1 , . . . , p6 , respectively.
2
We next define the notion of a goal default theory.
Definition 3. A goal default theory over a goal language G =
(F , |=G ) and a transition system T is a pair ⌃ = (D, W ) where
D is a set of p-defaults over G and W ✓ F .
Given a set of p-defaults D, we denote with cons(D) the set
cons(D) = {cons(d) | d 2 D}. A p-default d is applicable w.r.t. a
set of F formulae S if S |=G prec(d) and S 6|=G ¬just(d). Let us
denote with ⇧D (S) the set of p-defaults from D that are applicable
w.r.t. S.
Definition 4 (From [12]). Let ⌃ = (D, W ) be a goal default theory
over G = (F , |=G ) and T . An extension of ⌃ is a minimal set E ✓ F
that satisfies the condition E = Decl(W [ Cons(⇧D (E))). We say
that ⌃ is consistent if it has at least one extension.
From this definition, any default over the propositional language

=

> :

2

,;

(12)

• For every solution to the problem ⌃ there existsVa maximal
(w.r.t. ✓) set of preferences
✓ such that |=G
;
2
0 or
• For every pair of solutions and 0 of ⌃ , either
=
0 and
0 6✓
6✓
.

5

Goals Default Theories with Priorities

Proposition 1 shows that goal default theories can be used to specify planning problems with multiple preferences which might not be
consistent with each other. For instance, consider a traveler from New
York to San Francisco who has two preferences: reach the destination
as fast as possible ( 1 ) and spend the least amount of money ( 2 ). In
general, these two preferences cannot be satisfied at the same time. In
this case, it is more reasonable to assume that a plan satisfying one of
the criteria is an acceptable solution. Thus, ⌃{ 1 , 2 } is a reasonable
goal specification if the traveler is impartial about 1 and 2 . On
the other hand, if the traveler prefers 1 over 2 (or vice versa), we
will need to change the goal specification or provide additional ways
for the traveler to specify this priority. As it turns out, the literature
is rich with approaches for adding priorities to default theories [7, 8]
which can be easily adapted to goal default theories. We next define

goal default theories with priorities by adapting the work of [7] to
goal default theories.
Let us start by introducing static priorities, encoded by a wellordering relation among p-defaults—i.e., is transitive, irreflexive, and each set of elements admits the least element in the ordering.
We denote with min (X) the least element of X with respect to .
We define goal default theory with priorities as follows.
Definition 7. A goal default theory with priorities over a goal language G = (F , |=G ) and a transition system T is a triple (D, W, )
where D is a set of p-defaults over G, is a well-ordering relation
over D, and W ✓ F .
Following the general design of prioritizing default theory [7], the
notion of preferred extension can be defined by successively simplifying the structure of the defaults.
Let us identify a construction of preferred extension through the
application of defaults according to the ordering imposed by . Let
us introduce the PR operator which computes the next “preferred”
set of goal formulae from an existing one:
• PR (S) = Decl(S [ {cons(d)})
if ⇧⇤D (S) 6= ; ^ d = min ({x | x 2 ⇧⇤D (S)});
• PR (S) = S if ⇧⇤D (S) = ;
where ⇧⇤D (S) = {d | d 2 ⇧D (S), S 6|= cons(d)}. If the elements in D (for a goal default theory (D, W )) are supernormal,
then it is possible to use PR to produce a monotone sequence
of goal formulae, by setting S0 = Decl(W ), Si+1
S = PR (Si )
for any successor ordinal i + 1 and Si = Decl( ji Sj ) for any
limit ordinal i. WeSwill denote the result of this construction as
P ref (D, W ) = i 0 Si .
The process of determining a preferred extension will apply
P ref on a reduced version of the theory, in a style similar to that
used in the Gelfond-Lifschitz reduct. Following the model proposed
in [7], the reduct of a goal default theory with priorities (D, W, )
w.r.t. a set of goal formulae S, denoted (DS , W, S ), is obtained as
follows:
: just(d)
• Determine D0 = { > cons(d)
| d 2 D, S |=G prec(d)}

• Determine DS = {d 2 D0 | cons(d) 62 S or S 6|=G ¬just(d)}
and S is such that d01 S d02 if d1
d2 and d1 (d2 ) is the
-least element that introduced d01 (d02 ) in D0 .
We define preferred extensions as follows.
Definition 8. Let (D, W, ) be a goal default theory with priorities
over G = (F , |=G ) and T . A preferred extension E of (D, W, ) is
a set of goal formulae in F such that E is an extension of (D, W )
and E = P ref E (DE , W ).
Similar to [7], we can generalize the above definitions and define
(i) a goal default theory with priorities as a triple (D, W, ) where
(D, W ) is a goal default theory and is a partial order among defaults in D; and (ii) a set of formulae E is a preferred extension
of (D, W, ) if it is a preferred extension of some (D, W, E ) for
some well-ordering E which is an extension of . For brevity, we
omit the precise definitions. Definitions 5 and 6 can be extended in
the obvious way: a planning problem is a goal default theory with
priorities (D, W, ) and its solutions are preferred extensions of
(D, W, ).
Example 5. Let us consider the domain in Example 1. Let us assume
that, among the objects, there is a very valuable object o1 and a
dangerous object o2 . Furthermore, let us assume that the robot is
equipped with actions that can detect the object o2 whenever the
robot is at the same location as o2 . However, the equipment might not
be working. We will denote with working the fact that the equipment
is working properly. Let us consider the two formulae:

• ' := 3h(o
V 1 ): the robot should try to get the object o1
• := 2[ i2{1,...,k} (o at(o2 , i) ) ¬at(i))]: the robot should not
be at the same place with object o2 at any time.
With these formulae, we can define the following p-defaults:
g1 ⌘

> : working
^'

g2 ⌘

> : ¬working
'

g1 indicates that if the equipment is initially working, then the robot
will get o1 while trying to avoid o2 . g2 states that if the equipment
is not working, then the robot will only worry about getting o1 . The
theory ({g1 , g2 }, ;, {g1
g2 }) states that we prefer that the robot
tries to satisfy g1 before trying to satisfy g2 .

6

Related Work and Discussion

In this section, we relate goal default theories with priorities to existing goal specification languages. We then discuss possible applications of the new language.
• TEP formulae: TEP formulae have been implemented in a planner in [1]. Given a set of TEP formulae
= { 1 , . . . , n }, a
planning problem is an optimization problem that maximizes the
rewards obtained by satisfying the formulae in . Formally, the
reward over a plan is
⌃

i2

, |= i reward(

i)

⌃

i2

, 6|= i penalty(

i)

where reward( ) and penalty( ) denote the reward and penalty
for satisfying and not satisfying , respectively.
The planning problem can be expressed by a goal default theory
with priorities as follows. Let S be a set of formulae, S ✓ , and
dS be the default
V
V
> :
^
2S
2 \S ¬
V
V
^
2S
2 \S ¬
Let D = {dS | S ✓
where dS
dS 0 if
⌃
⌃

i 2S
i 2S

} and

be the partial order over D

reward( i ) ⌃ i 62S penalty( i )
reward( i ) ⌃ i 62S 0 penalty( i ).

0

We can show that (D , ;,
) is a goal default theory with priorities representing the given planning problem, i.e., any preferred
solution of (D , ;,
) is a solution of the original planning
problem and vice versa.
• PP: The language PP allows the specification of three types of
preferences. A basic desire ' is a preference over a trajectory and
therefore is a part of the basic goal language. An atomic preference
is an ordering among basic desires = 1
2 ...
k and
expresses that the preference i is more important than i+1 for
1  i < k 1. An atomic preference can be represented by the
following goal default theory with priorities
⇣n
o
⌘
> : i
i = 1, . . . , k , ;,
i
> :

j
where
is defined by > : i i
for 1i<jk.
j
A general preference is either an atomic preference or a combination of general preferences, such as & , | , and ! , where
and are general preferences. Intuitively, general preferences
add finitely many levels to the specification of preferences and

thus cannot be easily represented by goal default theories which
assume ceteris paribus over the preferences. Adding priorities allows only an extra layer of comparison between preferences. We
view this as a weakness of goal default theories and plan to further
investigate this issue.
• N-LTL and ER-LTL: These two languages allow the specification
of weak and strong exceptions within goal formulae represented
as LTL-formulae by introducing labels to LTL-formulae. By compiling away the labels as in [4], we can show that Gb subsumes
N-LTL and ER-LTL.
Observe that the constructs used in N-LTL and ER-LTL are fairly
close to default logic. This leads us to believe that interesting collections of N-LTL (ER-LTL) theories can be translated into goal
default theories—which would provide a reasonable semantics for
N-LTL (ER-LTL) theories with loops that have not been considered so far.
Finally, we would like to note that Gb can be easily extended to
consider N-LTL (ER-LTL) formulae by
– extending Fb with N-LTL (ER-LTL) formulae; and

– extending |=Gb to define that |=Gb S iff |=Gb c(S) where
c(S), a LTL formula, denotes the result of compiling S to an
LTL formula as described in [4, 5].
• ⇡-CTL⇤ and P-CTL⇤ : These two languages consider nondeterministic domains and define goals over policies but do not
consider preferences among goals. In addition, these languages
introduce the operators A, E, A⇡ , and E⇡ over paths and the two
quantifiers EP and AP over state formulae. Nevertheless, we can
show that the CTL⇤ part of ⇡-CTL⇤ can be expressed in Gb . Furthermore, Gb can be extended to allow formulae of ⇡-CTL⇤ . However, the two new state quantifiers are not expressible in our goal
language. We observe that as the goal language is parameterized
with the satisfaction relation, Gb can be easily extended with these
operators. We strongly believe that these extensions will be sufficient for goal default theories with priorities to capture P-CTL⇤ .
The above discussion highlights features from existing goal languages that can (or cannot) be expressed by our goal language. This
also shows that the proposed language can serve as a unified language for evaluating goal languages. The use of default theories as
the basic language also provides us with an advantage in the study
of computational complexity of goal languages. In this effort, we
expect that well-known complexity results on prioritized default theories [13] will be extremely useful. This will provide us with insights
for the use of existing goal languages as well as the development of
new goal languages.

7

Conclusions and Future Work

In this paper, we describe a default logic based approach to defining
non-monotonic goal specification languages. We start with a basic
goal specification language and use default logic (or prioritizing default logic) to provide a natural way for dealing with inconsistency
and priorities over goals. We show that the new language subsumes
some goal languages in the literature and can describe several features from other goal languages. We identify desirable features that
cannot be easily expressed by our goal language, among them is the
multi-level of preferences between goals, which we intend to investigate in the near future. We also discuss possible applications of the
proposed goal language.

REFERENCES
[1] Jorge A. Baier, Fahiem Bacchus, and Sheila A. McIlraith, ‘A heuristic search approach to planning with temporally extended preferences’,
Artif. Intell., 173(5-6), 593–618, (2009).
[2] Chitta Baral and Jicheng Zhao, ‘Goal specification in presence of nondeterministic actions’, in Proceedings of the 16th Eureopean Conference on Artificial Intelligence, ECAI’2004, including Prestigious Applicants of Intelligent Systems, PAIS 2004, Valencia, Spain, August 22-27,
2004, eds., Ramon López de Mántaras and Lorenza Saitta, pp. 273–277.
IOS Press, (2004).
[3] Chitta Baral and Jicheng Zhao, ‘Goal specification, non-determinism
and quantifying over policies’, in Proceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference, July 16-20,
2006, Boston, Massachusetts, USA. AAAI Press, (2006).
[4] Chitta Baral and Jicheng Zhao, ‘Non-monotonic temporal logics for
goal specification’, in IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India,
January 6-12, 2007, ed., Manuela M. Veloso, pp. 236–242, (2007).
[5] Chitta Baral and Jicheng Zhao, ‘Non-monotonic temporal logics that
facilitate elaboration tolerant revision of goals’, in Proceedings of the
Twenty-Third AAAI Conference on Artificial Intelligence, AAAI 2008,
Chicago, Illinois, USA, July 13-17, 2008, eds., Dieter Fox and Carla P.
Gomes, pp. 406–411. AAAI Press, (2008).
[6] M. Bienvenu, C. Fritz, and S. McIlraith, ‘Planning with qualitative temporal preferences’, in Proceedings of the 10th International Conference
on Principles of Knowledge Representation and Reasoning (KR06), pp.
134–144, Lake District, UK, (June 2006).
[7] G. Brewka and T. Eiter, ‘Prioritizing default logic’, in Intellectics
and Computational Logic, volume 19 of Applied Logic Series, 27–45,
Kluwer, (2000).
[8] James Delgrande and Torsten Schaub, ‘Expressing preferences in default logic’, Artificial Intelligence, 123, 41–87, (2000).
[9] E. A. Emerson, ‘Temporal and modal logic’, in Handbook of theoretical computer science: volume B, ed., J. van Leeuwen, 995–1072, MIT
Press, (1990).
[10] M. Fox and D. Long, ‘PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains’, Journal of Artificial Intelligence Research, 20, 61–124, (2003).
[11] A. Pnueli, ‘The temporal logic of programs’, in Proceedings of the 18th
IEEE Symposium on Foundations of Computer Science (FOCS), pp.
46–57, (1977).
[12] R. Reiter, ‘A logic for default reasoning’, Artificial Intelligence,
13(1,2), 81–132, (1980).
[13] Jussi Rintanen, ‘Complexity of prioritized default logics’, Journal of
Artificial Intelligence Research, 9, 423–461, (1998).
[14] Tran Cao Son and Enrico Pontelli, ‘Planning with Preferences using
Logic Programming’, Theory and Practice of Logic Programming, 6,
559–607, (2006).

Event-Object Reasoning with Curated
Knowledge Bases: Deriving Missing Information
Chitta Baral and Nguyen H. Vo

arXiv:1306.4411v2 [cs.AI] 20 Jun 2013

School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University, Tempe, Arizona, USA

Abstract. The broader goal of our research is to formulate answers
to why and how questions with respect to knowledge bases, such as
AURA. One issue we face when reasoning with many available knowledge
bases is that at times needed information is missing. Examples of this
include partially missing information about next sub-event, first subevent, last sub-event, result of an event, input to an event, destination of
an event, and raw material involved in an event. In many cases one can
recover part of the missing knowledge through reasoning. In this paper
we give a formal definition about how such missing information can be
recovered and then give an ASP implementation of it. We then discuss
the implication of this with respect to answering why and how questions.

1

Introduction

Our work in this paper is part of two related long terms goals: answering “How”,
“Why” and “What-if” questions and reasoning with the growing body of available knowledge bases 1 , some of which are crowd-sourced. Although answering
“How” and “Why” questions are important, so far little research has been done
on them. Our starting point to address them has been to formulate answers
to such questions with respect to abstract knowledge structures obtained from
knowledge bases. In particular, in the recent past we considered Event Description Graphs (EDGs) [1] and Event-Object Description Graphs (EODGs) [2] to
formulate answers to some “How” and “Why” questions with respect to the
Biology knowledge base AURA [3].
Going from the abstract structures to reasoning with real knowledge bases
(KBs) we noticed that the KBs often have missing pieces of information, such
as properties of an instance (of a class) or relations between two instances. For
example, AURA does not encode that Eukaryotic translation is the next event
of Synthesis of RNA in eukaryote; this may be because the two subevents of
“Protein synthesis” were encoded independently. The missing pieces make the
KB and the Description Graphs constructed from it fragmented and as a result
answers obtained with respect to them are not intuitive. Moreover, the KBs
like AURA often have two or more names that refer to the same entity. To
get intuitive answers they need to be resolved and merged into a single entity.
1

See for example, http://linkeddata.org/.

2

Chitta Baral and Nguyen H. Vo

Such finding of non-identical duplicates in the KB and merging them into one
is referred in the literature as entity resolution [4, 5].
In this paper, we start with introducing knowledge description graphs (KDGs)
as structures that can be (without much reasoning) obtained from frame based
KBs such as AURA. We discuss underspecified knowledge description graphs
(UDGs) and formulate notions of reasoning with respect to these graphs to obtain certain missing information. We then present our approach of entity resolution and use it in recovering additional missing information. We give an Answer
Set Programming (ASP) encoding of our formulation. We conclude with a discussion on the use of the above in answering “why” and “how” questions.

2

Background: Frame-based Knowledge Bases; ASP

The KB we used in this work is based on AURA [3] and was described in details in
[6]. AURA is a frame-based KB manually curated by biology experts; it contains
a large amount of frames describing biological entities events (or processes). One
important aspect of our KB is the class hierarchy. For example 2 : its basic class
is Thing, which has two children classes: Entity and Event. Entity is the ancestor
of all classes of biological entities; Event, of biologicalogy events. For instance,
Spatial entity, Eukaryote, Nucleus and mRNA are descendants of Entity, while
“Eukaryotic translation”, “Eukaryotic transcription” are descendant of Event.
Our KB is a set of facts of the form “has(A, slot name, B)” where A and B
are either classes or instances (of classes), slot name is the name of the relation
between A and B such as instance of, raw material or results. The statement
“eukaryotic translation is based on mRNA” is represented in our KB as follows.
has ( euka_transl4191 , instance_of , event ) .
has ( euka_transl4191 , instance_of , e u k a r y o t i c _ t r a n s l a t i o n ) .
has ( euka_transl4191 , base , mrna4642 ) .
has ( mrna4642 , instance_of , mrna ) .

This snippet reads as “eukaryotic translation4191 is an instance of class event
and an instance of class eukaryotic translation. eukaryotic translation4191 is
based on mrna4642, which is an instance of mrna”.
For the declarative implementation of our formulations, we use ASP [7]. That
allows us to use our earlier work [6] on using ASP to reason with frame-based
knowledge bases. ASP’s strong theoretical foundation [8] and its default negation
and recursion are useful in our encoding and in proving results about them.

3

Knowledge Description Graphs

An Underspecified Knowledge Description Graph (UDG) is a structure
to represent the facts about instances and classes of events, entities and relationships between them. An UDG is constructed from knowledge bases such as
AURA. Formal definition of the UDGs is given in the following.
Definition 1. An UDG is a directed graph with one type of node and five types
of directed edges: compositional edges, ordering edges, class edges, locational
edges and participant edges. Each node represents an instance (of a class) or
a class in our KBs.
2

Our examples are either directly from AURA, or are slightly modified from it.

Event-Object Reasoning with Curated Knowledge Bases

3

Edge type
Relation(s)
locational
happenings
class
instance-of, super-class
compositional subevent, first-subevent, has-part, has-region, has-basic-structural-unit
ordering
next-event, enables, causes, prevents, inhibits
participant
raw-materials, result, agent, destination, instrument, origin, site
Table 1. Types of edges in an UDG. An edge of relation Y from a node X to Z
represents X[Y ] = Z, meaning the slot Y of the entity X has value Z.

We used the slot names in KM [9] and AURA as a guide to categorize four
types of edges (Table 1).

Fig. 1. Types of edges in a KDG

A Knowledge Description Graph (KDG) (a slight generalization of
EODGs in [2]) is constructed from an UDG. A node in a KDG represents either
an instance of a biological entity, an instance of a biological process, or a class
of biological entity/event. The KDG structure allows us to answer “How” and
“Why” questions. More formally, a KDG is defined as follows.
Definition 2. A KDG is a directed graph with: (i) three types of nodes: event
nodes, entity nodes, and class nodes; and (ii) five types of directed edges: compositional edges, class edges, ordering edges, locational edges and participant edges.
A KDG has the property that there are no directed cycles within any combination
of compositional, locational and participant edges.
Figure 1 shows the types of edges in a KDG and the corresponding sources
and destinations of the edges. Edges in a KDG are from the edges of the UDG,
with additional type constraints of the source and destination nodes. For example, ordering edges must be from events to events; compositional edges are from
events to events or from entity to entity, depending on their specific relations.
Since UDGs and KDGs can be huge, we usually work on their smaller subgraphs that are rooted at an entity or an event. They are defined as follows.
Definition 3. Let Z be a node in a KDG G. The Knowledge Description Graph
(KDG) rooted at Z is the subgraph of G composed of: (1) The set N of all the
nodes of G that are accessible from Z through compositional edges, class edges,

4

Chitta Baral and Nguyen H. Vo

locational edges or participant edges; and (2) All the edges of G connecting two
nodes in N . We denote the KDG rooted at Z as KDG(Z) or the KDG of Z.
The UDG rooted at Z, denoted as U DG(Z), is defined similarly. Figure 2
shows an example of a KDG rooted at Eukaryote where every other nodes can
be reached from Eukaryote through edges with solid lines (compositional edges,
class edges, locational edges or participant edges).

Fig. 2. A KDG rooted at the entity Eukaryote. Event, entity, and class nodes are
respectively depicted by rectangles, circles and hexagons. Compositional edges are represented by solid-line arrows; ordering edges by dashed-line arrows; participant edges
by lines with a black-square pointed to the entity node; class edges by diamond head
arrows and locational edges by lines with a circle pointed to the event node.

4

Reasoning about Missing Info. in UDGs and KDGs

In this section, we discuss about missing information in the UDGs and the KDGs
and how we can recover some of it through reasoning.
4.1

Event, Next Event, First Sub-event and Last Sub-event

One can directly obtain event names by looking at facts of the form “has(E,
instance of, event)” in the KB; and concluding E in it as an event. However, for
some events such facts may be missing. In that case, we may be able to get the
fact from the UDG’s edges and the edge constraints of the KDGs (Figure 1).
More formally:
Definition 4. Let E be a node in the U DG(Z). E is an event if there is (i) a
participant edge or an ordering edge from E; (ii) a locational edge or an ordering
edge to E; (iii) a compositional edge (of subevent or first subevent relation)
from/to E; or (iv) a path of class edges from E to the class event.
Based on Definition 4, we can get that photosynthesis is an event because it has
compositional edges (of subevent relation) to light reaction and calvin cycle.
Next-event, first subevent and last subevent are amongst the most important
properties in describing events. However, they are not always directly available in
our KB. Fortunately, in many cases, we can recover them from other properties.
Definition 5. Let E and E 0 be two events in the KDG(Z). Event E 0 is the
next event of E if E enables, causes, prevents or inhibits E 0 .

Event-Object Reasoning with Curated Knowledge Bases

5

In other words, E 0 is the next event of E if there is an ordering edge from E to
E0.
Definition 6. Let S be the set of subevents of an event X in the KDG(Z).
Event E in S is the first subevent of X if there exists no other event E 0 in S
such that E is the next event of E 0 . Similarly, event E in S is the last subevent
of X if there exists no other event E 0 in S such that E 0 is the next event of E.
Here we assume that S was properly encoded in that there is only one chain
of subevents in S. In our KB, light reaction and calvin cycle are two subevents
of photosynthesis and light reaction enables calvin cycle. But their orders are
not defined. However, using Definition 5 and 6, we can identify that: calvin
cycle is the next event of light reaction; light reaction is the first subevent of
photosynthesis; and calvin cycle is the last subevent.
4.2

Input/Output of Events

Two types of events: In our KB there are two types of events: transport
events and operational events. In a transport event, there is only a change in
the locations; the input location and output location are different from each
other while the input entity and output entity are the same. All other events
are operational events. In an operational event, there is usually no change in its
location. We differentiate two types of events by their ancestor classes; transport
events are descendants of the classes move through, move into and move out of.
Input, Output, Input Location, Output Location: To reason about the
KDG, we need the input and output of each event as well as the input location
and the output location, which are not always available. In the following, we
show how to use various event’s relations - such as raw-material, destination,
location and others - to create four new relations (IO relations): input, output,
input-location and output-location. After that, we propose rules to complete the
KDG’s IO relations.
We created the IO relations of an event based on specific relations as shown
in Table 2. The meaning of relation “base” from AURA depends on the context.
For transport events, it is for input-location; for operational events, it is for
input.
Event type
IO relation type Relation(s)
Transport event input
object
Transport event output
object
Transport event input-location base, origin
Transport event output-location destination
Operational event input
object, base, raw-material
Operational event output
result
Operational event input-location site
Operational event output-location destination 3
Table 2. The IO properties of events and their corresponding relations.

Completing Missing Information of Input, Output, Input Location,
Output Location: We can obtain missing IO properties of an event from its
subevent(s). For instance, an input of the first subevent of E is also an input of
E.

6

Chitta Baral and Nguyen H. Vo

Definition 7. Let F SE and LSE respectively be the first subevent and last
subevent of event E in KDG(Z).
Let InputRelation be the input relation, input-location relation or one of
their corresponding relations (Table 2). If InputRelation is a relation from F SE
to X then InputRelation is also a relation from E to X.
Let OutputRelation be the output relation and output-location relation or
one of their corresponding relations. If OutputRelation is a relation from LSE
to X then OutputRelation is also a relation from E to X.
In our KB, photosynthesis has two subevents: light reaction and calvin cycle,
the next event of light reaction. Sunlight is the raw-material of the light reaction,
sugar is the result of calvin cycle. Using Definition 7, we have that sunlight is
the raw-material of photosynthesis and sugar is its result. Moreover, we also
have: sunlight is the input of light reaction as well as photosynthesis; sugar is
the output of both calvin cycle and photosynthesis.
Similarly, the output location of an operational event is often not defined in
the KB but we can use input location as the default value for output location.
Definition 8. Let E be an event in KDG(Z), E’s input location is also the
output location if E’s output location has not been specified.
Figure 3 shows the IO properties of events in Fig 2. The properties in bold
are the ones that were recovered using Definitions 7 and 8.

Fig. 3. The IO properties of events in Fig 2. The five blocks contain IO properties of
events: Synthesis of RNA in eukaryote, Eukaryotic translation, Move out, and Eukaryotic transcription. The middle rectangle of each block contains the event name. The top
rectangles are for input and output locations; the bottom rectangles are for input and
output. The properties in bold were recovered using Definitions 7 and 8

4.3

Main Class of an Instance

In our KB, one instance can belong to many classes. For example, dna strand19497
- the input of Eukaryotic transcription - is an instance of dna strand, dna sequence,
nucleic acid and polymer 4 . However, to reason about the equality between in4

For the sake of simplicity, in the previous figures and descriptions, we usually referenced the entities and events by their “main” class(es) and not by the instances’
names although our KB and our implementation works on instances’ names.

Event-Object Reasoning with Curated Knowledge Bases

7

stances, we need the “main” class(es), the most specific class(es) of that instance.
Our formal definition of “main” class is given below.
Definition 9. Let E be an instance in KDG(Z). ClassB is a main class of
instance E if (1) it is a class of E and (2) it is not the case that there is a
ClassA which is a class of E and (a) ClassB is ancestor of ClassA or (b)
ClassB is a general class but ClassA is not; where general classes in our KB
are thing, event, entity, spatial entity, tangible entity, and chemical entity.
The main classes of dna strand19497, according to the Definition 9, are
dna strand and dna sequence; the other classes of dna strand19497 are ancestors
of those two.

5

Entity Resolution

In the KBs such as AURA, the curation was done in many sessions and probably
by many people. (Same is true with respect to many other KBs; especially the
ones that are developed using crowd-sourcing.) The results are, in many cases,
(i) two different instance names were used when they are probably the same
instance; and (ii) parts of some biological process were encoded as independent
events. For example: the input of Eukaryotic translation (Figure 2) is mrna4642
whereas the output of Move out is mrna22911 ; Synthesis of RNA in eukaryote and Eukaryotic translation should be subevents of “Synthesis of protein in
eukaryote” but they are encoded as two separate events.
In this section, we propose methods to solve the first problem. These methods
are then used to solve the second problem in the next section. In order to compare
two instances in a KB, we define a match relation. Generally speaking, instance
A can match with instance B if A can be safely used in a context where a term
of B is expected. We defined matching relation with many confidence levels for
greater flexibility in future works.
Definition 10. Let A and B be two instances in KDG(Z). Let ClassA and
ClassB be main classes of A and B respectively.
1. A matches with B with high confidence if one of the following is true
(a) A = B (A and B are the same instance)
(b) A is cloned from B (Shortcut in AURA to specify that A has all the
properties of B)
(c) ClassA is an ancestor of ClassB.
2. A matches with B with medium confidence if A and B are both cloned from
an instance C.
3. A matches with B with low confidence if ClassA = ClassB (A and B are
instances of the same main class).
4. A matches with B with confidence min(Conf1 , Conf2 ) if
(a) A matches with C with confidence Conf1 and
(b) C matches with B with confidence Conf2
5. Otherwise, A does not match with B.

8

Chitta Baral and Nguyen H. Vo

Using Def.10, we can match mrna4642 - the input of Eukaryotic translation
- with mrna22911 - the output of Move out, because both have mrna as the
main class.
While Def.10 can match all the input and output in our aforementioned
example, it is not sufficient for matching location. For example, we can not
match an instance of cytoplasm to an instance of cytosol. However when we say
Event A occurs in cytosol, we can understand that Event A occurs in cytoplasm.
To overcome this shortcoming, we define the relation Spatially match as follows.
Definition 11. Instance A in KDG(Z) is a location instance if the class ClassA
of A is a descendant of the class spatial entity.
Definition 12. Let A and B be two location instances in KDG(Z). Let ClassA
and ClassB be main classes of A and B respectively.
1. Location A spatially matches with location B with confidence Conf if instance A matches with instance B with confidence Conf .
2. Location A spatially matches with location B with high confidence if one of
the following is true:
(a) B is inside A (the relation inside is encoded in our KB by slot name
is inside).
(b) B is part of A (the relation “part of ” is encoded in our KB by slot name
part of).
3. Location A spatially matches with location B with confidence min(Conf1 , Conf2 )
if
(a) A spatially matches with location C with confidence Conf1 , and
(b) C spatially matches with B with confidence Conf2 .
Suppose that in our KB we have: cytosol234 and cytosol987 all are instances
of cytosol; cytoplasm322 is an instance of cytoplasm and cytosol987 is inside
cytoplasm322. We can then conclude: cytosol234 and cytosol987 match with
each other with low confidence, according to Def.10.3; cytoplasm322 spatially
matches with cytosol987 with high confidence (Def.12.2.a); cytosol987 spatially
matches with cytosol234 with low confidence (Def.12.1); and cytoplasm322 spatially matches with cytosol234 with low confidence (Def.12.3) .

6

Finding the Possible Next Events

In this section, we demonstrate the usefulness of matching instances (Definition
10 and 12) in finding the possible next event(s) of a given event. While in simple
cases (Section 4.1), we can find the next event E’ of an event E by using Definition 5, there are still cases where there exists no ordering edges from E to E 0 .
For examples, Alteration of mrna ends and RNA splicing are two subevents of
RNA processing but no other relation between them was defined. However, they
all occur in nucleus16421 and Alteration of mrna ends’s output, pre mrna7690,
matches with RNA splicing’s input, rna8697. This information hints us that
RNA splicing is Alteration of mrna ends’s next event.
Following this intuition, our approach for finding the next event is that E 0
is the next event of E if the output of E matches the input of E 0 and output

Event-Object Reasoning with Curated Knowledge Bases

9

location of E matches the input location of E 0 . In the example in Figure 2, this
assumption holds in all three events: Eukaryotic transcription, RNA processing
and Move out; all of which are already defined in our KB as consequent events.
This assumption also suggests that Eukaryotic translation can be the next event
of either Synthesis of RNA in eukaryote or Move out. Armed with Definition 10
and 12, we define the following join relation.
Definition 13. Let A and B be two events in KDG(Z). Event A joins to event
B if all of the following conditions are true:
1. The output of A matches with the input of B or vice versa.
2. The output location of A spatially matches with the input location of B or
vice versa.
Applying this definition, we have: Alteration of mrna ends joins to RNA splicing, Eukaryotic transcription joins to RNA processing; RNA processing joins to
Move out; and both Synthesis of RNA in eukaryote and Move out are joined
from Eukaryotic translation. Since we want Eukaryotic translation to be a possible next event of Synthesis of RNA in eukaryote instead of its subevent Move out,
we define the possible next event as follows.
Definition 14. Let A and B be two events in KDG(Z) where A joins to B. B
is a possible next event of A if none of the following conditions is true:
1. A joins to AncestorB where AncestorB is the ancestor event of B (in other
words, there is a non-empty path of subevent relation from AncestorB to
B).
2. Ancestor event AncestorA of A joins to B.
3. A is an ancestor event of B.
4. B is an ancestor event of A.
5. A and B have the same ancestor event.
In our example, condition 14.2 gives us that Eukaryotic translation is not the
possible next event of Move out while 14 concludes that Eukaryotic translation
is the possible next event of Synthesis of RNA in eukaryote. We assume that an
event and its subevents are put in our KB as a whole, so the next event relations
between them are well defined. Thus conditions 14.3-5 take those relations out
of consideration.
When we have a path of possible next events, we can create an event SE,
which is the super event of all events in the path, and add suitable next event or
subevent relations. This new event would link the events that were mistakenly
encoded as independent events (that we mentioned earlier).

7

ASP Encodings

In this section we give ASP encodings of our formulations in the previous sections.
Encoding the Entities and Events: Rules t1-t2 in the following state that
an instance X is an event or an entity if and only if it is the instance of event

10

Chitta Baral and Nguyen H. Vo

class or entity class respectively. Rules t3-t4 identify E as an event if there is
an ordering edge to E (Definition 4.ii). Rule t5 encodes Definition 4.iv. “has(X,
ancestorclass, Y)” denotes the transitive closure of “has(M, superclass, N)” and
is encoded the standard way (rules t6-t7). The rest of Definition 4 are encoded
similarly in rules t8-t21.
t1 :
t2 :
t3 :
t4 :
t5 :

event ( X ) : - has (X , instance_of , event ) .
entity ( X ) : - has (X , instance_of , entity ) .
ordering_edge ( next_event ; enables ; causes ; prevents ; inhibits ) .
has (E , instance_of , event ) : - has (X , S , E ) , ordering_edge ( S ) .
has (E , instance_of , event ) : - has (X , instance_of , ClassY ) , has ( ClassY ,
ancestorclass , event ) .

Finding Next Events, First Subevents and Last Subevents: Rules
e1-e2 find the next events (Definition 5) and rules e3-e6 find the first subevents
and the last subevents (Definition 6).
e1 : predicates ( ordering_edge , enables ; causes ; prevents ; inhibits ) .
e2 : has ( E1 , next_event , E2 ) : - has ( E1 , Predicate , E2 ) ,
predicates ( ordering_edge , Predicate ) .
e3 : not_fse (Z , E ) : - has (Z , subevent , E ) , has (Z , subevent , E2 ) , E2 != E ,
has ( E2 , next_event , E ) .
e4 : not_lse (Z , E ) : - has (Z , subevent , E ) , has (Z , subevent , E2 ) , E2 != E ,
has (E , next_event , E2 ) .
e5 : has (Z , first_subevent , E ) : - has (Z , subevent , E ) , not not_fse (Z , E ) .
e6 : has (Z , last_subevent , E ) : - has (Z , subevent , E ) , not not_lse (Z , E ) .

Encoding Transport Events and Operational Events: t event(E) or
o event(E) is used to indicate a transport event or an operational event, respectively.
ev1 : predicates ( t_event , move_through ; move_into ; move_out_of ) .
ev2 : t_event ( E ) : - has (E , instance_of , T ra ns po r t_ cl a ss ) , predicates ( t_event ,
T ra ns po r t_ cl as s ) , event ( E ) .
ev3 : o_event ( E ) : - event ( E ) , not t_event ( E ) .

Encoding the Inputs and Outputs of Operational Events: We denote the input/output/input location/output location of an event by input,
output, input loc and output loc respectively. Rules i1-i5 get the IOs of operational events. IOs of transport events are encoded similarly (rules i6-i10).
i1 : input (E , A ) : - has (E , object , A ) , o_event ( E ) .
i2 : input (E , A ) : - has (E , base , A ) , o_event ( E ) .
i3 : input (E , A ) : - has (E , raw_material , A ) , o_event ( E ) .
i4 : output (E , A ) : - has (E , result , A ) , o_event ( E ) .
i5 : input_loc (E , A ) : - has (E , site , A ) , o_event ( E ) .

Getting the Missing Inputs and Outputs: Rule i11 gets the input of an
event from its first subevent (Definition 7.1). Rule i12 gets the object property of
a transport event from its first subevent. Other rules to get the input location,
output and output location as well as other properties, such as raw-material,
result, are encoded in a similar way (rules i13-i24). Rule i25 gets the default
output location of an event(Definition 8).
i11 : has (E , input , A ) : - has ( SE , input , A ) , has (E , first_subevent , SE ) .
i12 : has (E , object , A ) : - has ( SE , object , A ) , has (E , first_subevent , SE ) ,
t ra ns po r t_ ev en t ( E ) .
i25 : has (E , output_location , A ) : - not has (E , output_location , A2 ) , has (E ,
input_location , A ) , entity ( A2 ) , event ( E ) , A2 != A .

Event-Object Reasoning with Curated Knowledge Bases

11

Encoding the Main Class(es) of an Instance: ClassA is a main class of
instance A if ClassA is one of A’s classes and we do not have not main class(A, ClassA)
(which mean ClassA is not the main class of A).
m1 : general_class ( thing ; event ; entity ; sp atial_en tity ; t a ng ib l e_ en ti t y ;
c he mi ca l _e nt it y ) .
m2 : n ot_main _class (A , ClassB ) : - has (A , instance_of , ClassA ) , has (A ,
instance_of , ClassB ) , has ( ClassA , ancestorclass , ClassB ) .
m3 : n ot_main _class (A , ClassB ) : - has (A , instance_of , ClassA ) ) , has (A ,
instance_of , ClassB ) , general_class ( ClassB ) , not general_class ( ClassA ) .
m4 : main_class (A , ClassA ) : - has_class (A , ClassA ) , not n ot_main _class (A ,
ClassA ) .

Encoding Instance Matching: We use predicate match with(A, B, Conf idence)
to represent match with relation (Definition 10) from instance A to B; Conf idence
can be either low, medium or high. Rule ma1 encodes the sub-case 10.1.a of Definition 10. The last rule is for Definition 10.4, matching A to B transitively
through C. lowest conf idence(Conf 1, Conf 2, Conf ) means Conf is the lowest
confidence in Conf 1 and Conf 2 (Rules lc1-lc7). Rules for other cases of Definition 10 are skipped (rules ma2-ma5); locational instance matching is encoded in
a similar way (rules sma1-sma4).
ma1 :

match_with (A ,B , high ) : - main_class (A , ClassA ) , main_class (B , ClassB ) ,
A == B .
ma6 : match_with (A ,B , Conf ) : - match_with (A ,C , Conf1 ) , match_with (C ,B , Conf2 ) ,
A != B , A != C , B != C , l o w e s t _ c o n f i d e n c e ( Conf1 , Conf2 , Conf ) .

Encoding Possible-next-event Relation: In this section, we show how
Definition 14 is encoded. We use has(A, tc subevent, B) to represent transitive
closure of sub event relation between A and B (encoded by has(A, subevent, B)),
which is defined in the standard way (rules tcsub1-tcsub3). We also use join(A, B)
to encode that A joins to B according to Definition 13 (rules j1-j3). The two
rules below is corresponding to the sub-case 14.1. Other cases are skipped (rules
n2-n5).
n1 : _notNextEvent (A , B ) : - _join (A , SuperB ) , _join (A , B ) , has ( SuperB ,
tc_subevent , B ) .
n6 : p o s s i b l e _ n e x t _ e v e n t (A , B ) : - _join (A , B ) , not _notNextEvent (A , B ) .

Correctness of the ASP Rules:
Definition 15. The ASP program ΠZ is the answer set program consisting of
the facts of the form “has(X, S, V )” that are generated from all the nodes and
edges of KDG(Z) in the following way:
1. For each node N , generate “has(N, instance of, event)” if N is event node,
“has(N, instance of, entity) if N is entity nodes.
2. For each edge of relation R (Table 1) from E1 to E2, generate ‘has(E1, R, E2)”.
Definition 16. The ASP program Π is the answer set program consisting of
the following rules: t1 to t14 for events and entities, e1 to e6 for next events,
first subevents and last events, ev1 to ev3 for two types of events, i1 to 25 for
inputs, outputs of events, m1 to m4 for main class(es), lc1 to lc7 for the lowest
confidence, ma1 to ma6 for match relation, sma1 to sma4 for spatially match
relation, tcsub1 to tcsub2 for transitive closure of subevents, j1 to j3 for joined
events, and n1 to n6 for possible next events.

12

Chitta Baral and Nguyen H. Vo

Proposition 1: E is the last subevent of X in KDG(Z) iff
ΠZ ∪ Π |= has(Z, last subevent, E)
Proposition 2: A is the main class of E in KDG(Z) iff
ΠZ ∪ Π |= main class(E, A)
Proposition 3: Let A and B be two instances in KDG(Z). A matches with
B with the confidence level Conf iff ΠZ ∪ Π |= match with(A, B, Conf )
Proposition 4: Let A and B be two events in in KDG(Z). A is a possible
next event of B iff ΠZ ∪ Π |= possible next event(A, B)

8

Discussion: Answering “How” and “Why” Questions

In Section 4, we showed how to recover missing information using properties of
KDG’s structure. Completing this information not only allows us to improve the
KB that was used to construct the KDG, but also make it possible to reason
about large curated KB using KDG. In Section 5 and 6, we also solved an
important step in bringing the KB’s usage out of small examples: we proposed the
methods to compare instances and demonstrated their power in finding possible
next events.
Those efforts have enabled us to answer deep reasoning questions, such as
“How” and “Why” questions. We give examples of a few of them in the following.
Details about answering them are explained in another work of ours [2].
1. The answer of “How does X occur?” is simply a structure that basically
contains KDG(X) and all the nodes connected to/from X through ordering
edges.
2. The answer of “How does X produce Y?” is similar to “How does X occur?”
but X must produce Y .
3. The answer of “How are X and Y related?” is a simplified structure of
KDG(Z) that contains: two paths of component edges to X and Y from
their lowest common ancestor and all paths of ordering edges linking two
nodes in those two paths.
4. Similarly, the answer of “Why X is important to Y?” is the answer of “How
are X and Y related?” plus the path on “important” links which explains
why X is important to Y . An “important” link from A to B is defined in
KDG to indicates that A is important to B.
5. Other questions that KDG can answer includes “How does X participate in
process Y?”, “How does X do Y?”, “Why does X produce Y?” and others.

9

Conclusion

In this paper we have shown how to derive certain missing information from large
knowledge bases. Often such knowledge bases are created by multiple people;
sometimes even through crowd-sourcing. This often leads to some information
being not explicitly stated, even though the knowledge base contains clues to
derive that information. In our larger quest to formulate answers to “why” and
“how” questions, we focused on the frame based knowledge base AURA, noticed
several such omissions, and using those as examples, developed several general
formulations regarding missing knowledge about events. We also gave an ASP

Event-Object Reasoning with Curated Knowledge Bases

13

implementation of our formulations and used them in answering “why” and
“how” questions. We briefly discussed some of those question types and how their
answer can be obtained from Knowledge Description Graphs (KDGs). Thus, by
being able to obtain missing information and enriching the original KDGs one
can obtain more accurate and intuitive answers to the various ‘why” and “how”
questions.
One of our formulations was about entity resolution where we resolve multiple
entities that may have different names but may refer to the same entity. Our
method is different from other methods in the literature [4, 5]. Since each entity
resolution method heavily relies on the properties of the database it is working
on, and no other system we know of is about AURA or similar event centered
knowledge bases we were unable to directly compare our method with the others.
Our approach to use rules (albeit ASP rules) to derive missing information is
analogous to use of rules in data cleaning and in improving data quality [10–12].
However those works do not focus on issues that we discussed in this paper.

References
1. Baral, C., Vo, N.H., Liang, S.: Answering why and how questions with respect to
a frame-based knowledge base: a preliminary report. Technical Communications
of the 28th International Conference on Logic Programming (ICLP’12) 17 (2012)
26–36
2. Baral, C., Vo, N.: Formulating question answering with respect to event-object
description graphs. Unpublished paper submitted to a conference (2013)
3. Chaudhri, V.K., Clark, P.E., Mishra, S., Pacheco, J., Spaulding, A., Tien, J.:
AURA: capturing knowledge and answering questions on science textbooks. Technical report, SRI International (2009)
4. Getoor, L., Diehl, C.P.: Link mining: a survey. ACM SIGKDD Explorations
Newsletter 7(2) (2005) 3–12
5. Brizan, D.G., Tansel, A.U.: A survey of entity resolution and record linkage
methodologies. Communications of the IIMA 6(3) (2006) 41–50
6. Baral, C., Liang, S.: From knowledge represented in frame-based languages to
declarative representation and reasoning via ASP. 13th International Conference
on Principles of Knowledge Representation and Reasoning (2012)
7. Gelfond, M., Lifschitz, V.: The stable model semantics for logic programming. In
Kowalski, R., Bowen, K., eds.: Logic Programming: Proc. of the Fifth Int’l Conf.
and Symp., MIT Press (1988) 1070–1080
8. Baral, C.: Knowledge representation, reasoning and declarative problem solving.
Cambridge University Press (2003)
9. Clark, P., Porter, B., Works, B.: KM: The knowledge machine 2.0: Users manual.
Citeseer (2004)
10. Herzog, T.N., Scheuren, F.J., Winkler, W.E.: Data quality and record linkage
techniques. Springer (2007)
11. Rahm, E., Do, H.H.: Data cleaning: Problems and current approaches. IEEE Data
Engineering Bulletin 23(4) (2000) 3–13
12. Fan, W., Geerts, F., Jia, X.: Conditional dependencies: A principled approach to
improving data quality. In: Dataspace: The Final Frontier. Springer (2009) 8–20

Addressing a Question Answering Challenge by Combining Statistical Methods
with Inductive Rule Learning and Reasoning
Arindam Mitra

Chitta Baral

Arizona State University
amitra7@asu.edu

Arizona State University
chitta@asu.edu

Abstract
A group of researchers from Facebook has recently proposed a set of 20 question-answering tasks (Facebook’s
bAbl dataset) as a challenge for the natural language understanding ability of an intelligent agent. These tasks
are designed to measure various skills of an agent, such
as: fact based question-answering, simple induction, the
ability to find paths, co-reference resolution and many
more. Their goal is to aid in the development of systems that can learn to solve such tasks and to allow a
proper evaluation of such systems. They show existing
systems cannot fully solve many of those toy tasks. In
this work, we present a system that excels at all the tasks
except one. The proposed model of the agent uses the
Answer Set Programming (ASP) language as the primary knowledge representation and reasoning language
along with the standard statistical Natural Language
Processing (NLP) models. Given a training dataset containing a set of narrations, questions and their answers,
the agent jointly uses a translation system, an Inductive Logic Programming algorithm and Statistical NLP
methods to learn the knowledge needed to answer similar questions. Our results demonstrate that the introduction of a reasoning module significantly improves the
performance of an intelligent agent.

tasks where solving each task develops a new skill set into
an agent.
In the following paragraph, we provide some examples of
the tasks from (Weston et al. 2015). A detailed description
of all the tasks can be found there. Each task is noiseless,
provides a set of training and test data and a human can potentially achieve 100% accuracy.
Example 1. Task 8: List/Sets
Mary grabbed the football.
Mary traveled to the office.
Mary took the apple there.
What is Mary carrying? A:football,apple
Mary left the football.
Daniel went back to the bedroom.
What is Mary carrying? A:apple
Example 2. Task 19: Path Finding
The office is east of the hallway.
The kitchen is north of the office.
The garden is west of the bedroom.
The office is west of the garden.
The bathroom is north of the garden.
How do you go from the kitchen to the garden? A:s,e

Developing intelligent agents is one of the long term goals
of Artificial Intelligence. To track the progress towards
this goal, several challenges have been recently proposed
that employs a Question-Answering (QA) based strategy to
test an agent’s understanding. The Allen Institute for AI’s
flagship project ARISTO, (Richardson, Burges, and Renshaw 2013)’s MCTest and the Winograd Schema Challenge
(Levesque, Davis, and Morgenstern 2012) are all examples
of this. As mentioned in the work of (Weston et al. 2015),
even though these tasks are promising and provide real
world challenges, successfully answering their questions require competence on many sub-tasks (deduction, use of
common-sense, abduction, coreference etc.); which makes
it difficult to interpret the results on these benchmarks. Often the state-of-the-art systems are highly domain specific
and rely heavily on the prior knowledge. In this light, they
(Weston et al. 2015) have proposed a new dataset (Facebook
bAbl dataset) that put together several question-answering

In this work, we describe an agent architecture that simultaneously works with a formal reasoning model and a statistical inference based model to address the task of questionanswering. Human beings in their lifetime learn to perform
various tasks. For some tasks they may have a clear reasoning behind their actions. For example, the knowledge needed
to answer the previous question “What is Mary carrying?”
is clear and can be described formally. On the other hand,
there are tasks such as Named Entity Recognition that we
can do easily, however, we may not be able to describe it
well enough for anyone else to use the description for recognition. In these cases, a statistical inference model that allows to learn by observing a distribution may be a better fit.
In this research, thus, we work with a heterogeneous agent
model. In our current implementation, the agent model contains three layers.

c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

Statistical Inference Layer This layer contains statistical
NLP models. In this case study, it contains only an Ab-

stract Meaning Representation Parser (AMR) (Banarescu
et al. 2013; Flanigan et al. 2014).
Formal Reasoning Layer This layer is responsible for formal reasoning. It uses the Answer Set Programming
(ASP) (Gelfond and Lifschitz 1988) language as the
knowledge representation and reasoning language. The
knowledge required for reasoning is learned with a modified version of the Inductive Logic Programming algorithm XHAIL (Ray 2009). The reasoning module takes
sentences represented in the logical language of Event
calculus which is a temporal logic for reasoning about the
events and their efforts. The ontology of the Event calculus comprises of time points, fluent (i.e. properties which
have certain values in time) and event (i.e. occurrences
in time that may affect fluents and alter their value). The
formalism also contains two domain-independent axioms
to incorporate the commonsense law of inertia, according
to which fluents persist over time unless they are affected
by an event. The building blocks of Event calculus and its
domain independent axioms are presented in Table 1.
Translation layer The translation layer encodes the natural
language sentences to the syntax of Event calculus with
the help of the AMR parser from the statistical inference
layer. This layer communicates with both the other layers and allows information to be passed from one layer to
another. In this case study, we use a naive deterministic
algorithm to model the translation layer.
Predicate
happensAt(F, T )
initiatedAt(F, T )
terminatedAt(F, T )
holdsAt(F, T )
Axioms
holdsAt(F, T + 1)
← initiatedAt(F, T ).

Meaning
Event E occurs at time T
At time T a period of time
for which fluent F holds is
initiated
At time T a period of time
for which fluent F holds is
terminated
Fluent F holds at time T
holdsAt(F, T + 1) ←
holdsAt(F, T ),
not terminatedAt(F, T ).

Table 1: The basic predicates and axioms of Simple Discrete
Event Calculus (SDEC)
Given a question-answer text such as the one shown in
Example 1 (Task 8), the translation module first converts the
natural language sentences to the syntax of Event calculus.
While doing so, it first obtains the Abstract Meaning Representation (AMR) of the sentence from the AMR parser in
the statistical NLP layer and then applies a rule-based procedure to convert the AMR graph to the syntax of Event
calculus. Figure 1 & 2 show two AMR representations for
the sentence “Mary grabbed the football.” and the question
“What is Marry carrying?”. The representation of the sentences (narratives) and the question-answer pairs (annotation) of Example 1 in Event calculus is shown in Table 2.
The narratives in Table 2 describe that the event of grabbing

a football by Mary has happened at time point 1, then another event named travel has happened at time point 2 and
so on. The first two annotations state that both the fluents
specifying Mary is carrying an apple and Mary is carrying a
football holds at time point 3. The not holdsAt annotation
states that at time point 7 Mary is not carrying a football.
Given such a set of narratives and annotations the reasoning
module employs an Inductive Logic Programming algorithm
to derive a Hypothesis H, that can explain all the annotations. Given more examples it updates the existing Hypothesis H incrementally to remain consistent with the data.
The rest of the paper is organized as follows: in section
1, we provide a brief overview of Answer Set Programming
and Inductive Logic Programming; In section 2, we describe
the way the task specific ASP reasoning rules are learned.
Section 3 presents training of the coreference resolution system with reasoning. In section 4, we describe the related
works. In section 5, we present a detailed experimental evaluation of our system. Finally, section 6 concludes our paper.
Further details are available at http://goo.gl/JMzHbG.

Figure 1: AMR representation of “Mary grabbed the football.”

Figure 2: AMR representation of “What is Marry carrying?”
Narrative
happensAt(grab(mary,football),1).
happensAt(travel(mary,office),2).
happensAt(take(mary,apple),3).
happensAt(leave(mary,footbal;),5).
happensAt(go back(daniel,bedroom),6).
Annotation
holdsAt(carry(mary,football),4).
holdsAt(carry(mary,apple),4).
holdsAt(carry(mary,apple),7).
not holdsAt(carry(mary,football),7).
Table 2: Representation of the Example 1 in Event Calculus

1

Background

Answer Set Programming
An answer set program is a collection of rules of the form,
L0 ← L1 , ..., Lm , not Lm+1 , ..., not Ln
where each of the Li ’s is a literal in the sense of a classical
logic. Intuitively, the above rule means that if L1 , ..., Lm are

true and if Lm+1 , ..., Ln can be safely assumed to be false
then L0 must be true (Baral 2003) . The left-hand side of an
ASP rule is called the head and the right-hand side is called
the body. The semantics of ASP is based on the stable model
(answer set) semantics of logic programming (Gelfond and
Lifschitz 1988).
Example
initiatedAt(carry(A, O), T ) ←
happensAt(take(A, O), T ).

(Table 3). A modeb(s) declaration denote a literal s that can
appear in the body of a rule (Table 3). The argument s is
called schema and comprises of two parts: 1) an identifier
for the literal and 2) a list of placemakers for each argument
of that literal. A placemaker is either +type (input), -type
(output) or #type (constant), where type denotes the type
of the argument. An answer set rule is in the hypothesis
space defined by L (call it L(M)) iff its head (resp. each
of its body literals) is constructed from the schema s in a
modeh(s) (resp. in a modeb(s)) in L(M)) as follows:

(1)

- By replacing an output (-) placemaker by a new variable.

The above rule represents the knowledge that the fluent
carry(A, O), denoting A is carrying O, gets initiated at
time point T if the event take(A, O) occurs at T . Following Prolog’s convention, throughout this paper, predicates
and ground terms in logical formulae start with a lower
case letter, while variable terms start with a capital letter.
A rule with no head is often referred to as a constraint.
A rule with empty body is referred to as a f act. An answer set program P containing the above rule (Rule 1) and
the axioms of Event calculus (from Table 1) along with the
fact happensAt(take(mary, f ootball), 1) logically entails
(|=) that mary is carrying a football at time point 2 i.e.
holdsAt(carry(mary, f ootball), 2). Since it can be safely
assumed that mary is not carrying a football at time point
1, P |= not holdsAt(carry(mary, f ootball), 1) or equivalently P 6|= holdsAt(carry(mary, f ootball), 1).
It should be noted that it is also true that P |=
holdsAt(carry(mary, f ootball), 3), due to the axioms in
Table 1. However, if we add the following two rules in the
program P :

- By replacing an input (+) placemaker by a variable that
appears in the head or in a previous body literal.

terminatedAt(carry(A, O), T ) ←
happensAt(drop(A, O), T ).
happensAt(drop(marry, f ootball), 2).

- By replacing a ground (#) placemaker by a ground term.
modeh(initiatedAt(carrying(+arg1 ,+arg3 ),+time))
modeh(terminatedAt(carrying(+arg1 ,+arg3 ),+time))
modeb(happensAt(grab(+arg1 ,+arg2 ),+time))
modeb(happensAt(take(+arg1 ,+arg3 ),+time))
modeb(happensAt(go back(+arg1 ,+arg2 ),+time))
modeb(happensAt(leave(+arg1 ,+arg3 ),+time))
Table 3: Mode declarations for the problem of Task 8
Table 3 shows a set of mode declarations M for the example problem of Task 8. The Rule 1 of the previous section
is in this L(M) and so is the fact,
initiated(carrying(A, O), T ).
However the following rule is not in L(M)).
initiated(carrying(A, O), T ) ←

(2)
(3)

then the new program P will no longer entail
holdsAt(carry(mary, f ootball), 3) due the axioms
of Event calculus. This is an example of non-monotonic
reasoning when adding more knowledge changes one’s
previous beliefs and such thing is omnipresent in human
reasoning. First Order Logic does not allow non-monotonic
reasoning and this is one of the reasons why we have
used the Answer Set Programming language as the formal
reasoning language.

Inductive Logic Programming

happensAt(take(A, O), T 0 ).
The set E − is required to restrain H from being over generalized. Informally, given a ILP task, an ILP algorithm finds
a hypothesis H that is general enough to cover all the examples in E + and also specific enough so that it does not cover
any example in E − . Without E − , the learned H will contain
only facts. In this case study, negative examples are automatically generated from the positive examples by assuming the
answers are complete, i.e. if a question-answer pair says that
at a certain time point mary is carrying a football we assume
that mary is not carrying anything else at that time stamp.

2

Learning Answer Set Programs for QA

Inductive Logic Programming (ILP) (Muggleton 1991) is
a subfield of Machine learning that is focused on learning
logic programs. Given a set of positive examples E + ,
negative examples E − and some background knowledge
B, an ILP algorithm finds an Hypothesis H (answer set
program) such that B ∪ H |= E + and B ∪ H 6|= E − .

In this section, we illustrate the formulation of an ILP task
for a QA task and the way the answer set programs are
learned. We explain our approach with the XHAIL (Ray
2009) algorithm and specify why ILED (Katzouris, Artikis,
and Paliouras 2015) algorithm is needed. We continue with
the example of Task 8 and conclude with path finding.

The possible hypothesis space is often restricted with
a language bias that is specified by a series of mode
declarations M (Muggleton 1995). A modeh(s) declaration
denotes a literal s that can appear as the head of a rule

Task 8: Lists/Sets
Given an ILP task ILP (B, E = {E + ∪ E − }, M), XHAIL
derives the hypothesis in a three step process. For the example of task 8, B contains both the axioms of SDEC and the

narratives from Table 1. The set E comprises of the annotations from Table 1 which contains three positive and one
negative examples. M is the set of mode declarations in Table 2.
Step 1 In the first step the XHAIL algorithm finds a set
of ground (variable free) atoms 4 = ∪ni=1 αi such that
B ∪ 4 |= E where each αi is a ground instance of the
modeh(s) declaration atoms. For the example ILP problem
of task 8 there are two modeh declarations. Thus the set 4
can contain ground instances of only those two atoms described in two modeh declarations. In the following we show
one possible 4 that meets the above requirements for the
ILP task of Example 1.


 initiatedAt(carry(mary, f ootball), 1) 
4 = initiatedAt(carry(mary, apple), 3)


terminatedAt(carry(mary, f ootball), 5)
Step 2 In the second step, XHAIL computes a clause
αi ← δi1 ...δimi for each αi in 4, where B ∪ 4 |=
δij , ∀1 ≤ i ≤ n, 1 ≤ j ≤ mi and each clause αi ←
δi1 ...δimi is a ground instance of a rule in L(M). In the
running example, 4 contains three atoms that each must
lead to a clause ki , i = 1, 2, 3. The first atom α1 =
initiatedAt(carry(mary, f ootball), 1) is initialized to the
head of the clause k1 . The body of k1 is saturated by adding
all possible ground instances of the literals in modeb(s) declarations that satisfy the constraints mentioned above. There
are six ground instances (all the narratives) of the literals in
the modeb(s) declarations; however only one of them, i.e.
happensAt(grab(mary, f ootball), 1) can be added to the
body due to restrictions enforced by L(M). In the following we show the set of all the ground clauses K constructed
in this step and their variabilized version Kv that is obtained
by replacing all input and output terms by variables.


initiatedAt(carry(mary, f ootball), 1)







← happensAt(grab(mary, f ootball), 1).






 initiatedAt(carry(mary, apple), 3)

K=

← happensAt(take(mary, apple), 3). 








terminatedAt(carry(mary,
f
ootball),
6)






← happensAt(leave(mary, apple), 6).


initiatedAt(carry(X, Y ), T )








←
happensAt(grab(X,
Y
),
T
).






 initiatedAt(carry(X, Y ), T )

Kv =

← happensAt(take(X, Y ), T ). 







 terminatedAt(carry(X, Y ), T ) 





← happensAt(leave(X, Y ), T ).
Step 3 In this step XHAIL tries to find a compressive theory H by deleting from Kv as many literals (and clauses)
as possible while ensuring that B ∪ H |= E. In the running
example, working out this problem will lead to H = Kv .

Scalability of the learning algorithm The discovery of a
hypothesis H depends on the choice of 4. Since the value
of 4 that satisfies the constraints described in Step 1 is not
unique, we employ an iterative deepening strategy to select 4 of progressively increasing size until a solution is
found. Furthermore, in Step 2 of XHAIL we restricted the
algorithm to consider only those ground instances of modeb
declarations that are not from the future time points. This
method works when the size of the example is small. However, the dataset of Task 8 like other tasks contains 1000 examples, where each example comprises of a set of narrative
and annotations (as we have shown before) and the choice
of 4 will be numerous. This issue is addressed by learning
rules from each example and then using the learned rules to
learn new rules from yet unsolved examples. A recent incremental learning algorithm, ILED (Katzouris, Artikis, and
Paliouras 2015) can be used to address the scalability issue.
The first step of the ILED algorithm is the XHAIL algorithm. After finding an initial hypothesis H1 by XHAIL, the
ILED algorithm incrementally revises the current hypothesis
Hi when subjected to a new example Ei so that the revised
hypothesis Hi+1 is consistent with the current example Ei
and all the previous ones E0 , ..., Ei−1 . It will be interesting
to see if ILED can scale up to this dataset.

Task 19 : Path Finding
In this task (Example 2), each example first describes the
relative positions of several places and then asks a question
about moving from one place to another. The answer to the
question is then a sequence of directions. For the question
“How do you go from the kitchen to the garden?” in Example 2, the answer “s,e” tells that to reach garden from
kitchen, you should first head south and then head east.
Given such an example, an agent learns how moving towards a direction changes its current location with respect to
the particular orientation of the places. Let us say, mt(X, Y )
denotes the event of X moving towards the direction Y. Similar to the earlier problem, the natural language text is first
translated to the syntax of ASP (Table 4). However, in this
task the background knowledge B also contains the rules
learned from the task 4. In the following we show an example of such rules:
holdsAt(relative position(X, Y, east), T ) ←
holdsAt(relative position(Y, X, west), T ).
The above rule says that if Y is to the west of X at time
point T then X is to the east of Y at T. Similar rules were
learned for each direction pair from the Task 4 which were
used in the process of hypothesis generation for the task of
path finding. Table 4 shows the corresponding ILP task for
the example of path finding and the hypothesis generated by
the XHAIL algorithm. This example illustrates how the task
of path finding can be easily learned when a formal representation is used. While the state-of-the-art neural network
based systems have achieved 36% accuracy on this task with
an average of 93% on all tasks, our system is able to achieve
100% with the two compact rules shown in Table 4.

Input
Narrative
holdsAt(relative position(office,hallway,east),1).
holdsAt(relative position(kitchen,office,north),2).
holdsAt(relative position(garden,bedroom,west),3).
holdsAt(relative position(office,west,garden),4).
holdsAt(relative position(bathroom,garden,north),5).
holdsAt(location(you,kitchen),6).
happensAt(mt(you,south),6).
happensAt(mt(you,east),7).
Annotation
not holdsAt(location(you, garden), 6).
holdsAt(location(you, garden), 8).
not holdsAt(location(you, kitchen), 8).
Mode declarations
modeh(initiatedAt(location(+arg1 ,+arg2 ),+time))
modeh(terminatedAt(carrying(+arg1 ,+arg2 ),+time))
modeb(happensAt(mt(+arg1 ,+direction),+time))
modeb(holdsAt(location(+arg1 ,+arg2 ),+time))
modeb(holdsAt(relative position(+arg2 ,+arg2 ,
+direction),+time))
Background Knowledge
Axioms of SDEC (Table 1)
Output
initiatedAt(location(X, Y ), T ) ←
happensAt(move towards(X, D), T ),
holdsAt(relative position(Y, Z, D), T ),
holdsAt(location(X, Z), T ).
terminatedAt(location(X, Y ), T ) ←
happensAt(mt(X, D), T ).
Table 4: Hypothesis Generation For Path Finding

3

Learning Coreference Resolution with
Reasoning

The dataset contains contains two tasks related to coreference resolution : 1) task of basic coreference resolution and
2) task of compound coreference resolution. Examples of
the tasks are shown below :
Task 11: Basic Coreference
Mary went back to the bathroom.
After that she went to the bedroom.
Daniel moved to the office.
Where is Mary? bedroom
Task 13: Compound Coreference
Daniel and Sandra journeyed to the office.
Then they went to the garden.
Sandra and John travelled to the kitchen.
The office is west of the garden.
After that they moved to the hallway.
Where is Daniel? A:garden
We formulate both the coreference resolution tasks as ILP
problems and surprisingly it learns answer set rules that can

fully explain the test data. For the task of basic coreference,
it learns a total of five rules one for each of the five different
events go, travel, go back, move, journey that appeared in the
training data. The rule corresponding to the event go (Table
5) states that if a narrative at time point T + 1 contains a pronoun, then the pronoun is referring to the arg1 (agent) of the
event go that happened at time point T . Similar rules were
learned for the other four events. Here, coref Id(X, T ) denotes that the pronoun with id X has appeared in a narrative
at time point at T + 1.
initiatedAt(coref (X, Y ), T ) ← coref Id(X, T ),
happensAt(go(Y, Z), T ).
Table 5: One rule for coreference resolution
One drawback of the learned rules is, they are event dependent, i.e. if a coreference resolution text contains a pronoun which is referring to an argument of an previously unseen event, these rules will not be able to resolve the coreference. In spite of that, these rules reflect one of the basic
intuitions behind coreference resolution and all of them are
learned from data.

4

Related Works

In this section, we briefly describe the two other attempts
on this challenge. The attempt using Memory Network
(MemNN) (Weston, Chopra, and Bordes 2014) formulates
the QA task as a search procedure over the set of narratives.
This model takes as input the Question-Answering samples
and the set of facts required to answer each question. It then
learns to find 1) the supporting facts for a given question and
2) the word or set of words from the supporting facts which
are given as answer. Even though this model performs well
on average, the performance on the tasks of positional reasoning (65%) and path finding (36%) are far below from the
average (93%).
The attempt using Dynamic Memory Network (DMN)
(Kumar et al. 2015) also models the the QA task as a search
procedure over the set of narratives. The major difference
being the way supporting facts are retrieved. In the case of
the Memory Networks, given a question, the search algorithm scans the narratives in the reverse order of time and
finds the most relevant hypothesis. It then tries to find the
next most relevant narrative and the process continues until a special marker narrative is chosen to be the most relevant one in which case the procedure terminates. In the case
of Dynamic Memory Networks the algorithm first identifies a set of useful narratives conditioning on the question
and updates the agent’s current state. The process then iterates and in each iterations it finds more useful facts that
were thought to be irrelevant in the previous iterations. After
several passes the module finally summarizes its knowledge
and provides the answer. Both the models rely only on the
given narratives to answer a question. However, for many
QA tasks (such as task of Path finding) it requires additional
knowledge that is not present in the text (for path finding,
knowledge from Task 4), to successfully answer a question.

Both MemNN and DMN models suffer in this case whereas
our method can swiftly combine knowledge learned from
various tasks to handle more complex QA tasks.

5

Experiments

Table 6 shows the performance of our method on the set of
20 tasks. For each task, there are 1000 questions for training
and 1000 for testing. Our method was able to answer all the
question correctly except the ones testing basic induction.
In the following we provide a detail error analysis for the
task of Induction. For each task the modeh and the modeb
declarations were manually defined and can be found at the
project website. The test set of Task 5 (Three argument relations) contains 2 questions that have incorrect answers. The
result is reported on the corrected version of that test set.
The details on the error can be found on the project website.
Training of the tasks that are marked with (*) used the annotation of supporting facts present in the training dataset.
TASK
1: Single Supporting Fact
2: Two Supporting Facts
3: Three Supporting facts
4: Two Argument Relations
5: Three Argument Relations
6: Yes/No Questions
7: Counting
8: Lists/Sets
9: Simple Negation
10: Indefinite Knowledge
11: Basic Coreference
12: Conjunction
13: Compound Coreference
14: Time Reasoning
15: Basic Deduction
16: Basic Induction
17: Positional Reasoning∗
18: Size Reasoning
19: Path Finding
20: Agent’s Motivations∗
Mean Accuracy(%)

MemNN

DMN

100
100
100
100
98
100
85
91
100
98
100
100
100
99
100
100
65
95
36
100
93.3

100
98.2
95.2
100
99.3
100
96.9
96.5
100
97.5
99.9
100
99.8
100
100
99.4
59.6
95.3
34.5
100
93.6

Our
Method
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
93.6
100
100
100
100
99.68

Table 6: Performance on the set of 20 tasks

Error Analysis for Basic Induction This task tests basic
induction via potential inheritance of properties. The dataset
contains a series of examples like the one described below:
Lily is a frog.
Julius is a swan.
Julius is green.
Lily is grey.
Greg is a swan.
What color is Greg? green
The learning algorithm could not find a hypothesis that can
characterize the entire training data with the given set of

mode declarations. So, we took the hypothesis that partially
explained the data. This was obtained by ignoring the examples in the training data which resulted in a failure. The
resulted hypothesis then contained the following single rule:
holdsAt(color(X, C), T ) ← holdsAt(domain(Z, D), T ),
holdsAt(color(Z, C), T ),
holdsAt(domain(X, D), T ).
The above rule says that X has color C at time T if there
exists a Z which is of type D and has color C at time point
T, where X is also of type D. This rule was able to achieve
93.6% accuracy on the test set. However it failed for the examples of following kind where there are two different entity
of type D having two different colors:
Error Case 1
Lily is a frog.
Brian is a frog.
Greg is frog.
Lily is yellow.
Julius is a frog.
Brian is grey.
Julius is grey.
Greg is grey.
Bernhard is a frog.
What color is Bernhard? A:grey

Error Case 2
Lily is a rhino.
Lily is yellow.
Bernhard is a frog.
Bernhard is white.
Brian is a rhino.
Greg is a rhino.
Greg is yellow.
Julius is a rhino.
Julius is green.
What color is Brian?
A:green

Table 7: Failure cases for Induction
For the error case 1, the learned rule will produce two answers stating that Bernhard has the color grey and yellow.
Since, the more number of frogs are grey it may seem like
the correct rule should produce the color that has appeared
maximum number of times for that type (here, frog). However, error case 2 describes a complete opposite hypothesis.
There are two yellow rhino and one grey rhino and the color
of Brian which is a rhino is grey. The actual rule as it appears
is the one that determines the color on the basis of the latest evidence. Since, Memory Networks scans the facts in the
deceasing order of time, it always concludes from the recent
most narratives and thus has achieved a 100% accuracy.

6

Conclusion

This paper presents a learning approach for the task of
Question-Answering that benefits from the field of knowledge representation and reasoning, inductive logic programming and statistical natural language processing. We have
shown that the addition of a formal reasoning layer significantly increases the reasoning capability of an agent. We
anticipate our attempt to be a starting point for more sophisticated architectures of agents that combine statistical NLP
methods with formal reasoning methods.

7

Acknowledgement

We thank NSF for the DataNet Federation Consortium grant
OCI-0940841 and ONR for their grant N00014-13-1-0334
for partially supporting this research.

References
Banarescu, L.; Bonial, C.; Cai, S.; Georgescu, M.; Griffitt,
K.; Hermjakob, U.; Knight, K.; Koehn, P.; Palmer, M.; and
Schneider, N. 2013. Abstract meaning representation for
sembanking.
Baral, C. 2003. Knowledge representation, reasoning and
declarative problem solving. Cambridge university press.
Flanigan, J.; Thomson, S.; Carbonell, J.; Dyer, C.; and
Smith, N. A. 2014. A discriminative graph-based parser
for the abstract meaning representation.
Gelfond, M., and Lifschitz, V. 1988. The stable model semantics for logic programming. In ICLP/SLP, volume 88,
1070–1080.
Katzouris, N.; Artikis, A.; and Paliouras, G. 2015. Incremental learning of event definitions with inductive logic programming. Machine Learning 100(2-3):555–585.
Kumar, A.; Irsoy, O.; Su, J.; Bradbury, J.; English, R.; Pierce,
B.; Ondruska, P.; Gulrajani, I.; and Socher, R. 2015. Ask me
anything: Dynamic memory networks for natural language
processing. arXiv preprint arXiv:1506.07285.
Levesque, H. J.; Davis, E.; and Morgenstern, L. 2012. The
winograd schema challenge. In KR.
Muggleton, S. 1991. Inductive logic programming. New
generation computing 8(4):295–318.
Muggleton, S. 1995. Inverse entailment and progol. New
generation computing 13(3-4):245–286.
Ray, O. 2009. Nonmonotonic abductive inductive learning.
Journal of Applied Logic 7(3):329–340.
Richardson, M.; Burges, C. J.; and Renshaw, E. 2013.
Mctest: A challenge dataset for the open-domain machine
comprehension of text. In EMNLP, volume 1, 2.
Weston, J.; Bordes, A.; Chopra, S.; and Mikolov, T. 2015.
Towards ai-complete question answering: a set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.
Weston, J.; Chopra, S.; and Bordes, A. 2014. Memory networks. arXiv preprint arXiv:1410.3916.

From Images to Sentences through Scene Description Graphs
using Commonsense Reasoning and Knowledge

arXiv:1511.03292v1 [cs.CV] 10 Nov 2015

†

Somak Aditya† , Yezhou Yang*, Chitta Baral† , Cornelia Fermuller* and Yiannis Aloimonos*
School of Computing, Informatics and Decision Systems Engineering, Arizona State University
*
Department of Computer Science, University of Maryland, College Park

Abstract

the scene in view. In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes
[13, 26, 11, 49, 42, 43, 50] and activities are recognized
by detecting the motions, objects and contexts involved in
the activities [27, 32, 45, 17, 33, 46].
Recently, researchers have advanced the viewpoint that
if we are able to develop a semantic understanding of a visual scene, then we should be able to produce natural language descriptions of such semantics. This has given rise
to a new area in the field that integrates vision, knowledge
and natural language. Knowledge becomes especially important, as without background knowledge, it has become
increasingly hard to obtain a desirable level of accuracy in
this problem. And as such knowledge can be often mined
from text, the problem now stands at the intersection between Computer Vision and Natural Language Processing.
Mining such knowledge, storing it in a form that retains the
semantics, and reasoning using this knowledge to develop
a better understanding of scenes are the fundamental issues
that are addressed in this paper.
Current developments [31, 22, 8, 21, 44, 3] in Computer
Vision have shown that deep neural nets can be trained to
generate a caption for an arbitrary scene with decent success. It is indeed an exciting achievement. However, current state-of-the-art image captioning systems still have a
few drawbacks such as: 1) a brute-force image-to-text mapping makes it inconvenient to conduct Logical Reasoning
beyond just doing inferences from annotated data; 2) due
to the lack of intermediate semantic representations, they
are all language-dependent; and 3) most importantly, when
the system produces wrong results, it is almost impossible
to trace back the system and analyze the failure case (See
Figure 1).
Let us consider how humans accomplish this task. Human perception is active, selective and exploratory. We continuously shift our gaze to different locations in the scene.
After recognizing objects, we fixate again at a new location,
and so on. We interpret visual input by using our knowledge
of activities, events and objects. When we analyze a vi-

In this paper we propose the construction of linguistic
descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes
using an automatically constructed knowledge base. SDGs
are constructed using both vision and reasoning. Specifically, commonsense reasoning1 is applied on (a) detections obtained from existing perception methods on given
images, (b) a “commonsense” knowledge base constructed
using natural language processing of image annotations
and (c) lexical ontological knowledge from resources such
as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show
that in most cases, sentences auto-constructed from SDGs
obtained by our method give a more relevant and thorough
description of an image than a recent state-of-the-art image
caption based approach. Our Image-Sentence Alignment
Evaluation results are also comparable to that of the recent
state-of-the art approaches.

1. Introduction
“Imagine, for example, a computer that could look at an
arbitrary scene, anything from a sunset at a fishing village
to Grand Central Station at rush hour and produce a verbal
description. This is a problem of overwhelming difficulty,
relying as it does to finding solutions to both vision and
language and then integrating them. I suspect that scene
analysis will be one of the last cognitive tasks to be performed well by computers”. This fifteen year old quote,
attributed to A. Rosenfeld [41], one of the founders of the
field of Computer Vision, pointed to the fundamental problem of generating semantics of visual scenes. Since then,
researchers have attempted a few approaches that mostly
centered on asking “what” and “where” questions about
1 Commonsense

reasoning and commonsense knowledge can be of
many types [6]. Commonsense knowledge can belong to different levels
of abstraction [18, 28]. In this paper, we focus on capturing and reasoning
based on knowledge about natural activities.

1

(a)

(b)

Figure 1: Examples from [21]: (a) Positive example annotation:
construction worker in orange safety vest is working on road, (b)
Negative example annotation: a bunch of bananas are hanging
from a ceiling. Such annotations could be infrequent, but it is hard
to logically justify such contrasting outputs.

sual scene, visual processes continuously interact with our
high-level knowledge, some of which is represented in the
form of language. In some sense, perception and language
are engaged in an interaction, as they exchange information
that leads to meaning and understanding. Thus, our problem requires at least two modules for its solution: (a) a vision module and (b) a reasoning module that are interacting
with each other. In this paper we propose to model the early
stages of this process. The available datasets make it impossible to perform experiments that will consider vision as an
active process, although this is the ultimate goal. Thus, our
question becomes: if the vision module produces a number
of (probabilistic) detections, how much the reasoning module can infer about the scene if it possesses common sense
abilities? It turns out that the reasoning module can infer a
great deal.
Motivated by such intuitions, we present here an effort
to integrate deep learning based vision and state-of-the-art
concept modeling from commonsense knowledge obtained
from text. We use a deep learning-based perception system
to obtain the objects, scenes and constituents with probabilistic weights from an input image. To predict how the objects interact in the scene, we build a common-sense knowledge base from image annotations along with a Bayesian
Network capturing dependencies among commonly occurring objects and “Abstract Visual Concepts” (defined later).
These two precomputed resources help us infer the following: 1) the correct set of correlated objects based on
the high-confidence objects detected; 2) the most probable
events that these objects participate in; 3) the role that the
objects play in this event; and 4) given the events, objects
and constituents, the “Concept” that emerges from such information. Based on these inferences, we output a Scene
Description Graph (SDG) that depicts how these different
entities and events interact. In Figure 2, we show a possible
SDG for an example image. SDG is essentially a directed
labeled graph among entities and events2 that enables an
array of possibilities to do further analysis beyond visual
appearance, such as Event-Entity based analysis, question
2 Throughout this paper, we follow definition of Entities and Events
from [16, 2].

answering about the scene and flexible caption generation3 .
The fundamental contribution of this work is a novel
algorithm that uses automatically constructed Knowledge
Base to create an SDG from an image, which facilitates further reasoning and caption generation. SDGs have advantages over ground-truth sentences because : 1) they can be
easily processed by machines/AI systems in comparison to
sentences; 2) the output can be rich in information-content;
3) they are not bounded by specific templates, that are often used by researchers to convert labels into sentences and
4) the SDG also can be used to generate sentence descriptions. We also create a Knowledge Base which captures the
knowledge about the commonly-occurring Concepts, events
and entities. The knowledge base can be used to provide answers to the following queries: 1) the event or set of events
that connect two entities; 2) the role an entity plays in an
event and 3) a subset of all possible concepts involving the
entities and connecting events. Lastly, further inferences
about the scenes such as “Will the player holding the ball
be able to tackle the blocker and under what conditions”
can also be attempted by feeding the SDG output as predicates to Reasoning modules along with additional background knowledge.

2. Related Works
Our work is influenced by various lines of work where
researchers have proposed approaches to extract meaningful information from images and videos. As [21] suggests,
such works can be categorized into 1) dense image annotations, 2) generating textual descriptions, 3) grounding natural language in images and 4) neural networks in visual and
language domains.
According to the above categorization, we share our
roots with the works of generating textual descriptions. This
includes the works that retrieves and ranks sentences from
training sets given an image such as [19], [12],[34], [40].
[10], [24], [25], [47], [48] are some of the works that have
generated descriptions by stitching together annotations or
applying templates on detected image content.
Several works have shown promising efforts to acquire
and apply commonsense in different aspects of Scene Analysis. [52] uses abstraction to discover semantically similar
images. [7] proposes to learn all variations pertaining to all
concepts and [36] uses common-sense to learn actions.
Recently, [20] introduced scene graphs to describe
scenes and [37] creates scene graphs from descriptions.
However, we automatically construct the graph from an image, and we believe, due to the event-entity-attribute based
representation and meaningful edge-labels (borrowed from
KM-ontology[4]) , SDGs are more equipped to facilitate
symbolic-level reasoning.
3 One may note that such structures are also generated by Semantic
parsers such as K-parser(www.kparser.org).

Figure 2: Example Image and a possible corresponding SDG. Note, the SDG should contain a similar event wear2 for person2. We omit it
for space constraints. Note that, it is easy to augment spatial information to the above graph such as (person1,left,person2).

3. State-of-the-art Visual Detections
The recent development of deep neural networks based
approaches revolutionized visual recognition research. Different from the traditional hand-crafted features, a multilayer neural network architecture efficiently captures sophisticated hierarchies describing the raw data [1], which
has shown superior performance on standard scene recognition [51], object recognition [15] and image captioning
[21] benchmarks.
Image Dataset: In this paper, we use three image data
sets, which are popularly referred to as Flickr 8k, Flickr
30k and Coco datasets [19]. These three datasets have
8,092, 31,783 and more than 160K images respectively.
All the images from these datasets are accompanied with
5 hand-annotated sentences that describe the image. For all
datasets, we used the train-test splits from [21] and the 4000
testing images (1000 each from Flickr 8k and 30k; 2000
from MS-COCO validation set; denoted as I) serve as the
testing bed for our reasoning experiments.
Deep Object Recognition: We use the trained
bottom-up region proposals and convolutional neural networks(CNN) object detection method from [15]. It considers 200 common everyday object classes (denoted as N )
and trained on ILSVRC 2013 dataset. We apply the method
on the testing images(I) and then convert the object detection scores to Pr (n|I).
Deep Scene Recognition: We use the trained CNN
scene classification method from [51]. The classification
model is trained on 205 scene categories (denoted as S) and
each of the category has more than 5000 training samples.
We apply the method on the testing images and then convert
the scene classification scores to Pr (s|I).
Constituent Annotation Collection and Deep Constituent Recognition: Images from the wild cannot always be categorized into a limited number of Scene categories. However, scene constituents describing properties
or actions of objects, attributes of scenes occur frequently
across images and can be utilized to describe the image. In
this work, we further augment the Flickr 8K image dataset
with human annotation of constituents using Amazon Mechanical Turks. We specifically ask the human labeler to annotate not only objects, but what objects are doing or properties of objects.
We allow the labelers to use free-form text for describ-

ing constituents to reduce annotation effort. To obtain a
standardized set of constituents from the annotations, we
perform stop-words removal, parts-of-speech processing to
retain nouns, adjectives and verbs. We replace the nouns
with their superclasses such as man, boy, father by person,
and then, we rank the resulting phrases according to their
frequencies. Some of the top phrases are grass, dog run,
dog play, kid play, person wear short4 etc.
For the rest of our processing, we post-process the annotations for each training image and consider them if they
are among the 1000 top constituents (denoted as C). Recent
empirical results from a diverse range of visual recognition
tasks indicate that the generic descriptors extracted from the
CNN are very powerful [9, 35]. In this work, we use a
pre-trained CNN from [23]. For each image in I, we use
this pre-trained model to extract a 4096 dimensional feature
vector using [9, 23]. We then trained a multi-label SVM
to do constituents recognition using the deep features. The
trained model is applied on all the testing images and we
convert the classification scores to Pr (c|I).
The set of Pr (n|I), Pr (s|I), Pr (c|I) makes up the initial
visual perception output.

4. Constructing SDGs from Noisy Visual Detections
Next, we explain the reasoning framework to construct
SDGs from noisy labels with the aid of knowledge from
text. To provide a better understanding of this complex system, we provide a diagram of the architecture explaining the
reasoning process for an example image in Figure 3.
As shown, for each image, the above perception system produces object, scene and constituent detection tuples.
Each detection is provided with a confidence score. For objects, scores are provided for each bounding box. Top five
scene labels and top ten constituent detections are considered for the reasoning framework. Most of these detections
are quite noisy. We develop an elaborate reasoning framework to construct SDGs from such noisy detections, with
the help of pre-processed background knowledge.
4 All the phrases with their corresponding frequencies will be made publicly available in the final version.

dataset-independent and only needs to be augmented when
the set of object classifiers expands.
Scenes-to-AVCs Mapping Table (SM ): For each scene
in S, we added ontological information involving a set of
abstract concepts and a set of synonyms. To obtain the synonyms, we again used WordNet API. We hand-annotated
all the AVCs for each scene and learnt a prior belief for
each AVC in scene from human annotations. For example,
for the scene airport terminal, we add {waiting room, big
glass view, people} as the list of AVCs and terminal as the
synonym; and learn the priors 0.7, 0.6 and 0.9 respectively
for AVCs.
In the following sub-sections, we first introduce the Reasoning Framework briefly, followed by a description of the
construction of the Knowledge Base Kb and the Bayesian
Network Bn . Lastly, we describe our reasoning framework
in detail.

4.2. Reasoning Framework

Figure 3: SDG and Sentence Generation through Reasoning using
Knowledge Base and a Bayesian Network Bn

4.1. Pre-processing Phase Data Accumulation
In this phase, we collect Ontological information about
object classes in Object Meta-data table (OT ) and Scene
classes in Scene Metadata (SM ). We also store scene detection tuples (ST ) and human annotation of images (Ad )
for all training images. We create a Knowledge Base Kb ,
a Bayesian Network Bn and a Scenes to Abstract Visual
Concepts5 (AVC) Mapping Table (SM ).
Scene Detection tuples (ST ): We use the perception
system of the previous section to create scene detection tuples ({(si , Pr (si |Itr ))|i ∈ {1, .., 5}}) of set of training images (Itr ). These are used to learn the Bayes Net Bn .
Image Annotations (Ad ): We collect all textual descriptions of the training images provided with Image Datasets
and use them for building the knowledge base Kb and Bayes
Net Bn . However, both can be built using any repository
of sentences that describe day-to-day concepts.
Object Meta-data (OT ): For each of the 200 object
classes, we collected all synonyms, hyponyms and hypernyms. The list is prepared using Wordnet API. This is
5 Abstract Visual Concepts are higher-level scene constituents and they
describe commonly occurring visual concepts that can be observed across
images. In essence, they are can be compared to phrasal verbs. Like textual
phrases, they do not necessarily follow compositionality of its constituent
words. In case of AVC, this happens in the context of images. For example
waiting room suggests room, seating area, chairs, people waiting etc.
In comparison, Scene Constituents can be compared with phrases that
retain their compositionality such as people play, person wear shorts etc.
In context of an image, person wear shorts can be grounded as the objects
person and shorts; and the action wear.

Equipped with the background knowledge stored in the
form of (Kb , Bn , SM , OT ), we process the objects, scene
and constituent detections for an image to construct an appropriate SDG in the following way: i) we populate synonyms, hypernyms, hyponyms of objects and synonyms,
AVCs (with priors) of scenes; ii) (Scene Constituents:) we
extract entities and events from each constituent. Such as,
the constituent person wear short results in an event wear
with two edges: one labeled agent joining the entity person and another labeled recipient joining the entity short;
iii) (Abstract Visual Concepts:) we choose the AVCs iteratively that maximizes the conditional probability given
high confidence objects; iv) (Objects:) for low-scoring objects, we choose the sibling (in the hyponym-hypernym hierarchy) which maximizes the conditional probability given
high-confidence objects and AVCs; v) (Events:) we search
the Kb to find the most compatible events that connect pairs
of high-confidence objects. We add the events obtained
from Constituents to this set of compatible events; vi)
(Concepts 6 :) given the above events and AVCs, we search
the Kb for Concepts that best suits the events and AVCs, and
we also construct an SDG based on just high-confidence objects, events and AVCs.

4.3. Knowledge Base (Kb )
In Figure 4, we describe how we construct Kb from a set
of Image Annotations (Ad ) using the Stanford Parser and
K-Parser [39]. For each sentence, we first parse using the
6 The idea behind the representation of a Concept is inspired by that of a
Process in AURA [2]. The structure Process is a graph that represents a Biological Process in AURA-KB. and it symbolizes a higher-level event that
encapsulates smaller events, and the entities that participate in such events.
Similarly, a Concept represents a natural activity where a few events occur
and participating entities interact through the events. Our entire approach,
in essence, is about sequentially determining the participants (entities and
events) of a Concept and lastly, the Concept itself.

build the Kb . A visualization of a part of the Kb is given
in Figure 4(b). After parsing the annotated sentences in
Flickr8k, Kb consists of 1102 events, 2500 entities and 1869
traits. The total number of edges and distinct concepts in the
graph are 25271 and 14325.

4.4. Conditional Probability Estimation

(a)

(b)

Figure 4: (a) Constructing Knowledge Base From Annotations.
(b) A snapshot of the Kb . In this figure, Person and bench are
entities, lay is the connecting event. The entity Person can have
trait climber. The sub-graph essentially captures the knowledge of
the activity person laying on a bench. The figure on the left shows
the edge-labels.

Stanford Parser to get a dependency graph. The K-parser
then maps these dependency labels using a set of rules to a
set of meaningful labels from KM-Ontology[4] and the resulting graph is further augmented using ontological and semantic information from different sources (more details on
kparser.org). We then generalize each of these graphs
i.e. replace entities by their superclasses. Then we merge
them based on overlapping entities and events, and create a
single graph (Kb ).
Kb is defined as the tuple (G, C). G = (V, E) denoting
set of vertices V , set of edges E. Each vertex and edge has
a label. Each vertex can be of three types: events, entities
and traits. Events correspond to verbs, entities correspond
to superclasses of nouns that directly interact with events
and traits represent all other nouns. Edge labels in the Kb
are exactly the same as in the K-parser (Figure 4). C is a
set of concepts which corresponds to generalized K-parser
graphs of sentences and is essentially a sub-graph of G.
From Flickr8k annotations, we process nearly 16000
sentences provided for the images (2 per each image7 ) to
7 We do not consider all 5 sentences for an image, as most of the sentences are conceptually same.

In this sub-section, we describe the type of conditional
probabilities we estimate and the Bayesian Network we
learn to estimate such probabilities. We use conditional
probability calculations in two of the steps of our approach:
inferring the most probable collection of Abstract Visual
Concepts and rectifying low-scoring erroneous objects.
For inferring the most probable collection of AVCs, we
first make a list (Cf req ) of all the frequent AVCs (with frequency > 2 in our experiments) from all scenes detected
for a test image. Then we follow Algorithm 1 to get the
set of inferred concepts Cinf from the set of high-scoring
(score > αh 8 ) entities Oimg and the set of scenes Simg detected for image img ∈ I. We iterate till the entropy keeps
decreasing.
Algorithm 1 Infer Abstract-Visual-Concepts
1: procedure INFER M OST P ROBABLE AVC S(Simg , Oimg , Cf req )
2:
prevH ← 1
3:
while Cf req =
6 φ do
4:
Smax ← argmaxs∈Cf req P (s|Cinf , Oimg )
P
5:
H ←
s∈Cf req {−P (s|Cinf , Oimg )∗logP (s|Cinf , Oimg )}
6:
if H > prevH then break;
7:
prevH ← H
8:
Cinf ← Cinf ∪ {Smax }; Cf req ← Cf req \ {Smax }

Next, we attempt to rectify the low-scoring entities based
on high-scoring entities (Oimg ) and the above Cinf . For
each low-scoring entity, we get all its siblings i.e. we get all
the children of its hypernyms. For example, if bathing cap
is assigned a low score, the assigned superclass is headwear
and its children are headband, hat etc. We calculate the
following omax = argmaxo∈siblings P (o|Cinf , Oimg ) and
then add omax to the high-scoring entities list (Oimg ).
As the above paragraphs suggest, we need to estimate the conditional probabilities: P (s|Cinf , Oimg ) and
P (o|Cinf , Oimg ). To estimate the conditional probabilities,
we learn a Bayesian Network Bn using Ad and ST .
4.4.1 Learning the Bayesian Network Bn
To capture the knowledge of naturally co-occurring entities
and Abstract Visual Concepts, we learn a Bayesian Network
that represents the dependencies among them. We create
the training data D which is a set of tuples T = (ti )i , i ∈
1, .., N where N is the total number of entities and AVCs.
Each term ti is binary and denotes 1 if the ith entity (or
AVC) occurs in the tuple. Then, we use the Tabu Search
(tabu) algorithm to learn the structure and then we populate the Conditional Probability Tables using the R-bnlearn
8 The hyper-parameter (α ) are set based on performance on validation
h
data.

package [38]. A subgraph of the learnt Bayesian Network
is shown in the Figure 5.

Figure 5: A subgraph reflecting the dependencies captured in the
Learnt Bayesian Network Bn

To create the training data D, we process each training image (in Itr ), and we automatically detect entities and
AVCs and then output the tuple T . To detect entities, we
parse the image annotations (Ad ) and extract entities from
it. Some of the AVCs such as people and people wear
shorts are detected using rule-based techniques. However,
for scenes such as airport-terminal, it is unlikely that AVCs
such as waiting room can be found in human descriptions
of an image; as we tend to describe only the entities and
their interactions. Keeping this idea in mind, we ran the
scene classifier system from the previous Section 3, and we
consider all the AVCs of the scene with the highest score
(P r(s|Itr )), from the Scene-to-AVC lookup table (SM ).

4.5. Ranking and Inferring Final Concepts
Given the most relevant set of Abstract Visual Concepts
(Cinf ) and entities (Oimg ), we find Concepts that the image
describes. To do this, we use the Kb to search first for events
that these entities (i.e. objects) participate in and then we
use these events and entities together to search for Concepts
in the set of concepts C in Kb .
We rely on two assumptions about the Knowledge Base:
I) Kb reflects a more-or-less complete view of the relevant
world knowledge and hence we can find the most suitable
events from it. This assumption is valid if the images come
from the same domain; such as in our examples, we have
used the Flickr8k dataset and the domain corresponds to
pictures of humans and dogs in natural setting; and II) Kb
contains all concepts possible with the given events and entities. This is a strict assumption, which might not be true
even if we parse the whole Web. To alleviate the problem,
we give two final outputs: i) an SDG involving the entities,
AVCs and events and ii) another SDG of the top Concept
that is obtained from C in Kb .
Search Connecting Events: The motivation behind
building a Knowledge Base was to logically explain why
certain co-occurring events are suitable for the combination
of entities. For example, consider the entities person and
swimming trunks. Note, swimming trunks corresponds to
the vertex trunk in Kb . We get events such as sniff, climb,
wear etc., i.e., some corresponding to tree-trunk and others
to swimming-trunks. To logically find suitable events, we
find all connecting events from G in Kb and then filter spuri-

ous events based on ontological and background knowledge
from OT and C in Kb .
For a pair of entity in Oimg , we traverse the path from
one entity to another in the graph G and consider eventnodes on the path. As shown in Figure 4(b), two entities
can be connected by an event. However, in some cases,
they could be connected by a chain of events and entities.
We employ a greedy breadth-first search over the graph G
for such pairs. We denote the set of entities that are related
to each other by some event, by Oev .
For filtering spurious events, we introduce the notion
of Edge-Compatible Events. An event is edge-compatible
with respect to two entities if they are connected to the
event using edges with compatible labels. As these labels
are well-defined relations between entities and events from
KM-Ontology, the label-compatibility is easy to observe.
For example, (agent,recipient) is a compatible pair and only
an animate entity can be an agent. Based on the rules, the
event wear is edge-compatible with respect to entities person and trunk.
Even after this, we still obtain events like climb etc. To
filter such events, we consult the table OT and the set of
concepts C. We know that the entity swimming trunks belongs to the superclass clothing, and hence we retain only
those events that are connected to an entity trunk which is
of the same superclass, in some concept in C.
SDG Construction: After obtaining a set of suitable
events (such as wear), we construct an SDG using the
following set of rules: i) add has(scene, component, s)
for all AVC s in Cinf ; ii) add has(event, location, scene)
for the top detected events; iii) add all compatible edges
related to the events such as has(wear,agent,person) and
has(wear,recipient,trunk); and iv) for all entities oim in
(Oimg \ Oev ), do the following: if it is an animate entity,
add has(oim , location, scene); Otherwise, find the shortest
path from oI to the top detected event in the Kb and add the
edges on the path to the SDG.
Search Concepts: Given the events and entities (Oev ),
we search the set of Concepts C in Kb . Recall, in the Kb , a
Concept is a generalized K-parser graph of a sentence. We
consider a Concept as candidate if all edges from a detected
Edge-Compatible Event are present in it.
Next, we weight each candidate Concept using the remaining entities in (Oimg \ Oev ) and AVCs; i.e., increase
a counter if an entity or AVC occurs in the graph. We also
calculate a joint confidence-score for each Concept based
on the Pr (n|I), Pr (s|I), Pr (c|I) values of the object, scene
and constituents present in the Concept. Based on the counters and the joint confidence-score, we rank the Concepts.
Template Based Sentence Generation: We generate textual descriptions from the SDG using the
SimpleNLG[14] package. For example, for the edges
has(wear,agent,person) and has(wear,recipient,shorts), we

will generate the sentence “a person is wearing shorts”.
Based on the edge-labels (labels from KM-ontology) we
populate the verb, subject, object and adjectives (including
quantitative9 ) of sentences using simple rules. It should be
noted that these K-Parser labels are a direct mapping from
the set of Stanford Dependencies, and theoretically we can
populate all the parts-of-speeches of a sentence from the
SDG. Herein lies the effectiveness of producing an SDG
from an image.

5. Experiments and Results
The Knowledge-Structure representing a scene should
be rich in information-content and should carry enough semantics to describe the image. We adopted three sets of
experiments. First, we detect the accuracy with which our
system can detect events and entities present in the image.
We perform a qualitative evaluation (“relevance” and “thoroughness”) of the textual descriptions generated from SDGs
with the sentences generated by [21] using the Amazon Mechanical Turkers (AMT). And lastly, to evaluate the imagesentence alignment quality, we design an Image Retrieval
task and report our results on Image Search based on generated annotations. To conclude, we provide a few example
images and their SDGs.
For comparison purposes, we use the implementation
from [21] to generate a textual caption S for each testing
image. The method is based on a combination of CNN over
image regions, bidirectional recurrent neural networks over
sentences, and a structured multimodal embedding. We denote the set of captions as SN N .
Training Phase:
Our model can be represented
by the tuple (Kb , Bn , ST , Ad , OT , SM ). Among these,
ST , OT and SM are collected and stored once, and re-used
for all datasets. For our experiments, we re-use the same
Bayesian Network Bn learnt from Flickr8k data for all the
datasets. Though, we build the Kb each time from the annotated sentences, this can be easily avoided by using the
same Kb for all the datasets. In essence, for the reasoning
part, we donot require any training at all for new datasets.
Entity and Event Detection Accuracy: For this experiment, we extracted entities and events (gold-standard) from
constituent annotations for the 1000 test images of Flickr8k.
We manually checked them to remove noise. To provide a
baseline, we also extracted entities and events from SN N
automatically using K-parser. Subsequently, we compared
the gold-standard with entity-event set from [21] and the
SDG output from our system for each image. The statistics
of our evaluation is given in Table 1.
AMT Evaluation of Generated Sentences: Since sentence generation to describe a scene is innately a creative
process, a good metric is to ask humans to evaluate these
9 For high-scoring detections, we also consider the spatial information
from the bounding-boxes. For N such detections of an object obj, we
generate sentences like N obj’s are in the scene.

Type

Accuracy-SDG(%)

Precision-SDG(%)

Entities
Events

13.6
13.1

21.7
8

Accuracy(%)[21]
16.9
15.3

Precision(%)[21]
34.2
15.2

Table 1: Accuracy and Precision in events and entities prediction

sentences. The evaluation metrics: Relevance and Thoroughness, are therefore proposed as empirical measures of
how much the description conveys the image content (relevance) and how much of the image content is conveyed by
the description (thoroughness)10 . We engaged the services
of AMT to judge the generated descriptions based on a discrete scale ranging from 1–5 (low relevance/thoroughness
to high relevance/thoroughness). The average of the scores
and their deviation are summarized in Table 2 for Flickr8k,
Flickr30k test images and MS-COCO validation images.
For comparison, we asked the AMTs to also judge one random gold-standard description and the output from [21], a
state-of-the-art image captioning system.
In our experiments, we found that Kb from Flickr8k annotations can be used for Flickr30k without much effect
on accuracy. However, for MS-COCO datasets, Kb from
Flickr8k annotations falls short of producing a desired accuracy as the COCO data is much more varied.
Experiment
R ± D(8k)
T ± D(8k)
R ± D(30k)
T ± D(30k)
R±D(COCO)
T±D(COCO)

[21]
2.08 ± 1.35
2.24 ± 1.33
1.93 ± 1.32
2.17 ± 1.34
2.69 ± 1.49
2.55 ± 1.41

Our Method
2.82 ± 1.56
2.62 ± 1.42
2.43 ± 1.42
2.49 ± 1.42
2.14 ± 1.29
2.06 ± 1.24

Gold Standard
4.69 ± 0.78
4.32 ± 0.99
4.78 ± 0.61
4.52 ± 0.93
4.71 ± 0.67
4.37 ± 0.92

Table 2: Sentence generation relevance (R) and thoroughness (T)
human evaluation results with gold standard and [21] on Flickr 8k,
30k and MS-COCO datasets. D: Standard Deviation.

Image-Sentence Alignment Evaluation: Similar to
the experiments in [21, 20], we also evaluate the imagesentence alignment quality using ranking experiments. We
withhold the set of testing images and use the generated sentences as queries.
We process the textual query and construct Gquery =
(Vq , Eq ) using the same procedure by which we construct Kb . For each image, we take the SDG Gimg =
(Vimg , Eimg ) and calculate similarity between the SDG and
the query using the following formula:
P
Sim(Gquery , Gimg ) =

vq ∈Vq

maxvimg ∈Vimg (sim(vq , vimg ))
|Vq |

sim(vq , vimg ) = (wnsim(label(vq ), label(vimg ))+
Jaccard(neighbors(vq ), neighbors(vimg )))/2.

Similarity between two vertices are calculated based on
their word-meaning similarity and neighbor similarity. Here
10 For complete instructions provided to the turkers, please check out
Appendix.

(a)

(b)

(g)

(c)

(h)

(d)

(e)

(i)

(f)

(j)

Figure 6: The SDGs in (d), (e) and (f) corresponds to images (a), (b) and (c) respectively. For more detailed examples, please check
Appendix and http://bit.ly/1NJycKO.

wnsim(., .) is WordNet-Lin Similarity [29] between two
words and Jaccard(., .) is the standard Jaccard coefficient
similarity. Based on the above similarity measure, we give
the image retrieval results compared with few of the stateof-the-art results in Table 3.
Model
[21] BRNN
Our Method-SDG

R@1
11.8
18.1

[21] BRNN
Our Method-SDG

15.2
26.5

[21] BRNN (1k)
Our Method-SDG (1k)
Our Method-SDG (2k)

20.9
19.3
15.4

Flickr8k
R@5 R@10
32.1
44.7
39.0
50.0
Flickr30k
37.7
50.5
48.7
59.4
MS-COCO
52.8
69.2
35.5
49.0
32.5
42.2

the sentences and images can seamlessly converge to such
space of graphical representations. This could have huge
repercussions in search in Image and Textual space and storing knowledge from images and text together in a unified
Knowledge Base.

6. Conclusion
Med r
12.4
10.5
9.2
6.0
4.0
11.0
17.0

Table 3: Image-Search Results: We report the recall@K (for K =
1, 5 and 10) and Med r (Median Rank) metric for Flickr8k, 30k
and COCO datasets. For COCO, we experimented on first 1000
(1k) and random 2000 (2k) validation images.

One of the primary contributions of our work is the
Knowledge-Structure representation that bridges the gap
between semantic information in text and images. From
the results of this experiment, the benefit of having such an
intermediate representation is easy to observe.
Example Images and SDGs: As examples, we pick a
few images which produces objects and scene recognitions
with comparably good confidence scores. The images and
their corresponding SDGs are provided in Figure 6. As we
can observe, the information produced by these SDGs are
easily processed by machines. We can answer questions
such as how entities interact in an event, which possible
events are in the scene and how entities interact in a scene.
We should also mention that the concept-level modeling
provided by SDGs is what separates this work from other recent approaches [20]. Furthermore, comparing these structures with the K-Parser output in Figure 4, we can see how

This paper introduced a reasoning module to generate
textual descriptions from images by first constructing a new
intermediate semantic representation, namely the Scene Description Graph (SDG), which is later used to generate
sentences. The reasoning module uses an automatically
constructed Knowledge Base created from text, to capture
“commonsense” knowledge. Having built the Knowledge
Base, we proposed a method of obtaining such SDGs from
noisy labels using our prediction system. The SDG is a representation of the scene in view that integrates direct visual knowledge (objects and their locations in the scene)
with background commonsense knowledge. In addition,
the SDGs have a structure similar to semantic representations of sentences, thus facilitating the interaction between
Vision and Natural Language. The notion of the SDG has
great potential. Here we used the SDG for the automatic
creation of sentences describing the scene; but, equipped
with background knowledge, it also allows reasoning and
question/answering about the scene 11 .
To demonstrate the effectiveness of the sentences and
constructed SDGs, we performed a number of experiments.
Our AMT evaluations on popular datasets show that our
sentences performs comparatively well with respect to the
state-of-the-art in measures of relevance and thoroughness.
A Gold-Standard based evaluation shows that our output
SDGs can detect events and entities with comparable accuracy as a state-of-the-art system. And lastly, our Image
Retrieval experiment shows that the Image-Sentence alignment quality is comparable with state-of-the-art results.
11 Please

see appendix for an example.

References
[1] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern
Analysis and Machine Intelligence, IEEE Transactions on,
35(8):1798–1828, 2013. 3
[2] V. K. Chaudhri, B. E. John, S. Mishra, J. Pacheco, B. Porter,
and A. Spaulding. Enabling experts to build knowledge bases
from science textbooks. In Proceedings of the 4th International Conference on Knowledge Capture, K-CAP ’07, pages
159–166, New York, NY, USA, 2007. ACM. 2, 4
[3] X. Chen and C. L. Zitnick. Learning a recurrent visual representation for image caption generation. arXiv preprint
arXiv:1411.5654, 2014. 1
[4] P. Clark, B. Porter, and B. P. Works. Km-the knowledge machine 2.0: Users manual. Department of Computer Science,
University of Texas at Austin, 2004. 2, 5
[5] N. Dalal and B. Triggs. Histograms of oriented gradients for
human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference
on, volume 1, pages 886–893. IEEE, 2005. 1
[6] E. Davis and G. Marcus. Commonsense reasoning and commonsense knowledge in artificial intelligence. Commun.
ACM, 58(9):92–103, Aug. 2015. 1
[7] S. K. Divvala, A. Farhadi, and C. Guestrin. Learning everything about anything: Webly-supervised visual concept
learning. In 2014 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2014, Columbus, OH, USA, June
23-28, 2014, pages 3270–3277, 2014. 2
[8] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389, 2014. 1
[9] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Proceedings
of the 31st International Conference on Machine Learning
(ICML-14), pages 647–655, 2014. 3
[10] D. Elliott and F. Keller. Image description using visual dependency representations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1292–1302, 2013. 2
[11] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing
objects by their attributes. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on, pages
1778–1785. IEEE, 2009. 1
[12] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Generating sentences from images. In Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV’10, pages 15–29, Berlin, Heidelberg,
2010. Springer-Verlag. 2
[13] P. Felzenszwalb, D. McAllester, and D. Ramanan. A discriminatively trained, multiscale, deformable part model.
In Computer Vision and Pattern Recognition, 2008. CVPR
2008. IEEE Conference on, pages 1–8. IEEE, 2008. 1

[14] A. Gatt and E. Reiter. Simplenlg: A realisation engine
for practical applications. In Proceedings of the 12th European Workshop on Natural Language Generation, ENLG
’09, pages 90–93, Stroudsburg, PA, USA, 2009. Association
for Computational Linguistics. 6
[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic
segmentation. In Computer Vision and Pattern Recognition,
2014. 3
[16] D. Gunning, V. K. Chaudhri, P. Clark, K. Barker, S. yi Chaw,
B. Grosof, A. Leung, D. Mcdonald, S. Mishra, J. Pacheco,
B. Porter, A. Spaulding, D. Tecuci, and J. Tien. Project halo
updateprogress toward digital aristotle. 2
[17] A. Gupta and L. S. Davis. Objects in action: An approach
for combining action understanding and object perception. In
Computer Vision and Pattern Recognition, 2007. CVPR’07.
IEEE Conference on, pages 1–8. IEEE, 2007. 1
[18] C. Havasi, R. Speer, and J. Alonso. Conceptnet 3: a flexible,
multilingual semantic network for common sense knowledge. In Recent advances in natural language processing,
pages 27–29. Citeseer, 2007. 1
[19] M. Hodosh, P. Young, and J. Hockenmaier. Framing image
description as a ranking task: Data, models and evaluation
metrics. Journal of Artificial Intelligence Research, pages
853–899, 2013. 2, 3
[20] J. Johnson, R. Krishna, M. Stark, J. Li, M. Bernstein, and
L. Fei-Fei. Image retrieval using scene graphs. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2015. 2, 7, 8
[21] A. Karpathy and F.-F. Li. Deep visual-semantic alignments for generating image descriptions. arXiv preprint
arXiv:1412.2306, 2014. 1, 2, 3, 7, 8
[22] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying
visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014. 1
[23] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS
2012, 2013. 1, 3
[24] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg,
and T. L. Berg. Baby talk: Understanding and generating
image descriptions. In Proceedings of the 24th CVPR, 2011.
2
[25] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Y. Choi. Collective generation of natural image descriptions.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume
1, ACL ’12, pages 359–368, Stroudsburg, PA, USA, 2012.
Association for Computational Linguistics. 2
[26] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning
to detect unseen object classes by between-class attribute
transfer. In Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, pages 951–958. IEEE,
2009. 1
[27] I. Laptev. On space-time interest points. International Journal of Computer Vision, 64(2-3):107–123, 2005. 1
[28] D. B. Lenat. Cyc: A large-scale investment in knowledge
infrastructure. Commun. ACM, 38(11):33–38, Nov. 1995. 1

[29] D. Lin. An information-theoretic definition of similarity. In
ICML, volume 98, pages 296–304, 1998. 8
[30] D. G. Lowe. Object recognition from local scale-invariant
features. In Computer vision, 1999. The proceedings of the
seventh IEEE international conference on, volume 2, pages
1150–1157. Ieee, 1999. 1
[31] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille. Explain
images with multimodal recurrent neural networks. arXiv
preprint arXiv:1410.1090, 2014. 1
[32] R. Messing, C. Pal, and H. Kautz. Activity recognition using the velocity histories of tracked keypoints. In Computer
Vision, 2009 IEEE 12th International Conference on, pages
104–111. IEEE, 2009. 1
[33] A. S. Ogale, A. Karapurkar, and Y. Aloimonos. Viewinvariant modeling and recognition of human actions using
grammars. In R. Vidal, A. Heyden, and Y. Ma, editors, WDV,
volume 4358 of Lecture Notes in Computer Science, pages
115–126. Springer, 2006. 1
[34] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describing images using 1 million captioned photographs. In
J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. C. N. Pereira,
and K. Q. Weinberger, editors, NIPS, pages 1143–1151,
2011. 2
[35] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson.
Cnn features off-the-shelf: an astounding baseline for recognition. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on, pages 512–519.
IEEE, 2014. 3
[36] M. Santofimia, J. Martinez-del Rincon, and J.-C. Nebel.
Common-Sense Knowledge for a Computer Vision System
for Human Action Recognition. In J. Bravo, R. Hervás, and
M. Rodrı́guez, editors, Ambient Assisted Living and Home
Care, volume 7657 of Lecture Notes in Computer Science,
pages 159–166. Springer Berlin Heidelberg, 2012. 2
[37] S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D.
Manning. Generating semantically precise scene graphs
from textual descriptions for improved image retrieval. In
Proceedings of the Fourth Workshop on Vision and Language, pages 70–80, Lisbon, Portugal, September 2015. Association for Computational Linguistics. 2
[38] M. Scutari. Learning bayesian networks with the bnlearn R
package. Journal of Statistical Software, 35(3):1–22, 2010.
6
[39] A. Sharma, N. H. Vo, S. Aditya, and C. Baral. Towards addressing the winograd schema challenge - building and using a semantic parser and a knowledge hunting module. In
Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires,
Argentina, July 25-31, 2015, pages 1319–1325, 2015. 4
[40] R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y.
Ng. Grounded compositional semantics for finding and describing images with sentences. TACL, 2:207–218, 2014. 2
[41] D. G. Stork. HAL’s Legacy: 2001’s Computer as Dream and
Reality. MIT Press, 1998. 1
[42] C. L. Teo, C. Fermüller, and Y. Aloimonos. A gestaltist
approach to contour-based object recognition: Combining
bottom-up and top-down cues. The International Journal of
Robotics Research, page 0278364914558493, 2015. 1

[43] C. L. Teo, A. Myers, C. Fermuller, and Y. Aloimonos. Embedding high-level information into low level vision: Efficient object search in clutter. In Robotics and Automation
(ICRA), 2013 IEEE International Conference on, pages 126–
132. IEEE, 2013. 1
[44] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show
and tell: A neural image caption generator. arXiv preprint
arXiv:1411.4555, 2014. 1
[45] H. Wang, A. Klaser, C. Schmid, and C.-L. Liu. Action recognition by dense trajectories. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages
3169–3176. IEEE, 2011. 1
[46] Y. Yang, C. Fermüller, Y. Aloimonos, and A. Guha. A cognitive system for understanding human manipulation actions.
Advances in Cognitive Systems, 3:67–86, 2014. 1
[47] Y. Yang, C. L. Teo, H. Daumé, III, and Y. Aloimonos.
Corpus-guided sentence generation of natural images. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 444–454,
Stroudsburg, PA, USA, 2011. Association for Computational
Linguistics. 2
[48] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S. C. Zhu. I2t:
Image parsing to text description. Proceedings of the IEEE,
98(8):1485–1508, 2010. 2
[49] X. Yu and Y. Aloimonos. Attribute-based transfer learning for object categorization with zero/one training example.
In Computer Vision–ECCV 2010, pages 127–140. Springer
Berlin Heidelberg, 2010. 1
[50] X. Yu, C. Fermuller, C. L. Teo, Y. Yang, and Y. Aloimonos.
Active scene recognition with vision and language. In Computer Vision (ICCV), 2011 IEEE International Conference
on, pages 810–817. IEEE, 2011. 1
[51] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. NIPS, 2014. 3
[52] C. L. Zitnick and D. Parikh. Bringing semantics into focus
using visual abstraction. In CVPR, pages 3009–3016. IEEE,
2013. 2

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❆PP❊◆❉■❳ ♦❢ ✏❋r♦♠ ■♠❛❣❡s t♦ ❙❡♥t❡♥❝❡s t❤r♦✉❣❤ ❙❝❡♥❡
❉❡s❝r✐♣t✐♦♥ ●r❛♣❤s ✉s✐♥❣ ❈♦♠♠♦♥s❡♥s❡ ❘❡❛s♦♥✐♥❣ ❛♥❞
❑♥♦✇❧❡❞❣❡✑
❈♦♥t❡♥ts
✶

◗✉❡st✐♦♥✲❆♥s✇❡r✐♥❣ ❜❛s❡❞ ♦♥ ❙❉●s

✶

✷

■♠❛❣❡s✱ ❈♦rr❡s♣♦♥❞✐♥❣ ❙❉●s ❛♥❞ ●❡♥❡r❛t❡❞ ❙❡♥t❡♥❝❡s

✸

✸

❙♦♠❡ ▼♦r❡ ■♠❛❣❡s ❛♥❞ ●❡♥❡r❛t❡❞ ❙❡♥t❡♥❝❡s

✺

✹

❆▼❚ ■♥str✉❝t✐♦♥s

✽

✶

◗✉❡st✐♦♥✲❆♥s✇❡r✐♥❣ ❜❛s❡❞ ♦♥ ❙❉●s

❋✐❣✉r❡ ✶✿ ❆♥ ❊①❛♠♣❧❡ ■♠❛❣❡ ❢r♦♠ ❋❧✐❝❦r ✽❦✿ ❚❤❡ ❣r❡❡♥ ❜♦① s❤♦✇s ❛ ♣❡rs♦♥ ✐s ❞❡t❡❝t❡❞ ✇✐t❤ s❝♦r❡

1.36056✳

■♥ t❤✐s ✐♠❛❣❡✱ t❤✐s ✐s t❤❡ ♦♥❧② ♦♥❡ ✐♥st❛♥❝❡ ♦❢ ❝❧❛ss ✏♣❡rs♦♥✑ ❞❡t❡❝t❡❞ ✇✐t❤ ❤✐❣❤ ❝♦♥✜❞❡♥❝❡✳

❋♦r t❤❡ ✐♠❛❣❡ ✐♥ ❋✐❣✉r❡ ✶✱ t❤❡ ❣❡♥❡r❛t❡❞ ❢❛❝ts ✐♥ ❙❉● ❛r❡✿

has(scene,component,water).
has(scene,component,water_droplets).
has(scene,component,exterior_of_building).
has(person1,semantic_role,drinker).
has(water,semantic_role,liquid).
has(person1,semantic_role,creator).
has(drink,recipient,water).
has(drink,agent,person1).
has(drink,origin,fountain).
has(drink,next_event,make).
✶

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❖♥❡ ♦❢ t❤❡ ❛❞✈❛♥t❛❣❡ ♦❢ ❙❉● ✐s✱ ✇❡ ❝❛♥ ❞✐r❡❝t❧② ✉s❡ t❤❡ ❛❜♦✈❡ tr✐♣❧❡s ❛s ❢❛❝ts ❛♥❞ ❛s ❛♥ ✐♥♣✉t t♦ ❛♥
❆♥s✇❡r ❙❡t Pr♦❣r❛♠✳ ◆♦✇✱ ✐❢ ✇❡ ♣♦s❡ t❤❡ q✉❡st✐♦♥ t❤❛t ✏■s s♦♠❡♦♥❡ ❞r✐♥❦✐♥❣ ❢r♦♠ t❤❡ ❢♦✉♥t❛✐♥❄✑
✐♥ ❆❙P✱ ✇❡ ❝❛♥ ❛♥s✇❡r ❜❛s❡❞ ♦♥ t❤❡ ❛❜♦✈❡ ❢❛❝ts ✉s✐♥❣ t❤❡ ❢♦❧❧♦✇✐♥❣ ♣r♦❣r❛♠ ✐♥ ❈❧✐♥❣♦✲✸✿

entity(person;dog;water;shorts;frisbee).
animate(person;dog).
inanimate(A) :- not animate(A), entity(A).
drink_yes :- animate(A), has(drink,agent,A), has(drink,recipient,water).
yes_fountain(A) :- drink_yes, has(drink,agent,A), has(drink,origin, fountain).
#hide.
#show yes_fountain/1.
❲❡ ❡①❡❝✉t❡ t❤❡ ❛❜♦✈❡ ♣r♦❣r❛♠ ✐♥ ❈❧✐♥❣♦✲✸✱ ✇❡ ❣❡t t❤❡ ❛♥s✇❡r ❛s

✷

②❡s❴❢♦✉♥t❛✐♥✭♣❡rs♦♥✶✮✳

❆♣♣❡♥❞✐①

✷

◆♦✈❡♠❜❡r ✷✵✶✺

■♠❛❣❡s✱ ❈♦rr❡s♣♦♥❞✐♥❣ ❙❉●s ❛♥❞ ●❡♥❡r❛t❡❞ ❙❡♥t❡♥❝❡s

❋✐❣✉r❡ ✷✿ ❈❛s❡ ❙t✉❞②✲■

❋✐❣✉r❡ ✸✿ ❈❛s❡ ❙t✉❞②✲■■

✸

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❋✐❣✉r❡ ✹✿ ❈❛s❡ ❙t✉❞②✲■■■

❋✐❣✉r❡ ✺✿ ❈❛s❡ ❙t✉❞②✲■❱

✹

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❋✐❣✉r❡ ✻✿ ❈❛s❡ ❙t✉❞②✲❱

✸ ❙♦♠❡ ▼♦r❡ ■♠❛❣❡s ❛♥❞ ●❡♥❡r❛t❡❞ ❙❡♥t❡♥❝❡s

❋✐❣✉r❡ ✼✿ ❙❡♥t❡♥❝❡ ❊①❛♠♣❧❡s

✺

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❋✐❣✉r❡ ✽✿ ❙❡♥t❡♥❝❡ ❊①❛♠♣❧❡s

❋✐❣✉r❡ ✾✿ ❙❡♥t❡♥❝❡ ❊①❛♠♣❧❡s

✻

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❋✐❣✉r❡ ✶✵✿ ❙❡♥t❡♥❝❡ ❊①❛♠♣❧❡s

✼

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

✹ ❆▼❚ ■♥str✉❝t✐♦♥s

❋✐❣✉r❡ ✶✶✿ ❘❡❧❡✈❛♥❝❡ ❊✈❛❧✉❛t✐♦♥ ❚❛s❦ ■♥str✉❝t✐♦♥s ❛s ♣r♦✈✐❞❡❞ t♦ t❤❡ ❚✉r❦❡rs

❋✐❣✉r❡ ✶✷✿ ❚❤♦r♦✉❣❤♥❡ss ❊✈❛❧✉❛t✐♦♥ ❚❛s❦ ■♥str✉❝t✐♦♥s ❛s ♣r♦✈✐❞❡❞ t♦ t❤❡ ❚✉r❦❡rs

✽

Translating Simple Legal Text to Formal
Representations
Shruti Gaur, Nguyen H. Vo, Kazuaki Kashihara, and Chitta Baral
Arizona State University, Tempe, Arizona, USA
{shruti.gaur,nguyen.h.vo,kkashiha,chitta}@asu.edu

Abstract. Various logical representations and frameworks have been
proposed for reasoning with legal information. These approaches assume
that the legal text has already been translated to the desired formal representation. However, the approaches for translating legal text into formal representations have mostly focused on inferring facts from text or
translating it to a single representation. In this work, we use the NL2KR
system to translate legal text into a wide variety of formal representations. This will enable the use of existing logical reasoning approaches
on legal text(English), thus allowing reasoning with text.
Keywords: natural language processing ·natural language understanding ·natural language translation

1

Introduction and Motivation

One of the tasks of the Competition on Legal Information Extraction and Entailment [1] consists of finding whether a given statement is entailed by the
given legal article(s) or not. This is similar to the Recognition of Textual Entailment(RTE) challenge[3]. It has been observed by by Bos and Markert[7] that
classification based on shallow features alone performs better than theorem proving, for RTE. Androutsopoulos and Malakasiotis[3] state that most approaches
for RTE do not focus on converting natural language to its formal representation. However, we believe that approaches using statistical or machine learning
methods on shallow features do not offer much explanation about why a certain
sentence is entailed or not, hence providing little insight into the cause of entailment. In this respect, we consider the approaches based on logical reasoning to
be more promising.
There have been several works that propose logical representations and logics
for representing and reasoning with legal information[14,13,21,11,12]. Reasoning
rules and frameworks assume that the information given in the form of natural
language can somehow be understood and represented in the required form. However, current methods to convert legal text to formal representations[8,17,4,18]
either focus on extracting important facts or are not generalizable to a wide
variety of representations. Currently, there is no consensus on a single representation to express legal information. Therefore, a system that can translate
natural language to a wide variety of formal languages, depending on the application, is desired. In this paper we show how our NL2KR system can be used for

translation of simple legal sentences in English to various formal representations.
This will facilitate reasoning with various frameworks.

2

Related Work

Some approaches to translate text into formal representations focus on extraction of specific facts from the text. For example, Lagos et al.[17] present a semiautomatic method to extract specific information such as events, characters,
roles, etc. from legal text by using the Xerox Incremental Parser(XIP)[2]. The
XIP performs preprocessing, named entity extraction, chunking and dependency
extraction, and combination of dependencies to create new ones. Bajwa et al.[4]
propose an approach to automatically translate specification of business rules in
English to Semantic Business Vocabulary and Rules(SBVR). Their method is
essentially a rule-based information-extraction approach, which identifies SBVR
elements from text. The goal of other approaches like the work by McCarty[18]
is to obtain a semantic interpretation of the complete sentence. This approach
uses the output from the state-of-the-art statistical parser to obtain a semantic
representation called Quasi-Logical Form(QLF). QLF is a rich knowledge representation structure which is considered an intermediate step towards a fully
logical form.
There have been similar efforts in other languages. Nakamura et al.[20] present
a rule-based approach to convert Japanese legal text into a logical representation
conforming to Davidsonian style. They ascertain the structure of legal sentences
and identify cue phrases that indicate this structure, by manually analyzing
around 500 sentences. They also define transformation rules for some special
occurrences of nouns and verbs. In their subsequent work[16], they propose a
method to resolve references that point to other articles or itemized lists, by
replacing them with the relevant content.
As mentioned in the previous section, different legal reasoning frameworks
expect input in different logical representations. Even though Legal Knowledge
Interchange Format(LKIF)[15] was an attempt to standardize the representation
of legal knowledge in the semantic web , currently, no single representation has
been unanimously considered the de-facto standard for legal text. Therefore, we
need a system that can translate natural language to a particular representation
depending on the application.

3

The NL2KR Framework

NL2KR is a framework to develop translation systems that translate natural
language to a wide variety of formal language representations. It is easily adaptable to new domains according to the training data supplied. It is based on
the algorithms presented in Baral et al.[5]. The workflow using the NL2KR systems consists of two phases: 1.)learning and 2.)translation, as shown in Fig.1.
In the learning phase, the system takes training data, an initial dictionary and
any optional syntax overrides, as inputs. The training data consists of a number of natural language sentences along with their formal representations in

Fig. 1. The NL2KR system showing learning(left) and translation(right)

the desired target language. The initial dictionary (or lexicon) contains meanings of some words. The dictionary is manually supplied to the system. Using
these inputs, NL2KR tries to learn the meanings of as many words as possible.
Thus, the output of the learning phase is an updated dictionary which includes
the meanings of all newly learned words. The translation phase uses the dictionary created by the learning phase to translate previously unseen sentences.
At the core of NL2KR are two very
elegant algorithms, Inverse Lambda
and Generalization, which are used
Table 1. Example
to find meanings of unknown words
John
loves
Mary
in terms of lambda(λ) expressions.
NP
(S\N P )/N P
NP
1
NL2KR is inspired by Montague’s
john #y.#x.loves(x, y) mary
approach[19]. Every word has a λ exS\N P
pression meaning. The meaning of a
#x.loves(x,
mary)
sentence is successively built from the
combination of the λ expressions of
S
loves(john, mary)
words according to the rules of combination in Lambda(λ) calculus[9]. The
order in which the words should be
combined is given by the parse tree of the sentence according to a given Combinatory Categorial Grammar(CCG)[22]. As an example illustrating this approach,
consider the sentence “John loves Mary” shown in Table 1. The CCG category
of “loves” is (S\NP)/NP. This means that this word takes arguments of type
1

NL2KR cannot be said to be based on Montague Semantics as it does not use
intensional semantics. The translation of natural language to formal language with
the use of lambda calculus, however, is in the same spirit as Montague’s approach.

NP(noun-phrase) from the left and the right, to form a complete sentence. From
the CCG parse, we observe that “loves” and “Mary” combine first and then
their combination combines with “John” to form a complete parse. The λ expression corresponding to “loves” is #y.#x.loves(x, y)2 , which means that this
word takes two inputs, #x and #y as arguments and the application of this
word to the arguments results in a λ expression of the form loves(x, y).
The close correspondence between CCG syntax and λ calculus semantics
is very helpful in applying this method. In the first step, the λ expression for
“loves” is applied to “Mary”, with the former as the function and the latter
as the argument, in accordance with CCG categories. This application, denoted
as #y.#x.loves(x, y)@mary results in #x.loves(x, mary). Proceeding this way,
the meaning of the sentence is generated in terms of λ expressions. This is
a very elegant way to model semantics and has been widely used[6,23,10,5].
The problem, however, is that for longer sentences, λ expressions become too
complex for even humans to figure out. This problem is addressed by NL2KR by
employing the Inverse Lambda and Generalization algorithms to automatically
formulate λ expressions from words whose semantics are known.
Learning Algorithms: The two algorithms used to learn λ semantics of new
words are the Inverse Lambda and Generalization algorithms. When the λ expressions of a phrase and that of one of its sub-parts (children in the CCG parse
tree) are known, we can use this knowledge to find the λ expression of the unknown sub-part. The Inverse Lambda operation computes a λ expression F such
that H = F @G or H = G@F given H and G. These are called Inverse-L and
Inverse-R algorithms, respectively. For example, if we know the meaning of the
sentence “John loves Mary” (Table 1) as loves(john, mary) and the meaning of
John as john, we can find the meaning of “loves Mary” using Inverse Lambda, as
#x.loves(x, mary). Going further, if we know the meaning of “Mary” as mary,
we can find the meaning of “loves” using Inverse Lambda.
The Generalization algorithm is used to learn meanings of unknown words
from syntactically similar words with known meanings. It is used when Inverse
Lambda algorithms alone are not enough to learn new meanings of words or when
we need to learn meanings of words that are not even present in the training
data set. For example, we can generalize the meaning of the word “likes” with
CCG category (S\N P )/N P ), from the meaning of “loves”, which we already
know from the previous example. The meaning of “likes” thus generated will be
#y.#x.likes(x, y). We will illustrate learning in later sections with the help of
examples.
For every sentence in the training set, we first use the CCG Parser to obtain
all possible parse trees. Using the initial dictionary supplied by the user, the
system assigns all known meanings to the words (at the leaf) in each parse
tree. Moving bottom up, it combines as many words with each other as possible
(in the order dictated by the parse tree) by performing λ applications. The
meaning of each complete sentence is known from the training corpus. We need to
traverse top-down from this known translation, while simultaneously traversing
2

# is used in place of λ to enable typing into a terminal

bottom up, by filling in missing word or phrase meanings. Meanings of unknown
words and phrases are obtained using Inverse Lambda and Generalization, as
applicable, until nothing new can be learned.
Dealing with Ambiguity: To deal with ambiguity of words, a parameter
learning method[23] is used to estimate a weight for each word-meaning pair
such that the joint probability of the training sentences getting translated to
their given formal representation is maximized. However, this method might not
work in all cases and more complex approaches, possibly involving word sense
identification from context, might have to be used. Completely addressing this
problem is a part of future work.
Translation Approach: Given a sentence, we consider all the possible parse
trees, consisting of meanings of every word learned by the system or obtained
from Generalization algorithm. Then we use Probabilistic CCG(PCCG)[23] to
find the most probable tree, according to weights assigned to each word.
Availability: NL2KR is freely available for Windows, Linux and MacOSX systems at http://nl2kr.engineering.asu.edu. It is configurable for different domains
and can be adapted to work with a large number of formal representations. A
tutorial has also been provided.

4

Translating to Formal Legal Representations

NL2KR can be used to translate sentences into various logical representations,
either directly or by using an intermediate language 3 . It can be customized to
different domains based on the initial dictionary and training data provided. The
quality of these inputs affects NL2KR’s performance. A language class can be
considered a good analogy of NL2KR. The effectiveness of learning depends on
the richness of vocabulary imparted to the students beforehand(similar to initial
lexicon) and the sentences chosen to teach the language(training data). In our
experiments, we observed that learning simpler sentences before complex ones
aided learning. We will also give some guidelines for creating the initial dictionary. Several logics have been proposed in the literature for representing legal
information[14,13,21,11,12], from which we have selected a few. In this section,
we will illustrate the method of creating a good initial lexicon and demonstrate
how to use the system to learn new word meanings, with respect to these examples. We will start with simple examples and progress to more complicated
ones.
4.1 Translating to First Order Logic Representations
In this section, we demonstrate translating a sentence from the Competition
on Legal Information Extraction and Entailment[1] corpus to a first order logic
representation.
Sentence: Possessory rights may be acquired by an agent.
Translation: rights(X) ∧ type(X, possessory) ∧ agent(Y ) >
acquirable(X, Y, may)
3

The choice of intermediate language depends on the domain and target languages.
Once an intermediate language has been decided, the conversion can be automated.

Here > is used to denote implication. The form of an action, for e.g., “acquirable”
is action(X, Y, Z). It denotes X(possessory rights) is being acquired by Y(agent)
and the type of this action is Z. In the given example, acquiring is a possibility,
not an obligation, which is why we use may as its type.
Once we provide this training data and other required inputs to the NL2KR
learning interface (Fig.2), we can start the Learning process. We will describe
how to create inputs for learning in the next sub-section. The initial dictionary
contains a list of words and their meanings in terms of λ expressions. Even if we
do not know meanings of some words, we can use the system to figure them out
on its own, using Inverse Lambda or Generalization algorithms. Fig.2 shows that
the system learns the meaning of “rights” automatically using Inverse Lambda.

Fig. 2. NL2KR automatically learning the meaning of “rights” using Inverse Lambda
Algorithm : feature=rights : [N] : #x3.#x1.right(x1) ∧ type(x1,x3)

4.2 Translating Sentences with Temporal Information
Consider the following sentence and translation, which shows an example of temporal ordering.
Sentence: After the invoice is received the customer is obliged to pay.
Translation: implies(receipt(invoice, T 1)∧(T 2 > T 1), obl(pay(customer, T 2)))
Here implies(x, y) denotes x → y.The predicate obl denotes that the action
is an obligation (usually marked by words such as obliged to, shall, must, etc.)
in contrast to a possibility (usually marked by words such as may). T1 and T2
are the instances of time at which the two events occurred.

Words that do not contribute significantly to the meaning of the sentence can
be assigned the trivial meaning #x.x in the dictionary. It is a λ expression that
does not affect the meaning of other λ expressions. We can assign it to words such
as “is”,“to” and “the” since these do not carry much meaning in this example.
Next, we can start entering the meanings that are evident from looking at the
target representation. Since “invoice” occurs as itself, we can give it the simple
meaning invoice (similarly for “customer”). From the representation, we observe
that “received” is a function called receipt with two arguments, hence we can
give it the meaning #x.#t.receipt(x, t) (similarly for “pay”). Obliged is a more
complicated function because it takes another function (pay) as its argument
and therefore uses @y@t to carry forward the variables in pay to the next higher
level of the tree, where we obtain the real arguments (customer and T2). Once all
these meanings (Table 2) have been supplied, the system can automatically find
the meaning of the word “after” using Inverse Lambda algorithm (Fig.3). This is
remarkable from the perspective that the meaning of “after” looks complicated
and it might be tedious for users to supply such meanings manually in the initial
lexicon. This demonstrates one of the advantages of using NL2KR. The meaning
of “after” makes intuitive sense. The λ expression #x12.#x11.implies(x12 @
T1 ∧ T2 >T1,x11 @ T2) means that “after” is a λ function which takes two
inputs:x11 and x12, where the first input event (x12) occurs at time T1, T2>T1,
the second input event (x11) occurs at time T2 and x12 implies (or leads to) x11.
Hence, we were able to learn a significantly complicated meaning automatically
by providing relatively simple λ expressions in the initial dictionary.
Table 2. λ expressions and CCG categories in the initial dictionary for the sentence
“After the invoice is received the customer is obliged to pay.”
Word
invoice
is
received
customer
obliged
to
pay
the

Syntax
N
(S\NP)/NP
NP
N
NP/NP
NP/(S\NP)
S\NP
NP/N

Meaning
invoice
#x.x
#x.#t.receipt(x,t)
customer
#x.#y.#t.obl(x@y@t)
#x.x
#x.#t.pay(x,t)
#x.x

4.3 Translating to Temporal Deontic Action Laws
Giordano et al.[14] have defined a Temporal Deontic Action Language for defining temporal deontic action theories, by introducing a temporal deontic extension of Answer Set Programming(ASP) combined with Deontic Dynamic Linear
Time Temporal Logic(DDLTL). This language is used for expressing domain
description laws, for e.g., action laws, precondition laws, causal laws, etc., which
describe the preconditions and effects of actions. It is also used for expressing
obligations, for e.g., achievement obligations, maintenance obligations, contrary

Fig. 3. NL2KR auomatically learning the meaning of “after” using Inverse Lambda
Algorithm : feature=after : [(S/S)/S] : #x12.#x11.implies(x12 @ T1 ∧ T2 >T1,x11 @
T2)

to duty obligations, etc. We will take examples of several domain description
laws from the paper and demonstrate how to translate them automatically from
natural language to the Deontic action language, using NL2KR.
Since NL2KR does not support some special symbols used in the Temporal Deontic Action Language, we first use NL2KR to convert the natural language sentences to an intermediate representation which is directly convertible to
the Temporal Deontic Action Language. Then using the one-to-one correspondence between the intermediate language and the action language, we obtain
the desired representation. In the Intermediate representation shown below, we
have defined the predicate creates(x, y), which means x creates y. We have also
changed the representation of until to have two parameters a and b denoting “a
until b” (as defined by Giordano et al.[14]).
Action Law:
Sentence: The action accept price creates an obligation to pay.
Translation: [accept price]O(>U < pay > >)
Intermediate: creates(action(accept price),O(until(a(T),b(pay,T))))
The NL2KR learning component can be used to make the system learn words
and meanings from this sentence. The iterative learning process is depicted in
the screenshot in Figure 4. We start by giving meanings of simple words first.
We give trivial meanings #x.x to “the”,“an” and “to”, because they do not
significantly affect the meaning of the sentence. Next, we guess the meanings of
words from the target representation. Since “action” is a function that accepts a
single argument, we give it the meaning #x.action(x). Similarly, “accept price”
which occurs as itself is given the meaning accept price. Similarly, Obligation,
O is also a function, but it contains more structure, which can be obtained from

the target representation. We interpret an obligation to also have an implicit
notion of time by having “until” embedded in its meaning. However, the verb
“pay” should be replaceable, because there can be other sentences such as “The
action accept price creates an obligation to ship”. Therefore, we leave it as a
variable input. The word “pay” could have been simply pay but we assign it the
meaning #x.x@pay. This is because the node, “an obligation”, expects “to pay”
to be an argument to it, but their CCG categories dictate otherwise. In cases
where there is such inconsistency, we use meanings prefixed with #x.x@ for the
function (according to CCG categories), so that their role is f lipped to that
of arguments4 . The λ expressions and CCG categories of the constituent words
are shown in Table 3. After giving these meanings, we find that the meaning
of “creates” is obtained automatically by the system using the Inverse Lambda
algorithm (Fig.4).

Fig. 4. Screenshot of the Learning Process in NL2KR for the sentence “The action
accept price creates an obligation to pay.”.The meaning of “creates” is obtained automatically by the system using the Inverse Lambda algorithm

Once the learning process is complete, we can use the Translation component
of NL2KR to translate a new sentence. In this case, we use NL2KR to translate
the following action law.
Action Law:
Sentence: The action cancel payment cancels the obligation to pay.
Translation: [cancel payment]¬O(>U < pay > >)
Intermediate: cancels(action(cancel payment),O(until(a(T),b(pay,T))))
4

Let the required function be A and the required argument be B. Let the CCGdetermined function be B and the CCG-determined argument be A. Recall that
@ denotes λ application. By giving a meaning of the form #x.(x@b) to B, and
performing application as determined by CCG, we obtain the result as (#x.(x@b))@a
or a@b.

Table 3. λ expressions and CCG categories for the words in the action law “The action
accept price creates an obligation to pay.”
Word
the
action
accept price
an
obligation
to
pay

Syntax
NP/N
N/N
N
NP/N
N
(S\NP)/(S\NP)
S\NP

Meaning
#x.x
#x.action(x)
accept price
#x.x
#x.O(until(a(T),b(x,T)))
#x.x
#x.x@pay

The screenshot of the translation process is shown in Fig.5. We observe that
the sentence was automatically translated by the system successfully. This was
done by generating the meanings of unknown words (“cancel payment” and
“cancels”) using Generalization (Fig.6) on the words learned from the first action
law.

Fig. 5. Screenshot of the Translation Process in NL2KR for the sentence “The action
cancel payment cancels the obligation to pay.”

4.4 Translating to Temporal Object Logic in REALM
Regulations Expressed as Logical Models(REALM)[13] is a system that models
regulatory rules in temporal object logic. The concepts and relationships occurring in this rule are mapped to predefined types and relationships in a Unified

Modeling Language(UML) model. Using some examples from this paper, we will
show how NL2KR can be used to translate rules specified in natural language
to this temporal object logic representation.
As in the previous section, we have created an intermediate representation
which can directly be converted to the desired temporal object logic representation. This is needed due to unavailability of certain symbols in NL2KR’s vocabulary. We also assume that coreference in sentences has been resolved. For
example, in the following sentence, the second occurrence of “bank” has replaced
the pronoun “it”.
Sentence: Whenever a bank opens an account bank must verify customers identity within two days
Translation: topen (DoOnF (bank, open, a) →
♦tverif y (DoInputF (bank, verif y, a.customer.record) ∧ tverif y − topen ≤ 2[day] )
Intermediate: implies(g(do(bank, open, a, T 1)),
f (do(bank, verif y, a customer record, T 2)
∧equals(dif f erence(T 2, T 1), two days))) Similar to the previous examples, we

Fig. 6. Generating the meanings of unknown words(cancel payment and cancels) using
Generalization during the Translation Process in NL2KR for the sentence “The action
cancel payment cancels the obligation to pay.”

use NL2KR to learn unknown words from these sentences. We do not give the
meaning of “verify” for the second sentence (which is different from its meaning
in the first sentence) but the system is able to figure it out on its own. Moreover, it also generalizes the correct meaning of three days using the meaning of
two days from the previous sentence.
Sentence: Whenever a bank can not verify an identity bank has to close the
account within three days
Translation: topen (DoOnF (bank, open, a) →

♦tverif y (DoInputF (bank, verif y, a.customer.record) ∧ tverif y − topen ≤ 2[day] )
Intermediate:implies(g(do(bank, verif y, a customer record, T 1) ∧ isf alse),
f (do(bank, close, a, T 2) ∧ equals(dif f erence(T 2, T 1), three days)))
We observe that the initial dictionary for this case (Table 4) looks more
complicated than the one in Section 4.3. This is because the target language in
this case is such that the functions which would have been intuitive according to
their natural language meanings, for e.g., “opens”, “verify”, etc. are not functions
but arguments of an artificially created function, “do”. It is obvious that the
language of REALM was designed for different purposes than that of translation,
which is why such a situation exists. Our motivation here is to give the reader
an explanation of why some languages are easy for NL2KR to translate, while
others are more difficult.
Table 4. Initial Lexicon containing λ expressions and CCG categories for both REALM
examples
Word[Syntax]
whenever [(S/S)/S]
a [NP/N]
bank [N]
opens [(S\NP)/NP]
an [NP/N]
account [N]
must [(S\NP)/(S\NP)]
verify [(S\NP)/NP]

Meaning
#y.#x.implies(g(y@T1),f((x@T1)@T2))
#x.x
bank
#y.#x.#t1.do(x,open,y,t1)
#x.x
a
#x.x
#x.x@#x1.#x2.#x3.#x4.#x5.(do(x3,verify,x1,x5)
∧ x2@x4@x5)
customers [NP/N]
#x.x
identity [N]
a customer record
within [(NP\NP)/NP] #z.#y.#x.x@y@#t1.#t2.equals(difference(t2,t1),z)
has [(S\NP)/(S\NP)] #x.x
to [(S\NP)/(S\NP)]
#x.x
the [NP/N]
#x.x
can [(S\NP)/(S\NP)] #x.x
close [(S\NP)/NP]
#x.x@#x1.#x2.#x3.#x4.#x5.(do(x3,close,x1,x5)
∧ x2@x4@x5)
not [(S\NP)/(S\NP)] #y.#x.#t1.(y @ x @ t1 ∧ isfalse)
two days
two days [N]

5

Conclusion and Future Work

Although legal text is written in natural language, one needs to do some kind
of formal reasoning with it to draw conclusions. The first step to do that is
to translate legal text to an appropriate logical language. At present there is
no consensus on a single logical language to represent legal text. Therefore,
one cannot develop a translation system targeted to a single language. Thus, a
platform that can translate legal text to the desired logical language depending
on the application, is needed. We have developed such a system called NL2KR.

In this paper, we showed how NL2KR is useful in translating sentences from
legal texts in English to various formal representations defined in various works,
thereby bridging the gap from language to logical representation and enabling
the use of various logical frameworks over the information contained in such
texts.
So far we have experimented with a few small sentences picked from the literature on logical representation of legal texts. However, we need to expand this
approach to capture nuances of legal texts used in real laws and statutes. Further
enhancements are needed in NL2KR to equip it to deal with longer and more
complicated sentences. One approach that can be used would involve breaking
the sentence into smaller parts and subsequently dealing with each part separately. Such a parser, called L-Parser is available at http://bioai8core.fulton.asu.
edu/lparser. We also plan to combine statistical and logical methods in the future. In particular, we are considering using a combination of distributional
semantics and hand curated linguistic knowledge to characterize content words
(especially, noun, verbs and adjectives) and use logical characterization for grammatical words (prepositions, articles, quantifiers, negation, etc.).
Acknowledgements: We thank Arindam Mitra and Somak Aditya for their
work in developing the L-Parser. We thank NSF for the DataNet Federation
Consortium grant OCI-0940841 and ONR for their grant N00014-13-1-0334 for
partially supporting the development of NL2KR.

References
1. Jurisin legal information extraction and entailment competition. http://webdocs.
cs.ualberta.ca/~miyoung2/jurisin_task/index.html (2014)
2. Aı̈t-Mokhtar, S., Chanod, J.P., Roux, C.: Robustness beyond shallowness: Incremental deep parsing. Nat. Lang. Eng. 8(3), 121–144 (Jun 2002)
3. Androutsopoulos, I., Malakasiotis, P.: A survey of paraphrasing and textual entailment methods. J. Artif. Int. Res. 38(1), 135–187 (May 2010)
4. Bajwa, I., B., L.M., Behzad: Sbvr business rules generation from natural language
specification. In: AAAI 2011 Spring Symposium AI for Business Agility. pp. 2–8.
San Francisco, USA (2011)
5. Baral, C., Dzifcak, J., Gonzalez, M.A., Zhou, J.: Using Inverse lambda and Generalization to Translate English to Formal Languages. CoRR abs/1108.3843 (2011)
6. Blackburn, P., Bos, J.: Representation and Inference for Natural Language: A
First Course in Computational Semantics. Center for the Study of Language and
Information, Stanford, CA (2005)
7. Bos, J., Markert, K.: Recognising textual entailment with logical inference. In:
Proceedings of the Conference on Human Language Technology and Empirical
Methods in Natural Language Processing. pp. 628–635. HLT ’05, Association for
Computational Linguistics, Stroudsburg, PA, USA (2005)
8. Brüninghaus, S., Ashley, K.D.: Improving the representation of legal case texts with
information extraction methods. In: Proceedings of the 8th International Conference on Artificial Intelligence and Law. pp. 42–51. ICAIL ’01, ACM, New York,
NY, USA (2001)

9. Church, A.: An Unsolvable Problem of Elementary Number Theory. American
Journal of Mathematics 58(2), 345–363 (April 1936)
10. Costantini, S., Paolucci, A.: Towards Translating Natural Language Sentences into
ASP. In: Faber, W., Leone, N. (eds.) CILC. CEUR Workshop Proceedings, vol.
598. CEUR-WS.org (2010)
11. De Vos, M., Padget, J., Satoh, K.: Legal modelling and reasoning using institutions.
In: Proceedings of the 2010 International Conference on New Frontiers in Artificial
Intelligence. pp. 129–140. JSAI-isAI’10, Springer-Verlag, Berlin, Heidelberg (2011)
12. Distinto, I., Guarino, N., Masolo, C.: A well-founded ontological framework for
modeling personal income tax. In: Proceedings of the Fourteenth International
Conference on Artificial Intelligence and Law. pp. 33–42. ICAIL ’13, ACM, New
York, NY, USA (2013)
13. Giblin, C., Liu, A.Y., Müller, S., Pfitzmann, B., Zhou, X.: Regulations expressed as
logical models (REALM). In: Proceedings of the 2005 Conference on Legal Knowledge and Information Systems: JURIX 2005: The Eighteenth Annual Conference.
pp. 37–48. IOS Press, Amsterdam, The Netherlands, The Netherlands (2005)
14. Giordano, L., Martelli, A., Dupré, D.T.: Temporal deontic action logic for the
verification of compliance to norms in asp. In: Proceedings of the Fourteenth International Conference on Artificial Intelligence and Law. pp. 53–62. ICAIL ’13,
ACM, New York, NY, USA (2013)
15. Hoekstra, R., Breuker, J., Bello, M.D., Boer, E.: The LKIF core ontology of basic
legal concepts. In: Proceedings of the Workshop on Legal Ontologies and Artificial
Intelligence Techniques (LOAIT 2007 (2007)
16. Kimura, Y., Nakamura, M., Shimazu, A.: New frontiers in artificial intelligence.
chap. Treatment of Legal Sentences Including Itemized and Referential Expressions
— Towards Translation into Logical Forms, pp. 242–253. Springer-Verlag, Berlin,
Heidelberg (2009)
17. Lagos, N., Segond, F., Castellani, S., ONeill, J.: Event extraction for legal case
building and reasoning. In: Shi, Z., Vadera, S., Aamodt, A., Leake, D. (eds.) Intelligent Information Processing V, IFIP Advances in Information and Communication
Technology, vol. 340, pp. 92–101. Springer Berlin Heidelberg (2010)
18. McCarty, L.T.: Deep semantic interpretations of legal texts. In: Proceedings of
the 11th International Conference on Artificial Intelligence and Law. pp. 217–224.
ICAIL ’07, ACM, New York, NY, USA (2007)
19. Montague, R.: English as a Formal Language. In: Thomason, R.H. (ed.) Formal
Philosophy: Selected Papers of Richard Montague, pp. 188–222. Yale University
Press, New Haven, London (1974)
20. Nakamura, M., Nobuoka, S., Shimazu, A.: Towards translation of legal sentences
into logical forms. In: Proceedings of the 2007 Conference on New Frontiers in
Artificial Intelligence. pp. 349–362. JSAI’07, Springer-Verlag, Berlin, Heidelberg
(2008)
21. Riveret, R., Rotolo, A., Contissa, G., Sartor, G., Vasconcelos, W.: Temporal accommodation of legal argumentation. In: Proceedings of the 13th International
Conference on Artificial Intelligence and Law. pp. 71–80. ICAIL ’11, ACM, New
York, NY, USA (2011)
22. Steedman, M.: The Syntactic Process. MIT Press, Cambridge, MA (2000)
23. Zettlemoyer, L.S., Collins, M.: Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars. In: UAI. pp. 658–666.
AUAI Press (2005)

Encoding Higher Level Extensions of Petri Nets
in Answer Set Programming
Saadat Anwar1 , Chitta Baral1 , and Katsumi Inoue2
1

arXiv:1306.3548v2 [cs.AI] 24 Jun 2013

2

SCIDSE, Arizona State University, 699 S Mill Ave, Tempe, AZ 85281, USA
Principles of Informatics Research Divisions, National Institute of Informatics,
Japan

Abstract. Answering realistic questions about biological systems and
pathways similar to the ones used by text books to test understanding of
students about biological systems is one of our long term research goals.
Often these questions require simulation based reasoning. To answer such
questions, we need formalisms to build pathway models, add extensions,
simulate, and reason with them. We chose Petri Nets and Answer Set
Programming (ASP) as suitable formalisms, since Petri Net models are
similar to biological pathway diagrams; and ASP provides easy extension
and strong reasoning abilities. We found that certain aspects of biological
pathways, such as locations and substance types, cannot be represented
succinctly using regular Petri Nets. As a result, we need higher level constructs like colored tokens. In this paper, we show how Petri Nets with
colored tokens can be encoded in ASP in an intuitive manner, how additional Petri Net extensions can be added by making small code changes,
and how this work furthers our long term research goals. Our approach
can be adapted to other domains with similar modeling needs.

1

Introduction

One of our long term research objectives is to develop a system that can answer
questions similar to the ones given in the biological texts, used to test the understanding of the students. In order to answer such questions, we have to model
pathways, add interventions / extensions to them based on the question, simulate them, and reason with the simulation results. We found Petri Nets [1] to be
a suitable formalism for modeling biological pathways, as their graphical representation is very close to the biological pathways, and they can be extended to
add necessary assumptions and interventions relevant to the questions as shown
in our prequel to this paper [2]. Looking through the pathways, we found that
certain aspects of biological pathways, such as multiple locations and substance
types (perhaps connected to these locations) cannot be represented by regular
Petri Nets in a succinct manner.
Consider the simplified Petri Net model of the Electron Transport Chain [3]
in Figure 1. The chain removes high energy electrons (e) from NADH (nadh)
and delivers them to Oxygen (o2) by using electron carriers Coenzyme Q (q)
and Cytochrome C (cytc). During the process, H+ (h) ions are transported

from the Mitochondrial Matrix (mm) to the Intermembrane Space (is). The
cross-membrane H+ gradient thus produced drives ATP Synthase (not shown) to
produce ATP. The transitions t1−t4 represent multi-protein complexes that form
the chain. In order for t1 to fire, 2×NADH and 2×H+ (nadh/2, h/2) are required
at the Mitochondrial Matrix (mm). It is clear that the location information
embedded in this pathway is a vital part of its model. Regular Petri Nets that
were a focus of our previous work [2] cannot capture this location information
in a succinct way. In addition, when we use place nodes to represent locations
(as in Figure 1), regular (uncolored) tokens do not provide sufficient fidelity to
represent various token types needed as input to a transition. As a result, we have
to use Petri Nets with colored tokens [4] to model such biological pathways3 . In
contrast to our previous work, the place nodes in this Petri Net model represent
locations rather than substances. Even electron-carrier (substances) q, cytc are
locations for electrons (e) to be stored and shuttled. Colored tokens also provide a
mechanism for differentiating between separate quantities of the same substance
present in multiple locations (a common occurrence in biological systems).
t12

o2/1

h/2
nadh/2
h/2

t10

nadh/2
h/6

t1

e/2
q

nadp/2
mm

is
h/2

h/2
h2o/1

e/2
t3

h/2

o2/1

e/2
cytc

e/2

t4

h/6

Fig. 1. Petri Net with tokens of colors {e, h, h2o, nadh, nadp, o2}. Circles represent
places, and rectangles represent transitions. Arc weights such as “nadh/2, h/2”,
“h/2, h2o/1” specify the number of tokens consumed and produced during the execution of their respective transitions, where “nadh/2, h/2” means 2 tokens of color
nadh and 2 tokens of h. Similar notation is used to specify marking on places, when
not present, the place is assumed to be empty of tokens.

Numerous Petri Net modeling and simulation systems exist [5,6,7,8], but we
did not find them to be suitable for our application, either due to limited adaptability outside their intended application domain, limited extendibility, or ease
of extendibility. In addition, most systems did not explore all possible state evolutions, allowed different firing semantics, or provided a way to guide the search
through specification of partial state as way-points. We found the features, such
as intuitive encoding, easy extendibility, and strong reasoning capability in Answer Set Programming (ASP), which is a declarative programming language with
numerous competitive solvers [9]. It has been effectively used in various domains,
such as spacecrafts, work flows, natural language processing, and biological systems [10]. The suitability of ASP to analyze Petri Nets is further reinforced over
3

Though a Petri Net with colored tokens can be converted into a regular Petri Net
without colored tokens, they are usually too large and cumbersome, hence inconvenient.

other techniques, such as process algebra, temporal logics, and mathematical
equations when one considers the restrictions on Petri Nets imposed by mathematical techniques [11], the cumbersomeness of encoding in π-calculus even for
small models [12], or the lack of applicability to higher level Petri Net extensions.
Previous work on Petri Net to ASP translation has been limited to specific
classes of Petri Nets, such as regular Petri Nets [13] and Simple Logic Petri
Nets (SLPN) [14], focusing on analyzing their properties. Neither used colored
tokens. Please see our previous work [2] for more details. Though our focus
in the current work is on biological questions, our approach is equally suited
for hypothesis verification during drug design, drug interaction and biological
systems model development. It can also be applied to other domains where
Petri Nets are used for modeling and simulation, such as work flows, embedded
systems, and industrial control.
Thus, the main contributions of this paper are as follows. In Section 3 we
show how ASP allows intuitive declarative encoding of higher level Petri Net
extension of colored tokens [4]. We then show how additional extensions can be
incorporated in our encoding by making small changes. In this regard, we present
changing the firing semantics (Section 3.2), priority transitions (Section 4), and
timed transitions (Section 5). We show how Petri Nets and our encoding fit into
our ultimate research goals of answering questions about biological pathways.
We start with a brief background on ASP, multisets, and Petri Nets.

2

Fundamentals

Answer Set Programming (ASP) is a declarative logic programming language based on the Stable Model Semantics [15]. Code presented in this paper
follows the Clingo [16] syntax. The reader is referred to [16,17] for the syntax
and semantics of Answer Set Programs.
A multiset A over a domain set D is a pair hD, mi, where m : D → N
is a function giving the multiplicity of d ∈ D in A. Given two multsets A =
hD, mA i, B = hD, mB i, A  B if ∀d ∈ D : mA (d)  mB (d), where  ∈ {<, >
, ≤, ≥, =}, and A 6= B if ∃d ∈ D : mA (d) 6= mB (d). Multiset sum/difference is
defined in the usual way. We use the short-hands d ∈ A to represent mA (d) > 0,
A = ∅ to represent ∀d ∈ D, m(d) = 0, A ⊗ n to represent ∀d ∈ D, m(d) ⊗ n,
where n ∈ N, ⊗ ∈ {<, >, ≤, ≥, =, 6=}. We use the notation d/n ∈ A to represent
that d appears n-times in A; we drop A when clear from context. The reader is
referred to [18] for details.
A basic Petri Net [1] is a bipartite graph of a finite set of place nodes
P = {p1 , . . . , pn }, and transition nodes T = {t1 , . . . , tm } connected through
directed arcs E = E + ∪ E − . An arc goes from a place to a transition E − ⊆
P × T or a transition to a place E + ⊆ T × P . The state of a Petri Net is
defined by the token allocation of all place nodes, collectively called its marking
M = (M (p1 ), . . . , M (pn )), M (pi ) ∈ N. Arc weights W : E → N \ {0} specify
the number of tokens consumed or produced from place nodes at the head or
tail of the arcs due to firing of a transition. Modeling capability of basic Petri

Nets is enhanced by adding reset, inhibit and read arcs. Reset arcs R : T → 2P
remove all tokens from their source places when fired. Inhibitor arcs I : T → 2P
prevent their transitions from firing until their source places are empty. Read
arcs Q ⊆ P × T prevent their transitions from firing until their source places
have at least the tokens specified by read arc weights QW : Q → N \ {0}.
Higher level Petri Nets extend the notion of tokens to typed (or colored)
tokens. A Petri Net with Colored Tokens (with reset, inhibit and read arcs)
is a tuple P N C = (P, T, E, C, W, R, I, Q, QW ), where P, T, E, R, I, Q are the
same as for basic Petri Nets, C = {c1 , . . . , cl } is a finite set of colors (or types),
and arc weights W : E → hC, mi, QW : Q → hC, mi are specified as multisets of colored tokens over color set C. The state (or marking) of place nodes
M (pi ) = hC, mi, pi ∈ P is specified as a multiset of colored tokens over set C.
We will now define a number of concepts about Petri Nets used in this paper.
The initial marking is the initial token assignment of place nodes and is represented by M0 . The marking at time-step k is written as Mk . The pre-set (or
input-set) of a transition t is •t = {p ∈ P |(p, t) ∈ E − }, while the post-set (or
output-set) is t• = {p ∈ P |(t, p) ∈ E + }. A transition t is enabled with respect
to marking M , enabledM (t), if each of its input places p has at least the number
of colored-tokens as the arc-weight W (p, t)4 , each of its inhibiting places pi ∈ I(t)
have zero tokens and each of its read places pq : (pq , t) ∈ Q have at least the number of colored-tokens as the read-arc-weight QW (pq , t), i.e. (∀p ∈ •t, W (p, t) ≤
M (p)) ∧ (∀p ∈ I(t), M (p) = ∅) ∧ (∀(p, t) ∈ Q, M (p) ≥ QW (p, t)) for a given t.
Any number of enabled transitions may fire simultaneously as long as they don’t
conflict. The set Tk = {tk1 , . . . , tkn } ⊆ T of such simultaneously firing transitions
is called a firing set. Execution of a firing set Tk on a marking
P Mk computes a
new
marking
M
as:
∀p
∈
P
\R(T
),
M
(p)
=
M
(p)−
k+1
k
k+1
k
t∈Tk ∧p∈•t W (p, t)+
P
P
W
(t,
p),
∀p
∈
R(T
),
M
(p)
=
W
(t,
p), where R(Tk ) =
k
k+1
t∈Tk ∧p∈t•
S t∈Tk ∧p∈t•
R(t).
A
set
of
transitions
T
⊆
{t
:
enabled
(t)}
is
in conflict conc
Mk
t∈Tk
C
flict in P N with respect to Mk if firing them will consume more tokens than
are
P available at one ofPtheir common input places, i.e., ∃p ∈ P : Mk (p) <
( t∈Tc ∧p∈•t W (p, t) + t∈Tc ∧p∈R(t) Mk (p))5 . An execution sequence is the
simulation of a firing sequence σ = T0 , T1 , . . . , Tk , where each Ti ⊆ T, 0 ≤ i ≤ k
is a firing set. It is the transitive closure of executions, where subsequent markings become the initial marking for the next transition set. Thus in the execution
sequence X = M0 , T0 , M1 , T1 , . . . , Tk , Mk+1 , the firing of T0 at M0 produces M1 ,
which becomes initial marking for T1 .
4

5

In the following text, for simplicity, we will use W (p, t) to mean W (hp, ti). We use
similar simpler notation for QW .
The reset arc is involved here because we use a modified execution semantics of
reset arcs compared to the standard definition [19]. Even though both capture similar
operation, our definition allows us to model elimination of all quantity of a substance
as soon as it is produced, even in a maximal firing set semantics. Our semantics
considers reset arc’s token consumption in contention with other arcs, while the
standard definition does not.

If the Figure 1 Petri Net has the marking: M0 (mm) = [nadh/2, h/6], M0 (q) =
[e/2], M0 (cytc) = [e/2], M0 (is) = [o2/1], then transitions t1, t3, t4 are enabled.
However, either {t1, t3} or {t4} can fire simultaneously in a single firing at time
0 due to limited h tokens in mm. t4 is said to be in conflict with t1, t3.

3

Translating Petri Nets with Colored Tokens to ASP

In this section we present an ASP encoding of a Petri Net with Colored Tokens
P N C , with an initial marking M0 and a simulation length k. This work extends
our encoding of regular Petri Nets in [2]. The following sections will show how
Petri Net extensions can be easily added to this initial encoding. We represent the
Petri Net P N C with initial marking M0 , and simulation time with the following
facts and rules.
Facts place(pi ) where pi ∈ P is a place.
Facts trans(tj ) where tj ∈ T is a transition.
Facts col(ck ) where ck ∈ C is a color.
Rules ptarc(pi , tj , nc , c, tsk ) :- time(tsk ). for each (pi , tj ) ∈ E − , c ∈ C,
nc = mW (pi ,tj ) (c) : nc > 0.6
f5: Rules tparc(ti , pj , nc , c, tsk ) :- time(tsk ). for each (ti , pj ) ∈ E + , c ∈ C,
nc = mW (ti ,pj ) (c) : nc > 0.
f6: Rules ptarc(pi , tj , nc , c, tsk ) :- holds(pi , nc , c, tsk ), num(nc ), nc >0, time(tsk ).
for each (pi , tj ) : pi ∈ R(tj ), c ∈ C, nc = mMk (pi ) (c).
f7: Rules iptarc(pi , tj , 1, c, tsk ) :- time(tsk ). for each (pi , tj ) : pi ∈ I(tj ), c ∈ C.
f8: Rules tptarc(pi , tj , nc , c, tsk ) :- time(tsk ). for each (pi , tj ) ∈ Q, c ∈ C, nc =
mQW (pi ,tj ) (c) : nc > 0.
i1: Facts holds(pi , nc , c, 0). for each place pi ∈ P, c ∈ C, nc = mM0 (pi ) (c).
f9: Facts time(tsi ) where 0 ≤ tsi ≤ k are the discrete simulation time steps.
f10: Facts num(n) where 0 ≤ n ≤ ntok are token quantities7
f1:
f2:
f3:
f4:

Next, we encode Petri Net’s execution behavior, which proceeds in discrete time steps. For a transition ti to be enabled, it must satisfy the following
conditions: (i) @pj ∈ •ti : M (pj ) < E − (pj , ti ), (ii) @pj ∈ I(ti ) : M (pj ) > 0, and
(iii) @(pj , ti ) ∈ Q : M (pj ) < QW (pj , ti ) . These three conditions are encoded as
e1, e2, e3, respectively and we encode the absence of any of these conditions for
a transition as e4:
e1: notenabled(T,TS) :- ptarc(P,T, N,C,TS), holds(P,Q,C,TS), place(P),
trans(T), time(TS), num(N), num(Q), col(C), Q<N.

e2: notenabled(T,TS) :- iptarc(P,T,N,C,TS), holds(P,Q,C,TS), place(P),
trans(T), time(TS), num(N), num(Q), col(C), Q>=N.
6

7

The time parameter tsk allows us to capture reset arcs, which consume tokens equal
to the current (time-step based) marking of their source nodes.
The token count predicate num’s limit can be arbitrarily selected to be higher than
the expected token count. It is there for efficient ASP grounding and not to impose
a limit on the token quantity.

e3: notenabled(T,TS) :- tptarc(P,T,N,C,TS), holds(P,Q,C,TS), place(P),
trans(T), time(TS), num(N), num(Q), col(C), Q<N.

e4: enabled(T,TS) :- trans(T), time(TS), not notenabled(T,TS).
Rule e1 captures the existence of an input place P with insufficient number of
tokens for transition T to fire. Rule e2 captures existence of a non-empty source
place P of an inhibitor arc to T preventing T from firing. Rule e3 captures
existence of a source place P with less than arc-weight tokens required by the
read arc to transition T for T to be enabled. The, holds(P,Q,C,TS) predicate
captures the marking of place P at time T S as Q tokens of color C. Rule e4
captures enabling of transition T when no reason for it to be not enabled is
determined by e1, e2, e3. In a biological context, this enabling is equivalent to a
reaction’s pre-conditions being satisfied. A reaction can proceed when its input
substances are available in the required quantities, it is not inhibited, and any
required activation quantity of activating substances is available.
Any subset of enabled transitions can fire simultaneously at a given time-step.
We select a subset of fireable transitions using a choice rule:
a1: {fires(T,TS)} :- enabled(T,TS), trans(T), time(TS).
The choice rule a1 either picks an enabled transition T for firing at time T S
or not. The combined effect over all transitions is to pick a subset of enabled
transitions to fire. Whether these transitions are in conflict are checked by later
rules a2, a3, a4. In a biological context, the multiple firing models parallel processes occurring simultaneously. The marking is updated according to the firing
set using the following rules:
r1: add(P,Q,T,C,TS) :- fires(T,TS), tparc(T,P,Q,C,TS), time(TS).
r2: del(P,Q,T,C,TS) :- fires(T,TS), ptarc(P,T,Q,C,TS), time(TS).
r3: tot incr(P,QQ,C,TS) :- col(C), QQ = #sum[add(P,Q,T,C,TS) = Q : num(Q) :
trans(T)], time(TS), num(QQ), place(P).

r4: tot decr(P,QQ,C,TS) :- col(C), QQ = #sum[del(P,Q,T,C,TS) = Q : num(Q) :
trans(T)], time(TS), num(QQ), place(P).

r5: holds(P,Q,C,TS+1):-place(P),num(Q;Q1;Q2;Q3),time(TS),time(TS+1),col(C),
holds(P,Q1,C,TS), tot incr(P,Q2,C,TS), tot decr(P,Q3,C,TS), Q=Q1+Q2-Q3.

Rules r1 and r2 capture that Q tokens of color C will be added or removed
to/from place P due to firing of transition T at the respective time-step T S.
Rules r3 and r4 aggregate these tokens for each C for each place P (using
aggregate assignment QQ = #sum[...]) at the respective time-step T S. Rule r5
uses the aggregates to compute the next marking of P for color C at the timestep (T S + 1) by subtracting removed tokens and adding added tokens to the
current marking. In a biological context, this captures the effect of a process /
reaction, which consumes its inputs and produces outputs for the downstream
processes. We capture token overconsumption using the following rules:
a2: consumesmore(P,TS) :- holds(P,Q,C,TS), tot decr(P,Q1,C,TS), Q1 > Q.
a3: consumesmore :- consumesmore(P,TS).

a4: :- consumesmore.
Rule a2 determines whether firing set selected by a1 will cause overconsumption of tokens at P at time T S by comparing available tokens to aggregate tokens
removed as determined by r4. Rule a3 generalizes the notion of overconsumption, while rule a4 eliminates answer with such overconsumption. In a biological
context, conflict (through overconsumption) models the limitation of input substances, which dictate which downstream processes can occur simultaneously.
Proposition 1. Let P N C be a Petri Net with colored tokens, reset, inhibit, and
read arcs and M0 be an initial marking and let Π 3 (P N C , M0 , k) be the ASP
encoding of P N C and M0 over a simulation of length k as defined in Section 3.
Then X 3 = M0 , T0 , M1 , . . . , Tk is an execution sequence of P N C (with respect
to M0 ) iff there is an answer-set A of Π 3 (P N C , M0 , k) such that: {f ires(t, j) :
t ∈ Tj , 0 ≤ j ≤ k} = {f ires(t, ts) : f ires(t, ts) ∈ A} and {holds(p, q, c, j) : p ∈
P, c/q ∈ Mj (p), 0 ≤ j ≤ k} = {holds(p, q, c, ts) : holds(p, q, c, ts) ∈ A}
3.1

Example Execution

Given the above translation rules, the following facts and rules encode the Petri
Net in Figure 1 with an initial marking of zero tokens8 :
time(0..5). num(0..30). place(mm;is;q;cytc). trans(t1;t3;t4;t10;t12).
col(nadh;h;e;nadp;h2o;o2). holds(mm,0,nadh,0). holds(mm,0,h,0).
tparc(t12,is,1,o2,TS):-time(TS). tparc(t10,mm,6,h,TS):-time(TS).
tparc(t10,mm,2,nadh,TS):-time(TS). ptarc(mm,t1,2,nadh,TS):-time(TS).

We get thousands of answer-sets, for example9 :
fires(t10;t12,0) holds(is,1,o2,1) holds(mm,6,h,1) holds(mm,2,nadh,1)
fires(t1;t10;t12,1) holds(is,2,h,2) holds(is,2,o2,2) holds(mm,10,h,2)
holds(mm,2,nadh,2) holds(mm,2,nadp,2) holds(q,2,e,2)
fires(t1;t3;t10;t12,2) holds(cytc,2,e,3) holds(is,6,h,3) holds(is,3,o2,3)
holds(mm,12,h,3) holds(mm,2,nadh,3) holds(mm,4,nadp,3) holds(q,2,e,3)
fires(t1;t3;t4;t10;t12,3) holds(cytc,2,e,4) holds(is,12,h,4)
holds(is,1,h2o,4) holds(is,3,o2,4) holds(mm,8,h,4) holds(mm,2,nadh,4)
holds(mm,6,nadp,4) holds(q,2,e,4)
fires(t3;t4;t10;t12,4) holds(cytc,2,e,5) holds(is,16,h,5)
holds(is,2,h2o,5) holds(is,3,o2,5) holds(mm,6,h,5) holds(mm,4,nadh,5)
holds(mm,6,nadp,5) fires(t1;t10;t12,5)

3.2

Changing Firing Semantics

The above code implements a set firing semantics, which can produce a large
number of answer-sets10 . In biological domain, it is often preferable to simulate
8

9

10

We show a few of the tparc/5, ptarc/5, holds/4 to illustrate the approach, the
rest of them can be encoded in a similar fashion.
We are only showing colored tokens with non-zero quantity. Also, we are representing
fires(t1,ts),...,fires(tm,ts) as fires(t1;...;tm,ts) to conserve space.
A subset of a firing set can also be fired as a firing set by itself.

the maximum parallel activity at each time step. We accomplish this by enforcing
the maximal firing set semantics by extending its encoding for regular Petri
Nets in [2] to colored tokens as follows:
a5: could not have(T,TS):-enabled(T,TS),not fires(T,TS), ptarc(S,T,Q,C,TS),
holds(S,QQ,C,TS), tot decr(S,QQQ,C,TS), Q > QQ - QQQ.

a6: :- not could not have(T,TS), time(TS), enabled(T,TS), not fires(T,TS),
trans(T).

Rule a5 captures the fact that transition T , though enabled, could not have
fired at T S, as its firing would have caused overconsumption. Rule a6 eliminates any answers where an enabled transition could have fired without causing
overconsumption but did not. This modification reduces the number of answers
produced for the Petri Net in Figure 1 to 4. We can encode other firing semantics with similar ease11 . We now look at how additional extensions can be easily
encoded by making small code changes.

4

Extension - Priority Transitions

Priority transitions enable ordering of Petri Net transitions, favoring high priority transitions over lower priority ones [20]. In a biological context, this is used to
model primary (or dominant) vs. secondary pathways / processes in a biological
system. This prioritization may be due to an intervention (such as prioritizing
elimination of a metabolite over recycling it).
A Priority Colored Petri Net with reset, inhibit, and read arcs is a tuple
P N pri = (P, T, E, C, W, R, I, Q, QW, Z), where: P, T, E, C, W, R, I, Q, QW are
the same as for P N C , and Z : T → N is a priority function that assigns priorities to transitions. Lower number signifies higher priority. A transition ti
is enabled in P N pri if it would be enabled in P N C (with respect to M) and
there isn’t another transition tj that would be enabled in P N C (with respect to
M) s.t. Z(tj ) < Z(ti ). We add the following facts and rules to encode transition
priority and enabled priority transitions:
f11: Facts transpr(ti ,pri ) where pri is t0i s priority.
a7: notprenabled(T,TS) :- enabled(T,TS), transpr(T,P), enabled(TT,TS),
transpr(TT,PP), PP < P.

a8: prenabled(T,TS) :- enabled(T,TS), not notprenabled(T,TS).
Rule a7 captures that an enabled transition T is not priority-enabled, if
there is another enabled transition with higher priority at T S. Rule a8 captures
that transition T is priority-enabled at T S since there is no enabled transition
with higher priority. We replace rules a1, a5, a6 with a9, a10, a11 respectively to
propagate priority as follows:
11

For example, if interleaved semantics is desired, rules a5, a6 can changed to capture
and eliminate answer-sets in which more than one transition fires in a firing set as:
a5’: more than one fires :- fires(T1,TS),fires(T2,TS),T1!=T2,time(TS).
a6’: :-more than one fires.

a9: {fires(T,TS)} :- prenabled(T,TS), trans(T), time(TS).
a10: could not have(T,TS) :- prenabled(TS,TS), not fires(T,TS),
ptarc(S,T,Q,C,TS), holds(S,QQ,C,TS), tot decr(S,QQQ,C,TS), Q > QQ - QQQ.

a11: :- prenabled(tr,TS), not fires(tr,TS), time(TS).
Rules a9, a10, a11 perform the same function as a1, a5, a6, except that they
consider only priority-enabled transitions as compared all enabled transitions.
Proposition 2. Let P N pri be a Petri Net with colored tokens, reset, inhibit,
read arcs and priority based transitions and M0 be an initial marking and let
Π 5 (P N pri , M0 , k) be the ASP encoding of P N pri and M0 over a simulation of
length k as defined in Section 4. Then X 5 = M0 , T0 , M1 , . . . , Tk is an execution sequence of P N pri (with respect to M0 ) iff there is an answer-set A of
Π 5 (P N pri , M0 , k) such that: {f ires(t, j) : t ∈ Tj , 0 ≤ j ≤ k} = {f ires(t, ts) :
f ires(t, ts) ∈ A}, and {holds(p, q, c, j) : p ∈ P, c/q ∈ Mj (p), 0 ≤ j ≤ k}
= {holds(p, q, c, ts) : holds(p, q, c, ts) ∈ A}

5

Extension - Timed Transitions

Biological processes vary in time required for them to complete. Timed transitions [21] model this variation of duration. The timed transitions can be reentrant
or non-reentrant12 . We extend our encoding to allow reentrant timed transitions.

cytc'
e/2
h/2

[2]
q'

e/2

[2]

e/2
cytc
h/2

tq
e/2

t10

nadh/2
h/6

mm

nadp/2
nadh/2
h/2

e/2
t1

e/2

e/2
t3

e/2

tcytc

t12

q

o2/1

is

h/2
h2o/1
o2/1

t4

h/2

h/6

Fig. 2. An extended version of the Petri Net model from Fig. 1. The new transitions
tq, tcytc have a duration of 2 each (shown in square brackets (“[ ]”) next to the transition). When missing, transition duration is assumed to be 1.
12

A reentrant transition is like a vehicle assembly line, which accepts new parts
while working on multiple vehicles at various stages of completion; whereas a nonreentrant transition only accepts new input when the current processing is finished.

A Priority Colored Petri Net with Timed Transitions, reset, inhibit, and query arcs is a tuple P N D = (P, T, E, C, W, R, I, Q, QW, Z, D), where
P, T, E, C, W, R, I, Q, QW, Z are the same as for P N pri , and D : T → N \ {0} is
a duration function that assigns positive integer durations to transitions.
Figure 2 shows an extended version of Petri Net model of the Electron Transport Chain [3] shown in Figure 1. The new transitions tq and tcytc (shown in
dotted outline) are timed transitions modeling the speed of the small carrier
molecules, Coenzyme Q (q) and Cytochrome C (cytc) as an effect of membrane
fluidity. Higher numbers for transition duration represent slower movement of
the carrier molecules due to lower fluidity. Execution in P N D changes, since
the token update from Mk to Mk+1 can involve transitions that started at
some time l before time k, but finish at k + 1. Thus, the P
new marking is computed
as
follows:
∀p
∈
P
\
R(T
),
M
(p)
=
M
(p)
−
k
k+1
k
t∈Tk ∧p∈•t W (p, t) +
P
W
(t,
p),
and
∀p
∈
R(T
),
k
t∈Tl ∧p∈t•:l≤k,l+D(t)=k+1
P
Mk+1 (p) = t∈Tl ∧p∈t•:l≤k,l+D(t)=k+1 W (t, p), where R(Ti ) = ∪t∈Ti R(t).
A timed transition t produces its output D(t) time units after being fired.
We replace f 5 with f 12 adding transition duration and replace rule r1 with r6
that produces tokens at the end of transition duration 13 :
f12: Rules tparc(ti , pj , nc , c, tsk , D(ti )):-time(tsk ). for each (ti , pj ) ∈ E + , c ∈ C,
nc = mW (ti ,pj ) (c) : nc > 0.
r6: add(P,Q,T,C,TSS):-fires(T,TS),time(TS;TSS), tparc(T,P,Q,C,TS,D),
TSS=TS+D-1.

Proposition 3. Let P N D be a Petri Net with colored tokens, reset, inhibit,
read arcs and priority based timed transitions and M0 be an initial marking and
let Π 6 (P N D , M0 , k) be the ASP encoding of P N D and M0 over a simulation
of length k as defined in Section 5. Then X 6 = M0 , T0 , M1 , . . . , Tk is an execution sequence of P N D (with respect to M0 ) iff there is an answer-set A of
Π 6 (P N D , M0 , k) such that: {f ires(t, j) : t ∈ Tj , 0 ≤ j ≤ k} = {f ires(t, ts) :
f ires(t, ts) ∈ A}, and {holds(p, q, c, j) : p ∈ P, c/q ∈ Mj (p), 0 ≤ j ≤ k}
= {holds(p, q, c, ts) : holds(p, q, c, ts) ∈ A}

6

Example Use of Our Encoding and Reasoning Abilities

We illustrate the usefulness of our encoding by applying it to the following
simulation based reasoning question14 from [3]: “Membranes must be fluid to
function properly. How would decreased fluidity of the membrane affect the
efficiency of the electron transport chain?”
13

We can easily make these timed transitions non-reentrant by adding rule e5 that
disallows a transition from being enabled if it is already in progress:

e5: notenabled(T,TS1):-fires(T,TS0), num(N), TS1>TS0,
tparc(T,P,N,C,TS0,D), col(C), time(TS0), time(TS1), TS1<(TS0+D).
14

As it appeared in https://sites.google.com/site/2nddeepkrchallenge/

To answer this question, first we build a Petri Net model of the Electron
Transport Chain, including its interplay with the membrane. Our model is shown
in Figure 1. We base our model on [3, Figure 9.15], which shows multiple substances flowing through the protein complexes that form this chain. For example,
the first complex (t1), removes electrons from NADH (nadh) arriving at the Mitochondrial Matrix (mm) and delivers them to the mobile carrier Coenzyme Q
(q), converting NADH to NAD+ (nadp). As a side effect, t1 also moves H+ (h)
ions from mm to the Intermembrane Space (is). The speed at which q (which
lives in the membrane) shuttles electrons to next complex depends upon the
membrane fluidity.
To answer the question, we need to model change in fluidity and its impact
on the mobile carriers. Background knowledge tells us that lower fluidity leads
to slower movement of mobile carriers, leading to longer transit times. We model
this using an intervention to the Petri Net model of Figure 1, extending it with
additional delay transitions in the path of q and Cytochrome C (cytc), as shown
with dotted outline in Figure 2. We encode both models in ASP using encodings
from Sections 3 and 5, respectively, and simulate them for a fixed number of
time-steps ts using the maximal firing set semantics from Section 3.2. A plot of
H+ produced over time is shown in Figure 3. We compute the efficiency of the
electron transport chain as the quantity of H+ (“h”) ions moved (from “mm”)
h
”. We found that this value
to “is” over the simulation duration “ts”, i.e. “ ts
decreased from 4.5 to 3 with decrease of membrane fluidity (modeled as timed
transitions of duration 2). Thus, our results show that decreased fluidity of the
membrane results in lowering the efficiency of the electron transport chain.

45

normal fluidity
lower fluidity (dur=2); reentrant
lower fluidity (dur=2); non-reentrant

40
35
H+ quantity

30
25
20
15
10
5
0

0

1

2

3

4
time step

5

6

7

8

Fig. 3. H+ production in the intermembrane space over time for the normal fluidity,
lower fluidity (reentrant), and lower fluidity (non-reentrant transitions).

If we permit additional background knowledge about the mobile carriers, we
can refine our ASP encoding, modeling the mobile carriers with non-reentrant
timed transitions (Section 5 Footnote 13) 15 . Repeating our simulation with nonreentrant timed transitions results in an efficiency value of 2.5, which is a larger
reduction in the efficiency of the electron chain due to decreased fluidity.
ASP’s enumeration of the entire simulation evolution allows us to perform
additional reasoning not directly possible with Petri Nets. For example, partial
state or firing sequence can be encoded (as ASP constraints) as way-points to
guide the simulation. A simple use case is to enumerate answer-sets where a
transition t fires when one of its upstream source products S is found to be
depleted. These answer-sets are used to identify another upstream substance
responsible for t’s firing. Our encoding allows various Petri Net dynamic and
structural properties to be easily analyzed, as described in our previous work [2].

7

Related Work and Conclusion

Now we look at some of the existing Petri Net systems that support higher level
Petri Net constructs16 (focusing on the ones used for biological modeling). We
also look at some ways existing Petri Net tools are used for biological analysis
and put them in context of our research.
CPN Tools [5], Renew [8], Snoopy [6] all support Colored Petri Nets. All
but CPN Tools directly support inhibit and reset arcs. All but Snoopy are limited to one particular firing semantics, while Snoopy allows three distinct firing
semantics. Neither pursues more than one simulation and all break ties arbitrarily. Cell Illustrator [7] does not support colored tokens, but does provide
a rich graphical environment with pathway representation similar to standard
biological pathways. It also only supports one possible evolution.
Petri Nets have been previously used to analyze biological pathways [22,23,24],
but most of this analysis has been limited to dynamic and structural properties
of the Petri Net model. [25] took a different approach, where they surveyed the
Petri Net implementations and came up with questions answerable by each. Contrary to the previous work, we focus on real world biological questions as they
appear in college level biological text books; we model these questions as Petri
Net extensions; and leverage ASP as a rich reasoning environment.
Conclusion: In this paper we presented the suitability of using Petri Nets
with colored tokens for modeling biological pathways. We showed how such Petri
Nets can be intuitively encoded in ASP, simulated, and reasoned with, in order
to answer real world questions posed in the biological texts. We showed how our
initial encoding can be easily extended to include additional extensions, such
as maximal firing semantics, priority transitions, and timed transitions. Our
encoding has a low specification-implementation gap, it allows enumeration of
15

16

Similar modeling is also possible by using inhibitor arcs from mobile carriers to the
transitions preceding them.
The Petri Net Tools Database web-site summarizes a large slice of existing tools
http://www.informatik.uni-hamburg.de/TGI/PetriNets/tools/quick.html

all possible state evolutions, the ability to guide the search by specifying waypoints (such as partial state), and a strong reasoning ability. Our focus in this
work is more on encoding flexibility, exploring all possible state evolutions, and
reasoning capabilities; and less on performance. We showcased the usefulness of
our encoding by an example. We briefly compared our work to other Petri Net
systems and their use in biological modeling and analysis. In follow on papers,
we will extend our work to include the High-level Petri Net standard.

References
1. Petri, C.A.: Kommunikation mit Automaten. Technical report, Institut für Instrumentelle Mathematik, Bonn, Germany (1962)
2. Anwar, S., Baral, C., Inoue, K.: Encoding Petri nets in Answer Set Programming
for simulation based reasoning. http://arxiv.org/abs/1306.3542 (2013)
3. Reece, J., Cain, M., Urry, L., Minorsky, P., Wasserman, S.: Campbell Biology.
Pearson Benjamin Cummings (2010)
4. Peterson, J., et al.: A note on colored Petri nets. Information Processing Letters
11(1) (1980) 40–43
5. Jensen, K., Kristensen, L., Wells, L.: Coloured Petri Nets and CPN Tools for
modelling and validation of concurrent systems. International Journal on Software
Tools for Technology Transfer (STTT) 9(3) (2007) 213–254
6. Heiner, M., Herajy, M., Liu, F., Rohr, C., Schwarick, M.: Snoopy - a unifying Petri
net tool. In: Application and Theory of Petri Nets. Volume 7347 of Lecture Notes
in Computer Science. (2012) 398–407
7. Nagasaki, M., Saito, A., Jeong, E., Li, C., Kojima, K., Ikeda, E., Miyano, S.: Cell
illustrator 4.0: A computational platform for systems biology. In Silico Biology
10(1) (2010) 5–26
8. Kummer, O., Wienberg, F., Duvigneau, M.: Renew–the reference net workshop.
Petri Net Newsletter 56 (1999) 12–16
9. Gebser, M., Liu, L., Namasivayam, G., Neumann, A., Schaub, T., Truszczyński,
M.: The first answer set programming system competition. In: Logic Programming
and Nonmonotonic Reasoning. Springer Berlin Heidelberg (2007) 3–17
10. Brewka, G., Eiter, T., Truszczyński, M.: Answer set programming at a glance.
Communications of the ACM 54(12) (2011) 92–103
11. Murata, T.: Petri nets: Properties, analysis and applications. Proceedings of the
IEEE 77(4) (1989) 541–580
12. van der Aalst, W.: Pi calculus versus Petri nets: Let us eat “humble pie” rather
than further inflate the “pi hype”. BPTrends 3(5) (2005) 1–11
13. Heljanko, K., Niemelä, I.: Bounded LTL model checking with stable models. In:
Logic Programming and Nonmotonic Reasoning. Springer (2001) 200–212
14. Behrens, T.M., Dix, J.: Model Checking with Logic Based Petri Nets. Technical
report ifi-07-02, Insitut für Informatik, Technische Universität Clausthal JuliusAlbert Str. 4, 38678 Clausthal-Zellerfeld, Germany, Clausthal University of Technology, Dept of Computer Science (May 2007)
15. Gelfond, M., Lifschitz, V.: The stable model semantics for logic programming.
Logic Programming: Proceedings of the Fifth International Conference and Symposium (1988) 1070–1080

16. Gebser, M., Kaminski, R., Kaufmann, B., Ostrowski, M., Schaub, T., Schneider,
M.: Potassco: The Potsdam answer set solving collection. aicom 24(2) (2011)
105–124
17. Baral, C.: Knowledge Representation, Reasoning and Declarative Problem Solving.
Cambridge University Press (2003)
18. Syropoulos, A.: Mathematics of multisets. Multiset Processing (2001) 347–358
19. Araki, T., Kasami, T.: Some decision problems related to the reachability problem
for Petri nets. Theoretical Computer Science 3(1) (1976) 85–104
20. Best, E., Koutny, M.: Petri net semantics of priority systems. Theoretical Computer Science 96(1) (1992) 175–215
21. Ramchandani, C.: Analysis of asynchronous concurrent systems by Petri nets.
Technical report, DTIC Document (1974)
22. Sackmann, A., Heiner, M., Koch, I.: Application of Petri net based analysis techniques to signal transduction pathways. BMC Bioinformatics 2(7) (November
2006) 482
23. Hofestädt, R., Thelen, S.: Qualitative modeling of biochemical networks. In Silico
Biology 1 (1998) 39–53
24. Li, C., Suzuki, S., Ge, Q.W., Nakata, M., Matsuno, H., Miyano, S.: Structural
modeling and analysis of signaling pathways based on Petri nets. Journal of bioinformatics and computational biology 4(05) (2006) 1119–1140
25. Peleg, M., Rubin, D., Altman, R.B.: Using Petri net tools to study properties
and dynamics of biological systems. Journal of the American Medical Informatics
Association 12(2) (2005) 181–199

TOXICOLOGIC PATHOLOGY, vol 31(Suppl.), pp 49–52, 2003
C 2003 by the Society of Toxicologic Pathology
Copyright °
DOI: 10.1080/01926230390174922

Issues Related to the Use of Fish Models in Toxicologic Pathology:
Session Introduction
J. MCHUGH LAW
College of Veterinary Medicine, North Carolina State University, Raleigh, North Carolina 27606
ABSTRACT
Ready or not, fish models are “here to stay.” No longer are fish confined to a few specialized laboratories, nor are they exclusively the purview of
zoologists or environmental toxicologists. In fact, the institution that does not house at least 1 fish facility is probably not at the forefront of cutting edge
research. In toxicologic pathology, fish models are increasingly being used to provide high animal numbers at relatively low cost in carcinogenicity
testing and developmental research, and to provide mechanistic information on fundamental cellular processes. In this session, we attempt to provide
some perspective for the pathologist that is faced with planning or performing experiments or testing protocols using fish models, or with reading or
interpreting fish studies. First, we cover how to approach fish studies from the contract laboratory standpoint, including sectioning, quality control, and
GLP considerations. Then, we discuss specifics on the use of the rainbow trout, zebrafish, and Japanese medaka models. The rainbow trout has a rich
history in carcinogenicity and mechanistic cancer research. Similarly, the 2 workhorses in the small fish category, zebrafish and medaka, have found
their way into many laboratories doing developmental biology and genomics research as well as carcinogenicity testing. Some fascinating genetically
altered fish models have been developed with both of these species. This manuscript provides a session overview of the use of small fish models in
toxicologic pathology, along with some historical perspective on how these models have played a role in the current state of the science.
Keywords. Carcinogenicity testing; fish models; Japanese medaka; rainbow trout; zebrafish.

INTRODUCTION
The new century finds us more than ever before in an age of
rapid discovery, development, and marketing of pharmaceuticals and other chemicals that affect humans and, ultimately,
all other biological systems. Hence, the need for rapid safety
assessment of these compounds as well as for mechanistic
developments in basic research is greater than ever. Despite
calls for reducing the numbers of animals used in these efforts, ever increasing regulatory requirements and awareness
of product liability demand more precision in safety testing
and necessitate the use of a greater variety of test protocols
in whole animals. In environmental impact studies, the enhanced resolution afforded by technological advancements in
analytical instrumentation in turn demands the assessment of
lower and lower (ie, more realistic or “environmentally relevant”) dose levels, thus requiring larger and larger numbers
of test animals (higher “n”).
At the same time, societal pressures against animal testing have squeezed toxicologists and risk assessors from the
other direction. Short-term, in vitro assays have been useful
in mass screening of potentially toxic compounds. Although
many of these assays are rapid and economical, their validity has been limited somewhat by false positives and false
negatives, and by an inherent inability to determine target
organ-specific effects such as carcinogenicity or promoting
activity (20). In addition, the total milieu including cell-cell
and cell-matrix interactions is not included in such assessments. Because whole animal testing remains the benchmark
for safety assessment and even basic research, the use of

“alternative” animal models in toxicity and carcinogenicity
testing has received considerable attention (40). In 1993, the
US Congress instructed the National Institutes of Health to
investigate the use of alternative animal models. Specifically,
this called for replacing animals with in vitro tests, chemical reactions, and computer models; reducing the numbers
of animals used; and refining current methods, emphasizing
relief of pain, maximizing information obtained from each
animal, and utilizing animals lower on the phylogenetic tree
(40). In the year 2000, the National Toxicology Program Interagency Coordinating Committee on the Validation of Alternative Methods was established to assist in these efforts.
Transgenic mouse models, such as the Tg.AC (zetaglobin
promoted v-Ha-ras) transgenic mouse (48), will answer some
specific questions that require using whole animals, but these
specialized models are costly and often difficult to obtain in
large numbers needed for bioassays.
FISH MODELS IN TOXICOLOGIC PATHOLOGY:
HISTORICAL PERSPECTIVE
Perhaps because of their fecundity, small size, and economical maintenance and use, fish models are becoming well
established in many laboratories. In fact, the facility that does
not house at least 1 colony of zebrafish, medaka, or other fish
species is probably not at the forefront of biomedical research
(46). A number of recent reviews have pointed out the advantages of fish models for laboratory-based testing (1, 6, 21, 26,
38, 55). Small fish models have been shown to be sensitive to
a variety of known carcinogens and have a short time to tumorigenesis, yet have an exceedingly low spontaneous tumor
rate in potential target organs (16, 19).
Although it is widely recognized that aquatic ecosystems serve as the final sink for many chemicals, and that
water serves as the ultimate vehicle for exposure to many
toxic agents, relatively few methods exist to precisely and

Address correspondence to: J. M. Law, NCSU College of Veterinary
Medicine, Laboratory for Toxicologic and Molecular Pathology, 617 Hutton
Street, Raleigh, North Carolina 27606. Fax: (919) 515-4237; e-mail:
Mac Law@ncsu.edu

49
Downloaded from tpx.sagepub.com by guest on December 7, 2016

0192-6233/03$3.00+$0.00

50

LAW

practically assess health risks from exposure to pollutants
in the aquatic environment (55). In fact, it was historical
observations of tumors in wild fishes that prompted the
development of carcinogenicity testing utilizing small fish
species in the laboratory. Harshbarger and Clark (14) documented 41 geographic regions in North America in which
clusters, or epizootics, of cancer in wild fishes have occurred. The occurrence of neoplasms involving epithelial tissues such as the liver, pancreas, gastrointestinal tract, and
some epidermal neoplasms appear strongly correlated with
environmental contamination, that is, exposure to chemical carcinogens. Recent reviews provide more on these epizootics (5, 8, 9, 14, 15, 34). However, several reports of
tumors in wild fish have been pivotal and deserve special
mention.
English sole from contaminated areas of the Puget Sound,
Washington, had high prevalences of liver lesions that ranged
from megalocytosis to neoplasms (37). Several detailed studies (eg, 28–32) established statistically significant associations between the presence of polycyclic aromatic hydrocarbons (PAH) in the sediments and the prevalence of
liver neoplasia. Malins et al (33) identified a novel DNA
adduct, 2,6-diamino-4-hydroxy-5-formamidopyrimidine, in
neoplastic livers of English sole from carcinogen-impacted
areas of the Puget Sound.
On the East Coast, epizootic hepatic neoplasia in winter
flounder from Boston Harbor, Massachusetts, has been reported (35, 36). As in the case of the Puget Sound sole but
not as firmly established, the hepatic lesions in the winter
flounder were highly correlated with anthropogenic chemical contamination.
Although many incidences of cancer epizootics have occurred in fresh water fishes (5), none have been as well studied as the epizootics in the marine species the English sole
and winter flounder. Epizootics of neoplasia in fish populations of brown bullhead catfish (Ictalurus nebulosus) and
Atlantic tomcod (Microgadus tomcod) also should be noted.
Sediments rich in PAH have generally been considered the
principal causes of skin and liver neoplasia in brown bullheads in the contaminated Black River (Ohio), a tributary
of Lake Erie (2–4). In laboratory tests, medaka exposed to
extracts and fractions of PAH-contaminated sediments from
tributaries of the Great Lakes, including the Black River, developed liver neoplasia (11). Similarly, epizootics of hepatic
neoplasia have been reported from Atlantic tomcod from the
Hudson River (7, 41). Those liver neoplasms have been associated with elevated tissue levels of polychlorinated biphenyls
(PCBs) (22).
White suckers from industrially polluted areas of Lake
Ontario exhibited increased prevalences of hepatic and skin
neoplasia (17, 42). As in other epizootics, the neoplasms have
been associated with PAH contamination. Stalker et al (43)
showed that the progression of hepatocellular and bile duct
neoplasms in the white sucker is accompanied by a loss of immunoreactive glutathione S-transferases that usually catalyze
a major detoxification pathway.
Only a small number of reported cancer epizootics have
dealt with small fish species. Vogelbein and colleagues reported high prevalences of liver neoplasms in mummichog
(Fundulus heteroclitus) from a creosote-contaminated site
in the Elizabeth River, Virginia (53). Exocrine pancreatic

TOXICOLOGIC PATHOLOGY

neoplasms also apparently induced by contaminant exposure
were later reported in these fish (12, 52).
THE NEED FOR MECHANISM-BASED RESEARCH
Studies involving fish in toxicology currently use these
models either as surrogates for human health problems or
as indicators of environmental health. Human health and environmental health are, of course, inexorably linked, so the
2 concepts should not be separated. Instead, it is critical that
we provide as much mechanistic information as possible in
order to further validate these alternative test methods and
take every advantage of their utility for many types of studies.
Mechanistic information garnered across phyletic levels
may be more accurately applied to help substantiate findings
from field work. Additionally, it may unlock untold mysteries
of the basic mechanisms of cellular pathology and neoplasia
(recall the wealth of information garnered on apoptosis from
the lowly nematode, C. elegans) (13).
The basic metabolic machinery in small fish species, with
regard to Phase I and Phase II metabolism, is similar to that in
mammals. The Phase I metabolizing enzyme system, the cytochromes P450 (CYPs), have been perhaps the best characterized in aquatic species (44, 45). At this time, it appears that
in fish only members of the CYP 1A subfamily are induced
by environmental toxicants, and thus would have a major impact on the activation or detoxification of carcinogens (54).
Similar to mammals, compounds such as polycyclic aromatic
hydrocarbons, polyhalogenated biphenyls (PCBs), and polychlorinated dioxins have been shown to induce CYP1A in
fish. One important difference to note between fish and mammals, however, is in the response to phenobarbital. Whereas
phenobarbital classically induces the mammalian CYP2B
subfamily, fish CYP2B appears to be refractory to phenobarbital induction (23, 24). Recent studies indicate that phenobarbital can instead induce CYP1A in fish, perhaps via
enhancement of Ah receptor activation (10, 39). In a recent
review, Williams et al (54) point out that xenoestrogens, an
important class of aquatic pollutants, may alter the response
to carcinogens in fish through modulation of CYPs.
While these relationships are best characterized in the
rainbow trout model, comparatively little information is available for small fish models. CYP1A was found to be deficient in preneoplastic and neoplastic lesions in mummichog (Fundulus heteroclitus), an estuarine small fish model
that shows great promise for environmental toxicology research (51). This same group recently demonstrated tissuespecific expression of CYP1A in mummichog exposed to
benzo[a]pyrene in both aqueous and dietary exposures, and
developed a grading system for CYP1A staining intensity
(50). Studying the metabolism of trichloroethylene, a common groundwater contaminant, Lipscomb et al (27) found
that CYP1A was readily detectable in medaka liver by immunohistochemistry, whereas CYP2E1 was present at very
low levels.
Other enzyme systems have shown somewhat more variable results in fish studies. Immunostaining for gammaglutamyl transpeptidase (GGT), an important enzyme marker
in rodents, detected foci of cellular alteration in medaka
exposed to DEN (18). However, GGT staining showed conflicting results in rainbow trout studies (6). GlutathioneS-transferase, an important Phase II biotransformation

Downloaded from tpx.sagepub.com by guest on December 7, 2016

Vol. 31(Suppl.), 2003

FISH MODELS: INTRODUCTION

enzyme, has also shown variable results in preneoplastic and
neoplastic lesions in rainbow trout (6).
These reports support the utility of tissue-specific induction patterns for biotransformation enzymes in fish carcinogenesis research. However, it is also clear that there is a need
for more information on these enzyme systems, particularly
in small fish models. Future bioassays using these models
should include a battery of immunohistochemical stains, such
as those used by Van Veld et al (1997) or Lipscomb (1998).
Immunostains could provide valuable mechanistic information on carcinogen metabolism and help delineate enzyme
altered foci that might otherwise be missed using routine
staining methods. Furthermore, basic research is needed on
induction of enzymes by various classes of carcinogens in
small fish models such as the CYPs, GST, DNA repair enzymes, and the caspases involved in apoptosis. This information will be forthcoming as long as adequate funding is
maintained and balanced between basic, applied, and clinical
cancer research.
A virtual explosion of information is occurring in the area
of molecular biology with small fish models, much of which
is beyond the scope of this paper. See recent reviews for
further discussion of these developments (47, 49). These
molecular findings can only serve to bolster the validation
of these models. Recent discoveries have supported the use
of small fish models for human health risks from the environment. For example, the ras oncogene of fish has a high
homology to human K-ras; in goldfish, this homology is approximately 96% (49). Point mutations in Ki-ras occurred in
a high proportion of rainbow trout liver tumors induced by
aflatoxin B1, dimethylbenzanthracene (DMBA), methyl-N =
-nitro-N-nitrosoguanidine (MNNG), and DEN. The p53 tumor suppressor gene is another important example of molecular markers. The p53 gene has recently been cloned and
sequenced in medaka, but specific mutations have yet to be
demonstrated (25). However, we have seen overexpression
of the p53 protein product in liver neoplasms in several fish
bioassays (unpublished work). Stabilization of a nonfunctional form of p53 protein is thought to account for the increased expression of p53 in tumors, suggesting a failure of
this important gatekeeper of the cell cycle.
SESSION OVERVIEW
In this session, we attempt to provide some perspective
for the pathologist that is faced with planning or performing
experiments or testing protocols using fish models, or with
reading or interpreting fish studies. First, Jeff Wolf of Experimental Pathology Laboratories, Herndon, VA, discusses
how to approach fish studies from the contract laboratory
standpoint, including special sectioning considerations, quality control, and Good Laboratory Practices (GLP) considerations. Although universal GLP standards have yet to be
officially established between laboratories that conduct fish
studies, recent contract work conducted “in the spirit of GLP,”
including quality assurance and peer review, has gained rapid
acceptance and helped in moving toward interlaboratory
standardization.
Next, David Williams of Oregon State University,
Corvallis, Oregon, discusses specifics on the use of the rainbow trout model. Williams reviews some of the fascinating
work with rainbow trout carcinogenicity testing done pre-

51

dominantly at Oregon State University, including testing of
dietary carcinogens and those with anticarcinogenic properties. Historically, this fish model provided some critical
breakthroughs in the links between dietary aflatoxin exposure and liver cancer. Jan Spitzbergen, also of Oregon State
in Corvallis, next provides an overview of the multitude of
scientific accomplishments afforded by the zebrafish model,
particularly in the molecular mechanisms of development.
Spitzbergen also reviews the current state of toxicity and carcinogenicity testing with this model, and rightly articulates
the need for training of veterinary pathologists that can help
make sense of the explosion of transgenic animal models, including transgenic fish models. Last, William Hawkins, Institute of Marine Sciences, Ocean Springs, Mississippi, reviews
some recent studies using the Japanese medaka. The medaka
has emerged as the preeminent model for large scale carcinogenicity testing, and is helping to define the dose-response
curve for genotoxic carcinogens at relatively low and perhaps
more environmentally realistic exposure concentrations.
REFERENCES
1. Bailey GS, Williams DE, Hendricks JD (1996). Fish models for environmental carcinogenesis: The rainbow trout. Environ Health Perspec 104:
5–21.
2. Baumann PC (1989). PAH, metabolites, and neoplasia in feral fish populations. In: Metabolism of Polycyclic Aromatic Hydrocarbons in the Aquatic
Environment, Varanasi U (ed). CRC Press, Boca Raton, Florida, pp. 269–
290.
3. Baumann PC, Harshbarger JC, Hartman KJ (1990). Relationship between
liver tumors and age in brown bullhead populations from two Lake Erie
tributaries. Sci Total Environ 94: 71–87.
4. Baumann PC, Smith WD, Parland WK (1987). Tumor frequencies and contaminant concentrations in brown bullheads from an industrialized river and
a recreational lake. Trans Amer Fish Soc 116: 79–86.
5. Black JJ, Baumann PC (1991). Carcinogens and cancers in freshwater fishes.
Environ Health Perspec 90: 27–33.
6. Bunton TE (1996). Experimental chemical carcinogenesis in fish. Toxicol
Pathol 24: 603–618.
7. Cormier SM, Racine RN, Smith CE, Dey WP, Peck TH (1989). Hepatocellular carcinoma and fatty infiltration in the Atlantic tomcod, Microgadus
tomcod (Walbaum). J Fish Dis 12: 105–116.
8. Couch JA, Harshbarger JC (1985). Effects of carcinogenic agents on aquatic
animals: An environmental and experimental overview. Environ Carcinogen
Revs 3: 63–105.
9. Dawe CJ, Harshbarger JC, Kondo S (eds) (1981). Phyletic Approaches to
Cancer. Japan Scientific Societies Press, Tokyo.
10. Elskus AA, Stegeman JJ (1989). Further consideration of phenobarbital
effects on cytochrome P-450 activity in killifish, Fundulus heteroclitus.
Comp Biochem Physiol 92C: 223–230.
11. Fabacher DL, Besser JM, Schmitt CJ, Harshbarger JC, Peterman PH, Lebo
JA (1991). Contaminated sediments from tributaries of the Great Lakes:
Chemical characterization and carcinogenic effects in medaka (Oryzias
latipes). Arch Environ Contam Toxicol 20: 17–34.
12. Fournie JW, Vogelbein WK (1994). Exocrine pancreatic neoplasms in the
mummichog (Fundulus heteroclitus) from a creosote-contaminated site.
Toxicol Pathol 22: 237–247.
13. Fraser AG (1999). Programmed cell death in C. elegans. Cancer Metastasis
Rev 18: 285–294.
14. Harshbarger JC, Clark JB (1990). Epizootiology of neoplasms in bony fish
of North America. Sci Total Environ 94: 1–32.
15. Harshbarger JC, Spero PM, Wolcott NM (1993). Neoplasms in wild fish
from the marine ecosystem emphasizing environmental interactions. In:
Pathobiology of Marine and Estuarine Organisms, Couch JA, Fournie JW
(eds). CRC Press, Boca Raton, Florida, pp 157–176.

Downloaded from tpx.sagepub.com by guest on December 7, 2016

52

LAW

16. Hawkins WE, Overstreet RM, Walker WW (1988). Carcinogenicity tests
with small fish species. Aquatic Toxicol 11: 113–128.
17. Hayes MA, Smith IR, Rushmore TH, Crane TL, Thorm C, Kocal TE,
Ferguson HW (1990). Pathogenesis of skin and liver neoplasms from industrially polluted areas in Lake Ontario. Sci Total Environ 94: 105–123.
18. Hinton DE, Couch JA, The SJ, Courtney LA (1988). Cytological changes
during progression of neoplasia in selected fish species. Aquatic Toxicol 11:
77–112.
19. Hoover KL (ed) (1984). Use of Small Fish Species in Carcinogenicity Testing, Vol. 65, National Cancer Institute, Bethesda, Maryland.
20. Ito N, Tatematsu M, Hasegawa R, Tsuda H (1989). Medium-term bioassay
system for detection of carcinogens and modifiers of hepatocarcinogenesis
utlizing the GST-P positive liver cell focus as an endpoint marker. Toxicol
Pathol 17: 630–641.
21. Kazianis S, Walter RB (2002). Use of platyfishes and swordtails in biological
research. Lab Animal 31: 46–52.
22. Klauda RV, Peck TH, Rice GK (1981). Accumulation of polychlorinated
biphenyls in Atlantic tomcod (Microgadus tomcod) collected from the
Hudson River estuary, New York. Bull Environ Contam Toxicol 27: 829–
835.
23. Kleinow KM, Haasch ML, Williams DE, Lech JJ (1990). A comparison
of hepatic P450 induction in rat and trout (Onchorhynchus mykiss): Delineation of the site of resistance of fish to phenobarbital-type inducers. Comp
Biochem Physiol 96C: 259–270.
24. Kleinow KM, Melancon MJ, Lech JJ (1987). Biotransformation and induction: Implications for toxicity, bioaccumulation, and monitoring of environmental xenobiotics in fish. Environ Health Perspec 71: 105–119.
25. Krause M, Rhodes L, Van Beneden R (1997). Cloning of the p53 tumor
suppressor gene from the Japanese medaka (Oryzias latipes) and evaluation
of mutational hotspots in MNNG-exposed fish. Gene 189: 101–106.
26. Law JM (2001). Mechanistic considerations in small fish carcinogenicity
testing. ILAR J 42: 274–284.
27. Lipscomb J, Confer P, Miller M, Stamm S, Snawder J, Bandiera S (1998).
Metabolism of trichloroethylene and chloral hydrate by the Japanese
medaka (Oryzias latipes) in vitro. Environ Toxicol Chem 17: 325–332.
28. Malins DC, Krahn MM, Brown DW, Rhodes LD, Myers MS, McCain BB,
Chan S-L (1985). Toxic chemicals in marine sediment and biota from
Mukilteo, Washington: Relationships with hepatic neoplasms and other
hepatic lesions in English sole (Parophrys vetulus). J Natl Cancer Inst 74:
487–494.
29. Malins DC, Krahn MM, Myers MS (1985). Toxic chemicals in sediments
and biota from a creosote-polluted harbor: Relationships with hepatic
neoplasms and other hepatic lesions in English sole (Parophrys vetulus).
Carcinogenesis 6: 1463–1469.
30. Malins DC, McCain BB, Brown DW, Chan SL, Myers MS, Landahl JT,
Prohaska PG, Frideman AJ, Rhodes LD, Burrows DG, Gronlund WG,
Hodgins HO (1984). Chemical pollutants in sediments and diseases of
bottom-dwelling fish in Puget Sound, Washington. Environ Sci Technol 18:
705–713.
31. Malins DC, McCain BB, Landahl JT (1988). Neoplastic and other diseases
in fish in relation to toxic chemicals: An overview. Aquatic Toxicol 11:
43–67.
32. Malins DC, McCain BB, Myers MS (1987). Field and laboratory studies of
the etiology of liver neoplasms in marine fish from Puget Sound. Environ
Health Perspec 71: 5–16.
33. Malins DC, Ostrander GK, Haimanot R, Williams P (1990). A novel
DNA lesion in neoplastic livers of feral fish: 2,6-diamino-4-hydroxy-5formamidopyrimidine. Carcinogenesis 11: 1045–1047.
34. Mix MC (1986). Cancerous diseases in aquatic animals and their association with environmental pollutants: A critical review of the literature. Mar
Environ Res 20: 1–141.

TOXICOLOGIC PATHOLOGY

35. Murchelano RA, Wolke RE (1985). Epizootic carcinoma in winter flounder,
Pseudopleuronectes americanus. Science 228: 587–589.
36. Murchelano RA, Wolke RE (1991). Neoplasms and nonneoplastic lesions
in winter flounder, Pseudopleuronectes americanus, from Boston Harbor,
Massachusetts. Environ Health Perspec 90: 17–26.
37. Myers MS, Landahl JT, Krahn MM, McCain BB (1991). Relationships
between hepatic neoplasms and related lesions and exposure to toxic chemicals in marine fish from the U.S. West Coast. Environ Health Perspec 90:
7–15.
38. Powers DA (1989). Fish as model systems. Science 246: 352–358.
39. Sadar MD, Ash R, Sundquvist J, Olsson PE, Andersson TB (1996). Phenobarbital induction of CYP1A1 gene expression in a primary culture of
rainbow trout hepatocytes. J Biol Chem 271: 17635–17643.
40. Salem H, Katz S (ed) (1998). Advances in Animal Alternatives for Safety
and Efficacy Testing, Taylor & Francis, Philadelphia, PA.
41. Smith CE, Peck TH, Klauda RJ, McLaren JB (1979). Hepatomas in Atlantic
tomcod Microgadus tomcod (Walbaum) collected in the Hudson River
estuary in New York. J Fish Dis 2: 313–319.
42. Sonstegard RA (1977). Environmental carcinogenesis studies in fishes of
the Great Lakes of North America. Ann NY Acad Sci 298: 261–269.
43. Stalker MJ, Kirby GM, Kocal TE, Smith IR, Hayes MA (1991). Loss of
glutathione S-transferases in pollution-associated liver neoplasms in white
suckers (Catastomus commersoni) from Lake Ontario. Carcinogenesis 12:
2221–2226.
44. Stegeman JJ, Hahn ME (1994). Biochemistry and molecular biology of
monooxygenases: Current perspectives on forms, functions, and regulation
of cytochrome P450 in aquatic species. In: Aquatic Toxicology: Molecular,
Biochemical, and Cellular Perspectives, Malins DC, Ostrander GK (eds).
Lewis Publishers, Boca Raton, Florida, pp 87–206.
45. Stegeman JJ, Lech JJ (1991). Cytochrome P-450 monooxygenase systems
in aquatic species: Carcinogen metabolism and biomarkers for carcinogen
and pollutant exposure. Environ Health Perspec 90: 101–109.
46. Stoskopf MK (2002). Fish are here to stay. Lab Animal 31: 9.
47. Talbot W, Hopkins N (2000). Zebrafish mutations and functional analysis
of the vertebrate genome. Genes Devel 14: 755–762.
48. Tennant R, Tice R, Spalding J (1998). The transgenic Tg.AC mouse model
for identification of chemical carcinogens. Toxicol Lett 102–103: 465–
471.
49. Van Beneden RJ, Ostrander GK (1994). Expression of oncogenes and tumor suppressor genes in teleost fishes. In: Aquatic Toxicology: Molecular,
Biochemical, and Cellular Perspectives, Malins DC, Ostrander GK (eds).
Lewis Publishers, Boca Raton, Florida, pp 295–325.
50. Van Veld P, Vogelbein W, Cochran M, Goksoyr A, Stegeman J (1997).
Route-specific cellular expression of cytochrome P4501A (CYP1A) in
fish (Fundulus heteroclitus) following exposure to aqueous and dietary
benzo[a]pyrene. Toxicol Appl Pharmacol 142: 348–359.
51. Van Veld P, Vogelbein W, Smolowitz R, Woodin B, Stegeman J (1992).
Cytochrome P450IA1 in hepatic lesions of a teleost fish (Fundulus heteroclitus) collected from a polycyclic aromatic hydrocarbon-contaminated
site. Carcinogenesis 13: 505–507.
52. Vogelbein WK, Fournie JW (1994). Ultrastructure of normal and neoplastic
exocrine pancreas in the mummichog, Fundulus heteroclitus. Toxicol Pathol
22: 248–260.
53. Vogelbein WK, Fournie JW, Van Veld PA, Huggett RJ (1990). Hepatic
neoplasms in the mummichog Fundulus heteroclitus from a creosotecontaminated site. Cancer Res 50: 5978–5986.
54. Williams DE, Lech JJ, Buhler DR (1998). Xenobiotics and xenoestrogens
in fish: Modulation of cytochrome P450 and carcinogenesis. Mutation Res
399: 179–192.
55. Winn RN (2001). Transgenic fish as models in environmental toxicology.
ILAR J 42: 322–329.

Downloaded from tpx.sagepub.com by guest on December 7, 2016

arXiv:1611.06631v1 [cs.LO] 21 Nov 2016

On Selecting a Conjunction Operation in Probabilistic Soft Logic
Vladik Kreinovich

Chitta Baral

Department of Computer Science
University of Texas at El Paso
vladik@cs.utep.edu

School of Computing, Informatics and DSE
Arizona State University
chitta@asu.edu

Abstract
Probabilistic Soft Logic has been proposed and used in several applications as an efficient way to deal with inconsistency, uncertainty and relational representation. In several applications, this approach has led to an adequate description of
the corresponding human reasoning. In this paper, we provide
a theoretical explanation for one of the semi-heuristic choices
made in this approach: namely, we explain the choice of the
corresponding conjunction operations. Our explanation leads
to a more general family of operations which may be used in
future applications of probabilistic soft logic.

Introduction and Motivation
With the maturing of various sub-fields of AI we are
now ready to combine approaches and techniques from
multiple AI sub-fields to address broader issues. For example, we now have large bodies of knowledge that
are available. An example such a knowledge base is
ConceptNet (Liu and Singh 2004). Even in the same collection some of the knowledge may be manually curated, while parts may be automatically extracted. Sometimes all of the knowledge may be automatically obtained, such as the similarity knowledge based used in
(Beltagy, Erk, and Mooney 2010) that is obtained using distributional semantics (Bruni, Tran, and Baroni 2014) of natural language. There may be inconsistencies lingering inside
the knowledge base. For some of the knowledge, we may be
able to assign weights. Learning part of this knowledge and
reasoning with such knowledge requires approaches that can
handle inconsistencies, uncertainty, structured information,
and most importantly the approaches need to scale. Among
the various approaches that have been proposed Probabilistic Soft Logic (PSL) (Bach et al. 2010; Bach et al. 2013;
Kimmig et al. 2012) stands out as it can not only handle relational structure, inconsistencies and uncertainty, thus allowing one to express rich probabilistic graphical models (such
as Hinge-loss Markov random fields), but it also seems to
scale up better than its alternatives such as Markov Logic
Networks (Richardson and Domingos 2006).
Probabilistic soft logic (PSL) differs from most other
probabilistic formalisms in that its ground atoms, instead
c 2017, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

of having binary truth values, have continuous truth values
in the interval [0,1]. In the original PSL (Bach et al. 2010;
Bach et al. 2013; Kimmig et al. 2012) the syntactic structure of rules and the characterization of the logical
operations have been chosen judiciously so that the space
of interpretations with nonzero density forms a convex
polytope. This makes inference in PSL a convex optimization problem in continuous space, which in turn allows
efficient inference. The particular conjunction operation
used in the above mentioned PSL is the Lukasiewicz t-norm
(Klir and Yuan 1995). A different conjunction operation
is used in (Beltagy, Erk, and Mooney 2010), where the
resulting PSL is used for semantic textual similarity.
PSL has been used in many different applications such
as ones listed in(Bach et al. 2010; Bach et al. 2013;
Beltagy, Erk, and Mooney 2010;
Huang et al. 2012;
Kimmig et al. 2012;
Memory et al. 2012).
However,
none of these works precisely justify the particular selection
of conjunction operation they use, beyond listing a few
implications of using those operations.
What we plan to do. In this paper, we provide a
theoretical explanation of the conjunction operations
that are used in (Bach et al. 2010; Bach et al. 2013;
Beltagy, Erk, and Mooney 2010;
Huang et al. 2012;
Kimmig et al. 2012; Memory et al. 2012) and present a
more general family of operations which may be used in
future applications of probabilistic soft logic.
Plan of the paper. In this section, we recalled and contextualized Probabilistic Soft Logic and which conjunction operations are usually selected in this logic. In the next sections,
we provide our theoretical explanation for this selection.

Rules, implications and the conjunction
operation in Probabilistic Soft Logic
The corresponding real-life problem. In many practical
situations, we have rules r of the type a1 , . . . , an → b that
connect facts ai and b. For each such rule r, we know its “degree of importance” λr : the larger λr , the larger our degree
of confidence in this rule.
If we simply combine all these rules (and ignore their degrees of importance), then usually, the resulting set of rules
becomes inconsistent.

For example, sociologists known that in elections, a person tends to vote the same way as his friends. So, if a person
B has a friend A1 who voted for an incumbent and a friend
A2 who voted for a challenger, then:
• for the first friend, the above sociological observation implies that B voted for the incumbent, while
• for the second friend, the same sociological observation
implies that B voted against the incumbent.
In such situations, we cannot satisfy all the rules. So, it is
reasonable to look for solutions in which an (appropriately
defined) deviation from the ideal situation – when all the
rules are satisfied – is the smallest possible.
Need for probabilistic answers. If we have two conflicting
rules, then we cannot be 100% sure which of them is not
applicable in the current situation. If one of the rules is more
important than the other one, i.e., if its degrees of importance
λr is higher (λr > λr′ ), then most probably the first rule
is applicable – but it is also possible that in this particular
situation, the second rule is applicable as well.
Thus, from the inconsistent knowledge base, we cannot
extract the exact conclusion about the corresponding facts.
At best, we can estimate the probabilities that different facts
are true.
How to deal with implication. If a implies b, this means
that b holds in all situations in which a holds – and, maybe,
in other situations as well. Thus, the probability p(b) that b
is true is larger than or equal to the probability p(a) that a is
true: p(b) ≥ p(a).
From this viewpoint, if we know the probabilities p(a)
and p(b) of two statements a and b, and p(a) ≤ p(b) (i.e., the
difference p(a)−p(b) is non-positive), then this inequality is
consistent with the rule a → b. On the other hand, if p(a) >
p(b) (i.e., if the difference p(a)−p(b) is positive), this clearly
is inconsistent with the rule a → b. Intuitively, the larger the
positive difference p(a) − p(b), the larger the violation. It
is therefore reasonable to take max(p(a) − p(b), 0) as the
measure of severity of the rule’s violation.
We want to minimize the overall loss of adequacy. Depending on the rule’s degree of importance λr , the same degree of rule’s violation may lead to different severity. In general, this severity is a function of the rule’s degree of importance λr and of its degree of violation max(p(a) − p(b), 0):
d = s(λr , max(p(a) − p(b), 0)).
We want to find the probabilities for which the overall loss
of adequacy is the smallest possible. For rules r of the type
ar → br , this means minimizing the sum
X
s(λr , max(p(ar ) − p(br ), 0)).
(1)
r

In Probabilistic Soft Logic, the corresponding function
s(λ, d) usually has the form s(λ, d) = λ · dp for some real
number p. Most often, the value p = 1 is chosen. Sometimes
(but less frequently), the value p = 2 is chosen.
Need to deal with conjunction. In many rules, the condition ar is not a fact, but a conjunction of several facts

ar,1 & . . . & ar,nr . In other words, such rules have the form
(ar,1 & . . . & ar,nr ) → br .
To find the degree qr to which the rule’s condition ar is
satisfied, it is not sufficient to know the probabilities of all
the facts ar,i , we also need to know the dependence between
these random events. For example, if p(ar,1 ) = p(ar2 ) =
0.5, then we can have three different situations:
• if ar,1 and ar2 are independent, then p(ar,1 & ar2 ) =
p(ar,1 ) · p(ar2 ) = 0.25;
• if ar,2 is equivalent to ar,1 , then p(ar,1 & ar2 )
p(ar,1 ) = 0.5; and

=

• if ar,2 is equivalent to ¬ar,1 , then p(ar,1 & ar2 ) =
p(ar,1 & ¬ar,1 ) = 0.
In practice, we usually do not know the relation between the corresponding random events. In this case, we
need to come up with some estimate of the probability p(ar,1 & . . . & ar,nr ) based only on the known values
p(ar,i ). Let us denote the function corresponding to this estimating algorithm by ∧(p1 , . . . , pn ). In terms of this funcdef

tion, once we know the probabilities pi = p(ar,i ), we estimate the probability of the conjunction ar,1 & . . . & ar,nr
as ∧(p1 , . . . , pnr ).
Once we know the probabilities p(ar,i ) of individual
events ar,i , the set of possible values of the probability
p(ar,1 & . . . & ar,nr ) is determined by the Fréchet inequalities (see, e.g., (Nelsen 1999))
max(p(ar,1 ) + . . . + p(ar,nr ) − (nr − 1), 0) ≤
p(ar,1 & . . . & ar,nr ) ≤
min(p(ar,1 ), . . . , p(ar,nr )).

(2)

Fréchet inequalities explained. To present a better understanding, we now describe where the inequalities (2) come
from.
On the one hand, in every situation in which the conjunction ar,1 & . . . & ar,nr holds, each event ar,i also holds.
Thus, for every i, the class of all situations in which the conjunction holds is a subclass of the class of all the situations
in which the i-th event ar,i holds. Therefore, the probability
of the conjunction cannot exceed the probability of the i-th
event:
p(ar,1 & . . . & ar,nr ) ≤ p(ar,i ).
The probability of the conjunction is hence smaller than
or equal to n probabilities p(ar,1 ), . . . , p(ar,nr ). Thus, the
probability of the conjunction cannot exceed the smallest of
these n probabilities:
p(ar,1 & . . . & ar,nr ) ≤ min(p(ar,1 ), . . . , p(ar,nr )).
This explains the right-hand side of the inequality (2).
On the other hand, for every two events A and B, we have
p(A ∨ B) = p(A) + p(B) − p(A & B) and thus, p(A ∨ B) ≤
p(A) + p(B). By induction, we can conclude that
p(A1 ∨ . . . ∨ Anr ) ≤ p(A1 ) + . . . + p(Anr )

for every natural number nr . In particular, for the events
def
Ai = ¬ar,i for which p(Ai ) = 1 − p(ar,i ), we get

∧(p1 , . . . , pn ) = max(p1 + . . . + pn − (n − 1), 0). (4)

p((¬ar,1 ) ∨ . . . ∨ (¬ar,nr )) ≤

This operation clearly satisfies the inequality (3).

(1 − p(ar,1 )) + . . . + (1 − p(ar,nr )) =

Resulting formulation of the knowledge processing problems. Suppose that our knowledge base consists of R rules
of the type (ar,1 & . . . & ar,nr ) → br , r = 1, . . . , R, with
weights λr .
Then, once we have selected the function s(λ, d) and the
conjunction operation ∧(p1 , . . . , pn ), we can now find the
probabilities p(a) of different facts a by minimizing the sum

nr − (p(aa,1 + . . . + p(ar,nr ).
Due to de Morgan laws, the disjunction
(¬ar,1 ) ∨ . . . ∨ (¬ar,nr )
is equivalent to ¬(ar,1 & . . . & ar,nr ). Thus,
p((¬ar,1 ) ∨ . . . ∨ (¬ar,nr )) = 1 − p(ar,1 & . . . & ar,nr ),
and the above inequality takes the form

R
X

s(λr , max(∧(p(ar,1 , . . . , p(ar,nr ) − p(br ), 0)).

(5)

r=1

1 − p(ar,1 & . . . & ar,nr ) ≤
nr − (p(aa,1 ) + . . . + p(ar,nr )).
Through simple algebraic manipulation we conclude that
p(ar,1 & . . . & ar,nr ) ≥
p(aa,1 ) + . . . + p(ar,nr ) − (nr − 1).
Since the probability is always non-negative, we have
p(ar,1 & . . . & ar,nr ) ≥ 0. Since the probability is larger
than or equal to the two numbers, it is therefore larger than
the largest of these two numbers:
p(ar,1 & . . . & ar,nr ) ≥
max(p(aa,1 ) + . . . + p(ar,nr ) − (nr − 1), 0).
This explains the first of the inequalities (2).
Thus, Fréchet inequalities have been explained.
Comment: We have derived two inequalities (2). A natural
question is: Can other unrelated inequalities be similarly derived? It turns out that the two inequalities (2) are the only
limitation on the joint probability p(ar,1 & . . . & ar,nr ):
namely, for every tuple of values p1 , . . . , pnr , and for every
number p for which
max(p1 + . . . + pnr − (nr − 1), 0) ≤ p ≤
min(p1 , . . . , pnr ),
we can construct events aa,1 , . . . , ar,nr with probabilities
pi = ar,i for which the joint probability is equal exactly
to p: p(ar,1 & . . . & ar,nr ) = p.
Conclusion - the first desired property of the conjunction operation: Fréchet inequalities (2) implies that the desired conjunction operation ∧(p1 , . . . , pn ) should satisfy the
inequality
max(p1 + . . . + pn − (n − 1), 0) ≤ ∧(p1 , . . . , pn ) ≤
min(p1 , . . . , pn ).

Kimmig et al. 2012) , usually, the following conjunction operation is used:

(3)

Comment: In many of the Probabilistic Soft Logic
formulations
(Bach et al. 2010;
Bach et al. 2013;

Comments:
• The need to come up with some values for p(a & b)
(and p(a ∨ b)) when we only know the probabilities p(a) and p(b) is ubiquitous in practical applications. In particular, this need underlies the main
ideas behind fuzzy logic, where the corresponding
conjunction operation ∧(p1 , p2 ) is known as a tnorm; see, e.g., (Klir and Yuan 1995; Kreinovich 1992;
Nguyen and Kreinovich 1997; Nguyen and Walker 2006;
Zadeh 1965).
• So far, we only described the aspects of the Probabilistic Soft Logic that enable us to compute the
“most reasonable” (“most probable”) set of values
p(a). In principle, these techniques also enable us
to estimate the probability of other sets of values
p′ (a) 6= p(a) (Bach et al. 2010; Bach et al. 2013;
Beltagy, Erk, and Mooney 2010;
Huang et al. 2012;
Kimmig et al. 2012; Memory et al. 2012). However,
as mentioned in (Beltagy, Erk, and Mooney 2010), the
primary objective of the Probabilistic Soft Logic is to
provide the most reasonable probabilities. Because of
this, in the current paper, we will not describe how the
auxiliary (“second-order”) probabilities-of-probabilities
can be computed.
We need to make sure that the corresponding computations are efficient. Our goal is solve practical problems, and
in practice, the number of rules can be large. It is therefore
important to make sure that the corresponding optimization
problem can be solved by a feasible algorithm.
It is known that, in general, optimization problems are
NP-hard (in some formulations, even algorithmically undecidable), but they become feasible if we restrict ourselves to
convex objective functions; see, e.g., (Vavasis 1991). Moreover, in some reasonable sense, the class of convex objective
functions is the largest possible class for which optimization
is still feasible (Kearfott and Kreinovich 2005).
Thus, we need to make sure that the objective function (5)
is convex.
Conclusion - the second desired property of the conjunction operation: Since the function max(x, 0) is convex, and

the superposition of convex functions is always convex, it is
sufficient to make sure:
• that the function s(λ, d) is a convex function of d ≥ 0,
and
• that the conjunction operations ∧(p1 , . . . , pn ) are all convex.
Comment: The choices made in Probabilistic Soft Logic are
indeed convex:
• the function s(λ, d) = λ · dp is convex for all p ≥ 1, and
• the conjunction operation (4) is convex as well.
What we will do. We will show that not
only is the conjunction operation (4) convex,
it is the only possible logical convex conjunction operation.
Comment: The computational problem becomes even easier when the objective function is not only convex,
but also piece-wise linear, i.e., has the form f (x) =
max(ℓ1 (x), ℓ2 (x), . . .) for several linear functions ℓ1 (x),
ℓ2 (x), . . . In this case, minimizing the objective function is
equivalent to solving the corresponding linear programming
problem of minimizing t under linear constraints t ≥ ℓ1 (x),
t ≥ ℓ2 (x), . . . , and for linear programming problems, efficient algorithms are available.
The function max(a, 0) is clearly piece-wise linear. Since
superposition of linear functions is linear, superposition of
piece-wise linear functions is piece-wise linear, so to make
sure that the objective function is piece-wise linear, it is sufficient to make sure:
• that the function s(λ, d) is a piece-wise linear function of
d ≥ 0, and
• that the conjunction operations ∧(p1 , . . . , pn ) are all
piece-wise linear.
Comment: The choices usually made in Probabilistic Soft
Logic are indeed convex and piece-wise linear:
• the function s(λ, d) = λ·dp is piece-wise linear for p = 1,
and
• the conjunction operation (4) is piece-wise linear as well.

Discussion. This result explains why the probabilistic soft logic – that uses the operation (4)
– allows for efficient computation of inference,
learning,
etc.
(Bach et al. 2010;
Bach et al. 2013;
Beltagy, Erk, and Mooney 2010;
Huang et al. 2012;
Kimmig et al. 2012; Memory et al. 2012): indeed, the
fact that this operation is convex makes computations
efficient.
This results also shows that the operation (4) is the only
possible logical convex conjunction operation – which explains the use of this operation in Probabilistic Soft Logic.
Proof.
1◦ . One can easily see that (4) is a logical conjunction operation, and that it is convex – as maximum of two linear
(hence convex) functions p1 + . . . + pn − (n − 1) and 0.
2◦ . Let us now assume that a conjunction operation
∧(p1 , . . . , pn ) is convex. Due to inequalities (3), for binary
vectors t, in which each component is 0 or 1, we have
∧(1, . . . , 1) = 1 and ∧(t) = 0 for all t 6= (1, . . . , 1).
In particular, for every i from 1 to n, we have ∧(ti ) = 1
ti = (1, . . . , 1, 0, 1, . . . , 1), where 0 is in the i-th place.
3◦ . Let us show that every vector p = (p1 , . . . , pn ) with
n
n
P
P
ai · ti , with
pi = n − 1 can be represented as p =
Indeed, since pi ≤ 1, we have ai = 1 − pi ≥ 0 and the
n
n
P
P
ai = 1. For each
pi = n − 1 implies that
condition

min(p1 , . . . , pn ).

(3)

Proposition 1. The only convex logical conjunction operation is the function
∧(p1 , . . . , pn ) = max(p1 + . . . + pn − (n − 1), 0). (4)

n
P

(1 − pi ) · ti has

i=1

the form

sj = (1−p1 )+. . .+(1−pj−1 )+(1−pj+1 )+. . .+(1−pn )
=

n
X

(1 − pi ) − (1 − pj ).

i=1

Here,
n
X

Main Result

max(p1 + . . . + pn − (n − 1), 0) ≤ ∧(p1 , . . . , pn ) ≤

i=1

i=1

index j, the j-th component sj of the sum

i=1

We start with formally defining a conjunction operation.
Definition 1. A function ∧ : [0, 1]n → [0, 1] is called
a logical conjunction operation if for all possible inputs
p1 , . . . , pn , it satisfies the inequality

i=1

i=1

ai = 1 − p i .

(1 − pi ) = n −

n
X

pi = n − (n − 1) = 1,

(6)

i=1

hence sj = 1 − (1 − pj ) = pj . The statement is proven.
4◦ . Let us now prove that every vector p with

n
P

pi ≤ n − 1

i=1

can be represented as a convex combination of the binary
vectors t for which ∧(t) = 01 .
Indeed, let us start with such a vector p = (p1 , . . . , pn ).
n
n
P
P
Let us consider two cases: 1+ pi ≥ n−1 and 1+ pi <

n − 1.

i=2

i=2

1
As usual, by a convex combination of the vectors ta , . . . , tb ,
we mean a linear combination in which all the coefficients are nonnegative and add up to 1: t = αa · ta + . . . + αb · tb , with αa ≥ 0,
. . . , αb ≥ 0, and αa + . . . + αb = 1.

In the first case, for the vector (q1 , p2 , . . . , pn ), where
n
P
pi > p1 , we have q1 +p2 +. . .+pn = n−1
q1 = n−1−
i=2

and thus, due to Part 3 of this proof, this vector is a convex combination of the vectors t for which ∧(t) = 0. Here,
0 < p1 ≤ q1 thus:
• p1 is a convex combination of numbers 0 and q1 .

Indeed, since pi ≤ 1, we have ai = 1 − pi ≥ 0 and the
n
n
n
P
P
P
(1 − pi ) <
ai =
pi > n − 1 implies that
condition

i=1

• So, the vector (p1 , . . . , pn ) is a convex combination of the
vectors (0, p2 , . . . , pn ) and (q1 , p2 , . . . , pn ).

sj = a0 +a1 +. . .+aj−1 +aj+1 +. . .+an =

• Thus, the original vector p is also a convex combination
of such vectors.
In the second case, the original vector p is a convex combination of the vectors (0, p2 , . . . , pn ) and (1, p2 , . . . , pn ).
Thus, if we prove that the second vector (1, p2 , . . . , pn ) can
be represented as the desired convex combination, we will
thus prove that the original vector p can also be represented
in this way.
For this new vector p′ = (1, p2 , . . . , pn ), we similarly
check whether raising p2 to 1 will lead to the sum exceeding n − 1; if yes, we can similarly complete our proof.
If not, then we reduce the problem to a yet new vector
p′′ = (1, 1, p3 , . . . , pn ), etc. Eventually, this procedure will
stop, since when we already have (n − 2) 1s, then for the
corresponding vector (1, . . . , 1, pn−1 , pn ) replacement with
1 clearly leads to a vector (1, . . . , 1, 1, pn ) in which the sum
of components clearly exceeds n − 1. The statement is thus
proven.
5◦ . According to Part 4 of this proof, every vector p with
n
P
pi ≤ n − 1 can be represented as a convex combination
i=1
P
p =
aα · tα of vectors tα with ∧(tα ) = 0. Thus, by
α

convexity,

X

∧(p) = ∧

α

a α · tα

≤

X

aα · ∧(tα ) = 0.

(7)

1 − aj = 1 − (1 − pj ) = pj .

i=1

ai −aj =
(9)

The statement is proven.
n
P

7◦ . Due to convexity, when

pi > n − 1, Part 6 of the

i=1

proof implies that

∧(p) = ∧ a0 · t0 +

n
X

a i · ti

i=1

a0 · ∧(t0 ) +

n
X

!

ai · ∧(ti ).

≤

(10)

i=1

Here, ∧(t0 ) = 1 and ∧(ti ) = 0 for all i, thus,
∧(p) ≤ a0 =

n
X

pi − (n − 1).

(11)

i=1

On the other hand, due to the inequality (3), we have
!
n
X
pi − (n − 1), 0 .
(12)
∧(p) ≥ max
i=1

In this case, the difference

n
P

pi − (n − 1) is positive, so

i=1

α

Since the value of the conjunction operation is always nonnegative, this implies that ∧(p) = 0 for all such vectors p.
n
P
pi ≤ n − 1, the convex wedge operation
So, when

n
X
i=0

• The first vector is a convex combination of binary vectors
t = (0, . . .) for which ∧(t) = 0.

!

i=1

i=1

i=1

1, so a0 ≥ 0.
For each index j, the j-th component sj of the sum a0 +
n
P
ai · ti has the form

∧(p) ≥

n
X

pi − (n − 1).

(13)

i=1

From (11) and (13), we conclude that

indeed coincides with the desired formula (4).
6◦ . Let us now prove that every vector p with

n
P

∧(p) =

pi > n − 1

max

a0 = 1 −

i=1

n
n
X
X
pi − (n − 1). (8)
(1 − pi ) =
ai = 1 −
i=1

i=1

n
X
i=1

i=1

n
X

pi − (n − 1) =

i=1

i=1

can be represented as a convex combination of the vector
def
t0 = (1, . . . , 1) and vectors ti , namely, that p = a0 · t0 +
n
P
ai · ti , with ai = 1 − pi and

n
X

So, when

n
P

!

pi − (n − 1), 0 .

(14)

pi > n − 1, the convex wedge operation

i=1

also coincides with the desired formula (4). The proposition
is proven.

What If We Take into Account that Human
“And” Is Somewhat Different from the Formal
Conjunction
Human “and” is somewhat different from formal “and”.
In formal logic, if one of the conditions a1 , . . . , an is not
satisfied, then the whole conjunction a1 & . . . & an is false.
For example, when a1 and a2 are true, but a3 is false, we get
∧(1, 0, 1) = 0.
In human reasoning, this is not always so. For example,
when a department tries to hire a new faculty member, the
usual requirement is that this person should be a good researcher (a1 ), a very good teacher (a2 ), and a good colleague
(a3 . Ideally, this is who we want to hire: a person who satisfies the property a1 & a2 & a3 .
Let us now assume that one of the candidates is an excellent researcher who is going to be a very good colleague, but
whose teaching skills are not so good, i.e., for whom a1 = 1,
a2 = 0, and a3 = 0. Formally, in this case, one of the conditions is not satisfied, so the conjunction a1 & a2 & a3 is false.
However, in reality, this person has a reasonable chance of
being hired – meaning that, according to our human reasoning, the original “and”-rule is to some extent satisfied, i.e.,
∧(1, 0, 1) > 0.
How to formally describe this difference. To describe this difference, researchers have proposed several conjunction operations for which ∧(1, 0, 1) > 0;
see,
e.g.,
(Kreinovich 2004;
Trejo et al. 2002;
Zimmermann and Zysno 1980).
In the context of Probabilistic Soft Logic, the paper
(Beltagy, Erk, and Mooney 2010) uses an operation
n
1 X
∧(p1 , . . . , pn ) = ·
pi
n i=1

What we propose. We propose to use the following conjunction operation
∧(p1 , . . . , pn ) =
max c1 ·

n
X

pi

i=1

where c1 ∈



!

!

− (n · c1 − 1), 0 ,

(16)


1
,1 .
n

Properties of the proposed operation. The proposed operation is convex – and thus, similarly to the operation (4), it
leads to efficient optimization hence to efficient inference,
learning, etc.
However, this operation is no longer associative. For ex1
ample, for c1 = , the operation (16) turns into the arithn
metic average (15). For the arithmetic average,


0.5 + 1
= ∧(0, 0.75) =
∧(0, ∧(0.5, 1)) = ∧ 0,
2
0 + 0.75
= 0.375,
2
while

(15)

for which also ∧(1, 0, 1) = 2/3 > 0.
Which conjunction operation should we use: analysis of
the problem. As we have mentioned earlier, the easiestto-process convex objective functions are piece-wise linear
functions, i.e., maxima of several linear functions. The fewer
linear functions we have, the easier it is to process this problem. For the operation (4), we have two linear functions, so
let us use two linear functions here as well.
Also, the simpler one of these functions, the easier it is
to solve the corresponding linear programming problem. In
the formal-“and” case, one of this functions was 0, so let us
use 0 here as well. Thus, we are looking for a conjunction
operation of the type ∧(p) = max(ℓ(p), 0) for some linear
n
P
ci · p i .
function ℓ(p) = c0 +
i=1

and the second condition means that c0 + n · c1 = 1. So,
c0 = 1 − n · c1 , and the requirement that c0 ≤ 0 means that
1
c1 ≥ . Thus, we arrive at the following recommendation.
n

In general, “a1 and a2 ” means the same as “a2 and a1 ”.
Thus, the “truth value” ∧(p1 , . . . , pn ) should not change if
we simply permute the inputs. Hence, the corresponding linear function should be permutation-invariant, which implies
that c1 = c2 = . . . = cn .
We should have ∧(0, . . . , 0) = 0 and ∧(1, . . . , 1) = 1.
The fact that ∧(1, . . . , 1) > ∧(0, . . . , 0) implies that c1 > 0.
The first condition ∧(0, . . . , 0) = 0 then means that c0 ≤ 0,

∧(∧(0, 0.5), 1) = ∧



0 + 0.5
,1
2



= ∧(0.25, 1) =

0.25 + 1
= 0.625 6= 0.375.
2
The proposed family of operations contains both currently used operations of Probabilistic Soft Logic. The
1
parameter c1 can take all possible values from to 1.
n
• On one extreme, for c1 = 1, we get the usual formal“and”-based operation (4) used in (Bach et al. 2010;
Bach et al. 2013; Kimmig et al. 2012).
1
• At the other extreme, when c1 =
, we get
n
the arithmetic average operation (15) used in
(Beltagy, Erk, and Mooney 2010).
Potential advantage of the proposed family of operations.
In general, the family (16) provides an additional parameter
c1 that we can adjust to hopefully get an even better match
with human reasoning – while still retaining convexity and
thus, retaining computational efficiency.

Conclusion

References

Probabilistic Soft Logic (PSL) is a probabilistic formalism
that can be used in learning, representing and reasoning with
uncertain and possibly inconsistent (when weights are not
taken into account) knowledge, and it seems to scale up
better than its alternatives. A key aspect of PSL is its use
of continuous truth values. This necessitates alternatives to
boolean conjunction operations and at least two such operations have been mentioned in the literature.
In this paper we have presented formal justifications
of two different conjunction operations used in various PSL formulations and applications. We first showed
(in Proposition 1) that the conjunction operations used
in the original PSL (Bach et al. 2010; Bach et al. 2013;
Kimmig et al. 2012) is unique in that it is the only conjunction operation that satisfies a set of desired properties. Next we presented a family of conjunction operations (16) that satisfy a smaller set of desired properties. We show that this family of operations contain both
the operation used in the original PSL (Bach et al. 2010;
Bach et al. 2013; Kimmig et al. 2012) and the operation
used in (Beltagy, Erk, and Mooney 2010). Our presentation
of a family of operations gives us additional conjunction operations which may come in handy in other situations.

[Bach et al. 2010] Bach, S. H.; Broecheler, M.; Kok, S.; and
Getoor, L. 2010. Decision-driven models with probabilistic
soft logic. In Proceedings of the 2010 NIPS Workshop on
Predictive Models in Personalized Medicine.
[Bach et al. 2013] Bach, S. H.; Huang, B.; London, B.; and
Getoor, L. 2013. Hinge-loss Markov random fields: convex
inference for structured predictions. In Proceedings of the
Twenty-Ninth Conference on Uncertainty in Artificial Intelligence UAI’2013.
[Beltagy, Erk, and Mooney 2010] Beltagy, I.; Erk, K.; and
Mooney, R. 2010. Probabilistic soft logic for semantic textual similarity. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics ACL’2014.
[Bruni, Tran, and Baroni 2014] Bruni, E.; Tran, N.-K.; and
Baroni, M. 2014. Multimodal distributional semantics. J.
Artif. Intell. Res.(JAIR) 49(1-47).
[Huang et al. 2012] Huang, B.; Kimmig, A.; Getoor, L.; and
Golbeck, J. 2012. Probabilistic soft logic for trust analysis in social networks. In Proceedings of the 2012 International Workshop on Statistical Relational Artificial Intelligence StaRAI’2012.
[Kearfott and Kreinovich 2005] Kearfott, R. B., and
Kreinovich, V. 2005. Beyond convex? global optimization is feasible only for convex objective functions: a
theorem. Journal of Global Optimization 33(4):617–624.
[Kimmig et al. 2012] Kimmig, A.; Bah, S. H.; Broechelet,
M.; Huang, B.; and Getoor, L. 2012. A short introduction
to probabilistic soft logic. In Proceedings of the 2012 NIPS
Workshop on Probabilistic Programming: Foundations and
Applications.
[Klir and Yuan 1995] Klir, G., and Yuan, B. 1995. Fuzzy Sets
and Fuzzy Logic. Upper Saddle River, New Jersey: Prentice
Hall.
[Kreinovich 1992] Kreinovich, V. 1992. A review of “Uncertain Reasoning” by G. Shafer and J. Pearl (eds.). ACM
SIGART Bulletin 3(4):23–27.
[Kreinovich 2004] Kreinovich, V. 2004. Towards more realistic (e.g., non-associative) ‘and’- and ‘or’-operations in
fuzzy logic. Soft Computing 8(4):274–280.
[Liu and Singh 2004] Liu, H., and Singh, P. 2004. Conceptneta practical commonsense reasoning tool-kit. BT technology journal 22(4):211–226.
[Memory et al. 2012] Memory, A.; Kimmig, A.; Bach, S. H.;
Raschid, L.; and Getoor, L. 2012. Graph summarization in
annotated data using probabilistic soft logic. In Proceedings
of the 2012 International Workshop on Uncertainty Reasoning for the Semantic Web URSW’2012.
[Nelsen 1999] Nelsen, R. B. 1999. An Introduction to Copulas. Berlin, Heidelberg, New York: Springer Verlag.
[Nguyen and Kreinovich 1997] Nguyen, H.
T.,
and
Kreinovich, V.
1997.
Applications of Continuous
Mathematics to Computer Science. Dordrecht: Kluwer.
[Nguyen and Walker 2006] Nguyen, H. T., and Walker, E. A.
2006. A First Course in Fuzzy Logic. Boca Raton, Florida:
Chapman and Hall/CRC.

[Richardson and Domingos 2006] Richardson, M., and
Domingos, P. 2006. Markov logic networks. Machine
Learning 62(1-2):107136.
[Trejo et al. 2002] Trejo, R.; Kreinovich, V.; Goodman, I. R.;
Martinez, J.; and Gonzalez, R. 2002. A realistic (nonassociative) logic and a possible explanations of 7±2 law.
International Journal of Approximate Reasoning 29:235–
266.
[Vavasis 1991] Vavasis, S. A. 1991. Nonlinear Optimization:
Complexity Issues. New York: Oxford University Press.
[Zadeh 1965] Zadeh, L. A. 1965. Fuzzy sets. Information
and Control 8:338–353.
[Zimmermann and Zysno 1980] Zimmermann, H. H., and
Zysno, P. 1980. Latent connectives in human decision making. Fuzzy Sets and Systems 4:37–51.

Towards Addressing the Winograd Schema Challenge - Building and Using a
Semantic Parser and a Knowledge Hunting Module
Arpit Sharma, Nguyen H. Vo, Somak Aditya & Chitta Baral
School of Computing, Informatics & Decision Systems Engineering
Arizona State University
Tempe, AZ 85281, USA
{asharm73, nhvo1, saditya1, chitta}@asu.edu
Abstract
Concerned about the Turing test’s ability to correctly evaluate if a system exhibits human-like intelligence, the Winograd Schema Challenge (WSC)
has been proposed as an alternative. A Winograd
Schema consists of a sentence and a question. The
answers to the questions are intuitive for humans
but are designed to be difficult for machines, as
they require various forms of commonsense knowledge about the sentence. In this paper we demonstrate our progress towards addressing the WSC.
We present an approach that identifies the knowledge needed to answer a challenge question, hunts
down that knowledge from text repositories, and
then reasons with them to come up with the answer. In the process we develop a semantic parser
(www.kparser.org). We show that our approach
works well with respect to a subset of Winograd
schemas.

1

Introduction

With significant advances and success in many Artificial Intelligence subfields, and instances of gaming the Turing test,
there is now a need to more clearly define how to evaluate when a system is replicating humanoid intelligence. The
Winograd Schema Challenge (WSC) [Levesque et al., 2011]
is one such attempt. The WSC corpus consists of a set of
pairs of a sentence and a question such that the answer to the
question is about resolving a definite pronoun or possessive
adjective to one of its two probable co-referents in the sentence. The co-referents belong to same gender and they have
a number agreement between them (both are either singular
or plural). These make it harder to resolve the pronoun to
its antecedent. The sentence also contains a “special word”
which when replaced by another word (alternate word), the
answer to the question also changes. An example of such a
pair of Winograd Schema sentences is as follows.
Sentence: The man couldn’t lift his son because he was so
heavy. Question: Who was heavy ? Answer: son
Sentence: The man couldn’t lift his son because he was so
weak. Question: Who was weak ? Answer: man
The motivation behind this challenge is to evaluate humanlike reasoning in machines. For example, one of the many

ways in which humans answer the first sentence above is
by using the commonsense that “someone who could not be
lifted may be heavy”. Thus a human-like intelligent system that can answer such questions correctly needs to have
this knowledge and the ability to reason with that knowledge. Very recently, this aspect of human-like intelligence
has also been emphasized in several other (besides the Winograd challenge) research initiatives such as the call by Paul
G. Allen Family Foundation, and the Big Mechanism call
of DARPA. Since the existing knowledge repositories (such
as CYC, ConceptNet, and AURA) are not comprehensive
enough, and a comprehensive knowledge base may be too
unwieldy, we propose the following approach to address this
aspect of human-like intelligence: 1. determining what kind
of (commonsense) knowledge is needed; 2. finding a way to
acquire that knowledge [Knowledge Hunting]; and 3. reasoning with that knowledge [Reasoning].
In this paper we make a start with respect to WSC and illustrate our approach with respect to a subset of it. In particular, we identify two specific categories of WSC schemas and
focus on those. In the process we develop several needed
tools, such as a non domain-specific semantic parser (KParser, demo available at www.kparser.org), a Knowledge
Hunting module that is able to automatically extract knowledge needed for the two categories of WSC schemas that we
focus on, and a reasoning module that reasons with the KParser output. The semantic parser integrates various natural
language processing aspects, and incorporate ontologies and
knowledge. It is of independent interest, but here we only describe it to the extent used in this paper. We illustrate a simple graph based reasoning engine to answer the WSC schema
questions by using the semantic representation of both the input text and the automatically extracted knowledge.

2

The Corpus

The WSC corpus consists of 282 sentence and question pairs.
We evaluated our technique on a subset of the WSC corpus. The subset consists of a significant number of Winograd schemas that require two specific kind of commonsense
knowledge as mentioned below:
• Direct Causal Events - Event-event causality: In this category, the commonsense knowledge required has two mutually causal events (explained and convince in the example below) such that a pronoun participates in one of the event and

its candidate co-referent participates in another. For example,
for the text “Sid explained his theory to Mark but he could not
convince him .” and the question “Who could not convince ?”,
the expected answer is “Sid”. One way in which humans resolve he to Sid is by using the commonsense knowledge: IF
(X explained S to Y but Z could not convince) THEN (Z=X
i.e. agent of explained=agent of could not convince).
• Causal Attributive: In this category, the commonsense
knowledge required has an event and a participant entity in
that event has an associated attribute that is causally related
to the event. For example, for the text “Pete envies Martin
because he is very successful.” and the question “Who is
successful ?”, the expected answer is “Martin”. A kind of
commonsense knowledge required to get this answer is of the
form: X envies Y possibly because Y has trait successful.
Here Y is the participant entity and envies is the event in the
sentence.
A total of 711 WSC corpus sentences are categorized in the
above mentioned categories. The remaining 211 sentences2
do not fall into these categories because the kind of commonsense knowledge mentioned above is not enough to solve
them. For example, for the sentence “There is a pillar between me and the stage, and I can not see it” and the question
“What can not I see ?” the required commonsense knowledge includes: “If something big comes in between me and
the stage then my sight is blocked; pillar represents something big; and if my sight is blocked then I can not see.” As
we can see that this commonsense knowledge does not fit into
the above two categories.

3

Our Approach

Our approach is based on three main tasks, namely, semantic
parsing of text, automatic extraction of commonsense knowledge about the input text and using a graph based reasoning
on the semantic representations of both the input and the commonsense knowledge to get the answer to the WSC schema
questions. The sections below explain each of these tasks
along with the tools and techniques that we developed.

3.1

Semantic Parsing: K-Parser (kparser.org)

A semantic representation of text is considered good if it can
express the structure of the text, can distinguish between the
events and their environment in the text, uses a general set of
relations between the events and their participants, and is able
to represent the same events or entities in different perspectives. Keeping these features in mind, we have developed a
graph based semantic parser (Knowledge Parser or K-Parser)
that produces a semantic representation of the input text with
the following properties:
• Has an acyclic graphical representation for English text.
The representation is easy to read.
• Has a rich ontology (KM [2004]) to represent semantic
relations (Event-Event relations such as causes, caused by,
Event-Entity relations such as agent, and Entity-Entity relations such as related to).
1
including
#4,
#6,
#72
and
http://www.cs.nyu.edu/davise/papers/WS.html
2
including #34, #35, #41, #48, #50 and #43

#23

from

• Has special relations (instance of and prototype of ) to represent the existential and universal quantification of entities.
• Has two levels of conceptual class information for words.
• Accumulates semantic roles of entities based on PropBank
framesets [Palmer et al., 2005].
• Has tenses of the verbs in the input text.
• Has other features such as an optional Co-reference resolution, Word Sense Disambiguation and Named Entity Tagging.
Algorithm 1 demonstrates the steps used to create the semantic graph in K-Parser. The algorithm consists of five priAlgorithm 1 K-Parser Algorithm
1: procedure C REATE G RAPH(text)
2:
S ← E XTRACT S YN D EPS(text)
3:
G ← S EMANTIC M APPING(S, LKM )
4:
for all node v  G do
5:
G ← G + G ET C LASS(v, W Sv ) . W Sv =word
sense of v
6:
for all edge e  G do
7:
E DGE L ABEL C ORRECTION(G)
8:
G ← A DD F EATURE(G, SRLent ) . Semantic Roles
of Entities
9:
G ← A DD F EATURE(G, CRent )
. Co-reference
Resolution
10:
return G
. The Semantic Graph
mary modules. The first module ExtractSynDeps is used
to extract the syntactic dependency graph from the input text.
We used Stanford Dependency Parser [De Marneffe et al.,
2006] for this purpose.
The second module, SemanticM apping is used to map
the syntactic dependency relations to KM relations [2004]
and a few newly created relations. There are three methods
used for semantic mapping. First, we used mapping rules to
map syntactic dependencies into semantic relations. For example the nominal subject dependency is mapped to agent relation. Second, we developed a multi-class multilayer perceptron classifier for disambiguating different senses of prepositions and assign the semantic relations appropriately. The
training data for classification is taken from The Preposition
Project [Litkowski, 2013] and the sense ids for prepositions
are manually mapped to KM relations. Third, we used the
discourse connectives in the text to label the event-event relations. We labeled different connectives with different labels. For example, the coordinate connectives such as but,
and, comma (,) and stop(.) are labeled as next event. Other
connectives are also labeled based on their effect, such as because and so are labeled caused by and causes respectively.
The third module, GetClass accumulates two level of
classes for each node in the output of Semantic Mapping
function. Word Sense Disambiguation [Basile et al., 2007]
along with the lexical senses from WordNet [1995] are used
for this task. The fourth module, EdgeLabelCorrection corrects the mappings done by the mapping function by using
class information extracted by the third function. For example, if there is a relation is possessed by between two nodes
with their superclass as person, then the relation is corrected
to related to (because a person can not possess another per-

Figure 1: K-Parser output for “The man could not lift his son because he was so weak.
son). Lastly, the AddF eature module is used to implement
other features such as semantic roles of the entities by using Propbank Framesets [Palmer et al., 2005]. An option for
co-reference resolution is also provided in the system which
uses the Stanford Co-reference resolver [2010]. Furthermore,
many other tools are also used at various steps in the above
mentioned algorithm, such as Named Entity Tagging, WordNet [1995] database and Weka statistical classifier library
[Dimov et al., 2007]. Figure 1 demonstrates the output of KParser for the sentence The man could not lift his son because
he was so weak. We also defined an algorithm consisting of
a set of rules to match a question’s semantic representation
with that of the respective sentence’s and extract the pronoun
to be resolved from the sentence.
Kparser is a general semantic parser which has been used
beyond addressing WSC. In particular, it has been used in
parsing text in image interpretation and engineering design
specification, and is being used in interactive planning.

3.2

Knowledge Hunting

The questions in Winograd Schema Challenge can be easily
answered by human beings using simple knowledge that they
have learned over the years. One of the ways they learn is by
reading. For example, a Guardian article3 states that “There
is evidence that reading can increase levels of all three major categories of intelligence.” For example in the Winograd
Schema, Sentence: The man couldn’t lift his son because he
was so weak. Question: Who was so weak?; the answer to the
question can be achieved by using the commonsense knowledge that if X could not lift Y then X may be weak.
In our system we try to imitate this. The only difference
is that we retrieve the commonsense knowledge in an ondemand manner, and only relevant to the given sentence and
the question. We do that by creating string queries from the
3
http://www.theguardian.com/books/2014/jan/23/can-readingmake-you-smarter

concepts in the sentence and the question and use the queries
to retrieve sentences from a text repository.
We now use the above mentioned example to explain the
two step process. The first step is to create a query set by
using the representation of the given sentence and question.
Following are the sets of queries that are created:
• The first set of queries is created by using formal representations of both the Winograd sentence and the question.
All the nodes from the question’s formal representation (except the ones which represent “Wh” words) are mapped into
the formal representation of the given sentence. From the
mapped output, all the words/nodes which do not specify a
nominal entity are extracted and their different combinations
are joined together using a wild card (.*) and quotes (“”).
An example query for the lift example mentioned above is,
“.*could not lift.*because.*weak.*”.
• The second set of queries is created by replacing the verbs
in the previously created set of queries by their synonyms.
For example, a new query for the lift example that is generated is: “.*could not raise.*because.*weak.*”, where raise is
a synonym of lift.
Finally, a combined set of queries is formed by merging the
above two sets. The second sub-step in the commonsense
knowledge extraction process is to automatically search a
large corpus of English text using the queries and extract the
sentences which contain all the words (in any form) in the respective query. We used the Google search engine API along
with sentence splitting to get such sentences from the textual
part of WWW but the searching can be performed on other
text repositories as well. The idea here is to extract the sentences which contain the commonsense knowledge that is required to answer the question about the given Winograd sentence. One of the sentences extracted from Google by using the above mentioned queries for the lift example is “She
could not lift it because she is a weak girl.” In general, in
English language, when a sentence has two mentions of the
same pronoun (e.g. she) then, they both represent the same

Figure 2: K-Parser output for “She could not lift it because she is a weak girl.
entity. We use that in post-processing. Such post processing
is performed on the semantic representation of the sentence.
Figure 2 demonstrates the output of K-Parser for the sentence
“She could not lift it because she is a weak girl.”

3.3

Reasoning on Semantic Graphs

This section explains the module that answers the input questions using logical reasoning on the K-Parser output. The
module uses the semantic representation graphs of the given
sentence, and the sentences containing commonsense knowledge obtained by the knowledge hunting step. As mentioned in Section 2, we focus on the two categories of sentences: Direct Causal Events and Causal Attributive. We
use similar reasoning (i.e., matching) techniques for both of
them. Each given sentence and corresponding commonsense
knowledge sentences are translated into the semantic representation graphs, by using the K-Parser system. They are then
matched using a set of rules and logical constructs.
Consider our running example, where the semantic representation of the input sentence is shown in Figure 1, of the
common sense sentence is shown in Figure 2, and the question is “Who was so weak?”. From the question and the graph
of input sentence (Figure 1), we first find out that the pronoun to be resolved is he 9. We also find the event directly
connect to it (lif t 5), and the trait/property associated with it
(weak 12).
Next we extract similar features from the commonsense
knowledge sentence (Figure 2). We then match event lif t 4
and trait weak 13 to event lif t 5 and trait weak 12 in the
other graph. Next, we reason that the entity she 10 (Figure
2) must match to he 9 (Figure 1). Since the commonsense
knowledge sentence (in Figure 2) is simpler and there is no
ambiguity, we determine that she 10 and she 1 refer to the
same entity. From this, we deduce that the agent of event lift
is also its participant. Applying this knowledge on the input sentence (Figure 1), we have that the agent man 2 is the
co-referent of he 9, which is what we were looking for.

We have used Answer Set Programming (ASP) [Gelfond
and Lifschitz, 1988; Baral, 2003] to define the rules and constructs for reasoning. The semantic graphs are translated into
the ASP constructs by using quaternary predicate has. For
example, has(winograd,X,R,Y) means that (1) there exists an
edge in the semantic graph of the WSC sentence has end
nodes X and Y ; and (2) the semantic relation between them
is R. Alternatively, we use has(commonsense,X,R,Y) if the
edge is in the commonsense knowledge graph. We also use
toBeResolved(Pronoun) to define the pronoun to be resolved.
The following section gives more details about the reasoning rules. (A version of the ASP rules are available at
[Sharma, 2014].)
General Properties:
Three types of nodes: Events, Entities, and Classes (in
five possible types - see the legend in Figure 1) are used to
define the set of properties mentioned below.
• The basic transitivity relationship between two event nodes
is defined if an event node is reachable from another event
node, traversing along any directed edge in the semantic
graph.
• If two different nodes in different sentences (Winograd
or Commonsense) are instances of the same class then
they are defined as cross context siblings. For example
crossContextSiblings(lif t 5, lif t 4) follows from Figure
1 & 2.
• A node in any semantic graph is defined to have negative
polarity if it has an outgoing edge labeled as negative. For
example negativeP olarity(lif t 5) follows from Figure 1.
Type Specific Properties:
Type1: Causal Attributive:
• In the semantic graph of the Winograd sentence, if
there is an event node connected to the pronoun to be
resolved with an edge labeled participant, and there exists
an edge has(winograd, X, trait, B); then we define a
predicate attSubgraph(winograd, A, X, B). For example,

attSubgraph(winograd, lif t 5, he, weak) follows from
figure 1.
• A predicate that extracts nodes from the semantic
representation of the commonsense knowledge graph is
defined.
I.e., attSubgraph(commonsense, A, X, B),
if there exists attSubgraph(winograd, A1, X1, B1)
where
crossContextSiblings(A, A1)
and
crossContextSiblings(B, B1).
For
example
attSubgraph(commonsense, lif t 4, she 10, weak 13)
follows from figure 1 and 2.
• Finally the co-referent of the pronoun to be resolved
in
the
predicate
hasCoref erent(X, C)
if
attSubgraph(commonsense, A1, X1, B1),
has(commonsense, A1, R, X1)
and
there
exists an edge in the winograd sentence’s semantic graph has(winograd, A, R, C), (C6=X), where
crossContextSiblings(A, A1).
For
example,
hasCoref erent(he 9, man 2) follows from figure 1.
Type2: Direct Causal Events The reasoning for this type is
similar to the other type with a different set of predicates and
their definitions.
• There are two event nodes connected transitively in the
semantic graph of the Winograd sentence. First step is to
identify the similar chain of two transitive event nodes from
commonsense sentence’s semantic graph by using the general
properties defined in the previous section.
• A subgraph from the semantic representation of the given
Winograd sentence is extracted which consists of the pronoun
to be resolved and the events and entities required to resolve
it to its antecedent. A similar subgraph, based on the general
properties and matching event nodes extracted in the previous step is extracted from the semantic representation of the
commonsense sentence.
• Finally, both the subgraphs extracted in the previous steps
are compared (similar to Type1 reasoning mentioned above)
and the resolution of pronoun is done.

4

Evaluation and Error Analysis

Although, the purpose of this paper is to present a novel technique that hunts for commonsense knowledge and uses it to
answer difficult questions, we have evaluated our approach in
parts and as a whole. We first evaluated the K-parser component by itself and then evaluated the whole system with
respect to WSC.

4.1

K-Parser Evaluation

We developed K-Parser by using the training sentences collected from many sources such as the example sentences from
Stanford dependency manual [2008] and dictionary examples
for sentences with conjunctions. Though our initial intention behind developing K-Parser was to solve WSC, we realized that a non domain-specific semantic parser could help
us achieve our goal and would be a general contribution to
the NLP community. We evaluated the K-Parser’s output
for the test sentences from WSC against the manually created gold standard representations. We identified five important categories to assess the accuracy of K-Parser. The
categories are Number of Events, Number of Entities, Num-

Table 1: Evaluation Results table

Events
Entities
Classes
Event-Event Relations
Event-Entity Relations

Precision
0.94
0.97
0.86
0.91
0.94

Recall
0.92
0.96
0.79
0.79
0.89

ber of Classes, Number of Event-Event Relations and Number of Event-Entity Relations. We defined Precision and Recall of our system based on the above categories, Precision
= t1 /(t1 + t2 + t3 ), Recall = t1 /(t1 + t2 + t4 ), where, t1 =
identified and relevant and the label is correct; t2 = identified
and relevant and the label is wrong; t3 = identified, but not
relevant; t4 = not identified, but relevant.
Table 1 shows the evaluation results for K-Parser.

4.2

Overall System Evaluation and Error Analysis

There are 282 total sentence and question pairs in the Winograd Schema Challenge corpus. Out of those, we identified
a total of 71 sentences from both the categories Causal Attributive and Direct Causal Events. Among the 71 pairs, our
system is able to answer 53 and the remaining 18 pairs are
left unanswered. Out of the 53 answered, 49 are correctly answered. Four of them are incorrectly answered because the
commonsense knowledge found was inappropriate. For example the commonsense knowledge “I paid the price for my
stupidity. How grateful I am” was found for the Winograd
Schema sentence, Bob paid for Charlie’s college education,
he is very grateful. In this sentence, there is only one entity in the commonsense sentence (presented by the words, I
and my. As mentioned earlier, these words are post processed
as one in our semantic representation). Hence, this particular
extracted knowledge is not appropriate for the given sentence.
It must also be noted that our system does not return any
answer if no commonsense knowledge is found or it is not
sufficient to answer the question. This property is advantageous because it provides the ability to use another commonsense knowledge source and this process is repeatable. Furthermore, if the system finds multiple answers for the same
question, then the support for each answer is calculated in
terms of count and the answer with maximum support is the
final answer.

5

Related Works

The above sections explain two main components of our approach towards solving the WSC. In this section we compare
the tools we developed along with our approach in whole
with the currently existing tools and techniques that attempt
to solves these problems. Hence, we divide the comparison
section in two categories mentioned below.

5.1

Semantic Parsers

The semantic parsing systems available today belong to many
categories. For example, there are systems [Berant and Liang,

2014; Fader et al., 2014] that aim at translating factual questions into query representations. These queries are useful
to extract the answer to the given questions from a factual
knowledge base such as Freebase. Another category consists
of Semantic Role Labeling systems such as [Punyakanok et
al., 2004] and SEMAFOR parser [Das et al., 2010]. While
they assign semantic roles to entities and events in the text,
they lack the causal or non-causal relations between events or
actions. Furthermore, these systems do not correctly process
the implications, quantifications and conceptual class information about the text (eg. ”John” is an instance of ”person”
class). The category of semantic parsers that are non-domain
specific can be further classified to two classes. The first class
consists of systems which translate a given text into a logical
language. For example, Boxer [Bos, 2008] translates English
sentences into first order logic. Despite its many advantages,
it does not capture the event-event and event-entity relations
in the text. The inclusion of the homonym-hypernym information and resolution of identical meaning words are important for downstream reasoning. Such ontological information about entities or similarities between connectives are also
not captured in the Boxer system. The second class consists
of parsers which translate the given text into a graph based
representation. Each node in the graph represents a word or
concept and the edges (obtained from a predefined or newly
defined ontology) represent the relation between those concepts. For example Flanigan et al., [2014] illustrate a semantic parser that translates natural language strings into Abstract
Meaning Representation (AMR) [Banarescu et al., 2013].
AMR has limitations such as it does not have meaningful relation labels (e.g. ARG1, ARG2,... instead of agent, recipient...)
and event-event relations. TRIPS parser [Allen et al., 2007;
Dzikovska et al., 2003] also falls in the class of graph based
semantic parsers. It encodes features such as the conceptual
classes of the words, quantification of entities, and representation of the participants of an event. However, event-event
relations are not captured by this system.

5.2

Winograd Schema Challenge

There are few published techniques that aim at solving the
Winograd Schema Challenge or a similar corpus. One of
them is demonstrated by Rahman et al., [2012] which uses
a number of techniques to resolve pronouns in a Winograd
Schema like corpus. Their system uses various techniques
and combine their results on a corpus of 941 such schema.
There are a few issues with the techniques used in their system. For example a technique used by them creates string
queries from given sentences and finds the support for the
queries from Google search results. The issue with this technique is that sometimes the queries do not justify the outcome
of the technique. For example, a query for the sentence Lions
eat zebras because they are predators is “Lions are predators”. It makes sense to find the support for lions being predators based on this query. But if the sentence is changed to
Lions eat zebras because they are hungry then the support for
the query “Lions are hungry” is not capable of justifying the
fact that they in the sentence refers to Lions. This is because
zebras are equally likely to be hungry if no context is provided. A different work that attempts to solve the WSC by

using the modifications of above mentioned techniques along
with new ones is demonstrated in [Budukh, 2013]. However
the techniques performs decently on the WSC, the deeper
analysis of results leads to a conclusion that most of the correct answers do not follow humanoid reasoning, in fact they
are correct by chance.
Another work by Schuller [2014], demonstrates a graph
based technique and performs experiments on 4 out of 141
Winograd Schema pairs. It converts the given Winograd
sentence to a dependency graph using Stanford dependency
parser and then manually creates a background knowledge
dependency graph which is required to answer the question.
The main contribution of this work is to formalize a way to
combine both a given sentence dependency graph and the
manually created background knowledge dependency graph
in using relevance theory and then use Answer Set Programming (ASP) to extract the answer. But, unlike our approach,
commonsense knowledge is not automatically extracted by
this system.

6

Conclusion and Future Work

In this paper we report on our development of a semantic
parser (K-Parser) and a knowledge hunting mechanism and
their use in addressing the WSC. Given a WSC schema our
knowledge hunting mechanism searches a text repository to
find needed knowledge for that schema. For example, for the
sentence in Figure 1 and the question “Who was so weak”,
our system finds the sentence in Figure 2. The knowledge in
that sentence about X cannot lift Y when X is weak is needed
to answer the question. Indeed, our system uses that knowledge in answering the question. Whether this knowledge is a
deep background knowledge or is an evidence from the text
is a matter of where the line is drawn regarding when an evidence becomes knowledge. Regardless, our usage of knowledge of the form “X cannot lift Y when X is weak” is an
example of commonsense reasoning and is more meaningful
than bag-of-words or related statistical approaches commonly
used in NLP and NLU.
Although we achieved a notable accuracy on the two identified WSC categories (that covers 71 of 282 schemas), a lot
more remains to be done. As future work, we are working on identifying other categories of Winograd Schema sentences. We are also trying to create a knowledge repository of
commonsense knowledge by extracting the knowledge from
text repositories. But, additional success on these may imply that there is a need to develop a harder challenge schema
that would require deeper knowledge and more involved commonsense reasoning. We believe that to obtain more involved
knowledge from text, there is a need to go beyond standard
fact extraction approaches to semantic translation of text. The
research call by the Paul G. Allen foundation, and other research initiatives (such as the “Machine Reading” initiative)
have similar motivations and implications.

Acknowledgements
We thank NSF for the DataNet Federation Consortium grant
OCI-0940841 and ONR for their grant N00014-13-1-0334 for
partially supporting this research.

References
[Allen et al., 2007] James Allen, Mehdi Manshadi, Myroslava Dzikovska, and Mary Swift. Deep linguistic processing for spoken dialogue systems. In Proceedings of
the Workshop on Deep Linguistic Processing, pages 49–
56. ACL, 2007.
[Banarescu et al., 2013] Laura Banarescu, Claire Bonial,
Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and
Nathan Schneider. Abstract meaning representation for
sembanking. In Proc. of the Linguistic Annotation Workshop and Interoperability with Discourse. Citeseer, 2013.
[Baral, 2003] Chitta Baral. Knowledge representation, reasoning and declarative problem solving. Cambridge university press, 2003.
[Basile et al., 2007] Pierpaolo Basile, Marco Degemmis,
Anna Lisa Gentile, Pasquale Lops, and Giovanni Semeraro. The jigsaw algorithm for word sense disambiguation and semantic indexing of documents. pages 314–325,
2007.
[Berant and Liang, 2014] Jonathan Berant and Percy Liang.
Semantic parsing via paraphrasing. In Proceedings of
ACL, 2014.
[Bos, 2008] Johan Bos. Wide-coverage semantic analysis
with boxer. In Proceedings of the 2008 Conference on
Semantics in Text Processing, pages 277–286. ACL, 2008.
[Budukh, 2013] Tejas Ulhas Budukh. An intelligent coreference resolver for winograd schema sentences containing resolved semantic entities. Master’s thesis, Arizona
State University, 2013.
[Clark et al., 2004] Peter Clark, Bruce Porter, and Boeing Phantom Works. Km - the knowledge machine 2.0:
Users manual. Department of Computer Science, University of Texas at Austin, 2004.
[Das et al., 2010] Dipanjan Das, Nathan Schneider, Desai
Chen, and Noah A Smith. Semafor 1.0: A probabilistic
frame-semantic parser. Language Technologies Institute,
School of Computer Science, Carnegie Mellon University,
2010.
[De Marneffe and Manning, 2008] Marie-Catherine
De Marneffe and Christopher D Manning. Stanford
typed dependencies manual. URL http://nlp. stanford.
edu/software/dependencies manual. pdf, 2008.
[De Marneffe et al., 2006] Marie-Catherine De Marneffe,
Bill MacCartney, Christopher D Manning, et al. Generating typed dependency parses from phrase structure parses.
In Proceedings of LREC, volume 6, pages 449–454, 2006.
[Dimov et al., 2007] Rossen Dimov,
Michael Feld,
Dr Michael Kipp, Dr Alassane Ndiaye, and Dr Dominik Heckmann. Weka: Practical machine learning
tools and techniques with java implementations. AI Tools
SeminarUniversity of Saarland, WS, 6(07), 2007.
[Dzikovska et al., 2003] Myroslava O Dzikovska, James F
Allen, and Mary D Swift. Integrating linguistic and domain knowledge for spoken dialogue systems in multiple

domains. In Proc. of IJCAI-03 Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, 2003.
[Fader et al., 2014] Anthony Fader, Luke Zettlemoyer, and
Oren Etzioni. Open question answering over curated
and extracted knowledge bases. In Proceedings of the
20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1156–1165. ACM,
2014.
[Flanigan et al., 2014] Jeffrey Flanigan, Sam Thomson,
Jaime Carbonell, Chris Dyer, and Noah A Smith. A discriminative graph-based parser for the abstract meaning
representation. 2014.
[Gelfond and Lifschitz, 1988] Michael
Gelfond
and
Vladimir Lifschitz. The stable model semantics for
logic programming. In ICLP/SLP, volume 88, pages
1070–1080, 1988.
[Levesque et al., 2011] Hector J Levesque, Ernest Davis,
and Leora Morgenstern. The winograd schema challenge.
In AAAI Spring Symposium: Logical Formalizations of
Commonsense Reasoning, 2011.
[Litkowski, 2013] Ken Litkowski. The preposition project
corpora. Technical report, Technical Report 13-01. Damascus, MD: CL Research, 2013.
[Miller, 1995] George A Miller. Wordnet: a lexical database
for english. Communications of the ACM, 38(11):39–41,
1995.
[Palmer et al., 2005] Martha Palmer, Daniel Gildea, and
Paul Kingsbury. The proposition bank: An annotated
corpus of semantic roles. Computational linguistics,
31(1):71–106, 2005.
[Punyakanok et al., 2004] Vasin Punyakanok, Dan Roth,
Wen-tau Yih, and Dav Zimak. Semantic role labeling via
integer linear programming inference. In Proceedings of
the 20th international conference on Computational Linguistics, page 1346. ACL, 2004.
[Raghunathan et al., 2010] Karthik Raghunathan, Heeyoung
Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai
Surdeanu, Dan Jurafsky, and Christopher Manning. A
multi-pass sieve for coreference resolution. In Proceedings of the 2010 Conference on EMNLP, pages 492–501.
ACL, 2010.
[Rahman and Ng, 2012] Altaf Rahman and Vincent Ng. Resolving complex cases of definite pronouns: the winograd
schema challenge. In Proceedings of the 2012 Joint Conference on EMNLP and CoNLL, pages 777–789. ACL,
2012.
[Schuller, 2014] Peter Schuller. Tackling winograd schemas
by formalizing relevance theory in knowledge graphs. International Conference on Principles of Knowledge Representation and Reasoning, 2014.
[Sharma, 2014] Arpit Sharma. Solving winograd schema
challenge: Using semantic parsing, automatic knowledge
acquisition and logical reasoning. Master’s thesis, Arizona
State University, 2014.

Finitary S5-Theories
Tran Cao Son1 , Enrico Pontelli1 , Chitta Baral2 , and Gregory Gelfond2
1

2

Computer Science Department, New Mexico State University
{tson,epontell}@cs.nmsu.edu
Department of Computer Science Engineering, Arizona State University
{chitta,gelfond.greg}@asu.edu

Abstract. The objective of this paper is to identify a class of epistemic logic
theories with group knowledge operators which have the fundamental property
of being characterized by a finite number of finite models (up to equivalence). We
specifically focus on S5-theories. We call this class of epistemic logic theories as
finitary S5-theories. Models of finitary S5-theories can be shown to be canonical
in that they do not contain two worlds with the same interpretation. When the
theory is pure, these models are minimal and differ from each other only in the
actual world. The paper presents an algorithm for computing all models of a
finitary S5-theory. Finitary S5-theories find applications in several contexts—in
particular, the paper discusses their use in epistemic multi-agent planning.

1 Introduction and Motivation
Epistemic logics [2, 7, 8, 10] are a branch of modal logic that is concerned with representing and reasoning about the knowledge of collections of agents. These logics
allow us to represent and reason about the knowledge of an agent about the world, its
knowledge about other agents’ knowledge, group’s knowledge, common knowledge,
etc. The models of an epistemic theory are commonly given by pointed Kripke structures. Each pointed Kripke structure consists of a set of elements named worlds (also
known as points), a collection of binary relations between worlds (accessibility relations), a named valuation associated to each world, and an actual world—considered as
the “real state of the universe”. Models of an epistemic theory can be potentially infinite. Indeed, one can easily create an infinite model of an epistemic theory from a finite
one, by cloning its whole structure (including the accessibility relations, the worlds,
etc.). Bisimulation (e.g., [2]) can be used to reduce the size of a model. It is possible to
show that, given a theory that employs a single modal operator and has a finite signature,
there are only finitely many models with the property that (a) all of them are finite and
bisimulation-based minimal; and (b) any model of the theory is bisimilar (and, hence,
equivalent) to one of those models. This is not true, however, for multimodal theories,
i.e., theories with multiple modal operators.
In this paper, we study the questions of when a multimodal propositional epistemic
theory can be characterized by finitely many finite models (up to equivalence), and how
to compute these models. The motivation for these questions is twofold.
First, the question arises in the research on using epistemic theories in Multi-Agent
Systems (MAS), in particular, in the development of the Dynamic Epistemic Logic (DEL)
E. Fermé and J. Leite (Eds.): JELIA 2014, LNAI 8761, pp. 239–252, 2014.
c Springer International Publishing Switzerland 2014
⃝

240

T.C. Son et al.

[1, 3, 6, 11] for reasoning about effects of actions in MAS. This line of research has laid
the foundations for the study of the epistemic planning problem in multi-agent environments [5, 12, 14]. Yet, the majority of the research in epistemic planning assumes
that the set of initial pointed Kripke structures is given, and it is either finite [12, 14] or
recursively enumerable [5]. This creates a gap between the rich literature in theoretical
investigation of epistemic planning (e.g., formalization, complexity results) and the very
modest developments in automated epistemic planning systems—that can benefit from
the state-of-the-art techniques developed for planning systems in single-agent environments. In particular, there is a plethora of planners for single-agent environments, that
perform exceptionally well in terms of scalability and efficiency;1 the majority of them
are heuristic forward-search planners. On the other hand, to the best of our knowledge,
the systems described in [12, 14] are the only epistemic multi-agent planning prototypes
available, that search for solutions using breath-first search and model checking.
The second research motivation comes from the observation that the S5-logic is the
de-facto standard logic for reasoning and planning with sensing actions in presence of
incomplete information for single-agent domains. The literature is scarce on methods
for computing models of S5 multimodal epistemic theories. Works such as [15, 16]
are exceptions, and they focus on the least models of a modal theory. Several papers,
instead, assume that such models are, somehow, given. For instance, after describing
the muddy-children story, the authors of [7] present a model of the theory without detailing how should one construct such model and whether or not the theory has other
“interesting” models.
These observations show that, in order to be able to use epistemic logic as a specification language in practical MAS applications, such as epistemic multi-agent planning,
the issue of how to compute the set of models of a theory must be addressed.
In this paper, we address this question by identifying a class of finitary S5-theories
with group and common knowledge operators, that can be characterized by finitely
many finite models. We prove that each model of a finitary S5-theory is equivalent to
one of these canonical models, and propose an effective algorithm for computing such
set of canonical models. We discuss a representation of finitary S5-theories suitable for
use with the algorithm. We also discuss the impact of these results in epistemic multiagent planning.

2 Preliminary: Epistemic Logic
Let us consider the epistemic logic with a set AG = {1, 2, . . . , n} of n agents; we will
adopt the notation used in [2, 7]. The “physical” state of the world is described by a
finite set P of propositions. The knowledge of the world of agent i is described by a
modal operator Ki ; in particular, the knowledge of agents is encoded by knowledge
formulae (or formula) in a logic extended with these operators, and defined as follows.
• Atomic formulae: an atomic formula is built using the propositions in P and the
traditional propositional connectives ∨, ∧, →, ¬, etc. A literal is either an atom
f ∈ P or its negation ¬f . ⊤ (resp. ⊥) denotes true (resp. f alse).
1

E.g., http://ipc.icaps-conference.org/ lists 27 participants in the 2011 International Planning Competition.

Finitary S5-Theories

241

• Knowledge formulae: a knowledge formula is a formula in one of the following
forms: (i) An atomic formula; (ii) A formula of the form Ki ϕ, where ϕ is a knowledge formula; (iii) A formula of the form ϕ1 ∨ ϕ2 , ϕ1 ∧ ϕ2 , ϕ1 → ϕ2 , or ¬ϕ1 ,
where ϕ1 , ϕ2 are knowledge formulae; (iv) A formula of the form Eα ϕ or Cα ϕ
where ϕ is a formula and ∅ ̸= α ⊆ AG.
Formulae of the form Eα ϕ and Cα ϕ are referred to as group formulae. Whenever
α = AG, we simply write Eϕ and Cϕ to denote Eα ϕ and Cα ϕ, respectively. When no
confusion is possible, we will talk about formula instead of knowledge formula. Let us
denote with LP
AG the language of the knowledge formulae over P and AG. An epistemic
theory (or simply a theory) over the set of agents AG and propositions P is a set of
knowledge formulae in LP
AG . To illustrate the language, we will use the well-known
Muddy Children problem as a running example. For simplicity of the presentation, let
us consider the case with two children.
[Muddy Children] A father says to his two children that at least one of them
has mud on the forehead. He then repeatedly asks “do you know whether you
are dirty?” The first time the two children answer “no.” The second time both
answer “yes.” The father and the children can see and hear each other, but no
child can see his own forehead.
Let AG = {1, 2}. Let mi denote that child i is muddy. Some formulae in LP
AG are:
(i) mi (i is muddy); (ii) K1 m1 (child 1 knows he is muddy); (iii) K1 K2 m2 (child 1
knows that child 2 knows that he is muddy); and (iv) C{1,2} (m1 ∨ m2 ) (it is common
knowledge among the children that at least one is muddy).
The semantics of knowledge formulae relies on the notion of Kripke structures.
Definition 1 (Kripke Structure). A Kripke structure over AG = {1, . . . , n} and P is
a tuple ⟨S, π, K1 , . . . , Kn ⟩, where S is a set of points, π is a function that associates
an interpretation of P to each element of S (i.e., π : S,→2P ), and Ki ⊆ S × S for
1 ≤ i ≤ n. A pointed Kripke structure (or, pointed structure, for short) is a pair (M, s),
where M is a Kripke structure and s, called the actual world, belongs to the set of points
of M .
For readability, we use M [S], M [π], and M [i], to denote the components S, π, and Ki
of M , respectively. Using this notation, M [π](u) denotes the interpretation associated
to the point u.
Definition 2 (Satisfaction Relation). Given a formula ϕ and a pointed structure (M, s):
• (M, s) |= ϕ if ϕ is an atomic formula and M [π](s) |= ϕ;
• (M, s) |= Ki ϕ if for each t such that (s, t) ∈ Ki , (M, t) |= ϕ;
• (M, s) |= ¬ϕ if (M, s) ̸|= ϕ;
• (M, s) |= ϕ1 ∨ ϕ2 if (M, s) |= ϕ1 or (M, s) |= ϕ2 ;
• (M, s) |= ϕ1 ∧ ϕ2 if (M, s) |= ϕ1 and (M, s) |= ϕ2 ;
• (M, s) |= Eα ϕ if (M, s) |= Ki ϕ for every i ∈ α;
• (M, s) |= Cα ϕ if (M, s) |= Eαk ϕ for every k ≥ 0 where Eα0 ϕ = ϕ and Eαk+1 =
Eα (Eαk ϕ).
M |= ϕ denotes the fact that (M, s) |= ϕ for each s ∈ M [S], while |= ϕ denotes the
fact that M |= ϕ for all Kripke structures M . We will often depict a Kripke structure

242

T.C. Son et al.

M as a directed labeled graph, with S as the set of nodes and with edges of the form
(s, i, t) iff (s, t) ∈ Ki . We say that un is reachable from u1 if there is a sequence of
edges (u1 , i1 , u2 ), (u2 , i2 , u3 ), . . . , (un−1 , in−1 , un ) in M .
A Kripke structure denotes the possible “worlds” envisioned by the agents—and the
presence of multiple worlds denotes uncertainty and presence of different knowledge.
The relation (s1 , s2 ) ∈ Ki indicates that the knowledge of agent i about the real state
of the world is insufficient to distinguish between the state described by point s1 and
the one described by point s2 . For example, if (s1 , s2 ) ∈ Ki , M [π](s1 ) |= ϕ and
M [π](s2 ) |= ¬ϕ, everything else being the same, then this will indicate that agent i
is uncertain about the truth of ϕ. Figure 1 displays a possible pointed structure for the
1,2
s1:
m1

1,2
1

m2

m2

2
s3:
m1
¬m2

s2:
¬m1

2

1

1,2

s4:
¬m1
¬m2

1,2

Fig. 1. A possible pointed structure for the Muddy Children Domain

Muddy Children Domain. In Figure 1, a circle represents a point. The name and interpretation of the points are written in the circle. Labeled edges between points denote
the knowledge relations of the structure. A double circle identifies the actual world.
Various axioms are used to characterize epistemic logic systems. We will focus on
the S5-logic that contains the following axioms for each agent i and formulae ϕ, ψ:
(K)
|= (Ki ϕ ∧ Ki (ϕ ⇒ ψ)) ⇒ Ki ψ
(T)
|= Ki ψ ⇒ ψ
|= Ki ψ ⇒ Ki Ki ψ
(4)
|= ¬Ki ψ ⇒ Ki ¬Ki ψ
(5)
A Kripke structure is said to be an S5-structure if it satisfies the properties K, T, 4, and
5. It can be shown that the relations Ki of S5-structures are reflexive, transitive, and
symmetric. A theory plus the K, T, 4, and 5 axioms is often referred to as an S5-theory.
In the rest of this paper, we will consider only S5-theories. A theory T is said to be
satisfiable (or consistent) if there exists a Kripke structure M and a point s ∈ M [S]
such that (M, s) |= ψ for every ψ ∈ T . In this case, (M, s) is referred to as a model
of T . Two pointed structures (M, s) and (M ′ , s′ ) are equivalent if, for every formula
′ ′
ϕ∈LP
AG , (M, s) |= ϕ iff (M , s ) |= ϕ.
For simplicity of the presentation, we define
!
!
state(u) ≡
f∧
¬f
f ∈P, M[π](u)(f )=⊤

f ∈P, M[π](u)(f )=⊥

Finitary S5-Theories

243

for u ∈ M [S]. Intuitively, state(u) is the formula representing the complete interpretation associated to the point u in the structure M , i.e., M [π](u). We will often use
state(u) and M [π](u) interchangeably. We say that (M, s) is canonical if state(u) ̸≡
state(v) for every u, v ∈ M [S], u ̸= v. By M odsS5 (T ) we denote a set of S5-models
of a theory T such that: (a) there are no two equivalent models in M odsS5 (T ); and (b)
For each S5-model (M, s) of T , there exists a model (M ′ , s′ ) in M odsS5 (T ) such that
(M, s) is equivalent to (M ′ , s′ ).

3 Finitary S5-Theories: Definition and Properties
In this section, we define the notion of finitary S5-theories and show that a finitary
S5-theory can be characterized by finitely many finite models. We start with the specification of the types of formulae that we will consider. They are, in our observation,
sufficiently expressive for use in the specification of the description of the actual world
and the common knowledge among the agents. The allowed types of formulae are:
ϕ

(1)

C(Ki ϕ)
C(Ki ϕ ∨ Ki ¬ϕ)

(2)
(3)
(4)

C(¬Ki ϕ ∧ ¬Ki ¬ϕ)

where ϕ is an atomic formula. Intuitively, formulae of type (1) indicate properties that
are true in the actual world; formulae of type (2)-(3) indicate that all agents know that
agent i is aware of the truth value of ϕ; formulae of type (4) indicate that all agents
know that agent i is not aware of whether ϕ is true or false. Since our focus is on S5models of epistemic theories, it is easy to see that C(Ki ϕ) can be simplified to C(ϕ).
We say that a formula of the form (1)-(4) is in disjunctive form if its formula ϕ is a
disjunction over literals from P. A complete clause over P is a disjunction of the form
"
∗
∗
p∈P p where p is either p or ¬p.

Example 1. In the muddy children story, the knowledge of the children after the father’s
announcement but before the children look at each other can be encoded by a theory T0
consisting of the following formulae:
C(K1 (m1 ∨ m2 ))

C(¬K1 m1 ∧ ¬K1 ¬m1 )
C(¬K2 m1 ∧ ¬K2 ¬m1 )

C(K2 (m1 ∨ m2 ))

C(¬K1 m2 ∧ ¬K1 ¬m2 )
C(¬K2 m2 ∧ ¬K2 ¬m2 )

These formulae indicate that both children are aware that at least one of them is muddy
(the formulas in the first row), but they are not aware of who among them is muddy;
these items are all common knowledge.
If we take into account the fact that each child can see the other, and each child
knows if the other one is muddy, then we need to add to T0 the following formulae:
C(K1 m2 ∨ K1 ¬m2 )

C(K2 m1 ∨ K2 ¬m1 )

244

T.C. Son et al.

Definition 3 (Primitive Finitary S5-Theory). A theory T is said to be primitive finitary S5 if
• Each formula in T is of the form (1)-(4); and
• For each complete clause ϕ over P and each agent i, T contains either (i) C(Ki ϕ)
or (ii) C(Ki ϕ ∨ Ki ¬ϕ) or (iii) C(¬Ki ϕ ∧ ¬Ki ¬ϕ).
T is said to be in disjunctive form if all statements in T are in disjunctive form.
The second condition of the above definition deserves some discussion. It requires that
T contains at least |AG| × 2|P| formulae and could be unmanageable for large P. This
condition is introduced for simplicity of initial analysis of finitary S5-theories. This
condition will be relaxed at the end of this section by replacing the requirement “T
contains” with “T entails.” For example, the theory T0 is not a primitive finitary S5theory; T0 is a finitary S5-theory (defined later) as it entails a primitive finitary S5theory T1 .
Example 2. Let T1 be the theory consisting of:
C(Ki (m1 ∨ m2 ))
C(¬Ki (m1 ∨ ¬m2 ) ∧ ¬Ki (¬(m1 ∨ ¬m2 )))

C(¬Ki (¬m1 ∨ m2 ) ∧ ¬Ki (¬(¬m1 ∨ m2 )))

C(¬Ki (¬m1 ∨ ¬m2 ) ∧ ¬Ki (¬(¬m1 ∨ ¬m2 )))
where i = 1, 2. It is easy to see that T1 is a primitive finitary S5-theory—and it is
equivalent to T0 from Example 1.
Primitive finitary S5-theories can represent interesting properties.
Example 3. Consider the statement “it is common knowledge that none of the agents
knows anything.” The statement can be represented by the theory
T2 = {C(¬Ki ω ∧ ¬Ki ¬ω) | i ∈ AG, ω is a complete clause over P}.
We will show that a primitive finitary S5-theory can be characterized by finitely many
finite S5-models. The proof of this property relies on a series of lemmas. We will next
discuss these lemmas and provide proofs of the non-trivial ones. First, we observe that
points that are unreachable from the actual world in a pointed structure can be removed.
Lemma 1. Every S5-pointed structure (M, s) is equivalent to an S5-pointed structure
(M ′ , s) such that every u ∈ M ′ [S] is reachable from s.

The next lemma studies the properties of an S5-pointed structure satisfying a formula
of the form (2) or (3).
Lemma 2. Let (M, s) be an S5-pointed structure such that every u ∈ M [S] is reachable from s. Let ψ be an atomic formula. Then,
• (M, s) |= C(ψ) iff M [π](u) |= ψ for every u ∈ M [S].
• (M, s) |= C(Ki ψ∨Ki¬ψ) iff for every pair (u, v) ∈ M [i] it holds that M [π](u) |=
ψ iff M [π](v) |= ψ.

Finitary S5-Theories

245

Because C(Ki ψ) implies C(ψ) in an S5-pointed structure (M, s) the first item of
Lemma 2 shows that ψ is satisfied at every point in (M, s). The second item of Lemma 2
shows that every pair of points related by Ki either both satisfy or both do not satisfy
the formula ϕ in an S5-pointed structure (M, s) satisfying a formula of the form (3).
The next lemma shows that an S5-pointed structure satisfying a formula of the form
(4) must have at least one pair of points at which the value of the atomic formula mentioned in the formula differs. For a structure M and u, v ∈ M [S], M [π](u)(ψ) ̸=
M [π](v)(ψ) indicates that either (M [π](u) |= ψ and M [π](v) ̸|= ψ) or (M [π](u) ̸|= ψ
and M [π](v) |= ψ), i.e., the value of ψ at u is different from the value of ψ at v.
Lemma 3. Let (M, s) be an S5-pointed structure such that every u ∈ M [S] is reachable from s. Let ψ be an atomic formula. Then, (M, s) |= C(¬Ki ψ ∧ ¬Ki ¬ψ) iff
for every u ∈ M [S] there exists some v ∈ M [S] such that (u, v) ∈ M [i], and
M [π](u)(ψ) ̸= M [π](v)(ψ).
The proofs of Lemmas 1-3 follow from the definition of the satisfaction relation |= between a pointed structure and a formula and the fact that (M, s) |= C(ψ) iff (M, u) |=
ψ for every u reachable from s. For this reason, they are omitted.
We will now focus on models of primitive finitary S5-theories. Let M be a Kripke
structure. We define a relation ∼ among points of M as follows. For each u, v ∈ M [S],
u ∼ v iff state(u) ≡ state(v). Thus, u ∼ v indicates that the interpretations associated
to u and v are identical. It is easy to see that ∼ is an equivalence relation over M [S].
Let ũ denote the equivalence class of u with respect to the relation ∼ (i.e., ũ = [u]∼ ).
Lemma 4. Let (M, s) be an S5-model of a primitive finitary S5-theory such that every
u ∈ M [S] is reachable from s. Let ϕ be a complete clause and i ∈ AG. Given u ∈
M [S]:
• If (M, u) |= Ki ϕ then (M, s) |= C(Ki ϕ) or (M, s) |= C(Ki ϕ ∨ Ki ¬ϕ);
• If (M, u) |= ¬Ki ϕ then (M, s) |= C(¬Ki ϕ ∧ ¬Ki ¬ϕ).
The proof of Lemma 4 makes use of Lemmas 2-3 and the fact that (M, s) is an S5model of a primitive finitary S5-theory. The next lemma states a fundamental property
of models of primitive finitary S5-theories.
Lemma 5. Let (M, s) be an S5-model of a primitive finitary S5-theory such that every
u ∈ M [S] is reachable from s. Let u, v ∈ M [S] such that u ∼ v. Then, for every
i ∈ AG and x ∈ M [S] such that (u, x) ∈ M [i] there exists y ∈ M [S] such that
(v, y) ∈ M [i] and x ∼ y.
Proof. Let K(p, i) = {q | q ∈ M [S], (p, q) ∈ M [i]}—i.e., the set of points immediately related to p via M [i]. We consider two cases:
• Case 1: K(u, i) ∩ K(v, i) ̸= ∅. Since M [i] is an equivalent relation, we can conclude that K(u, i) = K(v, i) and the lemma is trivially proved (by taking x = y).
• Case 2: K(u, i) ∩ K(v, i) = ∅. Let us assume that there exists some x ∈ K(u, i)
such that there exists no y ∈ K(v, i) with x ∼ y. This means that (M, y) |=
¬state(x) for each y ∈ K(v, i). In other words, (M, v) |= Ki (¬state(x)). As
¬state(x) is a complete clause, this implies that (by Lemma 4):

246

T.C. Son et al.

(M, s) |= C(Ki ¬state(x)) or (M, s) |= C(Ki ¬state(x) ∨ Ki state(x)) (5)
On the other hand, (M, u) ̸|= Ki (¬state(x)), since x ∈ K(u, i) and (M, x) |=
state(x). This implies (M, s) |= C(¬Ki ¬state(x) ∧ ¬Ki state(x)) by Lemma 4.
This contradicts (5), proving the lemma.
!
Lemma 5 shows that the points with the same interpretation have the same structure in
an S5-model of a primitive finitary S5-theory, i.e., the accessibility relations associated
to these points are identical. This indicates that we can group all such points into a
single one, producing an equivalent model that is obviously finite. Let us show that this
# be the structure constructed as follows:
is indeed the case. Given a structure M , let M
#
• M [S] = {ũ | u ∈ M [S]}
#[π](ũ)(f ) = M [π](u)(f )
• For every u ∈ M [S] and f ∈ P, M
#[i] if there exists (u′ , v ′ ) ∈ M [i] such that u′ ∈ ũ and
• For each i ∈ AG, (ũ, ṽ) ∈ M
′
v ∈ ṽ.
#, s̃) the reduced pointed structure of (M, s) and prove that it is an S5-pointed
We call (M
structure equivalent to (M, s):

Lemma 6. Let (M, s) be an S5-model of a primitive finitary S5-theory T such that
#, s̃) be the reduced pointed
every u ∈ M [S] is reachable from s. Furthermore, let (M
#, s̃) is a finite S5-model of T that is equivalent to (M, s).
structure of (M, s). Then, (M

Proof. The proof of this lemma relies on Lemmas 2-3 and 5. We prove some representative properties.
#, s̃) is S5. Reflexivity and symmetry are obvious. Let us prove transitivity: as• (M
#[i] and (ṽ, w̃) ∈ M
#[i]. The former implies that there exists
sume that (ũ, ṽ) ∈ M
(u1 , v1 ) ∈ M [i] for some u1 ∈ ũ and v1 ∈ ṽ. The latter implies that there exists
(x1 , w1 ) ∈ M [i] for some x1 ∈ ṽ and w1 ∈ w̃. Since ∼ is an equivalence relation,
v1 ∼ x1 . Lemma 5 implies that there exists some w2 ∼w1 such that (v1 , w2 )∈M [i]
#[i], i.e.,
which implies that, by transitivity of M [i], (u1 , w2 )∈M [i], so (ũ, w̃)∈M
#[i] is transitive.
M
#, s̃) is a model of T . We have that
• (M
(M, s) |= C(Ki ψ ∨ Ki ¬ψ) iff
∀u, v ∈ M [S], (u, v) ∈ M [i] implies M [π](u) |= ψ iff M [π](v) |= ψ (by Lemma
2 w.r.t. (M, s)) iff
#[S], u ∈ p̃ and v ∈ q̃, (p̃, q̃) ∈ M
#[i] implies M
#[π](p̃)|=ψ iff M
#[π](q̃)|=ψ
∀p̃, q̃ ∈ M
#, s̃)) iff
(construction of (M
#
#, s̃)).
(M , s̃) |= C(Ki ψ ∨ Ki ¬ψ) (by Lemma 2 w.r.t. (M
The proof for other statements is similar.
#, s̃) is equivalent to (M, s). This is done by induction over the number of K
• (M
operators in a formula.
!
Let
$
%
#
#, s̃) | (M , s̃) is a reduced pointed structure
µM odsS5 (T ) = (M
| of a S5-model (M, s) of T
Since each S5-model of T is equivalent to its reduced pointed structure, which has at
most 2|P| points, we have the next theorem.

Finitary S5-Theories

247

Theorem 1. For a consistent primitive finitary S5-theory T , µM odsS5 (T ) is finite and
such that each (M, s) in µM odsS5 (T ) is also finite.
This theorem shows that primitive finitary S5-theories have the desired properties that
we are looking for. The next theorem proves interesting properties of models of primitive finitary S5-theories which are useful for computing µM odsS5 (T ).
Theorem 2. For a primitive finitary S5-theory T , every model (M, s) in µM odsS5 (T )
is canonical and |M [S]| is minimal among all models of T . Furthermore, for every
pair of models (M, s) and (W, w) in µM odsS5 (T ), M and W are identical, up to the
names of the points.
The first conclusion is trivial as each reduced pointed structure of a model of T is a
canonical model of T . The next lemma proves the second conclusion.
Lemma 7. Let T be a primitive finitary S5-theory, (M, s) and (V, w) in µM odsS5 (T ),
and let i ∈ AG.
• For each u ∈ M [S] there exists some v ∈ V [S] such that state(u) ≡ state(v).
• If (u, p) ∈ M [i] then there exists (v, q) ∈ V [i] such that state(u) ≡ state(v) and
state(p) ≡ state(q).
Proof. (Sketch) The proof of the first property is similar to the proof of Lemma 5, with
the minor modification that it refers to two structures and that both are models of T .
In fact, if u ∈ M [S] and there exists no v ∈ V [S] such that state(u) ≡ state(v)
then (V, w) |= C(Kk ¬state(u)) and (M, s) ̸|= C(Kk ¬state(u)) for k ∈ AG, a
contradiction. The proof of the second property uses a similar argument.
!
To prove that the set of points of a model in µM odsS5 (T ) is minimal, we use the
next lemma. We define:
F (T ) = {ϕ | ϕ appears in a formula of the form (2) of T }.
Lemma 8. Let (M, s) be a canonical model of a primitive finitary S5-theory T . Then,
the set M [S] is exactly the set of interpretations of F (T ) and each u ∈ M [S] is reachable.
Proof. (Sketch) First, it follows directly from Lemma 2 and C(Ki ψ) |= C(ψ) in an
S5-model that for each u ∈ M [S], state(u) |= ϕ for every ϕ ∈ F (T ). Second, because
T is primitive finitary, if there is some interpretation I of F (T ) such that there exists
no u ∈ M [S] and state(u) = I or there exists u ∈ M [S] with state(u) = I and
u is not reachable from s then we can conclude that (M, s) |= C(¬I), and because
T is a primitive finitary, we have that ¬I ∈ F (T ). This implies that I cannot be an
!
interpretation of F (T ), a contradiction. Both properties prove the lemma.
We are now ready to define the notion of a finitary S5-theory that allows for Theorem 1 to extend to epistemic theories consisting of arbitrary formulae.
Definition 4 (Finitary S5-Theory). An epistemic theory T is a finitary S5-theory if
T |= H and H is a primitive finitary S5-theory. T is pure if T contains only formulae
of the form (1)-(4).

248

T.C. Son et al.

We have that T0 (Example 1) is a finitary S5-theory, since T0 |= T1 and T1 is a primitive
finitary S5-theory. Since a model of T is also a model of H if T |= H, the following
theorem holds.
Theorem 3. Every finitary S5-theory T has finitely many finite canonical models, up
to equivalence. If T is pure then these models are minimal and their structures are
identical up to the name of the points.

4 Computing All Models of Finitary S5-Theories
In this section, we present an algorithm for computing µM odsS5 (T ) for a primitive
finitary S5-theory and discuss how this can be extended to arbitrary finitary S5-theories.
Lemma 8 shows that F (T ) can be used to identify the set of points of canonical models
of T . Applying this lemma to T0 (Example 1), we know that for every canonical model
(M, s) of T0 , M [S] = {s1 , s2 , s3 } where state(s1 ) = m1 ∧ m2 , state(s2 ) = m1 ∧
¬m2 , and state(s3 ) = ¬m1 ∧ m2 .
The next step is to determine the accessibility relations of i ∈ AG. We will rely on
Lemmas 2-3 and the following result:
Lemma 9. Let (M, s) be a canonical model of a consistent primitive finitary S5-theory
T and i ∈ AG. Assume that for each complete clause ϕ, if T ̸|= C(Ki ϕ) then T ̸|=
C(Ki ϕ ∨ Ki ¬ϕ). Then, (u, v) ∈ M [i] for every pair u, v ∈ M [S].
Proof. The proof of this lemma is by contradiction and uses an idea similar to that used
in the proof of Case 2 of Lemma 5. Since (M, s) is a canonical model, each u ∈ M [S] is
reachable from s. Assume that there exists a pair u, v ∈ M [S] such that (u, v) ̸∈ M [i].
We have that (M, u) |= Ki ¬state(v). As ¬state(v) is a complete clause, by Lemma 4:
(M, s) |= C(Ki ¬state(v))

or (M, s) |= C(Ki ¬state(v) ∨ Ki state(v))

(6)

On the other hand, since (u, v) ̸∈ M [i] and M is a S5-structure, we have that (M, v) ̸|=
Ki ¬state(v). This, together with the assumption of the lemma, contradicts (6).
6
⊓
Algorithm 12 computes all canonical minimal models of a primitive finitary S5-theory.
Its correctness follows from the properties of an S5-model of primitive finitary S5theories discussed in Lemmas 2-3 and 7-9. This algorithm runs in polynomial time in
the size of T , which, unfortunately, is exponential in the size of P.
Note that, for the theory T2 in Example 3, Algorithm 1 returns the set of pointed
structures (M, s) such that M [S] is the set of all interpretations of P, M [i] is a complete
graph on M [S], and s ∈ M [S].
Fig. 2 shows one model of T1 returned by Algorithm 1. Since C(Ki (l1 ∨l2 ) ∨
Ki ¬(l1 ∨l2 )) ̸∈ T1 for every complete clause over {m1 , m2 } that is different from
m1 ∨ m2 , there is a link labeled i between every pair of worlds of the model. Since
I(T1 ) is empty, µM odsS5 (T1 ) contains three models, which differ from each other
only in the actual world.
2

We assume that the theory is consistent.

Finitary S5-Theories

249

Algorithm 1. M odel(T )
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

Input: A primitive finitary S5-theory T
Output: µM odsS5 (T )
Compute I(T ) = {ϕ | ϕ appears in some (1) of T }
Compute F (T ) = {ϕ | ϕ appears in some (2) of T }
Σ = {u | u is an interpretation satisfying F (T )}
Let M [S] = Σ, M [π](u) = u, and M [i] = {(u, v) | u, v ∈ Σ}
for each C(Ki ϕ ∨ Ki ¬ϕ) in T do
remove (u, v) ∈ M [i] such that M [π](u)(ϕ) ̸= M [π](v)(ϕ)
end for
return {(M, s) | s satisfies I(T )}
1,2
s1:
m1

1,2
s2:
¬m1

1,2

m2

1,2

m2

1,2

s3:
m1
¬m2

1,2

Fig. 2. A model of the theory T1 in Example 2

The application of Algorithm 1 to an arbitrary finitary S5-theory T , where T |=
H for some primitive finitary S5-theory H, can be done in two steps: (a) Compute
µM odsS5 (H); and (b) Eliminate models from µM odsS5 (H) which are not a model
of T . Step (b) is necessary, since T can contain other formulae that are not entailed by
H.3 To accomplish (a), the following tasks need to be performed: (i) Verify that T is
finitary; (ii) Compute I(T ) = {ψ | ψ is an atomic formula and T |= ψ} (Line 3) and
F (T ) = {ϕ | T |= C(ϕ)} (Line 4); (iii) Test for entailment (Line 7); (iv) Eliminate
pointed structures that are not models of T (Line 10). Since these tasks are generally
computational expensive, it it naturally to seek ways to improve performance. In the
next section, we discuss a possible way to deal with (iv). We will next show that when
T is pure and in disjunctive form then the computation required in (i)-(iii) can be done
in polynomial time in the size of T .
Given a pure theory T in disjunctive form, Task (ii) can be done as described in
Lines 3 and 4 and does not require any additional computation. Given a pair (i, ϕ) of
an agent i and a complete clause ϕ, we would like to efficiently determine whether
T |= C(Ki ϕ), T |= C(Ki ϕ ∨ Ki ¬ϕ), or T |= C(¬Ki ϕ ∧ ¬Ki ¬ϕ) hold. This task
can be accomplished via a test for coverage
"defined as follows. We say that ϕ is covered
by a set W of disjunctions over P if ϕ ≡ ψ∈W ψ. A pair (i, ϕ) is covered by T if
3

A consequence of this elimination is that canonical models of a non-pure finitary S5-theory
may not have the same structure and/or different set of worlds.

250

T.C. Son et al.

• T contains some statement C(Kk ψ) (for some k ∈ AG) such that ψ |= ϕ; or
• ϕ is covered by some consistent set of disjunctions
W ⊆ {ψ | C(Ki ψ ∨ Ki ¬ψ) ∈ T }.
Intuitively, if (i, ϕ) is covered by T then it is common knowledge that i knows the
truth value of ϕ. The first item implies that T |= C(Ki ϕ), i.e., everyone knows ϕ.
The second item states that everyone knows that i knows ϕ—because (i) ϕ is covered by a set of disjunctions that are known by i, (ii) this is common knowledge, and
(iii) the axiom |= (Kψ1 ∨K¬ψ1 ) ∧ (Kψ2 ∨ K¬ψ2 ) ⇒ K((ψ1 ∨ψ2 )∨K¬(ψ1 ∨ψ2 )).
Thus, if ϕ is a complete clause and (i, ϕ) is not covered by T then T ̸|= C(Ki ϕ) and
T ̸|=C(Ki ϕ∨Ki ¬ϕ). It is easy to see that checking whether (i, ϕ) is covered by T can
be done in polynomial time in the size of T when T is pure and in disjunctive form.
The above discussion shows that, when T is pure and in disjunctive form, Algorithm 1 can compute all models of T , if it is finitary, without significant additional cost.
For example, T0 is pure and in disjunctive form and Algorithm 1 will return the same
set of models as if T1 is used as input.

5 Discussion
The previous sections focused on the development of the notion of a finitary S5-theory
and the computation of its models. We now discuss a potential use of finitary S5-theories
as a specification language. Specifically, we consider their use in the specification of
the initial set of pointed structures for epistemic multi-agent planning. Let us consider
a simple example concerning the Muddy Children Domain: “The father sees that his
two children are muddy. The children can see each other, hear the father, and truthfully
answer questions from the father but cannot talk to each other. They also know that
none of the children knows whether he is muddy or not. How can the father inform
his children that both of them are muddy without telling them the fact?” As we have
mentioned earlier, previous works in epistemic multi-agent planning assume that the set
of initial pointed structures is given, and these are assumed to be finite or enumerable.
However, a way to specify the set of initial pointed structures is not offered. Clearly,
finitary S5-theories can fill this need. Let us discuss some considerations in the use of
finitary S5-theories as a specification language.
The definition of a finitary S5-theory, by Definitions 3-4, calls for the test of entailment (or the specification) of |AG| × 2|P| formulae. Clearly, this is not desirable. To
address this issue, let us observe that Algorithm 1 makes use of formulae of the form (4)
implicitly (Line 6), by assuming that all but those complete clauses entailed by F (T )
are unknown to agent i and that is common knowledge. This means that we could reduce the task of specifying T by assuming that its set of statements of the form (4)
is given implicitly, i.e., by representing the information that the agents do not know
implicitly. This idea is similar to the use of the Closed World Assumption to represent
incomplete information. This can be realized as follows. For a theory T and an agent
i ∈ AG, let
'
&
C(T, i) = ϕ ϕ is a complete clause,T ̸|= C(Ki ϕ), T ̸|= C(Ki ϕ ∨ Ki ¬ϕ)
(
Let neg(T ) = i∈AG {C(¬Ki ϕ ∧ ¬Ki ¬ϕ) | ϕ ∈ C(T, i)}. The completion of T is
comp(T ) = T ∪ neg(T ).

Finitary S5-Theories

251

Given an arbitrary theory T , comp(T ) is a finitary S5-theory; as such, it could be
used as the specification of a finitary S5-theory. If comp(T ) is used and T is pure and
in disjunctive form, then the specification (of T ) only requires statements of the form
(1)-(3). As such, finitary S5-theories can be used in a manner similar to how the conventional PDDL problem specification describes the initial states—for epistemic multiagent planning. We expect that this can help bridging the gap between the development
of epistemic multi-agent planning systems and the research in reasoning about the effects of actions in multi-agent domains mentioned earlier since several approaches to
reasoning about actions and changes in multi-agent domains (e.g., [1, 3, 6, 11]) facilitate
the implementation of a forward search planner in multi-agent domains.
We close the section with a brief discussion on other potential uses of finitary S5theories. Finitary S5-theories are useful in applications where knowing that a property is
true/false is insufficient, e.g., knowing that a theorem is correct is good but knowing the
proof of the theorem (its witness) is necessary; knowing that a component of a system
malfunctions is a good step in diagnosis but knowing why this is the case is better;
knowing that a plan exists does not help if the sequence of actions is missing; etc.

6 Conclusion and Future Work
In this paper, we proposed the notion of finitary S5-theories and showed that a finitary
S5-theory has finitely many finite S5-models. We proved that models of primitive finitary S5-theories share the same structure and have minimal size in terms of the number
of worlds. We presented an algorithm for computing all canonical S5-models of a finitary S5-theory. We also argued that the algorithm runs in polynomial time in the size
of a pure finitary S5-theory in disjunctive form. We proposed the use of completion of
finitary S5-theories, enabling the implicit representation of negative knowledge, as a
specification language in applications like epistemic multi-agent planning.
As future work, we plan to expand this research in four directions. First, we will
experiment with the development of an epistemic multi-agent planner. Second, we will
investigate possible ways to relax the conditions imposed on finitary S5-theories, while
still maintaining its finiteness property. Third, we intend to investigate the relationships
between the notion of completion of finitary S5-theory and the logic of only knowing
for multi-agent systems developed by others (e.g., [13, 9]). Finally, we would like to
identify situations in which the S5-requirements can be lifted.
Acknowledgments. The research has been partially supported by NSF grants HRD1345232 and DGE-0947465. The authors would like to thank Guram Bezhanishvili and
Nguyen L. A for the useful discussions and suggestions.

References
[1] Baltag, A., Moss, L.: Logics for epistemic programs. Synthese (2004)
[2] van Benthem, J.: Modal Logic for Open Minds. Center for the Study of Language and
Information (2010)

252

T.C. Son et al.

[3] van Benthem, J., van Eijck, J., Kooi, B.P.: Logics of communication and change. Inf. Comput. 204(11), 1620–1662 (2006)
[4] Blackburn, P., Van Benthem, J., Wolter, F. (eds.): Handbook of Modal Logic. Elsevier
(2007)
[5] Bolander, T., Andersen, M.: Epistemic Planning for Single and Multi-Agent Systems. Journal of Applied Non-Classical Logics 21(1) (2011)
[6] van Ditmarsch, H., van der Hoek, W., Kooi, B.: Dynamic Epistemic Logic. Springer (2007)
[7] Fagin, R., Halpern, J., Moses, Y., Vardi, M.: Reasoning about Knowledge. MIT Press
(1995)
[8] Gabbay, D., Kurucz, A., Wolter, F., Zakharyaschev, M.: Many-Dimensional Modal Logics:
Theory and Application. Elsevier (2003)
[9] Halpern, J.Y., Lakemeyer, G.: Multi-agent only knowing. In: Shoham, Y. (ed.) Proceedings
of the Sixth Conference on Theoretical Aspects of Rationality and Knowledge, De Zeeuwse
Stromen, The Netherlands, pp. 251–265. Morgan Kaufmann (1996)
[10] Halpern, J., Moses, Y.: A guide to completeness and complexity for modal logics of knowledge and belief. Artificial Intelligence 54, 319–379 (1992)
[11] Herzig, A., Lang, J., Marquis, P.: Action Progression and Revision in Multiagent Belief
Structures. In: Sixth Workshop on Nonmonotonic Reasoning, Action, and Change, NRAC
(2005)
[12] van der Hoek, W., Wooldridge, M.: Tractable multiagent planning for epistemic goals. In:
Proceedings of The First International Joint Conference on Autonomous Agents & Multiagent Systems, AAMAS 2002, Bologna, Italy, pp. 1167–1174. ACM (2002)
[13] Lakemeyer, G., Levesque, H.J.: Only-knowing meets nonmonotonic modal logic. In:
Brewka, G., Eiter, T., McIlraith, S.A. (eds.) Principles of Knowledge Representation and
Reasoning: Proceedings of the Thirteenth International Conference, KR 2012, Rome, Italy,
June 10-14. AAAI Press (2012)
[14] Löwe, B., Pacuit, E., Witzel, A.: DEL planning and some tractable cases. In: van Ditmarsch,
H., Lang, J., Ju, S. (eds.) LORI 2011. LNCS, vol. 6953, pp. 179–192. Springer, Heidelberg
(2011)
[15] Nguyen, L.A.: Constructing the least models for positive modal logic programs. Fundam.
Inform. 42(1), 29–60 (2000)
[16] Nguyen, L.A.: Constructing finite least kripke models for positive logic programs in serial
regular grammar logics. Logic Journal of the IGPL 16(2), 175–193 (2008)

Knowledge Representation and Reasoning: What’s Hot
Chitta Baral

Giuseppe De Giacomo

Arizona State University
Arizona, USA
chitta@asu.edu

Sapienza University of Rome
Rome, Italy
degiacomo@dis.uniroma1.it

Knowledge representation and reasoning (KR) stems
from a deep tradition in logic. In particular, it aims at building systems that know about their world and are able to act in
an informed way in it, as humans do. A crucial part of these
systems is that knowledge is represented symbolically, and
that reasoning procedures are able to extract consequences
of such knowledge as new symbolic representations. Such
an ability is used to deliberate, in an informed fashion, about
the course of actions to take.
This very idea is radically new in human history
(Levesque 2014). It comes about after a long gestation,
stemming from Aristotle, who developed the initial notion
of logic though unrelated to the notion of computation; continued by Leibniz, who brought forward a notion of “thinking as computation,” though not yet symbolic; and later by
Frege, who developed the notion of symbolic logic, though
unrelated to computation; and finally by the breakthrough in
human thinking of the early part of last century with Church,
Godel, and Turing, who set the bases for symbolic logic
bound together with computation and ultimately for Computer Science, though even they did not think about logic as
a way of representing knowledge.
It was McCarthy (McCarthy 1959) who first had the very
radical idea of building intelligent systems by focusing not
on programming or on the details of the system architecture,
but on the knowledge necessary to behave in an intelligent
way. If a system were able to represent what it needed to
know and draw conclusions from it in an automatic way,
it would be able to deduce for itself how to behave intelligently.
Knowledge representation and reasoning is a wellestablished field of AI deriving directly from McCarthy’s
idea. Research in KR is currently present in the major AI
generalist conferences like the International Joint Conference on Artificial Intelligence (IJCAI), the AAAI Conference on Artificial Intelligence (AAAI), and European Conference on Artificial Intelligence (ECAI).
The series of International Conferences on Principles of
Knowledge Representation and Reasoning (KR) is one of
the most scientifically respected conferences in AI. The KR
conference series is a leading forum for timely in-depth presentation of progress in the theory and principles underlyc 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

ing the representation and computational management of
knowledge. The first KR conference was held 25 years ago
in 1989. The last KR edition KR 2014 was the 14th and was
held 25th year of the first KR conference. Since the first KR
conference there has been significant progress in KR and
now we have many well developed KR formalisms with supporting theories, building-block results, reasoning systems,
and applications. There are many books that present these
developments. We have reached a stage where various disciplines in AI and Computer Science are now using KR.
Among them is the use of KR in agents, semantic web applications, video understanding, natural language understanding, and many others.
The 2014 edition of KR was very special as it was held
as part of the Vienna Summer of Logic, the largest scientific
event in the history of logic. The Vienna Summer of Logic
(VSL, www.vsl2014.at) consisted of 12 large conferences
and 82 workshops.
The papers presented at KR 2014 can be broadly divided into the following areas: Description Logics, Reasoning about Actions and Processes; Belief Revision and
Nonmonotonicity; General Knowledge Representation and
Reasoning; Planning, Strategies, and Diagnosis; Answer Set
Programming and Logic Programming; Argumentation; Automated Reasoning and Computation; Causality; and Rationality and Uncertainty. In addition application oriented papers where presented as Reports from the Field. Prior to
the KR 2014 conference, in early 2013 the National Science Foundation of the USA sponsored and co-organized an
workshop1 titled “Research Challenges and Opportunities in
KR” where international researchers in KR discussed various aspects of KR as a field. It was a consensus that KR had
matured with a solid body of work, was being used in many
applications and there was a big interest in enhancing its use.
With that in mind the KR 2014 conference program had several invited talks and tutorials on emerging application areas as well as on traditional KR topics. This list of talks are
good pointers to what is hot in KR. They were in the topics
of: (a) role of KR in video understanding (Cohn. 2014); (b)
role of KR in natural language understanding (NLU), and
various available knowledge bases that can be and are being used in developing NLU systems (Ovchinnikova 2014);
1

http://krnsfworkshop.cs.illinois.edu/

(c) research in dynamic systems and multi-agent systems
(Baader 2014; Lomuscio 2014; Moss 2014) (d) connection between Databases, Ontologies and KR (Gottlob 2014;
Rosati 2014) and (e) Situation Calculus (McIlraith 2014).
Below we briefly elaborate on some of these and other hot
areas in current KR research.
Description Logics. Description logics have become one
of the main sub area of KR. However, description logics
are transforming deeply in these years. They are moving
from a component of a more general reasoning system to
the central representation tool for knowledge representation in data management settings. In particular the notion
of ontology-based data management has emerged, with the
idea of superimposing an ontology as a shared conceptual
layer (expressed in suitable description logics) over a set of
data sources forming a data layer, and using the ontology
as a virtual schema for querying the data sources. The ontology and the data sources are connected through declarative mappings that provide the semantic relationship between the two layers. This new setting is shaping the current research on description logics, where notions such as
light-weight description logics, data complexity, query answering, and query rewriting and processing, are becoming
main-stream research subjects. Practical realizability of the
proposed techniques is put forward and a rich set of tools
have been developed. This is complemented with work in
very different areas such as update, belief revision, forgetting, module extractions, and error pinpointing. Interestingly
some of the research in this field as become quite sophisticated mathematically, leveraging on model theory in modal
logic, connection with theory of databases, and connecting
with deep questions in CSP such as nonuniform complexity.
Reasoning about Actions. Reasoning about actions is another traditional area of KR. One of the most long lasting
line of research in this area is that on the Situation Calculus, first introduced by John McCarthy in 1963 as a way of
logically specifying dynamical systems. Such work has culminated with the publication of Ray Reiter’s seminal book
(Reiter 2001). Since then researchers continue to extend the
situation calculus, and use it to investigate and formalize
a variety of phenomena related to reasoning about actions.
Such a significant scientific deployment is providing crucial
help in the specification and implementation of a diversity
of automated reasoning endeavors including diagnosis, web
services composition and customization, and non-classical
automated planning. More traditional lines of research have
recently been complemented by a shift of interest from reasoning about single actions to reasoning about processes
and high-level programs. Generally speaking, the community is exhibiting an interest in program/process verification
aspects and in forms of model checking. Recently, some interesting decidability results of verification of processes in
situation calculus have been proven, which use techniques
that integrate model checking and databases. Work on process analysis using reasoning about action techniques and
frameworks have found applications in other research areas
such as service composition and orchestration and business
process analysis. Research in multi-agent systems have led

to new results and dimensions, such as techniques for processes synthesis and planning based on strategy logics such
as ATL, reasoning and verification with strategic knowledge,
model checking of autonomous agents systems, and reasoning about and planning with knowledge and belief goals.
KR Formalisms and techniques. While good progress has
been made in specific KR formalisms such as Answer Set
Programming, Logics of Uncertainty and the earlier mentioned Description Logic, there is now emerging interest in
combining the strengths of these individual formalisms so
that one can use a single formalism to reason about ontologies, about uncertainty, about quantitative parameters, about
defaults and about priorities.
Reasoning about causality and rationality continues to be
a hot area. The techniques of argumentation continue to be
improved and used in new ways and in new formalisms and
belief revision and merging continues to play an important
role in KR. For example, in multi-agent reasoning where one
may need to reason about agents’ beliefs and knowledge, belief revision is needed (instead of belief updates) to characterize effect of actions that change the belief of agents.
Emerging Applications. With the success of IBM’s Watson
and other developments there is now great interest in combining knowledge and reasoning with vision and natural language processing (NLP) to develop better scene understanding systems and natural language understanding systems, respectively. For example, the AAAI 2015 Spring symposia
has two different symposiums on KR, one on Commonsense reasoning and another on integrating symbolic and
neural approaches. A particular development that is driving
broader interest in KR applications is the automatic extraction of knowledge from other data, such as text and images.

References
Baader, F. 2014. Ontology-based monitoring of dynamic systems.
In VSL/KR. Keynote.
Cohn., A. 2014. Knowledge representation meets computer vision:
from pixels to symbolic activity descriptions. In KR. Invited Talk.
Gottlob, G. 2014. Datalog+/- : Questions and answers. In KR.
Invited Talk.
Levesque, H. 2014. On our best behavior. Artificial Intelligence
212:27–35.
Lomuscio, A. 2014. Verification of multi-agent systems against
epistemic specifications. In KR. Tutorial.
McCarthy, J. 1959. Programs with common sense. In Proceedings
of the Teddington Conference on the Mechanization of Thought
Processes, 756–791.
McIlraith, S. 2014. Situation Calculus: The last 15 years. In KR.
Invited Talk.
Moss, L. 2014. Dynamic epistemic logic and its interaction with
knowledge representation. In KR. Tutorial.
Ovchinnikova, E. 2014. Natural language understanding with
world knowledge and inference. In KR. Tutorial.
Reiter, R. 2001. Knowledge in Action: Logical Foundations for
Specifying and Implementing Dynamical Systems. MIT Press.
Rosati, R. 2014. Query answering and rewriting in ontology-based
data access. In KR. Tutorial.

Reasoning About the Beliefs of Agents in
Multi-Agent Domains in the Presence of State
Constraints: The Action Language mAL
Chitta Baral1 , Gregory Gelfond1 , Enrico Pontelli2 , and Tran Cao Son2
1

2

Arizona State University, Tempe, AZ 85281
New Mexico State University, Las Cruces, NM 88011

Abstract. Reasoning about actions forms the basis of many tasks such
as prediction, planning, and diagnosis in a dynamic domain. Within the
reasoning about actions community, a broad class of languages called
action languages has been developed together with a methodology for
their use in representing dynamic domains. With a few notable exceptions,
the focus of these efforts has largely centered around single-agent systems. Agents rarely operate in a vacuum however, and almost in parallel,
substantial work has been done within the dynamic epistemic logic community towards understanding how the actions of an agent may affect the
knowledge and/or beliefs of his fellows. What is less understood by both
communities is how to represent and reason about both the direct and
indirect effects of both ontic and epistemic actions within a multi-agent
setting. This paper presents a new action language, mAL, which brings
together techniques developed in both communities for reasoning about
dynamic multi-agent domains involving both ontic and epistemic actions,
as well as the indirect effects that such actions may have on the domain.

1

Introduction

Reasoning about actions and change has been one of the cornerstones of artificial
intelligence research ever since McCarthy’s description of the “advice taker
system” [16]. Since that time, a considerable body of work on a broad class of
languages called action languages together with a corresponding methodology
for their use has been developed [1,10,11]. A distinguishing characteristic of
such languages is their simple syntax and semantics which allow for concise and
natural representations of huge transition systems, and elegant solutions to the
frame problem [3,5,10,15]. With a few notable exceptions, [5,14], the focus of
such languages has been on representing an agent’s knowledge concerning sensing
and ontic actions (i.e., those which primarily affect the physical environment).
Agents rarely operate in isolation, often exchanging information, and consequently
almost in parallel, substantial work has been done within the Dynamic Epistemic
Logic (DEL) community towards understanding epistemic actions (i.e., those
which primarily affect the knowledge or beliefs of other agents) [2,8,7] and, to
a lesser extent ontic actions [6]. What is less understood by both communities

is how to represent and reason about both the direct and indirect effects of
both classes of actions in a multi-agent setting. In this paper we present a
new action language, mAL, which brings together techniques developed in both
communities for reasoning about dynamic multi-agent domains involving both
ontic and epistemic actions. Unlike prior works of both the action language [4],
and dynamic epistemic logic communities [6], mAL allows for the representation
of complex dependencies between fluents and provides a robust solution to the
ramification problem [17,12,13]. In addition, it is capable of representing domains
involving collaboration between agents for both ontic and epistemic actions.

Example 1 (A Multi-Agent “Lin’s Briefcase Domain”). Let us consider a multiagent variant of the “Lin’s Briefcase Domain” [13]: Three agents, A, B, and C, are
together in a room with a locked briefcase which contains a coin. The briefcase is
locked by two independent latches, each of which may be flipped open (or closed)
by an agent. Once both latches are open, the briefcase is unlocked and an agent
may peek inside to determine which face of the coin is showing. Suppose that
the briefcase is locked, and that this fact, together with the fact that none of the
agents knows which face of the coin is showing is common knowledge amongst
them. Furthermore, let us suppose that all of the agents are paying attention to
their surroundings, and that this is common knowledge as well. Lastly, let us
suppose that the coin is actually showing heads. How could agent A determine
the face of the coin while keeping B aware of his activities but leaving C in the
dark? One way could be as follows: A distracts C, causing him to look away;
once this is done, he flips open both latches, thereby unlocking the briefcase; and
finally A peeks inside.
Note that the domain in Ex. 1 contains both ontic (e.g., flipping the latches)
and epistemic (e.g., peeking into the briefcase) actions. In addition, the actions of
signaling/distracting an agent and flipping the latches have two classes of indirect
effects: those affecting the frames of reference (or degrees of awareness) that
agents have with respect to subsequent action occurrences, and those affecting
the physical properties of the domain. As an example of the former, once C is
distracted, he will be unaware of A’s subsequent activities. As an example of the
latter, flipping a latch open when its counterpart is as well, causes the briefcase
to become unlocked.
While the languages of action and update models developed within the DEL
community [2,6] provide an elegant means for deriving the direct effects of
both ontic and epistemic actions, they fall short when it comes to solving the
ramification problem, and consequently are unable to represent domains such as
the one presented in Ex. 1. Furthermore, their graphical nature and unification of
the distinct notions of an action and action occurrence, renders them inadequate
from a knowledge representation standpoint due to their lack of elaboration
tolerance. As we hope to show in this paper, both difficulties are overcome by
mAL.

2

The Action Language mAL

The action language mAL incorporates elements from the action languages AL
[11,9] and mA+ [4], adding to mA+ the ability to describe various dependencies
between fluents by the inclusion of state constraints.
2.1

Syntax

Theories of mAL are defined over a multi-agent domain D with a signature
Σ = (AG, F, A) where AG, F, and A, are finite, disjoint, non-empty sets of
symbols respectively defining the names of the agents within the domain, the
properties of the domain (or fluents), and the elementary actions which the
agents may perform. mAL supports two broad classes of actions: ontic and
epistemic actions, the former describing actions which affect the properties of the
domain represented by fluents, and the latter describing actions which primarily
affect the agents’ beliefs. Epistemic actions are further broken into two categories:
sensing and communication. Sensing actions represent actions which an agent
may perform in order to learn the value of a fluent, while communication actions
are used to represent actions which communicate information between agents.
Ontic properties of the domain are represented by fluents, while the various
epistemic properties are represented by modal formulae:
Definition 1 (Modal Formula [8]). Let D be a multi-agent domain with the
signature Σ = (AG, F, A). The set of modal formulae over Σ is defined as
follows:
– f ∈ F is a formula
– if ϕ is a formula, then ¬ϕ is a formula
– if ϕ1 and ϕ2 are formulae, then ϕ1 ∧ ϕ2 , ϕ1 ∨ ϕ2 , ϕ1 → ϕ2 , and ϕ1 ≡ ϕ2 are
formulae
– if α ∈ AG and ϕ is a formula, then Bα ϕ is a formula
– if γ ⊆ AG and ϕ is a formula, then Eγ ϕ and Cγ ϕ are formulae
As the modality of discourse is that of belief, we adopt the following readings of
modal formulae: Bα ϕ, is understood to mean that “agent α believes ϕ”; formulae
of the form Eγ ϕ denote that “every member of γ believes ϕ”, while those of the
form Cγ ϕ are read as “every member of γ believes ϕ, and every member of γ
believes that every member of γ believes ϕ, ad infinitum, (i.e. ϕ is a commonly
held belief amongst the agents of γ).”
The direct effects of ontic actions are described by dynamic causal laws which
are statements of the form:
a causes λ if φ
(1)
where a is an action, λ is a fluent literal, and φ is a conjunction of fluent literals.
Laws of this form are read as: “performing the action a in a state which satisfies
φ causes λ to be true.” If φ is a tautology, then we simply write the following:
a causes λ

(2)

Sensing actions are described by sensing axioms which have the form:
a determines f

(3)

where a is the name of an action, and f is a fluent. Statements of this form are
understood to mean: “if an agent performs the action a, he will learn the value of
the fluent f .” Communication actions are described by communication axioms
which have the form:
a communicates ϕ
(4)
where a is the name of an action, and ϕ is a modal formula. In mAL only truthful
announcements are allowed.
The constructs (1)–(4) only describe the direct effects of their respective
actions. In general, an agent’s actions may indirectly affect the knowledge/beliefs
of his fellows, as well as the values of various fluents. As in mA+, indirect effects
of the first form are determined by the frames of reference (or levels of awareness)
that the agents have with respect to the action. In general, for any given action
occurrence we divide the agents of the domain into three groups: those who are
fully aware of both the action occurrence and its effects; those who are aware
of the occurrence but not the full consequences of the action; and those agents
who are oblivious as to what has transpired. Frames of reference are dynamic in
nature and are described by perspective axioms which are statements of the form:
X observes a if φ

(5)

X aware of a if φ

(6)

where X is a set of agent names, a is an action, and φ is a modal formula.
Perspective axioms of the first form (called observation axioms) define the set of
agents who are fully aware of both the action occurrence and its effects. Those
of the second form (called awareness axioms) define the set of agents who are
aware of the occurrence, but only partially of its effects. By default, we assume
that all other agents within the domain are oblivious. As with dynamic causal
laws, if φ is a tautology, we adopt the following shorthand:
X observes a

(7)

X aware of a

(8)

The inclusion of observation axioms allows us to make explicit the assumption
that agents are aware of the actions they perform. In mAL, the only assumptions
made regarding the frames of reference of the agents are that those who are fully
aware of an action occurrence and its effects, as well as those who are aware only
of the occurrence, know the frames of reference of all of the agents within the
domain.
Unlike mA+, mAL includes state constraints which are statements of the
form:
λ if φ
(9)

where λ is a fluent literal and φ is a conjunction of fluent literals. Statements of
this form are read as: “if φ is true in a state, then λ must also be true in that
state.” State constraints are used to represent dependencies between fluents and
provide a powerful means for representing indirect effects of the second form.
Lastly, executability conditions, which are statements of the form:
impossible a if φ

(10)

where a is an action and φ is a modal formula, are used to describe when actions
may not be performed.
Definition 2 (Action Description of mAL). An action description, ∆, in
mAL is a collection of statements of the form (1)–(10).
Now that the syntax has been introduced, we present a detailed axiomatization
of the multi-agent variant of the Lin’s Briefcase Domain from Ex. 1.
Example 2 (Axiomatization of the Multi-Agent “Lin’s Briefcase Domain”). Let λ
be a variable ranging over the set {l1 , l2 } representing the latches governing the
briefcase. Similarly, let α, α1 , and α2 , be variables ranging over the set of agents
in our domain. We begin our representation by adopting the following domain
signature Σ = (AG, F, A) where:
AG = {A, B, C}
F = {open(λ), locked, heads, attentive(α)}
A = {f lip(α, λ), peek(α), signal(α1 , α2 ), distract(α1 , α2 )}
The direct effects of the action f lip(α, λ) are represented via the following pair
of dynamic causal laws:
f lip(α, λ) causes open(λ) if ¬open(λ)

(11)

f lip(α, λ) causes ¬open(λ) if open(λ)

(12)

The following state constraint models the indirect effects of the action, f lip(α, λ),
namely that the briefcase is unlocked once both latches are open.
¬locked if open(l1 ) ∧ open(l2 )

(13)

The agent directly performing the action f lip(α, λ), as well as any attentive
agents are considered to be fully aware of the action occurrence and of its full
effects. This information may be encoded by the following pair of perspective
axioms:
{α} observes f lip(α, λ)

(14)

{α2 } observes f lip(α1 , λ) if attentive(α2 )

(15)

The action, peek(α), is an epistemic action — in particular, it is a sensing action.
Consequently its direct effects are represented by the following sensing axiom:
peek(α) determines heads

(16)

The fact that an agent may not peek into a locked briefcase is represented by
the following executability condition:
impossible peek(α) if locked

(17)

Unlike the action f lip(α, λ), only the agent who is peeking is fully aware of the
occurrence and its full effects. Agents who are attentive, are only partially aware
of the action’s effects. This is represented by the following perspective axioms:
{α} observes peek(α)

(18)

{α2 } aware of peek(α1 ) if attentive(α2 )

(19)

Lastly, the actions signal(α1 , α2 ) and distract(α1 , α2 ) are represented in a similar
fashion:

2.2

signal(α1 , α2 ) causes attentive(α2 )

(20)

{α1 , α2 } observes signal(α1 , α2 )

(21)

{α} observes signal(α1 , α2 ) if attentive(α)

(22)

distract(α1 , α2 ) causes ¬attentive(α2 )

(23)

{α1 , α2 } observes distract(α1 , α2 )

(24)

{α} observes distract(α1 , α2 ) if attentive(α)

(25)

Semantics

Before we discuss the semantics of our language, we must first introduce the
notions of a Kripke structure and Kripke world.
Definition 3 (Kripke Structure [8]). Let D be a multi-agent domain with
signature, Σ = (AG, F, A), where AG = {α1 , . . . , αn }. A Kripke structure, M ,
is a tuple of the form (Ω, π, Rα1 , . . . , Rαn ) where:
– Ω is a nonempty set of possible worlds
– π is an interpretation function which for each ω ∈ Ω gives an interpretation,
π(ω) : F 7→ {true, f alse}
– each Rαi is a binary relation on Ω called an accessibility relation for agent
αi
Possible worlds and their respective interpretations describe potential physical
configurations of the domain, while the accessibility relations represent its various
epistemic properties. Intuitively, the pair (ωσ , ωτ ) ∈ Rαi represents the property
that from within possible world ωσ , agent αi cannot distinguish between ωσ and
ωτ .
Definition 4 (Kripke World [8]). A Kripke world is a pair, (M, ω), where
M is a Kripke structure, and ω is a possible world of M .

For a given Kripke world, (M, ω), ω denotes which possible world of M
corresponds to the real physical state of the world as known to an impartial
external observer.
Having defined the notions of a Kripke structure and a Kripke world, we can
now define the semantics of modal logic.
Definition 5 (Entailment Relation for Modal Formulae). Let (M, ωσ ) be
a Kripke world in a multi-agent domain, D, with the signature Σ = (AG, F, A).
–
–
–
–
–

(M, ωσ ) |= f where f ∈ F iff M.π(ωσ )(f ) = >
(M, ωσ ) |= ¬ϕ iff (M, ωσ ) 6|= ϕ
(M, ωσ ) |= ϕ1 ∧ ϕ2 iff (M, ωσ ) |= ϕ1 and (M, ωσ ) |= ϕ2
(M, ωσ ) |= ϕ1 ∨ ϕ2 iff (M, ωσ ) |= ϕ1 or (M, ωσ ) |= ϕ2
(M, ωσ ) |= Bα ϕ iff (M, ωτ ) |= ϕ for all ωτ such that (ωσ , ωτ ) ∈ M.Rα

Let E0γ ϕ be equivalent to ϕ and let Ek+1
ϕ be Eγ Ekγ ϕ.
γ
– (M, ωσ ) |= Eγ ϕ iff (M, ωσ ) |= Bα ϕ for each α ∈ γ
– (M, ωσ ) |= Cγ ϕ iff (M, ωσ ) |= Ekγ ϕ for k = 1, 2, 3, . . .
As is the case with other action languages, an action description of mAL
defines a transition diagram whose nodes correspond to states of the domain
— which we model as Kripke worlds — and whose arcs are labeled by actions.
Within a particular state, the possible worlds comprising the underlying Kripke
world correspond to complete consistent sets of fluent literals closed under the
state constraints of the action description.
Example 3 (Initial State of the Multi-Agent “Lin’s Briefcase Domain”). The
initial state, σ0 , of the domain from Ex. 1 corresponds to the Kripke world,
(M0 , ω1 ), shown in Fig. 1. σ0 consists of two possible worlds, ω1 and ω2 , where:
– M0 .π(ω1 ) = {heads, attentive(A), attentive(B), attentive(C), ¬open(l1 ),
¬open(l2 ), locked}
– M0 .π(ω2 ) = {¬heads, attentive(A), attentive(B), attentive(C), ¬open(l1 ),
¬open(l2 ), locked}
A graphical convention that we adopt in this work is to present Kripke structures/worlds as directed graphs whose nodes are drawn as circles with unbroken
lines. The possible world(s) designated as pertaining to the real state of the world
are marked by a double circle.
The semantics of mAL is defined via a transition function, Φ∆ (σ, a), which
when applied to a state, σ, and an action occurrence, a, yields the corresponding
successor state(s). The approach taken in this paper combines methods used in
defining the semantics of AL [9], with an approach based on that of [6]. The key
intuition behind our semantics is that reasoning about the effects of an action is
a two-step process: an agent first reasons about how his fellows may perceive his
action, thereby establishing an epistemic configuration for the successor state;
and then reasons about how his action may actually play out.

A, B, C

ω1

A, B, C

(a)

A, B, C

A, B, C

ω2

ω1

A, B, C

A, B, C

ω2

(b)

Fig. 1. The initial state, σ0 , of the Multi-Agent Lin’s Briefcase Domain. 1(a) shows the
underlying Kripke structure of σ0 , while 1(b) corresponds to σ0 itself.

Frames of Reference In order to reason about how his fellows will perceive his
actions, an agent must reason about their respective frames of reference; hence
the inclusion of perspective axioms, which allow one to dynamically specify the
levels of awareness that the agents have with respect to an action occurrence.
It must be emphasized, that frames of reference are thought of as attributes of
an action that differ from one action occurrence to another. This is in marked
contrast with the approach of [2,6], in which different frames of references yield
very different actions.
Definition 6 (Frames of Reference). Let ∆ be an action description of mAL,
σ = (M, ω) be a state of the transition diagram defined by ∆, and a be an action.
The various frames of reference of the agents are defined as follows:
– the set of agents who are fully aware of a, denoted by f (σ, a), is {α ∈ AG |
[α observes a if φ] ∈ ∆ ∧ (M, ω) |= φ}
– the set of agents who are partially aware of a, denoted by p(σ, a), is {α ∈
AG | [α aware of a if φ] ∈ ∆ ∧ (M, ω) |= φ}
– the set of agents who are oblivious of a, denoted by o(σ, a), is AG \ f (σ, a) ∪
p(σ, a)
Update Schema/Instantiations On a semantic level, an action occurrence is
represented by an update schema 3 , which may be thought of as a Kripke structure
capturing how the agents in a domain perceive various action occurrences. Rather
than possible worlds, an action occurrence is described by a number of scenarios,
each of which is associated with a necessary precondition. What the agents believe
about the scenarios is described by their respective accessibility relations. This
leads to the following definition:
Definition 7 (Update Schema). Let L denote the set of all modal formulae that may be defined over the multi-agent domain, D, with signature Σ =
(AG = {α1 , . . . , αn }, F, A). An update schema, U , is a tuple of the form
(Sc , Rα1 , . . . , Rαn , pre) where:
3

Update schema/instantiations are analogous to the certain structures described in
[2,6], but have been renamed to reflect different intuitions behind their use.

– Sc is a finite, non-empty set of scenarios
– each Rαi is a binary relation on ε called an accessibility relation for agent αi
– pre : Sc 7→ L assigns a precondition function to each scenario
An update schema only describes the beliefs of the agents with regards to
a particular action occurrence. As with Kripke structures, they do not describe
which scenario actually took place. To accomodate this additional information,
update schema are instantiated by specifying which scenario actually occurred.
Definition 8 (Update Instantiation). An update instantiation is a pair,
(U, ε), where U is an update schema, and ε is a scenario of U .
In the context of mAL, we define three particular update instantiations, for
ontic, sensing, and communication actions respectively: υo (σ, a), υs (σ, a), and
υc (σ, a).
The intuition behind υo (σ, a) is relatively straightforward: the agents are
either aware of the action occurrence or are oblivious. In addition, we make the
assumption that those agents who are aware of the action occurrences know
which agents are oblivious.
Definition 9 (Ontic Instantiation). The function υo (σ, a) yields the set of
update instantiations represented by the pair (U, Γ ) where U is defined as follows:
– U.Sc = {εp , εi }
– U.Rα = {(εp , εp ), (εi , εi )} for each agent in f (σ, a)
– U.Rα = {(εp , εi ), (εi , εi )} for each agent in o(σ, a)
Let Ψ = {φ | [impossible a if φ] ∈ ∆}.
W
– U.pre(εp ) = ¬( Ψ )
– U.pre(εi ) = >
and Γ = {εp }.
υs (σ, a) is based on the following intuition: the real value of f is revealed to
those agents who are performing the action, causing it to become a commonly
held belief amongst them; agents who observe the action learn that the value
of f has been revealed to those agents who were directly involved in it; and the
beliefs of oblivious agents remain unchanged.
Definition 10 (Sensing Instantiation). The function υs (σ, a) yields the set
of update instantiations represented by the pair (U, Γ ) where U is defined as
follows:
–
–
–
–

U.Sc = {εp , εn , εi }
U.Rα = {(εp , εp ), (εn , εn ), (εi , εi )} for each agent in f (σ, a)
U.Rα = {(εp , εp ), (εn , εn ), (εi , εi ), (εp , εn ), (εn , εp )} for each agent in p(σ, a)
U.Rα = {(εp , εi ), (εn , εi ), (εi , εi )} for each agent in o(σ, a)

Let f be the fluent determined by the sensing axiom for the action a, and let
Ψ = {φ | [impossible a if φ] ∈ ∆}.
W
– U.pre(εp ) = f ∧ ¬( Ψ )
W
– U.pre(εn ) = ¬f ∧ ¬( Ψ )
– U.pre(εi ) = >
and Γ = {εp , εn }.
The intuition behind υc (σ, a) similar to that of sensing actions: ϕ becomes a
commonly held belief amongst those agents who receive/hear the message; agents
who observe the action learn that the value of ϕ has been revealed to those agents
who heard it (they are however unaware of the truth of ϕ); and lastly, the beliefs
of oblivious agents are unchanged.
Definition 11 (Communication Instantiation). The function υc (σ, a) yields
the set of update instantiations represented by the pair (U, Γ ) where U is defined
as follows:
–
–
–
–

U.Sc = {εp , εn , εi }
U.Rα = {(εp , εp ), (εn , εn ), (εi , εi )} for each agent in f (σ, a)
U.Rα = {(εp , εp ), (εn , εn ), (εi , εi ), (εp , εn ), (εn , εp )} for each agent in p(σ, a)
U.Rα = {(εp , εi ), (εn , εi ), (εi , εi )} for each agent in o(σ, a)

Let ϕ be the formula specified by the communication axiom for the action a, and
let Ψ = {φ | [impossible a if φ] ∈ ∆}.
W
– U.pre(εp ) = ϕ ∧ ¬( Ψ )
W
– U.pre(εn ) = ¬ϕ ∧ ¬( Ψ )
– U.pre(εi ) = >
and Γ = {εp }.
Example 4 (Update Instantiations for the Action distract(A, C)). Consider an
action occurrence distract(A, C). Suppose that agent B is oblivious of the action
occurrence. In this case, the corresponding ontic instantiation, is shown in
Fig. 2(a). Now consider an occurrence of the action distract(A, C), where all
of the agents are fully aware of the action occurrence. This yields a different
ontic instantiation, shown in Fig. 2(b). Here we adopt the graphical convention
of presenting update schema/instantiations as directed graphs whose nodes are
drawn as rounded rectangles with solid lines. Once again, it bears emphasizing
that the action, distract(A, C), is the same in both instances. Only the attributes
corresponding to the agents’ frames of reference (which are represented by the
respective arcs) differ.

A, C

εp

B

(a)

A, B, C

A, B, C

εi

εp

(b)

Fig. 2. 2(a) shows the ontic instantiation for an occurrence of the action distract(A, C)
with A and C fully aware of the occurrence, and agent B oblivious. 2(b) show the ontic
instantiation of the same action with all agents fully aware of the occurrence.

Epistemic Configurations When reasoning about the effects of an action, an
agent first establishes what is called an epistemic configuration of the successor
state. This is done by the application of what we term an epistemic update to a
state, and the update instantiation of action occurrence in question.
An epistemic configuration defines the general graphical structure of the
successor state. Consequently it is similar to a Kripke structure, but does not
include the interpretation function. This intuition leads to the following pair of
definitions:
Definition 12 (Epistemic Schema). Let D be a multi-agent domain with
signature Σ = (AG = {α1 , . . . , αn }, F, A). An epistemic schema, E, is a tuple of
the form (S, Rα1 , . . . , Rαn ) where:
– S is a nonempty set of situations
– each Rαi is a binary relation on S called an accessibility relation for agent
αi
Definition 13 (Epistemic Configuration). An epistemic configuration is a
pair, (E, s), where E is an epistemic schema, and s is a situation of E.
In order to obtain the epistemic configuration of the successor state, we apply
an operation that we call the epistemic update operation, which when applied to
a state and an update instantiation, defines an epistemic configuration for the
successor state.
Definition 14 (Epistemic Update Operation). Given a state, σ = (M, ω),
and an update instantiation υ = (U, ε), such that σ |= U.pre(ε), Eu(σ, υ) defines
the epistemic configuration EC = (E, (ω, ε)) where:
– E.S = {(ωj , εj ) | ωj ∈ M.Ω, εj ∈ U.Sc , (M, ωj ) |= U.pre(εj )}
– E.Rα = {((ωj , εj ), (ωk , εk )) | (ωj , ωk ) ∈ M.Rα , (εj , εk ) ∈ U.Rα }
Example 5 (Applying the Epistemic Update). Recall from Ex. 1 that A first
distracts C. The action distract(A, C) is an ontic action which directly affects
the fluent attentive(C) as specified by the dynamic causal law (23). Perspective
axioms (24) and (25), together with the fact that the agents are initially attentive,

give f (σ0 , distract(A, C)) = {A, B, C} and o(σ0 , distract(A, C)) = ∅ as the
agents’ frames of reference. υo (σ0 , distract(A, C))) is shown in Fig. 3(a) and the
epistemic configuration of the successor state resulting from the occurrence of the
action distract(A, C) is given by Eu(σ0 , υo (σ0 , distract(A, C))), and is shown in
Fig. 3(b). For epistemic configurations, we adopt the convention of presenting
them as directed graphs whose nodes are drawn as dashed circles (the dashed
lines help express the notion that the scenarios are later expanded into possible
worlds). Those scenarios which correspond to potential real possible worlds are
marked by a double dashed line.

A, B, C

A, B, C

εp

(ω1,εp)

(a)

A, B, C

A, B, C

(ω2,εp)

(b)

Fig. 3. 3(a) shows the update instantiation for an occurrence of the action distract(A, C)
in σ0 , while Fig. 3(b) shows the epistemic configuration of the successor state resulting
from that action occurrence.

From Epistemic Configurations to States The epistemic update only describes how an agent reasons about how his actions are perceived by his fellows.
In order to obtain the full successor state, he must then reason about how his
actions may actually play out. This is accomplished by abstracting away the
presence of other agents, turning the problem into one concerning the effects of
an action in a single-agent domain. This is done by applying what we term an
ontic update operation to the epistemic configuration. Prior to defining the ontic
update, we must first describe how to relate an epistemic configuration to the
framework for reasoning about the effects of an action from the perspective of
AL.
Definition 15 (AL(σ, ω)). Let ∆ be an action description of mAL and σ be
a state of the transition diagram defined by ∆. Each possible world, ω of σ
corresponds to a complete consistent set of fluent literals, AL(σ, ω), defined as
follows:
{f | σ.ω.π(f ) = >} ∪ {¬f | σ.ω.π(f ) = ⊥}
Intuitively, an epistemic configuration describes the basic structure of the
successor state. Each situation s = (ω, ε) in an epistemic configuration may be
read as “scenario ε transpires in the possible world ω”, and corresponds to possibly
multiple possible worlds in the successor state. The possible worlds of the sucessor
state within the multi-agent transition diagram are obtained by applying the
McCain-Turner equation [15] to the possible worlds defined by AL(σ, ω).

Definition 16 (Scenario Expansion). Let σ be a state of the transition diagram defined by ∆, υ be an update instantiation corresponding to the occurrence
of an action, a in σ, EC be an epistemic configuration defined by Eu(σ, υ), and
s = (ω, ε) be a situation of EC . The expansion of the situation s consistent with
the state, σ, (denoted by C(σ, s)), is defined as follows:
– if ε = εi , then C(σ, s) = {AL(σ, ω)}, otherwise
– C(σ, s) = {τ (s) | τ (s) = Cn∆ (E(AL(σ, ω), a) ∪ (AL(σ, ω) ∩ τ (s)))}
The expansion of the entire epistemic configuration, C(σ, EC ), is defined in a
straightforward fashion as well:
[
C(σ, EC ) =
C(σ, s) for each s ∈ EC .S
Having defined this basic framework, we may now define the ontic update
operation.
Definition 17 (Ontic Update). Let ∆ be an action description of mAL, σ be
a state of the transition diagram defined by ∆, and EC = (E, ω) be an epistemic
configuration. Ou∆ (σ, EC ) defines a set of Kripke worlds (M 0 , RW ) where:
–
–
–
–
–

M 0 .Ω is the set of new symbols of the form ωτi (s) for each τi (s) ∈ C(σ, EC )
M 0 .π(ωτi (s) )(f ) = > if f ∈ τi (s)
M 0 .π(ωτi (s) )(f ) = ⊥ if ¬f ∈ τi (s)
M 0 .Rα = {(ωτi (s1 ) , ωτj (s2 ) ) | ωτi (s1 ) , ωτj (s2 ) ∈ M 0 .Ω, and (s1 , s2 ) ∈ EC .Rα }
RW = {ωτi (s) | τi (s) ∈ C(σ, ω)}

Example 6 (Applying the Ontic Update). Let EC 0 denote the epistemic configuration from Ex. 5. Application of the ontic update operation, Ou∆ (σ0 , EC 0 ),
gives us the successor state, σ1 = (M1 , ω3 ), depicted in Fig. 4. The Kripke

A, B, C

A, B, C

ω3

A, B, C

ω4

Fig. 4. Successor state, σ1 , resulting from the application of Ou∆ (σ0 , EC 0 ).

structure, M1 , shown in Fig. 4 consists of two possible worlds, ω3 = ωτ ((ω1 ,εp )) ,
and ω4 = ωτ ((ω2 ,εp )) where:
– M1 .π(ω3 ) = {heads, locked, ¬open(l1 ), ¬open(l2 ), attentive(A),
attentive(B), ¬attentive(C)}
– M1 .π(ω4 ) = {¬heads, locked, ¬open(l1 ), ¬open(l2 ), attentive(A),
attentive(B), ¬attentive(C)}

The Transition Function As was mentioned previously, the transition function
is based on the following intuition: an agent first reasons about how his action is
perceived, and then reasons about how it may actually play out. This intuition
is realized in the definition of our transition function, Φ∆ (σ, a).
Definition 18. Let ∆ be an action description of mAL, σ be a state of the
transition diagram defined by ∆, and a be an action. The successor state(s)
obtained by performing the action a in the state σ are defined as follows:


Ou∆ (σ, Eu(σ, υo (σ, a)) ontic action
Φ∆ (σ, a) = Ou∆ (σ, Eu(σ, υs (σ, a)) sensing action


Ou∆ (σ, Eu(σ, υc (σ, a)) otherwise
2.3

Properties of the Language

The syntax and semantics of mAL may be of interest in and of themselves, but
of particular interest is the fact that mAL satisfies certain useful properties
- namely that it correctly captures certain intuitions concerning the effects of
various types of actions. Space constraints preclude us from including the proofs
of the subsequent theorems, which we leave to a future journal paper (the paper
is currently in development and will cover mAL in greater detail, including
application of the languages towards modeling collaboration amongst agents for
both ontic and epistemic actions).
Theorem 1. Let ∆ be an action description of mAL; σ = (Mσ , ωσ ) be a state
of the transition diagram defined by ∆; a be an ontic action; and σ 0 = (Mτ , ωτ ) ∈
Φ∆ (σ, a). It holds that:
1. for every agent α ∈ f (σ, a) and dynamic causal law [a causes λ if φ] in ∆, if
(Mσ , ωσ ) |= Bα φ then (Mτ , ωτ ) |= Bα λ
2. for every agent α ∈ f (σ, a) and state constraint [λ if φ] in ∆, (Mσ , ωσ ) |=
Bα (φ → λ)
3. for each agent α ∈ o(σ, a) and literal, λ, (Mτ , ωτ ) |= Bα λ if and only if
(Mσ , ωσ ) |= Bα λ
Theorem 2. Let ∆ be an action description of mAL; σ = (Mσ , ωσ ) be a state
of the transition diagram defined by ∆; a be a sensing action described by the
axiom [a determines f ] in ∆; and σ 0 = (Mτ , ωτ ) ∈ Φ∆ (σ, a). It holds that:
1. (Mτ , ωτ ) |= Cf (σ,a) λ if and only if (Mσ , ωσ ) |= λ where λ ∈ {f, ¬f }
2. (Mτ , ωτ ) |= Cp(σ,a) (Cf (σ,a) f ∨ Cf (σ,a) ¬f )
3. for each agent α ∈ o(σ, a) and literal, λ, (Mτ , ωτ ) |= Bα λ if and only if
(Mσ , ωσ ) |= Bα λ
Theorem 3. Let ∆ be an action description of mAL; σ = (Mσ , ωσ ) be a state
of the transition diagram defined by ∆; a be a communication action described
by the axiom [a communicates ϕ] in ∆; and σ 0 = (Mτ , ωτ ) ∈ Φ∆ (σ, a). It holds
that:

1. (Mτ , ωτ ) |= Cf (σ,a) ϕ
2. (Mτ , ωτ ) |= Cp(σ,a) (Cf (σ,a) ϕ ∨ Cf (σ,a) ¬ϕ)
3. for each agent α ∈ o(σ, a) and literal, λ, (Mτ , ωτ ) |= Bα λ if and only if
(Mσ , ωσ ) |= Bα λ

3

Temporal Projection in mAL

Recall that in Ex. 1 we presented the following
sequence of actions by which A might achieve his
goal: A distracts C, causing him to look away;
ω5
ω6
A, B
once this is done, he then flips open both latches
C
C
on the briefcase, thereby unlocking it; and finally
C
C
A peeks inside. Space considerations preclude us
from examining the entire trajectory, and conseω7
ω8
A, B, C
quently we will only show in detail how to obtain the successor state resulting from A flipping
A, B, C
A, B, C
open the second latch (represented by the action
Fig. 5: Successor state, σ2 , re- f lip(A, l2 )).
sulting from the action sequence Let σ2 , shown in Fig. 5, be the state of the trandistract(A, C), f lip(A, l1 ).
sition diagram resulting from the sequence of actions: distract(A, C), f lip(A, l1 ).
σ2 consists of four possible worlds4 , ω5 , ω6 , ω7 , and ω8 where:
A, B

A, B

– M2 .π(ω5 ) = {heads, locked, open(l1 ), ¬open(l2 ), attentive(A),
attentive(B), ¬attentive(C)}
– M2 .π(ω6 ) = {¬heads, locked, open(l1 ), ¬open(l2 ), attentive(A),
attentive(B), ¬attentive(C)}
– M2 .π(ω7 ) = {heads, locked, ¬open(l1 ), ¬open(l2 ), attentive(A),
attentive(B), ¬attentive(C)}
– M2 .π(ω8 ) = {¬heads, locked, ¬open(l1 ), ¬open(l2 ), attentive(A),
attentive(B), ¬attentive(C)}
Like its predecessor, f lip(A, l2 ), is an ontic action, affecting only the values of the fluents of our possible worlds. Consequently, our intuition informs
us that the structure of the successor state should essentially be unchanged.
σ2 and observation axioms (14) and (14) give f (σ2 , f lip(A, l2 )) = {A, B} and
o(σ2 , f lip(A, l2 )) = {C} as the agents’ frames of reference. Being an ontic action,
the epistemic configuration, EC 2 , of the successor state resulting from f lip(A, l2 )
is given by Eu(σ2 , υo (σ2 , f lip(A, l2 ))) and is shown in Fig. 6(a). As we can see,
EC 2 is structurally similar to σ2 , confirming our aforementioned intuition.
According to the dynamic causal law (11), f lip(A, l2 ) causes open(l2 ). Furthermore, the state constraint (13), informs us that as a consequence of both
l1 and l2 being open, the briefcase itself should become unlocked (i.e., ¬locked
must now be true). The expansions of the scenarios in EC 2 , that are consistent
with σ2 :
4

The labels of the possible worlds have been abbreviated for legibility purposes.

– C(σ2 , (ω5 , εp )) = {{heads, ¬locked, open(l1 ), open(l2 ), attentive(A),
attentive(B), ¬attentive(C)}}
– C(σ2 , (ω6 , εp )) = {{¬heads, ¬locked, open(l1 ), open(l2 ), attentive(A),
attentive(B), ¬attentive(C)}}
– C(σ2 , (ω7 , εi )) = {{heads, locked, ¬open(l1 ), ¬open(l2 ), attentive(A),
attentive(B), ¬attentive(C)}}
– C(σ2 , (ω8 , εi )) = {{¬heads, locked, ¬open(l1 ), ¬open(l2 ), attentive(A),
attentive(B), ¬attentive(C)}}
confirm our intuition.
Let ω9 = ωτ ((ω5 ,εp )) , ω10 = ωτ ((ω6 ,εp )) , ω11 = ωτ ((ω7 ,εi )) , and ω12 = ωτ ((ω8 ,εi )) .
Application of the ontic update operation to σ2 and EC 2 , Ou∆ (σ2 , EC 2 ), yields
the successor state, σ3 , shown in Fig. 6(b). Careful examination of σ3 shows
that it entails a number of modal formulae, among which is C{A,B} ¬locked –
indicating that it is a commonly held belief amongst A and B that the briefcase
is unlocked. In addition, σ3 entails C{A,B} ¬BC BA ¬locked, as well as other
formulae illustrating that C is oblivious of all of the events that transpired since
he was distracted, and that this is a commonly held belief amongst A and B.

A, B

(ω5,εp)

A, B

A, B

(ω6,εp)

ω9

C

C

(ω8,εi)

ω11

A, B, C

A, B, C

C

C
C

(ω7,εi)

A, B

A, B, C

A, B, C

A, B

C

C

(a)

ω10

A, B

C

A, B, C

ω12

A, B, C

(b)

Fig. 6. 6(a) shows the epistemic configuration, EC 2 , resulting from an occurrence of
f lip(A, l2 ) in σ2 , while 6(b) shows the resulting successor state, σ3 .

4

Conclusions and Future Work

In this paper we presented a new multi-agent action language mAL, which
extends the language of mA+ [4] with state constraints from the language AL
[9]. The language’s application was presented in the context of representing and
performing temporal projection in the context of a multi-agent variant of the
Lin’s Briefcase Domain [13], which heretofore could not be represented in either
mA+ or the update model approaches of [2] and [6]. Future work includes a

thorough analysis of the language’s theoretical properties and formulation of
the planning and diagnosis problems within a multi-agent context. Additional
extensions to the language such as non-deterministic sensing actions, and false
communication are under consideration as well.

References
1. Balduccini, M., Gelfond, M.: The AAA Architecture: An Overview. In: AAAI Spring
Symposium 2008 on Architectures for Intelligent Theory-Based Agents (2008)
2. Baltag, A., Moss, L.S.: Logics for Epistemic Programs. Synthese 139(2), 165–224
(2004)
3. Baral, C.: Reasoning About Actions: Non-deterministic Effects, Constraints, and
Qualification. In: Proceedings of the 14th International Joint Conferences on Artificial Intelligence 95. pp. 2017–2023. IJCAI ’95, Morgan Kaufmann (1995)
4. Baral, C., Gelfond, G., Son, T.C., Pontelli, E.: An Action Language for Reasoning
about Beliefs in Multi-Agent Domains. In: Proceedings of the 14th International
Workshop on Non-Monotonic Reasoning (2012)
5. Baral, C., Gelfond, M.: Reasoning about effects of concurrent actions. Journal of
Logic Programming 31, 85–117 (1997)
6. van Benthem, J., van Eijck, J., Kooi, B.: Logics of communication and change.
Information and Computation 204(11), 1620–1662 (2006)
7. van Ditmarsch, H., van der Hoek, W., Kooi, B.: Dynamic Epistemic Logic. Springer
(2008)
8. Fagin, R., Halpern, J.Y., Moses, Y., Vardi, M.Y.: Reasoning About Knowledge.
MIT Press (1995)
9. Gelfond, M.: Handbook of Knowledge Representation – Chapter 7: Answer Sets.
Elsevier (2007)
10. Gelfond, M., Lifschitz, V.: Representing Action and Change by Logic Programs.
Journal of Logic Programming 17, 301–322 (1993)
11. Gelfond, M., Lifschitz, V.: Action Languages. Electronic Transactions on AI 3
(1998)
12. Lifschitz, V. (ed.): Formalizing Common Sense – Papers by John McCarthy. Ablex
Publishing Corporation (1990)
13. Lin, F.: Embracing Causality in Specifying the Indirect Effects of Actions. In:
Proceedings of the 14th International Joint Conferences on Artificial Intelligence.
IJCAI ’95, Morgan Kaufmann (1995)
14. Lin, F., Shoham, Y.: Provably correct theories of action. Journal of the ACM 42(2),
293–320 (1995)
15. McCain, N., Turner, H.: A Causal Theory of Ramifications and Qualifications. In:
Proceedings of the 14th International Joint Conferences on Artificial Intelligence.
IJCAI ’95, Morgan Kaufmann (1995)
16. McCarthy, J.: Programs with common sense. In: Semantic Information Processing.
pp. 403–418. MIT Press (1959)
17. McCarthy, J.: Mathematical Logic in Artificial Intelligence. Daedalus 117(1), 297–
311 (1988)

Learning to Automatically Solve Logic Grid Puzzles
Arindam Mitra
SCIDSE
Arizona State University
amitra7@asu.edu

Abstract
Logic grid puzzle is a genre of logic puzzles in which we are given (in a natural
language) a scenario, the object to be deduced and certain clues. The reader has
to figure out the solution using the clues
provided and some generic domain constraints. In this paper, we present a system, L OGICIA, that takes a logic grid puzzle and the set of elements in the puzzle and tries to solve it by translating it
to the knowledge representation and reasoning language of Answer Set Programming (ASP) and then using an ASP solver.
The translation to ASP involves extraction of entities and their relations from
the clues. For that we use a novel learning based approach which uses varied supervision, including the entities present
in a clue and the expected representation of a clue in ASP. Our system, L O GICIA , learns to automatically translate a
clue with 81.11% accuracy and is able to
solve 71% of the problems of a corpus.
This is the first learning system that can
solve logic grid puzzles described in natural language in a fully automated manner.
The code and the data will be made publicly available at http://bioai.lab.
asu.edu/logicgridpuzzles.

1

Introduction

Understanding natural language to solve problems
be it algebraic word problems (Kushman et al.,
2014; Hosseini et al., 2014) or questions from biology texts (Berant et al., 2014; Kim et al., 2011),
has attracted a lot of research interest over the past
few decades. For NLP, these problems are of particular interest as they are concise, yet rich in information. In this paper, we attempt to solve another problem of this kind, known as Logic Grid

Chitta Baral
SCIDSE
Arizona State University
chitta@asu.edu

Puzzle. Problem.1 shows an example of the same.
Puzzle problems in the same spirit as the previously mentioned science problems, do not restrict
the vocabulary; they use everyday language and
have diverse background stories. The puzzle problems, however, are unique in their requirement of
high precision understanding of the text. For a
puzzle problem, the solution is never in the text
and requires involved reasoning. Moreover, one
needs to correctly understand each of the given
clues to successfully solve a problem. Another interesting property is that only a small core of the
world knowledge, noticeably spatial, temporal and
knowledge related to numbers, is crucial to solve
these problems.
P ROBLEM .1 A LOGIC GRID PUZZLE
Waterford Spa had a full appointment calendar booked today. Help
Janice figure out the schedule by
matching each masseuse to her
client, and determine the total price
for each.
1. Hannah paid more than Teri’s client.
2. Freda paid 20 dollars more than
Lynda’s client.
3. Hannah paid 10 dollars less than
Nancy’s client.
4. Nancy’s client, Hannah and Ginger
were all different clients.
5. Hannah was either the person who
paid $180 or Lynda’s client.
Clients: Aimee, Ginger, Freda, Hannah.
Prices: $150, $160, $170, $180.
Masseuses: Lynda, Nancy, Tery, Whitney.
A logic grid puzzle contains a set of categories
and an equal number of elements in each category.

And the goal is to find out which elements are
linked together based on a series of given clues.
Each element is used only once. Each puzzle has
a unique solution and can be solved using logical
reasoning. A logic grid puzzle is called a (n, m)puzzle if it contains n categories and each category
has m elements. For the example in Problem.1,
there are three categories, namely clients, prices,
masseuses and each category has four elements
which are shown in the respective columns. A total of five clues are given in free text and the goal
is to find the members of the four tuples, where
each tuple shall contain exactly one element from
each category such that all the members in a tuple
are linked together.
To solve such a puzzle problem, it is crucial to
understand the clues (for example, “Hannah paid
more than Teri’s client.”). Each clue talks about
a set of entities (for example, “Hannah”, “client”,
“Terry”) and their relations (“a greater-than relation between Hannah and the client of Terry on
the basis of payment”). Our system, L OGICIA,
learns to discover these entities and the underlying semantics of the relations that exist between
them. Once the relations are discovered, a pair
of Answer Set Programming (ASP) (Baral, 2003)
rules are created. The reasoning module takes
these ASP rules as input and finds a group configuration that satisfies all the clues. L OGICIA has
“knowledge” about a fixed set of predicates which
models different relations that hold between entities in a puzzle world. Clues in the puzzle text
that are converted into ASP rules, use these predicates as building blocks. In this research, our goal
is to build a system which can automatically do
this conversion and then reason over it to find the
solution. The set of predicates that the reasoning
model is aware of is not sufficient to represent all
logic grid puzzles. The family of logic grid puzzles is broad and contains variety of clues. Our
future work involves dealing with such a diverse
set of relations. In this work we assume that the
relations in Table 1 are sufficient to represent the
clues. Following are some examples of clues that
cannot be modeled using the predicates in Table 1.
• Esther’s brother’s seat is at one end of the
block of seven.
• The writer of Lifetime Ambition has a first
name with more letters than that of the tennis star.

• Edward was two places behind Salim in one
of the lines, both being in odd-numbered positions.
• Performers who finished in the top three
places, in no particular order, are Tanya , the
person who performed the fox trot, and the
one who performed the waltz.
The rest of the paper is organized as follows:
in section 2, we describe the representation of a
puzzle problem in ASP and delineate how it helps
in reasoning; in section 3, we present our novel
method for learning to automatically translate a
logic problem described in natural language to its
ASP counterpart. In section 4, we describe the related works. In section 5, we discuss the detailed
experimental evaluation of our system. Finally,
section 6 concludes our paper.

2

Puzzle Representation

Answer Set Programming (ASP) (Baral, 2003;
Lifschitz, 1999; Gelfond and Lifschitz, 1991) has
been used to represent a puzzle and reason over
it. This choice is facilitated by the two important
reasons: 1) non-monotonic reasoning may occur
in a puzzle (Nagy and Allwein, 2004) and 2) ASP
constructs greatly simplify the reasoning module,
as we will see in this section. We now briefly describe a part of ASP. Our discussion is informal.
For a detailed account of the language, readers are
referred to (Baral, 2003).
2.1 Answer Set Programming
An answer set program is a collection of rules of
the form,
L0 | ... | Lk :- Lk+1 , ..., Lm , not Lm+1 , ..., not Ln .
where each of the Li ’s is a literal in the sense
of a classical logic. Intuitively, the above rule
means that if Lk+1 , ..., Lm are to be true and if
Lm+1 , ..., Ln can be safely assumed to be false
then at least one of L0 , ..., Lk must be true. The
left-hand side of an ASP rule is called the head
and the right-hand side is called the body. A rule
with no head is often referred to as a constraint.
A rule with empty body is referred to as a f act and
written as,
L0 | L1 | ... | Lk .

Example

2.3

fly(X) :- bird(X), not ab(X).
The above program represents the knowledge that
“Most birds fly”. If we add the following rule
(f act) to the program,
bird(penguin).
the answer set of the program will contain the
belief that penguins can fly, {bird(penguin),
f ly(penguin)}. However, adding one more fact,
‘ab(penguin).’, to convey that the penguin is an
abnormal bird, will change the belief that the penguin can fly and correspondingly the answer set,
{bird(penguin), ab(penguin)}, will not contain
the fact, f ly(penguin).

Solution to a logic grid puzzle is a set of tuples containing related elements. The tuple/3
predicate captures this tuple membership information of the elements. For example, the fact,
tuple(2, 1, aimee), states that the element aimee
from the category with index 1 is in the tuple 2.
The rel/m predicate captures all the elements in a
tuple for a (m, n)-puzzle and is defined using the
tuple/3 predicate.

Choice Rule
m {p(X) : q(X)} n : −L1 , ..., Lk , ..., not Ln .
Rules of this type allow inclusion in the program’s
answer sets of arbitrary collections S of atoms of
the form p(t) such that, m ≤| S |≤ n and if p(t) ∈
S then q(t) belongs to the corresponding answer
set.
2.2

Representing Puzzle Entities

A (m, n)-puzzle problem contains m categories
and n elements in each category. The term ‘puzzle entity’ is used to refer to any of them. Each
category is assigned an unique index, denoted by
the predicate cindex/1 (the number after the ‘/’
denotes the arity of the predicate). The predicate
etype/2 captures this association. Each element
is represented, by the element/2 predicate which
connects a category index to its element. The predicate eindex/1, denotes the tuple indices. The following blocks of code shows the representation of
the entities for the puzzle in Problem.1.

2.4

element(1,aimee;;1,ginger).
element(1,freda;;1,hannah).
element(2,150;;2,160).
element(2,170;;2,180).
element(3,lynda;;3,nancy).
element(3,teri;;3,whitney).

Domain Constraints

In the proposed approach, the logic grid puzzle
problem is solved as a constraint satisfaction problem. Given a puzzle problem the goal is to enumerate over all possible configurations of tuple/3,
and select the one which does not violate the constraints specified in the clues. However, 1) each
tuple in a logic grid puzzle will contain exactly
one element from each category and 2) an element
will belong to exactly one tuple. These constraints
come from the specification of a puzzle problem
and will hold irrespective of the problem instance.
Following blocks of code show an elegant representation of these domain constraints in ASP along
with the enumeration.
%enumerate over the tuple
%assignments with constraint#1
1 {
tuple(G,Cat,Elem):
element(Cat,Elem)
} 1 :- cindex(Cat),
eindex(G).
%domain constraint#2
:-tuple(G1,Cat,Elem),
tuple(G2,Cat,Elem),
G1!=G2.

cindex(1...3).
eindex(1...4).
etype(1,clients).
etype(2,prices).
etype(3,masseuses).

Representing Solution

2.5

Representing clues

Each clue describes some entities and the relations
that hold between them. In its simplest form, the
relations will suggest if the entities are linked together or not. However, the underlying semantics
of such relations can be deep such as the one in
clue 5 of Problem.1. There are different ways to
express the same relation that holds between entities. For example, in Problem.1, the possessive
relation has been used to express the linking between clients and masseuses; and the word paid

expresses the linking between the clients and the
prices. Depending on the puzzles the phrases that
are used to express the relations will vary and it
is crucial to identify their underlying semantics to
solve the problems in systematic way.
In the current version, the reasoning module has
knowledge of a selected set of relations and the
translation module tries to represent the clue as a
conjunction of these relations. All these relations
and their underlying meanings are described in table 1. In this subsection, we describe the representation of a clue in terms of these relations in ASP
and show how it is used by the reasoning module.
In the next section, we present our approach to automate this translation.

clue4 :diffTuple(hannah,1,ginger,1),
diffTuple(hannah,1,X,1),
diffTuple(X,1,ginger,1),
sameTuple(X,1,nancy,3).
:- not clue4.

[3] Hannah was either the person who paid $180
or Lynda’s client.
clue5 :eitherOr(hannah,1,X,1,Y,1),
sameTuple(X,1,180,2).
sameTuple(Y,1,lynda,3).
:- clue5.

Let us consider the clues and their representation from Problem.1:
[1] Hannah paid more than Teri’s client.

clue1 :greaterThan(hannah,1,X,1,2),
sameTuple(X,1,teri,3).
:- not clue1.

The first rule clue1 evaluates to true (will be in the
answer set) if the element from category 1 with
value hannah is linked to some element from category 2 which has a higher value than the element
from its own category which is linked to an element from category 1 which is linked to teri from
category 3. Since the desired solution must satisfy the relations described in the clue, the second
ASP rule is added. A rule of this form that does
not have a head is known as a constraint and the
program must satisfy it to have an answer set. As
the reasoning module enumerates over all possible configurations, in some cases the clue1 will
not hold and subsequently those branches will be
pruned. Similar constraints will be added for all
clues. In the below, we show some more examples. A configuration which satisfies all the clue
constraints and the domain constraints described
in the previous section, will be accepted as the solution to the puzzle.
[2] Nancy’s client, Hannah and Ginger were all
different clients.

3

Learning Translation

To automate the translation of a clue to the pair
of ASP rules, the translation module needs to
identify the entities that are present in the clue,
their category and their value; and the underlying interpretations of all the relations that hold
between them. Once all the relation instances
{R1 (arg1 , ..., argp1 ),..., Rq (arg1 , ..., argpq )} , in
the clue are identified, the ASP representation of
the clue is generated in the following way:
clue : −R1 (arg1 , ..., argp1 ), ..., Rq (arg1 , ..., argpq ).
The entity classification problem for logic grid
puzzles poses several challenges. First, the existence of wide variety in the set of entities. Entities can be names of objects, time related to some
event, numbers, dates, currency, some form of ID
etc. And it is not necessary that the entities in puzzles are nouns. It can be verbs, adjectives etc. Second and of paramount important, the “category”
of a puzzle “element” is specific to a puzzle problem. Same element may have different category
in different problems. Also, a constituent in a
clue which refers to an entity in a particular problem may not refer to an entity in another problem.
We formalize this problem in this section and propose one approach to solve the problem. Next,
we discuss the method that is used to extract relations from clues. To the best of our knowledge,
this type of entity classification problem has never
been studied before.

Relation
sameTuple(E1, C1, E2, C2)

referrent(E1, C1, E2, C2)
posDiff (E1, C1, E2, C2, N1, NC1)

greaterThan(E1, C1, E2, C2, NC1)

members(E1, C1, E2, C2,..., EN, CN)
eitherOr(E1, C1, E2, C2,..., EN, CN)

referrent22(E1, C1, E2, C2, E3, C3, E4, C4)

Interpretation
States that two elements, (C1,E1) and (C2,E2) are in
the same tuple. The dictionary also contains the negation of it, diffTuple(E1, C1, E2, C2).
States that the elements are identical.
If (C1,E1) is related to (NC1,X1) and (E2,C2) is related to (NC1,X2), then difference(X1,X2)=N1. Similarly the dictionary contains the predicate negDiff.
Similar to posDiff however the difference(X1,X2) >
0. The dictionary also contains its opposite predicate
lessThan.
All the elements are distinct and do not share a tuple.
The first element is related to one of the last N − 1
elements. The last N − 1 elements are assumed to be
different unless contradicts with other beliefs.
The first two elements are different and referring to
the last two elements.

Table 1: Describes the various relations that are part of the reasoning module.
3.1

Entity Classification

The entity classification problem is defined as follows:
Problem description Given m categories C1 , ...,
Cm and a text T , each category Ci , 1 ≤ i ≤ m,
contains a collection of elements Ei and an optional textual description di . The goal is to find the
class information of all the constituents in the text
T . Each category contributes two classes, where
one of them represents the category itself and the
other represents an instance of that category. Also,
a constituent may not refer to any category or any
instance of it, in that case the class of that constituent is null. So, there are a total 2m+1 classes
and a constituent will take one value from them.
Example In the puzzle of Problem.1, there are
3 categories with, C1 = {Aimee, Freda, Ginger,
Hannah}, C2 = {$150, $160, $170, $180}, C3
= {Lynda, Nancy, Terry, Whiteney} and d1 =
“clients”, d2 = “prices”, d3 = “masseuses”. The
text T , is the concatenation of all clues. In the
last clue, there are total 5 entities, namely “Hannah”, “person”, “$180”, “Lydia”,“client” and the
corresponding classes are “Instance of C1 ”, “Instance of C1 ”, “Instance of C2 ”, “Instance of C3 ”
and “Instance of C1 ” respectively. The remaining
constituents in that clue have the class value null.
The constituent “clients” in the fourth clue refers
to the category C1 .

Our approach We model the Entity Classification problem as a decoding query on Pairwise
Markov Network (Koller and Friedman, 2009;
Kindermann et al., 1980; Zhang et al., 2001). A
pairwise Markov network over a graph H, is associated with a set of node potentials {φ(Xi ) : i =
1, ..., n} and a set of edge potentials {φ(Xi , Xj ) :
(Xi , Xj ) ∈ H}. Each node Xi ∈ H, represents
a random variable. Here, each Xi can take value
from the set {1...2m + 1}, denoting the class of
the corresponding constituent in the text T .
In our implementation, the node potential captures the chances of that node to be classified as
one of the possible categories without being affected by the given text T . And the edge potentials captures hints from the context in T for classification. After constructing the pairwise Markov
network, a decoding query is issued to obtain the
configuration that maximizes the joint probability distribution of the pairwise Markov network in
consideration. The proposed approach is inspired
by the following two observations: 1) To find the
class of a constituent one needs some background
knowledge; 2) However, background knowledge
is not sufficient on its own, one also needs to understand the text to properly identify the class of
each constituent. For example, let’s consider the
word “person” in clue 5 of Problem.1. Just skimming through the categories, one can discover that
the word “person” is very unlikely to be a instance
of the category “prices”, which is from her knowledge about those constituents. However a proper

disambiguation may face an issue here as there are
two different categories of human beings. To properly classify the word “person” it is necessary to
go through the text.
The following paragraphs describe the construction of the grah H, and the algorithm that is
used in the computation of associated set of node
potentials and edge potentials.
Construction of the graph While constructing
the graph we assign a label, L, to each edge in
H which will be used in the edge potential computation. Let us say DG denotes the dependency
graph of the text T obtained from the Stanford dependency parser (Chen and Manning, 2014) and
dep(v1 , v2 ) denotes the grammatical relation between (v1 , v2 ) ∈ DG . Then the graph, H, is constructed as follows:

description di using the HSO WordNet similarity algorithm (Hirst and St-Onge, 1998). The
similarity score, sim(w,c), is chosen to be the
maximum of them.
•Class c is denoting a category Ci : sim(w,c)
is assigned the value of HSO Similarity between
the textual description and di .
•Class c is null : In this case similarity is calculated using the following formula:
sim(w, null) = M AXHSO − max sim(w, c)
c6=null

where M AXHSO denotes the maximum similarity score returned by HSO algorithm, which is 16.

1. Create a node in H for each constituent wj in
T if wj ∈ DG .

Node potential for each node Xi ∈ H, corresponding to the constituent wj , are then calculated
by,
φ(Xi = c) = 1 + sim(wj , c), ∀c

2. Add an edge (Xi , Xj ) to H if the corresponding edge (wp , wq ) ∈ DG . L(Xi , Xj ) :=
dep(wp , wq ).

Determining Edge potentials For each edge in
the graph H, the edge potential, φ(Xi , Xj ) is calculated using the following formula,

3. Add an edge between a pair of nodes
(Xi , Xj ) if the corresponding words are synonyms. L(Xi , Xj ) := synonymy.

φ(Xi = c1 , Xj = c2 ) =
(
P (Xi = Xj |L(Xi , Xj )), if c1 = c2
1+
P (Xi 6= Xj |L(Xi , Xj )), otherwise

4. Create a node for each element and category
specified in the puzzle and add an edge from
them to others if the corresponding string descriptions are ‘same’. In this case, the edges
are labeled as exact match.
5. If (Xi , Xj ) ∈ H and L(Xi , Xj ) =
exact match and both of them are referring to a verb, then add more edges (Xi0 , Xj0 )
to H with label spatial symmetry, where
L(Xi , Xi0 ) = L(Xj , Xj0 ).
Determining Node potentials For each element
in the m category a set of naive regular-expression
based taggers are used to detect it’s type (For example, “am-pm time”). Each element type maps
to a WordNet (Miller, 1995) representative (For
example, “time unit#n”). For each constituent w
a similarity score, sim(w,c), is calculated to each
class c ∈ {1...2m + 1}, in the following way:
•Class c is denoting instance of some category Ci
Similarity scores are computed between the textual description of the constituent to both the
WordNet representative of Ei and the textual

In the training phase, each entity in a clue is
tagged with its respective class. The probability
values are then calculated from the training dataset
using simple count.
3.2

Learning To Extract Relations

The goal here is to identify all the relations
R(arg1 , ..., argp ) that are present in a clue, where
each relation belongs to the logical vocabulary
described in Table 1 . This problem is known
as Complex relation extraction (McDonald et al.,
2005; Bach and Badaskar, 2007; Fundel et al.,
2007; Zhou et al., 2014). The common approach
for solving the Complex relation extraction problem is to first find the relation between each pair
of entities and then discover the complex relations
from binary ones using the definition of each relation.
Figure 1 depicts the scenario. The goal is to
identify the relation possDif f (E1, E2, E3),
where E1, E2, E3 are constituents having a nonnull class value. However instead of identifying
posDif f (E1, E2, E3) directly, first the relation

necting E1 and E2 in the dependency graph of the
clue.
Feature Set
Class of E1 and E2
All the grammatical relations between the
words in path(E1 , E2 )
All the adjectives and adverbs in path(E1 , E2 ).
POS tags of all the words in path(E1 , E2 )
TypeMatched = [[class of E1 = class of E2 ]]
IsE1 Numeric = [[class of E1 is Numeric ]]
IsE2 Numeric = [[class of E2 is Numeric ]]
All the words that appears in the following
grammatical relations advmod, amod, cop,
det with the words in path(E1 , E2 ).
hasNegativeWord = [[ ∃w ∈ path(E1 , E2 ) s.t.
w has a neg relation starting with it.]]

Figure 1: Binary representation of the relation
possDif f
between each pair of entities will be identified.
If the relations {posDif farg1 −arg2 (E1 , E2 ),
posDif farg2 −arg3 (E2 , E3 ), posDif farg1 −arg3
(E1 ,E3 ) } are identified, the extraction module
will infer that posDif f (E1, E2, E3) holds. In
the similar manner, a set of of total 39 binary
relations are created for all the relations described
in Table 1.
In the training phase, all the relations and
their respective arguments in each clue are given.
Using this supervision, we have built a Maximum Entropy based model (Berger et al., 1996;
Della Pietra et al., 1997) to classify the relation
between a pair entities present in a clue. Maximum entropy classifier has been successfully applied in many natural language processing applications (Charniak, 2000; Chieu and Ng, 2002;
Ratnaparkhi and others, 1996) and allows the inclusion of various sources of information without
necessarily assuming any independence between
the features. In this model, the conditional probability distribution is given by:
Q
P (c|d) = P

j=1...K

c0 ∈C

Q

eλi fi (d,c)

j=1...K

eλi fi (d,c0 )

Table 2: Features used in the classification task
The relation between each pair of entities in a
clue is the one which maximizes the conditional
probability in equation (1).
3.2.1 Missing Entity
In the case of comparative relations in Table 1 ,
such as greaterT han, the basis of the comparison can be hidden. For example, in clue 1 of
the example problem, the two entities, “Hannah”
and “client” have been compared on the basis of
“price”, however there is no constituent in the clue
which refers to an element from that category. The
basis of comparison is hidden in this case and is
implied by the word “paid”. In the current implementation, the translation module does not handle
this case. For puzzles that contain only one category consisting of numeric elements, the translation module goes with the obvious choice. This is
part of our future work.

(1)

where the denominator is the normalization
term and the parameter λi correspond to the
weight for the feature fi . Features in Maximum
Entropy model are functions from context and
classes to the set of real numbers. A detailed
description of the model or parameter estimation
method used - Generalized Iterative Scaling, can
be found at (Darroch and Ratcliff, 1972).
Table 2 describes the features that are used in
the classification task. Here, path(E1 , E2 ) denotes all the words that occur in the path(s) con-

4

Related Work

There has been a significant amount of work on the
representation of puzzle problems in a formal language (Gelfond and Kahl, 2014; Baral, 2003; Celik et al., 2009). However, there has not been any
work that can automatically solve a logic grid puzzle. The latest work (Baral and Dzifcak, 2012) on
this problem, assumes that the entities in a clue are
given and the authors manually simplify the sentences for translation. Furthermore their representation of logic grid puzzles does not consider the
category of a variable in the formal representation

i.e. uses element/1 and tuple/2 predicates and
thus cannot solve puzzles containing more than
one numeric categories.
In the same work (Baral and Dzifcak, 2012), the
authors propose to use a semantic parser to do the
translation. This method works well for simple
sentences such as “Donna dale does not have green
fleece” however it faces several challenges while
dealing with real world puzzle sentences. The
difficulty arises due to the restrictions enforced
in the translation models used by the existing semantic parsers. Traditional semantic parsers (Vo
et al., 2015; Zettlemoyer and Collins, 2005) assign meanings to each word in a dictionary and
combine the meaning of the words to characterize the complete sentence. A phrase structure
grammar formalism such as Combinatory Categorial Grammar (Steedman and Baldridge, 2011;
Vo et al., 2015; Zettlemoyer and Collins, 2005),
Context Free Grammar (Aho and Ullman, 1972;
Wong and Mooney, 2006), is normally used to obtain the way words combine with each other. In
the training phase, the semantic parser learns the
meanings of words given a corpus of <sentence,
meaning> pairs and stores them in a dictionary.
During translation, the semantic parser uses those
learned meanings to obtain the meaning of the sentence. Firstly, for the puzzle problems the meaning of the words changes drastically depending on
the puzzle. A word may be an entity in one puzzle, but, in a different problem it might not be an
entity or might belong to a different category altogether. Thus a learned dictionary may not be useful while translating clues in a new puzzle. Secondly, in puzzles relations are normally expressed
by phrases. For example, in the clue “The person who played at Eden Gardens played for India”, the phrases “played at” and “played for” are
used to express two different relations. Thus, using a model that assigns meaning to each word
may not be suitable here. Finally, it is difficult to
identify the participants of a relation with a parse
tree generated following a phrase structure grammar. For example, consider the parse tree of the
clue “The person who trekked for 8 miles started
at Bull Creek”. Even though, the relation “started
at” takes the word ‘person’ and ‘Bull Creek’ as its
input, it receives the entire phrase “the person who
trekked for 8 miles” as its argument along with the
other input ‘Bull Creek’.
The entity classification problem studied in this

Figure 2: Parse tree of an example sentence in
Combinatory categorial grammar
research shares many similarity with Named Entity Recognition (Nadeau and Sekine, 2007; Zhou
and Su, 2002) and the Word Sense disambiguation
(Stevenson and Wilks, 2003; Sanderson, 1994)
task. However, our work has a major difference;
in the entity classification problem, the class of an
entity varies with the problem and does not belong
to a known closed set, whereas for the other two
problems the possible classes are pre-specified.

5

Experimental Evaluation

Dataset To evaluate our method we have built
a dataset of logic grid puzzles along with their
correct solutions. A total of 150 problems are
collected from logic-puzzles.org. Out of
them 100 problems are fully annotated with the
entities and the relations information. The remaining 50 puzzles do not have any annotation except
their solution. The set of annotated puzzles contain a total of 467 clues, 5687 words, 1681 entities
and 862 relations. The set of 50 puzzles contain a
total of 222 clues with 2604 words.
Tasks We evaluate L OGICIA on three tasks: 1)
puzzle solving; 2) entity classification; and 3) relation extraction. We use the percentage of correct
answers as the evaluation metric for all the three
tasks. In case of a logic grid puzzle solving, an
answer is considered correct if it exactly matches
the solution of that puzzle.
Training-Testing Out of the 100 annotated puzzle problems 50 are used as training samples and
remaining 50 puzzles are used in testing. The set
of 50 unannotated puzzles are used solely for the
task of testing puzzle solving.

Entity classification
Total
Correct
Percentage

1766
1502
85.05%

Binary relation classification
with annotation
Yes
No
960
922
854
96.04% 88.95%

Complex relation extraction
with annotation
Yes
No
450
410
365
90.90% 81.11%

Solution
50
37
74%

Table 3: Accuracy on 50 annotated puzzle problems in the Test set.
Results Table 3 & 4 shows the efficacy of our
approach in solving logic grid puzzles with the selected set of relations. L OGICIA is able to classify
the constituents with 85.05% accuracy and is able
to solve 71 problems out of the 100 test puzzles.
It should be noted that puzzle problems requires
precise understanding of the text and to obtain the
correct solution of a puzzle problem all the entities
and their relations in the puzzle need to be identified. Columns 2 and 3 in Table 3 compares the performance on relation extraction when it is used in
conjunction with the entity classification and when
it directly uses the annotated entity.

Total
50

Puzzle Solving
Correct Percentage
34
68%

Table 4: Accuracy on unannotated 50 test puzzle
problems.

Error Analysis The errors in entity classification falls into two major categories. In the first
category, more knowledge of similarity is needed
than what is currently obtained from the WordNet.
Consider for example, the categories are “class
number” and “class size” and the constituent is
“20 students”. Even though the constituent is
closer to “class size”, standard WordNet based
similarity methods are unable to provide such information. In the second category, the WordNet
similarity of the constituent to one of the classes
is quite high due to their position in the WordNet
hierarchy; however with respect to the particular
problem the constituent is not an entity. The relation extraction task performs fairly well, however the binary relation classification task does not
jointly consider the relation between all the entities and because of that if one of the necessary binary relation of a complex relation is misclassified,
the extraction of the entire relation gets affected.

6

Conclusion & Future Work

This paper presents a novel approach to logic grid
puzzle solving. To the best of our knowledge, this
is a novel work with respect to the fact that that it
can automatically solve a given logic grid puzzle.
There are several advantages of our approach.
The inclusion of knowledge in terms of a vocabulary of relations makes it scalable. For puzzles,
which make use of different set of constraints,
such as “Lynda sat on an even numbered position”,
can be easily integrated into the vocabulary and
the system can then be trained to identify those
relations for new puzzles. Also, the proposed approach separates the representation from reasoning. The translation module only identifies the relation and their arguments; it is not aware of the
meaning of those relations. The reasoning module, on the other hand, knows the definition of each
relation and subsequently prunes those possibilities when relations appearing in a clue does not
hold. This separation of representation from reasoning allows the system to deal with the complex
relations that appears in a clue.
There are few practical and theoretical issues
which need to be addressed. One of those is updating the logical vocabulary in a scalable manner.
Logic grid puzzle is a wide family of puzzles and
it will require more knowledge of relations than
what is currently available. Another challenge that
needs to be addressed is the computation of similarity between complex concepts such as “size of
class” and “20 students”. Also, the case of “missing entity” (3.2) needs to be modeled properly.
This work is the first step towards further understanding these important issues.

Acknowledgements
We thank NSF for the DataNet Federation Consortium grant OCI-0940841 and ONR for their grant
N00014-13-1-0334 for partially supporting this research.

References
Alfred V Aho and Jeffrey D Ullman. 1972. The theory of parsing, translation, and compiling. PrenticeHall, Inc.
Nguyen Bach and Sameer Badaskar. 2007. A review
of relation extraction. Literature review for Language and Statistics II.
Chitta Baral and Juraj Dzifcak. 2012. Solving puzzles described in english by automated translation
to answer set programming and learning how to do
that translation. In Principles of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International Conference, KR 2012, Rome,
Italy, June 10-14, 2012.
Chitta Baral. 2003. Knowledge representation, reasoning and declarative problem solving. Cambridge
university press.
Jonathan Berant, Vivek Srikumar, Pei-Chun Chen,
Brad Huang, Christopher D Manning, Abby Vander Linden, Brittany Harding, and Peter Clark.
2014. Modeling biological processes for reading
comprehension. In Proc. EMNLP.
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational linguistics, 22(1):39–71.
Mehmet Celik, Halit Erdogan, Fırat Tahaoglu, Tansel
Uras, and Esra Erdem. 2009. Comparing asp and
cp on four grid puzzles. In Proceedings of the Sixteenth RCRA International Workshop on Experimental Evaluation of Algorithms for Solving Problems
with Combinatorial Explosion (RCRA09). CEUR
Workshop Proceedings.
Eugene Charniak.
2000.
A maximum-entropyinspired parser. In Proceedings of the 1st North
American chapter of the Association for Computational Linguistics conference, pages 132–139. Association for Computational Linguistics.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750.
Hai Leong Chieu and Hwee Tou Ng. 2002. Named entity recognition: a maximum entropy approach using
global information. In Proceedings of the 19th international conference on Computational linguisticsVolume 1, pages 1–7. Association for Computational
Linguistics.
John N Darroch and Douglas Ratcliff. 1972. Generalized iterative scaling for log-linear models. The
annals of mathematical statistics, pages 1470–1480.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.

Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 19(4):380–393.
Katrin Fundel, Robert Küffner, and Ralf Zimmer.
2007. Relexrelation extraction using dependency
parse trees. Bioinformatics, 23(3):365–371.
Michael Gelfond and Yulia Kahl. 2014. Knowledge representation, reasoning, and the design of
intelligent agents: The answer-set programming approach. Cambridge University Press.
Michael Gelfond and Vladimir Lifschitz. 1991. Classical negation in logic programs and disjunctive
databases. New generation computing, 9(3-4):365–
385.
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detection and correction of malapropisms. WordNet: An
electronic lexical database, 305:305–332.
Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb categorization. In Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun’ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In Proceedings of the BioNLP Shared Task 2011 Workshop,
pages 1–6. Association for Computational Linguistics.
Ross Kindermann, James Laurie Snell, et al. 1980.
Markov random fields and their applications, volume 1. American Mathematical Society Providence,
RI.
Daphne Koller and Nir Friedman. 2009. Probabilistic
graphical models: principles and techniques. MIT
press.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. ACL (1), pages 271–
281.
Vladimir Lifschitz. 1999. Answer set planning. In
Logic Programming and Nonmonotonic Reasoning,
pages 373–374. Springer.
Ryan McDonald, Fernando Pereira, Seth Kulick, Scott
Winters, Yang Jin, and Pete White. 2005. Simple
algorithms for complex relation extraction with applications to biomedical ie. In Proceedings of the
43rd Annual Meeting on Association for Computational Linguistics, pages 491–498. Association for
Computational Linguistics.
George A Miller.
1995.
Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.

David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3–26.
Benedek Nagy and Gerard Allwein. 2004. Diagrams and non-monotonicity in puzzles. In Diagrammatic Representation and Inference, pages 82–
96. Springer.
Adwait Ratnaparkhi et al. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the conference on empirical methods in natural language processing, volume 1, pages 133–142.
Philadelphia, PA.
Mark Sanderson. 1994. Word sense disambiguation
and information retrieval. In Proceedings of the 17th
annual international ACM SIGIR conference on Research and development in information retrieval,
pages 142–151. Springer-Verlag New York, Inc.
Mark Steedman and Jason Baldridge. 2011. Combinatory categorial grammar. Non-Transformational
Syntax: Formal and Explicit Models of Grammar.
Wiley-Blackwell.
Mark Stevenson and Yorick Wilks. 2003. Word sense
disambiguation. The Oxford Handbook of Comp.
Linguistics, pages 249–265.
Nguyen Ha Vo, Arindam Mitra, and Chitta Baral.
2015. The NL2KR platform for building natural
language translation systems. In Proceedings of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint
Conference on Natural Language Processing of the
Asian Federation of Natural Language Processing,
ACL 2015, July 26-31, 2015, Beijing, China, Volume
1: Long Papers, pages 899–908.
Yuk Wah Wong and Raymond J Mooney. 2006. Learning for semantic parsing with statistical machine
translation. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Computational Linguistics, pages 439–446. Association
for Computational Linguistics.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial
Grammars. In UAI, pages 658–666. AUAI Press.
Yongyue Zhang, Michael Brady, and Stephen Smith.
2001. Segmentation of brain mr images through
a hidden markov random field model and the
expectation-maximization algorithm.
Medical
Imaging, IEEE Transactions on, 20(1):45–57.
GuoDong Zhou and Jian Su. 2002. Named entity
recognition using an hmm-based chunk tagger. In
proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 473–480.
Association for Computational Linguistics.

Deyu Zhou, Dayou Zhong, and Yulan He. 2014.
Biomedical relation extraction: From binary to complex. Computational and mathematical methods in
medicine, 2014.

Recognizing Social Constructs from Textual Conversation
Somak Aditya and Chitta Baral and Nguyen H. Vo and Joohyung Lee and Jieping Ye,
Zaw Naung and Barry Lumpkin and Jenny Hastings
Dept. of Computer Science, Arizona State University
Richard Scherl
Dept. of Computer Science,
Monmouth University

Dawn M. Sweet
Dept. of Psychology,
Iowa State University

Abstract

chat corpuses (Shaikh et al., 2010), and developing
a socio-cultural phenomena model from discourse
with a small-scale implementation (Strzalkowski et
al., 2010). Other researchers have focused on automatically annotating social behavior in conversation
using statistical approaches (Mayfield et al., 2013).
The discourse structure of a conversation is modeled
as a Hidden Markov Model in (Stolcke, 2000) to determine dialogue acts such as Statement, Question
and Agreement. In (Prabhakaran et al., 2012) annotated email threads are presented for facilitating
detection of social relations.

In this paper we present our work on recognizing high level social constructs such as
Leadership and Status from textual conversation using an approach that makes use of the
background knowledge about social hierarchy
and integrates statistical methods and symbolic logic based methods. We use a stratified
approach in which we first detect lower level
language constructs such as politeness, command and agreement that help us to infer intermediate constructs such as deference, closeness and authority that are observed between
the parties engaged in conversation. These intermediate constructs in turn are used to determine the social constructs Leadership and
Status. We have implemented this system
successfully in both English and Korean languages and achieved considerable accuracy.

1

Daniela Inclezan
Dept. of Computer Science,
Miami University

Introduction and Related Works

The traditional information extraction paradigm has
seen success in extracting simplistic behaviors or
emotions from text. However, to detect high-level
social constructs such as leadership or status, we require robustly defined notions about language constructs that cannot always be directly inferred from
text. Hence, in this paper we focus on extracting
information from text that requires additional background knowledge and inference. There are a few
works in this direction, such as (Tari et al., 2010),
however our focus in this paper is to extract information pertaining to different social constructs from
textual conversation. The earlier research in analyzing conversations includes developing annotated

Among recent works, (Gilbert, 2012) uses linguistic cues to discover workplace hierarchy from
emails. The use of phrases to detect language use
such as “commands” is motivating. However, due
to the lack of logical explanation and robust definition, the effectiveness of this method decreases in
the semi-formally moderated Wikipedia community,
which has interplay of several different LUs such
as command, politeness and informal language. In
(Danescu-Niculescu-Mizil et al., 2012), the authors
explain how reflection of linguistic styles can shed
light on power differentials; though, a social community like Wikipedia might not always conform to
the lingustic style coordination assumption. For example, two friends who are coordinating on writing
an article may have the same status socially, but difference in their expertise will drive the conversation.
Other works such as (Gupte et al., 2011) have concentrated more on other features of the persons involved in social network, than linguistic cues. Also,
we feel that, the hierarchy depends on the task or
the context. In other words, one person could as-

1293
Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1293–1298,
c
Denver, Colorado, May 31 – June 5, 2015. 
2015
Association for Computational Linguistics

sume different roles in different context. The above
works do not seem to address this. (Prabhakaran et
al., 2012) achieves a commendable accuracy in detecting overt display of “power”. However, by our
definitions, this is a lower level attribute and is similar to authoritative behavior which is a lower level
concept than Leadership or Status. Hence, their results are not directly comparable to ours.
In this paper, we use a mixture of logic-based and
statistical approaches which better encodes the domain knowledge and infers higher-level constructs
from indirect textual cues. The aim of this paper is
to formalize the theory behind our work, highlight
the advantages of integration of statistical and logicbased approaches and present results from an empirical study.

fiers (basically rules) and probabilistic generative models
(Medhat et al., 2014), (Hutto and Gilbert, 2014), (Vanzo
et al., 2014) and (Saif et al., 2012). While their accuracy
on some datasets is quite satisfactory, it is not clear how
well they do on completely unseen data.
From our experience on such classifiers, we believe
that a higher level of accuracy with explainability can
be achieved by imposing a structure that encodes background knowledge about the social hierarchy that is observed in nature. With this motivation, we built a system
whose hierarchical architecture robustly defines the social constructs, the “hidden” concepts that induce them
and their inter-connections. We define notions of intermediate Language Use (LU) and lower level Language
Indicator (LI) categories1 . With the help of these robust definitions, our system properly explains how different emotions and behaviors interact to express status
and leadership among individuals.

2

3

Motivation by a use-case

We start our discussion by presenting a use-case and
explain how results of other traditional methods inspired us to come up with an integrated approach.
Consider the following conversation from
Wikipedia where the participants discuss about a
misleading animation that is used under the topic
Convolution.

{D: Put a message on the talk page of the guy who
made it. You’re right; g(t) should be g(tau - t), and
(f*g)(t) should be (f*g)(tau).
T: I don’t think he has a talk page. He’s provided the
code so I can reproduce it. I think he’s right with (f*g)(t),
though?
D: Actually, I think we’re both wrong. You need a variable of integration running opposite directions ....
T: I’ve updated ... I guess it’s not important, but would
be kind of cool. Feel free to suggest improvements to the
animations.}
As we understand, these conversations suggest that
participant D is supposed to hold a higher rank/status
than T . If we analyze manually, we understand that
phrases like Put a message, You’re right, I think we’re
both wrong together supports our conclusion. Considered separately, the above phrases might be misleading.
To conclude the example, our system outputs :
D has a higher status than T because D demonstrates
more language uses associated with status than T. Confidence: high.
The above example illustrates the degree of contextsensitivity of our problem. The current statistical literature suggests methods such as Decision Trees, Boosting methods comprising of a collection of Weak classi-

1294

Social Constructs

Our framework supports determination of various important Social Constructs such as Leadership, Status, Group
Cohesion and Sub-Group Formation. However, due to
the length constraints of the paper, we will only discuss
Leadership and Status.

3.1

Definitions and Architecture

We begin by first formally defining the two Social Constructs and the different Language Use categories.
Leadership: A leader is someone who guides a group
toward outcomes, controls the group actions, manages interactions between members and members usually recognize the leader.
Status: Status is defined as the social position of one
person with respect to another in a group.
The principal Language Use categories that we detect
are: Deference, Closeness, Authoritative Behavior and
Motivational Behavior. The following intuitions are used
to infer such LUs from text:
Deference is understood when one uses language that
shows respect to another conversational participant or defers to another’s expertise or knowledge or authority.
Closeness is understood when one uses language that
shows familiarity with another conversationalist. It is also
indicated by dialogues where conversationalists refer to
similar events, experiences etc.
Authoritative Behavior is understood when one uses
language that shows power, dominance and control over
a situation.
Motivational Behavior is understood when one uses
language that moves conversational participants toward
1

These definitions were proposed as part of the IARPA
Socio-Cultural Content In Language(SCIL) program.

B

Explanation:

A

Social
Constructs

Means that either the
presence or absence of A
contributes to B

B
A

Leader

Status

Language
Uses
Deference

• The signed LUs that contribute towards the SC Status are ordered based on their importance. We assume the following ordering exists: authoritative
behavior > motivational behavior > negative deference > positive deference in the opposite direction
> closeness. However, we do not assume such an
ordering for the SC Leadership.

Means that only the
presence of A indicates B

Closeness

Authority
Behavior

Motivational
Behavior

Language
Indicators
Respectful
Appellation

Politeness

Impoliteness

Seeking
Support

Apologetic
Behavior

Informal
Language

Informal
Address

Indexical
Statements

Command

Expertise

Agreement

Criticism

Negative
Expertise

Resource
Allocation

Disagreement

Praise

Agenda
Setting

Our extensive research and successful implementation of
our system for different natural languages leads us to believe that these notions are universal in application.

Encouragement

5
Figure 1: Social Construct-Language Use-Language Indicator hierarchy for English Language
sharing a common goal, collaboration, problem solving
and solidarity.
In Figure 1, we present the entire hierarchy and how the
categories are connected among each other. The arrows
in the figure show which of the LI categories are used to
infer a particular type of LU. It also demonstrates how
each of the LU contributes to the Social Constructs.

4

Behind the Curtain: Our Intuitions

One of the fundamental contributions in this paper is formally describing the hierarchy to determine the Social
Constructs, as shown in Figure 1. To come up with these
interconnections and each of the different pieces of the
puzzle, we went through an iterative process of discussions with many social scientists and linguists to analyze
a large number of example conversations. In this process,
we came up with the aforementioned hierarchy, definitions of SC, LU and LIs and most importantly, the following understanding:
• The Language Indicators as shown in the Figure 1,
suffice for the detection of Leadership and Status.
• Each detected LI is associated with an Intensity
Level that helps us to encode the dissimilar effects
of different words in inferring LIs.
• Each LI is associated with a Signed Language Use.
For example, the LI politeness is associated with the
signed LU positive deference.
• Indicators of an LU with a certain sign are counterindicators of the same LU with the opposite sign.
• A signed LU may contribute either favorably or
unfavorably towards its associated SC. For example, positive authoritative behavior contributes favorably towards higher status.

1295

Fundamentals of the implementation

After we parse each sentence using Stanford Dependency
parser to get the POS tags and mutual dependencies, the
detection of individual LIs and the mapping of LIs, LUs
to SCs are achieved using a combination of statistical and
logic based approach. Many of the ideas and insights
about the detection of LIs and their relations with the LUs
are motivated from (Simon, 1946), (Pennebaker et al.,
2003) , (Bernstein, 2010) , (Brown and Levinson, 1988)
and a few others. Some of our ideas for textual inference
have been inspired by (Scherl et al., 2010).

5.1

Determining the Language Indicators

The process of detection of language indicators from sentences uses a huge ensemble of complex rules. To create
these rules, we borrowed ideas from the researchers of social science and psychology (Simon, 1946; Pennebaker et
al., 2003).
With the help of POS tags, mutual dependencies and
regular expressions, we create a framework where we detect individual events, verbs, other sentence constituents
and their positive and negative sense. On top of this
framework, we use two different methods to detect language indicators. The ideas are similar for all the LIs. We
will only present a few examples for the LI “Command”.

5.1.1

Using Regular Expressions Alone

We
use
regular
expressions
of
the
form
“.*\b[wW]hy
don’?t
(you|YOU)
(start|read|submit|make|write|get)\s*\b.*” to detect
LIs such as “Command”. We employ a collection of such
expressions to cover several different linguistic styles
which indicates “Command” by an individual.
We achieved a very high recall (close to 1.0) for most
indicators with these rules on test data. However, in few
cases, the frequency of such indicators (such as politeness) were very low deeming the set of regular expressions as incomplete. This observation led us to refine the
regular expressions with Logical rules so that we can incorporate our domain knowledge and remove such bias
to the training set.

5.1.2

Using Logical rules on Regular
Expression output and Sentence
constituents

One example of the rules we use to detect “Command”
is: if the subject of the verb is second person and the verb
is associated with a modal verb which indicates a question that suggests command, then the LI “Command” is
detected.
Examples of such verbs are “Would you” and “Could
you” etc. It is to be noted that such a verb will denote
both politeness and command depending on the rest of
the sentence. This fascinating inter-dependency is one
reason why we have to collect all such Language Indicators before we infer the higher level Language Uses.

5.2

Mapping of LIs to LUs and LUs to Social
Constructs

Input: To encode one conversation we use a collection
of facts of the form participant(X) and addresses(X, Y,
LI, Level).
These facts essentially encode the identity of the participants and the Language Indicators observed in the
overall conversation among a pair of participants.
Output: The module outputs a collection of claim, evidence and confidence mappings.
For example one such mapping is: claim_mapping(X,
"is the leader", "because", X, "demonstrates <language
use>","(Confidence: <confidence level>)"). Here <language use> is one of the language uses, <confidence
level> is either low, medium, or high.
Algorithm: We employ statistical and logic-based
procedure in parallel to get the above output. On the
statistical side, we adopt a regression technique to learn
a function that can map the scores associated with LIs
to individual LUs based on annotated training data and
this function is then applied to test data to get confidence
score on LUs. The same procedure is adopted for mapping LUs to SCs.
In parallel to this procedure, we also employ a rulebased technique that uses quantized confidence scores
and outputs confidence levels along with explanations.
As we are able to get the explanation from logical reasoning, we use the output confidence scores as votes from
statistical learning to output the final confidence level.
The rules for logical reasoning are explained as definitions and intuitions in the following paragraphs.
Mapping LIs into LUs: A signed LU is said to be
exhibited by participant X towards participant Y with a
certain degree of confidence based on the number of indicators(LI) and counter-indicators(LI) of the signed LU
used by X when addressing Y. The confidence in LU is
directly proportional to the difference between the number of indicators and counter-indicators.

1296

We categorize LUs according to the number of indicators and apply slight variation to the above rules for
each such category. Also, there are a few LIs that, when
used, automatically override the computed confidence
level for an LU and increase it to high. For example,
“criticism” increases confidence level of positive “motivational behavior” to high.
Mapping LUs to SCs: The relative status of two participants is determined based on i) the number of relevant signed LUs exhibited by each participant towards
the other, ii) the ordering of relevant signed LUs and iii)
the confidence level in each exhibited signed LU.
The leader is determined based on the number of exhibited relevant LUs (both favorable and unfavorable).
Mapping LIs to SCs: As shown in Figure 1, we directly associate some of the LIs to Social Constructs. For
such an association, we again adopt the regression technique mentioned previously. In this case, the confidence
scores from LIs are directly mapped to the confidence
scores of SCs. We combine this confidence with the
above confidence levels using simplistic rules to output
final social constructs.
It should be noted that the constants used in the rules
are obtained from statistics on annotated conversations.
The annotation process involves labels about SCs, LUs
and LIs for each conversation data.

5.3

Brief Details and Results of the Regression
Technique

In this sub-section, we provide few details of the Sparse
Logistic Regression technique we have used alongside
the logical formulation and present few results from our
experiments with relevant statistical methods. We have
used a similar formulations for mapping LIs to LUs and
LUs to SCs. Here, we provide the example of formulating the entire problem of detection of Social Constructs
directly in the Classification paradigm.
Status and Leadership can be formulated as a threeclass and two-class problem respectively. For Status,
we had 102 samples with the 38(higher), 26(equal) and
38(lower) samples each for three classes. For Leadership, we had 149 samples with 108(not-leader) and
41(leader) samples for the two classes. For both the tasks,
we extracted 28 textual features. We used the one-vs-rest
scheme for multi-class problem. For each task, we evaluated the framework as follows: i. First, we randomly
separate the dataset into training set(p) and test set(1-p).
ii. In the training set, we use 10-fold cross validation to
select proper parameters. iii. We iterate the above procedure for 100 times, and accuracy is evaluated on the predictions in all iterations. iv. We select different p (from
0.25 to 0.9) and observe the change of accuracy.
We compared the accuracy achieved using Sparse Logistic Regression with SVM(with RBF Kernel) among

is perhaps unique in determining such social constructs
and evaluating on familiar and unfamiliar datasets. Table
Table 1: Results

(a)

SC
Task Leader
Task Leader
Status
Status
Task Leader
Status

(b)

Figure 2: (a) Training set percentage vs Accuracy graph
for Leadership problem, (b) Training set percentage vs
Accuracy graph for Status classification problem.
others. The accuracy comparison of the SVM(with RBF
kernel) and sparse Logistic Regression is provided in Figure 2. As we can observe, though the two methods are
comparable, in most cases Sparse Logistic regression performs better.

5.4

Advantages from the integrated approach

The primary advantages are the following:
In general, statistical approaches need a “lot of data”
to attain a certain level of accuracy. As the rules we use
are quite universal and compact, we can achieve a comparable(or higher) accuracy with much less training data.
Using the evidence and claim mappings, we give an
“explanation” as to why we detected such a particular SC
in the dialogue. Knowldege of such depth is very hard to
achieve with only statistical approaches.
Explicit representation of “context” specific information via rules results in improved accuracy in detection of
LIs such as criticism, praise, command etc.
Statistical modules complement the rule-based approach where our domain knowledge is “incomplete”.
We use ASP as the Logic Programming language of
our choice as its ability to represent defaults and exceptions eases the implementation procedure.

6

Results

We have implemented this system using ASP(Gelfond
and Lifschitz, 1988) and Java. The Wikipedia conversations are obtained by parsing the wiki dump from
http://dumps.wikimedia.org/. We also evaluated on the NWTRB (US Nuclear Waste Technical Review Board) dataset. The accuracy and F1 measure are
summarized in Table 1 for approximately two thousand
English and one thousand Korean Wikipedia conversations. We evaluated two types of questions - i. Yes-No
indicates questions like Is John the leader? and ii. List
indicates questions such as List all the leaders.. Our work

1297

Q-Type
Y-N
List
Y-N
List
Y-N
Y-N

Language
EN
EN
EN
EN
KO
KO

Accuracy
0.8900
0.6700
0.4700
0.6923
0.5667
0.4074

F1
0.6700
0.9900
0.3457
0.5200
0.4338
0.3900

1 reports evaluations on wikipedia dump. These values
are computed by comparing the results of our systems
with annotated data. Note, in our experiments, we have
performed strict evaluations. For example, the results
are only marked positive if the complete list of leaders
matches with a human-annotated list. Also, we consider
the “explanation” too while performing the evaluation.
The results are true positive only when the detected construct is correct alongwith the explanation provided by
the reasoning module. In general, the previous research
achieves an accuracy of 0.45 in comparable tasks such as
dialog act tagging (Stolcke, 2000).

7

Conclusion

In this paper, we have proposed a novel approach for logically recognizing social constructs from textual conversations. We have used both statistical classification and
logical reasoning to robustly detect status and leadership
as observed in virtual social networks. From our experiments, we show empirically how our approach achieves
a significant accuracy and provides logical explanation of
construct detection.
This research shows the merits of using logical rules
along with statistical techniques to determine Social Constructs. As per our understanding, this level of accuracy
and explainability needs integration of both statistical and
logic based methods. Our observations suggest that there
is an increasing need for such integration in various domains. We believe that this work is one of the early steps
in that direction.

8

Acknowledgement

We thank the IARPA SCIL program for supporting this
research. We also thank NSF for the DataNet Federation
Consortium grant OCI-0940841 and ONR for their grant
N00014-13-1-0334 for partially supporting this research.

References
[Bernstein2010] Basil Bernstein. 2010. A public language: some sociological implications of a linguistic
form. British Journal of Sociology, pages 53–69.
[Brown and Levinson1988] Penelope
Brown
and
STEPHEN C. Levinson. 1988. Politeness: Some Universals in Language Usage (Studies in Interactional
Sociolinguistics 4). Cambridge University Press.
[Danescu-Niculescu-Mizil et al.2012] Cristian DanescuNiculescu-Mizil, Lillian Lee, Bo Pang, and Jon Kleinberg. 2012. Echoes of power: Language effects and
power differences in social interaction. In Proceedings of the 21st International Conference on World
Wide Web, WWW ’12, pages 699–708, New York, NY,
USA. ACM.
[Gelfond and Lifschitz1988] Michael
Gelfond
and
Vladimir Lifschitz. 1988. The stable model semantics
for logic programming. pages 1070–1080. MIT Press.
[Gilbert2012] Eric Gilbert. 2012. Phrases that signal
workplace hierarchy. In Proceedings of the ACM
2012 Conference on Computer Supported Cooperative
Work, CSCW ’12, pages 1037–1046, New York, NY,
USA. ACM.
[Gupte et al.2011] Mangesh Gupte, Pravin Shankar, Jing
Li, S. Muthukrishnan, and Liviu Iftode. 2011. Finding hierarchy in directed online social networks. In
Proceedings of the 20th International Conference on
World Wide Web, WWW ’11, pages 557–566, New
York, NY, USA. ACM.
[Hutto and Gilbert2014] C. J. Hutto and Eric Gilbert.
2014. Vader: A parsimonious rule-based model for
sentiment analysis of social media text. In ICWSM.
[Mayfield et al.2013] Elijah Mayfield, David Adamson,
and Carolyn Penstein Rosé. 2013. Recognizing rare
social phenomena in conversation: Empowerment detection in support group chatrooms. pages 104–113.
[Medhat et al.2014] W. Medhat, A. Hassan, and H. Korashy. 2014. Sentiment analysis algorithms and applications: A survey. Ain Shams Engineering Journal,
5(4):1093 – 1113.
[Pennebaker et al.2003] James
W.
Pennebaker,
Matthias R. Mehl, and Kate G. Niederhoffer.
2003. Psychological aspects of natural language use:
Our words, our selves. Annual Review of Psychology,
54(1):547.
[Prabhakaran et al.2012] Vinodkumar
Prabhakaran,
Huzaifa Neralwala, Owen Rambow, and Mona Diab.
2012. Annotations for power relations on email
threads. In Proceedings of the Eight International
Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).

1298

[Saif et al.2012] Hassan Saif, Yulan He, and Harith Alani.
2012. Semantic sentiment analysis of twitter. In Proceedings of the 11th International Conference on The
Semantic Web - Volume Part I, ISWC’12, pages 508–
524, Berlin, Heidelberg. Springer-Verlag.
[Scherl et al.2010] R. Scherl, D. Inclezan, and M. Gelfond. 2010. Automated inference of socio-cultural
information from natural language conversations. In
IEEE International Conference on Social Computing,
pages 480–487, Aug.
[Shaikh et al.2010] Samira Shaikh, Tomek Strzalkowski,
Aaron Broadwell, Jennifer Stromer-Galley, Sarah Taylor, and Nick Webb. 2010. Mpc: A multi-party chat
corpus for modeling social phenomena in discourse.
In Proceedings of the Seventh International Conference on LREC, may.
[Simon1946] Herbert A. Simon. 1946. The proverbs
of administration. Public Administration Review,
6(1):53–67.
[Stolcke2000] Andreas Stolcke. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech.
[Strzalkowski et al.2010] Tomek
Strzalkowski,
George Aaron Broadwell, Jennifer Stromer-Galley,
Samira Shaikh, Sarah M. Taylor, and Nick Webb.
2010.
Modeling socio-cultural phenomena in
discourse. In COLING 2010, 23rd International
Conference on Computational Linguistics, pages
1038–1046.
[Tari et al.2010] Luis Tari, Saadat Anwar, Shanshan
Liang, James Cai, and Chitta Baral. 2010. Discovering drug-drug interactions: a text-mining and reasoning approach based on properties of drug metabolism.
Bioinformatics, 26(18).
[Vanzo et al.2014] Andrea Vanzo, Danilo Croce, and
Roberto Basili. 2014. A context based model for sentiment analysis in twitter. In Proceedings of COLING
2014, pages 2345–2354, Dublin, Ireland. Dublin City
University and Association for Computational Linguistics.

Encoding Petri Nets in Answer Set Programming for
Simulation Based Reasoning
Saadat Anwar1 , Chitta Baral1
Katsumi Inoue2
1

arXiv:1306.3542v2 [cs.AI] 24 Jun 2013

2

SCIDSE, Arizona State University, 699 S Mill Ave, Tempe, AZ 85281, USA
Principles of Informatics Research Division, National Institute of Informatics, Japan

Abstract. One of our long term research goals is to develop systems to answer
realistic questions (e.g., some mentioned in textbooks) about biological pathways
that a biologist may ask. To answer such questions we need formalisms that can
model pathways, simulate their execution, model intervention to those pathways,
and compare simulations under different circumstances. We found Petri Nets to
be the starting point of a suitable formalism for the modeling and simulation
needs. However, we need to make extensions to the Petri Net model and also
reason with multiple simulation runs and parallel state evolutions. Towards that
end Answer Set Programming (ASP) implementation of Petri Nets would allow
us to do both. In this paper we show how ASP can be used to encode basic Petri
Nets in an intuitive manner. We then show how we can modify this encoding to
model several Petri Net extensions by making small changes. We then highlight
some of the reasoning capabilities that we will use to accomplish our ultimate
research goal.
Keywords: Petri Nets, ASP, Knowledge Representation and Reasoning, Modeling and Simulation

1

Introduction

The main motivation behind this paper is to model biological pathways and answer
questions of the kind that a biologist would ask. Examples of such questions include
ones used in biology text books to test the understanding of students. We found Petri
Nets [1] to be the most suitable starting point for our needs as its graphical representation and semantics closely matches biological pathway diagrams.3 However, answering
those type of questions requires certain extensions to the Petri Net model and reasoning
with multiple simulations and parallel evolutions. Although numerous Petri Net modeling, simulation and analysis systems exist [8,9,10,11,12,13], we did not find these
systems to be a good match for all our needs. Some had limited adaptability outside
their primary application domain, while others, though quite capable, did not offer easy
extensibility. Most systems did not explore all possible state evolutions nor allowed
different firing semantics.
3

In addition to modeling of biological systems, Petri Nets are also used over a wide range of
domains, such as, workflows, embedded systems and industrial control [2,3,4,5,6,7].

To address the twin needs of easy extensibility and reasoning over multiple evolutions we propose encoding Petri Nets using Answer Set Programming (ASP), which is
a declarative programming language with competitive solvers 4 and has been used in
modeling domains such as spacecrafts [17], work flows [18], natural language processing [19,20] and biological systems modeling [21,22]. In our quest we found ASP to
be preferable to process algebra, temporal logics, and mathematical equations applied
to Petri Nets. Some of these techniques, like π-calculus is cumbersome even for small
Nets [23], while others, like mathematical equations, impose restrictions on the classes
of Petri Nets they can model [24].
Petri Net translation to ASP has been studied before [25,26]. However, these implementations are limited to specific classes of Petri Nets and have a different focus.
For example, [25] used ASP for the analysis of properties of 1-safe Petri Nets such as
reachability and deadlock detection. 1-safe Petri Nets are very basic in nature as they
can accommodate at most one token in a place and they don’t allow source/sink transitions or inhibition arcs. As a result their ASP translation does not require handling of
token aggregation, leading to simpler conflict resolution and weight-constraints than the
general case. In [26], a new class of Petri Nets called Simple Logic Petri Nets (SLPNs)
is presented and translated to ASP code. This class of Petri Nets uses simple logic expressions for arc weights and positive ground literals as tokens at places. Their implementation does not carry the notion of conflicting transitions, i.e., a token from a source
place can be used in multiple transitions in a single firing step. It also does not follow
the standard notion of token aggregation, i.e., if the same token arrives at a place from
two different transitions, it looses its individual identity and only gets counted as one.
Both of these implementations focus on analyzing properties of specific classes of Petri
Nets. We design our implementation to simulate general Petri Nets. To our knowledge
this has not been attempted in ASP before. However, some of our encoding scheme is
similar to their work. Our current focus in this work is less on performance and more
on the ease of encoding, extensibility, and exploring all possible state evolutions.
Thus the main contributions of this paper are: In Section 3 we show how ASP allows
intuitive declarative encoding of basic Petri Nets resulting in a low specification–implementation gap. We then show, how Petri Net extensions can be incorporated in our
encoding by making small changes. In particular, we explore change of firing semantics
(Section 4), and allowing reset arcs (Section 5), inhibitor arcs (Section 6) and read
arcs (Section 7). We show how our ASP encoding allows exploring all possible state
evolutions in a Petri Net simulation and how to reason with those simulation results.
We also show how Petri Nets fit into our ultimate goal of answering questions with
respect to biological pathways. While the goal of this paper is not to analyze Petri Net
properties of reachability, liveness, and boundedness etc., we briefly mention how these
can be analyzed using our encoding. We now start with a brief background on ASP and
Petri Nets.

4

See [14,15,16]

2

Background on ASP and Petri Nets

Answer Set Programming (ASP) is a declarative logic programming language based
on the Stable Model Semantics [27]. Please refer to the Clingo manual [14] for details
of the ASP syntax used in this paper.
A Petri Net is a graph of a finite set of nodes and directed arcs, where nodes are
split between places and transitions, and each arc either connects a place to a transition
or a transition to a place. Each place has a number of tokens (called the its marking).
Collective marking of all places in a Petri Net is called its marking (or state). Arc labels
represent arc weights. When missing, arc-weight is assumed as one, and place marking
is assumed as zero.
dhap

t5b
0
t3

f16bp

t5a
t4
g3p

t6

2

bpg13

Fig. 1: Petri Net graph (of sub-section of glycolysis pathway) showing places as circles, transitions as boxes and arcs as
directed arrows. Places have token count (or marking) written above them, assumed 0 when missing. Arcs labels represent
arc-weights, assumed 1 when missing.

The set of place nodes on incoming and outgoing arcs of a transition are called
its pre-set (input place set or input-set) and post-set (output place set or output-set),
respectively. A transition t is enabled when each of its pre-set place p has at least the
number of tokens equal to the arc-weight from p to t. An enabled transition may fire,
consuming tokens equal to arc-weight from p to t from each pre-set place p, producing
tokens equal to arc-weight from t to p to each post-set place p.
Multiple transitions may fire as long as they consume no more than the available
tokens, with the assumption that tokens cannot be shared. Fig. 1 shows a portion of the
glycolysis pathway [28], in which places represent reactants and products, transitions
represent reactions, and arc weights represent reactant quantity consumed or the product
quantity produced by the reaction.
Definition 1 (Petri Net). A Petri Net is a tuple P N = (P, T, E, W ), where, P =
{p1 , . . . , pn } is a finite set of places; T = {t1 , . . . , tm } is a finite set of transitions,
P ∩ T = ∅; E + ⊆ T × P is a set of arcs from transitions to places; E − ⊆ P × T is
a set of arcs from places to transitions; E = E + ∪ E − ; and W : E → N \ {0} is the
arc-weight function

Definition 2 (Marking). A marking M = (M (p1 ), . . . , M (pn )) is the token assignment of each place node pi ∈ P , where M (pi ) ∈ N. Initial token assignment M0 :
P → N is called the initial marking. Marking at step k is written as Mk .
Definition 3 (Pre-set & post-set of a transition). Pre-set / input-set of a transition t
is •t = {p ∈ P : (p, t) ∈ E − }, while the post-set / output-set is t• = {p ∈ P : (t, p) ∈
E+}
Definition 4 (Enabled Transition). A transition t is enabled with respect to marking
M , enabledM (t), if ∀p ∈ •t, W (p, t) ≤ M (p). An enabled transition may fire.
Definition 5 (Execution). An execution is the simulation of change of marking from
Mk to Mk+1 due to firing of transition t. Mk+1 is computed as follows:
∀pi ∈ •t, Mk+1 (pi ) = Mk (pi ) − W (pi , t)
∀pj ∈ t•, Mk+1 (pj ) = Mk (pj ) + W (t, pj )
Definition 6 (Conflicting Transitions). A set of enabled transitions Te = {t ∈ T :
enabledM (t)} conflict if their simultaneous firing will consume more tokens than are
available at an input place:
X
∃p ∈ P : M (p) <
W (p, t)
t∈Te ∧(p,t)∈E −

Definition 7 (Firing Set). A set of simultaneously firing, non-conflicting, enabled transitions Tk = {t1 , . . . , tm } ⊆ T is called a firing set. Its execution w.r.t. marking Mk
produces new marking Mk+1 as follows:
X
X
∀p ∈ P, Mk+1 (p) = Mk (p) −
W (p, t) +
W (t, p)
t∈Tk ∧p∈•t

t∈Tk ∧p∈t•

Definition 8 (Execution Sequence). An execution sequence is the simulation of a firing sequence σ = T1 , T2 , . . . , Tk . It is the transitive closure of executions, where subsequent markings become the initial marking for the next firing set. Thus, in the execution
sequence X = M0 , T0 , M1 , T1 , . . . , Mk , Tk , Mk+1 , the firing of T0 with respect to
marking M0 produces the marking M1 which becomes the initial marking for T1 .

3

Translating Basic Petri Net Into ASP

In this section we present ASP encoding of simple Petri Nets. We describe, how a given
Petri Net P N , and an initial marking M0 are encoded into ASP for a simulation length
k. Following sections will show how Petri Net extensions can be easily added to it. We
represent a Petri Net with the following facts:
f1:
f2:
f3:
f4:

Facts place(pi ). where pi ∈ P is a place.
Facts trans(tj ). where tj ∈ T is a transition.
Facts ptarc(pi , tj , W (pi , tj )). where (pi , tj ) ∈ E − with weight W (pi , tj ).
Facts tparc(ti , pj , W (ti , pj )). where (ti , pj ) ∈ E + with weight W (ti , pj ).

Petri Net execution simulation proceeds in discrete time-steps, these time steps are
encoded by the following facts:
f5: Facts time(tsi ) where 0 ≤ tsi ≤ k.

initial marking (or initial state) of the Petri Net is represented by the following
facts:
i1: Facts holds(pi , M0 (pi ), 0) for every place pi ∈ P with initial marking M0 (pi ).

ASP requires all variables in rule bodies be domain restricted. Thus, we add the
following facts to capture token quantities produced during the simulation 5 :
x1: Facts num(n)., where 0 ≤ n ≤ ntok

A transition ti is enabled if each of its input places pj ∈ •ti has at least arc-weight
W (pj , ti ) tokens. Conversely, ti is not enabled if ∃pj ∈ •ti : M (pj ) < W (pj , ti ),
and is only enabled when no such place pj exists. These are captured in e1 and e2
respectively:
e1: notenabled(T,TS):-ptarc(P,T,N),holds(P,Q,TS),Q<N, place(P),
trans(T), time(TS),num(N),num(Q).
e2: enabled(T,TS) :- trans(T), time(TS), not notenabled(T, TS).

The rule e1 encodes notenabled(T,TS) which captures the existence of an input
place P of transition T that violates the minimum token requirement N at time-step
T S. Where, the predicate holds(P,Q,TS) encodes the marking Q of place P at T S.
Rule e2 encodes enabled(T,TS) which captures that transition T is enabled at T S
since there is no input place P of transition T that violates the minimum input token
requirement at T S. In biological context, e2 captures the conditions when a reaction
(represented by T ) is ready to proceed. A subset of enabled transitions may fire simultaneously at a given time-step. This is encoded as:
a1: {fires(T,TS)} :- enabled(T,TS), trans(T), time(TS).

Rule a1 encodes fires(T,TS), which captures the firing of transition T at T S.
The rule is encoded with a count atom as its head, which makes it a choice rule. This
rule either picks the enabled transition T for firing at T S or not, effectively enumerating
a subset of enabled transitions to fire. Whether this set can fire or not in an answer set is
subject to conflict checking, which is done by rules a2, a3, a4 shown later. In biological
context, the selected transition-set models simultaneously occurring reactions and the
conflict models limited reactant supply that cannot be shared. Such a conflict can lead to
multiple choices in parallel reaction evolutions and different outcomes. The next set of
rules captures the consumption and production of tokens due to the firing of transitions
in a firing-set as well as their aggregate effect, which computes the marking for the next
time step:
r1: add(P,Q,T,TS) :- fires(T,TS), tparc(T,P,Q), time(TS).
5

Note that ntok can be arbitrarily chosen to be larger than the maximum expected token quantity produced during the simulation.

r2: del(P,Q,T,TS) :- fires(T,TS), ptarc(P,T,Q), time(TS).
r3: tot incr(P,QQ,TS) :- QQ=#sum[add(P,Q,T,TS)=Q:num(Q):trans(T)],
time(TS), num(QQ), place(P).
r4: tot decr(P,QQ,TS) :- QQ=#sum[del(P,Q,T,TS)=Q:num(Q):trans(T)],
time(TS), num(QQ), place(P).
r5: holds(P,Q,TS+1) :-holds(P,Q1,TS),tot incr(P,Q2,TS),time(TS+1),
tot decr(P,Q3,TS),Q=Q1+Q2-Q3,place(P),num(Q;Q1;Q2;Q3),time(TS).

Rule r1 encodes add(P,Q,T,TS) and captures the addition of Q tokens to place
P due to firing of transition T at time-step T S. Rule r2 encodes del(P,Q,T,TS) and
captures the deletion of Q tokens from place P due to firing of transition T at T S. Rules
r3 and r4 aggregate all add’s and del’s for place P due to r1 and r2 at time-step T S,
respectively, by using the QQ=#sum[] construct to sum the Q values into QQ. Rule r5
which encodes holds(P,Q,TS+1) uses these aggregate adds and removes and updates
P ’s marking for the next time-step T S +1. In biological context, these rules capture the
effect of a reaction on reactant and product quantities available in the next simulation
step. To prevent overconsumption at a place following rules are added:
a2: consumesmore(P,TS) :- holds(P,Q,TS), tot decr(P,Q1,TS), Q1 > Q.
a3: consumesmore :- consumesmore(P,TS).
a4: :- consumesmore.

Rule a2 encodes consumesmore(P,TS) which captures overconsumption of tokens at input place P at time T S due to the firing set selected by a1. Overconsumption
(and hence conflict) occurs when tokens Q1 consumed by the firing set are greater than
the tokens Q available at P . Rule a3 generalizes this notion of overconsumption and
constraint a4 eliminates answers where overconsumption is possible.
Definition 9. Given a Petri Net P N and its encoding Π(P N, M0 , k). We say that there
is a 1-1 correspondence between the answer sets of Π(P N, M0 , k) and the execution
sequences of P N iff for each answer set A of Π(P N, M0 , k), there is a corresponding
execution sequence X = M0 , T0 , M1 , . . . , Mk , Tk of P N such that
{f ires(t, j) : t ∈ Tj , 0 ≤ j ≤ k} = {f ires(t, ts) : f ires(t, ts) ∈ A}
{holds(p, q, j) : p ∈ P, q = Mj (p), 0 ≤ j ≤ k} = {holds(p, q, ts) : holds(p, q, ts) ∈ A}
Proposition 1. There is a 1-1 correspondence between the answer sets of Π0 (P N, M0 , k)
and the execution sequences of P N .
3.1

An example execution

Next we look at an example execution of the Petri Net shown in Figure 1. The Petri Net
and its initial marking are encoded as follows6 :
6

{holds(p1,0,0),...,holds(pN,0,0)},
{time(0),...,time(5)} have been written as
num(0..60), time(0..5), respectively, to save space.

{num(0),...,num(60)},
holds(p1;...;pN,0,0),

num(0..60).time(0..5).place(f16bp;dhap;g3p;bpg13).
trans(t3;t4;t5a;t5b;t6).tparc(t3,f16bp,1).ptarc(f16bp,t4,1).
tparc(t4,dhap,1).tparc(t4,g3p,1).ptarc(dhap,t5a,1).
tparc(t5a,g3p,1).ptarc(g3p,t5b,1).tparc(t5b,dhap,1).
ptarc(g3p,t6,1).tparc(t6,bpg13,2).holds(f16bp;dhap;g3p;bgp13,0,0).

we get thousands of answer-sets, for example7 :
holds(bpg13,0,0) holds(dhap,0,0) holds(f16bp,0,0) holds(g3p,0,0)
holds(bpg13,0,1) holds(dhap,0,1) holds(f16bp,1,1) holds(g3p,0,1)
holds(bpg13,0,2) holds(dhap,1,2) holds(f16bp,1,2) holds(g3p,1,2)
holds(bpg13,0,3) holds(dhap,2,3) holds(f16bp,1,3) holds(g3p,2,3)
holds(bpg13,2,4) holds(dhap,3,4) holds(f16bp,1,4) holds(g3p,2,4)
holds(bpg13,4,5) holds(dhap,4,5) holds(f16bp,1,5) holds(g3p,2,5)
fires(t3,0) fires(t3;t4,1) fires(t3;t4;t5a;t5b,2)
fires(t3;t4;t5a;t5b;t6,3) fires(t3;t4;t5a;t5b;t6,4)
fires(t3;t4;t5a;t5b;t6,5)

4

Changing Firing Semantics

The ASP code above implements the set firing semantics. It can produce a large number
of answer-sets, since any subset of a firing set will also be fired as a firing set. For
our biological system modeling, it is often beneficial to simulate only the maximum
activity at any given time-step. We accomplish this by defining the maximal firing set
semantics, which requires that a maximal subset of non-conflicting transitions fires at
a single time step8 . Our semantics is different from the firing multiplier approach used
by [30], in which a transition can fire as many times as allowed by the tokens available
in its source places. Their approach requires an exponential time firing algorithm in the
number of transitions. Our maximal firing set semantics is implemented by adding the
following rules to the encoding in Section 3:
a5: could not have(T,TS) :- enabled(T,TS), not fires(T,TS),
ptarc(S,T,Q), holds(S,QQ,TS), tot decr(S,QQQ,TS), Q > QQ - QQQ.
a6: :-not could not have(T,TS), enabled(T,TS), not fires(T,TS),
trans(T), time(TS).

Rule a5 encodes could not have(T,TS) which means that an enabled transition
T that did not fire at time T S, could not have fired because its firing would have resulted
in overconsumption. Rule a6 eliminates any answer-sets in which an enabled transition
did not fire, that could not have caused overconsumption. Intuitively, these two rules
guarantee that the only reason for an enabled transition to not fire is conflict avoidance (due to overconsumption). With this firing semantics, the number of answer-sets
produced for Petri Net in Figure 1 reduces to 2.
7

8

{fires(t1,ts1),...,fires(tN,ts1)}
have
been
written
as
fires(t1;...;tN;ts1) to save space.
Such a semantics reduces the reachable markings. See [29] for the analysis of its computational
power.

Proposition 2. There is 1-1 correspondence between the answer sets of Π1 (P N, M0 , k)
and the execution sequences of P N .
Other firing semantics can be encoded with similar ease9 . We now look at Petri Net
extensions and show how they can be easily encoded in ASP.

5

Extension - Reset Arcs

Definition 10 (Reset Arc). A Reset Arc in a Petri Net P N R is an arc from place p to
transition t that consumes all tokens from its input place p on firing of t. A Reset Petri
Net is a tuple P N R = (P, T, E, W, R) where, P, T, E, W are the same as for PN; and
R : T → 2P defines reset arcs

0
t3

f16bp

dhap

tr

t5a

t5b

g3p

t6

t4
2

bpg13

Fig. 2: Petri Net of Fig 1 extended with a reset arc from dhap to tr shown with double arrowhead.

Figure 2 shows an extended version of the Petri Net in Figure 1 with a reset arc
from dhap to tr (shown with double arrowhead). In biological context it models the
removal of all quantity of compound dhap. Petri Net execution semantics with reset
arcs is modified for conflict detection and execution as follows:
Definition 11 (Conflicting Transitions in P N R ). A set of enabled transitions conflict
in P N R w.r.t. Mk if firing them simultaneously will consume more tokens than are
available at any one of their common input-places. Te = {t ∈ T : enabledMk (t)}
conflict if:
X
X
∃p ∈ P : Mk (p) < (
W (p, t) +
Mk (p))
t∈Te ∧(p,t)∈E −
9

t∈Te ∧p∈R(t)

For example, if interleaved firing semantics is desired, replace rules a5, a6 with the following:

a5’: more than one fires :- fires(T1,TS), fires(T2, TS), T1 != T2,
time(TS).
a6’: :- more than one fires.

Definition 12 (Execution in P N R ). Execution of a transition set Ti in P N R has the
following effect:
X
X
∀p ∈ P \ R(Ti ), Mk+1 (p) = Mk (p) −
W (p, t) +
W (t, p)
t∈Ti ∧p∈•t

∀p ∈ R(Ti ), Mk+1 (p) =

t∈Ti ∧p∈t•

X

W (t, p)

t∈Ti ∧p∈t•

where R(Ti ) =

[

R(t) and represents the places emptied by Ti due to reset arcs.

t∈Ti

Since a reset arc from p to t, p ∈ R(t) consumes current marking dependent tokens,
we extend ptarc to include time and replace f 3, f 4, e1, r1, r2 with f 6, f 7, e3, r6, r7,
respectively in the Section 4 encoding and add rule f 8 for each reset arc:
f6: Rules ptarc(pi , tj , W (pi , tj ), tsk ):-time(tsk ). for each non-reset arc (pi , tj ) ∈ E −
f7: Rules tparc(ti , pj , W (ti , pj ), tsk ):-time(tsk ). for each non-reset arc (ti , pj ) ∈ E +
e3: notenabled(T,TS) :- ptarc(P,T,N,TS), holds(P,Q,TS), Q < N,
place(P), trans(T), time(TS), num(N), num(Q).
r6: add(P,Q,T,TS) :- fires(T,TS), tparc(T,P,Q,TS), time(TS).
r7: del(P,Q,T,TS) :- fires(T,TS), ptarc(P,T,Q,TS), time(TS).
f8: Rules ptarc(pi , tj , X, tsk ) :- holds(pi , X, tsk ), num(X), X > 0. for each
reset arc between pi and tj using X = Mk (pi ) as arc-weight at time step tsk .

Rule f 8 encodes place-transition arc with marking dependent weight to capture the
notion of a reset arc. The execution semantics of our definition are slightly different
from the standard definition in [31], even though both capture similar operations. Our
implementation considers token consumption by reset arc in contention with other token consuming arcs from the same place, while the standard definition considers token
consumption as a side effect, not in contention with other arcs.

pyr
glu

gly1
atp

amp

cw1

Fig. 3: Petri Net showing feedback inhibition arc from atp to gly1 with a bullet arrowhead. Inhibitor arc weight is assumed
1 when not specified.

We chose our definition to allow modeling of biological process that removes all
available quantity of a substance in a maximal firing set. Consider Figure 2, if dhap has
1 or more tokens, our semantics would only permit either t5a or tr to fire in a single
time-step, while the standard semantics can allow both t5a and tr to fire simultaneously,
such that the reset arc removes left over tokens after (dhap, t5a) consumes one token.

We could have, instead, extended our encoding to include self-modifying nets [32],
but our definition provides a simpler solution. Standard semantics, however, can be
easily encoded10 .
Proposition 3. There is 1-1 correspondence between the answer sets of Π2 (P N R , M0 , k)
and the execution sequences of P N R .

6

Extension - Inhibitor Arcs

Definition 13 (Inhibitor Arc). An inhibitor arc [33] is a place–transition arc that inhibits its transition from firing as long as the place has any tokens in it. An inhibitor arc
does not consume any tokens from its input place. A Petri Net with reset and inhibitor
arcs is a tuple P N RI = (P, T, E, W, R, I), where, P, T, E, W, R are the same as for
P N R ; and I : T → 2P defines inhibitor arcs.

h_is

3
25

3

h_mm

syn
adp
atp
p

Fig. 4: Petri Net with read arc from h is to syn shown with arrowhead on both ends. The transition syn will not fire unless
there are at least 25 tokens in h is, but when it executes, it only consumes 3 tokens.

Figure 3 shows a Petri Net with inhibition arc from atp to gly1 with a bulleted
arrowhead. It models biological feedback regulation in simplistic terms, where excess
10

Standard reset arc can be added to Section 4 encoding by splitting r5 into r5a0 , r5b0 and
adding f 80 , a70 as follows:

f8’: rptarc(pi ,tj ). - fact capturing a reset arc
a7’: reset(P,TS) :- rptarc(P,T), place(P), trans(T), fires(T,TS),
time(TS). - rule to capture if P will be reset at time T S due to firing of transition T that
has a reset arc on it from P to T .
r5a’: holds(P,Q,TS+1) :- holds(P,Q1,TS), tot_incr(P,Q2,TS),
tot_decr(P,Q3,TS), Q=Q1+Q2-Q3, place(P), num(Q;Q1;Q2;Q3),
time(TS), time(TS+1), not reset(P,TS). - rule to compute marking at
T S + 1 when P is not being reset.
r5b’: holds(P,Q,TS+1) :- tot_incr(P,Q,TS), place(P), num(Q),
time(TS), time(TS+1), reset(P,TS). - rule to compute marking at T S + 1
when P is being reset.

atp downstream causes the upstream atp production by glycolysis gly to be inhibited
until the excess quantity is consumed [28]. Petri Net execution semantics with inhibit
arcs is modified for determining enabled transitions as follows:
Definition 14 (Enabled Transition in P N RI ). A transition t is enabled with respect
to marking M , enabledM (t), if all its input places p have at least the number of tokens
as the arc-weight W (p, t) and all p ∈ I(t) have zero tokens, i.e. (∀p ∈ •t : W (p, t) ≤
M (p)) ∧ (∀p ∈ I(t) : M (p) = 0)
We add inhibitor arcs to our encoding in Section 5 as follows:
f9: Rules iptarc(pi , tj , 1, tsk ):-time(tsk ). for each inhibitor arc between pi ∈ I(tj )
and tj .
e4: notenabled(T,TS) :- iptarc(P,T,N,TS), holds(P,Q,TS), place(P),
trans(T), time(TS), num(N), num(Q), Q >= N.

The new rule e4 encodes another reason for a transition to be disabled (or not enabled). An inhibitor arc from p to t with arc weight N will cause its target transition t
to not enable when the number of tokens at its source place p is greater than or equal to
N , where N is always 1 per rule f 9.
Proposition 4. There is 1-1 correspondence between the answer sets of Π3 (P N RI , M0 , k)
and the execution sequences of P N .

7

Extension - Read Arcs

Definition 15 (Read Arc). A read arc (a test arc or a query arc) [34] is an arc from
place to transition, which enables its transition only when its source place has at
least the number of tokens as its arc weight. It does not consume any tokens from
its input place. A Petri Net with reset, inhibitor and read arcs is a tuple P N RIQ =
(P, T, W, R, I, Q, QW ), where, P, T, E, W, R, I are the same as for P N RI ; Q ⊆
P × T defines read arcs; and QW : Q → N \ {0} defines read arc weight.
Figure 4 shows a Petri Net with read arc from h is to syn shown with arrowhead on
both ends. It models the ATP synthase syn activation requiring a higher concentration
of H+ ions h is in the intermembrane space 11 . The reaction itself consumes a lower
quantity of H+ ions represented by the regular place-transition arc [28,35]. Petri Net
execution semantics with read arcs is modified for determining enabled transitions as
follows:
Definition 16 (Enabled Transition in P N RIQ ). A transition t is enabled with respect
to marking M , enabledM (t), if all its input places p have at least the number of tokens
as the arc-weight W (p, t), all pi ∈ I(t) have zero tokens and all pq : (pq , t) ∈ Q have
at least the number of tokens as the arc-weight W (p, t), i.e. (∀p ∈ •t : W (p, t) ≤
M (p)) ∧ (∀p ∈ I(t) : M (p) = 0) ∧ (∀(p, t) ∈ Q : M (p) ≥ QW (p, t))
11

This is an oversimplified model of syn (ATP synthase) activation, since the actual model
requires an H+ concentration differential across membrane.

We add read arcs to our encoding of Section 6 as follows:
f10: Rules tptarc(pi , tj , QW (pi , tj ), tsk ):-time(tsk ). for each read arc (pi , tj ) ∈ Q.
e5: notenabled(T,TS):-tptarc(P,T,N,TS),holds(P,Q,TS),
place(P),trans(T), time(TS), num(N), num(Q), Q < N.

The new rule e5 encodes another reason for a transition to not be enabled. A read
arc from p to t with arc weight N will cause its target transition t to not enable when
the number of tokens at its source place p is less than the arc weight N .
Proposition 5. There is a 1-1 correspondence between the answer sets of Π4 (P N RIQ , M0 , k)
and the execution sequences of P N RIQ .

8

Example Use Case of Our Encoding and Additional Reasoning
Abilities

We illustrate the usefulness of our encoding by applying it to the following simulation
based reasoning question from [28]12 :
Question 1. “At one point in the process of glycolysis, both dihydroxyacetone phosphate (DHAP) and glyceraldehyde 3-phosphate (G3P) are produced. Isomerase catalyzes the reversible conversion between these two isomers. The conversion of DHAP
to G3P never reaches equilibrium and G3P is used in the next step of glycolysis. What
would happen to the rate of glycolysis if DHAP were removed from the process of
glycolysis as quickly as it was produced?”
In order to answer this question, we create a Petri Net model of sub-portion of the
normal glycolysis pathway relevant to the question (Figure 1) and encode it in ASP
using the encoding in Section 3. We then extend this pathway by adding a reset arc to it
for modeling the immediate removal of dhap (Figure 2) and encode it in ASP using the
encoding in Section 5. We simulate both models for the same number of time steps using
the maximal firing set semantics from Section 4, since we are interested in the maximum
change (in bpg13 production) between the two scenarios. Figure 5 shows the average
quantity of bpg13 produced by the normal and the extended Petri Net models for a
15-step simulation and Figure 6 shows the spread of unique bpg13 quantities produced
during these simulations. We compute the rate of glycolysis as the ratio “bpg13/ts”
at the end of the simulation. We post process these results to determine the average
rates as well as the spread among the answer-sets. Our results show a lower rate of
bpg13 production and hence glycolysis in the extended pathway, with a spread of bpg13
quantity ranging from zero to the same amount as the normal pathway. Although we
are using average rates to answer this question, the ASP encoding produces all possible
state evolutions, which may be further analyzed by assigning different probabilities to
these state evolutions.
ASP allows us to perform additional reasoning not directly supported by various
Petri Net formalisms examined by us 13 . For example, if we are given partial state
12
13

As it appeared in https://sites.google.com/site/2nddeepkrchallenge/
We examined a significant number of Petri Net formalisms

Average quantity of bpg13 produced

35

normal
extended

bpg13 quantity

30
25
20
15
10
5
0
0

2

4

6

8
Time

10

12

14

16

Fig. 5: Average quantity of bpg13 produced by the normal and extended Petri Net models in Fig. 1 and Fig. 2 for a simulation
length of 15.

Unique amount of bpg13 produced

information, we can use it in ASP as way-points to guide the simulation. Consider
that we want to determine the cause of a substance S’s quantity recovery after it is
depleted. Using ASP, we can enumerate only those answer-sets where a substance S
exhausts completely and then recovers by adding constraints. The answer sets produced
can then be passed on to an additional ASP reasoning step that determines the general
cause leading to the recovery of S after its depletion. This type of analysis is possible,
since the answer-sets enumerate the entire simulation evolution and all possible state
evolutions.
Spread of unique quantity of bpg13 produced

35

normal
extended

30
25
20
15
10
5
0
0

2

4

6
8
10
Unique run serial number

12

14

16

Fig. 6: Spread of the quantity of bpg13 produced by the normal and extended Petri Net models in Fig. 1 and Fig. 2 for a
simulation length of 15.

Although not a focus of this work, various Petri Net properties can be easily analyzed using our ASP encoding. For example dynamic properties can be analyzed as
follows: one can test reachability of a state (or marking) S by adding a constraint that
removes all answer-sets that do not contain S; boundedness by adding a constraint on
token count; basic liveness by extending the Petri Net model with source transitions

connected to all places, switching to the interleaved firing semantics, and removing
answers where the subject transition to be tested is not fired. Intuitively, structural properties can be analyzed as follows: T-invariants can be extracted by enumerating transitions (ti , . . . , tk−1 ) whenever marking Mk = Mi , k > i using an interleaved firing
semantics. P-invariants can be extracted by using interleaved firing semantics to visit
all possible state (or marking) evolutions,
selecting aP
subset of places Pi ⊆ P , and
P
eliminating all answer-sets where p∈Pi Mk+1 (p) 6= p∈Pi Mk (p), k ≥ 0.

9

Related Work and Conclusion

Here we will elaborate a bit on some of the existing Petri Net systems14 , (especially
ones used in biological modeling) and put them in context of our research. We follow
that with some ways existing Petri Net tools are used for biological analysis and put
them in the context of our research.
CPN Tools [8] is a graphical tool for simulating Colored Petri Nets with discrete
tokens. It supports guards, timed transitions, and hierarchical transitions but currently
does not have direct support for inhibit or reset arcs. It pursues one simulation evolution, breaking transition choice ties randomly. Cell Illustrator [36] is a closed source
Java based graphical tool for simulating biological systems using Hybrid Functional
Petri Nets (HPFNe). HPFNs combine features from Continuous as well as Discrete
Petri Nets. Cell Illustrator only supports uncolored tokens and pursues one simulation
evolution, breaking transition choice ties randomly. Snoopy [9] is a graphical tool written in C++ for analyzing biological models. It supports simulating Colored, Stochastic,
Hybrid, and Continuous Petri Nets with read, reset, and inhibit arcs under a few firing
semantics, including maximal firing. However, it does not explore all possible evolutions and it is unclear if simulation results can be exported for further reasoning. Renew [13] is an open source simulation tool written in Java. It supports Colored tokens,
(reference semantics of) Object tokens, guard conditions, inhibit arcs, reset arcs, timed
tokens, arc delays, and token reservation. Token nets can be created on the fly. Its use requires some knowledge of Java. Some features like arc pre-delays when combined with
transition delays can lead to complicated semantics. It is unclear how ties are broken to
resolve transition conflicts and whether all possible state evolutions are explored.
[3,37,38] have previously used Petri Nets to analyze biological pathways, but their
analysis is mostly limited to dynamic and structural properties. [39] surveyed various
Petri Net implementations to study the properties and dynamics of biological systems.
They defined a series of question that can be answered by Petri Nets using simulation.
Contrary to their approach, we picked our questions from college level text books to
capture real world questions. Our approach is also different in that we model the questions as Petri Net extensions and we can perform further reasoning on simulation runs.
Conclusion: In this paper we discussed the appropriateness of Petri Nets–especially
ASP implementation of Petri Nets, for modeling biological pathways and answering realistic questions (the ones found in biology textbooks) with respect to such pathways.
14

The Petri Net Tools Database web-site summarizes a large slice of existing tools
http://www.informatik.uni-hamburg.de/TGI/PetriNets/tools/quick.html

We presented how simple Petri Nets can be encoded in ASP and showed that ASP provides an elaboration tolerant way to easily realize various Petri Net extensions and firing
semantics. The extensions include changing of the firing semantics, and allowing reset
arcs, inhibitor arcs and read arcs. Our encoding has a low specification-implementation
gap. It allows enumeration of all possible evolutions of a Petri Net simulation as well
as the ability to carry out additional reasoning about these simulations. We also presented an example use case of our encoding scheme. Finally, we briefly discussed other
Petri Net systems and their use in biological modeling and analysis and compared them
with our work. Our focus in this paper has been less on performance and more on ease
of encoding, extensibility, exploring all possible state evolutions, and strong reasoning
abilities not supported by other Petri Net implementations examined. In a follow on
enhanced version of this paper, we will carry out detailed performance analysis. In a sequel of this paper, we will present extension of this work to other Petri Net extensions,
such as colored tokens, priority transitions, and durative transitions.

References
1. Carl Adam Petri. Kommunikation mit automaten. Technical report, Technical Report 2
(Schriften des IIM), Institut für Instrumentelle Mathematik, Bonn, Germany, 1962.
2. W.M.P. van der Aalst, KM Van Hee, and GJ Houben. Modelling and analysing workflow
using a petri-net based approach. In Proceedings of the second Workshop on ComputerSupported Cooperative Work, Petri nets and related formalisms, pages 31–50. of, 1994.
3. A Sackmann, M Heiner, and I Koch. Application of petri net based analysis techniques to
signal transduction pathways. BMC Bioinformatics, 2(7):482, November 2006.
4. L.A. Cortés, P. Eles, and Z. Peng. A petri net based model for heterogeneous embedded
systems. In Proc. NORCHIP Conference, pages 248–255. Citeseer, 1999.
5. R. Zurawski and MengChu Zhou. Petri nets and industrial applications: A tutorial. Industrial
Electronics, IEEE Transactions on, 41(6):567 –583, dec 1994.
6. W.M.P. Van Der Aalst. The application of petri nets to workflow management. Journal of
Circuits, Systems and Computers, 08(01):21–66, 1998.
7. V.N. Reddy, M.L. Mavrovouniotis, M.N. Liebman, et al. Petri net representations in
metabolic pathways. In Proc Int Conf Intell Syst Mol Biol, volume 1, page 96038982, 1993.
8. K. Jensen, L.M. Kristensen, and L. Wells. Coloured Petri Nets and CPN Tools for modelling and validation of concurrent systems. International Journal on Software Tools for
Technology Transfer (STTT), 9(3):213–254, 2007.
9. Monika Heiner, Mostafa Herajy, Fei Liu, Christian Rohr, and Martin Schwarick. Snoopy a unifying petri net tool. In Application and Theory of Petri Nets, volume 7347 of Lecture
Notes in Computer Science, pages 398–407, 2012.
10. S. Kounev, C. Dutz, and A. Buchmann. QPME-queueing petri net modeling environment. In
Quantitative Evaluation of Systems, 2006. QEST 2006. Third International Conference on,
pages 115–116. IEEE, 2006.
11. B. Berthomieu, P.O. Ribet, and F. Vernadat. The tool TINA–construction of abstract state
spaces for petri nets and time petri nets. International Journal of Production Research,
42(14):2741–2756, 2004.
12. M. Nagasaki, A. Saito, E. Jeong, C. Li, K. Kojima, E. Ikeda, and S. Miyano. Cell illustrator
4.0: A computational platform for systems biology. In Silico Biology, 10(1):5–26, 2010.
13. O. Kummer, F. Wienberg, and M. Duvigneau. Renew–the reference net workshop. Petri Net
Newsletter, 56:12–16, 1999.

14. M. Gebser, R. Kaminski, B. Kaufmann, M. Ostrowski, T. Schaub, and M. Schneider.
Potassco: The Potsdam answer set solving collection. aicom, 24(2):105–124, 2011.
15. F. Lin and Y. Zhao. Assat: Computing answer sets of a logic program by sat solvers. Artificial
Intelligence, 157(1):115–137, 2004.
16. Martin Gebser, Lengning Liu, Gayathri Namasivayam, André Neumann, Torsten Schaub,
and Mirosław Truszczyński. The first answer set programming system competition. In Logic
Programming and Nonmonotonic Reasoning, pages 3–17. Springer Berlin Heidelberg, 2007.
17. M. Nogueira, M. Balduccini, M. Gelfond, R. Watson, and M. Barry. An a-prolog decision
support system for the space shuttle. Practical Aspects of Declarative Languages, pages
169–183, 2001.
18. M. Balduccini. Job schedule generation, March 3 2009. US Patent App. 12/396,640.
19. C. Baral, M. Gelfond, and R. Scherl. Using answer set programming to answer complex
queries. In Workshop on Pragmatics of Question Answering at HLT-NAAC2004, 2004.
20. E. Erdem and R. Yeniterzi. Transforming controlled natural language biomedical queries
into answer set programs. In Proceedings of the Workshop on Current Trends in Biomedical
Natural Language Processing, pages 117–124. Association for Computational Linguistics,
2009.
21. S. Dworschak, S. Grell, V.J. Nikiforova, T. Schaub, and J. Selbig. Modeling biological
networks by action languages via answer set programming. Constraints, 13(1):21–65, 2008.
22. M. Gebser, A. König, T. Schaub, S. Thiele, and P. Veber. The BioASP library: ASP solutions for systems biology. In 22nd IEEE International Conference on Tools with Artificial
Intelligence (ICTAI’10), volume 1, pages 383–389, 2010.
23. W.M.P. van der Aalst. Pi calculus versus petri nets: Let us eat “humble pie” rather than
further inflate the “pi hype”. BPTrends, 3(5):1–11, 2005.
24. T. Murata. Petri nets: Properties, analysis and applications. Proceedings of the IEEE,
77(4):541–580, 1989.
25. Keijo Heljanko and Ilkka Niemelä. Petri net analysis and nonmonotomic reasoning. Leksa
Notes in Computer Science, pages 7–19, 2000.
26. Tristan M. Behrens and Jürgen Dix. Model checking with logic based petri nets. Technical report ifi-07-02, Insitut für Informatik, Technische Universität Clausthal Julius-Albert
Str. 4, 38678 Clausthal-Zellerfeld, Germany, Clausthal University of Technology, Dept of
Computer Science, May 2007.
27. Michael Gelfond and Vladimir Lifschitz. The stable model semantics for logic programming. Logic Programming: Proceedings of the Fifth International Conference and Symposium, pages 1070–1080, 1988.
28. J.B. Reece, M.L. Cain, L.A. Urry, P.V. Minorsky, and S.A. Wasserman. Campbell Biology.
Pearson Benjamin Cummings, 2010.
29. Hans-Dieter Burkhard. Ordered firing in petri nets. Journal of Information Processing and
Cybernetics, (17):71–86, 1980.
30. Elzbieta Krepska, Nicola Bonzanni, Anton Feenstra, Wan Fokkink, Thilo Kielmann, Henri
Bal, and Jaap Heringa. Design issues for qualitative modelling of biological cells with petri
nets. Formal Methods in Systems Biology, pages 48–62, 2008.
31. Toshiro Araki and Tadao Kasami. Some decision problems related to the reachability problem for petri nets. Theoretical Computer Science, 3(1):85–104, 1976.
32. Rüdiger Valk. Self-modifying nets, a natural extension of petri nets. In Automata, Languages and Programming, volume 62 of Lecture Notes in Computer Science, pages 464–476.
Springer, 1978.
33. James L. Peterson. Petri nets. Computing Surveys, 9(3):223–252, September 1977.
34. Søren Christensen and Niels Damgaard Hansen. Coloured Petri nets extended with place
capacities, test arcs and inhibitor arcs. Springer, 1993.

35. Jeremy M Berg, John L Tymoczko, and Lubert Stryer. A proton gradient powers the synthesis
of atp. 2002.
36. Masao Nagasaki, Ayumu Saito, Atsushi Doi, Hiroshi Matsuno, and Satoru Miyano. Foundations of Systems Biology: Using Cell Illustrator and Pathway Databases. Computational
Biology. Springer, 2009.
37. R. Hofestädt and S. Thelen. Qualitative modeling of biochemical networks. In Silico Biology,
1:39–53, 1998.
38. Chen Li, Shunichi Suzuki, Qi-Wei Ge, Mitsuru Nakata, Hiroshi Matsuno, and Satoru
Miyano. Structural modeling and analysis of signaling pathways based on petri nets. Journal
of bioinformatics and computational biology, 4(05):1119–1140, 2006.
39. Mor Peleg, Daniel Rubin, and Russ B Altman. Using petri net tools to study properties and
dynamics of biological systems. Journal of the American Medical Informatics Association,
12(2):181–199, 2005.

An Action Language for Multi-Agent Domains: Foundations

arXiv:1511.01960v1 [cs.AI] 6 Nov 2015

Chitta Baral, Gregory Gelfond
School of Computing, Informatics & DSE
Arizona State University
{chitta,ggelfond}@asu.edu

Enrico Pontelli, Tran Cao Son
Department of Computer Science
New Mexico State University
{epontell,tson}@cs.nmsu.edu

Abstract
In multi-agent domains, an agent’s action may not just change the world and the agent’s knowledge and beliefs
about the world, but also may change other agents’ knowledge and beliefs about the world and their knowledge and
beliefs about other agents’ knowledge and beliefs about the world. Similarly, the goals of an agent in a multi-agent
world may involve manipulating the knowledge and beliefs of other agents’ and again, not just their knowledge1
about the world, but also their knowledge about other agents’ knowledge about the world. The goal of this paper is to
present an action language, called mA+, that has the necessary features to address the above aspects in representing
and reasoning about actions and change in multi-agent domains.
This action language can be viewed as a generalization of the single-agent action languages extensively studied
in the literature, to the case of multi-agent domains. The language allows the representation of and reasoning about
different types of actions that an agent can perform in a domain where many other agents might be present—such as
world-altering actions, sensing actions, and announcement/communication actions. The action language also allows
the specification of agents’ dynamic awareness of action occurrences which has future implications on what agents’
know about the world and other agents’ knowledge about the world. The language mA+ considers three different
types of awareness: full awareness, partial awareness, and complete oblivion of an action occurrence and its effects.
This keeps the language simple, yet powerful enough to address a large variety of knowledge manipulation scenarios
in multi-agent domains.
The semantics of the language relies on the notion of state, which is described by a pointed Kripke model and
is used to encode the agent’s knowledge and the real state of the world. The semantics is defined by a transition
function that maps pairs of actions and states into sets of states. The paper illustrates properties of the action theories,
including properties that guarantee finiteness of the set of initial states and their practical implementability. Finally,
the paper relates mA+ to other related formalisms that contribute to reasoning about actions in multi-agent domains.

Keywords: Action Languages, Multi-Agent Domains, Planning.

1

Introduction and Motivation

Reasoning about actions and change has been a research focus since the early days of artificial intelligence [1]; languages for representing actions and their effects were proposed soon after [2]. Although the early papers on this [2]
did not include formal semantics, papers with formal semantics came some years later [3]. The approach adopted in
this paper is predominantly influenced by the methodology for representing and reasoning about actions and change
(RAC) proposed by Gelfond and Lifschitz [4]. In this approach, actions of agents are described in a high-level language, with an English-like syntax and a transition function based semantics. Among other things, action languages
provide a succinct way for representing dynamic domains. The approach proposed in this paper is also related to
action description languages developed for planning, such as [5, 6].
Over the years, several action languages (e.g., A, B, and C) have been developed [7]. Each of these languages addresses some important problems in RAC (e.g., the ramification problem, concurrency, actions with duration, knowledge of agents). Action languages have also provided the foundations for several successful approaches to automated
1 Often,

we will simply say “knowledge” to mean both “knowledge” and “beliefs.” This will be clear from the context.

1

planning; for example, the language C is used in the planner C-P LAN [8] and the language B is used in C PA [9].
However, the main focus of these research efforts has been about reasoning within single agent domains.
In single agent domains, reasoning about actions and change mainly involves reasoning about what is true in
the world, what the agent knows about the world, how the agent can manipulate the world (using world-changing
actions) to reach particular states, and how the agent (using sensing actions) can learn unknown aspects of the world.
In multi-agent domains an agent’s action may not just change the world and the agent’s knowledge about the world,
but also may change other agents’ knowledge about the world and their knowledge about other agents’ knowledge
about the world. Similarly, goals of an agent in a multi-agent world may involve manipulating the knowledge of
other agents—in particular, this may involve not just their knowledge about the world, but also their knowledge
about other agents’ knowledge about the world. Although there is a large body of research on multi-agent planning
[10, 11, 12, 13, 14, 15, 16, 17, 18], very few efforts address the above aspects of multi-agent domains which pose a
number of new research challenges in representing and reasoning about actions and change. The following example
illustrates some of these issues.
Example 1 (Three Agents and the Coin Box) Three agents, A, B, and C, are in a room. In the middle of the room
there is a box containing a coin. It is common knowledge that:
• None of the agents knows whether the coin lies heads or tails up;
• The box is locked and one needs to have a key to open it; only agent A has the key of the box;
• In order to learn whether the coin lies heads or tails up, an agent can peek into the box. This, however, requires
the box to be open;
• If an agent is looking at the box and someone peeks into the box, then the first agent will notice this fact and
will be able to conclude that the second agent knows the status of the coin; yet, the first agent’s knowledge about
which face of the coin is up does not change;
• Distracting an agent causes that agent to not look at the box;
• Signaling an agent to look at the box causes this agent to look at the box;
• Announcing that the coin lies heads or tails up will make this a common knowledge among the agents that are
listening.
Suppose that the agent A would like to know whether the coin lies heads or tails up. She would also like to let the
agent B know that she knows this fact. However, she would like to keep this information secret from C. (Note that
the last two sentences express goals that are about agents’ knowledge about other agents’ knowledge.) Intuitively, she
could achieve her goal by:
1. Distracting C from looking at the box;
2. Signaling B to look at the box;
3. Opening the box; and
2

4. Peeking into the box.

This simple story presents a number of challenges for research in representing and reasoning about actions and their
effects in multi-agent domains. In particular:
• The domain contains several types of actions:
– Actions that allow the agents to change the state of the world (e.g., opening the box);
– Actions that change the knowledge of the agents (e.g, peeking into the box, announcing head/tail);
– Actions that manipulate the beliefs of other agents (e.g., peeking while other agents are looking); and
– Actions that change the observability of agents with respect to awareness about future actions (e.g., distract
and signal actions before peeking into the box).
We observe that the third and fourth types of actions are not considered in single agent systems.

2

• The reasoning process that helps A to realize that steps (1)-(4) will achieve her goal requires A’s ability to reason
about the effects of her actions on several entities:
– The state of the world—e.g., opening the box causes the box to become open;
– The agents’ awareness of the environment and of other agents’ actions—e.g., distracting or signaling an
agent causes this agent not to look or to look at the box; and
– The knowledge of other agents about her own knowledge—e.g., someone following her actions would
know what she knows.
While the first requirement is the same as for an agent in single agent domains, the last two are specific to
multi-agent domains.
With respect to planning, the above specifics of multi-agent systems raise an interesting question:
“How can one generate a plan for the agent A to achieve her goal, given the description in Example 1?”
To the best of our knowledge, except a dynamic epistemic modeling system called DEMO [19], there exists no
automated planner that can address the complete range of issues as exemplified in Example 1. This is in stark contrast
to the landscape of automated planning for single agent domains, where we can find several automated planners
capable of generating plans consisting of hundreds of actions within seconds—especially building on recent advances
in search-based planning.
Among the main reasons for the lack of planning systems capable of dealing with the issues like those shown in
Example 1 are: (i) lack of action-based formalisms that can address the above mentioned issues and that can actually be
orchestrated, and (ii) the fact that logical approaches to reasoning about knowledge of agents in multi-agent domains
are mostly model-theoretical, and not as amenable to an implementation in search-based planning systems. This is
not the case for single-agent domains, where the de-facto standard Planning Domain Description Language (PDDL)
provides a transition-based semantics which allows the creation of highly efficient heuristic search based planners.
In terms of related work, multi-agent actions have been explored in Dynamic Epistemic Logics (DEL) (e.g., [20,
21, 22, 23, 24]). However, as discussed later in the paper, DEL does not offer an intuitive view of how to orchestrate
or execute a single multi-agent action. In addition, the complex representation of multi-agent actions (akin to Kripke
structures) drastically increases the number of possible multi-agent actions—thus, making it difficult to search for
multi-agent action sequences. The research in DEL has also not addressed some critical aspects of multi-agent searchbased planning, such as the determination of the initial state of a planning domain instance. Moreover, research in
DEL did not explore the link between the state of the world and the observability encoded in multi-agent actions, and
hence preventing the dynamic evolution of the observational capabilities and awareness of agents with respect to future
actions. In some ways, the DEL approach is similar to the formulations of belief updates (e.g., [25, 26, 27]), and most
of the differences and similarities between belief updates and reasoning about actions carry over to the differences and
similarities between DEL and our formulation of RAC in multi-agent domains. We will elaborate on these differences
in a later section of the paper.

1.1

Contributions and Assumptions

Our goal in this paper is to develop a framework that allows reasoning about actions and their effects in a multi-agent
domain; the framework is expected to address the above-mentioned issues, e.g., actions’ capability to modify agents’
knowledge about other agents’ knowledge. To this end, we propose a high-level action language for representing and
reasoning about actions in multi-agent domains. The language provides an initial set of elements of a planning domain
description language for multi-agent systems. The main contributions of the paper are:
• The action language mA+, which considers different types of actions—such as world-altering actions, announcement actions, and sensing actions—for formalizing multi-agent domains, along with actions that affect
the dynamic awareness and observation capabilities of the agents;
• A transition function based semantics for mA+, that enables hypothetical reasoning and planning in multi-agent
domains;
3

• The notion of definite action theories, that characterizes action theories for which the computation of the initial
state is possible—which is an essential component in the implementation of a heuristic search-based planner for
domains described in mA+; and
• Several theorems relating the semantics of mA+ to multi-agent actions characterizations using update models
in DEL of [20].
In developing mA+, we make several assumptions and design decisions. The key decision is that actions in our
formalism can be effectively executed.2 We also assume that actions are deterministic. This assumption can be lifted
in a relatively simple manner by generalizing the treatment of non-deterministic actions, state constraints, and parallel
actions studied in the context of single-agent domains.
Next, although we have mentioned both knowledge and beliefs, in this paper we will follow [24, 20] and focus only
on formalizing the changes of beliefs of agents after the execution of actions. Following the considerations in [22], the
epistemic operators used in this paper can be read as “to the best my information.” Note that, in a multi-agent system,
there may be a need to distinguish between knowledge and beliefs of an agent about the world. For example, let us
consider Example 1 and let us denote with p the proposition “nobody knows whether the coin lies heads or tails up.”
Initially, all of the three agents know that p is true. However, after agent A executes the sequence of actions (1)-(4), A
will know that p is false. Furthermore, B also knows that p is false, thanks to her awareness of A’s execution of the
actions of opening the box and looking into it. However, C—being unaware of the execution of the actions performed
by A—will still believe that p is true. If this were considered as a part of C’s knowledge, then C would result in having
false knowledge.
The investigation of the discrepancy between knowledge and beliefs has been an intense research topic in dynamic
epistemic logic and in reasoning about knowledge, which has lead to the development of several modal logics (e.g.,
[28, 24]). Since our main aim is the development of an action language for hypothetical reasoning and planning, we
will be concerned mainly with beliefs of agents. A consequence of this choice is that we will not consider situations
where sensing actions, or other knowledge acquisition actions, are employed to correct a wrong belief of agents. For
example, an agent will not be able to use a sensing action to realize that her beliefs about a certain property of the
world is wrong; or an agent will not be able to use an announcement action to correct an incorrect belief of other
agents. Some preliminary steps in this direction have been explored in the context of the DEL framework [21, 29]. We
leave the development of an action based formalism that takes into consideration the differences between beliefs and
knowledge as future work.

1.2

Paper Organization

The rest of the paper is organized as follows. Section 2 reviews the basics definitions and notation of a modal logic
with belief operators and the update model based approach to reasoning about actions in multi-agent domains. This
section also describes a set of operators on Kripke structures, that will be used in defining the semantics of mA+.
Section 3 presents the syntax of mA+. Section 4 defines the transition function of mA+ which maps pairs of actions
and states into states; the section also presents the entailment relation between mA+ action theories and queries.
Section 5 explores the modeling of the semantics of mA+ using the update models approach. Section 6 identifies a
class of mA+ theories whose initial states can be finitely characterized and effectively computed. Section 7 provides
an analysis of mA+ with respect to the existing literature, including a comparison with DEL. Section 8 provide some
concluding remarks and directions for future work.
For simplicity of presentation, the proofs of the main lemmas, propositions, and theorems are placed in the appendix.

2

Preliminaries

We begin with a review of the basic notions from the literature on formalizing knowledge and reasoning about effects
of actions in multi-agent systems. Subsection 2.1 presents the notion of Kripke structures. Subsection 2.2 reviews the
2 This is not the case in DEL [24], where actions are complex graph structures, similar to Kripke structures, possibly representing a multi-modal
formula, and it is not clear if and how such actions can be executed.

4

notion of update models developed by the dynamic epistemic logic community for reasoning about effects of actions in
multi-agent systems. In Subsection 2.3, we investigate a collection of basic operators for modifying Kripke structures,
that will be used in our semantic formalization in the following sections.

2.1

Belief Formulae and Kripke Structures

Let us consider an environment with a set AG of n agents. The real state of the world (or real state, for brevity) is
described by a set F of propositional variables, called fluents. We are concerned with the beliefs of agents about the
environment and about the beliefs of other agents. For this purpose, we adapt the logic of knowledge and the notations
used in [28, 24]. We associate to each agent i ∈ AG a modal operator Bi and represent the beliefs of an agent as belief
formulae in a logic extended with these operators. Formally, we define belief formulae as follows:
• Fluent formulae: a fluent formula is a propositional formula built using the propositional variables in F and the
traditional propositional operators ∨, ∧, →, ¬, etc. In particular, a fluent atom is a formula containing just an
element f ∈ F, while a fluent literal is either a fluent atom f ∈ F or its negation ¬f . We will use > and ⊥ to
denote true and false, respectively.
• Belief formulae: a belief formula is a formula in one of the following forms:
– a fluent formula;
– a formula of the form Bi ϕ where ϕ is a belief formula;
– a formula of the form ϕ1 ∨ ϕ2 , ϕ1 ∧ ϕ2 , ϕ1 ⇒ ϕ2 , or ¬ϕ1 where ϕ1 , ϕ2 are belief formulae;
– a formula of the form Eα ϕ or Cα ϕ, where ϕ is a formula and ∅ =
6 α ⊆ AG.
Formulae of the form Eα ϕ and Cα ϕ are referred to as group formulae. Whenever α = AG, we simply write Eϕ and
Cϕ to denote Eα ϕ and Cα ϕ, respectively. Let us denote with LAG the language of the belief formulae over F and
AG.
Intuitively, belief formulae are used to describe the beliefs of one agent concerning the state of the world as well
as about the beliefs of other agents. For example, the formula B1 B2 p expresses the fact that “Agent 1 believes that
agent 2 believes that p is true,” while B1 f states that “Agent 1 believes that f is true.”
In what follows, we will simply talk about “formulae” instead of “belief formulae,” whenever there is no risk of
confusion. The notion of a Kripke structure is defined next.
Definition 1 (Kripke Structure) A Kripke structure is a tuple hS, π, B1 , . . . , Bn i, where
• S is a set of worlds,
• π : S 7→ 2F is a function that associates an interpretation of F to each element of S, and
• For 1 ≤ i ≤ n, Bi ⊆ S × S is a binary relation over S.
A pointed Kripke structure is a pair (M, s) where M = hS, π, B1 , . . . , Bn i is a Kripke structure and s ∈ S. In a
pointed Kripke structure (M, s), we refer to s as the real (or actual) world.
Intuitively, a Kripke structure describes the possible worlds envisioned by the agents—and the presence of multiple
worlds identifies uncertainty and presence of different beliefs. The relation (s1 , s2 ) ∈ Bi denotes that the belief of
agent i about the real state of the world is insufficient for her to distinguish between the world described by s1 and the
one described by s2 . The world s in the state (M, s) identifies the world in M [S] that corresponds to the actual world.
We will often view a Kripke structure M = hS, π, B1 , . . . , Bn i as a directed labeled graph, whose set of nodes is
S and whose set of edges contains (s, i, t)3 if and only if (s, t) ∈ Bi . (s, i, t) is referred to as an edge coming out of
(resp. into) the world s (resp. t).
3 (s, i, t)

denotes the edge from node s to node t, labeled by i.

5

For the sake of readability, we use M [S], M [π], and M [i] to denote the components S, π, and Bi of M , respectively. We write M [π](u) to denote the interpretation associated to u via π and M [π](u)(ϕ) to denote the truth value
of a fluent formula ϕ with respect to the interpretation M [π](u).
Following [24], we will refer to a pointed Kripke structure (M, s) as a state and often use these two terms interchangeably.
In keeping with the tradition of action languages, we will often refer to π(u) as the set of fluent literals defined by
{f | f ∈ F, M [π](u)(f ) = >} ∪ {¬f | f ∈ F, M [π](u)(f ) = ⊥}.
Given a consistent and complete set of literals S, i.e., |{f, ¬f } ∩ S| = 1 for every f ∈ F, we write π(u) = S or
M [π](u) = S to indicate that the interpretation M [π](u) is defined in such a way that π(u) = S.
The satisfaction relation between belief formulae and a state is defined as next.
Definition 2 Given a formula ϕ, a Kripke structure M = hS, π, B1 , . . . , Bn i, and a world s ∈ S:
(i) (M, s) |= ϕ if ϕ is a fluent formula and π(s) |= ϕ;
(ii) (M, s) |= Bi ϕ if for each t such that (s, t) ∈ Bi , (M, t) |= ϕ;
(iii) (M, s) |= ¬ϕ if (M, s) 6|= ϕ;
(iv) (M, s) |= ϕ1 ∨ ϕ2 if (M, s) |= ϕ1 or (M, s) |= ϕ2 ;
(v) (M, s) |= ϕ1 ∧ ϕ2 if (M, s) |= ϕ1 and (M, s) |= ϕ2 .
(vi) (M, s) |= Eα ϕ if (M, s) |= Bi ϕ for every i ∈ α.
(vii) (M, s) |= Cα ϕ if (M, s) |= Eαk ϕ for every k ≥ 0, where Eα0 ϕ = ϕ and Eαk+1 = Eα (Eαk ϕ).
For a Kripke structure M and a formula ϕ, M |= ϕ denotes the fact that (M, s) |= ϕ for each s ∈ M [S], while |= ϕ
denotes the fact that M |= ϕ for every Kripke structure M .
Example 2 (State) Let us consider a simplified version of Example 1 in which the agents are concerned only with
the status of the coin. The three agents A, B, C do not know whether the coin has ‘heads’ or ‘tails’ up and this is a
common belief. Let us assume that the coin is heads up. The beliefs of the agents about the world and about the beliefs
of other agents can be captured by the state of Figure 1.

S1:
¬tail

A, B, C

S2:
tail

A, B, C

A, B, C

Figure 1: An example of a state
In the figure, a circle represents a world. The name and interpretation of the world are written in the circle.
Labeled edges between worlds denote the belief relations of the structure. A double circle identifies the real world.
We will occasionally be interested in Kripke structures that satisfy certain conditions. In particular, given a Kripke
structure M = hS, π, B1 , . . . , Bn i we identify the following properties:
• K: for each agent i and formulae ϕ, ψ, we have that M |= (Bi ϕ ∧ Bi (ϕ ⇒ ψ)) ⇒ Bi ψ.
• T: for each agent i and formula ψ, we have that M |= Bi ψ ⇒ ψ.
6

• 4: for each agent i and formula ψ, we have that M |= Bi ψ ⇒ Bi Bi ψ.
• 5: for each agent i and formula ψ, we have that M |= ¬Bi ψ ⇒ Bi ¬Bi ψ.
• D: for each agent i we have that M |= ¬Bi ⊥.
A Kripke structure is said to be a T-Kripke (4-Kripke, K-Kripke, 5-Krikpe, D-Kripke, respectively) structure if it
satisfies property T (4, K, 5, D, respectively). A Kripke structure is said to be a S5 structure if it satisfies the properties
K, T, 4, and 5. Consistency of a set of formulae is defined next.
Definition 3 A set of belief formulae X is said to be p-satisfiable (or p-consistent) for p ∈ {S5, K, T, 4, 5} if there
exists a p-Kripke structure M and a world s ∈ M [S] such that (M, s) |= ψ for every ψ ∈ X. In this case, (M, s) is
referred to as a p-model of X.
Finally, let us introduce a notion of equivalence between states.
Definition 4 A state (M, s) is equivalent to a state (M 0 , s0 ) if (M, s) |= ϕ iff (M 0 , s0 ) |= ϕ for every formula
ϕ ∈ LAG .

2.2

Update Models

Update models are used to describe transformations of (pointed) Kripke structures according to a predetermined transformation pattern. An update model uses structures similar to pointed Kripke structures and they describe the effects
of a transformation on states using an update operator [20, 23].
Let us start with some preliminary definitions. An LAG -substitution is a set {p1 → ϕ1 , . . . , pk → ϕk }, where
each pi is a distinct fluent in F and each ϕi ∈ LAG . We will implicitly assume that for each p ∈ F \ {p1 , . . . , pk },
the substitution contains p → p. SU BLAG denotes the set of all LAG -substitutions.
Definition 5 (Update Model) Given a set AG of n agents, an update model Σ is a tuple hΣ, R1 , . . . , Rn , pre, subi
where
(i) Σ is a set, whose elements are called events;
(ii) each Ri is a binary relation on Σ;
(iii) pre : Σ → LAG is a function mapping each event e ∈ Σ to a formula in LAG ; and
(iv) sub : Σ → SU BLAG is a function mapping each event e ∈ Σ to a substitution in SU BLAG .
An update instance ω is a pair (Σ, e) where Σ is an update model hΣ, R1 , . . . , Rn , pre, subi and e, referred to as a
designated event, is a member of Σ.
Intuitively, an update model represents different views of an action occurrence which are associated with the observability of agents. Each view is represented by an event in Σ. The designated event is the one that agents who are
aware of the action occurrence will observe. The relation Ri describes agent i’s uncertainty on action execution—i.e.,
if (σ, τ ) ∈ Ri and event σ is performed, then agent i may believe that event τ is executed instead. pre defines the
action precondition and sub specifies the changes of fluent values after the execution of an action.
Definition 6 (Updates by an Update Model) Let M be a Kripke structure and Σ = hΣ, R1 , . . . , Rn , pre, subi be an
update model. The update operator induced by Σ defines a Kripke structure M 0 = M ⊗ Σ, where:
(i) M 0 [S] = {(s, τ ) | s ∈ M [S], τ ∈ Σ, (M, s) |= pre(τ )};
(ii) ((s, τ ), (s0 , τ 0 )) ∈ M 0 [i] iff (s, τ ), (s0 , τ 0 ) ∈ M 0 [S], (s, s0 ) ∈ M [i] and (τ, τ 0 ) ∈ Ri ;
(iii) For all f ∈ F, M 0 [π]((s, τ )) |= f iff f → ϕ ∈ sub(τ ) and (M, s)|=ϕ.

7

The structure M 0 is obtained from the component-wise cross-product of the old structure M and the update model
Σ, by (i) removing pairs (s, τ ) such that (M, s) does not satisfy the action precondition (checking for satisfaction of
action’s precondition), and (ii) removing links of the form ((s, τ ), (s0 , τ 0 )) from the cross product of M [i] and Ri if
(s, s0 ) 6∈ M [i] or (τ, τ 0 ) 6∈ Ri (ensuring that each agent’s accessibility relation is updated according to the update
model).
An update template is a pair (Σ, Γ) where Σ is an update model with the set of events Σ and Γ ⊆ Σ. The update
of a state (M, s) given an update template (Σ, Γ) is a set of states, denoted by (M, s) ⊗ (Σ, Γ), where
(M, s) ⊗ (Σ, Γ) = {(M ⊗ Σ, (s, τ )) | τ ∈ Γ, (M, s) |= pre(τ )}
Remark 1 In the following, we will often represent an update instance by a graph with rectangles representing events,
double rectangles designated events, and labeled links between rectangles representing the relation of agents as in the
graphical representation of a Kripke model.

2.3

Kripke Structures Operators

The execution of an action in a multi-agent environment will change the real state of the world and/or the beliefs of
agents. As we will see later, we will employ the notion of a pointed Kripke structure in representing the real state of
the world and the beliefs of agents. As such, the execution of an action in a pointed Kripke structure will modify it.
Such a change can be a combination of the following basic changes:4
• Removing some worlds and/or edges from the structure;
• Adding some worlds and/or edges to the structure; or
• Replicating a structure or merging different structures into one.
In this subsection, we describe some basic operators on Kripke structures that will satisfy the above needs. We will
assume that a Kripke structure M is given. The first operator on Kripke structures is to remove a set of worlds from
M.
s

Definition 7 (World Subtraction) Given a set of worlds U ⊆ M [S], M 	 U is a Kripke structure M 0 defined by:
(i) M 0 [S] = M [S] \ U ;
(ii) M 0 [π](s)(f ) = M [π](s)(f ) for every s ∈ M 0 [S] and f ∈ F; and
(iii) M 0 [i] = M [i] \ {(t, v) | (t, v) ∈ M [i], {t, v} ∩ U 6= ∅}.
s

Intuitively, M 	 U is the Kripke structure obtained from M by removing the worlds in U and all edges connected to
these worlds. The next two operators are used to remove edges from or introduce edges into a given structure.
a

Definition 8 (Edge Subtraction) For a set of edges X in M , M 	 X is a Kripke structure, M 0 , defined by:
(i) M 0 [S] = M [S];
(ii) M 0 [π] = M [π]; and
(iii) M 0 [i] = M [i] \ {(u, i, v) | (u, i, v) ∈ X} for every i ∈ AG.
a

M 	 X is the Kripke structure obtained from M by removing from M all the edges in X. Given a state (M, s) and a
a

set of agents α ⊆ AG, we denote with (M |α , s) the state (M 	 X, s) where
[
X=
{(u, i, v) | (u, v) ∈ M [i]}.
i∈AG\α
a

Definition 9 (Edge Addition) For a set of edges X in M , M ⊕ X is a Kripke structure, M 0 , defined by:
4 We

will focus on viewing a (pointed) Kripke structure as a labeled graph.

8

(i) M 0 [S] = M [S];
(ii) M 0 [π] = M [π]; and
(iii) M 0 [i] = M [i] ∪ {(u, v) | (u, i, v) ∈ X} for every i ∈ AG.
a

a

While 	 removes edges from the structure, ⊕ is used to add edges to an existing structure. Recall that the nodes/edges
in a Kripke structure indicate uncertainty in the agents’ beliefs. As such, these operators are useful for updating the
beliefs of the agents in a domain when the beliefs of the agents change.
Definition 10 (Equivalence Modulo c) Given two Kripke structures M1 and M2 , we say that M1 is c-equivalent5 to
c
M2 (denoted as M1 ∼ M2 ) if there exists a bijective function c : M1 [S] → M2 [S], such that for every u ∈ M1 [S] and
f ∈ F, we have that:
1. M1 [π](u)(f ) = > if and only if M2 [π](c(u))(f ) = >; and
2. for every i ∈ AG and u, v ∈ M1 [S], (u, v) ∈ M1 [i] if and only if (c(u), c(v)) ∈ M2 [i].
The execution of several types of actions leads to the creation of “duplicates” of an original Kripke structure—e.g., to
represent how different groups of agents maintain different perspectives after the execution of an action.
Definition 11 (Replica) Given a Kripke structure M , a c-replica of M is a Kripke structure M 0 such that c is a
c
bijection between M [S] and M 0 [S], M [S] ∩ M 0 [S] = ∅, and M 0 ∼ M .
0 0
Given a state (M, s), a c-replica of (M, s) is a state (M , s ) where M 0 is a c-replica of M and s0 = c(s).
We say that M1 and M2 are compatible if for every s ∈ M1 [S] ∩ M2 [S] and every f ∈ F, M1 [π](s)(f ) =
M2 [π](s)(f ).
κ

Definition 12 (Knowledge Updates) For two compatible Kripke structures M1 and M2 , we define M1 ∪ M2 as the
Kripke structure, M 0 , where:
(i) M 0 [S] = M1 [S] ∪ M2 [S];
(ii) M 0 [π](s) = M1 [π](s) for s ∈ M1 [S] and M 0 [π](s) = M2 [π](s) for s ∈ M2 [S] \ M1 [S]; and
(iii) M 0 [i] = M1 [i] ∪ M2 [i].
For a pair of Kripke structures M1 and M2 such that M1 [S] ∩ M2 (S) = ∅, |M1 [S]| = |M2 (S)|, α ⊆ AG, and a
one-to-one function λ : M2 [S] → M1 [S], we define M1 ]λα M2 as the Kripke structure, M 0 , where:
(i) M 0 [S] = M1 [S] ∪ M2 [S];
(ii) M 0 [π](s) = M1 [π](s) for s ∈ M1 [S] and M 0 [π](s) = M2 [π](s) for s ∈ M2 [S]; and
(iii) for each i ∈ α, we have that M 0 [i] = M1 [i] ∪ M2 [i], and for each i ∈ AG \ α we have that
M 0 [i] = M1 [i] ∪ M2 [i] ∪ {(u, v) | u ∈ M2 [S], v ∈ M1 [S], (λ(u), v) ∈ M1 [i]}.
Given two states (M1 , s1 ) and (M2 , s2 ) such that M1 [S]∩M2 [S] = ∅, |M1 [S]| = |M2 (S)|, α ⊆ AG, and a one-to-one
function λ : M2 [S] → M1 [S]:
(M1 , s1 ) ]λα (M2 , s2 ) = (M1 ]λα M2 , s2 ).
κ

Intuitively, the operators ∪ and ]λα allow us to combine different Kripke structures representing the beliefs of different
κ

groups of agents, thereby creating a structure representing the beliefs of “all agents.” The ∪ operation effectively
unions two Kripke structures (that might have already some worlds in common). The ]λα also combines two Kripke
structures albeit in a more specific way. It relies on a function λ to create additional edges between the two structures
for agents in AG \ α.
5 Similar

to the notion of bisimulation in [20].

9

The language mA+: Syntax

3

In this paper, we consider multi-agent domains in which the agents are truthful and no false information may be
announced or observed. Furthermore, the underlying assumptions guiding the semantics of our language are the
rationality principle and the idea that beliefs of an agent are inertial. In other words, agents believe nothing which they
are not forced to believe, and the beliefs of an agent remain the same unless something causes them to change.
In this section and the next section, we introduce the language mA+ for describing actions and their effects
in multi-agent environment. The language builds over a signature hAG, F, Ai, where AG is a finite set of agent
identifiers, F is a set of fluents, and A is a set of actions. Each action in A is an action the agents in the domain are
capable of performing.
Similar to any action language developed for single-agent environments, mA+ consists of three components which
will be used in describing the actions and their effects, the initial state, and the query language (see, e.g., [7]). We will
next present each of these components. Before we do so, let us denote the multi-agent domain in Example 1 by D1 .
For this domain, we have that AG = {A, B, C}. The set of fluents F for this domain consists of:
• tail: the coin lies tails up (head is often used in place of ¬tail);
• has key(X): agent X has the key of the box;
• opened: the box is open; and
• looking(X): agent X is looking at the box.
The set of actions for D1 consists of:
• open(X): agent X opens the box;
• peek(X): agent X peeks into the box;
• signal(X, Y ): agent X signals agent Y (to look at the box);
• distract(X, Y ): agent X distracts agent Y (so that Y does not look at the box); and
• shout tail(X): agent X announces that the coin lies tail up.
where X, Y ∈ {A, B, C} and X 6= Y . We start with the description of actions and their effects.

3.1

Actions and effects

We envision three types of actions that an agent can perform: world-altering actions (also known as ontic actions),
sensing actions, and announcement actions. Intuitively,
• A world-altering action is used to explicitly modify certain properties of the world—e.g., the agent A opens the
box in Example 1, or the agent A distracts the agent B so that B does not look at the box (also in Example 1);
• A sensing action is used by an agent to refine its beliefs about the world, by making direct observations—e.g.,
an agent peeks into the box; the effect of the sensing action is to reduce the amount of uncertainty of the agent;
• An announcement action is used by an agent to affect the beliefs of the agents receiving the communication—we
operate under the assumption that agents receiving an announcement always believe what is being announced.
For the sake of simplicity, we assume that each action a ∈ A falls in exactly one of the three categories.6
In general, an action can be executed only under certain conditions, called its executability conditions. For example,
the statement “to open the box, an agent must have its key” in Example 1 describes the executability condition of the
action of opening a box. The first type of statements in mA+ is used to describe the executability conditions of actions
and is of the following form:
6 It

is easy to relax this condition, but it would make the presentation more tedious.

10

executable a if ψ

(1)

where a ∈ A and ψ is a belief formula. A statement of type (1) will be referred to as the executability condition of
a. ψ is referred as the precondition of a. For simplicity of the representation, we will assume that each action a is
associated with exactly one executability condition. When ψ = >, the statement will be omitted.
For a world-altering action a, such as the action of opening the box, we have statements of the following type that
express the change that may be caused by such actions:
a causes ` if ψ

(2)

where ` is a fluent literal and ψ is a belief formula. Intuitively, if the real state of the world and of the beliefs match
the condition described by ψ, then the real state of the world is affected by the change that makes the literal ` true after
the execution of a. When ψ = >, the part “ if ψ” will be omitted from (2). We also use
a causes Φ if ψ
where Φ is a set of fluent literals as a shorthand for the set {a causes ` if ψ | ` ∈ Φ}.
Sensing actions, such as the action of looking into the box, allow agents to learn about the value of a fluent in the
real state of the world (e.g., learn whether the coin lies head or tail up). We use statements of the following kind to
represent sensing actions:
a determines f

(3)

where f is a fluent. Statements of type (3) encode a sensing action a which enables the agent(s) to learn the value of
the fluent f . f is referred to as a sensed fluent of the sensing action a.
For actions such as an agent telling another agent that the coin lies heads up, we have statements of the following
kind, that express the change that may be caused by such actions:
a announces ϕ

(4)

where ϕ is a fluent formula. a is called an announcement action.
Let us illustrate the use of statements of type (1)-(4) to represent the actions of the domain D1 .
Example 3 The actions of domain D1 can be specified by the following statements:
executable
executable
executable
executable
executable

open(X) if has key(X), BX (has key)
peek(X) if opened, looking(X), BX (opened), BX (looking(X))
shout tail(X) if BX (tail), tail
signal(X, Y ) if looking(X), ¬looking(Y ), BX (¬looking(Y )), BX (looking(X))
distract(X, Y ) if looking(X), looking(Y ), BX (looking(X)), BX (looking(Y ))

open(X) causes opened
signal(X, Y ) causes looking(Y )
distract(X, Y ) causes ¬looking(Y )
peek(X) determines tail
shout tail(X) announces tail
where X and Y are different agents in {A, B, C}. The first five statements encode the executability conditions of the
five actions in the domain. The next three statements describe the effects of the three world-altering actions. peek(X)
is an example of a sensing action. Finally, shout tail(X) is an example of an announcement action.

11

3.2

Observability: observers, partial observers, and others

One of the key differences between single-agent and multi-agent domains lies in how the execution of an action
changes the beliefs of agents. This is because, in multi-agent domains, an agent can be oblivious about the occurrence
of an action or unable to observe the effect of an action. For example, watching another agent open the box allows the
agent to know that the box is open after the execution of the action; however, the agent would still believe that the box
is closed if she is not aware of the action occurrence. On the other hand, watching another agent peeking into the box
does not help the observer in learning whether the coin lies heads or tails up; the only thing she would learn is that the
agent who is peeking into the box has knowledge of the status of the coin.
mA+ needs to have a component for representing the fact that not all the agents may be completely aware of the
presence of actions being executed. Depending on the action and the current situation, we can categorize agents in
three classes:
• Full observers,
• Partial observers, and
• Oblivious (or others).
This categorization is dynamic: changes in the state of the world may lead to changes to the agent’s category w.r.t.
each action. In this paper, we will consider the possible observability of agents for different action types as detailed in
Table 1.
action type
world-altering actions
sensing actions
announcement actions

full observers
√
√
√

partial observers
√
√

oblivious/others
√
√
√

Table 1: Action types and agent observability

The first row indicates that, for a world-altering action, an agent can either be a full observer, i.e., completely aware
of the occurrence of that action, or oblivious of the occurrence of the action. In the second case, the observability of
the agent is categorized as other. Note that we assume that the observer agents know about each others’ status and
they are also aware of the fact that the other agents are oblivious. The oblivious agents have no clue of anything.
For a sensing action, an agent can either be a full observer—i.e., the agent is aware of the occurrence of that action
and of its results—a partial observer—i.e., gaining knowledge that the full observers have performed a sensing action
but without knowledge of the result of the observation—or oblivious of the occurrence of the action (i.e., other). Once
again, we assume that the observer agents know about each others’ status and they also know about the agents partially
observing the action and about the agents that are oblivious. The partially observing agents know about each others’
status, and they also know about the observing agents and the agents that are oblivious. The oblivious agents have no
clue of anything. The behavior is analogous for the case of announcement actions.
The dynamic nature of the agents observability can be manipulated and this is specified using agent observability
statements of the following form:7
X observes a if ϕ

(5)

X aware of a if ψ

(6)

where X ∈ AG is the name of an agent, a ∈ A, and ϕ and ψ are fluent formulae. Statements of type (5) indicate that
X is a full observer of the execution of a if ϕ holds. Statements of type (6) state that X is a partial observer of the
execution of a if ψ holds. X, a, and ϕ (resp. ψ) are referred to as the agent, the action, and the condition of (5) (resp.
(6)).
7 As

discussed earlier, the “ if >” are omitted from the statements.

12

In the following, we will assume that, for every agent X ∈ AG and for every action a ∈ A, if X occurs in a
statement of the form (5), then it will not occur in a statement of the form (6) such that ϕ ∧ ψ is consistent and vice
versa.
Definition 13 An mA+ domain is a collection of statements of the forms (1)-(6).

3.3

More Examples of mA+ Domains

In this section, we illustrate the use of mA+ in representing multi-agent domains. We start by completing the specification of the domain D1 .
Example 4 (Observability in D1 ) The actions of D1 are described in Example 3. The observability of agents in D1
can be described by the set O1 of statements
X observes
Y observes
Y observes
X observes
Y observes

open(X)
open(X) if looking(Y )
shout tail(X)
distract(X, Y )
distract(X, Y )

X observes
Y aware of
X observes
X observes
Y observes

peek(X)
peek(X) if looking(Y )
shout tail(X)
signal(X, Y )
signal(X, Y )

where X and Y denote different agents in {A, B, C}. The above statements say that agent X is a fully observant agent
when open(X), peek(X), distract(X, Y ), signal(X, Y ), or shout tail(X) are executed; Y is a fully observant
agent if it is looking (at the box) when open(X) is executed. Y is a partially observant agent if it is looking when
peek(X) is executed. An agent different from X or Y is oblivious in all cases.
The next example represents a domain in which the agent who executes an action might not be a full observer of the
action occurrence.
Example 5 Let us consider a domain with two agents A = {A, B}. The two agents are operating in a room; agent A
is blind while B is not. Both agents are aware that by flipping a switch it is possible to change the status of the light
in the room, and both agents can perform such action. On the other hand, the effect of the execution of the action will
be visible only to B. This action can be described by the following statements of mA+:
flip causes on if ¬on
flip causes ¬on if on
B observes flip
We will next describe several examples that contain common actions that are typical to multi-agent domains in mA+.
We refer to these actions as reference setting actions, such as the action of distracting another agent. These types of
actions are interesting, because it is often necessary for agents to execute them in order to allow subsequent actions to
achieve their intended effects (e.g., sharing a secret).
Example 6 (Distraction) Agent A wishes to access some files on C’s computer without C’s knowledge. Agent C is
rather observant therefore, in order for A to succeed, A will need to first cause a distraction (such as pulling the fire
alarm) in order to pull C’s attention away from the computer. Once C is distracted, A may access the file on the
computer. This can be described by the following statements:
distract(A, C) causes distracted(C)
A observes distract(A, C)
C observes distract(A, C)
executable accessFile(A, C) if distracted(C)
The action that helps an agent to form or dissolve a group is also frequently needed in multi-agent domains. Groups
enable, for example, the execution of communications that are only local to the group (e.g., a secret conversation).
13

Example 7 (Group Formation/Dissolution and Secret Communication) Continuing with Example 6, now that A
has access to the file, she needs the assistance of agent B to learn its contents because the file is encrypted and the
expertise of B is required for decryption. In order to read the file, A must first establish a connection with B—agent
A must open/invite a communications channel to B. Let linked(X, Y ) denote that X and Y are connected and
distracted(Z) denote that Z is distracted. This action of A inviting B to connect via some communication channel
can be represented using the action openChannel(A, B) with the following following specification:
openChannel(A, B) causes linked(A, B)
A observes openChannel(A, B)
B observes openChannel(A, B)
C observes openChannel(A, B) if ¬distracted(C)
executable openChannel(A, B) if ¬linked(A, B)
Once a channel has been opened, agents A and B are linked and they may together read the file. Once they have read
the file, they disconnect the channel in order to leave no trace of their activity. This action can be represented using
the action closeChannel(A, B) with the following specification:
closeChannel(A, B) causes ¬linked(A, B)
A observes closeChannel(A, B)
B observes closeChannel(A, B)
C observes closeChannel(A, B) if ¬distracted(C)
executable closeChannel(A, B) if linked(A, B)
Reading the file allows A and B to understand its content. Let us assume that the file indicates whether tomorrow is
the date set for a cyber-attack against A’s organization. This can be represented as follows.
readFile(A) determines attackDate(tomorrow)
A observes readFile(A)
B observes readFile(A) if linked(A, B)
C aware of readFile(A) if ¬distracted(C)
executable readFile(A) if linked(A, B)
If a channel is open, it can be used to share the knowledge of an impeding attack. However, the communication is
secure only if the third party is distracted. This action is an announcement action and can be represented using the
following statements.
warnOfAttack(A, B) announces attackDate(tomorrow)
A observes warnOfAttack(A, B)
B observes warnOfAttack(A, B)
C observes warnOfAttack(A, B) if ¬distracted(C)
executable warnOfAttack(A, B) if linked(A, B) ∧ attackDate(tomorrow)
A more general version of the actions of secretly creating/dissolving a group is given in the next example.
Example 8 Consider an agent X joining an agent Y to gain visibility of everything that the agent X does; this can
be modeled by the action join(Y, X), where Y joins X at the same level of visibility of X’s actions:
join(Y, X) causes group member(Y, group(X))
X observes join(Y, X)
Y observes join(Y, X)
This could be refined by adding the need to be invited to join X:
executable join(Y, X) if invited(Y, X)
The effect of gaining visibility of the actions of X can be described by
14

Y observes aX if group member(Y, group(X))
where aX is any action that is executed by agent X. The symmetrical operation is the operation of leaving the group,
leading the agent Y to completely loose visibility of what agent X is doing:
leave(Y, X) causes ¬group member(Y, group(X))
X observes leave(Y, X)
Y observes leave(Y, X)
The agent Y may also decide to take the action of separating from the group, through the action separate(Y, X), where
the agent Y will observe X from “a distance”, with the consequent loss of the intimate knowledge of X actions’ effects:
separate(Y, X) causes ¬group member(Y, group(X)) ∧ group observer(Y, group(X))
X observes separate(Y, X)
Y observes separate(Y, X)
Y aware of aX if group observer(Y, group(X))
Distracting an agent is not only necessary for secure communication, it is also important for certain world-altering
actions to achieve their intended effects, as in Example 6. Another example that highlights the importance of this type
of actions can be seen next.
Example 9 Agent D is a prisoner, having been captured by C. Agent A is in charge of rescuing agent D. In order to
do so, he must first distract C, and then trigger a release on C’s computer. Once the release has been triggered, D
may escape. The distract(A, C) action has already been presented in Example 6. Let us consider the other actions.
Rescuing an agent means that the rescued agent is released. We use the following statements:
rescue(A, D) causes released(D)
A observes rescue(A, D)
D observes rescue(A, D)
C observes rescue(A, D) if ¬distracted(C)
executable rescue(A, D) if distracted(C)
The action of escaping can have different effects.
escape(D) causes dead(D) if ¬distracted(C)
escape(D) causes free(D) if distracted(C)
A observes escape(D)
D observes escape(D)
C observes escape(D) if ¬distracted(C)

3.4

Initial State

A domain specification encodes the actions and their effects and the observability of agents in each situation. The
initial state, that encodes both the initial state of the world and the initial beliefs of the agents, is specified in mA+
using initial statements of the following form:
initially ϕ

(7)

where ϕ is a formula. Intuitively, this statement says that ϕ is true in the initial state. We will later discuss restrictions
on the formula ϕ to ensure the computability of the Kripke structures describing the initial state.

15

Example 10 (Representing Initial State of D1 ) Let us reconsider Example 1. The initial state of D1 can be expressed by the following statements:
initially
initially
initially
initially
initially
initially

C(has key(A))
C(¬has key(B))
C(¬has key(C))
C(¬opened)
C(¬BX tail ∧ ¬BX ¬tail) for X ∈ {A, B, C}
C(looking(X))
for X ∈ {A, B, C}

These statements indicate that everyone knows that A has the key and B and C do not have the key, the box is closed,
no one knows whether the coin lies head or tail up, and everyone is looking at the box.
The notion of an action theory in mA+ is defined next.
Definition 14 (Action Theory) A mA+-action theory is a pair (I, D) where D is a mA+ domain and I is a set of
initially statements.

4

Transition Function for mA+ Domains

A mA+ domain D specifies a transition system, whose nodes are “states” that encode the description of the state of
the world and of the agents’ beliefs. This transition system will be described by a transition function ΦD , which maps
pairs of actions and states to states. For simplicity of the presentation, we assume that only one action is executed at
each point in time—it is relatively simple to extend it to cases where concurrent actions are present, and this is left as
future work. As we have mentioned in Section 2, we will use pointed Kripke structures to represent states in mA+
action theories. A pointed Kripke structure encodes three components:
• The actual world;
• The state of beliefs of each agent about the real state of the world; and
• The state of beliefs of each agent about the beliefs of other agents.
These components are affected by the execution of actions. Observe that the notion of a state in mA+ action theories
is more complex than the notion of state used in single-agent domains (i.e., a complete set of fluent literals).
Let us consider a state (M, s). We say that an action a ∈ A is executable in (M, s) if the executability condition of
a, given by a statement of the type (1) in D, is satisfied by (M, s), i.e., (M, s) |= ψ. The effect of executing an action
a in (M, s) is, in general, a set of possible states denoted by ΦD (a, (M, s)). Since each type of action will impact
(M, s) in a different manner, we define
 w
if a is a world-altering action
 ΦD (a, (M, s))
if a is a sensing action
ΦD (a, (M, s)) = ΦsD (a, (M, s))
 a
ΦD (a, (M, s))
if a is an announcement action
In other words, we will define ΦD for each type of actions separately. As we have mentioned earlier, the occurrence
of an action can change the state of the world and/or the state of the agents’ beliefs. The change in an agent’s beliefs
depends on the degree of awareness of the agent about the action occurrence, which in turn depends on the current
state. Therefore, we need to first introduce the notion of a frame of reference in order to define the function ΦD .

4.1

Actions Visibility

Given a state (M, s) and an action a ∈ A, let us define
FD (a, M, s) = {X ∈ AG | [X observes a if ϕ] ∈ D such that (M, s) |= ϕ}
PD (a, M, s) = {X ∈ AG | [X aware of a if ϕ] ∈ D such that (M, s) |= ϕ}
OD (a, M, s) = AG \ (FD (a, M, s) ∪ PD (a, M, s))
16

We will refer to the tuple (FD (a, M, s), PD (a, M, s), OD (a, M, s)) as the frame of reference for the execution of a
in (M, s). Intuitively, FD (a, M, s) (resp. PD (a, M, s) and OD (a, M, s)) are the agents that are fully observant (resp.
partially observant and oblivious/other) of the execution of action a in the pointed Kripke structure (M, s). For the
sake of simplicity, we will assume that the set of agent observability statements in D is consistent, in the sense that for
each pair of an action a ∈ A and a pointed Kripke structure (M, s), the sets FD (a, M, s) and PD (a, M, s) are disjoint.
Thus, the domain specification D and the pointed Kripke structure (M, s) determine a unique frame of reference for
each action occurrence.
The introduction of frames of reference allows us to elegantly model several types of actions that are aimed at
modifying the frame of reference (referred to as reference setting actions). Some possibilities are illustrated in the
following examples.
Example 11 (Reference Setting Actions) Example 4 shows two reference setting actions: signal(X, Y ) and
distract(X, Y ). The action signal(X, Y ) allows agent X to promote agent Y into a higher level of observation
for the effect of the action peek(X). On the other hand, the action distract(X, Y ) allows agent X to demote agent Y
into a lower level of observation. The net effect of executing these actions is a change of frame.
Let us consider the action signal(X, Y ) and a state (M, s). Furthermore, let us assume that (M 0 , s0 ) is a state
resulting from the execution of signal(X, Y ) in (M, s). The frames of reference for the execution of the action a =
peek(X) in these two states are detailed in Figure 2.
signal(X,Y )

in (M, s) −−−−−−−→
in (M 0 , s0 )
FD (a, M, s)
FD (a, M, s)
PD (a, M, s)
PD (a, M, s) ∪ {Y }
OD (a, M, s)
OD (a, M, s) \ {Y }
Figure 2: Frame of reference for peek(X) before (in (M, s)) and after signal(X, Y ) (in (M 0 , s0 ))
Intuitively, after the execution of signal(X, Y ), looking(Y ) becomes true because of the statement
signal(X, Y ) causes looking(Y )
in D1 . By definition, the statement
Y aware of peek(X) if looking(Y )
indicates that Y is partially observant.
Similar argument shows that distract(X, Y ) demotes Y to the lowest level of visibility, i.e., it will cause agent
Y to become oblivious of the successive peek(X) action. Again, for a = peek(X), we have the change of frame of
reference summarized in Figure 3.
distract(X,Y )

in (M, s) −−−−−−−−→
in (M 0 , s0 )
FD (a, M, s)
FD (a, M, s) \ {Y }
PD (a, M, s)
PD (a, M, s) \ {Y }
OD (a, M, s)
OD (a, M, s) ∪ {Y }
Figure 3: Frame of reference for peek(X) before (in (M, s)) and after distract(X, Y ) (in (M 0 , s0 ))

Reference setting actions can be used in modeling group formation activities.
Example 12 Consider the join and leave actions from Example 8. The frame setting changes of these actions can be
summarized as in Figure 4, where a(X) denotes an arbitrary action of X and (M 0 , s0 ) denotes the state resulting from
the execution of the corresponding action in (M, s).

17

in (M, s)
FD (a(X), M, s)
PD (a(X), M, s)
OD (a(X), M, s)

join(Y,X)

−−−−−−→

in (M 0 , s0 )
FD (a(X), M, s) ∪ {Y }
PD (a(X), M, s) \ {Y }
OD (a(X), M, s) \ {Y }

leave(Y,X)

in (M, s)
−−−−−−→
in (M 0 , s0 )
FD (a(X), M, s)
FD (a(X), M, s) \ {Y }
PD (a(X), M, s)
PD (a(X), M, s)
OD (a(X), M, s)
OD (a(X), M, s) ∪ {Y }
separate(Y,X)

in (M, s)
−−−−−−−−→
in (M 0 , s0 )
FD (a(X), M, s)
FD (a(X), M, s) \ {Y }
PD (a(X), M, s)
PD (a(X), M, s) ∪ {Y }
OD (a(X), M, s)
OD (a(X), M, s)
Figure 4: Frame of reference for a(X) before (in (M, s)) and after the execution of a joint/leave/separate action (in
(M 0 , s0 ))

4.2

World-Altering Actions: Φw
D

Intuitively, the execution of a world-altering (a.k.a. ontic) action will take place in the actual world—thus, it will affect
the interpretation associated to the world s of the state (M, s) in which the action is executed. The definition of Φw
D
must describe this change as well as the change in beliefs of agents with different degrees of awareness. Recall that for
a world-altering action a, the frame of reference for its execution in a state (M, s) consists of two elements: the fully
observers (FD (a, M, s)) and the oblivious ones (OD (a, M, s)). Intuitively, an agent in FD (a, M, s) should believe
that the effects of a are true in the successor state. In addition, she should also be able to remove from consideration
those worlds in which the action is non-executable. On the other hand, an agent in OD (a, M, s) will not be able to
observe the effects of the execution of a and, hence, will believe that there are no changes, i.e., its accessible possible
worlds remain unchanged. We can conclude that the successor state Φw (a, (M, s)) will contain two substructures, one
for each group of agents.
Let us construct the structure for the group of agents FD (a, M, s). As each agent in this group is fully aware of the
effects of a, the possible worlds that are present after the occurrence of a will be those resulting from the execution of
a in the possible worlds belonging to M . To give a precise definition, we need the following notations.
For a fluent literal `, let ` denote its complement, i.e., for f ∈ F, f = ¬f and ¬f = f . For a set of literals S,
S = {` | ` ∈ S}. For a world-altering action a, the effects of the action in the Kripke structure M w.r.t. the world
u ∈ M [S] are
eD (a, M, u) = {` | [a causes ψ if ϕ] ∈ D such that (M, u) |= ϕ, ` ∈ ψ}
In the following, we will assume that for every world-altering action a and state (M, s), eD (a, M, u) is a consistent
set of literals for every u ∈ M [s] (i.e., the domain specification is consistent). It is easy to see that, given a, M and u,
eD (a, M, u) is unique.
An agent observing the execution of the action will learn the outcomes of the execution; nevertheless, the agents
have uncertainty about the actual world—which implies that the agent may have to maintain such uncertainty after the
execution of the action, by applying the effects of the action in each world that is considered possible by the agent.
This leads us to the following construction of the substructure for the agents in FD (a, M, s). Let us denote with
Res(a, M, s) the Kripke structure defined as follows
1. Res(a, M, s)[S] = {r(a, u) | u ∈ M [S], a is executable in (M, u)} where each r(a, u) is a new world;
2. Res(a, M, s)[π](r(a, u)) = M [π](u) \ eD (a, M, u) ∪ eD (a, M, u);
3. (r(a, u), r(a, v)) ∈ Res(a, M, s)[i] iff
◦ (u, v) ∈ M [i]
18

◦ {r(a, u), r(a, v)} ⊆ Res(a, M, s)[S]
◦ i ∈ FD (a, M, s)
Intuitively, this is a copy of the original Kripke structure M , where:
• The worlds receive new names (of the form r(a, u)) and are kept only if a is executable in them (item 1),
• Only the edges labeled by fully observant agents are kept (item 3), and
• The interpretations attached to the worlds are updated to reflect the consequences of action a (item 2).
Observe that this construction could be optimized—e.g., the execution of the action might completely remove the
uncertainty of agents, thus removing the need for multiple worlds.
Since the agents in O(a, M, s) are not aware of the action occurrence and its effects, the structure encoding their
beliefs is exactly the original structure. So, the successor state Φw
D (a, (M, s)) is obtained by combining Res(a, M, s)
and (M, s). This combination should maintain the beliefs of agents in O(a, M, s). This is achieved by creating edges
of the form (r(a, u), i, v) for each (u, v) ∈ M [i] and i ∈ O(a, M, s). The complete definition of Φw
D (a, (M, s)) is as
follows.
Definition 15 (Step Transition for World-Altering Actions) Let a be a world-altering action and (M, s) be a state.
Then

a is not executable in (M, s)
∅
Φw
(8)
D (a, (M, s)) =

{(M 0 , s0 )} a is executable in (M, s)
where
◦ s0 = r(a, s)
◦ M 0 [S] = M [S] ∪ Res(a, M, s)[S]
◦ M 0 [π](u) = M [π](u) for each u ∈ M [S] and M 0 [π](u) = Res(a, M, s)[π](u) for each u ∈ Res(a, M, s)[S]
◦ M 0 [i] = M [i] ∪ Res(a, M, s)[i] ∪ Link(a, M, s, i), where:
– for each i ∈ FD (a, M, s) we have that Link(a, M, s, i) = ∅;
– for each i ∈ OD (a, M, s) we have that
Link(a, M, s, i) = {(r(a, u), v) | r(a, u) ∈ Res(a, M, s)[S], u, v ∈ M [S], (u, v) ∈ M [i]}.
We illustrate the definition with some simple examples. First, let us continue with the example presented in the
introduction.
Example 13 The action open is a world-altering action. The observability of agents with respect to its execution is
given in Example 4. If we assume that agent A opens the box when agent B is looking at it while C is not, then
the transformation of a state is shown in Figure 5 (for the sake of readability and the simplicity of the graph, we
omit the other fluents, such as has key(X), in the world representations). The frame of reference for open(A) is
FD (open(A), M, s) = {A, B} and OD (open(A), M, s) = {C}. Note that the two worlds in the upper row represent
the same configuration of beliefs as the starting situation (due to the fact that agent C is oblivious of any change),
while the two worlds in the bottom row reflect the new perspective of the agents A and B—in particular, in the actual
world, the box is now opened.
The next example shows the effect of the execution of the action f lip from Example 5.

19

A,B,C

A,B,C

A,B,C

¬opened
looking(B)
¬tail

¬opened
looking(B)
tail

A,B,C

A,B,C

A,B,C

¬opened
looking(B)
¬tail

C

C

opened
looking(B)
¬tail

¬opened
looking(B)
tail

C

C
A,B

opened
looking(B)
tail

A,B

A,B

Figure 5: Execution of the action open
Example 14 Let us consider the state (M, s1 ) shown on top in Figure 6. More precisely, we have that M [S] =
{s1 , s2 }, M [π](s1 )(on) = ⊥, M [π](s2 )(on) = >, and M [A] = {(s1 , s1 ), (s1 , s2 ), (s2 , s1 ), (s2 , s2 )} and M [B] =
{(s1 , s1 ), (s2 , s2 )}. Observe that (M, s1 ) identifies a state where the light is off, but agent A is unaware of that (while
agent B is). The bottom part of Figure 6 illustrates the resulting state after the execution of flip in (M, s1 ).
The top part of the resulting state (worlds s1 and s2 ) represent an exact copy of the original state—this reflects
the view of agent A, which is unaware of the effects of the action flip being executed. The bottom part reflects instead
the view that agents B has after executing the action, including the fact that the new state of the world (denoted by
r(f lip, s1 )) has a different interpretation w.r.t. s1 . The arrows labeled A leaving the state r(f lip, s1 ) indicate that for
agent A nothing has changed (since they point back to the original copy of the Kripke structure).
We can prove the following lemma.
Lemma 1 Given a consistent domain D, a state (M, s), and a world-altering action a executable in (M, s), we have
that Φw
D (a, (M, s)) is a singleton.
Proof. This result follows trivially from the construction of (M 0 , s0 ), the fact that Res(a, M, s) is unique, and that for
each u ∈ Res(a, M, s)[S], the interpretation of r(a, u) is also unique.
2
The next proposition shows that the execution of a world-altering action changes the beliefs of agents according to
their classification.
Proposition 2 Let D be a consistent domain, (M, s) be a state, a be a world-altering action that is executable in
0 0
(M, s) and Φw
D (a, (M, s)) = {(M , s )}. The following holds:
20

S1:
¬on

S2:
on

A

A, B

A, B

S1:
¬on

S2:
on

A
A

A

A, B

A

A, B

r(flip,S1):

r(flip,S2):

on

¬on

B

A

B

Figure 6: Execution of the action flip
1. For every world r(a, u) ∈ M 0 [S] \ M [S] and for every literal ` ∈ eD (a, M, u), we have that (M 0 , r(a, u)) |= `;
2. For every world r(a, u) ∈ M 0 [S] \ M [S] and for every literal ` ∈ F \ {`, `¯ | ` ∈ eD (a, M, u)}, we have that
(M, u) |= ` iff (M 0 , r(a, u)) |= `;
3. For every i ∈ OD (a, M, s) and formula ϕ, (M 0 , s0 ) |= Bi ϕ iff (M, s) |= Bi ϕ.
2

Proof. See Appendix.

The first case of the proposition indicates that the effects of the action are happening in each world where the
action is executable (and visible). The second case indicates that inertia is in effect on those worlds where the action
is performed (and visible). Finally, the last case indicates that no changes are observed by the other agents.

4.3

Sensing Actions: ΦsD

Sensing actions differ from world-altering actions in that they do not change the actual state of the world. Rather, they
change the beliefs of the agents who can observe the effects of the actions. Let us consider the action peek(A) in the
domain of Example 1. If A were to peek into the box then A should know whether the coin lies head or tail up. Let
us consider instead B, who is looking at the box (or at A) when A is peeking into the box. We expect that B should
know that A knows whether the coin lies head or tail up. However, B’s uncertainty about whether or not the coin lies
head up still remains. Now consider C, who is not looking at the box (or at A) when A is peeking into the box. We
expect that C will not see any changes in its beliefs about either A or B; C will also not modify its beliefs about the
status of the coin.
In general, for a sensing action a such that
a determines f
belongs to D, the execution of a will have different effects on different groups of agents:
• Each agent i ∈ FD (a, M, s) will have no uncertainty about the truth value of f , i.e., Bi f ∨ Bi ¬f is true in the
successor state;

21

• Each agent i ∈ PD (a, M, s) will be aware that the agents in FD (a, M, s) know the truth value of f , but i itself
will not know such value; that is Bi (Bj f ∨ Bj ¬f ) is true in the successor state for j ∈ FD (a, M, s);
• Each agent i ∈ OD (a, M, s) will not change its beliefs.
Given a sensing action a, the set of fluents being sensed by a is defined as follows.
SensedD (a) = {f | [a determines f ] ∈ D}.
Let us proceed with the development of the step transition function for sensing actions. Similarly to the construction
of the successor state for world-altering actions, we can see that the new state will consist of two substructures. The
first structure encodes the beliefs of agents whose beliefs change after the execution of a. The second one represents
agents who are oblivious. Since sensing actions do not change the world, the first structure can be obtained from the
original structure M by:
(i) Removing all worlds in M such that the executability condition of a is not satisfied with respect to them, i.e.,
the set
Remstates
(M ) = {u | (M, u) 6|= ϕ}
a
where ϕ is the executability condition of a;
(ii) Removing all links related to oblivious agents (OD (a, M, s)); and
(iii) Removing all links of the form (u, i, v) where i ∈ FD (a, M, s) such that the interpretation of some f ∈
SensedD (a) in u and v is different, i.e., the set
Remlinks
(M ) = {(u, i, v) | i ∈ FD (a, M, s), f ∈ SensedD (a), (u, v) ∈ M [i], M [π](u)(f ) 6= M [π](v)(f )}.
a
Using the operations on Kripke structures (Subsection 2.3), we can formulate this process as follows:
• Take a replica (M r , c(s)) of (M, s) and restrict the structure to contain only information related to agents in
s

(M r ), c(s)) |FD (a,M,s)∪PD (a,M,s) ;
FD (a, M, s) ∪ PD (a, M, s) which is (M r 	 Remstates
a
• Remove all links related to the uncertainty about fluents in SensedD (a) for agents in FD (a, M, s) from this
a

s

(M r ), c(s)); and
(M r )) |FD (a,M,s)∪PD (a,M,s) 	Remlinks
structure which is ((M r 	 Remstates
a
a
• Link the states in the replica to the states of the original Kripke structure via the links of agents in OD (a, M, s)
−1
which is represented by the operator ]cFD (a,M,s)∪PD (a,M,s) on the two structures.
This leads us to the following definition of ΦsD .
Definition 16 (Step Transition for Sensing Actions) Let a be a sensing action and (M, s) be a state. Then,

∅
a is not executable in (M, s)
s
ΦD (a, (M, s)) =
{(M 0 , s0 )} a is executable in (M, s)
where (M 0 , s0 ) is given by
−1

(M 0 , s0 ) = (M, s) ]cFD (a,M,s)∪PD (a,M,s) (M 00 , s00 )
with
s

a

• (M 00 , s00 ) = ((M r 	 Remstates
(M r )) |FD (a,M,s)∪PD (a,M,s) 	Remlinks
(M r ), c(s)) and
a
a
• (M r , c(s)) is a c-replica of (M, s).
We illustrate the definition in the next example.
22

(9)

S1:
¬tail

S2:
tail

A,B,C

A,B,C

A,B,C

S1:
¬tail
C

C

S2:
tail

A,B,C

Agent C

C

A,B,C

A,B,C

S3:

B

¬tail

C

S4:
tail

Agent A
A,B

A,B

Agent B

Figure 7: Execution of the sensing action peek(A)
Example 15 Let us consider a slight modification of the scenario used in Example 1. Assume that it is common belief
that the box is open and C is not looking at the box while A and B both look at the box. Assume that A peeks into the
box. Starting from the state in Figure 1 (we omit all other fluent literals from the world representations, in this case:
opened, has key(A), ¬has key(B), ¬has key(C), looking(A), looking(B), and ¬looking(C)), the execution of
the action peek(A) will lead to the state in Figure 7.
We can recognize three components in this resulting state:
• Agent A (see the bottom-left box in Figure 7, the green box) has exact belief of the property ¬tail being true,
without any uncertainty—i.e., there are no possible worlds accessible by A where tail is true.
• Agent B operates on the two possible worlds in the last row of Figure 7 (the bottom/blue box); the agent itself
does not know whether tail is true or false, but the agent knows that A is aware of the value of tail, i.e., the agent
B sees the formula BA tail ∨ BA ¬tail as valid.
• Agent C operates on the two possible worlds in the middle row of Figure 7 (the middle/red box), denoting
complete ignorance about tail as well as about the other agents’ beliefs about tail.
Observe that the bottom structure of the successor state (blue area) is obtained from a copy (M r , c(s)) of the
r
original structure (top), by removing Remstates
peek(A) (M ) = ∅, restricting the links to those labeled by agents in the set
{A, B}, and removing the set of links labeled A that connect worlds with different interpretation of the fluent tail, i.e.,
r
Remlinks
peek(A) (M ) = {(s3 , A, s4 ), (s4 , A, s3 )}.
Having defined the step transition function for sensing actions, let us focus on its properties. The first aspect we want
to mention is the need to understand the restriction imposed in the introduction—i.e., we intend to use sensing actions
to remove uncertainty but not to correct wrong beliefs. The next example shows that even a fully observant agent with
a wrong belief could become “ignorant” after the execution of a sensing action.
Example 16 Consider a simple action a that is always executable and determines the fluent f and an agent A who is
fully observant of the execution of a in the state given in the left of Figure 8.
23

S0:

A

S1:

S0:

¬f

f

f

S1:

¬f

A

A

Figure 8: Execution of a sensing action by agents with false beliefs
It is easy to see that the execution of a results in the state on the right of Figure 8. Observe that A becomes ignorant
in s0 , the actual world, after the execution of a. This is because A has the wrong belief about the fluent f in the state
in which a is executed and the execution of a does not update her belief.
The example shows that in general, the execution of a sensing action in a multi-agent environment might have different
effects on the agent’s knowledge and beliefs. Taking the difference between knowledge and belief in formalizing
actions in multi-agent environment is a challenging issue that will need to be addressed. We have taken the first
step into to address this issue in a separate work [30]. We observe that under a certain condition, called consistency
preservation, the execution of a sensing action will not result in agents being completely ignorant as in Example 16.
Definition 17 A state (M, s) is consistency preserving for a sensing action a if (M, u) 6|= Bi f ∨ Bi ¬f for every
u ∈ M [S], i ∈ FD (a, M, s) ∪ PD (a, M, s), and f ∈ SensedD (a).
We will next present a proposition which is similar to Proposition 2 for sensing action which states that after the
execution of a sensing action in a consistency preserving state, fully observant agents have no uncertainty about the
sensed fluents, partially observant agents are aware that the beliefs of fully observant agents are changed, and other
agents’ beliefs do not change.
Proposition 3 Let D be a consistent domain, a be a sensing action, and (M, s) be a state. Suppose that a is executable
in (M, s) and (M, s) is consistency preserving for a. Assume that (M 0 , s0 ) ∈ ΦsD (a, (M, s)). Then, the following
holds
1. for every f ∈ SensedD (a) and ` ∈ {f, ¬f }, if (M, s) |= ` then (M 0 , s0 ) |= CFD (a,M,s) `;
2. for every f ∈ SensedD (a), (M 0 , s0 ) |= CPD (a,M,s) (CFD (a,M,s) f ∨ CFD (a,M,s) ¬f ); and
3. for every i ∈ OD (a, M, s) and formula ψ, (M 0 , s0 ) |= Bi ψ iff (M, s) |= Bi ψ.
2

Proof. See Appendix.

4.4

Announcement Actions: ΦaD

The announcement actions are used to communicate a piece of information to a group of agents. In this paper, we
make the following assumptions:
• The announcement is truthful—i.e., the properties being announced are true in the real state of the world; thus,
if the announcement of ϕ is made in the state (M, s), we expect M [π](s) |= ϕ.
• The agents receiving the announcement believe the property being announced.
• The announcement action occurs in at most one statement of the form (4).
• The execution of an announcement action occurs only in states in which fully observant agents do not have
wrong beliefs about the announced formula (this is similar to the requirement for the execution of a sensing
action).
24

Similar to sensing actions, an announcement action will not change the world. Agents who are aware of the action
occurrence, will be able to reason that the current state of the world must satisfy the executability condition of the
action. Furthermore, an occurrence of an announcement action only changes the beliefs of agents who are aware or
partially aware of the effects of the action. Finally,
• Agents who are fully observant will know the truth value of the formula being announced;
• Agents who are partially observant will not know the truth value of the formula being announced, but they will
know that fully observant agents are knowledgeable about it;
• Oblivious agents will have the same beliefs as before the execution of the action.
The construction of the successor state ΦaD (a, (M, s)) for a state (M, s) and announcement action a is very similar to
the one employed for the sensing actions.
Definition 18 (Step Transition for Announcements) Let a be an announcement action with
a announces ψ
in D and the executability condition ϕ and (M, s) be a state. Then,

∅
a is not executable in (M, s) or (M, s) 6|= ϕ
ΦaD (a, (M, s)) =
{(M 0 , s0 )} a is executable in (M, s) and (M, s) |= ϕ

(10)

where (M 0 , s0 ) is given by
−1

(M 0 , s0 ) = (M, s) ]cFD (a,M,s)∪PD (a,M,s) (M 00 , s00 )
with
• (M r , c(s)) is a c-replica of (M, s);
• Remstates
(M r ) = {c(u) | u ∈ M [S], (M, u) 6|= ϕ}
a


i ∈ FD (a, M, s), (u, v) ∈ M r [i], ( M r [π](u) |= ψ and M r [π](v) |= ¬ψ )
links
• Rema (M ) = (u, i, v)
or ( M r [π](u) |= ¬ψ and M r [π](v) |= ψ )
s

a

• (M 00 , s00 ) = ((M r 	 Remstates
(M r )) |FD (a,M,s)∪PD (a,M,s) 	Remlinks
(M ), c(s))
a
a
In the next example, we illustrate the definition with the execution of a public announcement action.
Example 17 Let consider the initial situation described in Figure 9 along with the action shout tail(A) from Example 3, whose observability is given in Example 4. Intuitively, an announcement of the truth of tail is made to all agents.
This is an example of a “public announcement,” after which all agents are aware of the truth value of the announced
formula (tail). This results in the successor state as in Figure 9. Observe that the bottom row looses one of the states
as the executability condition does not hold.
The next example considers a private announcement action.
Example 18 Let us assume that A and B have agreed to a scheme of informing each other if the coin lies heads up
by raising an hand. B can only observe A if B is looking at the box (or looking at A). C is completely ignorant about
the meaning of A’s raising her hand. This can be modeled by the following statements:
executable raising hand(X) if BX (¬tail), ¬tail
raising hand(A) announces ¬tail
A observes raising hand(A) if true
B observes raising hand(A) if looking(B)
If A knows the coin lies heads up and raises her hand, B will be aware that the head is up and C is completely
ignorant about this. The execution of raising hand(A) in the state where A knows that the coin lies heads up and B
is looking is given in Figure 10.
25

S1:
tail

S2:
¬tail

B,C

A,B,C

A,B,C

S1:
tail

S2:
¬tail

B,C

A,B,C

No Oblivious
Agents

A,B,C

S3:
tail

No Partially
Observant Agents

Agents
A,B,C

A,B,C

Figure 9: Execution of the action shout tail(A)

S1:
¬tail

S2:
tail

B,C

A,B,C

A,B,C
AGENT C

S1:
¬tail

S2:
tail

B,C
C

C

A,B,C
S3:

A,B,C

AGENTS A,B

¬tail

A,B

Figure 10: Execution of the action raising hand(A)

26

The execution of an announcement action will have similar effects on agents’ beliefs if it is executed in a consistency
preserving structure. First, we extend Definition 17 for announcement actions as follows.
Definition 19 A state (M, s) is called consistency preserving for an announcement action a which announces ϕ if,
for every u ∈ M [S] and i ∈ FD (a, M, s) ∪ PD (a, M, s), we have that (M, u) 6|= Bi ¬ϕ.
The next proposition is similar to Proposition 3.
Proposition 4 Let D be a consistent mA+ action theory, (M, s) be a state, and
a announces ϕ
in D. Assume that (M, s) is consistency preserving for a, a is executable in (M, s), and ΦaD (a, (M, s)) = {(M 0 , s0 )}.
Then, the following holds
• (M 0 , s0 ) |= CFD (a,M,s) ϕ;
• (M 0 , s0 ) |= CPD (a,M,s) (CFD (a,M,s) ϕ ∨ CFD (a,M,s) ¬ϕ); and
• for every i ∈ OD (a, M, s) and formula ψ, (M 0 , s0 ) |= Bi ψ iff (M, s) |= Bi ψ.
Proof. Because of the assumption that announcement action is truthful, we have that a is executable only if (M, s) |=
ϕ. The proof of this proposition is then similar to that of Proposition 3 and is omitted for brevity.
2
Observe that our formalization does not distinguish between public announcement and private announcement,
as done in the several of the previously published works. This distinction is already captured by the observability
statements.

4.5

Entailment in mA+ Action Theories

We are now ready to define the notion of entailment in mA+ action theories. It will be defined between mA+ action
theories and queries of the following form:
ϕ after δ

(11)

where ϕ is a belief formula and δ is a sequence of actions a1 ; . . . ; an (n ≥ 0)—referred to as a plan. Let us observe that
the entailment can be easily extended to consider more general forms of conditional plans, that include conditional
statements (e.g., if-then) or even loops (e.g., while)—as discussed in [31, 32]. We leave these relatively simple
extensions for future work.
The description of an evolution of a system will deal with sets of states. We refer to a set of states as a belief state
(or a b-state). We need the following definitions. For a b-state B and an action a, we say that a is executable in B if
ΦD (a, (M, s)) 6= ∅ for every state (M, s) in B. With a slight abuse of notation, we define

{⊥}
if ΦD (a, (M, s)) = ∅ in some state (M, s) in B or B = {⊥}
ΦD (a, B) = S
Φ
(a,
(M,
s))
otherwise
D
(M,s)∈B
where {⊥} denotes that the execution of a in B fails. Note that we assume that no action is executable in ⊥.
Let δ be a plan and B be a b-state. The set of b-states resulting from the execution of δ in B, denoted by Φ∗D (δ, B),
is defined as follows:
• If δ is the empty plan [ ] then Φ∗D ([ ], B) = B;
• If δ is a plan of the form a; δ 0 (with a ∈ A), then Φ∗D (a; δ 0 , B) = Φ∗D (δ 0 , ΦD (a, B)).
Intuitively, the execution of δ in B can go through several paths, each path might finish in a set of states. It is easy
to see that if one of the states reached on a path during the execution of δ is ⊥ (the failed state) then the final result of
the execution of δ in B is {⊥}. Φ∗D (δ, B) = {⊥} indicates that the execution of δ in B fails.
To complete the definition of the notion of entailment, we need the following definition.
27

Definition 20 (Initial State/b-State) Let (I, D) be an action theory. An initial state of (I, D) is a state (M, s) such
that for every statement
initially ϕ
in I, (M, s) |= ϕ. (M, s) is an initial S5-state if it is an initial state and M is a S5 Kripke structure. The initial b-state
of (I, D) is the collection of all initial states of (I, D). The initial S5-b-state of (I, D) is the collection of all initial
S5-states of (I, D).
We are now ready to define the notion of entailment.
Definition 21 (Entailment) An action theory (I, D) entails the query
ϕ after δ,
denoted by (I, D) |= ϕ after δ, if
(a) Φ∗D (δ, I0 ) 6= {⊥} and
(b) (M, s) |= ϕ for each (M, s) ∈ Φ∗D (δ, I0 )
where I0 is the initial b-state of (I, D).
We say that (I, D) S5-entails the query ϕ after δ, denoted by (I, D) |=S5 ϕ after δ, if the two conditions (a)-(b)
are satisfied with respect to I0 being the initial S5-b-state of (I, D).
The next example illustrates these definitions.
Example 19 Let D1 be the domain specification given in Examples 3 and 4 and I1 be the set of initial statements
given in Example 10. Furthermore, let δA be the sequence of actions:
δA = distract(A, C); signal(A, B); open(A); peek(A).
We can show that
(I1 , D1 ) |= (BA tail ∨ BA ¬tail) ∧ BA (BB (BA tail ∨ BA ¬tail)) after δA
(I1 , D1 ) |= BB (B
V A tail ∨ BA ¬tail) ∧ (¬BB tail ∧ ¬BB ¬tail) after δA
(I1 , D1 ) |= BC [ i∈{A,B,C} (¬Bi tail ∧ ¬BA ¬tail)] after δA
under the assumptions that initially, the beliefs of the agents satisfy the S5-axioms.
To see how the above conclusions hold, let us construct an initial state for (I1 , D1 ) (see also Figure 11). Since the
0
0
0
i where
, BC
, BB
truth values of all fluents but tail are known to every agent, we have that M0 = h{s0 , s1 }, π0 , BA
π0 (s0 ) = {¬opened, has key(A), ¬has key(B), ¬has key(C), looking(A), looking(B), looking(C), ¬tail}
and
π0 (s1 ) = {¬opened, has key(A), ¬has key(B), ¬has key(C), looking(A), looking(B), looking(C), tail}.
0
0
0
Furthermore, BA
= BB
= BC
= {(s0 , s0 ), (s0 , s1 ), (s1 , s0 ), (s1 , s1 )}.
The execution of the action distract(A, C) in (M0 , s0 ) results in the state (M1 , s2 ) as shown in Figure 12 where
1
1
1
M1 = h{s0 , s1 , s2 , s3 }, π1 , BA
, BB
, BC
i. The top part of the new structure is a replica of (M0 , s0 ), encoding the
beliefs of B, who is ignorant of the action occurrence. The bottom part encodes the beliefs of A and C, who are
observers of the action occurrence. It includes two new worlds, s2 and s3 , which represent the result of the execution
of distract(A, C) in s0 and s1 respectively, where π1 (s0 ) = π0 (s0 ), π1 (s1 ) = π0 (s1 ),

π1 (s2 ) = π0 (s0 ) \ {looking(C)} ∪ {¬looking(C)}
and
π1 (s3 ) = π0 (s1 ) \ {looking(C)} ∪ {¬looking(C)}.
28

A, B, C

A, B, C

A, B, C

S0
B

S0

A, B, C

A, B, C

S1

S1
B

B

B

S2

A, C

S3

A, B, C

Figure 11: (M0 , s0 ): an initial state of (I1 , D1 )

A, C

Figure 12:
(M1 , s2 ):
distract(A, C) in (M0 , s0 )

A, C

result of execution of

1
0
1
0
Finally, BA
= BA
∪ {(s2 , s2 ), (s2 , s3 ), (s3 , s2 ), (s3 , s3 )}, BC
= BC
∪ {(s2 , s2 ), (s2 , s3 ), (s3 , s2 ), (s3 , s3 )}, and
0
1
BB = BB ∪ {(s2 , s0 ), (s2 , s1 ), (s3 , s0 ), (s3 , s1 )}.
The execution of signal(A, B) in (M1 , s2 ) results in a new state (M2 , s6 ) where
2
2
2
M2 = h{s0 , . . . , s7 }, π2 , BA
, BB
, BC
i

where:
• For i ∈ {0, . . . , 3}, the state si+4 is the result of executing signal(A, B) in si . We have that π2 (si+4 ) = π1 (si )
because B is looking at the box already and thus the execution of this action does not change the state;
1
1
2
};
∪ {(si+4 , sj+4 ) | 0 ≤ i, j ≤ 3 and (si , sj ) ∈ BA
= BA
• BA
1
1
2
}; and
∪ {(si+4 , sj+4 ) | 0 ≤ i, j ≤ 3 and (si , sj ) ∈ BB
= BB
• BB
2
1
1
• BC
= BC
∪ {(si+4 , sj ) | 0 ≤ i, j ≤ 3 and (si , sj ) ∈ BC
}.

The execution of open(A) in (M2 , s6 ) will result in a new state (M3 , s14 ) where
3
3
3
M3 = h{s0 , . . . , s15 }, π3 , BA
, BB
, BC
i

where:
• For i ∈ {0, . . . , 7}, the state si+8 is the result of executing open(A) in si . We have that π3 (si+8 ) = π2 (si ) \
{¬opened} ∪ {opened};
3
2
2
• BA
= BA
∪ {(si+8 , sj+8 ) | 0 ≤ i, j ≤ 7 and (si , sj ) ∈ BA
};
3
2
2
• BB
= BB
∪ {(si+8 , sj+8 ) | 0 ≤ i, j ≤ 7 and (si , sj ) ∈ BB
}; and
3
2
2
• BC
= BC
∪ {(si+8 , sj ) | 0 ≤ i, j ≤ 7 and (si , sj ) ∈ BC
}.

Finally, the execution of peek(A) in (M3 , s14 ) results in a new state (M4 , s30 ) where
4
4
4
M4 = h{si | 0 ≤ i ≤ 31}, π4 , BA
, BB
, BC
i

where:
29

• For i ∈ {0, . . . , 15}, the interpretation π4 (si+15 ) = π3 (si ) since the execution of a sensing action does not
change the state of the world;
4
3
3
• BA
= BA
∪ {(si+16 , sj+16 ) | 0 ≤ i, j ≤ 15 and (si , sj ) ∈ BA
and ((¬tail ∈ π3 (si ) ∩ π3 (sj )) ∨ (tail ∈
π3 (si ) ∩ π3 (sj ))};
4
3
3
• BB
= BB
∪ {(si+16 , sj+16 ) | 0 ≤ i, j ≤ 15 and (si , sj ) ∈ BB
}; and
4
3
3
• BC
= BC
∪ {(si+16 , sj ) | 0 ≤ i, j ≤ 15 and (si , sj ) ∈ BC
}.

We will finally show that (M4 , s30 ) |= BA tail ∨ BA ¬tail. In fact, since (M0 , s0 ) |= ¬tail, we will show that
4
(M4 , s30 ) |= BA ¬tail. To prove this, we need to show that (M4 , sj ) |= ¬tail for every sj ∈ M4 [S], (s30 , sj ) ∈ BA
.
4
3
Since s30 6∈ M3 [S], we have that (s30 , sj ) ∈ BA iff j ≥ 16, (s14 , sj−16 ) ∈ BA , and ¬tail ∈ π3 (s14 ) ∩ π3 (sj−16 )
(because ¬tail ∈ π3 (s14 )). This implies that (M4 , s30 ) |= BA ¬tail. The proof of other conclusions is similar.
We conclude the example with the observation that another initial state for (I1 , D1 ) is (M0 , s1 ) and the execution
of δA in this state results in a new state (M 0 , s0 ) with the property that (M 0 , s0 ) |= BA tail. Theoretically, this fact and
the fact that (M4 , s30 ) |= BA ¬tail are insufficient for us to conclude that (I1 , D1 ) |=S5 BA tail ∨ BA ¬tail holds.
This, however, holds under some additional assumption and thanks to Proposition 15 (see Section 6).

5

Update Model Based Transitions for mA+ Domains

The previous section presented a transition function for mA+ domains in the style of action languages. In this section,
we will develop an alternative definition of transitions between states in mA+ domains that has its roots in dynamic
epistemic logic. For each action a and state (M, s), we will describe an update instance whose application to (M, s)
results in states equivalent to states belonging to ΦD (a, (M, s)). Since an update instance describes the effect of a
particular action occurrence, it will depend not only on the action specifications but also on a frame of reference. As
discussed in the previous section, a frame of reference in a mA+ domain over the signature hAG, F, Ai is a tuple
(F, P, O) where F ∪ P ∪ O = AG and F , P , and O are pairwise disjoint.
Let us start by considering the world-altering actions.
Definition 22 (Update Model/Instance for World-Altering Actions) Given a world-altering action a with the precondition ψ and a frame of reference ρ = (F, P, O), the update model for a and ρ, denoted by ω(a, ρ), is defined by
hΣ, R1 , . . . , Rn , pre, subi where
◦ Σ = {σ, };
◦ Ri = {(σ, σ), (, )} for i ∈ F and Ri = {(σ, ), (, )} for i ∈ O;
◦ pre(σ) = ψ and pre() = >; and
+
−
◦ sub() = ∅ and
(p, a))W| p ∈ F}, where
W sub(σ) = {p → Ψ (p, a) ∨ (p ∧ ¬Ψ
+
−
Ψ (p, a) = {ϕ | [a causes p if ϕ] ∈ D} and Ψ (p, a) = {ϕ | [a causes ¬p if ϕ] ∈ D}.

The update instance for the world-altering action a and the frame of reference ρ is (ω(a, ρ), {σ}).
Observe that the update model of the world-altering action a has only two events. Each event corresponds to a group of
agents. The links in the update model for each group of agents reflect the state of beliefs each group would have after
the execution of the action. For example, fully observant agents (in F ) will have no uncertainty. The next example
illustrates this definition.
Example 20 Going back to our original example, the action open(A) assumes that everyone is aware that C is not
looking at the box while B and A are. Figure 13 (top right) depicts the state. For simplicity, in the worlds we report
only the components of the interpretation related to the opened and tail fluents. The frame of reference for open(A)
in this situation is ({A, B}, ∅, {C}). The corresponding update instance for open(A) and the frame of reference
({A, B}, ∅, {C}) is given in Figure 13 (top left). The bottom part of Figure 13 shows the result of the application of
the update instance to the state on the top right.
30

A,B,C
A,B

A,B,C

σ

!

C

pre: has_key(A)
sub: opened ➝ ⊤∨ opened

A,B,C

S1

S2

A,B,C

¬opened
¬tail

¬opened
tail

pre: ⊤
sub: ∅

A,B,C

A,B,C

(S1,!)

(S2,!)

A,B,C

¬opened
¬tail

C

¬opened
tail

C

(S1,σ)

C

C
A,B

(S2,σ)

opened
¬tail

opened
tail

A,B

A,B

Figure 13: Update Instance (ω(open, ({A, B}, ∅, {C})), {σ}) and its application
In the next definition, we provide the update instance for a sensing action given a frame of reference. For simplicity
of presentation, we will assume that the set of sensed fluents of the action is a singleton.
Definition 23 (Update Model/Instance for Sensing Actions) Let a be a sensing action with SensedD (a) = {f }, let
its precondition be ψ, and let ρ = (F, P, O) be a frame of reference. The update model for a and ρ, ω(a, ρ), is defined
by hΣ, R1 , . . . , Rn , pre, subi where:
◦ Σ = {σ, τ, };
◦ Ri is given by


 {(σ, σ), (τ, τ ), (, )}
Ri = {(σ, σ), (τ, τ ), (, ), (σ, τ ), (τ, σ)}

{(σ, ), (τ, ), (, )}

if i ∈ F
if i ∈ P
if i ∈ O

◦ The preconditions pre are defined by

ψ∧f
pre(x) = ψ ∧ ¬f

>

if x = σ
if x = τ
if x = 

◦ sub(x) = ∅ for each x ∈ Σ.
The update instance for the sensing action a and the frame of reference ρ is (ω(a, ρ), {σ, τ }).
Observe that an update instance of a sensing action has three events, each one corresponding to a group of agents.
Each event is associated with a group of states in which the truth value of sensed fluent is either known to be true,
known to be false, or unknown.
31

Example 21 Let us consider the occurrence of the action peek(A) in the state described in Figure 14 (top right). The
frame of reference for this occurrence of peek(A) is ({A}, {B}, {C}). The corresponding update instance is given in
Figure 14 (top left). Its update on the given state results in the same state as in Example 15 (bottom, Figure 14).
pre: opened ⋀ looking(A) ⋀ ¬tail

A,B

σ

A,B,C
C

B
A,B

S1:
¬tail

!

C

S2:
tail

A,B,C

pre: ⊤

τ

A,B,C

A,B,C

pre: opened ⋀ looking(A) ⋀ tail

(S1,!):

(S2,!):

A,B,C

¬tail

tail
C

C

C

A,B,C

(S1,σ):

A,B,C

B

¬tail

C

(S2,τ):
tail

A,B

A,B

Figure 14: Update Instance (ω(peek(A), ({A}, {B}, {C})), {σ, τ }) and its application

We will conclude the section with a discussion on the update model of for announcement actions.
Definition 24 (Update Model/Instance for Announcement Actions) Given an announcement action a ∈ A that announces ϕ with the precondition ψ and a frame of reference ρ = (F, P, O), the update model for a and ρ, ω(a, ρ), is
defined by hΣ, R1 , . . . , Rn , pre, subi where:
◦ Σ = {σ, τ, };
◦ Ri is defined by


 {(σ, σ), (τ, τ ), (, )}
Ri = {(σ, σ), (τ, τ ), (, ), (σ, τ ), (τ, σ)}

{(σ, ), (τ, ), (, )}

if i ∈ F
if i ∈ P
if i ∈ O

◦ pre is defined by

ψ∧ϕ
pre(x) = ψ ∧ ¬ϕ

>

if x = σ
if x = τ
if x = 

◦ sub is defined as sub(x) = ∅ for any x ∈ Σ.
The update instance for the announcement action a with respect to the frame of reference ρ is (ω(a, ρ), {σ}).
32

As we can see, an update model for an announcement action and a frame of reference is structure-wise identical to the
update model for a sensing action and a frame of reference. The main distinction lies in the set of designated events in
the update instance for each type of actions. There is only a single designated event for announcement actions while
there are two for sensing actions.
Example 22 Let us consider the two versions of the action raising hand(A) described in Example 17 and the state in
which B is looking at the box and both A and B are aware of it.
For the first version of the action the frame of reference for its occurrence is ρ = ({A}, {B}, {C}). The update
instance (ω(raising hand(A), ρ), {σ}) is illustrated in Figure 15. Similarly, the update instance for the second version
of the action in the same state, which results in the frame of reference is ({A, B}, ∅, {C}), is shown in Figure 16.
pre: T ∧ ¬tail

A,B

σ

pre: T ∧ ¬tail

A,B
C

!

B

A,B

τ

C

σ

C

!

A,B,C

pre: T

A,B

pre: T ∧ tail

τ

C

A,B,C

pre: T

pre: T ∧ tail

Figure 15: Update instance for the raising hand(A) action and ρ = ({A}, {B}, {C})

Figure 16: Update instance for the raising hand(A) action and ρ = ({A, B}, ∅, {C})

Definitions 22-24 allow us to define the following notion.
Definition 25 Let a be an action and (M, s) be a state. The update instance of a in (M, s), denoted by Ω(a, (M, s)),
is defined as follows:
• Ω(a, (M, s)) = ∅ if a is not executable in (M, s);
• Ω(a, (M, s)) is the update instance for a and the frame of reference (FD (a, M, s), PD (a, M, s), OD (a, M, s))
as defined in the Definitions 22-24.
The following proposition relates the two approaches to define the successor states of an action occurrence in mA+.
Proposition 5 Given an action a and a state (M, s) such that ΦD (a, (M, s) 6= ∅, we have that for each (M 0 , s0 ) ∈
(M, s) ⊗ Ω(a, (M, s)) there exists an equivalent state in ΦD (a, (M, s)) and vice versa, where (M, s) ⊗ ∅ = ∅.

6

Definite Action Theories

In this section, we identify a class of mA+ action theories that can be each described by a finite number of initial
states, up to a notion of equivalence (Definition 4). The motivation for this task lies in the desire to use available
technologies (e.g., answer set solvers and/or forward search planners) in the computation of the entailment of mA+
theories—which requires the presence of a finite number of initial states. We observe that this problem does not arise
in single-agent domains, where the size of the state space is bounded by 2|F | . On the other hand, theoretically, there
could be infinitely many initial states for an arbitrary mA+ theory. For example, given a state (M, s) and a set of
formulae Σ such that (M, s) |= Σ, a new state (M 0 , s) that also satisfies Σ can be constructed from M by adding a
new world and keeping everything else unchanged.
One way to cope with the aforementioned problem is to limit the type of formulae occurring in the initial statements
of the action theory. Another way to deal with it is to limit the type of Kripke structures considered as initial states.
33

Indeed, we can observe that several examples found in the literature have the following properties: (i) the initial state
can be described by a set of statements involving the common knowledge among all agents; (ii) the common knowledge
relates to whether a particular agent is aware of (or not aware of) a property of the world; and (iii) the beliefs of the
agents coincide with their knowledge about the world. This suggests that several interesting problem domains can be
captured by limiting our attention to specific types of initial Kripke structures and to specific types of initial statements.
Specifically, we will focus on S5-initial states and consider initial statements of the following forms:
initially ϕ

(12)

initially Cϕ

(13)

initially C(Bi ϕ)

(14)

initially C(Bi ϕ ∨ Bi ¬ϕ)

(15)

initially C(¬Bi ϕ ∧ ¬Bi ¬ϕ)

(16)

where ϕ is a fluent formula. Intuitively, statements of type (12) indicate properties that are true in the real state of the
world; statements of type (13) denote properties that all agents believe to be true (and all agents know about the other
agents’ beliefs about such property); statements of type (14)-(15) indicate that all agents believe that agent i is aware
of whether ϕ is true or false; statements of type (16) indicate that all agents believe that agent i is not aware of whether
ϕ is true or false.
We will now formally define the notion of a definite action theory. For an action theory (I, D), let
TI = {ϕ | [initially ϕ] ∈ I}.
We will say that (I, D) is consistent if TI and D are consistent.
Definition 26 (Definite Action Theory) An action theory (I, D) is said to be definite if
• Each initial statement in I is of the form (12)-(16); and
• For each fluent formula ϕ and agent i, I contains an initial statement of the form (14), (15), or (16) in which i
and ϕ occurs.
We prove that S5-initial states of definite action theories are finitely computable in the next proposition.
Proposition 6 For a consistent definite action theory (I, D), there exists a finite number of initial S5-states
(M1 , s1 ), . . . , (Mk , sk ) such that every initial S5-state (M, s) of (I, D) is equivalent to some (Mi , si ). Furthermore,
for each pair of i 6= j and u ∈ Mi [S] there exists some v ∈ Mj [S] such that Mi [π](u) ≡ Mj [π](v).
Note that the last part of the proposition indicates that the set of interpretations used in each S5-state is the same.
The proof of the above proposition relies on a series of lemmas. Let us start by introducing some useful notations.
Given a state (M, s) and u, v ∈ M [S], we will refer to a path between u and v as a sequence of worlds u =
u0 , u1 , . . . , un = v in M [S], where for j = 0, . . . , n − 1, there exists some ij ∈ AG such that (uj , uj+1 ) ∈ M [ij ].
We say that v is reachable from u if there exists a path from u to v. We will also often make use of the fact that, for
a S5-structure, the relations Bi is symmetric, transitive, and reflective (Theorem 3.1.5, [28]). We list these lemmas
below. Proofs are provided in the appendix.
Lemma 7 Every S5-state (M, s) is equivalent to a S5-state (M 0 , s) such that, for every world u ∈ M 0 [S], we have
that u is reachable from s.
The above lemma indicates that for a S5-state (M, s), worlds that are unreachable from s can be removed. The
next lemma deals with initial statements of the form (12)-(14).
Lemma 8 Let (M, s) be a S5-state such that every world u ∈ M [S] is reachable from s. Let ψ be a formula. Then,
(M, s) |= C(ψ) iff M [π](u) |= ψ for every world u ∈ M [S].

34

The lemma shows that for a S5-state (M, s) that satisfies a statement of the form (12)-(14), the literal ` or the
formula ϕ appearing in the statement must be satisfied at every world in M . The next lemma characterizes S5-states
satisfying initial statements of the form (15).
Lemma 9 Let (M, s) be a S5-state such that every world u ∈ M [S] is reachable from s. Let ψ be a fluent formula.
Then:

(M, s) |= C(Bi ψ ∨ Bi ¬ψ) iff
∀u, v ∈ M [S] : (u, v) ∈ M [i] ⇒ M [π](u) |= ψ iff M [π](v) |= ψ
The lemma proves that for each S5-state (M, s) satisfying a statement of the form (15), every pair of worlds related
by Bi either both satisfy or both do not satisfy the formula ϕ appearing in the statement. The next lemma deals with
initial statements of the form (16).
Lemma 10 Let (M, s) be a S5-state such that every u ∈ M [S] is reachable from s. Let ψ be a fluent formula. Then,
(M, s) |= C(¬Bi ψ ∧ ¬Bi ¬ψ) iff for every u ∈ M [S] there exists some v ∈ M [S] such that (u, v) ∈ M [i], and
M [π](u)(ψ) 6= M [π](v)(ψ).
The lemma proves that for a S5-state (M, s) that satisfies a statement of the form (16), there must be at least one
pair of worlds in which the formula in the statement is not satisfied in both worlds. Observe that Lemmas 7-10 provide
a first characterization of S5-states satisfying initial statements of the forms (12)-(16). These lemmas do not take into
consideration the second condition in Definition 26. The next lemma, together with Lemmas 7-10, focuses on this
condition and allows us to determine properties of interpretations associated to the worlds in an initial S5-states of a
definite action theory.
Lemma 11 Let (M, s) be an initial S5-state of a definite action theory, such that every u ∈ M [S] is reachable from
s. Let ϕ be a fluent formula and i ∈ AG. Then,
• If (M, u) |= Bi ϕ for some u ∈ M [S] then (M, s) |= C(Bi ϕ) or (M, s) |= C(Bi ϕ ∨ Bi ¬ϕ);
• If (M, u) |= ¬Bi ϕ for some u ∈ M [S] then (M, s) |= C(¬Bi ϕ ∧ Bi ¬ϕ).
In order to complete the proof of Proposition 6, we need some additional notation. Consider a Kripke structure
M = hS, π, B1 , . . . , Bn i. We define a relation ∼ among worlds of M as follows. For each u, v ∈ M [S], u ∼ v iff
π(u) = π(v). Thus, u ∼ v indicates that the interpretations associated to u and v (i.e., M [π](u) and M [π](v)) are
identical. It is easy to see that ∼ is an equivalence relation over M [S]. Let ũ denote the equivalence class of u with
respect to the relation ∼ (i.e., ũ = [u]∼ ). The next lemma is critical for the proof of Proposition 6.
Lemma 12 Let (M, s) be an initial S5-state of a definite action theory, such that every u ∈ M [S] is reachable from
s. Let u, v ∈ M [S] such that u ∼ v. Then, for every i ∈ AG and x ∈ M [S] such that (u, x) ∈ M [i] there exists
y ∈ M [S] such that (v, y) ∈ M [i] and x ∼ y.
The above lemma allows us to collapse all worlds with the same interpretation into a single world. This is made
f be the structure constructed as follows:
precise as follows. Given a structure M , let M
f[S] = {ũ | u ∈ S}
• M
f[π](ũ)(f ) = M [π](u)(f )
• For every u ∈ M [S] and f ∈ F, M
f[i] if there exists (u0 , v 0 ) ∈ M [i] for some u0 ∈ ũ and v 0 ∈ ṽ.
• For each i ∈ AG, (ũ, ṽ) ∈ M
f is obtained from M by replacing each equivalence class in M with a single world. We will call (M
f, s̃)
Intuitively, M
the reduced state of (M, s). Thanks to the previously considered properties, the following holds:
f, s̃) of
Lemma 13 Let (M, s) be a S5-state such that every u ∈ M [S] is reachable from s. Then, the reduced state (M
(M, s) is a finite S5-state.
35

The next lemma shows that a reduced state of an initial S5-state of a definite action theory is also an initial S5-state
of the action theory.
f, s̃) be
Lemma 14 Let (M, s) be a S5-state such that every state u in M [S] is reachable from s. Furthermore, let (M
the reduced state of (M, s). It holds that
f, s̃) |= ψ;
1. (M, s) |= ψ iff (M
f, s̃) |= C(ψ);
2. (M, s) |= C(ψ) iff (M
f, s̃) |= C(Bi ψ);
3. (M, s) |= C(Bi ψ) iff (M
f, s̃) |= C(Bi ψ ∨ Bi ¬ψ));
4. (M, s) |= C(Bi ψ ∨ Bi ¬ψ) iff (M
f, s̃) |= C(¬Bi ψ ∧ ¬Bi ¬ψ));
5. (M, s) |= C(¬Bi ψ ∧ ¬Bi ¬ψ) iff (M
where i ∈ AG and ψ is a fluent formula.
Lemmas 14 and 13, together with the result on the existence of an initial S5-state with finite number of worlds in [28],
allow us to prove Proposition 6. Observe that Proposition 6 opens the door to the use of well-known propositional
tools, such as answer set solvers, to compute the entailment relation in definite action theories. On the other hand,
the number of initial Kripke structures could still be prohibitively large. We are next interested in domains which
determine, modulo equivalence among the Kripke structures, a single initial S5-state. Observe that a state (M, s) has
two components, the Kripke structure M and the actual world s. In order for a definite action theory (I, D) to have a
unique initial S5-state, the possible interpretation of the real state of the world must be unique. For M to be unique, we
observe that in the singe-agent case (e.g., in action language A, B, etc.), the initial state is unique when the knowledge
of the agent is complete. We generalize this to the case of multi-agent domains as follows.
Definition 27 A definite action theory (I, D) is complete if
{` | ` appears in one of the statements of the form (12)—(14) in I}
is a complete interpretation of F.
Intuitively, a complete definite action theory is such that the knowledge of each agent is known to all other agents and
the actual world is completely specified.
Proposition 15 For a consistent and complete action theory (I, D), there is a unique initial S5-state (M0 , s0 ) with
|M0 [S]| ≤ 2|F | such that every initial S5-state (M, s) of (I, D) is equivalent to (M0 , s0 ).
Propositions 6-15 show that definite action theories have finitely many (or unique) initial S5-states. The second
condition of Definition 26, on the other hand, indicates that the size of the set of initial statements of a definite action
theory is infinite as there are infinitely many fluent formulae. Clearly, this is not desirable. To address this issue, we will
next propose a simplification of Definition 26. This simplification is similar to the use of the Closed World Assumption
(CWA) to represent incomplete information in databases. More specifically, we propose that initial statements of the
form (16) are given implicitly, i.e., by representing the information that the agents do not know implicitly. This can be
realized as follows.
For a set of fluents F, a disjunction ϕ over F is called a complete clause if for every f ∈ F, either f or ¬f appears
in ϕ but not both. For a set of initial statements I and an agent i ∈ AG, let
	

C(I, i) = ϕ ϕ is a complete clause,TI 6|= C(Bi ϕ) and TI 6|= C(Bi ϕ ∨ Bi ¬ϕ)
S
Let neg(I) = i∈AG {initially C(¬Bi ϕ∧¬Bi ¬ϕ) | ϕ ∈ C(I, i)}. The completion of I is comp(I) = I ∪neg(I). It
is easy to see that for any action theory (I, D) such that I contains only statements of the form (12)-(16), (comp(I), D)
is a definite action theory. This means that we can specify a definite action theory by specifying only statements of
the form (12)-(15). Such a specification is obviously finite. Under this consideration, we can define a definite action
theory as follows.
36

Definition 28 (Definite Action Theory under CWA) An action theory (I, D) is called a definite action theory under
the CWA if I contains only initial statements of the form (12)-(16).
An initial state of a definite action theory under the CWA (I, D) if it is an initial state of (comp(I), D).
Observe that if we consider the action theory (I1 , D1 ) in Example 19 under the CWA, (M0 , s0 ) and (M0 , s1 ) are the
only two initial S5-states of (comp(I1 ), D1 ) and thus the entailment proved in Example 19 indeed hold.
It is easy to see that S5-entailment of definite action theories under the CWA can be computed thanks to Proposition 6. Studying the complexity of this problem and developing algorithms for computing the S5-entailment in definite
action theories under the CWA are interesting research topics and the focus of our future work. O

7

Related Work and Discussion

In this section, we connect our work to related efforts in reasoning about actions and their effects in multi-agent
domains. The background literature spans multiple areas. We will give a quick introduction and focus our attention
on the most closely related works.

7.1

Related Logics

The research discussed in this paper relates to a broad variety of logics and languages used to deal with reasoning about
actions and their effects in multi-agent domains—e.g., classical logic, non-monotonic logics, causal logics, high level
action languages, modal logics, epistemic logics, dynamic logics and dynamic epistemic logics. Here, we provide a
very brief overview of their relevance to reasoning about actions and their effects in multi-agent domains.
Classical logics, in particular, propositional and first-order logic, are often used to specify the physical state of the
world. For example, the propositional formula on table a ∧ on table b ∧ ¬on table c expresses that the blocks a and
b are on the table and the block c is not of the table. Similarly, the first order formula on table(a) ∧ on table(b) ∧
¬on table(b)∧∀X(ontable(X) ⇒ color(X, red)), describes a situation where the blocks a and b are on the table, the
block c is not on the table, and all blocks on the table are red. Propositional logic and first-order logic can be used to
represent the effects of actions and to reason about them. However, straightforward encodings require a large number
of axioms, especially to represent the inertia axioms—the properties of the world that do not change when a particular
action is performed. Two approaches can be considered to address this problem: (i) By using non-monotonic logics,
that can naturally express statements of the type “Normally an action does not affect a property” and can express
exceptions to this statement; (ii) By an alternative approach, used for example in [33], where the effects of actions
on various properties of the world are expressed using a high-level logic, which is then translated, using sophisticated
compilation techniques, into succinct encodings of inertia axioms in a classical logic.
While reasoning about the effect of actions, the relationship between some properties of the world may give rise to
“qualification” and “ramification”. Expressing this in classical logic leads to problems in many cases, especially when
the relationship between the properties are causal in nature. For example consider the two statements:
(a) A person cannot be in two places at the same time and
(b) A person cannot be married to two persons at the same time.
In classical logic, their representations are very similar: at(X) ∧ at(Y ) ⇒ X = Y and married to(X) ∧
married to(Y ) ⇒ X = Y . But let us assume that, initially, a person is at location p and he performs the action
of moving to a location q different from p. The statement (a) causes a ramification—since we would like to infer that,
after the execution of the action, the person is at p and not at q. On the other hand, let us assume that initially a person
is married to p and he (tries to) perform the action of marrying q (in a courthouse with marriage records) different
from p. The statement (b), this time, introduces a qualification, because of which the person is unable to perform the
action of marrying q.
Causal logic allows us to express (a) and (b) in a different way: at(X) ∧ X 6= Y causes ¬at(Y ) and
married to(X) ∧ married to(Y ) ∧ X 6= Y causes F ALSE. Such causal relationships can be expressed using logic programming, which has a non-classical connective “←.” In logic programming (a) and (b) can be expressed
as: ¬at(Y ) ← at(X), X 6= Y and ← married to(X), married to(Y ), X 6= Y .
37

High-level action languages were introduced to (a) provide a English-like specification language to describe the
effects of actions on properties of the world and the relationships between these properties, and to provide a semantics
that uses simple set theoretical notations; and (b) provide a framework that can be used to prove correctness of encodings in various logics for reasoning about effect of actions. For example, specifications in the language A [4] are of
the forms: (i) a causes f if p1 , . . . , pn ; and (ii) f after a1 , . . . an .
Modal logics add modal operators to various logics and their semantics is often defined using Kripke structures.
Two simple modal logics that are relevant to reasoning about actions are temporal logics and epistemic logics. One of
the simplest temporal logic is the forward linear temporal logic; it includes the operators 2, 
, 3, and U, meaning
“always in the future”, “next time step”, “sometime in the future” and “until.” Formulae in this logic are often used
to express goals that specify how we want the world to evolve. Reasoning is commonly focused on action-plans, to
verify if an action-plan satisfies a desired temporal specification, or to derive action-plans that satisfy a goal, given as
a temporal specification. Epistemic logics use the modal operators Ki , where Ki f means that agent i knows that f is
true. In this paper, we used belief logics which are similar to epistemic logics, the main difference being that we use
the modal operators Bi , where Bi f means that the agent i believes that f is true.
The community working on reasoning about actions and change initially used only the sequencing constructs
to build action-plans as sequence of actions and reason about such sequences. This has been extended [34, 32] to
allow “if-then” and other procedural features, that become necessary when sensing actions are needed to be part of
plans. GOLOG [31], an acronym for “alGOl in LOGic”, borrows programming constructs from procedural languages
to express complex plans which can be “evaluated” to generate valid action sequences. The evaluation of GOLOG
programs, as well as other action-plans, is realized on top of action theories that allow one to reason about a single
action and its impact on the world—and, in the process, take into account the issues regarding inertia, qualification
and ramification.
A related area of research is focused on the exploration of Dynamic logics. Dynamic logics were originally developed to reason about program correctness. In the programming domain, the basic actions are assignments of values to
variables. Reasoning about the effects of such assignment actions and the associated inertia is straightforward, since
all variables retain their old values except for the variables being assigned. In traditional procedural programming languages, one does not have to worry about qualification and ramification, since assigning a value to a variable does not
have any implicit impact on other variables.8 Any needed impact is explicitly written as new assignment statements.
The original focus of dynamic logics is to reason about programs that are built by composing simple assignment statements. Such programs are built using constructs such as: (i) α ∪ β (i.e., execute α or β), (ii) α; β (i.e., execute α
followed by β), (iii) α∗ (i.e., iterate α a finite number of times), (iv) p? (i.e., test whether α holds), and (v) λ (i.e.,
no-op).
To reason about programs in dynamic logics, programs are used as modal operators; various axioms and inference
rules have been developed that allow one to reason about the programs. The modal constructs used are of the form:
(i) [a]p, meaning that, after performing a, it is necessarily the case that p is true in the world, and (ii) haip, meaning
that, after performing a, it is possible that p is true in the world. These modal constructs allow one to specify effects
of arbitrary actions. For example, to express that an action a makes the formula φ true, one can write [a]φ. Similarly,
to express that a makes the formula φ true only when executed in a state where ψ is true, one can write [λ](ψ ⇒ [a]φ).
However, when we go beyond assignment actions, one needs to account for inertia. Similarly, when one goes beyond
the programming environment and variable assignments, one needs to worry about qualification and ramification. The
inertia axioms can be expressed in dynamic logic as shown in [35], which is similar to writing inertia axioms using
classical logic—i.e., they are encoded using a large number of formulae, that list one by one what properties are not
affected by an action. Such approach to account for ramification is tedious and involves the use of logic programming,
pre-compilation and then generating dynamic logic formulae based on such pre-compilation. In [36], where dynamic
logic is used for reasoning about actions and change, inertia is avoided by referring to it as an undesirable overcommitment; however, the authors admit that their formulation cannot address qualification, and they indicate that
“non-monotonic logics are clearly superior” in that regard. Overall, when reasoning about actions and change (beyond
simple variable assignments) using dynamic logics, one still needs to worry about inertia and the frame problem,
qualification and ramification issues.
8 If

we ignore issues of aliasing, e.g., through pointers.

38

Many early works about action and change, as well as dynamic logics, reason about the world and do not worry
about agent’s knowledge about the world. When sensing actions are considered, one has to distinguish between the
world and a single agent’s knowledge about the world. This leads to the use of epistemic logics and Kripke structures,
and frame axioms need to be developed with respect to knowledge formulae. This has been achieved by having frame
axioms for the accessibility relations used in the Kripke structures. High level languages that can express sensing
actions and their effects have been developed and matched with logical encodings. This paper extends such approach
to the case of multi-agent domains, where other knowledge actions (besides sensing) are considered. A new dimension
that emerges is the observability of the various agents as part of an action; some may have full observability, some
others may have partial observability, and the rest have no observability. The result of such actions and such varied
observability is that different agents have different knowledge and beliefs about the world and about each other’s
knowledge and beliefs.
Several researchers have considered reasoning about actions in a multi-agent domain using the dynamic logics
approach. These proposals are focused on extending dynamic logics to Dynamic Epistemic Logics (DEL), to reason
about the agent’s knowledge about the world and about other agents’ knowledge in presence of multi-agent actions.
The extensions are twofold. In particular, in the formula [α]p: (i) In DEL, p is a formula in epistemic logic, while in
dynamic logic p is a classical logic formula, and (ii) In DEL, α is a more general action than in dynamic logic. In
[24], α has the usual dynamic logic constructs for complex actions, plus constructs such as: (a) LA ?ϕ whose intuitive
meaning is that the agents in the group A learn that ϕ is true; and (b) (α!α0 ) whose intuitive meaning is that between
α and α0 , we choose α. The latter is often written as (!α ∪ α0 ). Using these actions, the authors of [24] show that one
can express sensing actions (refereed to as “read” actions) in a multi-agent setting. Specifically, the fact that we have
two agents a and b, a senses the value of p as being true, and it is common knowledge between a and b that b observes
a sensing, can be modeled as the complex action Lab (!La ?p ∪ La ?¬p). However, the language of update models [20]
is more general and is currently preferred by the DEL community to express such multi-agent actions. In the language
of update models, the action discussed above can be expressed as in Figure 17.

Figure 17: A program model for Lab (!La ?p ∪ La ?¬p)

7.2

Relating mA+ and DEL with Update Models: Differences

The similarities between mA+ and the update models based approach has been discussed in detail in Section 5. We
next detail the differences between the two approaches.
7.2.1

Multi-Agent Actions

A key difference between the formalism proposed in this paper and DEL with update models is with respect to the
simplest of actions in presence of multiple agents.
Let us consider the simplified version of the coin in a box problem as presented in Example 2—with three agents
A, B, and C, a box containing a coin, and initially it is common knowledge that none of the agents knows whether
the coin lies heads up or tails up. Let us assume that agent A peeks into the box. In our formalism, we express this
action as peek(A). In DEL, the update model for the same action, as given in Figure 14, will also include additional
information about all three agents A, B, and C encoding their “roles” or “perspectives” while A is peeking into the
box. By roles or perspectives we mean information about what the agents are doing, in terms of who is watching
whom and who knows about that.
It is evident that our representation of the action simply as peek(A) is much simpler than the DEL representation
in Figure 14. But our representation does not include the information about what else the agents A, B, and C are
39

doing while A is peeking into the box. In our formulation, such information is part of the state, and is expressed
by using perspective fluents, such as looking(B)—that encodes the information that B is looking at the box—and
group member(B,group(A))—that encodes the information that B and A are together in the same group.
Thus, it appears that a critical difference between the mA+ approach to representing multi-agent actions and
the approach used in DEL with update models lies in the way we encode the information about agents roles and
perspectives—as part of the action in DEL with update models and as part of the state in mA+. At first glance, this
difference may not appear far reaching. However, there are some far reaching implications of such difference, as
discussed in the following subsections.
Narratives and Dynamic Evolution of Multi-agent Actions: Let us consider a scenario with two agents A and B.
Initially, agent B is looking at agent A. Agent A lifts a block and, after some time, agent A puts down the block. Some
time later, agent B is distracted while agent A again lifts the block.
In our formulation, this narrative can be formalized by first describing the initial situation, and then describing the
sequence of actions that occurred, which for this example is:
liftBlock(A); putDown(A); distract(B); liftBlock(A).
The description of this evolution of scenario in DEL is not as simple: each action occurrence will have to be described
as an update model containing information about both agents A and B. In addition, such a description (in DEL) will
be partly superfluous, as it will have to record information about B looking (or not looking) at A in the update model,
when that information is already part of the state. Thus, the approach used in mA+ to describe this narrative is more
natural than the representation in DEL.
Observe that, in our narrative, the action liftBlock(A) appears twice. However, due to the difference in the roles
and perspectives over time, the two occurrences of liftBlock(A) correspond to two different update models. This shows
how, using the mA+ formulation, we can support the dynamic evolution of update models, as result of changes in
perspective fluents in the state. In DEL, the two update models are distinct and there is no direct connection between
them and neither one does evolve from the other.
In order to further reinforce this point, let us consider another narrative example. Let us consider a scenario with
three agents, A, B, and C. Initially, it is common knowledge that none of the agents knows whether the coin in the
box is lying heads up or tails up. In addition, let us assume that initially A and B are looking at the box, while C is
looking away. Let us consider the narrative where A peeks into the box; afterwards, A realizes that C is distracted
and signals C to look at the box as well; finally A peeks into the box one more time. In mA+, this situation can be
described again by a sequence of actions:
peek(A); signal(C); peek(A)
The two occurrences of peek(A) correspond to two different update models; the second occurrence is an evolution
of the first caused by the execution of signal(C). In DEL, the relevance of the intermediate action signal(C), and its
impact on the second occurrence of peek(A), is mostly lost—and this results in the use of two distinct update models
for peek(A) with complete information about the whole action scenario.
The key aspect that allows a natural representation of narratives and evolution of update models in mA+ is the
presence of the agents’ perspectives and roles encoded as perspective fluents of a state, and their use to dynamically
generate the update models of the actions. While DEL can include perspective fluents as part of the states as well, it
does not have a way to take advantage of them in a similar way as mA+.
Separation of Specification of Actions and their Effects and the Observability of an Agent: As alluded in the
previous two sections, a core difference between mA+ and the DEL specification of actions in multi-agent domains,
lies in that mA+ separates the specification of actions and action effects from the description of observability of the
action occurrence by each agent. In both [24] and [20], the observability of agents is hard-coded in the specification
of each complex action. We discuss this difference in more detail using an example.
Let us reconsider the domain D1 . In order to describe the possible histories of the domain, we need to develop
the update models for every action occurrence. Let us consider, for example, the action peek(A); we need to have an
update model for all of the following cases:
40

• Both B and C are looking;
• Either B or C is looking but not both; and
• Both B C are not looking.
In our approach, the above example is specified in a very different way: the action is about sensing tail. The agents
who sense it, who observe the sensing take place, and who are oblivious can be specified directly or can be specified
indirectly in terms of conditions, such as which agents are near the sensing, which ones are watching from far, and
which ones are looking away, respectively. Actions can be planned and executed to change the observers and partial
observers. In other words, in mA+, if we have a complex action α; β, by executing α we may be able to change the
world, in terms of who is fully observing, who is partially observing, and who is oblivious with respect to the next
action β. This is not possible in DEL. Thus, while mA+ allows us to develop plans where an agent can manipulate
the observability of other agents, such planning cannot be done in straightforward manner in DEL.
Simplicity by Design: The formulation adopted in this paper is limited in expressivity to ensure simplicity. It is
limited by the (perspective) fluents we have and how we use them. On the other hand, DEL is more complex and also
more expressive.
One advantage of our simplicity is that it limits the number of possible plans of a particular length, contributing to
the feasibility of multi-agent planning. In DEL, since update models are analogous to Kripke models, even in presence
of a small number fluents, it is possible to generate an infinite number of update models. One can limit the number of
update models by imposing restrictions on their structure. Nevertheless, as discussed earlier, an update model is much
more complex than actions mA+, which are often9 single units.
As an example,10 Figure 18 displays an update model that cannot be represented in mA+. The intuition behind
this update model is as follows. When A executes the action peek(A), A believes that both A and B can see the
outcome of sensing tail—i.e., A and B are fully observant. In reality, B is oblivious. This shows that, in multi-agent
domains, an agent’s observability could also be considered as beliefs, and as such affect the beliefs of an agent about
other agents’ beliefs after the execution of an action. The present mA+ language does not allow for such specification.
pre: tail

pre: tail

A

!

"

A,B

B
#

A,B

pre: T

Figure 18: An update model of the peek(A) action without an equivalent mA+ representation
The simplicity of our formulation is by design, and not an inherent flaw of our approach. Indeed, one could
envision developing a complete encoding of the complex graph structure of an update model as part of state, using
an extended collection of perspective fluents—but, at this time, we do not have a corresponding theory of change to
guide us in using these more expressive perspective fluents to capture the full expressive power of update models in
DEL. Hence, our current formalism is less expressive than DEL. However, the higher expressiveness of update models
provides us with a target to expand mA+ and capture more general actions. Expanding mA+ to express actions as
the one in Fig. 18 will be one of our immediate future goal.
On the other hand, as remarked earlier, the research on DEL with update models lacks at present an exploration of
how update models can evolve as result of action executions.
9 We
10

could allow parallel actions.
We thank an anonymous reviewer of an earlier version of this paper who suggested a similar example.

41

Analogy with Belief Update: Another approach to explore the differences between mA+ and DEL builds on the
analogy to the corresponding differences between belief updates and the treatment of actions and change in early
action languages [7].
Papers on belief updates define and study the problem of updating a formula φ with a formula ψ. In contrast, in
reasoning about actions and change, the focus is on defining the resulting state of the world after a particular action is
performed in a particular world, given a description of (i) how the action may change the world, (ii) when the action
can be executed; and (iii) how the fluents in the world may be (possibly causally) related to each other. In such a
context, given a state s and an action a, it is possible to see the the determination of the resulting state as the update
of s by a formula ϕ; But, what is important to consider is that the ϕ is not just the collection of effects of the action
a, but incorporates several other components, that take into account the static causal laws as well as which conditions
(part of the conditional effects of a) are true in s.
This situation is not dissimilar to the distinction between DEL update models and mA+. An update model can
be encoded by an action formula, and the resulting state can be obtained by updating the starting state with such
formula. In DEL, such action formula has to be given directly. Instead, our considerations in mA+ are in the spirit
of the early research in reasoning about actions and change—where we focus on describing actions and their effects,
their executability conditions, and where a resulting “state” is determined by applying these descriptions to the “state”
where a particular action is performed. Thus, the action formula in this latter case is not explicitly given, but derived
from the description of the actions, their effects, and executability conditions.
Taking the analogy further, an important application of reasoning about actions is to determine action sequences
or plan structures that achieve a given goal. This is different from searching for a formula ψ which, if used to update
a given initial state, will generate a goal state; the difference is partly due to the fact that the the space of formulae
is infinite. Similarly, the space of update models is infinite, and it is not viable to look for an update model (or
a sequence of update models) that will cause a transition from a given initial state to a goal state. Instead, mA+
supports the traditional way of planning by finding action sequences or plan structures that achieve a goal.
Executing Actions: The notion of actions adopted in mA+ is designed to enable their executions by one or multiple
agents. For example, the action peek(A) can be executed by agent A, by peeking into the box. On the other hand,
an action modeled using update models in DEL is not executable, in the normal sense. For example, how does the
action expressed in Figure 14 get executed? Who does execute such action? What does executing the various edges of
Figure 14 mean? The answers to these questions are not clear.
Furthermore, let us turn around such questions and focus on the perspective fluents: how does one execute the
perspective fluents, such as looking(B)? The answer is that they are fluents, and they are not required or supposed
to be executed. A more appropriate question would be: how do they become true? The answer is that, while our
formulation could have some actions that make them true, when describing a state we need not worry about how
exactly the facts in the state came about to be true. This is not the case when describing actions: when describing
actions we need to describe something that can be executed. In summary, actions, or parts of actions, are supposed to
be something that can be executed, while states, or parts of states, do not have such requirement.
Hence, our representation of actions is more appropriate, and follows the common meaning of an action,11 than
the representation of action in DEL.
Value of Update Models: Having discussed the differences between mA+ and update models, we would like to
point out that update models present a very good technical tool for the understanding of effects of actions in multiagent domains. In fact, the transition function Φ for mA+ action theories can be effectively characterized using update
models, as described in Section 5.
7.2.2

Specifying the Initial State

An important aspect of many algorithms for reasoning about actions and change (including planning) is to have a finite
set of possible “initial states”. Although most of the examples in DEL papers show a finite number of possible initial
11 For example, the relevant dictionary meaning of “action is (1) something done or performed; act; deed. (2) an act that one consciously wills
and that may be characterized by physical or mental activity.

42

states (often a single Kripke structure), they do not focus on constraining the knowledge specified about the initial
state to guarantee the finiteness of the set of possible initial states. This is an important concern of our paper, and
we propose a restricted class of knowledge about the initial states that guarantees finiteness and yet is able to capture
most of the examples in the literature. Observe that this condition identifies a class of epistemic planning problems as
defined in [37, 38, 39] whose solutions can be computed using heuristic forward search.
We note that this problem is related to the finite model property in modal logics—which defines when a theory has
(at least) one finite model [40, 41]. The problem addressed in Section 6 could be viewed as the problem of identifying a
class of epistemic theories that has (up to equivalence) finitely many finite models and is, thus, a stronger problem than
the finite model property problem. To the best of our knowledge, this more complex problem has not been addressed
in multi-modal logics before. We believe that this is an important contribution of our development of mA+.

7.3

Previous Work by the Authors

Early attempts to adapt action languages to formalize multi-agent domains can be found in [42, 43, 44]. In these
works, the action languages A, B, and C have been extended to formalize multi-agent domains.
The works in [43, 44] investigate the use of action language in multi-agent planning context and focus on the
generation of decentralized plans for multiple agents, to either jointly achieve a goal or individual goals.
In [42], we show that several examples found in the literature—created to address certain aspect in multi-agent
systems (e.g., [45, 46, 47, 48, 49, 50])—can be formalized using an extension of the action language C. Yet, most
of the extensions considered in [42, 43, 44] are inadequate for formalizing multi-agent domains in which reasoning
about knowledge of other agents is critical. To address this shortcoming, we have developed and investigated several
preliminary versions of mA+ [51, 52, 53]. We started with an attempt to formulate knowledge of multiple agents in
[51]; we successively extended this preliminary version of mA+ with the use of static observability specifications in
[52]. The language developed in this paper subsumes that of [52]. In [53], we demonstrated the use of update models
to describe the transition function for the action language mA+ of [52].

7.4 mA+ and Action Languages for Single-Agent Domains
mA+ is a high-level action language for multi-agent domains. It is therefore instructive to discuss the connection
between mA+ and action languages for single-agent domains. First, let us observe that mA+ has the following
multi-agent domain specific features:
• it includes announcement actions; and
• it includes specification of the agents’ observability of action occurrences.
As it turns out, if we remove all features that are specific to multi-agent domains from mA+, and consider the S5entailment as its semantics, then the language is equivalent to the language AK from [32]. Formally, let us consider
a mA+ definite action theory (I, D) over the signature hAG, F, Ai such that |AG| = 1 and D does not contain
statements of the form (4) (announcement actions) and statements of the form (5)-(6). Let us define
IAK = {ϕ | ϕ appears in a statement of the form (12), (13), or (14) in I}.
Then, the following holds
(comp(I), D) |=S5 ϕ after δ iff (IAK , D) |=AK ϕ after δ.
This shows that mA+ is indeed a generalization of action languages for single-agent domains to multi-agent domains.
This also supports the claim that other elements that have been considered in action languages of single-agent domains,
such as static causal laws, non-deterministic actions, or parallel actions could potentially be generalized to mA+. This
is one of our goals for future work.

43

8

Conclusions and Future Works

In this paper, we developed an action language for representing and reasoning about effects of actions in multi-agent
domains. The language considers world-altering actions, sensing actions, and announcement actions. It also allows
the dynamic specification of agents’ observability with respect to action occurrences, enabling varying degrees of
visibility of action occurrences and action effects. The semantics of the language relies on the notion of states (pointed
Kripke structures), used as representations of the states of the world and states of agents’ knowledge and beliefs; the
semantics builds on a transition function, which maps pairs of states and actions to sets of states.
We discussed several properties of the transition function and identified a class of theories (definite action theories)
whose set of initial S5-states is finite, thus allowing for the development of algorithms for the S5-entailment relation
that is critical in applications such as planning and temporal reasoning. We also relate the proposed language to the
update model based approaches for representing and reasoning about effects of actions in multi-agent domains.
The development of mA+ is our first step towards our goal of developing automated reasoning and planning
systems in multi-agent domains. This will be our focus in the near future. In addition, we plan to extend the language
to deal with lying and/or misleading actions, refine the distinction between knowledge and beliefs of the agents, expand
the language to include non-deterministic actions and static causal laws, and specify more general models of agents’
observability, to capture some of the capabilities of update models that are missing from mA+.

Acknowledgments
The work has been partially supported by NSF grants HRD-1345232 and DGE-0947465.

44

Bibliography
References
[1] J. McCarthy, Programs with common sense, in: Proceedings of the Teddington Conference on the Mechanization
of Thought Processes, Her Majesty’s Stationery Office, London, 1959, pp. 75–91.
[2] R. Fikes, N. Nillson, STRIPS: a new approach to the application of theorem proving to problem solving, Artificial
Inteligence 2 (1971) 189–208.
[3] V. Lifschitz, On the semantics of STRIPS, in: M. Georgeff, A. Lansky (Eds.), Reasoning about Actions and
Plans, Morgan Kaufmann, San Mateo, CA., 1987, pp. 1–9.
[4] M. Gelfond, V. Lifschitz, Representing actions and change by logic programs, Journal of Logic Programming
17 (2,3,4) (1993) 301–323.
[5] E. Pednault, ADL: Exploring the middle ground between STRIPS and the situation calculus, in: R. Brachman,
H. Levesque, R. Reiter (Eds.), Proceedings of the First International Conference on Principles of Knowledge
Representation and Reasoning, Morgan Kaufmann, 1989, pp. 324–332.
[6] M. Ghallab, A. Howe, C. Knoblock, D. McDermott, A. Ram, M. Veloso, D. Weld, D. Wilkins, PDDL — the
Planning Domain Definition Language. Version 1.2, Tech. Rep. CVC TR98003/DCS TR1165, Yale Center for
Comp, Vis and Ctrl (1998).
[7] M. Gelfond, V. Lifschitz, Action Languages, Electronic Transactions on Artificial Intelligence 3 (6).
[8] C. Castellini, E. Giunchiglia, A. Tacchella, Improvements to sat-based conformant planning, in: Proceedings of
6th European Conference on Planning (ECP-01), 2001.
[9] T. C. Son, P. H. Tu, M. Gelfond, R. Morales, Conformant Planning for Domains with Constraints — A New
Approach, in: Proceedings of the Twentieth National Conference on Artificial Intelligence, 2005, pp. 1211–
1216.
[10] E. Durfee, Distributed Problem Solving and Planning, in: Muliagent Systems (A Modern Approach to Distributed Artificial Intelligence), MIT Press, 1999, pp. 121–164.
[11] M. de Weerdt, A. Bos, H. Tonino, C. Witteveen, A resource logic for multi-agent plan merging, Ann. Math. Artif.
Intell. 37 (1-2) (2003) 93–130.
[12] M. de Weerdt, B. Clement, Introduction to planning in multiagent systems, Multiagent Grid Systems 5 (2009)
345–355.
[13] M. Allen, S. Zilberstein, Complexity of decentralized control: Special cases, in: 23rd Annual Conference on
Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver,
British Columbia, Canada, Curran Associates, Inc., 2009, pp. 19–27.
[14] D. S. Bernstein, R. Givan, N. Immerman, S. Zilberstein, The complexity of decentralized control of markov
decision processes, Math. Oper. Res. 27 (4) (2002) 819–840.
[15] C. V. Goldman, S. Zilberstein, Decentralized control of cooperative systems: Categorization and complexity
analysis, Journal of Artificial Intelligence Resesearch (JAIR) 22 (2004) 143–174.
[16] C. Guestrin, D. Koller, R. Parr, Multiagent planning with factored mdps, in: T. G. Dietterich, S. Becker,
Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada],
MIT Press, 2001, pp. 1523–1530.

45

[17] R. Nair, M. Tambe, M. Yokoo, D. V. Pynadath, S. Marsella, Taming decentralized pomdps: Towards efficient
policy computation for multiagent settings, in: G. Gottlob, T. Walsh (Eds.), IJCAI-03, Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, Acapulco, Mexico, August 9-15, 2003, Morgan
Kaufmann, 2003, pp. 705–711.
[18] L. Peshkin, V. Savova, Reinforcement learning for adaptive routing, in: Proceedings of the Int. Joint Conf. on
Neural Networks, 2002.
[19] J. van Eijck, Dynamic epistemic modelling, Tech. rep. (2004).
[20] A. Baltag, L. Moss, Logics for epistemic programs, Synthese.
[21] A. Herzig, J. Lang, P. Marquis, Action Progression and Revision in Multiagent Belief Structures, in: Sixth
Workshop on Nonmonotonic Reasoning, Action, and Change (NRAC), 2005.
[22] J. van Benthem, Dynamic logic of belief revision, Journal of Applied Non-Classical Logics 17(2) (2007) 129–
155.
[23] J. van Benthem, J. van Eijck, B. P. Kooi, Logics of communication and change, Inf. Comput. 204 (11) (2006)
1620–1662.
[24] H. van Ditmarsch, W. van der Hoek, B. Kooi, Dynamic Epistemic Logic, Springer, 2007.
[25] N. Friedman, J. Halpern, Modeling Belief in Dynamic Systems: Revision and Update, Journal of Artificial
Intelligence Research 10 (1999) 117–167.
[26] H. Katsuno, A. Mendelzon, On the difference between updating a knowledge base and revising it, in: Proceedings of KR 92, 1992, pp. 387–394.
[27] del Val A., Y. Shoham, A unified view of belief revision and update, Journal of Logic and Computation, Special
issue on Actions and processes, M. Georgeff (ed.).
[28] R. Fagin, J. Halpern, Y. Moses, M. Vardi, Reasoning about Knowledge, MIT press, 1995.
[29] T. Son, E. Pontelli, C. Baral, G. Gelfond, A sufficient syntactic condition for the maintenance the kd45n property
using update models, (Unpublished manuscript) (2014).
[30] T. C. Son, E. Pontelli, C. Baral, G. Gelfond, A sufficient syntactic condition for the maintenance of the KD45n
property using update models, Tech. rep. (2014).
[31] H. Levesque, R. Reiter, Y. Lesperance, F. Lin, R. Scherl, GOLOG: A logic programming language for dynamic
domains, Journal of Logic Programming 31 (1-3) (1997) 59–84.
[32] T. C. Son, C. Baral, Formalizing sensing actions - a transition function based approach, Artificial Intelligence
125 (1-2) (2001) 19–91.
[33] R. Reiter, KNOWLEDGE IN ACTION: Logical Foundations for Describing and Implementing Dynamical Systems, MIT Press, 2001.
[34] R. Scherl, H. Levesque, Knowledge, action, and the frame problem, Artificial Intelligence 144 (1-2).
[35] J.-J. Meyer, Dynamic logic for reasoning about actions and agents, in: J. Minker (Ed.), Logic-Based Artificial
Intelligence, Boston/Dordrecht: Kluwer, 2000, pp. 281–311, chapter 13.
[36] H. Prendinger, G. Schurz, Reasoning about action and change - a dynamic logic approach, Journal of Logic,
Language, and Information 5 (1996) 5–209.

46

[37] W. van der Hoek, M. Wooldridge, Tractable multiagent planning for epistemic goals, in: The First International
Joint Conference on Autonomous Agents & Multiagent Systems, AAMAS 2002, July 15-19, 2002, Bologna,
Italy, Proceedings, ACM, 2002, pp. 1167–1174.
[38] B. Löwe, E. Pacuit, A. Witzel, Del planning and some tractable cases, in: H. van Ditmarsch, J. Lang, S. Ju
(Eds.), Logic, Rationality, and Interaction, Vol. 6953 of Lecture Notes in Computer Science, Springer Berlin /
Heidelberg, 2011, pp. 179–192.
[39] T. Bolander, M. Andersen, Epistemic Planning for Single and Multi-Agent Systems, Journal of Applied NonClassical Logics 21 (1).
[40] D. Gabbay, A. Kurucz, F. Wolter, M. Zakharyaschev, Many-Dimensional Modal Logics: Theory and Application,
Elsevier, 2003.
[41] J. van Benthem, Modal Logic for Open Minds, Center for the Study of Language and Information, 2010.
[42] C. Baral, T. C. Son, E. Pontelli, Reasoning about multi-agent domains using action language C: A preliminary
study, in: J. Dix, M. Fisher, P. Novák (Eds.), Computational Logic in Multi-Agent Systems - 10th International
Workshop, CLIMA X, Hamburg, Germany, September 9-10, 2009, Revised Selected and Invited Papers, Vol.
6214 of Lecture Notes in Computer Science, Springer, 2010, pp. 46–63.
[43] T. Son, E. Pontelli, C. Sakama, Logic Programming for Multiagent Planning with Negotiation, in: P. M. Hill,
D. S. Warren (Eds.), Logic Programming, 25th International Conference, ICLP 2009, Pasadena, CA, USA, July
14-17, 2009. Proceedings, Vol. 5649 of Lecture Notes in Computer Science, Springer, 2009, pp. 99–114.
[44] T. Son, C. Sakama, Reasoning and planning with cooperative actions for multiagents using answer set programming, in: M. Baldoni, J. Bentahar, J. Lloyd, B. van Riemsdijk (Eds.), Declarative Agent Languages and Technologies VI, 6th International Workshop, DALT 2009, Budapest, Hungary, 2009, Revised Selected and Invited
Papers, Vol. 5948, Springer, 2009, pp. 208–227.
[45] G. Boella, L. W. N. van der Torre, Enforceable social laws, in: F. Dignum, V. Dignum, S. Koenig, S. Kraus,
M. P. Singh, M. Wooldridge (Eds.), 4rd International Joint Conference on Autonomous Agents and Multiagent
Systems (AAMAS 2005), July 25-29, 2005, Utrecht, The Netherlands, ACM, 2005, pp. 682–689.
[46] J. Gerbrandy, Logics of propositional control, in: H. Nakashima, M. P. Wellman, G. Weiss, P. Stone (Eds.),
5th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS 2006), Hakodate,
Japan, May 8-12, 2006, ACM, 2006, pp. 193–200.
[47] W. van der Hoek, W. Jamroga, M. Wooldridge, A logic for strategic reasoning, in: F. Dignum, V. Dignum,
S. Koenig, S. Kraus, M. P. Singh, M. Wooldridge (Eds.), 4rd International Joint Conference on Autonomous
Agents and Multiagent Systems (AAMAS 2005), July 25-29, 2005, Utrecht, The Netherlands, ACM, 2005, pp.
157–164.
[48] A. Herzig, N. Troquard, Knowing how to play: uniform choices in logics of agency, in: H. Nakashima, M. P.
Wellman, G. Weiss, P. Stone (Eds.), 5th International Joint Conference on Autonomous Agents and Multiagent
Systems (AAMAS 2006), Hakodate, Japan, May 8-12, 2006, 2006, pp. 209–216.
[49] L. Sauro, J. Gerbrandy, W. van der Hoek, M. Wooldridge, Reasoning about action and cooperation, in: AAMAS
’06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, ACM,
New York, NY, USA, 2006, pp. 185–192.
[50] M. T. J. Spaan, G. J. Gordon, N. A. Vlassis, Decentralized planning under uncertainty for teams of communicating agents, in: H. Nakashima, M. P. Wellman, G. Weiss, P. Stone (Eds.), 5th International Joint Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2006), Hakodate, Japan, May 8-12, 2006, 2006, pp.
249–256.

47

[51] C. Baral, G. Gelfond, E. Pontelli, T. Son, Modeling multi-agent scenarios involving agents knowledge about
other’s knowledge using ASP, in: Proceedings of the 9th International Conference on Autonomous Agents and
Multiagent Systems, International Foundation for Autonomous Agents and Multiagent Systems, 2010, pp. 259–
266.
[52] E. Pontelli, T. Son, C. Baral, G. Gelfond, Logic programming for finding models in the logics of knowledge and
its applications: A case study, Theory and Practice of Logic Programming 10 (4-6) (2010) 675–690.
[53] C. Baral, G. Gelfond, On representing actions in multi-agent domains, in: Proceedings of the Symposium on
Constructive Mathematics, 2010.

48

Appendix: Proofs
Proposition 2 Let D be a consistent domain, (M, s) be a state, a be a world-altering action that is executable in
0 0
(M, s) and Φw
D (a, (M, s)) = {(M , s )}. Then, the following holds
1. For every pair of a world r(a, u) ∈ M 0 [S] \ M [S] and literal ` ∈ eD (a, M, u), (M 0 , r(a, u)) |= `;
2. For every pair of a world r(a, u) ∈ M 0 [S] \ M [S] and literal ` ∈ F \ {`, `¯ | ` ∈ eD (a, M, u)}, (M, u) |= ` iff
(M 0 , r(a, u)) |= `;
3. For every i ∈ OD (a, M, s) and formula ϕ, (M 0 , s0 ) |= Bi ϕ iff (M, s) |= Bi ϕ.
Proof.
1. We have that r(a, u) ∈ M 0 [S] \ M [S] implies that r(a, u) ∈ Res(a, M, s) and thus there exits some u ∈ M [S]
such that a is executable in (M, u). Because of the definition of Res(a, M, s), we have that for ` ∈ eD (a, M, u),
M 0 [π](r(a, u))(`) = >, and thus (M 0 , r(a, u)) |= `.
2. Similar to the above item, the conclusion for this item follows immediately from the definition of
Res(a, M, s)[π].
3. Assume that i ∈ OD (a, M, s) and ϕ an arbitrary formula. By the definition of M 0 , we have that (s0 , v) ∈ M 0 [i]
iff v ∈ M [S] and (s, v) ∈ M [i]. Furthermore, for each u ∈ M [S] and j ∈ AG we have that (v, u) ∈ M [j] iff
(v, u) ∈ M 0 [j]. This shows that (M 0 , s0 ) |= Bi ϕ iff (M, s) |= Bi ϕ.
2
Lemma 15 Let (M, s) be a state, α a group of agents, and f a fluent. Let us assume that for every i ∈ α and
(u, v) ∈ M [i], M [π](u)(f ) = M [π](v)(f ). Furthermore, for each u ∈ M [S] and i ∈ α, {v | (u, v) ∈ M [i]} =
6 ∅.
Then, (M, s) |= Cα f if (M, s) |= f and (M, s) |= Cα ¬f if (M, s) |= ¬f .
Proof. Without loss of generality, let us assume that (M, s) |= f . We say that a world u ∈ M [S] is reachable
from s via α if there exists a sequence of agents i1 , . . . , it and world s = v0 , v1 , . . . , vt = u such that ij ∈ α and
(vj−1 , vj ) ∈ M [ij for j = 1, . . . , t. It is easy to see that (M, u) |= f for every u reachable from s via α. Let us prove
by induction on k that (M, u) |= Eαk f for every u ∈ M [S] reachable from s via α. The base case is trivial, since
(M, u) |= f and f = Eα0 f . Let us now assume that (M, u) |= Eαk f for every u reachable from s via α. We will show
that (M, u) |= Bi Eαk f for i ∈ α and u is reachable via α. Consider v such that (u, v) ∈ M [i]. Clearly, v is reachable
from s via α. By inductive hypothesis, we have that (M, v) |= Eαk f . Since this holds for every v ∈ M [S] such that
(u, v) ∈ M [i] and there exists at least on such world v, we can conclude that (M, u) |= Bi Eαk f . This implies the
conclusion for the inductive step and proves the lemma for the case (M, s) |= f .
The proof for the case (M, s) |= ¬f is analogous.
2
Proposition 3 Let D be a consistent domain, a be a sensing action, and (M, s) be a state. Suppose that a is executable
in (M, s) and (M, s) is consistency preserving for a. Assume that (M 0 , s0 ) ∈ ΦsD (a, (M, s)). Then, the following
holds
1. for every f ∈ SensedD (a), if (M, s) |= ` then (M 0 , s0 ) |= CFD (a,M,s) `, where ` ∈ {f, ¬f };
2. for every f ∈ SensedD (a), (M 0 , s0 ) |= CPD (a,M,s) (CFD (a,M,s) f ∨ CFD (a,M,s) ¬f ); and
3. for every i ∈ OD (a, M, s) and formula ϕ, (M 0 , s0 ) |= Bi ϕ iff (M, s) |= Bi ϕ.
Proof. Recall that

−1

(M 0 , s0 ) = (M, s) ]cFD (a,M,s)∪PD (a,M,s) (M 00 , s00 )
where

49

a

• (M 00 , s00 ) = (M r |FD (a,M,s)∪PD (a,M,s) 	Rema (M r ), c(s)) and
• (M r , c(s)) is a c-replica of (M, s).
We observe that for every i ∈ FD (a, M, s) ∪ PD (a, M, s),
(u, v) ∈ M 0 [i] iff (u, v) ∈ M 00 [i]

(17)

and for each u ∈ M 00 [i], it holds that {v | (u, v) ∈ M 00 [i]} =
6 ∅ because M is consistency preserving for a.
1. Consider the structure (M 00 , s00 ) and some f ∈ SensedD (a). It holds that M 00 [π](u)(f ) = M 00 [π](v)(f ) for i ∈
FD (a, M, s) and (u, v) ∈ M 00 [i]. Thus, (M 00 , s00 ) |= CFD (a,M,s) f or (M 00 , s00 ) |= CFD (a,M,s) ¬f (Lemma 15).
Because of (17), we have that (M 0 , s0 ) |= CFD (a,M,s) ` iff (M 00 , s00 ) |= CFD (a,M,s) ` for ` ∈ {f, ¬f }. This
proves the first item.
2. Observe that for a sequence of links (s0 , j1 , s1 ), . . . , (sk−1 , jk , sk ) in M 0 such that jl ∈ PD (a, M, s) for l =
1, . . . , k, we have that si ∈ M 00 [S]. This, together with the fact that (M 00 , u) |= CFD (a,M,s) f ∨ CFD (a,M,s) ¬f
for every u ∈ M 00 [S] and (17), implies the second item.
3. The third item is similar to the third item of Proposition 2.
2
Proposition 5 Given an action a and a state (M, s), we have that for each (M 0 , s0 ) ∈ (M, s) ⊗ Ω(a, (M, s)) there
exists an equivalent element in ΦD (a, (M, s)) and vice versa.
Proof. If a is not executable in (M, s) then (M, s) ⊗ Ω(a, (M, s)) = ΦD (a, (M, s)) = ∅ so the proposition is trivial
for this case.
Let us assume that a is executable in (M, s) and the precondition of a is ψ. Furthermore, let ρ =
(FD (a, M, s), PD (a, M, s), OD (a, M, s)). We consider three cases:
1. a is a world-altering action. Following Definitions 15 and 25, we have that ΦD (a, (M, s)) = {(M 0 , s0 )} and
Ω(a, (M, s)) = (ω(a, ρ), {σ}) with ω(a, ρ) as in Definition 22.
By the definition of ⊗ operator between a state and an update instance we have that (M, s) ⊗ Ω(a, (M, s)) is
a singleton. Thus, let us denote the unique element in (M, s) ⊗ Ω(a, (M, s)) with (W, w) where each world in
W [S] is of the form (u, σ) or (u, ) for some u ∈ M [S] and w = (s, σ).
We will show that (W, w) is equivalent to (M 0 , s0 ). We define the function h : W [S] → M 0 [S] as follows:

r(a, u) if γ = σ
h((u, γ)) =
u
if γ = 
Observe that (u, σ) ∈ W [S] iff (M, u) |= pre(σ) = ψ iff a is executable in u iff r(a, u) ∈ Res(a, M, s)[S] iff
r(a, u) ∈ M 0 [S].
Furthermore, (u, ) ∈ W [S] iff u ∈ M [S] iff u ∈ M 0 [S] \ Res(a, M, s)[S]. This allows us to conclude that h
is a bijective function f from W [S] to M 0 [S].
(*)
We will show next that W [π]((u, γ)) ≡ M 0 [π](h((u, γ))).

(**)
0

0

Consider γ = . For each u ∈ M [S], we have that W [π]((u, )) ≡ M [π](u) ≡ M [π](u) = M [π](f ((u, ))).
We now show that W [π]((u, σ)) ≡ M 0 [π](r(a, u)). Consider p ∈ F. We have that W [π]((u, σ))(p) = >
iff p → Ψ+ (p, a) ∨ (p ∧ ¬Ψ− (p, a)) ∈ sub(σ) and (M, u) |= Ψ+ (p, a) ∨ (p ∧ ¬Ψ− (p, a))
iff either (a) there exists a statement “a causes p if ϕ” in D such that (M, u) |= ϕ; or (b) (M, u) |= p and
(M, u) |= ¬ϕ for every statement “a causes ¬p if ϕ” in D
iff either (a) p ∈ eD (a, M, u) or (b) p 6∈ eD (a, M, u) and ¬p 6∈ eD (a, M, u) iff M 0 [π](r(a, u))(p) = >. This
implies that (**) holds.
Given (*) and (**), it is easy to see that the equivalence between (W, w) and (M 0 , s0 ) will be proved if we have
that ((u, γ1 ), (v, γ2 )) ∈ W [i] iff (h(u, γ1 ), h(v, γ2 )) ∈ M 0 [i]. Consider two cases:
50

• i ∈ FD (a, M, s). There are two cases:
– ((u, σ), (v, σ)) ∈ W [i] iff (u, v) ∈ M [i] and (M, u) |= pre(σ) and (M, v) |= pre(σ) iff (u, v) ∈
M [i] and r(a, u), r(a, v) ∈ Res(a, M, s)[S] iff (r(a, u), r(a, v)) ∈ M 0 [i];
– ((u, ), (v, )) ∈ W [i] iff (u, v) ∈ M [i] iff (u, v) ∈ M [i] and u, v ∈ M 0 [S] \ Res(a, M, s)[S] iff
(u, v) ∈ M 0 [i].
• i ∈ OD (a, M, s). There are two cases:
– ((u, σ), (v, )) ∈ W [i] iff (u, v) ∈ M [i] and (M, u) |= pre(σ) iff (u, v) ∈ M [i] and r(a, u) ∈
Res(a, M, s)[S] and v ∈ M 0 [S] \ Res(a, M, s)[S] iff (r(a, u), v) ∈ M 0 [i];
– ((u, ), (v, )) ∈ W [i] iff (u, v) ∈ M [i] iff (u, v) ∈ M [i] and u, v ∈ M 0 [S] \ Res(a, M, s)[S] iff
(u, v) ∈ M 0 [i].
The above two cases and (*) and (**) show that (M 0 , s0 ) is equivalent to (W, w), i.e., the proposition is proved
for this case.
2. a is a sensing action. Without the loss of generality, assume that a senses a single fluent f and the
executability condition of a is ψ. By Definition 23, (ω(a, ρ), {σ, τ }) is the update instance for a and
ρ = (FD (a, M, s), PD (a, M, s), OD (a, M, s)). Furthermore, for each u ∈ M [S], the set of worlds of
(M, s) ⊗ Ω(a, (M, s)) contains either (u, σ) or (u, τ ) but not both. Thus, (M, s) ⊗ Ω(a, (M, s)) is again a
singleton. Let us denote the unique element of (M, s) ⊗ Ω(a, (M, s)) by (W, w). Let (M r , c(s)) be the replica
of (M, s) that is used in the construction of (M 0 , s0 ) ∈ ΦD (a, M, s). Let h be the function
h : W [S] → M 0 [S]
with


h((u, λ)) =

c(u)
u

if (u, λ) ∈ W [S], λ ∈ {σ, τ }
if (u, λ) ∈ W [S], λ = 

The rest of the proof follows exactly the steps of the proof for the case of a world-altering action.
3. a is an announcement action. The proof is analogous to the case of a sensing action.
2
The remaining part of this appendix is devoted to prove Propositions 6-15. The proofs will be organized in a
collection of lemmas. Let us start with a lemma that allows us to ignore in a state those worlds that are not reachable
from the real state of the word.
Lemma 7 Every S5-state (M, s) is equivalent to a S5-state (M 0 , s) such that, for every state u ∈ M 0 [S], we have
that u is reachable from s.
Proof. The result derives from the fact that, if there is a world u which is not reachable from s, then for each formula
ψ we have that (M, s) |= ψ iff (M 0 , s) |= ψ, where M 0 is defined as follows: (i) M 0 [S] = M [S] \ {u} (i.e., we
remove the unreachable world u); (ii) M 0 [π](v) ≡ M [π](v) for every v ∈ M 0 [S] (i.e., all interpretations associated
to the worlds remain the same); and (iii) M 0 [i] = M [i] \ {(p, q) | (p, q) ∈ M [i], p 6∈ M 0 [S] or q 6∈ M 0 [S]}, (i.e., we
maintain the same belief relations except for removing all cases related to the world u).
2
The next lemma characterizes the accessibility relations for states satisfying a common knowledge formula.
Lemma 8 Let (M, s) be a S5-state such that every state u ∈ M [S] is reachable from s. Let ψ be a formula. Then,
(M, s) |= C(ψ) iff M [π](u) |= ψ for every world u ∈ M [S].
Proof. Because of the reflexive, transitive, and symmetric properties of the relations Bi in M and the definition of
the satisfaction of C(ψ), we have that (M, s) |= C(ψ) implies that (M, u) |= ψ for each u ∈ M [S] (since each u is
reachable from s), which implies M [π](u) |= ψ. The converse is obvious from the fact that each state is reachable
from s.
2
51

Lemma 8, in particular, indicates that for each i ∈ AG and for each fluent formula we have: ψ (M, s) |= C(Bi ψ) iff
∀u ∈ M [S]. M [π](u) |= ψ.
The following lemmas allow us to characterize the topological properties of the Kripke structures implied by the
satisfaction of the different types of statements in a definite action theory.
Lemma 9 Let (M, s) be a S5-state such that every world u ∈ M [S] is reachable from s. Let ψ be a fluent formula.
Then:

(M, s) |= C(Bi ψ ∨ Bi ¬ψ) iff
∀u, v ∈ M [S] : (u, v) ∈ M [i] ⇒ M [π](u) |= ψ iff M [π](v) |= ψ
Proof. Thanks to the Lemma 8, we have that (M, s) |= C(Bi ψ ∨ Bi ¬ψ) holds if and only if (M, u) |= Bi ψ ∨ Bi ¬ψ
for every u ∈ M [S]. The proof of the lemma then follows immediately from the fact that M is a S5-structure.
2
A consequence of Lemma 9 is the following lemma, that provides a characterization of S5-states with respect to
formulae of the form C(¬Bi ψ ∧ ¬Bi ¬ψ). For simplicity of presentation, for a structure M and u, v ∈ M [S] we
write M [π](u)(ψ) 6= M [π](v)(ψ) to denote either (M [π](u) |= ψ and M [π](v) 6|= ψ) or (M [π](u) 6|= ψ and
M [π](v) |= ψ), i.e., the value of ψ at u is different from the value of ψ at v.
Lemma 10 Let (M, s) be a S5-state such that every u ∈ M [S] is reachable from s. Let ψ be a fluent formula. Then,
(M, s) |= C(¬Bi ψ ∧ ¬Bi ¬ψ) iff for every u ∈ M [S] there exists some v ∈ M [S] such that (u, v) ∈ M [i], and
M [π](u)(ψ) 6= M [π](v)(ψ).
Let us prove a few properties of initial S5-states of a definite action theory.
Lemma 11 Let (M, s) be an initial S5-state of a definite action theory, such that every u ∈ M [S] is reachable from
s. Let ϕ be a fluent formula and i ∈ AG. Then,
• If (M, u) |= Bi ϕ for some u ∈ M [S] then (M, s) |= C(Bi ϕ) or (M, s) |= C(Bi ϕ ∨ Bi ¬ϕ);
• If (M, u) |= ¬Bi ϕ for some u ∈ M [S] then (M, s) |= C(¬Bi ϕ ∧ Bi ¬ϕ).
Proof. This result can be proved by contradiction. Let us consider the first item. If the consequence of the first item
does not hold then (M, s) |= C(¬Bi ϕ ∧ Bi ¬ϕ)—since (M, s) is an initial state of a definite action theory. Thus,
(M, u) 6|= Bi ϕ due to Lemma 10 (since there are two states in which the formula ϕ has a different truth value). This
leads to a contradiction.
Let us consider the second item. If the consequence of the second item does not hold then we have that (M, s) |=
C(Bi ϕ) or (M, s) |= C(Bi ϕ ∨ Bi ¬ϕ) (since we are dealing with definite action theories). This implies (M, u) |=
Bi ϕ or (M, u) |= Bi ¬ϕ (Lemma 8 and Lemma 9). This leads to a contradiction.
2
In the following, for u ∈ M [S], let
state(u) =

^

f∧

f ∈F , M [π](u)(f )=>

^

¬f

f ∈F , M [π](u)(f )=⊥

Lemma 12 Let (M, s) be an initial S5-state of a definite action theory, such that every u ∈ M [S] is reachable from
s. Let u, v ∈ M [S] such that u ∼ v. Then, for every i ∈ AG and x ∈ M [S] such that (u, x) ∈ M [i] there exists
y ∈ M [S] such that (v, y) ∈ M [i] and x ∼ y.
Proof. Let K(p, i) = {q | q ∈ M [S], (p, q) ∈ M [i]}—i.e., the set of worlds immediately related to p via M [i]. We
consider two cases:
• Case 1: K(u, i) ∩ K(v, i) 6= ∅. Because of the transitivity of M [i], we can conclude that K(u, i) = K(v, i)
and the lemma is trivial (by taking x = y).

52

• Case 2: K(u, i) ∩ K(v, i) = ∅. Let us assume that there exists some x ∈ K(u, i) such that there exists
no y ∈ K(v, i) with x ∼ y. This means that (M, y) |= ¬state(x) for each y ∈ K(v, i). In other words,
(M, v) |= Bi ¬state(x). This implies that (by Lemma 11):
(M, s) |= C(Bi ¬state(x))

or

(M, s) |= C(Bi ¬state(x) ∨ Bi state(x))

(18)

On the other hand, (M, u) 6|= Bi ¬state(x), since x ∈ K(u, i) and (M, x) |= state(x). This implies (M, s) |=
C(¬Bi ¬state(x) ∧ ¬Bi state(x)) by Lemma 11. This contradicts (18). This means that our assumption is
incorrect, i.e., the lemma is proved.
2
f, s̃) of
Lemma 13 Let (M, s) be a S5-state such that every u ∈ M [S] is reachable from s. Then, the reduced state (M
(M, s) is also a finite S5-state.
Proof. Let us start by observing that, for each u, v ∈ M [S] such that u ∼ v, we have: ũ = ṽ. Since the number of
possible interpretations of F is finite (being F itself finite), and since there can be no two distinct worlds ũ and ṽ such
f[S] is finite.
that M [π](u) = M [π](v), then we can conclude that the number of worlds in M
(1)
f[S].
Let us consider an agent i ∈ AG and worlds ũ, ṽ, w̃ ∈ M
f[i] since (u, u) ∈ M [i] for every u ∈ M [S]. This implies that M
f[i] is reflexive.
• Clearly, (ũ, ũ) ∈ M

(2)

f[i], if (ũ, ṽ) ∈ M
f[i] then there exists (u0 , v 0 ) ∈ M [i] for some u0 ∈ ũ and v 0 ∈ ṽ. Because
• By construction of M
f[i],
M is a S5-structure, and thus M [i] is symmetric, we have that (v 0 , u0 ) ∈ M [i]. This implies that (ṽ, ũ) ∈ M
f
i.e., M [i] is symmetric
(3)
f[i] and (ṽ, w̃) ∈ M
f[i]. The former implies that there exists (u0 , v 0 ) ∈ M [i] for
• Now assume that (ũ, ṽ) ∈ M
0
0
some u ∈ ũ and v ∈ ṽ. The latter implies that there exists (x0 , w0 ) ∈ M [i] for some x0 ∈ ṽ and w0 ∈ w̃.
f[S], we have that v 0 ∼ x0 —since they belong to the same equivalence class
Thanks to the construction of M
for ∼. Lemma 12 implies that there exists some w00 ∼ w0 such that (v 0 , w00 ) ∈ M [i] which implies that, by
f[i], i.e., M
f[i] is transitive.
transitivity of M [i], (u0 , w00 ) ∈ M [i]. Thus, (ũ, w̃) ∈ M
(4)
(1)-(4) prove the conclusion of the lemma.
2
The next lemma shows that a reduced state of an initial S5-state of a definite action theory is also an initial state of
the action theory.
f, s̃) be
Lemma 14 Let (M, s) be a S5-state such that every state u in M [S] is reachable from s. Furthermore, let (M
the reduced state of (M, s). It holds that
f, s̃) |= ψ;
1. (M, s) |= ψ iff (M
f, s̃) |= C(ψ);
2. (M, s) |= C(ψ) iff (M
f, s̃) |= C(Bi ψ);
3. (M, s) |= C(Bi ψ) iff (M
f, s̃) |= C(Bi ψ ∨ Bi ¬ψ));
4. (M, s) |= C(Bi ψ ∨ Bi ¬ψ) iff (M
f, s̃) |= C(¬Bi ψ ∧ ¬Bi ¬ψ));
5. (M, s) |= C(¬Bi ψ ∧ ¬Bi ¬ψ) iff (M
where i ∈ AG and ψ is a fluent formula.
Proof.
f[π](s̃) |= ψ (by construction of (M
f, s̃)) iff
1. (M, s) |= ψ iff M [π](s) |= ψ (by definition) iff M
f
(M , s̃) |= ψ.
53

2. (M, s) |= C(ψ) iff (M, u) |= ψ for each u ∈ M [S] (by Lemma 8 with respect to (M, s)) iff
f, ũ) |= ψ for each ũ ∈ M
f[S] (by construction of (M
f, s̃)) iff
(M
f
f
(M , s̃) |= C(Bi ψ) (by Lemma 8 with respect to (M , s̃)).
3. (M, s) |= C(Bi ψ) iff (M, u) |= ψ for each u ∈ M [S] (by Lemma 8 with respect to (M, s)) iff
f, ũ) |= ψ for each ũ ∈ M
f[S] (by construction of (M
f, s̃)) iff
(M
f, s̃) |= C(Bi ψ) (by Lemma 8 with respect to (M
f, s̃)).
(M
4. (M, s) |= C(Bi ψ ∨ Bi ¬ψ) iff
for every pair of u and v in M [S], (u, v) ∈ M [i] implies M [π](u) |= ψ iff M [π](v) |= ψ (by Lemma 9 with
respect to (M, s)) iff
f[S], u ∈ p̃ and v ∈ q̃, (p̃, q̃) ∈ M
f[i] implies M
f[π](p̃) |= ψ iff M
f[π](q̃) |= ψ
for every pair of states p̃ and q̃ in M
f
(by construction of (M , s̃)) iff
f, s̃) |= C(Bi ψ ∨ Bi ¬ψ) (by Lemma 9 with respect to (M
f, s̃)).
(M
5. (M, s) |= C(¬Bi ψ ∧ ¬Bi ¬ψ) iff
for every u ∈ M [S] there exists some v ∈ M [S] such that (u, v) ∈ M [i] and M [π](u) |= ψ and M [π](v) 6|= ψ
(by Lemma 10 with respect to (M, s)) iff
f[S] there exists ṽ ∈ M
f[S] such that (ũ, ṽ) ∈ M
f[i], M
f[π](ũ) |= ψ, and M
f[π](ṽ) 6|= ψ (by
for every ũ ∈ M
f
construction of (M , s̃)) iff
f, s̃) |= C(¬Bi ψ ∧ ¬Bi ¬ψ)) (by Lemma 10 with respect to (M
f, s̃)).
(M
2
Lemma 16 Let I be a set of initial statements of a definite action theory. Every S5-state satisfying I is equivalent to
a S5 state (M 0 , s0 ) such that |M 0 [S]| ≤ 2|F | .
f, s̃) is also
Proof. By Lemma 7, we can assume that every world in M is reachable from s. Lemma 14 shows that (M
|F |
f
a S5-state satisfying I. Obviously, |M [S]| ≤ 2 —since each interpretation M [π](v) can be modeled as a subset of
f[S]| is bounded by the number of distinct interpretations. We will show that the reduced state (M
f, s̃) of
F, and |M
(M, s) satisfies the conclusions of the lemma. To complete the proof, we need to show that for an arbitrary formula ϕ
and u ∈ M [S], the following holds:
(M, u) |= ϕ

iff

f, ũ) |= ϕ.
(M

(19)

We will prove (19) by induction over the depth of ϕ.
f, s̃).
• Base: depth(ϕ) = 0 implies that ϕ is a fluent formula. (19) is trivial from the construction of (M
• Step: Assume that (19) holds for ϕ with depth(ϕ) ≤ k. Consider the case depth(ϕ) = k + 1. There are four
cases:
– ϕ = Bi ψ. (M, u) |= ϕ iff for every v ∈ M [S] s.t. (u, v) ∈ M [i] we have that (M, v) |= ψ
f[S], (ũ, ṽ) ∈ M
f[i], (M
f, ṽ) |= ψ (inductive hypothesis and construction of (M
f, s̃))
iff for every ṽ ∈ M
f
iff (M , ũ) |= Bi ψ.
– The proof for other cases, when ϕ is either ¬ψ, φ ∨ ψ, or φ ∧ ψ is similar.
Since the truth value of group formulae of the form Eα ϕ and Cα ϕ is defined over the truth value of Bi ϕ for i ∈ α,
we can easily show that (19) holds for an arbitrary formula in the language LAG . This proves the lemma.
2
The above lemma leads to the following proposition.
Proposition 6 For each consistent definite action theory (I, D), there exists a finite number of initial S5-states
(M1 , s1 ), . . . , (Mk , sk ) such that every initial S5-state (M, s) of (I, D) is equivalent to some (Mi , si ). Furthermore,
for each pair of i 6= j and u ∈ Mi [S] there exists some v ∈ Mj [S] such that Mi [π](u) ≡ Mj [π](v).
54

Proof. Observe that the existence of a S5-state satisfying I is proved in [28]. By Lemma (16), we know that each
f, s̃). The proof of the first conclusion of the
initial S5-state (M, s) of (I, D) is equivalent to its reduced state (M
|F |
f
proposition follows trivially from the fact that |M [S]| ≤ 2 and there are only finitely many Kripke structures whose
size is at most 2|F | .
To prove the second conclusion of the proposition, let us recall the following definition given earlier: for s ∈ M [S],
let
^
^
state(s) =
f∧
¬f
f ∈F , M [π](s)(f )=>

f ∈F , M [π](s)(f )=⊥

Consider two initial S5-states (Mi , si ) and (Mj , sj ). Let us assume, by contradiction, that there is some u ∈ Mi [S]
such that for every v ∈ Mj [S], Mi [π](u) 6≡ Mj [π](v). Let ϕ = ¬state(u). Because of our assumption, we have
that ϕ is false in every state of Mj ; thanks to Lemma 8, we can infer that (Mj , sj ) |= C(Bi ϕ). Since (D, I) is a
definite action theory and (Mj , sj ) is an initial S5-state of (D, I), we have that “initially C(Bi ϕ)” is in I; on the
other hand, (Mj , u) |= ¬ϕ, which implies that (Mj , sj ) is not an initial S5-state of (I, D). The conclusion is proved
by contradiction.
2
Proposition 6 and the definition of complete definite action theories give raise to the following proposition.
Proposition 7 For a consistent and complete action theory (I, D), there exists a unique initial S5-state (M0 , s0 ) with
|M0 [S]| ≤ 2|F | such that every initial S5-state (M, s) of (I, D) is equivalent to some (M0 , s0 ).
Proof. Let us consider two arbitrary S5-states (M1 , s1 ) and (M2 , s2 ) satisfying I. From Lemma 16, we can assume
that both M1 and M2 are in reduced form; this guarantees that the number of worlds in M1 and M2 is bounded by the
number of possible interpretations, which is 2|F | . Furthermore, from proposition 6 we can also assume that M1 and
M2 have the same worlds (since each interpretation in M1 appears in M2 and vice versa, and no interpretation can be
associated to two distinct worlds, since the states are in reduced form).
For the sake of simplicity, let us assume that M1 [S] = M2 [S]; let us assume, by contradiction, that there are
u, v ∈ M1 [S] such that (u, v) ∈ M1 [i] and (u, v) 6∈ M2 [i]. Let ϕ = ¬state(v). Because of the construction, we can
see that (M2 , u) |= Bi ϕ. Since the two states are initial states for I, then this means that they are models of one of
the following formulate: C(Bi ϕ) or C(Bi ϕ ∨ Bi state(v)). On the other hand, since (u, v) ∈ M1 [i], it is easy to see
that (M1 , u) 6|= Bi ϕ and (M1 , u) 6|= Bi state(v). Thus, this contradicts the fact that both (M1 , s1 ) and (M2 , s2 ) are
models of I (a complete definite theory).
2

55

