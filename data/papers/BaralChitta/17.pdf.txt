From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge
Somak Aditya , Yezhou Yang*, Chitta Baral , Cornelia Fermuller* and Yiannis Aloimonos* School of Computing, Informatics and Decision Systems Engineering, Arizona State University * Department of Computer Science, University of Maryland, College Park



arXiv:1511.03292v1 [cs.CV] 10 Nov 2015

Abstract
In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning1 is applied on (a) detections obtained from existing perception methods on given images, (b) a "commonsense" knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.

1. Introduction
"Imagine, for example, a computer that could look at an arbitrary scene, anything from a sunset at a fishing village to Grand Central Station at rush hour and produce a verbal description. This is a problem of overwhelming difficulty, relying as it does to finding solutions to both vision and language and then integrating them. I suspect that scene analysis will be one of the last cognitive tasks to be performed well by computers". This fifteen year old quote, attributed to A. Rosenfeld [41], one of the founders of the field of Computer Vision, pointed to the fundamental problem of generating semantics of visual scenes. Since then, researchers have attempted a few approaches that mostly centered on asking "what" and "where" questions about
reasoning and commonsense knowledge can be of many types [6]. Commonsense knowledge can belong to different levels of abstraction [18, 28]. In this paper, we focus on capturing and reasoning based on knowledge about natural activities.
1 Commonsense

the scene in view. In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46]. Recently, researchers have advanced the viewpoint that if we are able to develop a semantic understanding of a visual scene, then we should be able to produce natural language descriptions of such semantics. This has given rise to a new area in the field that integrates vision, knowledge and natural language. Knowledge becomes especially important, as without background knowledge, it has become increasingly hard to obtain a desirable level of accuracy in this problem. And as such knowledge can be often mined from text, the problem now stands at the intersection between Computer Vision and Natural Language Processing. Mining such knowledge, storing it in a form that retains the semantics, and reasoning using this knowledge to develop a better understanding of scenes are the fundamental issues that are addressed in this paper. Current developments [31, 22, 8, 21, 44, 3] in Computer Vision have shown that deep neural nets can be trained to generate a caption for an arbitrary scene with decent success. It is indeed an exciting achievement. However, current state-of-the-art image captioning systems still have a few drawbacks such as: 1) a brute-force image-to-text mapping makes it inconvenient to conduct Logical Reasoning beyond just doing inferences from annotated data; 2) due to the lack of intermediate semantic representations, they are all language-dependent; and 3) most importantly, when the system produces wrong results, it is almost impossible to trace back the system and analyze the failure case (See Figure 1). Let us consider how humans accomplish this task. Human perception is active, selective and exploratory. We continuously shift our gaze to different locations in the scene. After recognizing objects, we fixate again at a new location, and so on. We interpret visual input by using our knowledge of activities, events and objects. When we analyze a vi1

(a)

(b)

Figure 1: Examples from [21]: (a) Positive example annotation: construction worker in orange safety vest is working on road, (b) Negative example annotation: a bunch of bananas are hanging from a ceiling. Such annotations could be infrequent, but it is hard to logically justify such contrasting outputs.

sual scene, visual processes continuously interact with our high-level knowledge, some of which is represented in the form of language. In some sense, perception and language are engaged in an interaction, as they exchange information that leads to meaning and understanding. Thus, our problem requires at least two modules for its solution: (a) a vision module and (b) a reasoning module that are interacting with each other. In this paper we propose to model the early stages of this process. The available datasets make it impossible to perform experiments that will consider vision as an active process, although this is the ultimate goal. Thus, our question becomes: if the vision module produces a number of (probabilistic) detections, how much the reasoning module can infer about the scene if it possesses common sense abilities? It turns out that the reasoning module can infer a great deal. Motivated by such intuitions, we present here an effort to integrate deep learning based vision and state-of-the-art concept modeling from commonsense knowledge obtained from text. We use a deep learning-based perception system to obtain the objects, scenes and constituents with probabilistic weights from an input image. To predict how the objects interact in the scene, we build a common-sense knowledge base from image annotations along with a Bayesian Network capturing dependencies among commonly occurring objects and "Abstract Visual Concepts" (defined later). These two precomputed resources help us infer the following: 1) the correct set of correlated objects based on the high-confidence objects detected; 2) the most probable events that these objects participate in; 3) the role that the objects play in this event; and 4) given the events, objects and constituents, the "Concept" that emerges from such information. Based on these inferences, we output a Scene Description Graph (SDG) that depicts how these different entities and events interact. In Figure 2, we show a possible SDG for an example image. SDG is essentially a directed labeled graph among entities and events2 that enables an array of possibilities to do further analysis beyond visual appearance, such as Event-Entity based analysis, question
2 Throughout this paper, we follow definition of Entities and Events from [16, 2].

answering about the scene and flexible caption generation3 . The fundamental contribution of this work is a novel algorithm that uses automatically constructed Knowledge Base to create an SDG from an image, which facilitates further reasoning and caption generation. SDGs have advantages over ground-truth sentences because : 1) they can be easily processed by machines/AI systems in comparison to sentences; 2) the output can be rich in information-content; 3) they are not bounded by specific templates, that are often used by researchers to convert labels into sentences and 4) the SDG also can be used to generate sentence descriptions. We also create a Knowledge Base which captures the knowledge about the commonly-occurring Concepts, events and entities. The knowledge base can be used to provide answers to the following queries: 1) the event or set of events that connect two entities; 2) the role an entity plays in an event and 3) a subset of all possible concepts involving the entities and connecting events. Lastly, further inferences about the scenes such as "Will the player holding the ball be able to tackle the blocker and under what conditions" can also be attempted by feeding the SDG output as predicates to Reasoning modules along with additional background knowledge.

2. Related Works
Our work is influenced by various lines of work where researchers have proposed approaches to extract meaningful information from images and videos. As [21] suggests, such works can be categorized into 1) dense image annotations, 2) generating textual descriptions, 3) grounding natural language in images and 4) neural networks in visual and language domains. According to the above categorization, we share our roots with the works of generating textual descriptions. This includes the works that retrieves and ranks sentences from training sets given an image such as [19], [12],[34], [40]. [10], [24], [25], [47], [48] are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content. Several works have shown promising efforts to acquire and apply commonsense in different aspects of Scene Analysis. [52] uses abstraction to discover semantically similar images. [7] proposes to learn all variations pertaining to all concepts and [36] uses common-sense to learn actions. Recently, [20] introduced scene graphs to describe scenes and [37] creates scene graphs from descriptions. However, we automatically construct the graph from an image, and we believe, due to the event-entity-attribute based representation and meaningful edge-labels (borrowed from KM-ontology[4]) , SDGs are more equipped to facilitate symbolic-level reasoning.
3 One may note that such structures are also generated by Semantic parsers such as K-parser(www.kparser.org).

Figure 2: Example Image and a possible corresponding SDG. Note, the SDG should contain a similar event wear2 for person2. We omit it for space constraints. Note that, it is easy to augment spatial information to the above graph such as (person1,left,person2).

3. State-of-the-art Visual Detections
The recent development of deep neural networks based approaches revolutionized visual recognition research. Different from the traditional hand-crafted features, a multilayer neural network architecture efficiently captures sophisticated hierarchies describing the raw data [1], which has shown superior performance on standard scene recognition [51], object recognition [15] and image captioning [21] benchmarks. Image Dataset: In this paper, we use three image data sets, which are popularly referred to as Flickr 8k, Flickr 30k and Coco datasets [19]. These three datasets have 8,092, 31,783 and more than 160K images respectively. All the images from these datasets are accompanied with 5 hand-annotated sentences that describe the image. For all datasets, we used the train-test splits from [21] and the 4000 testing images (1000 each from Flickr 8k and 30k; 2000 from MS-COCO validation set; denoted as I ) serve as the testing bed for our reasoning experiments. Deep Object Recognition: We use the trained bottom-up region proposals and convolutional neural networks(CNN) object detection method from [15]. It considers 200 common everyday object classes (denoted as N ) and trained on ILSVRC 2013 dataset. We apply the method on the testing images(I ) and then convert the object detection scores to Pr (n|I ). Deep Scene Recognition: We use the trained CNN scene classification method from [51]. The classification model is trained on 205 scene categories (denoted as S ) and each of the category has more than 5000 training samples. We apply the method on the testing images and then convert the scene classification scores to Pr (s|I ). Constituent Annotation Collection and Deep Constituent Recognition: Images from the wild cannot always be categorized into a limited number of Scene categories. However, scene constituents describing properties or actions of objects, attributes of scenes occur frequently across images and can be utilized to describe the image. In this work, we further augment the Flickr 8K image dataset with human annotation of constituents using Amazon Mechanical Turks. We specifically ask the human labeler to annotate not only objects, but what objects are doing or properties of objects. We allow the labelers to use free-form text for describ-

ing constituents to reduce annotation effort. To obtain a standardized set of constituents from the annotations, we perform stop-words removal, parts-of-speech processing to retain nouns, adjectives and verbs. We replace the nouns with their superclasses such as man, boy, father by person, and then, we rank the resulting phrases according to their frequencies. Some of the top phrases are grass, dog run, dog play, kid play, person wear short4 etc. For the rest of our processing, we post-process the annotations for each training image and consider them if they are among the 1000 top constituents (denoted as C ). Recent empirical results from a diverse range of visual recognition tasks indicate that the generic descriptors extracted from the CNN are very powerful [9, 35]. In this work, we use a pre-trained CNN from [23]. For each image in I , we use this pre-trained model to extract a 4096 dimensional feature vector using [9, 23]. We then trained a multi-label SVM to do constituents recognition using the deep features. The trained model is applied on all the testing images and we convert the classification scores to Pr (c|I ). The set of Pr (n|I ), Pr (s|I ), Pr (c|I ) makes up the initial visual perception output.

4. Constructing SDGs from Noisy Visual Detections
Next, we explain the reasoning framework to construct SDGs from noisy labels with the aid of knowledge from text. To provide a better understanding of this complex system, we provide a diagram of the architecture explaining the reasoning process for an example image in Figure 3. As shown, for each image, the above perception system produces object, scene and constituent detection tuples. Each detection is provided with a confidence score. For objects, scores are provided for each bounding box. Top five scene labels and top ten constituent detections are considered for the reasoning framework. Most of these detections are quite noisy. We develop an elaborate reasoning framework to construct SDGs from such noisy detections, with the help of pre-processed background knowledge.
4 All the phrases with their corresponding frequencies will be made publicly available in the final version.

dataset-independent and only needs to be augmented when the set of object classifiers expands. Scenes-to-AVCs Mapping Table (SM ): For each scene in S , we added ontological information involving a set of abstract concepts and a set of synonyms. To obtain the synonyms, we again used WordNet API. We hand-annotated all the AVCs for each scene and learnt a prior belief for each AVC in scene from human annotations. For example, for the scene airport terminal, we add {waiting room, big glass view, people} as the list of AVCs and terminal as the synonym; and learn the priors 0.7, 0.6 and 0.9 respectively for AVCs. In the following sub-sections, we first introduce the Reasoning Framework briefly, followed by a description of the construction of the Knowledge Base Kb and the Bayesian Network Bn . Lastly, we describe our reasoning framework in detail.

4.2. Reasoning Framework
Equipped with the background knowledge stored in the form of (Kb , Bn , SM , OT ), we process the objects, scene and constituent detections for an image to construct an appropriate SDG in the following way: i) we populate synonyms, hypernyms, hyponyms of objects and synonyms, AVCs (with priors) of scenes; ii) (Scene Constituents:) we extract entities and events from each constituent. Such as, the constituent person wear short results in an event wear with two edges: one labeled agent joining the entity person and another labeled recipient joining the entity short; iii) (Abstract Visual Concepts:) we choose the AVCs iteratively that maximizes the conditional probability given high confidence objects; iv) (Objects:) for low-scoring objects, we choose the sibling (in the hyponym-hypernym hierarchy) which maximizes the conditional probability given high-confidence objects and AVCs; v) (Events:) we search the Kb to find the most compatible events that connect pairs of high-confidence objects. We add the events obtained from Constituents to this set of compatible events; vi) (Concepts 6 :) given the above events and AVCs, we search the Kb for Concepts that best suits the events and AVCs, and we also construct an SDG based on just high-confidence objects, events and AVCs.

Figure 3: SDG and Sentence Generation through Reasoning using Knowledge Base and a Bayesian Network Bn

4.1. Pre-processing Phase Data Accumulation
In this phase, we collect Ontological information about object classes in Object Meta-data table (OT ) and Scene classes in Scene Metadata (SM ). We also store scene detection tuples (ST ) and human annotation of images (Ad ) for all training images. We create a Knowledge Base Kb , a Bayesian Network Bn and a Scenes to Abstract Visual Concepts5 (AVC) Mapping Table (SM ). Scene Detection tuples (ST ): We use the perception system of the previous section to create scene detection tuples ({(si , Pr (si |Itr ))|i  {1, .., 5}}) of set of training images (Itr ). These are used to learn the Bayes Net Bn . Image Annotations (Ad ): We collect all textual descriptions of the training images provided with Image Datasets and use them for building the knowledge base Kb and Bayes Net Bn . However, both can be built using any repository of sentences that describe day-to-day concepts. Object Meta-data (OT ): For each of the 200 object classes, we collected all synonyms, hyponyms and hypernyms. The list is prepared using Wordnet API. This is
5 Abstract Visual Concepts are higher-level scene constituents and they describe commonly occurring visual concepts that can be observed across images. In essence, they are can be compared to phrasal verbs. Like textual phrases, they do not necessarily follow compositionality of its constituent words. In case of AVC, this happens in the context of images. For example waiting room suggests room, seating area, chairs, people waiting etc. In comparison, Scene Constituents can be compared with phrases that retain their compositionality such as people play, person wear shorts etc. In context of an image, person wear shorts can be grounded as the objects person and shorts; and the action wear.

4.3. Knowledge Base (Kb )
In Figure 4, we describe how we construct Kb from a set of Image Annotations (Ad ) using the Stanford Parser and K-Parser [39]. For each sentence, we first parse using the
6 The idea behind the representation of a Concept is inspired by that of a Process in AURA [2]. The structure Process is a graph that represents a Biological Process in AURA-KB. and it symbolizes a higher-level event that encapsulates smaller events, and the entities that participate in such events. Similarly, a Concept represents a natural activity where a few events occur and participating entities interact through the events. Our entire approach, in essence, is about sequentially determining the participants (entities and events) of a Concept and lastly, the Concept itself.

build the Kb . A visualization of a part of the Kb is given in Figure 4(b). After parsing the annotated sentences in Flickr8k, Kb consists of 1102 events, 2500 entities and 1869 traits. The total number of edges and distinct concepts in the graph are 25271 and 14325.

4.4. Conditional Probability Estimation
In this sub-section, we describe the type of conditional probabilities we estimate and the Bayesian Network we learn to estimate such probabilities. We use conditional probability calculations in two of the steps of our approach: inferring the most probable collection of Abstract Visual Concepts and rectifying low-scoring erroneous objects. For inferring the most probable collection of AVCs, we first make a list (Cf req ) of all the frequent AVCs (with frequency > 2 in our experiments) from all scenes detected for a test image. Then we follow Algorithm 1 to get the set of inferred concepts Cinf from the set of high-scoring (score > h 8 ) entities Oimg and the set of scenes Simg detected for image img  I . We iterate till the entropy keeps decreasing. Algorithm 1 Infer Abstract-Visual-Concepts
1: procedure INFER M OST P ROBABLE AVC S(Simg , Oimg , Cf req ) 2: prevH  1 3: while Cf req =  do 4: Smax  argmaxsCf req P (s|Cinf , Oimg ) 5: H  sCf req {-P (s|Cinf , Oimg )  logP (s|Cinf , Oimg )} 6: if H > prevH then break; 7: prevH  H 8: Cinf  Cinf  {Smax }; Cf req  Cf req \ {Smax }

(a)

(b)

Figure 4: (a) Constructing Knowledge Base From Annotations. (b) A snapshot of the Kb . In this figure, Person and bench are entities, lay is the connecting event. The entity Person can have trait climber. The sub-graph essentially captures the knowledge of the activity person laying on a bench. The figure on the left shows the edge-labels.

Stanford Parser to get a dependency graph. The K-parser then maps these dependency labels using a set of rules to a set of meaningful labels from KM-Ontology[4] and the resulting graph is further augmented using ontological and semantic information from different sources (more details on kparser.org). We then generalize each of these graphs i.e. replace entities by their superclasses. Then we merge them based on overlapping entities and events, and create a single graph (Kb ). Kb is defined as the tuple (G , C ). G = (V, E ) denoting set of vertices V , set of edges E . Each vertex and edge has a label. Each vertex can be of three types: events, entities and traits. Events correspond to verbs, entities correspond to superclasses of nouns that directly interact with events and traits represent all other nouns. Edge labels in the Kb are exactly the same as in the K-parser (Figure 4). C is a set of concepts which corresponds to generalized K-parser graphs of sentences and is essentially a sub-graph of G . From Flickr8k annotations, we process nearly 16000 sentences provided for the images (2 per each image7 ) to
7 We do not consider all 5 sentences for an image, as most of the sentences are conceptually same.

Next, we attempt to rectify the low-scoring entities based on high-scoring entities (Oimg ) and the above Cinf . For each low-scoring entity, we get all its siblings i.e. we get all the children of its hypernyms. For example, if bathing cap is assigned a low score, the assigned superclass is headwear and its children are headband, hat etc. We calculate the following omax = argmaxosiblings P (o|Cinf , Oimg ) and then add omax to the high-scoring entities list (Oimg ). As the above paragraphs suggest, we need to estimate the conditional probabilities: P (s|Cinf , Oimg ) and P (o|Cinf , Oimg ). To estimate the conditional probabilities, we learn a Bayesian Network Bn using Ad and ST . 4.4.1 Learning the Bayesian Network Bn To capture the knowledge of naturally co-occurring entities and Abstract Visual Concepts, we learn a Bayesian Network that represents the dependencies among them. We create the training data D which is a set of tuples T = (ti )i , i  1, .., N where N is the total number of entities and AVCs. Each term ti is binary and denotes 1 if the ith entity (or AVC) occurs in the tuple. Then, we use the Tabu Search (tabu) algorithm to learn the structure and then we populate the Conditional Probability Tables using the R-bnlearn
8 The hyper-parameter ( ) are set based on performance on validation h data.

package [38]. A subgraph of the learnt Bayesian Network is shown in the Figure 5.

Figure 5: A subgraph reflecting the dependencies captured in the Learnt Bayesian Network Bn

To create the training data D, we process each training image (in Itr ), and we automatically detect entities and AVCs and then output the tuple T . To detect entities, we parse the image annotations (Ad ) and extract entities from it. Some of the AVCs such as people and people wear shorts are detected using rule-based techniques. However, for scenes such as airport-terminal, it is unlikely that AVCs such as waiting room can be found in human descriptions of an image; as we tend to describe only the entities and their interactions. Keeping this idea in mind, we ran the scene classifier system from the previous Section 3, and we consider all the AVCs of the scene with the highest score (P r(s|Itr )), from the Scene-to-AVC lookup table (SM ).

4.5. Ranking and Inferring Final Concepts
Given the most relevant set of Abstract Visual Concepts (Cinf ) and entities (Oimg ), we find Concepts that the image describes. To do this, we use the Kb to search first for events that these entities (i.e. objects) participate in and then we use these events and entities together to search for Concepts in the set of concepts C in Kb . We rely on two assumptions about the Knowledge Base: I) Kb reflects a more-or-less complete view of the relevant world knowledge and hence we can find the most suitable events from it. This assumption is valid if the images come from the same domain; such as in our examples, we have used the Flickr8k dataset and the domain corresponds to pictures of humans and dogs in natural setting; and II) Kb contains all concepts possible with the given events and entities. This is a strict assumption, which might not be true even if we parse the whole Web. To alleviate the problem, we give two final outputs: i) an SDG involving the entities, AVCs and events and ii) another SDG of the top Concept that is obtained from C in Kb . Search Connecting Events: The motivation behind building a Knowledge Base was to logically explain why certain co-occurring events are suitable for the combination of entities. For example, consider the entities person and swimming trunks. Note, swimming trunks corresponds to the vertex trunk in Kb . We get events such as sniff, climb, wear etc., i.e., some corresponding to tree-trunk and others to swimming-trunks. To logically find suitable events, we find all connecting events from G in Kb and then filter spuri-

ous events based on ontological and background knowledge from OT and C in Kb . For a pair of entity in Oimg , we traverse the path from one entity to another in the graph G and consider eventnodes on the path. As shown in Figure 4(b), two entities can be connected by an event. However, in some cases, they could be connected by a chain of events and entities. We employ a greedy breadth-first search over the graph G for such pairs. We denote the set of entities that are related to each other by some event, by Oev . For filtering spurious events, we introduce the notion of Edge-Compatible Events. An event is edge-compatible with respect to two entities if they are connected to the event using edges with compatible labels. As these labels are well-defined relations between entities and events from KM-Ontology, the label-compatibility is easy to observe. For example, (agent,recipient) is a compatible pair and only an animate entity can be an agent. Based on the rules, the event wear is edge-compatible with respect to entities person and trunk. Even after this, we still obtain events like climb etc. To filter such events, we consult the table OT and the set of concepts C . We know that the entity swimming trunks belongs to the superclass clothing, and hence we retain only those events that are connected to an entity trunk which is of the same superclass, in some concept in C . SDG Construction: After obtaining a set of suitable events (such as wear), we construct an SDG using the following set of rules: i) add has(scene, component, s) for all AVC s in Cinf ; ii) add has(event, location, scene) for the top detected events; iii) add all compatible edges related to the events such as has(wear,agent,person) and has(wear,recipient,trunk); and iv) for all entities oim in (Oimg \ Oev ), do the following: if it is an animate entity, add has(oim , location, scene); Otherwise, find the shortest path from oI to the top detected event in the Kb and add the edges on the path to the SDG. Search Concepts: Given the events and entities (Oev ), we search the set of Concepts C in Kb . Recall, in the Kb , a Concept is a generalized K-parser graph of a sentence. We consider a Concept as candidate if all edges from a detected Edge-Compatible Event are present in it. Next, we weight each candidate Concept using the remaining entities in (Oimg \ Oev ) and AVCs; i.e., increase a counter if an entity or AVC occurs in the graph. We also calculate a joint confidence-score for each Concept based on the Pr (n|I ), Pr (s|I ), Pr (c|I ) values of the object, scene and constituents present in the Concept. Based on the counters and the joint confidence-score, we rank the Concepts. Template Based Sentence Generation: We generate textual descriptions from the SDG using the SimpleNLG[14] package. For example, for the edges has(wear,agent,person) and has(wear,recipient,shorts), we

will generate the sentence "a person is wearing shorts". Based on the edge-labels (labels from KM-ontology) we populate the verb, subject, object and adjectives (including quantitative9 ) of sentences using simple rules. It should be noted that these K-Parser labels are a direct mapping from the set of Stanford Dependencies, and theoretically we can populate all the parts-of-speeches of a sentence from the SDG. Herein lies the effectiveness of producing an SDG from an image.

Type Entities Events

Accuracy-SDG(%) 13.6 13.1

Precision-SDG(%) 21.7 8

Accuracy(%)[21] 16.9 15.3

Precision(%)[21] 34.2 15.2

Table 1: Accuracy and Precision in events and entities prediction

5. Experiments and Results
The Knowledge-Structure representing a scene should be rich in information-content and should carry enough semantics to describe the image. We adopted three sets of experiments. First, we detect the accuracy with which our system can detect events and entities present in the image. We perform a qualitative evaluation ("relevance" and "thoroughness") of the textual descriptions generated from SDGs with the sentences generated by [21] using the Amazon Mechanical Turkers (AMT). And lastly, to evaluate the imagesentence alignment quality, we design an Image Retrieval task and report our results on Image Search based on generated annotations. To conclude, we provide a few example images and their SDGs. For comparison purposes, we use the implementation from [21] to generate a textual caption S for each testing image. The method is based on a combination of CNN over image regions, bidirectional recurrent neural networks over sentences, and a structured multimodal embedding. We denote the set of captions as SN N . Training Phase: Our model can be represented by the tuple (Kb , Bn , ST , Ad , OT , SM ). Among these, ST , OT and SM are collected and stored once, and re-used for all datasets. For our experiments, we re-use the same Bayesian Network Bn learnt from Flickr8k data for all the datasets. Though, we build the Kb each time from the annotated sentences, this can be easily avoided by using the same Kb for all the datasets. In essence, for the reasoning part, we donot require any training at all for new datasets. Entity and Event Detection Accuracy: For this experiment, we extracted entities and events (gold-standard) from constituent annotations for the 1000 test images of Flickr8k. We manually checked them to remove noise. To provide a baseline, we also extracted entities and events from SN N automatically using K-parser. Subsequently, we compared the gold-standard with entity-event set from [21] and the SDG output from our system for each image. The statistics of our evaluation is given in Table 1. AMT Evaluation of Generated Sentences: Since sentence generation to describe a scene is innately a creative process, a good metric is to ask humans to evaluate these
9 For high-scoring detections, we also consider the spatial information from the bounding-boxes. For N such detections of an object obj , we generate sentences like N obj 's are in the scene.

sentences. The evaluation metrics: Relevance and Thoroughness, are therefore proposed as empirical measures of how much the description conveys the image content (relevance) and how much of the image content is conveyed by the description (thoroughness)10 . We engaged the services of AMT to judge the generated descriptions based on a discrete scale ranging from 1­5 (low relevance/thoroughness to high relevance/thoroughness). The average of the scores and their deviation are summarized in Table 2 for Flickr8k, Flickr30k test images and MS-COCO validation images. For comparison, we asked the AMTs to also judge one random gold-standard description and the output from [21], a state-of-the-art image captioning system. In our experiments, we found that Kb from Flickr8k annotations can be used for Flickr30k without much effect on accuracy. However, for MS-COCO datasets, Kb from Flickr8k annotations falls short of producing a desired accuracy as the COCO data is much more varied.
Experiment R ± D(8k) T ± D(8k) R ± D(30k) T ± D(30k) R±D(COCO) T±D(COCO) [21] 2.08 ± 1.35 2.24 ± 1.33 1.93 ± 1.32 2.17 ± 1.34 2.69 ± 1.49 2.55 ± 1.41 Our Method 2.82 ± 1.56 2.62 ± 1.42 2.43 ± 1.42 2.49 ± 1.42 2.14 ± 1.29 2.06 ± 1.24 Gold Standard 4.69 ± 0.78 4.32 ± 0.99 4.78 ± 0.61 4.52 ± 0.93 4.71 ± 0.67 4.37 ± 0.92

Table 2: Sentence generation relevance (R) and thoroughness (T) human evaluation results with gold standard and [21] on Flickr 8k, 30k and MS-COCO datasets. D: Standard Deviation.

Image-Sentence Alignment Evaluation: Similar to the experiments in [21, 20], we also evaluate the imagesentence alignment quality using ranking experiments. We withhold the set of testing images and use the generated sentences as queries. We process the textual query and construct Gquery = (Vq , Eq ) using the same procedure by which we construct Kb . For each image, we take the SDG Gimg = (Vimg , Eimg ) and calculate similarity between the SDG and the query using the following formula:
Sim(Gquery , Gimg ) =
vq Vq

maxvimg Vimg (sim(vq , vimg )) |V q |

sim(vq , vimg ) = (wnsim(label(vq ), label(vimg ))+ Jaccard(neighbors(vq ), neighbors(vimg )))/2.

Similarity between two vertices are calculated based on their word-meaning similarity and neighbor similarity. Here
10 For complete instructions provided to the turkers, please check out Appendix.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

Figure 6: The SDGs in (d), (e) and (f) corresponds to images (a), (b) and (c) respectively. For more detailed examples, please check Appendix and http://bit.ly/1NJycKO.

wnsim(., .) is WordNet-Lin Similarity [29] between two words and Jaccard(., .) is the standard Jaccard coefficient similarity. Based on the above similarity measure, we give the image retrieval results compared with few of the stateof-the-art results in Table 3.
Model [21] BRNN Our Method-SDG [21] BRNN Our Method-SDG [21] BRNN (1k) Our Method-SDG (1k) Our Method-SDG (2k) R@1 11.8 18.1 15.2 26.5 20.9 19.3 15.4 Flickr8k R@5 R@10 32.1 44.7 39.0 50.0 Flickr30k 37.7 50.5 48.7 59.4 MS-COCO 52.8 69.2 35.5 49.0 32.5 42.2 Med r 12.4 10.5 9.2 6.0 4.0 11.0 17.0

the sentences and images can seamlessly converge to such space of graphical representations. This could have huge repercussions in search in Image and Textual space and storing knowledge from images and text together in a unified Knowledge Base.

6. Conclusion
This paper introduced a reasoning module to generate textual descriptions from images by first constructing a new intermediate semantic representation, namely the Scene Description Graph (SDG), which is later used to generate sentences. The reasoning module uses an automatically constructed Knowledge Base created from text, to capture "commonsense" knowledge. Having built the Knowledge Base, we proposed a method of obtaining such SDGs from noisy labels using our prediction system. The SDG is a representation of the scene in view that integrates direct visual knowledge (objects and their locations in the scene) with background commonsense knowledge. In addition, the SDGs have a structure similar to semantic representations of sentences, thus facilitating the interaction between Vision and Natural Language. The notion of the SDG has great potential. Here we used the SDG for the automatic creation of sentences describing the scene; but, equipped with background knowledge, it also allows reasoning and question/answering about the scene 11 . To demonstrate the effectiveness of the sentences and constructed SDGs, we performed a number of experiments. Our AMT evaluations on popular datasets show that our sentences performs comparatively well with respect to the state-of-the-art in measures of relevance and thoroughness. A Gold-Standard based evaluation shows that our output SDGs can detect events and entities with comparable accuracy as a state-of-the-art system. And lastly, our Image Retrieval experiment shows that the Image-Sentence alignment quality is comparable with state-of-the-art results.
11 Please

Table 3: Image-Search Results: We report the recall@K (for K = 1, 5 and 10) and Med r (Median Rank) metric for Flickr8k, 30k and COCO datasets. For COCO, we experimented on first 1000 (1k) and random 2000 (2k) validation images.

One of the primary contributions of our work is the Knowledge-Structure representation that bridges the gap between semantic information in text and images. From the results of this experiment, the benefit of having such an intermediate representation is easy to observe. Example Images and SDGs: As examples, we pick a few images which produces objects and scene recognitions with comparably good confidence scores. The images and their corresponding SDGs are provided in Figure 6. As we can observe, the information produced by these SDGs are easily processed by machines. We can answer questions such as how entities interact in an event, which possible events are in the scene and how entities interact in a scene. We should also mention that the concept-level modeling provided by SDGs is what separates this work from other recent approaches [20]. Furthermore, comparing these structures with the K-Parser output in Figure 4, we can see how

see appendix for an example.

References
[1] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798­1828, 2013. 3 [2] V. K. Chaudhri, B. E. John, S. Mishra, J. Pacheco, B. Porter, and A. Spaulding. Enabling experts to build knowledge bases from science textbooks. In Proceedings of the 4th International Conference on Knowledge Capture, K-CAP '07, pages 159­166, New York, NY, USA, 2007. ACM. 2, 4 [3] X. Chen and C. L. Zitnick. Learning a recurrent visual representation for image caption generation. arXiv preprint arXiv:1411.5654, 2014. 1 [4] P. Clark, B. Porter, and B. P. Works. Km-the knowledge machine 2.0: Users manual. Department of Computer Science, University of Texas at Austin, 2004. 2, 5 [5] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886­893. IEEE, 2005. 1 [6] E. Davis and G. Marcus. Commonsense reasoning and commonsense knowledge in artificial intelligence. Commun. ACM, 58(9):92­103, Aug. 2015. 1 [7] S. K. Divvala, A. Farhadi, and C. Guestrin. Learning everything about anything: Webly-supervised visual concept learning. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pages 3270­3277, 2014. 2 [8] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389, 2014. 1 [9] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 647­655, 2014. 3 [10] D. Elliott and F. Keller. Image description using visual dependency representations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1292­1302, 2013. 2 [11] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1778­1785. IEEE, 2009. 1 [12] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Generating sentences from images. In Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV'10, pages 15­29, Berlin, Heidelberg, 2010. Springer-Verlag. 2 [13] P. Felzenszwalb, D. McAllester, and D. Ramanan. A discriminatively trained, multiscale, deformable part model. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1­8. IEEE, 2008. 1

[14] A. Gatt and E. Reiter. Simplenlg: A realisation engine for practical applications. In Proceedings of the 12th European Workshop on Natural Language Generation, ENLG '09, pages 90­93, Stroudsburg, PA, USA, 2009. Association for Computational Linguistics. 6 [15] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition, 2014. 3 [16] D. Gunning, V. K. Chaudhri, P. Clark, K. Barker, S. yi Chaw, B. Grosof, A. Leung, D. Mcdonald, S. Mishra, J. Pacheco, B. Porter, A. Spaulding, D. Tecuci, and J. Tien. Project halo updateprogress toward digital aristotle. 2 [17] A. Gupta and L. S. Davis. Objects in action: An approach for combining action understanding and object perception. In Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on, pages 1­8. IEEE, 2007. 1 [18] C. Havasi, R. Speer, and J. Alonso. Conceptnet 3: a flexible, multilingual semantic network for common sense knowledge. In Recent advances in natural language processing, pages 27­29. Citeseer, 2007. 1 [19] M. Hodosh, P. Young, and J. Hockenmaier. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, pages 853­899, 2013. 2, 3 [20] J. Johnson, R. Krishna, M. Stark, J. Li, M. Bernstein, and L. Fei-Fei. Image retrieval using scene graphs. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. 2, 7, 8 [21] A. Karpathy and F.-F. Li. Deep visual-semantic alignments for generating image descriptions. arXiv preprint arXiv:1412.2306, 2014. 1, 2, 3, 7, 8 [22] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014. 1 [23] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS 2012, 2013. 1, 3 [24] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. Baby talk: Understanding and generating image descriptions. In Proceedings of the 24th CVPR, 2011. 2 [25] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and Y. Choi. Collective generation of natural image descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL '12, pages 359­368, Stroudsburg, PA, USA, 2012. Association for Computational Linguistics. 2 [26] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 951­958. IEEE, 2009. 1 [27] I. Laptev. On space-time interest points. International Journal of Computer Vision, 64(2-3):107­123, 2005. 1 [28] D. B. Lenat. Cyc: A large-scale investment in knowledge infrastructure. Commun. ACM, 38(11):33­38, Nov. 1995. 1

[29] D. Lin. An information-theoretic definition of similarity. In ICML, volume 98, pages 296­304, 1998. 8 [30] D. G. Lowe. Object recognition from local scale-invariant features. In Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 1150­1157. Ieee, 1999. 1 [31] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille. Explain images with multimodal recurrent neural networks. arXiv preprint arXiv:1410.1090, 2014. 1 [32] R. Messing, C. Pal, and H. Kautz. Activity recognition using the velocity histories of tracked keypoints. In Computer Vision, 2009 IEEE 12th International Conference on, pages 104­111. IEEE, 2009. 1 [33] A. S. Ogale, A. Karapurkar, and Y. Aloimonos. Viewinvariant modeling and recognition of human actions using grammars. In R. Vidal, A. Heyden, and Y. Ma, editors, WDV, volume 4358 of Lecture Notes in Computer Science, pages 115­126. Springer, 2006. 1 [34] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describing images using 1 million captioned photographs. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. C. N. Pereira, and K. Q. Weinberger, editors, NIPS, pages 1143­1151, 2011. 2 [35] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. Cnn features off-the-shelf: an astounding baseline for recognition. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on, pages 512­519. IEEE, 2014. 3 [36] M. Santofimia, J. Martinez-del Rincon, and J.-C. Nebel. Common-Sense Knowledge for a Computer Vision System for Human Action Recognition. In J. Bravo, R. Herv´ as, and M. Rodr´ iguez, editors, Ambient Assisted Living and Home Care, volume 7657 of Lecture Notes in Computer Science, pages 159­166. Springer Berlin Heidelberg, 2012. 2 [37] S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D. Manning. Generating semantically precise scene graphs from textual descriptions for improved image retrieval. In Proceedings of the Fourth Workshop on Vision and Language, pages 70­80, Lisbon, Portugal, September 2015. Association for Computational Linguistics. 2 [38] M. Scutari. Learning bayesian networks with the bnlearn R package. Journal of Statistical Software, 35(3):1­22, 2010. 6 [39] A. Sharma, N. H. Vo, S. Aditya, and C. Baral. Towards addressing the winograd schema challenge - building and using a semantic parser and a knowledge hunting module. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 1319­1325, 2015. 4 [40] R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng. Grounded compositional semantics for finding and describing images with sentences. TACL, 2:207­218, 2014. 2 [41] D. G. Stork. HAL's Legacy: 2001's Computer as Dream and Reality. MIT Press, 1998. 1 [42] C. L. Teo, C. Ferm¨ uller, and Y. Aloimonos. A gestaltist approach to contour-based object recognition: Combining bottom-up and top-down cues. The International Journal of Robotics Research, page 0278364914558493, 2015. 1

[43] C. L. Teo, A. Myers, C. Fermuller, and Y. Aloimonos. Embedding high-level information into low level vision: Efficient object search in clutter. In Robotics and Automation (ICRA), 2013 IEEE International Conference on, pages 126­ 132. IEEE, 2013. 1 [44] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. arXiv preprint arXiv:1411.4555, 2014. 1 [45] H. Wang, A. Klaser, C. Schmid, and C.-L. Liu. Action recognition by dense trajectories. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 3169­3176. IEEE, 2011. 1 [46] Y. Yang, C. Ferm¨ uller, Y. Aloimonos, and A. Guha. A cognitive system for understanding human manipulation actions. Advances in Cognitive Systems, 3:67­86, 2014. 1 [47] Y. Yang, C. L. Teo, H. Daum´ e, III, and Y. Aloimonos. Corpus-guided sentence generation of natural images. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP '11, pages 444­454, Stroudsburg, PA, USA, 2011. Association for Computational Linguistics. 2 [48] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S. C. Zhu. I2t: Image parsing to text description. Proceedings of the IEEE, 98(8):1485­1508, 2010. 2 [49] X. Yu and Y. Aloimonos. Attribute-based transfer learning for object categorization with zero/one training example. In Computer Vision­ECCV 2010, pages 127­140. Springer Berlin Heidelberg, 2010. 1 [50] X. Yu, C. Fermuller, C. L. Teo, Y. Yang, and Y. Aloimonos. Active scene recognition with vision and language. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 810­817. IEEE, 2011. 1 [51] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. NIPS, 2014. 3 [52] C. L. Zitnick and D. Parikh. Bringing semantics into focus using visual abstraction. In CVPR, pages 3009­3016. IEEE, 2013. 2



r 

PP  r s t ts tr  srt rs s ss s   tts
    stsr s  s s rrs s  rt ts  r s  rt ts  strts    



stsr s  s

r     r r   r  ss  rs s tt t sr

1.36056

 ts  ts s t   st  ss rs tt t  

r t   r  t rt ts   r

has(scene,component,water). has(scene,component,water_droplets). has(scene,component,exterior_of_building). has(person1,semantic_role,drinker). has(water,semantic_role,liquid). has(person1,semantic_role,creator). has(drink,recipient,water). has(drink,agent,person1). has(drink,origin,fountain). has(drink,next_event,make).




r 

  t t   s   rt s t  trs s ts  s  t t  sr t Prr    s t qst tt s s r r t t  P   sr s  t  ts s t  rr  

entity(person;dog;water;shorts;frisbee). animate(person;dog). inanimate(A) :- not animate(A), entity(A). drink_yes :- animate(A), has(drink,agent,A), has(drink,recipient,water). yes_fountain(A) :- drink_yes, has(drink,agent,A), has(drink,origin, fountain). #hide. #show yes_fountain/1.
 t t  rr    t t sr s

strs





r 



s rrs s  rt ts

r  s t

r  s t





r 

r  s t

r  s t





r 

r  s t

  r s  rt ts

r  t s





r 

r  t s

r  t s





r 

r  t s





r 

  strts

r   t s strts s r t t rrs

r  rss t s strts s r t t rrs



