An Action Language for Multi-Agent Domains: Foundations

arXiv:1511.01960v1 [cs.AI] 6 Nov 2015

Chitta Baral, Gregory Gelfond
School of Computing, Informatics & DSE
Arizona State University
{chitta,ggelfond}@asu.edu

Enrico Pontelli, Tran Cao Son
Department of Computer Science
New Mexico State University
{epontell,tson}@cs.nmsu.edu

Abstract
In multi-agent domains, an agent’s action may not just change the world and the agent’s knowledge and beliefs
about the world, but also may change other agents’ knowledge and beliefs about the world and their knowledge and
beliefs about other agents’ knowledge and beliefs about the world. Similarly, the goals of an agent in a multi-agent
world may involve manipulating the knowledge and beliefs of other agents’ and again, not just their knowledge1
about the world, but also their knowledge about other agents’ knowledge about the world. The goal of this paper is to
present an action language, called mA+, that has the necessary features to address the above aspects in representing
and reasoning about actions and change in multi-agent domains.
This action language can be viewed as a generalization of the single-agent action languages extensively studied
in the literature, to the case of multi-agent domains. The language allows the representation of and reasoning about
different types of actions that an agent can perform in a domain where many other agents might be present—such as
world-altering actions, sensing actions, and announcement/communication actions. The action language also allows
the specification of agents’ dynamic awareness of action occurrences which has future implications on what agents’
know about the world and other agents’ knowledge about the world. The language mA+ considers three different
types of awareness: full awareness, partial awareness, and complete oblivion of an action occurrence and its effects.
This keeps the language simple, yet powerful enough to address a large variety of knowledge manipulation scenarios
in multi-agent domains.
The semantics of the language relies on the notion of state, which is described by a pointed Kripke model and
is used to encode the agent’s knowledge and the real state of the world. The semantics is defined by a transition
function that maps pairs of actions and states into sets of states. The paper illustrates properties of the action theories,
including properties that guarantee finiteness of the set of initial states and their practical implementability. Finally,
the paper relates mA+ to other related formalisms that contribute to reasoning about actions in multi-agent domains.

Keywords: Action Languages, Multi-Agent Domains, Planning.

1

Introduction and Motivation

Reasoning about actions and change has been a research focus since the early days of artificial intelligence [1]; languages for representing actions and their effects were proposed soon after [2]. Although the early papers on this [2]
did not include formal semantics, papers with formal semantics came some years later [3]. The approach adopted in
this paper is predominantly influenced by the methodology for representing and reasoning about actions and change
(RAC) proposed by Gelfond and Lifschitz [4]. In this approach, actions of agents are described in a high-level language, with an English-like syntax and a transition function based semantics. Among other things, action languages
provide a succinct way for representing dynamic domains. The approach proposed in this paper is also related to
action description languages developed for planning, such as [5, 6].
Over the years, several action languages (e.g., A, B, and C) have been developed [7]. Each of these languages addresses some important problems in RAC (e.g., the ramification problem, concurrency, actions with duration, knowledge of agents). Action languages have also provided the foundations for several successful approaches to automated
1 Often,

we will simply say “knowledge” to mean both “knowledge” and “beliefs.” This will be clear from the context.

1

planning; for example, the language C is used in the planner C-P LAN [8] and the language B is used in C PA [9].
However, the main focus of these research efforts has been about reasoning within single agent domains.
In single agent domains, reasoning about actions and change mainly involves reasoning about what is true in
the world, what the agent knows about the world, how the agent can manipulate the world (using world-changing
actions) to reach particular states, and how the agent (using sensing actions) can learn unknown aspects of the world.
In multi-agent domains an agent’s action may not just change the world and the agent’s knowledge about the world,
but also may change other agents’ knowledge about the world and their knowledge about other agents’ knowledge
about the world. Similarly, goals of an agent in a multi-agent world may involve manipulating the knowledge of
other agents—in particular, this may involve not just their knowledge about the world, but also their knowledge
about other agents’ knowledge about the world. Although there is a large body of research on multi-agent planning
[10, 11, 12, 13, 14, 15, 16, 17, 18], very few efforts address the above aspects of multi-agent domains which pose a
number of new research challenges in representing and reasoning about actions and change. The following example
illustrates some of these issues.
Example 1 (Three Agents and the Coin Box) Three agents, A, B, and C, are in a room. In the middle of the room
there is a box containing a coin. It is common knowledge that:
• None of the agents knows whether the coin lies heads or tails up;
• The box is locked and one needs to have a key to open it; only agent A has the key of the box;
• In order to learn whether the coin lies heads or tails up, an agent can peek into the box. This, however, requires
the box to be open;
• If an agent is looking at the box and someone peeks into the box, then the first agent will notice this fact and
will be able to conclude that the second agent knows the status of the coin; yet, the first agent’s knowledge about
which face of the coin is up does not change;
• Distracting an agent causes that agent to not look at the box;
• Signaling an agent to look at the box causes this agent to look at the box;
• Announcing that the coin lies heads or tails up will make this a common knowledge among the agents that are
listening.
Suppose that the agent A would like to know whether the coin lies heads or tails up. She would also like to let the
agent B know that she knows this fact. However, she would like to keep this information secret from C. (Note that
the last two sentences express goals that are about agents’ knowledge about other agents’ knowledge.) Intuitively, she
could achieve her goal by:
1. Distracting C from looking at the box;
2. Signaling B to look at the box;
3. Opening the box; and
2

4. Peeking into the box.

This simple story presents a number of challenges for research in representing and reasoning about actions and their
effects in multi-agent domains. In particular:
• The domain contains several types of actions:
– Actions that allow the agents to change the state of the world (e.g., opening the box);
– Actions that change the knowledge of the agents (e.g, peeking into the box, announcing head/tail);
– Actions that manipulate the beliefs of other agents (e.g., peeking while other agents are looking); and
– Actions that change the observability of agents with respect to awareness about future actions (e.g., distract
and signal actions before peeking into the box).
We observe that the third and fourth types of actions are not considered in single agent systems.

2

• The reasoning process that helps A to realize that steps (1)-(4) will achieve her goal requires A’s ability to reason
about the effects of her actions on several entities:
– The state of the world—e.g., opening the box causes the box to become open;
– The agents’ awareness of the environment and of other agents’ actions—e.g., distracting or signaling an
agent causes this agent not to look or to look at the box; and
– The knowledge of other agents about her own knowledge—e.g., someone following her actions would
know what she knows.
While the first requirement is the same as for an agent in single agent domains, the last two are specific to
multi-agent domains.
With respect to planning, the above specifics of multi-agent systems raise an interesting question:
“How can one generate a plan for the agent A to achieve her goal, given the description in Example 1?”
To the best of our knowledge, except a dynamic epistemic modeling system called DEMO [19], there exists no
automated planner that can address the complete range of issues as exemplified in Example 1. This is in stark contrast
to the landscape of automated planning for single agent domains, where we can find several automated planners
capable of generating plans consisting of hundreds of actions within seconds—especially building on recent advances
in search-based planning.
Among the main reasons for the lack of planning systems capable of dealing with the issues like those shown in
Example 1 are: (i) lack of action-based formalisms that can address the above mentioned issues and that can actually be
orchestrated, and (ii) the fact that logical approaches to reasoning about knowledge of agents in multi-agent domains
are mostly model-theoretical, and not as amenable to an implementation in search-based planning systems. This is
not the case for single-agent domains, where the de-facto standard Planning Domain Description Language (PDDL)
provides a transition-based semantics which allows the creation of highly efficient heuristic search based planners.
In terms of related work, multi-agent actions have been explored in Dynamic Epistemic Logics (DEL) (e.g., [20,
21, 22, 23, 24]). However, as discussed later in the paper, DEL does not offer an intuitive view of how to orchestrate
or execute a single multi-agent action. In addition, the complex representation of multi-agent actions (akin to Kripke
structures) drastically increases the number of possible multi-agent actions—thus, making it difficult to search for
multi-agent action sequences. The research in DEL has also not addressed some critical aspects of multi-agent searchbased planning, such as the determination of the initial state of a planning domain instance. Moreover, research in
DEL did not explore the link between the state of the world and the observability encoded in multi-agent actions, and
hence preventing the dynamic evolution of the observational capabilities and awareness of agents with respect to future
actions. In some ways, the DEL approach is similar to the formulations of belief updates (e.g., [25, 26, 27]), and most
of the differences and similarities between belief updates and reasoning about actions carry over to the differences and
similarities between DEL and our formulation of RAC in multi-agent domains. We will elaborate on these differences
in a later section of the paper.

1.1

Contributions and Assumptions

Our goal in this paper is to develop a framework that allows reasoning about actions and their effects in a multi-agent
domain; the framework is expected to address the above-mentioned issues, e.g., actions’ capability to modify agents’
knowledge about other agents’ knowledge. To this end, we propose a high-level action language for representing and
reasoning about actions in multi-agent domains. The language provides an initial set of elements of a planning domain
description language for multi-agent systems. The main contributions of the paper are:
• The action language mA+, which considers different types of actions—such as world-altering actions, announcement actions, and sensing actions—for formalizing multi-agent domains, along with actions that affect
the dynamic awareness and observation capabilities of the agents;
• A transition function based semantics for mA+, that enables hypothetical reasoning and planning in multi-agent
domains;
3

• The notion of definite action theories, that characterizes action theories for which the computation of the initial
state is possible—which is an essential component in the implementation of a heuristic search-based planner for
domains described in mA+; and
• Several theorems relating the semantics of mA+ to multi-agent actions characterizations using update models
in DEL of [20].
In developing mA+, we make several assumptions and design decisions. The key decision is that actions in our
formalism can be effectively executed.2 We also assume that actions are deterministic. This assumption can be lifted
in a relatively simple manner by generalizing the treatment of non-deterministic actions, state constraints, and parallel
actions studied in the context of single-agent domains.
Next, although we have mentioned both knowledge and beliefs, in this paper we will follow [24, 20] and focus only
on formalizing the changes of beliefs of agents after the execution of actions. Following the considerations in [22], the
epistemic operators used in this paper can be read as “to the best my information.” Note that, in a multi-agent system,
there may be a need to distinguish between knowledge and beliefs of an agent about the world. For example, let us
consider Example 1 and let us denote with p the proposition “nobody knows whether the coin lies heads or tails up.”
Initially, all of the three agents know that p is true. However, after agent A executes the sequence of actions (1)-(4), A
will know that p is false. Furthermore, B also knows that p is false, thanks to her awareness of A’s execution of the
actions of opening the box and looking into it. However, C—being unaware of the execution of the actions performed
by A—will still believe that p is true. If this were considered as a part of C’s knowledge, then C would result in having
false knowledge.
The investigation of the discrepancy between knowledge and beliefs has been an intense research topic in dynamic
epistemic logic and in reasoning about knowledge, which has lead to the development of several modal logics (e.g.,
[28, 24]). Since our main aim is the development of an action language for hypothetical reasoning and planning, we
will be concerned mainly with beliefs of agents. A consequence of this choice is that we will not consider situations
where sensing actions, or other knowledge acquisition actions, are employed to correct a wrong belief of agents. For
example, an agent will not be able to use a sensing action to realize that her beliefs about a certain property of the
world is wrong; or an agent will not be able to use an announcement action to correct an incorrect belief of other
agents. Some preliminary steps in this direction have been explored in the context of the DEL framework [21, 29]. We
leave the development of an action based formalism that takes into consideration the differences between beliefs and
knowledge as future work.

1.2

Paper Organization

The rest of the paper is organized as follows. Section 2 reviews the basics definitions and notation of a modal logic
with belief operators and the update model based approach to reasoning about actions in multi-agent domains. This
section also describes a set of operators on Kripke structures, that will be used in defining the semantics of mA+.
Section 3 presents the syntax of mA+. Section 4 defines the transition function of mA+ which maps pairs of actions
and states into states; the section also presents the entailment relation between mA+ action theories and queries.
Section 5 explores the modeling of the semantics of mA+ using the update models approach. Section 6 identifies a
class of mA+ theories whose initial states can be finitely characterized and effectively computed. Section 7 provides
an analysis of mA+ with respect to the existing literature, including a comparison with DEL. Section 8 provide some
concluding remarks and directions for future work.
For simplicity of presentation, the proofs of the main lemmas, propositions, and theorems are placed in the appendix.

2

Preliminaries

We begin with a review of the basic notions from the literature on formalizing knowledge and reasoning about effects
of actions in multi-agent systems. Subsection 2.1 presents the notion of Kripke structures. Subsection 2.2 reviews the
2 This is not the case in DEL [24], where actions are complex graph structures, similar to Kripke structures, possibly representing a multi-modal
formula, and it is not clear if and how such actions can be executed.

4

notion of update models developed by the dynamic epistemic logic community for reasoning about effects of actions in
multi-agent systems. In Subsection 2.3, we investigate a collection of basic operators for modifying Kripke structures,
that will be used in our semantic formalization in the following sections.

2.1

Belief Formulae and Kripke Structures

Let us consider an environment with a set AG of n agents. The real state of the world (or real state, for brevity) is
described by a set F of propositional variables, called fluents. We are concerned with the beliefs of agents about the
environment and about the beliefs of other agents. For this purpose, we adapt the logic of knowledge and the notations
used in [28, 24]. We associate to each agent i ∈ AG a modal operator Bi and represent the beliefs of an agent as belief
formulae in a logic extended with these operators. Formally, we define belief formulae as follows:
• Fluent formulae: a fluent formula is a propositional formula built using the propositional variables in F and the
traditional propositional operators ∨, ∧, →, ¬, etc. In particular, a fluent atom is a formula containing just an
element f ∈ F, while a fluent literal is either a fluent atom f ∈ F or its negation ¬f . We will use > and ⊥ to
denote true and false, respectively.
• Belief formulae: a belief formula is a formula in one of the following forms:
– a fluent formula;
– a formula of the form Bi ϕ where ϕ is a belief formula;
– a formula of the form ϕ1 ∨ ϕ2 , ϕ1 ∧ ϕ2 , ϕ1 ⇒ ϕ2 , or ¬ϕ1 where ϕ1 , ϕ2 are belief formulae;
– a formula of the form Eα ϕ or Cα ϕ, where ϕ is a formula and ∅ =
6 α ⊆ AG.
Formulae of the form Eα ϕ and Cα ϕ are referred to as group formulae. Whenever α = AG, we simply write Eϕ and
Cϕ to denote Eα ϕ and Cα ϕ, respectively. Let us denote with LAG the language of the belief formulae over F and
AG.
Intuitively, belief formulae are used to describe the beliefs of one agent concerning the state of the world as well
as about the beliefs of other agents. For example, the formula B1 B2 p expresses the fact that “Agent 1 believes that
agent 2 believes that p is true,” while B1 f states that “Agent 1 believes that f is true.”
In what follows, we will simply talk about “formulae” instead of “belief formulae,” whenever there is no risk of
confusion. The notion of a Kripke structure is defined next.
Definition 1 (Kripke Structure) A Kripke structure is a tuple hS, π, B1 , . . . , Bn i, where
• S is a set of worlds,
• π : S 7→ 2F is a function that associates an interpretation of F to each element of S, and
• For 1 ≤ i ≤ n, Bi ⊆ S × S is a binary relation over S.
A pointed Kripke structure is a pair (M, s) where M = hS, π, B1 , . . . , Bn i is a Kripke structure and s ∈ S. In a
pointed Kripke structure (M, s), we refer to s as the real (or actual) world.
Intuitively, a Kripke structure describes the possible worlds envisioned by the agents—and the presence of multiple
worlds identifies uncertainty and presence of different beliefs. The relation (s1 , s2 ) ∈ Bi denotes that the belief of
agent i about the real state of the world is insufficient for her to distinguish between the world described by s1 and the
one described by s2 . The world s in the state (M, s) identifies the world in M [S] that corresponds to the actual world.
We will often view a Kripke structure M = hS, π, B1 , . . . , Bn i as a directed labeled graph, whose set of nodes is
S and whose set of edges contains (s, i, t)3 if and only if (s, t) ∈ Bi . (s, i, t) is referred to as an edge coming out of
(resp. into) the world s (resp. t).
3 (s, i, t)

denotes the edge from node s to node t, labeled by i.

5

For the sake of readability, we use M [S], M [π], and M [i] to denote the components S, π, and Bi of M , respectively. We write M [π](u) to denote the interpretation associated to u via π and M [π](u)(ϕ) to denote the truth value
of a fluent formula ϕ with respect to the interpretation M [π](u).
Following [24], we will refer to a pointed Kripke structure (M, s) as a state and often use these two terms interchangeably.
In keeping with the tradition of action languages, we will often refer to π(u) as the set of fluent literals defined by
{f | f ∈ F, M [π](u)(f ) = >} ∪ {¬f | f ∈ F, M [π](u)(f ) = ⊥}.
Given a consistent and complete set of literals S, i.e., |{f, ¬f } ∩ S| = 1 for every f ∈ F, we write π(u) = S or
M [π](u) = S to indicate that the interpretation M [π](u) is defined in such a way that π(u) = S.
The satisfaction relation between belief formulae and a state is defined as next.
Definition 2 Given a formula ϕ, a Kripke structure M = hS, π, B1 , . . . , Bn i, and a world s ∈ S:
(i) (M, s) |= ϕ if ϕ is a fluent formula and π(s) |= ϕ;
(ii) (M, s) |= Bi ϕ if for each t such that (s, t) ∈ Bi , (M, t) |= ϕ;
(iii) (M, s) |= ¬ϕ if (M, s) 6|= ϕ;
(iv) (M, s) |= ϕ1 ∨ ϕ2 if (M, s) |= ϕ1 or (M, s) |= ϕ2 ;
(v) (M, s) |= ϕ1 ∧ ϕ2 if (M, s) |= ϕ1 and (M, s) |= ϕ2 .
(vi) (M, s) |= Eα ϕ if (M, s) |= Bi ϕ for every i ∈ α.
(vii) (M, s) |= Cα ϕ if (M, s) |= Eαk ϕ for every k ≥ 0, where Eα0 ϕ = ϕ and Eαk+1 = Eα (Eαk ϕ).
For a Kripke structure M and a formula ϕ, M |= ϕ denotes the fact that (M, s) |= ϕ for each s ∈ M [S], while |= ϕ
denotes the fact that M |= ϕ for every Kripke structure M .
Example 2 (State) Let us consider a simplified version of Example 1 in which the agents are concerned only with
the status of the coin. The three agents A, B, C do not know whether the coin has ‘heads’ or ‘tails’ up and this is a
common belief. Let us assume that the coin is heads up. The beliefs of the agents about the world and about the beliefs
of other agents can be captured by the state of Figure 1.

S1:
¬tail

A, B, C

S2:
tail

A, B, C

A, B, C

Figure 1: An example of a state
In the figure, a circle represents a world. The name and interpretation of the world are written in the circle.
Labeled edges between worlds denote the belief relations of the structure. A double circle identifies the real world.
We will occasionally be interested in Kripke structures that satisfy certain conditions. In particular, given a Kripke
structure M = hS, π, B1 , . . . , Bn i we identify the following properties:
• K: for each agent i and formulae ϕ, ψ, we have that M |= (Bi ϕ ∧ Bi (ϕ ⇒ ψ)) ⇒ Bi ψ.
• T: for each agent i and formula ψ, we have that M |= Bi ψ ⇒ ψ.
6

• 4: for each agent i and formula ψ, we have that M |= Bi ψ ⇒ Bi Bi ψ.
• 5: for each agent i and formula ψ, we have that M |= ¬Bi ψ ⇒ Bi ¬Bi ψ.
• D: for each agent i we have that M |= ¬Bi ⊥.
A Kripke structure is said to be a T-Kripke (4-Kripke, K-Kripke, 5-Krikpe, D-Kripke, respectively) structure if it
satisfies property T (4, K, 5, D, respectively). A Kripke structure is said to be a S5 structure if it satisfies the properties
K, T, 4, and 5. Consistency of a set of formulae is defined next.
Definition 3 A set of belief formulae X is said to be p-satisfiable (or p-consistent) for p ∈ {S5, K, T, 4, 5} if there
exists a p-Kripke structure M and a world s ∈ M [S] such that (M, s) |= ψ for every ψ ∈ X. In this case, (M, s) is
referred to as a p-model of X.
Finally, let us introduce a notion of equivalence between states.
Definition 4 A state (M, s) is equivalent to a state (M 0 , s0 ) if (M, s) |= ϕ iff (M 0 , s0 ) |= ϕ for every formula
ϕ ∈ LAG .

2.2

Update Models

Update models are used to describe transformations of (pointed) Kripke structures according to a predetermined transformation pattern. An update model uses structures similar to pointed Kripke structures and they describe the effects
of a transformation on states using an update operator [20, 23].
Let us start with some preliminary definitions. An LAG -substitution is a set {p1 → ϕ1 , . . . , pk → ϕk }, where
each pi is a distinct fluent in F and each ϕi ∈ LAG . We will implicitly assume that for each p ∈ F \ {p1 , . . . , pk },
the substitution contains p → p. SU BLAG denotes the set of all LAG -substitutions.
Definition 5 (Update Model) Given a set AG of n agents, an update model Σ is a tuple hΣ, R1 , . . . , Rn , pre, subi
where
(i) Σ is a set, whose elements are called events;
(ii) each Ri is a binary relation on Σ;
(iii) pre : Σ → LAG is a function mapping each event e ∈ Σ to a formula in LAG ; and
(iv) sub : Σ → SU BLAG is a function mapping each event e ∈ Σ to a substitution in SU BLAG .
An update instance ω is a pair (Σ, e) where Σ is an update model hΣ, R1 , . . . , Rn , pre, subi and e, referred to as a
designated event, is a member of Σ.
Intuitively, an update model represents different views of an action occurrence which are associated with the observability of agents. Each view is represented by an event in Σ. The designated event is the one that agents who are
aware of the action occurrence will observe. The relation Ri describes agent i’s uncertainty on action execution—i.e.,
if (σ, τ ) ∈ Ri and event σ is performed, then agent i may believe that event τ is executed instead. pre defines the
action precondition and sub specifies the changes of fluent values after the execution of an action.
Definition 6 (Updates by an Update Model) Let M be a Kripke structure and Σ = hΣ, R1 , . . . , Rn , pre, subi be an
update model. The update operator induced by Σ defines a Kripke structure M 0 = M ⊗ Σ, where:
(i) M 0 [S] = {(s, τ ) | s ∈ M [S], τ ∈ Σ, (M, s) |= pre(τ )};
(ii) ((s, τ ), (s0 , τ 0 )) ∈ M 0 [i] iff (s, τ ), (s0 , τ 0 ) ∈ M 0 [S], (s, s0 ) ∈ M [i] and (τ, τ 0 ) ∈ Ri ;
(iii) For all f ∈ F, M 0 [π]((s, τ )) |= f iff f → ϕ ∈ sub(τ ) and (M, s)|=ϕ.

7

The structure M 0 is obtained from the component-wise cross-product of the old structure M and the update model
Σ, by (i) removing pairs (s, τ ) such that (M, s) does not satisfy the action precondition (checking for satisfaction of
action’s precondition), and (ii) removing links of the form ((s, τ ), (s0 , τ 0 )) from the cross product of M [i] and Ri if
(s, s0 ) 6∈ M [i] or (τ, τ 0 ) 6∈ Ri (ensuring that each agent’s accessibility relation is updated according to the update
model).
An update template is a pair (Σ, Γ) where Σ is an update model with the set of events Σ and Γ ⊆ Σ. The update
of a state (M, s) given an update template (Σ, Γ) is a set of states, denoted by (M, s) ⊗ (Σ, Γ), where
(M, s) ⊗ (Σ, Γ) = {(M ⊗ Σ, (s, τ )) | τ ∈ Γ, (M, s) |= pre(τ )}
Remark 1 In the following, we will often represent an update instance by a graph with rectangles representing events,
double rectangles designated events, and labeled links between rectangles representing the relation of agents as in the
graphical representation of a Kripke model.

2.3

Kripke Structures Operators

The execution of an action in a multi-agent environment will change the real state of the world and/or the beliefs of
agents. As we will see later, we will employ the notion of a pointed Kripke structure in representing the real state of
the world and the beliefs of agents. As such, the execution of an action in a pointed Kripke structure will modify it.
Such a change can be a combination of the following basic changes:4
• Removing some worlds and/or edges from the structure;
• Adding some worlds and/or edges to the structure; or
• Replicating a structure or merging different structures into one.
In this subsection, we describe some basic operators on Kripke structures that will satisfy the above needs. We will
assume that a Kripke structure M is given. The first operator on Kripke structures is to remove a set of worlds from
M.
s

Definition 7 (World Subtraction) Given a set of worlds U ⊆ M [S], M 	 U is a Kripke structure M 0 defined by:
(i) M 0 [S] = M [S] \ U ;
(ii) M 0 [π](s)(f ) = M [π](s)(f ) for every s ∈ M 0 [S] and f ∈ F; and
(iii) M 0 [i] = M [i] \ {(t, v) | (t, v) ∈ M [i], {t, v} ∩ U 6= ∅}.
s

Intuitively, M 	 U is the Kripke structure obtained from M by removing the worlds in U and all edges connected to
these worlds. The next two operators are used to remove edges from or introduce edges into a given structure.
a

Definition 8 (Edge Subtraction) For a set of edges X in M , M 	 X is a Kripke structure, M 0 , defined by:
(i) M 0 [S] = M [S];
(ii) M 0 [π] = M [π]; and
(iii) M 0 [i] = M [i] \ {(u, i, v) | (u, i, v) ∈ X} for every i ∈ AG.
a

M 	 X is the Kripke structure obtained from M by removing from M all the edges in X. Given a state (M, s) and a
a

set of agents α ⊆ AG, we denote with (M |α , s) the state (M 	 X, s) where
[
X=
{(u, i, v) | (u, v) ∈ M [i]}.
i∈AG\α
a

Definition 9 (Edge Addition) For a set of edges X in M , M ⊕ X is a Kripke structure, M 0 , defined by:
4 We

will focus on viewing a (pointed) Kripke structure as a labeled graph.

8

(i) M 0 [S] = M [S];
(ii) M 0 [π] = M [π]; and
(iii) M 0 [i] = M [i] ∪ {(u, v) | (u, i, v) ∈ X} for every i ∈ AG.
a

a

While 	 removes edges from the structure, ⊕ is used to add edges to an existing structure. Recall that the nodes/edges
in a Kripke structure indicate uncertainty in the agents’ beliefs. As such, these operators are useful for updating the
beliefs of the agents in a domain when the beliefs of the agents change.
Definition 10 (Equivalence Modulo c) Given two Kripke structures M1 and M2 , we say that M1 is c-equivalent5 to
c
M2 (denoted as M1 ∼ M2 ) if there exists a bijective function c : M1 [S] → M2 [S], such that for every u ∈ M1 [S] and
f ∈ F, we have that:
1. M1 [π](u)(f ) = > if and only if M2 [π](c(u))(f ) = >; and
2. for every i ∈ AG and u, v ∈ M1 [S], (u, v) ∈ M1 [i] if and only if (c(u), c(v)) ∈ M2 [i].
The execution of several types of actions leads to the creation of “duplicates” of an original Kripke structure—e.g., to
represent how different groups of agents maintain different perspectives after the execution of an action.
Definition 11 (Replica) Given a Kripke structure M , a c-replica of M is a Kripke structure M 0 such that c is a
c
bijection between M [S] and M 0 [S], M [S] ∩ M 0 [S] = ∅, and M 0 ∼ M .
0 0
Given a state (M, s), a c-replica of (M, s) is a state (M , s ) where M 0 is a c-replica of M and s0 = c(s).
We say that M1 and M2 are compatible if for every s ∈ M1 [S] ∩ M2 [S] and every f ∈ F, M1 [π](s)(f ) =
M2 [π](s)(f ).
κ

Definition 12 (Knowledge Updates) For two compatible Kripke structures M1 and M2 , we define M1 ∪ M2 as the
Kripke structure, M 0 , where:
(i) M 0 [S] = M1 [S] ∪ M2 [S];
(ii) M 0 [π](s) = M1 [π](s) for s ∈ M1 [S] and M 0 [π](s) = M2 [π](s) for s ∈ M2 [S] \ M1 [S]; and
(iii) M 0 [i] = M1 [i] ∪ M2 [i].
For a pair of Kripke structures M1 and M2 such that M1 [S] ∩ M2 (S) = ∅, |M1 [S]| = |M2 (S)|, α ⊆ AG, and a
one-to-one function λ : M2 [S] → M1 [S], we define M1 ]λα M2 as the Kripke structure, M 0 , where:
(i) M 0 [S] = M1 [S] ∪ M2 [S];
(ii) M 0 [π](s) = M1 [π](s) for s ∈ M1 [S] and M 0 [π](s) = M2 [π](s) for s ∈ M2 [S]; and
(iii) for each i ∈ α, we have that M 0 [i] = M1 [i] ∪ M2 [i], and for each i ∈ AG \ α we have that
M 0 [i] = M1 [i] ∪ M2 [i] ∪ {(u, v) | u ∈ M2 [S], v ∈ M1 [S], (λ(u), v) ∈ M1 [i]}.
Given two states (M1 , s1 ) and (M2 , s2 ) such that M1 [S]∩M2 [S] = ∅, |M1 [S]| = |M2 (S)|, α ⊆ AG, and a one-to-one
function λ : M2 [S] → M1 [S]:
(M1 , s1 ) ]λα (M2 , s2 ) = (M1 ]λα M2 , s2 ).
κ

Intuitively, the operators ∪ and ]λα allow us to combine different Kripke structures representing the beliefs of different
κ

groups of agents, thereby creating a structure representing the beliefs of “all agents.” The ∪ operation effectively
unions two Kripke structures (that might have already some worlds in common). The ]λα also combines two Kripke
structures albeit in a more specific way. It relies on a function λ to create additional edges between the two structures
for agents in AG \ α.
5 Similar

to the notion of bisimulation in [20].

9

The language mA+: Syntax

3

In this paper, we consider multi-agent domains in which the agents are truthful and no false information may be
announced or observed. Furthermore, the underlying assumptions guiding the semantics of our language are the
rationality principle and the idea that beliefs of an agent are inertial. In other words, agents believe nothing which they
are not forced to believe, and the beliefs of an agent remain the same unless something causes them to change.
In this section and the next section, we introduce the language mA+ for describing actions and their effects
in multi-agent environment. The language builds over a signature hAG, F, Ai, where AG is a finite set of agent
identifiers, F is a set of fluents, and A is a set of actions. Each action in A is an action the agents in the domain are
capable of performing.
Similar to any action language developed for single-agent environments, mA+ consists of three components which
will be used in describing the actions and their effects, the initial state, and the query language (see, e.g., [7]). We will
next present each of these components. Before we do so, let us denote the multi-agent domain in Example 1 by D1 .
For this domain, we have that AG = {A, B, C}. The set of fluents F for this domain consists of:
• tail: the coin lies tails up (head is often used in place of ¬tail);
• has key(X): agent X has the key of the box;
• opened: the box is open; and
• looking(X): agent X is looking at the box.
The set of actions for D1 consists of:
• open(X): agent X opens the box;
• peek(X): agent X peeks into the box;
• signal(X, Y ): agent X signals agent Y (to look at the box);
• distract(X, Y ): agent X distracts agent Y (so that Y does not look at the box); and
• shout tail(X): agent X announces that the coin lies tail up.
where X, Y ∈ {A, B, C} and X 6= Y . We start with the description of actions and their effects.

3.1

Actions and effects

We envision three types of actions that an agent can perform: world-altering actions (also known as ontic actions),
sensing actions, and announcement actions. Intuitively,
• A world-altering action is used to explicitly modify certain properties of the world—e.g., the agent A opens the
box in Example 1, or the agent A distracts the agent B so that B does not look at the box (also in Example 1);
• A sensing action is used by an agent to refine its beliefs about the world, by making direct observations—e.g.,
an agent peeks into the box; the effect of the sensing action is to reduce the amount of uncertainty of the agent;
• An announcement action is used by an agent to affect the beliefs of the agents receiving the communication—we
operate under the assumption that agents receiving an announcement always believe what is being announced.
For the sake of simplicity, we assume that each action a ∈ A falls in exactly one of the three categories.6
In general, an action can be executed only under certain conditions, called its executability conditions. For example,
the statement “to open the box, an agent must have its key” in Example 1 describes the executability condition of the
action of opening a box. The first type of statements in mA+ is used to describe the executability conditions of actions
and is of the following form:
6 It

is easy to relax this condition, but it would make the presentation more tedious.

10

executable a if ψ

(1)

where a ∈ A and ψ is a belief formula. A statement of type (1) will be referred to as the executability condition of
a. ψ is referred as the precondition of a. For simplicity of the representation, we will assume that each action a is
associated with exactly one executability condition. When ψ = >, the statement will be omitted.
For a world-altering action a, such as the action of opening the box, we have statements of the following type that
express the change that may be caused by such actions:
a causes ` if ψ

(2)

where ` is a fluent literal and ψ is a belief formula. Intuitively, if the real state of the world and of the beliefs match
the condition described by ψ, then the real state of the world is affected by the change that makes the literal ` true after
the execution of a. When ψ = >, the part “ if ψ” will be omitted from (2). We also use
a causes Φ if ψ
where Φ is a set of fluent literals as a shorthand for the set {a causes ` if ψ | ` ∈ Φ}.
Sensing actions, such as the action of looking into the box, allow agents to learn about the value of a fluent in the
real state of the world (e.g., learn whether the coin lies head or tail up). We use statements of the following kind to
represent sensing actions:
a determines f

(3)

where f is a fluent. Statements of type (3) encode a sensing action a which enables the agent(s) to learn the value of
the fluent f . f is referred to as a sensed fluent of the sensing action a.
For actions such as an agent telling another agent that the coin lies heads up, we have statements of the following
kind, that express the change that may be caused by such actions:
a announces ϕ

(4)

where ϕ is a fluent formula. a is called an announcement action.
Let us illustrate the use of statements of type (1)-(4) to represent the actions of the domain D1 .
Example 3 The actions of domain D1 can be specified by the following statements:
executable
executable
executable
executable
executable

open(X) if has key(X), BX (has key)
peek(X) if opened, looking(X), BX (opened), BX (looking(X))
shout tail(X) if BX (tail), tail
signal(X, Y ) if looking(X), ¬looking(Y ), BX (¬looking(Y )), BX (looking(X))
distract(X, Y ) if looking(X), looking(Y ), BX (looking(X)), BX (looking(Y ))

open(X) causes opened
signal(X, Y ) causes looking(Y )
distract(X, Y ) causes ¬looking(Y )
peek(X) determines tail
shout tail(X) announces tail
where X and Y are different agents in {A, B, C}. The first five statements encode the executability conditions of the
five actions in the domain. The next three statements describe the effects of the three world-altering actions. peek(X)
is an example of a sensing action. Finally, shout tail(X) is an example of an announcement action.

11

3.2

Observability: observers, partial observers, and others

One of the key differences between single-agent and multi-agent domains lies in how the execution of an action
changes the beliefs of agents. This is because, in multi-agent domains, an agent can be oblivious about the occurrence
of an action or unable to observe the effect of an action. For example, watching another agent open the box allows the
agent to know that the box is open after the execution of the action; however, the agent would still believe that the box
is closed if she is not aware of the action occurrence. On the other hand, watching another agent peeking into the box
does not help the observer in learning whether the coin lies heads or tails up; the only thing she would learn is that the
agent who is peeking into the box has knowledge of the status of the coin.
mA+ needs to have a component for representing the fact that not all the agents may be completely aware of the
presence of actions being executed. Depending on the action and the current situation, we can categorize agents in
three classes:
• Full observers,
• Partial observers, and
• Oblivious (or others).
This categorization is dynamic: changes in the state of the world may lead to changes to the agent’s category w.r.t.
each action. In this paper, we will consider the possible observability of agents for different action types as detailed in
Table 1.
action type
world-altering actions
sensing actions
announcement actions

full observers
√
√
√

partial observers
√
√

oblivious/others
√
√
√

Table 1: Action types and agent observability

The first row indicates that, for a world-altering action, an agent can either be a full observer, i.e., completely aware
of the occurrence of that action, or oblivious of the occurrence of the action. In the second case, the observability of
the agent is categorized as other. Note that we assume that the observer agents know about each others’ status and
they are also aware of the fact that the other agents are oblivious. The oblivious agents have no clue of anything.
For a sensing action, an agent can either be a full observer—i.e., the agent is aware of the occurrence of that action
and of its results—a partial observer—i.e., gaining knowledge that the full observers have performed a sensing action
but without knowledge of the result of the observation—or oblivious of the occurrence of the action (i.e., other). Once
again, we assume that the observer agents know about each others’ status and they also know about the agents partially
observing the action and about the agents that are oblivious. The partially observing agents know about each others’
status, and they also know about the observing agents and the agents that are oblivious. The oblivious agents have no
clue of anything. The behavior is analogous for the case of announcement actions.
The dynamic nature of the agents observability can be manipulated and this is specified using agent observability
statements of the following form:7
X observes a if ϕ

(5)

X aware of a if ψ

(6)

where X ∈ AG is the name of an agent, a ∈ A, and ϕ and ψ are fluent formulae. Statements of type (5) indicate that
X is a full observer of the execution of a if ϕ holds. Statements of type (6) state that X is a partial observer of the
execution of a if ψ holds. X, a, and ϕ (resp. ψ) are referred to as the agent, the action, and the condition of (5) (resp.
(6)).
7 As

discussed earlier, the “ if >” are omitted from the statements.

12

In the following, we will assume that, for every agent X ∈ AG and for every action a ∈ A, if X occurs in a
statement of the form (5), then it will not occur in a statement of the form (6) such that ϕ ∧ ψ is consistent and vice
versa.
Definition 13 An mA+ domain is a collection of statements of the forms (1)-(6).

3.3

More Examples of mA+ Domains

In this section, we illustrate the use of mA+ in representing multi-agent domains. We start by completing the specification of the domain D1 .
Example 4 (Observability in D1 ) The actions of D1 are described in Example 3. The observability of agents in D1
can be described by the set O1 of statements
X observes
Y observes
Y observes
X observes
Y observes

open(X)
open(X) if looking(Y )
shout tail(X)
distract(X, Y )
distract(X, Y )

X observes
Y aware of
X observes
X observes
Y observes

peek(X)
peek(X) if looking(Y )
shout tail(X)
signal(X, Y )
signal(X, Y )

where X and Y denote different agents in {A, B, C}. The above statements say that agent X is a fully observant agent
when open(X), peek(X), distract(X, Y ), signal(X, Y ), or shout tail(X) are executed; Y is a fully observant
agent if it is looking (at the box) when open(X) is executed. Y is a partially observant agent if it is looking when
peek(X) is executed. An agent different from X or Y is oblivious in all cases.
The next example represents a domain in which the agent who executes an action might not be a full observer of the
action occurrence.
Example 5 Let us consider a domain with two agents A = {A, B}. The two agents are operating in a room; agent A
is blind while B is not. Both agents are aware that by flipping a switch it is possible to change the status of the light
in the room, and both agents can perform such action. On the other hand, the effect of the execution of the action will
be visible only to B. This action can be described by the following statements of mA+:
flip causes on if ¬on
flip causes ¬on if on
B observes flip
We will next describe several examples that contain common actions that are typical to multi-agent domains in mA+.
We refer to these actions as reference setting actions, such as the action of distracting another agent. These types of
actions are interesting, because it is often necessary for agents to execute them in order to allow subsequent actions to
achieve their intended effects (e.g., sharing a secret).
Example 6 (Distraction) Agent A wishes to access some files on C’s computer without C’s knowledge. Agent C is
rather observant therefore, in order for A to succeed, A will need to first cause a distraction (such as pulling the fire
alarm) in order to pull C’s attention away from the computer. Once C is distracted, A may access the file on the
computer. This can be described by the following statements:
distract(A, C) causes distracted(C)
A observes distract(A, C)
C observes distract(A, C)
executable accessFile(A, C) if distracted(C)
The action that helps an agent to form or dissolve a group is also frequently needed in multi-agent domains. Groups
enable, for example, the execution of communications that are only local to the group (e.g., a secret conversation).
13

Example 7 (Group Formation/Dissolution and Secret Communication) Continuing with Example 6, now that A
has access to the file, she needs the assistance of agent B to learn its contents because the file is encrypted and the
expertise of B is required for decryption. In order to read the file, A must first establish a connection with B—agent
A must open/invite a communications channel to B. Let linked(X, Y ) denote that X and Y are connected and
distracted(Z) denote that Z is distracted. This action of A inviting B to connect via some communication channel
can be represented using the action openChannel(A, B) with the following following specification:
openChannel(A, B) causes linked(A, B)
A observes openChannel(A, B)
B observes openChannel(A, B)
C observes openChannel(A, B) if ¬distracted(C)
executable openChannel(A, B) if ¬linked(A, B)
Once a channel has been opened, agents A and B are linked and they may together read the file. Once they have read
the file, they disconnect the channel in order to leave no trace of their activity. This action can be represented using
the action closeChannel(A, B) with the following specification:
closeChannel(A, B) causes ¬linked(A, B)
A observes closeChannel(A, B)
B observes closeChannel(A, B)
C observes closeChannel(A, B) if ¬distracted(C)
executable closeChannel(A, B) if linked(A, B)
Reading the file allows A and B to understand its content. Let us assume that the file indicates whether tomorrow is
the date set for a cyber-attack against A’s organization. This can be represented as follows.
readFile(A) determines attackDate(tomorrow)
A observes readFile(A)
B observes readFile(A) if linked(A, B)
C aware of readFile(A) if ¬distracted(C)
executable readFile(A) if linked(A, B)
If a channel is open, it can be used to share the knowledge of an impeding attack. However, the communication is
secure only if the third party is distracted. This action is an announcement action and can be represented using the
following statements.
warnOfAttack(A, B) announces attackDate(tomorrow)
A observes warnOfAttack(A, B)
B observes warnOfAttack(A, B)
C observes warnOfAttack(A, B) if ¬distracted(C)
executable warnOfAttack(A, B) if linked(A, B) ∧ attackDate(tomorrow)
A more general version of the actions of secretly creating/dissolving a group is given in the next example.
Example 8 Consider an agent X joining an agent Y to gain visibility of everything that the agent X does; this can
be modeled by the action join(Y, X), where Y joins X at the same level of visibility of X’s actions:
join(Y, X) causes group member(Y, group(X))
X observes join(Y, X)
Y observes join(Y, X)
This could be refined by adding the need to be invited to join X:
executable join(Y, X) if invited(Y, X)
The effect of gaining visibility of the actions of X can be described by
14

Y observes aX if group member(Y, group(X))
where aX is any action that is executed by agent X. The symmetrical operation is the operation of leaving the group,
leading the agent Y to completely loose visibility of what agent X is doing:
leave(Y, X) causes ¬group member(Y, group(X))
X observes leave(Y, X)
Y observes leave(Y, X)
The agent Y may also decide to take the action of separating from the group, through the action separate(Y, X), where
the agent Y will observe X from “a distance”, with the consequent loss of the intimate knowledge of X actions’ effects:
separate(Y, X) causes ¬group member(Y, group(X)) ∧ group observer(Y, group(X))
X observes separate(Y, X)
Y observes separate(Y, X)
Y aware of aX if group observer(Y, group(X))
Distracting an agent is not only necessary for secure communication, it is also important for certain world-altering
actions to achieve their intended effects, as in Example 6. Another example that highlights the importance of this type
of actions can be seen next.
Example 9 Agent D is a prisoner, having been captured by C. Agent A is in charge of rescuing agent D. In order to
do so, he must first distract C, and then trigger a release on C’s computer. Once the release has been triggered, D
may escape. The distract(A, C) action has already been presented in Example 6. Let us consider the other actions.
Rescuing an agent means that the rescued agent is released. We use the following statements:
rescue(A, D) causes released(D)
A observes rescue(A, D)
D observes rescue(A, D)
C observes rescue(A, D) if ¬distracted(C)
executable rescue(A, D) if distracted(C)
The action of escaping can have different effects.
escape(D) causes dead(D) if ¬distracted(C)
escape(D) causes free(D) if distracted(C)
A observes escape(D)
D observes escape(D)
C observes escape(D) if ¬distracted(C)

3.4

Initial State

A domain specification encodes the actions and their effects and the observability of agents in each situation. The
initial state, that encodes both the initial state of the world and the initial beliefs of the agents, is specified in mA+
using initial statements of the following form:
initially ϕ

(7)

where ϕ is a formula. Intuitively, this statement says that ϕ is true in the initial state. We will later discuss restrictions
on the formula ϕ to ensure the computability of the Kripke structures describing the initial state.

15

Example 10 (Representing Initial State of D1 ) Let us reconsider Example 1. The initial state of D1 can be expressed by the following statements:
initially
initially
initially
initially
initially
initially

C(has key(A))
C(¬has key(B))
C(¬has key(C))
C(¬opened)
C(¬BX tail ∧ ¬BX ¬tail) for X ∈ {A, B, C}
C(looking(X))
for X ∈ {A, B, C}

These statements indicate that everyone knows that A has the key and B and C do not have the key, the box is closed,
no one knows whether the coin lies head or tail up, and everyone is looking at the box.
The notion of an action theory in mA+ is defined next.
Definition 14 (Action Theory) A mA+-action theory is a pair (I, D) where D is a mA+ domain and I is a set of
initially statements.

4

Transition Function for mA+ Domains

A mA+ domain D specifies a transition system, whose nodes are “states” that encode the description of the state of
the world and of the agents’ beliefs. This transition system will be described by a transition function ΦD , which maps
pairs of actions and states to states. For simplicity of the presentation, we assume that only one action is executed at
each point in time—it is relatively simple to extend it to cases where concurrent actions are present, and this is left as
future work. As we have mentioned in Section 2, we will use pointed Kripke structures to represent states in mA+
action theories. A pointed Kripke structure encodes three components:
• The actual world;
• The state of beliefs of each agent about the real state of the world; and
• The state of beliefs of each agent about the beliefs of other agents.
These components are affected by the execution of actions. Observe that the notion of a state in mA+ action theories
is more complex than the notion of state used in single-agent domains (i.e., a complete set of fluent literals).
Let us consider a state (M, s). We say that an action a ∈ A is executable in (M, s) if the executability condition of
a, given by a statement of the type (1) in D, is satisfied by (M, s), i.e., (M, s) |= ψ. The effect of executing an action
a in (M, s) is, in general, a set of possible states denoted by ΦD (a, (M, s)). Since each type of action will impact
(M, s) in a different manner, we define
 w
if a is a world-altering action
 ΦD (a, (M, s))
if a is a sensing action
ΦD (a, (M, s)) = ΦsD (a, (M, s))
 a
ΦD (a, (M, s))
if a is an announcement action
In other words, we will define ΦD for each type of actions separately. As we have mentioned earlier, the occurrence
of an action can change the state of the world and/or the state of the agents’ beliefs. The change in an agent’s beliefs
depends on the degree of awareness of the agent about the action occurrence, which in turn depends on the current
state. Therefore, we need to first introduce the notion of a frame of reference in order to define the function ΦD .

4.1

Actions Visibility

Given a state (M, s) and an action a ∈ A, let us define
FD (a, M, s) = {X ∈ AG | [X observes a if ϕ] ∈ D such that (M, s) |= ϕ}
PD (a, M, s) = {X ∈ AG | [X aware of a if ϕ] ∈ D such that (M, s) |= ϕ}
OD (a, M, s) = AG \ (FD (a, M, s) ∪ PD (a, M, s))
16

We will refer to the tuple (FD (a, M, s), PD (a, M, s), OD (a, M, s)) as the frame of reference for the execution of a
in (M, s). Intuitively, FD (a, M, s) (resp. PD (a, M, s) and OD (a, M, s)) are the agents that are fully observant (resp.
partially observant and oblivious/other) of the execution of action a in the pointed Kripke structure (M, s). For the
sake of simplicity, we will assume that the set of agent observability statements in D is consistent, in the sense that for
each pair of an action a ∈ A and a pointed Kripke structure (M, s), the sets FD (a, M, s) and PD (a, M, s) are disjoint.
Thus, the domain specification D and the pointed Kripke structure (M, s) determine a unique frame of reference for
each action occurrence.
The introduction of frames of reference allows us to elegantly model several types of actions that are aimed at
modifying the frame of reference (referred to as reference setting actions). Some possibilities are illustrated in the
following examples.
Example 11 (Reference Setting Actions) Example 4 shows two reference setting actions: signal(X, Y ) and
distract(X, Y ). The action signal(X, Y ) allows agent X to promote agent Y into a higher level of observation
for the effect of the action peek(X). On the other hand, the action distract(X, Y ) allows agent X to demote agent Y
into a lower level of observation. The net effect of executing these actions is a change of frame.
Let us consider the action signal(X, Y ) and a state (M, s). Furthermore, let us assume that (M 0 , s0 ) is a state
resulting from the execution of signal(X, Y ) in (M, s). The frames of reference for the execution of the action a =
peek(X) in these two states are detailed in Figure 2.
signal(X,Y )

in (M, s) −−−−−−−→
in (M 0 , s0 )
FD (a, M, s)
FD (a, M, s)
PD (a, M, s)
PD (a, M, s) ∪ {Y }
OD (a, M, s)
OD (a, M, s) \ {Y }
Figure 2: Frame of reference for peek(X) before (in (M, s)) and after signal(X, Y ) (in (M 0 , s0 ))
Intuitively, after the execution of signal(X, Y ), looking(Y ) becomes true because of the statement
signal(X, Y ) causes looking(Y )
in D1 . By definition, the statement
Y aware of peek(X) if looking(Y )
indicates that Y is partially observant.
Similar argument shows that distract(X, Y ) demotes Y to the lowest level of visibility, i.e., it will cause agent
Y to become oblivious of the successive peek(X) action. Again, for a = peek(X), we have the change of frame of
reference summarized in Figure 3.
distract(X,Y )

in (M, s) −−−−−−−−→
in (M 0 , s0 )
FD (a, M, s)
FD (a, M, s) \ {Y }
PD (a, M, s)
PD (a, M, s) \ {Y }
OD (a, M, s)
OD (a, M, s) ∪ {Y }
Figure 3: Frame of reference for peek(X) before (in (M, s)) and after distract(X, Y ) (in (M 0 , s0 ))

Reference setting actions can be used in modeling group formation activities.
Example 12 Consider the join and leave actions from Example 8. The frame setting changes of these actions can be
summarized as in Figure 4, where a(X) denotes an arbitrary action of X and (M 0 , s0 ) denotes the state resulting from
the execution of the corresponding action in (M, s).

17

in (M, s)
FD (a(X), M, s)
PD (a(X), M, s)
OD (a(X), M, s)

join(Y,X)

−−−−−−→

in (M 0 , s0 )
FD (a(X), M, s) ∪ {Y }
PD (a(X), M, s) \ {Y }
OD (a(X), M, s) \ {Y }

leave(Y,X)

in (M, s)
−−−−−−→
in (M 0 , s0 )
FD (a(X), M, s)
FD (a(X), M, s) \ {Y }
PD (a(X), M, s)
PD (a(X), M, s)
OD (a(X), M, s)
OD (a(X), M, s) ∪ {Y }
separate(Y,X)

in (M, s)
−−−−−−−−→
in (M 0 , s0 )
FD (a(X), M, s)
FD (a(X), M, s) \ {Y }
PD (a(X), M, s)
PD (a(X), M, s) ∪ {Y }
OD (a(X), M, s)
OD (a(X), M, s)
Figure 4: Frame of reference for a(X) before (in (M, s)) and after the execution of a joint/leave/separate action (in
(M 0 , s0 ))

4.2

World-Altering Actions: Φw
D

Intuitively, the execution of a world-altering (a.k.a. ontic) action will take place in the actual world—thus, it will affect
the interpretation associated to the world s of the state (M, s) in which the action is executed. The definition of Φw
D
must describe this change as well as the change in beliefs of agents with different degrees of awareness. Recall that for
a world-altering action a, the frame of reference for its execution in a state (M, s) consists of two elements: the fully
observers (FD (a, M, s)) and the oblivious ones (OD (a, M, s)). Intuitively, an agent in FD (a, M, s) should believe
that the effects of a are true in the successor state. In addition, she should also be able to remove from consideration
those worlds in which the action is non-executable. On the other hand, an agent in OD (a, M, s) will not be able to
observe the effects of the execution of a and, hence, will believe that there are no changes, i.e., its accessible possible
worlds remain unchanged. We can conclude that the successor state Φw (a, (M, s)) will contain two substructures, one
for each group of agents.
Let us construct the structure for the group of agents FD (a, M, s). As each agent in this group is fully aware of the
effects of a, the possible worlds that are present after the occurrence of a will be those resulting from the execution of
a in the possible worlds belonging to M . To give a precise definition, we need the following notations.
For a fluent literal `, let ` denote its complement, i.e., for f ∈ F, f = ¬f and ¬f = f . For a set of literals S,
S = {` | ` ∈ S}. For a world-altering action a, the effects of the action in the Kripke structure M w.r.t. the world
u ∈ M [S] are
eD (a, M, u) = {` | [a causes ψ if ϕ] ∈ D such that (M, u) |= ϕ, ` ∈ ψ}
In the following, we will assume that for every world-altering action a and state (M, s), eD (a, M, u) is a consistent
set of literals for every u ∈ M [s] (i.e., the domain specification is consistent). It is easy to see that, given a, M and u,
eD (a, M, u) is unique.
An agent observing the execution of the action will learn the outcomes of the execution; nevertheless, the agents
have uncertainty about the actual world—which implies that the agent may have to maintain such uncertainty after the
execution of the action, by applying the effects of the action in each world that is considered possible by the agent.
This leads us to the following construction of the substructure for the agents in FD (a, M, s). Let us denote with
Res(a, M, s) the Kripke structure defined as follows
1. Res(a, M, s)[S] = {r(a, u) | u ∈ M [S], a is executable in (M, u)} where each r(a, u) is a new world;
2. Res(a, M, s)[π](r(a, u)) = M [π](u) \ eD (a, M, u) ∪ eD (a, M, u);
3. (r(a, u), r(a, v)) ∈ Res(a, M, s)[i] iff
◦ (u, v) ∈ M [i]
18

◦ {r(a, u), r(a, v)} ⊆ Res(a, M, s)[S]
◦ i ∈ FD (a, M, s)
Intuitively, this is a copy of the original Kripke structure M , where:
• The worlds receive new names (of the form r(a, u)) and are kept only if a is executable in them (item 1),
• Only the edges labeled by fully observant agents are kept (item 3), and
• The interpretations attached to the worlds are updated to reflect the consequences of action a (item 2).
Observe that this construction could be optimized—e.g., the execution of the action might completely remove the
uncertainty of agents, thus removing the need for multiple worlds.
Since the agents in O(a, M, s) are not aware of the action occurrence and its effects, the structure encoding their
beliefs is exactly the original structure. So, the successor state Φw
D (a, (M, s)) is obtained by combining Res(a, M, s)
and (M, s). This combination should maintain the beliefs of agents in O(a, M, s). This is achieved by creating edges
of the form (r(a, u), i, v) for each (u, v) ∈ M [i] and i ∈ O(a, M, s). The complete definition of Φw
D (a, (M, s)) is as
follows.
Definition 15 (Step Transition for World-Altering Actions) Let a be a world-altering action and (M, s) be a state.
Then

a is not executable in (M, s)
∅
Φw
(8)
D (a, (M, s)) =

{(M 0 , s0 )} a is executable in (M, s)
where
◦ s0 = r(a, s)
◦ M 0 [S] = M [S] ∪ Res(a, M, s)[S]
◦ M 0 [π](u) = M [π](u) for each u ∈ M [S] and M 0 [π](u) = Res(a, M, s)[π](u) for each u ∈ Res(a, M, s)[S]
◦ M 0 [i] = M [i] ∪ Res(a, M, s)[i] ∪ Link(a, M, s, i), where:
– for each i ∈ FD (a, M, s) we have that Link(a, M, s, i) = ∅;
– for each i ∈ OD (a, M, s) we have that
Link(a, M, s, i) = {(r(a, u), v) | r(a, u) ∈ Res(a, M, s)[S], u, v ∈ M [S], (u, v) ∈ M [i]}.
We illustrate the definition with some simple examples. First, let us continue with the example presented in the
introduction.
Example 13 The action open is a world-altering action. The observability of agents with respect to its execution is
given in Example 4. If we assume that agent A opens the box when agent B is looking at it while C is not, then
the transformation of a state is shown in Figure 5 (for the sake of readability and the simplicity of the graph, we
omit the other fluents, such as has key(X), in the world representations). The frame of reference for open(A) is
FD (open(A), M, s) = {A, B} and OD (open(A), M, s) = {C}. Note that the two worlds in the upper row represent
the same configuration of beliefs as the starting situation (due to the fact that agent C is oblivious of any change),
while the two worlds in the bottom row reflect the new perspective of the agents A and B—in particular, in the actual
world, the box is now opened.
The next example shows the effect of the execution of the action f lip from Example 5.

19

A,B,C

A,B,C

A,B,C

¬opened
looking(B)
¬tail

¬opened
looking(B)
tail

A,B,C

A,B,C

A,B,C

¬opened
looking(B)
¬tail

C

C

opened
looking(B)
¬tail

¬opened
looking(B)
tail

C

C
A,B

opened
looking(B)
tail

A,B

A,B

Figure 5: Execution of the action open
Example 14 Let us consider the state (M, s1 ) shown on top in Figure 6. More precisely, we have that M [S] =
{s1 , s2 }, M [π](s1 )(on) = ⊥, M [π](s2 )(on) = >, and M [A] = {(s1 , s1 ), (s1 , s2 ), (s2 , s1 ), (s2 , s2 )} and M [B] =
{(s1 , s1 ), (s2 , s2 )}. Observe that (M, s1 ) identifies a state where the light is off, but agent A is unaware of that (while
agent B is). The bottom part of Figure 6 illustrates the resulting state after the execution of flip in (M, s1 ).
The top part of the resulting state (worlds s1 and s2 ) represent an exact copy of the original state—this reflects
the view of agent A, which is unaware of the effects of the action flip being executed. The bottom part reflects instead
the view that agents B has after executing the action, including the fact that the new state of the world (denoted by
r(f lip, s1 )) has a different interpretation w.r.t. s1 . The arrows labeled A leaving the state r(f lip, s1 ) indicate that for
agent A nothing has changed (since they point back to the original copy of the Kripke structure).
We can prove the following lemma.
Lemma 1 Given a consistent domain D, a state (M, s), and a world-altering action a executable in (M, s), we have
that Φw
D (a, (M, s)) is a singleton.
Proof. This result follows trivially from the construction of (M 0 , s0 ), the fact that Res(a, M, s) is unique, and that for
each u ∈ Res(a, M, s)[S], the interpretation of r(a, u) is also unique.
2
The next proposition shows that the execution of a world-altering action changes the beliefs of agents according to
their classification.
Proposition 2 Let D be a consistent domain, (M, s) be a state, a be a world-altering action that is executable in
0 0
(M, s) and Φw
D (a, (M, s)) = {(M , s )}. The following holds:
20

S1:
¬on

S2:
on

A

A, B

A, B

S1:
¬on

S2:
on

A
A

A

A, B

A

A, B

r(flip,S1):

r(flip,S2):

on

¬on

B

A

B

Figure 6: Execution of the action flip
1. For every world r(a, u) ∈ M 0 [S] \ M [S] and for every literal ` ∈ eD (a, M, u), we have that (M 0 , r(a, u)) |= `;
2. For every world r(a, u) ∈ M 0 [S] \ M [S] and for every literal ` ∈ F \ {`, `¯ | ` ∈ eD (a, M, u)}, we have that
(M, u) |= ` iff (M 0 , r(a, u)) |= `;
3. For every i ∈ OD (a, M, s) and formula ϕ, (M 0 , s0 ) |= Bi ϕ iff (M, s) |= Bi ϕ.
2

Proof. See Appendix.

The first case of the proposition indicates that the effects of the action are happening in each world where the
action is executable (and visible). The second case indicates that inertia is in effect on those worlds where the action
is performed (and visible). Finally, the last case indicates that no changes are observed by the other agents.

4.3

Sensing Actions: ΦsD

Sensing actions differ from world-altering actions in that they do not change the actual state of the world. Rather, they
change the beliefs of the agents who can observe the effects of the actions. Let us consider the action peek(A) in the
domain of Example 1. If A were to peek into the box then A should know whether the coin lies head or tail up. Let
us consider instead B, who is looking at the box (or at A) when A is peeking into the box. We expect that B should
know that A knows whether the coin lies head or tail up. However, B’s uncertainty about whether or not the coin lies
head up still remains. Now consider C, who is not looking at the box (or at A) when A is peeking into the box. We
expect that C will not see any changes in its beliefs about either A or B; C will also not modify its beliefs about the
status of the coin.
In general, for a sensing action a such that
a determines f
belongs to D, the execution of a will have different effects on different groups of agents:
• Each agent i ∈ FD (a, M, s) will have no uncertainty about the truth value of f , i.e., Bi f ∨ Bi ¬f is true in the
successor state;

21

• Each agent i ∈ PD (a, M, s) will be aware that the agents in FD (a, M, s) know the truth value of f , but i itself
will not know such value; that is Bi (Bj f ∨ Bj ¬f ) is true in the successor state for j ∈ FD (a, M, s);
• Each agent i ∈ OD (a, M, s) will not change its beliefs.
Given a sensing action a, the set of fluents being sensed by a is defined as follows.
SensedD (a) = {f | [a determines f ] ∈ D}.
Let us proceed with the development of the step transition function for sensing actions. Similarly to the construction
of the successor state for world-altering actions, we can see that the new state will consist of two substructures. The
first structure encodes the beliefs of agents whose beliefs change after the execution of a. The second one represents
agents who are oblivious. Since sensing actions do not change the world, the first structure can be obtained from the
original structure M by:
(i) Removing all worlds in M such that the executability condition of a is not satisfied with respect to them, i.e.,
the set
Remstates
(M ) = {u | (M, u) 6|= ϕ}
a
where ϕ is the executability condition of a;
(ii) Removing all links related to oblivious agents (OD (a, M, s)); and
(iii) Removing all links of the form (u, i, v) where i ∈ FD (a, M, s) such that the interpretation of some f ∈
SensedD (a) in u and v is different, i.e., the set
Remlinks
(M ) = {(u, i, v) | i ∈ FD (a, M, s), f ∈ SensedD (a), (u, v) ∈ M [i], M [π](u)(f ) 6= M [π](v)(f )}.
a
Using the operations on Kripke structures (Subsection 2.3), we can formulate this process as follows:
• Take a replica (M r , c(s)) of (M, s) and restrict the structure to contain only information related to agents in
s

(M r ), c(s)) |FD (a,M,s)∪PD (a,M,s) ;
FD (a, M, s) ∪ PD (a, M, s) which is (M r 	 Remstates
a
• Remove all links related to the uncertainty about fluents in SensedD (a) for agents in FD (a, M, s) from this
a

s

(M r ), c(s)); and
(M r )) |FD (a,M,s)∪PD (a,M,s) 	Remlinks
structure which is ((M r 	 Remstates
a
a
• Link the states in the replica to the states of the original Kripke structure via the links of agents in OD (a, M, s)
−1
which is represented by the operator ]cFD (a,M,s)∪PD (a,M,s) on the two structures.
This leads us to the following definition of ΦsD .
Definition 16 (Step Transition for Sensing Actions) Let a be a sensing action and (M, s) be a state. Then,

∅
a is not executable in (M, s)
s
ΦD (a, (M, s)) =
{(M 0 , s0 )} a is executable in (M, s)
where (M 0 , s0 ) is given by
−1

(M 0 , s0 ) = (M, s) ]cFD (a,M,s)∪PD (a,M,s) (M 00 , s00 )
with
s

a

• (M 00 , s00 ) = ((M r 	 Remstates
(M r )) |FD (a,M,s)∪PD (a,M,s) 	Remlinks
(M r ), c(s)) and
a
a
• (M r , c(s)) is a c-replica of (M, s).
We illustrate the definition in the next example.
22

(9)

S1:
¬tail

S2:
tail

A,B,C

A,B,C

A,B,C

S1:
¬tail
C

C

S2:
tail

A,B,C

Agent C

C

A,B,C

A,B,C

S3:

B

¬tail

C

S4:
tail

Agent A
A,B

A,B

Agent B

Figure 7: Execution of the sensing action peek(A)
Example 15 Let us consider a slight modification of the scenario used in Example 1. Assume that it is common belief
that the box is open and C is not looking at the box while A and B both look at the box. Assume that A peeks into the
box. Starting from the state in Figure 1 (we omit all other fluent literals from the world representations, in this case:
opened, has key(A), ¬has key(B), ¬has key(C), looking(A), looking(B), and ¬looking(C)), the execution of
the action peek(A) will lead to the state in Figure 7.
We can recognize three components in this resulting state:
• Agent A (see the bottom-left box in Figure 7, the green box) has exact belief of the property ¬tail being true,
without any uncertainty—i.e., there are no possible worlds accessible by A where tail is true.
• Agent B operates on the two possible worlds in the last row of Figure 7 (the bottom/blue box); the agent itself
does not know whether tail is true or false, but the agent knows that A is aware of the value of tail, i.e., the agent
B sees the formula BA tail ∨ BA ¬tail as valid.
• Agent C operates on the two possible worlds in the middle row of Figure 7 (the middle/red box), denoting
complete ignorance about tail as well as about the other agents’ beliefs about tail.
Observe that the bottom structure of the successor state (blue area) is obtained from a copy (M r , c(s)) of the
r
original structure (top), by removing Remstates
peek(A) (M ) = ∅, restricting the links to those labeled by agents in the set
{A, B}, and removing the set of links labeled A that connect worlds with different interpretation of the fluent tail, i.e.,
r
Remlinks
peek(A) (M ) = {(s3 , A, s4 ), (s4 , A, s3 )}.
Having defined the step transition function for sensing actions, let us focus on its properties. The first aspect we want
to mention is the need to understand the restriction imposed in the introduction—i.e., we intend to use sensing actions
to remove uncertainty but not to correct wrong beliefs. The next example shows that even a fully observant agent with
a wrong belief could become “ignorant” after the execution of a sensing action.
Example 16 Consider a simple action a that is always executable and determines the fluent f and an agent A who is
fully observant of the execution of a in the state given in the left of Figure 8.
23

S0:

A

S1:

S0:

¬f

f

f

S1:

¬f

A

A

Figure 8: Execution of a sensing action by agents with false beliefs
It is easy to see that the execution of a results in the state on the right of Figure 8. Observe that A becomes ignorant
in s0 , the actual world, after the execution of a. This is because A has the wrong belief about the fluent f in the state
in which a is executed and the execution of a does not update her belief.
The example shows that in general, the execution of a sensing action in a multi-agent environment might have different
effects on the agent’s knowledge and beliefs. Taking the difference between knowledge and belief in formalizing
actions in multi-agent environment is a challenging issue that will need to be addressed. We have taken the first
step into to address this issue in a separate work [30]. We observe that under a certain condition, called consistency
preservation, the execution of a sensing action will not result in agents being completely ignorant as in Example 16.
Definition 17 A state (M, s) is consistency preserving for a sensing action a if (M, u) 6|= Bi f ∨ Bi ¬f for every
u ∈ M [S], i ∈ FD (a, M, s) ∪ PD (a, M, s), and f ∈ SensedD (a).
We will next present a proposition which is similar to Proposition 2 for sensing action which states that after the
execution of a sensing action in a consistency preserving state, fully observant agents have no uncertainty about the
sensed fluents, partially observant agents are aware that the beliefs of fully observant agents are changed, and other
agents’ beliefs do not change.
Proposition 3 Let D be a consistent domain, a be a sensing action, and (M, s) be a state. Suppose that a is executable
in (M, s) and (M, s) is consistency preserving for a. Assume that (M 0 , s0 ) ∈ ΦsD (a, (M, s)). Then, the following
holds
1. for every f ∈ SensedD (a) and ` ∈ {f, ¬f }, if (M, s) |= ` then (M 0 , s0 ) |= CFD (a,M,s) `;
2. for every f ∈ SensedD (a), (M 0 , s0 ) |= CPD (a,M,s) (CFD (a,M,s) f ∨ CFD (a,M,s) ¬f ); and
3. for every i ∈ OD (a, M, s) and formula ψ, (M 0 , s0 ) |= Bi ψ iff (M, s) |= Bi ψ.
2

Proof. See Appendix.

4.4

Announcement Actions: ΦaD

The announcement actions are used to communicate a piece of information to a group of agents. In this paper, we
make the following assumptions:
• The announcement is truthful—i.e., the properties being announced are true in the real state of the world; thus,
if the announcement of ϕ is made in the state (M, s), we expect M [π](s) |= ϕ.
• The agents receiving the announcement believe the property being announced.
• The announcement action occurs in at most one statement of the form (4).
• The execution of an announcement action occurs only in states in which fully observant agents do not have
wrong beliefs about the announced formula (this is similar to the requirement for the execution of a sensing
action).
24

Similar to sensing actions, an announcement action will not change the world. Agents who are aware of the action
occurrence, will be able to reason that the current state of the world must satisfy the executability condition of the
action. Furthermore, an occurrence of an announcement action only changes the beliefs of agents who are aware or
partially aware of the effects of the action. Finally,
• Agents who are fully observant will know the truth value of the formula being announced;
• Agents who are partially observant will not know the truth value of the formula being announced, but they will
know that fully observant agents are knowledgeable about it;
• Oblivious agents will have the same beliefs as before the execution of the action.
The construction of the successor state ΦaD (a, (M, s)) for a state (M, s) and announcement action a is very similar to
the one employed for the sensing actions.
Definition 18 (Step Transition for Announcements) Let a be an announcement action with
a announces ψ
in D and the executability condition ϕ and (M, s) be a state. Then,

∅
a is not executable in (M, s) or (M, s) 6|= ϕ
ΦaD (a, (M, s)) =
{(M 0 , s0 )} a is executable in (M, s) and (M, s) |= ϕ

(10)

where (M 0 , s0 ) is given by
−1

(M 0 , s0 ) = (M, s) ]cFD (a,M,s)∪PD (a,M,s) (M 00 , s00 )
with
• (M r , c(s)) is a c-replica of (M, s);
• Remstates
(M r ) = {c(u) | u ∈ M [S], (M, u) 6|= ϕ}
a


i ∈ FD (a, M, s), (u, v) ∈ M r [i], ( M r [π](u) |= ψ and M r [π](v) |= ¬ψ )
links
• Rema (M ) = (u, i, v)
or ( M r [π](u) |= ¬ψ and M r [π](v) |= ψ )
s

a

• (M 00 , s00 ) = ((M r 	 Remstates
(M r )) |FD (a,M,s)∪PD (a,M,s) 	Remlinks
(M ), c(s))
a
a
In the next example, we illustrate the definition with the execution of a public announcement action.
Example 17 Let consider the initial situation described in Figure 9 along with the action shout tail(A) from Example 3, whose observability is given in Example 4. Intuitively, an announcement of the truth of tail is made to all agents.
This is an example of a “public announcement,” after which all agents are aware of the truth value of the announced
formula (tail). This results in the successor state as in Figure 9. Observe that the bottom row looses one of the states
as the executability condition does not hold.
The next example considers a private announcement action.
Example 18 Let us assume that A and B have agreed to a scheme of informing each other if the coin lies heads up
by raising an hand. B can only observe A if B is looking at the box (or looking at A). C is completely ignorant about
the meaning of A’s raising her hand. This can be modeled by the following statements:
executable raising hand(X) if BX (¬tail), ¬tail
raising hand(A) announces ¬tail
A observes raising hand(A) if true
B observes raising hand(A) if looking(B)
If A knows the coin lies heads up and raises her hand, B will be aware that the head is up and C is completely
ignorant about this. The execution of raising hand(A) in the state where A knows that the coin lies heads up and B
is looking is given in Figure 10.
25

S1:
tail

S2:
¬tail

B,C

A,B,C

A,B,C

S1:
tail

S2:
¬tail

B,C

A,B,C

No Oblivious
Agents

A,B,C

S3:
tail

No Partially
Observant Agents

Agents
A,B,C

A,B,C

Figure 9: Execution of the action shout tail(A)

S1:
¬tail

S2:
tail

B,C

A,B,C

A,B,C
AGENT C

S1:
¬tail

S2:
tail

B,C
C

C

A,B,C
S3:

A,B,C

AGENTS A,B

¬tail

A,B

Figure 10: Execution of the action raising hand(A)

26

The execution of an announcement action will have similar effects on agents’ beliefs if it is executed in a consistency
preserving structure. First, we extend Definition 17 for announcement actions as follows.
Definition 19 A state (M, s) is called consistency preserving for an announcement action a which announces ϕ if,
for every u ∈ M [S] and i ∈ FD (a, M, s) ∪ PD (a, M, s), we have that (M, u) 6|= Bi ¬ϕ.
The next proposition is similar to Proposition 3.
Proposition 4 Let D be a consistent mA+ action theory, (M, s) be a state, and
a announces ϕ
in D. Assume that (M, s) is consistency preserving for a, a is executable in (M, s), and ΦaD (a, (M, s)) = {(M 0 , s0 )}.
Then, the following holds
• (M 0 , s0 ) |= CFD (a,M,s) ϕ;
• (M 0 , s0 ) |= CPD (a,M,s) (CFD (a,M,s) ϕ ∨ CFD (a,M,s) ¬ϕ); and
• for every i ∈ OD (a, M, s) and formula ψ, (M 0 , s0 ) |= Bi ψ iff (M, s) |= Bi ψ.
Proof. Because of the assumption that announcement action is truthful, we have that a is executable only if (M, s) |=
ϕ. The proof of this proposition is then similar to that of Proposition 3 and is omitted for brevity.
2
Observe that our formalization does not distinguish between public announcement and private announcement,
as done in the several of the previously published works. This distinction is already captured by the observability
statements.

4.5

Entailment in mA+ Action Theories

We are now ready to define the notion of entailment in mA+ action theories. It will be defined between mA+ action
theories and queries of the following form:
ϕ after δ

(11)

where ϕ is a belief formula and δ is a sequence of actions a1 ; . . . ; an (n ≥ 0)—referred to as a plan. Let us observe that
the entailment can be easily extended to consider more general forms of conditional plans, that include conditional
statements (e.g., if-then) or even loops (e.g., while)—as discussed in [31, 32]. We leave these relatively simple
extensions for future work.
The description of an evolution of a system will deal with sets of states. We refer to a set of states as a belief state
(or a b-state). We need the following definitions. For a b-state B and an action a, we say that a is executable in B if
ΦD (a, (M, s)) 6= ∅ for every state (M, s) in B. With a slight abuse of notation, we define

{⊥}
if ΦD (a, (M, s)) = ∅ in some state (M, s) in B or B = {⊥}
ΦD (a, B) = S
Φ
(a,
(M,
s))
otherwise
D
(M,s)∈B
where {⊥} denotes that the execution of a in B fails. Note that we assume that no action is executable in ⊥.
Let δ be a plan and B be a b-state. The set of b-states resulting from the execution of δ in B, denoted by Φ∗D (δ, B),
is defined as follows:
• If δ is the empty plan [ ] then Φ∗D ([ ], B) = B;
• If δ is a plan of the form a; δ 0 (with a ∈ A), then Φ∗D (a; δ 0 , B) = Φ∗D (δ 0 , ΦD (a, B)).
Intuitively, the execution of δ in B can go through several paths, each path might finish in a set of states. It is easy
to see that if one of the states reached on a path during the execution of δ is ⊥ (the failed state) then the final result of
the execution of δ in B is {⊥}. Φ∗D (δ, B) = {⊥} indicates that the execution of δ in B fails.
To complete the definition of the notion of entailment, we need the following definition.
27

Definition 20 (Initial State/b-State) Let (I, D) be an action theory. An initial state of (I, D) is a state (M, s) such
that for every statement
initially ϕ
in I, (M, s) |= ϕ. (M, s) is an initial S5-state if it is an initial state and M is a S5 Kripke structure. The initial b-state
of (I, D) is the collection of all initial states of (I, D). The initial S5-b-state of (I, D) is the collection of all initial
S5-states of (I, D).
We are now ready to define the notion of entailment.
Definition 21 (Entailment) An action theory (I, D) entails the query
ϕ after δ,
denoted by (I, D) |= ϕ after δ, if
(a) Φ∗D (δ, I0 ) 6= {⊥} and
(b) (M, s) |= ϕ for each (M, s) ∈ Φ∗D (δ, I0 )
where I0 is the initial b-state of (I, D).
We say that (I, D) S5-entails the query ϕ after δ, denoted by (I, D) |=S5 ϕ after δ, if the two conditions (a)-(b)
are satisfied with respect to I0 being the initial S5-b-state of (I, D).
The next example illustrates these definitions.
Example 19 Let D1 be the domain specification given in Examples 3 and 4 and I1 be the set of initial statements
given in Example 10. Furthermore, let δA be the sequence of actions:
δA = distract(A, C); signal(A, B); open(A); peek(A).
We can show that
(I1 , D1 ) |= (BA tail ∨ BA ¬tail) ∧ BA (BB (BA tail ∨ BA ¬tail)) after δA
(I1 , D1 ) |= BB (B
V A tail ∨ BA ¬tail) ∧ (¬BB tail ∧ ¬BB ¬tail) after δA
(I1 , D1 ) |= BC [ i∈{A,B,C} (¬Bi tail ∧ ¬BA ¬tail)] after δA
under the assumptions that initially, the beliefs of the agents satisfy the S5-axioms.
To see how the above conclusions hold, let us construct an initial state for (I1 , D1 ) (see also Figure 11). Since the
0
0
0
i where
, BC
, BB
truth values of all fluents but tail are known to every agent, we have that M0 = h{s0 , s1 }, π0 , BA
π0 (s0 ) = {¬opened, has key(A), ¬has key(B), ¬has key(C), looking(A), looking(B), looking(C), ¬tail}
and
π0 (s1 ) = {¬opened, has key(A), ¬has key(B), ¬has key(C), looking(A), looking(B), looking(C), tail}.
0
0
0
Furthermore, BA
= BB
= BC
= {(s0 , s0 ), (s0 , s1 ), (s1 , s0 ), (s1 , s1 )}.
The execution of the action distract(A, C) in (M0 , s0 ) results in the state (M1 , s2 ) as shown in Figure 12 where
1
1
1
M1 = h{s0 , s1 , s2 , s3 }, π1 , BA
, BB
, BC
i. The top part of the new structure is a replica of (M0 , s0 ), encoding the
beliefs of B, who is ignorant of the action occurrence. The bottom part encodes the beliefs of A and C, who are
observers of the action occurrence. It includes two new worlds, s2 and s3 , which represent the result of the execution
of distract(A, C) in s0 and s1 respectively, where π1 (s0 ) = π0 (s0 ), π1 (s1 ) = π0 (s1 ),

π1 (s2 ) = π0 (s0 ) \ {looking(C)} ∪ {¬looking(C)}
and
π1 (s3 ) = π0 (s1 ) \ {looking(C)} ∪ {¬looking(C)}.
28

A, B, C

A, B, C

A, B, C

S0
B

S0

A, B, C

A, B, C

S1

S1
B

B

B

S2

A, C

S3

A, B, C

Figure 11: (M0 , s0 ): an initial state of (I1 , D1 )

A, C

Figure 12:
(M1 , s2 ):
distract(A, C) in (M0 , s0 )

A, C

result of execution of

1
0
1
0
Finally, BA
= BA
∪ {(s2 , s2 ), (s2 , s3 ), (s3 , s2 ), (s3 , s3 )}, BC
= BC
∪ {(s2 , s2 ), (s2 , s3 ), (s3 , s2 ), (s3 , s3 )}, and
0
1
BB = BB ∪ {(s2 , s0 ), (s2 , s1 ), (s3 , s0 ), (s3 , s1 )}.
The execution of signal(A, B) in (M1 , s2 ) results in a new state (M2 , s6 ) where
2
2
2
M2 = h{s0 , . . . , s7 }, π2 , BA
, BB
, BC
i

where:
• For i ∈ {0, . . . , 3}, the state si+4 is the result of executing signal(A, B) in si . We have that π2 (si+4 ) = π1 (si )
because B is looking at the box already and thus the execution of this action does not change the state;
1
1
2
};
∪ {(si+4 , sj+4 ) | 0 ≤ i, j ≤ 3 and (si , sj ) ∈ BA
= BA
• BA
1
1
2
}; and
∪ {(si+4 , sj+4 ) | 0 ≤ i, j ≤ 3 and (si , sj ) ∈ BB
= BB
• BB
2
1
1
• BC
= BC
∪ {(si+4 , sj ) | 0 ≤ i, j ≤ 3 and (si , sj ) ∈ BC
}.

The execution of open(A) in (M2 , s6 ) will result in a new state (M3 , s14 ) where
3
3
3
M3 = h{s0 , . . . , s15 }, π3 , BA
, BB
, BC
i

where:
• For i ∈ {0, . . . , 7}, the state si+8 is the result of executing open(A) in si . We have that π3 (si+8 ) = π2 (si ) \
{¬opened} ∪ {opened};
3
2
2
• BA
= BA
∪ {(si+8 , sj+8 ) | 0 ≤ i, j ≤ 7 and (si , sj ) ∈ BA
};
3
2
2
• BB
= BB
∪ {(si+8 , sj+8 ) | 0 ≤ i, j ≤ 7 and (si , sj ) ∈ BB
}; and
3
2
2
• BC
= BC
∪ {(si+8 , sj ) | 0 ≤ i, j ≤ 7 and (si , sj ) ∈ BC
}.

Finally, the execution of peek(A) in (M3 , s14 ) results in a new state (M4 , s30 ) where
4
4
4
M4 = h{si | 0 ≤ i ≤ 31}, π4 , BA
, BB
, BC
i

where:
29

• For i ∈ {0, . . . , 15}, the interpretation π4 (si+15 ) = π3 (si ) since the execution of a sensing action does not
change the state of the world;
4
3
3
• BA
= BA
∪ {(si+16 , sj+16 ) | 0 ≤ i, j ≤ 15 and (si , sj ) ∈ BA
and ((¬tail ∈ π3 (si ) ∩ π3 (sj )) ∨ (tail ∈
π3 (si ) ∩ π3 (sj ))};
4
3
3
• BB
= BB
∪ {(si+16 , sj+16 ) | 0 ≤ i, j ≤ 15 and (si , sj ) ∈ BB
}; and
4
3
3
• BC
= BC
∪ {(si+16 , sj ) | 0 ≤ i, j ≤ 15 and (si , sj ) ∈ BC
}.

We will finally show that (M4 , s30 ) |= BA tail ∨ BA ¬tail. In fact, since (M0 , s0 ) |= ¬tail, we will show that
4
(M4 , s30 ) |= BA ¬tail. To prove this, we need to show that (M4 , sj ) |= ¬tail for every sj ∈ M4 [S], (s30 , sj ) ∈ BA
.
4
3
Since s30 6∈ M3 [S], we have that (s30 , sj ) ∈ BA iff j ≥ 16, (s14 , sj−16 ) ∈ BA , and ¬tail ∈ π3 (s14 ) ∩ π3 (sj−16 )
(because ¬tail ∈ π3 (s14 )). This implies that (M4 , s30 ) |= BA ¬tail. The proof of other conclusions is similar.
We conclude the example with the observation that another initial state for (I1 , D1 ) is (M0 , s1 ) and the execution
of δA in this state results in a new state (M 0 , s0 ) with the property that (M 0 , s0 ) |= BA tail. Theoretically, this fact and
the fact that (M4 , s30 ) |= BA ¬tail are insufficient for us to conclude that (I1 , D1 ) |=S5 BA tail ∨ BA ¬tail holds.
This, however, holds under some additional assumption and thanks to Proposition 15 (see Section 6).

5

Update Model Based Transitions for mA+ Domains

The previous section presented a transition function for mA+ domains in the style of action languages. In this section,
we will develop an alternative definition of transitions between states in mA+ domains that has its roots in dynamic
epistemic logic. For each action a and state (M, s), we will describe an update instance whose application to (M, s)
results in states equivalent to states belonging to ΦD (a, (M, s)). Since an update instance describes the effect of a
particular action occurrence, it will depend not only on the action specifications but also on a frame of reference. As
discussed in the previous section, a frame of reference in a mA+ domain over the signature hAG, F, Ai is a tuple
(F, P, O) where F ∪ P ∪ O = AG and F , P , and O are pairwise disjoint.
Let us start by considering the world-altering actions.
Definition 22 (Update Model/Instance for World-Altering Actions) Given a world-altering action a with the precondition ψ and a frame of reference ρ = (F, P, O), the update model for a and ρ, denoted by ω(a, ρ), is defined by
hΣ, R1 , . . . , Rn , pre, subi where
◦ Σ = {σ, };
◦ Ri = {(σ, σ), (, )} for i ∈ F and Ri = {(σ, ), (, )} for i ∈ O;
◦ pre(σ) = ψ and pre() = >; and
+
−
◦ sub() = ∅ and
(p, a))W| p ∈ F}, where
W sub(σ) = {p → Ψ (p, a) ∨ (p ∧ ¬Ψ
+
−
Ψ (p, a) = {ϕ | [a causes p if ϕ] ∈ D} and Ψ (p, a) = {ϕ | [a causes ¬p if ϕ] ∈ D}.

The update instance for the world-altering action a and the frame of reference ρ is (ω(a, ρ), {σ}).
Observe that the update model of the world-altering action a has only two events. Each event corresponds to a group of
agents. The links in the update model for each group of agents reflect the state of beliefs each group would have after
the execution of the action. For example, fully observant agents (in F ) will have no uncertainty. The next example
illustrates this definition.
Example 20 Going back to our original example, the action open(A) assumes that everyone is aware that C is not
looking at the box while B and A are. Figure 13 (top right) depicts the state. For simplicity, in the worlds we report
only the components of the interpretation related to the opened and tail fluents. The frame of reference for open(A)
in this situation is ({A, B}, ∅, {C}). The corresponding update instance for open(A) and the frame of reference
({A, B}, ∅, {C}) is given in Figure 13 (top left). The bottom part of Figure 13 shows the result of the application of
the update instance to the state on the top right.
30

A,B,C
A,B

A,B,C

σ

!

C

pre: has_key(A)
sub: opened ➝ ⊤∨ opened

A,B,C

S1

S2

A,B,C

¬opened
¬tail

¬opened
tail

pre: ⊤
sub: ∅

A,B,C

A,B,C

(S1,!)

(S2,!)

A,B,C

¬opened
¬tail

C

¬opened
tail

C

(S1,σ)

C

C
A,B

(S2,σ)

opened
¬tail

opened
tail

A,B

A,B

Figure 13: Update Instance (ω(open, ({A, B}, ∅, {C})), {σ}) and its application
In the next definition, we provide the update instance for a sensing action given a frame of reference. For simplicity
of presentation, we will assume that the set of sensed fluents of the action is a singleton.
Definition 23 (Update Model/Instance for Sensing Actions) Let a be a sensing action with SensedD (a) = {f }, let
its precondition be ψ, and let ρ = (F, P, O) be a frame of reference. The update model for a and ρ, ω(a, ρ), is defined
by hΣ, R1 , . . . , Rn , pre, subi where:
◦ Σ = {σ, τ, };
◦ Ri is given by


 {(σ, σ), (τ, τ ), (, )}
Ri = {(σ, σ), (τ, τ ), (, ), (σ, τ ), (τ, σ)}

{(σ, ), (τ, ), (, )}

if i ∈ F
if i ∈ P
if i ∈ O

◦ The preconditions pre are defined by

ψ∧f
pre(x) = ψ ∧ ¬f

>

if x = σ
if x = τ
if x = 

◦ sub(x) = ∅ for each x ∈ Σ.
The update instance for the sensing action a and the frame of reference ρ is (ω(a, ρ), {σ, τ }).
Observe that an update instance of a sensing action has three events, each one corresponding to a group of agents.
Each event is associated with a group of states in which the truth value of sensed fluent is either known to be true,
known to be false, or unknown.
31

Example 21 Let us consider the occurrence of the action peek(A) in the state described in Figure 14 (top right). The
frame of reference for this occurrence of peek(A) is ({A}, {B}, {C}). The corresponding update instance is given in
Figure 14 (top left). Its update on the given state results in the same state as in Example 15 (bottom, Figure 14).
pre: opened ⋀ looking(A) ⋀ ¬tail

A,B

σ

A,B,C
C

B
A,B

S1:
¬tail

!

C

S2:
tail

A,B,C

pre: ⊤

τ

A,B,C

A,B,C

pre: opened ⋀ looking(A) ⋀ tail

(S1,!):

(S2,!):

A,B,C

¬tail

tail
C

C

C

A,B,C

(S1,σ):

A,B,C

B

¬tail

C

(S2,τ):
tail

A,B

A,B

Figure 14: Update Instance (ω(peek(A), ({A}, {B}, {C})), {σ, τ }) and its application

We will conclude the section with a discussion on the update model of for announcement actions.
Definition 24 (Update Model/Instance for Announcement Actions) Given an announcement action a ∈ A that announces ϕ with the precondition ψ and a frame of reference ρ = (F, P, O), the update model for a and ρ, ω(a, ρ), is
defined by hΣ, R1 , . . . , Rn , pre, subi where:
◦ Σ = {σ, τ, };
◦ Ri is defined by


 {(σ, σ), (τ, τ ), (, )}
Ri = {(σ, σ), (τ, τ ), (, ), (σ, τ ), (τ, σ)}

{(σ, ), (τ, ), (, )}

if i ∈ F
if i ∈ P
if i ∈ O

◦ pre is defined by

ψ∧ϕ
pre(x) = ψ ∧ ¬ϕ

>

if x = σ
if x = τ
if x = 

◦ sub is defined as sub(x) = ∅ for any x ∈ Σ.
The update instance for the announcement action a with respect to the frame of reference ρ is (ω(a, ρ), {σ}).
32

As we can see, an update model for an announcement action and a frame of reference is structure-wise identical to the
update model for a sensing action and a frame of reference. The main distinction lies in the set of designated events in
the update instance for each type of actions. There is only a single designated event for announcement actions while
there are two for sensing actions.
Example 22 Let us consider the two versions of the action raising hand(A) described in Example 17 and the state in
which B is looking at the box and both A and B are aware of it.
For the first version of the action the frame of reference for its occurrence is ρ = ({A}, {B}, {C}). The update
instance (ω(raising hand(A), ρ), {σ}) is illustrated in Figure 15. Similarly, the update instance for the second version
of the action in the same state, which results in the frame of reference is ({A, B}, ∅, {C}), is shown in Figure 16.
pre: T ∧ ¬tail

A,B

σ

pre: T ∧ ¬tail

A,B
C

!

B

A,B

τ

C

σ

C

!

A,B,C

pre: T

A,B

pre: T ∧ tail

τ

C

A,B,C

pre: T

pre: T ∧ tail

Figure 15: Update instance for the raising hand(A) action and ρ = ({A}, {B}, {C})

Figure 16: Update instance for the raising hand(A) action and ρ = ({A, B}, ∅, {C})

Definitions 22-24 allow us to define the following notion.
Definition 25 Let a be an action and (M, s) be a state. The update instance of a in (M, s), denoted by Ω(a, (M, s)),
is defined as follows:
• Ω(a, (M, s)) = ∅ if a is not executable in (M, s);
• Ω(a, (M, s)) is the update instance for a and the frame of reference (FD (a, M, s), PD (a, M, s), OD (a, M, s))
as defined in the Definitions 22-24.
The following proposition relates the two approaches to define the successor states of an action occurrence in mA+.
Proposition 5 Given an action a and a state (M, s) such that ΦD (a, (M, s) 6= ∅, we have that for each (M 0 , s0 ) ∈
(M, s) ⊗ Ω(a, (M, s)) there exists an equivalent state in ΦD (a, (M, s)) and vice versa, where (M, s) ⊗ ∅ = ∅.

6

Definite Action Theories

In this section, we identify a class of mA+ action theories that can be each described by a finite number of initial
states, up to a notion of equivalence (Definition 4). The motivation for this task lies in the desire to use available
technologies (e.g., answer set solvers and/or forward search planners) in the computation of the entailment of mA+
theories—which requires the presence of a finite number of initial states. We observe that this problem does not arise
in single-agent domains, where the size of the state space is bounded by 2|F | . On the other hand, theoretically, there
could be infinitely many initial states for an arbitrary mA+ theory. For example, given a state (M, s) and a set of
formulae Σ such that (M, s) |= Σ, a new state (M 0 , s) that also satisfies Σ can be constructed from M by adding a
new world and keeping everything else unchanged.
One way to cope with the aforementioned problem is to limit the type of formulae occurring in the initial statements
of the action theory. Another way to deal with it is to limit the type of Kripke structures considered as initial states.
33

Indeed, we can observe that several examples found in the literature have the following properties: (i) the initial state
can be described by a set of statements involving the common knowledge among all agents; (ii) the common knowledge
relates to whether a particular agent is aware of (or not aware of) a property of the world; and (iii) the beliefs of the
agents coincide with their knowledge about the world. This suggests that several interesting problem domains can be
captured by limiting our attention to specific types of initial Kripke structures and to specific types of initial statements.
Specifically, we will focus on S5-initial states and consider initial statements of the following forms:
initially ϕ

(12)

initially Cϕ

(13)

initially C(Bi ϕ)

(14)

initially C(Bi ϕ ∨ Bi ¬ϕ)

(15)

initially C(¬Bi ϕ ∧ ¬Bi ¬ϕ)

(16)

where ϕ is a fluent formula. Intuitively, statements of type (12) indicate properties that are true in the real state of the
world; statements of type (13) denote properties that all agents believe to be true (and all agents know about the other
agents’ beliefs about such property); statements of type (14)-(15) indicate that all agents believe that agent i is aware
of whether ϕ is true or false; statements of type (16) indicate that all agents believe that agent i is not aware of whether
ϕ is true or false.
We will now formally define the notion of a definite action theory. For an action theory (I, D), let
TI = {ϕ | [initially ϕ] ∈ I}.
We will say that (I, D) is consistent if TI and D are consistent.
Definition 26 (Definite Action Theory) An action theory (I, D) is said to be definite if
• Each initial statement in I is of the form (12)-(16); and
• For each fluent formula ϕ and agent i, I contains an initial statement of the form (14), (15), or (16) in which i
and ϕ occurs.
We prove that S5-initial states of definite action theories are finitely computable in the next proposition.
Proposition 6 For a consistent definite action theory (I, D), there exists a finite number of initial S5-states
(M1 , s1 ), . . . , (Mk , sk ) such that every initial S5-state (M, s) of (I, D) is equivalent to some (Mi , si ). Furthermore,
for each pair of i 6= j and u ∈ Mi [S] there exists some v ∈ Mj [S] such that Mi [π](u) ≡ Mj [π](v).
Note that the last part of the proposition indicates that the set of interpretations used in each S5-state is the same.
The proof of the above proposition relies on a series of lemmas. Let us start by introducing some useful notations.
Given a state (M, s) and u, v ∈ M [S], we will refer to a path between u and v as a sequence of worlds u =
u0 , u1 , . . . , un = v in M [S], where for j = 0, . . . , n − 1, there exists some ij ∈ AG such that (uj , uj+1 ) ∈ M [ij ].
We say that v is reachable from u if there exists a path from u to v. We will also often make use of the fact that, for
a S5-structure, the relations Bi is symmetric, transitive, and reflective (Theorem 3.1.5, [28]). We list these lemmas
below. Proofs are provided in the appendix.
Lemma 7 Every S5-state (M, s) is equivalent to a S5-state (M 0 , s) such that, for every world u ∈ M 0 [S], we have
that u is reachable from s.
The above lemma indicates that for a S5-state (M, s), worlds that are unreachable from s can be removed. The
next lemma deals with initial statements of the form (12)-(14).
Lemma 8 Let (M, s) be a S5-state such that every world u ∈ M [S] is reachable from s. Let ψ be a formula. Then,
(M, s) |= C(ψ) iff M [π](u) |= ψ for every world u ∈ M [S].

34

The lemma shows that for a S5-state (M, s) that satisfies a statement of the form (12)-(14), the literal ` or the
formula ϕ appearing in the statement must be satisfied at every world in M . The next lemma characterizes S5-states
satisfying initial statements of the form (15).
Lemma 9 Let (M, s) be a S5-state such that every world u ∈ M [S] is reachable from s. Let ψ be a fluent formula.
Then:

(M, s) |= C(Bi ψ ∨ Bi ¬ψ) iff
∀u, v ∈ M [S] : (u, v) ∈ M [i] ⇒ M [π](u) |= ψ iff M [π](v) |= ψ
The lemma proves that for each S5-state (M, s) satisfying a statement of the form (15), every pair of worlds related
by Bi either both satisfy or both do not satisfy the formula ϕ appearing in the statement. The next lemma deals with
initial statements of the form (16).
Lemma 10 Let (M, s) be a S5-state such that every u ∈ M [S] is reachable from s. Let ψ be a fluent formula. Then,
(M, s) |= C(¬Bi ψ ∧ ¬Bi ¬ψ) iff for every u ∈ M [S] there exists some v ∈ M [S] such that (u, v) ∈ M [i], and
M [π](u)(ψ) 6= M [π](v)(ψ).
The lemma proves that for a S5-state (M, s) that satisfies a statement of the form (16), there must be at least one
pair of worlds in which the formula in the statement is not satisfied in both worlds. Observe that Lemmas 7-10 provide
a first characterization of S5-states satisfying initial statements of the forms (12)-(16). These lemmas do not take into
consideration the second condition in Definition 26. The next lemma, together with Lemmas 7-10, focuses on this
condition and allows us to determine properties of interpretations associated to the worlds in an initial S5-states of a
definite action theory.
Lemma 11 Let (M, s) be an initial S5-state of a definite action theory, such that every u ∈ M [S] is reachable from
s. Let ϕ be a fluent formula and i ∈ AG. Then,
• If (M, u) |= Bi ϕ for some u ∈ M [S] then (M, s) |= C(Bi ϕ) or (M, s) |= C(Bi ϕ ∨ Bi ¬ϕ);
• If (M, u) |= ¬Bi ϕ for some u ∈ M [S] then (M, s) |= C(¬Bi ϕ ∧ Bi ¬ϕ).
In order to complete the proof of Proposition 6, we need some additional notation. Consider a Kripke structure
M = hS, π, B1 , . . . , Bn i. We define a relation ∼ among worlds of M as follows. For each u, v ∈ M [S], u ∼ v iff
π(u) = π(v). Thus, u ∼ v indicates that the interpretations associated to u and v (i.e., M [π](u) and M [π](v)) are
identical. It is easy to see that ∼ is an equivalence relation over M [S]. Let ũ denote the equivalence class of u with
respect to the relation ∼ (i.e., ũ = [u]∼ ). The next lemma is critical for the proof of Proposition 6.
Lemma 12 Let (M, s) be an initial S5-state of a definite action theory, such that every u ∈ M [S] is reachable from
s. Let u, v ∈ M [S] such that u ∼ v. Then, for every i ∈ AG and x ∈ M [S] such that (u, x) ∈ M [i] there exists
y ∈ M [S] such that (v, y) ∈ M [i] and x ∼ y.
The above lemma allows us to collapse all worlds with the same interpretation into a single world. This is made
f be the structure constructed as follows:
precise as follows. Given a structure M , let M
f[S] = {ũ | u ∈ S}
• M
f[π](ũ)(f ) = M [π](u)(f )
• For every u ∈ M [S] and f ∈ F, M
f[i] if there exists (u0 , v 0 ) ∈ M [i] for some u0 ∈ ũ and v 0 ∈ ṽ.
• For each i ∈ AG, (ũ, ṽ) ∈ M
f is obtained from M by replacing each equivalence class in M with a single world. We will call (M
f, s̃)
Intuitively, M
the reduced state of (M, s). Thanks to the previously considered properties, the following holds:
f, s̃) of
Lemma 13 Let (M, s) be a S5-state such that every u ∈ M [S] is reachable from s. Then, the reduced state (M
(M, s) is a finite S5-state.
35

The next lemma shows that a reduced state of an initial S5-state of a definite action theory is also an initial S5-state
of the action theory.
f, s̃) be
Lemma 14 Let (M, s) be a S5-state such that every state u in M [S] is reachable from s. Furthermore, let (M
the reduced state of (M, s). It holds that
f, s̃) |= ψ;
1. (M, s) |= ψ iff (M
f, s̃) |= C(ψ);
2. (M, s) |= C(ψ) iff (M
f, s̃) |= C(Bi ψ);
3. (M, s) |= C(Bi ψ) iff (M
f, s̃) |= C(Bi ψ ∨ Bi ¬ψ));
4. (M, s) |= C(Bi ψ ∨ Bi ¬ψ) iff (M
f, s̃) |= C(¬Bi ψ ∧ ¬Bi ¬ψ));
5. (M, s) |= C(¬Bi ψ ∧ ¬Bi ¬ψ) iff (M
where i ∈ AG and ψ is a fluent formula.
Lemmas 14 and 13, together with the result on the existence of an initial S5-state with finite number of worlds in [28],
allow us to prove Proposition 6. Observe that Proposition 6 opens the door to the use of well-known propositional
tools, such as answer set solvers, to compute the entailment relation in definite action theories. On the other hand,
the number of initial Kripke structures could still be prohibitively large. We are next interested in domains which
determine, modulo equivalence among the Kripke structures, a single initial S5-state. Observe that a state (M, s) has
two components, the Kripke structure M and the actual world s. In order for a definite action theory (I, D) to have a
unique initial S5-state, the possible interpretation of the real state of the world must be unique. For M to be unique, we
observe that in the singe-agent case (e.g., in action language A, B, etc.), the initial state is unique when the knowledge
of the agent is complete. We generalize this to the case of multi-agent domains as follows.
Definition 27 A definite action theory (I, D) is complete if
{` | ` appears in one of the statements of the form (12)—(14) in I}
is a complete interpretation of F.
Intuitively, a complete definite action theory is such that the knowledge of each agent is known to all other agents and
the actual world is completely specified.
Proposition 15 For a consistent and complete action theory (I, D), there is a unique initial S5-state (M0 , s0 ) with
|M0 [S]| ≤ 2|F | such that every initial S5-state (M, s) of (I, D) is equivalent to (M0 , s0 ).
Propositions 6-15 show that definite action theories have finitely many (or unique) initial S5-states. The second
condition of Definition 26, on the other hand, indicates that the size of the set of initial statements of a definite action
theory is infinite as there are infinitely many fluent formulae. Clearly, this is not desirable. To address this issue, we will
next propose a simplification of Definition 26. This simplification is similar to the use of the Closed World Assumption
(CWA) to represent incomplete information in databases. More specifically, we propose that initial statements of the
form (16) are given implicitly, i.e., by representing the information that the agents do not know implicitly. This can be
realized as follows.
For a set of fluents F, a disjunction ϕ over F is called a complete clause if for every f ∈ F, either f or ¬f appears
in ϕ but not both. For a set of initial statements I and an agent i ∈ AG, let
	

C(I, i) = ϕ ϕ is a complete clause,TI 6|= C(Bi ϕ) and TI 6|= C(Bi ϕ ∨ Bi ¬ϕ)
S
Let neg(I) = i∈AG {initially C(¬Bi ϕ∧¬Bi ¬ϕ) | ϕ ∈ C(I, i)}. The completion of I is comp(I) = I ∪neg(I). It
is easy to see that for any action theory (I, D) such that I contains only statements of the form (12)-(16), (comp(I), D)
is a definite action theory. This means that we can specify a definite action theory by specifying only statements of
the form (12)-(15). Such a specification is obviously finite. Under this consideration, we can define a definite action
theory as follows.
36

Definition 28 (Definite Action Theory under CWA) An action theory (I, D) is called a definite action theory under
the CWA if I contains only initial statements of the form (12)-(16).
An initial state of a definite action theory under the CWA (I, D) if it is an initial state of (comp(I), D).
Observe that if we consider the action theory (I1 , D1 ) in Example 19 under the CWA, (M0 , s0 ) and (M0 , s1 ) are the
only two initial S5-states of (comp(I1 ), D1 ) and thus the entailment proved in Example 19 indeed hold.
It is easy to see that S5-entailment of definite action theories under the CWA can be computed thanks to Proposition 6. Studying the complexity of this problem and developing algorithms for computing the S5-entailment in definite
action theories under the CWA are interesting research topics and the focus of our future work. O

7

Related Work and Discussion

In this section, we connect our work to related efforts in reasoning about actions and their effects in multi-agent
domains. The background literature spans multiple areas. We will give a quick introduction and focus our attention
on the most closely related works.

7.1

Related Logics

The research discussed in this paper relates to a broad variety of logics and languages used to deal with reasoning about
actions and their effects in multi-agent domains—e.g., classical logic, non-monotonic logics, causal logics, high level
action languages, modal logics, epistemic logics, dynamic logics and dynamic epistemic logics. Here, we provide a
very brief overview of their relevance to reasoning about actions and their effects in multi-agent domains.
Classical logics, in particular, propositional and first-order logic, are often used to specify the physical state of the
world. For example, the propositional formula on table a ∧ on table b ∧ ¬on table c expresses that the blocks a and
b are on the table and the block c is not of the table. Similarly, the first order formula on table(a) ∧ on table(b) ∧
¬on table(b)∧∀X(ontable(X) ⇒ color(X, red)), describes a situation where the blocks a and b are on the table, the
block c is not on the table, and all blocks on the table are red. Propositional logic and first-order logic can be used to
represent the effects of actions and to reason about them. However, straightforward encodings require a large number
of axioms, especially to represent the inertia axioms—the properties of the world that do not change when a particular
action is performed. Two approaches can be considered to address this problem: (i) By using non-monotonic logics,
that can naturally express statements of the type “Normally an action does not affect a property” and can express
exceptions to this statement; (ii) By an alternative approach, used for example in [33], where the effects of actions
on various properties of the world are expressed using a high-level logic, which is then translated, using sophisticated
compilation techniques, into succinct encodings of inertia axioms in a classical logic.
While reasoning about the effect of actions, the relationship between some properties of the world may give rise to
“qualification” and “ramification”. Expressing this in classical logic leads to problems in many cases, especially when
the relationship between the properties are causal in nature. For example consider the two statements:
(a) A person cannot be in two places at the same time and
(b) A person cannot be married to two persons at the same time.
In classical logic, their representations are very similar: at(X) ∧ at(Y ) ⇒ X = Y and married to(X) ∧
married to(Y ) ⇒ X = Y . But let us assume that, initially, a person is at location p and he performs the action
of moving to a location q different from p. The statement (a) causes a ramification—since we would like to infer that,
after the execution of the action, the person is at p and not at q. On the other hand, let us assume that initially a person
is married to p and he (tries to) perform the action of marrying q (in a courthouse with marriage records) different
from p. The statement (b), this time, introduces a qualification, because of which the person is unable to perform the
action of marrying q.
Causal logic allows us to express (a) and (b) in a different way: at(X) ∧ X 6= Y causes ¬at(Y ) and
married to(X) ∧ married to(Y ) ∧ X 6= Y causes F ALSE. Such causal relationships can be expressed using logic programming, which has a non-classical connective “←.” In logic programming (a) and (b) can be expressed
as: ¬at(Y ) ← at(X), X 6= Y and ← married to(X), married to(Y ), X 6= Y .
37

High-level action languages were introduced to (a) provide a English-like specification language to describe the
effects of actions on properties of the world and the relationships between these properties, and to provide a semantics
that uses simple set theoretical notations; and (b) provide a framework that can be used to prove correctness of encodings in various logics for reasoning about effect of actions. For example, specifications in the language A [4] are of
the forms: (i) a causes f if p1 , . . . , pn ; and (ii) f after a1 , . . . an .
Modal logics add modal operators to various logics and their semantics is often defined using Kripke structures.
Two simple modal logics that are relevant to reasoning about actions are temporal logics and epistemic logics. One of
the simplest temporal logic is the forward linear temporal logic; it includes the operators 2, , 3, and U, meaning
“always in the future”, “next time step”, “sometime in the future” and “until.” Formulae in this logic are often used
to express goals that specify how we want the world to evolve. Reasoning is commonly focused on action-plans, to
verify if an action-plan satisfies a desired temporal specification, or to derive action-plans that satisfy a goal, given as
a temporal specification. Epistemic logics use the modal operators Ki , where Ki f means that agent i knows that f is
true. In this paper, we used belief logics which are similar to epistemic logics, the main difference being that we use
the modal operators Bi , where Bi f means that the agent i believes that f is true.
The community working on reasoning about actions and change initially used only the sequencing constructs
to build action-plans as sequence of actions and reason about such sequences. This has been extended [34, 32] to
allow “if-then” and other procedural features, that become necessary when sensing actions are needed to be part of
plans. GOLOG [31], an acronym for “alGOl in LOGic”, borrows programming constructs from procedural languages
to express complex plans which can be “evaluated” to generate valid action sequences. The evaluation of GOLOG
programs, as well as other action-plans, is realized on top of action theories that allow one to reason about a single
action and its impact on the world—and, in the process, take into account the issues regarding inertia, qualification
and ramification.
A related area of research is focused on the exploration of Dynamic logics. Dynamic logics were originally developed to reason about program correctness. In the programming domain, the basic actions are assignments of values to
variables. Reasoning about the effects of such assignment actions and the associated inertia is straightforward, since
all variables retain their old values except for the variables being assigned. In traditional procedural programming languages, one does not have to worry about qualification and ramification, since assigning a value to a variable does not
have any implicit impact on other variables.8 Any needed impact is explicitly written as new assignment statements.
The original focus of dynamic logics is to reason about programs that are built by composing simple assignment statements. Such programs are built using constructs such as: (i) α ∪ β (i.e., execute α or β), (ii) α; β (i.e., execute α
followed by β), (iii) α∗ (i.e., iterate α a finite number of times), (iv) p? (i.e., test whether α holds), and (v) λ (i.e.,
no-op).
To reason about programs in dynamic logics, programs are used as modal operators; various axioms and inference
rules have been developed that allow one to reason about the programs. The modal constructs used are of the form:
(i) [a]p, meaning that, after performing a, it is necessarily the case that p is true in the world, and (ii) haip, meaning
that, after performing a, it is possible that p is true in the world. These modal constructs allow one to specify effects
of arbitrary actions. For example, to express that an action a makes the formula φ true, one can write [a]φ. Similarly,
to express that a makes the formula φ true only when executed in a state where ψ is true, one can write [λ](ψ ⇒ [a]φ).
However, when we go beyond assignment actions, one needs to account for inertia. Similarly, when one goes beyond
the programming environment and variable assignments, one needs to worry about qualification and ramification. The
inertia axioms can be expressed in dynamic logic as shown in [35], which is similar to writing inertia axioms using
classical logic—i.e., they are encoded using a large number of formulae, that list one by one what properties are not
affected by an action. Such approach to account for ramification is tedious and involves the use of logic programming,
pre-compilation and then generating dynamic logic formulae based on such pre-compilation. In [36], where dynamic
logic is used for reasoning about actions and change, inertia is avoided by referring to it as an undesirable overcommitment; however, the authors admit that their formulation cannot address qualification, and they indicate that
“non-monotonic logics are clearly superior” in that regard. Overall, when reasoning about actions and change (beyond
simple variable assignments) using dynamic logics, one still needs to worry about inertia and the frame problem,
qualification and ramification issues.
8 If

we ignore issues of aliasing, e.g., through pointers.

38

Many early works about action and change, as well as dynamic logics, reason about the world and do not worry
about agent’s knowledge about the world. When sensing actions are considered, one has to distinguish between the
world and a single agent’s knowledge about the world. This leads to the use of epistemic logics and Kripke structures,
and frame axioms need to be developed with respect to knowledge formulae. This has been achieved by having frame
axioms for the accessibility relations used in the Kripke structures. High level languages that can express sensing
actions and their effects have been developed and matched with logical encodings. This paper extends such approach
to the case of multi-agent domains, where other knowledge actions (besides sensing) are considered. A new dimension
that emerges is the observability of the various agents as part of an action; some may have full observability, some
others may have partial observability, and the rest have no observability. The result of such actions and such varied
observability is that different agents have different knowledge and beliefs about the world and about each other’s
knowledge and beliefs.
Several researchers have considered reasoning about actions in a multi-agent domain using the dynamic logics
approach. These proposals are focused on extending dynamic logics to Dynamic Epistemic Logics (DEL), to reason
about the agent’s knowledge about the world and about other agents’ knowledge in presence of multi-agent actions.
The extensions are twofold. In particular, in the formula [α]p: (i) In DEL, p is a formula in epistemic logic, while in
dynamic logic p is a classical logic formula, and (ii) In DEL, α is a more general action than in dynamic logic. In
[24], α has the usual dynamic logic constructs for complex actions, plus constructs such as: (a) LA ?ϕ whose intuitive
meaning is that the agents in the group A learn that ϕ is true; and (b) (α!α0 ) whose intuitive meaning is that between
α and α0 , we choose α. The latter is often written as (!α ∪ α0 ). Using these actions, the authors of [24] show that one
can express sensing actions (refereed to as “read” actions) in a multi-agent setting. Specifically, the fact that we have
two agents a and b, a senses the value of p as being true, and it is common knowledge between a and b that b observes
a sensing, can be modeled as the complex action Lab (!La ?p ∪ La ?¬p). However, the language of update models [20]
is more general and is currently preferred by the DEL community to express such multi-agent actions. In the language
of update models, the action discussed above can be expressed as in Figure 17.

Figure 17: A program model for Lab (!La ?p ∪ La ?¬p)

7.2

Relating mA+ and DEL with Update Models: Differences

The similarities between mA+ and the update models based approach has been discussed in detail in Section 5. We
next detail the differences between the two approaches.
7.2.1

Multi-Agent Actions

A key difference between the formalism proposed in this paper and DEL with update models is with respect to the
simplest of actions in presence of multiple agents.
Let us consider the simplified version of the coin in a box problem as presented in Example 2—with three agents
A, B, and C, a box containing a coin, and initially it is common knowledge that none of the agents knows whether
the coin lies heads up or tails up. Let us assume that agent A peeks into the box. In our formalism, we express this
action as peek(A). In DEL, the update model for the same action, as given in Figure 14, will also include additional
information about all three agents A, B, and C encoding their “roles” or “perspectives” while A is peeking into the
box. By roles or perspectives we mean information about what the agents are doing, in terms of who is watching
whom and who knows about that.
It is evident that our representation of the action simply as peek(A) is much simpler than the DEL representation
in Figure 14. But our representation does not include the information about what else the agents A, B, and C are
39

doing while A is peeking into the box. In our formulation, such information is part of the state, and is expressed
by using perspective fluents, such as looking(B)—that encodes the information that B is looking at the box—and
group member(B,group(A))—that encodes the information that B and A are together in the same group.
Thus, it appears that a critical difference between the mA+ approach to representing multi-agent actions and
the approach used in DEL with update models lies in the way we encode the information about agents roles and
perspectives—as part of the action in DEL with update models and as part of the state in mA+. At first glance, this
difference may not appear far reaching. However, there are some far reaching implications of such difference, as
discussed in the following subsections.
Narratives and Dynamic Evolution of Multi-agent Actions: Let us consider a scenario with two agents A and B.
Initially, agent B is looking at agent A. Agent A lifts a block and, after some time, agent A puts down the block. Some
time later, agent B is distracted while agent A again lifts the block.
In our formulation, this narrative can be formalized by first describing the initial situation, and then describing the
sequence of actions that occurred, which for this example is:
liftBlock(A); putDown(A); distract(B); liftBlock(A).
The description of this evolution of scenario in DEL is not as simple: each action occurrence will have to be described
as an update model containing information about both agents A and B. In addition, such a description (in DEL) will
be partly superfluous, as it will have to record information about B looking (or not looking) at A in the update model,
when that information is already part of the state. Thus, the approach used in mA+ to describe this narrative is more
natural than the representation in DEL.
Observe that, in our narrative, the action liftBlock(A) appears twice. However, due to the difference in the roles
and perspectives over time, the two occurrences of liftBlock(A) correspond to two different update models. This shows
how, using the mA+ formulation, we can support the dynamic evolution of update models, as result of changes in
perspective fluents in the state. In DEL, the two update models are distinct and there is no direct connection between
them and neither one does evolve from the other.
In order to further reinforce this point, let us consider another narrative example. Let us consider a scenario with
three agents, A, B, and C. Initially, it is common knowledge that none of the agents knows whether the coin in the
box is lying heads up or tails up. In addition, let us assume that initially A and B are looking at the box, while C is
looking away. Let us consider the narrative where A peeks into the box; afterwards, A realizes that C is distracted
and signals C to look at the box as well; finally A peeks into the box one more time. In mA+, this situation can be
described again by a sequence of actions:
peek(A); signal(C); peek(A)
The two occurrences of peek(A) correspond to two different update models; the second occurrence is an evolution
of the first caused by the execution of signal(C). In DEL, the relevance of the intermediate action signal(C), and its
impact on the second occurrence of peek(A), is mostly lost—and this results in the use of two distinct update models
for peek(A) with complete information about the whole action scenario.
The key aspect that allows a natural representation of narratives and evolution of update models in mA+ is the
presence of the agents’ perspectives and roles encoded as perspective fluents of a state, and their use to dynamically
generate the update models of the actions. While DEL can include perspective fluents as part of the states as well, it
does not have a way to take advantage of them in a similar way as mA+.
Separation of Specification of Actions and their Effects and the Observability of an Agent: As alluded in the
previous two sections, a core difference between mA+ and the DEL specification of actions in multi-agent domains,
lies in that mA+ separates the specification of actions and action effects from the description of observability of the
action occurrence by each agent. In both [24] and [20], the observability of agents is hard-coded in the specification
of each complex action. We discuss this difference in more detail using an example.
Let us reconsider the domain D1 . In order to describe the possible histories of the domain, we need to develop
the update models for every action occurrence. Let us consider, for example, the action peek(A); we need to have an
update model for all of the following cases:
40

• Both B and C are looking;
• Either B or C is looking but not both; and
• Both B C are not looking.
In our approach, the above example is specified in a very different way: the action is about sensing tail. The agents
who sense it, who observe the sensing take place, and who are oblivious can be specified directly or can be specified
indirectly in terms of conditions, such as which agents are near the sensing, which ones are watching from far, and
which ones are looking away, respectively. Actions can be planned and executed to change the observers and partial
observers. In other words, in mA+, if we have a complex action α; β, by executing α we may be able to change the
world, in terms of who is fully observing, who is partially observing, and who is oblivious with respect to the next
action β. This is not possible in DEL. Thus, while mA+ allows us to develop plans where an agent can manipulate
the observability of other agents, such planning cannot be done in straightforward manner in DEL.
Simplicity by Design: The formulation adopted in this paper is limited in expressivity to ensure simplicity. It is
limited by the (perspective) fluents we have and how we use them. On the other hand, DEL is more complex and also
more expressive.
One advantage of our simplicity is that it limits the number of possible plans of a particular length, contributing to
the feasibility of multi-agent planning. In DEL, since update models are analogous to Kripke models, even in presence
of a small number fluents, it is possible to generate an infinite number of update models. One can limit the number of
update models by imposing restrictions on their structure. Nevertheless, as discussed earlier, an update model is much
more complex than actions mA+, which are often9 single units.
As an example,10 Figure 18 displays an update model that cannot be represented in mA+. The intuition behind
this update model is as follows. When A executes the action peek(A), A believes that both A and B can see the
outcome of sensing tail—i.e., A and B are fully observant. In reality, B is oblivious. This shows that, in multi-agent
domains, an agent’s observability could also be considered as beliefs, and as such affect the beliefs of an agent about
other agents’ beliefs after the execution of an action. The present mA+ language does not allow for such specification.
pre: tail

pre: tail

A

!

"

A,B

B
#

A,B

pre: T

Figure 18: An update model of the peek(A) action without an equivalent mA+ representation
The simplicity of our formulation is by design, and not an inherent flaw of our approach. Indeed, one could
envision developing a complete encoding of the complex graph structure of an update model as part of state, using
an extended collection of perspective fluents—but, at this time, we do not have a corresponding theory of change to
guide us in using these more expressive perspective fluents to capture the full expressive power of update models in
DEL. Hence, our current formalism is less expressive than DEL. However, the higher expressiveness of update models
provides us with a target to expand mA+ and capture more general actions. Expanding mA+ to express actions as
the one in Fig. 18 will be one of our immediate future goal.
On the other hand, as remarked earlier, the research on DEL with update models lacks at present an exploration of
how update models can evolve as result of action executions.
9 We
10

could allow parallel actions.
We thank an anonymous reviewer of an earlier version of this paper who suggested a similar example.

41

Analogy with Belief Update: Another approach to explore the differences between mA+ and DEL builds on the
analogy to the corresponding differences between belief updates and the treatment of actions and change in early
action languages [7].
Papers on belief updates define and study the problem of updating a formula φ with a formula ψ. In contrast, in
reasoning about actions and change, the focus is on defining the resulting state of the world after a particular action is
performed in a particular world, given a description of (i) how the action may change the world, (ii) when the action
can be executed; and (iii) how the fluents in the world may be (possibly causally) related to each other. In such a
context, given a state s and an action a, it is possible to see the the determination of the resulting state as the update
of s by a formula ϕ; But, what is important to consider is that the ϕ is not just the collection of effects of the action
a, but incorporates several other components, that take into account the static causal laws as well as which conditions
(part of the conditional effects of a) are true in s.
This situation is not dissimilar to the distinction between DEL update models and mA+. An update model can
be encoded by an action formula, and the resulting state can be obtained by updating the starting state with such
formula. In DEL, such action formula has to be given directly. Instead, our considerations in mA+ are in the spirit
of the early research in reasoning about actions and change—where we focus on describing actions and their effects,
their executability conditions, and where a resulting “state” is determined by applying these descriptions to the “state”
where a particular action is performed. Thus, the action formula in this latter case is not explicitly given, but derived
from the description of the actions, their effects, and executability conditions.
Taking the analogy further, an important application of reasoning about actions is to determine action sequences
or plan structures that achieve a given goal. This is different from searching for a formula ψ which, if used to update
a given initial state, will generate a goal state; the difference is partly due to the fact that the the space of formulae
is infinite. Similarly, the space of update models is infinite, and it is not viable to look for an update model (or
a sequence of update models) that will cause a transition from a given initial state to a goal state. Instead, mA+
supports the traditional way of planning by finding action sequences or plan structures that achieve a goal.
Executing Actions: The notion of actions adopted in mA+ is designed to enable their executions by one or multiple
agents. For example, the action peek(A) can be executed by agent A, by peeking into the box. On the other hand,
an action modeled using update models in DEL is not executable, in the normal sense. For example, how does the
action expressed in Figure 14 get executed? Who does execute such action? What does executing the various edges of
Figure 14 mean? The answers to these questions are not clear.
Furthermore, let us turn around such questions and focus on the perspective fluents: how does one execute the
perspective fluents, such as looking(B)? The answer is that they are fluents, and they are not required or supposed
to be executed. A more appropriate question would be: how do they become true? The answer is that, while our
formulation could have some actions that make them true, when describing a state we need not worry about how
exactly the facts in the state came about to be true. This is not the case when describing actions: when describing
actions we need to describe something that can be executed. In summary, actions, or parts of actions, are supposed to
be something that can be executed, while states, or parts of states, do not have such requirement.
Hence, our representation of actions is more appropriate, and follows the common meaning of an action,11 than
the representation of action in DEL.
Value of Update Models: Having discussed the differences between mA+ and update models, we would like to
point out that update models present a very good technical tool for the understanding of effects of actions in multiagent domains. In fact, the transition function Φ for mA+ action theories can be effectively characterized using update
models, as described in Section 5.
7.2.2

Specifying the Initial State

An important aspect of many algorithms for reasoning about actions and change (including planning) is to have a finite
set of possible “initial states”. Although most of the examples in DEL papers show a finite number of possible initial
11 For example, the relevant dictionary meaning of “action is (1) something done or performed; act; deed. (2) an act that one consciously wills
and that may be characterized by physical or mental activity.

42

states (often a single Kripke structure), they do not focus on constraining the knowledge specified about the initial
state to guarantee the finiteness of the set of possible initial states. This is an important concern of our paper, and
we propose a restricted class of knowledge about the initial states that guarantees finiteness and yet is able to capture
most of the examples in the literature. Observe that this condition identifies a class of epistemic planning problems as
defined in [37, 38, 39] whose solutions can be computed using heuristic forward search.
We note that this problem is related to the finite model property in modal logics—which defines when a theory has
(at least) one finite model [40, 41]. The problem addressed in Section 6 could be viewed as the problem of identifying a
class of epistemic theories that has (up to equivalence) finitely many finite models and is, thus, a stronger problem than
the finite model property problem. To the best of our knowledge, this more complex problem has not been addressed
in multi-modal logics before. We believe that this is an important contribution of our development of mA+.

7.3

Previous Work by the Authors

Early attempts to adapt action languages to formalize multi-agent domains can be found in [42, 43, 44]. In these
works, the action languages A, B, and C have been extended to formalize multi-agent domains.
The works in [43, 44] investigate the use of action language in multi-agent planning context and focus on the
generation of decentralized plans for multiple agents, to either jointly achieve a goal or individual goals.
In [42], we show that several examples found in the literature—created to address certain aspect in multi-agent
systems (e.g., [45, 46, 47, 48, 49, 50])—can be formalized using an extension of the action language C. Yet, most
of the extensions considered in [42, 43, 44] are inadequate for formalizing multi-agent domains in which reasoning
about knowledge of other agents is critical. To address this shortcoming, we have developed and investigated several
preliminary versions of mA+ [51, 52, 53]. We started with an attempt to formulate knowledge of multiple agents in
[51]; we successively extended this preliminary version of mA+ with the use of static observability specifications in
[52]. The language developed in this paper subsumes that of [52]. In [53], we demonstrated the use of update models
to describe the transition function for the action language mA+ of [52].

7.4 mA+ and Action Languages for Single-Agent Domains
mA+ is a high-level action language for multi-agent domains. It is therefore instructive to discuss the connection
between mA+ and action languages for single-agent domains. First, let us observe that mA+ has the following
multi-agent domain specific features:
• it includes announcement actions; and
• it includes specification of the agents’ observability of action occurrences.
As it turns out, if we remove all features that are specific to multi-agent domains from mA+, and consider the S5entailment as its semantics, then the language is equivalent to the language AK from [32]. Formally, let us consider
a mA+ definite action theory (I, D) over the signature hAG, F, Ai such that |AG| = 1 and D does not contain
statements of the form (4) (announcement actions) and statements of the form (5)-(6). Let us define
IAK = {ϕ | ϕ appears in a statement of the form (12), (13), or (14) in I}.
Then, the following holds
(comp(I), D) |=S5 ϕ after δ iff (IAK , D) |=AK ϕ after δ.
This shows that mA+ is indeed a generalization of action languages for single-agent domains to multi-agent domains.
This also supports the claim that other elements that have been considered in action languages of single-agent domains,
such as static causal laws, non-deterministic actions, or parallel actions could potentially be generalized to mA+. This
is one of our goals for future work.

43

8

Conclusions and Future Works

In this paper, we developed an action language for representing and reasoning about effects of actions in multi-agent
domains. The language considers world-altering actions, sensing actions, and announcement actions. It also allows
the dynamic specification of agents’ observability with respect to action occurrences, enabling varying degrees of
visibility of action occurrences and action effects. The semantics of the language relies on the notion of states (pointed
Kripke structures), used as representations of the states of the world and states of agents’ knowledge and beliefs; the
semantics builds on a transition function, which maps pairs of states and actions to sets of states.
We discussed several properties of the transition function and identified a class of theories (definite action theories)
whose set of initial S5-states is finite, thus allowing for the development of algorithms for the S5-entailment relation
that is critical in applications such as planning and temporal reasoning. We also relate the proposed language to the
update model based approaches for representing and reasoning about effects of actions in multi-agent domains.
The development of mA+ is our first step towards our goal of developing automated reasoning and planning
systems in multi-agent domains. This will be our focus in the near future. In addition, we plan to extend the language
to deal with lying and/or misleading actions, refine the distinction between knowledge and beliefs of the agents, expand
the language to include non-deterministic actions and static causal laws, and specify more general models of agents’
observability, to capture some of the capabilities of update models that are missing from mA+.

Acknowledgments
The work has been partially supported by NSF grants HRD-1345232 and DGE-0947465.

44

Bibliography
References
[1] J. McCarthy, Programs with common sense, in: Proceedings of the Teddington Conference on the Mechanization
of Thought Processes, Her Majesty’s Stationery Office, London, 1959, pp. 75–91.
[2] R. Fikes, N. Nillson, STRIPS: a new approach to the application of theorem proving to problem solving, Artificial
Inteligence 2 (1971) 189–208.
[3] V. Lifschitz, On the semantics of STRIPS, in: M. Georgeff, A. Lansky (Eds.), Reasoning about Actions and
Plans, Morgan Kaufmann, San Mateo, CA., 1987, pp. 1–9.
[4] M. Gelfond, V. Lifschitz, Representing actions and change by logic programs, Journal of Logic Programming
17 (2,3,4) (1993) 301–323.
[5] E. Pednault, ADL: Exploring the middle ground between STRIPS and the situation calculus, in: R. Brachman,
H. Levesque, R. Reiter (Eds.), Proceedings of the First International Conference on Principles of Knowledge
Representation and Reasoning, Morgan Kaufmann, 1989, pp. 324–332.
[6] M. Ghallab, A. Howe, C. Knoblock, D. McDermott, A. Ram, M. Veloso, D. Weld, D. Wilkins, PDDL — the
Planning Domain Definition Language. Version 1.2, Tech. Rep. CVC TR98003/DCS TR1165, Yale Center for
Comp, Vis and Ctrl (1998).
[7] M. Gelfond, V. Lifschitz, Action Languages, Electronic Transactions on Artificial Intelligence 3 (6).
[8] C. Castellini, E. Giunchiglia, A. Tacchella, Improvements to sat-based conformant planning, in: Proceedings of
6th European Conference on Planning (ECP-01), 2001.
[9] T. C. Son, P. H. Tu, M. Gelfond, R. Morales, Conformant Planning for Domains with Constraints — A New
Approach, in: Proceedings of the Twentieth National Conference on Artificial Intelligence, 2005, pp. 1211–
1216.
[10] E. Durfee, Distributed Problem Solving and Planning, in: Muliagent Systems (A Modern Approach to Distributed Artificial Intelligence), MIT Press, 1999, pp. 121–164.
[11] M. de Weerdt, A. Bos, H. Tonino, C. Witteveen, A resource logic for multi-agent plan merging, Ann. Math. Artif.
Intell. 37 (1-2) (2003) 93–130.
[12] M. de Weerdt, B. Clement, Introduction to planning in multiagent systems, Multiagent Grid Systems 5 (2009)
345–355.
[13] M. Allen, S. Zilberstein, Complexity of decentralized control: Special cases, in: 23rd Annual Conference on
Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver,
British Columbia, Canada, Curran Associates, Inc., 2009, pp. 19–27.
[14] D. S. Bernstein, R. Givan, N. Immerman, S. Zilberstein, The complexity of decentralized control of markov
decision processes, Math. Oper. Res. 27 (4) (2002) 819–840.
[15] C. V. Goldman, S. Zilberstein, Decentralized control of cooperative systems: Categorization and complexity
analysis, Journal of Artificial Intelligence Resesearch (JAIR) 22 (2004) 143–174.
[16] C. Guestrin, D. Koller, R. Parr, Multiagent planning with factored mdps, in: T. G. Dietterich, S. Becker,
Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada],
MIT Press, 2001, pp. 1523–1530.

45

[17] R. Nair, M. Tambe, M. Yokoo, D. V. Pynadath, S. Marsella, Taming decentralized pomdps: Towards efficient
policy computation for multiagent settings, in: G. Gottlob, T. Walsh (Eds.), IJCAI-03, Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, Acapulco, Mexico, August 9-15, 2003, Morgan
Kaufmann, 2003, pp. 705–711.
[18] L. Peshkin, V. Savova, Reinforcement learning for adaptive routing, in: Proceedings of the Int. Joint Conf. on
Neural Networks, 2002.
[19] J. van Eijck, Dynamic epistemic modelling, Tech. rep. (2004).
[20] A. Baltag, L. Moss, Logics for epistemic programs, Synthese.
[21] A. Herzig, J. Lang, P. Marquis, Action Progression and Revision in Multiagent Belief Structures, in: Sixth
Workshop on Nonmonotonic Reasoning, Action, and Change (NRAC), 2005.
[22] J. van Benthem, Dynamic logic of belief revision, Journal of Applied Non-Classical Logics 17(2) (2007) 129–
155.
[23] J. van Benthem, J. van Eijck, B. P. Kooi, Logics of communication and change, Inf. Comput. 204 (11) (2006)
1620–1662.
[24] H. van Ditmarsch, W. van der Hoek, B. Kooi, Dynamic Epistemic Logic, Springer, 2007.
[25] N. Friedman, J. Halpern, Modeling Belief in Dynamic Systems: Revision and Update, Journal of Artificial
Intelligence Research 10 (1999) 117–167.
[26] H. Katsuno, A. Mendelzon, On the difference between updating a knowledge base and revising it, in: Proceedings of KR 92, 1992, pp. 387–394.
[27] del Val A., Y. Shoham, A unified view of belief revision and update, Journal of Logic and Computation, Special
issue on Actions and processes, M. Georgeff (ed.).
[28] R. Fagin, J. Halpern, Y. Moses, M. Vardi, Reasoning about Knowledge, MIT press, 1995.
[29] T. Son, E. Pontelli, C. Baral, G. Gelfond, A sufficient syntactic condition for the maintenance the kd45n property
using update models, (Unpublished manuscript) (2014).
[30] T. C. Son, E. Pontelli, C. Baral, G. Gelfond, A sufficient syntactic condition for the maintenance of the KD45n
property using update models, Tech. rep. (2014).
[31] H. Levesque, R. Reiter, Y. Lesperance, F. Lin, R. Scherl, GOLOG: A logic programming language for dynamic
domains, Journal of Logic Programming 31 (1-3) (1997) 59–84.
[32] T. C. Son, C. Baral, Formalizing sensing actions - a transition function based approach, Artificial Intelligence
125 (1-2) (2001) 19–91.
[33] R. Reiter, KNOWLEDGE IN ACTION: Logical Foundations for Describing and Implementing Dynamical Systems, MIT Press, 2001.
[34] R. Scherl, H. Levesque, Knowledge, action, and the frame problem, Artificial Intelligence 144 (1-2).
[35] J.-J. Meyer, Dynamic logic for reasoning about actions and agents, in: J. Minker (Ed.), Logic-Based Artificial
Intelligence, Boston/Dordrecht: Kluwer, 2000, pp. 281–311, chapter 13.
[36] H. Prendinger, G. Schurz, Reasoning about action and change - a dynamic logic approach, Journal of Logic,
Language, and Information 5 (1996) 5–209.

46

[37] W. van der Hoek, M. Wooldridge, Tractable multiagent planning for epistemic goals, in: The First International
Joint Conference on Autonomous Agents & Multiagent Systems, AAMAS 2002, July 15-19, 2002, Bologna,
Italy, Proceedings, ACM, 2002, pp. 1167–1174.
[38] B. Löwe, E. Pacuit, A. Witzel, Del planning and some tractable cases, in: H. van Ditmarsch, J. Lang, S. Ju
(Eds.), Logic, Rationality, and Interaction, Vol. 6953 of Lecture Notes in Computer Science, Springer Berlin /
Heidelberg, 2011, pp. 179–192.
[39] T. Bolander, M. Andersen, Epistemic Planning for Single and Multi-Agent Systems, Journal of Applied NonClassical Logics 21 (1).
[40] D. Gabbay, A. Kurucz, F. Wolter, M. Zakharyaschev, Many-Dimensional Modal Logics: Theory and Application,
Elsevier, 2003.
[41] J. van Benthem, Modal Logic for Open Minds, Center for the Study of Language and Information, 2010.
[42] C. Baral, T. C. Son, E. Pontelli, Reasoning about multi-agent domains using action language C: A preliminary
study, in: J. Dix, M. Fisher, P. Novák (Eds.), Computational Logic in Multi-Agent Systems - 10th International
Workshop, CLIMA X, Hamburg, Germany, September 9-10, 2009, Revised Selected and Invited Papers, Vol.
6214 of Lecture Notes in Computer Science, Springer, 2010, pp. 46–63.
[43] T. Son, E. Pontelli, C. Sakama, Logic Programming for Multiagent Planning with Negotiation, in: P. M. Hill,
D. S. Warren (Eds.), Logic Programming, 25th International Conference, ICLP 2009, Pasadena, CA, USA, July
14-17, 2009. Proceedings, Vol. 5649 of Lecture Notes in Computer Science, Springer, 2009, pp. 99–114.
[44] T. Son, C. Sakama, Reasoning and planning with cooperative actions for multiagents using answer set programming, in: M. Baldoni, J. Bentahar, J. Lloyd, B. van Riemsdijk (Eds.), Declarative Agent Languages and Technologies VI, 6th International Workshop, DALT 2009, Budapest, Hungary, 2009, Revised Selected and Invited
Papers, Vol. 5948, Springer, 2009, pp. 208–227.
[45] G. Boella, L. W. N. van der Torre, Enforceable social laws, in: F. Dignum, V. Dignum, S. Koenig, S. Kraus,
M. P. Singh, M. Wooldridge (Eds.), 4rd International Joint Conference on Autonomous Agents and Multiagent
Systems (AAMAS 2005), July 25-29, 2005, Utrecht, The Netherlands, ACM, 2005, pp. 682–689.
[46] J. Gerbrandy, Logics of propositional control, in: H. Nakashima, M. P. Wellman, G. Weiss, P. Stone (Eds.),
5th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS 2006), Hakodate,
Japan, May 8-12, 2006, ACM, 2006, pp. 193–200.
[47] W. van der Hoek, W. Jamroga, M. Wooldridge, A logic for strategic reasoning, in: F. Dignum, V. Dignum,
S. Koenig, S. Kraus, M. P. Singh, M. Wooldridge (Eds.), 4rd International Joint Conference on Autonomous
Agents and Multiagent Systems (AAMAS 2005), July 25-29, 2005, Utrecht, The Netherlands, ACM, 2005, pp.
157–164.
[48] A. Herzig, N. Troquard, Knowing how to play: uniform choices in logics of agency, in: H. Nakashima, M. P.
Wellman, G. Weiss, P. Stone (Eds.), 5th International Joint Conference on Autonomous Agents and Multiagent
Systems (AAMAS 2006), Hakodate, Japan, May 8-12, 2006, 2006, pp. 209–216.
[49] L. Sauro, J. Gerbrandy, W. van der Hoek, M. Wooldridge, Reasoning about action and cooperation, in: AAMAS
’06: Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, ACM,
New York, NY, USA, 2006, pp. 185–192.
[50] M. T. J. Spaan, G. J. Gordon, N. A. Vlassis, Decentralized planning under uncertainty for teams of communicating agents, in: H. Nakashima, M. P. Wellman, G. Weiss, P. Stone (Eds.), 5th International Joint Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2006), Hakodate, Japan, May 8-12, 2006, 2006, pp.
249–256.

47

[51] C. Baral, G. Gelfond, E. Pontelli, T. Son, Modeling multi-agent scenarios involving agents knowledge about
other’s knowledge using ASP, in: Proceedings of the 9th International Conference on Autonomous Agents and
Multiagent Systems, International Foundation for Autonomous Agents and Multiagent Systems, 2010, pp. 259–
266.
[52] E. Pontelli, T. Son, C. Baral, G. Gelfond, Logic programming for finding models in the logics of knowledge and
its applications: A case study, Theory and Practice of Logic Programming 10 (4-6) (2010) 675–690.
[53] C. Baral, G. Gelfond, On representing actions in multi-agent domains, in: Proceedings of the Symposium on
Constructive Mathematics, 2010.

48

Appendix: Proofs
Proposition 2 Let D be a consistent domain, (M, s) be a state, a be a world-altering action that is executable in
0 0
(M, s) and Φw
D (a, (M, s)) = {(M , s )}. Then, the following holds
1. For every pair of a world r(a, u) ∈ M 0 [S] \ M [S] and literal ` ∈ eD (a, M, u), (M 0 , r(a, u)) |= `;
2. For every pair of a world r(a, u) ∈ M 0 [S] \ M [S] and literal ` ∈ F \ {`, `¯ | ` ∈ eD (a, M, u)}, (M, u) |= ` iff
(M 0 , r(a, u)) |= `;
3. For every i ∈ OD (a, M, s) and formula ϕ, (M 0 , s0 ) |= Bi ϕ iff (M, s) |= Bi ϕ.
Proof.
1. We have that r(a, u) ∈ M 0 [S] \ M [S] implies that r(a, u) ∈ Res(a, M, s) and thus there exits some u ∈ M [S]
such that a is executable in (M, u). Because of the definition of Res(a, M, s), we have that for ` ∈ eD (a, M, u),
M 0 [π](r(a, u))(`) = >, and thus (M 0 , r(a, u)) |= `.
2. Similar to the above item, the conclusion for this item follows immediately from the definition of
Res(a, M, s)[π].
3. Assume that i ∈ OD (a, M, s) and ϕ an arbitrary formula. By the definition of M 0 , we have that (s0 , v) ∈ M 0 [i]
iff v ∈ M [S] and (s, v) ∈ M [i]. Furthermore, for each u ∈ M [S] and j ∈ AG we have that (v, u) ∈ M [j] iff
(v, u) ∈ M 0 [j]. This shows that (M 0 , s0 ) |= Bi ϕ iff (M, s) |= Bi ϕ.
2
Lemma 15 Let (M, s) be a state, α a group of agents, and f a fluent. Let us assume that for every i ∈ α and
(u, v) ∈ M [i], M [π](u)(f ) = M [π](v)(f ). Furthermore, for each u ∈ M [S] and i ∈ α, {v | (u, v) ∈ M [i]} =
6 ∅.
Then, (M, s) |= Cα f if (M, s) |= f and (M, s) |= Cα ¬f if (M, s) |= ¬f .
Proof. Without loss of generality, let us assume that (M, s) |= f . We say that a world u ∈ M [S] is reachable
from s via α if there exists a sequence of agents i1 , . . . , it and world s = v0 , v1 , . . . , vt = u such that ij ∈ α and
(vj−1 , vj ) ∈ M [ij for j = 1, . . . , t. It is easy to see that (M, u) |= f for every u reachable from s via α. Let us prove
by induction on k that (M, u) |= Eαk f for every u ∈ M [S] reachable from s via α. The base case is trivial, since
(M, u) |= f and f = Eα0 f . Let us now assume that (M, u) |= Eαk f for every u reachable from s via α. We will show
that (M, u) |= Bi Eαk f for i ∈ α and u is reachable via α. Consider v such that (u, v) ∈ M [i]. Clearly, v is reachable
from s via α. By inductive hypothesis, we have that (M, v) |= Eαk f . Since this holds for every v ∈ M [S] such that
(u, v) ∈ M [i] and there exists at least on such world v, we can conclude that (M, u) |= Bi Eαk f . This implies the
conclusion for the inductive step and proves the lemma for the case (M, s) |= f .
The proof for the case (M, s) |= ¬f is analogous.
2
Proposition 3 Let D be a consistent domain, a be a sensing action, and (M, s) be a state. Suppose that a is executable
in (M, s) and (M, s) is consistency preserving for a. Assume that (M 0 , s0 ) ∈ ΦsD (a, (M, s)). Then, the following
holds
1. for every f ∈ SensedD (a), if (M, s) |= ` then (M 0 , s0 ) |= CFD (a,M,s) `, where ` ∈ {f, ¬f };
2. for every f ∈ SensedD (a), (M 0 , s0 ) |= CPD (a,M,s) (CFD (a,M,s) f ∨ CFD (a,M,s) ¬f ); and
3. for every i ∈ OD (a, M, s) and formula ϕ, (M 0 , s0 ) |= Bi ϕ iff (M, s) |= Bi ϕ.
Proof. Recall that

−1

(M 0 , s0 ) = (M, s) ]cFD (a,M,s)∪PD (a,M,s) (M 00 , s00 )
where

49

a

• (M 00 , s00 ) = (M r |FD (a,M,s)∪PD (a,M,s) 	Rema (M r ), c(s)) and
• (M r , c(s)) is a c-replica of (M, s).
We observe that for every i ∈ FD (a, M, s) ∪ PD (a, M, s),
(u, v) ∈ M 0 [i] iff (u, v) ∈ M 00 [i]

(17)

and for each u ∈ M 00 [i], it holds that {v | (u, v) ∈ M 00 [i]} =
6 ∅ because M is consistency preserving for a.
1. Consider the structure (M 00 , s00 ) and some f ∈ SensedD (a). It holds that M 00 [π](u)(f ) = M 00 [π](v)(f ) for i ∈
FD (a, M, s) and (u, v) ∈ M 00 [i]. Thus, (M 00 , s00 ) |= CFD (a,M,s) f or (M 00 , s00 ) |= CFD (a,M,s) ¬f (Lemma 15).
Because of (17), we have that (M 0 , s0 ) |= CFD (a,M,s) ` iff (M 00 , s00 ) |= CFD (a,M,s) ` for ` ∈ {f, ¬f }. This
proves the first item.
2. Observe that for a sequence of links (s0 , j1 , s1 ), . . . , (sk−1 , jk , sk ) in M 0 such that jl ∈ PD (a, M, s) for l =
1, . . . , k, we have that si ∈ M 00 [S]. This, together with the fact that (M 00 , u) |= CFD (a,M,s) f ∨ CFD (a,M,s) ¬f
for every u ∈ M 00 [S] and (17), implies the second item.
3. The third item is similar to the third item of Proposition 2.
2
Proposition 5 Given an action a and a state (M, s), we have that for each (M 0 , s0 ) ∈ (M, s) ⊗ Ω(a, (M, s)) there
exists an equivalent element in ΦD (a, (M, s)) and vice versa.
Proof. If a is not executable in (M, s) then (M, s) ⊗ Ω(a, (M, s)) = ΦD (a, (M, s)) = ∅ so the proposition is trivial
for this case.
Let us assume that a is executable in (M, s) and the precondition of a is ψ. Furthermore, let ρ =
(FD (a, M, s), PD (a, M, s), OD (a, M, s)). We consider three cases:
1. a is a world-altering action. Following Definitions 15 and 25, we have that ΦD (a, (M, s)) = {(M 0 , s0 )} and
Ω(a, (M, s)) = (ω(a, ρ), {σ}) with ω(a, ρ) as in Definition 22.
By the definition of ⊗ operator between a state and an update instance we have that (M, s) ⊗ Ω(a, (M, s)) is
a singleton. Thus, let us denote the unique element in (M, s) ⊗ Ω(a, (M, s)) with (W, w) where each world in
W [S] is of the form (u, σ) or (u, ) for some u ∈ M [S] and w = (s, σ).
We will show that (W, w) is equivalent to (M 0 , s0 ). We define the function h : W [S] → M 0 [S] as follows:

r(a, u) if γ = σ
h((u, γ)) =
u
if γ = 
Observe that (u, σ) ∈ W [S] iff (M, u) |= pre(σ) = ψ iff a is executable in u iff r(a, u) ∈ Res(a, M, s)[S] iff
r(a, u) ∈ M 0 [S].
Furthermore, (u, ) ∈ W [S] iff u ∈ M [S] iff u ∈ M 0 [S] \ Res(a, M, s)[S]. This allows us to conclude that h
is a bijective function f from W [S] to M 0 [S].
(*)
We will show next that W [π]((u, γ)) ≡ M 0 [π](h((u, γ))).

(**)
0

0

Consider γ = . For each u ∈ M [S], we have that W [π]((u, )) ≡ M [π](u) ≡ M [π](u) = M [π](f ((u, ))).
We now show that W [π]((u, σ)) ≡ M 0 [π](r(a, u)). Consider p ∈ F. We have that W [π]((u, σ))(p) = >
iff p → Ψ+ (p, a) ∨ (p ∧ ¬Ψ− (p, a)) ∈ sub(σ) and (M, u) |= Ψ+ (p, a) ∨ (p ∧ ¬Ψ− (p, a))
iff either (a) there exists a statement “a causes p if ϕ” in D such that (M, u) |= ϕ; or (b) (M, u) |= p and
(M, u) |= ¬ϕ for every statement “a causes ¬p if ϕ” in D
iff either (a) p ∈ eD (a, M, u) or (b) p 6∈ eD (a, M, u) and ¬p 6∈ eD (a, M, u) iff M 0 [π](r(a, u))(p) = >. This
implies that (**) holds.
Given (*) and (**), it is easy to see that the equivalence between (W, w) and (M 0 , s0 ) will be proved if we have
that ((u, γ1 ), (v, γ2 )) ∈ W [i] iff (h(u, γ1 ), h(v, γ2 )) ∈ M 0 [i]. Consider two cases:
50

• i ∈ FD (a, M, s). There are two cases:
– ((u, σ), (v, σ)) ∈ W [i] iff (u, v) ∈ M [i] and (M, u) |= pre(σ) and (M, v) |= pre(σ) iff (u, v) ∈
M [i] and r(a, u), r(a, v) ∈ Res(a, M, s)[S] iff (r(a, u), r(a, v)) ∈ M 0 [i];
– ((u, ), (v, )) ∈ W [i] iff (u, v) ∈ M [i] iff (u, v) ∈ M [i] and u, v ∈ M 0 [S] \ Res(a, M, s)[S] iff
(u, v) ∈ M 0 [i].
• i ∈ OD (a, M, s). There are two cases:
– ((u, σ), (v, )) ∈ W [i] iff (u, v) ∈ M [i] and (M, u) |= pre(σ) iff (u, v) ∈ M [i] and r(a, u) ∈
Res(a, M, s)[S] and v ∈ M 0 [S] \ Res(a, M, s)[S] iff (r(a, u), v) ∈ M 0 [i];
– ((u, ), (v, )) ∈ W [i] iff (u, v) ∈ M [i] iff (u, v) ∈ M [i] and u, v ∈ M 0 [S] \ Res(a, M, s)[S] iff
(u, v) ∈ M 0 [i].
The above two cases and (*) and (**) show that (M 0 , s0 ) is equivalent to (W, w), i.e., the proposition is proved
for this case.
2. a is a sensing action. Without the loss of generality, assume that a senses a single fluent f and the
executability condition of a is ψ. By Definition 23, (ω(a, ρ), {σ, τ }) is the update instance for a and
ρ = (FD (a, M, s), PD (a, M, s), OD (a, M, s)). Furthermore, for each u ∈ M [S], the set of worlds of
(M, s) ⊗ Ω(a, (M, s)) contains either (u, σ) or (u, τ ) but not both. Thus, (M, s) ⊗ Ω(a, (M, s)) is again a
singleton. Let us denote the unique element of (M, s) ⊗ Ω(a, (M, s)) by (W, w). Let (M r , c(s)) be the replica
of (M, s) that is used in the construction of (M 0 , s0 ) ∈ ΦD (a, M, s). Let h be the function
h : W [S] → M 0 [S]
with


h((u, λ)) =

c(u)
u

if (u, λ) ∈ W [S], λ ∈ {σ, τ }
if (u, λ) ∈ W [S], λ = 

The rest of the proof follows exactly the steps of the proof for the case of a world-altering action.
3. a is an announcement action. The proof is analogous to the case of a sensing action.
2
The remaining part of this appendix is devoted to prove Propositions 6-15. The proofs will be organized in a
collection of lemmas. Let us start with a lemma that allows us to ignore in a state those worlds that are not reachable
from the real state of the word.
Lemma 7 Every S5-state (M, s) is equivalent to a S5-state (M 0 , s) such that, for every state u ∈ M 0 [S], we have
that u is reachable from s.
Proof. The result derives from the fact that, if there is a world u which is not reachable from s, then for each formula
ψ we have that (M, s) |= ψ iff (M 0 , s) |= ψ, where M 0 is defined as follows: (i) M 0 [S] = M [S] \ {u} (i.e., we
remove the unreachable world u); (ii) M 0 [π](v) ≡ M [π](v) for every v ∈ M 0 [S] (i.e., all interpretations associated
to the worlds remain the same); and (iii) M 0 [i] = M [i] \ {(p, q) | (p, q) ∈ M [i], p 6∈ M 0 [S] or q 6∈ M 0 [S]}, (i.e., we
maintain the same belief relations except for removing all cases related to the world u).
2
The next lemma characterizes the accessibility relations for states satisfying a common knowledge formula.
Lemma 8 Let (M, s) be a S5-state such that every state u ∈ M [S] is reachable from s. Let ψ be a formula. Then,
(M, s) |= C(ψ) iff M [π](u) |= ψ for every world u ∈ M [S].
Proof. Because of the reflexive, transitive, and symmetric properties of the relations Bi in M and the definition of
the satisfaction of C(ψ), we have that (M, s) |= C(ψ) implies that (M, u) |= ψ for each u ∈ M [S] (since each u is
reachable from s), which implies M [π](u) |= ψ. The converse is obvious from the fact that each state is reachable
from s.
2
51

Lemma 8, in particular, indicates that for each i ∈ AG and for each fluent formula we have: ψ (M, s) |= C(Bi ψ) iff
∀u ∈ M [S]. M [π](u) |= ψ.
The following lemmas allow us to characterize the topological properties of the Kripke structures implied by the
satisfaction of the different types of statements in a definite action theory.
Lemma 9 Let (M, s) be a S5-state such that every world u ∈ M [S] is reachable from s. Let ψ be a fluent formula.
Then:

(M, s) |= C(Bi ψ ∨ Bi ¬ψ) iff
∀u, v ∈ M [S] : (u, v) ∈ M [i] ⇒ M [π](u) |= ψ iff M [π](v) |= ψ
Proof. Thanks to the Lemma 8, we have that (M, s) |= C(Bi ψ ∨ Bi ¬ψ) holds if and only if (M, u) |= Bi ψ ∨ Bi ¬ψ
for every u ∈ M [S]. The proof of the lemma then follows immediately from the fact that M is a S5-structure.
2
A consequence of Lemma 9 is the following lemma, that provides a characterization of S5-states with respect to
formulae of the form C(¬Bi ψ ∧ ¬Bi ¬ψ). For simplicity of presentation, for a structure M and u, v ∈ M [S] we
write M [π](u)(ψ) 6= M [π](v)(ψ) to denote either (M [π](u) |= ψ and M [π](v) 6|= ψ) or (M [π](u) 6|= ψ and
M [π](v) |= ψ), i.e., the value of ψ at u is different from the value of ψ at v.
Lemma 10 Let (M, s) be a S5-state such that every u ∈ M [S] is reachable from s. Let ψ be a fluent formula. Then,
(M, s) |= C(¬Bi ψ ∧ ¬Bi ¬ψ) iff for every u ∈ M [S] there exists some v ∈ M [S] such that (u, v) ∈ M [i], and
M [π](u)(ψ) 6= M [π](v)(ψ).
Let us prove a few properties of initial S5-states of a definite action theory.
Lemma 11 Let (M, s) be an initial S5-state of a definite action theory, such that every u ∈ M [S] is reachable from
s. Let ϕ be a fluent formula and i ∈ AG. Then,
• If (M, u) |= Bi ϕ for some u ∈ M [S] then (M, s) |= C(Bi ϕ) or (M, s) |= C(Bi ϕ ∨ Bi ¬ϕ);
• If (M, u) |= ¬Bi ϕ for some u ∈ M [S] then (M, s) |= C(¬Bi ϕ ∧ Bi ¬ϕ).
Proof. This result can be proved by contradiction. Let us consider the first item. If the consequence of the first item
does not hold then (M, s) |= C(¬Bi ϕ ∧ Bi ¬ϕ)—since (M, s) is an initial state of a definite action theory. Thus,
(M, u) 6|= Bi ϕ due to Lemma 10 (since there are two states in which the formula ϕ has a different truth value). This
leads to a contradiction.
Let us consider the second item. If the consequence of the second item does not hold then we have that (M, s) |=
C(Bi ϕ) or (M, s) |= C(Bi ϕ ∨ Bi ¬ϕ) (since we are dealing with definite action theories). This implies (M, u) |=
Bi ϕ or (M, u) |= Bi ¬ϕ (Lemma 8 and Lemma 9). This leads to a contradiction.
2
In the following, for u ∈ M [S], let
state(u) =

^

f∧

f ∈F , M [π](u)(f )=>

^

¬f

f ∈F , M [π](u)(f )=⊥

Lemma 12 Let (M, s) be an initial S5-state of a definite action theory, such that every u ∈ M [S] is reachable from
s. Let u, v ∈ M [S] such that u ∼ v. Then, for every i ∈ AG and x ∈ M [S] such that (u, x) ∈ M [i] there exists
y ∈ M [S] such that (v, y) ∈ M [i] and x ∼ y.
Proof. Let K(p, i) = {q | q ∈ M [S], (p, q) ∈ M [i]}—i.e., the set of worlds immediately related to p via M [i]. We
consider two cases:
• Case 1: K(u, i) ∩ K(v, i) 6= ∅. Because of the transitivity of M [i], we can conclude that K(u, i) = K(v, i)
and the lemma is trivial (by taking x = y).

52

• Case 2: K(u, i) ∩ K(v, i) = ∅. Let us assume that there exists some x ∈ K(u, i) such that there exists
no y ∈ K(v, i) with x ∼ y. This means that (M, y) |= ¬state(x) for each y ∈ K(v, i). In other words,
(M, v) |= Bi ¬state(x). This implies that (by Lemma 11):
(M, s) |= C(Bi ¬state(x))

or

(M, s) |= C(Bi ¬state(x) ∨ Bi state(x))

(18)

On the other hand, (M, u) 6|= Bi ¬state(x), since x ∈ K(u, i) and (M, x) |= state(x). This implies (M, s) |=
C(¬Bi ¬state(x) ∧ ¬Bi state(x)) by Lemma 11. This contradicts (18). This means that our assumption is
incorrect, i.e., the lemma is proved.
2
f, s̃) of
Lemma 13 Let (M, s) be a S5-state such that every u ∈ M [S] is reachable from s. Then, the reduced state (M
(M, s) is also a finite S5-state.
Proof. Let us start by observing that, for each u, v ∈ M [S] such that u ∼ v, we have: ũ = ṽ. Since the number of
possible interpretations of F is finite (being F itself finite), and since there can be no two distinct worlds ũ and ṽ such
f[S] is finite.
that M [π](u) = M [π](v), then we can conclude that the number of worlds in M
(1)
f[S].
Let us consider an agent i ∈ AG and worlds ũ, ṽ, w̃ ∈ M
f[i] since (u, u) ∈ M [i] for every u ∈ M [S]. This implies that M
f[i] is reflexive.
• Clearly, (ũ, ũ) ∈ M

(2)

f[i], if (ũ, ṽ) ∈ M
f[i] then there exists (u0 , v 0 ) ∈ M [i] for some u0 ∈ ũ and v 0 ∈ ṽ. Because
• By construction of M
f[i],
M is a S5-structure, and thus M [i] is symmetric, we have that (v 0 , u0 ) ∈ M [i]. This implies that (ṽ, ũ) ∈ M
f
i.e., M [i] is symmetric
(3)
f[i] and (ṽ, w̃) ∈ M
f[i]. The former implies that there exists (u0 , v 0 ) ∈ M [i] for
• Now assume that (ũ, ṽ) ∈ M
0
0
some u ∈ ũ and v ∈ ṽ. The latter implies that there exists (x0 , w0 ) ∈ M [i] for some x0 ∈ ṽ and w0 ∈ w̃.
f[S], we have that v 0 ∼ x0 —since they belong to the same equivalence class
Thanks to the construction of M
for ∼. Lemma 12 implies that there exists some w00 ∼ w0 such that (v 0 , w00 ) ∈ M [i] which implies that, by
f[i], i.e., M
f[i] is transitive.
transitivity of M [i], (u0 , w00 ) ∈ M [i]. Thus, (ũ, w̃) ∈ M
(4)
(1)-(4) prove the conclusion of the lemma.
2
The next lemma shows that a reduced state of an initial S5-state of a definite action theory is also an initial state of
the action theory.
f, s̃) be
Lemma 14 Let (M, s) be a S5-state such that every state u in M [S] is reachable from s. Furthermore, let (M
the reduced state of (M, s). It holds that
f, s̃) |= ψ;
1. (M, s) |= ψ iff (M
f, s̃) |= C(ψ);
2. (M, s) |= C(ψ) iff (M
f, s̃) |= C(Bi ψ);
3. (M, s) |= C(Bi ψ) iff (M
f, s̃) |= C(Bi ψ ∨ Bi ¬ψ));
4. (M, s) |= C(Bi ψ ∨ Bi ¬ψ) iff (M
f, s̃) |= C(¬Bi ψ ∧ ¬Bi ¬ψ));
5. (M, s) |= C(¬Bi ψ ∧ ¬Bi ¬ψ) iff (M
where i ∈ AG and ψ is a fluent formula.
Proof.
f[π](s̃) |= ψ (by construction of (M
f, s̃)) iff
1. (M, s) |= ψ iff M [π](s) |= ψ (by definition) iff M
f
(M , s̃) |= ψ.
53

2. (M, s) |= C(ψ) iff (M, u) |= ψ for each u ∈ M [S] (by Lemma 8 with respect to (M, s)) iff
f, ũ) |= ψ for each ũ ∈ M
f[S] (by construction of (M
f, s̃)) iff
(M
f
f
(M , s̃) |= C(Bi ψ) (by Lemma 8 with respect to (M , s̃)).
3. (M, s) |= C(Bi ψ) iff (M, u) |= ψ for each u ∈ M [S] (by Lemma 8 with respect to (M, s)) iff
f, ũ) |= ψ for each ũ ∈ M
f[S] (by construction of (M
f, s̃)) iff
(M
f, s̃) |= C(Bi ψ) (by Lemma 8 with respect to (M
f, s̃)).
(M
4. (M, s) |= C(Bi ψ ∨ Bi ¬ψ) iff
for every pair of u and v in M [S], (u, v) ∈ M [i] implies M [π](u) |= ψ iff M [π](v) |= ψ (by Lemma 9 with
respect to (M, s)) iff
f[S], u ∈ p̃ and v ∈ q̃, (p̃, q̃) ∈ M
f[i] implies M
f[π](p̃) |= ψ iff M
f[π](q̃) |= ψ
for every pair of states p̃ and q̃ in M
f
(by construction of (M , s̃)) iff
f, s̃) |= C(Bi ψ ∨ Bi ¬ψ) (by Lemma 9 with respect to (M
f, s̃)).
(M
5. (M, s) |= C(¬Bi ψ ∧ ¬Bi ¬ψ) iff
for every u ∈ M [S] there exists some v ∈ M [S] such that (u, v) ∈ M [i] and M [π](u) |= ψ and M [π](v) 6|= ψ
(by Lemma 10 with respect to (M, s)) iff
f[S] there exists ṽ ∈ M
f[S] such that (ũ, ṽ) ∈ M
f[i], M
f[π](ũ) |= ψ, and M
f[π](ṽ) 6|= ψ (by
for every ũ ∈ M
f
construction of (M , s̃)) iff
f, s̃) |= C(¬Bi ψ ∧ ¬Bi ¬ψ)) (by Lemma 10 with respect to (M
f, s̃)).
(M
2
Lemma 16 Let I be a set of initial statements of a definite action theory. Every S5-state satisfying I is equivalent to
a S5 state (M 0 , s0 ) such that |M 0 [S]| ≤ 2|F | .
f, s̃) is also
Proof. By Lemma 7, we can assume that every world in M is reachable from s. Lemma 14 shows that (M
|F |
f
a S5-state satisfying I. Obviously, |M [S]| ≤ 2 —since each interpretation M [π](v) can be modeled as a subset of
f[S]| is bounded by the number of distinct interpretations. We will show that the reduced state (M
f, s̃) of
F, and |M
(M, s) satisfies the conclusions of the lemma. To complete the proof, we need to show that for an arbitrary formula ϕ
and u ∈ M [S], the following holds:
(M, u) |= ϕ

iff

f, ũ) |= ϕ.
(M

(19)

We will prove (19) by induction over the depth of ϕ.
f, s̃).
• Base: depth(ϕ) = 0 implies that ϕ is a fluent formula. (19) is trivial from the construction of (M
• Step: Assume that (19) holds for ϕ with depth(ϕ) ≤ k. Consider the case depth(ϕ) = k + 1. There are four
cases:
– ϕ = Bi ψ. (M, u) |= ϕ iff for every v ∈ M [S] s.t. (u, v) ∈ M [i] we have that (M, v) |= ψ
f[S], (ũ, ṽ) ∈ M
f[i], (M
f, ṽ) |= ψ (inductive hypothesis and construction of (M
f, s̃))
iff for every ṽ ∈ M
f
iff (M , ũ) |= Bi ψ.
– The proof for other cases, when ϕ is either ¬ψ, φ ∨ ψ, or φ ∧ ψ is similar.
Since the truth value of group formulae of the form Eα ϕ and Cα ϕ is defined over the truth value of Bi ϕ for i ∈ α,
we can easily show that (19) holds for an arbitrary formula in the language LAG . This proves the lemma.
2
The above lemma leads to the following proposition.
Proposition 6 For each consistent definite action theory (I, D), there exists a finite number of initial S5-states
(M1 , s1 ), . . . , (Mk , sk ) such that every initial S5-state (M, s) of (I, D) is equivalent to some (Mi , si ). Furthermore,
for each pair of i 6= j and u ∈ Mi [S] there exists some v ∈ Mj [S] such that Mi [π](u) ≡ Mj [π](v).
54

Proof. Observe that the existence of a S5-state satisfying I is proved in [28]. By Lemma (16), we know that each
f, s̃). The proof of the first conclusion of the
initial S5-state (M, s) of (I, D) is equivalent to its reduced state (M
|F |
f
proposition follows trivially from the fact that |M [S]| ≤ 2 and there are only finitely many Kripke structures whose
size is at most 2|F | .
To prove the second conclusion of the proposition, let us recall the following definition given earlier: for s ∈ M [S],
let
^
^
state(s) =
f∧
¬f
f ∈F , M [π](s)(f )=>

f ∈F , M [π](s)(f )=⊥

Consider two initial S5-states (Mi , si ) and (Mj , sj ). Let us assume, by contradiction, that there is some u ∈ Mi [S]
such that for every v ∈ Mj [S], Mi [π](u) 6≡ Mj [π](v). Let ϕ = ¬state(u). Because of our assumption, we have
that ϕ is false in every state of Mj ; thanks to Lemma 8, we can infer that (Mj , sj ) |= C(Bi ϕ). Since (D, I) is a
definite action theory and (Mj , sj ) is an initial S5-state of (D, I), we have that “initially C(Bi ϕ)” is in I; on the
other hand, (Mj , u) |= ¬ϕ, which implies that (Mj , sj ) is not an initial S5-state of (I, D). The conclusion is proved
by contradiction.
2
Proposition 6 and the definition of complete definite action theories give raise to the following proposition.
Proposition 7 For a consistent and complete action theory (I, D), there exists a unique initial S5-state (M0 , s0 ) with
|M0 [S]| ≤ 2|F | such that every initial S5-state (M, s) of (I, D) is equivalent to some (M0 , s0 ).
Proof. Let us consider two arbitrary S5-states (M1 , s1 ) and (M2 , s2 ) satisfying I. From Lemma 16, we can assume
that both M1 and M2 are in reduced form; this guarantees that the number of worlds in M1 and M2 is bounded by the
number of possible interpretations, which is 2|F | . Furthermore, from proposition 6 we can also assume that M1 and
M2 have the same worlds (since each interpretation in M1 appears in M2 and vice versa, and no interpretation can be
associated to two distinct worlds, since the states are in reduced form).
For the sake of simplicity, let us assume that M1 [S] = M2 [S]; let us assume, by contradiction, that there are
u, v ∈ M1 [S] such that (u, v) ∈ M1 [i] and (u, v) 6∈ M2 [i]. Let ϕ = ¬state(v). Because of the construction, we can
see that (M2 , u) |= Bi ϕ. Since the two states are initial states for I, then this means that they are models of one of
the following formulate: C(Bi ϕ) or C(Bi ϕ ∨ Bi state(v)). On the other hand, since (u, v) ∈ M1 [i], it is easy to see
that (M1 , u) 6|= Bi ϕ and (M1 , u) 6|= Bi state(v). Thus, this contradicts the fact that both (M1 , s1 ) and (M2 , s2 ) are
models of I (a complete definite theory).
2

55

