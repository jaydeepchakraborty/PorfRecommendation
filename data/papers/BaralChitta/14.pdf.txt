Recognizing Social Constructs from Textual Conversation
Somak Aditya and Chitta Baral and Nguyen H. Vo and Joohyung Lee and Jieping Ye, Zaw Naung and Barry Lumpkin and Jenny Hastings Dept. of Computer Science, Arizona State University Richard Scherl Dept. of Computer Science, Monmouth University Dawn M. Sweet Dept. of Psychology, Iowa State University Daniela Inclezan Dept. of Computer Science, Miami University

Abstract
In this paper we present our work on recognizing high level social constructs such as Leadership and Status from textual conversation using an approach that makes use of the background knowledge about social hierarchy and integrates statistical methods and symbolic logic based methods. We use a stratified approach in which we first detect lower level language constructs such as politeness, command and agreement that help us to infer intermediate constructs such as deference, closeness and authority that are observed between the parties engaged in conversation. These intermediate constructs in turn are used to determine the social constructs Leadership and Status. We have implemented this system successfully in both English and Korean languages and achieved considerable accuracy.

chat corpuses (Shaikh et al., 2010), and developing a socio-cultural phenomena model from discourse with a small-scale implementation (Strzalkowski et al., 2010). Other researchers have focused on automatically annotating social behavior in conversation using statistical approaches (Mayfield et al., 2013). The discourse structure of a conversation is modeled as a Hidden Markov Model in (Stolcke, 2000) to determine dialogue acts such as Statement, Question and Agreement. In (Prabhakaran et al., 2012) annotated email threads are presented for facilitating detection of social relations. Among recent works, (Gilbert, 2012) uses linguistic cues to discover workplace hierarchy from emails. The use of phrases to detect language use such as "commands" is motivating. However, due to the lack of logical explanation and robust definition, the effectiveness of this method decreases in the semi-formally moderated Wikipedia community, which has interplay of several different LU s such as command, politeness and informal language. In (Danescu-Niculescu-Mizil et al., 2012), the authors explain how reflection of linguistic styles can shed light on power differentials; though, a social community like Wikipedia might not always conform to the lingustic style coordination assumption. For example, two friends who are coordinating on writing an article may have the same status socially, but difference in their expertise will drive the conversation. Other works such as (Gupte et al., 2011) have concentrated more on other features of the persons involved in social network, than linguistic cues. Also, we feel that, the hierarchy depends on the task or the context. In other words, one person could as-

1

Introduction and Related Works

The traditional information extraction paradigm has seen success in extracting simplistic behaviors or emotions from text. However, to detect high-level social constructs such as leadership or status, we require robustly defined notions about language constructs that cannot always be directly inferred from text. Hence, in this paper we focus on extracting information from text that requires additional background knowledge and inference. There are a few works in this direction, such as (Tari et al., 2010), however our focus in this paper is to extract information pertaining to different social constructs from textual conversation. The earlier research in analyzing conversations includes developing annotated 1293

Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1293­1298, Denver, Colorado, May 31 ­ June 5, 2015. c 2015 Association for Computational Linguistics

sume different roles in different context. The above works do not seem to address this. (Prabhakaran et al., 2012) achieves a commendable accuracy in detecting overt display of "power". However, by our definitions, this is a lower level attribute and is similar to authoritative behavior which is a lower level concept than Leadership or Status. Hence, their results are not directly comparable to ours. In this paper, we use a mixture of logic-based and statistical approaches which better encodes the domain knowledge and infers higher-level constructs from indirect textual cues. The aim of this paper is to formalize the theory behind our work, highlight the advantages of integration of statistical and logicbased approaches and present results from an empirical study.

fiers (basically rules) and probabilistic generative models (Medhat et al., 2014), (Hutto and Gilbert, 2014), (Vanzo et al., 2014) and (Saif et al., 2012). While their accuracy on some datasets is quite satisfactory, it is not clear how well they do on completely unseen data. From our experience on such classifiers, we believe that a higher level of accuracy with explainability can be achieved by imposing a structure that encodes background knowledge about the social hierarchy that is observed in nature. With this motivation, we built a system whose hierarchical architecture robustly defines the social constructs, the "hidden" concepts that induce them and their inter-connections. We define notions of intermediate Language Use (LU) and lower level Language Indicator (LI) categories1 . With the help of these robust definitions, our system properly explains how different emotions and behaviors interact to express status and leadership among individuals.

2

Motivation by a use-case

3

Social Constructs

{D: Put a message on the talk page of the guy who made it. You're right; g(t) should be g(tau - t), and (f*g)(t) should be (f*g)(tau). T: I don't think he has a talk page. He's provided the code so I can reproduce it. I think he's right with (f*g)(t), though? D: Actually, I think we're both wrong. You need a variable of integration running opposite directions .... T: I've updated ... I guess it's not important, but would be kind of cool. Feel free to suggest improvements to the animations.} As we understand, these conversations suggest that participant D is supposed to hold a higher rank/status than T . If we analyze manually, we understand that phrases like Put a message, You're right, I think we're both wrong together supports our conclusion. Considered separately, the above phrases might be misleading. To conclude the example, our system outputs : D has a higher status than T because D demonstrates more language uses associated with status than T. Confidence: high. The above example illustrates the degree of contextsensitivity of our problem. The current statistical literature suggests methods such as Decision Trees, Boosting methods comprising of a collection of Weak classi-

We start our discussion by presenting a use-case and explain how results of other traditional methods inspired us to come up with an integrated approach. Consider the following conversation from Wikipedia where the participants discuss about a misleading animation that is used under the topic Convolution.

Our framework supports determination of various important Social Constructs such as Leadership, Status, Group Cohesion and Sub-Group Formation. However, due to the length constraints of the paper, we will only discuss Leadership and Status.

3.1

Definitions and Architecture

We begin by first formally defining the two Social Constructs and the different Language Use categories. Leadership: A leader is someone who guides a group toward outcomes, controls the group actions, manages interactions between members and members usually recognize the leader. Status: Status is defined as the social position of one person with respect to another in a group. The principal Language Use categories that we detect are: Deference, Closeness, Authoritative Behavior and Motivational Behavior. The following intuitions are used to infer such LU s from text: Deference is understood when one uses language that shows respect to another conversational participant or defers to another's expertise or knowledge or authority. Closeness is understood when one uses language that shows familiarity with another conversationalist. It is also indicated by dialogues where conversationalists refer to similar events, experiences etc. Authoritative Behavior is understood when one uses language that shows power, dominance and control over a situation. Motivational Behavior is understood when one uses language that moves conversational participants toward
These definitions were proposed as part of the IARPA Socio-Cultural Content In Language(SCIL) program.
1

1294

B

Explanation:

A

Means that either the presence or absence of A contributes to B

B A

Means that only the presence of A indicates B

Social Constructs

Leader

Status

Language Uses Deference Closeness

Authority Behavior

Motivational Behavior

Language Indicators
Politeness Respectful Appellation Informal Language Informal Address Command Negative Expertise Agenda Setting Expertise Resource Allocation Agreement Criticism

· The signed LU s that contribute towards the SC Status are ordered based on their importance. We assume the following ordering exists: authoritative behavior > motivational behavior > negative deference > positive deference in the opposite direction > closeness. However, we do not assume such an ordering for the SC Leadership. Our extensive research and successful implementation of our system for different natural languages leads us to believe that these notions are universal in application.

Disagreement

Impoliteness

Apologetic Behavior

Indexical Statements

Praise

Encouragement

Seeking Support

5
Figure 1: Social Construct-Language Use-Language Indicator hierarchy for English Language sharing a common goal, collaboration, problem solving and solidarity. In Figure 1, we present the entire hierarchy and how the categories are connected among each other. The arrows in the figure show which of the LI categories are used to infer a particular type of LU. It also demonstrates how each of the LU contributes to the Social Constructs.

Fundamentals of the implementation

After we parse each sentence using Stanford Dependency parser to get the POS tags and mutual dependencies, the detection of individual LI s and the mapping of LI s, LU s to SCs are achieved using a combination of statistical and logic based approach. Many of the ideas and insights about the detection of LI s and their relations with the LU s are motivated from (Simon, 1946), (Pennebaker et al., 2003) , (Bernstein, 2010) , (Brown and Levinson, 1988) and a few others. Some of our ideas for textual inference have been inspired by (Scherl et al., 2010).

5.1

Determining the Language Indicators

4

Behind the Curtain: Our Intuitions

One of the fundamental contributions in this paper is formally describing the hierarchy to determine the Social Constructs, as shown in Figure 1. To come up with these interconnections and each of the different pieces of the puzzle, we went through an iterative process of discussions with many social scientists and linguists to analyze a large number of example conversations. In this process, we came up with the aforementioned hierarchy, definitions of SC, LU and LI s and most importantly, the following understanding: · The Language Indicators as shown in the Figure 1, suffice for the detection of Leadership and Status. · Each detected LI is associated with an Intensity Level that helps us to encode the dissimilar effects of different words in inferring LI s. · Each LI is associated with a Signed Language Use. For example, the LI politeness is associated with the signed LU positive deference. · Indicators of an LU with a certain sign are counterindicators of the same LU with the opposite sign. · A signed LU may contribute either favorably or unfavorably towards its associated SC. For example, positive authoritative behavior contributes favorably towards higher status.

The process of detection of language indicators from sentences uses a huge ensemble of complex rules. To create these rules, we borrowed ideas from the researchers of social science and psychology (Simon, 1946; Pennebaker et al., 2003). With the help of POS tags, mutual dependencies and regular expressions, we create a framework where we detect individual events, verbs, other sentence constituents and their positive and negative sense. On top of this framework, we use two different methods to detect language indicators. The ideas are similar for all the LIs. We will only present a few examples for the LI "Command".

5.1.1

Using Regular Expressions Alone

We use regular expressions of the form ".*\b[wW]hy don'?t (you|YOU) (start|read|submit|make|write|get)\s*\b.*" to detect LIs such as "Command". We employ a collection of such expressions to cover several different linguistic styles which indicates "Command" by an individual. We achieved a very high recall (close to 1.0) for most indicators with these rules on test data. However, in few cases, the frequency of such indicators (such as politeness) were very low deeming the set of regular expressions as incomplete. This observation led us to refine the regular expressions with Logical rules so that we can incorporate our domain knowledge and remove such bias to the training set.

1295

5.1.2

Using Logical rules on Regular Expression output and Sentence constituents

One example of the rules we use to detect "Command" is: if the subject of the verb is second person and the verb is associated with a modal verb which indicates a question that suggests command, then the LI "Command" is detected. Examples of such verbs are "Would you" and "Could you" etc. It is to be noted that such a verb will denote both politeness and command depending on the rest of the sentence. This fascinating inter-dependency is one reason why we have to collect all such Language Indicators before we infer the higher level Language Uses.

5.2

Mapping of LIs to LUs and LUs to Social Constructs

Input: To encode one conversation we use a collection of facts of the form participant(X) and addresses(X, Y, LI, Level). These facts essentially encode the identity of the participants and the Language Indicators observed in the overall conversation among a pair of participants. Output: The module outputs a collection of claim, evidence and confidence mappings. For example one such mapping is: claim_mapping(X, "is the leader", "because", X, "demonstrates <language use>","(Confidence: <confidence level>)"). Here <language use> is one of the language uses, <confidence level> is either low, medium, or high. Algorithm: We employ statistical and logic-based procedure in parallel to get the above output. On the statistical side, we adopt a regression technique to learn a function that can map the scores associated with LI s to individual LUs based on annotated training data and this function is then applied to test data to get confidence score on LU s. The same procedure is adopted for mapping LU s to SCs. In parallel to this procedure, we also employ a rulebased technique that uses quantized confidence scores and outputs confidence levels along with explanations. As we are able to get the explanation from logical reasoning, we use the output confidence scores as votes from statistical learning to output the final confidence level. The rules for logical reasoning are explained as definitions and intuitions in the following paragraphs. Mapping LIs into LUs: A signed LU is said to be exhibited by participant X towards participant Y with a certain degree of confidence based on the number of indicators(LI ) and counter-indicators(LI ) of the signed LU used by X when addressing Y. The confidence in LU is directly proportional to the difference between the number of indicators and counter-indicators.

We categorize LU s according to the number of indicators and apply slight variation to the above rules for each such category. Also, there are a few LI s that, when used, automatically override the computed confidence level for an LU and increase it to high. For example, "criticism" increases confidence level of positive "motivational behavior" to high. Mapping LUs to SCs: The relative status of two participants is determined based on i) the number of relevant signed LUs exhibited by each participant towards the other, ii) the ordering of relevant signed LUs and iii) the confidence level in each exhibited signed LU. The leader is determined based on the number of exhibited relevant LUs (both favorable and unfavorable). Mapping LIs to SCs: As shown in Figure 1, we directly associate some of the LIs to Social Constructs. For such an association, we again adopt the regression technique mentioned previously. In this case, the confidence scores from LI s are directly mapped to the confidence scores of SCs. We combine this confidence with the above confidence levels using simplistic rules to output final social constructs. It should be noted that the constants used in the rules are obtained from statistics on annotated conversations. The annotation process involves labels about SCs, LUs and LIs for each conversation data.

5.3

Brief Details and Results of the Regression Technique

In this sub-section, we provide few details of the Sparse Logistic Regression technique we have used alongside the logical formulation and present few results from our experiments with relevant statistical methods. We have used a similar formulations for mapping LIs to LUs and LUs to SCs. Here, we provide the example of formulating the entire problem of detection of Social Constructs directly in the Classification paradigm. Status and Leadership can be formulated as a threeclass and two-class problem respectively. For Status, we had 102 samples with the 38(higher), 26(equal) and 38(lower) samples each for three classes. For Leadership, we had 149 samples with 108(not-leader) and 41(leader) samples for the two classes. For both the tasks, we extracted 28 textual features. We used the one-vs-rest scheme for multi-class problem. For each task, we evaluated the framework as follows: i. First, we randomly separate the dataset into training set(p) and test set(1-p). ii. In the training set, we use 10-fold cross validation to select proper parameters. iii. We iterate the above procedure for 100 times, and accuracy is evaluated on the predictions in all iterations. iv. We select different p (from 0.25 to 0.9) and observe the change of accuracy. We compared the accuracy achieved using Sparse Logistic Regression with SVM(with RBF Kernel) among

1296

is perhaps unique in determining such social constructs and evaluating on familiar and unfamiliar datasets. Table Table 1: Results
SC Task Leader Task Leader Status Status Task Leader Status Q-Type Y-N List Y-N List Y-N Y-N Language EN EN EN EN KO KO Accuracy 0.8900 0.6700 0.4700 0.6923 0.5667 0.4074 F1 0.6700 0.9900 0.3457 0.5200 0.4338 0.3900

(a)

(b)

Figure 2: (a) Training set percentage vs Accuracy graph for Leadership problem, (b) Training set percentage vs Accuracy graph for Status classification problem. others. The accuracy comparison of the SVM(with RBF kernel) and sparse Logistic Regression is provided in Figure 2. As we can observe, though the two methods are comparable, in most cases Sparse Logistic regression performs better.

5.4

Advantages from the integrated approach

The primary advantages are the following: In general, statistical approaches need a "lot of data" to attain a certain level of accuracy. As the rules we use are quite universal and compact, we can achieve a comparable(or higher) accuracy with much less training data. Using the evidence and claim mappings, we give an "explanation" as to why we detected such a particular SC in the dialogue. Knowldege of such depth is very hard to achieve with only statistical approaches. Explicit representation of "context" specific information via rules results in improved accuracy in detection of LIs such as criticism, praise, command etc. Statistical modules complement the rule-based approach where our domain knowledge is "incomplete". We use ASP as the Logic Programming language of our choice as its ability to represent defaults and exceptions eases the implementation procedure.

1 reports evaluations on wikipedia dump. These values are computed by comparing the results of our systems with annotated data. Note, in our experiments, we have performed strict evaluations. For example, the results are only marked positive if the complete list of leaders matches with a human-annotated list. Also, we consider the "explanation" too while performing the evaluation. The results are true positive only when the detected construct is correct alongwith the explanation provided by the reasoning module. In general, the previous research achieves an accuracy of 0.45 in comparable tasks such as dialog act tagging (Stolcke, 2000).

7

Conclusion

6

Results

In this paper, we have proposed a novel approach for logically recognizing social constructs from textual conversations. We have used both statistical classification and logical reasoning to robustly detect status and leadership as observed in virtual social networks. From our experiments, we show empirically how our approach achieves a significant accuracy and provides logical explanation of construct detection. This research shows the merits of using logical rules along with statistical techniques to determine Social Constructs. As per our understanding, this level of accuracy and explainability needs integration of both statistical and logic based methods. Our observations suggest that there is an increasing need for such integration in various domains. We believe that this work is one of the early steps in that direction.

We have implemented this system using ASP(Gelfond and Lifschitz, 1988) and Java. The Wikipedia conversations are obtained by parsing the wiki dump from http://dumps.wikimedia.org/. We also evaluated on the NWTRB (US Nuclear Waste Technical Review Board) dataset. The accuracy and F1 measure are summarized in Table 1 for approximately two thousand English and one thousand Korean Wikipedia conversations. We evaluated two types of questions - i. Yes-No indicates questions like Is John the leader? and ii. List indicates questions such as List all the leaders.. Our work

8

Acknowledgement

We thank the IARPA SCIL program for supporting this research. We also thank NSF for the DataNet Federation Consortium grant OCI-0940841 and ONR for their grant N00014-13-1-0334 for partially supporting this research.

1297

References
[Bernstein2010] Basil Bernstein. 2010. A public language: some sociological implications of a linguistic form. British Journal of Sociology, pages 53­69. [Brown and Levinson1988] Penelope Brown and STEPHEN C. Levinson. 1988. Politeness: Some Universals in Language Usage (Studies in Interactional Sociolinguistics 4). Cambridge University Press. [Danescu-Niculescu-Mizil et al.2012] Cristian DanescuNiculescu-Mizil, Lillian Lee, Bo Pang, and Jon Kleinberg. 2012. Echoes of power: Language effects and power differences in social interaction. In Proceedings of the 21st International Conference on World Wide Web, WWW '12, pages 699­708, New York, NY, USA. ACM. [Gelfond and Lifschitz1988] Michael Gelfond and Vladimir Lifschitz. 1988. The stable model semantics for logic programming. pages 1070­1080. MIT Press. [Gilbert2012] Eric Gilbert. 2012. Phrases that signal workplace hierarchy. In Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work, CSCW '12, pages 1037­1046, New York, NY, USA. ACM. [Gupte et al.2011] Mangesh Gupte, Pravin Shankar, Jing Li, S. Muthukrishnan, and Liviu Iftode. 2011. Finding hierarchy in directed online social networks. In Proceedings of the 20th International Conference on World Wide Web, WWW '11, pages 557­566, New York, NY, USA. ACM. [Hutto and Gilbert2014] C. J. Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In ICWSM. [Mayfield et al.2013] Elijah Mayfield, David Adamson, and Carolyn Penstein Rosé. 2013. Recognizing rare social phenomena in conversation: Empowerment detection in support group chatrooms. pages 104­113. [Medhat et al.2014] W. Medhat, A. Hassan, and H. Korashy. 2014. Sentiment analysis algorithms and applications: A survey. Ain Shams Engineering Journal, 5(4):1093 ­ 1113. [Pennebaker et al.2003] James W. Pennebaker, Matthias R. Mehl, and Kate G. Niederhoffer. 2003. Psychological aspects of natural language use: Our words, our selves. Annual Review of Psychology, 54(1):547. [Prabhakaran et al.2012] Vinodkumar Prabhakaran, Huzaifa Neralwala, Owen Rambow, and Mona Diab. 2012. Annotations for power relations on email threads. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12), Istanbul, Turkey, may. European Language Resources Association (ELRA).

[Saif et al.2012] Hassan Saif, Yulan He, and Harith Alani. 2012. Semantic sentiment analysis of twitter. In Proceedings of the 11th International Conference on The Semantic Web - Volume Part I, ISWC'12, pages 508­ 524, Berlin, Heidelberg. Springer-Verlag. [Scherl et al.2010] R. Scherl, D. Inclezan, and M. Gelfond. 2010. Automated inference of socio-cultural information from natural language conversations. In IEEE International Conference on Social Computing, pages 480­487, Aug. [Shaikh et al.2010] Samira Shaikh, Tomek Strzalkowski, Aaron Broadwell, Jennifer Stromer-Galley, Sarah Taylor, and Nick Webb. 2010. Mpc: A multi-party chat corpus for modeling social phenomena in discourse. In Proceedings of the Seventh International Conference on LREC, may. [Simon1946] Herbert A. Simon. 1946. The proverbs of administration. Public Administration Review, 6(1):53­67. [Stolcke2000] Andreas Stolcke. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. [Strzalkowski et al.2010] Tomek Strzalkowski, George Aaron Broadwell, Jennifer Stromer-Galley, Samira Shaikh, Sarah M. Taylor, and Nick Webb. 2010. Modeling socio-cultural phenomena in discourse. In COLING 2010, 23rd International Conference on Computational Linguistics, pages 1038­1046. [Tari et al.2010] Luis Tari, Saadat Anwar, Shanshan Liang, James Cai, and Chitta Baral. 2010. Discovering drug-drug interactions: a text-mining and reasoning approach based on properties of drug metabolism. Bioinformatics, 26(18). [Vanzo et al.2014] Andrea Vanzo, Danilo Croce, and Roberto Basili. 2014. A context based model for sentiment analysis in twitter. In Proceedings of COLING 2014, pages 2345­2354, Dublin, Ireland. Dublin City University and Association for Computational Linguistics.

1298

