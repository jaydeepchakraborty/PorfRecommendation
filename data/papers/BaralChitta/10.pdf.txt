(Presented in 6th Multidisciplinary Workshop on Advances in Preference Handling, at ECAI 2012, Aug 2012)

A Non-Monotonic Goal Specification Language for
Planning with Preferences
Tran Cao Son⇤ and Enrico Pontelli⇤ and Chitta Baral+ 1
Abstract. This paper introduces a default logic based approach to
defining goal specification languages that can be non-monotonic and
allow for the specification of inconsistencies and priorities among
goals. The paper starts by presenting a basic goal specification language for planning with preferences. It then defines goal default theories (resp. with priorities) by embedding goal formulae into default
logic (resp. prioritizing default logic). It is possible to show that the
new language is general, as it can express several features of previously developed goal specification languages. The paper discusses
how several other features can be subsumed by extending the basic
goal specification language. Finally , we identify features that might
be important in goal specification that cannot be expressed by our
language.

1

Introduction

An important component of autonomous agent design is goal specification. In classical planning, goals deal with reaching one of a
particular set of states. Nevertheless, often goals of agents are not
just about reaching a particular state; goals are often about satisfying desirable conditions imposed on the trajectory. For example, a
person can have the following desire in preparing travel plans to conferences:
(*) I prefer to fly to the conference site (since it is usually too
far to drive).
The user’s preference restricts the means that can be used in achieving her goal of reaching the conference site, which leads to the selection of a plan that reaches the conference site by airplane, whenever
possible. Ultimately, this affects what actions the person should take
in order to achieve the goal.
These observations led to the development of languages for the
specification of soft goals in planning, e.g., PP introduced in [14]
and modified in [6]. In PP, a basic desire is a temporal formula
describing desirable properties of a plan. Atomic and general preferences are particular classes of formulae built over basic desires. A
preference formula defines a preference order
among the trajectories that achieve the hard goal of the problem, i.e., for every pair
of trajectories ↵ and , ↵
indicates that ↵ is preferable to .
is often a partial order and its definition relies on the notion of
satisfaction between trajectories and a preference specification. Similar ideas have been considered in the planning community and led
to extensions of the planning domain description language PDDL,
with features for representing classes of preferences over plans using
temporal extended preferences (e.g., [10]).
1 ⇤ Department

of Computer Science New Mexico State University, Las
Cruces, New Mexico, USA, email: tson|epontell@cs.nmsu.edu
and + Department of Computer Science and Engineering, Arizona State
University, Tempe, Arizona, USA, email: chitta@asu.edu

In [4], the authors argue that a goal specification language should
be non-monotonic for various reasons, such as elaboration tolerance
and simplicity of goal specification. For example, the same traveler
with the preference (*) would probably not mind driving at most
three hours to the conference site if the only flight to the destination requires to travel the day before the conference starts. In this
case, her preference becomes:
(**) Normally, I prefer to fly to the conference site (since it is usually too far to drive). However, if there are no flights on the same day
of the conference and the driving time is at most three hours, then I
will drive.
To address this issue, an extension of LTL [11], called N-LTL, has
been proposed, allowing weak and strong exceptions to certain rules.
A weakness of this language is that it requires the classification of
weak and strong exceptions when a goal is specified. In [5], the language ER-LTL is introduced to address this limitation of N-LTL.
Similarly to PP, the semantics of N-LTL and ER-LTL relies on the
notion of satisfaction between plans and N-LTL or ER-LTL specifications. Observe that the issue of non-monotonicity is dealt within
PP and in the extensions of PDDL by revising the soft goals, which
is an approach that N-LTL specifically tries to avoid.
We observe that the focus of the work in [1, 4, 5, 6, 10] is on
classical planning, i.e., the planning domains are deterministic and
the initial state is complete, while the work in [14] considers nondeterministic domains and only discusses preferences among weak
plans. In [2], it is argued that plans for non-deterministic domains
should be policies (i.e., a partial function from the set of states to
the set of actions) and the language ⇡-CTL⇤ is developed for specifying goals in non-deterministic domains. ⇡-CTL⇤ is an extension
of CTL⇤ [9] with two modalities A⇡ and E⇡ for considering all or
some trajectories w.r.t. a given policy. In [3], the language ⇡-CTL⇤ is
extended with quantifiers over policies to increase its expressiveness.
Policies satisfying a goal specification are viewed as the solutions of
a planning problem.
In this paper, we explore an approach based on prioritizing default
logic for defining a goal specification language. The new language,
called goal default theories with priorities, is a variation of prioritizing default logic, in which formulae occurring within a default can
be temporal extended preference formulae. We show that the core of
the new language subsumes several features from existing goal languages and can be extended to subsume several other features from
other goal languages. Finally, we discuss the possible applications
of the new language in the study of existing goal languages and the
development of new ones.

2

Background

In this section, we briefly review the basic definitions of planning,
linear temporal logic (LTL) and its extension for specifying prefer-

ences in planning.

2.1

LTL and Temporal Extended Preferences

Let L be a propositional language. By hpi we denote a propositional
formula from L. LTL-formulae are defined by the following syntax
hf i

::=

hpi | hf i ^ hf i | hf i _ hf i |
¬hf i | hf i | 2hf i | 3hf i | hf iUhf i

(1)

The semantics of LTL-formulae is defined with respect to sequences
of interpretations of L. For later use, we will refer to an interpretation
of L as a state and a possibly infinite sequence of interpretations of
L, s0 , s1 , . . ., as a trajectory. For a trajectory = s0 , s1 , . . . , by i
we denote the suffix si , si+1 , . . . of . A trajectory = s0 , s1 , . . .
satisfies an LTL-formula f , denoted by |= f , if 0 |= f where
• j |= p iff sj |= p
• j |= ¬f iff j 6|= f
• j |= f1 ^ f2 iff j |= f1 and j |= f2
• j |= f1 _ f2 iff j |= f1 or j |= f2
• j |= f iff j+1 |= f
• j |= 2f iff k |= f , for all k j
• j |= 3f iff i |= f for some i j
• j |= f1 U f2 iff there exists k j such that
k |= f2 and for all i, j  i < k, i |= f1 .
A finite trajectory s0 , . . . , sn satisfies an LTL-formula f if its extension s0 , . . . , sn , sn+1 , . . . satisfies f , where sk = sn for k > n.
In order to deal with planning problems, LTL is extended with the
following constructs
at end hpi | hpi sometime before hpi |
hpi sometime after hpi

(2)

Formulae of the extended LTL are referred to as Temporal Extended
Preferences (TEP). Note that the last two are syntactic sugars for LTL
formulae. Temporal extended preferences are interpreted over finite
trajectories. The notion of satisfaction for standard LTL-formulae is
defined as above, while satisfaction of TEP formulae is as follows:
given a finite trajectory = s0 , . . . , sn :
• |= at end p iff sn |= p;
•
|= p1 sometime before p2 iff for every i, 0  i  n, if
i |= p1 then j |= p2 for some i  j  n; and
• j |= p1 sometime after p2 iff for every i, 0  i  n, if
i |= p1 then j |= p2 for some 0  j < i  n.

2.2

Planning

In this paper, we describe a dynamic domain as a labeled transition
system T = (F, A, S, L), where:
• F is a set of fluents (or propositions),
• A is a set of actions,
• S is a set of interpretations (or states) of F , and
• L ✓ S ⇥ A ⇥ S.
Each triple hs1 , a, s2 i 2 L indicates that the execution of the action
a in the state s1 might result in the state s2 . T is deterministic if for
each state s and action a, L contains at most one triple hs, a, s2 i;
otherwise, T is non-deterministic.
Given a transition system T , a finite or infinite sequence
s0 a0 s1 a1 . . . sn an sn+1 . . . of alternate states and actions is called
a run if hsi , ai , si+1 i 2 L for every i = 0, . . . A policy ⇡ in a transition system T is a partial function ⇡ : S ! A from the set of states

to the set of actions. A run s0 a0 s1 a1 . . . sk ak sk+1 . . . is said to be
induced by a policy ⇡ if ai = ⇡(si ) for every i = 0, . . . , k, . . .
Definition 1. A planning problem is a triple hT, Si , Sf i where T =
(F, A, S, L) is a transition system, Si ✓ S is the set of initial states,
and Sf ✓ S is the set of final states.
Intuitively, a planning problem asks for a plan which transforms
the transition system from any state belonging to Si to some state in
Sf . In the rest of the discussion, we assume Si and Sf to be finite
sets. We distinguish two classes of planning problems:
Deterministic planning: in this case, T is deterministic and a solution (or plan) of hT, Si , Sf i is an action sequence [a0 ; . . . ; an ]
such that, for every s0 2 Si , s0 a0 s1 a1 . . . an sn+1 is a run in T
and sn+1 2 Sf ;
Non-deterministic planning: in this case, T is non-deterministic
and a solution (or plan) of hT, Si , Sf i is a policy ⇡ such that, for
every s0 2 Si and every run induced by ⇡ in T , ⇡ is finite and is
of the form s0 a0 s1 a1 . . . sk ak sk+1 where sk+1 2 Sf .
In the following, whenever we refer to a possible plan in a transition
system T , we mean a sequence of actions (resp. a policy) if T is deterministic (resp. non-deterministic) that can generate a correct run.
Let us illustrate these basic definitions using the following simple
example.
Example 1. Consider a transportation robot. There are different locations, say l1 , . . . , lk , whose connectivity is given by a graph and
there might be different objects at each location. Let O be a set of objects. The robot can travel between two directly connected locations.
It can pick up objects at a location, hold them, drop them, and carry
them between locations. We assume that, for each pair of connected
locations li and lj , the robot has an action ai,j for traveling from li
to lj . The robot can hold only one object at a time. The domain can
be represented by a transition system T1 = (F, A, S, L):2
• F contains the following types of propositions:
at(i) denotes that the robot is at the location li ;
o at(o, i) denotes that the object o is at the location li ;
h(o) denotes that the robot is holding the object o.
• A contains of the following types of actions:
ai,j the robot moves from li to lj ;
release(o) the robot drops the object o;
pickup(o) the robot picks up the object o.
• S contains the interpretations of F which satisfy the basic constraints, such as the robot is at one location at a time, it holds only
one object, etc.
• L contains transitions of the form hs, a, s0 i such that s0 is the
result of the execution of a in s; for example, if a = ai,j and
at(i) 2 s then s0 = s \ {at(i)} [ {at(j)}.
T1 is a deterministic transition system. We will also refer to T2 as
the non-deterministic version of T1 by defining T2 = (F, A, S, L0 )
where L0 = L[{hsi , ai,j , si i | ai,j 2 A} and at(i) 2 s. Intuitively,
T2 encodes the fact that the action ai,j might fail and, when it does,
the robot will stay where it was after the execution of ai,j .
A planning problem P in this domain is given by specifying the
initial location of the robot and of the objects and the final location of the robot and of the objects. It is deterministic (resp. nondeterministic) if T1 (resp. T2 ) is considered.
For example, Pi = hTi , {{at(1)}}, Sf i where for each s 2 Sf ,
at(k) 2 s is a planning problem for Ti . A solution for P1 is a sequence [a1,2 ; . . . ; ak 1,k ]. On the other hand, a solution for P2 is a
policy ⇡ defined by ⇡(s) = at,t+1 iff at(t) 2 s for t < k.
2

We simplify the definitions of S and L for readability.

3

A Basic Goal Specification Language for
Planning with Preferences

In the literature, a planning problem with preferences is defined
as a pair (P, ) of a planning problem P = hT, Si , Sf i, where
T = (F, A, S, L), and a preference formula in a goal specification language. A plan of P is called a preferred plan if it is a plan
for P and satisfies , where the notion of satisfaction of a preference
formula by a plan is language dependent.
In general, we can characterize a goal specification language G
over a transition system T by a set of preference formulae F and a
satisfaction relation |=G between the set of possible plans of T and
formulae in F . We will write |=G
to denote that the plan
satisfies the formula under the language G.
For later use, we will define a basic goal specification language for
a transition system T = (F, A, S, L), written as Gb = (Fb , |=Gb ), as
follows:
• the set of preference formulae Fb is the set of TEP-formulae over
F [ A, and
• for a planning problem P = hT, Si , Sf i, |=Gb is defined as follows:
if T is deterministic, a plan = [a0 , . . . , an ] for a planning
problem P is said to satisfy a formula
in Fb if for every s0 2 Si , s0 a0 s1 a1 . . . an sn+1 is a run in T and (s0 [
{a0 }), . . . , (sn [ {an }), sn+1 is a trajectory satisfying (in
the TEP-language over F [ A);
if T is non-deterministic, a solution (policy) ⇡ for P is said
to satisfy a formula
in Fb if for every s0 2 Si and every run s0 a0 s1 a1 . . . sk ak sk+1 in T induced by ⇡, (s0 [
{a0 }), . . . , (sn [ {an }), sn+1 is a trajectory satisfying (in
the TEP-language over F [ A).
In the following, we will assume that any goal specification language
G is a conservative extension of Gb , i.e., (i) G contains all formulae
in Gb ; and (ii) for every planning problem P and a formula in G, if
2 Gb and |=Gb with respect to Gb then |=G with respect
to G.
Example 2. Some preference formulae in Gb for the transition systems in Ex. 1 are:
• 3at(2): the robot should visit the location l2 during the execution of the plan;
• at(1)^3at(2): the robot must (i) start in a state satisfying at(1)
(or the robot is at the location l1 initially); and (ii) visit the location l2 at some point during the execution of the plan;
W
• 2[at(2) ) ( i6=2 a2i )]: whenever the robot visits l2 , it should
leave that location immediately by executing an action going to
one of its neighbors;
• h(o) )
¬h(o): if the robot holds an object o in the initial
state then it should release o after the execution of one action;
• 2[h(o) )
¬h(o)]: whenever the robot holds an object o
it should release o after the execution of an action;
• h(o) sometime before at(5): whenever the robot holds the
object o, it must visit the location l5 thereafter before reaching
the goal;
V
• at end [ o2O ¬h(o)]: at the end, the robot should not hold any
object.
2
With a slight abuse of notation, let us view a state s as a formula

V

s |= f

f^

V

s |= ¬f

¬f . Let Si and Sf be two sets of states and

2 _

6
=4

s2Si

s ^ at end [

| {z }
1

|

_

s2Sf

{z

2

s]
}

3
7
5

It is easy to see that any plan satisfying requires its execution to
start from a state satisfying 1 , which is one of the states in Si , and
end in a state satisfying 2 , which is one of the states in Sf . For this
reason, the description of the initial and final states can be folded into
a preference formula. We will therefore define planning problems as
follows.
Definition 2. Given a transition system T and a goal specification
language G = (F , |=G ) over T , a goal formula in F is called a
planning problem. A solution of is a plan in T such that |=G .
By Def. 2, a goal formula represents a planning problem. The literature is quite diversified when a user faces two or more goal formulae which are contradictory with each other. ForV
example, the formula 3at(2) is contradictory with 2¬at(2); 2¬( o2O h(o)) conflicts with 3h(o1 ); etc. A possibility is to consider a possible plan
as solution if it satisfies some goal formulae. Another possibility is
to rank the goal formulae and identify solutions as plans that satisfy
the formula with the highest possible ranking. In the following, we
will show that a uniform framework for dealing with conflicting goal
formulae can be obtained by embedding goal formulae into Reiter’s
default logic.

4

Goal Default Theories

In this section, we will introduce a new goal specification language,
called goal default theory. A goal default theory is a variation of Reiter’s default theory [12], whose defaults can contain preference formulae. Goal default theories provide a possible treatment of planning
with multiple goal formulae.
A goal default theory is defined over a transition system T =
(F, A, S, L) and a goal specification language G = (F , |=G ).
Given a goal specification language (F , |=G ), we say that two formulae ', in F are equivalent w.r.t. |=G if, for each plan of T ,
we have that |=G ' , .3 We can easily extend this notion to
define the notion of logical consequence w.r.t. |=G —if S is a set of
formulae from F and f is another formula
in F , then S |=G f if for
V
each plan of T we have that |=G '2S ' implies |=G f . Given
a set of formulae S, we define Decl(S) = {' | ' 2 F , S |=G '}.
A preference default (or p-default) d over G is of the following
form
↵ :
(3)
where ↵, , and are formulae in F . We call ↵ the precondition,
the justification, and the consequence of d, and we denote them
with prec(d), just(d), and cons(d), respectively. A default d is said
to be
• Normal if its justification is equivalent to its conclusion;
• Prerequisite-free if its precondition is equivalent to true; and
• Supernormal if it is normal and prerequisite-free.
Given a set of formulae S from F , a default d is said to be defeated
in S if S |= ¬just(d). Some preferences and their representation as
p-defaults over Gb for the domain from Example 1 are given next.
Example 3. In these examples, o denotes a particular object in the
domain.
3

',

is a shorthand for (' ^ ) _ (¬' ^ ¬ ).

• If there is no evidence that the robot is initially at the location l2 ,
then it should go to l2 :
> : ¬at(2)
3at(2)

(4)

• Assume that objects might be defective, represented by the proposition def ective. We can write
> : 2[¬defective(o)]
2[at(2) ) h(o)]

(5)

to indicate that normally, we would like that the robot holds the
object o whenever it is at the location l2 . An exception to this rule
is possible if the object o is defective.
• If the robot is not required to hold the object o in the final state
and there is no evidence that it initially holds o, then it should not
execute the action of picking up the object o:
>

:

at end (¬h(o)) ^ ¬h(o)
2[¬pickup(o)]

(6)

• If there is no evidence that the object o is initially in the wrong
place then the robot should not start by executing the action of
picking up the object o:
V
at end (o at(o, i)) :
i6=j ¬o at(o, j)
(7)
¬pickup(o)
• A stronger version of (7) is
at end (o at(o, i)) :

V

i6=j

2¬pickup(o)

¬o at(o, j)

(8)

indicates that the robot should never pick up the object o if o could
already be in the desired final location.
• If there is the possibility that the robot might reach location l2 ,
then it must leave the location immediately after its arrival at l2 .
> : 3[at(2)]
W
2[at(2) )
i6=2 a2,i ]

(9)

satisfies either or . The following result generalizes this observation.
Proposition 1. Let T = (F, A, S, L) be a transition system, G =
(F , |=G ) be a goal specification language, and = { 1 , . . . , n }
be a set of preference formulae in F . Furthermore, let
⌃

• If there is no evidence that an object o will ever appear in location i then the robot should never go there.
> : 2[¬o at(o, i)]
W
2[ j6=i ¬aj,i ]

F [A is a p-default, and any Reiter’s default theory over the language
F [ A is a goal default theory.
Definition 5. Given a transition system T = (F, A, S, L) and a goal
specification language G = (F , |=G ) over T , a planning problem
over T and G is a goal default theory ⌃ = (D, W ) over G and T .
The notion of a solution to a planning problem is modified as follows.
Definition 6. Given a transition system T = (F, A, S, L), a goal
specification language G = (F , |=G ) over T , and a planning problem ⌃ over T and G, a solution of ⌃ is a plan in T such that
|=G E for some extension E of ⌃.
Some planning problems over the transition systems in Exp. 1 and
the language Gb are given in the next example.
Example 4 (Continuation of Example 3). • Let
⌃1
=
({p1 }, {at(1), at end at(5)}) where p1 is the default (4).
Intuitively, we have that ⌃1 identifies plans where the robot starts
at location l1 , goes through the location l2 , and ends in location
l5 .
• Let ⌃2 = ({p6 }, {at(1), at end at(5)}) where p6 is the default
(9). This identifies plans where the robot starts at location l1 , ends
in location l5 , and either (i) never goes through the location l2 ; or
(ii) never stays in the location l2 within two consecutive steps. 2
The planning problems in Example 4 are simple, in that they are
specified by goal default theories whose set of defaults is a singleton.
Let us consider a more complicated example. Assume that we have
two temporal formulae and such that there exists no plan that can
satisfy both and . In this case, the use of goal default theory as a
goal formula comes in handy. Indeed, every solution of the planning
problem expressed by the goal default theory
✓⇢
◆
>:¬
>:¬
⌃ , =
,
,;
(11)

(10)

In the following, we will refer to the p-defaults in (4)-(9) by
p1 , . . . , p6 , respectively.
2
We next define the notion of a goal default theory.
Definition 3. A goal default theory over a goal language G =
(F , |=G ) and a transition system T is a pair ⌃ = (D, W ) where
D is a set of p-defaults over G and W ✓ F .
Given a set of p-defaults D, we denote with cons(D) the set
cons(D) = {cons(d) | d 2 D}. A p-default d is applicable w.r.t. a
set of F formulae S if S |=G prec(d) and S 6|=G ¬just(d). Let us
denote with ⇧D (S) the set of p-defaults from D that are applicable
w.r.t. S.
Definition 4 (From [12]). Let ⌃ = (D, W ) be a goal default theory
over G = (F , |=G ) and T . An extension of ⌃ is a minimal set E ✓ F
that satisfies the condition E = Decl(W [ Cons(⇧D (E))). We say
that ⌃ is consistent if it has at least one extension.
From this definition, any default over the propositional language

=

> :

2

,;

(12)

• For every solution to the problem ⌃ there existsVa maximal
(w.r.t. ✓) set of preferences
✓ such that |=G
;
2
0 or
• For every pair of solutions and 0 of ⌃ , either
=
0 and
0 6✓
6✓
.

5

Goals Default Theories with Priorities

Proposition 1 shows that goal default theories can be used to specify planning problems with multiple preferences which might not be
consistent with each other. For instance, consider a traveler from New
York to San Francisco who has two preferences: reach the destination
as fast as possible ( 1 ) and spend the least amount of money ( 2 ). In
general, these two preferences cannot be satisfied at the same time. In
this case, it is more reasonable to assume that a plan satisfying one of
the criteria is an acceptable solution. Thus, ⌃{ 1 , 2 } is a reasonable
goal specification if the traveler is impartial about 1 and 2 . On
the other hand, if the traveler prefers 1 over 2 (or vice versa), we
will need to change the goal specification or provide additional ways
for the traveler to specify this priority. As it turns out, the literature
is rich with approaches for adding priorities to default theories [7, 8]
which can be easily adapted to goal default theories. We next define

goal default theories with priorities by adapting the work of [7] to
goal default theories.
Let us start by introducing static priorities, encoded by a wellordering relation among p-defaults—i.e., is transitive, irreflexive, and each set of elements admits the least element in the ordering.
We denote with min (X) the least element of X with respect to .
We define goal default theory with priorities as follows.
Definition 7. A goal default theory with priorities over a goal language G = (F , |=G ) and a transition system T is a triple (D, W, )
where D is a set of p-defaults over G, is a well-ordering relation
over D, and W ✓ F .
Following the general design of prioritizing default theory [7], the
notion of preferred extension can be defined by successively simplifying the structure of the defaults.
Let us identify a construction of preferred extension through the
application of defaults according to the ordering imposed by . Let
us introduce the PR operator which computes the next “preferred”
set of goal formulae from an existing one:
• PR (S) = Decl(S [ {cons(d)})
if ⇧⇤D (S) 6= ; ^ d = min ({x | x 2 ⇧⇤D (S)});
• PR (S) = S if ⇧⇤D (S) = ;
where ⇧⇤D (S) = {d | d 2 ⇧D (S), S 6|= cons(d)}. If the elements in D (for a goal default theory (D, W )) are supernormal,
then it is possible to use PR to produce a monotone sequence
of goal formulae, by setting S0 = Decl(W ), Si+1
S = PR (Si )
for any successor ordinal i + 1 and Si = Decl( ji Sj ) for any
limit ordinal i. WeSwill denote the result of this construction as
P ref (D, W ) = i 0 Si .
The process of determining a preferred extension will apply
P ref on a reduced version of the theory, in a style similar to that
used in the Gelfond-Lifschitz reduct. Following the model proposed
in [7], the reduct of a goal default theory with priorities (D, W, )
w.r.t. a set of goal formulae S, denoted (DS , W, S ), is obtained as
follows:
: just(d)
• Determine D0 = { > cons(d)
| d 2 D, S |=G prec(d)}

• Determine DS = {d 2 D0 | cons(d) 62 S or S 6|=G ¬just(d)}
and S is such that d01 S d02 if d1
d2 and d1 (d2 ) is the
-least element that introduced d01 (d02 ) in D0 .
We define preferred extensions as follows.
Definition 8. Let (D, W, ) be a goal default theory with priorities
over G = (F , |=G ) and T . A preferred extension E of (D, W, ) is
a set of goal formulae in F such that E is an extension of (D, W )
and E = P ref E (DE , W ).
Similar to [7], we can generalize the above definitions and define
(i) a goal default theory with priorities as a triple (D, W, ) where
(D, W ) is a goal default theory and is a partial order among defaults in D; and (ii) a set of formulae E is a preferred extension
of (D, W, ) if it is a preferred extension of some (D, W, E ) for
some well-ordering E which is an extension of . For brevity, we
omit the precise definitions. Definitions 5 and 6 can be extended in
the obvious way: a planning problem is a goal default theory with
priorities (D, W, ) and its solutions are preferred extensions of
(D, W, ).
Example 5. Let us consider the domain in Example 1. Let us assume
that, among the objects, there is a very valuable object o1 and a
dangerous object o2 . Furthermore, let us assume that the robot is
equipped with actions that can detect the object o2 whenever the
robot is at the same location as o2 . However, the equipment might not
be working. We will denote with working the fact that the equipment
is working properly. Let us consider the two formulae:

• ' := 3h(o
V 1 ): the robot should try to get the object o1
• := 2[ i2{1,...,k} (o at(o2 , i) ) ¬at(i))]: the robot should not
be at the same place with object o2 at any time.
With these formulae, we can define the following p-defaults:
g1 ⌘

> : working
^'

g2 ⌘

> : ¬working
'

g1 indicates that if the equipment is initially working, then the robot
will get o1 while trying to avoid o2 . g2 states that if the equipment
is not working, then the robot will only worry about getting o1 . The
theory ({g1 , g2 }, ;, {g1
g2 }) states that we prefer that the robot
tries to satisfy g1 before trying to satisfy g2 .

6

Related Work and Discussion

In this section, we relate goal default theories with priorities to existing goal specification languages. We then discuss possible applications of the new language.
• TEP formulae: TEP formulae have been implemented in a planner in [1]. Given a set of TEP formulae
= { 1 , . . . , n }, a
planning problem is an optimization problem that maximizes the
rewards obtained by satisfying the formulae in . Formally, the
reward over a plan is
⌃

i2

, |= i reward(

i)

⌃

i2

, 6|= i penalty(

i)

where reward( ) and penalty( ) denote the reward and penalty
for satisfying and not satisfying , respectively.
The planning problem can be expressed by a goal default theory
with priorities as follows. Let S be a set of formulae, S ✓ , and
dS be the default
V
V
> :
^
2S
2 \S ¬
V
V
^
2S
2 \S ¬
Let D = {dS | S ✓
where dS
dS 0 if
⌃
⌃

i 2S
i 2S

} and

be the partial order over D

reward( i ) ⌃ i 62S penalty( i )
reward( i ) ⌃ i 62S 0 penalty( i ).

0

We can show that (D , ;,
) is a goal default theory with priorities representing the given planning problem, i.e., any preferred
solution of (D , ;,
) is a solution of the original planning
problem and vice versa.
• PP: The language PP allows the specification of three types of
preferences. A basic desire ' is a preference over a trajectory and
therefore is a part of the basic goal language. An atomic preference
is an ordering among basic desires = 1
2 ...
k and
expresses that the preference i is more important than i+1 for
1  i < k 1. An atomic preference can be represented by the
following goal default theory with priorities
⇣n
o
⌘
> : i
i = 1, . . . , k , ;,
i
> :

j
where
is defined by > : i i
for 1i<jk.
j
A general preference is either an atomic preference or a combination of general preferences, such as & , | , and ! , where
and are general preferences. Intuitively, general preferences
add finitely many levels to the specification of preferences and

thus cannot be easily represented by goal default theories which
assume ceteris paribus over the preferences. Adding priorities allows only an extra layer of comparison between preferences. We
view this as a weakness of goal default theories and plan to further
investigate this issue.
• N-LTL and ER-LTL: These two languages allow the specification
of weak and strong exceptions within goal formulae represented
as LTL-formulae by introducing labels to LTL-formulae. By compiling away the labels as in [4], we can show that Gb subsumes
N-LTL and ER-LTL.
Observe that the constructs used in N-LTL and ER-LTL are fairly
close to default logic. This leads us to believe that interesting collections of N-LTL (ER-LTL) theories can be translated into goal
default theories—which would provide a reasonable semantics for
N-LTL (ER-LTL) theories with loops that have not been considered so far.
Finally, we would like to note that Gb can be easily extended to
consider N-LTL (ER-LTL) formulae by
– extending Fb with N-LTL (ER-LTL) formulae; and

– extending |=Gb to define that |=Gb S iff |=Gb c(S) where
c(S), a LTL formula, denotes the result of compiling S to an
LTL formula as described in [4, 5].
• ⇡-CTL⇤ and P-CTL⇤ : These two languages consider nondeterministic domains and define goals over policies but do not
consider preferences among goals. In addition, these languages
introduce the operators A, E, A⇡ , and E⇡ over paths and the two
quantifiers EP and AP over state formulae. Nevertheless, we can
show that the CTL⇤ part of ⇡-CTL⇤ can be expressed in Gb . Furthermore, Gb can be extended to allow formulae of ⇡-CTL⇤ . However, the two new state quantifiers are not expressible in our goal
language. We observe that as the goal language is parameterized
with the satisfaction relation, Gb can be easily extended with these
operators. We strongly believe that these extensions will be sufficient for goal default theories with priorities to capture P-CTL⇤ .
The above discussion highlights features from existing goal languages that can (or cannot) be expressed by our goal language. This
also shows that the proposed language can serve as a unified language for evaluating goal languages. The use of default theories as
the basic language also provides us with an advantage in the study
of computational complexity of goal languages. In this effort, we
expect that well-known complexity results on prioritized default theories [13] will be extremely useful. This will provide us with insights
for the use of existing goal languages as well as the development of
new goal languages.

7

Conclusions and Future Work

In this paper, we describe a default logic based approach to defining
non-monotonic goal specification languages. We start with a basic
goal specification language and use default logic (or prioritizing default logic) to provide a natural way for dealing with inconsistency
and priorities over goals. We show that the new language subsumes
some goal languages in the literature and can describe several features from other goal languages. We identify desirable features that
cannot be easily expressed by our goal language, among them is the
multi-level of preferences between goals, which we intend to investigate in the near future. We also discuss possible applications of the
proposed goal language.

REFERENCES
[1] Jorge A. Baier, Fahiem Bacchus, and Sheila A. McIlraith, ‘A heuristic search approach to planning with temporally extended preferences’,
Artif. Intell., 173(5-6), 593–618, (2009).
[2] Chitta Baral and Jicheng Zhao, ‘Goal specification in presence of nondeterministic actions’, in Proceedings of the 16th Eureopean Conference on Artificial Intelligence, ECAI’2004, including Prestigious Applicants of Intelligent Systems, PAIS 2004, Valencia, Spain, August 22-27,
2004, eds., Ramon López de Mántaras and Lorenza Saitta, pp. 273–277.
IOS Press, (2004).
[3] Chitta Baral and Jicheng Zhao, ‘Goal specification, non-determinism
and quantifying over policies’, in Proceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference, July 16-20,
2006, Boston, Massachusetts, USA. AAAI Press, (2006).
[4] Chitta Baral and Jicheng Zhao, ‘Non-monotonic temporal logics for
goal specification’, in IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India,
January 6-12, 2007, ed., Manuela M. Veloso, pp. 236–242, (2007).
[5] Chitta Baral and Jicheng Zhao, ‘Non-monotonic temporal logics that
facilitate elaboration tolerant revision of goals’, in Proceedings of the
Twenty-Third AAAI Conference on Artificial Intelligence, AAAI 2008,
Chicago, Illinois, USA, July 13-17, 2008, eds., Dieter Fox and Carla P.
Gomes, pp. 406–411. AAAI Press, (2008).
[6] M. Bienvenu, C. Fritz, and S. McIlraith, ‘Planning with qualitative temporal preferences’, in Proceedings of the 10th International Conference
on Principles of Knowledge Representation and Reasoning (KR06), pp.
134–144, Lake District, UK, (June 2006).
[7] G. Brewka and T. Eiter, ‘Prioritizing default logic’, in Intellectics
and Computational Logic, volume 19 of Applied Logic Series, 27–45,
Kluwer, (2000).
[8] James Delgrande and Torsten Schaub, ‘Expressing preferences in default logic’, Artificial Intelligence, 123, 41–87, (2000).
[9] E. A. Emerson, ‘Temporal and modal logic’, in Handbook of theoretical computer science: volume B, ed., J. van Leeuwen, 995–1072, MIT
Press, (1990).
[10] M. Fox and D. Long, ‘PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains’, Journal of Artificial Intelligence Research, 20, 61–124, (2003).
[11] A. Pnueli, ‘The temporal logic of programs’, in Proceedings of the 18th
IEEE Symposium on Foundations of Computer Science (FOCS), pp.
46–57, (1977).
[12] R. Reiter, ‘A logic for default reasoning’, Artificial Intelligence,
13(1,2), 81–132, (1980).
[13] Jussi Rintanen, ‘Complexity of prioritized default logics’, Journal of
Artificial Intelligence Research, 9, 423–461, (1998).
[14] Tran Cao Son and Enrico Pontelli, ‘Planning with Preferences using
Logic Programming’, Theory and Practice of Logic Programming, 6,
559–607, (2006).

