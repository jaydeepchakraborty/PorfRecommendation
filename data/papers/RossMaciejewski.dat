IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 18,

NO. 10,

OCTOBER 2012

1731

Feature-Driven Data Exploration
for Volumetric Rendering
Insoo Woo, Member, IEEE, Ross Maciejewski, Member, IEEE,
Kelly P. Gaither, Member, IEEE, and David S. Ebert, Fellow, IEEE
Abstract—We have developed an intuitive method to semiautomatically explore volumetric data in a focus-region-guided or valuedriven way using a user-defined ray through the 3D volume and contour lines in the region of interest. After selecting a point of interest
from a 2D perspective, which defines a ray through the 3D volume, our method provides analytical tools to assist in narrowing the
region of interest to a desired set of features. Feature layers are identified in a 1D scalar value profile with the ray and are used to
define default rendering parameters, such as color and opacity mappings, and locate the center of the region of interest. Contour lines
are generated based on the feature layer level sets within interactively selected slices of the focus region. Finally, we utilize featurepreserving filters and demonstrate the applicability of our scheme to noisy data.
Index Terms—Direct volume rendering, transfer function, focus+context visualization.

Ç
1

INTRODUCTION

T

HE emergence of web-based scientific simulation portals
[1], [2] has created an abundance of volumetric data,
and an effective means of exploring volumetric data is
through direct volume rendering. One of the most common
methods for direct volume rendering is to employ the use of
interactive transfer function widgets which users interactively use to define a mapping of the volumetric data values
to optical properties (color and opacity). Unfortunately,
creating an appropriate transfer function often involves
tedious adjustment and fine tuning of parameters, resulting
in a trial-and-error type approach by the user. This problem
is further compounded in scientific portals (e.g., the
NanoHub [3]), as the scientists who need to analyze the
data are often novices in volumetric rendering. Furthermore, relevant sparse features within the volumetric data
can easily be occluded by semitransparent surrounding
values, making it difficult to find the specific value range
within the occluded regions that should be enhanced for
better visualization and analysis. This volumetric exploration problem is illustrated in Fig. 1.
In Fig. 1, a scientist has created an Indium-Arsenic (InAs)
quantum dot simulation [4], and the scientist wants to

. I. Woo is with the Purdue Visual Analytics Center, Purdue University, PO
Box 519, 465 Northwestern Ave., West Lafayette, IN 47907.
E-mail: iwoo@purdue.edu.
. R. Maciejewski is with the Arizona State University, PO Box 878809,
Tempe, AZ 85287. E-mail: rmacieje@asu.edu.
. K.P. Gaither is with the Texas Advanced Computing Center, University of
Texas, Research Office Complex 1.101, J.J. Pickle Research Campus,
Building 196, 10100 Burnet Road, Austin R8700, TX 78758-4497.
E-mail: kelly@tacc.utexas.edu.
. D.S. Ebert is with Purdue Visual Analytics Center, Purdue University,
465 Northwestern Ave., West Lafayette, IN 47907.
E-mail: ebertd@purdue.edu.
Manuscript received 2 July 2010 ; revised 15 Nov. 2010; accepted 23 Dec.
2011; published online 26 Jan. 2012.
Recommended for acceptance by T. Möller.
For information on obtaining reprints of this article, please send e-mail to:
tvcg@computer.org, and reference IEEECS Log Number TVCG-2010-07-0132.
Digital Object Identifier no. 10.1109/TVCG.2012.24.
1077-2626/12/$31.00 ß 2012 IEEE

explore the electron wave function data encapsulated in the
resultant volumetric output. Here, the important structure
to the end user is the vertical stack consisting of three InAs
quantum dots. However, this structure is not easily found
using 1D and 2D histogram widgets. The transfer function
widget in Fig. 1 (middle) uses a 2D histogram widget [5]
plotting the value versus value gradient magnitude of the
volumetric data. The user has drawn a series of rectangular
boxes within the widget. The structures found in region A
of the histogram are of little to no importance to the
scientist, and, while the most important structures of the
data are located in the histogram region of Fig. 1 marked B,
the data in region B are so sparse that there are no visual
cues to guide the user to select those regions in the
histogram space. Note that both a linear and logarithmic
mapping of the opacity was applied in the 2D histogram
with similar results.
Thus, novice end users of direct volume rendering tools
need methods where rendering parameters can be suggested in a way that incorporates the end-user’s expert
knowledge of interest within the data set and the properties
of the volumetric data. In this work, we propose a novel
interaction approach to semiautomatically generate appropriate rendering parameters to help users efficiently explore
and analyze their volumetric data sets. Key contributions
include the following:
.

.

A hardware-accelerated graphics pipeline (Fig. 2)
that enables interactive visualization through a userspecified region of interest. Features within the
region of interest are analyzed and automatically
mapped to a set of rendering parameters based on
the features. Moreover, our pipeline includes feature-preserving denoising methods (e.g., median
filters) to support noisy volumetric data analysis.
A novel selection scheme for color mapping and
data enhancement based on data types (interval and
ratio), utilizing measurement theory. Features of
interest can be highlighted based on regions and
focus values.

Published by the IEEE Computer Society

1732

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 18,

NO. 10,

OCTOBER 2012

Fig. 1. Comparison of transfer function design between a 1D histogram widget (left), a 2D histogram widget (center), and our semiautomated transfer
function widget (right). In the 2D histogram, the region A includes the majority of the volumetric data; however, the most important information is
found in the sparse areas (B) in the 2D histogram. In our method, the user is able to highlight meaningful boundaries using the value profile for the
region of interest. The “top view” (rightmost figure) shows the ray (in red) passing through the volume and the line graph displays the scalar values
and gradient magnitudes along the ray (in red).

Fig. 2. Schematic diagram of our semiautomated data exploration method. The sequence consists of user’s region of interest acquisition, feature
extraction, color selection, and data enhancement.

.

An intuitive user interface for transfer function
design (Fig. 3), modification, and interaction utilizing
line charts and contour lines within a slice view for
enhancing local data features. Level sets are extracted

Fig. 3. The user-interface controls of our interactive feature-driven
exploration tool. 1: The value profile widget. 2: The contour line widget.
3: The color palette selection widget. 4: The focus region widget.

from a slice view using a CUDA [6] kernel, contour
lines are displayed on the view, and then colors can be
changed on the slice view based on the level set.
The use of the feature selection tool enables users to
generate appropriate transfer functions more effectively.
Furthermore, the addition of the value profile widget
provides simulation scientists with another view in which
to analyze their data. Finally, our semiautomated tools make
use of line graphs and heightmaps that are generated by
sampling scalar values along a specific line segment, or on a
specific slice surface within the volumetric data. By
applying both line graph and contour line metaphors within
a volume visualization application, users can further
investigate and analyze data features by specifying the
spatial region of interest within the volumetric data or by
varying the scalar value range of interest to highlight features.
In this paper, we will discuss the advantages of our
pipeline and interactive widgets and their ability to enhance
the overall visualization and analysis process for the end
user. In the next section, we summarize the related work in
transfer function design, color selection, and image denoising. In Section 3, we describe our pipeline and algorithms

WOO ET AL.: FEATURE-DRIVEN DATA EXPLORATION FOR VOLUMETRIC RENDERING

for interactive transfer function design. In Section 4, we
explain the implementation details of the algorithm. Finally,
we discuss our results and present feedback from end users
in Section 5 and propose future work in Section 6.

2

RELATED WORK

Interactive transfer function design has been addressed
through many different approaches. Direct manipulation
widgets help users to explore volumetric data sets by using
data probes, clipping planes, classification, shading, and
color picking widgets [5], [7]. Some approaches have
utilized design galleries [8] or parallel coordinate style
interfaces [9] for transfer function design, while other recent
approaches have focused on user-oriented design schemes.
For example, Rezk-Salama et al. [10] provided a high-level
transfer function model using widgets that map directly to
volume features (such as controls for bone color and
opacity). Wu and Qu [11] designed a framework to
manipulate rendered images for multiple volumetric objects
and merge the images into a single rendering. Bruckner and
Gröller [12] employed a stylized transfer function widget
that enables users to intuitively select a nonphotorealistic
shading style from the style transfer function. Bajaj et al.
[13] introduced the contour spectrum interface that presents
users with a collection of data characteristics to select
significant isovalues for isosurfacing. Lundström et al. [14]
introduced the -histogram which incorporates spatial
coherence as a means of automated transfer function
design. This work divides the data into subblocks (spatial
sub regions), computes local histograms, amplifies peaks by
raising histogram values to the power of , and calculates
an  histogram by summing up all the amplified histograms. The work by Lundström et al. provides transfer
functions on a global scale as opposed to our work, which
focuses on enhancing local data features. Correa and Ma
[15] proposed a new volume exploration technique by
introducing the concept of a scale field that allows users to
assign colors and opacities based on size. However, the
concept of a scale field works best when the data sets have a
well-defined underlying shape, which is often not the case
in scientific simulations.
Besides ill-defined volumetric structures, another issue is
that the size and complexity of scientific simulations has
also been drastically increasing. As such, recent work has
focused on the use of focus+context visualization [16]. In
order to enable users to select a region of interest with ease,
research has focused on two primary tasks: defining a focus
and context area, and effectively emphasizing the focused
area. Lu et al. [17] inferred a user’s regions of interest by
capturing eye gaze positions as the user watched a rotating
volume. Li et al. [18] proposed a system for automatically
generating a variety of cut-away illustrations. Viola et al.
[19] introduced cut-away views for volume rendering based
on importance by suppressing less important information.
However, their method requires data segmentation and
user-assigned importance values to each data segment.
Ropinski et al. [20] introduced a stroke-based transfer
function design scheme computing a difference histogram
for all pairs of the control points along inner and outer
strokes [20]. For better peak detection, this work used

1733

weights based on the voxels’ visibility. It works properly
when the data have distinctive volume object boundaries
since the user can easily draw stroke lines along the
intensity boundary. However, many simulation data sets
lack well-defined edges and boundaries.
Along with focus+context methods, raycasting-based
data analysis methods have also been employed to aid
users when exploring their volumetric data. These methods
create automated or semiautomated rendering parameter
settings or provide an interface supporting volumetric layer
exploration. Opacity peeling [21] generates an image of an
opacity layer (image buffer) whenever an accumulated
alpha value exceeds a threshold and allows the user to
explore the opacity layers. Malik et al. [22] extended the
concept by analyzing values sampled along rays through
the volumetric data and extracting feature layers. Instead of
accumulating opacity, Malik’s work searches for transition
points from the sampled values to extract feature layers
with prominent peaks. Correa et al. [23] introduced the
visibility histogram where, for a given viewpoint, visibility
is weighted using opacity, which is considered as importance, and used to maximize the visibility of the value range
of interest. Kohlmann et al. [24] used ray profiles to
facilitate the process of finding and highlighting interest
points in 2D slice views starting from the users’ mouse click
location in the 3D view. However, their main focus is on the
construction of a ray profile library that stores ray profile
samples for different CT data sets and on matching profiles
to find close similarities.
Another factor to consider in scientific simulation data is
that the data are often temporal in nature. As such, previous
work has also focused on defining a transfer function for
time-varying volumetric data. Recent work by Akiba and
Ma [25], Akiba et al. [26] utilized parallel coordinate plots to
create a volume rendering interface for exploring multivariate time-varying data sets. By means of a predictioncorrection process, Muelder and Ma [27] proposed a
method to predict the feature regions in the previous frame,
making the feature tracking coherent and easy to extract the
actual features of interest. Maciejewski et al. [28] used
density estimation for creating a temporally coherent set of
transfer functions of a group of feature space histograms.
While these previous methods have attempted to provide
user friendly interfaces for transfer function manipulation,
there are still barriers to the adoption of the methods by
scientists and general users. In this paper, we focus on a new
data exploration and transfer function design scheme. We
provide the end user with familiar ray profile plots and an
interactive means of defining regions of interests, as shown
in Fig. 3. From these tools, we are able to semiautomatically
generate rendering parameters and reduce some of the
transfer function design burden for the end user.

3

INTERACTIVE FEATURE-DRIVEN DATA
EXPLORATION

Since analysis methods are affected by the structure and
nature of data [29], we define our visual mapping parameter
choices based on the underlying data type. In measurement
theory, the measurement scale is categorized into four

1734

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 18,

NO. 10,

OCTOBER 2012

TABLE 1
Color and Opacity Selections Based on Their Data Type

categories (nominal, ordinal, interval, and ratio) [30]. Nominal data have no order, ordinal data have order, interval data
have meaningful intervals, and ratio data have the height
level of measurement. These different types of data have
features that need to be highlighted in different ways [29]. In
the case of volumetric data, we consider two categories of
data (Table 1), interval and ratio data, and generalize a set of
guidelines and rules for automated or semiautomated
methods for our visualization techniques. Nominal and
ordinal data types are left for future consideration.
The color selection schemes, emphasized areas, and
filtering parameters are semiautomatically determined by
the user-defined data type. These schemes are applied
automatically in our transfer function generation pipeline
depending on the data type. Fig. 2 illustrates the sequence of
operations in our approach. First, we provide the user with
an initial rendering of their data. During our first pass,
transfer function parameters are generated using a sine
function that maps the scalar values to an opacity, thus
creating high and low contours [31] as seen in Fig. 2 (input
volume data). Next, using the initial volume rendering, the
user interacts with the volume. Mouse clicks indicate the
user’s region of interest (Fig. 2, ROI acquisition), and a focus
point and a focus region are created resulting in a focus line
that is a viewing ray passing through the focus point. Then,
our system extracts feature layers to compute the number of
colors and the data enhancement points and displays the
results in a value profile widget. A color transfer function
and a weight map are automatically defined based on the
extracted feature layers (Fig. 2, transfer function and weight
map). The weight map is utilized to modulate the opacity. In
this way, users can explore their data in focus-region-based
or focus-value-driven ways (Fig. 2, volume composition). In
addition, we apply nonlinear filters to the value profiles and
slice view images to support our transfer function generation approach for noisy volumetric data sets.

3.1 Region of Interest (ROI) Acquisition
The region of interest acquisition is the first step in our data
exploration method. Earlier methods for ROI selection
included the use of the mouse cursor along with several
prespecified context layers from the volume data set [32].
However, this method entails the tedious process of
manually creating several context layers that capture
important features within the data set. Our work chooses
to utilize a simpler interface design in which mouse movements are used to obtain the ROI within the volume data
from the 2D screen. First, the user mouse clicks on the region
of interest and drags the mouse in order to define the size of
their region of interest. The mouse click defines the initial
center of the ROI.

Once the ROI is determined, a ray profile is obtained and
analyzed. The initial center of the ROI is then repositioned
from the original location to a new locally optimal location
based on analysis of the ray profile in the area selected and
the user selected datatype. Based on the datatype, the ROI is
centered at either the maximum gradient magnitude or at
the maximum scalar value in the 3D neighborhood. Then,
the user can shift the default position of the ROI along the
viewing ray, allowing him or her to acquire a different ROI
along this focus line.
The repositioning and analysis of the ray profile is the
main step in our semiautomated data exploration. Since
features can be identified with local peaks and maximum
gradient magnitudes obtained by sampling along the ray at
regular intervals, we employ the concept of a feature layer
to generate the visual mapping based on these local peaks
and maximum gradient magnitudes. Each layer corresponds to the minimal pairwise spatial distance calculated
between peaks along the ray profile. A feature layer stores
the minimum and maximum of the local scalar values and
the minimum and maximum of the local gradient magnitudes to determine a value range for the color and data
enhancement within each layer. Depending on the type of
volumetric data (interval or ratio as defined by the user), the
maxima and minima of these values are used to determine
the transition points (as shown in the value profile widget
of Fig. 3 and detailed in Fig. 4) that divide the profile into
multiple layers.
We first extract feature layers based on the local extrema
along the ray profile and then use these feature layers for
data exploration. Malik et al. [22] employed such a ray
profile for feature peeling. Multiple rays are cast to extract
scalar values along the rays. This approach allows the users
to explore volumetric data sets layer-by-layer. We also use
the variation of scalar values along the rays to extract
feature layers. In addition, we use the local peaks, both from
scalar values and gradient magnitude, of each layer to
provide color assignment and data enhancement for
features of interest in layers according to the data type of
the input volume data set. Thus, each layer is found

Fig. 4. The larger color bar on the top displays the scalar (black) and
gradient magnitude (red) value profiles. Three vertical black lines
indicate the positions of transition points that create four feature layers.
The vertical bar on the right is divided into four regions bounded by the
scalar value ranges of the corresponding colors in the color map shown
in the color bar below.

WOO ET AL.: FEATURE-DRIVEN DATA EXPLORATION FOR VOLUMETRIC RENDERING

1735

Fig. 5. Value profile of a noisy MRI data. (a) Original value profile of the
scalar values and (b) same profile filtered with a median low pass filter.

between two extrema of either the scalar or gradient values
along the ray profile. The ordering of the layers by scalar
value is done so that the correct color scheme will be
mapped to each layer. As shown in Fig. 4, the layers are
perceptually ordered by color; however, the ordering of the
layers on the ray profile would not match the perceptual
color ordering without this step.
In the color selection stage (Section 3.2.1), we assign a
unique color to each identified layer. In our work, we utilize
the Color Brewer [33] color schemes and match each scheme
to the appropriate data type (nominal maps to qualitative,
interval to sequential, ratio to divergent, and ordinal to
sequential). However, in some cases, we may not be able to
extract the necessary number of layers either due to the
difficulty in determining feature layers (for example, in
the case of gradually increasing scalar values) or due to the
presence of excessive layers. To overcome this, we add a
user-defined parameter to specify the number of feature
layers, i.e., the number of colors, to use in these cases. When
the number of extracted layers does not exceed this userdefined parameter, we iteratively sort the layers based on
their value range. Then, we select the largest layer, and split
it into two until the number of layers is equal to the userspecified parameter value. When the number of extracted
layers exceeds this user-defined parameter, we automatically merge layers with similar local peaks until the number
of layers is equal to the specified parameter value. The layers
are sorted in increasing order of the maximum peak values,
the peaks of two consecutive layers are compared and the
two layers are merged into one if the distance between the
two peaks was the smallest distance between all other peaks.
In the presence of noisy volumetric data, the problem of
extracting layers from volume profiles is further compounded. In our data exploration and transfer function
design scheme, we use contour lines to represent significant
value ranges in the region of interest. Unfortunately,
contour lines for noisy volumetric data sets contain many
artifacts which can confuse the end user. Our work follows
the contour conventions proposed by Viola et al. [34] while
providing a more general interface for generating contours
while removing noise. Our system uses nonlinear filters,
parallelized using CUDA [6], and we compare the quality of
the resulting data sets in Section 4. Fig. 5a shows a noisy
value profile, here one can imagine that the number of
transitions found would result in a large number of
nonessential layers. As such, we utilize nonlinear filters
such as the median filter that are used to smooth such value
variations [22] as shown in Fig. 5b. This smoothes the ray
profile and allows for easier feature layer extraction.
The denoising is only done on the ray profile and the
slice view. The original data are never denoised, so the
output of the pipeline is only slightly different in the noisy
versus nonnoisy data. This is due to the fact that a user
specifies a maximum number of layers, and layers found

Fig. 6. Initial rendering of a fuel simulation data set with the default
parameter settings. (a) Shows the changes of scalar values (in black on
the top right) and gradient magnitudes (in red) and the regions enhanced
based on this profile. (b) Shows the contour lines generated
automatically to convey the shapes within the region of interest.

during the computational analysis that are only at a small
distance from each other will be merged. In the case of
noisy local peaks, the local peaks typically become merged.
The denoising is only needed for the contour views.

3.2 Transfer Function Generation
In our approach, the value profile and contour lines are
used to provide an interactive environment for data
exploration. Fig. 6 illustrates the initial data visualization
based on our value profile and contour lines using default
color and opacity selection. The line graph on the top right
in Fig. 6a shows the scalar and gradient value variations
from the value profile. Fig. 6b shows the shapes of the
volumetric objects with contour lines in the region of
interest. Based on the feature layers, the number of colors
and emphasized areas are computed. The color bar on the
top left of the figure shows the color transfer function. A
cool color is selected as an outer color and a warm color is
selected as an inner object using the Color Brewer’s
diverging color scheme (Red-Yellow-Blue). The user can
change the colors directly on the slice view. The ticks above
the line graph indicate important features (maximum
intensities in each local area) to be enhanced. The user can
add or remove ticks for data enhancement.
3.2.1 Color Selection
We assign a unique color to every layer identified during
the ROI acquisition phase. Depending on the data type, we
use Color Brewer’s color schemes (sequential, diverging,
and qualitative) by default as listed in Table 1. Fig. 4 shows
the assignment of a color map with four colors to a value
profile with four layers. The scalar range of each color in the
color map is obtained by computing the mean value of each
set of adjacent peak values as marked on the vertical line on
the right in this figure.
Ideally, the number of colors to be used can be determined
from the number of layers. The process of merging similar
layers and specifying the number of desired layers in the
feature extraction step prevents the assignment of different
colors to similar layers and generates an appropriate color
transfer function.
3.2.2 Data Enhancement
Our method utilizes color to define a feature; however,
feature enhancement is also an important setting for
transfer function design. There are several approaches to

1736

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 18,

NO. 10,

OCTOBER 2012

Fig. 7. Enhancement regions identified from the value profile of a region
of interest based on the data type. (a) Local peaks of each layer are
enhanced for interval data. (b) Local maximum gradients of each layer
are enhanced for ratio data.

data feature enhancement. We modulate the opacity values
to enhance the data features based on the extracted critical
values. As a default opacity setting, we apply a sine
function to map the scalar data value to opacity, which
creates alternating low and high opacity contours [31],
shown on the left side in Fig. 2.
However, different data types have different characteristics that require the emphasis of appropriate regions in the
value profile as shown in Fig. 7 and Table 1. For example,
ratio data are characterized by the difference of values rather
than the sequence of values. Therefore, the boundaries of
each layer are more important and need to be emphasized
instead of the peaks of each layer. Fig. 8 illustrates the
difference between enhancing local peaks and local maximum gradients for a ratio data set (the tooth data set).
For the data enhancement, the ranges of local peaks and
maximum gradient magnitudes obtained from the value
profile are maintained in a weight map and the weights are
multiplied by initial opacities to obtain final opacities. We
employ a 1D weight map to enhance the features of interest
(e.g., local maximum value and local maximum gradient
magnitude). Local maximum gradient magnitudes are
usually used for the boundary enhancement [35]. However,
simulation data sets (e.g., the probability distribution of
atoms) tend to have smooth data variations. Therefore, we
use the local maximum value for simulation data sets as the
default setting, as shown in Fig. 7a. In Kindlmanns
approach, the boundaries of objects are defined by sharp
curves in the histogram. By utilizing the ray profile, the
changes in density along the profile allow us to determine
transitions between features that are not necessarily visible
in the histogram (as shown in Fig. 1).
The weight map assigns a scalar value scaled from zero
to one for each voxel as the voxel’s weight. By default, the
initial values for all the density bins are set to 0.2. The
empirical value 0.2 deemphasizes the high opacities from
the default opacities. Typically, the maximum scalar value
of a layer and the maximum gradient magnitude should be
emphasized in the case of sequential data sets (e.g.,
simulation data) and ratio data sets (e.g., medical data),
respectively. We compute a value range [i ; i ] to be
highlighted for each peak by obtaining the minimum and
maximum of scalar values of the N closest neighboring
sampling points of the ith peak along the ray. We set the
value of N to 7 in this paper.
The weight map is updated using

1;
i  vj  i ; j 2 ½0; num of binsÞ;
ð1Þ
wj ¼
0:2; otherwise;
where wj is the jth weight in the ray profile histogram and
vj is the voxel scalar value. However, even weighted

Fig. 8. Images showing the differences arising from enhancing different
regions of the tooth data set. (a) Image obtained by enhancing local
peaks of each layer. (b) Image obtained by enhancing local maximum
gradients of each layer.

opacities cannot be used to display the values with zero
opacity, as shown by Zhou et al. [36]. Thus, we use (2) to
compute the opacity for the range of focus values
vd ¼ maxfo ; w g  wj ;

ð2Þ

where, vd is the enhanced opacity, o is original opacity, and
w is the opacity of the windowing function as defined in
[36]. Values obtained for the weight mapping and nearest
neighbor sampling were obtained through visual analysis of
repeated trials working with end users. Future work will
focus on determining automatic optimal parameter settings.
Additionally, we apply focus-region-based data enhancement along a ray cast through the volume data. For
this enhancement, we assume that the region is cylindrical
or spherical. For spherical regions, the center point (C) of
the focus region and the radius (r) of the ROI are used to
compute the weighted opacities, (5). Data enhancement is
performed based on the distance between the position of a
voxel and the center point of the focus region.
For cylindrical regions that emphasize the information
around the focus line, the two end points (x1 , x2 ) of a focus
line, a radius (r) of a focus region, and the distance (d)
between a sample position (vp ) and the line (x1 , x2 ) are used
for the computation of weighted opacities. The opacity
values are computed according to their shape. For a
cylinder, the opacity is computed as




d
rd ¼ vd  k þ 2 
ð3Þ
 Ic ðvp Þ ;
r

Ic ðvp Þ ¼

1; d  r;
0; otherwise:

ð4Þ

For a sphere


rd




jvp  Cj
¼ vd  k þ 2 
 Is ðvp Þ ;
r

Is ðvp Þ ¼

1; jvp  Cj  r;
0; otherwise;

ð5Þ

ð6Þ

WOO ET AL.: FEATURE-DRIVEN DATA EXPLORATION FOR VOLUMETRIC RENDERING

1737

Fig. 9. User-defined color selection on a slice view displaying contour lines. (a) Shows a slice view. (b) Shows the resulting visualization after
manually selecting colors and enhancing features of interest. (c) Shows the final resulting image.

where vd is the enhanced opacity from (2), rd is the
resultant opacity, k is a constant (in this paper, we use k ¼
0:5 for all images for focus-region-based data enhancement), vp is a sample position, and Ic ðvp Þ and Is ðvp Þ are
indicator functions that identify whether a voxel is within
the region of interest or not.

3.2.3 Contour Line Extraction
Using a transfer function design interface to highlight a
region on the slice view or change the color of the region is
often unintuitive as there is no direct link between the slice
view and the transfer function interface. In order to better
guide users, our system employs the use of contour lines in
the slice view to provide intuitive cues for ROI selection.
Contour lines have been shown to be effective in nonphotorealistic rendering to convey shape information [37]
and can be used to illustrate value regions or shapes of
simulation data. Moreover, contour lines on a slice view in
the region of interest provide intuitive clues to select and
enhance the region of interest. As such, we extract and
visualize contour lines on the slice view to allow users to
change visual properties of the region of interest as well as
to convey shape in the selected regions of interest. Users can
control the number of level sets by adjusting the sampling
space of the contour lines as well as the number of the
extracted feature layers. Whenever a user changes the
position of the ROI, the view on the slice plane is updated
and contour lines are regenerated. We extract contour lines
from the image in the slice view to illustrate the shape of the
volumetric objects based on the critical boundaries of the
data in the regions of interest. Contour lines are extracted
using the Marching Square algorithm [38] instead of imagebased edge detection algorithms since 3D line primitives
obtained from the former can be regenerated and displayed
clearly regardless of the magnification and camera position
unlike the image-based methods. A 2D slice of the userspecified size is placed and moved along the ray and the
slice captures the values and stores them to a render target.
The color buffer is then used to extract contour lines.
The number of the level sets of the contour lines is
computed and isovalues ranging from 0 to 1 (where the
value is internally scaled) are determined for each level to
generate contour lines based on the extracted layers from
the value profile. For volumetric data with no noise, the
contour lines can be directly generated to show critical

boundaries as shown in Fig. 6b. Moreover, we also allow
users to select and assign colors to various regions formed
by the contour lines on the slice view plane. Fig. 9 shows the
user-defined color selection using a slice view.
Unfortunately, noisy volumetric data produce many
contour lines, as shown in Fig. 10, that clutter the display,
reducing their utility in conveying shape information.
Therefore, we apply median and bilateral filters to the
image of a slice view to remove the noise before generating
contours. We describe the implementation details of
contour line generation in the next Section.

4

IMPLEMENTATION

We implemented the data exploration pipeline on a
Windows XP PC with an Intel Pentium 4 3.40 GHz CPU,
2 GB RAM, and a GeForce 8800 GTX graphics card. The
median filtering with a 3  3 window and contour line

Fig. 10. Images of the foot data set rendered by a median filter with
various sizes. (a) Shows contour lines when no filters are applied. (b),
(c), and (d) Show the contour lines extracted from the image filtered by a
3  3, 5  5, and 9  9 size median filter, respectively.

1738

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 18,

NO. 10,

OCTOBER 2012

Fig. 11. Pipeline for contour line generation using the CUDA kernel. The first step, implemented in the Cg shader, generates a texture containing
scalar values sampled from the volumetric data, which is then mapped to colors on the slice plane. The second step filters noisy data sets to
remove noise. The third step computes contour lines and performs stream reduction on the CUDA kernel and the final step renders the computed
contour lines.

generation takes 5.7-7.8 and 9.9-10.8 milliseconds for 128 
128 and 256  256 grid textures on the GPU, respectively.
This section describes the implementation details of the
key aspects in our data exploration pipeline namely value
profiling, slice view generation, contour line generation,
and image denoising.
Value profiling. To obtain a value profile from the user’s
mouse click in the ROI, we compute two points, one each on
the near and far planes of the view frustum using
gluUnproject. These two points, determined using the ray
and bounding box intersection test, define the line segment
for the value profile. To sample scalar and gradient
magnitude values along this ray, sampling points are
generated from the starting point to the ending point of the
line segment and are stored in a 1D RGB component texture.
A rectangle is rendered with this texture to pass the sampling
points to the graphics pipeline. In a pixel shader, each texel is
used as a sampling position to obtain the scalar value and the
gradient magnitude at the corresponding position in the 3D
volume texture. The sampled scalar value and the 3D
gradient magnitude are stored in the render target. For data
sets with noise, scalar, and gradient magnitude values are
filtered with a median filter using the host CPU due to the
low computational cost. The sampled values are copied to
the host memory in order to compute the initial center
position of the ROI as well as the feature layers.
Slice view generation. The pipeline for the slice view
produces two color buffers in the render target. One stores the
scalar and gradient magnitude values for the contour line
generation and the other stores the color (RGB) of each scalar
value and the transparency (Alpha) to display in the slice
view. Images are generated from the slice view by treating it
as a circle, and an orthographic camera facing this plane is
used to sample values within the circle of the focus region.
Regions outside of the circle are rendered as translucent by
adjusting the alpha component as shown in Fig. 11.
Contour line generation. Level sets of the contour lines
are determined based on the feature layers obtained from
the value profile. Fig. 11 illustrates the contour line
generation pipeline. The image (2D texture) is treated as a
grid and contour lines are computed using the Marching
Squares algorithm [38]. However, for better memory
efficiency, triangle primitives are used instead of squares
because each test using triangles produces zero or one
contour line, while the Marching Square test produces zero

to three contour lines. It reduces the memory consumption
during the stream reduction operation [39], [40] while the
number of iterations over the CUDA kernel becomes twice
as many as the level sets. This doubling is due to the fact
that the contour lines are extracted from triangles (two
triangles per a grid cell). This tradeoff in using triangle
primitives over squares is justified because efficient
memory consumption is of greater concern than computation time in most web-based portals, such as NanoHub [3],
that have to support multiple simultaneous users.
Each triangle primitive is assigned to a thread, and each
thread generates zero or one line, as shown in the third
module (Contour lines generation) in Fig. 11, and the
number of lines is stored in the flag in Fig. 12. The fourth
step produces a scattered array of line vertices, lines
(scattered) in Fig. 12. Unwanted elements (no line primitives)
from the scattered array are removed by applying stream
reduction (using the CUDA Data Parallel Primitive
(CUDPP) library [40]) and the resultant array becomes
compact like lines (compact) in Fig. 12. An OpenGL buffer
object is created to store all the line primitives since the
CUDA kernel and the OpenGL pipeline can both access the
OpenGL buffer. The number of contour lines computed in
each iteration is obtained from the stream reduction index
array and is used as the offset into the memory block to
obtain the memory starting point of the next iteration. Using
the CUDA architecture, contour lines of five level sets are
generated within 3-4 milliseconds for a 128  128 grid
texture on the GPU.
Image denoising. Noisy volumetric data sets result in
messy contour lines as shown in Fig. 10. To avoid this, the

Fig. 12. A schematic diagram of the reduction for contour line generation.
flags stores the flags that indicate each triangle has a line. target array
index stores indices for lines to be stored in the compact array of lines.
Finally, line, the mapped OpenGL VBO, stores compact lines.

WOO ET AL.: FEATURE-DRIVEN DATA EXPLORATION FOR VOLUMETRIC RENDERING

1739

the pipeline to explore a variety of data sets including ratio
and interval data sets (both with and without noise). In this
section, we present a gallery of results highlighting various
aspects of our data exploration pipeline and its ability to
extract important features in various data sets as well as
feedback from our various user constituents.

Fig. 13. Images from the neghip data set rendered using various color
schemes provided by the Color Brewer [33]. (a) Uses the Yellow-Green
sequential color scheme. (b) Uses the Red-Yellow-Blue diverging color
scheme. (c) and (d) Use the Dark2 and the Accent qualitative color
schemes, respectively.

image obtained from the slice view is filtered using a
median filter or a bilateral filter. The median filter has been
implemented based on the HDR Image Processing Library
[41] and designed for the CUDA kernel. In our implementation, a 3  3 median filter is applied to the slice view image
by default. However, users are given the option to change
both the filter and its parameters.

5

RESULTS AND DISCUSSION

During the design and evaluation process of our work, we
have collaborated with computational nanotechnology and
flow simulation researchers. In fact, the need for a
simplified interface for volume rendering on nanohub.org
was the impetus for this work. Based on our initial success
with computational nanotechnology data, we have applied

5.1 Nonnoisy Volumetric Data
Fig. 6a shows the results of applying the value profiling and
data enhancement aspects of our pipeline to a fuel simulation
data set. The value profile shows that important regions
within the data are occluded by outer layers from a specific
viewpoint. Our pipeline automatically extracts four layers
based on the value profile and the peak value of each layer
is enhanced. This enhancement, combined with the default
diverging Color Brewer scheme, allows a user to easily
identify important parts previously occluded within the
data set. Based on the value profile, four layers are extracted
and the peak values of each layer are enhanced. The
number of colors is equal to the number of feature layers.
Various Color Brewer schemes can be applied to a data set
depending on its type. Fig. 13 shows different color schemes
applied to the neghip data set ranging from a sequential
color scheme Fig. 13a to a diverging scheme in Fig. 13b and
qualitative color settings in Figs. 13c and 13d.
Automatic parameter specification in our pipeline is
especially useful for scientists who are unfamiliar with
volume rendering and visualization techniques. However,
our pipeline also supports advanced visualization users by
allowing them to tweak rendering parameters such as the
color, opacity, and the number of colors.
Fig. 14 shows different rendering results using traditional 1D and 2D transfer function widgets and our widget.
We worked with a nanoscientist who had previously been
working with volumetric rendering tools for analyzing
quantum dot simulations. This scientist was familiar with
1D and 2D transfer functions, and using these traditional
tools, the user was able to visualize and highlight
structures within the data set. In the 1D transfer function
space, Fig. 14a, the scientist was able to explore the scalar
data values and create a suitable transfer function;
however, he reported that the automatic parameter settings
provided a quicker and better rendering in terms of his

Fig. 14. Plots of a wave function using a 1D histogram widget, a 2D histogram widget, and our data exploration widget. (a) The result using the 1D
histogram. (b) The result using the 2D histogram. Most of the values are binned within the left small box, but the area does not have any inner
structure within it. (c) The result with the local maximum highlighted. (d) The result with the local maximum gradient magnitude highlighted.

1740

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 18,

NO. 10,

OCTOBER 2012

Fig. 15. Results of a tornado data set rendered by our data exploration method. The left image is a result of enhancing local peaks in the region of
interest, the middle image shows a result from enhancing local maximum gradient magnitudes, and the right shows a resulting image from userdefined data enhancements.

analysis. In the 2D transfer function case, Fig. 14b, since the
important structure within the simulation data is sparse,
the upper right portion of the 2D transfer function
histogram appears to contain no data; however, this region
needs to be selected to create an appropriate visualization.
In this case, the user had difficulty in determining the
correct place to search for data and utilized a hunt and
peck approach until finally settling on an image with inner
structures of the data blurred out. By using our method,
Figs. 14c and 14d, the scientist reported that he was able to
reduce his search time and gain insights about the changes
in wave functions within his data. According to the user,
Fig. 14d is better than both Figs. 14a and 14b as the user
was able to better highlight the inner structure of the data
and gain an understanding of the electron potential fields
within the data.
Figs. 14c and 14d show the difference between emphasizing local scalar maxima and local gradient magnitude
maxima, respectively. In comparing Fig. 14c to 14d, the user
found both images to be useful. Fig. 14c provided the
scientist with a better understanding of the electron
potential clouds, while Fig. 14d provided more details
about the inner structures of the data.
Informal feedback from computational nanotechnology
researchers and computational flow researchers has been
very positive. These scientists find this system interface
more effective and intuitive for exploring, analyzing, and
understanding the features in their simulation data than
existing interfaces. In terms of the usefulness, this tool
helped them to better understand the distribution of the
wave functions within the quantum dot area. From the line
plot (value profile) that cuts through the center of their
simulation data (e.g., quantum dot), they could see their
simulation result (e.g., the wave function data) along critical
directions in real space and directly highlight features of
interest. In addition, they told us that this helped them in
performing quantitative analysis which was absent in
previously available visualization tools that only provide
qualitative analysis. Previously, the scientist had developed
scripting programs (e.g., MATLAB programs) to perform
quantitative analysis by generating line plots in critical
directions. Finally, our system also helped them calculate
optical matrix elements in various directions for their
simulation. While obtaining this informal feedback, we also

provided the end user with commonly used viewing
directions (e.g., top, front, and þ45 rotation views) to
help better extract the value profile. We found that the
addition of these viewing angles were also a very popular
feature and were able to further reduce the amount of time
needed to analyze and visualize a data set.
The usefulness (to the scientist) was in the reduced
amount of exploration time needed to generate the image
(note that without the semiautomatic approach the transfer
function needs 4 peaks (Fig. 14a). The addition of the ray
profile tool provided them with a means to perform
quantitative analysis that was previously ported to other
software tools. Thus, the addition of the tools for the
semiautomatic generation and quantitative analysis are able
to both reduce the burden of transfer function creation on
the user while enhancing their overall analytic capability.
As such, our data exploration method more closely
couples the physics governing the data and techniques for
mapping this data using a judiciously chosen transfer
function. Allowing the tighter integration of the data with
the transfer function provides a more intuitive interface for
manipulating parameters present in the data itself and is
one of the key strengths of the method. Because users are
allowed to interactively manipulate the data and data
gradients and see an illustration on the screen, they get a
much more intuitive notion of how to best communicate
and understand the physics under consideration.

5.2 Time-Varying Data
We further applied our methodology to the investigation of
fluid flow. Fig. 15 shows a tornado data set and compares
results obtained using the default and user-defined parameter settings. Fig. 16 shows the resultant images obtained
by setting rendering parameters to extract interesting
regions and track them in the convection data set. These
images show a time advection of the flow in this data set.
Informal feedback from a computational fluid dynamics
expert also yielded positive results. This researcher indicated that by using our visualization techniques, the
convection layers can clearly be identified. This allows the
user to better understand the time-varying behavior of this
complex data, allowing for better identification of salient
structures. With respect to the convection in a box, Fig. 16,
the user is able to clearly see the nested structure of the flow

WOO ET AL.: FEATURE-DRIVEN DATA EXPLORATION FOR VOLUMETRIC RENDERING

1741

Fig. 16. Results of our data enhancement technique applied to visualize temporal advection in a convection simulation. The series of images (Top
and Bottom) show every second sequential time step starting from 220. (Top) Using the sinusoidal transfer function method from Svakhine et al. [31].
(Bottom) Our semiautomatic transfer function generation procedure. Note that the layers in the bottom image visualize several different structures of
the flow, particularly near the top portions of the image.

gradients. By allowing users to interactively investigate and
pinpoint boundaries of interest, we can create visualizations
that show global structures without occluding regions of
interest that are limited to a more local region of interest.
We also applied our methods to the tornado data. Fig. 15
shows the nested structure of the flow and enables users to
pinpoint the structure of the gradients. Visualizing flow in
this manner is a powerful means for viewing these nested
structures and how they change over time. Note that for all
temporal data sets, a single time step was used to calculate
the visual parameters. As such, a static transfer function is
used for all time step renderings in order to keep coherency
between the mapping of color and opacity to a particular
scalar or gradient value. Depending on the time step chosen
to generate the transfer function, occlusion of structures in
future time steps may occur. By keeping the color mapping
coherent (i.e., for all time steps, the same color maps to the
same scalar value), as the data change, some structures may
be lost. As such, the user could recolor the data based on the
ray profile at any time step. Future work will focus on
solutions to this issue of the tradeoff between color
coherency and the occlusion of structures.

5.3 Noisy Data
While our previous examples showed smooth data sets,
noisy data sets require further user intervention to generate
useful results. Other parameters, such as the median
window size, can be modified by the user to interactively
select an optimal size that generates the best results. This is
demonstrated in Fig. 10 where a noisy data set is rendered
without any filtering in Fig. 10a. Application of the 3  3,
5  5, and 9  9 size median filter yields better results as
shown in Figs. 10b to 10d. However, as we increase the
window size, the rendered image starts losing important
feature lines. Therefore, a default size of 3  3 is adopted in
our implementation.
Fig. 17 shows another example with feature boundaries
enhanced by our default rendering parameters. While these
initial results are good at showing overall feature boundaries,
in some cases, the default rendering parameters will not
produce a clear visualization. This is illustrated in Fig. 18a.
Here we show a rendering example using noisy data where
the default rendering parameters are suboptimal. In this case,

Fig. 17. Images of (a) bonsai data set. (b) aneurism data set rendered
with default settings showing enhanced boundary areas.

Fig. 18. Images of the feet data set rendered (a) with the default systemgenerated settings, and (b) after the user adjusts the emphasis points.

1742

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

the user may edit the transfer function to obtain better results,
as seen in Fig. 18b.
We explored both Median and Gaussian filters using
guided exploration with expert users. Users expressed that
they could tell no difference between the applied filters and
were likely to simply use the default values in the cases
where filtering is needed.

[2]

6

[6]

CONCLUSION AND FUTURE WORK

In this paper, we have presented a data exploration method
for volumetric rendering using value profiles and contour
lines. We devised methods to automatically provide default
rendering parameter settings based on the features in the
region of interest. This technique is applicable to interval
and ratio data types with or without noise to support a
variety of volumetric data sets, such as medical data and
scientific simulation data. Based on the data types, we
extract feature layers from the data based on a value profile
and provide contour lines from the obtained feature layers.
The extracted contour lines help to illustrate the shape of
the data set in the region of interest.
To support data sets with noise, we denoise the value
profile and the image captured on a slice view by filtering
them with a feature preserving nonlinear filter (median or
bilateral filter). We also modulate the opacities based on the
feature layers to emphasize boundary features and maximum
intensities in the region of interest. Moreover, we parameterized a weight map, the radius of an ROI, and a focus line.
We plan to include the other two data types (nominal and
ordinal) [42] and add further automation to the system by
providing users with an optimal viewpoint that can show
the important data features by default [43]. Further, we also
plan to extend our data exploration approach to support
multidimensional transfer functions. Since a slice view has
more sampled voxels, 2D ROI specification based on slices
will be useful to generate a local feature-driven 2D transfer
function using nonparametric clustering algorithms (e.g.,
kernel density estimation) [28]. Illustrative visualization
techniques have been used to convey important features of
data sets by abstracting away unnecessary details [18], [37].
We would like to incorporate some of these techniques to
improve users’ perception [44] of the extracted feature
layers. Also, since scientific simulations typically return
multivariate data sets, we plan to extend our technique to
look at the ray profile across several variables at once. This
will generate a variety of layers, and exploration will be
done on ways to merge layers for enhanced visualization.

[3]
[4]

[5]

[7]

[8]

[9]

[10]

[11]

[12]
[13]
[14]

[15]
[16]

[17]
[18]
[19]

[20]

ACKNOWLEDGMENTS
This work has been supported by the US Department of
Homeland Security’s VACCINE Center under Award Number 2009-ST-061-CI0001 and the US National Science Foundation (NSF) under Grants 0328984, 0121288, and 0906379.

REFERENCES
[1]

C. Catlett, W.E. Allcock, P. Andrews, and R. Aydt, “TeraGrid:
Analysis of Organization, System Architecture, and Middleware
Enabling New Types of Applications,” Proc. HPC and Grids in
Action, 2007.

[21]
[22]
[23]
[24]
[25]

VOL. 18,

NO. 10,

OCTOBER 2012

The Open Science Grid Consortium, http://www.openscience
grid.org/, 22 Apr., 2011.
nanoHUB.org, http://www.nanohub.org/ 22 Apr., 2011.
G. Klimeck, S.S. Ahmed, N. Kharche, M. Korkusinski, M. Usman,
M. Prada, and T. Boykin, “Atomistic Simulation of Realistically
Sized Nanodevices Using Nemo 3D Part II: Applications,”
http://nanohub.org/resources/3825, 22 Apr., 2011, Jan. 2008.
J. Kniss, G. Kindlmann, and C. Hansen, “Multidimensional
Transfer Functions for Interactive Volume Rendering,” IEEE
Trans. Visualization and Computer Graphics, vol. 8, no. 3, pp. 270285, July-Sept. 2002.
NVIDIA Corporation, “NVIDIA CUDA Compute Unified Device
Architecture,” http://www.nvidia.com/object/cuda_home_
new.html, 22 Apr., 2011, Nov. 2007.
J. Kniss, G. Kindlmann, and C. Hansen, “Interactive Volume
Rendering Using Multi-Dimensional Transfer Functions and
Direct Manipulation Widgets,” Proc. Conf. Visualization, pp. 255562, 2001.
J. Marks, B. Andalman, P.A. Beardsley, W. Freeman, S. Gibson, J.
Hodgins, T. Kang, B. Mirtich, H. Pfister, W. Ruml, K. Ryall, J.
Seims, and S. Shieber, “Design Galleries: A General Approach to
Setting Parameters for Computer Graphics and Animation,”
Computer Graphics, vol. 31, pp. 389-400, 1997.
M. Tory, S. Potts, and T. Möller, “A Parallel Coordinates Style
Interface for Exploratory Volume Visualization,” IEEE Trans.
Visualization and Computer Graphics, vol. 11, no. 1, pp. 71-80, Jan./
Feb. 2005.
C. Rezk-Salama, M. Keller, and P. Kohlmann, “High-level User
Interfaces for Transfer Function Design with Semantics,” IEEE
Trans. Visualization and Computer Graphics, vol. 12, no. 5, pp. 10211028, Sept./Oct. 2006.
Y. Wu and H. Qu, “Interactive Transfer Function Design Based on
Editing Direct Volume Rendered Images,” IEEE Trans. Visualization and Computer Graphics, vol. 13, no. 5, pp. 1027-1040, Sept./Oct.
2007.
S. Bruckner and M.E. Gröller, “Style Transfer Functions for
Illustrative Volume Rendering,” Computer Graphics Forum, vol. 26,
no. 3, pp. 715-724, Sept. 2007.
C.L. Bajaj, V. Pascucci, and D.R. Schikore, “The Contour
Spectrum,” Proc. Conf. Visualization, pp. 167-173, 1997.
C. Lundström, A. Ynnerman, P. Ljung, A. Persson, and H.
Knutsson, “The Alpha-Histogram: Using Spatial Coherence to
Enhance Histograms and Transfer Function Design,” Proc.
Eurographics/IEEE-VGTC Symp. Visualization, pp. 227-234, 2006.
C. Correa and K.-L. Ma, “Size-Based Transfer Functions: A New
Volume Exploration Technique,” IEEE Trans. Visualization and
Computer Graphics, vol. 14, no. 6, pp. 1380-1387, Nov./Dec. 2008.
L. Wang, Y. Zhao, K. Mueller, and A. Kaufman, “The Magic
Volume Lens: An Interactive Focus+Context Technique for
Volume Rendering,” Proc. IEEE Conf. Visualization, pp. 367-374,
2005.
A. Lu, R. Maciejewski, and D.S. Ebert, “Volume Composition
Using Eye Tracking Data,” Proc. Eurographics/IEEE-VGTC Symp.
Visualization, pp. 115-122, 2006.
W. Li, L. Ritter, M. Agrawala, B. Curless, and D. Salesin,
“Interactive Cutaway Illustrations of Complex 3D Models,”
ACM Trans. Graphics, vol. 26, no. 3, pp. 31-40, 2007.
I. Viola, A. Kanitsar, and M.E. Gröller, “Importance-Driven
Feature Enhancement in Volume Visualization,” IEEE Trans.
Visualization and Computer Graphics, vol. 11, no. 4, pp. 408-418,
July-Aug. 2005.
T. Ropinski, J.-S. Praßni, F. Steinicke, and K.H. Hinrichs, “StrokeBased Transfer Function Design,” Proc. IEEE/EG Int’l Symp.
Volume and Point-Based Graphics, pp. 41-48, 2008.
C. Rezk-Salama and A. Kolb, “Opacity Peeling for Direct Volume
Rendering,” Computer Graphics Forum, vol. 25, no. 3, pp. 597-606,
2006.
M.M. Malik, T. Möller, and M.E. Gröller, “Feature Peeling,” Proc.
Graphics Interface, pp. 273-280, 2007.
C.D. Correa and K.-L. Ma, “Visibility-Driven Transfer Functions,”
Proc. IEEE Pacific Visualization Symp., pp. 177-184, 2009.
P. Kohlmann, S. Bruckner, A. Kanitsar, and M.E. Gröller,
“Contextual Picking of Volumetric Structures,” Proc. IEEE Pacific
Visualization Symp., pp. 185-192, 2009.
H. Akiba and K.-L. Ma, “A Tri-Space Visualization Interface for
Analyzing Time-Varying Multivariate Volume Data,” Proc.
Eurographics/IEEE-VGTC Symp. Visualization, pp. 115-122, 2007.

WOO ET AL.: FEATURE-DRIVEN DATA EXPLORATION FOR VOLUMETRIC RENDERING

[26] H. Akiba, K.-L. Ma, J.H. Chen, and E.R. Hawkes, “Visualizing
Multivariate Volume Data from Turbulent Combustion Simulations,” Computing in Science and Eng., vol. 9, no. 2, pp. 76-83, Mar.Apr. 2007.
[27] C. Muelder and K.-L. Ma, “Interactive Feature Extraction and
Tracking by Utilizing Region Coherency,” Proc. IEEE-VGTC Pacific
Visualization Symp., pp. 17-24, Apr. 2009.
[28] R. Maciejewski, I. Woo, W. Chen, and D. Ebert, “Structuring
Feature Space: A Non-Parametric Method for Volumetric Transfer
Function Generation,” IEEE Trans. Visualization and Computer
Graphics, vol. 15, no. 6, pp. 1473-1480, Nov./Dec. 2009.
[29] C. Ware, Information Visualization: Perception for Design. Morgan
Kaufmann, 2004.
[30] S.S. Stevens, “On the Theory of Scales of Measurement,” Science,
vol. 103, no. 2684, pp. 677-680, 1946.
[31] N. Svakhine, Y. Jang, D. Ebert, and K. Gaither, “Illustration and
Photography Inspired Visualization of Flows and Volumes,” Proc.
IEEE Conf. Visualization, pp. 687-694, 2005.
[32] J. Krüger, J. Schneider, and R. Westermann, “ClearView: An
Interactive Context Preserving Hotspot Visualization Technique,”
IEEE Trans. Visualization and Computer Graphics, vol. 12, no. 5,
pp. 941-948, Sep./Oct. 2006.
[33] M. Harrower and C.A. Brewer, “Colorbrewer.org: An Online Tool
for Selecting Colour Schemes for Maps,” Cartographic J., vol. 40,
no. 1, pp. 27-37, June 2003.
[34] I. Viola, A. Kanitsar, and M.E. Gröller, “Hardware-based Nonlinear Filtering and Segmentation Using High-level Shading
Languages,” Proc. IEEE Conf. Visualization, pp. 309-316, 2003.
[35] G. Kindlmann and J.W. Durkin, “Semi-Automatic Generation of
Transfer Functions for Direct Volume Rendering,” Proc. IEEE
Symp. Volume Visualization (VVS ’98), pp. 79-86, 1998.
[36] J. Zhou, M. Hinz, and K.D. Tönnies, “Focal Region-guided
Feature-Based Volume Rendering,” Proc. First Int’l Symp. 3D Data
Processing, Visualization, and Transmission, pp. 87-90, 2002.
[37] D. Ebert and P. Rheingans, “Volume Illustration: Nonphotorealistic Rendering of Volume Models,” Proc. IEEE Conf.
Visualization, pp. 195-202, 2000.
[38] W.E. Lorensen and H.E. Cline, “Marching Cubes: A High
Resolution 3D Surface Construction Algorithm,” Proc. Conf.
Computer Graphics and Interactive Techniques, pp. 163-169, 1987.
[39] D. Horn, “Stream Reduction Operations for GPGPU Applications,” Proc. GPU Gems 2: Programming Techniques for HighPerformance Graphics and General-Purpose Computation, pp. 573583, 2007.
[40] M. Harris, S. Sengupta, and J.D. Owens, “Parallel Prefix Sum
(scan) with CUDA,” GPU Gems 3, H. Nguyen, ed. Addison
Wesley, pp. 851-876, 2007.
[41] HDR Image Processing Library, http://courses.ece.uiuc.edu/
ece498, 22 Apr., 2011.
[42] L.D. Bergman, B.E. Rogowitz, and L.A. Treinish, “A Rule-Based
Tool for Assisting Colormap Selection,” Proc. Conf. Visualization,
pp. 118-125, 1995.
[43] S. Takahashi and Y. Takeshima, “A Feature-Driven Approach to
Locating Optimal Viewpoints for Volume Visualization,” Proc.
IEEE Conf. Visualization, pp. 495-502, 2005.
[44] M.-Y. Chan, Y. Wu, W.-H. Mak, W. Chen, and H. Qu, “PerceptionBased Transparency Optimization for Direct Volume Rendering,”
IEEE Trans. Visualization and Computer Graphics, vol. 15, no. 6,
pp. 1283-1290, Nov./Dec. 2009.

1743

Insoo Woo received the BS degree in computer
engineering in 1998 from Dong-A University in
South Korea. He is currently working toward the
PhD degree in the School of Electrical and
Computer Engineering at Purdue University and
a research assistant in the Purdue University
Rendering and Perception Lab. He worked as a
software engineer from 1997 to 2006. His
research interests include GPU-aided Techniques for Computer Graphics and Visualization.
He is a member of the IEEE.

Ross Maciejewski received the PhD degree in
electrical and computer engineering from Purdue University in December, 2009. He is
currently an assistant professor at Arizona State
University in the School of Computing, Informatics & Decision Systems Engineering. Prior to
this, he served as a visiting assistant professor
at Purdue University and worked at the Department of Homeland Security Center of Excellence
for Command Control and Interoperability in the
Visual Analytics for Command, Control, and Interoperability Environments (VACCINE) group. His research interests include geovisualization, visual analytics, and nonphotorealistic rendering. He is a member
of the IEEE and the IEEE Computer Society.
Kelly P. Gaither received the masters and
bachelors degree in computer science from
Texas A&M University in 1992 and 1988,
respectively, and the doctoral degree in computational engineering from Mississippi State University in May, 2000. While working toward the
PhD degree, she worked full time at the
Simulation and Design Center in the National
Science Foundation Engineering Research Center as the leader of the visualization group. She
is the director of Data & Information Analysis at the Texas Advanced
Computing Center (TACC), is leading the scientific visualization, data
management & collections, and data mining & statistics programs at
TACC while conducting research in scientific visualization and data
analysis. She is a research scientist, also serves as the area director for
visualization in the National Science Foundation funded TeraGrid
project. She has a number of refereed publications in fields ranging
from Computational Mechanics to Supercomputing Applications to
Scientific Visualization. She has given a number of invited talks. Over
the past 10 years, she has actively participated in the IEEE Visualization
conference and served as the IEEE Visualization conference general
chair in 2004. She is currently serving on the IEEE Visualization and
Graphics Technical Committee. She is a member of the IEEE.
David S. Ebert received the PhD degree in
computer science from Ohio State University.
He is the silicon valley professor in the School of
Electrical and Computer Engineering at Purdue
University, a University Faculty Scholar, the
director of the Purdue University Rendering
and Perceptualization Lab, and the director of
the Visual Analytics for Command, Control and
Interoperability Environments Center of Excellence. His research interests include novel
visualization techniques, visual analytics, volume rendering, information
visualization, perceptually based visualization, illustrative visualization,
and procedural abstraction of complex, massive data. He is a fellow of
the IEEE and the IEEE Comptuer Society, and a member of the IEEE
Computer Society’s Board of Governors.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

,(((,QWHUQDWLRQDO&RQIHUHQFHRQ%LJ'DWD

A Novel Visual analytics Approach for Clustering Large-Scale Social Data
Zhangye Wang
State Key Lab of CAD&CG
Zhejiang University
Hangzhou, China
zywang@cad.zju.edu.cn
Chang Chen
College of Software Engineering
University of Science and Technology of China
Hefei, China
lawangel@163.com

Juanxia Zhou
State Key Lab of CAD&CG
Zhejiang University
Hangzhou, China
xjzhou@zjucadcg.zju.edu.cn
Jiyuan Liao
State Key Lab of CAD&CG
Zhejiang University
Hangzhou, China
jyliao@zjucadcg.zju.edu.cn

Ross Maciejewski
School of CIDSE
Arizona State University
USA
rmacieje@asu.edu

type of data as user tag data.
Given the ubiquity of such data, methods for exploring
and analyzing social data links have become a critical
research topic. By analyzing user tags and user behavior
data, it is expected that insights into societal patterns can
be gleaned and used to inform governmental policy and
business decisions as well as influence human behavior [2].
For example, previous work [3] explores clustering users
into groups to develop targeted market strategy based on
demographics and behavioral patterns. However, our ability to extract knowledge from these large heterogeneous
data sources is still limited due to the latent, subtle and
unpredictable relationships that may be hidden within the
data. These problems are further compounded due to issues
of data size and the non-linear computational complexity
related to the data size.
One means of reducing the computational complexity of
an analysis task is through the application of parallelization
using distributed computing [4] and super-computing [5].
However, for analysis tasks that require frequent data
transfer and interaction, parallelization is still problematic.
Another means of reducing the computational complexity
would be through the use of divide-and-conquer schemes.
The divide-and-conquer algorithm works by recursively
breaking down a problem into sub-problems that can then
be recombined to address the original problem.
Recent work has demonstrated the effectiveness of divideand-conquer approaches for handling large data. For example, RHIPE [6] is built upon R and has been specifically
developed to provide a divide-and-conquer solution for the
statistical analysis of large data. RHIPE utilizes the MapReduce framework, and MapReduce itself has been used in
large scale data analysis (e.g., [5]). Furthermore, in the
data mining community, it is believed that the divide-andrecombine scheme will be adapted to handle the scalability
problems found in many complex machine learning tasks [7].

Abstract—Social data refers to data individuals create that is
knowingly and voluntarily shared by them and is an exciting
avenue into gaining insight into interpersonal behaviors and
interaction. However, such data is large, heterogeneous and
often incomplete, properties that make the analysis of such
data extremely challenging. One common method of exploring
such data is through cluster analysis, which can enable analysts
to find groups of related users, behaviors and interactions. This
paper presents a novel visual analysis approach for detecting
clusters within large-scale social networks by utilizing a divideanalyze-recombine scheme that sequentially performs data
partitioning, subset clustering and result recombination within
an integrated visual interface. A case study on a microblog
messaging data (with 4.8 millions users) is used to demonstrate
the feasibility of this approach and comparisons are also
provided to illustrate the performance benefits of this approach
with respect to existing solutions.
Keywords-Divide and Recombine; Cluster Analysis; Kmeans; Visual Analysis;

I. I NTRODUCTION
As humans have become more intrinsically connected
to technology, details of their behaviors and interpersonal
connections have become increasingly transparent. Activities
and communications can automatically be collected during
interpersonal activities such as mobile phone communications, e-commerce transactions and internet activity (Tweets,
Facebook posts, blog entries), and such data can be classified
into two components: behavioral data and personal data.
Behavioral data can be thought of as a record of a user
activity within society, for example, a mobile phone call
between individuals, a video conference, and sending a
tweet all provide links between a single user and other
individuals. Such data tends to reflect a user’s daily patterns
and behaviors [1], and we will refer to this type of data as
user behavior data. Personal data can be thought of as being
intrinsically tied to a single individual and would include
things such as their age and gender. We will refer to this

978-1-4799-1293-3/13/$31.00 ©2013 IEEE

Wei Chen
State Key Lab of CAD&CG
Zhejiang University
Hangzhou, China
chenwei@cad.zju.edu.cn



This paper presents our proposed methodology for enhancing the visual analysis of large-scale social data utilizing the divide-and-recombine scheme. Our approach is
specifically designed to handle a central problem for social
network analysis: clustering users into groups by leveraging
the user tags and user behavior information. The key idea
is to sequentially perform data partitioning, subset clustering and result recombination utilizing an integrated visual
interface. The entire pipeline integrates a suite of visual
analysis techniques to provide an effective workspace for
the partitioning of large collections of instances, determining
cluster parameters and merging multiple clusters. Our main
contributions include:
• The identification and systematic implementation of the
divide-and-recombine scheme for clustering of social
data in an integrated visual exploration process;
• An incremental data clustering technique that enhances
the conventional K-means algorithm in both the clustering quality and performance;
• A novel context-aware subset-clustering analysis.

problems including matrix factorization and statistical inference have been modified to utilize a divide-and-conquer
methodology [7]. For example, the RHIPE system that is
built on R utilizes an underlying Hadoop structure and has
been applied to a variety of analytical domains [6]. Results
from this work demonstrate the feasibility of utilizing divideand-conquer for exploring large scale data.
C. Social Data Analysis

II. R ELATED W ORK

Recently, many tools have been developed to explore
the complex spatiotemporal relationships underlying such
data. Work by Field et al. [14] explored the analysis of
microblog data and others like crime investigators [15] and
urban planners [16] have also begun utilizing social data.
MacEachren et al. [17] developed Senseplace as a tool for
exploring the message density of actual or textually inferred
Twitter message locations, ScatterBlogs [18] presented a
scalable system enabling analysts to work on quantitative
findings within a large set of Microblog messages.However,
no previous work in this area (to our knowledge) utilizes a
divide-and-recombine scheme for large scale clustering of
users and behaviors in social data analysis.

In this section, we will briefly review related works in
each of these categories.

III. A V ISUAL A NALYSIS A PPROACH FOR C LUSTERING
L ARGE -S CALE S OCIAL DATA
The focus of this work is on large-scale (multi-million)
user data sets. As input, our social data contains a set of user
provided information (age, gender, etc.) that we denote as
user tags and their associated behavioral information which
we define as user behavior. The user behavior information
provides links between users.
Within such data, there are expected to be clusters of
individuals and patterns which can be analyzed for marketing purposes, criminal activity or various other research
questions. A key challenge in clustering such data is that
the application of clustering algorithms to a set of user tags
requires the formulation of user tag and behavior information into a point data set where each point is associated
with multiple attributes. The set of attributes form a user
attribute vector. Note that the construction of the user
attribute vectors varies with specific data and tasks, and
details for formulating this vector for each dataset will be
given in the case study. Our goal is to provide analysts
with a means of clustering the user attribute vectors to find
commonalities between users and behaviors. Conceptually,
our approach consists of four stages (Figure 1): data division,
data clustering, result recombination, and visual exploration.
In the first stage, a massive collection of user attribute
vectors is generated from the social data. It is randomly
subdivided into many subsets by means of a visually assisted
data division process. In the second stage, a subset of the
data is clustered using an improved k-means algorithm, in
which a novel context-aware technique is leveraged to optimize the clustering parameters. Subsequently, the result is

A. Data Clustering
Recently, the application of interactive supervised learning
has become a major focus in the visual analytics community.
Pelekis et al. [8] designed a novel distance function as
a similarity measurement for the analysis of movement
data. Guo et al. [9] proposed an approach for steering the
clustering process by building a hierarchical spatial cluster
structure within the high-dimensional feature space.
Unsupervised learning algorithms can be divided into
two major categories: hierarchical clustering and partitioning
clustering [10]. Perhaps the most commonly used algorithm
for partitioning data is the K-means algorithm [11]. Unfortunately, such partitioning algorithms tend to be nonlinear in their runtime, despite efforts to improve such
techniques [12]. As the computational complexity increases
non-linearly with the data size, new solutions are necessary.
There are also some pioneered work on parallel clustering algorithms. For instance, Zhao et al. [13] proposed a
parallel K-means clustering algoithm based on MapReduce
framework. Yet, this
B. Divide-And-Conquer for Data Analysis
Divide-and-Conquer has proven to be an efficient mechanism for reducing the runtime of complicated procedures
such as sorting, the multiplication of large numbers and
discrete Fourier transforms. Recently, it has been applied
to address the underlying issues of computational complexity associated with big data. Several fundamental statistics

80

Adaptive Data Division

Incremental Data Clustering

Result Recombination
......

...

. . .. . .

Subset 2
Clusters: 1

2

k

Outliers

k

Outliers

k

Outliers

......

Subset N
Clusters: 1 2

The entire user set

......

......

Context-aware Subset Clustering
......

Clusters: 1 2

......

Parameters

Subset 1
Pixel chart view

Social data

Visual Exploration and Interactions

1

2

...

Clusters:

k

Outliers:

Figure 1. Conceptual overview of our approach. (a) Visual assisted adaptive data division; (b) Context-aware subset clustering; (c) Incremental recombination
of clusters; (d) Visual exploration of clusters and unusual data points.

propagated into the other subsets one-by-one and clustering
is then done on each subset. In the third stage, the clusters
of all subsets are then merged with a hierarchical clustering
process. An integrated visual interface is designed to allow
analysts to explore the user tags and associated user behavior
information of each cluster. Compared with existing divideand-recombine approaches, our approach combines data
mining techniques and interactive visuals, which enables the
analyst to explore patterns in large-scale social data.

sum of the distances between each point pair in the clusters:
D(K) =

k

∑ ∑

r=1 xi ,x j ∈Xr

xi − x j 

(1)

The clustering quality depends on the specification of
the initial centroids [19]. Typical K-means solutions either
repeat the computation of centroids several times using
different initial seed points and then choose optimal result
based on the quality of the clusters, or apply specific
optimization which can result in long runtimes and storage
overheads when handling large-sized data. Our approach to
improving the performance of K-means in large-scale data is
to utilize a divide-and-recombine scheme to reduce the data
complexity of K-means clustering. Furthermore, in order to
adapt the K-means algorithm specifically to social data, there
are several key aspects that need to be addressed:

A. K-Means Clustering
One of the key parts of our data analysis process is the utilization of the K-means algorithm. The K-means algorithm
is one of the most popular clustering methods [10], and is
employed in our approach due to its simplicity, efficiency
and robustness. For a set of points X = {xi |xi ∈ Rd }, K
data points are randomly selected as the initial centroids
of the intended K clusters. Two steps are then recursively
performed until the algorithm converges:
• Step 1: Assign each data point to its closest centroid.
• Step 2: Update the centroids with the mean value of all
the data points assigned to them.
Suppose that the resultant clusters are X1 , X2 , ..., XK . The
quality of the clustering subject to K can be measured by the

•

•

81

Data Division How to determine the appropriate subset
size so that the combination of clustered results of the
subsets can yield the same quality as the conventional
K-means algorithm?
Data Clustering How to select K and a distance metric
for multi-dimensional user attribute vectors to achieve
optimal clustering results?

•

Recombination and Analysis How to evaluate the
clustering quality and discover interesting users and
patterns from the clustered results?

determining an optimal subset size. Figure 3 shows a group
of pixel charts with a subset size 50.

B. Adaptive Data Division
In terms of clustering, one criteria for dividing a massive
dataset into smaller subsets is to preserve the statistical
distribution in each subset [20], which poses a challenging
problem in determining initial splits in the divide-andrecombine process. We propose to randomly subdivide the
entire set into subsets with uniform sizes [10] and adaptively
determine the subset size by considering both the performance and quality.
Our solution is to employ the use of a visually assisted
adaptive data division scheme. For each subset, a pixel
chart [21] is generated to show the statistical distribution of
each attribute value of the user attribute vector. In particular,
all subsets are organized into a 2D array, which is further
represented with a set of 2D cells. The saturation of each
cell encodes the percentage of users with the corresponding
attribute value in each subset, as illustrated in Figure 2.

Figure 3. The pixel charts with respect to 42 user attribute values with
50 subsets.

The algorithm for adaptive data division is summarized
as follows:

Cluster ID

1

2

3

4

5

6

7

8

9

Algorithm 1 Visual Assisted Adaptive Data Division
Initialization: set a small subset size Ns (e.g., 5K for a set
with 1M points) to generate a relatively large number of
subsets (e.g., 200).
while TRUE do
Generate a pixel chart for each attribute.
Visually explore the pixel charts of all attributes.
if All pixel charts show similar appearance then
Return the current subset size and subsets.
FALSE.
else
Ns ++.
end if
end while

1

0
Percentage

Figure 2. A pixel chart visualizes the statistical distribution of certain
attribute with respect to a subset size. Here, each cell represents a subset,
and encodes the statistical value of the subset.

If the pixel chart of an attribute exhibits an approximately
uniform appearance, one can assume that the subset has a
statistical distribution that is similar to the overall distribution of a given attribute. If the subset size is appropriate
for all attributes, then one can assume that it represents
a reasonable statistical sample of the original dataset with
respect to the specific user attribute vector. In this manner,
an analyst can quickly assess the suggested subdivision and
modify the current data division to try and improve the
results. Surely, statistical sampling techniques would work
in these situations, but may require more user interaction
time. In order to enable such actions, our system provides
simple user interactions in the pixel chart view: specifying
the subset size, studying the distribution of a specific subset
size, comparing the distributions of different subsets, and

C. Context-aware Subset Clustering
Once the analyst is satisfied with the data subset choice,
the data subsets then enter the clustering phase. In clustering
the first subset, two parameters are essential to the clustering
quality: the cluster number, K; and the distance metric for
the points.
Cluster Number K There have been many methods to
find an optimal cluster number K [22]. In our system, K
is heuristically determined by computing and plotting the
sum of the distance function D(K) (Equation 1). Typically,
the point where the curve becomes flat (shown in red in
Figure 4) is identified as a reasonable choice for K [22].
Attribute Weights K-means partitions the points based
on the distances between points in the high-dimensional

82

Distance

250

The set of {H(m, n), m = 1, 2..., M, n = 1, 2, 3, ..., M} forms
an M × M matrix. By normalizing and encoding each element of the matrix with a color, a sensitivity map associated
with the underlying attribute is generated (Figure 5). This
mapping implies the proximity among the clustered results
under different weighting configurations. A low value of
H(m, n) indicates that two weighting configurations yield
similar clustered results, and vice versa. The overall distribution of the sensitivity map and its average value can be
used to study the relevance of the attribute to the clustering.
A sensitivity map can be generated for each attribute. The
set of the maps characterizes the influence of a set of user
attributes on the variation of the clustering process and can
be used to guide the weight function. The weight of each
attribute is specified as to be proportional to the average
value of its associated map. The user can manually adjust
the sensitivity of a map by moving it vertically. The weights
corresponding to the sensitivities are shown with a radial
map.

20

15

10

5

0

1

2

3

4

5

6

7

8

9

10

11

12

13

14

Figure 4. The relationship between the cluster number K (the x axis) and
the sum of distance D(K) (the y axis).

space, and the choice of distance metric chosen will greatly
influence the results. In this case, we cluster the underlying
dataset based on the user attribute vectors, where the problem is identical to specifying an appropriate weight for each
attribute: a high weight can be used for a salient attribute,
and an attribute has little influence on the clustering if its
weight is low. For clarity we define the weights of a user attribute vector to be x = (x1 , x2 , ..., xd ) as w = (w1 , w2 , ..., wd ).
However, the influence of each attribute on the clustering result and the relations among attributes are difficult
to model and represent. Furthermore, attributes may have
different data types (numerical, ordinal and categorical) and
distinctive value ranges. Thus, it is desirable to discover the
significantly relevant attributes and associated configurations
by studying the influences of different weights on the clustering results, and design an enhanced weight computation
scheme.
In our approach, the weight of an attribute subject to a
clustering process is determined by evaluating the sensitivity
of the clustered result with respect to different weights. For
the jth attribute, a sequence of weighting configurations
{w(k), k = 1, 2, 3, ..., M} is generated:
 l
if
l = j;
w (k) = 1/k,
(2)
wl (k) = (1 − 1/k)/(M − 1), else.

x∈C(m) y∈C(n)

1.0

weight

0.1

1.0

0

where M is an adjustable constant.
For each weighting configuration {w(k)}, a K-means
clustering is performed by using the weights in the distance
metric. We further compute the Hausdorff Distance [23]
between the centroid sets of the clustered results C(m) and
C(n) associated with {w(m)} and {w(n)}:
H(m, n) = max min x − y

weight

0.1

Figure 5.

sensitivity

1

A sensitivity map is generated for each attribute.

D. Incremental data clustering
The first subset can be either randomly selected, or can
be manually specified by the users with the help of the
pixel charts of subsets. After clustering the first subset, an
incremental data clustering procedure is applied to cluster
all other subsets. The clustering configuration (the weights,
etc.) for the first subset is applied in the clustering process of

(3)

where x and y denote a centroid of the clustered results C(m)
and C(n), respectively.

83

2000

all other subsets. Algorithm 2 briefly summarizes the details.

Sum of Distances
Standard
Incremental

Algorithm 2 Incremental Data Clustering
for Each subset do
Set the clustered centroids of the first subset as the
initial centroids.
Set the cluster number and attribute weights of the first
subset as the ones for the underlying subset.
Perform the K-means clustering.
end for

1500 1

2 3

4

5

6

7 8

9

10 11 12 13 14 15 16 17 18

cluster Number

(a)
30

Time (s)

25

Incremental
Standard

The incremental data clustering scheme not only allows us
to utilize the divide-cluster-recombine mechanism, but it also
achieves higher performance and quality when compared
with the standard K-means algorithm. Figure 6 (a) compares
the sum of point-to-centroid distances of the cluster results
of both our approach and the standard K-means algorithm.
In principal, the incremental data clustering scheme achieves
higher accuracy because the cluster centroids in clustering
each subset are determined in handling the first subset. With
the standard K-means algorithm, the cluster centroids are
randomly initialized when clustering each subset, leading to
varied clustering results [24]. The benefit of the incremental
scheme is also significant with respect to the running time.
Figure 6 (b) reports the total running time of both approaches
for the same dataset. In general, our approach runs faster
with different cluster numbers K, and achieves an average
10% - 100% acceleration. More importantly, the clustering
of subsets are parallelizable because the subsets are independent. Concrete performance analysis will be given in the
case study.
To allow for visual exploration of the clusters of each
subset, our system employs the principle component analysis
(PCA) algorithm to project all data points of a subset or the
entire set into the 2D space. PCA is chosen because it is
fast with a linear computational complexity, and can handle
a large amount of points.

20

15

10

5

0 1

2 3

4

5

6

7 8

9

10 11 12 13 14 15 16 17 18

Cluster Number

(b)

Figure 6. Comparing the sum of point-to-centroid distances (a) and the
running time (b) of our incremental data clustering scheme (in dark blue)
and the standard K-means algorithm (in light blue).

Algorithm 3 Hierarchical clustering of the clusters of all
subsets
Initialization: Load all clusters of all subsets.
while TRUE do
Compute the distances between each pair of all clusters
of all subsets.
Combine two clusters whose distance is the minimal.
if There are only K clusters then
Output the clusters.
FALSE.
else
TRUE.
end if
end while

E. Result Recombination
After all the subsets are clustered, we utilize the standard
hierarchical clustering method [10] to recombine the results
of all subsets. Again, the cluster number K for clustering
the entire set is set to be the same as that of the first subset.
The distance between cluster A and cluster B is defined as:
Δ(A, B) =

IV. C ASE S TUDY
Our clustering algorithm and visual interface were developed using Java. A parallelization to our clustering kernel
is also implemented with the multi-thread feature of Java.
We also implemented the standard K-means algorithm. The
three implementations are named as DR, PDR and STD. The
performance reports are made on a PC with an i7 3.40 GHZ
CPU, 16 G memory, and an Nvidia 680 video card.
We have conducted experiments on a Microblog Dataset
consisting of 4.8 million users. It was collected from the
largest social network (http://www.weibo.com) in China, i.e.,

n(A)n(B)
centroid(A) − centroid(B)2 (4)
n(A) + n(B)

where is n(·) denotes the size of a cluster, and centroid(·)
denotes the cluster centroid.
The key idea of the hierarchical clustering algorithm is to
recursively combine all clusters (see Algorithm 3).

84

2000000

Sina Weibo. The total data sizes is 399 MB.
In particular, the Microblog Dataset consists of 4,838,573
users and 18 user attributes. In our experiments, six attributes
are used: OnlineTime, Bi-friendNumber, FollowerNumber,
FriendNumber, PostNumber and FavouritePostNumber. Table I lists the meaning of these attributes. The entire set
is divided into 76 subsets, each of which contains 64,000
users.

DR

1500000

1000000

500000

Table I
S IX ATTRIBUTES OF THE M ICROBLOG D ATASET
Attribute Name(Abbr.)

Meaning

OnlineTime

The time the user spend on Microblog

BiFriendNumber

the number of people with which the
user follows each other

FollowerNumber

the number of people who follow the
user to see his/ her updates

FriendNumber

the number of people the user follows

PostNumber

the number of messages the user sends
out to his/her followers through
Microblog

FavouritePostNumber

the number of microblogs the user
collects

STD

Sum of Distances

0

K
4

5

6

7

8

9

10

11

12

(a)
2000

Time (s)
PDR
DR
STD

1500

1000

500

0

Quality and Performance Applying our approach to the
dataset found that the optimal cluster number is six. Figure 7
(a) compares the clustering quality of our approach and
the standard K-means algorithm. In particular, the summed
values of point-to-centroid distances with respect to different
cluster number K are plotted.
To compare the performance, the timings with DR, PDR,
and STD with respect to different cluster number K are
displayed in Figure 7 (b). The comparison indicates that
the naive implementation of our approach achieves higher
performance than the standard algorithm. The most time
consuming part is the incremental clustering of subsets
due to the I/O operations for clustering each subset. This
inefficiency is addressed in our parallel implementation,
which yields a stable and high performance record.
Social Data Analysis In addition to the 6 clusters, a group
of outlier points are detected in the clustering process
(Figure 8). By further exploring the views of user attributes,
some interesting facts are discovered. In particular, the pixel
charts with respect to different attributes enable us to identify
two specific groups of user patterns. One group has very high
attribute values, while the other has low values. Commonly
they have a high value of FollowerNumber, but a low
value of FriendNumber (almost near zero). In addition, their
BiFriendNumbers are zero, and their registered addresses are
all overseas.

K
4

5

6

7

8

9

10

11

12

(b)

Figure 7. Results for the Microblog dataset. (a) The clustering quality
comparison of DR and STD; (b) The performance comparison of DR, PDR
and STD.

analytics pipeline. The entire pipeline integrates a suite of
visual analysis techniques to provide an effective workspace
for the partition of a large collection of instances, the
determination of clustering parameters, and the merging
of multiple clusters. Experimental results verify that our
approach outperforms conventional solutions in both the
quality and efficiency.
VI. ACKNOWLEDGEMENT
This paper is supported by NSFC (61232012,61272302),
National High Technology Research and Development Program of China (2012AA12090), Zhejiang Provincial Natural Science Foundation of China (LR13F020001), Doctoral
Fund of Ministry of Education of China (20120101110134).
R EFERENCES
[1] J.Moody, D.McFarland, and S. Bender-deMoll, “Dynamic
network visualization,” American Journal of Sociology, pp.
1206–1241, 2005.

V. C ONCLUSIONS

[2] J. Wei, Z. Shen, N. Sundaresan, and K.-L. Ma, “Visual cluster
exploration of web clickstream data,” in IEEE Conference on
Visual Analytics Science and Technology, 2012, pp. 3–12.

This paper presents an effective divide-and-recombine
approach for clustering massive data assisted with a visual

85

[11] Z. Ahmed and C. Weaver, “An adaptive parameter spacefilling algorithm for highly interactive cluster exploration,”
in IEEE Conference on Visual Analytics Science and Technology, July 2012, pp. 13–22.
[12] A. Vakali, J. Pokornoy, and T. Dalamagas, “An overview
of web data clustering practices,” in Proceedings of EDBT
Workshops, 2004, pp. 597–606.
[13] W. Zhao, H. Ma, and Q. He, “Parallel k-means clustering
based on mapreduce,” Cloud Computing, Springer Berlin
Heidelberg, pp. 674–679, 2009.
[14] K. Field and J. O’Brien, “Cartoblography: Experiments in
using and organising the spatial context of micro-blogging,”
Transactions in GIS, vol. 14, pp. 5–23, 2010.
[15] R. E. Roth and J. White, “Twitterhitter: Geovisual analytics
for harvesting insight from volunteered geographic information,” in GIScience, 2010.

Figure 8.
outliers.

[16] S. Wakamiya, R. Lee, and K. Sumiya, “Crowd-based
urban characterization: extracting crowd behavioral patterns
in urban areas from twitter,” in Proceedings of the 3rd
ACM SIGSPATIAL International Workshop on LocationBased Social Networks, ser. LBSN ’11. New York,
NY, USA: ACM, 2011, pp. 77–84. [Online]. Available:
http://doi.acm.org/10.1145/2063212.2063225

A group of points within the red rectangle are specified as

[3] Z. Shen and K.-L. Ma, “Mobivis: A visualization system for
exploring mobile data,” in IEEE Pacific Visualization, 2008,
pp. 175–182.

[17] A. MacEachren, A. Jaiswal, A. Robinson, S. Pezanowski,
A. Savelyev, P. Mitra, X. Zhang, and J. Blanford, “Senseplace2: Geotwitter analytics support for situational awareness,” in IEEE Conference on Visual Analytics Science and
Technology, 2011, pp. 181–190.

[4] W. A. Pike, J. Bruce, B. Baddeley, D. Best, L. Franklin,
R. May, D. M.Rice, R. Riensche, and K. Younkin, “The
Scalable Reasoning System: Lightweight Visualization for
Distributed Analytics,” Infomation Visualization, pp. 171–
184, 2008.

[18] H. Bosch, D. Thom, M. Worner, S. Koch, D. Puttmann,
D. Jackle, and T. Ertl, “Scatterblogs: Geo-spatial document
analysis,” in IEEE Conference on Visual Analytics Science
and Technology, 2011, pp. 309–310.

[5] H. Vo, J. Bronson, B. Summa, J. Comba, J. Freire, B. Howe,
V. Pascucci, and C. Silva, “Parallel visualization on large
clusters using MapReduce,” in IEEE Symposium on Large
Data Analysis and Visualization (LDAV), 2011, pp. 27–34.

[19] A. K. Jain, M. Murthy, and P. Flynn, “Data clustering: A
review,” ACM Computing Reviews, vol. 31, no. 3, pp. 264–
323, 1999.

[6] S. Guha, R. Hafen, J. Rounds, J. Xia, J. Li, B. Xi, and W. S.
Cleveland, “Large Complex Data: Divide and Recombine
with RHIPE,” The ISI’s Journal for the Rapid Dissemination
of Statistics Research, 2012.

[20] J. B. Macqueen, “Some methods of classification and analysis
of multi-variate observations,” in Proceedings of the Fifth
Berkeley Symposiumon Mathematical Statistics and Probability, July 1967, pp. 281–297.
[21] D. A. Keim and H.-P. Kriegel, “VisDB: Database Exploration using Multidimensional Visualization,” IEEE Computer
Graphics and Application, vol. 14, no. 5, pp. 40–49, 1994.

[7] M. I. Jordan, “Divide-and-conquer and statistical inference
for big data,” in ACM SIGKDD international conference on
Knowledge discovery and data mining, 2012.

[22] D. Arthur and S. Vassilvitskii, “K-means++: The Advantages
of Careful Seeding,” in Proceedings of the eighteenth annual
ACM-SIAM symposium pn Discrete algorithms, 2007, pp.
1027–1035.

[8] N. Pelekis, G. Andrienko, N. Andrienko, I. Kopanakis,
G. Marketos, and Y. Theodoridis, “Visually exploring movement data via similarity-based analysis,” Journal of Intelligent
Information Systems, vol. 38, no. 2, pp. 343–391, 2012.

[23] D. P. Huttenlocher, G. A. Klanderman, and W. J. Rucklidge,
“Comparing Images Using the Hausdorff Distance,” IEEE
Transactions on PAMI, pp. 264–323, 1933.

[9] D. Guo, D. Peuquet, and M. Gahegan, “ICEAGE: Interactive
clustering and exploration of large and high-dimensional
geodata,” GeoInformatica, vol. 7, no. 3, pp. 229–253, 2003.

[24] A. K. Jain, “Data clustering: 50 years beyond k-means,”
Pattern Recognition Letters, vol. 31, no. 8, pp. 651–666, June
2010.

[10] P. Berkhin, “Survey of clustering data mining techniques,”
Technical Report, 2002.

86

Visual Analytics Decision Support Environment for Epidemic Modeling
and Response Evaluation
Shehzad Afzal∗

Ross Maciejewski†

David S. Ebert‡

Purdue University Visualization and Analytics Center

Figure 1: The decision history tree view. As users interact in the model view, the decisions made generate a history tree. Paths of the tree
are plotted over time on the x-axis, with the y-axis representing the cumulative deviation from the baseline simulation. Mousing over on a node
brings up a thumbnail view of the decision measures implemented at that point in the simulation. Legend symbols represent mitigative response
measure types. Each symbol in the decision history tree represents the insertion point of the decision path. A unique color is assigned to each
symbol and the corresponding decision path.

A BSTRACT
In modeling infectious diseases, scientists are studying the mechanisms by which diseases spread, predicting the future course of
the outbreak, and evaluating strategies applied to control an epidemic. While recent work has focused on accurately modeling disease spread, less work has been performed in developing interactive decision support tools for analyzing the future course of the
outbreak and evaluating potential disease mitigation strategies. The
absence of such tools makes it difficult for researchers, analysts and
public health officials to evaluate response measures within outbreak scenarios. As such, our research focuses on the development
of an interactive decision support environment in which users can
explore epidemic models and their impact. This environment provides a spatiotemporal view where users can interactively utilize
mitigative response measures and observe the impact of their decision over time. Our system also provides users with a linked decision history visualization and navigation tool that support the simultaneous comparison of mortality and infection rates corresponding
to different response measures at different points in time.
∗ e-mail:

safzal@purdue.edu

† e-mail:rmacieje@purdue.edu
‡ e-mail:ebertd@purdue.edu

IEEE Symposium on Visual Analytics Science and Technology
October 23 - 28, Providence, RI, USA
978-1-4673-0014-8/11/$26.00 ©2011 IEEE

1

I NTRODUCTION

Federal, state, and local community public health officials must prepare and exercise complex plans to deal with a variety of potential
mass casualty events [1, 7, 13]. The planning stages often utilize
knowledge gained during tabletop exercises [7], or summary details
based on information and trends provided via very complex modeling. Moreover, such plans are developed with only a few specific
scenarios or pre-event concepts in mind and often ignore the fact
that the solutions dealing with a disease outbreak are very dependent on its underlying traits and actual characteristics, which may
not be known a priori.
In order to better prepare and plan for events, analysts and decision makers have begun incorporating computer based simulations
to model potential disease. These models employ a variety of parameters and output complex multivariate data which needs to be
analyzed and explored. Furthermore, as parameters are modified,
new outcomes occur, requiring further analysis to compare various scenarios. In regards to infectious disease simulations, the results need to be compared across space and time to evaluate decision measures as they are implemented over a variety of state
spaces. Analysts need to work in an environment where they can
explore the impact mitigative measures (i.e., school closures during
a pandemic, spraying pesticides for insect borne illnesses). These
decision measures are not used to determine the best solution to
the model; instead, these decision measures are placed at different
points to help analysts understand and illustrate the effects that cer-

191

tain responses will have. In this way, decision makers can better
understand the effects of delaying responses, lack of supplies for
implementing a response and the potential impact of an outbreak
under varying conditions. Figure 1 illustrates such a decision tree
analysis structure.
In this paper, we present a suite of predictive visual analytic tools
that not only provides insight into the effectiveness of a decision,
but also provides an interactive visual analytics environment for the
investigation of multiple courses of responses as well as comparisons of the effectiveness of each component of a response plan.
These tools can be utilized during training exercises to help decision makers and first responders better understand the impact of
their decisions, as well as during crisis management where model
parameters can be adjusted to model the current spread and predict
potential future outcomes.
In order to facilitate enhanced model exploration and decision
analysis, we have developed a linked spatiotemporal visual analytics tool (Figure 2) designed for advanced model simulation and exploration for epidemiologists, local public health officials and other
healthcare officials. The system was designed from its inception
in collaboration with health experts, state healthcare officials and
epidemiologists to address their needs. Our system features include
the following:
• Flexible decision history support trees that can link to multiple
simulation runs (Figure 1);
• Interactive controls for exploring decision measures and decision points within the simulation;
• Simulation replay and path exploration visualizations for decision analysis.
As part of the simulation, users may interactively deploy various
resources as a means of lessening the disease impact or preventing further spread. These decision points include both spatial and
temporal locations, creating a large and complex decision space.
In order to understand this decision space, our work focuses on
advanced interactive visualization and analysis methods providing
linked environments of geospatial data and time series graphs that
allow end users to explore infectious disease outbreak models, as
shown in Figure 2. In this view, the map can be interactively filtered
to show a variety of statistical measures about the disease spread
(e.g., percentage ill, percentage dead), and plots in this view provide temporal details of the spread over a user selected geographic
region.
Furthermore, in the geospatial view, users are able to interactively explore the simulation and insert decisions (e.g., quarantine
counties, spray pesticides, enforce social distancing). The effects of
these decisions are captured in the decision support tree space. The
decision support space shows how the simulation paths vary over
time where the height of the path is based on a user-defined metric of the decision impact as compared to the baseline metric (i.e.,
the simulation path in which no interdictions have taken place). In
this manner, users are able to explore path choices and analyze the
global impact over time. This allows users to explore both short
and long-term ramifications of the decision measures employed.
2

R ELATED W ORK

The exploration and visualization of simulation models and outputs has been a topic of much exploration. Matkovic et al. [20]
proposed a simulation model view that provides a visual outline of
the simulation process and the corresponding simulation model in
an effort to bridge the gap between simulation model behavior and
the dataset. Bruckner et al. [5] introduced a visual exploration
approach for investigating parameter spaces for visual effects design utilizing sampling and spatiotemporal clustering techniques to

192

generate an overview of resultant variations and temporal evolution. Kohlhammer et al [16] presented an overview of work on
situational awareness, naturalistic decision making and decisioncentered visualization. Guo [11] proposed a visual analytics approach to discover interesting patterns in spatial interaction data in
order to design effective pandemic mitigative strategies and facilitate decision making process. The proposed approach consists of a
new graph partitioning method to segment large interaction graph,
a reorderable matrix to visualize major patterns and a modified flow
map linked with reorderable matrix. Waser et al. introduced World
lines [29] which integrated simulation, visualization and computational steering into a single unified system enabling user exploration
of the entire solution space searching. Their steering mode enables
the user to generate and control multiple simulation runs while visualization mode facilitates comparison between simulation runs
while searching for an optimal solution. Our work follows many of
the conventions employed in world lines, such as collapsing decision spaces and formatting the decision history tree to be temporally
aligned. The major difference between the two works is the manner in which our decision history tree is formatted to represent the
overall impact of a decision at each time point. This visualization
creates paths which allow users to quickly explore the impact of
decision at a given time point as well as the total impact at the end
of the simulation run.
In each of the previous simulation analysis works, a key component is the tracking, evaluation and exploration of user events
and interactions. Previous work in this area includes applications
such as TimeTree [6], Grasparc [4], HyperScribe [30] and VisTrails
[24]. TimeTree [6] is an interactive visual analytic tool that supports browsing large data sets while keeping the exploration process cognitively tractable. Brodlie et al. [4] introduced Grasparc, a
framework that helps manage the problem solving process through
the use of history trees to represent the solution exploration process. HyperScribe [30], an extension to Grasparc, provides a data
management facility to organize and retrieve solution data in computational experiments. VisTrails [24] supports exploratory visualization by maintaining a detailed record of changes made to the
workflows during the parameter exploration process and provides
side by side comparison of results. Again, a history tree structure is
utilized in order to manage the provenance data.
In conjunction with history tree structures, a variety of other visualization tools and systems have been developed to track other
types of historical data. Baur et al [2] proposed an interactive visualization tool for displaying music listening histories along with
contextual information and support to learn and understand these
histories. LifeLines [22] provided a visualization environment for
personal medical histories summarized in the form of lines and
events highlighting relationships via search tool. Lifelines 2 [28] introduced several extensions to their earlier work which emphasized
visualizing temporal categorical data for multiple records. AsbruView [17] extended the LifeLines concept to implement temporal
view used to display hierarchical plan structures in medical therapy
planning.
Heer et al. [12] performed the design space analysis of interaction histories and contributed a prototype graphical history interface for Tableau [19, 25] visualization system. This interface not
only provides tools for history navigation and management but also
supports sense-making, search and communication through some
additional operations. This history information can be used to evaluate visualization design. CzSaw [15] captures the analysis process and corresponding user interactions, represented in the form
of a scripting language which can then be viewed by analyst to
identify repetitive patterns. Analysts can then look for different
avenues without losing track of the previous analysis. Suntinger et
al. [26] proposed an event-tunnel visualization and analysis framework which is based on two views of the cylinder: the side view

Figure 2: Visual analytics decision support environment. (Left) The spatiotemporal model view display. In this view, users can watch the spread
of the model over space and time and introduce changes to the simulation as well as incorporate mitigative response measures to try and slow
the disease spread. (Right) The decision history tree view. As users interact in the model view display, the different paths the simulation can
take are calculated and visualized. The decision paths are plotted over time on the x-axis, with the y-axis representing the cumulative deviation
from the baseline simulation.

(plotting the events in temporal order) and the top view (that looks
into the stream of events along the time axis). These two interactive views of the event data can be embedded into a configurable
workspace that supports analysis and mining. Work by Shrinivasan
and van Wijk [23] also suggest the incorporation of history views
into the analytical reasoning process for information visualization.
Similar to the previous work in history visualization and analysis,
our work also incorporates history trees and navigational structures;
however, we modify the decision history space to allow for quick
comparison between not only the decisions made but also the outcomes of the decision.

igative decision measures, the decision history tree is stored and a
visualization of the user’s choices can be displayed and analyzed.
Our framework is based upon recommendations in the work by
Jankun-Kelly and Ma [14] and Shrinivasan and van Wijk [23], both
of which suggest that capturing metadata of the visual exploration
process is a key component of the analysis process. Our history
tree visualization is able to record users’ decisions and allow them
to compare, modify and insert new decisions. In this section, we describe the details of our system components and the related control
structures.
3.1

3

S YSTEM OVERVIEW

Our visual analytics decision support environment consists of two
main views as shown in Figure 2. The first view (Figure 2-(Left))
is the spatiotemporal model view in which users can interactively
adjust model parameters and explore the effect of decision measures over space and time. As users interactively scroll through
time and explore the epidemic spread model over the underlying
population structure, and detailed views of the impact over time for
a user selected region is shown in the small multiples plots on the
right-hand side of Figure 2-(Left). In this window, users can interactively choose to employ decision measures for mitigating the
outbreak. Decision measures are based on the model under investigation, and detailed examples of use cases are given in Section
4.
In the spatiotemporal model view, users may only interact with
their current scenario, and modifications in this scenario are captured in our second view, the decision history view (Figure 2(Right)). In the decision history view, users may explore the different decision paths and compare the cumulative outcomes over time.
The decision history view supports path highlighting and branch
collapsing as a means of reducing clutter and enabling effective
exploration. Each of our two views supports a particular form of
analysis and enables users to evaluate the effectiveness of various
disease mitigation decisions.
These views are driven by an epidemic spread simulator in which
a given model is integrated into the system. The model input parameters are fed into the simulator and the results of the simulation
are modeled based on user-defined decisions. As users apply mit-

Epidemiological Spread Simulator

In order to allow our system to be fully functional and adaptable
to other models, our system contains an interchangeable epidemiological spread simulator component. This component generates
a large scale spatial simulations over census tract, zip code and/or
county level populations. Population and demographic[27] data is
provided as input to the back-end simulation functions. The simulation then outputs information on the number of sick and dead within
a given population by areal unit (e.g., county, zip code) and provides color coded geographical representations of the data. System
control menus are then defined to incorporate the appropriate mitigative response measures that users can apply. Specific simulations
used as case studies are described in Section 4. For a given model,
the epidemiological spread simulator generates the epidemic spread
data for specified number of days based on the given scenario and
model parameters.
3.2

Spatiotemporal Model View

The spread data is then mapped into an interactive spatiotemporal
view shown in Figure 2-(Left). This view facilitates the exploration
of the disease spread through an interactive time spinner control
seen at the upper left corner of the window. However, such exploration only provides slices of spatial data at a given time or an
aggregate thereof. In order to understand these slices, users need
to know the trends of previous data (and, if possible, model future
data trends).
Users may interact with the system through a variety of viewing
and modeling modalities. As shown in Figure 2-(Left), the main
viewing area is the spatiotemporal view, and the three windows

193

Figure 3: Creating a decision history tree. In this figure, we show the insertion of a decision point on day 200 of a Rift Valley fever simulation.
The insertion of the decision points adds a node, and the resulting colored line shows the cumulative effects of this decision (as compared to the
baseline) over time. If a path is above the x-axis, that decision has cumulatively performed better than the baseline up to that point in time. In
this manner, users may track the magnitude of the disease spread with respect to the global impact.

on the right provide a time series view of the population statistics
based on the underlying population and model parameters. These
graphs provide a detailed view of the areal unit selected (with selections being indicated by a darker border) in the main viewing area,
where each graph shows a different population statistic with respect
to time. Both the geospatial and time series viewing windows are
linked to the time control at the upper left portion of the screen.
These linked views allow for a quick comparison of trends across
various spatial regions.
As users explore the disease spread over time, they can also introduce mitigative response measures into the scenario. After each
mitigative measure, the epidemiological spread model updates the
scenario from that point in time. This form of exploration, which
involves the human analyst inserting decision points into the scenario, provides a means for both creating training scenarios, as well
as predicting possible future outcomes during an ongoing epidemic.
3.3

Decision History View

As the user inserts decisions points, scrolls through time, and revisits other scenarios, these interactions are tracked and displayed
in the decision history view. This view keeps track of all the mitigative response measures performed and the corresponding mortality/sickness rate in a form of single visualization. This tool not
only keeps track of the decision histories but also shows the consequences of each decision in terms of net gain or loss over time,
creating the branching paths structures seen in Figure 2-(Right) and
Figure 3. Currently the comparison can only be done according to
a single criterion, and future work will explore ways of visualizing
multivariate outcomes within a decision history tree. Note that all
simulations used are designed to run until the disease has run its
natural course, thus, the limits on the x-axis of our history view are
derived from the model itself.
3.3.1

Path Building

In order to build the paths of Figure 3, a cumulative summation of
the overall magnitude of the outbreak (in terms of lives lost or another user-defined variable) is calculated. In the original path, we
consider the cumulative summation of lives lost on day t of the scenario to be the baseline. All other paths branch from this baseline
such that the decision history view is visualizing the overall deci-

194

sion impact:
Pv (t) =

N

t

i=0

i=0

∑ Pbi (t) − ∑ P0i (t)

(1)

Here, Pv (t) is the overall impact of the current decision path (Pbi )
in geographical area i with respect to the impact of the original unmitigated simulation (P0i ) at time t overall all N geographical areas
in the simulation space. Pv (t) is plotted on the decision history tree
branching off from the last active decision path.
When the first mitigative response is added by the user, a decision path originates from the x-axis (representing time). The symbols shown in the legend represent the different types of mitigative
response measures. Each decision path and corresponding symbol
is assigned a unique color. The color of the decision path is temporarily changed to black whenever the user mouses over a decision
path. In Figure 3 the brown triangle represents the point in time in
which the user deploys some predefined resource as a means of mitigating the response. After this point, the height of the new path is
calculated using Equation 1 for each time point left in the simulation. The height of the decision path along the y-axis represents the
difference between the original (un-mitigated path) and the current
branch.
Users may return to the original simulation as well and add other
decision paths for comparison. Path selection is done through a
right click within the interface and users may quickly move between
scenarios. For each decision taken, a unique color is assigned to the
path in order to facilitate analysis, as shown in Figure 1.
The height of these branches are again plotted with respect to the
original decision path (as opposed to the parent path), thereby maintaining a consistent basis for comparison. If the branch is above the
x-axis (positive), then the current decision path has helped mitigate
the spread of the disease. In Figure 1, we can see one path that
falls below the x-axis for a portion of time; however, it ends above
the x-axis when the simulation run completes. Since the y-axis values represent a cumulative summation, this indicates that while this
decision path may have seemed to be detrimental for a time during
the simulation, it eventually proved effective in reducing the overall
impact. However, in Figure 1, we can also see three other decision
paths that end below the x-axis. Two of these paths would have
initially appeared to be highly mitigative responses as they remain
above the x-axis for a long period of time; however, we see that by
the end of the simulation, this path actually would have worsened

the overall impact of the spread. By using the decision history view,
analysts may now see the end result as well as the path that it took
to reach there. Furthermore, in the decision history view, analysts
can see the effect of the decision and then utilize the model view
display to explore periods of poor performance to look for other
potential mitigative measures that could be added.

fected population will die. These numbers are based on rates from
the 1918 influenza pandemic [3, 8].
4.1.1

Mitigative Response Measures

In order to demonstrate our tools, we present two use case studies
for two unique epidemiological spread models. Both models were
adapted into the epidemic spread simulation component and minor changes to the user interface spatiotemporal model view were
added to allow for the appropriate mitigative response measures.
In this section, we briefly discuss the underlying model for each
infectious disease scenario and then explore the decision analysis
process.

Within our modeling tool, users are able to choose from three different global decision measures: (1) school closures; (2) media alerts;
and (3) strategic national stockpile deployment (SNS). These decision measures were decided on based on requirements from the
Indiana State Department of Health in order to best accommodate
their training exercises. The choice of these decision measures is
also influenced by previous work. Historical records of past pandemics illustrate the efficacy of social distancing with regards to
lessening the impact of a pandemic. Furthermore, other researchers
have noted the expected reduction of influenza transmission based
on school closures or quarantines, and the effects of containing pandemic influenza through the use of antiviral agents and stockpiles
have been well documented. Detailed descriptions of the effects of
various decision measure strategies can also be found in [10] and
[21], along with others.
Figure 4 shows how a user can simply toggle on and off decision points within the model view display to see their effects on the
pandemic impact. Figure 4 (Left) shows the model on Day 36 with
no decision measures employed. Using the controls on the lower
left portion of the screen, the analyst chooses to deploy the SNS antivirals on day 3. The control widget shown in Figure 4 (Right) allows the user to set the day of the simulation on which the decision
measure was enacted, the number of days it will take the decision
measure to reach full effect, and the impact the decision measure is
expected to have in reducing the infection. In the graphs of Figure 4
(Right), the user can immediately see how the use of the (SNS) has
helped mitigate the magnitude of the pandemic.These graphs represent the number of sick cases, people who need hospitalization and
number of deaths for selected counties in the spatiotemporal view.
Through these controls, the user can interactively toggle decision
points on and off and explore the effects that decisions taking place
in the past would have on the current situation. Interactive toggling allows the user to understand the magnitude of the change by
watching both the graphs and map display colors change for a given
day as decision measures are implemented.
In this model, all decision points are designed to mitigate the
spread, and each decision measure may only be deployed once. As
such, the decision history view will show that all paths improve the
outcome when compared to the base scenario. We use this example
to demonstrate features of our tool and show that it is adaptable to
multiple models.

4.1

4.1.2

3.3.2

Path Analysis Tools

In order to explore a given decision path, users simply click on a
path and choose the ‘Activate Path’ option from a menu. This path
is then the path being explored in the spatiotemporal model view.
Path activation requires us to introduce persistence support in the
system and our system saves each previous state of the simulation
before performing any mitigative measure. This saved state can be
reloaded once any deactivated decision path is activated. Unfortunately, as the number and complexity of the scenario created by the
user increases, the complexity of the visualization also increases.
Mouse over on a node also provides a thumbnail view of what the
geographical state space looked like at the time of the decision, as
seen in Figure 1.
In order to reduce clutter due to the addition of a large number of
decision measures, each decision path can be expanded or collapsed
to any level. Whenever a path is expanded or collapsed, the last
expansion/collapsed state is preserved for all sub paths. However,
in some cases, users may wish to view a portion of the decision
path and hide other obstructing nodes or lines. After introducing
several branches within a given path, users may wish only to see the
optimal path (the path with the highest cumulative score). In order
to perform this operation, a user can right click on any path and
select option ‘Show Only Best Path Components’. Furthermore,
in the presence of large number of decision paths/branches, it may
become difficult to differentiate decision lines. Thus, our decision
history tree also supports path highlighting to provide additional
cues about path recognition.
4

U SE C ASE S TUDIES

Pandemic Influenza

Our first epidemiological spread model utilizes a Gaussian mixture
model that simulates the spread of a pandemic influenza across the
United States starting from a user defined point source location and
incorporating airport traffic models [18]. The model makes use of a
person-to-person contact model spread with a constant rate of diffusion in order to simulate a spatiotemporal outbreak. The model
was designed to determine the number of influenza outbreak infections, hospitalizations, and deaths on a daily basis. As input, it requires the pandemic influenza characteristics, county data including
population, demographics, and hospital beds, and decision measure
anticipated impact. Spread vectors based on the point of origin and
distance traveled per day are calculated, and effects on different age
groups and population densities are taken into account.
Default parameters to our model are based on information from
the U.S. National Strategic Plan [13]. In this plan, states are charged
with the task of preparing for a pandemic influenza wave under the
prediction that up to 35% of the population could be infected, 50%
of the infected population will seek medical care, 20% of those
seeking care will require hospitalization, and up to 2% of the in-

Model Exploration

In this example, the user has created four different paths for exploration as shown in Figure 5. We have included a variety of decision
measures along each path, including combinations of all three mitigative response. The maps surrounding the decision tree structure
in Figure 5 represent day 45 of the simulation with respect to a given
decision path as indicated by the labels. The decision measures and
the times at which they are implemented are provided in Table 1.
Note that each decision made in the table generates a branch; however, in order to evaluate the total path, we have collapsed the intermediate paths. In this manner, we only show the decision path
resulting from the combination of the decisions shown in Table 1.
Here, the user can quickly see that Path D1 of Figure 5 is the
optimal choice in terms of mitigating the outbreak based on the
available decision metrics. It is clear that the earlier a decision is
made, the more impact it can have on reducing the spread. As each
decision point only has a positive effect on the disease reduction,
the exploration task in this simulation is relatively trivial. However,
this example is included to illustrate that this tool is easily adaptable
to multiple models.

195

Figure 4: Here we illustrate the effects of utilizing decision measures within the confines of a pandemic influenza simulation. In the left image,
the analyst has used no decision measures and is visualizing the spread of the pandemic on day 36 of the simulation. In the right image, the
analyst has decided to see what effects (on day 36) deploying the strategic national stockpile on day 3 would have had on the pandemic.

Figure 5: Pandemic Influenza Case Study. Here the user has introduced a variety of different decision measures at various points in time and
in different combinatorial order. We explore the resultant simulation spaces in the geographical space with the maps surrounding the central
image. Each map corresponds to a different decision tree branch as denoted by the corresponding label.

4.2

Rift Valley Fever (RVF)

Our second epidemiological spread model utilizes a differential
equation model that simulates the spread of RVF through a simulated mosquito and cattle population in Texas [9]. As input, it
requires the underlying county populations of both the cattle and
two types of mosquitoes (Aedes and Culex) in the area. The differential equation model then accounts for the transmission of the
disease both through mosquito to cattle infections and cattle to cattle infections. Mosquito larvae are also considered in this model
as a means of the disease being prevalent at the mosquitoes’ birth.
Lives lost/saved in this case are referring to the underlying cattle
population. Note the simulation stops at the state border.
4.2.1

Mitigative Response Measures

Within our modeling tool, users are able to choose from two
different local decision measures: (1) pesticides; and (2) quarantine. Users are able to interactively apply a quarantine or
pesticide spray to any individual county or multiple counties at
once during the simulation. This is done by mouse clicking on
counties and then selecting to spray or enforce a quarantine on
those counties. Analysts can combine Aedes and Culex pesticides

196

Table 1: Summary of decision paths generated for the pandemic influenza simulation. Each entry represents the day a type of decision
was employed in the path.

Decision Path
D1
D2
D3
D4

School Closure
6
12
40
35

Media
25
25
35
45

SNS
2
16
20
30

for a combined spray. This is represented by a legend item(‘Both
Pesticides’) in Figure 7. Our system can easily be modified to allow
additional combinations without any performance penalty. In the
case of multiple simultaneous decision measures, the parameters
associated with the combined mitigative response measures will be
updated and passed to the model simulator that will recalculate the
simulation results.
Pesticides:
In order to apply pesticides, an analyst selects the set of geographical regions for this operation and also the type of pesticide to kill
a particular type of mosquito species or both. Figure 6 shows that

Figure 6: Here we illustrate the effects of utilizing decision measures within the confines of the Rift Valley fever simulation. In the left image, the
analyst has employed both the use of quarantine and pesticide spray to try and reduce the disease spread. However, as infected mosquito eggs
have already propagated to neighboring counties, they find that the decision measures taken have less impact then expected. The spread of
the disease after the application of pesticides and quarantine is seen in the middle and right figures. The graphs in the right panel represent the
population of livestock and population of Aedes & Culex mosquito types for selected counties within the spatiotemporal view. Selected counties
are highlighted by drawing bold black boundaries.

Table 2: Summary of decision paths generated for the Rift Valley
fever simulation. Each entry represents the day (or days) a type of
decision was employed in the path.

A
B

Pesticide Aedes
110,262
143

Pesticide Culex
262
143,263

Quarantine
85

after applying the pesticides even if all the mosquito population is
killed along with the eggs, mosquitoes from neighboring counties
may still migrate to the area. A portion of this migration is caused
by the transfer of livestock from the neighboring regions or existing
infected livestock may infect new uninfected mosquitoes. Analyst
can try different combination of regions for pesticides and based
on the simulated spread results and analyze which combination
may work best within a given scenario.
Quarantine:
The second mitigative measure supported is the quarantine operation. In this operation, a user selects a set of geographical regions
(as seen in Figure 6) which will no longer allow transport of cattle
into or out of the region. Quarantine results in setting the travel rates
for the livestock across selected regions to zero; however, mosquito
travel is still unrestricted.
4.2.2

Model Exploration

In this model, the space of possible mitigative responses is limitless since the user can select any subset of geographical regions for
pesticides or quarantine and the order/frequency of these mitigative
responses may also vary. With 254 counties in Texas, the user can
choose from a variety of decision measures. As previously stated,
the goal of this tool is not necessarily to find the optimal solution;
instead, the goal is to allow users to play out various scenarios based
on their underlying knowledge of the resources available. Such a
tool can then potentially alert decision makers to shortcomings in
their plans or resources.
In this example, the user has created two main paths for exploration as shown in Figure 7. The decision measures and times at
which they are implemented are provided in Table 2. The maps surrounding the decision tree structure Figure 7 represent the days in
which responses were taken and the highlighted counties are those
in which a response was implemented. Note that at no point during the exploration do the simulation parameters change. The only
things that can impact the result of the simulation are the mitigative
responses.
In the initial exploration, the goal was to explore the effects of

not using a quarantine. Depending on the time of year the outbreak
may occur, a quarantine could have significant impacts on beef
sales. Thus, decision makers may wish to only spray for mosquitoes
near those affected counties in an attempt to prevent the spread. The
first decision in path A, is thus to spray. However, a pesticide that is
only effective against one type of mosquito (Culex) was used, representing a deficiency in local supplies. Path A initially saved some
lives but ended up slightly below the horizontal axis.
After seeing that Path A appears to have a mostly positive effect
(with regards to reducing the disease impact on the cattle population), the user then decides to spray for mosquitoes at the apex of
the current Path A. Counties near two major rivers are selected, and
sprayed for both major types of mosquitoes. However, the resultant
impact is actually worsened.
Without the decision support history tree to observe the response
of A, it would be difficult to visually tell the resulting difference between the original simulation and the initial decision branch as the
two result in a nearly identical loss of cattle. However, if exploring this initially, the user would see a gain and may have concluded
that the decision path chosen would result in saving lives. In fact,
the decisions taken here would only waste resources and result in
approximately the same (or an even worse) outcome.
In path A, the user wanted to explore the effects of enforcing an
early quarantine. Initially, this path appeared to provide little impact with respect to the baseline, following closely to the horizontal
axis; however, by the end of the simulation, we see that the initial
quarantine (the blue line labeled as a branch of path B) actually
results in a fair number of lives save.
From this analysis, the user wants to see if they can do better by
inserting a decision measure near the first uptick of the blue path
at day 143. Here, the user preemptively sprays counties near the
two major rivers and at the edge of the oncoming spread. The green
decision path (again labeled as a branch of path B) is generated.
Here, we see that the green path actually outperforms the blue path,
resulting in an even higher number of lives saved.
Finally, the user inserts another decision measure at the apex
of the green path (day 263), this time spraying for only Aedes
mosquitoes. The spray is done over the same major rivers as the
previous injection, and initially a large upswing is seen. Unfortunately, by the end of the simulation, the end result actually is worse
than the quarantine alone and the quarantine combined with a secondary spray. This is labeled as Path B. From this, we see that early
preventative measures work better (as expected); furthermore, late
term measures can actually negatively impact the disease spread.
Based on the decisions taken, users can clearly see that the green
branch of path B in Figure 7 is the optimal choice as compared to

197

Figure 7: Rift Valley fever Case Study. Here the user has introduced a variety of different decision measures at various points in time and in
different combinatorial order. We explore the resultant simulation spaces in the geographical space with the maps surrounding the central image.
Each map corresponds to a different decision tree branch as denoted by the corresponding label.

other explored paths. It can be observed that by adding in the secondary decision measure, we reduced the effect of the downswing
seen in the blue path near day 275. Users may also choose to go
back and explore what went wrong in path A as this resulted in the
largest loss of life.
Clearly, as the decision state space becomes more complex, the
resultant decision history tree also grows in complexity. However,
the current system allows users to interactively hide branches, thus
allowing them to quickly remove suboptimal decision paths from
the visualization. Future work will also focus on suggesting potential solutions based on the current simulation day and other potential user defined constraints. We also plan to allow for local history
decision visualization (as opposed to the current global history option). Future work will explore the inclusion of small multiples
embedded in mouse overs of the map viewing window as a means
of displaying local history graphs for a given county.
4.3

Memory Requirements and Performance

Whenever a mitigative response measure is performed, the model
recalculates the epidemic spread data beginning from the selected

198

day through the end of the simulation. This newly calculated spread
data, along with the mitigative measure description, is saved in
a system structure that keeps track of the complete decision history. The last decision path added is automatically marked as active
and any further decision will branch from the active decision path.
Among all decision paths, only one path can remain active at a time.
Users may reactivate any decision path by mousing over the desired
decision path and selecting the activate path option. This mechanism enables the user to recover any intermediate state and obtain
a frame-by-frame simulation. A frame-by-frame simulation shows
the epidemic spread for the active path. The memory requirements
to support such a mechanism depend on the number of geographical
regions (e.g., counties/cities), nature of the model, and the number
of days in the simulation. In our current implementation, there is a
system constant that allows us to define the upper limit for the maximum number of decision paths that can be created. This constant
can be adjusted by the user. In future extensions of this work, we
plan to include save and reload option, that would support saving
the least frequently used decision history paths to disk. An alternative approach would be to discard the simulation data for previous

decision paths and save only the minimum possible information required to recalculate the spread data if a user reactivates the decision path. Unfortunately, this approach would be very costly if the
simulation is expensive. This approach could be useful in scenarios
where the memory requirements for keeping decision history paths
are large and the simulation is relatively inexpensive.
We have tested the performance of the system for pandemic influenza and RVF epidemiological models. The system was tested
on a machine with an Intel Xeon E5504 2 GHz Quad-Core processor with 6 GB physical memory and Windows 7 (64 bit) operating system. For pandemic influenza, the response was generated
at interactive rates for 80 days of simulation for the entire United
States. For the Rift Valley fever simulation, the system took on
average 48.768719 milliseconds to calculate epidemic spread data
for one day of simulation for the entire state of Texas at the county
level (254 counties). The system took approximately 17.8 seconds
to calculate the spread data for a complete year. The average response time was similar for all possible combinations of mitigative response measures. Most of the execution time was spent on
the calculations by the RVF ordinary differential equation (ODE)
solver.

5

C ONCLUSIONS

AND

F UTURE

WORK

The proposed decision support environment facilitates researchers,
analysts and public health officials in their study of epidemic
spreads under varying scenarios and decision measures. Our decision history visualization and navigational support helps the users
analyze the consequences of their decisions over time and understand both the short term and long term impact of their mitigative
responses. Users can also drill down into a given scenario using the
spatiotemporal model view in order to better assess the effects of
individual paths. The architecture proposed in this paper provides
flexibility in terms of incorporating different epidemiological models and applying the same set of tools to identify suitable mitigative
strategies in case of an outbreak. In our future work, we plan to include an economic model into the system that will help us visualize
the impact of the epidemic spread and corresponding responses on
local economy. We also plan to include additional mitigative tools
in the set of available mitigative measures.
We also plan to investigate if the model could be extended with
information such as weather conditions, population characteristics
etc. This would enable us to incorporate additional factors while
creating hypothetical spread scenarios and get more accurate spread
modeling. We also plan to add new features to the visual interface
including play/stop buttons to automatically animate the simulation. We also plan to add decision path save and reload features
that would allow users to save the least frequently used decision
path to disk and later restore them. This feature will make the system more scalable.
Furthermore, decision history trees can be integrated with any system in which the user wants to keep track of multiple simulation
paths and compare the end results. For example, if we have a financial model capable of generating spatiotemporal data and an
associated model simulator, we can attach a user interface to the
model simulator. This interface would allow the insertion of decisions paths at different points in time. Each decision initiates a new
simulation path and the corresponding end results. In addition, our
system can also be used to validate how effectively the model captures a real life epidemic spread. For this case, we must have the
past data available for validation. By initializing the model simulator with a given scenario and comparing the results of model with
actual data, we can gain insight into the accuracy of the model with
respect to real life behavior.

ACKNOWLEDGEMENTS
The authors wish to thank David Hartley and Tianchan Niu for providing details on the Rift Valley fever simulation and Min Chen,
Harold Bosch and Denis Thom for their helpful discussions in editing this paper. This work was supported in part by the U.S. Department of Homeland Security’s VACCINE Center under Award
Number 2009-ST-061-CI0002, the Foreign Animal and Zoonotic
Disease Center, and the Defense Threat Reduction Agency under
Award Number HDTRA 1-10-0083.
R EFERENCES
[1] Atlanta: Centers for Disease Control and Prevention. Community
strategy for pandemic influenza mitigation in the United States - early,
targeted, layered use of nonpharmaceutical interventions. Atlanta:
Centers for Disease Control and Prevention, 2007.
[2] D. Baur, F. Seiffert, M. Sedlmair, and S. Boring. The streams of our
lives: Visualizing listening histories in context. IEEE Transactions on
Visualization and Computer Graphics, 16(6):1119 –1128, 2010.
[3] M. Billings. The influenza pandemic of 1918, 1997.
[4] K. Brodlie, A. Poon, H. Wright, L. Brankin, G. Banecki, and A. Gay.
Grasparc: A problem solving environment integrating computation
and visualization. In Proceedings of the 4th conference on Visualization, pages 102–109, 1993.
[5] S. Bruckner and T. Möller. Result-driven exploration of simulation
parameter spaces for visual effects design. IEEE Transactions on Visualization and Computer Graphics, 16(6):1468 –1476, 2010.
[6] S. K. Card, B. Sun, B. A. Pendleton, J. Heer, and J. W. Bodnar. Time
tree: Exploring time changing hierarchies. IEEE Symposium On Visual Analytics Science And Technology, pages 3–10, 2006.
[7] D. Dausey, J. Aledort, and N. Lurie. Tabletop exercises for pandemic influenza preparedness in local public health agencies. TR-319DHHS, prepared for the U. S. Department of Health and Human Services Office of the Assistant Secretary for Public Health Emergency
Preparedness, 2005.
[8] L. R. Elveback, J. P. Fox, E. Ackerman, A. Langworthy, M. Boyd,
and L. Gatewood. An influenza simulation model for immunization
studies. American Journal of Epidemiology, 103(2):152–165, 1976.
[9] H. D. Gaff, D. M. Hartley, and N. P. Leahy. An epidemiological model
of Rift Valley Fever. Electronic Journal of Differential Equations,
115, 2007.
[10] T. C. Germann, K. Kadau, I. M. Longini, and C. A. Macken. Mitigation strategies for pandemic influenza in the United States. Proceedings of the National Academy of Sciences, 103(15):5935–5940, Apr
2006.
[11] D. Guo. Visual analytics of spatial interaction patterns for pandemic
decision support. International Journal of Geographic Information
Science, 21:859–877, January 2007.
[12] J. Heer, J. Mackinlay, C. Stolte, and M. Agrawala. Graphical histories
for visualization: Supporting analysis, communication, and evaluation. IEEE Transactions on Visualization and Computer Graphics,
14(6):1189 –1196, 2008.
[13] Homeland Security Council. National strategy for pandemic influenza. The White House website, November 2005.
[14] T. J. Jankun-Kelly, K.-L. Ma, and M. Gertz. A model and framework
for visualization exploration. IEEE Transactions on Visualization and
Computer Graphics, 13(2):357–369, 2007.
[15] N. Kadivar, V. Chen, D. Dunsmuir, E. Lee, C. Qian, J. Dill, C. Shaw,
and R. Woodbury. Capturing and supporting the analysis process. In
IEEE Symposium On Visual Analytics Science And Technology, pages
131 –138, 2009.
[16] J. Kohlhammer, T. May, and M. Hoffmann. Visual analytics for the
strategic decision making process. In R. D. Amicis, R. Stojanovic,
and G. Conti, editors, GeoSpatial Visual Analytics, NATO Science for
Peace and Security Series C: Environmental Security, pages 299–310.
Springer Netherlands, 2009.
[17] R. Kosara and S. Miksch. Metaphors of movement: A visualization and user interface for time-oriented, skeletal plans. In Artificial
Intelligence in Medicine, Special Issue: Information visualiztion in
medicine, pages 111–131, 2001.

199

[18] R. Maciejewski, P. Livengood, S. Rudolph, T. F. Collins, D. S. Ebert,
R. T. Brigantic, C. D. Corley, G. A. Muller, and S. W. Sanders. A
pandemic influenza modeling and visualization tool. Journal of Visual
Languages and Computing, To appear 2011.
[19] J. Mackinlay, P. Hanrahan, and C. Stolte. Show me: Automatic presentation for visual analysis. IEEE Transactions on Visualization and
Computer Graphics, 13:1137–1144, 2007.
[20] K. Matkovic, D. Gracanin, M. Jelovic, A. Ammer, A. Lez, and
H. Hauser. Interactive visual analysis of multiple simulation runs using the simulation model view: Understanding and tuning of an electronic unit injector. IEEE Transactions on Visualization and Computer
Graphics, 16(6):1449 –1457, 2010.
[21] G. Miller, S. Randolph, and J. Patterson. Responding to Simulated
Pandemic Influenza in San Antonio, Texas. Infection Control and
Hospital Epidemiology, 29(4):320–326, April 2008.
[22] C. Plaisant, B. Milash, A. Rose, S. Widoff, and B. Shneiderman. Lifelines: visualizing personal histories. In Proceedings of the SIGCHI
conference on Human factors in computing systems: common ground,
CHI ’96, pages 221–227, 1996.
[23] Y. B. Shrinivasan and J. J. van Wijk. Supporting the analytical reasoning process in information visualization. In Proceeding of the twentysixth annual SIGCHI conference on Human factors in computing systems, pages 1237–1246, New York, NY, USA, 2008. ACM.
[24] C. Silva, J. Freire, and S. Callahan. Provenance for visualizations: Reproducibility and beyond. Computing in Science Engineering, 9(5):82
–89, 2007.
[25] C. Stolte, D. Tang, and P. Hanrahan. Polaris: A system for query,
analysis, and visualization of multidimensional relational databases.
IEEE Transactions on Visualization and Computer Graphics, 8:52–
65, 2002.
[26] M. Suntinger, H. Obweger, J. Schiefer, and M. Gröller. Event tunnel:
Exploring event-driven business processes. IEEE Computer Graphics
and Applications, 28(5):46 –55, 2008.
[27] United States Census Bureau. Population demographics, 2000.
[28] T. D. Wang, C. Plaisant, B. Shneiderman, N. Spring, D. Roseman,
G. Marchand, V. Mukherjee, and M. Smith. Temporal summaries:
Supporting temporal categorical searching, aggregation and comparison. IEEE Transactions on Visualization and Computer Graphics,
15:1049–1056, 2009.
[29] J. Waser, R. Fuchs, H. Ribicic, B. Schindler, G. Bloschl, and
E. Groller. World lines. IEEE Transactions on Visualization and Computer Graphics, 16(6):1458 –1467, 2010.
[30] H. Wright and J. P. R. B. Walton. Hyperscribe: A data management
facility for the dataflow visualization pipeline. In IRIS Explorer Technical Report IETR/4, NAG Ltd, 1996.

200

Multi-Modal Perceptualization of Volumetric Data and Its Application to
Molecular Docking
Ross Maciejewski Seungmoon Choi David S. Ebert Hong Z. Tan
School of ECE, Purdue University, USA
E-mail: {rmacieje, chois, ebertd, hongtan}@purdue.edu

Abstract
In this paper, we present a multi-modal data perceptualization system used to analyze the beneﬁts of augmenting a
volume docking problem with other perceptual cues, particularly stereoscopic vision and haptic rendering. This
work focuses on the problem of matching complex threedimensional shapes in order to reproduce known conﬁgurations. Speciﬁcally, we focus on the docking of two proteins, actin and coﬁlin, responsible for cellular locomotion.
Users were shown examples of coﬁlin combining with actin
and asked to reproduce this match. Accuracy of the match
and completion time were measured and analyzed in order
to quantify the beneﬁts of augmenting tools for such a task.

1. Introduction
Starting in the early nineties, a push for data perceptualization resulted in the development of many haptic rendering systems. Early systems [1, 5] used the local gradient as the surface normal and force transfer functions1 .
Newer systems have incorporated proxy-based haptic rendering techniques for volumetric data rendering [4, 6]. A
pioneering work where both visual and haptic perceptual
cues were used was Project GROPE [3] developed at the
University of North Carolina, Chapel Hill. While recently,
one of the most popular molecular visualization packages,
Visual Molecular Dynamics (VMD), has also been augmented with force feedback [10].
Despite the signiﬁcant progress in computational models and techniques for data perceptualization, much work
remains for the quantitative evaluation of data perceptualization systems in terms of their effectiveness in transmitting information to the user. Moreover, researchers and scientists have been slow in adopting the new technologies. In
light of this, there is a pressing need to quantify the usefulness of data visualization systems in order to demonstrate
their applicability to scientists.
Our research group recently developed an Interactive
Volume Illustration System (IVIS) [9] to create illustrations of three-dimensional datasets. IVIS provides a graphical user interface in which the user can easily control the

(a) Actin.

(b) Coﬁlin.

(c) Combined.

Figure 1: Volumetric datasets for actin and coﬁlin.

shape of a transfer function. By applying the user-deﬁned
transfer function to the dataset, IVIS can instantaneously
update the visual representation, enabling the user to interactively explore the dataset.
We have extended IVIS into an Interactive Volume Perceptualization System (IVPS) providing 1) the simultaneous rendering of multiple volumetric datasets, 2) stereoscopic images using active stereo vision, 3) the sense of
touch via a force-feedback haptic interface, and 4) interactive transfer functions for both vision and touch. IVPS was
used to perceptualize the correct docking conﬁguration of
actin and coﬁlin (essential proteins for cell-motility; see
Figure 1 for examples and [8] for details). The effects of
the sensory modalities added to IVPS were quantitatively
assessed by a psychophysical experiment where a subject
was asked to use IVPS to ﬁnd the best docking conﬁguration of coﬁlin onto actin.
The remainder of this paper is as follows. Section 2 describes the IVPS rendering techniques. The experimental
design is presented in Section 3, and results are summarized in Section 4. We conclude the paper in Section 5.

2. IVPS: Interactive Volume Perceptualization
System
The visual renderer of IVPS is based on IVIS [9]. IVIS
is a texture-based volume renderer that allows the user to
interactively control transfer functions and explore their
data. IVIS takes advantage of a graphic processing unit’s

1 A transfer function refers to a mapping of a data variable (eg, density
at a voxel) to a display attribute (eg, opacity, force, etc.).
Proceedings of the First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems
0-7695-2310-2/05 $20.00 © 2005 IEEE

Histogram

computational power when compared to using the CPU
alone.

Opacity

Transfer
Function

Data Value
Color

2.1. Visual Rendering
For data visualization, IVIS takes a scalar volumetric
dataset as its input and incorporates the data into threedimensional texture units. The gradient magnitude and direction at each voxel are preprocessed. Afterwards, an image is generated by slicing the volume with view-aligned
quadrilaterals rendered in a back-to-front order. These
slices are then combined to form the ﬁnal image with opacity and color of each slice deﬁned by a transfer function.
In IVPS, we augment the visualization capabilities of
IVIS in two aspects: multiple volume rendering and stereoscopic vision. The stereoscopic approach used in our system is the parallel axis asymmetric frustum perspective
projection method [2]. Implementation details of these aspects can be found in [7].

(a) Visual transfer function.
Stiffness

Data Value

(b) Haptic transfer function.

Figure 2: IVPS interactive transfer function widgets.
rule was:
F=−

(2)

where N is the number of sample points, and F is the force
rendered by the haptic interface.
The sample points for a coﬁlin dataset were selected
to be distributed evenly across the surface of each coﬁlin
molecule. In order to achieve the typical haptic update rate
of 1 kHz, the maximum number of samples for each coﬁlin
dataset was limited to approximately 100 points. See [7]
for further details on sampling.

2.2. Haptic Rendering
Currently, IVPS can haptically render either one or two
volumetric datasets. In both cases, one dataset is centered
in the haptic interface workspace and remains stationary
while force feedback is enabled. Since IVPS calculates
and stores the gradient of each voxel of the dataset for
visual rendering, forces for haptic rendering can be computed most effectively using the gradient force method introduced in [1]. We also explicitly incorporate a haptic
transfer function in the force computation. For a probe of
the haptic interface positioned at x, the force displayed was
deﬁned as:
F(x) = −Cg(V (x))∇V (x),

1 N
∑ Cg(V (xi ))∇V (xi ),
N i=1

2.3. Interactive Transfer Functions
The user of IVPS can independently control transfer functions for both visual and haptic rendering using the widgets
shown in Figure 2. For the visual transfer function (Figure
2(a)), the user is provided with settings for both color and
opacity. The height of this function maps the associated
data value to an opacity. Below this is a bar where the user
may map colors, chosen from the color map, to different
data values. Similarly for the haptic transfer function, users
can create a piecewise linear transfer function which will
map data values to a stiffness coefﬁcient used in the force
rendering equations. In Figure 2(b), we illustrate a mock
transfer function showing how users may deﬁne their transfer function with any shape and value over the data range.

(1)

where V (·) is a value of the dataset at a given position,
g(·) is a (normalized) transfer function, and C is a scaling
factor. The force rendered is essentially of the form F(x) =
−kx, where k would be the stiffness coefﬁcient of a spring.
By inspection, k is the haptic transfer function, g(V (x)),
representing a variable stiffness coefﬁcient dependent upon
the probe’s position in the dataset.
This force rendering method has been extended to multiple volumes. Of the two volumes, one is treated as a
probe and the other as the volume being probed. In our
example, the actin molecule was treated as a stationary object being probed in the workspace of the haptic interface.
The coﬁlin dataset was attached to the stylus tip. The user
manipulated the position of the coﬁlin dataset by moving
the stylus. We computed the interaction forces between
the two datasets by sampling a set of points representing
the shape of the coﬁlin molecules in the dataset. Forces
for each point were computed using Equation 1 and then
averaged. It follows that for a given set of point samples of
coﬁlin molecules, {xi |i = 1, · · · , N}, our haptic rendering

3. Experimental Design
In order to evaluate the beneﬁts of added sensory
modalities (stereo vision and touch) in IVPS, we performed a psychophysical experiment using the volume
docking problem between actin and coﬁlin.

3.1. Apparatus
The IVPS used in our experiment consisted of a computer with an Intel Xeon 2.0 GHz CPU, an Nvidia Quadro
FX 3000 graphics card, a stereo-capable monitor, stereo

Proceedings of the First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems
0-7695-2310-2/05 $20.00 © 2005 IEEE

2

Table 1: Experimental conditions.
Condition
Visual
Haptics

C1
Mono
None

C2
Mono
gh (·)

C3
Mono
gs (·)

C4
Stereo
None

C5
Stereo
gh (·)

C6
Stereo
gs (·)

3.3. Procedures

(a) Model 1. (b) Model 2. (c) Model 3.

We tested six experimental conditions summarized in Table 1. For visual rendering, we compared the two cases
of mono and stereo rendering. For haptic rendering, the
two transfer functions for hard and soft contacts (gh (x) and
gs (x)) were used along with the no force-feedback condition. The order of experimental conditions presented to
each subject was randomized. For each condition the subject was presented three different coﬁlin models. The models were presented in random order, and 18 trials were performed for the experiment. Each trial averaged forty-ﬁve
seconds, and the total experiment ran for approximately
thirty minutes. The subject’s task was to control the position and orientation of the coﬁlin model with the PHANToM stylus and to explore the actin data until a best docking conﬁguration was found. Once the subject felt a best ﬁt
was found, the subject was asked to press the ‘Enter’ key
of a keyboard with their free hand. The computer program
would then record the docking position and time taken.
Before the experiment, each subject went through a
training session to familiarize themselves with the PHANToM and learn what was considered a correct ﬁt. Training
sessions typically lasted thirty minutes. Training continued until the subject felt ready to perform the docking task.
Three docking cases, different from those used in the main
experiment, were used for training.

Figure 3: Actin and coﬁlin models used in the experiment.
Force

Density

(a) gh (·): For a hard surface contact.
Force

Density

(b) gs (·): For a volumetric soft contact.

Figure 4: Haptic transfer functions used in the experiment.
shutter goggles coupled with an infared transmitter, and
a force-feedback device. The shutter goggles used were
Crystal EYES3 from StereoGraphics. The monitor was
a Diamond Pro 2070SB running at 120 Hz. The forcefeedback device used was a PHANToM Desktop model
from Sensable Technologies.

3.2. Stimuli

Ten subjects (S1 – S10) participated in the experiment.
We used one actin model and three coﬁlin models shown
in Figure 3 throughout the experiment. The three coﬁlin
3.4. Data Analysis
models were obtained by segmenting the data seen in FigWe used two metrics to evaluate performance: docking erure 1. Each model consisted of a unique combination of
ror and completion time. The docking error was deﬁned
three coﬁlin molecules, differentiated by their position and
as an average Euclidian distance between the correct conorientation relative to one another. The volume data were
ﬁguration of the coﬁlin dataset and the ﬁnal conﬁguration
sliced to make 37 × 37 × 37 voxels for both the actin and
chosen by the subject. Speciﬁcally, this is the average discoﬁlin datasets. Our preliminary experiment showed that
tance that each voxel is displaced from its correct ﬁt posiﬁnding the best docking conﬁguration with these models
tion. The completion time was measured from the start of
was moderately difﬁcult (see [7]). For haptic rendering,
a trial to the time when the subject pressed the ‘Enter’ key
we designed two haptic transfer functions to simulate hard
to commit a response.
and soft surface contacts. The transfer functions were used
4. Experimental Results and Discussion
together with the haptic rendering algorithm discussed in
Section 2.2 creating haptic feedback. When the coﬁlins,
The results of the psychophysical experiment are sumattached to the PHANToM stylus, touched the actin, the
marized in Figure 5 as bar graphs. Figures 5(a) and 5(b)
transfer function gh (x), Figure 4(a), results in the sensation
show the average docking error and completion time, reof touching a hard and rigid object. The function gs (x),
spectively, over all trials for each condition, along with
Figure 4(b), rendered a relatively soft and permeable obstandard error. From the docking error data, we can object. The volume dataset was mapped to a 200 × 200 × 200
serve that using stereo vision (C4 – C6) generally reduced
mm3 cube inside the PHANToM workspace. A displacethe error as compared to mono vision (C1 – C3). Usment of 5 mm of the PHANToM stylus corresponded to 1
ing stereo vision (C4) showed a statistically signiﬁcant devoxel. Under this setup, IVPS rendered visual images at
crease in docking error compared to using only mono viapproximately 4 frames per second and haptic forces at 1
sion (C1). C4 reduced the error found in C1 by 0.8 voxels,
kHz. of the First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems
Proceedings
0-7695-2310-2/05 $20.00 © 2005 IEEE

3

mean docking errors. It also increased the time it took the
subjects to ﬁnd a docking position. However, we hasten to
point out that our results do not imply that haptic rendering
is not useful in a data perceptualization task. There were
many more haptic transfer functions that could have been
used for the current dataset. The state-of-the-art in haptics
research does not provide general guidelines on how to design haptic transfer functions that can aid the user of a data
perceptualization system to perform a task well.

Acknowledgement

(a) Docking error.

The authors would like to thank Nikolai Svakhine for
providing the IVIS system, and Dr. Amy McGough and
her lab for providing datasets and participants in this study.
This material is based upon work supported by the National Science Foundation under Grant Nos. 0081581,
0121288, and 0328984. The second author has also been
supported in part by the Envision Center at Purdue University.
(b) Completion time.

References
Figure 5: Experimental results.
[1] R. S. Avila and L. M. Sobierajski. A haptic interaction
method for volume visualization. In Proceedings of the Conference on Visualization, pp. 197–204, 1996.
[2] P. Bourke.
3d stereo rendering using OpenGL (and
GLUT). Published Online http://astronomy.swin.edu.
au/∼pbourke/opengl/stereogl/, May 2002.
[3] F. P. Brooks, M. Ouh-Young, J. J. Batter, and P. J. Kilpatrick.
Project GROPE - haptic displays for scientiﬁc visualization.
Computer Graphics, 24(4):177–185, 1990.
[4] M. Ikits, J. D. Brederson, C. Hansen, and C. Johnson. A
constraint-based technique for haptic volume exploration. In
Proceedings of the Conference on Visualization, pp. 263 –
269, 2003.
[5] H. Iwata and H. Noma. Volume haptization. In Proceedings of the IEEE Virtual Reality Annual International Symposium, pp. 16–23, 1993.
[6] K. Lundin, A. Ynnerman, and B. Gudmundsson. Proxybased haptic feedback from volumetric density data. In Proceedings of Eurohaptics, pp. 104–109, 2002.
[7] R. Maciejewski. Exploring the value of adding haptic and
stereoscopic rendering to volume rendering. Master’s thesis, Electrical and Computer Engineering, Purdue University, 2004.
[8] A. McGough, B. Pope, W. Chiu, and A. Weeds. Coﬁlin
changes the twist of f-actin: Implications for actin ﬁlament
dynamics and cellular function. The Journal of Cell Biology,
138(4):771–781, 1997.
[9] N. Svakhine and D. S. Ebert. Interactive volume illustration
and feature halos. In Proceedings of the 11th Paciﬁc Conference on Computer Graphics and Applications, pp. 347–352,
2003.
[10] W. Wriggers and S. Birmanns. Interactive ﬁtting augmented
by force-feedback and virtual reality. The Journal of Cell
Biology, 144:123–131, 2003.

which corresponds to approximately 4 mm in the PHANToM workspace. Also, using all the enhanced features of
IVPS (C6 with stereo vision and gs (·)) showed a statistically signiﬁcant decrease in docking error compared to
using mono vision (C1). C6 reduced the error found in
C1 by 1.1 voxels, which corresponds to approximately 5.5
mm in the PHANToM workspace. In terms of haptic rendering conditions, it is not clear which haptic rendering
method produced the most accurate ﬁt as no other conditions showed a statistical signiﬁcance compared to C1.
However, the results show a trend toward better accuracy
using gs (·) with stereo which we plan to investigate further.
From the completion time data shown in Figure 5(b), we
can observe that adding force-feedback tended to increase
the overall response time (C2, C3, C5, and C6) as compared to the purely visual conditions (C1 and C4). Each
of C3, C5 and C6 indicated a statistically signiﬁcant increase in completion time as compared to C1. This can be
explained by the fact that haptic perception is achieved by
local and sequential explorations of an object, while visual
perception is global and parallel.

5. Conclusions
We have presented an Interactive Volume Perceptualization System that incorporates both stereoscopic and haptic
rendering. Interactive visual and haptic transfer functions
provided users with the ability to see and feel their data
in different manners. The addition of haptic rendering to
our system did not show a statistically signiﬁcant beneﬁt in
reducing docking error, although it did reduce the overall

Proceedings of the First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems
0-7695-2310-2/05 $20.00 © 2005 IEEE

4

Eurographics/ IEEE-VGTC Symposium on Visualization 2009
H.-C. Hege, I. Hotz, and T. Munzner
(Guest Editors)

Volume 28 (2009), Number 3

Bivariate Transfer Functions on Unstructured Grids
Yuyan Song †1 and Wei Chen‡2 and Ross Maciejewski†1 and Kelly P. Gaither§3 and David S. Ebert†1
1 Purdue

University Rendering & Perceptualization Lab, Purdue University, USA
2 State Key Lab of CAD&CG, Zhejiang University, China
3 Texas Advanced Computing Center, USA

Abstract
Multi-dimensional transfer functions are commonly used in rectilinear volume renderings to effectively portray
materials, material boundaries and even subtle variations along boundaries. However, most unstructured grid
rendering algorithms only employ one-dimensional transfer functions. This paper proposes a novel pre-integrated
Projected Tetrahedra (PT) rendering technique that applies bivariate transfer functions on unstructured grids. For
each type of bivariate transfer function, an analytical form that pre-integrates the contribution of a ray segment
in one tetrahedron is derived, and can be precomputed as a lookup table to compute the color and opacity in
a projected tetrahedron on-the-fly. Further, we show how to approximate the integral using the pre-integration
method for faster unstructured grid rendering. We demonstrate the advantages of our approach with a variety of
examples and comparisons with one-dimensional transfer functions.
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Picture/Image
Generation—Viewing algorithms I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Color,
shading, shadowing, and texture

1. Introduction and Motivation
Multidimensional transfer functions are widely used for feature enhancement on rectilinear volume datasets [KKH02].
Concerning the efficiency, one common choice is a bivariate
transfer function, which is built upon the two-dimensional
histogram of the scalar quantity and the gradient magnitude within the underlying dataset [KD98]. This induces
one challenging task, that is, how to effectively modulate a
transfer function for visualizing complex structures within
the volume data. In the past decades, many automatic or
semi-automatic two-dimensional transfer function design
approaches [PLB∗ 01, KPI∗ 03] have been proposed.
No matter how complicated a two-dimensional transfer
function is, its color and opacity maps can be represented
and manipulated with a collection of specific design toolkits,
such as iso-region, Gaussian, triangle, and sine curve based
classification widgets, as shown in Figure 1. Other geometric
† Email: {song7|rmacieje|ebertd}@purdue.edu
‡ Email: chenwei@cad.zju.edu.cn
§ Email: kelly@tacc.utexas.edu
c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

shapes can also be used, such as trapezoids, tents, boxes and
ramps presented in [KG01]. The Gaussian transfer function
approach [KPI∗ 03] employs a sum of Gaussians to approximate a transfer function, which can be analytically integrated
over a line segment under the assumption that data values
vary linearly between two sampled points.
While two-dimensional transfer functions have been popularized for rectilinear volume datasets, most unstructured
grid rendering algorithms are limited to one-dimensional
transfer functions, leaving materials, material boundaries or
subtle variations along boundaries undiscovered.
Our work introduces a high-quality unstructed volume
rendering algorithm, taking advantage of the power of twodimensional transfer functions by means of a new preintegrated bivariate transfer function scheme. For volume
rendering, we utilize the Projected Tetrahedra (PT) due to
its efficient parallelization, and we employ the use of Gaussian, triangle wave and sine wave transfer functions to enhance features in complex unstructured grids. Furthermore,
we simplify the rendering computations by subdividing an
integral range into small subintervals and accumulate their
contributions through computationally inexpensive analyti-

784

Y. Song & W. Chen & R. Maciejewski & K. P. Gaither & D. S. Ebert / Bivariate Transfer Functions on Unstructured Grids

(a)

(b)

(c)

(d)

Figure 1: Four widgets used in two-dimensional transfer function design: (a) Uniform; (b) Gaussian; (c) Triangle wave; (d)
Sine wave.

cal approximations. Figure 2 compares the results that are
produced with conventional one-dimensional transfer functions and our new scheme.

Figure 2: Rendering of the Bluntfin dataset with different
pre-integrated rendering algorithms. (a) One-dimensional
transfer function; (b) two-dimensional transfer function.
Corresponding transfer functions are shown in the upper
left-hand corners of the images. Homogeneous areas are
made transparent by 2D transfer functions in (b) therefore
the features can be better separated and visualized.
The remainder of this paper is organized as follows. Related work is discussed in Section 2. We present our solution
for a hardware-acceleratd PT rendering algorithm in Section 3. The analytical integrals of common bivariate transfer functions are derived in Section 4. We introduce another
simplified method for interactive rendering in Section 5.
Section 6 presents experimental results and detailed discussions. Finally, we draw the conclusions in Section 7.

The projected tetrahedra algorithm [ST90] is another objectspace approach which decomposes the projected shape of
a tetrahedron into multiple triangles and computes their
contributions individually. It is quite suitable for hardwareaccelerated volume rendering for tetrahedral grids or even
more complex unstructured meshes, e.g., the work presented
in [KQE04].
Pre-integration techniques [EKE01] can reduce sampling
artifacts by pre-computing the integration in a small sampling interval. It is useful in hardware-accelerated regular
volume rendering [HKRs∗ 06] because it improves quality
without the high computational overhead. This scheme has
also been extended to unstructured grid rendering [MA04,
SET∗ 06]. However, only the use of one-dimensional transfer functions or partial pre-integration is employed, where as
our work employs two-dimensional transfer functions.
3. GPU-based Projected Tetrahredra Rendering
Our PT rendering algorithm is built upon the hardwareaccelerated approach introduced in [KQE04]. Every stage
of the PT pipeline can be parallelized using programmable
graphics hardware. Challenges in this include how to perform cell culling to preserve the sparsity of the underlying
data, and how to perform the cell decomposition during runtime. In this section we describe our schemes that leverage
the newest features of programmable graphics hardware to
accomplish cell-based classification and decomposition. We
also demonstrate how to incorporate two-dimensional transfer function designs within the framework of a pre-integrated
projected tetrahedra rendering pipeline.

2. Related Work
Previous work on visualizing unstructured grids can be
roughly divided into two classes. The first category uses a
backward projection scheme, ray casting, that resamples the
data along the ray from the viewpoint and composites the
contributions. Alternatively, the standard volume splatting
algorithm [CM93] processes each element individually by
computing and accumulating its contribution in image space.

3.1. Conservative Cell-based Culling
To reduce the number of processed cells, without sacrificing quality, we design a conservative cell culling technique
based on the assumption that the two-dimensional value
pairs (s, g) are linearly distributed in a tetrahedron. At the
beginning, all cells are loaded into the video memory. Once
the transfer function is modified, the cells will be validated
c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

Y. Song & W. Chen & R. Maciejewski & K. P. Gaither & D. S. Ebert / Bivariate Transfer Functions on Unstructured Grids

against the transfer function and the identifications of the
valid cells are updated into an index buffer for rendering. To
validate a cell, we check the two-dimensional value pairs of
its four vertices in the CPU. If any of the vertices is in the
valid range of the employed transfer function, i.e., the opacity after applying the transfer function is not zero, this cell
is valid and should be rendered. In this way, cell culling is
conservative, and only requires several arithmetic comparisons between the value ranges of the four vertices and the
bounding box of each individual transfer function widget.

3.2. Cell Decomposition
The primary obstacle of cell decomposition in the GPU
has historically been a lack of vertex neighbor information
within the graphics pipeline. Our approach classifies the projected silhouette of a tetrahedron on the image plane into two
classes, as shown in Figure 3 (a-b). After projection, the silhouette of a tetrahedron is either a triangle (the inside case),
or a quadrilateral (the outside case). In both cases, the image
space position and scalar value of the thickest point in the
tetrahedron can be determined by computing the barycentric coordinates from the image space coordinates of four
vertices. The decomposition yields three or four triangles,
where each vertex contains the image and object space coordinates and scalar values of the corresponding front and back
points for later rasterization. The rendering of a tetrahedron
is performed in three steps:
The positions of four vertices in both the screen space and
the object space are computed in the vertex processing
stage.
The cell decomposition is carried out using the geometry shader. Perspective corrections are applied to the image space coordinates before the classification of the projected silhouette. For the inside case, the barycentric coordinates are computed and used to interpolate the values
of the thickest point. For the outside case, the intersection
parameters and the thickest point are calculated.
The object-space coordinates, scalars of the front and back
points, and other parameters are used for volume illumination in the fragment processing stage.
(αb , cb )
D

(α f , c f )

(a)

(b)

(c)

Figure 3: (a) The inside case; (b) The outside case; (c) The
integral in a tetrahedron.

c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

785

3.3. Cell Shading
Standard pre-integrated volume rendering [EKE01] precomputes a three-dimensional lookup table for each triple of
(s f , sb , D), where s f and sb are the scalar values of the entry point and exit point along an integral ray, and D is the
thickness of the ray within the tetrahedron:
Z D

λ
λ
)s f + sb )
D
D
Z λ
ρ
ρ
α((1 − )s f + sb ) dρ)dλ
· exp(−
D
D
0
Z D
λ
λ
α = 1−exp(−
α((1 − )s f + sb ) dλ)
D
D
0

C=

0

c((1 −

(1)

where α(·) and c(·) denote the employed transfer functions
for opacity and color respectively (see Figure 3 (c)). Please
note that throughout this paper, we use the subscripts f and
b to denote the properties of the entry and exit points.
One main drawback of this scheme is that the twodimensional transfer function can not be used for unstructured grids [KQE04] because the integral presented in Equation 1 only relates to the input scalar values. If we incorporate the gradient magnitude into the transfer function (i.e.,
the α(·) and c(·) items in the integral), a five dimensional
pre-integration table is required.
In volume visualization, there are two data classification modes, namely, post-classification and preclassification [HKRs∗ 06]. Logically the standard PT rendering uses a post-classification shading scheme, which
avoids densely sampling the input data by means of an
one-dimensional pre-integrated kernel. To facilitate efficient
multi-dimensional transfer function design, we propose two
high-quality schemes based on the post-classification and
pre-classification modes separately.

3.3.1. Post-classification with Analytical Pre-Integration
For each type of transfer function widget, we derive an analytic integration form in the post-classification mode, which
will be explained in Section 4. The renderings for view rays
involve the volume rendering integral evaluations with the
pre-integrated tables for a certain type of transfer function.
Theoretically, this mode yields better quality than that of the
approximated mode, which is discussed below.

3.3.2. Pre-classification with Approximated
Pre-Integration
We denote the color and opacity of the entry and exit points
of a viewing ray as (c f , α f ) and (cb , αb ), and assume that
the color and opacity are linear between the two points in a

786

Y. Song & W. Chen & R. Maciejewski & K. P. Gaither & D. S. Ebert / Bivariate Transfer Functions on Unstructured Grids

given tetrahedron (Figure 3 (c)):

opacity at each point of the segment:

Z D

λ
λ
C=
((1 − )c f + cb )
D
D
0
Z λ
ρ
ρ
((1 − )α f + αb ) dρ)dλ
· exp(−
D
D
0
Z D
λ
λ
α = 1−exp(−
((1 − )α f + αb ) dλ)
D
D
0

Z D

λ
λ
λ
λ
)s + s , (1 − )g f + gb )
D f D b
D
D
Z λ
ρ
ρ
ρ
ρ
α((1 − )s f + sb , (1 − )g f + gb ) dρ)dλ
·exp(−
D
D
D
D
0
Z D
λ
λ
λ
λ
α =1 − exp(−
α((1 − )s f + sb , (1 − )g f + gb ) dλ)
D
D
D
D
0
(4)

C=

0

(2)

c((1 −

It yields:
Straightforwardly implementing it would be quite timeconsuming
and be impractical for real applications. Our soC = c f F(α f , αb , D) + cb B(α f , αb , D)
lution is based on a simple analysis to the domain space
D
α = 1 − exp(− (α f + αb ))
(3)
of the two-dimensional transfer functions. An arbitrary two2
dimensional transfer function widget covers a region of the
domain. Meanwhile, the range of the density and gradient
where
magnitude along one ray segment in a tetrahedron forms a
Z D
Z λ
rectangle. The rectangle may be inside, outside or intersectλ
ρ
ρ
F(α f , αb , D) =
(1 − )exp(−
((1 − )α f + αb ) dρ)dλ
ing with the region covered by the transfer function widget
D
D
D
0
0
Z D
Z λ
in the two-dimensional histogram space (see Figure 4).
λ
ρ
ρ
exp(−
((1 − )α f + αb ) dρ)dλ
B(α f , αb , D) =
D
D
0 D
0
We pre-compute F(α f , αb , D) and B(α f , αb , D) for all
triples of (α f , αb , D) and store them in a 3D texture. In
the fragment processing stage, (α f , αb , D) is used as an index to access the corresponding values F(α f , αb , D) and
B(α f , αb , D) from the pre-computed texture. Thereafter,
Equation 3 can be quickly evaluated with simple fragment
instructions. Note that the determination of (c f , cb , α f , αb )
is independent of the pre-integration table, and thus can be
computed with any two-dimensional transfer function. This
is significantly different from the standard pre-integrated PT
rendering algorithm with a one-dimensional transfer function [KQE04]. Our approach is similar to the partial preintegration method that was discussed in [MA04]. We go
beyond the work to derive the approximation method that is
more suitable for 2D transfer function renderings.
This mode may lead to unpleasant results if we use front
and back points directly after cell decomposition because the
transfer functions of the underlying tetrahedra may vary significantly along the viewing rays. We propose an approximation method that subdivides a viewing ray into a fixed number of equal length segments and computes the total contribution of the segments, where a segment’s contribution can
be easily obtained from the above pre-computed texture. The
method is discussed in Section 5.

4. Analytical Bivariate Transfer Functions
Given the density and gradient magnitude of two ending
points of a ray segment, (s f , g f ) and (sb , gb ) the values
in the segment are linearly distributed. Ideally, the postclassification mode computes and integrates the color and

(s f , g f )

(sb , gb )

a ray segment

B
C

(max(s f , sb ), max(g f , gb ))

A

(min(s f , sb ), min(g f , gb ))
Figure 4: Three cases of the relationship between a value
range in a ray segment and a two-dimensional transfer function widget.
For the first two cases, the ray segment is either visible or
invisible. If it is visible (case A in Figure 4), the values of the
ray segment can be fully modeled with the transfer function
widget. If the transfer function widget itself has an analytical form, computing the color and opacity of the ray segment can be represented in a closed form. In the following
sections, we show how to derive the forms for some representative transfer function widgets. In the third case, the ray
segment can be adaptively divided into a list of smaller intervals, of which the value ranges are either inside or outside
of the underlying transfer function widget. This process can
be efficiently handled by means of the technique introduced
in Section 5.
The analytic integrals along ray segments for common
2D transfer functions are derived below, including uniform
transfer function, Gaussian transfer function, banded triangle waves and banded sinusoid waves. Uniform transfer functions are suitable for rendering homogeneous areas.
Gaussian transfer functions are well suited for classification
and visualization of local features in the transfer function doc 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

Y. Song & W. Chen & R. Maciejewski & K. P. Gaither & D. S. Ebert / Bivariate Transfer Functions on Unstructured Grids

main. Triangle waves and sinusoid waves are used for illustrative rendering effects while sinusoid wave transfer functions tend to give smoother transitions between layers.
4.1. Uniform Transfer Functions
Uniform 2D transfer functions assign uniform color and
opacities to all vertices of a cell. Therefore, one ray segment
for this cell has the contribution:
C=

Z D
0

c × exp(−

Z λ

αdρ)dλ =

0

c
(1 − e−αD )
α

(5)

787

the integral part of above equation can be evaluated as
table(P, AD + B) − table(P, B).
A special situation occurs when ks and kg are 0.
In this case the ray contribution can be evaluated with
the uniform transfer function in Section 4.1 with α =
−s0 )2
−g0 )2
αmax exp(−( (sa2σ
+ (ga2σ
)). Figure 5 demonstrates the
2
2
s

g

effects of a 2D uniform transfer function and a 2D Gaussian transfer function respectively. The 2D Gaussian transfer
function clearly shows the vortex tubes and bow shock of the
flow.

4.2. Gaussian Transfer Functions
Often, a two-dimensional Gaussian transfer function widget is specified with a constant color and a two-dimensional
Gaussian opacity function whose center and variation are
(s0 , g0 ) and (σs , σg ) respectively. The two-dimensional
Gaussian distribution function for the opacity is:
(s − s0 )2 (g − g0 )2
GT F(s, g) = αmax exp(−(
+
))
2σ2s
2σ2g

(6)

where αmax is the maximum opacity of the Gaussian distribution.
For a Gaussian transfer function, the color is constant,
meaning color(s(λ)) = c. Thus, we have:
s(λ) = s f +

gb − g f
sb − s f
× λ, g(λ) = g f +
×λ
D
D

(7)

By specifying the α(·) function to be the two-dimensional
Gaussian distribution, and substituting the linear relationships Eq. 7 into Eq. 4, we have the contribution as:
(s(ρ) − s0 )2 (g(ρ) − g0 )2
+
))dρdλ
2σ2s
2σ2g

C=

Z D Z λ

exp(−(

=

Z D Z λ

exp(−(Aρ + B)2 −C)dρdλ

c

0

0

c

0

0

B .

The above integral can be further written as
√ −C
Z D
πe
(er f (Aλ + B) − er f (B)))dλ
C =
c · exp(−αmax
2 A
0
Z
1 AD+B
= c · exp(P · er f (B))
exp(−P · er f (t))dt
A B
here er f (·) is the error function and P = αmax

Figure 5: (a) Applying a uniform 2D transfer function to
the M6 Wing dataset. (b) Two-dimensional Gaussian transfer function applied to the NASA X38 dataset and the vortex
tubes and bow shock of the flow are clearly shown in this
image.

4.3. Triangle Waves Transfer Functions
Triangle wave bivariate transfer functions are often used to
create contour rendering effects. Figure 1 (c) is a typical
transfer function with two triangle peaks. If we assume there
is only one peak along the view ray in a tetrahedron cell, the
contribution of the ray can be computed in an increasing and
a decreasing part.
The opacity can be described as the following equations
for the increasing and decreasing parts are:
α(λ) =

gb −ga
a
Here, ks = sb −s
D , kg = D , ds = sa − s0 , dg = ga − g0 ,
r
k2
ks ·ds /2σ2s +kg ·dg /2σ2g
d2
k2
d2
, C = ( 2σs 2 + 2σg2 )−
A = 2σs 2 + 2σg2 , B =
A
s
g
s
g
2

√ −C
πe
2 A .

The two dimensional pre-integration table can be computed and looked up on-the-fly based on (P,t). Thus,
c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

(b)

(a)

αmax
λ,
Dt

α(λ) = αmax (1 −

λ
)
Dt

(8)

where Dt is the segment length for interval [0, αmax ].
To obtain the contribution of a ray segment from λ = d1 to
λ = d2 from the increasing part, we can rewrite the integral
as:
C=

Z d2
d1

c · exp(−

Z λ
d1

kα · ρdρ)dλ

(9)

for a uniform color triangle wave transfer function.
Equation 9 becomes:
C = c

Z d2
d1

=

r

exp(−

kα 2 λ
ρ |d1 )dλ
2

r
r
kα
π
kα
kα
c · exp( d12 )[er f (
d2 ) − er f (
d )]
kα
2
2
2 1

Y. Song & W. Chen & R. Maciejewski & K. P. Gaither & D. S. Ebert / Bivariate Transfer Functions on Unstructured Grids

788

where er f (·) is error function.
The contribution of a segment in the decreasing part from
λ = d3 to λ = d4 can be evaluated as:
C =

Z d4
d3

c · exp(−

Z λ
d3

αmax (1 −

d3 − Dt 2
= c · exp(−αmax ( √
) )
2Dt

ρ
)dρ)dλ
Dt

Z d4
d3

λ − Dt 2
exp(αmax ( √
) )dλ
2Dt

The integral part of the above equation can be pre-integrated
and stored as a two-dimensional look-up table with a look-up
√ t | , αmax ). Note that the look-up entry is bounded
entry ( |λ−D
2Dt
with Dt as the maximum edge length in the grid. When there
is no opacity variation between the front and back points,
we have the same volume rendering integral as the uniform
transfer function discussed in Section 4.1. Figure 6 (a) shows
the effect for a banded triangle wave transfer function with
no lighting applied on the Heatsink dataset.

as a two-dimensional texture with (t, αmax /kπ ) as the lookup entry of this integrated table. Figure 6 (b) demonstrate
the effects of a 2D banded sinusoid wave transfer function
with gradient-based diffuse lighting applied to the Heatsink
dataset.
5. Approximated Bivariate Transfer Functions
Our pre-classification with approximated pre-integration
scheme is valid given that the color and opacity vary linearly
between the two ends of a viewing ray segment. Often, the
color and opacity vary non-linearly or even a peak appears
along the viewing ray between the entry and exit points of a
tetrahedron. However, if the viewing ray is divided into a set
of segments, each segment can be seen as having the color
and opacity vary linearly between its two ends. The contribution of the ray can be approximated if each segment’s contribution is known.
Our approximation method divides a viewing ray into a
fixed number of equal length segments. For each segment, its
two ends can have two-dimensional values linearly interpolated along the ray, and their corresponding colors and opacities can also be obtained. The segment’s contribution can
then be looked up from the pre-integration texture provided
by our pre-classification with approximated pre-integration
scheme. The total contribution of the viewing ray is computed by blending the segments.

(a)

(b)

Figure 6: Volume rendering on the Heatsink dataset with
(a)a banded triangle wave transfer function with no lighting;
(b) a banded sinusoid wave transfer function with gradientbased diffuse lighting.

4.4. Banded Sinusoid Transfer Functions
Banded sinusoidal transfer functions describe the opacity
with a section of a sinusoidal function (see Figure 1 (d)).
If we assume there is at most one peak inside a tetrahedron
cell and a ray segment’s entry and exit points are within one
cycle of the sinusoid, we will have the volume rendering integral for a uniform color segment as:
C=

Z d2
d1

c · exp(−

Z λ
d1

ρ
αmax sin(π )dρ)dλ
Dπ

(10)

where Dπ is the angular frequency.
Let kπ = π/Dπ and Equation 10 becomes:
C = c·
= c

Z d2
d1

exp(

αmax
(cos(kπ λ) − cos(kπ d1 )))dλ
kπ

Dπ
αmax
cos(kπ d1 ))
· exp(−
π
kπ

Z kπ d2
kπ d1

exp(

αmax
cost)dt
kπ

The above integral part can be pre-computed and stored

Figure 7: Rendering results of the NASA X38 dataset with
the analytic form (left) and approximated pre-integration
mode (right). A two-dimensional Gaussian transfer function
is used.
Figure 7 compares the effects with the analytical and
approximated pre-integration schemes. Both share a twodimensional Gaussian transfer function, while the latter uses
10 samples along viewing rays for approximation. The results show that both schemes perform well on revealing the
internal structures of the dataset.
6. Results and Discussions
We have implemented and tested our system on a PC with a
Xeon 2.0 GHZ CPU, 16.0GB RAM and a NVDIA GeForce
8800 GTS video card. The Bluntfin, Heatsink, Cylinder, ONERA M6 Wing, NASA X38, and Delta Wing datasets are
c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

Y. Song & W. Chen & R. Maciejewski & K. P. Gaither & D. S. Ebert / Bivariate Transfer Functions on Unstructured Grids

rendered using our system. Table 1 lists the data configuration and performances with conventional one-dimensional
transfer functions, the analytical two-dimensional Gaussian
transfer functions, and the approximated pre-integrated 2D
Gaussian transfer functions.

789

function. Figure 8(b) is generated using our 2D banded sinusoid transfer function and it shows the same internal structures compared to (a) with much sharper material boundaries.
6.2. Illustrative Rendering Effects

Table 1: Datasets and performance comparison in FPS. In
the last column, the performances with the analytical and
approximated bivariate transfer functions are reported.
Data

#Tetrahedra

1D

Blunt Fin
Heatsink
Cylinder
M6 Wing
X38
Delta Wing

224874
121668
762048
287962
1943483
3853502

1.27
1.91
0.42
1.27
0.18
0.08

2D
Analy.
0.16
0.23
0.07
0.11
0.04
0.03

2D Approx.
0.91
1.00
0.36
1.27
0.18
0.07

6.1. Volume Rendering

(a)

(b)

Figure 8: Rendering the Cylinder dataset using (a) an onedimensional transfer function [SET∗ 06]; (b) our approximated bivariate sinusoid transfer function.

Although unstructured grids can be visualized with onedimensional transfer functions, our two-dimensional transfer function design scheme provides much more flexibility
for achieving various feature enhancement effects. Figure 2
compares the results of a standard pre-integral PT rendering [KQE04] and our new scheme on visualizing Bluntfin
dataset. The same banded patterns based on the scalar values can be seen in the transfer functions. It is apparent that
our result (Figure 2 (b)) more clearly separates and displays
the internal structures by making homogeneous areas transparent through 2D transfer functions.
Another comparison between 1D and 2D transfer functions is applied to the Cylinder dataset. Figure 8 (a) is generated following the method proposed in [SET∗ 06], which employs gradient-based shading on top of a banding 1D transfer
c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

Figure 9: Illustrative rendering effects for the Delta Wing
dataset: (top) semi-transparent rendering based on a banded
triangle wave transfer function; (bottom) view-dependent
enhancement using a bivariate triangle wave transfer functions.
We can also employ two-dimensional transfer function
design and per-vertex gradient computation to perform feature enhancement. Various illustrative effects can be supported through our schemes for effectively depicting the
oriented features. Semi-transparent isosurface layers can be
produced when only the peak lines of the banded transfer
functions are rendered. The view-dependent enhancement,
or the standard silhouette enhancement, can also be fulfilled
by incorporating the viewing vector ~V in the pre-integrated
opacity α pre−int :
α = α pre−int × |1 − ∇g · ~V | p
where p is the enhancement coefficient. ∇g is the unit vector
in the direction of the gradient of the scalar function s where
∇s
.
∇g = k∇sk
Figure 9 shows the semi-transparent multi-layer rendering
and silhouette illustration applied on the Delta Wing dataset
respectively.

790

Y. Song & W. Chen & R. Maciejewski & K. P. Gaither & D. S. Ebert / Bivariate Transfer Functions on Unstructured Grids

6.3. Discussion
For the derivation of the triangle wave and sinusoid wave
transfer functions to be valid, there is at most one peak along
a view ray in a cell. This requirement can not be met when
the frequency of the banded transfer functions is too high.
To determine if a frequency is too high for a dataset, data
analysis on the minimum value difference among vertices
of each cell is performed upon loading of the dataset. The
corresponding frequency will be used as the frequency constraint in our system on the banded transfer function design.
Our implementation of GPU-based Projected Tetrahedra
is successful. Cell decomposition and cell shading are both
performed on the GPU with much less data needing to
be streamed from the CPU. However, these improvements
are significantly slowed down by our fragment processing, which appears as a bottleneck. The per fragment clipping and mathematical computations applied in our methods
cause our system to be slower than other current systems.
We expect our system to achieve much better speed with
the significant improvement of fragment processing power
of future 3D graphics hardware.
The pre-classification with approximated pre-integration
mode performs much faster than the post-classification with
analytical pre-integration mode, as seen in Table 1. Though
the approximation error may not be guaranteed to below a
certain level, the viewing ray can be better approximated if
we increase the number of segments.
Though our method is slower than 1D transfer function
based pre-integration method, our system supports bivariate or even higher dimensional transfer function rendering
on unstructured grids. Such features have not yet been developed or exploited for unstructured grid rendering. Twodimensional transfer function rendering, gradient shading,
semi-transparent isosurfaces and silhouette rendering effects
can be achieved through our system.
7. Conclusions
We have shown that our novel pre-integrated Projected
Tetrahedra (PT) rendering technique is able to apply bivariate transfer functions on unstructured grids. Analytical
forms that pre-integrate the contribution of a ray segment
in one tetrahedron have been derived for common bivariate transfer function primitives. The pre-integration table is
used to compute the color and opacity of ray segments onthe-fly. Moreover, approximated volume rendering of bivariate transfer functions is achieved by subdividing the integral
range into small ones and accumulating their contributions.
In the future, we expect to further simplify the analytic
form computation of the proposed bivariate transfer functions and derive more forms for other ones. More illustrative
rendering techniques can be incorporated into our system for
effective feature enhancements. An automated method for

subdividing the viewing ray into a set of segments so that
the error is below a manageable level will be studied.
8. Acknowledgment
This work has been funded by the US Department of Homeland Security Regional Visualization and Analytics Center
(RVAC) Center of Excellance (Nos. 0081581, 0121288), the
US National Science Foundation (No. 0328984), and Natural Science Foundations China (No. 60873123).
References
[CM93] C RAWFIS R., M AX N.: Texture splats for 3D scalar and
vector field visualization. In Proceedings of IEEE Visualization
(1993), pp. 261–266.
[EKE01] E NGEL K., K RAUS M., E RTL T.: High-quality preintegrated volume rendering using hardware-accelerated pixel
shading. In Proceedings of Eurographics/SIGGRAPH Workshop
on Graphics Hardware (October 2001), pp. 9–16.
[HKRs∗ 06] H ADWIGER M., K NISS J. M., R EZK - SALAMA C.,
W EISKOPF D., E NGEL K.: Real-time Volume Graphics. A. K.
Peters, Ltd., 2006.
[KD98] K INDLMANN G., D URKIN J.: Semi-automatic generation of transfer functions for direct volume rendering. In IEEE
Symposium On Volume Visualization (1998), pp. 79–86.
[KG01] K ONIG A., G ROELLER M. E.: Mastering transfer function specification by using VolumePro technology. In Proceedings of the 17th Spring Conference on Computer Graphics
(2001), pp. 279–286.
[KKH02] K NISS J., K INDLMANN G., H ANSEN C.: Multidimensional transfer functions for interactive volume rendering.
IEEE Transactions on Visualization and Computer Graphics 8,
3 (2002), 270–285.
[KPI∗ 03] K NISS J., P REMOZE S., I KITS M., L EFOHN A.,
H ANSEN C., P RAUN E.: Gaussian transfer functions for multifield volume visualization. In Proceedings of IEEE Visualization
(2003), pp. 497–504.
[KQE04] K RAUS M., Q IAO W., E BERT D. S.: Projecting tetrahedra without rendering artifacts. In Proceedings of IEEE Visualization (2004), pp. 27–34.
[MA04] M ORELAND K., A NGEL E.: A fast high accuracy volume renderer for unstructured data. In Proceedings of IEEE Symposium on Volume Visualization and Graphics (October 2004),
pp. 9–16.
[PLB∗ 01] P FISTER H., L ORENSEN W., B AJAJ C.,
G.K INDLMANN , S CHROEDER W., AVILA L., M ARTIN
K., M ACHIRAJU R., L EE J.: The transfer function bake-off.
IEEE Computer Graphics and Applications 21, 3 (2001), 16–22.
[SET∗ 06] S VAKHINE N., E BERT D., T EJADA E., E RTL T.,
G AITHER K.: Pre-integrated flow illustration for tetrahedral
meshes. In Proceedings of the International Workshop on Volume Graphics (2006), pp. 687–694.
[ST90] S HIRLEY P., T UCHMAN A.: A polygonal approximation
to direct scalar volume rendering. In Proceedings of Volume Visualization (1990), pp. 9–16.

c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

2013 46th Hawaii International Conference on System Sciences

A Role for Reasoning in Visual Analytics
Tera Marie Green
School of Interactive Arts + Technology
Simon Fraser University
terag@sfu.ca

Abstract
Analysis supported by interactive visual interfaces
is a complex process. It involves computational
analytics (the visualization of both raw and derived
data) and an analytical process which requires a
human to extract knowledge from the data by directly
interacting and manipulating both the visual and
analytical components of the system. These two types
of analytics are complementary and the goal of this
paper is to understand interplay between the two. In
this paper we discuss how a study of human reasoning
and
reasoning-supported
cognitive
processes
complement the current emphasis on computational
analysis and visualization. We define this process as
reasoning analytics and present mechanisms by which
this process may be studied.
1. Introduction
As the amount of data available for analysis and
decision making has increased, researchers have begun
utilizing interactive visual interfaces as a means of
incorporating human reasoning into the computational
analysis process. Thanks to the science of data
visualization, much research has focused on methods
of data analysis (referred to as computational analytics
in this paper) and the visualization of those data and
analyses. These tools are imperative to the analytical
process, as they allow the analyst to approach and
interact with the data in a meaningful way. The goal of
such visual analytic tools is to enable analysts to find
patterns, filter, sort, and prioritize their data, ultimately
gaining
knowledge
and
understanding.
The
visualization of these data and analyses also buttress a
human working memory that can juggle only a handful
of concepts at any one time [27].
However, the visualization and computational
analysis are simply tools for the analyst to use. These
tools alone cannot directly reason with the data. The
analyst provides goals, motivation, and cognitive
heuristics and synthesis (also known as a cognitive
toolkit) that computational analytics does not have.
1530-1605/12 $26.00 © 2012 IEEE
DOI 10.1109/HICSS.2013.58

Ross Maciejewski
Arizona State University
rmacieje@asu.edu

Thus, a complete science of visual analytics requires
not only a study of computational analytics, it also
requires an understanding of what we will refer to in
this paper as reasoning analytics : the analyses done by
human reasoning. This reasoning can be an individual
effort between one user and the visualization. Or it can
be a collaborative effort between multiple users and
multiple visualizations. In summary, it is the process of
interpreting the results of the computational analyses
and the data presented within the visualization.
Due to variations in the analytical process (e.g., the
number of users, the complexity of the computational
analytics, and the question(s) to be solved) there is
variation in complexity that affects the needs and
demands of the human-visualization collaborative. In
this paper we will argue that visual analytics is a joint
study of computational analyses resulting in visualized
data coupled with the study of the human analyst and
their analytical process. Because the interaction
between the two sides of visual analytics – the
computational analysis and the cognitive analysis is so
inter-joined, we will argue that they need to be studied
both separately ( to understand their component parts)
and together (to understand the analytical process as a
whole). Furthermore, as we will soon discuss, no
matter what the cognition-- solving problems, making
decisions, evaluating the validity of an idea or
statement, or categorizing concepts -- every cognitive
analytical process beyond perception involves human
reasoning. In this paper we will argue a need for the
study of a reasoning analytics which complements a
study of visualized computational analytics and
broadens our study of visual analytics as a whole. The
question of whether there is a reasoning analytics and
whether it has a place in today’s science of visual
analytics must begin with an evaluation of the current
analytics, a survey of human reasoning, and whether
reasoning behaviors might be quantifiable or reducible
to a sufficient degree as to support a predictive or
informative analytics. Further, supporting a reasoning
analytics though interactive visualization depends on
whether a sufficient understanding of reasoning during

1493
1495

interaction can be harnessed to inform interface design.
We will start addressing these questions by exploring
the current uses of computational analytics in visual
analytics environments and then define what reasoning
analytics might be, before discussing how the two
types of analytics inform each other

2. Computational Analytics
As stated previously, the amount of data available
for analysis has reached unprecedented levels, such
that no human alone could sort, filter and explore the
data for all relevant pieces of information.
Furthermore, the visual representations used to help
analysts gain insight into their data are limited by the
number of visual variables a human can perceive [6].
A recent approach is to utilize computational methods
of dimensional reduction and clustering as a means of
reducing the data to its most relevant information, and
then visualizing this reduced data for the analyst to
explore.
Typical methods would include multidimensional scaling (MDS) [28], principle component
analysis (PCA) [24], and k-means clustering [7].
Variations of these methods are found in many visual
analytics systems as a means of classifying data.
However, in order to visualize the results of
computational analyses such as MDS and PCA,
variables are reduced and combined, resulting in new
spaces which require further reasoning by the analyst.
For example, Jeong et al. [13] utilize interactive visuals
as a means of explaining the results of principle
component analysis to the analysts.
These issues of scaling, projection and
computation are even further compounded in
traditional
scientific
visualization
examples.
Volumetric visualization uses a variety of
computational analyses in determining how to map and
render voxel data. Often times this involves the design
of transfer functions [20] which project the voxel
information into 1 or 2 dimensional histograms.
Unfortunately, the projection of the volumetric data
properties down to a 2D space will obscure features,
making it difficult for users to reason about how to
separate regions of interest within the volume with
only a 2D transfer function. In order to overcome these
difficulties, much research has focused on enhancing
transfer function design through the addition of other
data properties. Examples include work by Lundstrom
et al. which introduced the partial range histogram
[22], [23] and the α-histogram [20] as means for
incorporating spatial relations into the transfer function
design. More recently, Maciejewski et al. [23] utilized
non-parametric clustering and density estimations to
better extract data patterns and make transfer functions
more effective.

Along with using computational analyses as a
means of directly reducing the problem space,
techniques for determining the efficacy of the data are
also employed. For example, one common type of
statistical analysis is user-set confidence values; these
are usually set to indicate degree of uncertainty in the
validity of the visualized artifact, such as in the
Scalable Reasoning System [29].
Other systems directly utilize computational
analyses as a means of finding anomalies within the
data. For example, Maciejewski et al. [17] utilize
control chart methods and spatial scan statistics to
directly extract and present regions of anomalous
health events to the analysts. As in all of the examples
provided in this section, the analyst interacts with the
data resulting from the computational analysis.
As we have seen, the topic of computational
analytics covers a broad area. One thing that becomes
apparent during a browsing of the literature is that the
analytical technique is co-joined with how the analyses
are visualized. The visualization itself is often seen as
an end-product of the analysis. What the user does with
the visualization is often assumed (for example, as
variables of interaction used to refine the data display)
or not considered (for example, how the analytical
process is supported – or not – by the display) and
interaction. This is where a reasoning analytics enters
the picture.

3. The Makeup of a Reasoning Analytics
Generally speaking, most computational analysis
is not equipped to adjust to the user; the user is
expected to adapt to the visualized analysis. User
interactive behavior is seen as a metaphorical, if not
computational, constant; its veracity is usually not
questioned. Whatever the actual state of the user (e.g.,
they may be completely lost within the interface and
making decisions only to learn the interface by trial
and error, or they may be an expert that sees something
the analyses cannot see), the computational analysis
often has no way to compute it. Attempts at intent
analysis (see for example [38]) depend on explicit user
behavior such as web histories. Subtle changes in user
affect or intrinsic motivation are a black box to these
analyses. In short, computational analyses assume
reduction, relative certainty, and computational
constancy. Ill-formed problems whose solutions are
derived from large data may or may not satisfy these
computational assumptions, and human analysts, with
their variety of individual perspectives, cognitive
variances and differential expertise almost never do.
As such, the computational solution lacks human
insight and reasoning process.
Computational
solutions return the needle in the haystack; however, as

1496
1494

the stacks become larger, the problem of producing a
needle from a haystack becomes a problem of
producing a relevant needle from a stack of needles.
Filters and computation narrow down options; they do
not in and of themselves choose the relevant option.
This is left to the human analyst, and for good reason.
A reasoning analytics would focus on the human
analyst and her ability to make sense of information,
create mental models, adapt to rapid changes, generate
hypotheses and defend conclusions drawn with
evidence. These processes are currently outside the
purview of computational analyses and can only be
done by a human analyst.
Reasoning – and the processes it supports such as
decision-making and problem-solving -- are all
analytical processes at which human analysts are
proficient, especially if the choices have been filtered
to a manageable number by computational analysis and
presented in a comprehensible manner through
visualization. Finding associations between items,
creating relationships between them, looking for
evidence to support these relationships and
constructing valid hypotheses or narratives that explain
and motivate the relationships are all analytical
processes humans do daily in a variety of settings.
Each of these analytical processes involve decision
making and problem solving. Decision-making,
informally defined, is the choice of one option among
available alternatives [19], [21]. It’s a simple definition
for an involved mechanism. Defining problem-solving,
however, requires defining a problem and a solution. A
problem is any situation or position that differs from a
desired goal. A problem’s solution is the decision or
series of decisions which attain the originally desired
goal. When viewed this way, problem-solving could be
seen as the more complex of the two analytical
structures, as it utilizes decision-making throughout.
You can make decisions without problem-solving but
you cannot problem solve with out making decisions.
Additionally, problem-solution is very flexible, and
addresses a wide variety of problem states, from simple
and concrete to unbelievably complex and abstract.
The problem may be well-defined, with clearly
outlined boundaries and solved algorithmically. Wellformed problems are the type that may be solved with
a computational approach; following the algorithm will
arrive at the goal state. Ill-formed problems, however,
such as those tackled by visual analytics, lack clear
problem definition and can involve daunting
complexity. There are no clear paths to these solutions.
But the solution is likely to employ a wide variety of
reasoning methods including trial and error, means-end
analysis [34], and analogy [9]. Some problems require
analogical and abstract reasoning to reorganize before
acquiring a solution; the solution for some of these
problems is so “impossible” that research still has not

been able to explain how or why the participant was
able to find the solution [39]. Humans can restructure
problems by rearranging information in ways that
computational analysis quite literally could never
imagine. The way a human does this reorganization is
not completely understood, even by the human analyst,
and can vary from a Gestalt organization to the
unpredictable a-ha! moment [39],[18].
As previously stated, problem-solving depends on
decision-making. Decision-making can be algorithmic,
and is computable enough to have become the basis of
several artificial intelligence systems [14],[32]. The
central normative theory of decision-making in
psychological study is Expected Utility Theory (EUT)
[44],[15]). EUT uses smaller decisions about the
weight (expected value) of each possible option,
combined with probability logic, to make a decision
about which option is preferred. However, humans do
not always make decisions in a normative (or best
practices) fashion, and so other theorists have focused
on the differences between normative decision-making
and how humans are observed to have made the
decision[15]. Whatever the decision-making domain,
decision-making is empowered by multiple reasoning
heuristics and systems. Heuristics such as saticficing
[34] and elimination-by-aspects [14] are used, as are
more complicated reasoning systems such as analytical
reasoning and mental model creation [21]. Decisionmaking directly informs and interacts with reasoning
[15] and either directly or through decision-making,
interacts with problem-solving. While some argument
may be made as to where reasoning ends and decisionmaking and/or problem-solving begin, it is fair to
assume there would be little higher cognition without
reasoning; reasoning is the glue that holds the whole
analytic process together. Further, many of the
decision-making processes have been studied to the
point of understanding how and why decisions were
made as they were [14]. This is not necessarily true of
reasoning; unlike decision-making, there are no
computable models of reasoning. In seeking
comprehension of the analytical process, understanding
and predicting reasoning completes the narrative.
Therefore, in our exploration of a reasoning analytics,
reasoning is a logical place to focus.

4. Reasoning
As we have seen, humans who use visual interfaces
manipulate concepts and ideas through reasoning.
Human reasoning as a construct, however, is rarely
wholly defined on its own. Any general definition is
subject to tweak and critique from a variety of
scholarly disciplines such as philosophy, psychology,
and the computational sciences (in the case of formal

1497
1495

logic). Thus, every definition tends to be narrowed to a
specific perspective or to a particular type of reasoning
(abductive, moral, etc.). In our discussion of a
reasoning analytics, we will focus predominantly on

the perspectives of psychology and cognitive sciences,
which define human reasoning based on the task and
human
behavior
involved.

The Terms for the Two Systems Used by a Variety of Theorists
and the Properties of a Dual-Process Theories of Reasoning
S ystem 1
System 2
Dual Process Theories:
Sloman (1996)
Evans (1984, 1989)
Evans & Over (1996)
Reber (1993)
Levinson (1995)
Epstein (1994)
Pollock (1991)
Hammond (1996)
Klein (1998)

associative system
heuristic processing
tacit thought processes
implicit cognition
interactional intelligence
experiential system
quick & inflexible modules
intuitive cognition
recognition-primed decisions

rule-based system
analytic processing
explicit thought processes
explicit cognition
analytical intelligence
rational system
Intellection
analytic cognition
rational choice strategy

Figure 1. Nine theorists and their dual system theories. Adapted from [36], pg 145.
himself, arguing that reasoning and decision-making
inform each other, but the two are separate cognitive
processes [15]. He goes on to argue that there are
computational models of decision-making, but no
counterparts exist for reasoning, largely because
reasoning has little to no observable behavior.
Reasoning must be studied indirectly through the
output of other cognitive processes.
Another example of how reasoning is not the
outcome but the method of reaching the outcome is
Gigerenzer & Goldstein’s Fast and Frugal reasoning
[10]. This is a short series of “one-decision”
reasoning heuristics, or decisions made through
simple but strong elimination reasoning.
These heuristics can be used for a variety of
decisions (most notably about comparisons between
ideas), but the heuristics themselves are not the
decisions. This type of reasoning is also called
“bounded rationality.” Bounded rationality refers to a
reasoning and/or decision-making process that is
bounded by limited information [35].
Human reasoning is a complicated proposition.
For every type of reasoning defined (deductive,
inferential, sentinel, etc.) there is a frame or context
in which the reasoning defined and studied. In reallife usage however, the different reasonings tend to
run together and inform each other with little or no
noticeable transition. In addition to types, there are
also reasoning systems. The most common type of
system is the dual process theory. There are at least
nine published dual process theories ([36], pg. 145),
and each theory shares characteristics with the others.
(See Figure 1.) The first process, or System 1, tends

Visual analytics is the science of analytical reasoning
supported by visual interfaces [37]. Analytical
reasoning can be defined in a variety of ways. In
addition to the Kantian idea of analytical reasoning as
an evaluation of the validity or virtue of the
proposition itself, we will consider analytical
reasoning also as a determination about the value of
given associations between concepts or statements.
Notice that other than determinations about validity,
there are no other required outcomes for analytical
reasoning. This is important because it highlights a
core characteristic: reasoning has little or no explicit
observable behavior. Reasoning is usually not
defined as the outcome; it is defined as how the
outcome is made possible. This may not be explicitly
stated, but it is a common assumption in the
psychology of reasoning literature. Because
reasoning and the cognitive processes it informs are
so closely interrelated, they are often studied
together. Decision-making and problem-solving both
have explicit behavioral outcomes, and reasoning is
often studied through evaluation of the decision made
and solutions created through reasoning.
For
example, Johnson-Laird (e.g. [41]) studies mental
models through the decisions that participants make
about formal syllogisms through deductive reasoning.
His research demonstrates that these models are used
to make decisions and solve problems, but a model or
a system of mental models can be used to make a
variety of decisions or create multiple problem
solutions. That is to say, that model is not the
decision or problem solution; it is how the decision or
solution is reached. Johnson-Laird postulated this

1498
1496

to be a quicker and/or more superficial reasoning. It
is heuristic [12], or based on recognition, such as
Klein’s priming [19]. This “first” system is quick,
and as a result tends to be rather inflexible. It tends to
be heavily dependent on rules, clear boundaries or
other devices for quick elimination. Bounded
rationality is a System 1 reasoning. The first system
can be used to make superficial decisions, or to
“narrow down the field” in the case of more difficult
tasks. It can reduce a blindingly cluttered field of
choices to a more manageable number.
But for more difficult decisions, or when System 1
reasoning can no longer tackle the complexity, such
as those propositions which involve abstract thinking,
dual process theorists purport a System 2. Analytical
reasoning is a System 2 process [31]. System 2
reasoning is powerful and flexible; it allows the
reasoner to modify mental models and wrestle with
complicated concepts. It can make the implicit
explicit. System 2 can be more difficult to study; the
concepts are more complicated; it is informed by
System 1 processes and the transitions are not always
clear. All of the referenced theorists purport that the 2
systems do interact; System 1 is usually seen as the
first step to tackling the current task, but System 2
processes also inform System 1 as the reasoning
evolves through the task.
Reasoning could be seen as the Swiss Army knife
of human cognition. Humans reason early and often.
It is arguable that reasoning gets involved as early in
the process as image understanding, in which an
object is perceived and identified, and during which
semantic meaning begins to be attached. Biederman
suggests this identification is done by reducing the
larger visual scene to smaller recognizable
components, and then using that decomposition to
“understand” the visual image. This understanding
persists even if the edges of the component are
broken or missing[4]. Category learning is usually
accomplished by inferencing or information
integration, both of which are reasoning processes;
information integration is the more System 2 of the
two. Categorization feeds mental model creation,
which, as has already been discussed, is a deductive
reasoning process. The beginning of categorization,
is a form of reasoning inference[11]. Hypothesis
generation and insight creation, both identified as
integral cognitive tasks in visual analytics [11] are
reasoning processes that that start with reasoning in
image understanding and end with mental models and
analytical reasoning. Each decision or problem
solution requires reasoning to acquire. No matter how
advanced the computational analysis, it is arguable
that there is no visual analytics without human
cognition, and that human cognition depends

reasoning.
Because visual analytics problems are so complex,
the visualization itself cannot derive the conclusions
or generate the hypotheses on its own. This is slowly
becoming a more common focus in visual analytics
[42]. However, the study of “reasoning analytics” has
a much shallower literature than computational
analytics. Much of the reason for that is that
reasoning is hard to study, difficult to evaluate, and to
date, nearly impossible to quantify. If it is to have a
role similar to its counterpart computational
analytics, more effort needs to be invested in
understanding how analytical cognition impacts and
is impacted by its computational analytics, within the
context of the interactive visualization.

5. How to Study a Reasoning Analytics
If we are to pursue a reasoning analytics, we need to
understand reasoning and the roles it plays in visual
analytics. In order to build that understanding, it
would seem appropriate to build on the research that
has been reported in the behavioral sciences.
Reasoning in the behavioral sciences is studied
almost exclusively by evaluating the decision made
using reasoning, and, if possible, the methods or
heuristics used to reach those decisions. As has
previously been discussed, it can be quite difficult to
discern between the decision-making and the
reasoning used to reach that decision. However, with
a focus on describing the narrative of reasoning and
then aggregating the behaviors observed to
understand the analytical structure of the reasoning
process, we can begin to describe the analytics.
At the same time, studying reasoning through the
study of decision-making and problem-solving also
allows a continuing study on how decisions are made
during interface interaction. As the task changes,
how the human analyst uses the visualization
changes; the available interactions and the changes in
the view are variables that can impact cognition, with
some techniques being better than others depending
on the goal. We have already seen these differences
in Ware, Neufield, & Bartram’s analysis of the best
techniques for visualizing a causal association[42].
Understanding these cognitive transitions will add to
the understanding of reasoning analytics as well. An
interactive visualization is not the typical artifact, and
it is important also to evaluate how reasoning may or
may not be influenced by an artifact that is at once
virtual and tactile and conceptual. For example, the
interaction metaphors chosen for the interface may
impact reasoning. Ashby, Eh, and Waldron found in
their study of learning behaviors that participant’
learning performance changed when the input

1499
1497

method changed [50]. These are just a handful of
variables that could impact reasoning analytics, and
to date they have only been studied in piecemeal
fashion, if they have been studied at all. Unlike
computational analytics however, the analytical
machine that is the human analyst cannot be easily
modularized or even compartmentalized. Each aspect
of cognition is present and influencing the analytical
process as a whole. Therefore, “reasoning analytics”
must be reduced into pieces small enough to study,
and then allowed to inform the study of the other
pieces. This has already been done in a small way
through evaluations of “sensemaking,” which
involved a study of analysts solving a particular kind
of task was evaluated not only for the holistic process
but for the decomposable subprocesses [33].
One way to study reasoning as part of the narrative
of cognition is to use more holistic methods of
evaluation, such as field studies, case studies,
ethnographies or other types of in situ protocols. See
Figure 5. The strength of in situ is the context it
provides, not only on reasoning at every stage of
analysis, but of the analytical process as a whole.
This allows the researcher to see how reasoning
interacts with visualization, and how reasoning
informs decision-making and problem-solving. It also
provides context for computational and reasoning
analytics interact within the visualization. It is
arguable that in situ studies could not replace
carefully constructed laboratory tasks; neither method
answers all the questions that inform a reasoning
analytics. Depending on the topic of inquiry, both
methods should be employed at some point in the
investigation.
Placing reasoning inside the architecture of
cognition is another way to study the narrative of a
reasoning analytics. Several cognitive architectures
have been used to model cognition and test extant
reasoning theories. It is probably safe to assume that
no current cognitive architecture could capture the
complexity of a reasoning analytics, but a brief
survey of one or two provide an idea of how useful
they can be in the study of complex cognition in
visual analytic
ACT-R is a hierarchical cognitive architecture
which modularizes cognition into modules, buffers
and production systems [1]. The production system
evaluates the state of the each of the modules and
their respective buffers. There are two primary
modules: perceptual-motor and memory (See Figure
2.) These modules execute as directed by the
production system. Also part of the basic architecture
is the patter matcher, which looks for a in-process
production which matches the current state, as only
one production may fire at a time.

Figure 2.
The core
architecture. From [1].

of

the

ACT-R

Figure 3. The Soar Architecture. From (35).
ACT-R demonstrates how the cognitive processes
interact and feed each other. Many of the simpler
cognitive processes, including some forms of
categorization, have been modeled with ACT-R (see
http://act-r.psy.cmu.edu/ for more). But the
architecture is still too limited to allow for the
complexity of visual analytics cognition: one
production at a time is no where near enough;
sequential processing is not adequate when working
solutions to ill-formed problems.
Soar is a non-hierarchical cognitive architecture.
The core of the architecture consists of a long-term
memory and a short term memory. The long-term

1500
1498

useful.
Lastly, one way to view the interaction between
computational and reasoning analytics is to consider
a metaphor within reasoning itself. Much like dual
process theories of reasoning, visual analytics has a
dual process mechanism. (See Figure 4.)
Computational methods could be seen as a semblance
of a System 1; they are powerful but cognitively
(although not computationally) simple methods,
rather inflexible. Their objective is to narrow down or
filter the data in a meaningful way. They can be used
over and over again, in part because, thanks to the
speed of today’s hardware, they are quick and easy to
execute. And, in an appropriately-written interactive
interface, their variables are simple to manipulate and
change. And finally, much like with System 1
reasoning, they are rarely sufficient to complete the
analytic process in and of themselves.

memory is shared with the short-term memory, which
stores the information as in a graph of associations
and relations between information. See Figure 3. The
decision structure uses these associations to decide
which rules apply to the decision, and all rules which
apply fire at the same time. This decision procedure
more closely mimics human reasoning, which can
certainly handle more than one variable at a time.
Further, Soar handles fuzzy logic in its basic
architecture, which makes it a better representation of
how humans would handle ambiguity. Soar has been
used to model some learning and problem-solving
behaviors (see http://sitemaker.umich.edu/soar/home
for more.) Its flexibility is preferable to ACT-R, but
once again, it is limited in modeling problem-solving
and more complicated decision processes. However,
as a way of modeling aspects of visual analytics
cognition as an attempt to describe how
computational and reasoning analytics interact, it is

Complementary Strengths of Computational and Reasoning Analytics
Computational Analytics
Narrows the field of available choices
Juggle many variables
Make simple decisions
Rationalize without bias
Simplification of noisy data scenes
Support human memory (though
visualization)

Reasonable Analytics
Make holistic sense of data
Develop mental models of analytic
concepts
Rapid adaptation and accommodation
of new information
Categorization with ambiguous rules
Superior problem reorganization
Abstract reasoning
Hypothesis generation and analysis

Figure 4. The strengths of a complementary analytics.
results of System 1. System 1 feeds and informs
System 2 and System 2 refers back to System 1
whenever further narrowing of the choices or
reference to the computation analyses is useful. This
is a somewhat imperfect metaphor, as humans are
affected by a limited working memory, which a
computer with gigabytes of RAM is not. Another
advantage is that computational analyses can operate
without cognitive biases. These biases can impact
human analytical processes as early as System 1
reasoning, influencing which data are considered.
This can mean relevant data is overlooked or
irrelevant data being considered important. A
computational analysis is immune to cognitive bias,
unless, of course, it is programmed to have one.

Visual analytics’ reasoning analytics could be seen as
System 2 analysis. Much like System 2 reasoning,
reasoning analytics are flexible, adaptable, and
sometimes enigmatic. They are often complex
methods which can take considerable time, effort,
and perhaps collaboration with other involved parties
and artifacts, including interactive visualization. They
also tend to be more holistic and/or systemic in their
approach to the problem. They are harder to study
and predict. They are harder to study and predict,
and, thus, they are often poorly understood. When
viewed in this way, the interaction between
computational and reasoning analytics becomes
clearer. As a System 1 process, computational
analytics narrows down choices and (hopefully)
simplifies the problem set. Reasoning analytics as a
System 2 process makes sense of and manipulates the

1501
1499

Methods for Studying a Reasoning Analytics
1. Laboratory tests of decision-making and problem-solving
a. Design the tasks carefully so the heuristics or reasoning methods being utilized become apparent.
b. Compare performance in laboratory tasks of decision-making and problem-solving with performance
using more traditional artifacts (pencil/paper, spreadsheets, etc.)
2. Ethnographic and field studies which place reasoning in a narrative of the analytical process.
3. Place what is learned from laboratory and field studies within the context of a cognitive architecture
Figure 5. Methods for studying a reasoning analytics.

6. Conclusion
Visual analytics requires both a computational
analytics and a reasoning analytics working together.
Interactive visualization supports reasoning analytics
currently by providing tools that augment the
“reasoning analytics” process. The large data
commonly associated with “wicked” problems are
overwhelming stimuli even for the superior reasoning
capacities of the human analyst. The computational
analytics applied to large data narrow the field of
what has to be visually considered by the human. It
also frames the problem and primes the reasoning
process to see the analysis in a particular way. This
can be a strength of the visualization or it can be a
weakness. When the computational analysis detects
patterns and visualizes them, it also does so without
apparent bias. In an effort to narrow the data to be
considered, it is not uncommon for humans to bias
their elimination heuristics in an attempt to move
make the process easier [8]. This is an area where
computational analytics can support reasoning
analytics. By not over-weighting its analyses, humans
can work with all pertinent data and not overlook
what might be pertinent.
Further, human reasoning depends on human
working memory, which is easily overwhelmed and
can be effected supported by the visual representation
that an interactive visualization provides. It is not

uncommon for humans to use artifacts to remember
pertinent information which overwhelms Miller’s 7+chunks of information [26], which is yet another way
that interactive visualization supports reasoning
analytics. In addition to supporting the need for biasfree filtering of large data, a visualization provides a
way to remember the many data that need to be
remembered and considered in the generation of a
hypothesis or considered in a decision. It can also
provide a place where human analysts can work
collaboratively, providing a shared artifact [12] and
supporting asynchronous human to human
interaction.
In conclusion, computational analytics and
reasoning analytics are different but equally
necessary parts of visual analytics. Without the
statistical analyses, the human reasoner would have
an overwhelming task that would likely prove
impossible. And without reasoning analytics, the
visualization of computational analytics produces
pretty pictures of questionable relevance. Further,
both types of analytics have much to learn from each
other. Each informs the other, and supports the other,
as the strengths of each are complementary. (See
Figure 4 & 6.) While more difficult to study, a
reasoning analytics is imperative to the study of a
successful visual analytics system.

1502
1500

Figure 6. The interaction of computational and reasoning analytics.

North C. “Observation-level interaction with statistical
models for visual analytics.” IEEE VAST 2011. 121-130
[9] Gick, M.L., & R.S. Lockhart, “Cognitive and affective
components of insight.” In R.J. Sternberg & J.E. Davidson
(Eds.), The Nature of insight. Cambridge, MA: MIT Press.
1995. pp. 197–228
[10] Gigerenzer, G., & D.G. Goldstein. “Reasoning the fast
and frugal way: Models of bounded rationality. “
Psychological Review, 103(4), 61996. 50–669.
[11] Green, T.M. W. Ribarsky and B. Fisher “Building
and applying a human cognition model for visual
analytics,” Information Visualization 8(1), 2009. 1-13
[12] Heer, J., & M. Agrawala, Design considerations for
collaborative visual analytics. Information Visualization, 7,
2008.49–62.
[13] Jeong, D.H. Ziemkiewicz, C., Fisher, B., Ribarsky,
W., Chang, R. “iPCA: An Interactive System for PCAbased Visual Analytics,” Computer Graphics Forum
(Eurovis 2009). pp. 767-774, 2009.
[14] Johnson-Laird, P. N. (2006). Models and
heterogeneous reasoning. Journal of Experimental and
Theoretical Artificial intelligence, 18(2), 121–148.
[15] Johnson-Laird, P. N., & E. Shafir. The interaction
between reasoning and decision making: an introduction.
Cognition, 49, 1993. 1–9.
[16] Kadivar, N., V. Chen, D. Dunsmuir, E. Lee, C. Qian,
J. Dill, C. Shaw, “Capturing and supporting the analysis

7. References
[1] About ACT-R. http://act-r.psy.cmu.edu/about/
[2] Ashby, F. G., S.W. Ell, & E.M. Waldron, “Procedural
learning in perceptual categorization.” Memory &
Categorization, 31(7), 2003. pp. 1114–1125.
[3] Bell, D.E., H. Raiffa, & A. Tversky. “Descriptive,
normative, and prescriptive interactions in decision making.
“In D. Bell, H. Raiffa, 81 A. Tversky (Eds.), Decision
making: descriptive, normative, and prescriptive
interactions. Cambridge, UK: Cambridge University Press.
1988.
[4] Biederman, I. “Recognition-by-components: A theory
of human image understanding.” Psychological Review,
94(2), 1987. pp. 115–147.
[5] Borg, I., P. Groenen
Modern Multidimensional
Scaling: theory and applications (2nd ed.). New York:
Springer-Verlag. 2005. pp. 207–212.
[6] Braine, M.D.S., & D.P.O. O’Brien. “A theory of “if”:
a lexical entry, reasoning program, and pragmatic
principles.” Psychological Review, 98, 1991 182-203.
[7] J. Choo, Lee, H., Kihm, K., Park, H. “VisClassifier:
An interactive visual analytics system for classification
based on supervised dimension reduction.” IEEE VAST
2010 27-34
[8] A. Endert, Han, C., Maiti, D., House, L., Leman, S.,

1503
1501

process.” Visual Analytics Science and Technology, 2009.
VAST 2009. IEEE Symposium on (pp. 131–138).
Presented at the Visual Analytics Science and Technology,
2009. VAST 2009.
[17] Keim, D. A. Information Visualization and Visual
Data Mining. IEEE Transactions on Visualization and
Computer Graphics, 8(1), 1–8. 2002.
[18] Klein, G. Sources of Power: How people make
decisions. Cambridge, MA: MIT Press. 1998.
[19] Kniss, J., G. L. Kindlmann, C. D. Hansen. “Interactive
Volume Rendering Using Multi-Dimensional Transfer
Functions and Direct Manipulation Widgets.” IEEE
Visualization 2001
[20] Kozielecki, J. “Elements of a psychological decision
theory, “ Studia Psychologica, 13(1), 1971.. p. 53-60
[21] Ljung, C. L. P. and A. Ynnerman. Local histograms
for design of transfer functions in direct volume rendering.
IEEE Transactions on Visualization and Computer
Graphics, 12(6). 2006.1570–1579.
[22] Lundstrom, C., A. Ynnerman, P. Ljung, A. Persson,
and H. Knutsson. “The alpha-histogram: Using spatial
coherence to enhance histograms and transfer function
design.” In
Proceedings Eurographics/IEEE-VGTC
Symposium on Visualization 2006, May 2006.
[23] Maciejewski, R. , I. Woo, W. Chen, and D. Ebert,
“Structuring feature space: A non-parametric method for
volumetric
transfer
function
generation.”
IEEE
Transactions on Visualization and Computer Graphics,
15(6). 2009. 1473 – 1480.
[24] MacQueen, J. B. "Some Methods for classification
and Analysis of Multivariate Observations". 1. Proceedings
of 5th Berkeley Symposium on Mathematical Statistics and
Probability. University of California Press. 1967. pp. 281–
297
[25] Meyer, J., J. Thomas, S. Diehl, B. Fisher, B. & D.
Keim. “From Visualization to Visually Enabled
Reasoning.” Scientific Visualization: Advanced Concepts,
Dagstuhl Follow-Ups 2009. p. 227–245. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.
[26] Miller, G.A. The magic number seven, plus or minus
two: Some limits on our capacity for processing
information, Psychological Review, 63, 1956. 81-97.
[27] Newell, A., & H.A. Simon, The simulation of human
thought. Santa Monica, Calif: Rand Corp. 1959.
[28] Pearson, K. "On Lines and Planes of Closest Fit to
Systems of Points in Space" Philosophical Magazine 2 (6):
1901. 559–572.
[29] Pike, W. A., J. Bruce, B. Baddeley, D Best, L.
Franklin, R. May, D.M. Rice, (n.d.). “The scalable
reasoning system: lightweight visualization for distributed
analytics.” Visual Analytics Science and Technology,
2008. VAST’08. IEEE Symposium. pp. 131–138.
[30] Reason, J. Human Error. New York: Cambridge Press.
1990.
[31] Ribarsky, W., B. Fisher, & W.M. Pottenger. “Science
of analytical reasoning.” Information Visualization, 8(4),
2009. 254–262.
[32] Rips, L.J. “Cognitive processes in propositional
reasoning.” Psychological Review, 90, 1983. 38-71.

[33] Russell, D. M., M.J. Stefik, P. Pirolli, & S.K. Card, (
The Cost Structure of Sensemaking. ACM SIG
INTERACT & CHI 1993. 1993. pp. 269–276.
[34] Simon, H. “Bounded Rationality and Organizational
Learning. “ Organization Science 2 (1). 1991. 125–134.
[35] Soar/Architecture.
http://cogarch.org/index.php/Soar/Architecture
[36] Stanovich, K. E. (1999). Who is rational? Studies of
individual differences in reasoning. Mahwah, NJ: Lawrence
Erlbaum.
[37] Thomas, J.J. and K. A. Cook (Ed.) Illuminating the
Path: The R&D Agenda for Visual Analytics National
Visualization and Analytics Center. 2005.
[38] Tololinski, S. and R. Reber. “Gaining insight in the
‘aha’ experience,” Current directions in Psychological
Science, 19(6). 2010. Pp. 402-405
[39] Tversky, A. “Elimination by aspects: A theory of
choice.” Psychological Review 79 (4). 1972. 281–299.
[40] Van der Henst, J. B., Y. Yang, & P. N. Johnson-Laird,
“Strategies in sentential reasoning.” Cognitive Science, 26,
2002.425–468.
[41] Van Wijk, J. and R. van Liere. “HyperSlice:
Visualization of scalar functions of many variables.
“Proceedings of the 4th Conference on Visualization. 1993.
119 – 125.
[42] Ware, C., E. Neufield & L. Bartram. “Visualizing
causal relations.” Information Visualization, Proceeding
Late Breaking Topics. 1999.
[43] Yamauchi, T., & A.B. Markman, Inference using
categories. Journal of Experimental Psychology, 26(3),
2000. 776–795.
[44] Yi, J.S., Y.A. Kang, J.T. Stasko, and J.A. Jacko.
“Toward a deeper understanding of the role of interaction
in information visualization.” IEEE Transactions on
Visualization and Computer Graphics 13(6). 2007. 12241231.

1504
1502

Front. Comput. Sci., 2017, 11(2): 192–207
DOI 10.1007/s11704-016-6028-y

Recent progress and trends in predictive visual analytics
Junhua LU1 , Wei CHEN

1

1

, Yuxin MA1 , Junming KE2 , Zongzhuang LI1 , Fan ZHANG3 ,
Ross MACIEJEWSKI4

State Key Lab of Computer Aided Design and Computer Graphics, Zhejiang University, Hangzhou 310058, China
2 College of Science, Zhejiang University of Technology, Hangzhou 310023, China
3 College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou 310023, China
4 School of Computing, Informatics and Decision Systems Engineering, Arizona State University,
Tempe AZ 85287-8809, USA

c Higher Education Press and Springer-Verlag Berlin Heidelberg 2016


Abstract A wide variety of predictive analytics techniques
have been developed in statistics, machine learning and data
mining; however, many of these algorithms take a black-box
approach in which data is input and future predictions are
output with no insight into what goes on during the process.
Unfortunately, such a closed system approach often leaves
little room for injecting domain expertise and can result in
frustration from analysts when results seem spurious or confusing. In order to allow for more human-centric approaches,
the visualization community has begun developing methods
to enable users to incorporate expert knowledge into the prediction process at all stages, including data cleaning, feature
selection, model building and model validation. This paper
surveys current progress and trends in predictive visual analytics, identifies the common framework in which predictive
visual analytics systems operate, and develops a summarization of the predictive analytics workflow.
Keywords predictive visual analytics, visualization, visual
analytics, data mining, predictive analysis

1

Introduction

Predictive analytics is the process of extracting information
from large data sets in order to determine trends and patterns
Received January 15, 2016; accepted May 17, 2016
E-mail: chenwei@cad.zju.edu.cn

that can be used to generate models and predict behaviors
of interest [1]. This type of analysis occurs across all areas
of science as data is collected and mined to assess the likelihood that a similar trend, pattern or behavior may occur
across similar samples. With the current instrumentation of
smart systems and cyber-enabled infrastructure, an unprecedented amount of large-scale, real-time data is being generated that could be used in predictive analytics. As such, this
Big Data wave of analysis has taken the analytic community
by storm, promising insight into how the world operates. Success stories take the form of the United Parcel Service (UPS)
developing new truck routing systems to save on fuel costs
based on data insights that their trucks were idling far too
long waiting for left turns [2].
However, for every Big Data success story, there are
Big Data failures. Take for example Google Flu Trends [3].
Launched in 2008, Google had developed a linear model that
predicted rates of influenza based on word search frequencies. The initial paper touted an accuracy of 97%; however,
in February 2013, Nature [4] reported that Google Flu Trends
was predicting more than double the cases of influenza-likeillness than that of the Centers for Disease Control [5], and a
recent article by Lazer et al. [6] noted that, in fact, Google Flu
Trends has been missing high “100 out of 108 weeks starting
with August 2011.” Thus, while Big Data and predictive analytics methods (e.g., classification, regression, support vector
machines, etc.) provide us with immense opportunities for
understanding our world, what is needed in predictive ana-

Junhua LU et al. Recent progress and trends in predictive visual analytics

lytics is algorithmic transparency to improve analytic intelligence. This requires data cleaning and linkage combined with
model construction and analysis in order to generate actionable insight.
One means of facilitating such requirements is through the
application of visual analytics. Visual analytics is an extension of information visualization that combines the power of
automated analysis with human-centric methods [7]. Recent
work in the visual analytics community has focused on predictive visual analytics which links predictive analytics methods with interactive visualizations. While statistical models
and machine learning algorithms often outperform humans,
domain knowledge, reasoning, and, ultimately, decision making rest in the hands of an end user. What predictive visual
analytics provides is methods that enable end-users and decision makers to interact with underlying statistical algorithms,
apply the algorithms at the appropriate temporal and spatial
scales, deal with noisiness inherent in real-world data, and
then apply their domain knowledge to reason about the outcomes of the analysis.
The purpose of this paper is to survey the current progress
and trends in predictive visual analytics. We summarize
works from the field of information visualization, visual analytics, machine learning and data mining and identify a common pipeline for predictive visual analytics. For each step in
the pipeline, we describe how visualization is combined with
automated predictive approaches and how users retrieve information and gain insight from data and predictive models.
Section 2 presents our conceptual framework of predictive visual analytics. Details of data preprocessing during predictive
visual analytics is discussed in Section 3. Section 4 explores
how visualization is applied in feature selection and generation, and Section 5 discusses methods of integrating visualization in model training, along with various visualization
techniques for model selection and model validation. Section
6 provides a summary of recent tools and case studies in the
predictive visual analytics community. Finally, the survey is
concluded and challenges and future goals for predictive visual analytics are highlighted in Section 7.

2 A conceptual framework for predictive visual analytics
Given the focus of visual analytics on linking automatic analyses with interactive visuals, shift towards predictive analytics was relatively straight forward. For example, Bertini et
al.’s [8] recent survey discussed the complementary role of
automatic data analysis and visualization in knowledge dis-

193

covery. This concept of taking the best of machine learning
algorithms and complementing them with insight from analysts is one of the fundamental pillars of visual analytics,
and recent work has focused on formalizing this concept of
human-in-the-loop analytics and applying it to various domains. Sacha et al. [9] further formalized this concept by
developing a knowledge generation model for visual analytics in which tasks for the machine (analysis and visualization) were separated from tasks for the human (exploration,
verification and knowledge generation). Similarly, work by
El-Assady et al. [10] proposed a formalization of the predictive analytics pipeline defining critical steps as feature selection, filtering, automatic prediction and adjustment. Krause et
al. [11] also proposed a formalization of the predictive analytics pipeline where the critical steps were cohort construction,
feature construction, cross validation, feature selection and
classification.
Along with this concept of including the human-in-theloop to improve knowledge generation and enable prediction, Gleicher [12] pointed out the need for predictions to
be comprehensible as well, noting that there are a variety of
stakeholders trying to understand the underlying processes
driving the predictive models and each of these stakeholders may require drastically diﬀerent visualizations to help improve their comprehensibility. The need for user involvement
is further echoed by Lazer et al. [6] who recommend several
items including increased data and algorithmic transparency
to improve analytic intelligence and data linkage between
smaller, cleaner datasets, and model construction. From this
standpoint, the application of visualization can directly enable these recommendations.
Common among these discussions of predictive analytics
and predictive visual analytics is the description of an underlying process for predictive analytics. Starting from the
predictive visual analytics pipeline described by El-Assady
et al. [10] and Krause et al. [11], we have identified three
major components that can be used to describe the predictive
visual analytics pipeline: data preprocessing, feature selection and generation, and model training, selection and validation. As shown in Fig. 1, for each step of the pipeline,
diﬀerent visualization and interaction methods (the adjustment loop) can be applied. For example, in data preprocessing, a major challenge is insuring data quality and enabling
data wrangling [13]. This stage would likely have very diﬀerent requirements compared with the feature selection stage
in which an analyst might wish to interactively define key
data items of interest. During each of these phases, diﬀerent
data mining algorithms can be used to help guide users, pro-

194

Front. Comput. Sci., 2017, 11(2): 192–207

vide feedback and enable adjustment. The choice of data, algorithms and visualization can help aid in comprehensibility
and can also serve as a basis for capturing the provenance features of the prediction generation. Furthermore, by enabling
an adjustment phase during all stages of the pipeline, predictive visual analytics can enable users’ domain knowledge and
experience to be included in all stages of predictive analysis.

becomes increasingly large, the cost of processing the data is
simultaneously increased. Unfortunately, there is no one size
fits all data cleaning method, and Rahm and Do [14] identified a variety of issues related to data attributes, records and
sources including missing values, misspellings, abbreviations
and others, and a systematic categorization of dirty data was
explored by Kim et al. [15]. Simply put, the purpose of data
cleaning is to identify and remove such errors when possible.
However, the data types and sources used in predictive analytics are extremely varied. We categorize data that is commonly
used in predictive visual analytics into six types:
• tabular data (e.g., Refs. [16–18]);
• time-series data (e.g., Refs. [19–22]);

Fig. 1

The predictive visual analytics pipeline

• spatial-temporal data (e.g., Refs. [23–26]);
• textual data (e.g., Refs. [27–29]);

3

Data pre-processing

One of the most critical steps in predictive analytics is insuring data completeness and correctness. Fortunately, one
of the major strengths of visualization is enabling users to
quickly identify erroneous data. Take, for example, the problem of predictive policing. In this case, the analyst would
like to predict the likelihood of a crime occurring within a
given neighborhood. The data used for prediction would be
historical crime records. By plotting these data on the map,
obvious errors, such as geo-coordinates at 0 degrees latitude
and 0 degrees longitude, or crimes being assigned to the default location of a local police precinct often become obvious. Thus, as the first step in the predictive visual analytics
pipeline, proper pre-processing of data can provide both a
useful overview into the problem set as well as provide the
opportunity to correct obvious data errors prior to modeling.
Furthermore, data integration and data fusion are also critical in the data pre-processing step. As more data is being
generated, modelers and analysts need the ability to combine
data that is collected from diﬀerent sources at diﬀerent resolutions. As such, we divide the data pre-processing phase
into three steps: data cleaning, data transformation and data
integration and fusion. In this section we will discuss visualization techniques that have proven useful for understanding
dirty data, re-organizing and re-formatting data, and integrating and fusing data from multiple sources.
3.1 Data cleaning
The first issue that needs to be handled in any predictive analytics task is data cleaning. However, as the volume of data

• image data (e.g., Refs. [30, 31]);
• networks (e.g., Refs. [32–35]).
Unfortunately, each data type may require one (or more)
visualization methods in order to eﬀectively explore errors.
Take for example time series data. Gschwandtner et al. [36]
presented a thorough categorization of time series data and
identified issues such as date mismatches (e.g., is 12-1-2012
December 1st, 2012 or January 12th, 2012), missing data
(e.g., the collection system went down), or wrong data (e.g.,
someone entered 112 instead of 12). While many of these issues can be seen with a simple line plot, similar issues in tabular data would require other methods. As we have alluded
to, many factors contribute to the dirty data problem, including input/user error, diﬀerent sources of data with diﬀerent
formats and conventions, and various other factors. Most of
these problems need human intervention (or at least human
rule building) to solve. What visualization provides is an intuitive way to help identify data quality issues while serving
as an interactive means of correcting problems. However, as
identified by Eaton et al. [37] and Templ et al. [38] missing
data can also impact the visualization, and care needs to be
taken in the visualization choice to insure that data errors are
perceivable to the user, and users can interact with visualization techniques to correct the problems.
3.2 Data transformation
While visualizations enable users to explore and correct data
quality issues, as data becomes larger, manually exploring
and correcting data quickly becomes ineﬃcient. As such, a
variety of research has been proposed to enable the automatic

Junhua LU et al. Recent progress and trends in predictive visual analytics

transformation of one data element into another through data
reformatting, extraction, conversion, etc. The idea is that a
rule can be identified that will automatically find and correct
dirty data. For example, spatial data, if a data field is expected
to be all country names, a program can automatically go and
compare the names to a gazetteer to correct for spelling and
flag entries that are not obvious spelling mismatches with entries in the gazetteer. While there is a variety of work on interactively transformation data, e.g., Refs. [39–41], the common focus is on text extraction and transformation. Unfortunately, this does not handle the problem of missing data, or
mis-entered data.
Recent work in the visualization community has begun developing solutions to better enable the data transformation
problem. Wrangler [13] is an interactive system which combines users’ operation with computers’ computing ability to
implement data transformations. The basis of Wrangler is a
scripting language for users to interact with the data coupled
with an inference engine to suggest transformation options.
Various visualizations are also applied to enable quick exploration of the applied transformations in order to assess the
overall quality. However, the creators of Wrangler note that
this only handles a subset of the data cleaning problems faced
by Wrangler, and a recent review paper by Kandel et al. [42]
provides further guidance on data wrangling issues.
3.3 Data integration and fusion
Along with data cleaning and transformation comes data integration and fusion. Given our current ability to collect data
from a variety of sources, it is critical that we use these
diﬀerent sources to perform predictive analytics. In some
cases, the datasets can be joined by their shared attributes
or by schema mapping [43], and a variety of commercial
tools are designed for ETL (extraction, transformation and
loading) [44, 45] to support data integration. However, many
datasets that analysts may wish to join do not necessarily have
matched attributes or schemas. Take, for example, linking internet search traﬃc data with the Center for Disease Control
influenza rates (as discussed in Google Flu Trends [3]). In
this case, the CDC aggregates flu rates to weekly or monthly
counts while Google is able to capture the number of keyword searches related to flu at the second level. Of course,
the keyword search data can be aggregated to a coarser granularity, but this illustrates some of the underlying issues in
data integration.
As such, data fusion has emerged as an oﬀshoot of data
integration where datasets come from broader sources with
unbalanced scales, distributions or densities. Zheng [46] clas-

195

sified this cross-domain data fusion problem into three categories: stage-based fusion in which the methods that will be
applied to each dataset diﬀers, thus only a loose data coupling/integration is required; feature level-based fusion in
which data features are extracted and merged across datasets;
and semantic meaning-based fusion in which a priori knowledge about features is used in the fusion stage. For each of
these categories, a variety of techniques exist to help merge
data; however, few of these techniques involve visualization.
This is not surprising as data fusion has been largely ignored
in the visualization community. We see this as a great opportunity for the field of predictive visual analytics. Existing
visualization techniques such as uncertainty visualization and
semantic visualization can be leveraged to help analysts explore the quality of the data fusion process, and fields ranging
from business intelligence to databases could benefit.

4

Feature selection and generation

Once data is pre-processed, the real predictive analytics work
begins. Critical to predictive analytics is determining what
features (data attributes) should be used to create a predictive model. Ideally, features must capture the most expressive
aspects of data and show as little redundancy as possible in
order to create the most salient models. Without appropriate
selection, redundant and non-descriptive features may lead to
unexpected model training results and poor predictions. However, as data sets get larger, feature selection methods may
perform poorly prior to removing unwanted (i.e., redundant
features) [47]. While reducing the number of features drastically reduces the running time of a learning algorithm, relevant features are often unknown a priori. Dash and Liu [47]
specified feature selection as the attempt to select the minimally sized subset of features which must a) not significantly
decrease the classification accuracy, and b) must match (as
close as possible) the original class distribution given all features. Dash and Liu also note that ideally feature selection
will search all subsets of features; however, this exhaustive
procedure is often too costly and thus practically prohibitive
even for relatively small feature set sizes.
More recent feature selection algorithms have focused on
combining data attributes in order to build high-level features of low-level features, e.g., Refs. [48–50]. However, automated algorithms may hide subtle diﬀerences between features and are not always eﬀective. Furthermore, these highlevel features are often not easily interpretable by users, and,
in some cases, when the amount of training data is limited,

196

Front. Comput. Sci., 2017, 11(2): 192–207

the model might be over-fit after training. Given these problems inherent in feature selection, the visual analytics community has focused on developing methods to incorporate human knowledge into the feature selection and generation process as a means of reducing data features to a manageable
subset that is salient to the problem domain.
As previously stated, the critical aspect of feature selection
is improving classification accuracy and matching the original class distribution. One classical method for numerical
feature is selecting an individual feature or every two features, displaying them and then applying clustering, sorting
or ranking techniques to these features, and a variety of visual analytics solutions have been developed to enable such
an approach. For example, Guo [51] developed an interactive geographical visualization environment for feature selection in high-dimensional cancer data. The Rank-by-Feature
framework leveraging low dimensional projection and user
specified ranking criteria was explored by Seo and Shneiderman [52], and Piringer et al. [53] described extensions to the
Rank-by-Feature Framework for guiding the user through a
high-dimensional dataset by ranking dimensions according to
statistical summaries, thus enabling the user to focus on both
local and global features. They employed specific metrics to
measure the relationships between features (i.e., maximum
conditional entropy (MCE) and user-specific ranking scores),
and depicted them with appropriate visualization techniques.
More recent work includes SmartStripes [54] which uses a
pixel based visualization approach combined with statistical
measures of feature redundancy (e.g., Kohavi and John [55])
to enable users to investigate dependencies and independencies between diﬀerent feature and entity subsets, and the 3D
Regression Heat Map [56] which encodes two to three dimensions of independent features in a geographical overlay
for analysis. An example of a fully developed visual analytics
system for feature selection is the INFUSE (INteractive FeatUre SElection) system [11]. INFUSE visualizes the ranking
information of diﬀerent feature selection algorithms and uses
a classification results view to help users score the quality of
the prediction (classification) through interactive glyphs, as
shown in Fig. 2. Both methods enumerate the key information of features, and allow for comparative and visual analysis. In addition, the latter supports explorations of diﬀerent
levels of details with more user interactions.
While such visual analytics methods and tools are good
for improving the comprehensibility of feature selection and
model creation, the classical methods of clustering, sorting
and ranking are most applicable to numerical and ordinal
data. However, large amounts of data are now being captured

as unstructured text. In order to enable feature selection and
exploration for text based data, work has been done in converting textual features to numerical scores (such as sentiment, e.g., Lu et al. [57]) as well as methods for semantically
grouping and classifying text into categories (e.g., Brooks et
al. [58]). Work by both Lu et al. [57] and Brooks et al. [58]
explores methods for visual summaries of text as well as introduces interactive methods of relabeling misclassified data,
and Brooks et al. [58] demonstrated that such visual summaries could improve feature ideation.

Fig. 2 Glyph representation of a feature in the INFUSE system [11] (The
glyph is divided into four sections to show that the feature is ranked by four
feature selection algorithms. Each section is further divided into ten fold
slices representing each of the cross-validation folds. Each fold slice features
an inward-filling bar that represents the rank of this feature in that fold.)

A variety of methods have been established in the visual
analytics community to enable feature selection and exploration under the criteria of improving the prediction quality.
However, the major focus has been on removing redundant
features and enabling users to select what they feel are the
most salient features of the data. Little work has been done
with regards to Dash and Liu’s [47] second criteria of insuring that the chosen subset of features match the original class
distribution. Finally, the predictive visual analytics community also needs more formal validation studies in this area
to demonstrate that user guided feature selection is at least
equivalent to automatic methods, as well as studies to demonstrate that the comprehensibility of the features used in modeling is improved through a visual analytics approach.

5

Model training, selection and validation

In feature selection, a critical task is identifying the most
salient features with the least amount of redundancy that
should be used for model training. However, once features
are chosen, then a model must be selected, trained and validated. First users must choose the most appropriate prediction or classification algorithm from their problem, train the
model, and then validate the model results after prediction.
These steps can help identify which model has better performance, or help analysts explore biases between diﬀerent

Junhua LU et al. Recent progress and trends in predictive visual analytics

prediction methods.
Recently, the visual analytics community has begun developing interactive tools and techniques for model selection
and comparison. For example, Bögl et al. [59] defined a visual analytics process for iterative model selection (Fig. 3).
This work focused on the development of time-series modeling and forecasting and explored how to best enable analysts
to adjust models based on the residual plots. In Ref. [60], a
visual analytics process for prediction was explored in which
social media data was used to predict future earnings for upcoming movies. Lu et al.’s system allowed users to iteratively
and visually change the features, training sets and model
types to improve the resultant prediction. While subjects who
used the system were positive about the approach, the overall time for generating predictions was much greater than a
fully automated method. Key aspects of the work by Lu et al.
include the use of provenance in the form of a history table
to allow the user to track the models and features used for
exploration.

Fig. 3

A visual analytic process for model selection presented in Ref. [59]

Such model selection methods are a critical part of the
model validation phase as refinement and comparison between models can give insight into diﬀerent data features
that may have initially been hidden to the user. Two key
strategies for model validation are: a) the comparison of
models (parameters), and b) the identification of problematic
features and samples. Based on these two strategies, diﬀerent visualization methods have been developed. For example, Piringer et al.’s [61] HyperMoVal applied a set of linked
views of diﬀerent data projections to support outlier analysis
and model comparison. Mühlbacher et al. [62] adopted different heatmap-based visualization to encode the diﬀerence
between models from diﬀerent angles for comparison, and
Gotz et al. [63] designed visualization techniques to identify
samples and features that are perhaps leading to poor prediction performance in the model. These methods employ diﬀer-

197

ent strategies, and design targeted visualization techniques to
enable visual comparison of models and model selection.
Model selection and model validation are tightly connected
to the former steps of our pipeline, specifically feature selection and model training, and looping back to those previous
steps is also critical in some cases. In the case of predictive
visual analytics, the goal is to insert the user into the model
training, selection and validation loop in order to help iteratively refine the feature selection, thereby improving the overall model performance. However, this is a very challenging
task as model training methods are primarily classified into
black box (methods which take a set of data and parameters
as input and then run to completion providing only output for
validation) or white box (fully transparent methods in which
a user can adjust parameters at each step of the process) approaches.
5.1 White box methods
Given the desire to integrate humans into the model training,
selection and validation phase, preferred models for the predictive visual analytics community would be white box methods. As such, some of the most frequently studied methods in
the visual analytics community are the decision tree [64] and
support vector machine (SVM) [65]. Both of these methods
can be interactively manipulated through visualization techniques and are the subject of a variety of visual analytics papers.
For example, decision tree modeling has an explicit treelike structure which lends itself well to a variety of visualization techniques, e.g., [66, 67], but while a variety of visualization methods exist for visualizing tree structures, there
are critical issues for enabling interaction. A core problem
in constructing decision trees is how to split tree nodes. Several interactive methods have been developed to support node
splitting. For example, a bivariate split method using 2D
polygons on the points in a 2D plane was supported by Ware
et al. [68]. Ankerst et al. [69] introduced a method of splitting decision nodes into multiple intervals through user interaction, and Van den Elzen and Van Wijk [70] described the
BaobabView utilizing streamgraphs and confusion matrices
to display the attribute characteristics, correlations, and the
results of the decision tree based on varying the split values.
In the BaobabView, users are able to directly modify the tree
while viewing the structure of resulting trees of training data.
The BaobabView can be seen as an improvement of previous
ones in that it supports illustrative visualization of features
and intuitive interactions.

198

Front. Comput. Sci., 2017, 11(2): 192–207

Along with visual analytics methods for exploring decision trees, a variety of systems have been built for other
modeling and classification techniques. For example, in
Ref. [71], bar charts and pie charts are used to help build simple Bayesian classifier. Caragea et al. [72] used a projectionbased tour method to help users navigate through highdimensional spaces and better understand SVMs. Ma et
al. [73] proposed an open-box approach to help users understand the primary structure of an SVM and the classified results. Their EasySVM system (Fig. 4) explores visualization
for both model building and rule extraction. Visualizing the
process of building the decision trees is quite straightforward
and self-descriptive. However, intuitively understanding the
building SVM classifiers is diﬃcult for non-expert end users,
and the assistance of other techniques (i.e., projection methods) is required to reveal the building process.
5.2 Black box methods
While white box methods are perhaps some of the easiest methods to integrate into a predictive visual analytics

pipeline, many of the models that users can select for prediction (e.g., naïve Bayes [74], and random forests [75]) inherently closed to inspection (i.e., black box). Users need visual
aids to view the input and output of an algorithm or model,
and while black box methods are easily amenable to exploring inputs and outputs, they are not amenable to input during the modeling process. Furthermore, a major concern is
comprehensibility [12] as the black box methods do not provide mechanisms for exploring the underlying process of the
model.
Current work in visual analytics has focused on techniques
for engaging users with black box methods. Bertini et al. [8]
suggested that a “feedback loop” is critical for integrating
data mining and visualization, and Mülbacher et al. [76]
noted that in order to enable a user to truly interact with many
machine learning algorithms, the black-box of these algorithms would need to be opened. Mülbacher et al.’s work formalizes a set of strategies to engage a human-in-the-loop in
machine learning algorithms including: data subsetting (performing computations through additional passes to enable

Fig. 4 Building a support vector machine model with EasySVM [73]. (a) The global SVM model building process; (b) the local model
processing (if a non-linear decision boundary is found, the user can enter the local model building process)

Junhua LU et al. Recent progress and trends in predictive visual analytics

users to see intermediate results), complexity selection (performing computations for less complex parameter configurations before computing the full complex case), divide and
combine (subdivide the workload to show partial results),
and dependent subdivision (dividing the problem into equal
sequential dependent steps). Recent work has attempted to
further develop visual analytics methods to optimize the
output of classification tasks and build better models, e.g.,
Refs. [77, 78]. For example, EnsembleMatrix [78] presents
a graphic view of confusion matrices to help users understand merits of diﬀerent classifiers and then build combination models.
While the visual analytics community has recently explored a variety of solutions for model selection, training and
validation, the number of diﬀerent data types and domain areas leaves much room for future research. Furthermore, more
complicated processes may require cascading models where
input from one model can be used to inform another. For example, in work from Lu et al. [57], the authors utilize both
a time series model and a linear regression model as part of
their prediction process; however, the burden of coupling the
two models was oﬄoaded to the analyst. As such, tools that
can help reveal insights through model comparison, model
coupling, and model adjustment could prove valuable for improving and comprehending the predictive analytics process.
Previous methods also tend to focus less on uncertainty, or
provide only a variety of simple statistics to enable model
validation. More complex strategies and mixed-initiative solutions should be considered for future work.

6

Applications

The visual analytics community has been making large
strides in developing methods to improve the predictive analytics process. However, there is no one-size-fits-all method
for predictive visual analytics method. Depending on the data
domain, use case and analyst requirements, predictive visual analytics tools can have vastly diﬀerent elements. In this
section, we will describe a variety of recent predictive visual analytics tools. While all of these tools have the same
underlying pipeline of data cleaning, feature selection and
model validation, all of them engage with the user in diﬀerent ways depending on the domain. We classify these applications based on the primary data types used in prediction
and described in Section 3. However, network data has been
severely neglected in the realm of predictive visual analytics. Large amounts of work have focused on interacting with

199

networks (e.g., Refs. [35, 79]), yet, to our knowledge, no current application exists to explore predictions within network
data (e.g., predicting emerging links, predicting new communities, predicting users leaving a network). As such, we focus
on tabular, temporal, spatiotemporal, text and image data.
6.1 Tabular data
In tabular data, each row or column can be regarded as a feature vector. Data is typically in the form of quantitative data
(or categories), and classification and regression models are
most frequently applied to deal with such kind of prediction
problems. For example, the INFUSE [11] system supports
users in selecting the most informative features from a large
potential feature set as part of building an interactive classifier. INFUSE was applied for predicting patient outcomes
from medical records where users can help identify features
that may lead to abnormal medical conditions, as shown in
Fig. 5. Similarly, Stolper et al. [80] designed a system for
the visual exploration of classification models for Forensic
Psychiatry, again identifying features to classify and predict
abnormal behaviors to provide risk assessment.

Fig. 5 Use INFUSE [11] to interactively select features and build models

6.2 Time-series data
Temporal analysis is another area that is rife with predictive analytics models. Financial analysts constantly track and
project trends in the stock market, manufacturers want to
track sales per day, and power companies want to anticipate ebbs and flows in power consumption. Key techniques
for time-series data include regression models and sequential mining methods. Examples include PARAMO [81] which
provides a parallel predictive modeling platform developed
for electronic health records data. It utilizes linked views for
building models (tasks), organizing them through visualization and executing them in parallel using MapReduce. It is a
great contribution in the field of building analytic pipelines
that are specialized for health data researchers. Mühlbacher

200

Front. Comput. Sci., 2017, 11(2): 192–207

et al. [62] used linked views (focusing on scatterplots), but
provided strong interaction methods to combine subset selection, model validation and model building. Supported types
of model include generalized linear models, support vector regression based on LIBSVM [82] and piece-wise linear
regression trees. Besides showing correlations between features, the system also allows user dynamically adding or removing features. While refining the model, errors and bias of
models will be displayed at the same time for users to know
the gained accuracy for each iteration, as shown in Fig. 6.
It also presents an example of how an analyst uses this system to model natural gas consumption. TiMoVA [59] focuses
on parameter selection and analysis for ARIMA time series
modeling. TiMoVA enables users to validate the results of a
model and decide to accept or retrain the model. This system was extended to TiMoVA-predict [83] which provides a
prediction control toolbox that displays prediction of future
values and one-step-ahead values. In all of these systems, the
goal is to guide users to choose better parameters or models
to improve prediction accuracy.

developed across various domains. For example, Andrienko
et al. [84] explored traﬃc predictions and designed a visual
analytics system that centers on the simulated movement of
private cars in a space-time cube. They used dependencies
between traﬃc intensity and mean speed to predict unusual
traﬃc events, and periodically checked real data against simulated data to see if the simulation model needs update. Work
by Maciejewski et al. [85] focused on disease modeling, using kernel density estimation (spatial) and Seasonal Trend
Decomposition based on Loess [86] (temporal) to identify
future disease hotspots of interest. Aurisano et al. explored
methods of predicting terrorism using diﬀerent aggregation
and visualization techniques. Malik et al. [23] presented the
Dynamic Covariance Kernel Density Estimation Technique
to predict future criminal incidence complaints, and Bryan et
al. [87] designed a workflow allowing non-statistical modeling experts to visually predict the spread of epidemic. The
goal of such tools is to enable users to explore potential futures and make plans for resource allocation to help reduce
the impacts of local disasters.
6.4 Textual data

Fig. 6 Refinement of regression models on time-series data through analyzing prediction bias [62]

6.3 Spatio-temporal data
Extending from purely temporal prediction is spatio-temporal
prediction where the focus is on anticipating when and where
events may occur. For example, weather forecasters need to
know the possible trajectory of a hurricane, criminal analysts
want to forecast violence, and health practitioners need to explore where diseases may spread to in the future. As such, a
variety of predictive visual analytics applications have been

Another interesting data type is text. In fields such as law,
developing methods to guide users to related documents or
predict trends is becoming increasingly important, and journals and teachers need support to classify documents that
may be plagiarized. While this category of data tends less
towards prediction and more towards classification, we consider classification methods to be a subset of prediction. Examples of applications focusing on text include FeatureInsight [58]. FeatureInsight is a classifier for textual data that
enables users to examine documents and provides side-byside comparisons using highlight techniques, and visual summaries of errors help users eﬃciently group documents and
identify ones that are irrelevant to their current search. Similar work by Chuang and Socher [88] focuses on predicting
sentence-level sentiment in a high accuracy. Other researches
explore not only document classification, but how such classifications change over time. For example, Yeon et al. [89]
presented a system which enables users to visually composite
patterns extracted from diﬀerent data source with contextual
information to predict when and where similar events will occur in the future. OpinionFlow [90] is a system that explores
opinion diﬀusion in streaming media users. Their focus is on
enabling analysts to predict what will happen if opinion leaders enhance their positive promotion of a product. In OpinionFlow, users can adjust the opinion of one leader with a sliding
bar and modifications will be shown both in the stream graph

Junhua LU et al. Recent progress and trends in predictive visual analytics

and pop-up windows as shown in Fig. 7.

201

6.6 Quantitative analysis of predictive visual analytics
We list the comparisons between baseline predictive methods
(before ‘vs’) and predictive visual analytics (after ‘vs’), to
demonstrate the eﬀectiveness and eﬃciency of predictive visual analytics. Because diﬀerent measures of the performance
are adopted in diﬀerent tasks of diﬀerent cases, we make an
average of normalized scores of tasks:
• text classification/feature selection in Ref. [58]: 1.316
vs 1.429;

Fig. 7

• ad-hoc classifier for video analysis in Ref. [92]: need
550 more labels vs need 100 more labels;

OpinionFlow [90] for predicting opinion diﬀusion

• visual classifier training for text document retrieval in
Ref. [93]: 0.80 vs 0.95;

6.5 Image data
Similar to text data, a variety of work has been done on building interactive classifiers for images. Here, users focus on
predicting if a person or scene is the same based on subtle feature changes (e.g., snow on a mountain, facial hair, etc.). Examples include iVisClassifier [91] which enables users to interactively perform classification tasks on high-dimensional
data like image data. This work employs linear discriminant
analysis (LDA) to reduce dimension and uses parallel coordinates and a scatterplot for dimension reduction and exploration. Höferlin et al. [92] proposed an inter-active learning
strategy for video visual analytics. Their system allows users
to incorporate their expertise in classification as well as aids
them to better understand and adjust the model by visualization. Besides, users could build trust in the classifier through
the visual feedback of the training process. The coordinated
view in Fig. 8 shows how they implement this strategy.

• prediction the sales of movies in Ref. [60]: 0.428 vs
0.286.
Unfortunately, many papers do not provide the numerical
comparisons between prediction methods with/without visualization techniques. With the aforementioned cases, we can
still validate the usefulness of predictive visual analytics.

7

Conclusion and challenges

While many methods have been developed for generating
predictive models, there is a pressing need for exploring, manipulating, and understanding the models generated, and visualization is one of the most important, and commonly used
methods of analyzing and interpreting data. Throughout this
paper, we have identified a variety of frameworks, techniques
and application areas in predictive visual analytics.

Fig. 8 Typical workspace of the visual analytics system developed for inter-active learning after learning with a couple of training examples.
Multiple coordinated views are provided for this strategy

202

Front. Comput. Sci., 2017, 11(2): 192–207

As most techniques mentioned in this survey are demonstrated case by case, and thus the generality of the methods
of predictive visual analytics needs to be discussed. Inspired
by Munzner’s four nested levels of visualization design [94],
we present a specific model to characterize the predictive visual analytics as shown in Fig. 9. The domain level denotes
the predictive domains widely applied in the research area
and daily life. Visualization can assist in incorporating analysts’ knowledge into the predictive analysis process. The
data/task abstraction comprises data pre-processing, feature
selection, and model validation which are elaborated in Section 3, Section 4, and Section 5, respectively. Multiple visual
encodings are utilized to visualize the data or information
produced at diﬀerent steps of predictive analysis: statistics diagram based, relationship based, data point based, time-series
based, etc. Besides, a rich set of interactions are supported to
facilitate exploration by utilizing the visual encodings. Last
but not least, algorithms are critical because applicable predictive analysis requires high eﬃciency and low latency to
enable real-time prediction and interactions.

Fig. 9 A four nested level of predictive visual analytics

More and more, data analysts are developing new techniques for classification and prediction to anticipate future
events. However, it is not yet clear what the current role of
visualization is within the predictive analytics process. The
visual analytics community has an emerging body of work
suggesting that visualization and interaction are highly beneficial in arriving at optimal analysis results. However, evidence from the business market sector [95] indicates that automated algorithms out-perform “gut instinct” when playing
the stock market. While even more recent studies [96] show
that a combination of managerial judgment and model output is likely to outperform either alone. The question then
becomes, “how much human should be in the loop?”
While there is little controversy over having analysts check
for data quality, during the data modeling stage or having an

analyst verify the validity of a model, one can imagine two
extremes of the data modeling stage.
One extreme is that in which a user has no control of any
parameters and simply sends their data to a black box and receives a response. Another extreme is that in which the user
must choose all possible parameters and features that are required by an algorithm. Neither extreme seems exceptionally
appealing. Anecdotal evidence from various visualization applications demonstrates that users will not respond well to a
large panel of knobs and sliders in which they can modify the
output of the visualization ad infinitum, and, on the black box
side, throwing data into a container and retrieving an output
can be incomprehensible to an end user that must make a decision and needs to understand something of the process of
how predictions, correlations and classifications were made.
As such, it seems that some combination of computer supported prediction is critical, and there are a variety of ways in
which adding a human into the predictive analytics pipeline
can have benefits.
1) Incorporating user feedback into model performance
based on multiple metrics (for example crossvalidation, pruning and model comparison techniques)
could serve as measures to indicate to a user when overfitting is occurring. By providing hidden information
to a user, visual analytics may have the opportunity to
nudge their analytic process to become less biased.
2) By enabling users to explore success and failure cases
may help the user gain further insight into the data. Unfortunately data is large and predictive models may often result in complex processes (i.e., non-linear models
are often diﬃcult for users to interpret), but visual analytics could play an important role in enabling comprehension [12].
3) Recent work seems to indicate that a human in the loop
can improve model accuracy [57], but clear definitions
of best practices and ways in which visual analytics
might hinder the predictive analytics process are still
unclear.
However, one major challenge of adding a human in the
loop is overfitting the data. While domain experts can provide insight into the data, they can also bring with them their
own biases. These “gut reactions” to results from predictive
algorithms can lead to re-weighting parameters, adding new
features or generally ignoring results. If the analyst’s prediction is right even for a small fraction of future events, then this
can become a confirmation bias. In developing tools, tech-

Junhua LU et al. Recent progress and trends in predictive visual analytics

Refs. [97, 98], oﬀers basic ideas and examples of adaptive visual analytics, but this area is still relatively
nascent.

niques and processes for predictive analytics, we need to be
cognizant of these issues and insure that methods being developed are not further contributing to overfitting or confirmation bias, which is a challenge in its own right.

• The predictive visual analytics community also needs
more formal validation studies in this area to demonstrate that user guided feature selection is at least equivalent to automatic methods, as well as studies to demonstrate that the comprehensibility of the features used
in modeling is improved through a visual analytics approach.

Overall, there is an emerging body of work exploring the
boundaries of predictive analytics. We have identified potential strengths and weaknesses of the predictive analytics
pipeline, but there are also clear areas in which research is
lacking.
• As mentioned in Section 3, visualization techniques developed for data fusion are scarce. Though various techniques have been developed to view high-dimensional
datasets and integrate data from diﬀerent sources, there
is no eﬃcient method to integrate cross-domain data
with an inference of potential connections among different sources.
• Little work in the visual analytics community has focused on model coupling, and tools and techniques
that can help reveal insights through model comparison, model coupling, and model adjustment could prove
valuable for improving the predictive analytics process.
• Current visual analytics methods tend to focus less on
model uncertainty, and more work on incorporating uncertainty will be needed, especially as the need to link
models together becomes more imperative.
• In some domains, predictions need to be updated in
real-time in order to make critical decisions (e.g., stockbrokers), and data input, feature selection and model
building may need larger amounts of computation time.
How can (or should) visualizations best report partial
results and still be helpful in critical situations?
• Many visualization techniques and systems have been
developed for visualizing networks and network dynamics; however, little work in the visual analytics
community has explored predictive analysis in the dynamic network.
• Designing an adaptive predictive system is not easy.
An adaptive system means the system is able to learn
to oﬀer information to users according to their background (like their familiarity with certain field) and help
them better understand and use the system. This may require machine learning techniques itself to capture the
characteristics of users with diﬀerent backgrounds. Understanding the visualization technique is also important since novel techniques developed for visual analytics are necessarily complicated. Recent work, e.g.,

203

Overall, the area of predictive visual analytics has demonstrated promise through a variety of applications ranging
from crime [23] to traﬃc [84] to health [80]. While there are
challenges to integrating domain analysts directly into the
modeling process, the promise of improved comprehensibility and the potential for more robust models remains a critical
selling point. Furthermore, this area is still in need of design
guidelines and best practices for visualization design, and a
variety of rich problem areas still remain open challenges. As
such, the area of predictive visual analytics should prove a
fruitful research area for both interdisciplinary sciences that
support the predictive analytic process and fundamental science that explores new visualization and interaction methods
while studying the impact of these methods on the analytics
process.
Acknowledgements This work was supported by National Basic Research Program of China (973 Program) (2015CB352503), Major Program of the National Natural Science Foundation of China (61232012),
the National Natural Science Foundation of China (Grant Nos. 61303141,
61422211, u1536118, u1536119), Zhejiang Provincial Natural Science
Foundation of China (LR13F020001), the Fundamental Research Funds for
the Central Universities, the Innovation Joint Research Center for CyberPhysical-Society System, and the United State’s National Science Foundation (1350573).

References
1.

Larose D T, larose C D. Data Mining and Predictive Analytics, 2nd ed.
Hoboken: John Wiley & Sons, 2015

2.

Schlangenstein M. UPS crunches data to make more routes more
eﬃcient, save gas. http://www.bloomberg.com/news/articles/2013-1030/ups-uses-big-data-to-make-routes-more-eﬃcient-save-gas, 2013

3.

Ginsberg J, Mohebbi M H, Patel R S, Brammer L, Smolinski M S, Brilliant L. Detecting influenza epidemics using search engine query data.
Nature, 2009, 457(7232): 1012–1014

4.

Butler D. When Google got flu wrong. Nature, 2013, 494(7436): 155–
156

5.

Culotta A. Towards detecting influenza epidemics by analyzing Twitter messages. In: Proceedings of the 1st Workshop on Social Media
Analytics. 2010, 115–122

204

Front. Comput. Sci., 2017, 11(2): 192–207
22.

7. Keim D A, Kohlhammer J, Ellis G, Mansmann F. Mastering the Information Age — Solving Problems with Visual Analytics. Goslar: Florian Mansmann, 2010

Hao M C, Marwah M, Janetzko H, Dayal U, Keim D A, Patnaik D, Ramakrishnan N, Sharma R K. Visual exploration of frequent patterns in
multivariate time series. Information Visualization, 2012, 11(1): 71–83

23.

8. Bertini E, Lalanne D. Surveying the complementary role of automatic
data analysis and visualization in knowledge discovery. In: Proceedings of the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery: Integrating Automated Analysis with Interactive Exploration. 2009, 12–20

Malik A, Maciejewski R, Towers S, McCullough S, Ebert D S. Proactive spatiotemporal resource allocation and predictive visual analytics
for community policing and law enforcement. IEEE Transactions on
Visualization and Computer Graphics, 2014, 20(12): 1863–1872

24.

9. Sacha D, Stoﬀel A, Stoﬀel F, Kwon B C, Ellis G, Keim D. Knowledge
generation model for visual analytics. IEEE Transactions on Visualization and Computer Graphics, 2014, 20(12): 1604–1613

Hollt T, Magdy A, Zhan P, Chen G, Gopalakrishnan G, Hoteit I, Hansen
C D, Hadwiger M. Ovis: a framework for visual analysis of ocean
forecast ensembles. IEEE Transactions on Visualization and Computer
Graphics, 2014, 20(8): 1114–1126

25.

Doraiswamy H, Ferreira N, Damoulas T, Freire J, Silva C T. Using
topological analysis to support event-guided exploration in urban data.
IEEE Transactions on Visualization and Computer Graphics, 2014,
20(12): 2634–2643

26.

Chen W, Guo F, Wang F Y. A survey of traﬃc data visualization.
IEEE Transactions on Intelligent Transportation Systems, 2015, 16(6):
2970–2984

27.

Koch S, John M, Worner M, Muller A, Ertl T. Varifocalreader-in-depth
visual analysis of large text documents. IEEE Transactions on Visualization and Computer Graphics, 2014, 20(12): 1723–1732

28.

Zhao J, Cao N, Wen Z, Song Y, Lin Y R, Collins C M. # FluxFlow:
visual analysis of anomalous information spreading on social media.
IEEE Transactions on Visualization and Computer Graphics, 2014,
20(12): 1773–1782

6. Lazer D, Kennedy R, King G, Vespignani A. The parable of Google
flu: traps in big data analysis. Science, 2014, 343(6176): 1203–1205

10.

El-Assady M, Jentner W, Stein M, Fischer F, Schreck T, Keim D. Predictive visual analytics — approaches for movie ratings and discussion
of open research challenges. In: Proceedings of IEEE VIS Workshop:
Visualization for Predictive Analytics. 2014

11.

Krause J, Perer A, Bertini E. INFUSE: interactive feature selection for
predictive modeling of high dimensional data. IEEE Transactions on
Visualization and Computer Graphics, 2014, 20(12): 1614–1623

12.

Gleicher M. Position paper: towards comprehensible predictive modeling. In: Proceedings of IEEE VIS Workshop: Visualization for Predictive Analytics. 2014

13.

Kandel S, Paepcke A, Hellerstein J, Heer J. Wrangler: interactive visual specification of data transformation scripts. In: Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems. 2011,
3363–3372

29.

14.

Rahm E, Do H H. Data cleaning: problems and current approaches.
IEEE Data Eng. Bull., 2000, 23(4): 3–13

Sun G, Wu Y, Liu S, Peng T Q, Zhu J J, Liang R. EvoRiver: visual
analysis of topic coopetition on social media. IEEE Transactions on
Visualization and Computer Graphics, 2014, 20(12): 1753–1762

30.

15.

Kim W, Choi B J, Hong E K, Kim S K, Lee D. A taxonomy of dirty
data. Data Mining and Knowledge Discovery, 2003, 7(1): 81–99

Klemm P, Oeltze-Jafra S, Lawonn K, Hegenscheid K, Volzke H, Preim
B. Interactive visual analysis of image-centric cohort study data. IEEE
Transactions on Visualization and Computer Graphics, 2014, 20(12):
1673–1682

16.

Ganuza M L, Ferracutti G, Gargiulo M F, Castro S M, Bjerg E, Gröller
E, Matković K. The spinel explorer — interactive visual analysis of
spinel group minerals. IEEE Transactions on Visualization and Computer Graphics, 2014, 20(12): 1913–1922

31.

Arietta S M, Efros A, Ramamoorthi R, Agrawala M. City forensics:
using visual elements to predict non-visual city attributes. IEEE Transactions on Visualization and Computer Graphics, 2014, 20(12): 2624–
2633

17.

Brown E T, Ottley A, Zhao H, Lin Q, Souvenir R, Endert A, Chang
R. Finding waldo: learning about users from their interactions. IEEE
Transactions on Visualization and Computer Graphics, 2014, 20(12):
1663–1672

32.

Ma Y X, Xu J Y, Peng D C, Zhang T, Jin C Z, Qu H M, Chen W, Peng Q
S. A visual analysis approach for community detection of multi-context
mobile social networks. Journal of Computer Science and Technology,
2013, 28(5): 797–809

18.

Born S, Sundermann S H, Russ C, Hopf R, Ruiz C E, Falk V, Gessat M.
Stent maps — comparative visualization for the prediction of adverse
events of transcatheter aortic valve implantations. IEEE Transactions
on Visualization and Computer Graphics, 2014, 20(12): 2704–2713

33.

Van den Elzen S, Holten D, Blaas J, Van Wijk J J. Dynamic network visualization with extended massive sequence views. IEEE Transactions
on Visualization and Computer Graphics, 2014, 20(8): 1087–1099

34.

19.

Xie C, Chen W, Huang X X, Hu Y Q, Barlowe S, Yang J. VAET: a
visual analytics approach for e-transactions time-series. IEEE Transactions on Visualization and Computer Graphics, 2014, 20(12): 1743–
1752

Van den Elzen S, Van Wijk J J. Multivariate network exploration and
presentation: From detail to overview via selections and aggregations.
IEEE Transactions on Visualization and Computer Graphics, 2014,
20(12): 2310–2319

35.

20.

Madhavan K, Elmqvist N, Vorvoreanu M, Chen X, Wong Y, Xian H,
Dong Z, Johri A. Dia2: Web-based cyberinfrastructure for visual analysis of funding portfolios. IEEE Transactions on Visualization and Computer Graphics, 2014, 20(12): 1823–1832

Van den Elzen S, Holten D, Blaas J, Van Wijk J J. Reducing snapshots to points: a visual analytics approach to dynamic network exploration. IEEE Transactions on Visualization and Computer Graphics,
2016, 22(1): 1–10

36.

Gschwandtner T, Gärtner J, Aigner W, Miksch S. A taxonomy of dirty
time-oriented data. In: Proceedings of International Conference on
Availability, Reliability, and Security. 2012, 58–72

37.

Eaton C, Plaisant C, Drizd T. Visualizing missing data: graph interpretation user study. In: Proceedings of IFIP Conference on Human-

21.

Hao M C, Janetzko H, Mittelstädt S, Hill W, Dayal U, Keim D A, Marwah M, Sharma R K. A visual analytics approach for peak-preserving
prediction of large seasonal time series. Computer Graphics Forum,
2011, 30(3): 691–700

Junhua LU et al. Recent progress and trends in predictive visual analytics
Intelligence, 1997, 97(1): 273–324

Computer Interaction. 2005, 861–872
56.

Lin J, Wong J, Nichols J, Cypher A, Lau T A. End-user programming
of mashups with vegemite. In: Proceedings of the 14th International
Conference on Intelligent User Interfaces. 2009, 97–106

Klemm P, Lawonn K, Glaßer S, Niemann U, Hegenscheid K, Völzke
H, Preim B. 3D regression heat map analysis of population study data.
IEEE Transactions on Visualization and Computer Graphics, 2016,
22(1): 81–90

57.

Scaﬃdi C, Myers B, Shaw M. Intelligently creating and recommending
reusable reformatting rules. In: Proceedings of the 14th International
Conference on Intelligent User Interfaces. 2009, 297–306

Lu Y, Wang F, Maciejewski R. Business intelligence from social media:
a study from the vast box oﬃce challenge. IEEE Computer Graphics
and Applications, 2014, 34(5): 58–69

58.

Brooks M, Amershi S, Lee B, Drucker S M, Kapoor A, Simard P. Featureinsight: visual support for error-driven feature ideation in text classification. In: Proceedings of the IEEE Conference on Visual Analytics
Science and Technology. 2015, 105–112

59.

Bögl M, Aigner W, Filzmoser P, Lammarsch T, Miksch S, Rind A. Visual analytics for model selection in time series analysis. IEEE Transactions on Visualization and Computer Graphics, 2013, 19(12): 2237–
2246

60.

Lu Y, Kruger R, Thom D, Wang F, Koch S, Ertl T, Maciejewski R. Integrating predictive analytics and social media. In: Proceedings of the
IEEE Conference on Visual Analytics Science and Technology. 2014,
193–202

61.

Piringer H, Berger W, Krasser J. Hypermoval: Interactive visual validation of regression models for real-time simulation. Computer Graphics
Forum, 2010, 29(3): 983–992

62.

Mühlbacher T, Piringer H. A partition-based framework for building
and validating regression models. IEEE Transactions on Visualization
and Computer Graphics, 2013, 19(12): 1962–1971

63.

Gotz D, Sun J. Visualizing accuracy to improve predictive model performance. In: Proceedings of the IEEE VIS Workshop on Visualization
for Predictive Analytics. 2014

38.

Templ M, Alfons A, Filzmoser P. Exploring incomplete data using visualization techniques. Advances in Data Analysis and Classification,
2012, 6(1): 29–47

39.

40.

41.

42.

205

Ives Z, Knoblock C, Minton S, Jacob M, Talukdar P, Tuchinda R, Ambite J L, Muslea M, Gazen C. Interactive data integration through smart
copy & paste. In: Proceedings of the Biennial Conference on Innovative Data Systems Research. 2009
Kandel S, Heer J, Plaisant C, Kennedy J, Van Ham F, Riche N H,
Weaver C, Lee B, Brodbeck D, Buono P. Research directions in data
wrangling: visualizations and transformations for usable and credible
data. Information Visualization, 2011, 10(4): 271–288

43.

Robertson G G, Czerwinski M P, Churchill J E. Visualization of mappings between schemas. In: Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems. 2005, 431–439

44.

Altova. Data integration: opportunities, challenges, and altova mapforce. http://www.altova.com/whitepapers/mapforce.pdf, 2014

45.

Informatica. The informatica data quality methodology: a framework to achieve pervasive data quality through enhanced businessit collaboration. https://www.informatica.com/downloads/7130-DQMethodology-wp-web.pdf, 2010

46.

Zheng Y. Methodologies for cross-domain data fusion: an overview.
IEEE Transactions on Big Data, 2015, 1(1): 16–34

47.

Dash M, Liu H. Feature selection for classification. Intelligent Data
Analysis, 1997, 1(3): 131–156

64.

Quinlan J R. Induction of decision trees. Machine Learning, 1986, 1(1):
81–106

48.

Fogarty J, Hudson S E. Toolkit support for developing and deploying
sensor-based statistical models of human situations. In: Proceedings
of the SIGCHI Conference on Human Factors in Computing Systems.
2007, 135–144

65.

Suykens J A, Vandewalle J. Least squares support vector machine classifiers. Neural Processing Letters, 1999, 9(3): 293–300

66.

Johnson B, Shneiderman B. Tree-maps: a space-filling approach to the
visualization of hierarchical information structures. In: Proceedings of
the IEEE Conference on Visualization. 1991, 284–291

67.

Stasko J, Zhang E. Focus+context display and navigation techniques
for enhancing radial, space-filling hierarchy visualizations. In: Proceedings of the IEEE Symposium on Information Visualization. 2000,
57–65

49.

Markovitch S, Rosenstein D. Feature generation using general constructor functions. Machine Learning, 2002, 49(1): 59–98

50.

Schuller B, Reiter S, Rigoll G. Evolutionary feature generation in
speech emotion recognition. In: Proceedings of the IEEE International
Conference on Multimedia and Expo. 2006, 5–8

51.

Guo D S. Coordinating computational and visual approaches for interactive feature selection and multivariate clustering. Information Visualization, 2003, 2(4): 232–246

68.

Ware M, Frank E, Holmes G, Hall M, Witten I H. Interactive machine learning: letting users build classifiers. International Journal of
Human-Computer Studies, 2001, 55(3): 281–292

52.

Seo J, Shneiderman B. A rank-by-feature framework for unsupervised
multidimensional data exploration using low dimensional projections.
In: Proceedings of the IEEE Symposium on Information Visualization.
2004, 65–72

69.

Ankerst M, Elsen C, Ester M, Kriegel H P. Visual classification: an interactive approach to decision tree construction. In: Proceedings of the
5th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining. 1999, 392–396

53.

Piringer H, Berger W, Hauser H. Quantifying and comparing features
in high-dimensional datasets. In: Proceedings of the 12th International
Conference on Information Visualization. 2008, 240–245

70.

Van den Elzen S, Van Wijk J J. Baobabview: Interactive construction
and analysis of decision trees. In: Proceedings of the IEEE Conference
on Visual Analytics Science and Technology. 2011, 151–160

54.

May T, Bannach A, Davey J, Ruppert T, Kohlhammer J. Guiding feature subset selection with an interactive visualization. In: Proceedings
of the IEEE Conference on Visual Analytics Science and Technology.
2011, 111–120

71.

Becker B, Kohavi R, Sommerfield D. Visualizing the simple Baysian
classifier. In: Fayyad U, Grinstein G G, Wierse A, eds. Information Visualization in Data Mining and Knowledge Discovery. San Francisco:
Morgan Kaufmann Publishers Inc., 2002

55.

Kohavi R, John G H. Wrappers for feature subset selection. Artificial

72.

Caragea D, Cook D, Honavar V G. Gaining insights into support vec-

206

Front. Comput. Sci., 2017, 11(2): 192–207
Proceedings of the IEEE VIS Workshop on Visualization for Predictive Analytics. 2014

tor machine pattern classifiers using projection-based tour methods.
In: Proceedings of the 7th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. 2001, 251–256

89.

73.

Ma Y. EasySVM: a visual analysis approach for open-box support vector machines. In: Proceedings of the IEEE VIS Workshop on Visualization for Predictive Analytics. 2014

Yeon H, Jang Y. Predictive visual analytics using topic composition. In:
Proceedings of the 8th International Symposium on Visual Information
Communication and Interaction. 2015, 1–8

90.

74.

John G H, Langley P. Estimating continuous distributions in bayesian
classifiers. In: Proceedings of the 11th Conference on Uncertainty in
Artificial Intelligence. 1995, 338–345

Wu Y C, Liu S X, Yan K, Liu M C, Wu F Z. OpinionFlow: visual
analysis of opinion diﬀusion on social media. IEEE Transactions on
Visualization and Computer Graphics, 2014, 20(12): 1763–1772

91.

75.

Ho T K. Random decision forests. In: Proceedings of the 3rd International Conference on Document Analysis and Recognition. 1995, 278–
282

Choo J, Lee H, Kihm J, Park H. iVisClassifier: an interactive visual
analytics system for classification based on supervised dimension reduction. In: Proceedings of the IEEE Symposium on Visual Analytics
Science and Technology. 2010, 27–34

76.

Mühlbacher T, Piringer H, Gratzl S, Sedlmair M, Streit M. Opening the
black box: strategies for increased user involvement in existing algorithm implementations. IEEE Transactions on Visualization and Computer Graphics, 2014, 20(12): 1643–1652

92.

Höferlin B, Netzel R, Höferlin M, Weiskopf D, Heidemann G. Interactive learning of ad-hoc classifiers for video visual analytics. In: Proceedings of the IEEE Conference on Visual Analytics Science and
Technology. 2012, 23–32

77.

Paiva J G S, Schwartz W R, Pedrini H, Minghim R. An approach to
supporting incremental visual data classification. IEEE Transactions
on Visualization and Computer Graphics, 2015, 21(1): 4–17

93.

Heimerl F, Koch S, Bosch H, Ertl T. Visual classifier training for text
document retrieval. IEEE Transactions on Visualization and Computer
Graphics, 2012, 18(12): 2839–2848

78.

Talbot J, Lee B, Kapoor A, Tan D S. EnsembleMatrix: interactive visualization to support machine learning with multiple classifiers. In:
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2009, 1283–1292

94.

Munzner T. Visualization Analysis and Design. Boca Raton: CRC
Press, 2014

95.

Delevingne L. Hedge fund robots crushed human rivals in 2014.
http://www.cnbc.com/2015/01/05/hedge-fund-robots-crushed-humanrivals-in-2014.html, 2015

96.

Seifert M, Hadida A L. On the relative importance of linear model and
human judge(s) in combined forecasting. Organizational Behavior and
Human Decision Processes, 2013, 120(1): 24–36

97.

Ruchikachorn P, Mueller K. Learning visualizations by analogy: promoting visual literacy through visualization morphing. IEEE Transactions on Visualization and Computer Graphics, 2015, 21(9): 1028–
1044

98.

Amini F, Rufiange S, Hossain Z, Ventura Q, Irani P, McGuﬃn M J. The
impact of interactivity on comprehending 2D and 3D visualizations
of movement data. IEEE Transactions on Visualization and Computer
Graphics, 2015, 21(1): 122–135

79.

80.

81.

Wu Y, Pitipornvivat N, Zhao J, Yang S, Huang G, Qu H. egoSlider:
visual analysis of egocentric network evolution. IEEE Transactions on
Visualization and Computer Graphics, 2016, 22(1): 260–269
Stolper C D, Perer A, Gotz D. Progressive visual analytics: user-driven
visual exploration of in-progress analytics. IEEE Transactions on Visualization and Computer Graphics, 2014, 20(12): 1653–1662
Ng K, Ghoting A, Steinhubl S R, Stewart W F, Malin B, Sun J.
PARAMO: a PARAllel predictive MOdeling platform for healthcare
analytic research using electronic health records. Journal of Biomedical Informatics, 2014, 48: 160–170

82.

Chang C C, Lin C J. LIBSVM: a library for support vector machines.
ACM Transactions on Intelligent Systems and Technology, 2011, 2(3):
27

83.

Bögl M, Aigner W, Filzmoser P, Gschwandtner T, Lammarsch T,
Miksch S, Rind A. Visual analytics methods to guide diagnostics for
time series model predictions. In: Proceedings of the IEEE VIS Workshop on Visualization for Predictive Analytics. 2014

84.

Andrienko N, Andrienko G, Rinzivillo S. Experiences from supporting
predictive analytics of vehicle traﬃc. In: Proceedings of the IEEE VIS
Workshop on Visualization for Predictive Analytics. 2014

85.

Maciejewski R, Hafen R, Rudolph S, Larew S G, Mitchell M, Cleveland W S, Ebert D S. Forecasting hotspots — a predictive analytics
approach. IEEE Transactions on Visualization and Computer Graphics, 2011, 17(4): 440–453

86.

Cleveland R B, Cleveland W S, McRae J E, Terpenning I. STL: a
seasonal-trend decomposition procedure based on loess. Journal of Official Statistics, 1990, 6(1): 3–73

87.

88.

Junhua Lu is currently working toward the
PhD degree with the State Key Laboratory
of Computer Aided Design and Computer
Graphics, Zhejiang University, China. His
research interests include visualization and
visual analytics.

Wei Chen is currently a professor at the
State Key Laboratory of Computer Aided
Design and Computer Graphics, Zhejiang

Bryan C, Wu X, Mniszewski S, Ma K L. Integrating predictive analytics into a spatiotemporal epidemic simulation. In: Proceedings of the
IEEE Conference on Visual Analytics Science and Technology. 2015,
17–24

University, China. He has published more

Chuang J, Socher R. Interactive visualizations for deep learning. In:

ing committee member of the IEEE Pacific

than 60 papers in international journals and
conferences. Prof. Chen served as a steer-

Junhua LU et al. Recent progress and trends in predictive visual analytics

207

Visualization, the conference chair of the IEEE Pacific Visualization

Fan Zhang is currently an associate profes-

2015, and a paper cochair of the IEEE Pacific Visualization 2014.

sor at the Zhejiang University of Technol-

He is an awardee of the NSFC Excellent Young Scholars Program

ogy, China. His research interests include

in 2014.

visual analytics and parallel computing.
Yuxin Ma is currently working toward the
PhD degree with the State Key Laboratory
of Computer Aided Design and Computer
Graphics, Zhejiang University, China. His

Ross Maciejewski is an assistant profes-

research interests include visual analytics

sor in the School of Computing, Informat-

and visual data mining.

ics & Decision Systems Engineering, Arizona State University, USA. His primary
research interests are in the areas of geo-

Junming Ke is an undergraduate student of

graphical visualization and visual analytics

Zhejiang University of Technology, China.

focusing on public health, dietary analy-

He is undergoing an internship in the State

sis, social media, and criminal incident re-

Key Laboratory of Computer Aided Design

ports. He has served on the organizing committee for the IEEE Con-

and Computer Graphics, Zhejiang Univer-

ference on Visual Analytics Science and Technology (2012–2013,

sity, China.

2015) and the IEEE/VGTC EuroVis Conference (2014–2016) and
has been involved in award winning submissions to the IEEE Visual Analytics Contest (2010, 2013, 2015). He is also a fellow of

Zongzhuang Li is an undergraduate student

the Global Security Initiative at ASU and the recipient of an NSF

of Zhejiang University (ZJU), China. He is

CAREER Award (2014).

working on his graduation proposal in the
State Key Laboratory of Computer Aided
Design and Computer Graphics, ZJU.

Volume Composition and Evaluation Using
Eye-Tracking Data
AIDONG LU
University of North Carolina at Charlotte
and
ROSS MACIEJEWSKI and DAVID S. EBERT
Purdue University

4
This article presents a method for automating rendering parameter selection to simplify tedious user interaction and improve
the usability of visualization systems. Our approach acquires the important/interesting regions of a dataset through simple user
interaction with an eye tracker. Based on this importance information, we automatically compute reasonable rendering parameters using a set of heuristic rules, which are adapted from visualization experience and psychophysical experiments. A user study
has been conducted to evaluate these rendering parameters, and while the parameter selections for a specific visualization result
are subjective, our approach provides good preliminary results for general users while allowing additional control adjustment.
Furthermore, our system improves the interactivity of a visualization system by significantly reducing the required amount of
parameter selections and providing good initial rendering parameters for newly acquired datasets of similar types.
Categories and Subject Descriptors: I.3.6 [Computer Graphics]: Methodology and Techniques—interaction techniques; I.3.7
[Computer Graphics]: Three-Dimensional Graphics and Realism—color, shading, and texture
General Terms: Algorithms, Experimentation
Additional Key Words and Phrases: Usability and human factors in visualization, eye tracker, interaction, illustrative visualization, volume rendering.
ACM Reference Format:
Lu, A., Maciejewski, R., and Ebert, D. S. 2010. Volume composition and evaluation using eye-tracking data. ACM Trans. Appl.
Percept. 7, 1, Article 4 (January 2010), 20 pages.
DOI = 10.1145/1658349.1658353 http://doi.acm.org/10.1145/1658349.1658353

1.

INTRODUCTION

In order to create a meaningful and aesthetically pleasing computer visualization, substantial user
effort is involved in manually adjusting the rendering parameters. Generally, each rendering approach
has its own special parameters that need to be adjusted, such as color and opacity transfer functions for
direct volume rendering. Many volume-rendering approaches also require users to choose the volume
placement and viewing direction about the display. While the free selection of these parameters does
provide a flexible environment for the user to generate various results, it also requires a significant
This research is supported by DOE DE-FG02-06ER25733, NSF 0081581, 0121288, 0328984, and 0633150.
Authors’ addresses: Aidong Lu, University of North Carolina at Charlotte; email: Aidong.lu@uncc.edu
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided
that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page
or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to
lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be
requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481,
or permissions@acm.org.
c 2010 ACM 1544-3558/2010/01-ART4 $10.00

DOI 10.1145//1658349.1658353 http://doi.acm.org/10.1145//1658349.1658353
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

4:2

•

A. Lu et al.

amount of work and some knowledge about the applied rendering algorithm in order to obtain a satisfying result. As such, suitable automation of the rendering settings and parameters can simplify some
of the tedious user interaction and improve the overall usability of a visualization system. To create
such automation, a series of rendering rules must be generated to ensure appropriate placement, view
direction, and other key parameter choices.
In building our set of rules, we turn to artistic and scientific illustrations to look at the principles
that guide their selection of such parameters when creating images. Such illustrations have already
shown their expressiveness in representing various subjects, and they are widely used in science and
engineering. Furthermore, illustrators usually follow certain methodologies and procedures to effectively convey important figure aspects to the viewer. A key aspect of the perceived image quality is its
composition, which usually emphasizes the focal points and achieves a coherent image structure. While
the relative beauty of an image is subjective, some heuristics used by artists to create images are shared
by general illustrative works and can be used as guidelines for the selection of rendering parameters.
Although these guidelines are difficult to apply directly in complex environments, it is possible to
render volumetric datasets automatically according to several features of volume rendering, such as
fixed object shapes and positions. We can treat volume composition as the problem of organizing a set
of objects with constant positions and sizes effectively. We then extract features to determine regions of
interest within a scene and automate the rendering parameter selection to highlight key features. While
some volume features can be extracted through image processing and statistical approaches, determining regions of interest is a subjective issue. For example, given an image of a human hand, a physician
might focus on the joints of the hand, while general viewers might be interested in the bone structure.
To account for these discrepancies, our system utilizes an eye tracker to determine what areas of the volume attract a subject’s general interest. This approach creates a high-quality rendering tuned to a user’s
regions of interest while still providing users with the flexibility to adjust parameters to their liking.
In order to determine if our automated parameter selection is effective in drawing attention to regions
of interest, we have conducted a user study in which subjects are asked to find the important/highlighted
regions of previously rendered volumes. Each of these previously rendered volumes has been automatically rendered with one region of interest. Our composition rules give more illustrative emphasis to the
important areas, and as such, these areas should be discernible from other structures in the images.
Our results show that the parameters generated for different rendering styles are able to highlight
importance information for a significant portion of our subjects.
In the following, sections, we will first summarize related work on importance-based rendering, composition, and eye-tracker studies. Then, in Section 3, we describe the procedure for gathering importance
information from a user with an eye tracker. In Section 4, we present the procedure to process the importance information according to eye movement behaviors. In Section 5, we summarize a set of heuristic
composition rules and use them to generate a plausible general-volume visualization automatically.
Section 6 describes our user study, and the results are reported in Section 7. Finally, we discuss our
results and propose future work in Section 8.
2.
2.1

RELATED WORK
Importance-Based Rendering

Illustrative renderings can be more expressive than photographs because of their ability to emphasize
important objects and simplify irrelevant details [Gooch and Gooch 2001; Strothotte and Schlechtweg
2002]. The important objects are given different meanings under different contexts, such as calculated
salience or user-specified importance information. In computer graphics and visualization, there are
several research topics that are closely related to this control of level of detail. First, importance-driven
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

Volume Composition and Evaluation Using Eye-Tracking Data

•

4:3

approaches render objects at different levels according to their importance information. For example,
Seligman and Feiner [1991] presented an automated intent-based approach using rule-based methods
and evaluators, and Viola et al. [2004] extended the cut-away views to an automatic focus, and context
display based on assigned importance values. Similarly, focus+context visualization renders objects
in focus with more obvious styles. For example, Helbing et al. [1998] used emphasized rendering to
communicate relevance and guide user focus, and Svakhine et al. [2005] presented an interactive
medical illustration system with various combinations of rendering enhancements/styles. Third, cutaway views [Diepstraten et al. 2003; McGuffin et al. 2003] can also be included as one approach to
emphasize important objects through cutting away or distorting the less important objects in the front.
Fourth, stylization and abstraction topics emphasize important features or salience for both images and
3D objects. For example, Hamel and Strothotte [1999] used templates to describe and transfer rendering
styles to save tuning the parameters. DeCarlo and Santella [2002] used eye-tracking data to stylize
and abstract photographs into a line-drawing style with highlighted interested regions. They further
validated their approach through user studies [2004]. Design principles have also been explored for
creating effective assembly instructions based on cognitive psychology research [Agrawala et al. 2003].
Stylized focus for 3D models has also been developed to draw viewer’s gaze to an emphasized area [Cole
et al. 2006]. These approaches utilized techniques from different fields to emphasize important objects
during the rendering process, which is one important criterion we consider to develop this composition
approach.
2.2

Composition

Both interactive and automatic methods have been developed to reduce user interaction for selecting
rendering parameters. Among these methods, rule-based approaches have been developed for several
graphics and visualization applications. For example, Mackinlay [1986] developed a framework for automatic graphical presentation creation and evaluation. Beshers and Feiner [1993] designed rule-based
visualization principles that take into account the characteristics of data, tasks, and hardware capability. Gooch et al. [2001] presented an overview of compositional principles and an approach for finding
a good composition for 3D objects. Strothotte et al. [1994] designed sketch rendering and discussed
the function of rendering choices on the perception and understanding of the viewers. Also, suitable
user interaction has been introduced to guide composition algorithms. For example, Rist et al. [1994]
argued that semiautomation was a reasonable compromise for computer-generated illustrations and it
could release users from routine subtasks. Bergman et al. [1995] presented an interactive approach to
guide the user’s selection of color maps for various visualization tasks. Kowalski et al. [2001] guided
the rendering parameter selections based on compositional needs in an interactive animation scene.
Similar to these approaches, our method is also based on composition rules and incorporate a small
amount of user interaction. The main difference is that we fully automate volume renderings according
to regions of interest collected from the user’s eye movements.
2.3

Eye-Tracker Studies

To acquire the information of regions of interest, eye tracking has been one popular approach in computer graphics, human–computer interaction, and psychology. We have divided the related work into two
groups based on application type. The first group focuses on using the eye-gaze information to improve
the understanding of knowledge. Research in psychology has shown that the eye movement patterns
during complex scene perception are related to the information of the scene, as well as to the cognitive
processing of the scene [Rayner 2004]. For example, eye-tracking data can be used to extract visual
features from 2D/3D figures [Chung et al. 2004]. Comparing eye movement strategies can provide further information for professional education training [Law et al. 2004] and 2D/3D display analysis [Tory
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

4:4

•

A. Lu et al.

et al. 2005]. Eye trackers have also been used to characterize low and high comprehenders [Bednarik
et al. 2006].
The second group of approaches focuses on using eye gaze information to improve the performance
of various systems. For example, Levoy and Whitaker [1990] let the high-resolution sweet spot follow
the user’s gaze to achieve a trade-off between the computation resource and the represented details. In
human-computer interaction, eye-based interaction has been applied to design a gaze-controlled navigator [Starker and Bolt 1990; Tanriverdi and Jacob 2000] and replace the keyboard and mouse [Majaranta
and Raiha 2002]. Eye tracking has also been used to improve the rendering speed in the simulation
of a realistic collision [O’Sullivan and Dingliana 2001; O’Sullivan et al. 2003] and natural eye contact
between users in a video conference [Jerald and Daily 2002]. In this article, we use eye tracker as a convenient interaction tool, and we use collected information to calculate regions of interest for volumetric
datasets.
3.

ACQUISITION OF IMPORTANCE INFORMATION

To select regions of interest for composition tasks, a useful technique in visualization is to adjust
transfer functions [Bordoloi and Shen 2005; Takahashi et al. 2005]. Although transfer functions have
been shown to be powerful visualization tools, they are not necessarily intuitive for general users. Our
design principle is to develop an intuitive approach that can be used by both scientific experts and
general users. We use the eye tracker as a convenient interaction tool and calculate regions of interest
from the patterns of a user’s eye movement. A user only needs to look at their interested regions in the
volume on the screen and his/her eye movements during that time period will be automatically recorded.
In the remainder of this section, we briefly discuss eye movement theory to show the foundation of using
an eye tracker for 3D volume composition. Then, we describe our procedure of using an eye tracker to
gather importance information.
It is known that eye movements seldom perform wasted motions, and typically focus near the best
place to gather desired visual information [Sekuler and Blake 1994]. Therefore, one can determine
what regions of an object a user is interested in by analyzing eye movements. Such an analysis has been
mainly used for 2D or 2D-oriented applications, such as improving rendering speed [Levoy and Whitaker
1990; O’Sullivan and Dingliana 2001; O’Sullivan et al. 2003] and abstracting data information [DeCarlo
and Santella 2002]. There are two types of basic eye movements: saccades and fixations. A saccade is
a rapid intermittent eye movement that occurs when the eyes fix on one point after another, or for the
purpose of lubrication. A fixation is when eye movements stop and focus on a particular object. It has
been shown that fixations correspond to informative locations and the time actually spent fixating on
a particular location indicates that processing of the object is taking place [Rayner 2004]. As such, we
use the eye fixations to determine the user’s regions of interest. We then group the eye data spatially
and temporally and calculate the sets of sequential fixations. The lengths of the fixations are related to
the interest degrees of the processed information.
To collect the importance information, we have designed a simple procedure for general users. As
opposed to most 2D and 2D-oriented research with eye trackers, our objective is to gather the importance
information for the 3D voxels of a volume. Since the eye tracker returns 2D point positions on the image
plane, we need additional information to reconstruct the 3D points of the focal regions. Our approach
is to let users look at a constantly rotating volume while we gather their eye movement data. With the
rotation information, we can locate the 3D regions of interest from multiple consecutive eye data if the
user keeps looking at the same position. As previously discussed, fixations are the sources for us to locate
a user’s region of interest and fixations usually take a significantly longer time than saccades; therefore,
we can gather the importance information by rotating the observed volumes. The rotation pace is set
at 10 seconds per 360 degrees, which is slow enough for a user to observe details and fast enough to
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

Volume Composition and Evaluation Using Eye-Tracking Data

•

4:5

Fig. 1. A sequence of eye movement during the volume rotation and isovalue changing. The red point indicates the position of
the eye gaze.

avoid wandering and boredom. The rotation direction can be interactively adjusted by users. During
this interaction, their eye data is discarded, as the eye movements may involve other factors due to the
interaction. This procedure ends when users feel that they have already explored the volume contents.
To measure the amount of detail from the eye-tracking data, we use the concept of visual acuity, which
is a measure of the smallest detail that an observer can resolve under ideal conditions. Reddy [2001]
summarized measure models from the vision literature into a single equation for visual acuity, H(v, e).
As shown in Equation (1), H(v, e) is computed from the contrast sensitivity G(v) and sensitivity drop-off
M (e), when having a velocity of v degrees/second and a peripheral extent e degrees. It shows that the
highest sensitivity is located centrally in the fovea and varies across the retina. We use H(v, e) as the
weight of observed information for the collected eye data, where v is the volume’s rotation speed on the
image plane and e is measured from the distance between the user to the screen and the pixel position
to the screen center.
⎧
60.0, v ≤ 0.825 deg/s
⎪
⎨
G(v) = 57.69 − 27.78 log10 (v), 0.825 deg/s < v ≤ 118.3 deg/s
⎪
⎩ 0.1, v > 118.3 deg/s

1.0, e ≤ 5.79 deg
M (e) =
7.49/(0.3e + 1)2 , e > 5.79 deg
H(v, e) = G(v)M (e)

(1)

Another challenge with 3D volumes is that all the information from a volume cannot be shown to
a user at once, as opposed to 2D images. To explore the objects in the volume, we use a standard
volume rendering approach: transparent isosurfaces. While the volume is rotating, subjects can change
the normalized isosurface values from 0 to 1. This allows the subjects to explore any of their regions of
interest in the volume. The window for adjusting the isosurface value is independent from the rendering
window. The eye tracker returns 0 when the subjects look outside the rendering window; therefore, the
eye movements for adjusting the isosurface value do not affect the gathered importance information.
Figure 1 shows a sequence of eye gaze positions that we recorded when a user traversed the isosurfaces
of a human hand dataset.
4.

PROCESSING OF IMPORTANCE INFORMATION

The data collected during the acquisition phase is a list of 2D eye movement points on the image plane
and their corresponding volume rotation sequences. With this information, we reconstruct and cluster
an eye gaze volume to generate importance maps that indicate the user’s interest. We generate importance maps for both 3D space and data value space and will use them in the automatic composition
phase.
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

4:6

•

A. Lu et al.

Fig. 2. A pipeline overview of eye data process. We first collect 2D eye points when users are looking at a rotating volume. These
2D eye points are used to construct an eye gaze volume recording 3D locations of fixations. We then process and cluster this eye
gaze volume to generate a 3D importance volume. Also, from the clustering result of Section 4.2, we construct the fixations on
a 2D value space (histogram) and generate a 2D importance space in Section 4.3. Both the 3D importance volume and the 2D
importance space are used to decide visualization parameters automatically in Section 5.

4.1

Reconstruction of the Eye Gaze Volume

Eye trackers generally collect eye movements at a fixed rate (such as 60 data-points per second) during
an indicated time period. To correctly construct an eye gaze volume, we need to correspond eye movements with a volume rotation sequence. Since our eye-tracking data is acquired on a separate machine,
we place a time stamp whenever we rotate the volume. This way, we can clearly link the volume rotations to the eye movements by setting each volume rotation matrix to all the eye movements within
the corresponding time range.
As discussed in the previous section, we are only interested in using the fixations of the eye movements
to gather the information of regions of interest. Therefore, we remove the eye data that moves faster
than a normal fixation speed, which indicates a saccade. As Ohshima et al. [1996] suspended rendering
when the eye moves faster than 180 deg/s, we use this value as the eye velocity threshold to distinguish
saccades from fixations.
For each 2D point on the image plane, given its corresponding volume rotation matrix, it represents
a line passing through the volume. Since the eye observance is located on the image plane, we let all
the voxels projected on the same area to share its visual acuity value and use the same importance
weight for all these voxels, regardless of their depth. According to the 2D visual acuity model, each eye
data point represents that viewers received information from all the voxels that are projected on the
image plane within a small area, so each eye point corresponds to a subvolume in the 3D space. For
orthogonal views, this subvolume is a cylinder, since depth values do not affect projection positions. For
projective views, the radius of this subvolume becomes smaller as the depth increases.
When viewing a 3D volume at each time stamp, users generally focus on a 3D point instead of an
entire subvolume. Since a single 2D eye point on the image plane cannot suggest any depth information,
we use consecutive eye points to locate the focus point. This can also identify the exact 3D location when
multiple voxels are projected onto the same eye point. We cannot simply calculate the intersection of
subvolumes, since the constructed shape of a focus point will be affected by its fixation duration. The
shape of a focus point may vary from a thin line, when fixation duration is short, to a sphere, when
fixation duration is long. To better measure the importance degrees around a 3D focus point, we change
the 2D visual acuity model into a 3D model by assigning the same weight to all the voxels with equal
distance to the center. The model center is set as the intersection point between two consecutive eye
focus points. Instead of providing a visual acuity model for 3D space, we use this 3D model to remove
the artifacts of the importance map from collected viewing angles.
The process of reconstruction starts from the second eye data. For each eye point, we test the velocity
requirement, calculate the focus point with the previous point, and add the weights from the 3D visual
acuity model to the eye gaze volume. After we generate the eye gaze volume, the regions with high
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

Volume Composition and Evaluation Using Eye-Tracking Data

•

4:7

values indicate higher interests or importances to the user. We then normalize the generated eye gaze
volume for the clustering procedure.
4.2

Clustering of the Eye Gaze Volume

When observing an object, human eyes often choose several suitable locations to look at the same object.
Also, the positions of fixations may indicate areas that are difficult for users to understand, instead of
a single region of interest. To combine close fixations that correspond to the same regions of interest,
we need a clustering algorithm to group focus points from the eye gaze volume.
To prepare for clustering, we first select voxels that have higher importance values above an assigned
threshold in the eye gaze volume. This helps to remove extra regions that are introduced just because
they were projected close to some eye points on the image plane. We can use these voxel locations as
a list of 3D points and input them to clustering algorithms to group fixation locations. A difficulty for
such a clustering task is that we do not know the number of clusters ahead of time. This restricts us
from using standard clustering algorithms, such as K-means. The mean shift algorithm [Comaniciu and
Meer 2002; Georgescu et al. 2003], based on the gradient direction, has been shown to be a flexible and
robust clustering algorithm. It can be used without the knowledge of cluster number and cluster shape.
We adopted the mean shift procedure to decide cluster number, cluster centers (modes) and assign each
input point to the closest cluster. We continue to merge clusters if their center distance is within the
eye-tracker accuracy range. Small clusters with the size of less than 5% of the total point number are
discarded, since they are not the main focus of the volume.
We further smooth focus regions by fitting a Gaussian model to each cluster set. All the points assigned
to a cluster are used to calculate the parameters of a Gaussian model. The valid window size is located
by including at least 90% of the points in this cluster. The cluster results are used to generate a smooth
importance map, which will be further used to determine rendering parameters.
4.3

Clustering of Value Ranges

In addition to the 3D importance map, we also generate an importance map for value space. Since data
values of focus points may indicate value ranges that users are interested in, this information can be
used to detect data features of a volume with eye movements. We use important value ranges to design
rendering parameters.
The generation process for importance map of value space is similar to the previously mentioned
process of 3D importance map. We use a common 2D histogram (voxel value and gradient magnitude) [Kindlmann and Durkin 1998] as our value space. We first generate an initial 2D importance
map by collecting the distribution of 3D focus points in the value space. Then, we cluster the initial 2D
importance map with the mean shift algorithm and smooth cluster results with Gaussian models. This
procedure is similar to the generation of 3D importance map and helps us produce a 2D importance map
on the value space. Each cluster is shown as one Gaussian shape on the 2D histrogram and corresponds
to one 3D object in the volume.
We use the information of value importance map to finalize the 3D importance map for segmented
and unsegmented volumes, respectively. For segmented volumes, we use the value importance map to
hit on the objects in the volume. The hit numbers are normalized as object importance values so that we
can further emphasize the regions that might not be in the user’s focus regions but share some common
features with them. For un-segmented volumes, we can use the value clusters to segment the volume
automatically by revisiting the volume with the value importance map as a transfer function. Voxels
that have data values and gradient magnitudes belong to a 2D importance cluster will be classified as
portions of the corresponding 3D object. Therefore, we can treat segmented and unsegmented datasets
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

4:8

•

A. Lu et al.

as volumes that are composed of several stable objects, and we can process these datasets in the same
way as described in the following section.
5.

AUTOMATIC RENDERING SETTINGS

Based on the importance maps acquired from previous sections, we can automate the process of volume
rendering, which usually requires a user to manually adjust some rendering parameters. The main
criterion we try to follow is that a good visualization often captures object features and emphasizes
regions of interest in a manner that is not only scientifically accurate but also visually pleasing to the eye.
Here, we will concentrate on producing a computer-generated visualization with emphasized important
regions automatically. We will explore approaches for several necessary rendering parameters shared
among general-volume visualization methods. From the previous two sections, we have prepared the
following three data types to use in the automation process:
—Iv (): the importance value for each voxel (Section 4.2)
—I D(): an object ID for each voxel (Section 4.3)
—Io (): the importance value for each object (Section 4.3)
5.1

View Direction

Psychologists use “canonical view” to refer to the viewpoint that is preferred by most viewers [Blanz
et al. 1999]. The study of canonical view searches for the factors that can affect our perception and
understanding by observing the consistency across subjects, instead of locating a unique best view for
certain types of objects. There are consistent heuristics for choosing view directions from both artists
and psychology results, such as to pick an off-axis view from a natural eye height. Since most of them
match our criteria of a good visualization image, we can use the common factors of a canonical view to
guide our automatic viewpoint selection.
Canonical views have been studied for face recognition [Laeng and Rouw 2001], procedural graphics [Krull et al. 2003], 3D models [Denton et al. 2004], and animation generation [Kawai et al. 1993;
He et al. 1996]. Gooch et al. [2001] chose initial viewpoint according to the proportions of projection
areas and perturbed viewing parameters guided by heuristic rules for layout. Vázquez et al. [2001] used
viewpoint entropy to compute good viewing positions and select good views for scene understanding
of polygon models. Recently, the amount of information or viewpoint entropy are also explored for selecting good viewpoints for volume, time-varying, and isosurface renderings [Bordoloi and Shen 2005;
Takahashi et al. 2005; Ji and Shen 2006; Viola et al. 2006]. Most work measured the “goodness” of a
view direction in a certain way, such as designing objective functions or user experiments. For a volume,
since we do not have any specific information about the geometry shapes of the objects in the volume,
the viewpoint selection becomes more challenging. The major difference in this article from the previous
work is that we treat parameter selections as a more subjective issue, and we use an eye tracker to
acquire this information from the user, which is more intuitive than adjusting transfer functions and
does not require the knowledge of volume rendering.
Here, we briefly list the factors for viewpoint selections and describe our interpretation. We remove
the factors that are related to experiences, since these are impossible to quantize without understanding
the context of a dataset.
Salience: A view that shows the salience and significance of features is preferred by most observers.
For volume data, we use several standard values to represent data features, including the gradient
magnitude G, curvature magnitude C, and an edge detection value, E. The salience of a voxel is
represented as a weighted sum of all the feature factors.
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

Volume Composition and Evaluation Using Eye-Tracking Data

•

4:9

Occlusion: Occlusion is used to avoid too many crucial features overlapping with each other. A visualization result is always rendered in a view direction that the information would be sufficient or
clear for object recognition except for special purposes. Since volume visualization can show the inside
information as well as surfaces, occlusion is a very important factor in evaluating the quality of a view
direction. We measure the occlusion by projecting voxels onto the image plane and a good view direction
should include fewer overlapping voxels that have high salience or importance values.
Stability: A view from which small rotations will not produce significant changes is preferred to avoid
ambiguities. Stability is also related to the occlusion factor, since there would be no stability problem if
any two objects/saliences were not overlapping on the image plane. We measure the variance of a view
direction within a small region as the stability factor.
Familiarity: The views that are encountered most frequently or during initial learning period are
generally preferred. Since familiarity is difficult to measure without context information, we match the
familiarity by presenting the user-interest objects closer to the eye position as an approximation. For
instance, when looking at a volume, an observer is usually interested in the facial portion of a head or
the bones of a foot.
Combining these four factors, we calculate a weight W () for each voxel v using salience, importance,
and the normalized distance to the viewer location Distance(v), as shown in Equation (2). We use equal
weights ws , wi , wd , since all the values have been normalized already.
Salience


	


W (v) = ws ∗ (G + C + E) ∗Io (v) + wi ∗ Iv (v)

(2)

Familiarity


	


+ wd ∗ Iv (v) ∗ (1 − Distance(v))
To measure the occlusion degree of a view, we adopt a procedure similar to the splatting algorithm
[Zwicker et al. 2001]. We use a temporary buffer, with the same size of the image plane and initialize
all the values to be 1. Each voxel in the volume is projected onto the image plane and the corresponding
buffer values are modified with the voxel weight calculated in Equation (2). To avoid the handling of
voxel sequences, we multiply the weights of voxels and their corresponding buffer values so that the
occlusion degrees for all the views can be measured with this procedure.
O(view) =





p ∈ Image plane

Occlusion

	




(1.0 + W (v))

(3)

v ∈ Volume and v projects on p

The evaluation (“badness”) function of a view direction view is calculated as the negative of the sum of
all the items in the buffer and the variances.
Stability


	


Evaluation(view) = −(O(view) + Variance(O(view)))

(4)

To find a minimum value of such an implicit function (caused by the variance portion), we use an
optimization process, which only requires the function values instead of the derivatives of the objective
function. Gooch et al. [2001] use the downhill simplex method, which is well behaved for the problem
with a small computational burden. Since the design of our objective function involves more calculations,
we adapt the direction set method for faster processing [Press et al. 1992]. We divide the view space into
a set of samples by tessellating a sphere. Then, we choose the initial view direction with the minimum
evaluation value from the sample set. Finally, a minimum view is searched within the divided range,
which is a much smaller space than the whole view space. This process is guaranteed to find a local
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

4:10

•

A. Lu et al.

minimum value, which is good enough to find a plausible answer. With an increase in the initial view
sampler set, we can find a near-global minimum result.
5.2

Volume Center Position

After determining the view direction, we need to locate the volume center in space. Artistic illustrations
often achieve a balanced structure, attract viewer’s attention, and avoid equal divisions of an image.
Also, most visualization systems have fixed rendering window size and always put the volume at the
center of the screen. For a practical visualization system, we keep fixed rendering window size and put
the volume on the center so that we do not need to constantly change the volume center position to fit
all the objects inside the screen while rotating the volume.
We use the golden ratio (1:1.63) as the ratio of object size to the rendering window, since it is shown
to be more appealing than others [Livio 2002]. We calculate the bounding box of important clusters on
the image plane. The volume center is located by setting the ratio of the bounding box size and the
image plane to the golden ratio.
5.3

Rendering Parameters

The rendering settings of view direction and volume center position are designed for the whole volume,
while other rendering parameters are different for each object inside the volume. After a volume is put in
a good position to observe with the first two settings, we design the rendering parameters for each object
in this section so that they can be distinguished from the surroundings and the important objects can
stand out in the results. Since rendering parameters are dependent on the specific rendering algorithms,
we concentrate on the parameters that are common to general-volume rendering approaches.
Our parameters are designed according to the relations between each pair of objects that are measured through their positions and sizes. Previously, spatial relationships are often considered using
9-intersection model [Egenhofer and Herring 1990] and the dimensional model [Zlatanova et al. 2002].
We calculate the impact factor of one object to another to approximate objects’ spatial relations, which
will be used later to decide rendering parameters. For every object, we scan the volume from front
to back to measure the overlapping area of every other object in front of it. The overlapping area is
divided by the object projection area to overcome the impact of the object size and overlapping size,
since a smaller sized object is considered to have less influence than a larger one. For objects A and
B, the impact factor Impact(A, B) is considered through the overlapping regions Overlap(A, B) on the
image plane and the distance between two objects Dis(A, B).
Area(A, B)
Area(B)
Impact(A, B) = Overlap(A, B) + 1/Dis(A, B)

Overlap(A, B) =

(5)

5.3.1 Rendering Elements. When a volume contains multiple objects, a visualization system usually
assigns different rendering elements to each object to distinguish them, such as with different colors or
patterns. We use the relation matrix Impact(A, B) and the importance map generated from the previous
section to assign rendering elements. For each object, we assign several rendering elements from the
aspects of distinguishing and emphasizing, assuming that users do not have any preknowledge of the
real objects. To effectively visualize a dataset, we choose different elements for each object and keep
large perception distances among them. The most effective elements will be selected for emphasizing
important objects, which are acquired from the importance map. We concentrate on color selections in
this section and show that the same procedure can be used to assign other elements, such as patterns.
Since colors play an important role in data visualization, approaches have been devoted to colormapping designs [Rheingans and Tebbs 1990; Rheingans and Landreth 1995; Bergman et al. 1995] and
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

Volume Composition and Evaluation Using Eye-Tracking Data

•

4:11

color representation validations [Healey 1996]. When users interactively select colors for rendering, they
generally prefer to use more distinctive colors for objects with higher importance values; therefore, we
assign colors according to their importance value distribution. To balance the effects of color luminance,
our color map is chosen from the perceptual isoluminance color map [Kindlmann et al. 2002] and the
Euclidean distance between different colors is measured in a perceptually balanced color model (CIE
LUV) [Fairchild 1998]. We use the following procedure to ensure that our color assignments satisfy
their importance value distribution. For a volume with n objects, we equally divide the color map into
n sections and randomly select one color near the center of each section. To map the color distance
and object relation information, we normalize them to the same value region, respectively. Then, we
generate all the possible assignments and the final colors are chosen by minimizing the differences
between the object relations and the color distances ColorDis(i, j ) for colors i and j .

M (i, j ) =
(ColorDis(i, j ) − Impact(i, j ))2
(6)
i, j

The same procedure can be used to assign other rendering elements with predesigned sensitive values,
such as patterns. The rendering patterns can also influence the perception effects for users, as shown
in Kim et al. [2004]. We simply assign the sensitive values with the size of the pattern primitives. The
objects with higher importance values are rendered with the patterns composed of larger primitives,
while the least important object is rendered with small points. Figure 5(c, d) shows the automatically
generated results with different patch patterns.
5.3.2 Rendering Degrees. We use rendering degrees to refer to the parameters related to the level
of details or opacity values. Intuitively, a more important object will be rendered with more detail and
a higher opacity value. If we use the object importance value to assign the rendering degree for each
object directly, a problem arises when there are objects overlapping each other. The most important
object will occlude all the objects behind it and the less important object may occlude part of the more
important objects. Therefore, we use the overlapping relationships and the importance map to assign
rendering degree.
Our two basic rules are based on the overlapping relations that can affect our understanding of object
shapes and locations [Dowling 1987]. If there is no overlapping relation, the object is rendered at the
highest degree. When two objects overlap in the image plane, the object at the back is rendered with a
degree as high as possible; while the front object is rendered at a suitable degree to show part of the
back object, no matter its importance value.
We use the following procedure to calculate all the rendering degrees. Initially, the rendering degrees
RenderDegree() of all the objects are set to 1. Then, we traverse the object with positive importance
values in the decreasing order. For each object, we update the degree of the objects in front of it using
the overlapping area proportion and the importance values. Assuming object A is in front of object B,
we update the degree of A with the overlapping ratio of A on B Overlap(A, B) as a weight:
RenderDegree(A) = Overlap(A, B) ∗ f (Io (A), Io (B))
+ (1 − Overlap(A, B)) ∗ RenderDegree(A)
where f (x, y) = 0, x ≤ y; x−x y , x > y.

(7)

The calculated rendering degree can be used to determine the rendering parameters directly for some
algorithms. For example, the degree can be used to decide the opacity values for transfer functions. We
set the opacities with Opacity(x) = 0.1 + RenderDegree(x) ∗ 0.7 according to our experiences, so that
all objects are not totally transparent, and we always show the volume inside. For the silhouette effect
−−−−−−→ −−→
of nonphotorealistic rendering (NPR), the silhouette power in Opacity(x) = (Gradient · View) Sp(x) is
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

4:12

•

A. Lu et al.

Fig. 3. The reconstructed eye gaze volumes, segmented clusters, and our composition results for a segmented hand dataset (top)
and a foot dataset (bottom). For the hand dataset, since the bones are viewed as more important than the skin, they are less
transparent and rendered with less silhouette enhancement and a warmer color. The viewpoint is also selected for better bone
observance. For the foot dataset, although the user is more interested in the bones on the first and second toes, all the bones are
highlighted because of their value similarities.

Fig. 4. Two pairs of eye gaze volumes and composition results for a segmented feet dataset. The left pair focuses on the bones,
and the right pair focuses on the skin portion. These images show that different composite visualizations are generated according
to the user interests for best observance of objects of interest.

set as Sp(x) = 10 − 9 ∗ RenderDegree(x) so that the least important object will be rendered only with
silhouettes. Figures 3 and 4 show several automatic composition results in which colors are only selected
from the yellow to red hue range for more natural results.
It is relatively difficult for other rendering algorithms to summarize the changes of parameters
according to their desired rendering degrees. This problem corresponds to the parameter selections
of different rendering motifs [Svakhine et al. 2005]. For these approaches, the rendering degree is
used to determine the parameters on a high level, which are extracted from the input of experienced
users, while the lower-level interface is used to explicitly adjust the rendering parameters by users.
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

Volume Composition and Evaluation Using Eye-Tracking Data

•

4:13

Fig. 5. The parameter selections of two illustrative rendering styles from the collected importance information. (a,b) Composition
results of hand and abdomen datasets in a stippling rendering style. The focused regions are the bones of the hand and the lungs
in the abdomen. (c,d) Composition results of foot and feet datasets in a patch-based rendering style. The focused regions are the
bones of (c) and skins of (d).

Figure 5 (a, b) shows the stipple rendering result with the high-level interfaces that use only several
0 − 1 scalar values, including a gradient slider, sharpness slider, orientation slider, distance slider, and
interior slider, to control all the required parameters in the rendering process [Lu et al. 2003].
6.

EXPERIMENT DESIGN

We have performed a psychophysical experiment to evaluate the effects of different rendering styles
and parameters on user interaction. Since the parameters are designed to reflect important objects
in a voulme, we expect that these regions of interest in the volume composition results draw more
attentions from viewers. Therefore, we hypothesize that subjects shown images rendered with our
automatic parameter generation technique will be able to visually locate which area of the image was
considered to be a region of interest. This hypothesis leads us to evaluate partial effects of volume
composition results and provides some useful results for our future work. We also expect that the
effects of parameters vary from different rendering styles and have chosen three rendering styles of
both the abdomen and hand datasets in the experiment.
6.1

Apparatus

The system used in our experiment, as shown in Figure 6, consisted of a computer with an Intel Xeon
2.0 GH CPU and an ISCAN ETL-400 eye tracker. In order to acquire accurate results, subjects were
required to place their heads in a chin rest to minimize head movement. Rendering was done as a
preprocess step, and a set of static images was used for evaluation.
6.2

Subjects

Eight subjects (S1–S8) participated in the experiment. Five subjects were female. All subjects had little
to no familiarity with the datasets. All subjects had normal or corrected vision. The subjects ages ranged
from 15 to 43 years of age. No testing was performed to assess color vision impairment.
6.3

Stimuli

Two datasets were used in this experiment. They included the hand dataset and the abdomen dataset,
see Figure 7. Each dataset was rendered in three different styles and highlighted three different portions
of the data based on a predefined region of interest. The rows in Figure 7 represent each rendering
style applied to a given dataset. Each column in Figure 7 represent a particular region of interest. For
the hand dataset, one rendering highlighted the ulna, another the wrist, and the last, the thumb. For
the abdomen dataset, one rendering highlighted the liver, another the kidney opposite the liver, and the
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

4:14

•

A. Lu et al.

Fig. 6. A user participating in the eye-track study.

last, the heart. Each of the three renderings from the datasets were scaled to 700 × 700 pixels. These
renderings were then rotated and flipped to provide five different viewing angles of each image. This
created a set of 90 images.
6.4

Procedures

We tested the subject’s ability to visually determine which portion of the dataset was the region of
interest by displaying each image for 3 seconds and asking subjects to focus their attention on the
portion of the image that they determined to be “highlighted.” Prior to each experiment, a subject was
presented with a test set of images demonstrating exactly what was meant by highlighting. Subjects
were placed 320mm away from the monitor. Images were then presented in random order to each
subject. Each image was shown for exactly 3 seconds. This was followed by a black screen with a square
placed in one corner of the viewing area for 1 second to redirect the subject’s focus. The experiment
length was approximately 10 minutes including calibration time. Specifically, subjects were instructed
to search the image for what appeared to be a highlighted segment and then focus their gaze on that
segment until the the image changed to a black screen.
7.

EXPERIMENTAL RESULTS AND ANALYSIS

In order to evaluate the automatic parameter generation, we analyzed the subjects’ eye gaze patterns
to determine if they were able to accurately identify areas within the image that were highlighted. We
did not measure the time which a subject took to find the highlighted area. Instead, we determined the
average time it took subjects to come to a consistent gaze location, removing all cases where subjects
failed to settle on any location. The average time was approximately 1.5 seconds to locate a region of
interest. From there, we focused only on the last 1 second of data collected, as this should be the time
in which subjects are most focused on what they consider to be the region of interest in the image.
For each subject, we considered three cases for the hand dataset: (i) the subject looks at the thumb,
(ii) the subject looks at the wrist, (iii) the subject looks at the ulna. We also consider three cases for
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

Volume Composition and Evaluation Using Eye-Tracking Data

•

4:15

Table I. Summary of Subjects Able to Distinguish
Different Areas in Images
Hand
Abdomen

Stipple
87.5% (7/8)
87.5% (7/8)

Realistic
62.5% (5/8)
62.5% (5/8)

Illustrative
62.5% (5/8)
25.0% (2/8)

the abdomen dataset: (i) the subject looks at the image with the kidney highlighted, (ii) the subject
looks at the image with the heart highlighted, (iii) the subject looks at the highlighted liver opposite
of the kidney. For each subject, we looked at each rendering style to see if the subject could accurately
determine the highlighted object. To do this, we would compare cases (i), (ii), and (iii) to see if each case
was statistically different in each rendering style for each dataset. For each case and rendering style, the
subject looked at five images. Each image was rotated or mirrored in each of the five cases such that the
subject would not learn the exact location of the highlighted portion for the repeated trials. Each image
and the corresponding gaze locations were rotated back to a normal viewing position. The last second
for each trial was then aggregated into a single dataset such that for each case and rendering style,
we now have a dataset consisting of the last second of data for five trials. These gaze location datasets
were then analyzed in a pair-wise comparison test to determine if the subject was able to distinguish
different areas in cases (i), (ii), and (iii) of each dataset and rendering style. For example, within the
rendering style of stippling, we perform a pair-wise comparison test between the gaze locations from the
trials in which the subject looked at images with the thumb highlighted and compared these locations
to the gaze locations from the trials in which the subject looked at images with the wrist highlighted,
and so on. Under the pair-wise comparison test, we test the null hypothesis that the means of the gaze
locations for cases (i), (ii), and (iii) are equal. If the null hypothesis cannot be rejected at the 5% level,
then we have shown that the subject was not able to distinguish different areas of the image in cases
(i), (ii), and (iii) for the rendering style under question.
We analyze differences within subjects and images across highlighting areas through a Wilcoxon
Rank-Sum test to do a pair-wise comparison of cases (i), (ii), and (iii) separately. We use this test, which
is the nonparametric analog of the t-test, rather than a t-test because we believe the data are not
normally distributed, which is confirmed through visual analysis of histograms and the Shapiro-Wilk
test ( p < 0.001, rejection of normal distribution). Furthermore, under the assumption that subjects
have found the area in question during the last second of viewing, it would stand to reason that the
data would not be normally distributed during this time. The results of the Wilcoxon Rank-Sum test
are summarized in Table I. Note that the location along the x-axis is tested separately from location
along the y-axis; however, only one must be significantly different for the location to be confirmed as
statistically different. For example, if between cases (i) and (ii) the mean X value was not statistically
different, but the mean Y value was, then the 2D gaze point location should be at a different location.
For the stipple hand cases, seven our of eight subjects were able to find each highlighted object
( p < .02). For the hand images rendered in the realistic style, five of eight subjects were able to find each
highlighted object ( p < .04). For the hand images rendered in the illustrative style, five of eight subjects
were able to find each highlighted object ( p < .04). So, from the eye-tracker results, we can see that
for the hand stipple analysis only one out of eight subjects could not identify the highlighted/important
region in the images. For the hand images rendered in the realistic style, three out of eight subjects
could not identify the highlighted/important region in the images. Particularly, this comes from subjects
having difficulty separating the ulna from the wrist. For the hand images rendered in the illustrative
style, three out of eight subjects could not identify the highlighted/important region in the images.
Here, subjects are unable to separate any of the three objects, indicating this rendering style is the
least useful for directing attention.
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

4:16

•

A. Lu et al.

From the hand images, we can argue that stippling is the most effective for guiding users to a
particular focus. This is most likely due to the fact that the detail in the other images drawing attention
away from objects. We believe it would be possible to use stippling to guide a user’s focus and then change
the rendering style to provide more detail.
These same results were confirmed in the abdomen dataset. For the stipple abdomen cases, seven
out of the eight subjects were able to find each highlighted object ( p < .02). For the abdomen images
rendered in the realistic style, five out of the eight subjects were able to find each highlighted object
( p < .04). For the abdomen images rendered in the illustrative style, two out of the eight subjects were
able to find each highlighted object ( p < .04). For the abdomen stipple images, we can see that only
one out of eight subjects could not identify the highlighted/important region in the images. This subject
was the same subject who had difficulty in the stipple hand images. For the abdomen dataset rendered
in the realistic style, three out of eight subjects could not identify the highlighted/important region in
the images. Two of the subjects are the same as in the hand images. Here, there is more variance in
the data due to the fact that subjects have a larger area to look at in terms of the number of distinct
objects visible in the abdomen. For the abdomen dataset rendered in the illustrative style, six out of
eight subjects could not identify the highlighted/important region in the image. Here, subjects were
unable to separate any of the three objects, again indicating that this rendering style is the worst for
directing attention.
From this experiment, we can see that the parameters generated for both stipple and realistic rendering styles are able to adequately highlight importance information for over 50% of our subjects.
However, for the illustrative style, we find that our automatically generated parameters do not adequately focus the user’s attention to the intended targets. This is most likely due to the inconsistency
of texture details and the importance information of the acquired regions of interest, since all the textures are just selected from two medical illustrations. Further studies will be needed for exploring the
selections of textures according to the importance information due to their complex factors.
While we have now shown that we can guide subjects to focus on different areas of the image for
different highlighting, it is also necessary to see if the focus areas were statistically significantly close
to the area of interest that was highlighted. We have plotted the mean and standard deviations of
the gaze locations for the subjects to further confirm their gaze locations. In Figure 7, each subject’s
mean gaze location (over the last second of viewing) is plotted over the image viewed. The radius of the
corresponding circle is the standard deviation. Again, we see that in the stippled images (rows 1 and 4)
subjects are more likely to find the highlighted portion of the image when compared to the illustrative
style (rows 2 and 5) and the realistic style (row 3 and 6).
Since the design of this experiment concentrates on the highlighting aspect of volume composition
results, we call it an initial user study. Also, among the three rendering styles, stippling is the easiest
to create areas of large contrast, which draw a users gaze, as opposed to other more realistic rendering
styles. A more comprehensive study should include more perception and cognition aspects to evaluate
rendering parameters and styles.
8.

DISCUSSION AND FUTURE WORK

Volume composition aims to improve a practical issue of general-volume visualization systems and
reduce the repetitive and tedious user interaction. Although such a rule-based approach cannot compete
with the results generated by real users, the user interaction needed for several most common tasks can
be significantly reduced. Our interface allows users to concentrate on their specific tasks and are more
intuitive for general users, which can be used further to improve the usability of a visualization system.
To produce visualization with different concentrations, we believe that human factors should be
included in the visualization design. An eye tracker is a good tool in this case, since it provides input
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

Volume Composition and Evaluation Using Eye-Tracking Data

•

4:17

Fig. 7. Example visualization results for the user study. The focuses and variances of individual subjects are represented as
circles in the images.
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

4:18

•

A. Lu et al.

from users and is convenient to use. We show that the importance information acquired using an eye
tracker can be used to choose viewpoint, volume center, and multiple rendering parameters. We believe
that this importance information can be explored to develop automatic composition approaches for more
visualization parameters.
With an eye tracker, we have built a simple interface that can be used by both professional and
general users. Without the knowledge of the rendering approach, general users can still explore volume
data and achieve satisfying visualization results. Our initial user study demonstrates the potential
effectiveness of this automatic parameter selection method. This method is more effective for acquiring
immediate instinct reactions from users. We are planning to use an eye tracker as an additional input
and evaluation method to further simplify user interaction. There are also limitations of this approach
due to the eye movement behaviors. When several small objects are very close to each other, it may
be difficult to locate the exact object that users are looking at. We believe that more studies on the
correspondence of eye movements and cognition process will help us to improve the accuracy of this
approach.
The unoptimized composition algorithm for the results in Figures 3 and 4 takes 10 to 20 minutes,
which is mainly spent on the clustering part. Once the algorithm is done, the rendering is interactive
and the user can explore the volume based on their interests. We will develop faster algorithms to
provide more instant feedback, since our final goal is to use the eye tracker as an interactive input
method. We plan to work on more approaches to guide the parameter selections for general and specific
rendering methods.
We plan to perform future user studies to assess the effectiveness of visualization results. Beyond
our initial experiment, we plan to study the factors due to the rotation of the images, since it is possible
that the viewing direction plays a confounding role in the subjects’ ability to disseminate information,
as these images are naturally occurring scenes. We believe that it is worth a series of experiments to
evaluate rendering parameters and styles from more perception and cognition aspects.
ACKNOWLEDGMENTS

We would like to thank the editors and reviewers for their insightful comments.
REFERENCES
AGRAWALA, M., PHAN, D., HEISER, J., HAYMAKER, J., KLINGNER, J., HANRAHAN, P., AND TVERSKY, B.
by-step assembly instructions. In Proceedings of SIGGRAPH. ACM, New York, 828–837.

2003.

Designing effective step-

BEDNARIK, R., MYLLER, N., SUTINEN, E., AND TUKIAINEN, M. 2006. Program visualization: Comparing eye-tracking patterns with
comprehension summaries and performance. In Proceedings of the 18th Workshop of the Psychology of Programming Interest
Group. ACM, New York, 68–82.
BERGMAN, L. D., ROGOWITZ, B. E., AND TREINISH, L. A. 1995. A rule-based tool for assisting color map selection. In Proceedings
of the IEEE Conference on Visualization. IEEE, Los Alamitos, CA, 118–125.
BESHERS, C. AND FEINER, S. 1993.
Graph. Appl. 13, 4, 41–49.

Auto-visual: Rule-based design of interactive multivariate visualizations. IEEE Comput.

BLANZ, V., TARR, M. J., AND BULTHOFF, H. H.

1999.

What object attributes determine canonical views? Perception 28, 5.

BORDOLOI, U. AND SHEN, H. 2005. View selection for volume rendering. In Proceedings of the IEEE Conference on Visualization.
IEEE, Los Alamitos, CA, 487–494.
CHUNG, A. J., DELIGIANNI, F., HU, X.-P., AND YANG, G.-Z. 2004. Visual feature extraction via eye tracking for saliency driven
2D/3D registration. In Proceedings of the Eye Tracking Research AND Applications Symposium. ACM, New York, 49–54.
COLE, F., DECARLO, D., FINKELSTEIN, A., KIN, K., MORLEY, K., AND SANTELLA, A. 2006. Directing gaze in 3D models with stylized
focus. In Proceedings of the Euro-Graphics Symposium on Rendering. ACM, New York, 377–387.
COMANICIU, D. AND MEER, P. 2002.
Mach. Intell. 24, 5, 603–619.

Mean shift: A robust approach toward feature space analysis. IEEE Trans. Pattern Anal.

ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

Volume Composition and Evaluation Using Eye-Tracking Data

•

4:19

DECARLO, D. AND SANTELLA, A. 2002. Stylization AND abstraction of photographs. In Proceedings of SIGGRAPH. ACM, New
York, 769–776.
DENTON, T., ABRAHAMSON, J., AND SHOKOUFANDEH, A. 2004. Approximation of canonical sets AND their applications to 2D view
simplification. In Proceedings of the IEEE Conference on Computer Vision AND Pattern Recognition. IEEE, Los Alamitos, CA.
DIEPSTRATEN, J., WEISKOPF, D., AND ERTL, T. 2003. Interactive cutaway illustrations. In Proceedings of Eurographics. ACM, New
York, 523–532.
DOWLING, J. E. 1987. The Retina: An Approachable Part of the Brain. Belknap Press, Cambridge, MA.
EGENHOFER, M. J. AND HERRING, J. R. 1990. A mathematical framework for the definition of topological relationships. In Proceedings of the 4th International Symposium on Spatial Data Handling. ACM, New York, 803–813.
FAIRCHILD, M. D. 1998. Color Appearance Models. Addison-Wesley, Upper Saddle River, NJ.
GEORGESCU, B., SHIMSHONI, I., AND MEER, P. 2003. Mean shift based clustering in high dimensions: A texture classification
example. In Proceedings of the 9th International Conference on Computer Vision. IEEE, Los Alamitos, CA, 456–463.
GOOCH, B. AND GOOCH, A. 2001. Nonphotorealistic Rendering. A. K. Peters, Natick, MA.
GOOCH, B., REINHARD, E., MOULDING, C., AND SHIRLEY, P. 2001. Artistic composition for image creation. In Proceedings Eurographics Rendering Workshop. ACM, New York, 83–88.
HAMEL, J. AND STROTHOTTE, T. 1999. Capturing AND re-using rendition styles for non-photorealistic rendering. In Proceedings
of Eurographics. IEEE, Los Alamitos, CA.
HE, L., COHEN, M. F., AND SALESIN, D. 1996. The virtual cinematographer: A paradigm for automatic real-time camera control
AND directing. In Proceedings of SIGGRAPH. ACM, New York, 217–224.
HEALEY, C. G. 1996. Choosing effective colors for data visualization. In Proceedings of the IEEE Conference on Visualization.
IEEE, Los Alamitos, CA, 263–270.
HELBING, R., HARTMANN, K., AND STROTHOTTE, T. 1998. Dynamic visual emphasis in interactive technical documentation. In
Proceedings of the European Conference on Artificial Intelligence. John Wiley and Sons, Hoboken, NJ.
JERALD, J. AND DAILY, M. 2002. Eye-gaze correction for videoconferencing. In Proceedings of the Human Factors in Computing
Systems Conference. ACM, New York, 77–81.
JI, G. AND SHEN, H.-W. 2006. Dynamic view selection for time-varying volumes. In Proceedings of the IEEE Conference on
Visualization. IEEE, Los Alamitos, CA.
KAWAI, J. K., PAINTER, J. S., AND COHEN, M. F. 1993. Radioptimization: Goal-based rendering. In Proceedings of the SIGGRAPH
Conference. ACM, New York.
KIM, S., HAGH-SHENAS, H., AND INTERRANTE, V. 2004. Conveying shape with texture: Experimental investigations of texture’s
effects on shape categorization judgments. IEEE Trans. Visual Comput. Graph. 10, 4, 471–483.
KINDLMANN, G. AND DURKIN, J. W. 1998. Semi-automatic generation of transfer functions for direct volume rendering. In Proceeding of the IEEE Symposium on Volume Visualization. IEEE, Los Alamitos, CA, 79–86.
KINDLMANN, G., REINHARD, E., AND CREEM, S. 2002. Face-based luminance matching for perceptual color map generation. In
Proceedings of the IEEE Conference on Visualization. IEEE, Los Alamitos, CA, 299–306.
KOWALSKI, M. A., HUGHES, J. F., RUBIN, C. B., AND OHYA, J. 2001. User-guided composition effects for art-based rendering. In
Proceedings of the Symposium on Interactive 3D. ACM, New York, 99–102.
KRULL, R., SHARP, M., AND ROY, D. 2003. Canonical views in procedural graphics. In Proceeding of the International Professional
Communication Conference. IEEE, Los Alamitos, CA.
LAENG, B. AND ROUW, R. 2001. Canonical views of faces AND the cerebral hemispheres. Laterality 6, 3, 193–224.
LAW, B., ATKINS, M. S., KIRKPATRICK, A. E., AND LOMAX, A. J. 2004. Eye-gaze patterns differentiate novice AND experts in a virtual
laparoscopic surgery training environment. In Proceedings of the Eye Tracking Research and Applications Symposium. ACM,
New York, 41–48.
LEVOY, M. AND WHITAKER, R. 1990. Gaze-directed volume rendering. Comput. Graph. 24, 2, 217–223.
LIVIO, M. 2002. The Golden Ratio: The Story of Phi, the World’s Most Astonishing Number. Broadway Books, New York.
LU, A., MORRIS, C., TAYLOR, J., EBERT, D., RHEINGANS, P., HANSEN, C., AND HARTNER, M. 2003. Illustrative interactive stipple
rendering. IEEE Trans. Visual Comput. Graph. 9, 2, 127–139.
MACKINLAY, J. D. 1986. Automating the design of graphical presentations of relational information. ACM Trans. Graph. 5, 2,
110–141.
MAJARANTA, P. AND RAIHA, K.-J. 2002. Twenty years of eye typing: Systems and design issues. In Proceedings of the Eye Tracking
Research AND Applications Symposium. ACM, New York.
MCGUFFIN, M., TANCAU, L., AND BALAKRISHNAN, R. 2003. Using deformations for browsing volumetric data. In Proceedings of the
IEEE Conference on Visualization. IEEE, Los Alamitos, CA, 401–408.
ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

4:20

•

A. Lu et al.

OHSHIMA, T., YAMAMOTO, H., AND TAMURA, H. 1996. Gaze-directed adaptive rendering for interacting with virtual space. In
Proceedings of the IEEE Virtual Reality Annual International Symposium. IEEE, Los Alamitos, CA, 103–110.
O’SULLIVAN, C. AND DINGLIANA, J. 2001. Collisions AND perception. ACM Trans. Graph. 20, 3.
O’SULLIVAN, C., DINGLIANA, J., AND HOWLETT, S. 2003. Eye-movements AND interactive graphics. In The Mind’s Eyes: Cognitive
AND Applied Aspects of Eye Movement Research, J. Hyona, R. Radach, AND H. Deubel, Eds., Elsevier Science, 555–571.
PRESS, W., FLANNERY, B., TEUKOLSKY, S., AND VETTERLING, W. 1992. Numerical Recipes in C: The Art of Scientific Computing.
Cambridge University Press, Cambridge, UK.
RAYNER, K. 2004. Eye-movements as reflections of perceptual AND cognitive processes. In Proceedings of the Eye Tracking
Research AND Applications Symposium. ACM, New York, 9–10.
REDDY, M. 2001. Perceptually optimized 3D graphics. IEEE Comput. Graph. Appl. 21, 5, 68–75.
RHEINGANS, P. AND LANDRETH, C. 1995. Perceptual principles for effective visualizations. Perceptual Issues Visual. 59–74.
RHEINGANS, P. AND TEBBS, B. 1990. A tool for dynamic explorations of color mappings. Comput. Grap. 24, 2, 145–146.
RIST, T., KRGER, A., SCHNEIDER, G., AND ZIMMERMANN, D. 1994. Awi: A workbench for semi-automated illustration design. In
Proceedings of the Workshop on Advanced Visual Interfaces. ACM, New York, 59–68.
SANTELLA, A. AND DECARLO, D. 2004. Visual interest AND npr: An evaluation and manifesto. In Proceedings of the Conference on
Non-Photorealistic Animation AND Rendering. ACM, New York, 71–78.
SEKULER, R. AND BLAKE, R. 1994. Perception. McGraw-Hill, New York.
SELIGMAN, D. D. AND FEINER, S. K. 1991. Automated generation of intent-based 3D illustrations. In Proceedings of SIGGRAPH.
ACM, New York.
STARKER, I. AND BOLT, R. 1990. A gaze-responsive self-disclosing display. In Proceedings of the Human Factors in Computing
Systems Conference. ACM, New York, 3–9.
STROTHOTTE, T., PREIM, B., RAAB, A., SCHUMANN, J., AND FORSEY, D. R. 1994. How to render frames AND influence people. Comput.
Graph. Forum 13, 3, 455–466.
STROTHOTTE, T. AND SCHLECHTWEG, S. 2002. Non-Photorealistic Computer Graphics: Modeling, Rendering and Animation.
Morgan Kaufmann, San Francisco, CA.
SVAKHINE, N., EBERT, D. S., AND STREDNEY, D. 2005. Illustration motifs for effective medical volume illustration. IEEE Comput.
Graph. Appl. 25, 3, 31–39.
TAKAHASHI, S., FUJISHIRO, I., TAKESHIMA, Y., AND NISHITA, T. 2005. A feature-driven approach to locating optimal viewpoints for
volume visualization. In Proceedings of the IEEE Conference on Visualization. IEEE, Los Alamitos, CA.
TANRIVERDI, V. AND JACOB, R. 2000. Interacting with eye-movements in virtual environments. In Proceedings of the Human
Factors in Computing Systems Conference. ACM, New York, 265–272.
TORY, M., ATKINS, M. S., KIRKPATRICK, A. E., NICOLAOU, M., AND YANG, G. 2005. Eye-gaze area-of-interest analysis of 2D AND 3D
combination displays. In Proceedings of the IEEE Conference on Visualization. IEEE, Los Alamitos, CA.
V’AZQUEZ, P., FEIXAS, M., SBERT, M., AND HEIDRICH, W. 2001. Viewpoint selection using viewpoint entropy. In Proceedings of
Vision, Modeling, AND Visualization Conference. 273–280.
VIOLA, I., FEIXAS, M., SBERT, M., AND GRÖ LLER, M. E. 2006. Importance-driven focus of attention. IEEE Trans.Visual. Comput.
Graph. 12, 5, 933–940.
VIOLA, I., KANITSAR, A., AND GR¨OLLER, M. E. 2004. Importance-driven volume rendering. In Proceedings of the IEEE Conference
on Visualization. IEEE, Los Alamitos, CA, 139–145.
ZLATANOVA, S., RAHMAN A.A., AND SHI W. 2002. Topology for 3D spatial objects. In Proceedings of the International Symposium
AND Exhibition on Geoinformation.
ZWICKER, M., PFISTER, H., VAN BAAR, J., AND GROSS, M. 2001. EWA volume splitting. In Proceedings of the IEEE Conference on
Visualization. IEEE, Los Alamitos, CA, 29–36.

Received August 2008; revised June 2008, November 2008; accepted November 2008

ACM Transactions on Applied Perception, Vol. 7, No. 1, Article 4, Publication date: January 2010.

Development of a Mobile User Interface for Image-based
Dietary Assessment
SungYe Kim† , TusaRebecca Schap‡ , Marc Bosch† , Ross Maciejewski†
Edward J. Delp† , David S. Ebert† , Carol J. Boushey‡
†

Electrical and Computer Engineering, ‡ Foods and Nutrition
Purdue University, West Lafayette, IN, USA
{inside, tschap, mboschru, rmacieje, ace, ebertd, boushey}@purdue.edu

ABSTRACT
In this paper, we present a mobile user interface for imagebased dietary assessment. The mobile user interface provides a front end to a client-server image recognition and
portion estimation software. In the client-server configuration, the user interactively records a series of food images
using a built-in camera on the mobile device. Images are
sent from the mobile device to the server, and the calorie
content of the meal is estimated. In this paper, we describe
and discuss the design and development of our mobile user
interface features. We discuss the design concepts, through
initial ideas and implementations. For each concept, we discuss qualitative user feedback from participants using the
mobile client application. We then discuss future designs,
including work on design considerations for the mobile application to allow the user to interactively correct errors in
the automatic processing while reducing the user burden associated with classical pen-and-paper dietary records.

Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous;
H.5.2 [User Interfaces]: Mobile Dietary System

Keywords
Mobile user interfaces, User interface design, Mobile devices,
Health monitoring tools, Dietary assessment system.

1.

INTRODUCTION

As smart telephone technology continues to advance in
sensor capability and computational efficiency, mobile telephones are being used for purposes other than just dialing
and receiving calls. Advances in mobile technology have led
to enhanced features, such as built-in cameras, games and
texting on phones. Furthermore, researchers have employed
mobile technologies in a variety of tools ranging in applications from personal health and wellness to biological research

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
MUM ’10, December 1-3, 2010, Limassol, Cyprus
Copyright 2010 ACM 978-1-4503-0424-5/10/12 ...$10.00.

to emergency response [3, 6, 14, 29]. Our research focuses
on employing mobile technology for dietary assessment.
Given the mounting concerns of childhood and adult obesity [20, 21], advanced tools for keeping personal dietary
records are needed as a means for accurately and effectively
monitoring energy and nutrient intakes. Such records provide researchers with information for hypothesis generation
and assessment regarding health conditions and nutrient imbalances. In light of these issues, research has focused on a
means to utilize mobile technology for dietary assessment
(e.g., [10, 15, 22, 27]). Previously, we presented a dietary
record assessment system [18] focused on creating a serverclient architecture in which the client mobile device was used
to capture a pair of images (before and after) of meals and
snacks. In our system design, the images are sent to the
server where image analysis and volume estimation calculations are performed and the results are returned to nutrition
researchers (or dietitians). In this paper, we discuss the design and development of the mobile interface for our clientserver system. The development of the mobile user interface
of our dietary assessment system has been a collaboration
with researchers in the Department of Foods and Nutrition.
We intersperse our discussion with lessons learned based on
user feedback, providing guidelines for the development of
such applications. Compared to our previous papers, this is
the first time our mobile application is described in depth.
Contributions of this paper include:
• Field specific design considerations and strategies: We
describe design considerations for our mobile user interface for dietary assessment and development strategies based on user evaluation.
• Improved user interface: From the point of view of an
integrated system, we present details of the data that
is created in the mobile application and describe the
communication protocol between the mobile client to
the server.
• Flexible eating patterns: We extend a previous design
for recording food images to support flexible eating
patterns such as portion refills so that each eating occasion can include not only a pair but also a series of
food images.
• Minimal data exposure and user interaction: Through
filtering eating occasions, we display only some of the
images containing information that needs user confirmation. Such a process is used to minimize user interaction, thus reducing user burden.

This paper is organized as follows: Section 2 discusses the
related research. Section 3 presents our image-based mobile
dietary assessment system and the design considerations of a
mobile user interface. Section 4 presents the framework and
functionality of our mobile dietary assessment tools. Finally,
Section 5 presents conclusions and discussion.

2.

RELATED WORK

Many systems incorporating mobile devices (e.g., PDAs,
cell phones) for data collection and management, have been
proposed in various fields [7, 12, 15, 28, 29]. Denning et al.
[9] proposed a wellness management system using mobile
phones and a mobile sensing platform unit. In this system,
users manually input food consumption information with a
mobile telephone. The mobile sensing unit computes the
user’s energy expenditure and transfers this data to the mobile telephone. Arsand et al. [4, 5] studied how to design
easily usable mobile self-monitoring tools for diabetes.
Other dietary assessment systems also make use of mobile phones. Wang et al. [27] describes the evaluation of
personal dietary intake using mobile devices with a camera. Users took food images and sent them to dietitians by
a mobile telephone. Reddy et al. [24] introduced the DietSense architecture that consists of mobile phones, a data
repository, data analysis tools, and data collection protocol management tools. Mobile phones with a GPS receiver
were used to capture food images as well as relevant context,
such as audio, time and location. The food images were collected into a data repository to be analyzed by dietitians.
Donald et al. [10] discussed the effect of mobile phones in
dietary assessment by performing a user study with adolescent patients with diabetes. In their work, users were asked
to complete food diaries and to capture images of all food
and drink by using mobile phones. Then, food images were
downloaded to a computer and a dietitian estimated portion
sizes from the diaries and images.
In first developing our mobile user interface, we looked
at research which focused on general guidelines and requirements for mobile interface design. Nilsson [19] presented
user interface patterns for mobile applications and discussed
a series of challenges regarding screen space and interaction
mechanisms. Gong and Tarasewich [13] discussed design requirements for mobile user interfaces compared to those for
the desktop environment based on Shneiderman’s Golden
Rules of Interface Design [25] and proposed guidelines for
mobile devices such as consistency for a desktop counterpart and design for top-down interaction. Chong et al. [8]
focused on designing a touch screen based user interface. In
this work, they proposed several key guidelines influencing
a good user interface including; simplicity, aesthetics, productivity, and customizability.
We also examined research [4, 5, 16] focusing specifically
on design concepts for self-health monitoring tools utilizing the capability of mobile devices. Designing field specific
interfaces requires careful consideration and even field background knowledge [17]. Based on our initial system prototype and user feedback, we adopted design concepts proposed by Arsand et al. [4]. Our system design was done
in conjunction with interactive feedback from nutrition researchers. Our system was developed to provide researchers
with an automated analysis for the estimation of daily food
intake. The dietary assessment tool uses a built-in camera on a mobile telephone to capture food images, and the

Figure 1: Overview of an image-based dietary assessment system in a server-client architecture.
user interactions of taking meal images and verifying the assessment demands careful design considerations in terms of
usability and flexibility in order to reflect various situations
in daily life. Specifically, we incorporate an automatic data
transfer scheme (hiding the data transfer paradigm from the
users) and an easy data entry scheme through the use of the
mobile telephone camera.

3.

SYSTEM DESIGN

The focus of our dietary assessment system is to better
capture meal patterns of users using automated food and
portion analysis. Our dietary assessment system utilizes a
server-client configuration.

3.1

System Overview

In the record method, food images are captured using a
built-in camera on a mobile device and the corresponding
management data for food images is generated. The captured food images are transferred to a server for analysis
where the various types of food in the images are identified and the portion sizes (volumes) of food are estimated
to compute the amount of food consumed by the user. This
analysis part is broken into two sub parts; food identification [31] and volume estimation [30]. These results (food
type and volume) are returned to the user for review. In
the review method, the user confirms or adjusts the results
produced in the server. Figure 1 shows the overview of our
image-based dietary assessment system. Additionally, an alternate method is considered for cases where the user cannot
capture food images.
The record and review methods and the alternate method
are deployed on client mobile phones. The analysis components run on the server to minimize the burden of computation on the mobile telephone. This paper focuses on the
client side of the system and we present the design and development of the mobile user interfaces for an image-based
dietary assessment system. For details of the analysis components, see Zhu et al. [31] and Woo et al. [30].

3.2

Design Considerations for Mobile Dietary
Assessment Tools

Figure 2: (Left) the framework of our mobile dietary assessment tool and (right) an eating occasion structure
consisting of a series of food images, eating occasion data and food tags. The framework is broken into two
tracks; one uses eating occasions with food images and the other deals with eating occasions without food
images. Each food tag includes food labels and volumes. Note the dashed structure in the eating occasion
diagram. This was part of our initial concept framework and was later removed based on user feedback.
Please see Section 4.1 for a detailed discussion of the design concept and lessons learned.
Based on design discussions with our nutrition research
partners and analysis of various eating patterns in real life,
we derived several design considerations for our mobile dietary assessment tools.
1) Easy and fast collection of daily food images: Capturing
food images by using a mobile telephone with a builtin camera provides users with easier, faster and potentially more accurate ways to record meals in comparison
to methods of remembering or noting what they ate [1].
Thus, we chose to utilize the built-in cameras on mobile
phones as a data collection tool.
2) Minimum user interaction: Since users of a dietary assessment system are as various as users of mobile phones,
we need to consider a wide range of users from adolescents to senior citizens. Hence, the user interface should
be easy to use and the interaction needed to record or
review eating occasions should be minimal.
3) Flexible eating patterns: The system should be flexible
enough so that various eating patterns in real life, such
as multiple eating occasions per day and the addition
of foods during an eating occasion, can be collected and
managed in the system.
4) Protection of personal data: Users’ food images and additional private data should be kept private. Hence, food
images on the mobile telephone should be hidden from
other people.
5) Automatic data processing: A server-client architecture
reduces the burden of analysis and computation on the
mobile telephone and increases the overall system performance. While employing this, the data transfer between

Figure 3: Data communication between a server and
mobile applications (record and review) based on
Figure 1. In this figure, step 5 occurs from users’
interaction, and the other steps are automatically
processed. On the right side, an example database
on the server is presented.
a mobile telephone and a server should be hidden from
the users. Moreover, users do not need to know details
of server processing.
6) Exceptional situations: Additional methods should provide users with tools to manually save or modify eating occasions. For instance, in cases that users cannot
capture food images due to situational or environmental conditions, users should be able to create their eating
occasion data without food images. Moreover, if the results from the analysis components contain errors, users
should also be able to correct them and update the server.

4.

MOBILE USER INTERFACE FOR
IMAGE-BASED DIETARY ASSESSMENT
SYSTEM

Based on the design considerations in Section 3.2, we describe the framework of our mobile dietary assessment tool.
In this section, we describe our system tools and implementations. Furthermore, we discuss lessons learned and present
qualitative user feedback on each of the deployed components. User studies have included 117 adolescents (10∼18
years) and 57 adults (21∼65 years).
Figure 2 shows the framework for our mobile interface and
the basic structure of an eating occasion. As shown in Figure 2 (left), our mobile tool includes three main methods;
the record, review and alternate method. The record and review methods share eating occasion data with food images,
whereas the alternate method supports users in creating eating occasions even when they cannot capture food images.
Each eating occasion may contain several food images, eating occasion data created by the record method, and food
tags including food labels and volumes as shown in Figure
2 (right).
Due to programming constraints (i.e., no multi-threading)
with iOS 3.x and earlier, we separated the record and review
methods into two different applications. Eating occasion
data is created in the record method in one application and
transferred to the server. The review method in the other
application receives the eating occasion data from the server
after completing the analysis on the server. User feedback
indicated that such a separation was sub-optimal. Fortunately, the new iOS 4.x allows an application to remain in
the background by providing multi-threading so that data
communication between a mobile phone and a server will be
available without such separation. Figure 3 shows a diagram
for data communication between the server and the separated two record and review applications and an example
database on the server. In the following sections, we present
the detailed functionality, development, lessons learned and
user feedback of each method for our mobile dietary assessment.

4.1

Recording the Eating Occasion

By utilizing the record method, the user captures food images with the built-in camera on the mobile device. While
recording food images, the eating occasion data for the food
images (*.rec) (see Figure 2 (right)) is generated for each
eating occasion containing the device ID, user ID, date/time
and file names of all food images. The date and time represent when the first image was captured. All food images
and eating occasion data are transferred to the server (step
1 in Figure 3). The guide lines denoting the top and bottom
of the image, superimposed on the camera view were key to
helping users fit foods into the screen. More importantly,
the top and bottom notation insured correct orientation of
a mobile phone. Prior to implementing this feature, images
were captured in a variety of orientations, confounding image segmentation and volume estimation.
Handling various eating patterns: To record and
manage multiple helpings (portion refills), we allowed users
to capture a series of images for a given eating occasion.
Each image in the sequence was a before or after image except for the first image (which can only be a before image)
and the last image (which can only be an after image). All

these images were then saved in the order of their capture
time to reflect the time progression of the eating occasion.
Figure 4 shows the food image recording interface that was
composed of three buttons. Capturing the first image creates a new eating occasion and capturing the last image
sends the current eating occasion to the server for analysis.
To implement this concept, we included three buttons:
First, In-Between and Last. Adolescent users found the InBetween button to be confusing. Many users interpreted it
to mean they should take an image mid-way through their
meals. Therefore, the button was disabled and a series of
First/Last images were used to accommodate portion refills
and multiple courses. This sequencing was well received.
Recommendations for training and recording: The
user interface development and available technology can provide many solutions to aid in the automated analysis. However, some issues must be addressed through user training.
Suggestions to the users for improved training and recording
include:
1) Full image acquisition: Foods should be captured large
enough and around the center area in the images so that
they can easily be recognized during analysis.
2) Highlight and shadow condition: Highlights and shadows
hinder the automatic analysis by making the recognition
of food areas difficult.
3) Spatial consistency: When several foods exist in the images, keeping them at the same position helps the analysis part to automatically recognize foods in different images of the same eating occasion.
4) Camera setup: Significant perspective distortion can occur in the image according to the distance and the angle of view. We experimentally found angles between
30∼45 ◦ to be satisfactory.

4.2

Reviewing the Eating Occasion

After recording eating occasions, the users review the analyzed results from the server to either confirm or change
food types (food names) and volumes (the amount of intake for each food). As shown in Figure 1, the analysis is
automatically decided by the food identification and saved
into food label data (*.tag), and the food volume is algorithmically estimated in the volume estimation and saved into
food volume data (*.vol) to be returned to the users. The
purpose of this review method is to provide user interfaces
on mobile phones for reviewing the data transferred from
the server. When the user starts the review method, an application requests the eating occasion data of food images
generated in the record method and analysis results to the
server (steps 4-1 and 4-2 in Figure 3).
Filtering eating occasions: First, a list of eating occasions are displayed so that the users can select one eating
occasion to review as shown in Figure 5. At this moment,
we delay loading the food images and related context information (i.e., food tag data) on the memory of the mobile device until the user selects one to review among the
eating occasions. This lazy loading [11] strategy is applied
to all the other methods to reduce the memory consumption on the mobile device. Conceptually, users view images
they recorded so that they can review them and make adjustments or corrections as necessary to the foods’ type or
volume.

Figure 4: User interface for the record method.
The dot indicators were proposed to remind users
of which image has been taken. In this example,
four red dots indicate that the first image and three
in between images have been taken. The right most
grey dot among the dots shows that the last image
has yet to be taken.

If the list of eating occasions displayed includes all eating occasions recorded in the record method, it could be
extremely long after continuous usage. Although performance issues of handling long lists could be averted with
efficient coding of the application, the concern is that the
users would have to scroll through the long list of eating occasions in order to look for a specific one. Hence, we filter
eating occasions to display on the review method using three
strategies:
1) When all the food tags in an image have been confirmed
by the user, the corresponding image should be hidden
from the eating occasion.
2) When all the images in the eating occasion have been
confirmed by the user, the corresponding eating occasion
is hidden from the user. (The eating occasion is not displayed on the list anymore.)
3) Eating occasions whose images are yet to be processed
by the server are not displayed on the list.
Displaying food tags: Once the user selects an eating
occasion, food images with food tags (food labels) are displayed in order. Figure 6 (left) shows the image with food
labels. Each label is transparent to reduce occluding food
areas and is matched to the food where the pin icon with the
same color is placed. In the case that there are several food
labels, the positions of the labels are rearranged to minimize
cluttering.
User confirmation and adjustment: In the case that
the analysis results from the server contain errors, we engage
the confirmation and adjustment of the food tags by the
users (step 5 in Figure 3). The confirmation reports to the
server that all food tags in the image are correct and updates
the server when food tags are changed (step 5-1 in Figure

Figure 5: A list of eating occasions in the review
method with a thumbnail, date, time, the number
of food images with food tags and the number of
confirmed images in each eating occasion.

3), and the adjustment allows users to locally make changes
to food labels and volumes (step 5-2 in Figure 3).
To correct wrong food labels in the food tags, we employ the stemming algorithm [23] for keyword search, simplified for the Food and Nutrient Database for Dietary Studies
(FNDDS) 3.0 [2]. However, utilizing general keyword search
for food data often produces undesired results. For example,
keyword search for ‘apple’ returns a list of 158 food names
from the FNDDS 3.0 and the raw apple is buried at least 50
foods deep. Hence, another potential way to search foods is
to use a set of predefined keywords. Since defining search
keywords for foods requires domain specific knowledge and
user evaluation, our nutrition colleagues have been helped
to define the keywords. In future versions of our user interface, new search method will be incorporated using these
predefined keywords. Further, feedback from adults and
adolescents also indicated that a search mechanism needs
to accommodate misspellings and synonym names for foods.
Integration of these concepts will improve the efficiency of
the tool used to search for foods.

4.3

Alternate Method

The main concept of our dietary assessment system is to
utilize food images recorded by the users during eating occasions. However, it may, at times, be impossible for users to
take food images. For instance, the user may not have had
his or her mobile telephone or may have forgotten to take
food images. To support such situations, we also provide
an alternate method that is based on user interaction and a
manual food search.
This alternate method is developed as an independent
track in the framework of the client side as shown in Figure 2 (left) and provides users with tools for creating eating
occasions containing enough information for nutrient analysis, including date and time, food name, measure description and the amount of intake as shown in Figure 7. In this
method, food names are searched by using the same method
employed for the user confirmation in the review method.

Figure 7: An alternate method. (Left) a list of eating occasions without food images, (middle) food
search results, and (right) the settings of food portion unit and the amount of food consumed.
Figure 6: Displaying food tags. (Left) food labels on
an image and (right) a 3D food volume shape. The
3D shape of food volume and corresponding user
interfaces for translating (green arrows) and scaling
(green minus and plus) are displayed when the user
selected a specific food tag.

The portion units for the amount consumed depend on the
food that the user selects and come from FNDDS 3.0. From
user feedback, users have expressed that the portion units
need to be polished (i.e., the number ‘1’ in front of each portion unit is confusing (Figure 7 (right))). This is an issue
linked to modification of the FNDDS rather than user interface design. Our nutrition colleagues have also been worked
to update the FNDDS. Thus, in our future versions of the
alternate method, this issue will be improved.

5.

CONCLUSION AND DISCUSSION

We have described the design and development of a mobile user interface for an interactive, image-based dietary assessment system using mobile devices. As a self-monitoring
system for dietary assessment, our system uses food images
captured using a built-in camera on mobile devices and analyzes the images on the server in order to automatically label
foods and estimate the amount of food consumed. For flexibility, our system also considers an alternate method based
on user interaction and food search instead of food images.
Hence, our mobile client system using mobile phones is composed of the record, review and alternate methods.
The current user interface design considerations are used
to handle capturing meal images and reporting mistakes during the automatic food identification. We have begun work
on implementing user controls for also adjusting errors in
the volume estimation. As an initial idea, we are currently
proposing a system that will provide 3D primitive template
shapes and interactions for translating, rotating and scaling
the overlayed volumes. Figure 6 (right) shows the image
with cylindrical food volume. A transparent 3D shape is
superimposed on the food area to provide users with tools
to either confirm or adjust the food volumes estimated on
the server. User interfaces for translating and scaling the
3D shape are displayed when the user selects a specific food

tag in order to minimize cluttering the screen.
While designing and developing our mobile user interface,
we collaborated with researchers in the Department of Foods
and Nutrition to collect feedback on the interaction design
through several user studies [26]. Based on the feedback,
we tailored the user interface of our client mobile system for
improving usability as well as system flexibility. To demonstrate, we developed and deployed our client mobile tools on
the iPhone 3GS devices running iOS 4.0.

6.

ACKNOWLEDGMENTS

We would like to thank our collaborators and colleagues,
Bethany Six of the Department of Foods and Nutrition and
Deb Kerr of the School of Public Health, Curtin Institute of
Technology for feedback about mobile user interfaces through
user studies, Insoo Woo, JungHoon Chae, Fengqing Zhu of
the School of Electrical and Computer Engineering for their
work in developing food volume estimation and food image
analysis on the server and Ahmad M. Razip of the School
of Electrical and Computer Engineering for his help in revising the user interface. Support for this work comes from
the National Cancer Institute (1U01CA130784-01) and the
National Institute of Diabetes, Digestive, and Kidney Disorders (1R01-DK073711-01A1). More information about our
project can be found at www.tadaproject.org.

7.

REFERENCES

[1] [Diet History Questionnaire: Web-based DHQ,
National Cancer Institute]
http://riskfactor.cancer.gov/DHQ/webquest/
(accessed 10 September 2010).
[2] [The Food and Nutrient Database for Dietary Studies]
http://www.ars.usda.gov/services/docs.htm?
docid=7673 (accessed 10 September 2010).
[3] A. Ahtinen, M. Isomursu, M. Mukhtar, J. Mäntyjärvi,
J. Häkkilä, and J. Blom. Designing social features for
mobile and ubiquitous wellness applications. In MUM
’09: Proceedings of the 8th International Conference
on Mobile and Ubiquitous Multimedia, pages 1–10,
New York, NY, USA, 2009. ACM.
[4] E. Arsand, J. T. Tufano, J. D. Ralston, and
P. Hjortdahl. Designing mobile dietary management

[5]

[6]

[7]

[8]

[9]

[10]

[11]
[12]

[13]

[14]

[15]

[16]

[17]

[18]

support technologies for people with diabetes. In
Journal of Telemedicine and Telecare, volume 14,
pages 329–332, July 2008.
E. Arsand, R. Varmedal, and G. Hartvigsen. Usability
of a mobile self-help tool for people with diabetes: the
easy health diary. In Automation Science and
Engineering, 2007. CASE 2007. IEEE International
Conference on, pages 863–868, Sept. 2007.
O. Baecker, T. Ippisch, F. Michahelles, S. Roth, and
E. Fleisch. Mobile claims assistance. In MUM ’09:
Proceedings of the 8th International Conference on
Mobile and Ubiquitous Multimedia, pages 1–9, New
York, NY, USA, 2009. ACM.
D. C. Baumgart. Personal digital assistants in health
care: experienced clinicians in the palm of your hand.
In Lancet, volume 366, pages 1210–22, Oct. 2005.
P. Chong, P. So, P. Shum, X. Li, and D. Goyal. Design
and implementation of user interface for mobile
devices. Consumer Electronics, IEEE Transactions on,
50(4):1156 – 1161, November 2004.
T. Denning, A. Andrew, R. Chaudhri, C. Hartung,
J. Lester, G. Borriello, and G. Duncan. Balance:
towards a usable pervasive wellness application with
accurate activity inference. In HotMobile’09:
Proceedings of the 10th workshop on Mobile
Computing Systems and Applications, pages 1–6, 2009.
H. Donald, V. Franklin, and S. Greene. The use of
mobile phones in dietary assessment in young people
with type 1 diabetes. In Journal of Human Nutrition
and Dietetics, volume 22, pages 256–257, June 2009.
M. Fowler. Patterns of Enterprise Application
Architecture. Addison-Wesley, 2002.
D. Gammon, E. rsand, O. A. Walseth, N. Andersson,
M. Jenssen, and T. Taylor. Parent-child interaction
using a mobile and wireless system for blood glucose
monitoring. In Journal of Medical Internet Research,
volume 7, page e57, Nov. 2005.
J. Gong and P. Tarasewich. Guidelines for handheld
mobile device interface design. In In Proceedings of the
2004 DSI Annual Meeting, pages 3751–3756, 2004.
S. Kim, R. Maciejewski, K. Ostmo, E. J. Delp, T. F.
Collins, and D. S. Ebert. Mobile analytics for
emergency response and training. Information
Visualization, 7(1):77–88, 2008.
M. J. Kretsch, C. A. Blanton, D. Baer, R. Staples,
W. F. Horn, and N. L. Keim. Measuring energy
expenditure with simple low cost tools. In Journal of
the American Dietetic Association, volume 104,
page 13, Aug. 2004.
T.-H. Lee, H.-J. Kwon, D.-J. Kim, and K.-S. Hong.
Design and implementation of mobile self-care system
using voice and facial images. In ICOST ’09:
Proceedings of the 7th International Conference on
Smart Homes and Health Telematics, pages 249–252,
Berlin, Heidelberg, 2009. Springer-Verlag.
J. Lumsden. Handbook of Research on User Interface
Design and Evaluation for Mobile Technology
(2-Volume Set). Information Science Reference, 1
edition, January 2008.
A. Mariappan, M. B. Ruiz, F. Zhu, C. J. Boushey,
D. A. Kerr, D. S. Ebert, and E. J. Delp. Personal
dietary assessment using mobile devices. In

[19]

[20]

[21]

[22]

[23]
[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

Proceedings of the IS&T/SPIE Conference on
Computational Imaging VII, volume 7246, pages
72460Z–72460Z–12, Jan. 2009.
E. G. Nilsson. Design patterns for user interface for
mobile applications. In Advances in Engineering
Software, volume 40, pages 1318–1328, December
2009.
C. L. Ogden, M. D. Carroll, L. R. Curtin, M. A.
McDowell, C. J. Tabak, and K. M. Flegal. Prevalence
of overweight and obesity in the united states,
1999-2004. In JAMA: the journal of the American
Medical Association, volume 295, pages 1549–55, Apr.
2006.
C. L. Ogden, M. D. Carroll, and K. M. Flegal. High
body mass index for age among us children and
adolescents, 2003-2006. In JAMA: the journal of the
American Medical Association, volume 299, pages
2401–5, May 2008.
K. Patrick, W. G. Griswold, F. Raab, and S. S. Intille.
Health and the mobile phone. In American Journal of
Preventive Medicine, volume 35, pages 177–181, 2008.
M. F. Porter. An algorithm for suffix stripping. pages
313–316, 1997.
S. Reddy, A. Parker, J. Hyman, J. Burke, D. Estrin,
and M. Hansen. Image browsing, processing, and
clustering for participatory sensing: lessons from a
dietsense prototype. In EmNets ’07: Proceedings of the
4th workshop on Embedded networked sensors, pages
13–17, 2007.
B. Shneiderman. Designing the User Interfae Strategies for Effective Human-Computer Interaction.
1998.
B. L. Six, T. E. Schap, F. M. Zhu, A. Mariappan,
M. Bosch, E. J. Delp, D. S. Ebert, D. A. Kerr, and
C. J. Boushey. Evidence-based development of a
mobile telephone food record. 110(1):74 – 79, 2010.
D. H. Wang, M. Kogashiwa, and S. Kira. Development
of a new instrument for evaluating individuals’ dietary
intakes. In Journal of the American Dietetic
Association, pages 1588–1593, 2006.
D. H. Wang, M. Kogashiwa, S. Ohta, and S. Kira.
Validity and reliability of a dietary assessment
method: the application of a digital camera with a
mobile phone card attachment. In Journal of
Nutritional Science and Vitaminology, volume 48,
pages 498–504, Dec. 2002.
S. M. White, D. Marino, and S. Feiner. Designing a
mobile user interface for automated species
identification. In CHI ’07: Proceedings of the SIGCHI
conference on Human factors in computing systems,
pages 291–294, New York, NY, USA, 2007. ACM.
I. Woo, K. Ostmo, S. Kim, D. S. Ebert, E. J. Delp,
and C. J. Boushey. Automatic portion estimation and
visual refinement in mobile dietary assessment. In
Proceedings of the IS&T/SPIE Conference on
Computational Imaging VIII, volume 7533, pages
75330O–75330O–10, January 2010.
F. Zhu, M. Bosch, I. Woo, S. Kim, C. J. Boushey,
D. S. Ebert, and E. J. Delp. The use of mobile devices
in aiding dietary assessment and evaluation. Selected
Topics in Signal Processing, IEEE Journal of, 4(4):756
–766, aug. 2010.

Volume Estimation Using Food Speciﬁc Shape Templates in Mobile
Image-Based Dietary Assessment
Junghoon Chaea , Insoo Wooa , SungYe Kima , Ross Maciejewskia , Fengging Zhua , Edward J. Delpa ,
Carol J. Bousheyb , David S. Eberta
a School

of Electrical and Computer Engineering
of Foods and Nutrient
Purdue University, West Lafayette, Indiana USA
b Department

ABSTRACT
As obesity concerns mount, dietary assessment methods for prevention and intervention are being developed. These
methods include recording, cataloging and analyzing daily dietary records to monitor energy and nutrient intakes. Given
the ubiquity of mobile devices with built-in cameras, one possible means of improving dietary assessment is through
photographing foods and inputting these images into a system that can determine the nutrient content of foods in the images.
One of the critical issues in such the image-based dietary assessment tool is the accurate and consistent estimation of food
portion sizes. The objective of our study is to automatically estimate food volumes through the use of food speciﬁc shape
templates. In our system, users capture food images using a mobile phone camera. Based on information (i.e., food name
and code) determined through food segmentation and classiﬁcation of the food images, our system choose a particular food
template shape corresponding to each segmented food. Finally, our system reconstructs the three-dimensional properties
of the food shape from a single image by extracting feature points in order to size the food shape template. By employing
this template-based approach, our system automatically estimates food portion size, providing a consistent method for
estimation food volume.
Keywords: Food volume estimation, shape templates, dietary assessment, mobile image

1. INTRODUCTION
With growing concerns about adolescent and adult obesity and other health problems related to diet,1, 2 education programs
for obesity prevention have been developed to inform the general populace of health risks of being overweight and encourage healthy eating patterns.3 One such means of tracking and analyzing eating patterns is through dietary assessments.
Dietary assessment methods provide researchers with valuable information needed for assessment and hypothesis generation regarding dietary imbalance.4 The dietary assessment methods include recording, cataloging and analyzing daily
dietary records to monitor energy and nutrient intakes in a self-monitoring environment. One means of improving the data
collected in these self assessed environments is through the use of technology. Technology assisted assessment is valid,
feasible and promising; it can improve dietary assessment quality, and reduce the burden on both users and dietitians.5, 6
Given the ubiquity of mobile devices with built-in cameras, one possible means of improving dietary assessment is
through photographing foods and inputting these images into a system that can determine the nutrient content of foods in
the images.7, 8 Such image-based nutrient analysis methods have focused on food identiﬁcation and classiﬁcation,9 whereas
little research has been done on automated food volume estimation because of the difﬁculty of obtaining accurate volume
estimates from a single image.10 In addition, image-based food volume estimation is highly affected by the prerequisite
image analysis, such as the segmentation of food regions and extraction of geometric parameters to reconstruct 3D shapes.
Particularly, false segmentation caused by shadows and reﬂections in an image and noise along the segmentation boundaries
can seriously deteriorate the accuracy of volume estimates.
In our previous work,11 we proposed an automated volume estimation method for approximating food volumes with
3D primitive shapes reconstructed from a single image and demonstrated that volume estimates can be improved through
user interaction. Compared to our previous work, this study employs food speciﬁc shape templates to support a variety of
food shapes. Furthermore, we improve the accuracy of volume estimation by minimizing the false-segmented regions and
smoothing the segmentation boundaries of foods.
This work was sponsored grants from the National Institutes of Health under grants NIDDK 1R01DK073711-01A1 and NCI
1U01CA130784-01. Address all correspondence to David S. Ebert, ebertd@purdue.edu or see www.tadaproject.org

Computational Imaging IX, edited by Charles A. Bouman, Ilya Pollak, Patrick J. Wolfe,
Proc. of SPIE-IS&T Electronic Imaging, SPIE Vol. 7873, 78730K · © 2011 SPIE-IS&T
CCC code: 0277-786X/11/$18 · doi: 10.1117/12.876669
SPIE-IS&T/ Vol. 7873 78730K-1
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

Figure 1. Overview of our food volume estimation using food speciﬁc shape templates.

2. RELATED WORK
Previously, many approaches have tried to solve shape matching problems: measuring the similarity and ﬁnding a set
of correspondences between shapes using geometric information.12–15 In template based matching approaches, Berg and
Malik14 try to solve the problem of ﬁnding points correspondences in images through robust template matching. Gavrila15
employed a template based tree structure to take into account coarse and ﬁne level shape matching. For local feature based
shape matching methods, Belongie et al.12 considers neighborhood points on the contour of a shape for efﬁcient 2D shape
matching. However, the purely geometric information used in the previous template based matching works is not sufﬁcient
for volume estimation of complex real world food-shapes. To recover deﬁcient information, our shape template approach
allows us to apply multiple feasible algorithms and various adjustable methods.
Geometric shape analysis methods can be classiﬁed according to different criteria. Pavlidis16 propose two classiﬁcations: the use of the interior (global) and the boundary (local) of a shape as opposed to the interior of the shape. The
interior and boundary shape analysis methods include medial axis and mathematical morphology, respectively. In this
work both the medial axis and mathematical morphology methods are used to increase the accuracy of volume estimation.
Mathematical morphology, as a set of mathematical tools for image analysis, has been used for geometric manipulation of
boundaries of an object.17, 18 Morphological opening and closing are useful in the smoothing of not only binary images
(the methods were originally developed based on binary images), but also multi-type images.19–21 Peters22 describes a new
morphological image cleaning algorithm for noise reduction, while preserving thin features in gray-scale images. Yang et
al.23 propose a method for image processing including dilation, erosion and edge extraction and noise reduction in binary
morphology. We use binary mathematical morphology to make a segmented image appear smooth for removing false
information in an image.
In computer vision, some approaches have proposed hierarchical skeletal shape descriptions for topological shape
matching using the medial axis.24–26 Ho et al.27 utilized the medial axis for shape smoothing. In our implementation the
medial axis is used for global geometric feature extraction to support robust volume computation.

3. FOOD SPECIFIC SHAPE TEMPLATE FOR FOOD VOLUME ESTIMATION
The goal of our work is to obtain the most accurate volume estimation of food from a single image using shape templates
while minimizing user interactions. The underlying concept of our template based approach is to factor in the geometric
properties of the segmented region using speciﬁc template shapes associated with the extracted food name/code.
Our shape template based approach consists of camera calibration, false information removal, feature extraction, and
3D shape reconstruction to estimate food volumes as illustrated in Figure 1. We use two images (one is a meal image taken
by the user and the other is the segmented region of a food obtained using Zhu et al.’s10 approach) and food information
(food name and code obtained from image segmentation and classiﬁcation methods10 ) as inputs. In the camera calibration

SPIE-IS&T/ Vol. 7873 78730K-2
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

Figure 2. Our false information removal using binary morphology. When considering every three images as a group, from left to right in
each group, an original food image, a segmented image and our smoothed result of the segmented image. Each group shows full poured
milk, half poured milk and half poured apple juice from the left to the right.

step, using the original food image including a ﬁducial marker, camera parameters are calibrated to reconstruct an actualsized 3D shape of a food.11 Given the food name and code from the food classiﬁcation, we associate each food with a
template shape. For example, a glass of milk would correspond to a generalized cylindrical shape, an orange to spherical
one, etc. Once we ﬁnd a best-matched template shape, we minimize the false information in the segmented region to
improve the next step, feature extraction. We extract feature points from the optimized segmented region using our feature
point detection algorithm and shape analysis techniques: medial axis and active contour. The extracted feature points are
then used to determine geometric information for a food, such as the height, radius and area. Finally, we reconstruct the
3D shape to compute the food volume using geometric information determined in the previous step.
By applying this template-based approach, we are able to automatically estimate portion sizes from food images. This
reduces the burden of users having to estimate portions consumed. Furthermore, our approach allows us to provide a
consistent method for supporting various kinds of food. In this work, we focus on a particular subset of shapes for volume
estimation including a cylinder and generalized extruded solids with ﬂat tops and bottoms (e.g., bread slice).

3.1 False Information Removal
Automatic image segmentation generally includes undesired noise, such as holes, gaps, and bulges, or else the segmented
may exclude necessary features. Such segmentation errors would cause errors in our feature extraction because noise along
the boundaries of a segmented region would deform the original shape of a food. Since our feature extraction algorithm
depends on the quality of the segmented image, we improve the segmented region of a food by removing false information
such as noise on the boundaries. To ﬁlter the noise, we use mathematical binary morphology. Hence, from the segmented
image (as shown in Figure 2, the middle image in each group) we generate a smoothed version of the segmented images
(as shown in Figure 2, the right image in each group) note now unnecessary gaps and perturbations are removed.

3.2 Feature Extraction
Given a well-qualiﬁed segmented image, our algorithm extracts concrete features to size a food template shape. Although
these features depend on template shapes, in our method, they contain three feature points and an average width obtained
from the segmented image as well as interior edges from the original food image. These detected features determine the
actual size of geometric elements to compute an estimated food volume.
Feature Points Extraction: To compute the shape of a food, we need at least three feature points: two points to
estimate the width of the bottom area and one point to ﬁnd a height, as shown in Figure 3 (right) (see the three red dots).
To ﬁnd these points, we extract the contour of a segmented region and trace points (pi = (xi , yi )) on the contour lines in
order to detect the three points with high curvature. In Equation 1, the curvature Ci on the point pi is computed from the
difference between two standard deviations, Sxi and Syi , on the point pi . To control the effect of change in the x coordinate,
we used a weight value, λ , depending on a template shape. For the result in Figure 3 (right), we used λ = 1.5. When the
curvature Ci is higher than 1.0, we select the point pi as a feature point.

Ci

=
=

Syi − λ Sxi


1 K
1 K
1
2
k
(yi − yi ) − λ
∑
∑ (xi k − xi 1 )2
K k=1
K k=1

SPIE-IS&T/ Vol. 7873 78730K-3
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

(1)

Figure 3. (Left) an original food image and (right) three feature points.

where, xi 1 is an average of following K points starting from the point pi in the x coordinate as shown in Equation 2. We
used K = 50 for all results shown in this paper.
1 K
xi 1 = ∑ xi k
(2)
K k=1
Global Geometric Feature Extraction: Since the boundaries of a segmented image are inﬂuenced by reﬂection and
shadow from the picture-taking environment (e.g., lighting condition), global geometric properties of the segmented region
is essential for our food volume estimation. The medial axis (also called the symmetric axis)28 has been frequently used to
represent and describe the global shape features.24–26
The medial axis of a shape is formally deﬁned as at least two skeleton points that are equidistant from the the shape
boundary. The use of the medial axis helps us to minimize volume estimation errors caused inaccuracies in detecting
feature points. To this end, we ﬁrst distinguish a central axis from the whole topology of the medial axis and use the central
axis for extracting additional feature points (Figure 4 (middle right)). Next, we ﬁnd point pairs on the boundary of the
object that are on either side of the central axis (Figure 4 (rightmost)). The average length of these point pairs determine a
potential width (radius for a cylindrical shape) of our template shape.

Figure 4. Geometric feature extraction using the medial axis. (Leftmost) a food image, (middle left) inaccurate feature point extraction,
(middle right) medial axis generation and (rightmost) an average width of a shape along with the central axis of the medial axis.

Interior Edge Detection: The analysis of the shape boundary is not enough in some cases where imperative geometric
properties could be found within the interior area of a shape. Figure 5 shows an example of a bread slice with distinct
interior edges. Here, our goal is to differentiate the side from the top surface such that the height from the bottom rim to the
top surface can be computed. Moreover, the segmentation of the top surface provides us with more a accurate food shape
when compared to an approximation using an ellipse. Thus, the volume of the bread slice is computed by multiplying
these two measurements assuming that the shape of the bottom surface is the same to the top surface. As such, interior
edge detection is utilized for speciﬁc foods with fairly ﬂat and identical outlines of the top and bottom surfaces and the
side distinguished by color. These cases would contain most sliced breads, such as a piece of toast, a garlic bread slice or
a baguette slice.
For such interior edge detection and top surface segmentation, we employ the active contour methodology.29 In computer vision, the active contour method has been used for in a wide range of problems including segmentation and edge
detection.30–33 The idea in the active contour method (or snakes) is, starting with a contour initialized near the desired
object, the algorithm allows the contour (or snake) to deform so as to optimize the combination of internal and external
energy. Internal energy encourages an elastic and smooth shape, whereas external energy is based on strong edge attraction. In the classical edge based active contour models,29, 34 the snakes depend on the gradient of an image to stop evolving

SPIE-IS&T/ Vol. 7873 78730K-4
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

Figure 5. Feature extraction using the active contour. (Leftmost) a food image, (middle left) a segmented image, (middle right) active
contour and (rightmost) height estimation using segments from active contour.

Figure 6. Computational results of our volume estimation along with their real measurement and segmentation image.

contour on the boundary of an object. Our implementation, however, utilizes the region based active contour technique by
Lankton and Tannenbaum35 which is more robust with respect to initial contour placement and segmentation accuracy.
Given a food image (Figure 5 (leftmost)) and a corresponding segmented image (Figure 5 (middle left)), our algorithm
initially cuts off the minimal rectangular region containing a food from the food image to reduce the computational overhead. It then places the initial snake on the top surface of a food to iteratively evolve the snake until the contour is on the
boundary of the top surface (Figure 5 (middle right)). Therefore, we obtain interior edges and the top surface segmentation
of a food. For calculating the height, as shown in Figure 5 (rightmost), we use two contour segments; the top segment (red
line) along the interior edges and the bottom segment (white line) generated by translating the top contour segment to the
bottom boundary of the segmented image. This provides an distance of the point pairs; one from the top segment and one
the bottom segment.

4. RESULTS
To verify the accuracy of our volume estimation algorithm, we performed validation experiments using images of beverages
(milk and orange juice in transparent container) and bread slices taken with the Apple iPhone (3GS) and Canon Powershot
SD1100. For beverage images, we used automatically segmented images, and manually segmented images for bread
images. The results of the beverage estimation indicated an improved accuracy of approximately 30% improved accuracy
when removing false information in the segmented image using binary morphology. Further, using medial axis resulted
in 10∼ 20% improvement when compared to the use of only noise reduced images. Figure 6 shows examples of the
computational results of our volume estimation algorithm, their real measurements with a beaker and used the segmented
images. Table 1 shows the differences and ratios of the estimated volumes to the real measurements of 17 beverage images.
The average relative error and standard deviation were about 11% and 8, respectively. We also compare the differences
between estimated and measured volumes; 8 (N = 1 to 8) were underestimated and 9 (N = 9 to 17) overestimated. For bread
slices as shown in Table 2, the results were 8% overestimated. Figure 7 shows some examples of our 3D reconstructed
shape. We also found that our volume estimation method is inﬂuenced by the camera angle (viewing angle). In our
experiments, the range of the appropriate camera angle is 30∼45 degrees.

5. CONCLUSION AND FUTURE WORK
In this paper, we have presented a novel food volume estimation method from a single image by introducing food speciﬁc shape templates to deal with the complex and various food shapes and overcome insufﬁcient information for 3D

SPIE-IS&T/ Vol. 7873 78730K-5
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

Figure 7. Results of 3D volume reconstruction for different bread slice images. From left to right, food images, interior edge detection
and 3D volumes reconstructed.

food volume reconstruction. In our approach, a novel feature point extraction algorithm was proposed including binary
mathematical morphology to improve the accuracy of our algorithm by reducing noises in the segmented image. We also
demonstrated how medial axis was used to extract geometrical features. Further, for volume estimation of free-shaped
foods with the ﬂat top and bottom surfaces, we applied the active contour method to determine interior edges and the top
surface. Finally, we performed experiments to evaluate our approach using beverage images and achieved reasonable results (11% error). For generalized extruded solids seen as bread slices, promising computational volume results (8% error)
was indicated. As future work, we plan to extend the range of shape templates to provide volume estimation for various
foods. Since accurate volume estimation for arbitrary shapes from a single image is still a computationally intractable
problem, we are also planning to investigate a user-assisted method to improve accuracy and get a computationally feasible
solution.

REFERENCES
[1] Ogden, C. L., Flegal, K. M., Carroll, M. D., and Johnson, C. L., “Prevalence and trends in overweight among us
children and adolescents, 1999-2000,” JAMA: The Journal of the American Medical Association 288, 1728–32 (Oct.
2002).
[2] Ogden, C. L., Carroll, M. D., Curtin, L. R., McDowell, M. A., Tabak, C. J., and Flegal, K. M., “Prevalence of
overweight and obesity in the united states, 1999-2004,” JAMA: the journal of the American Medical Association 295,
1549–55 (Apr. 2006).
[3] Rockville, M., [The Surgeon General’s call to action to prevent and decrease obesity], Washington DC: U.S. Department of Health and Human Services, Public Health Service, Ofﬁce of the Surgeon General (2001).
[4] Taren, D., Dwyer, J., Freedman, L., and Solomons, N. W., “Dietary assessment methods: where do we go from
here?,” Public Health Nutrient , 1001–1003 (2002).
[5] Joy Ngo, Anouk Engelen, M. M. J. R. P. G.-S. and Serra-Majem, L., “A review of the use of information and
communication technologies for dietary assessment,” British Journal of Nutrition 101, S102–S112 (2009).
[6] Lassen, A., Poulsen, S., Ernst, L., Andersen, K., Biltoft-Jensen, A., and Tetens, I., “Evaluation of a digital method to
assess evening meal intake in a free-living adult population,” Food and Nutrition Research 1(0) (2010).

SPIE-IS&T/ Vol. 7873 78730K-6
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

Table 1. Volume estimation results for beverages.

N

Measured
volume
137
205
137
135
205
220
220
135
70
220
137
137
205
65
70
65
65

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

Estimated
volume
112.13
169.15
120.58
121.2
183.85
206
212.86
131.15
71.46
226.07
143.78
146.14
229.61
73.61
81.98
77.14
85.8

Difference
-24.87
-35.85
-16.42
-13.8
-21.15
-14
-7.14
-3.85
1.46
6.07
6.78
9.14
24.61
8.61
11.98
12.14
20.8

Ratio of estimated volume to
real measurement
0.82
0.83
0.88
0.9
0.9
0.94
0.97
0.97
1.02
1.03
1.05
1.07
1.12
1.13
1.17
1.19
1.32

Table 2. Volume estimation results for bread slices.

N
1
2
3
4
5
6
7
8
9

Top area
(sq in)
6.71
6.74
6.62
6.00
6.19
6.33
5.42
5.50
5.48

Height
(in)
0.80
0.81
0.77
0.78
0.84
0.81
0.80
0.78
0.70

Volume
(cubic in)
5.37
5.46
5.10
4.68
5.20
5.13
4.34
4.29
3.84

Ratio of estimated volume to
real measurement
1.25
1.27
1.19
1.00
1.11
1.09
0.99
0.98
0.88

[7] Glanz, K., Murphy, S., Moylan, J., Evensen, D., and Curb, J. D., “Improving dietary self-monitoring and adherence
with hand-held computers: A pilot study,” American Journal of Health Promotion (2006).
[8] Boushey, C. J., Kerr, D. A., Wright, J., Lutes, K. D., Ebert, D. S., and Delp, E. J., “Use of technology in children’s
dietary assessment,” European Journal of Clinical Nutrition 63, S50–S57 (2009).
[9] Yang, L., Zheng, N., Cheng, H., Fernstrom, J. D., Sun, M., and Yang, J., “Automatic dietary assessment from fast
food categorization,” Proceedings of the IEEE 34th Annual Northeast Bioengineering Conference (2008).
[10] Zhu, F., Mariappan, A., Boushey, C. J., Kerr, D., Lutes, K. D., Ebert, D. S., and Delp, E. J., “Technology-assisted
dietary assessment,” Proceedings of the IS&T/SPIE Conference on Computational Imaging VI 6814(1), 681411, SPIE
(2008).
[11] Woo, I., Otsmo, K., Kim, S., Ebert, D. S., Delp, E. J., and Boushey, C. J., “Automatic portion estimation and visual
reﬁnement in mobile dietary assessment,” Computational Imaging VIII 7533(1), 75330O, SPIE (2010).
[12] Belongie, S., Malik, J., and Puzicha, J., “Shape matching and object recognition using shape contexts,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 2(4), 509 – 522 (2002).
[13] Zhu, Q., Wang, L., Wu, Y., and Shi, J., “Contour context selection for object detection: A set-to-set contour matching
approach,” in [Proceedings of the European Conference on Computer Vision], (October 2008).
[14] Berg, A. C. and Malik, J., “Geometric blur for template matching,” in [Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition], 607–614 (December 2001).
[15] Gavrila, D., “A bayesian exemplar-based approach to hierarchical shape matching,” IEEE Transactions on Pattern
Analysis and Machine Intelligence (PAMI) 29, 1408–1421 (2007).

SPIE-IS&T/ Vol. 7873 78730K-7
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

[16] Pavlidis, T., “A review of algorithms for shape analysis,,” Computer Graphics and Image Processing 7(2), 243 – 258
(1978).
[17] Serra, J., [Image Analysis and Mathematical Morphology], Academic Press, Inc., Orlando, FL, USA (1983).
[18] Vincent, L., “Current topics in applied morphological image analysis,” in [Current Trends in Stochastic Geometry
and its Applications], (1997).
[19] Jeyalakshmi, T. and K.Ramar, “A modiﬁed method for speckle noise removal in ultrasound medical images,” International Journal of Computer and Electrical Engineering 2 (February 2010).
[20] Schulze, M. A. and Wu, Q. X., “Noise reduction in synthetic aperture radar imagery using a morphology-based nonlinear ﬁlter,” in [Proceedings of Digital Image Computing: Techniques and Applications, Conference of the Australian
Pattern Recognition Society], 661–666 (1995).
[21] Heijmans, H. J. A. M., “Self-dual morphological operators and ﬁlters,” Journal of Mathematical Imaging and Vision 6, 15–36 (1996). 10.1007/BF00127373.
[22] Peters, R.A., I., “A new algorithm for image noise reduction using mathematical morphology,” IEEE Transactions on
Image Processing 4, 554 –568 (May 1995).
[23] Yang, G.-Q., Jiang, L.-H., and Li, Y., “Application of rough sets in binary morphology,” in [Machine Learning and
Cybernetics, 2006 International Conference on], 3446 –3449 (2006).
[24] Siddiqi, K., Shokoufandeh, A., Dickinson, S. J., and Zucker, S. W., “Shock graphs and shape matching,” International
Journal of Computer Vision 35, 13–32 (1999). 10.1023/A:1008102926703.
[25] Sundar, H., Silver, D., Gagvani, N., and Dickinson, S., “Skeleton based shape matching and retrieval,” in [Shape
Modeling International, 2003], 130 – 139 (2003).
[26] Pizer, S. M., Oliver, W. R., and Bloomberg, S. H., “Hierarchical shape description via the multiresolution symmetric
axis transform,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 9(4), 505 –511 (1987).
[27] Ho, S.-B. and Dyer, C. R., “Shape smoothing using medial axis properties,” IEEE Transactions on Pattern Analysis
and Machine Intelligence (PAMI) 8(4), 512 –520 (1986).
[28] Blum, H., “Biological shape and visual science (part i),” Journal of Theoretical Biology 38(2), 205 – 287 (1973).
[29] Kass, M., Witkin, A., and Terzopoulos, D., “Snakes: Active contour models,” International Journal of Computer
Vision 1, 321–331 (1988).
[30] Blake, A. and Isard, M., [Active Contours: The Application of Techniques from Graphics,Vision,Control Theory and
Statistics to Visual Tracking of Shapes in Motion], 1st ed. (1998).
[31] Paragios, N., Chen, Y., and Faugeras, O., [Handbook of Mathematical Models in Computer Vision], Springer-Verlag
New York, Inc., Secaucus, NJ, USA (2005).
[32] Leymarie, F. and Levine, M. D., “Tracking deformable objects in the plane using an active contour model,” IEEE
Transactions on Pattern Analysis and Machine Intelligence (PAMI) 15, 617–634 (June 1993).
[33] Chan, T. and Vese, L., “Active contours without edges,” IEEE Transactions on Image Processing 10, 266 –277 (Feb.
2001).
[34] Caselles, V., Catte, F., Coll, T., and Dibos, F., “A geometric model for active contours in image processing,” Numerische Mathematik 66, 1–31 (1993). 10.1007/BF01385685.
[35] Lankton, S. and Tannenbaum, A., “Localizing region-based active contours,” IEEE Transactions on Image Processing 17(11), 2029 –2039 (2008).

SPIE-IS&T/ Vol. 7873 78730K-8
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

Eurographics/ IEEE-VGTC Symposium on Visualization 2009
H.-C. Hege, I. Hotz, and T. Munzner
(Guest Editors)

Volume 28 (2009), Number 3

SDViz: A Context-Preserving Interactive Visualization
System for Technical Diagrams
Insoo Woo†1 SungYe Kim†1 Ross Maciejewski†1 David S. Ebert†1 Timothy D. Ropp†2 Krystal Thomas‡3
1 Electrical

and Computer Engineering, Purdue University, USA
Technology, Purdue University, USA
3 Air Force Research Laboratory, USA

2 Aviation

Abstract
When performing daily maintenance and repair tasks, technicians require access to a variety of technical diagrams. As technicians trace components and diagrams from page-to-page, within and across manuals, the contextual information of the components they are analyzing can easily be lost. To overcome these issues, we have
developed a Schematic Diagram Visualization System (SDViz) designed for maintaining and highlighting contextual information in technical documents, such as schematic and wiring diagrams. Our system incorporates
various features to aid in the navigation and diagnosis of faults, as well as maintaining contextual information
when tracing components/connections through multiple diagrams. System features include highlighting relationships between components and connectors, diagram annotation tools, the animation of flow through the system,
a novel contextual blending method, and a variety of traditional focus+context visualization techniques. We have
evaluated the usefulness of our system through a qualitative user study in which subjects utilized our system in
diagnosing faults during a standard aircraft maintenance exercise.
Categories and Subject Descriptors (according to ACM CCS): I.3.6 [Computer Graphics]: Methodology and
Techniques—Interaction techniques I.3.8 [Computer Graphics]: Applications—

1. Introduction
In electronic/mechanical maintenance and repair tasks, 2D
technical documents, such as schematic and wiring diagrams, are used by technicians to guide their work. Typical tasks include component finding and circuit tracing in
which technicians hypothesize and test what components
along a circuit path are working. To facilitate these tasks,
maintenance workers often print a variety of documents to
annotate and highlight as they perform their analysis. Many
of these documents contain multi-page spreads of complex
wiring diagrams. Technicians will often need to find a component in a schematic diagram, search through the manual
for the related wiring diagram, and then trace paths through
the wiring diagram, while maintaining contextual information about where these wires and components may be in
† e-mail:{iwoo|inside|rmacieje|ebertd|tropp}@purdue.edu
‡ e-mail:Krystal.Thomas@wpafb.af.mil
c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

relation to the schematic diagram. As such, tracing paths
from page-to-page and in-and-out of components can be tedious. Furthermore, the amount of documents needed can be
cumbersome, and maintaining contextual information when
switching between related diagrams is difficult. Meanwhile,
over the last decade, as technical documents have grown in
size, manufacturers have begun transferring them from the
printed pages to electronic files. Unfortunately, given that
many of these manuals consisted of multi-page fold-out diagrams, scrolling through pdfs and trying to maintain context
amongst various components is not a viable option.
In light of these issues and after performing cognitive task
analysis on a typical maintenance task, we have developed a
novel visualization system to aid technicians in their analysis of schematic and wiring diagrams for maintenance tasks.
Our system provides the following capabilities:
• Context preservation: preserving highlighting, working
status, switch status, and spatial information

944

I. Woo & S. Kim & R. Maciejewski & D. S. Ebert & T. D. Ropp & K. Thomas / SDViz

• Single diagram navigation: jumping to a connected
neighbor component, finding circuit breakers, overviewing a diagram
• Inter-diagram navigation: moving to the related diagram, transition by blending
• Highlighting: highlighting components and connections,
marking working status of components, changing switch
status
• Distortion viewing: magic lens, fisheye lens, relational
lens
• Animation: flow animation
• Searching: keyword searching, tree-viewing file explorer
• Zooming: zoom in and out, marquee zoom
In the following section, we describe our system design
and categorize the design principles.
2. System Design
Recent work by Barnard et al. [BRM06, BR06] noted that
individuals use technical documents differently and develop
their own mental models while interacting with systems.
This work noted the need for task-oriented and user-adapted
electronic technical documentation. Further, work by Heiser
[HTAH03, HPA∗ 04] detailed procedures on finding and instantiating design principles through cognitive experiments.
Based on ideas from these works, we have designed the SDViz system by consulting with aviation repair technicians in
order to develop an appropriate knowledge base of what user
needs should be met. A routine tabletop maintenance exercise, used for training, was performed by three technicians of
varying skill levels. A task analysis of the technicians’ tasks
was performed, and our system design was based on these
results.
Our system could be further enhanced through the application of graph-based visualization tools (e.g., Tulip
[Aub03]). A graph based framework for our system would
provide various manipulation tools based on the graph properties. However, unlike other information, there exist specific notations for schematic and wiring diagram drawings.
Furthermore, the details of components and connections between the components are often not consistent in technical
diagrams. Thus, it is difficult to apply graph visualization
techniques to schematic and wiring diagrams. Moreover, all
the information for system maintenance is system-oriented
and uses a hierarchical structure (e.g., system, subsystem,
and component) to organize various documents in relation
to each other. Technicians can search related diagrams using
the hierarchy of the component or document identifications
in the diagrams. As such, our work focuses on using this
hierarchical structure. We provide the user with appropriate
information, maintaining the layout and style of schematics
and the conventions in these diagrams. We have chosen not
to apply graph or network visualization techniques as many
technicians are not familiar with them. However, such techniques are valuable and could be applied in future systems.

For example, Van Ham [vH03] and Abello et al. [AvH04]
described visualization techniques using matrix representations providing multi-level zooming to efficiently abstract
information. Unfortunately, such matrix representation fails
to provide appropriate views for some tasks like diagram visualization for actual maintenance.
Other considerations in system design include the fact that
our system deals with legacy data (e.g., pdf, CGM (Computer Graphics Metafile) format diagrams, and paper manuals), and generating the appropriate data from these various legacy data is also an important issue. This issue again
leads us to choose a hierarchical tree structure (as opposed
to other graph or network visualization schemes) based on
SVG and XML specifications in order to store, as well as
represent technical diagrams. This also allows our system to
maintain the layout and style of diagrams without abstraction or rearrangement of components and links, and at the
same time, we can take various manipulations based on the
tree structure. Unfortunately, the data provided in the legacy
formats do not yield themselves to effective automatic analysis (they are lacking needed information). Moreover, we
cannot assume that a user would have all the necessary input
to enable effective analysis across the large set of schematics and wiring diagrams available. Instead, our system enables users to effectively analyze circuits in diagrams visualized and highlight important components, focusing on their
maintenance task at hand. Therefore, we focus only on the
practical system design and integration as well as a system
evaluation within the context of a realistic maintenance task.
2.1. Tabletop Exercise
The tabletop scenario consisted of a two fault error analysis design in which a lefthand side window heating unit in
a Boeing 737 is found to be inoperable. The two faults included a short in the wiring leading from the power supply to the window heater, and a bent pin in the P5-9 component. During the tabletop exercise, the technicians diagnosed the issues using printed manuals. When measurements
were needed (for example when a technician wanted to know
if component x had power) the exercise leader would provide that information as they proceeded through their steps.
As the technicians performed, they spoke outloud, detailing
their actions. Notes were taken about the technicians’ actions. Three technicians participated in this stage of knowledge gathering.
We noted that there were underlying actions taken by
all technicians. First, the technicians searched the maintenance manual (MM) and schematic diagram manual (SDM)
for keywords relating to the components under question.
Second, the technicians utilized the wiring diagram manual
(WDM) indexed from the keywords found in the MM and
SDM. In the WDM, technicians traced the path from the
window unit to the power source to determine what components may be contributing to the fault. While searching
c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

I. Woo & S. Kim & R. Maciejewski & D. S. Ebert & T. D. Ropp & K. Thomas / SDViz

and identifying system components, subjects systematically
checked the wiring continuity beginning with the circuit
breaker, wire run, panel switch, and window connections,
spending most of their time using the SDM.
2.2. Design Principles
From the task analysis, we derived the system needs as follows:
•
•
•
•

Providing seamless visualization compared to legacy data
Allowing users to find what they want in multi-diagrams
Allowing users to highlight components and links
Following the flow of electricity from source to sink components
• Preserving component context between diagrams
• Enabling users to easily move back and forth among related diagrams
The most important observation from the task analysis
was that preserving context among related diagrams can
enormously help technicians find desired information providing a starting point when they refer to related diagrams.
Hence, we focus on preserving component context (e.g.,
highlighting, working status, switch status, and spatial information) in the visualization system for technical documents,
particularly, technical diagrams.
3. SDViz: A Schematic Diagram Visualization System
Based on the design principles developed in our knowledge
acquisition phase, we have developed a Schematic Diagram
Visualization System (SDViz) to aid technicians in the navigation/diagnosis of faults and the traversal of multiple linked
diagrams. Unlike general document visualization, we have
chosen not to map the technical documents into multivariate
space as such views are not familiar to technicians. Instead,
our system provides a seamless transition from paper manuals to the visualization realm by adding tools and features
that are intuitive to technicians and designed based on observations of technicians’ work. System features include highlighting relationships between components/connectors and
components/components, maintaining context between several related diagrams, animation of electrical flow through
the system, component/sub-component annotation (marking
a component working/not-working), and improving navigation and linking of diagrams through advanced blending and
other novel visualization techniques.
In comparison to related work, Li et al. [LAS04] proposed an interactive image-based exploded view diagrams
using 2D images. Further, Boose et al. [BSB03] migrated 2D
illustrated part drawings to Class IV Interactive Electronic
Technical Manual (IETM). In both work, technical diagram
visualization reuses the legacy documents in a raster format without recreating diagram data. By contrast, Setlur and
Chen [SC05] and Fredj et al. [FD06] used the 2D vector format in their technical document visualization systems. More
c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

945

recently, the PDF Schematic Tracing [PST] system was developed as commercial plug-in software allowing users to
trace the connections in schematic diagrams in PDF format by highlighting the path components, and by providing hyperlinks between components and their descriptions.
The PDF Schematic Tracing has the most similar purpose
to our system in terms that it was developed to help maintenance personnel to look through many diagram manuals,
and it provides navigation in technical diagrams without any
modification of the layout and style of diagrams.
In our system, we use two data for each diagram; SVG
data converted from a legacy diagram, and the eXtensible
Markup Language (XML) [Ext] data containing semantic
information within the diagram. Hence, our system is intended to reuse legacy data (e.g., pdf, CGM, and paper diagram scanned as a bitmap) as well as to take advantage of 2D
vector graphics by using SVG format. As such, our system
builds on previous work, creating new techniques and providing a quantitative analysis of these techniques for technical document visualization. The following subsections will
describe the data representation for context preserving information systems and key features of our system.
3.1. Data Generation
To relate documents from different sources, a structured data
representation is required. Moreover, context preservation
during document navigation and investigation requires semantic information to identify the focus component, the contextual components, and the connections. Setlur and Chen
[SC05] identified, categorized and stored the characteristics
of each object in the XML based data files in order to enhance key objects by maintaining the objects’ size while
zooming out. A higher level graphical structure semantic
markup language (GraSSML) has been proposed to store the
structure and semantics of diagrams [FD06].
All diagrams used in our system follow specifications
such as IETM, SVG and SGML/XML, which enable the effective querying and displaying of diagrams along with providing resolution-independent visualization. Each diagram
is converted to SVG format for graphics information, and
then XML file is generated by our semantic editor, which is
a very simple tool developed as a part of our entire system
to authorize semantic information of a given technical diagram. This data generation step introduces the hierarchical
structure and semantics of components and connectors in a
diagram, and allows us to take advantage of searching information based on XML-tagged Document Instances (DI),
which forms the basis for our context-preservation work.
3.2. Preserving Contextual Information
While troubleshooting, technicians need to use multiple diagrams, requiring a contextual shift as they move from highlevel schematic diagrams to low-level wiring diagrams. For

946

I. Woo & S. Kim & R. Maciejewski & D. S. Ebert & T. D. Ropp & K. Thomas / SDViz

Figure 1: Contextual information preserved between a
schematic (left) and the related wiring (right) diagrams in
our system.
instance, any component in a schematic diagram may be the
cause of a system fault. A technician would begin their analysis by finding their component of interest and then loading
up a wiring diagram. Our system provides technicians with
tools that help maintain context when shifting between documents by maintaining spatial coherency between components, component annotation, and highlighting information
during a document shift. In the example of changing from
the high-level schematic diagram to the detailed schematic
diagram, preserving the contextual information of the component such as highlighting information, working status,
etc., provides users with a seamless transition. Moreover,
our system introduces a feature in which spatial information is maintained by blending the two documents gradually.
By blending, we are able to more effectively allow users to
perceive the related information between documents in comparison to switching back and forth between pages (although
that option exists as well in our system). Figure 1 shows how
contextual information is preserved between two related diagrams distorted with the rectangular fisheye lens. In Figure 1, the user’s interactions (component’s working status
and a user-selected focus component) with the schematic diagram are preserved and visible in the related wiring diagram maintaining contextual information.
3.3. Navigation
Single diagram navigation For navigating within a given
diagram, we consider common methods used in maintenance
and repair using technical diagrams. Since most components
are connected to other components through wire connections, we provide the capability to center any of the connected components by selecting the component’s name from
a popup menu. In many maintenance tasks dealing with large
diagrams, this jumping by selecting a component name can
be useful to navigate within a diagram without tracing wire
connections. Additionally, the quick identification of special components, such as circuit breakers, can be essential
in helping technicians find the fault while navigating in a
diagram. Hence, we have designed a method to find all components of a specified type that are connected to a focus

Figure 2: Finding circuit breakers (leftmost highlighted
component) connected to the focused component (rightmost
highlighted component).

component. For a graph representing the relationship of wire
connections between components generated with the connection information of a diagram, we apply a breadth first
search, and paths from the focused component to the circuit
breakers are displayed. For example, when a component has
been selected by a user, all wire connections and neighbor
components are highlighted on the backward path from the
focused component to the circuit breakers. Figure 2 shows
an example of using this feature for the focused component,
highlighting circuit breaker components at the leftmost end
on the highlighted wire connections.
Inter-diagram navigation In complex electrical and mechanical systems, most diagrams are inter-related to other
diagrams. For instance, each schematic diagram is associated with the related schematic and wiring diagrams. Hence,
moving to the related diagrams without explicitly searching will be essential for maintenance tasks. For such interdiagram navigation, preserving the contextual information
is indispensable because it enables users to keep their focus while switching diagrams. To preserve contextual information, all components are assigned an unique identifier so
that user interactions can be applied to all components in related diagrams. In maintaining highlighting information between diagrams, the schematic diagrams are often simpler
than wiring diagrams. Therefore, the parent component including any focused component in a wiring diagram may be
solely highlighted in a schematic diagram, or children components within the focused component in a schematic diagram may be highlighted together in a wiring diagram. In
those cases, the hierarchical information is used to find appropriate components in the related diagrams to apply the
changes by users’ interaction.
The speed of the diagram switching is another consideration of a visualization system for technical diagrams. We
propose Transition by blending, a method to show the related
diagram within the view of the current diagram by blending
the diagrams under user-controlled speed. Figure 3 shows
the example of procedures using the Transition by blending
for a focused component from a schematic to a wiring diagram. While diagrams are blended, the highlighting as well
as spatial context of a focused component are maintained
to provide a user with consistent contextual information. In
c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

947

I. Woo & S. Kim & R. Maciejewski & D. S. Ebert & T. D. Ropp & K. Thomas / SDViz

(a)

(b)

(c)

(d)

Figure 3: Transition by blending from a schematic diagram to a related wiring diagram preserving spatial information of a
selected component. (a) Schematic diagram, (b) initial view of Transition by blending combined with our Relational lens, (c)
60% blending, and (d) final transition to the related wiring diagram.
Figure 3(b) and 3(c), the size of the focused component in a
schematic diagram is smaller than that of the component in
the related wiring diagram. Hence, the green rectangle is enlarged more and more while being blended with the wiring
diagram. The green rectangle in Figure 3(b) indicates our
Relational lens which we describe in subsection 3.5.
3.4. Highlighting
Highlighting enhances the information relevant to the user’s
task and immediately draws the user’s attention by using
colors or marks. We highlight the names of components,
the focused component, neighbor components connected to
the focused component, and wire connections between highlighted components in various colors and with marks indicating the status of a component. Figure 4 shows several
components and connections highlighted. The highlighting
of each component is blended with that of a parent component. For use in highlighting, the relationship between components can be generated from the connection information
in XML data, since each diagram includes XML data for the
semantic information. Unlike components, connections have
a directional property, such as a source or a sink. Hence, we
interpolate colors of the end points of each connection from
a source (red) to a sink (blue) to present the directional cue.
Annotations, placing comments on the diagrams which
are meaningful to users, are another method to highlight
diagrams. Marshall [Mar97] studied where students made
marks in their text book and defined the form and function of these annotations. The annotations were placed either within text or in marginal or blank space, and were
categorized into two categories: telegraphic (e.g., underlining, stars, and circles) and explicit one (e.g., brief notes
and short phrases). In the SDViz system, we support annotations by placing special marks (switch off/on, component working/not-working), which are then used to provide
a symbolic note when performing maintenance tasks across
multiple diagrams. These special marks are preserved in all
related diagrams as contextual information in our system by
managing each component’s unique identifier.
c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

Figure 4: Highlighting components and connections in various colors (green: marked as working status, red: marked
as not-working status, cyan: locked focusing, purple: highlighted as a neighbor) and with different marks (switch
on/off, working/not-working status).
3.5. Distortion Viewing
Unlike general documents, distortion viewing for technical
diagrams is less preferred by maintenance personnel because it changes the structural view of contents within the
diagrams. Despite this limitation, a few distortion viewing
techniques can be effective for the visualization of relatively large diagrams on a computer display and can enhance a user’s area of interest within the context of a diagram. Furthermore, Baudisch et al. [BGBS02] showed
that focus+context visualization reduced the time to perform tasks by evaluating focus+context, overview+detail,
and pan+zoom interfaces for large detailed data. Hence,
we provide the magic lens and the rectangular fisheye
lens. The magic lens uses a fixed-size magnifying area occluding neighbor regions, whereas the rectangular fisheye
lens [Rau99,RJS01] distorts neighbor regions around a userdefined area providing contextual information unoccluded.
Our system also incorporates work by Gutwin [Gut02] in
which focus-targeting is improved by accounting for the
users’ activity (speed of mouse movements) to modulate the
rate of distortion while supporting navigation and inspection.
In addition, we propose the Relational lens that uses a
fixed-size lens defined by the boundary of a focused com-

948

I. Woo & S. Kim & R. Maciejewski & D. S. Ebert & T. D. Ropp & K. Thomas / SDViz

(a)

(b)

(c)

(d)

Figure 5: Relational lens applied to the focused component in (a) a schematic diagram and (c) a wiring diagram to see more
detail wiring information. (b) The wiring information shown through the Relational lens in a schematic diagram, and (d) more
detail wiring information shown through the Relational lens in a wiring diagram.
ponent. The main goal of the Relational lens is to display
information from related diagrams by overlaying the related information on a focused component while intrinsically preserving the spatial context. For example, the Relational lens overlays the detail view of the related wiring
diagram on a focused component within a schematic diagram or the overview of the related schematic diagram on a
focused component within a wiring diagram. Figure 5 shows
an example using the Relational lens on the schematic and
wiring diagrams to see more detailed wiring information of
a focused component. The outmost components highlighted
in Figure 5(a) and 5(c) show components to which the Relational lens is applied resulting in Figure 5(b) and 5(d),
respectively. The Relational lens is toggled by user interaction, and applied to the component focused by mouse over
or highlighted by double clicking. What users see through
the Relational lens is information for the same component
from a related diagram. It can be from any related SDM
(abstracted information) or WDM (detailed information). In
Figure 5(b) and 5(d), the green rectangles show the boundary of the Relational lens applied to a focused component in
SDM and WDM, respectively. The part of a diagram inside
each green rectangle is taken from a corresponding related
WDM. A similar concept, called semantic zooming, was introduced by Frisch et al. [FDB08] by visualizing overview
and details based on focus+context technique for a focused
node within a UML diagram. However, it differs from our
Relational lens in terms that our system visualizes the different level of details as well as all contextual information (i.e.,
highlighting, switch status, working/not-working status) occurred dynamically by user interaction. From our observations, the matching the different scales of the same component in related diagrams can result in a distorted aspect ratio,
confusing the users. Hence, the information from the related
diagram through the Relational lens is displayed at the same
scale as the current diagram. As shown in Figure 3, the Relational lens can also be used with the Transition by blending.

3.6. Flow Animation
Based on our task analysis, we found that most technicians
follow the flow of electricity during troubleshooting operations. Furthermore, work by Robertson et al. [RFF∗ 08]
has demonstrated the effectiveness of animation in illustrating data trends. As such, we allow users to view flow animations using a texture mapping technique on connections
to show system functions and flow. In technical diagrams,
sources and sinks can be identified as technicians usually
think. We had a senior student in Aviation Technology at
Purdue University identify sources, sinks, and flow directions in diagrams we used, before we generate XML data for
semantic information from the diagrams. Hence, our XML
data contains the information about sources and sinks resulting in the flow of electricity highlighted from the source
to sink component. The flow animation is affected by both
the switches and working status of components. Therefore,
flow animation can also be utilized as the method for single diagram navigation providing a dynamic visualization.
Figure 6 shows an example of our flow animation. The electricity flows from the selected component (leftmost highlighted component) to the components connected to the selected one. The upper flow of electricity is stopped, whereas
the lower flow passes through the switch due to the switch
status (upper switch is on, and lower switch is off).

Figure 6: Flow animation mapped with an arrow texture animated along the flow direction.

c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

I. Woo & S. Kim & R. Maciejewski & D. S. Ebert & T. D. Ropp & K. Thomas / SDViz

(a)

949

(b)

Figure 7: Results from the field study by 7 senior students in the school of Aviation Technology at Purdue University. (a)
Usability for each function, and (b) usefulness for each feature from the field study.
4. Qualitative Field Study
In order to evaluate the effectiveness of SDViz, a qualitative
field study was conducted with 10 college seniors from the
school of Aviation Technology at Purdue University.

and were also asked to write any other comments they had
about the system. Subjects also ranked the functions of SDViz based on their preference of use.
4.3. Qualitative Results

4.1. Scenario

Participants were given a pre-experiment questionnaire to
evaluate their relative background knowledge and experience as well as were classified into two groups. Subjects
in Group A used the full functionality of SDViz to perform
the diagnoses. They were provided with tutorial videos and
training prior to beginning. Furthermore, they could also
consult the SDViz user manual at any time during troubleshooting. Subjects in Group B were given a basic pdf
viewer with pan and zoom navigation controls to perform
the diagnoses. Training was provided for those subjects unfamiliar with pdf navigation.

Figure 7 shows the results from our field study by the 7 subjects in Group A. The horizontal axis lists each function of
our SDViz, and the vertical axis represents the number of
subjects. In Figure 7(a), most subjects who used SDViz for
the troubleshooting commented that most of functions were
“very easy” or “easy” to use for the usability of our SDViz. Particularly, all subjects chose “very easy” or “easy” for
the usability of the highlighting components, working status,
magic lens, and flow animation. For the Transition by Blending, subjects agreed that it provided efficient context preservation keeping their focus, but two of them also answered it
was “not easy” enough to use because they were not familiar with such an interface. Figure 7(b) shows the usefulness
evaluation results for each SDViz feature. All participants
who used SDViz for the troubleshooting agreed that highlighting, moving to the related diagram, and flow animation
features were “very useful” or “useful.” In addition, 6 among
7 subjects chose “very useful” or “useful” for the working
status and maintaining the highlighting features. Moreover,
some of them proposed that it would be useful to highlight
more than one component as focused components. Our analysis from consulting with repair technicians and our task
analysis revealed that distortion viewing, using the magic
lens and the fisheye lens, were not preferred by subjects. For
each distortion method, 2 among 7 subjects answered “not
useful.”

Upon completion of the scenario, subjects in Group A
were given a post-experiment questionnaire to collect qualitative feedback for each functional component of SDViz.
Subjects answered questions on the usability and usefulness
of the SDViz capabilities, ranking them either from “difficult” to “very easy” or from “not useful” to “very useful”

Although the purpose of the field study was to collect
qualitative feedback, we also observed the troubleshooting
time to complete the scenario for both groups. The participants who used a pdf viewer took 17-32 minutes to troubleshoot the component, spending most of their time finding
related diagrams between various manuals, whereas partic-

We reused the scenario performed in the knowledge acquisition phase. To reiterate, the scenario is that a window
heater was malfunctioning in a Boeing 737 aircraft. No voltage is indicated when the left side window control switch
is positioned to ON. No students in the field study participated in the knowledge acquisition phase, nor had they previously performed this task in their class. The subjects were
asked to troubleshoot the problem using relevant maintenance, schematic, and wiring diagram manuals. Again, when
measurements were needed, the exercise leader would provide this information.
4.2. Experimental Setup

c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

950

I. Woo & S. Kim & R. Maciejewski & D. S. Ebert & T. D. Ropp & K. Thomas / SDViz

ipants using the SDViz system took 9-15 minutes to troubleshoot, thereby decreasing the task time by 46-52%.
5. Conclusion and Future Work
In SDViz, we applied various visualization techniques such
as highlighting, locating a 2D viewpoint, distortion viewing,
blending for transition, and animating for the exploration of
technical diagrams in order to help users understand the diagrams as well as find the desired information effectively.
Furthermore, we conducted a field study by students in the
school of Aviation Technology at Purdue University to evaluate the capabilities of our system. Based on the results from
our field study, we believe that our system can provide maintenance personnel with an effective and efficient visualization tool for maintenance, repair, and training tasks.
There are several issues to be considered as future work.
First, we can consider improving the capability of interoperations between various units such as components, pages,
diagrams, subsystems and systems. Such interoperations can
help technicians understand the current trouble with a broad
point of view about an entire system and cooperate with
other technicians. Second, integration with multimedia data
like 3D models, Macromedia Flash files, and video clips will
make SDViz be more effective for maintenance tasks. Each
component or subsystem can be displayed with 3D models
providing users with a variety of interactive operations such
as interactive viewpoints, internal 3D views, and assembly
as well as on/off operations. Video clips can provide useful
information to describe how to work with each component
and subsystem. Third, more customized focus+context techniques need to be developed for dealing with the technical
diagrams containing various complexities. Finally, using a
touch screen device may also be considered to provide maintenance personnel with more intuitive interfaces.
6. Acknowledgement

[BR06] BARNARD Y., R EISS M.: User-centred innovation of
electronic documentation for maintenance. In Developments
in Human Factors in Transportation, Design and Evaluation
(2006), Shaker Publishing, pp. 129–142.
[BRM06] BARNARD Y., R EISS M., M AZOYER P.: Mental models of users of aircraft maintenance documentation. In Proceedings of the International Conference on Human-Computer Interaction in Aeronautics (2006), pp. 232–239.
[BSB03] B OOSE M., S HEMA D., BAUM L.: A scalable solution
for integrating illustrated parts drawings into a class iv interactive
electronic technical manual. vol. 1, pp. 309–313.
[Ext] [W3C:
Extensible
Markup
Language
(XML)]
http://www.w3.org/XML/ (accessed 8 March 2009).
[FD06] F REDJ Z. B., D UCE D. A.: Grassml: accessible smart
schematic diagrams for all. In W4A: Proceedings of the 2006
international cross-disciplinary workshop on Web accessibility
(W4A) (2006), ACM, pp. 57–60.
[FDB08] F RISCH M., DACHSELT R., B RÜCKMANN T.: Towards
seamless semantic zooming techniques for uml diagrams. In SoftVis ’08: Proceedings of the 4th ACM symposium on Software visuallization (New York, NY, USA, 2008), ACM, pp. 207–208.
[Gut02] G UTWIN C.: Improving focus targeting in interactive
fisheye views. In CHI ’02: Proceedings of the SIGCHI conference on Human factors in computing systems (2002), ACM.
[HPA∗ 04] H EISER J., P HAN D., AGRAWALA M., T VERSKY B.,
H ANRAHAN P.: Identification and validation of cognitive design
principles for automated generation of assembly instructions. In
AVI ’04: Proceedings of the working conference on Advanced
visual interfaces (2004), ACM, pp. 311–319.
[HTAH03] H EISER J., T VERSKY B., AGRAWALA M., H ANRA HAN P.: Cognitive design principles for visualizations: Revealing and instantiating. In CogSci ’03: Proceedings of 25th annual
meeting of the cognitive science society (2003), pp. 545–550.
[LAS04] L I W., AGRAWALA M., S ALESIN D. H.: Interactive
Image-Based Exploded View Diagrams. Heidrich W., Balakrishnan R., (Eds.), vol. 62 of ACM International Conference Proceeding Series, Canadian Human-Computer Communications
Society, A K Peters, LTD.
[Mar97] M ARSHALL C. C.: Annotation: from paper books to the
digital library. In DL ’97: Proceedings of the second ACM international conference on Digital libraries (1997), ACM Press.

This work has been funded by the U.S. Air Force Research
Laboratory Human Effectiveness Directorate under Contract
FA8560-04-D-6546 and the U.S. Department of Homeland
Security Regional Visualization and Analytics Center of Excellence.

[PST] [PDF
Schematic
http://www.bcltechnologies.com/
March 2009).

References

[RFF∗ 08] ROBERTSON G., F ERNANDEZ R., F ISHER D., L EE
B., S TASKO J.: Effectiveness of animation in trend visualization. vol. 14, pp. 1325–1332.

[Aub03] AUBER D.: Tulip : A huge graph visualisation framework. In Graph Drawing Softwares, Mutzel P., Jünger M., (Eds.),
Mathematics and Visualization. Springer-Verlag, 2003.
[AvH04] A BELLO J., VAN H AM F.: Matrix zoom: A visual interface to semi-external graphs. In INFOVIS ’04: Proceedings of
the IEEE Symposium on Information Visualization (Washington,
DC, USA, 2004), IEEE Computer Society, pp. 183–190.
[BGBS02] BAUDISCH P., G OOD N., B ELLOTTI V., S CHRAED LEY P.: Keeping things in context: a comparative evaluation of
focus plus context screens, overviews, and zooming. In CHI ’02:
Proceedings of the SIGCHI conference on Human factors in computing systems (2002), ACM Press, pp. 259–266.

Tracing]
(accessed 8

[Rau99] R AUSCHENBACH U.: The rectangular fish eye view as an
efficient method for the transmission and display of large images.
In ICIP ’99: Proceedings of IEEE International Conference on
Image Processing (1999), pp. 115–119.

[RJS01] R AUSCHENBACH U., J ESCHKE S., S CHUMANN H.:
General rectangular fisheye views for 2d graphics. vol. 24,
pp. 609–617.
[SC05] S ETLUR V., C HEN X.: Retargeting vector animation for
small displays. In MUM ’05: Proceedings of the 4th international
conference on Mobile and ubiquitous multimedia (2005), ACM.
[vH03] VAN H AM F.: Using multilevel call matrices in large software projects. In INFOVIS ’03: Proceedings of the IEEE Symposium on Information Visualization (Los Alamitos, CA, USA,
2003), IEEE Computer Society, pp. 227–232.

c 2009 The Author(s)

c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

Stippling by Example
SungYe Kim∗

Ross Maciejewski∗

Tobias Isenberg†
William M. Andrews‡
Wei Chen§
Mario Costa Sousa¶
∗
David S. Ebert
∗ Purdue University
† University of Groningen
‡ Medical College of Georgia
§ Zhejiang University
¶ University of Calgary

Figure 1: Our results generated by synthesizing textures based on example textures extracted from stippling artist’ tone maps.

Abstract

1 Introduction

In this work, we focus on stippling as an artistic style and discuss
our technique for capturing and reproducing stipple features unique
to an individual artist. We employ a texture synthesis algorithm
based on the gray-level co-occurrence matrix (GLCM) of a texture
field. This algorithm uses a texture similarity metric to generate
stipple textures that are perceptually similar to input samples, allowing us to better capture and reproduce stipple distributions.First,
we extract example stipple textures representing various tones in order to create an approximate tone map used by the artist. Second,
we extract the stipple marks and distributions from the extracted
example textures, generating both a lookup table of stipple marks
and a texture representing the stipple distribution. Third, we use
the distribution of stipples to synthesize similar distributions with
slight variations using a numerical measure of the error between the
synthesized texture and the example texture as the basis for replication. Finally, we apply the synthesized stipple distribution to a
2D grayscale image and place stipple marks onto the distribution,
thereby creating a stippled image that is statistically similar to images created by the example artist.

Stippling is an artistic technique that relies on numerous small,
repetitive marks (stipples) to visually describe forms and objects
[Hodges 1989]. It is a black-and-white technique, i. e., 100% black
marks are made on a 100% white ground (or vice versa); there are
no gray marks. However, because the size, shape and density of the
marks can be varied, shades of gray are perceived within the stippled image. As such, stippling is capable of capturing a very wide
dynamic range of tones, from white to black. While stippling is a
well defined technique, choices in mark size, density, and irregularities in shape can all lead to slight variations, giving rise to various
styles of stippling.
In the graphics community, many algorithms have been developed
in an attempt to approximate hand-drawn stippling (e. g., Deussen
et al. [2000], Secord [2002], Lu et al. [2003], Kopf et al. [2006]),
most relying on a procedural random distribution of stipple positions rendered as round marks. Recently, researchers started
comparing computer-generated pen-and-ink techniques to illustrations hand-drawn by professional artists, including stippling. Isenberg et al. [2006] used a qualitative pile-sorting technique to determine how participants understand and assess both hand-drawn
and computer-generated pen-and-ink illustrations. This study found
that participants, in general, could correctly distinguish between

CR Categories: I.3.m [Computer Graphics]: Miscellaneous—
Non-Photorealistic Rendering
Keywords: Computer-generated stippling, stippling by example,
texture analysis and synthesis, stipple mark distribution, statistical
similarity.

Copyright © 2009 by the Association for Computing Machinery, Inc.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for commercial advantage and that copies bear this notice and the full citation on the
first page. Copyrights for components of this work owned by others than ACM must be
honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on
servers, or to redistribute to lists, requires prior specific permission and/or a fee.
Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail
permissions@acm.org.
NPAR 2009, New Orleans, Louisiana, August 1 – 2, 2009.
© 2009 ACM 978-1-60558-604-5/09/0008 $10.00

∗ {inside | rmacieje | ebertd}@purdue.edu
† isenberg@cs.rug.nl
‡ bandrews@mcg.edu
§ chenwei@cad.zju.edu.cn
¶ mario@cpsc.ucalgary.ca

41

used (i. e., images, surfaces, or volumes) as well as by the type of
output they produce: short strokes, analytic stipple points or a pixel
matrix. In this work, we focus on 2D image stippling.

Artist’s stippling work

Extraction of a tone map

Statistical Analysis

Synthesis of distribution

Stipple Distribution

Rendering with distribution

Image-based stippling uses an image, typically in gray scale, as input and tries to reproduce its tonal properties with stipple points
by choosing appropriate point placements, sizes, and shapes. Early
techniques used iterative relaxation approaches based on Lloyd’s
method which uses centroidal Voronoi diagrams to distribute the
generated stipple points. Deussen et al. [2000] presented an interactive, tool-based method which allows users to manipulate and adjust
an initial point distribution. Secord [2002] showed an automatic
technique using weighted centroidal Voronoi diagrams. Hiller et al.
[2003] later extended the Lloyd’s method to place line segments or
polygons by using their Voronoi diagrams. Dalal et al. [2006] further extended the centroidal Voronoi diagram technique to be based
on area and a primitive’s perimeter for re-centering, allowing them
also to stipple with arbitrary shapes.

Extraction of stipple marks

Placement of stipple marks

Unfortunately, these image based approaches tend to exhibit certain
artifacts, such as the tendency to form lines of stipple points. Many
methods attempt to overcome these issues by incorporating more
randomness into the stipple placement. Schlechtweg et al. [2005]
used autonomous agents (RenderBots) to place stipple points based
on an input image using relatively simple behaviors such as trying to go to dark areas and avoiding other RenderBots. Kopf et al.
[2006] used progressive and recursive Wang tiles to rapidly produce point distributions that exhibit blue noise qualities. Mould
[2007] demonstrated how to use path search in a weighted regular
graph to produce stipple point distributions that are more irregular
and follow linear features, albeit with a minimum point distance.
Vanderhaeghe et al. [2007] presented a point distribution method
for stroke-based rendering including stippling considering temporal
coherence for animation of 3D scenes and video input. Directional
stippling introduced by Kim et al. [2008] created points aligned to
edge features within an image in a structured manner that is different from previous stippling research.

Figure 2: Conceptual diagram of our stippling system.

hand-drawn and computer-generated images. In addition, Isenberg et al. found several issues associated directly with stippling
techniques. One was that participants often noticed that computergenerated stipple images employed a higher number of stipples per
unit area, and the stipple marks themselves were too regular in
comparison to their hand-drawn counterparts. Further work was
done on directly quantifying these differences by Maciejewski et al.
[2008]. This work employed statistical texture measures to quantify the differences between hand-drawn and computer-generated
stipples and suggested that such texture measurements could be
employed in texture generation. Both studies indicate that subtle
variations of stippling are detectable and affect how images are perceived.
Based on observations and discussions by Isenberg et al. [2006] and
Maciejewski et al. [2008], we have developed a novel stippling algorithm to capture and reproduce these stylistic differences. The
goal of our work is to replicate the appearance of hand-drawn stipple textures in scientific illustration. Our system takes a sample
stipple image and extracts example stipple textures. These extracted
samples are then used to generate new statistically similar textures,
which are in turn used to render 2D images in the same style as
the artist’s example image. Sample results of our stippling algorithm are shown in Figure 1. To summarize, our method, shown in
Figure 2, consists of six major steps:

Our work differs from previous work in that we focus on synthesizing stipple distributions from example stipple textures while reusing
stipple marks. Tonal textures are extracted, the stipple distributions
of the tone textures are recreated based on texture statistics, and
the distributions are used for stipple rendering while preserving the
local tonal qualities of the image. In terms of an example-based
method, Barla et al. [2006a] focused on hatching and stippling by
example through analysis and synthesis of stroke patterns based on
the similarity between neighborhoods by Barla et al. [2006b]. Ijiri
et al. [2008] introduced a procedural system for the arrangement of
2D elements that is also based on matching neighborhoods by local growth and relaxation methods. In contrast, we use the analysis
and synthesis of statistical characteristics of textures. Hatching by
example by Jodoin et al. [2002] presented a conceptually similar
approach to ours, which collects hatching strokes from an example
image, generates patches of synthetic strokes based on a statistical
model, and uses them to render 2D images or 3D objects. However,
they mainly focused on the synthesis of example strokes remaining
other parts such as automatically extracting strokes as a long term
goal. In contrast, we deal with the entire approach for examplebased stippling; from extracting example tone textures to rendering
images with our synthesized textures.

1. Automatic extraction of an artist’s tone map from an example
stipple image.
2. Extraction of the stipple distribution and stipple marks from
the tone textures.
3. Statistical analysis of texture properties.
4. Synthesis of stipple distribution to match the statistical texture
properties.
5. Given any gray-scale image, render the image using the synthesized textures including stipple distributions.
6. Placement of stipple marks.

3
2

Stipple Extraction, Analysis and Synthesis

Related Work
As previously stated, stippling is a technique that relies on repetitive marks to create perceived shades of gray within an image, and
artists commonly use a tone map as a reference to block out an image (determine the broad areas of same or similar tone). Figure 3

There are a number of approaches in NPR that attempt to replicate stippling using computer graphics. Generally, it is possible to
distinguish between techniques by the model representation being

42

Figure 4: Left: blastoid shell (© Emily S. Damstra, used with permission); middle: cell component values from our extraction algorithm; right: extracted sample textures. Cells with neighboring
numbers join together to create a large sample from which to extract uniform texture tones.

Figure 3: Tone maps created by artist William M. Andrews (©
William M. Andrews, used with permission). From left-top, it starts
from tone 0 to tone 9 in a clockwise direction.

shows a typical stippling tone map with 10 levels (including black
and white). When stippling, the artist will naturally dither the edges
between two adjacent tones to create a seamless transition.

3.2

Our method (depicted in Figure 2) utilizes the concept of an artist’s
tone map, taking an example stipple image and automatically extracting various tone levels. In each example tone, we capture the
statistics of the artist’s rendering style in terms of the distribution
of stipple marks. We synthesize a new texture to capture the artist’s
stipple distribution. Further, to maintain the stipple shape, we also
extract individual stipples from the example input and place these
marks based on the simulated distribution, thereby maintaining the
stipple distribution, size and shape used by the artist. Once new
synthesized textures are generated from the example distribution,
our system can take a grayscale image as input and stipple the image in a style similar to the given stipple artist.

Stipple Distribution and Marker Extraction

The next step in our process is to separate the stipple marks from
their distribution. To this end, we analyze the example stipple texture and create a new texture containing only the stipple distribution. Before starting an algorithmic method, we classify the example stipple textures into three groups: light, merged, and dark level.
In the light level, example stipple textures have separate black stipple marks on a white background (tones 1 to 4 in Figure 3), whereas
in the dark level, they contain separate white stipple marks on a
black background (tones 7 to 8). In the merged level, several stipple
marks merge together resulting in blobby shapes that are difficult to
analyze (tones 5 to 6). Tone 0 and 9 are pure white and black respectively. In this work, we consider textures in this merged level
to have black stipple marks on a white background.

3.1 Extracting a Tone Map

Our method for extracting the stipple distribution uses two different techniques, depending on whether or not the stipples in a given
tone level can be considered as separate or conjoined entities. For
textures containing individual (non-merged) stipples, we perform a
connected component analysis using an 8-neighborhood system to
segment stipple marks. For each segmented stipple mark, a center
position is computed. The segmented stipples are then stored for
future use in the marker placement step (Section 4.2). The resulting
texture, the example distribution texture, includes all representative
positions of stipple marks as distribution information.

In order to capture an artist’s tone map, we use a segmentation
method. We take an artist’s stipple drawing and divide the image into equally sized N × N blocks. (N depends on the size of
the artist’s stipple image, and should be small enough to represent
a single tone. In Figure 4, we use N=30.) The average gray level in
each block is then calculated as the sum of all gray values over the
total number of pixels in the block. Since we use a stipple image
containing only black or white pixels, we can calculate the average
gray level as the ratio of black and white pixels in the block.
To group blocks that have similar average gray levels within a
threshold, T1 , we perform a connected component analysis using
an 8-point neighborhood system. During the analysis, each block is
assigned a number in order to identify a segmented class of blocks,
and blocks assigned with the same number form one segmented region. We then sort the segmented regions by their average gray level
and determine the difference of gray levels between two neighboring regions. If the tonal values assigned to the segmented regions
are separated evenly within a threshold, T2, we extract a rectangular
texture from that segmented region. Figure 4 (middle) shows the
segmented regions identified by class numbers for a hand-drawn
stipple image (Figure 4 (left)) and shows eight example textures
(Figure 4 (right)) extracted from the stipple image. Although our
approach allows us to extract example textures from an artist’s stipple image, this cannot be applied to all stipple images because our
method assumes that there exists large enough regions in the stipple image for each tone level. In addition, the initial generation of
blocks may split across two apparent tones. Therefore, extraction
should be somewhat controlled by adjusting the size of a block and
the two thresholds. In the cases where our extraction method fails,
the tones may be manually extracted. In later parts of this paper,
we use William M. Andrews’ tone maps (Figure 3) as the example
stippling textures to explain our algorithm.

To create the example distribution texture in the merged level, we
use the constrained Lloyd algorithm proposed by Kim et al. [2008]
that aligns points to a feature area in an image. In our work, the
constraint is black stipple marks merged in an example stipple texture. We begin with an appropriate number of initial points selected
regularly, and then we generate the centroidal Voronoi diagrams to
spread the initial points. During optimization using the constrained
Lloyd algorithm, the points move toward the black stipple areas as
shown in Figure 5 (right) (see [Kim et al. 2008] for details).
We also perform a simple geometrical analysis for use in the synthesis process (Section 3.4), calculating the average stipple size, Savg
and offsets between the positions of stipple marks: the minimum
(Omin ), maximum (Omax ), and average (Oavg ) geometrical distances
from any stipple mark to the closest neighbor stipple mark.

3.3

Statistical Analysis

Once the tone maps are extracted and the stipple properties captured, the next step of our process is to use these example distribution textures to create synthetic distribution textures.

43

Energy = ∑ ∑ Nd~2 [i, j]
i

(3)

j

∑ ∑(i − µi )( j − µ j )Nd~ [i, j]
Correlation =
Figure 5: Distribution extracted from an example texture in a
merged level. Left is a tone 6 texture in William M. Andrews’ tone
maps. Middle shows the stipple centers (as white dots) found using
the constrained Lloyd algorithm. Right is the extracted distribution
texture with stipple centers represented as equally sized circles for
visualization purposes.

3.3.1

3.3.2

(4)

GLCMs for Stipple Textures

Given the example distribution and seed textures, the texture
GLCMs are calculated. Since the input textures are bitonal images,
we consider only two gray levels (black and white), meaning that
all GLCMs calculated will be of size 2 × 2 matrices. We calculate
(n − 1) GLCMs for the n × n example distribution and seed textures
for a given displacement offset vector. Since the GLCM calculation
depends on the direction of the offset vector if only one direction is
used for the offset vector, undesired patterns can occur along other
directions during synthesis. To reduce this issue we need to combine several different spatial relationships, linearly increasing the
number of GLCMs. Here, we use three different spatial relationships; horizontal (0°), vertical (90°), and left-down diagonal (45°)
for GLCM generation. Therefore, we calculate 3 × (n − 1) GLCMs
for each of the n × n example distribution and seed textures, and use
them for our synthesis process.

We use the GLCM algorithm to analyze texture statistics as opposed
to more recent algorithms, such as advanced implementations of the
Gray-Level Difference Histograms [Chetverikov 1994] and GrayLevel Aura Matrices [Qin and Yang 2005] or the use of joint texture
statistics [Portilla and Simoncelli 2000]. While methods such as
those are appropriate for texture analysis and synthesis, their ability to identify structures in textures is not needed because stipple
artists try to avoid producing oriented textures or unintended patterns [Hodges 1989]. As such, methods that measure the anisotropy
and texture symmetry are not necessary and may not be adequate
for comparing stipple textures.

3.4

Synthesis of Stipple Distribution

Our synthesis step generates new distributions from the example
distribution textures by iteratively minimizing the error between the
example distribution texture and the seed texture’s GLCMs while
the current error between them is reduced within a maximum iteration. This is very time-consuming step if an accelerating method
is not provided. In our work, combining geometrical information
such as Omin , Omax , Oavg , and Savg (computed while extracting the
stipple distribution from the example texture in Section 3.2) into the
synthesis process improves our performance by reducing the number of considered pixels.

A GLCM [Haralick et al. 1973] is a two-dimensional array L in
which the rows (r) and columns (c) represent a set of possible gray
values G. The value of Ld~ [i, j] indicates how many times value i
~
co-occurs with value j in a given spatial relationship defined by d.
If we set d~ to be a displacement offset vector [dr, dc] where dr is the
displacement in rows and dc is the displacement in columns, then
the co-occurrence matrix Ld~ [i, j] for some image, I, is defined by

Traditional 2D texture synthesis techniques can be roughly grouped
into two categories: non-parametric and parametric texture synthesis. The former aims to directly fetch samples from the input texture in either a per-pixel [Efros and Leung 1999; Wei and Levoy
2000] or a per-patch approach [Liang et al. 2001; Cohen et al. 2003;
Kwatra et al. 2003]. Parametric texture synthesis constructs a generative model with a set of parameters to guide the synthesis process. Representative work includes histogram matching [Heeger
and Bergen 1995], minimum entropy statistics [Zhu et al. 1997]
and GLCM/GLAM-based techniques [Copeland et al. 2001; Qin
and Yang 2005]. These works synthesize new textures by matching
the corresponding joint statistics of the input and output images.

(1)

Previous work in analyzing stipple textures [Maciejewski et al.
2008] demonstrated that GLCM statistics can be used to illustrate
the differences between hand-drawn and computer-generated stipple images. The following three texture statistics were used: contrast, energy, and correlation:

i

σi σ j

Our algorithm (Figure 2) uses a sample stipple texture representing
a single tone as an input. We refer to this input as the example stipple texture. We then modify the example stipple texture to contain
only the stipple distribution resulting in the example distribution
texture (as described in Section 3.2). Next, our algorithm synthesizes a new stipple distribution that is similar to the example distribution texture with slight variations, allowing us to generate various
distributions from only one example distribution. To do this, our algorithm requires another input texture, denoted as the seed texture,
that can be an empty white texture.

There are two common approaches to analyzing textures, the structural approach and the statistical approach. While the structural
approach works well for regular patterns, the lack of discernible
patterns in stippling requires using the statistical approach. There
are many statistical texture analysis algorithms designed to represent textures for comparison; for example, GLCM [Haralick et al.
1973], Fourier power spectra, and texture spectra are some of the
more common approaches. We have chosen the GLCM algorithm
to analyze stipple textures because perceptual psychology studies
[Julesz et al. 1973] have shown that GLCM closely matches levels
of human perception. Furthermore, previous research [Davis et al.
1979; Gotlieb and Kreyszig 1990; Lohmann 1994; Copeland et al.
2001] has shown that GLCM is a powerful tool for texture analysis,
synthesis, segmentation, and classification.

Contrast = ∑ ∑(i − j)2 Nd~ [i, j]

j

~ µi , µ j are the means and
where Nd~ is a normalized GLCM for d,
σi , σ j are the standard deviations of the row and column sums of
~ N~ .
the normalized GLCM for d,
d

Texture Statistics

Ld~ [i, j] = |{[r, c] | I[r, c] = i and I[r + dr, c+dc] = j}|

i

(2)

j

44

the algorithm proceeds to the next randomly chosen pixel until all
pixels in the newly synthesized texture are considered. This pixel
flipping reduces the error between the GLCM values of our example
distribution texture and the synthesized texture, resulting in a newly
synthesized texture with a minimum error in terms of the GLCMbased characteristics such as Equation (2), (3) and (4). This also indicates our algorithm is a discrete grid-based method assuming that
all stipples are placed on discrete pixel positions such as black pixels on a white background or white pixels on a black background.
3.4.2

During the GLCM error minimization phase, the pixels in the
newly synthesized texture are chosen randomly for flipping;
however, we constrain this random choice to maintain the average
offset (Oavg ) computed in Section 3.2. Once a random pixel is
chosen, a stipple region around the pixel is determined with a
radius D as follows:

Figure 6: Texture synthesis of example textures using Portilla and
Simoncelli’s [2000] method. Top: example stipple textures; bottom: synthesized texture after 25 iterations. Obvious artifacts are
denoted with red rectangles in the synthesized textures.

D=O+R
O = {Omin , Oavg or Omax }

While such an approach fails to properly capture the stipple shape,
the properties captured are sufficient. Other issues arise when trying to capture stipple shape in non-parametric models. For example, Figure 6 shows the texture synthesis results using the method
developed in Portilla and Simoncelli [2000]. The results are representative of the known cases where the technique fails (e. g., a
set of ellipses in which the synthesis fails to close the ellipses).
Note the open stipples and smudges in the synthesized textures. As
such, synthesizing stipple textures with appropriate statistics is not
as straightforward as it may seem.
3.4.1


 random(0, Oavg -Omin )
0
R=
 random(O -O , 0)
avg max

We employ the average co-occurrence error (ACE) [Copeland et al.
2001] as a metric to minimize error between the GLCMs because it
has been found to be highly correlated with human judgments of the
visual distinctness between textures [Copeland and Trivedi 1998]:
1

1

1

∑ ∑ ∑ |Nt,d~ [i, j] − Ns,d~ [i, j]|

TNGLC ~

if O=Omin
if O=Oavg
if O=Omax

where D is the radius of a circle for a stipple region, and O is assigned one of three offsets. The offset assigned is chosen based on
a probability distribution (we use 0.1, 0.1, and 0.8 for minimum,
maximum, and average offsets, respectively). R is a random factor based on the offset selected. Thus, D has the range of Omin to
Omax and tries to be close to Oavg . The offset information (Omin ,
Oavg , and Omax ) is updated whenever new stipple distribution is
placed into the synthesized texture. If the random pixel is flipped
in the MSF algorithm, we set all pixels within the radius D of the
newly flipped pixel to “considered,” meaning that the pixels cannot
be chosen for flipping during the current iteration. As such, we can
avoid random selections within any stipple regions. However, if all
pixels become marked as considered, no pixels are left for random
selection although the MSF algorithm may not yet have reached its
minimum or the average gray value of the example distribution texture. In this case, we arbitrarily assign Omin to D until pixel flipping
can occur in the GLCM error minimization. This stipple region determination using geometrical information allows us to efficiently
reduce the number of pixels considered during the synthesis process and maintains the geometrical information from the example
texture.

GLCM Error Minimization

ACE =

Combining geometrical information

(5)

d∈D i=0 j=0

where TNGLC is the number of displacements, D is the set of displacement offset vectors (see [Copeland et al. 2001] for details), and
Nt,d~ [i, j] and Ns,d~ [i, j] are the normalized GLCM values of the example distribution texture and the synthesized texture respectively
for i, j, and d~ in the current iteration.

3.5

The ACE calculation is then performed for each GLCM directional
pair. We calculate the ACE between the GLCMs calculated with
a horizontal offset vector for both the example distribution texture
and the seed texture, and repeat this calculation for other displacement offset vectors used in the GLCM calculations. This maximizes texture similarity in three directions and reduces the possibility of unwanted patterns emerging. Using the ACE metric, we
employ the Metropolis Spin-Flip (MSF) algorithm in Metropolis
et al. [1953] to modify the seed texture. In the MSF algorithm, the
gray value of a random pixel, which is chosen from our seed texture, is changed from either black to white or white to black. Since
we are now modifying the seed texture, it is more appropriate to
refer to it as the synthesized texture. If the resultant ACE between
the example distribution texture and the newly synthesized texture
is lower than the ACE between the example distribution texture and
the previously synthesized texture for all displacement offset vectors, the pixel remains flipped, otherwise, the pixel flips back and

Synthesis Results and Analysis

In order to demonstrate our stipple texture synthesis process, we
have synthesized a series of textures from an artist’s example textures. Figure 7 illustrates several examples textures (top row), the
distributions extracted from the examples textures (middle row),
and our synthesized distributions (bottom row) corresponding to
the example distribution textures. To verify the accuracy of our
synthesis, we have performed a GLCM statistical analysis on the
textures, comparing the texture properties of the synthesized distribution textures to the example distribution textures. Figure 8 shows
this analysis for the correlation texture statistics using a horizontal offset vector. Results are comparable for other offset vectors
(e.g., vertical, diagonal). In Figure 8, our synthesized distribution
textures (indicated by light red boxes in Figure 7) for tones 1, 4,
6 and 8 are compared to their corresponding example distribution
textures (light blue boxes in Figure 7) in William M. Andrews’ tone

45

Figure 7: From top to bottom, hand-drawn stipple tone textures by William M. Andrews, example distribution extracted by our method in
Section 3.2, and our synthesized distribution textures by the algorithm in Section 3.4 corresponding to the example distribution textures.
The colored boxes at tones 1, 4, 6 and 8 indicate the textures used for comparison in Figure 8. (For visualization purpose, pixels for the
distribution are drawn larger than they actually are.)
0.7
Andrews−Example−Dist.
Andrews−Synthetic−Dist.

0.7
Andrews−Example−Dist.
Andrews−Synthetic−Dist.

0.6

0.5

0.4

0.4

0.4

Correlation

0.5

0.3

0.3
0.2

0.2

0.1

0.1

0.1

0

0

0

2

4

6

8
10
12
Horizontal Offset

14

16

18

20

−0.1
0

2

4

6

8
10
12
Horizontal Offset

14

16

18

−0.1
0

20

Andrews−Example−Dist.
Andrews−Synthetic−Dist.

0.7
0.6
0.5

0.3

0.2

−0.1
0

0.8
Andrews−Example−Dist.
Andrews−Synthetic−Dist.

0.6

0.5

Correlation

Correlation

0.6

Correlation

0.7

0.4
0.3
0.2
0.1
0

2

4

6

8
10
12
Horizontal Offset

14

16

18

20

−0.1
0

2

4

6

8
10
12
Horizontal Offset

14

16

18

20

Figure 8: From left to right, comparison of texture correlations as a function of offset between artist’s stippling distribution and our synthesized distribution for tones 1, 4, 6, and 8 in Figure 7.

4

maps. Here we see that the correlation property of the synthesized
distribution textures is similar to that of the example distribution
textures.

Stipple Rendering

Having demonstrated that our stippling synthesis algorithm generates textures with texture statistics similar to the example stipple
textures given as input, we now proceed to use these textures to
create 2D stipple images in a manner similar to hand-drawn illustrations. Some artists use a continuous tone map as a reference, especially during the early development stages of a drawing when an
illustrator is blocking out while determining the broad areas of the
same or similar tone. For this purpose, we first create a distribution
image using only the synthesized distribution textures in per-pixel
rendering. Then, to render stipple marks, we use the stipple marks
extracted from example stipple textures in Section 3.2.

We also computed a correlation coefficient measure of the similarity between the correlation statistics of the synthesized and example
distributions. This correlation coefficient measures the strength and
the direction of a linear relationship between two variables so that
its absolute value is 0 to 1, providing a simple metric for quick
analysis. Generally, the absolute value of a coefficient greater than
0.8 is described as strong, whereas the absolute value of a coefficient less than 0.5 is described as weak. From our computation, our
synthesized distribution textures (correlation coefficients = 0.9993,
0.9996, 0.9971, 1.0 for tones 1, 4, 6 and 8) are highly correlated
with those of the corresponding example distribution textures. Note
that the graphs represent the correlation of black and white pixels
with respect to their distance from the origin where as the correlation coefficient denotes the similarity of the correlation statistics
between the synthesized and example distributions.

4.1

Rendering with stipple distribution

To render 2D stippling images, we approximate a technique used
by illustrators to match the continuous target tone by using discrete
tone maps as done in many previous NPR works (e. g., [Winkenbach and Salesin 1994; Praun et al. 2001; Lee et al. 2006]). We utilize the previously synthesized distribution textures and apply them
to grayscale images. However, the method in which we apply the
distribution texture differs from the previous methods that use texture mapping or blending: our method deals solely with pixels describing the distribution of stipple primitives. Thus, all synthesized
distribution textures are assigned their average gray value through
pre-simulation in which stipple marks are placed on the synthesized

The performance of our synthesis step depends on the texture size
(n), the number of stipples, and the structural information. In our
system, the average synthesis time is approximately 12 minutes
for a 256 × 256 texture taken from a William M. Andrews’ tone
map (maximum iteration is set to 30, Omin = 14.3, Omax = 40.8,
Oavg = 29.7, and Savg = 33.7). Note that this synthesis only needs
to be performed once for each artist’s example stipple textures.
Once completed, the synthesized textures are used to render images
without repeating the synthesis.

46

P

pixel

1.0
tone 1 synthesized
distribution texture

tone 2 synthesized
distribution texture

tone 3 synthesized
distribution texture

grayscale image

pl

tone N synthesized
distribution texture

pr

distribution image

gavg,(i-1)
Figure 9: Our rendering using stipple distribution textures.

gavg,(i+1)

Figure 10: Probability-based tone level selection used in our rendering step using distribution textures.

distribution. We then fill every pixel in an input grayscale image
with our synthesized distribution textures by using our tone level
selection based on probability of the gray values within the input
image.

place white stipple marks. Therefore, we need to know which pixel
in the distribution image is assigned from which tone level. Strictly
speaking, we only need to know which pixel comes from the tone
levels using either black stipple marks or white stipple marks. Thus,
we use a level map that maintains a tone level for each stipple position to assign the appropriate stipple color. Finally, from a set of
stipple marks, a random mark is uniformly selected and placed onto
the distribution image, resulting in the final stipple image.

Our rendering from the stipple distribution is illustrated in Figure 9,
resulting in a distribution image as an output. In Figure 9, we determine a tone level for each pixel based on a gray value in the
input image (light blue lines), and set a pixel value from our synthesized distribution texture corresponding to the tone level into the
distribution image (red lines). In this manner, the distribution image contains the entire distribution information of stipple marks for
a grayscale image.

Our stipple marks placement, using the distribution image based on
the probability of the gray values, creates unnecessary black and
white pixels in the area rendered with middle level textures, although the overall appearance shows a smooth tone transition as
shown in Figure 11 (left). In halftoning, there was also a similar problem, and there have been efforts such as Jodoin and Ostromoukhov [2001] and Zhou and Fang [2003] to solve this problem. We have tried to address this issue by identifying the small
pixel chunks, both white on a black background and black on a
white background, and moving them toward their true nearest stipple mark (see Figure 11 for a before and after image). While this
removes the individual pixels successfully, it is not able to reproduce the blobby merging visible in hand-drawn stippling images.
A more advanced technique for reproducing this pattern is necessary, which we leave for future work.

Due to the limited number of tone levels, quantization artifacts may
be introduced during the selection of a tone level for each pixel
in a grayscale image. This is related to the difficulty of extracting
appropriate example textures from an artist’s work. To solve this
problem, we employ a method based on probability for a gray value
from an image. In Figure 10, gn is a gray value of a pixel n within an
input image, and gavg,i is the average gray value of the ith tone level.
We select either ith or (i + 1)th distribution texture with probability
pl or pr respectively for the pixel n. This method preserves the
statistical characteristics in our synthesized distribution textures as
well as creates an image with continuous tones between discrete
tone levels.
Our method is resolution dependent. Hence, to insure high quality
results, the input grayscale images should have a similar resolution
to that of the example stipple image from which we extracted the
example stipple textures. Moreover, since we reuse stipple marks
extracted from the example stipple textures, the resolution of an
input grayscale image considerably affects the final image quality.
Another issue in terms of resolution is that our synthesized distribution textures may be smaller than an input grayscale image. In
such cases, we generate larger distribution textures by using multiple synthesized distribution textures. However, undesirable patterns
may emerge in such a method. Our synthesis process minimizes
these issues as each synthesized distribution texture will have slight
variations. We further minimize these patterns by applying a random rotation to each small texture when merging them into their
larger counterparts. Other possible techniques that could be used
to minimize such patterns are image quilting by Efros and Freeman
[2001] and Wang tiles by Cohen et al. [2003]. In this work, all results are rendered with 4096 × 4096 distribution textures stitched by
randomly rotating 256 × 256 synthesized distribution textures.

4.2

gavg,i gn

Figure 11: Limitation: middle tonal ranges are not yet true to
hand-drawn examples; before & after removing small pixel chunks.

5

Results and Discussion

To demonstrate our method, we used hand-drawn stipple images by
William M. Andrews, as sample inputs for our system. We generate
a set of textures based on these examples and apply the synthesized
textures to various grayscale images (see Figures 1, 12, and 13).Figure 12 compares our result to a similar hand-drawn stippling image.
Figure 13 shows our result compared to other computer-generated
stippling images by Secord [2002] and Schlechtweg et al. [2005].
The results illustrate the strength of our method in comparison to
other computer generated methods in terms of appearing to have
qualities similar to a hand-drawn image. Note that when comparing
our result to the other computer generated results, we have less inherent structure, providing a less rigid feel within the images. However, our is not without limitations; Figure 12 (right) presents more
chunky stipples than that of the original hand-drawn image, due
to the random selection of stipple marks without considering the
subtle size and orientation of stipple marks. Furthermore, the tone

Placement of Stipple Marks

For the placement of stipple marks into the distribution image from
Section 4.1, we use the stipple marks extracted from the example stipple textures to better simulate actual hand-drawn stippling.
Since our method uses two colors of stipple marks (black and
white), we separate this stipple placement into two steps; first, we
place black stipple marks onto the distribution image, and then we

47

Figure 12: Stipple result for a grayscale image (middle) of a polygonal rendering of a kidney model using textures synthesized from William
M. Andrews tone maps (right). The left image is an original hand-drawn stippled medical illustration by William M. Andrews of a kidney, not
based on the same model as our result (© William M. Andrews, used with permission).

Figure 13: Gallery of our stipple result using textures synthesized from William M. Andrews’ tone maps. Left image shows grayscale input,
middle-left is our result. Middle-right and right images are the computer-generated results by [Secord 2002] and [Schlechtweg 2005].

transition issue mentioned in Section 4.2 needs to be improved for
higher quality rendering.

synthesized distributions are statistically similar to that of the example textures as shown through the use of GLCMs, and our rendering method creates stipple images while maintaining the statistical characteristics, creating a perceptually similar appearance to
the example stipple work. Our results move past “machine” rendering and its mathematical precision to “emulative” rendering and its
subtle variations. Our technique can further be applied to computergenerated stipple renderings and other hand-drawn images to cap-

6 Conclusions and Future Work
We have shown that our stippling by example technique is capable of capturing and reproducing stylistic variations of artists. Our

48

ture any stippling style, whether it is mechanical or expressive. As
such, a system can be developed in which a user can extract stipple samples from their favorite artist and reproduce their style on
any number of images. Furthermore, this technique bridges the gap
found between images rendered using current NPR techniques and
those rendered by hand.

Human Perceptual Cues. Optical Engineering 37, 2 (Feb.), 582–
591.
C OPELAND , A. C., R AVICHANDRAN , G., AND T RIVEDI , M. M.
2001. Texture Synthesis Using Gray-level Co-occurrence Models: Algorithms, Experimental Analysis, and Psychophysical
Support. Optical Engineering 40, 11 (Nov.), 2655–2673.

Overall, our current results show that it is possible to capture and
replicate properties from example stipple inputs. However, our
method has several limitations. First, it is not always possible to extract example textures from hand-drawn stipple images as we mention in Section 3.1. Moreover, there are issues with representing
the middle tonal ranges since we assume white stipple marks in a
dark tone level as mentioned in Section 4.2. However, it is reasonable to do this algorithmically because stipple artists strive for the
same distributions of the white spots in dark regions as they do for
the black stipples on a white background. In addition, stippling is
more than statistics. Hence, there are many issues such as structure
awareness. However, in this work we focus on capturing and reproducing statistical characteristics represented in hand-drawn stipple
images. Furthermore, in our method, support of resolution independence, such as in the work by Kopf et al. [2006], and the representation of continuous positions of stipple marks on a discrete pixel
grid is left for future work.

DALAL , K., K LEIN , A. W., L IU , Y., AND S MITH , K. 2006. A
Spectral Approach to NPR Packing. In Proc. NPAR, ACM, New
York, 71–78.
DAVIS , L., J OHNS , S., AND AGGARWAL , J. 1979. Texture Analysis Using Generalized Co-occurrence Matrices. IEEE Trans.
PAMI 1, 3, 251–259.
D EUSSEN , O., H ILLER , S., VAN OVERVELD , C., AND
S TROTHOTTE , T. 2000. Floating Points: A Method for Computing Stipple Drawings. Computer Graphics Forum 19, 3 (Sept.),
40–51.
E FROS , A., AND F REEMAN , W. 2001. Image Quilting for Texture
Synthesis and Transfer. In Proc. SIGGRAPH, ACM Press, New
York, 341–346.
E FROS , A., AND L EUNG , T. 1999. Texture Synthesis by NonParametric Sampling. In Proc. ICCV, IEEE, Los Alamitos,
vol. 2, 1033–1038.

Even with these limitations, our algorithm is able to better approximate hand-drawn stippling than other current comparable methods
as shown in Figure 13. Furthermore, once the texture synthesis has
been performed, rendering can be done on the fly. For example, Figure 1 (left) (4096 × 4096 resolution, stipple count = 33104 (black),
6064 (white)) takes less than 30 seconds (average 28.16 seconds)
on Intel Xeon(R) CPU 2.66GHz processor and 3 GB of RAM. As
such, our system is able to readily render any grayscale images in
an artist’s style once the tone map extraction and synthesis are completed.

G OTLIEB , C. C., AND K REYSZIG , H. E. 1990. Texture Descriptors Based on Co-occurrence Matrices. Computer Vision, Graphics and Image Processing 51, 1 (July), 70–86.
H ARALICK , R. M., S HANMUGAM , K., AND D INSTEIN , I. 1973.
Textural Features for Image Classification. IEEE Trans. Systems,
Man, and Cybernetics 3, 6 (Nov.), 610–621.
H EEGER , D. J., AND B ERGEN , J. R. 1995. Pyramid-Based Texture Analysis/Synthesis. In Proc. SIGGRAPH, ACM Press, New
York, 229–238.

Acknowledgements
We would like to thank Emily S. Damstra and Tobias Germer for
permission to use their images in Figure 4 and 13 respectively. We
also thank the anonymous reviewers for their careful and valuable
comments and suggestions. This research has been funded by the
U.S. National Science Foundation (NSF) and the U.S. Department
of Homeland Security (DHS), and supported in part by Discovery
Grants from the Natural Sciences and Engineering Research Council of Canada (NSERC) and Natural Science Foundations of China
(Nr.60873123).

H ILLER , S., H ELLWIG , H., AND D EUSSEN , O. 2003. Beyond
Stippling – Methods for Distributing Objects on the Plane. Computer Graphics Forum 22, 3 (Sept.), 515–522.

References

I SENBERG , T., N EUMANN , P., C ARPENDALE , S., S OUSA , M. C.,
AND J ORGE , J. A. 2006. Non-Photorealistic Rendering in Context: An Observational Study. In Proc. NPAR, ACM Press, New
York, 115–126.

H ODGES , E. R. S., Ed. 1989. The Guild Handbook of Scientific
Illustration. Van Nostrand Reinhold, Hoboken, NJ.
I JIRI , T., M ECH , R., I GARASHI , T., AND M ILLER , G. 2008.
An example-based procedural system for element arrangement.
Computer Graphics Forum 27, 2, 429–436.

BARLA , P., B RESLAV, S., M ARKOSIAN , L., AND T HOLLOT, J.
2006. Interactive hatching and stippling by example. Tech. rep.,
INRIA.

J ODOIN , P.-M., AND O STROMOUKHOV, V. 2001. Error Diffusion With Blue-Noise Properties for Midtones. In Proceedings of SPIE Color Imaging: Device-Independent Color, Color
Hardcopy, and Applications VII, SPIE, Bellingham, Washington,
R. Eschbach and G. G. Marcu, Eds., vol. 4663 of SPIE Proceedings Series, 293–301.

BARLA , P., B RESLAV, S., T HOLLOT, J., S ILLION , F. X., AND
M ARKOSIAN , L. 2006. Stroke Pattern Analysis and Synthesis.
Computer Graphics Forum 25, 3 (Sept.), 663–671.
C HETVERIKOV, D. 1994. GLDH Based Analysis of Texture
Anisotropy and Symmetry: An Experimental Study. In Proc.
ICPR, IEEE Computer Society, Los Alamitos, vol. 1, 444–448.
C OHEN , M. F., S HADE , J., H ILLER , S., AND D EUSSEN , O. 2003.
Wang Tiles for Image and Texture Generation. ACM Transactions on Graphics 22, 3 (July), 287–294.

J ODOIN , P.-M., E PSTEIN , E., G RANGER -P ICH É , M., AND O S TROMOUKHOV, V. 2002. Hatching by example: a statistical approach. In NPAR ’02: Proceedings of the 2nd international symposium on Non-photorealistic animation and rendering, ACM,
New York, NY, USA, 29–36.

C OPELAND , A. C., AND T RIVEDI , M. M. 1998. Signature
Strength Metrics For Camouflaged Targets Corresponding To

J ULESZ , B., G ILBERT, E. N., S HEPP, L. A., AND F RISCH , H. L.
1973. Inability of Humans to Discriminate Between Visual Tex-

49

tures that Agree in Second-Order Statistics – Revisited. Perception 2, 4, 391–405.

W EI , L.-Y., AND L EVOY, M. 2000. Fast Texture Synthesis using Tree-Structured Vector Quantization. In Proc. SIGGRAPH,
ACM Press, New York, 479–488.

K IM , D., S ON , M., L EE , Y., K ANG , H., AND L EE , S. 2008.
Feature-Guided Image Stippling. Computer Graphics Forum 27,
4, 1209–1216.

W INKENBACH , G., AND S ALESIN , D. 1994. Computer-Generated
Pen-and-Ink Illustration. In Proc. SIGGRAPH, ACM, New York,
91–100.

KOPF, J., C OHEN -O R , D., D EUSSEN , O., AND L ISCHINSKI , D.
2006. Recursive Wang Tiles for Real-Time Blue Noise. ACM
Transactions on Graphics 25, 3 (July), 509–518.

Z HOU , B., AND FANG , X. 2003. Improving mid-tone quality of
variable-coefficient error diffusion using threshold modulation.
In SIGGRAPH ’03: ACM SIGGRAPH 2003 Papers, ACM, New
York, NY, USA, 437–444.

K WATRA , V., S CH ÖDL , A., E SSA , I., T URK , G., AND B OBICK ,
A. 2003. Graphcut Textures: Image and Video Synthesis Using
Graph Cuts. ACM Transactions on Graphics 22, 3 (July), 277–
286.

Z HU , S. C., W U , Y. N., AND M UMFORD , D. 1997. Minimax Entropy Principle and its Application to Texture Modeling. Neural
Computation 9, 8 (Nov.), 1627–1660.

L EE , H., K WON , S., AND L EE , S. 2006. Real-Time Pencil Rendering. In Proc. NPAR, ACM Press, New York, 37–45.
L IANG , L., L IU , C., X U , Y.-Q., G UO , B., AND S HUM , H.-Y.
2001. Real-Time Texture Synthesis by Patch-Based Sampling.
ACM Transactions on Graphics 20, 3 (July), 127–150.
L OHMANN , G. 1994. Co-occurrence-based Analysis and Synthesis of Textures. In Proc. Pattern Recognition, IEEE Computer
Society, Los Alamitos, vol. 1, 449–453.
L U , A., M ORRIS , C. J., TAYLOR , J., E BERT, D. S., H ANSEN ,
C. D., R HEINGANS , P., AND H ARTNER , M. 2003. Illustrative
Interactive Stipple Rendering. IEEE TVCG 9, 2 (Apr.–June),
127–138.
M ACIEJEWSKI , R., I SENBERG , T., A NDREWS , W. M., E BERT,
D. S., S OUSA , M. C., AND C HEN , W. 2008. Measuring Stipple Aesthetics in Hand-Drawn and Computer-Generated Images.
IEEE Computer Graphics & Applications 28, 2 (Mar./Apr.), 62–
74.
M ETROPOLIS , N., ROSENBLUTH , A., ROSENBLUTH , M.,
T ELLER , A., AND T ELLER , E. 1953. Equation of State Calculations by Fast Computing Machines. Journal of Chemical
Physics 21, 6 (June), 1087–1092.
M OULD , D. 2007. Stipple Placement using Distance in a
Weighted Graph. In Proc. CAe, Eurographics Assoc., Aire-laVille, Switzerland, 45–52.
P ORTILLA , J., AND S IMONCELLI , E. P. 2000. A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients. International Journal of Computer Vision 40, 1 (Oct.),
49–70.
P RAUN , E., H OPPE , H., W EBB , M., AND F INKELSTEIN , A.
2001. Real-Time Hatching. In Proc. SIGGRAPH, ACM Press,
New York, 581–590.
Q IN , X., AND YANG , Y.-H. 2005. Basic Gray Level Aura Matrices: Theory and its Application to Texture Synthesis. In Proc.
ICCV, IEEE Computer Society, Los Alamitos, vol. 1, 128–135.
S CHLECHTWEG , S., G ERMER , T., AND S TROTHOTTE , T. 2005.
RenderBots—Multi Agent Systems for Direct Image Generation. Computer Graphics Forum 24, 2 (June), 137–148.
S ECORD , A. 2002. Weighted Voronoi Stippling. In Proc. NPAR,
ACM Press, New York, 37–43.
VANDERHAEGHE , D., BARLA , P., T HOLLOT, J., AND S ILLION ,
F. 2007. Dynamic point distribution for stroke-based rendering.
In Rendering Techniques 2007 (Proceedings of the Eurographics
Symposium on Rendering), 139–146.

50

A Correlative Analysis Process in a Visual Analytics Environment
Abish Malik∗

Ross Maciejewski†

Yun Jang‡

Whitney Huang∗

Purdue University, USA

Arizona State University, USA

Sejong University, South Korea

Purdue University, USA

Niklas Elmqvist∗

David S. Ebert∗

Purdue University, USA

Purdue University, USA

A BSTRACT
Finding patterns and trends in spatial and temporal datasets has
been a long studied problem in statistics and different domains of
science. This paper presents a visual analytics approach for the
interactive exploration and analysis of spatiotemporal correlations
among multivariate datasets. Our approach enables users to discover correlations and explore potentially causal or predictive links
at different spatiotemporal aggregation levels among the datasets,
and allows them to understand the underlying statistical foundations that precede the analysis. Our technique utilizes the Pearson’s
product-moment correlation coefficient and factors in the lead or
lag between different datasets to detect trends and periodic patterns
amongst them.
Keywords: Visual analytics, correlative analysis
1 I NTRODUCTION
With more and more agencies collecting and storing data pertaining
to different processes, modern datasets often become complex and
too large to handle, triggering a need for solutions that make the
exploration and analysis of these datasets more manageable. One
specific opportunity created by such multivariate and multisource
data is the potential to look for correlations and explore possible
causal or predictive links from these datasets. The challenges with
such analysis, though, include end-users and decision makers understanding the underlying statistical algorithms, applying the algorithms at the appropriate temporal and spatial scale, and dealing
with the noisiness of the real-world data. Therefore, a visual analytics environment to enable analysts to explore and understand
potential correlations at different scales and with various certainties
is necessary for effective decision making.
In this paper, we present a visual analytics approach for exploring correlations among multivariate spatiotemporal datasets. Our
correlative analysis framework enables users to detect trends and
patterns among datasets using statistical techniques that are oftentimes complex and difficult to understand by both novice and advanced users. Furthermore, we argue that if the users were to rely
on mere statistical results without the aid of any interactive visualization or visual analytic tools, there remains the potential that
they may miss certain important details. The use of visual representations of data and of the different analytical processes minimizes these risks and provides more insights to users than traditional methods.
Our correlative visual analytics framework also enables users to
understand the benefits and harms of performing their analyses over
aggregate statistics, where datasets are aggregated and compared
over spatial (e.g., by census blocks or census tracts), or temporal
(e.g., by day, week, month or year) units. For example, users may
∗ e-mail:

{amalik|huang251|elm|ebertd}@purdue.edu
rmacieje@asu.edu
‡ e-mail: jangy@sejong.edu
† e-mail:

IEEE Symposium on Visual Analytics Science and Technology 2012
October 14 - 19, Seattle, WA, USA
978-1-4673-4753-2/12/$31.00 ©2012 IEEE

find no relevant patterns or anomalies when they perform an analysis over large spatial or temporal regions but may discover trends
over customized spatiotemporal regions. We incorporate this concept in our system, and allow users to interactively correlate any
data subsets to assist them in the discovery of underlying patterns
and anomalies.
Our current work builds upon our previous framework [26] that
has been developed to explore and analyze spatiotemporal datasets.
A screenshot of our system is shown in Figure 1. This system provides users with a suite of analytical tools that are coupled with
interactive visual interfaces for exploring criminal, traffic and civil
(CTC) datasets. Our system allows users to observe patterns and
quickly identify regions with higher probabilities of activity. Our
current work focuses on interactive correlative visualization and
analysis methods of spatiotemporal datasets in a visual analytics
environment. We emphasize that although the examples provided
in this paper are based in the law enforcement domain, our approach
is extendible to any spatiotemporal dataset framework.
2

R ELATED W ORK

In recent years, there has been much work exploring multivariate
datasets in the spatiotemporal domain that enables the discovery
and detection of trends, patterns and anomalies (e.g., [14, 25, 30,
32]). In this section, we discuss previous work in the areas of spatiotemporal analysis and visualization.
2.1

Time series visualizations

The visualization of time series data is one of the most common
problems in any data domain, and, as such, much work has been
done to address the challenges associated with visualizing time series data. Common methods of visualizing time series datasets include line graphs and bar charts. An overview of some existing
techniques for visualizing time series datasets can be found in [1].
Our work primarily utilizes line and bar graphs, clock views and
calendar view visualizations [33] to support a temporal visualization of the data.
Researchers have also explored different techniques that provide
interactive visual analysis methods for time series datasets. Bade et
al. [4] present different interactive visualization techniques that reveal data at different levels of detail and abstraction. They augment
the standard temporal visualizations to include implicit information
with regards to the data as well as a priori knowledge pertaining to
the data. Zhao et al. [34] present an exploratory time-series visualization technique that allows users to focus on areas of interest in
the data through magnification and visual filtering. Javed et al. [19]
also explore user performance for different line graph techniques
that involve multiple time series to provide guidelines for designers
designing temporal visualization applications. Buono et al. [8] provide a review of techniques that enable users to interactively query
time series datasets and present a data exploration tool that allows
users to explore multidimensional datasets in an Overview+Detail
design. Their system enables users to interactively search for existing patterns to find similar temporal occurrences and trends. We
adapt an Overview+Detail design for the visualization of datasets
by providing users with different temporal aggregation levels.

33

Figure 1: A screenshot of our correlative visual analytics system. Here, the user is visualizing all crimes related to noise complaints and
drunkenness/public intoxication in Tippecanoe County, IN. The main viewing area (a) shows the map view with the points showing the locations
of the selected offenses on the map for the week of 1/3/2011 to 1/9/2011. The top-right window (b) shows an interactive menu that allows users
to filter through the variables of the datasets. The middle window (c) shows the weekly time-series view of the selected incident reports, where
the user is performing a correlative analysis of the system, the results of which are displayed in the correlation vs. lead/lag graph (d). The region
of overlap between the time series in (c) is colored by the corresponding correlation value based on a divergent color scale (e). The left window
(f) shows the calendar view for the year 2011, and the right window (g) shows a clock view of the selected incident report data for the week of
1/3/2011 to 1/9/2011. Finally, the bottom-left window (h) shows the time slider with radio buttons that allow different temporal aggregation levels.

Datasets that are large in size often tend to be incomplete, unreliable and contradictory. This often leads to wrong conclusions
and results and it becomes imperative for system designers to make
users aware of data reliability issues [20]. Aigner et al. [2] provide a visualization technique that represents uncertainties in the
temporal domain. Their approach provides users with temporal
glyphs that support planning and controlling tasks. We make a
note here that data accuracy and reliability becomes an important
measure for analysts to review and understand in any correlative
analysis process. This becomes especially important to consider
when the datasets being analyzed are sparse, as correlating such
datasets may produce unreliable results. Our system allows users
to visualize sparse datasets during the analysis process by showing
a percent of cases that have zero values in the time series. This is
especially helpful in case of incident report datasets (e.g., CTC incidents) where there may be a certain categories that have very few
incidents thereby leading to sparse datasets. In addition, we also
allow users to choose different data aggregation levels as a method
of aggregating sparse data to appropriate levels, thereby allowing
them to utilize their domain knowledge to determine the appropriate scales for analyses.
2.2

Time series data pattern and anomaly detection

Works in the field of time series data exploration, pattern and
anomaly detection includes work by Krstajic et al. [22] who present
an incremental time-series visualization technique that interactively

34

distorts time-based representations of multiple event datasets. Their
technique uses a timeline distortion technique to accommodate for
recent data within individual items. The authors note that many
analytical tasks can be more efficiently solved by using interactive
visual analytic tools that include the analyst at different stages of the
exploration process. We incorporate the same concept in our system, and provide a correlative analysis framework that incorporates
the analyst in the visual analytics loop. Hao et al. [16] also present
a framework for visualizing large sets of time series datasets. Their
approach involves importance-driven space-filling layout schemes
for time series datasets. Lin et al. [23] present a time series pattern discovery and visualization system that summarizes the global
and local structures of time series datasets. Their system enables
the discovery of motifs and anomalies and uses a coefficient to
measure the dissimilarity between any two time series. Kincaid
[21] uses a Focus+Context approach to visualize electronic test and
measurement system datasets and allows users to filter and detect
and visualize computationally detected motifs in the data. Malik et
al. [27] utilize a visual analytics approach for exploring linear correlations across a variety of spatial aggregations to allow analysts
to explore global temporal correlations, as well as explore the underlying spatial factors that may potentially influence trends among
their datasets. Corman and Mocan [12] utilize high frequency time
series criminal incident datasets and perform time series analyses to
explore the relationship among crime, deterrence and drug use. Our
approach deals with determining the correlations among datasets in

a visual analytics environment that enable analysts to compare different datasets by factoring in temporal leads/lags among them.
Other methods in statistics for detecting relationships between
time series datasets include computing the cross-correlation of two
different signals [15]. This approach applies a time-lag function to
one of the time series signals while keeping the other time series
fixed. The process of computing the correlation of a time series
signal with itself (with lag) is called autocorrelation, and the resulting plot of the sample autocorrelations plotted against the time lags
is called a correlogram [10]. This technique is widely used in the
detection of repeating patterns and randomness within the datasets.
We adapt a similar process of correlating the datasets by sliding the
datasets against one another and providing an interactive interface
that enables users to explore and understand potential correlations
between the datasets at different spatiotemporal scales.
Recent work in exploring time series similarity detection methods include Morse and Patel [28] who propose a method that can
be used to evaluate different threshold value techniques that measure the similarity between time series datasets. Bollobas et al. [5]
present deterministic and randomized algorithms for determining
the similarity between two time series datasets. Das et al. [13] also
consider the problem of finding rules that relate different patterns
within the same time series. They focus on the discovery of local
patterns in multivariate time series datasets. Many of these data
mining techniques rely on comparing elements of one time series
dataset with the elements of the other datasets.
2.3

Spatiotemporal exploration of predictive links

Researchers have also utilized multivariate datasets for forecasting purposes and have developed systems that factor in categorical
datasets in order to generate alerts and projections into the future.
Buono et al. [9] present a data driven forecasting method that employs pattern matching search using historic datasets. Maciejewski et al. [24] also implement predictive analytical tools in analyzing syndromic surveillance data. Toole et al. [18] utilize crosscorrelation measures and other techniques to identify spatiotemporal patterns using multivariate datasets. Rodrigues and Diggle [29]
adapt Log-Cox processes to model the spatiotemporal crime intensities where the probabilistic predictions of the crime intensities are
made in the Bayesian framework. Andrienko et al. [3] apply combinations of clustering and dimensionality reduction methods to support the visual analysis of spatiotemporal datasets.
3

C ORRELATIVE E XPLORATION IN A V ISUAL A NALYTICS
E NVIRONMENT

Understanding relationships between spatial and temporal trends
among multivariate datasets is an important part in the analysis process of spatiotemporal datasets. Such analyses often act as a precursor to creating predictive models from the data. Moreover, such
analyses can provide invaluable insights into the workings of real
world environments and can aid analysts in their hypothesis generation and exploration process. However, this process of determining
these potential relationships remains a challenging problem due to
reasons ranging from datasets being noisy to analysts being strained
due to the nature and complexity of their datasets.
Our approach focuses on determining the correlations between
different spatiotemporal datasets. Correlation is a single number
that describes the relationship between two variables. In the case
of temporal datasets, we use correlation to describe the degree to
which two variables are related to each other. Our method applies to
collections of categorical spatiotemporal datasets and allows users
to compute the correlations between any pair of variables among
any of the given datasets, each of which may consist of different
number of variables. We use the Pearson product-moment correlation coefficient (Pearson’s correlation - rxy ) in our analysis, which

is defined as:

∑N
i=1 (xi − x̄)(yi − ȳ)
(1)
(N − 1)Sx Sy
where N is the length of the time series datasets, x̄ and ȳ are the
means of the time series x and y respectively, and Sx and Sy are the
standard deviations of the time series x and y between the regions of
overlap. We also apply a two sided t-test to check if the correlation
obtained is significant within a 95% confidence interval with N-2
degrees of freedom. This two sided t-test is given by:
rxy =

t=q

rxy

(2)

2 )/(N − 2)
(1 − rxy

We also observe that there may be different forces that determine
the nature of the relationships between different datasets. For example, one event may be a precursor to a series of other events.
This would require the exploration of leads and lags between different datasets in order to understand if one dataset is related to the
other at a different temporal location (i.e., a lead or lag value). Our
system incorporates this concept in the analysis process, and allows
users to determine the lead or lag that maximizes the correlation,
thereby enabling them to determine the lead/lag values that maximize the similarity among the datasets.
3.1 Visual Analytics Environment
Our visual analytics framework builds upon our previous work [26]
and provides analysts and decision makers with the ability to visualize and model criminal, traffic and civil (CTC) incidents. Figure 1 shows a screenshot of our correlative visual analytics system.
Our system has been organized into a dashboard view and provides
users with linked dynamic windows to explore their spatiotemporal
datasets. The main viewing region (Figure 1 (a)) is the map view
that plots all selected incidents on a map. We utilize a kernel density estimation technique [24] to generate heatmaps to allow users
to quickly detect hotspots among the CTC incidents. Figure 1 (b)
shows an interactive menu that is used to filter through the different dataset variables (e.g., offenses, agencies). Figure 1 (c) shows a
time series view of the selected incidents, along with a correlation
vs. lead/lag graph (Figure 1 (d)) that shows the correlation of one
dataset against the other at different lead and lags between them.
The region of overlap between the time series (Figure 1 (c)) is also
encoded by a color on a divergent color scheme [7] (Figure 1 (e))
to reflect the correlation value at the current lead/lag value. These
points will be expanded upon later in Section 3.2 of the paper. Figure 1 (f) implements the calendar view approach adapted from [31]
and displays the time series data in the format of a calendar, with
the rows corresponding to weeks and the columns corresponding
to the days of the week. The bar charts at the end of each row
and column of the calendar view encode the sum of incidents for a
particular week and day, respectively. Figure 1 (g) shows a clock
view that shows the hourly temporal data in the form of a clock.
Finally, Figure 1 (h) shows the interactive time slider that allows
users to temporally slide through their datasets and provides them
with multiple temporal data aggregation levels.
3.2 Temporal Correlation Exploration
We provide three different interactive options for users to determine
the temporal correlation between datasets. The first option involves
providing the user with control over the correlation computation
process and allows him/her to directly set the lead or lag between
the datasets for use in correlation calculations by manually dragging
any of the selected time series datasets. The second option automatically computes the correlations at different leads or lags between
the selected datasets and outputs the lead/lag that maximizes the
correlation. The third option utilizes a brute force algorithm and
computes the correlations between all possible permutations of the
different data variables and saves the results in the decreasing order

35

Figure 2: Interactive calculation of correlation between datasets. The
user has clicked and dragged the time series data in red to the right
by 8 units. The system automatically computes the correlation and
significance (using the t-test) of the two time series data between the
regions of overlap (shown by the black rectangle).

Figure 3: Cycling the non-overlapping region of the data. The user
has clicked the top time series data and dragged it to the right by 12
units. The 12 units at the end are cycled to the beginning of the time
series, resulting in the new time series data shown at the bottom.

to y. Similarly, when the time series x leads y by a value of d
units, Equation 1 changes to:
N

of the number of significant correlation values. This allows analysts to review all potentially significant correlations among their
datasets. We describe these options in detail below.
3.2.1

Manual dragging of datasets

This option allows users to determine the correlation between any
selected datasets at any desired lag/lead. In order to determine the
correlations between these different time series datasets, our system allows users to simply click on a line on the time series graph
(Figure 1 (c)) and drag it either to the left or the right. This action automatically computes the correlation between the selected
time series dataset, and the other selected datasets and interactively
displays the results to the user. In addition to displaying the correlation value, the system also colors the region of overlap between
the time series datasets on a divergent color scale (Figure 1 (e)) to
encode the current correlation value. The system also displays the
current lead/lag value corresponding to the length of the difference
between the time series datasets (which, in turn, corresponds to the
length of the drag by the user). In our implementation, if the user
selects multiple datasets at the same time and clicks on a line graph
to compute the correlation, we take a sum of all the other temporal datasets and correlate the selected time series dataset with this
summed dataset. We plan on investigating other methods of computing the correlations among multiple datasets and leave this as
future work. We also note that this process of manually dragging
the datasets provides users with complete control over the correlation process by setting any desired lead/lag between the time series
datasets. This enables them to visually explore the potential correlations between the datasets. Furthermore, users may apply any
desired spatiotemporal filters to their datasets and acquire the time
series for any of the desired variables pertaining to their datasets.
Since Pearson’s correlation calculation requires the two time series datasets be of equal lengths, we utilize two different approaches
with regards to the non-overlapping regions of the datasets when the
user introduces a lead/lag by dragging the time series graph either
to the left or right:
• Calculating the correlation between the regions of overlap: This
approach calculates the correlation between only the overlapped
regions of the time series datasets, and ignores all the nonoverlapping temporal regions. Correspondingly, when the time
series x lags behind the time series y, the correlation calculation
(Equation 1) changes to:
N

rxy (d) =

d
(xi+d − x̄)(yi − ȳ)
∑i=1
(Nd − 1)Sx Sy

(3)

where Nd = (N − d), x̄ and ȳ are the means of the time series x and y respectively, Sx and Sy are the standard deviations
of the time series x and y between the regions of overlap, and
d = {0, 1, ...., ⌈ N4 ⌉} is the lag between time series x with respect

36

d
(xi − x̄)(yi+d − ȳ)
∑i=1
(4)
(Nd − 1)Sx Sy
The corresponding t-test equation (Equation 2) thus becomes:

rxy (d) =

t=q

rxy (d)
2 (d))/(N
(1 − rxy
d

(5)
− 2)

An example of this method can be seen in Figure 2, where
the user has chosen to visualize two temporal datasets, and has
dragged the series in the red color to the right by a distance of
8 units. The other time series (black color) remains stationary.
We then compute the correlation of the two time series between
the overlapping region only and display this value as a number,
along with the corresponding significance as a result of applying
the t-test.
• Cycling the non-overlapping regions: This method allows users
to cycle the non-overlapping temporal regions of a selected time
series dataset to either its beginning or end as the user drags the
time series to the right or left on the graph, respectively. This
cycling strategy is demonstrated in Figure 3, where the user has
dragged the top time series to the right by 12 units, resulting in
the time series shown at the bottom of the Figure. The system
automatically cycles the points at the end of the top time series
to its beginning, resulting in the time series shown at the bottom.
This newly generated time series is then utilized in the correlation
calculations (using Equations 1 and 2). This approach, coupled
with temporal region selection described in Section 3.2.3, allows
analysts to determine the cyclic trends in their datasets.
3.2.2 Automatic correlation computation against lead/lags
In addition to computing the correlation at a desired lead/lag dictated by the user (Section 3.2.1), the system also provides users
with the option to automatically determine the lead/lag that maximizes the correlation between the selected datasets. When this option is selected, the user starts by simply clicking on the desired
time series plot that he/she wants to perform the correlative analysis on. Figure 1 (c) shows an example where the user is visualizing the weekly drunkenness/public intoxication offenses against
noise complaints for Tippecanoe County, Indiana, U.S.A. for all 52
weeks of the year 2011. In this example, the user has selected the
time series plot of drunkenness/public intoxication offenses (shown
in red color), and is analyzing how these compare against noise
complaints. The system computes the correlation values at corresponding lead/lag values (ranging between ±N/4, where N is the
total length of the datasets), and displays them as a bar graph of
the computed correlation values with respect to the lead/lag (Figure
1 (d)). The ±N/4 metric has been adapted from [6], and is chosen because as the lead/lag value increases, the region of overlap
between the datasets (and hence the correlation sample size) decreases, thereby making the correlation estimate of the overlapping

datasets less reliable. We also note that the resulting plot (Figure
1 (d)) resembles a correlogram in that we plot the results of the
correlation process as correlation values against time lags.
The system also applies the t-test for the correlation values obtained at each lead/lag value and checks whether the correlation is
significant or not at a given lead/lag. The statistically significant
correlation values are highlighted with green colored bars (for positive correlation), or red colored bars (for negative correlation) at
the corresponding lead/lag value. This is done in order to reduce the
risk of users making wrong inferences from the correlation results,
as positive and negative correlation values have different statistical meanings and implications in real world scenarios. The system
also animates the two graphs by iterating through lead/lag values
with a step size of 1 time unit, updating the two graphs simultaneously. This animation starts from a lead value of −N/4 and ends
at a lag value of +N/4, and lasts for about 5 seconds for N = 365
days. We find this animation length to be enough time for users to
understand the underlying process [17]. The corresponding correlation vs. lead/lag graph is dynamically linked to the main graph, and
the correlation value corresponding to the current lead/lag value is
highlighted on the global correlation graph (using a blue colored
outline at the current lead/lag value).
Finally, when the current lead/lag value reaches the maximum
lag value of +N/4, the animation proceeds and ends at the lead/lag
value that maximizes the correlation between the datasets (this happens at a lead/lag value of 3 weeks in Figure 1 (d)). Note that
the system also allows users to interactively click on any of the
time series datasets and drag it to the left or right to interactively
move it (just like in the manual correlation option described in Section 3.2.1), and visualize the datasets at a desired lead/lag corresponding to the correlation vs. lead/lag graph. We note that the user
may choose to disable the animation at any time in which case the
system resorts to displaying only the end results (e.g., Figure 1 (d)).
We further note that all the options described in Section 3.2.1 apply equally in the automatic correlation computation mode. Specifically, in the auto-computation mode, we allow users to: (a) discard
the non-overlapping temporal regions while calculating the correlation values, and (b) cycle the non-overlapping region to either the
beginning or the end of the selected time series.
3.2.3

Temporal windows for correlation computation

We provide users with options to restrict the temporal correlation
calculations to a user specified temporal window. This feature allows users to interactively click and hold the left mouse button,
and drag the mouse on the time series graph to select the temporal region between the two clicked locations. The user can then
choose to perform the correlation analyses described in either of
sections 3.2.1 and 3.2.2. The system automatically restricts the
correlation analysis to the user selected temporal window. Furthermore, if the user chooses to cycle the non-overlapping temporal regions of their datasets in the correlation computation procedure, and selects a temporal window, the system replicates the selected time series within the temporal window and concatenates it
to its beginning and end, thereby forming a new time series dataset.
The system then uses this newly generated temporal dataset and
performs the correlation analysis with the other (unaltered) userselected datasets.
We observe that this temporal windowing method is especially
useful when the user is interested in determining the correlations
between different datasets within a specific temporal window. This
may be because certain temporal distributions may be highly correlated only within certain temporal periods (e.g., holiday periods,
weekends). For example, noise complaints and drunkenness/public
intoxication offenses may have a high correlation pattern within
university academic semesters, and may not have significant patterns within the summer months when most students are not on-

campus.
3.2.4

Automatic computation of correlation between different datasets

In order to assist analysts in the discovery of potential high correlation between datasets, we utilize a brute-force algorithm that applies the method described in Section 3.2.2 to all possible combinations of the user-selected datasets. We then count the total number
of significant correlation values at different lead/lag values (using
the t-test), and save the results in descending order of the number
of significant correlation values in a text file. In addition to this
text file, we also provide users the option to save the corresponding
global correlation graphs obtained for all the combinations as images on their local machines for the analysts to review. This method
allows analysts to easily analyze and discover the datasets that are
potentially correlated to one another. We note, however, that this
process acts only as a starting point for analysis and only provides
a hint towards possible correlations among the datasets. An analyst
who has domain knowledge of the underlying datasets remains a
necessary component in this correlative visual analytics process.
3.3

Correlation Exploration in Geospace

In addition to performing a temporal analysis of data, our system
also provides analysts with a suite of geospatial data aggregation
tools that allows them to compare the correlations between the incident distribution levels within and across selected spatial regions.
This method allows users to restrict their analysis to specific regions and generate and test hypotheses pertinent to their domain
knowledge of the datasets. We also allow analysts to factor in the
lead/lag values obtained as a result of performing a temporal correlation analysis and visualize their datasets in geo-space by displaying the incidents at different temporal lead or lag levels. We
describe both these methods in detail below.
3.3.1

Spatial region selection

We provide analysts with the ability to spatially aggregate their
datasets by either drawing arbitrary regions on the map, or by an
underlying spatial boundary (e.g., census block, county). This action aggregates the incidents falling within the selected spatial regions and extracts the temporal datasets of an aggregate of all user
selected data categories for these regions. This allows users to compare the incidents between multiple spatial regions and determine
how closely related the selected incident distributions are to both
within, and across the spatial regions. We note that this spatial region selection also enables the correlation analysis and comparison
of a smaller spatial region against the entire region. For example,
an analyst may choose to compare the burglary crimes of a certain
neighborhood by drawing a shape around the neighborhood, and
draw another boundary around the entire city, and explore how the
two selected regions compare against one another.
3.3.2

Visualizing the incident distributions in space at selected temporal lead/lag values

In order to visually compare the incidents across multiple temporal lead/lag values in space, we allow users to interactively select
a temporal lead/lag value using the global correlation graph (Figure 1 (d)). Once the lead/lag value is selected, the spatial map
display (Figure 1 (a)) shows the geo-spatial distribution of the incidents of the selected category at an offset applied to the current
time. This offset is given by the selected lead/lag value, and the
current time is dictated by the time slider (Figure 1 (g)). The geospatial distribution of the incidents of the other selected categories,
on the other hand, are shown on the map at the current time. This
allows the user to visualize the spatial incident distributions of one
category with respect to the other selected categories at a selected
lag, and allows them to visually observe spatial patterns among the

37

Figure 4: Correlating drug abuse violations and burglary crimes in Tippecanoe County, IN. The top-left graph (a) shows the correlation vs. lead/lag
graph of the two crimes for the year of 2011. The correlation vs. lead/lag graphs shown in (b-d) correspond to correlating the two crimes within
temporal windows of university academic Spring, Summer and Fall semesters.

different incident categories across time to explore potential relationships and causations.
4

C ASE S TUDY: E XPLORING C ORRELATIONS IN C RIMINAL ,
T RAFFIC AND C IVIL (CTC) I NCIDENT R EPORT DATASETS

In this section, we demonstrate our work by applying the methods
described so far to CTC incident reports from Tippecanoe County,
IN, U.S.A. This dataset provides several categorical incident distributions including offense reported, entities against whom crimes
are committed and agencies that respond to the incident. In addition, every incident has an associated geographic location and a
time stamp. We now provide several examples that show how our
correlative visual analytics system can be used and highlight some
of the features of our system. We also discuss the insights that the
system provides into the spatiotemporal CTC incident activity levels as a result of the analysis done using our system.
4.1 Drug abuse violations vs. burglaries
In this example, we provide a hypothetical scenario where an analyst is interested in exploring the correlations among drug abuse
violations and burglaries (both residential and commercial) in order to find patterns and trends among the two crimes. To begin the
process, the analyst filters and selects the offenses related to drug
abuse violations and burglaries using the system’s interactive menu
(Figure 1 (b), and visualizes the daily incident temporal distributions of the offenses for the year 2011. The analyst now chooses
to perform a correlative analysis over the entire region, and allows
the system to determine the correlations between the two crimes
at different temporal lead/lag values using the automatic computation method described in Section 3.2.2. Specifically, the analyst
selects drug abuse violations (by clicking on its time series graph),
and allows the system to compute the temporal correlations with respect to burglary crimes within a temporal lead/lag values of ±N/4
days (where N is the total temporal duration of 365 days). This
action animates the main graph by moving the time series data for
drug abuse violations from a lead value of −N/4 to a lag value
of +N/4, and computes the correlation between the overlapping
regions of the time series datasets. The results are shown in the correlation vs. lead/lag graph alongside the main graph, as shown in
Figure 4 (a).

38

The analyst first notes that there occurs a significant negative
correlation between drug abuse violations and burglaries at a lead
value of -1 days between the two offenses, providing an indication
that an increase in drug abuse violation arrests or reports lead to a
decrease in burglaries after one day. He hypothesizes that this may
be because with more drug abuse violations and arrests that follow these drug busts, there may be less criminals out in the streets
to commit crimes, thereby suggesting that these criminals may be
have a relationship with the burglary crimes in the county. He also
notes a significant correlation when drug abuse violations lead burglary crimes by 11 and 26 days (seen by the significant correlation
at a lag value of -11 and -26 days from Figure 4 (a)). The analyst
notes that these may be attributed to these drug offenders getting
bailed out of jail and burglarizing a house in order to get some fast
cash to satisfy their drug needs. He further observes other significant correlations at the outliers of the correlation vs. lead/lag graph.
The analyst notes that these may be a consequence of the offenders
serving jail time, or simply due to random chance. In this process
however, he is quick to note that correlation does not imply a causation, and just because there is a strong positive or negative correlation between the two offenses, it does not imply that one offense
directly causes the other. He notes that this is especially true since
the offenses are aggregated over the entire county as different locations are governed by different spatial demographics and variables.
Temporal windowing by academic calendar
The analyst now notes that the population in Tippecanoe County
fluctuates with the university academic calendar year as it is home
to a large university, and investigates whether the academic calendar has an effect on drug abuse violations and burglaries in the
county. As was done earlier, he repeats the automatic correlation
computation analysis, but now restricts the analysis to the temporal windows of academic Spring, Summer and Fall semesters
(using the technique described in Section 3.2.3). The correlation
vs. lead/lag plots have been shown in Figures 4 (b-d). He first investigates whether any of the patterns discovered between drug abuse
violations and burglaries in the time analysis for the entire year hold
with respect to the academic semesters, which would hint to a pattern among the offenses that could be attributed to the academic calendar. He first observes a significant negative correlation when drug
abuse violations lead burglary crimes by two days (at a lead/lag

Figure 5: Geospatial heatmaps of drug abuse violations (Left) and burglary crimes (Right) for Tippecanoe County, IN for the year 2011.

value of -2 days in Figure 4 (b)) in the Spring semester. He also
finds a negative correlation at a lead/lag value of -1 day (although
the t-test indicates that this is not statistically significant). This pattern, however, does not hold in the Fall semester (Figure 4 (d)) (although a slightly non-significant negative correlation does exist at a
lead/lag value of -2 days of the Fall semester). The analyst observes
that the correlation vs. lead/lag graph for the summer session (Figure 4 (c)) shows no observable pattern as compared to the rest of
the academic year, which he attributes to the fact that a majority of
college students leave town during the summer months causing an
economic shift in the region.
The analyst also observes that the significant positive correlation observed at a lead/lag value of -11 days in the global temporal analysis (Figure 4 (a)) is also observed around the same time
in the Spring and Fall semesters. This provides further supporting
evidence to his hypothesis that the drug offenders may be getting
bailed out of jail by that time, resulting in a positive correlation
value at around 11 days (thereby hinting to an increase of burglary
offenses). With these results, the analyst may decide to investigate
further whether there is a pattern between drug offenders getting
bailed out and an increase in burglaries after a period of around 11
days in Tippecanoe county.
Exploring crime correlations over geospatial hotspots
The analysis done so far considers the drug abuse violations and
burglaries for the entire county. However, different neighborhoods
in a large spatial region have different attributes, and as such, behave differently in terms of crime patterns and trends. With this in
mind, the analyst now decides to use the system and find the spatial hotspots of both drug abuse violations and burglary crimes. To
achieve this, he aggregates the data first for drug violations for the
year 2011, and uses our system’s spatial heatmap feature to visualize the hotspots for drug violations. He interactively selects the
region with the highest drug violations (region D1 in Figure 5 (left))
for the year 2011 by drawing a rectangle over it. This action lets the
system automatically extract the time series plot of all drug violations for this region. Similarly, the analyst visualizes the heatmap
for burglary crimes, and interactively selects the hotspot (given
by region B1) in the neighborhood of the selected drug violation
hotspot, thereby extracting the time series plot for all the burglary

offenses for this region. He then performs the correlative analysis
for these subsets of data, comparing the temporal trends of the drug
abuse violation hotspot (region D1) against the neighboring burglary crime hotspot (region B1). The correlation vs. lead/lag graph
resulting from this is shown in Figure 6 (a). The analyst observes
something very unusual from these results. He finds a high significant positive correlation at a lag value of zero days between the
two spatial regions, giving an indication that as drug abuse violation
rates increase in the selected region, burglary rates also increase in
the other neighborhood (and vice versa).
The analyst next selects the drug abuse violations for the region
selected previously (region D1), and compares these to the burglary
offenses for another burglary hotspot (region B2) in another city.
The results of these are shown in Figure 6 (b). He however finds
that these two regions do not share the same pattern that regions
D1 and B1 do. Specifically, he finds no positive correlation at a
lag of zero days, hinting to the fact that the drug abuse violations
and burglary crimes do not share the same pattern across two different cities. The analyst now compares the drug abuse violations and
burglary offenses for two other regions in the same neighborhood
in the other city, that is, he compares the drug abuse violations for
region D2 and burglary offenses for region B2. These results are
shown in Figure 6 (c). He finds that these two regions share the
same pattern as regions B1 and D1. That is, there exists a significant positive correlation between these two regions at a lag of
zero days. Finally, he compares the drug abuse violations for region D2 and the burglaries for region B1. The resulting correlation
vs. lag/lead graph is shown in Figure 6 (d). Again, as was the case
with regions D1 and B2, there are no significant correlations at a lag
value of zero days. These results suggest that drug abuse violations
are highly correlated to burglary crimes in the same neighborhoods
in Tippecanoe County at a lag of zero days, but not across different
neighborhoods. He infers that this may be because the criminals
lack transportation methods to travel across different cities, or that
these criminals may have a tendency to target their own neighborhoods (a fact known from domain knowledge). This approach thus
allows the analyst to analyze and compare the crime patterns of individual neighborhoods, and test and form hypothesis using their
domain knowledge. As can be observed from this process, domain
expert knowledge is a critical part in this analytical process.

39

Figure 6: Correlating the temporal distributions of drug abuse violation hotspots against burglary crime hotspots in Tippecanoe County, IN. The
graphs (a-d) correlate the drug abuse violation hotspots of regions D1 and D2 against the burglary hotspots of regions B1 and B2 in Figure 5.

Figure 7: The correlation vs. lead/lag graph obtained as a result of correlating the drunkenness/public intoxication offenses against noise
complaints for the year 2011 in the cities of Lafayette and West Lafayette, IN.

4.2

Drunkenness/public intoxication vs. noise complaints

We now provide an example in which an analyst is investigating
the correlations between drunkenness/public intoxication offenses
and noise complaints in the cities of Lafayette and West Lafayette,
Indiana. He works under the hypothesis that there exists a causal
relationship between the two offenses that repeats periodically by a
period of seven days. This is because drunkenness and noise complaints generally go hand in hand, and usually have higher occurrences over the weekends. To begin this analysis, the analyst first
selects both these offenses and visualizes the distributions of the
two offenses by a weekly temporal aggregation level to detect any
significant patterns over a weekly temporal scale. He first visualizes
the offenses in geospace by interactively scrolling in time using the
system’s time slider (Figure 1 (h)), and observes that the hotspots
for the two offenses occur near the university campus and have a
strong co-occurrence with the locations of the local pubs and bars.
He also uses the system’s calendar view and observes that the daily

40

distributions are higher over the weekends, with most complaints
occurring between Thursdays and Sundays. This is obvious to the
analyst as he knows from his domain knowledge that college students tend to party over the weekends.
The analyst now applies the automatic correlation computation
method (Section 3.2.2) on these two datasets with a weekly temporal aggregation scaling for the year 2011. The results from this
analysis are shown in Figure 1 (d). In this image, the analyst
has selected drunkenness/public intoxication offenses (shown in red
color). The system automatically animates and moves this time series graph from a lead value of −N/4 to a lag value of +N/4 (here,
N = 52 weeks) with a step size of 1. The system computes the correlation at each lead/lag step, and displays the resulting correlation
in the correlation vs. lead/lag graph (Figure 1 (d)). However, the
analyst notices no significant patterns as a result of this analysis.
He notes however that the problem lies with the weekly temporal
aggregation level chosen. He then chooses a daily temporal scale
level, and performs the same analysis for the two datasets over a
period of one year. The resulting correlation vs. lead/lag graph has

Figure 8: Correlating an aggregate of drunkenness/public intoxication and noise complaints against traffic accidents for the cities of Lafayette
and West Lafayette, IN for the year 2011 at different temporal lead/lag values between the datasets. The datasets are correlated on a daily
temporal scale in the left plot, and on an hourly temporal scale in the right plot.

been shown in Figure 7. These results show a clear day-of-the-week
effect between the two datasets, with the correlation (significantly)
peaking after every 7 days. The dashed blue region in the graph
highlights the lag value that maximizes the correlation (28 days).
We note here that if the system only displayed the lag value that
maximizes the correlation, the analyst would not get the insights
that are provided by the visualization resulting from this visual analytics process. This further demonstrates the value of visualization
and visual analytics in this analysis process.
4.3

Drunkenness/public intoxication and noise complaints vs. traffic accidents

In our final example, we demonstrate our visual analytic correlation technique with a scenario in which an analyst is investigating
the correlations between drunkenness/public intoxication offenses,
noise complaints and traffic accidents in Tippecanoe County, IN.
Using his domain knowledge, the analyst decides to compare
the datasets for the spatial hotspots (as was done in the example provided in Section 4.1) for traffic accidents against drunkenness/public intoxication and noise complaints combined, and selects a temporal scaling by day for the year 2011. The correlation
vs. lead/lag graph is shown in Figure 8 (Left), which shows a similar day-of-the-week effect as was observed in the example given
in Section 4.2, with the pattern repeating every 7 days. Also, the
correlation peak occurs at a lag of zero days, suggesting that drunkenness and noise complaints go hand-in-hand with traffic accidents,
and are highly correlated to one another.
The analyst now decides to compare drunkenness/public intoxication and noise complaints combined against traffic accidents for
the two hotspot regions selected for the year of 2011. In this case
however, he decides to visualize the data by hour by binning the
data from Monday to Sunday, with the total bins equaling 7 × 24.
By this convention, the incidents for all Mondays for the year of
2011 fall in the bin range of 0-23, the incidents occurring on Tuesdays fall in the bin range of 24-47 hours, and so on. The results
of running the correlative analysis on these two time series datasets
have been shown in Figure 8 (Right). These results show a high
positive correlation between drunkenness/public intoxication and
noise complaints combined and traffic accidents within the selected
regions within a lead/lag values between -5 to +4 hours, indicating
that drunkenness/public intoxication, noise complaints and traffic
accidents go hand-in-hand with each other within a 24 hour period.
Exploring geospatial patterns between selected offenses
The analyst now utilizes the method described in Section 3.3.2,
and chooses to visualize the geospatial distribution of drunkenness/public intoxication and noise complaints against traffic accidents at an hourly lag between -5 to 0 hours to see if a spatial pattern emerges that shows a spatial association between these offenses
and traffic accidents. He observes that there occur many traffic accidents after a few hours of noise/drunkenness complaints around

the neighborhoods of local pubs and bars. The analyst notes that
this may indicate that more police officers should be on the lookout
for traffic accidents when the rates of noise complaints and drunkenness increase around the regions of local pubs and bars.
5 C ONCLUSIONS AND F UTURE W ORK
In this work, we have presented a correlative analysis framework
for computing spatiotemporal correlations in a visual analytics environment. We utilize the Pearson’s correlation coefficient and factor in the lead or lag between different datasets for pattern and
anomaly detection, thereby providing the tools for analysts and decision makers to quickly obtain the potential relationships between
different datasets. We provide interactive tools that enable users
to compute the correlations between multiple multivariate datasets.
Our work explores the potential of visual analytics in the correlation exploration domain, and provides users with an interactive way
of understanding and applying the underlying statistical algorithms
at different appropriate temporal and spatial scales.
Our future work includes expanding our current work further
in the spatial domain, and allowing users to interactively explore
the correlations among the datasets in the spatial view. We plan
on utilizing advanced statistical methods to enable users to detect
spatial anomalies, that would provide a starting point in the correlative analysis process. We also plan on investigating different
methods of computing and displaying correlations among multiple
time series datasets at different lead/lag values (as was noted in Section 3.2.1). We note that Pearson’s correlation coefficient enables
a linear correlation between two variables, and we plan on investigating and incorporating non-linear correlation techniques among
multivariate datasets in the future. We also note that a time series
can be viewed as the sum of multiple components of variation, and
can be separated into its various components using, for example,
Seasonal-trend decomposition methods based on loess [11]. We
plan on utilizing these techniques to separate the time series into its
components and then applying our methods to further reveal finer
trends among the datasets. We also plan on expanding our system
to enable a correlation analysis of multiple data sources (e.g., CTC
incidents vs. climate and moon phase datasets). We also plan on developing natural scale templates for automatically aggregating data
to appropriate spatiotemporal levels for use in the correlation analysis process. Furthermore, we plan on conducting a formal userevaluation of our system in the future to evaluate the efficiency of
our system with varied amounts of data. Finally, we plan on deploying our correlative analysis framework to our public safety consortium members for further refinement and testing.
6 ACKNOWLEDGEMENTS
We would like to thank Timothy F. Collins, Dr. Min Chen and Dr.
SungYe Kim for their valuable feedback. This work is supported
by the U.S. Department of Homeland Security’s VACCINE Center
under Award Number 2009-ST-061-CI0003.

41

R EFERENCES
[1] W. Aigner, S. Miksch, W. Muller, H. Schumann, and C. Tominski. Visual methods for analyzing time-oriented data. IEEE Transactions on
Visualization and Computer Graphics, 14(1), January/February 2008.
[2] W. Aigner, S. Miksch, B. Thurnher, and S. Biffl. Planninglines: Novel
glyphs for representing temporal uncertainties and their evaluation. In
Proceedings of the 9th International Conference Information Visualisation, 2005.
[3] G. Andrienko, N. Andrienko, S. Bremm, T. Schreck, T. Von Landesberger, P. Bak, and D. Keim. Space-in-Time and Time-in-Space SelfOrganizing Maps for Exploring Spatiotemporal Patterns. Computer
Graphics Forum, 29(3):913–922, 2010.
[4] R. Bade, S. Schlechtweg, and S. Miksch. Connecting time-oriented
data and information to a coherent interactive visualization. In CHI
’04: Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems, pages 105–112, New York, NY, USA, 2004.
ACM.
[5] E. Bollob’as, G. Das, D. Gunopulos, and H. Mannila. Time-series similarity problems and well-separated geometric sets. In Nordic Journal
of Computing, pages 454–456, 1997.
[6] G. Box. Time series analysis: Forecasting and control. Holden-Day,
San Francisco, 1970.
[7] C. A. Brewer. Designing Better Maps: A Guide for GIS users. ESRI
Press, 2005.
[8] P. Buono, A. Aris, C. Plaisant, A. Khella, and B. Shneiderman. Interactive pattern search in time series. In Proceedings of Conference on
Visualization and Data Analysis, pages 175–186, 2005.
[9] P. Buono, C. Plaisant, A. Simeone, A. Aris, G. Shmueli, and W. Jank.
Similarity-based forecasting with simultaneous previews: A river plot
interface for time series forecasting. In Proceedings of the 11th
International Conference Information Visualization, pages 191–196,
Washington, DC, USA, 2007. IEEE Computer Society.
[10] C. Chatfield. Time-Series Forecasting. Statistics (Chapman and
Hall/CRC). Chapman & Hall/CRC, 2001.
[11] R. B. Cleveland, W. S. Cleveland, J. E. McRae, and I. Terpenning.
STL: A seasonal-trend decomposition procedure based on loess (with
discussion). Journal of Official Statistics, 6:3–73, 1990.
[12] H. Corman and H. N. Mocan. A time-series analysis of crime, deterrence, and drug abuse in New York City. American Economic Review,
90(3):584–604, June 2000.
[13] G. Das, K. ip Lin, H. Mannila, G. Renganathan, and P. Smyth. Rule
discovery from time series. pages 16–22. American Association for
Artificial Intelligence, 1998.
[14] D. Guo, J. Chen, A. MacEachren, and K. Liao. A visualization system
for space-time and multivariate patterns (VIS-STAMP). IEEE Transactions on Visualization and Computer Graphics, 12(6):1461 –1474,
Nov.-Dec. 2006.
[15] J. D. Hamilton. Time-series analysis. Princeton Univerity Press, 1
edition, Jan. 1994.
[16] M. C. Hao, U. Dayal, D. A. Keim, and T. Schreck. Importance-driven
visualization layouts for large time series data. In J. T. Stasko and
M. O. Ward, editors, Proceedings of the International Conference of
Information Visualization, page 27. IEEE Computer Society, 2005.
[17] J. Heer and G. Robertson. Animated transitions in statistical data
graphics. IEEE Transactions on Visualization and Computer Graphics, 13(6):1240–1247, Nov. 2007.
[18] J. P. J. Toole, N. Eagle. Quantifying behavioral data sets of criminal
activity. Technical report, AAAI Symposium on Artificial Intelligence
for Development, 2010.

42

[19] W. Javed, B. McDonnel, and N. Elmqvist. Graphical perception of
multiple time series. IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE InfoVis 2010), 16(6):927–934, 2010.
[20] D. Keim, J. Kohlhammer, G. Ellis, and F. Mansmann, editors. Mastering the information age: Solving problems with Visual Analytics.
EuroGraphics, 2010.
[21] R. Kincaid. Signallens: Focus+context applied to electronic time series. IEEE Transactions on Visualization and Computer Graphics,
16(6):900 –907, Nov.-Dec. 2010.
[22] M. Krstajic, E. Bertini, and D. Keim. Cloudlines: Compact display of
event episodes in multiple time-series. IEEE Transactions on Visualization and Computer Graphics, 17(12):2432 –2439, Dec. 2011.
[23] J. Lin, E. Keogh, and S. Lonardi. Visualizing and discovering nontrivial patterns in large time series databases. Information Visualization, 4(2):61–82, 2005.
[24] R. Maciejewski, R. Hafen, S. Rudolph, S. Larew, M. Mitchell,
W. Cleveland, and D. Ebert. Forecasting hotspots: A predictive analytics approach. IEEE Transactions on Visualization and Computer
Graphics, 17(4):440 –453, April 2011.
[25] R. Maciejewski, S. Rudolph, R. Hafen, A. Abusalah, M. Yakout,
M. Ouzzani, W. S. Cleveland, S. J. Grannis, M. Wade, and D. S. Ebert.
A visual analytics approach to understanding spatiotemporal hotspots.
IEEE Transactions on Visualization and Computer Graphics, 16:205–
220, Mar. - Apr. 2010.
[26] A. Malik, R. Maciejewski, T. F. Collins, and D. S. Ebert. Visual analytics law enforcement toolkit. In IEEE International Conference on
Technologies for Homeland Security, pages 222–228, 2010.
[27] A. Malik, R. Maciejewski, E. Hodgess, and D. S. Ebert. Describing
temporal correlation spatially in a visual analytics environment. In
Proceedings of the Hawaii International Conference on System Sciences, pages 1–8, 2011.
[28] M. D. Morse and J. M. Patel. An efficient and accurate method for
evaluating time series similarity. In Proceedings of the 2007 ACM
SIGMOD international conference on Management of data, SIGMOD
’07, pages 569–580, New York, NY, USA, 2007. ACM.
[29] A. Rodrigues and P. J. Diggle. Bayesian estimation and prediction
for inhomogeneous spatiotemporal log-gaussian cox processes using
low-rank models, with application to criminal surveillance. Journal of
the American Statistical Association, 107(497):93–101, 2012.
[30] J. Stasko, C. Gorg, Z. Liu, and K. Singal. Jigsaw: Supporting investigative analysis through interactive visualization. In Proceedings
of the IEEE Symposium on Visual Analytics Science and Technology,
pages 131–138, 2007.
[31] J. J. Van Wijk and E. R. Van Selow. Cluster and calendar based visualization of time series data. In INFOVIS ’99: Proceedings of the 1999
IEEE Symposium on Information Visualization, page 4, Washington,
DC, USA, 1999. IEEE Computer Society.
[32] C. Weaver. Cross-filtered views for multidimensional visual analysis.
IEEE Transactions on Visualization and Computer Graphics, 16:192–
204, Mar. - Apr. 2010.
[33] J. J. V. Wijk and E. R. V. Selow. Cluster and calendar based visualization of time series data. In Proceedings of the IEEE Symposium on
Information Visualization, pages 4–9, San Francisco, CA, USA, 1999.
IEEE Computer Society.
[34] J. Zhao, F. Chevalier, E. Pietriga, and R. Balakrishnan. Exploratory
analysis of time-series with chronolenses. IEEE Transactions on Visualization and Computer Graphics, 17(12):2422 –2431, dec. 2011.

Computational Aesthetics

Measuring Stipple Aesthetics
in Hand-Drawn and
Computer-Generated Images
Ross Maciejewski ■ Purdue University
Tobias Isenberg ■ University of Groningen
William M. Andrews ■ Medical College of Georgia
David S. Ebert ■ Purdue University
Mario Costa Sousa ■ University of Calgary
Wei Chen ■ Zhejiang University

F

rom hastily sketched figures on napkins to complex medical illustrations, hand-drawn images
have long been a means of conveying information. Often, an artist condenses the information to
the most important details, creating a simple, clear,
and meaningful image. The artist accomplishes this refinement
When people compare
by directing attention to relevant
a computer-generated
features, simplifying complex feaillustration to a hand-drawn
tures, or exposing obscured feaillustration of the same
tures. This selective inclusion of
object, they usually perceive
detail provides levels of expression
differences. This seems to
often not found in photographs.
Computer graphics artists have
indicate that the two kinds
adopted many traditional illustraof images follow different
aesthetic principles. To explore tion techniques in nonphotorealistic rendering (NPR). They have
and explain these differences,
particularly focused on traditional
the authors compare texture
pen-and-ink techniques, attemptstippling in hand-drawn
ing to mimic artists’ strokes,
and computer-generated
textures, and tones through the
illustrations, using imageplacement of lines and points of
processing analysis techniques. varying thickness and density in
computer-generated images. In
this article, to highlight the differences between
computer-generated images and hand-drawn im62	

March/April 2008	

ages, we will focus solely on stippling, a pen-andink subset. Figure 1 shows examples of stippling in
hand-drawn illustrations.
In stippling, the artist places dots on a surface of
contrasting color to obtain subtle shifts in value.
These dots can vary in size, volume, and arrangement to create the illusion of different texture,
tone, and shape. To visually describe forms and
objects, the artist begins by placing dots randomly
on the surface and then gradually fills in areas
from these seed dots.1 Thus, stippling represents
fine details and textures using points. Since points
are the simplest graphic primitives, automating
stippling is an attractive goal. Researchers have developed many techniques for creating interactive,
computer-generated stipple renderings.2,3 However,
an important question has gone unanswered: Do
computer-generated stipple images have the same
aesthetics as hand-drawn stipple images?
To answer this question, we use image-processing texture analysis techniques. The first step in
texture analysis is defining the concept of texture.
Image processing uses two main approaches to defining texture: structural and statistical. The structural approach defines texture as a set of primitive
texels that contain a regular or repeated relation-

Published by the IEEE Computer Society

0272-1716/08/$25.00 © 2008 IEEE

Computer stippling systems
Many computer illustration systems incorporate
stippling algorithms. Deussen and colleagues were
among the first researchers to computationally
create stipple images.2 They render polygonal models into a continuous tone image and then convert
these target images into a stipple representation.
To do this, they apply half-toning techniques to
arrive at an initial stipple distribution and then
interactively apply relaxation based on centroidal
Voronoi diagrams (Lloyd’s algorithm), using specialized brushes to space the stipple points more
evenly. They suggest using a Poisson disc distribution to simulate the artistic stipple distribution.
In contrast to this interactive approach, Secord uses a fast probabilistic method that places
small stipple primitives.3 He also performs iterative relaxation using centroidal Voronoi diagrams
but doesn’t require interactive adjustment of the
stipple spacing with brushes. Instead, he weights
the computation of the centroidal Voronoi dia	

Copyright William M. Andrews

ship; the statistical approach defines texture as a
quantitative measure of the arrangement of intensities in a region. Examples of structural textures
are wood grain and brush strokes; statistical textures include random patterns such as those of
stone or ice. To apply common texture analysis
methods to NPR techniques, we must first determine which type of texture the technique produces:
structural or statistical.
A cursory glance at a stipple image makes it clear
that structural analysis is not appropriate because
isolating a pattern for analysis would be difficult,
if such a pattern even exists. Moreover, most stippling systems rely on a random distribution of
stipple points. Therefore, statistical texture analysis is the most appropriate approach for analyzing
stipple textures, and structural analysis methods
found in recent texture synthesis algorithms (for
example, in Wang and colleagues4) are not appropriate. To analyze the aesthetics of hand-drawn
and computer-generated stipple textures, as well
as comparable photographs of natural textures,
we use image analysis metrics associated with the
statistical texture approach—specifically, the graylevel co-occurrence matrix.5 Although we could
use other statistical methods (for example, in Qin
and Yang6), we have found GLCM most appropriate for analyzing stipple textures.
From the GLCM, we calculate texture properties such as energy, correlation, and contrast. By
comparing these properties in an array of stipple
images, we can learn which features these textures
have in common and where current stipple algorithms fail in producing textures similar to handdrawn ones.

(a)

(b)

Figure 1. Stippling in hand-drawn scientific illustrations, depicting tone
and shape features.

gram on the basis of the input image’s local gray
level. As a result, stipples are automatically packed
more densely in dark regions and more sparsely in
lighter regions.
Schlechtweg, Germer, and Strothotte (hereafter
referred to as Schlechtweg) take a different general
approach.7 They have created a multiagent system
called RenderBots to position NPR strokes, including stipples, based on a stack of geometric buffers
generated from a 3D model. Each stipple bot represents a single stipple dot and sees only its local
neighborhood. It is capable of simple actions such
as trying to go to dark regions while avoiding other
stipple bots. By simulating this behavior, RenderBots achieves a similar stippling distribution as
the previously mentioned techniques, albeit with
more randomness. Sousa and colleagues (hereafter referred to as Sousa) approximate stippling by
using short, serrated ink strokes modeled directly
over the mesh’s edge.8 This technique places and
stylizes strokes using parameters extracted from
the mesh’s surface, resulting in the precise-ink illustration style.

Two stippling aesthetics
Although NPR stipple techniques capture many
aspects of hand-drawn image styles, there are visible dissimilarities between computer-generated and
hand-drawn images. In a recent study in which
participants compared hand-drawn and computer-generated pen-and-ink drawings, Isenberg and
colleagues found that participants usually could
distinguish between the two categories.9 The distinguishing features of stipple images were stipple
point density, the use of shading, and the presence
of artifacts. Artifacts included unwanted regularity in computer-guided dot placement, leading to
the formation of lines, in contrast to the more
random placement of dots in the hand-drawn images. There were also intentional artifacts in the
shape of hand-placed dots, in contrast to the very
regular, rounded computer-generated dots. These
IEEE Computer Graphics and Applications

63

Computational Aesthetics

differences might serve a purpose beyond added
visual interest: an analysis of texture properties
of hand-drawn and computer-generated images
could potentially be used to classify them into two
separate categories if the differences are common
within a set.
Isenberg and colleagues’ study also showed that
the differences between hand-drawn and computer-generated images did not necessarily lead people
to appreciate or like one category more than the
other. On the contrary, participants said that they
liked both categories of images, each for different reasons, and would use them in different domains. Thus, we can conclude that although NPR
techniques have potential for improvement, handdrawn and computer-generated stippling involve
two different aesthetics.

Although no formal	
metric has been introduced
for distinguishing handdrawn and computergenerated stippling,
differences clearly exist.
Hand-drawn stippling aesthetics
As a visual style, stippling fills an important role
in medical, scientific, and technical illustration.
With its ability to depict tonal variations, stippling
is well suited for illustrating objects with fine detail, subtle texture, and small changes in shading.
A limitation of line illustration techniques in general is that the individual marks must be smaller
than the finest detail to be depicted. In stippling,
this is rarely a concern. Gradients and soft edges
are relatively easy to create by varying the size and
density of marks. However, long lines and hard
edges are relatively difficult to create using stipples. For particular illustrations, stippling is the
preferred choice because other pen-and-ink techniques, such as hatching, might be mistaken for
contours in the images. The random placement of
stipples in medical illustrations provides for tone
and shape, while not creating any undesired directional cues. This is not to say, however, that creating structures in stippling is always undesired.
In creating stippled illustrations, artists must
address several interrelated issues. First, they must
choose the physical characteristics of the marks,
including size, variability, and frequency. Second,
they must choose edge- and shape-handling techniques, which affect shape and form recognition
as well as depth cues from interacting shapes. Fi64	

March/April 2008

nally, they must choose form-shading techniques,
which emphasize and deemphasize objects. All
these factors contribute to the perceived aesthetics of hand-drawn stipples.

NPR stippling aesthetics
Computer artists typically create stipples in NPR
by placing points explicitly or using small short
strokes that approximate stippling. NPR stipple
creation involves choosing a stipple primitive and a
stipple distribution. Algorithmic stipple placement
enables computer-generated stipple illustrations to
use a far greater number and thus a higher density of
stipple points. This means that smaller dots can be
used, resulting in potentially finer detail. Strict use
of a model and a shading computation leads to an
almost realistic depiction of the illustrated shapes.
Another factor influencing the aesthetics of NPR
stippling is the choice of dot or line shapes. Explicit point placement techniques usually employ
dots ranging from perfectly symmetric to slightly
irregular, asymmetric, but still rounded marks to
simulate hand-drawn dot-by-dot stippling.2,3 Short
strokes are typically asymmetric to replicate the
precise-ink stippling technique.8 Depending on the
type of rendering, the mark size is sometimes close
to the final resolution of the pixel image, leading
to pixels or small groups of pixels representing one
dot or one short stroke. Again, all these factors
contribute to the perceived stipple aesthetics.

Distinguishing the aesthetics
Although no formal metric has been introduced
for distinguishing hand-drawn and computergenerated stippling, differences clearly exist. One
difference might be the potential preciseness of
computer-generated stipples compared with handdrawn stipples, as discussed by Isenberg and colleagues.9 This preciseness could lead viewers to
sense sterility or rigidness in computer-generated
stipples. Such qualities are not always undesirable.
More detailed structures can show more accurate
shape, shading, and illumination. Furthermore,
computers are very good at creating patterns,
which can enhance the perception of object features. Such structures might be more difficult
to represent in hand-drawn images. In contrast,
hand-drawn images can seem less sterile to viewers because many natural surfaces have statistical
properties that imply self-similarity, meaning that
any extracted sample will have properties similar to the whole. Self-similarity is often found in
natural textures. If these natural properties exist
in hand-drawn images, but not in computer-generated images, this difference could explain the
visible dissimilarities between hand-drawn and
computer-generated images. However, if we con-

sider this in terms of textures, it is possible that
placing points explicitly, as opposed to placing
small marks, will fall into separate texture classes
(that is, statistical or structural).

Statistical texture analysis methods
As we previously stated, the two common texture
analysis methods are structural and statistical. A
structural approach works well for regular patterns, but the lack of discernible patterns in stippling requires using a statistical approach. There
are many statistical texture analysis algorithms
designed to represent textures for comparison.
GLCM, Fourier power spectra, and texture spectra
are some of the more common approaches.
We have chosen the GLCM algorithm to analyze stipple textures for several reasons. First, studies in perceptual psychology have shown that the
GLCM closely matches levels of human perception.10 Second, many studies have shown that this
method outperforms others in texture discrimination. For example, Weszka, Dyer, and Rosenfeld
analyzed GLCM performance in comparison with
three other algorithms on a set of aerial-photographic terrain samples.11 Their study showed
that GLCM outperformed other algorithms with
respect to textural feature derivation. Finally, we
use the GLCM algorithm rather than more recent
algorithms, such as advanced implementations of
the gray-level difference histograms12 and graylevel aura matrices.6 Although these methods are
appropriate for texture analysis and synthesis,
we don’t need their ability to catch structures in
textures because stipple artists avoid producing
oriented textures or unintended patterns.1 Thus,
algorithms that measure anisotropy and texture
symmetry are not necessary and might not be adequate for comparing stipple textures.

Applying GLCM to stipple textures
A GLCM is a two-dimensional array L in which
rows (r) and columns (c) represent a set of possible gray-tone values G. The value L(i, j) indicates
how many times value i co-occurs with value j in a
given spatial relationship defined by d. If we allow
d to be a displacement vector (dr, dc), where dr is
the displacement in rows and dc is the displacement in columns, the co-occurrence matrix Ld for
an image, I, is defined
Ld = |{[r, c]| I[r, c] = i and I[r + dr, c + dc] = j}|
Figure 2 illustrates this equation with a simple 4
× 4 image I and two different co-occurrence matrices for I: L[0, 1] and L[1, 1]. In this example, I
consists of three different gray levels, denoted 0,
1, and 2. For L[0, 1], we have used a horizontal
	

J
0

0

2

2

0

0

2

2

1

1

0

0

1

0

0

1

I

J

0

1

2

0

1

2

0

4

0

2

0

3

1

1

1

2

2

0

1

1

1

0

2

0

0

2

2

1

0

1

Image I

L[0, 1]

d:

i

j

d:

i
j

L[1, 1]

Figure 2. Two GLCMs (L[0, 1] and L[1, 1]) for sample image I, with three
levels of gray values (0, 1, 2). The GLCMs are indexed by the gray values,
and each matrix entry contains the co-occurrence of value i with value j
in spatial relationship d.

displacement vector of unit length, and position
[0, 0] in the GLCM has a value of 4, indicating
that j = 0 appears directly to the right of i = 0 four
times in I. Similarly, in L[0, 1] position [0, 1] has
a value of 0, indicating that j = 1 never appears directly to the right of i = 0 in I. If we use a diagonal
displacement vector of unit length and direction
(1, 1), our GLCM for I will be L[1, 1]. Here, in L[1,
1], position [0, 0] has a value of 3, indicating that
j = 0 appears directly diagonal to i = 0 three times
in the image.
Once the GLCM, Ld, is calculated, it is typically
normalized to a matrix, Nd, so that the values lie
between 0 and 1. Thus, we can regard the co-occurrence values of Nd as the probability that one
gray level will occur with respect to another gray
level in direction d:
Nd =

Ld [i, j]
i
j Ld [i, j]

∑∑

From this result, we can see that the GLCM
captures texture properties but is not directly useful for further analysis, such as comparing two
textures. Instead, we compute textural statistics
from the GLCM to represent the texture more
compactly. Of the 14 textural statistics proposed
by Haralick, Shanmugam, and Dinstein,5 Baraldi
and Parmiggiani found six independent texture
measures: energy, contrast, variance, correlation,
entropy, and inverse difference movement. Baraldi and Parmiggiani investigated the meanings of
these statistical GLCM parameters.13 They found
energy and contrast the most efficient parameters
for discriminating different textural patterns.
Furthermore, they found that in an image without probable linear dependencies, correlation is
efficient in statistically discriminating areas with
low textural content. Given the random placement
of the initial stipple distribution by artists and
computer programs alike, we can assume that the
probability of linear dependencies in a stipple texture is low. Thus, our work uses only three texture
IEEE Computer Graphics and Applications

65

Computational Aesthetics

statistics for comparison: contrast, energy, and
correlation. We derive these features from the
normalized co-occurrence matrix.
The first property we analyze is texture contrast. Contrast represents the difference between
the highest and lowest values of a contiguous set
of pixels. This implies that a low-contrast image
is not characterized by low gray levels but rather
by low spatial frequencies. Thus, GLCM contrast
is highly correlated with spatial frequencies. We
define contrast as
Contrast =

∑ ∑(i − j) N [i, j]
2

i

d

j

The next property we analyze is energy. Energy
measures textural uniformity. When only similar
gray-level pixels are present in an image patch, a
few GLCM elements will be close to 1, while many
elements will be close to 0. In that case, energy
reaches values close to its maximum. Therefore,
high energy values occur when the texture’s graylevel distribution has either a constant or a periodic form. We define energy as
Energy =

∑ ∑ N [i, j]
i

2
d

j

Finally, we look at texture correlation. GLCM
correlation is expressed as the correlation coefficient between two gray levels in the texture. High
correlation values (near 1) imply a linear relationship between the gray levels of a pair of pixels.
Correlation can be measured either in high- or
low-energy situations and is uncorrelated with the
GLCM contrast metric. We define correlation as

Correlation =

∑ ∑(i − µ )( j − µ )N [i, j]
i

j

i

j

d

σiσ j

where µi and µj are the means and σi and σj are
the standard deviations of the row and column
sums of Nd.
To effectively compare different stipple textures, we gathered a set of computer-generated
images representing systems that use 3D geometry, producing vector graphic output or pixel images. 3,7,8 In addition, four medical and scientific
illustrators provided hand-drawn images for our
study: William M. Andrews, Andrew Swift, Emily
S. Damstra, and Gerald P. Hodge. We compared
the computer-generated images with the images
of one or more of the artists. We also compared
two hand-drawn images with two natural-texture photographs.
To compare the texture samples shown in Figure
66	

March/April 2008

3a and other figures in this article, we performed
several image preprocessing steps. We scan-converted and down-sampled each vector image so
that the smallest stipple covered approximately one
pixel in each image, thus letting us apply GLCM
to analyze stipple density. Similarly, we scanned
the hand-drawn images in black and white at high
resolution and down-sampled them to approximately the same resolution as the vector images.
After down-sampling all the images, we cropped a
small texture sample of 50 × 50 pixels from each
image. Next, we performed an initial analysis of
these texture samples to determine the optimal
parameters needed to create the GLCM.
To use the GLCM effectively for texture analysis,
we must first make reasonable choices of GLCM
parameters. Texture analysis requires choosing
window size, offset direction, offset distance,
number of gray levels to quantify the picture, and
statistical measures. We have already explained
the texture measures we use—contrast, energy,
and correlation. Furthermore, given the randomness of stipple patterns and the fact that stipple
images are essentially black and white, we planned
to create texture statistics along only one directional vector and quantize the image to a small
number of gray levels.
The purpose of our first analysis, therefore, was
to make sure that the offset direction would not
affect our texture statistics. The relation between
the reference pixel and its neighbor can be in any
one of eight directions (north, south, east, west,
or the four diagonals). Since north is opposite of
south, and likewise east of west, we can reduce
the number of directions to be analyzed to four.
We computed texture statistics using four offset
directions: northeast, east, southeast, and south.
We calculated 40 different GLCMs for each offset
direction, with magnitudes of 1, 2, …, 40, where
we used the maximum magnitude 40 to avoid
problems near the edges of the texture. We used
12 texture samples, spanning our range of handdrawn and computer-generated images. We then
plotted texture statistics, and the results of this
analysis confirmed that the offset direction does
not affect the texture statistics for stipple textures.
This analysis ensures that the stipple patterns lack
any consistent directionality.
In addition to ensuring the independence of
texture statistics from the offset direction, we
had to analyze the effects of down-sampling and
scanning. In our stipple images, only two unique
gray levels should represent black and white. To
verify that scanning and down-sampling didn’t
introduce any graying artifacts, we calculated the
GLCM multiple times by changing the number of
gray levels used to quantify an image. We com-

Courtesy of Isenberg et al.9; copyright respective artists

(a)
0.8

30

1.0

0.7
0.5

0.8
0.7

20

0.4
0.3

Energy

Contrast

Correlation

0.9

25

0.6

15

0.2
0.1

10

0
−0.1

0.1

0

(b)

10
15
20
25
30
Horizontal offset magnitude

35

Swift (artist)

40

0.4
0.2

5

5

0.5
0.3

−0.2
0

0.6

0
0

5

10
15
20
25
30
Horizontal offset magnitude
Secord

Sousa

35

40

0

5

10
15
20
25
30
Horizontal offset magnitude

35

40

Schlechtweg

Figure 3. Hand-drawn and computer-generated stippling in anatomical illustrations. (a) Stipple images of a partial skeleton,
with detail samples indicated by red squares. Images from left are a hand-drawn illustration by Andrew Swift and illustrations
generated with techniques of Secord,3 Sousa,8 and Schlechtweg.7 (b) Texture statistics from samples in (a): correlation (left),
contrast (middle), energy (right).

puted texture statistics using the same texture
samples as the directional analysis and varying
the gray levels rather than the direction. We used
gray-level values of 2, 8, 64, and 128 for quantization. We used a horizontal offset vector and
calculated 40 different GLCMs for each quantization. We then plotted texture statistics. The
results confirmed that the number of gray levels
used for quantization doesn’t affect the energy or
correlation statistics for stipple textures, implying
that only two different gray levels exist in the images. Contrast, on the other hand, varied widely
because this property is scaled by the number of
gray levels; however, results remained consistent
within the scaling.
From our earlier analysis, we chose to create GLCMs for texture analysis using 50 × 50-pixel texture samples. We then created GLCMs from these
samples, using horizontal offsets of (0, 1), (0, 2),
…, (0, 40). Each GLCM uses eight different gray
levels to quantize the image. Then we calculated
	

and compared correlation, energy, and contrast
statistics for the textures.
The sample size choice was necessary to avoid
taking unwanted lines or other structures from
the images under analysis. However, a sample of
any size can be used as long as it contains only the
stipple texture. The results remain consistent even
when the sample size varies.

Results
Figure 3b presents the three texture statistics
plots of the texture samples in Figure 3a. Each
plot displays the corresponding texture statistic
in relation to the magnitude of the offset direction. Figure 3b (left) shows the texture correlation of the four samples. Three of the correlation
plots (Swift’s, Secord’s, and Schlechtweg’s) have
smooth curves falling from a high correlation at
an offset of one pixel to a low correlation as the
offset approaches approximately five pixels. In contrast, Sousa’s texture correlation plot resembles
IEEE Computer Graphics and Applications

67

Computational Aesthetics

that of a random distribution. The smooth falloff of Schlechtweg’s and Secord’s plots is the only
characteristic similar to Swift’s correlation plot.
Schlechtweg’s plot contains an oscillating function, indicating an underlying pattern in the
texture structure. Secord’s plot shows similar oscillations along with a sharp secondary spike, also
not present in the artist’s plot.
Figure 3b (middle) shows the texture contrast
of the same four samples in relation to the offset magnitude. Again, we see smooth curves in
Schlechtweg’s and Swift’s graphs. These curves
indicate that the contrast to neighboring pixels
is low, but farther from the reference pixel, the
contrast becomes high. Schlechtweg’s graph again
displays periodicity, indicating an underlying pattern in the texture. Secord’s plot also indicates a
low contrast to neighboring pixels; however, the
change in contrast levels in this image is noticeably sharper than in Schlectweg’s image or the
hand-drawn image (Swift). Again, Sousa’s plot
indicates a random distribution of pixels with a
varying contrast among pixels across the board.
Figure 3b (right) shows the samples’ texture energy in relation to the offset magnitude. Again,
we see discrepancies among the techniques. The
hand-drawn image has a smooth curve, with a
slightly higher set of energy values corresponding
to pixels neighboring the reference. This value levels off to a constant value, indicating that once the
offset is a certain distance (about five pixels) from
the reference, the texture maintains a constant
level of uniformity, where a value of 1.0 for energy would indicate a constant image. Schlechtweg’s
plot has a curve similar to Swift’s, simply offset at
a lower value. Secord’s plot has a similar but less
pronounced shape, with a value near zero, indicating that the uniformity of the texture produced
is very low. Similarly, Sousa’s plot is a constant
value with a fairly high energy level, indicating a
random stipple distribution. The high energy most
likely results from a low stipple distribution over
the image.
These plots show that hand-drawn textures tend
to have a strong local correlation among neighboring pixels, indicating that hand-drawn stipple
points tend to have other stipple points drawn
nearby, and the distribution tapers off as the artist works outward from that area. This strong local correlation also accounts for the low contrast
among neighboring pixels and the uniform energy
level. In contrast, the computer-generated textures
either show no local correlation (Sousa) or contain unwanted patterns (periodicity for Secord
and Schlechtweg).
We analyzed another set of images from the
same systems and artist. Figure 4a shows the four
68	

March/April 2008

stipple images. Again, we took texture samples
from each image, analyzed them with the GLCM,
and plotted the same texture statistics. Comparing
the graphs in Figure 4b with those in Figure 3b
shows that the textures from the stippled arrowheads have similar texture statistics to those of the
stippled skeletons. The main difference in Figure
4 is in Sousa’s texture, where the contrast and energy values have changed because the sample came
from a densely stippled rather than a sparsely stippled area. To strengthen our findings, we took a
second sample set from these images and applied
the GLCM analysis. The results (Figure 4c) are
comparable to those in Figure 4b.
Our next analysis used images and texture samples from the same three computer-stippling systems, as well as hand-drawn images by William M.
Andrews and Andrew Swift. Figure 5a (page 70)
shows the images and samples. Figures 5b-5d show
texture statistics for the samples. Here, the two
hand-drawn textures have similar texture statistics,
although the two samples exhibit different tones.
When we compare the texture statistics in Figures
5b-5d with our previous results, we see similar features among the different systems.
Next, to demonstrate the lack of directionality
in stipple textures, we applied the GLCM algorithm to the texture samples of Figure 5a, varying
the offset vector direction. Figure 5b shows texture
statistics based on a horizontal offset. In Figure 5c,
the statistics are based on a vertical offset. Finally,
Figure 5d shows the statistics based on a southeast
offset. In each case, changing the directional vector had little effect on the textural descriptors, so
we conclude that these textures are directionally
invariant. We applied other directional vectors to
these textures as well as to other texture samples,
and all results were consistent with the findings
presented here.
Finally, we compared two hand-drawn images (a
cicada by Gerald P. Hodge and an artifact by Emily
S. Damstra) with two natural-texture photographs.
Figure 6a (page 71) shows the images and samples.
The two natural textures (granite and quartzite)
appear to have a random surface pattern, making them appropriate for GLCM analysis. Figure
6b shows the texture statistics for these samples.
Although the curves representing the hand-drawn
images aren’t quite as smooth, they are still similar
to those of the previous hand-drawn textures. The
natural textures show a strong local coherency in
neighboring pixels with a much more pronounced
fall-off curvature than even the hand-drawn textures. Likewise, the low local-contrast level and
the low energy values of the real-world textures are
much closer to the statistics of hand-drawn textures
than to those of computer-generated textures.

1

2

1

2

2

1

1

(a)
0.8

30

1.0

0.7
0.5

0.8
0.7

20

0.4
0.3

Energy

Contrast

Correlation

0.9

25

0.6

15

0.2
0.1

10

0
−0.1

5

(b)

10
15
20
25
30
Horizontal offset magnitude

35

40

0.8

0
0

5

10
15
20
25
30
Horizontal offset magnitude

35

40

30

Energy

0.1

10

0
−0.1

(c)

35

40

5

10
15
20
25
30
Horizontal offset magnitude

35

40

0.5
0.4

0.1

0
10
15
20
25
30
Horizontal offset magnitude

0.6

0.2

5

5

40

0.3

−0.2
0

35

0.7

15

0.2

10
15
20
25
30
Horizontal offset magnitude

0.8

Contrast

0.3

5

0.9

20

0.4

0

1.0

25

0.5

0.4

0.1

0.7
0.6

0.5

0.2

0
0

0.6

0.3

5

−0.2

Correlation

Courtesy of Isenberg et al.9; copyright respective artists

2

0
0

5

Swift (artist)

10
15
20
25
30
Horizontal offset magnitude
Secord

35

40

Sousa

0

Schlechtweg

Figure 4. Hand-drawn and computer-generated stippling in archaeological illustrations, with two samples each. (a) Stipple
images of an arrowhead, with enlarged images of stipple samples. Images from left, are a hand-drawing by Swift and computergenerated illustrations from Secord, Sousa, and Schlechtweg. (b) Texture statistics from sample set 1 and (c) texture statistics
from sample set 2.

To further verify these results, we applied the
GLCM algorithm to a larger texture sample in our
images. Figure 7a (page 72) shows one example.
Here, each sample is approximately 150 × 150
pixels and is down-sampled so that the smallest
stipple covers one pixel. We created GLCMs from
these samples using horizontal offsets of (0, 1),
(0, 2), …, (0, 140). We took the third sample in
Figure 7a from a slightly different portion of the
plant to obtain a larger number of stipples. Figure
7b shows texture statistics similar to those of Fig	

ure 5b. We also performed the same analysis for
larger samples of the arrowhead images in Figure
4a, with similar results.

Conclusions
We have shown that stipple distribution statistics vary among hand-drawn, computer-generated, and natural stipple textures. These differences
affect the aesthetics associated with the given
stipple characteristics. By applying a GLCM texture analysis, we can mathematically define these
IEEE Computer Graphics and Applications

69

Courtesy of Isenberg et al.9; copyright respective artists

Computational Aesthetics

(a)
0.8

30

1.0

0.7
0.5

0.8
0.7

20

0.4
0.3

Energy

Contrast

Correlation

0.9

25

0.6

15

0.2
0.1

10

0
−0.1

0.1

0

(b)

10
15
20
25
30
Horizontal offset magnitude

35

40

0.8

0
0

5

10
15
20
25
30
Horizontal offset magnitude

35

40

30

0

Energy

15

0.2
0.1

10

0
−0.1

(c)

10
15
20
25
30
Horizontal offset magnitude

35

40

0.8

5

10
15
20
25
30
Horizontal offset magnitude

35

40

0

Energy

10

0
−0.1

(d)

Andrews (artist)

35

40

40

0.4

0.1

0
10
15
20
25
30
Horizontal offset magnitude

0.5

0.2

5

5

0.6

0.3

−0.2
0

35

0.7

Contrast

Correlation

0.1

10
15
20
25
30
Horizontal offset magnitude

0.8

15

0.2

5

0.9

20

0.3

40

1.0

25

0.4

35

0
0

30

0.5

10
15
20
25
30
Horizontal offset magnitude

0.4

0.1

0.7
0.6

5

0.5

0.2

0
5

0.6

0.3

5

−0.2
0

40

0.7

Contrast

Correlation

0.3

35

0.8

20

0.4

10
15
20
25
30
Horizontal offset magnitude

0.9

25

0.5

5

1.0

0.7
0.6

0.4
0.2

5

5

0.5
0.3

−0.2
0

0.6

0
0

Swift

5

10
15
20
25
30
Horizontal offset magnitude
Secord

35

40

0

Sousa

Schlechtweg

Figure 5. Hand-drawn and computer-generated stippling in botanical illustrations, with varied GLCM offset directions. (a) Stipple
images of tropical pitcher plant’s trap, with detailed samples. Images from left are hand-drawings by William M. Andrews and
Andrew Swift and computer-generated illustrations from Secord, Sousa, and Schlechtweg. (b) Texture statistics with horizontal
offset, (c) texture statistics with vertical offset, and (d) texture statistics with southeast offset.
70	

March/April 2008

Copyrights Hodge, Damstra, and Juracek, respectively

(a)
0.8

30

1.0

0.7
0.5

0.8
0.7

20

0.4
0.3

Energy

Contrast

Correlation

0.9

25

0.6

15

0.2
0.1

10

0
−0.1

0.1

0

(b)

10
15
20
25
30
Horizontal offset magnitude
Damstra (artist)

35

40

0.4
0.2

5

5

0.5
0.3

−0.2
0

0.6

0
0

5

Hodge

10
15
20
25
30
Horizontal offset magnitude

35

40

Juracek (Pinot Noir Granite)

0

5

10
15
20
25
30
Horizontal offset magnitude

35

40

Juracek (Quartzite)

Figure 6. Comparing hand-drawn stippling with natural textures. (a) Hand-drawn and photographic images with detailed
samples. Images from left are hand-drawings by Gerald P. Hodge1 and Emily S. Damstra and photographs of pinot noir granite
and quartzite (from Juracek14). (b) Texture statistics.

differences and provide quantitative metrics that
explain why these different aesthetics occur.
Our results show that hand-drawn stippling has
an aesthetic similar to natural textures. Furthermore, with respect to our downsampling normalization, hand-drawn textures favor the placement
of stipples within approximately a 5-pixel radius
from existing pixels. As artists move out from
their seed stipple points, the texture reaches a
uniform distribution with a low correlation between the seed stipples and neighboring areas. We
found this pattern in all four artists’ textures, and
although the curves of their textures have slight
variations, the underlying pattern remains the
same. We also found that natural textures exhibit
a strong local correlation among texture pixels,
decreasing in correlation as the distance from the
source increases.
In contrast, stipple placement in computergenerated textures doesn’t follow the same patterns
as hand-drawn and natural samples. Computer	

generated texture statistics show undesired correlation across the texture or a lack of correlation
near the seed stipple point.
To quantitatively describe what we visually observed in the stipple texture correlation statistics, we also measured the correlation coefficient
between the GLCM correlation statistics to determine whether there is a linear relationship between our texture samples’ correlation statistics.
The correlation coefficient ρ of two variables is
defined in terms of their covariance and standard
deviations as
ρ = [cov(X, Y)]/(σXσY)
where X and Y are the texture correlation statistics
being compared. This gives us a quick and easy way
to compare the relationship between two textures’
correlation statistics. If no relationship exists, the
correlation coefficient is 0. If the correlation statistics match perfectly, the correlation coefficient is 1.
IEEE Computer Graphics and Applications

71

Courtesy of Isenberg et al.9; copyright respective artists

Computational Aesthetics

(a)
0.8

6

1.0

0.7
0.5

0.8
0.7

0.4
0.3
0.2
0.1

Energy

4
Contrast

Correlation

0.9

5

0.6

3
2

0
−0.1

0.1

0

(b)

10
15
20
25
30
Horizontal offset magnitude

35

40

0.4
0.2

1

5

0.5
0.3

−0.2
0

0.6

0
0

Andrews (artist)

5

10
15
20
25
30
Horizontal offset magnitude

Secord

35

40

Sousa

0

5

10
15
20
25
30
Horizontal offset magnitude

35

40

Schlechtweg

Figure 7. Hand-drawn and computer-generated stippling, with larger sample sizes. (a) Stipple images of a tropical pitcher plant’s
trap, with detailed samples. Images from left are a hand-drawing by William M. Andrews and computer-generated images from
Secord, Sousa, and Schlechtweg. (b) Texture statistics.

Table 1. Correlation coefficients of GLCM statistical correlations from Figure 5b (left).
Artist or technique

Andrews

Swift

Secord

Sousa

Schlechtweg

Andrews

1.0

0.8558

0.2564

0.3031

0.8369

Swift

0.8558

1.0

0.3893

0.3961

0.7459

Secord

0.2564

0.3893

1.0

–0.1064

0.1946

Sousa

0.3031

0.3961

–0.1064

1.0

0.3186

Schlechtweg

0.8369

0.7459

0.1946

0.3186

1.0

We tested the correlation coefficient for all previously discussed plots. Table 1 shows the correlation
coefficients for the texture correlation statistics
shown in Figure 5b (left). These coefficients are
representative of all the other texture correlation
statistics discussed in this article. Table 1 shows
that the GLCM correlation coefficient between
hand-drawn images is large (0.8558), indicating
that the signals are closely related. When we compare the hand-drawn GLCM correlation statistics
to those of the computer-generated images, we find
that the correlation coefficient is very low (less
72	

March/April 2008

than 0.4), indicating that these signals are not related. However, comparing the hand-drawn GLCM
correlation statistics with Schlechtweg’s shows that
the correlation coefficient is large. You can see this
easily in Figure 5b, where both Schlechtweg’s and
the hand-drawn image plots have smooth curves,
falling from a high correlation at an offset of one
pixel to a low correlation as the offset approaches
five pixels. At the same time, however, it is easy to
see that Schlechtweg’s texture contains a repeated
pattern not present in the hand-drawn stipples.
Overall, current computer-generated stipple tex-

tures, generated with mathematical distribution
functions such as Poisson and Voronoi, don’t compare
well statistically with hand-drawn stipples. However,
stipple renderers that base stipple placement on factors such as shading and illumination seem to come
closer to matching artistic textures. Therefore, generating stipple distribution functions that approximate these local-coherency patterns might match
hand-drawn stipple patterns more closely.
Here, we have shown only a small sample of our
analyzed stipple textures. The samples we chose
for this article are representative of a wide variety of systems. These images provide a realistic
comparison since both the artists and the NPR algorithms used the same input—3D models of the
skeleton, arrowhead, and tropical pitcher plant.
We have other texture samples, as well as samples
from different portions of the presented images,
that also have the same texture statistics.
Because the GLCM is defined by the orientation d while taking into account other statistical
texture properties, computer artists could use the
GLCM in creating NPR textures. Researchers have
already used the GLCM for texture synthesis. Copeland, Ravichandran, and Trivedi presented an
analysis of texture synthesis algorithms based on
the GLCM of a texture field.15 They used this work
to synthesize pigskin, raffia, and wood textures
and conducted a perceptual study to analyze the
effectiveness of the synthesis. We plan to extend
this work, using our analysis of stippling as the
basis of texture generation.
The discrepancies between hand-drawn and computer-generated texture statistics show that there is
still room for improvement in computer-generated
stipple textures. In the small sample we analyzed,
hand-drawn texture statistics seem to have a higher
correlation to real textures. From the GLCM statistics we presented in this article, we plan to develop
stipple distribution functions and a new set of principles to guide stipple texturing.

References
1.
2.

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

Acknowledgments
We thank Emily Damstra, Tobias Germer, Gerald P.
Hodge, Aidong Lu, Adrian Secord, and Andrew Swift
for providing the stipple images used in this article
or the software for producing them. We also thank
Edward Delp for his discussions on gray-level cooccurrence matrices. Our work is supported by the
National Science Foundation under grants 0081581,
0121288, and 0328984 and by Alberta Ingenuity. It
is also supported by discovery grants from the Natural Sciences and Engineering Research Council of Canada. Wei Chen is supported by the National Science
Foundation of China under grant 60503056 and by
the 863 program of China (2006AA01Z314).
	

13.

14.
15.

E.R.S. Hodges, ed., The Guild Handbook of Scientific
Illustration, Van Nostrand Reinhold, 1989.
O. Deussen et al., “Floating Points: A Method for
Computing Stipple Drawings,” Computer Graphics
Forum, vol. 19, no. 3, Aug. 2000, pp. 40-51.
A. Secord, “Weighted Voronoi Stippling,” Proc.
2nd Int’l Symp. Non-Photorealistic Animation and
Rendering (NPAR 02), ACM Press, 2002, pp. 37-43.
B. Wang et al., “Efficient Example-Based Painting
and Synthesis of 2D Directional Texture,” IEEE
Trans. Visualization and Computer Graphics, vol. 10,
no. 3, May/June 2004, pp. 266-277.
R.M. Haralick, K. Shanmugam, and I. Dinstein,
“Textural Features for Image Classification,” IEEE
Trans. Systems, Man, and Cybernetics, vol. 3, no. 6,
Nov. 1973, pp. 610-621.
X. Qin and Y.-H. Yang, “Basic Gray Level Aura
Matrices: Theory and Its Application to Texture
Synthesis,” Proc. 10th IEEE Int’l Conf. Computer
Vision (ICCV 05), IEEE CS Press, 2005, vol. 1, pp.
128-135.
S. Schlechtweg, T. Germer, and T. Strothotte,
“RenderBots—Multi-Agent Systems for Direct Image
Generation,” Computer Graphics Forum, vol. 24, no.
2, June 2005, pp. 137-148.
M. Costa Sousa et al., “Precise Ink Drawing of 3D
Models,” Computer Graphics Forum, vol. 22, no. 3,
Sept. 2003, pp. 369-379.
T. Isenberg et al., “Non-Photorealistic Rendering in
Context: An Observational Study,” Proc. 6th Int’l
Symp. Non-Photorealistic Animation and Rendering
(NPAR 06), ACM Press, 2006, pp. 115-126.
B. Julesz et al., “Inability of Humans to Discriminate
between Visual Textures That Agree in Second-Order
Statistics—Revisited,” Perception, vol. 2, no. 4, 1973,
pp. 391-405.
J.S. Weszka, C.R. Dyer, and A. Rosenfeld, “A
Comparative Study of Texture Measures for Terrain
Classification,” IEEE Trans. Systems, Man, and
Cybernetics, vol. 6, no. 4, Apr. 1976, pp. 269-285.
D. Chetverikov, “GLDH Based Analysis of Texture
Anisotropy and Symmetry: An Experimental Study,”
Proc. 12th Int’l Conf. Pattern Recognition (ICPR 94),
IEEE Press, 1994, vol. 1, pp. 444-448.
A. Baraldi and F. Parmiggiani, “An Investigation of
the Textural Characteristics Associated with Gray
Level Cooccurence Matrix Statistical Parameters,”
IEEE Trans. Geoscience and Remote Sensing, vol. 33,
no. 2, Mar. 1995, pp. 293-304.
J.A. Juracek, Surfaces: Visual Research for Artists,
Architects, and Designers, W.W. Norton, 1996.
A.C. Copeland, G. Ravichandran, and M.M. Trivedi,
“Texture Synthesis Using Gray-Level Co-Occurrence
Models, Algorithms, Experimental Analysis and
Psychophysical Support,” Optical Engineering, vol.
40, no. 11, Nov. 2001, pp. 2655-2673.
IEEE Computer Graphics and Applications

73

Computational Aesthetics

Ross Maciejewski is a PhD student in electrical and computer
engineering at Purdue University.
His research interests include nonphotorealistic rendering and visual
analytics. Maciejewski has an MS
in electrical and computer engineering from Purdue University and BS degrees from
the University of Missouri, Columbia. Contact him at
rmacieje@purdue.edu.
Tobias Isenberg is an assistant
professor of computer graphics and
interactive systems at the University of Groningen, the Netherlands.
His research interests include nonphotorealistic rendering, illustrative rendering and visualization,
and the interaction with such graphics. Isenberg did his
doctoral studies at the University of Magdeburg, Germany, before completing a post-doctoral fellowship at
the University of Calgary, Canada. Contact him at
isenberg@cs.rug.nl.
William M. Andrews is a certified medical illustrator and an
associate professor of medical illustration at the Medical College of
Georgia. He is also a PhD student
in health promotion, education,
and behavior at the University of
South Carolina, Columbia. His research interests include health literacy and education, visual perception,
and communication theory. Andrews has an MA in
biomedical communications from the University of
Texas Southwestern Medical Center at Dallas. Contact him at bandrews@mcg.edu.
David S. Ebert is a professor and
university faculty scholar in the
School of Electrical and Computer
Engineering at Purdue University,
where he is also director of both
the Rendering and Perceptualization Lab and the Purdue Univer-

sity Regional Visualization and Analytics Center. His
research interests include novel visualization techniques, visual analytics, volume rendering, perceptually based visualization, illustrative visualization,
nonphotorealistic rendering, and procedural abstraction of complex, massive data. Ebert has a PhD in
computer science from Ohio State University. Contact
him at ebertd@purdue.edu.
Mario Costa Sousa is an associate professor in the Department of
Computer Science at the University of Calgary and a member of the
university’s Computer Graphics
and Visualization Research Lab.
His research interests include nonphotorealistic rendering, illustrative visualization,
sketch-based interfaces and modeling, volume graphics, and interactive simulations. Sousa has a PhD in
computer science from the University of Alberta. Contact him at mario@cpsc.ucalgary.ca.
Wei Chen is an associate professor in the State Key CAD and
Computer Graphics Lab at Zhejiang University, PR China. He is
currently a visiting scholar at Purdue University, where he participated in the work described in this
article. His research interests include illustrative visualization, visual analytics, and interactive modeling.
Chen has a PhD in computer graphics from Zhejiang
university and the Fraunhofer Institute for Graphics,
Germany. Contact him at chen23@purdue.edu.

For further information on this or any other computing topic, please visit our Digital Library at http://
www.computer.org/csdl.

IEEE Computer Society members

save 25%

on all conferences sponsored by the
IEEE Computer Society

www.computer.org/join
74	

March/April 2008

220	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016

Exploring Evolving Media Discourse Through Event Cueing
Yafeng Lu, Michael Steptoe, Sarah Burke, Hong Wang, Jiun-Yi Tsai,
Hasan Davulcu, Douglas Montgomery, Steven R. Corman, Ross Maciejewski, Senior Member, IEEE

Fig. 1: Overview of the event cueing visual analytics framework. The map view provides a geographical visual analytics environment to enable exploration of frames and entities over space and time. The detailed view to the right of the map switches
between entity wordles and list-based displays. The time series view contains a hierarchical frame analysis visualization. Each
line visualizes significant events and the sentiment associated with a media frame or a frame class in the expanded leaf nodes. The
control pane, which consists of the top left donuts, shows the distribution of frames and events and is used to filter categorical
variables in the linked views.
Abstract— Online news, microblogs and other media documents all contain valuable insight regarding events and responses to
events. Underlying these documents is the concept of framing, a process in which communicators act (consciously or unconsciously)
to construct a point of view that encourages facts to be interpreted by others in a particular manner. As media discourse evolves, how
topics and documents are framed can undergo change, shifting the discussion to different viewpoints or rhetoric. What causes these
shifts can be difficult to determine directly; however, by linking secondary datasets and enabling visual exploration, we can enhance
the hypothesis generation process. In this paper, we present a visual analytics framework for event cueing using media data. As
discourse develops over time, our framework applies a time series intervention model which tests to see if the level of framing is
different before or after a given date. If the model indicates that the times before and after are statistically significantly different, this
cues an analyst to explore related datasets to help enhance their understanding of what (if any) events may have triggered these
changes in discourse. Our framework consists of entity extraction and sentiment analysis as lenses for data exploration and uses two
different models for intervention analysis. To demonstrate the usage of our framework, we present a case study on exploring potential
relationships between climate change framing and conflicts in Africa.
Index Terms—Media Analysis, Time Series Analysis, Event Detection

1

I NTRODUCTION

Recently, the visual analytics community has begun developing a variety of tools for analyzing media collections. These tools tend to focus
on event detection from text streams [39], correlation analysis [28],
• Yafeng Lu, Michael Steptoe, Sarah Burke, Hong Wang, Jiun-Yi Tsai,
Hasan Davulcu, Douglas Montgomery, Steven R. Corman, and Ross
Maciejewski, are with Arizona State University. E-mail:
{lyafeng,msteptoe,seburke2,hxwang,jtsai8,HasanDavulcu,
doug.montgomery,steve.corman, rmacieje}@asu.edu.
Manuscript received
received 31
31 Mar.
Mar. 2015;
2015;accepted
accepted 11Aug.
Aug.2015;
2015;date
dateof
of
Manuscript
publication xx
20 Aug.
Aug. 2015;
2015; date
date of
of current
current version
version 25
25 Oct.
Oct. 2015.
For information on obtaining reprints of this article,
article, please
please send
send
e-mail to: tvcg@computer.org.
Digital Object Identifier no. 10.1109/TVCG.2015.2467991

and topic evolution [15]. These tools are often concerned with understanding an ongoing narrative from structured text and focus on
enabling the user to place news stories within the context of other
ongoing events. However, very few tools [11, 12, 13] explore how
media is being framed, and, to our knowledge, none have explored
changes in frames over time and space. In studying public communications, framing is the use of rhetorical devices (e.g., words, phrases,
metaphors, images) to encourage one interpretation of a set of facts
and discourage other interpretations [16]. Examples include efforts by
U.S. conservatives in the 1990s to reframe the estate tax as a “death
tax”, and competing frames of the Occupy Wall Street protests, “the
99%” (vs. the 1%) as opposed to “makers vs. takers”. Framing affects
the attitudes and behavior of audiences [9], and it is also regarded as
a key media effect, in that media “actively set the frames of reference
that readers or viewers use to interpret and discuss public events” [35].

1077-2626 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

LU ET AL.: EXPLORING EVOLVING MEDIA DISCOURSE THROUGH EVENT CUEING

Understanding framing in the media is vital as it influences the way
people interpret the topic under analysis. Framing is also critical to
the success of social movements and can be a driver for change or
stagnation [4]. What is of interest is how these frames are applied and
how they evolve over time in the context of other events. However, it
can be quite difficult to determine when changes in framing occur and
what events may have contributed to changing attitudes.
In this work, we present a visual analytics framework for event cueing from media. For a given collection of documents (related by topic
and coded with frames), we enable analysts to explore ongoing media
discourse with respect to the overall framing and related sentiment of
the narratives. In order to understand when and how framing about a
topic has shifted, we employ intervention models for time series analysis. Such models examine how a measure changes over time and how
this measure is affected by some external event, or intervention, at a
given time t. If the measure is significantly different before and after the intervention, then one can hypothesize that an intervention is
associated with a change in the measurement. By highlighting these
statistically significant intervention points, we can cue analysts to time
periods of interest. Then, by linking the media data source with secondary sources of information relevant to the topic, an analyst can
explore the frame evolution within the context of ongoing events.
This work is directly related to previous works, such as Narratives [18] and EventRiver [27], which focus on placing media stories
into their historical and social context by allowing people to explore
topics and keywords and associate them with other ongoing stories
and events. Unlike previous work, our framework utilizes intervention modeling strategies and multisource data. Our goal is to enable
analysts to cue to important dates in the dataset. Media can then
be explored in the context of the changing sentiment of the framed
documents as well as linked to concurrent events that may have impacted the media discourse. While previous work from Diakopolous
et al. [11, 12, 13] developed tools for frame analysis, their work provided no support for entity extraction, sentiment analysis, or linking
multisource data. Our contributions include:
1) An ensemble of intervention modeling techniques for event cueing and hypothesis generation,
2) The application of visual analytics for media framing in the context of entities, sentiment, geography and multisource data.
2 R ELATED W ORK
As media sources have broadened from network news coverage to microblogs, Twitter, etc., a variety of tools and techniques have been
developed to analyze and explore such data sources. Given that media
data generates events over time in unstructured text, the majority of
tools and techniques developed have focused on temporal visualizations, topic analysis, and pattern and anomaly detection.
2.1 Time Series Visual Analytics
Visualization has been successfully applied to analyze time-oriented
data, most commonly through the use of line graphs and their variations [17], as well as calendar views and clock views for periodical or seasonal patterns [3, 22, 37]. A variety of enhancements to
these techniques have been proposed over the years to enable better
sensemaking of events and records. For example, LifeFlow [42] combines a list-based display for intra-record pattern analysis and an aggregated overview display for inter-record trends analysis to visualize
time-point based event sequences. EventFlow [30] extends LifeFlow
to handle interval events and explore the relationship between event
sequences and associated outcomes. Another extension of LifeFlow,
Outflow [41] aggregates multiple event sequences, visualizes them as
the pathways through different event states, and connects the pathways to their associated outcomes so that users can explore progression paths and results. Of interest to our work is that OutFlow also
incorporates external factors which may influence the event sequence.
Our work differs in that we focus on cueing analysts to events in time
series datasets through the use of intervention models. These models
enable users to find sequences in the data that appear to have deviated.

221

Our framework then links these deviations to external data sources to
identify potential causes to these deviations.
The incorporation of statistical techniques into time series visualization has led to the development of a variety of visual analytics solutions. A typical example is the visual analytics process proposed
in Bogl et al. [5] where visualization is used to guide domain experts in statistical model parameter selection. Their prototype system,
TiMoVA, is developed to facilitate the process of parameter settings
in autoregressive integrated moving average (ARIMA) and seasonal
ARIMA models. A probabilistic decision tree learner is used in the
e-transaction time-series visual analytics system VAET in [43] to explore transaction patterns among multiple users in a temporal context.
These tools focus on enabling users to visually develop statistical models of the data. In contrast, our work focuses primarily on using statistical models for cueing analysts to events of interest in the time series.
Our proposed type of cueing is similar to work in event detection, and visual analytics has posed a variety of solutions for anomaly
and event detection [8]. Classification-based event detection methods
have been applied in many visualization systems. For example, Scatterblogs [6, 36] is a scalable system enabling analysts to find quantitative information and detect spatiotemporal anomalies within a large
volume of geo-located microblog messages. Work by Chae et al. [7]
utilizes a seasonal-trend-decomposition method to determine anomalous changes in topics in social media. Gotz et al. developed DecisionFlow [20], which integrates interactive multi-view visualizations and
ad hoc statistical models to support the analysis of high-dimensional
temporal event sequence data. While a variety of statistical methods
have been applied for visual analytics of temporal data, these methods
typically focus on anomalous behavior. In our framework, we focus
on the concept of an external intervention causing the system to deviate. This framework requires different statistical analysis and also
needs to integrate multi-source data for analysis. To our knowledge,
this approach is the first such application in visual analytics to explore
time series data in the context of interventions.
2.2

Media Visual Analytics

While applicable to a variety of domains, our focus is specifically on
media data, such as online news and microblogs. Recently, much attention has been paid to this domain area in the visual analytics community, with techniques focusing on knowledge expression, topic extraction, pattern analysis, and storytelling [14, 19, 21, 23, 24]. CloudLines [23] focuses on the detection of visual clusters in a compressed
view of multiple time series to enable the scalable analysis of media streams. To improve sensemaking, LeadLine [14] explores named
entities, locations, and bursts of topic related events by visualizing
the shift of topic volume for different time streams and emphasizing detected events. Contextifier [21] is designed for contextualizing visualization by providing customized annotations for the stock
timeline graph with reference to the content in a news article. StoryTracker [24] combines interactive visualization and text mining techniques to facilitate the analysis of similar topics that split and merge
over time. NewsViews [19] is a novel automated news visualization
system that creates thematic maps automatically for news articles. It
leverages text mining to identify key concepts and locations discussed
in articles. TopicPanorama [25] visualizes the full picture of relevant
topics from different sources to analyze common and distinctive topics. Similar to previous works, we also leverage text mining techniques on media articles. Our system extracts entities and their associated geolocations. Unlike previous works on media frame visual
analytics [11, 12, 13], which typically focus on topic analysis and cooccurring words, our system focuses on frame analysis in conjunction
with multisource data. We focus on a single topic and explore how it
is being discussed (i.e., framed), rather than focusing on multiple topics. In this work, frames are organized into a hierarchical set and the
change in how documents are framed (with respect to space and time)
can be explored. By visualizing statistical results together with the hierarchical frames, we can enhance the hypothesis generation process.

222	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016

(a) Choropleth

(b) Cluster Pie

(c) Weighted Category Choropleth

Fig. 2: Categorical data spatial distribution visualization view. View (a) shows the default choropleth map which colors each country based
on the density of all frames. View (b) shows pie glyphs on the map displaying the proportional distribution of different frame categories in
each cluster. View (c) shows a weighted choropleth map which colors each country based on the weighted frame density. The weights on each
category can be changed interactively by the analysts.
3 E VENT C UEING E NVIRONMENT
The goal of our visual analytics framework is to facilitate the hypothesis generation process by linking multisource data through statistical
event cueing in the form of intervention models. Our framework consists of three main views: 1) the spatial view (top left Figure 1), which
visualizes the geographic location of media steams and events coded in
secondary data sources; 2) the detail view (top right Figure 1), which
provides a lens into the media text and detailed descriptions of events
from secondary data sources chosen by the analyst, and; 3) the time series view (bottom Figure 1), which shows a hierarchical frame-coded,
time-orientated media stream with sentiment and intervention analysis. All views are linked by the overview timeline shown in the middle
of Figure 1 which displays the trend of a secondary dataset.
3.1 Task Characterization
The basis for this work is founded on an interdisciplinary collaboration between computer science and communication. Partners from the
Hugh Downs School of Human Communication at Arizona State University are interested in applying their knowledge of framing to issues
of national security risks related to climate change. Their work focuses
on exploring the framing of climate change research in Africa and how
(if at all) this is impacted by ongoing conflicts in the region. They posit
that, in order to understand how the media reflect different organizations’ interests in addressing climate change as a social problem, it is
necessary to supplement the social movement focus on resource mobilization to framing processes of collective action problems. To do this,
they developed a nuanced typology for studying climate change framing and its adequacy for supporting social movement that would be
necessary to overcome the collective action problem. They apply this
framework to examine framing of climate change in media and social
media texts collected from the Niger Basin region over eight months
from August 2014 to March 2015, applying a novel coding technique
to assess diagnostic, prognostic and motivational framing as the keys
to effective social movements. While the datasets and examples given
in this application focus on media with regards to climate change and
social unrest, our techniques can be adopted to any multi-source data
in which analysts are looking for changes in media frames due to associated events (for example, severe flooding, prolonged droughts). We
have identified three major intents of the communication scientists in
the context of media analysis:
1) Analysts would like to know how frames are spatially distributed
to understand the international context of framing;
2) Analysts would like to know when the distribution of frames
change and quickly be able to explore events that may have impacted this change in media framing;
3) Analysts would like to know what people, locations and organizations are being discussed in the media before, during and after
changes in framing occur.
As such, our framework has been designed to support the spatiotemporal analysis of frames and cue analysts to when the distribution of

frames has changed. These cues then suggest time windows in which
to explore links to secondary datasets.
3.2

Datasets

To illustrate our framework, we use a climate change media dataset
and the Armed Conflict Location & Event Data Project (ACLED)
dataset [1] as an example. However the proposed framework is flexible
for analyzing any media data.
Media: The media dataset is composed of RSS feeds from 122 English language news outlets in the Niger basin countries since August
2014. RSS feeds were scanned hourly and filtered for relevance in
a two-stage process. First, news texts were matched against a set of
222 keywords developed from the Intergovernmental Panel on Climate
Change (IPCC) report and supplemented by project experts. Subsequently, texts passing the keyword test were analyzed by a machine
classifier, trained on a set of 1,000 texts classified by coders as relevant or irrelevant to social discourse of climate change. News articles passing both tests were placed into the database for analysis.
The RSS news dataset collected 1245 relevant articles with 9070 sentences. For this study, each sentence was coded by trained coders
into one (or none) of 25 categories comprising four classes (cause,
problems/threat, solution, motivation) that represent different types of
framing for climate change. Then each article was represented by a
vector of frame counts normalized by the number of sentences coded.
The average Krippendorff α reliability of the coders on a set of training documents was 0.81 and judged to be acceptable. Future work will
use trained classifiers for frame extraction.
ACLED: The ACLED dataset contains information on the dates and
locations of all reposted political violence events in over 50 developing
countries, with a focus on Africa. Each event record contains information on date, location, event type and actors involved. From August to
December 2014, it contains approximately 6500 events.
3.3

Media Data Processing

Media messages contain large amounts of information which can be
complex to effectively analyze. Our framework applies a variety of
automatic data preprocessing techniques including entity, geolocation,
and sentiment extraction.
Entity Extraction: Entities, such as person names, locations, and organizations, inform much of the underlying media discourse. A variety
of named entity recognition methods have been proposed for different
contexts in natural language processing. We applied the well-known
natural language processing tool CoreNLP [29] to a streaming RSS
news dataset and the secondary dataset (ACLED in our example) to
extract named entities. For the 1245 articles from August to December
2014, we have 19,756 entities in which there are 2107 persons, 5791
locations and 3146 organizations extracted from the RSS dataset. The
same entity recognition process was performed on the ACLED dataset
for the notes in each record, extracting 367 persons, 998 locations, 286
organizations.
Geolocation Extraction: An article may have location attributes, either based on where the article is posted or the region the article

LU ET AL.: EXPLORING EVOLVING MEDIA DISCOURSE THROUGH EVENT CUEING

223

Fig. 3: Entity lens on the map shows the most frequently appearing entities recognized from documents that are geo-located in the lens’ area.
The left figure shows the named entities for the RSS news dataset, the middle figure shows the actors given in the ACLED dataset,and the right
shows a comparison lens with the RSS news’ entities on the left and ACLED events’ actors on the right. The example shown in this Figure
covers the data from Oct. 11th to Oct. 27th for all the ACLED event type and problem frames in the RSS news.
discusses; however, this information is not always explicitly coded.
Given that framing may differ by geographic region, our framework
preprocesses the media stream to extract geographic locations. We use
the Data Science Toolkit [2] to extract and geocode this information.
Sentiment Extraction: Media data encapsulates information about
events, responses, and reviews. In exploring media data, sentiment
analysis can provide a quick overview of the attitude a media document’s author might have with regards to the underlying story. To
extract the sentiment embedded in the media data, three sentiment
analysis classifiers are applied at the per sentence level. Details on
the classifiers are provided in Section 3.6.
3.4 Geographical View
Both media data and event sequences from the secondary dataset have
geolocation information. The geographical view is built to explore the
distribution of frames and compare entities between media data and
other data sources.
3.4.1 Exploring Spatial Distribution of Frames
Previous work on frame visualization focused on document keywords.
In this work, we want to allow users to explore frames by country, entities, and sentiment. To analyze the spatial distribution of different
frames, we created a categorical spatial data distribution visualization
view, Figure 2. To show the cumulative frame distribution of a dataset,
Figure 2(a) displays a choropleth map colored by the density of frames
in each country. Users can select any subset of frame categories to analyze. If only one class of frames are selected, the map color matches
the color of the class, otherwise it uses gray. A drawback of this visualization is that only one variable/feature of the underlying data can be
represented, even though there are multiple categories of frames in the
data. To allow for multivariate encoding, we also use a symbol map
with a pie chart, where each segment of the pie represents the number
of sentences of a given frame (Figure 2(b)), and a weighted category
choropleth map (Figure 2(c)) where colors correspond to a multivariate criteria function. Additionally, a tooltip displaying the histogram
of different categories within a country is enabled to help better explore frame distributions.
3.4.2 Exploring Geo-located Entities
Our framework considers two types of entities in the data. One is recognized name entities, which are people, locations and organizations.
The other is the predefined entities that may exist in the structured
datasets that an analyst wishes to explore in the context of media discourse. We created an entity lens to explore geo-located text data.
The geocoding of the entities derives from a sentence’s geocoding for
the RSS news dataset and the reported geolocation from the ACLED
dataset. The entity lens is shown in Figure 3, where the most frequently referenced entities within the lens’s area are extracted and organized around the lens. The most frequent entities are mapped closest
to the lens’s circumference based on available canvas space. The font
size is dependent on the entities’ frequency within the lens’s circumference. The more frequent an entity is, the larger the text.

To link different datasets and find relationships between them, this
entity lens has three modes: media data entity mode which shows only
the RSS news entities (Figure 3(Left)), secondary data entity mode
which shows only the ACLED actors (Figure 3(Middle)), and the combination mode which is a two-sided lens to encode entities for multisource data (Figure 3(Right)). The combination mode shows the top
entities from the RSS news dataset on the left of the lens and ACLED
actors on the right with a dashed line in the middle to separate one
from the other. All modes are also enabled in a coordinated view in
which the lens can move over the map and update the wordles.
3.5

Hierarchical Frames Timeline View

The previous views are necessary to allow overview and detail views;
however, the major contribution of this paper is the event cueing which
is enabled through the hierarchical frames timeline. Previously mentioned techniques enable exploratory data analysis, the problem is that
purely exploratory techniques put the burden of analysis completely on
the analyst. Our goal is to cue the analyst to events that are statistically
interesting in the data. To enable this, we begin with the timeline view
showing the relative volume of frames per day. Specifically, each document has a number of sentences that are encoded with a single frame.
The percent of framing of a document is the number of sentences in
a document associated with a given frame divided by the total number of sentences framed. The frame volume can be visualized by the
average document percent per day, the average number of sentences
encoded with a frame across all documents in a day, or a variety of
other metrics.
To detect possible interventions, we applied two time series analysis
models and visualized the results on the timeline to cue analysts’ exploration. In addition, sentiment information associated to the underlying data is also revealed by a two-side uncertainty-based stack river.
Figure 4 shows our hierarchical timeline view of the media frames.
Here an analyst expanded the cause frame to explore sentences that
frame the cause of climate change to be due to human, natural effects,
policies, or one uncertain of the cause.
Our approach is centered around the concept of an intervention.
We assume that there may be events intervening with media reactions
that cause a shift in how frames are distributed. We use statistical
hypothesis test to detect the intervention dates. For each day in the
dataset, we assume an intervention may have occurred. If a date under
test indicates that the times before and after are statistically different,
this then cues an analyst to explore related datasets to help enhance
their understanding of what (if any) events may have triggered these
changes in discourse.
Intervention Modeling Intervention models are used to explore what
(if any) impact there is between an event and some secondary measure,
for example, the impact that 9-11 had on George Bush’s approval rating. In this case, we consider our events to be armed conflicts and
the measure to be the amount of sentences that are framed in a document with respect to one of the 25 climate categories. Note that such
models can be used for any event and measure dataset combination.

224	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016

Fig. 4: Hierarchical timeline view showing intervention modeling results, Before-During-After analysis results and sentiment river for each
expanded frame or frame category are shown. The frame structure is displayed as a dendrogram on the left. Clicking on the node can expand/collapse its children. The timeline associated with each leaf node is shown on the right.
Mathematically, when a time series model is affected by another input
time series, a transfer function-noise model can be used to improve the
model. The general form of this type of model is:
yt = v(B)xt + Nt

(1)

where yt is the time series of interest, v(B) is an autoregressive, integrated, moving average (ARIMA) model for the time series yt , xt is
the input time series, and Nt is a noise process [31]. A specific case
of transfer function-noise models is an intervention model, where the
input time series is an indicator variable that specifies whether some
event has taken place at time t. Such an event may have a temporary
(or permanent) effect on the level or mean of the time series of interest.
An intervention model can model the effect of a known event on
the time series. However, another common application of intervention
models is to identify outliers in the time series. In this case, we do
not know the exact time period in which the event (outlier) has taken
place. The transfer-function model for this application then becomes:
(t ∗ )

(t ∗ )

yt = v(B)εt + ωIT , where It

=

 1 if
0 if

t = t∗
t = t ∗

(2)

where ω is the change in the mean of the time series at time t ∗ and
(t ∗ )
It is an indicator function assuming that the effect of the outlier is
temporary and only occurs at time period t ∗ . Other models can be used
to model the case where an outlier may have a lasting impact on the
mean of the time series. An iterative procedure is used to identify multiple outliers in the time series. In this scenario, multiple intervention
(t ∗ )
models are fit, updating It for t ∗ = 1, . . . , N for a time series with N
time periods.
For the media data explored in this paper, intervention models were
used to detect outliers, i.e. cues to events that may be of interest to
the analyst, for each of the 25 frames over the time period of August
2 to December 31, 2014. For our intervention model, Figure 4 shows
the trend of several frame categories. A black dot represents a statistically significant shift in frames between the week before and after this
date. Users can then use the coordinated views to explore events that
occurred at this time and begin forming hypotheses on the impact that
events may have had on the media framing. Note that this is for event

cueing and hypothesis generation. Events and frames of interest found
require further investigation. Initial analysis of each time series (each
frame) indicated that there was no significant autocorrelation present.
Therefore, the intervention model can be simplified to:
(t ∗ )

yt = µ + ωIt

+ εt

(3)

where µ represents the overall mean of the time series and εt
represents the error. Outliers at time t ∗ ,t ∗ = 1, . . . , N, can be identified
by comparing the estimated value of ω, ω̂, to its standard error [31].
A significance level of α = 0.05 was used to determine whether the
value of the frame at time t ∗ was an outlier. The presence of an
outlier cues the analyst to investigate what caused this change in the
frame distribution. Although the intervention model is simplified
because the frame time series were not autocorrelated, this approach
is still valid for time series data that does have autocorrelation and
Equation 2 would be used in such cases. Such models are sensitive
to the time period under exploration. In this case, our analysts were
exploring short term changes (1 week prior to the event, 1 week after
the event). As such, the results of the intervention model tend to
highlight peaks in the data; however, this is likely an artifact of the
chosen window sizes. Future work will explore visual representations
for exploring interventions under varying window sizes.
Before-During-After Analysis Since there was no autocorrelation
in the data, a secondary model which requires an assumption of data
independence, can be applied. The second intervention test defines
a Before, During, and After period (where the during period can be
seen as the intervention) and tests their location based on the data
distributions. We let t denote the start time of the During period,
and the time windows for the Before, During and After segments are
represented by WB ,WD , and WA respectively. In this manner, the three
time segments cover the following time periods: Be f ore : (t −WB ,t),
During : (t,t + WD ), and A f ter : (t + WD ,t + WD + WA ). The data
samples for the three segments are denoted as DB = {x1 , x2 , . . . , xnB },
DD = {y1 , y2 , . . . , ynD }, DA = {z1 , z2 , . . . , znA } and they may vary
in length. Each data sample is the percentage of the frame in one
document. Because there was no significant autocorrelation present
in our underlying dataset, each sample is assumed to be independent
and identically distributed (i.i.d.) where Di ∼ N(µi , σi2 ). Therefore

LU ET AL.: EXPLORING EVOLVING MEDIA DISCOURSE THROUGH EVENT CUEING

225

we form the problem to be tested as follows:
A1 : µD is not significantly different than µB
A2 : µD is not significantly different than µA
H0 : µD is not associated with an intervention (A1 ∩ A2 )
H1 : µD is associated with an intervention (A¯1 ∪ A¯2 )
We test H0 by testing A1 and A2 . To test A1 and A2 , we applied a two-sample location test, Welch’s t-test [40], on DB , DD and
DD , DA individually with significance level α. In these two t-tests,
the statement is the null hypothesis. Because of the multiple comparisons problem (in our case we have two tests one for DB and DD ,
and another for DD and DA ), and based on the Bonferroni inequality
P(A1 ∩A2 ) ≥ 1−2α, we applied Bonferroni correction and set the significance level for each test according to the following equation [32],
α=

αF
,
#test

(4)

where α is the significance level for each two-sized t-test, αF is the
family significance level for the multiple comparison for each During time period, and #test is the number of tests applied at each time
period. In our case, #test equals 2 (the tests of A1 and A2 ). We set
αF = 0.05, which guarantees that the overall significance level for
the 2 hypothesis tests at each frame period is 0.05. To guarantee
αF = 0.05, we set α = 0.025 for each single test on the pair of consequent segments. Given the test result and the estimated µ, we can
form 9 types of volume change patterns listed in Table 1. The 9 types
are visualized in different color blocks on the time line for each frame,
as shown in Figure 4 and Figure 5.
The color scheme also denotes the group of patterns as increasing,
decreasing and oscillating. To change the interval length of each test
time period, the user can change the size of the three windows for
Before During and After using the spinners on the left. To better focus on a particular Before-During-After pattern, the user can click on
the pattern legend to gray out options. In addition to knowing the intervention time point from the results of the intervention model, the
Before-During-After analysis provides an adjustable window size and
shows any significant changes.
The statistical tests’ results are visualized in our timeline view for
each frame and frame categories, shown in Figure 4 and Figure 5.
The intervention modeling result is a set of binary indicators denoting
the statistically significant intervention points. This result is represented as a black dot on our timeline view. The Before-During-After
analysis’s result is a set of patterns describing statistically significant
changes in frame distribution over time. In both cases, the analyst can
adjust the before, after and intervention (during) periods using the controls seen in Figure 1 (lower left). In our case study, the analysts were
interested in a single day intervention with a 7 day news cycle.
Table 1: The pattern summary for Before-During-After analysis. Each
pattern is associated with a unique hue as shown in the lower lefthand
legend of Figure 1
pattern
B=D=A

sketch

description
no significant change

B=D<A
B<D=A

increase

B<D<A
. B=D>A

(blues)

B>D=A

decrease

B>D>A

(greens)

B>D<A

oscillating

B<D>A

(oranges)

Fig. 5: Sentiment stacked area chart on bi-side of the time series view.
The blue area represents positive sentiment and the red area river represents negative sentiment. The darker the area color is the more certain the label is for those sentences’ sentiment class.
3.6

Frame Sentiment Visual Analytics

The underlying sentiment of the media and its relation to the framing
can also provide insight. Sentiment analysis visualization has been
successfully applied across a variety of domains, such as political election analysis [38], and merchandise reviews [33]. However, most classifiers are text context sensitive and need to be trained on a particular
domain’s data to boost performance. Furthermore, the limitation of
sentiment classification accuracy is a problem in sentiment analysis
and is subject to uncertainty [10]. In our visual analytics framework,
we employ anl entropy-based sentiment river to reveal the uncertainty
of sentiment over time using an ensemble voting scheme from multiple
classifiers to determine the final sentiment label [26].
In our previous work [26], the uncertainty was visualized in each
time period along the entropy sentiment river. However, the time information associated with RSS media data is not as precise as online
social media data, such as Twitter. In general, the time parsed out
from the RSS news is at the granularity of a day. In one day, there
can be multiple articles collected relating to the target topic and each
article also contains several frame coded sentences. Instead of exploring only the change of the certainty over the media stream, the volume
of certain and uncertain sentiment labels is also explored. To enhance
the understanding of the volume change for both certain and uncertain sentiment labels, a stacked area graph is used to represent each
uncertain level with a stacked area and low uncertain area is stacked
at bottom. Figure 5 shows this view, where the positive sentiment is
colored in blue on top of the time series, while the negative sentiment
is colored in red on bottom of the time series. The volume of relatively
certain sentiment values are shown with a darker color and the uncertain volume with a lighter color. The height of the stacked area graph
shows the average volume of sentences per document in each sentiment polarity over time as well as the portion of uncertainty. In this
way, we can explore the positive and negative sentiment of the media
documents in conjunction with their underlying frames.
3.7

Detail View

The detailed view, Figure 6, contains two modes, the entity wordle
display and the list-based summary display. The data under analysis
for this view changes along with the time period selection, the subset
data selection for both media data and the secondary data, and the
geospatial selection. When a user is only exploring the frame class
‘Problem’ which is colored in red, only the RSS articles containing at
least one sentence being framed as ‘Problem’ will be displayed in the
detail view. For a geospatial selection, e.g. the user selects a country
to explore, the data displayed in the detail view updates to filter for
only the articles and ACLED events related to this country.
In the entity wordle view, the most frequently named entities extracted from the two datasets are displayed in two wordles. Based on
the entity’s class, which is either person, location, or organization, the
word is colored in red, green, or blue respectively. The actors in the
ACLED dataset, being entities as well, are colored in black. The size
of those entities displayed here is also proportional to its frequency.
In the list-based summary view, the RSS news article is summarized
by showing the title and a list of colored squares, where each square
represents each framed sentence colored by its frame class’s color. The
ACLED data, being the secondary data here, displays its notes for each
event in the selected time period. The background color of each note
matches the color for its event type. To analyze events containing

226	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016

(a) Detail view in entity wordle display

(b) Detail view in list-based summary display

Fig. 6: The detail view showing the most frequently named entities in a wordle display and document summary information in a list-based
display. Here we show data from Oct.11 to Oct 27. View (a) is the entity wordle display in which user can choose three classes of entities
(person, location, organization) to show. Black text in the ACLED wordle indicates an actor in the events. View (b) is the list-based summary
display in which the title of media articles and the summary information of the secondary dataset are listed in order of time. The frame
information of each article is summarized into colored squares (the color of the square matches the frame class) in the sentence order from the
article. In this example, the ACLED event notes are filtered by clicking on the entity text ‘Boko Haram’.
a particular entity of interest, a user can click on a particular entity
shown in the wordle display and information containing that entity
will show up in the summary display. Users may also filter by location
by selecting a country in the geographical view.
4

C ASE S TUDY: C LIMATE C HANGE F RAMING AND A RMED
C ONFLICT IN A FRICA
In this section, we demonstrate our work by applying the methods described so far to the RSS news dataset collected on Climate Change
from African countries and the ACLED data set, which focuses on
armed conflicts and political violence in Africa. Collaborators were
interested in linking these two data sets based on previous articles and
reports that discussed the impacts of climate change on armed conflicts [34]. Their goal was to explore the framing of news stories related to climate change and see what, if any, armed conflict events may
be linked to that discourse. In this manner, social scientists can begin
to develop models and theories about how framing can help drive political change, or conversely, how armed conflict is driving discourse.
4.1 Exploring Problem Frames in Africa
The analyst begins with an overview of the system and explores the
distribution of frames over the entire time period of data collection.
The main points of interest are the spatial and temporal distributions of
frames, Figure 7. First, the analyst explores the spatial distribution of
frames, looking at the weighted majority choropleth map. The analyst
notes that most regions are discussing climate change either in terms
of problems (red) or solutions (green). Only a few countries, such
as the Republic of Guinea-Bissau and the Republic of Côte d’Ivoire,
have a majority distribution related to causes of climate change, and
Congo has more motivation frames. The analyst drills down into the
data by selecting a country and quickly learns that only one document
has geographic information related to these countries. Thus the analyst
determines that these outliers are of little interest.
Given that the discourse seems to focus heavily on both problems
and solutions, the analyst decides to explore the temporal view with a
focus on problems. The analyst searches the top level problem hierarchy looking for significant events found in both the intervention model
and Before-During-After model. The analyst finds a time period in
late October (highlighted as circle a in Figure 7) with several points
of interest, and highlights this time period for inspection. The analyst then expands the tree and explores the leaf node problem frames,
Figure 7(bottom). The analyst notes that there are significant interventions in many categories, but very few frames regarding security
threats and water problems in this time period. The analyst further
comments on the lack of water framing in the documents noticing that
climate change is often associated with extreme weather, including
drought, yet there is little discussion in Africa about problems related

to water. The analyst does notice that there are many documents discussing problems with food.
The analyst decides to first focus on the food problem frame and
the events leading to this change in the frame distribution. The analyst narrows the time period to October 11th to October 28th, and
then filters for RSS news articles containing frame category ‘ProblemThreat’ and ACLED events Riots and Battles. The analyst wants
to explore what geographic regions are seeing large amounts of armed
conflict during this time period. The analyst selects the most prevalent
ACLED events (Riots-yellow and Battles-red) using the donut control.
The analyst notes that the largest amount of conflicts are occurring in
Nigeria, Sudan and Somalia. Given the importance of the Niger River
Basin in the area, the analyst chooses to explore events in Nigeria that
may be driving the discourse on climate change. The analyst notes
that it is interesting that there is a clear separation between the riots (in
the south) and the battles (in the north). The analyst selects Nigeria
to filter the detailed view to only RSS documents and ACLED events
that are geocoded to Nigeria.
Looking at the RSS articles’ titles, the analyst finds many reports
talking about the problem of food security and famine in Africa. While
exploring the ACLED events in the same time period, the analyst locates several riots and battles discussing the impact of Boko Haram
on farmers, where militarists are killing farmers and forcing them to
flee their homes, exacerbating the food problem. Example articles and
events are shown in Figure 8(Left). What is interesting to the analyst
is that articles are already discussing the famine problems that Africa
will face due to climate change. If this is further exacerbated by wars,
the problem cycle may become more prevalent resulting in displacement, migration, and potential social unrest. From a social science
perspective, our analysts are interested in how to model such phenomena. By cueing them to such events, they are able to begin looking at
how ongoing events could be modeled to predict future problems.
After discussing the events surrounding the food frame cue, the analyst then decides to also explore the ProbThreatHealth frame (problems associated with health) next. The analyst is interested in the
two significant events that occurred between October 11th and October 28th. Again, the analyst begins exploring related ACLED events
during this time period, and quickly finds several riot/protest events
related to the mistreatment of healthcare workers in the region. The
analyst again noted their interest in these articles and the fact that the
event cueing was able to narrow down their search to potentially relevant information. While there are some obvious links between food
security, armed conflicts and riots (for example, Boko Haram displacing farmers), subtle social issues involved with riots may be harder to
spot. Furthermore, given that such riots are taking place at this time
and there is a shift in frames, the analyst hypothesized that this could
represent a shift in the discourse in the hopes to alleviate concerns

LU ET AL.: EXPLORING EVOLVING MEDIA DISCOURSE THROUGH EVENT CUEING

227

Fig. 7: Exploring the whole time period on the RSS news dataset spatially and temporally. The spatial map shows a weighted choropleth map
with all frame class equally weighted. The add-on histogram shows the frame volume and distribution of the Republic of Côte d’Ivoire. The top
timeline view shows the level of four frame classes and two black circles highlight the time period of interest in ProblemThreat and Motivation.
The bottom timeline view shows the expanded timelines in the ProblemThreat frame class and the time period of interest is highlighted.
from the general population. While no definitive conclusions could
be made at this time, this example further illustrates how our framework can enhance the hypothesis generation process. By specifically
cueing an analyst to a time of interest, we can dramatically cut their exploratory analysis time. For example, there are over 40 ACLED events
per day, each with an associated set of documents. Uncued analysis of
such work would be an extremely laborious process.
4.2

Exploring Motivation Frames in Africa

The analyst concentrates on examining press coverage between
November 1st and November 14th, and identifies events accounting
for notable intervention points on November 6th based on the BeforeDuring-After model. Results indicate an increasing trend in the media
discourse on calling for policy actions on November 2nd with a negative tone. The statistically significant interventions and the burst of
the sentiment can be found in the Figure 7(highlighted in circle b).
The changing pattern is predominantly associated with the launch of
an updated synthesis report by the UN’s Intergovernmental Panel on
Climate Change (IPCC) on November 2nd. Several articles reporting
IPCC can be easily found and shown in Figure 9. As the most comprehensive assessment that attracts worldwide attention, the new IPCC
report summarizes alarming evidence detailing severe impacts of climate change. Adverse impacts range from increased risks of extreme
weather events, food shortages, and violent conflicts. The alarming
messages, circulated by several media outlets, were framed in mostly

negative words (e.g. serious impacts, severe impact, dangerous, catastrophic). In addition, analysts find prevalent explicit statements calling
for international governments to take actions now. The following sentences describe examples of motivational framing.
• “Massive cuts to greenhouse gas emissions are needed in the
coming decades to curb temperature rises to no more than 2C, the
level at which it is thought dangerous impact of climate change
will be felt.”
• “Leaders must act.”
• “There is cause for hope if governments take action.”
• “A binding meaning and enforceable framework is needed to
limit the consequences of global warming.”’
• “The world’s largest polluters, the United States and China,
should take the lead in reducing emissions.”
Conversely, there are noticeable spikes of positive sentiment values
between November 9th and November 12th. The pattern is largely
associated with favorable coverage of U.S. and China announcing a
historical climate change agreement on November 11 when President
Obama visited Beijing for the Asia Pacific Economic Cooperation
(APEC) summit. Together, motivation frames in West Africa reflect
a focus of relying on international actors to drive policy negotiation.

228	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016

Fig. 8: The geographical and detail view for exploring RSS news and ACLED data. This figure shows the analyzing time from October 11th to
October 28th. The geographical view color each country by the majority frame class and displays riots (orange dots) to represent the ACLED
events. The detail view lists the Riots events related to health problem within and outside Nigeria. The left side detail view shows examples of
RSS articles discussing food problem in Africa and the ACLED events are riots and battles expressing problem of food supply.
Results of analysis on motivational frames should be viewed in
light of limitations. In the 1,245 relevant articles collected from West
African news media and twitter links, there is little evidence of motivation framing, as less than 10% of a news story contained statements calling for definitive courses of actions, That is, motivational
frames are very uncommon compared to other three frame classes
(cause, problem/threat, and solution). When a set of news stories highlighted explicit calls for actions to solve climate change issues within
the same time period, it is highly possible that the consistent pattern in
press coverage was statistically significantly different than before and
after in the time series analysis. Despite the low presence of motivational statements in the current dataset, the visualization tool allows
researchers, analysts, and policy makers to explore the potential underlying mechanisms linking adverse impacts of climate change and
increased risk of political conflicts.
4.3

Analyst Feedback

Our case study involved two analysts from the Department of Communication at Arizona State University. Feedback on the system was
positive with analysts indicating that the event cueing features were extremely useful in providing a starting point for searching linked data.
Case Study 1 was done as a paired analysis demonstrating the tool
with the computer scientists manipulating the controls and discussing
how the system worked. Case Study 2 was done at the communication
lab with no assistance from the computer science group (the tool is
web-deployed).
Overall feedback was positive with the analysts stating that they
were “fascinated by the visualization tool’s ability to map out temporal and spatial components of media discourse”. In addition, the
analysts also mentioned that this tool can help to tackle co-occurrence
patterns of conflicting events, limiting the possibility of bridging distinct lines of scholarship together–media research, climate change and
conflicts. However, there were suggestions for future work and improvement. Specifically, the analysts were interested in the difference
between the change models and their disagreements. For example, in
Figure 1, there is an intervention marker (black dot) near October 5th
for motivation, but no colored squares from the before-during-after
analysis. The relationship between these two models required more
explanation and future work will explore creating a single ensemble
metric. Along with the intervention model, the analysts also requested
the ability to reconfigure layouts for improved storytelling. They indicated that they would be able to better explore relationships with a
series of small multiples and better alignment between the temporal
components of the unrest data and the framing data.

Fig. 9: Example RSS articles and the entity wordle for the time period of Oct. 28th to Nov. 11th exploring motivation frame. The left
side article summaries show examples of news report relating to the
IPCC and the right side wordle emphasizes the most frequent entities
appearing in those articles, such as IPCC, Obama, and China.

F UTURE W ORK

enables users to explore more complex hypotheses that can enable
analysts to link potential cues between disparately collected sources.
While several visual analytics methods [11, 12, 13] have explored
frames in the context of comparing corpora of text and topical terms
within these text, our framework enables sentiment analysis and intervention modeling which can provide different insights than previous
work.
Our framework was evaluated through collaboration with domain
experts from the School of Communication and findings from their
exploration have prompted new questions and directions to explore.
While our examples focused on climate change and conflicts in Africa,
the tools developed are applicable for a variety of media sources. Furthermore, it is important to note that our intervention strategy can be
applied to any temporal variable, and, by utilizing multiple models,
we are able to strengthen the analysts’ confidence in the findings. This
was particularly evident in the exploration process. Anomaly detection methods, intervention models and others often have a large false
positive rate. By using an ensemble of models, one can begin defining uncertainty. Future work will focus on a combination of anomaly
models and intervention models as well as a weighted output for defining uncertainty in the detection, similar to our sentiment modeling approach. We also plan to explore a combination of sentiment analysis, frames and clustering for defining geo-political regions that share
common framing strategies. We believe that such methods can further
enable multisource data exploration and provide new cues to analysts
who are developing hypotheses and exploring the evolution of topics,
events and discourse both locally and globally.

In this paper, we have demonstrated a framework for event cueing that
enables the exploration of evolving media discourse. Our framework
focuses on both the spatial and temporal distribution of frames, and
allows experts to quickly explore spatial trends in the underlying discourse. By linking multisource data for exploration, our framework

ACKNOWLEDGEMENT
Some of the material presented here was sponsored by Department of
Defense and is approved for public release, case number 15-365 and
upon work supported by the NSF under Grant No. 1350573.

5

C ONCLUSIONS

AND

LU ET AL.: EXPLORING EVOLVING MEDIA DISCOURSE THROUGH EVENT CUEING

R EFERENCES
[1] Armed conflict location & event data project. http://http://www.
acleddata.com/. Accessed: 2015-03-28.
[2] Data science toolkit. http://www.datasciencetoolkit.org.
Accessed: 2015-03-18.
[3] W. Aigner, S. Miksch, H. Schumann, and C. Tominski. Visualization of
time-oriented data. Springer Science & Business Media, 2011.
[4] R. D. Benford and D. A. Snow. Framing processes and social movements:
An overview and assessment. Annual Review of Sociology, pages 611–
639, 2000.
[5] M. Bogl, W. Aigner, P. Filzmoser, T. Lammarsch, S. Miksch, and A. Rind.
Visual analytics for model selection in time series analysis. IEEE Transactions on Visualization and Computer Graphics, 19(12):2237–2246,
2013.
[6] H. Bosch, D. Thom, M. Worner, S. Koch, E. Puttmann, D. Jackle, and
T. Ertl. Scatterblogs: Geo-spatial document analysis. In Proceedings of
IEEE Conference on Visual Analytics Science and Technology (VAST),
pages 309–310. IEEE, 2011.
[7] J. Chae, D. Thom, H. Bosch, Y. Jang, R. Maciejewski, D. S. Ebert, and
T. Ertl. Spatiotemporal social media analytics for abnormal event detection and examination using seasonal-trend decomposition. In Proceedings of IEEE Conference on Visual Analytics Science and Technology
(VAST), pages 143–152. IEEE, 2012.
[8] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey.
ACM Computing Surveys (CSUR), 41(3):15, 2009.
[9] D. Chong and J. N. Druckman. Framing theory. Annual Review of Political Science, 10:103–126, 2007.
[10] N. F. da Silva, E. R. Hruschka, and E. R. Hruschka. Tweet sentiment
analysis with classifier ensembles. Decision Support Systems, 66:170–
179, 2014.
[11] N. Diakopoulos, D. Elgesem, A. Salway, A. Zhang, and K. Hofland.
Compare clouds: Visualizing text corpora to compare media frames. In
Proceedings of IUI Workshop on Visual Text Analytics, 2015.
[12] N. Diakopoulos, A. Zhang, D. Elgesem, and A. Salway. Identifying and
analyzing moral evaluation frames in climate change blog discourse. In
Proceedings of International Conference on Weblogs and Social Media
(ICWSM), 2014.
[13] N. Diakopoulos, A. X. Zhang, and A. Salway. Visual analytics of media frames in online news and blogs. In Proceedings of IEEE InfoVis
Workshop on Text Visualization, 2013.
[14] W. Dou, X. Wang, D. Skau, W. Ribarsky, and M. X. Zhou. Leadline:
Interactive visual analysis of text data through event identification and
exploration. In IEEE Conference on Visual Analytics Science and Technology (VAST), pages 93–102. IEEE, 2012.
[15] W. Dou, L. Yu, X. Wang, Z. Ma, and W. Ribarsky. Hierarchicaltopics:
Visually exploring large text collections using topic hierarchies. IEEE
Transactions on Visualization and Computer Graphics, 19(12):2002–
2011, Dec 2013.
[16] R. M. Entman. Framing: Toward clarification of a fractured paradigm.
Journal of Communication, 43(4):51–58, 1993.
[17] P. Federico, S. Hoffmann, A. Rind, W. Aigner, and S. Miksch. Qualizon
graphs: Space-efficient time-series visualization with qualitative abstractions. In Proceedings of the 2014 International Working Conference on
Advanced Visual Interfaces, pages 273–280. ACM, 2014.
[18] D. Fisher, A. Hoff, G. Robertson, and M. Hurst. Narratives: A visualization to track narrative events as they develop. In IEEE Symposium on
Visual Analytics Science and Technology, pages 115–122. IEEE, 2008.
[19] T. Gao, J. R. Hullman, E. Adar, B. Hecht, and N. Diakopoulos.
Newsviews: An automated pipeline for creating custom geovisualizations
for news. In Proceedings of the 32nd annual ACM conference on Human
Factors in Computing Systems, pages 3005–3014. ACM, 2014.
[20] D. Gotz and H. Stavropoulos. Decisionflow: Visual analytics for highdimensional temporal event sequence data. IEEE Transactions on Visualization and Computer Graphics, 20(12):1783–1792, 2014.
[21] J. Hullman, N. Diakopoulos, and E. Adar. Contextifier: Automatic generation of annotated stock visualizations. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, CHI ’13, pages
2707–2716, New York, NY, USA, 2013. ACM.
[22] S. Ko, S. Afzal, S. Walton, Y. Yang, J. Chae, A. Malik, Y. Jang, M. Chen,
and D. Ebert. Analyzing high-dimensional multivariate network links
with integrated anomaly detection, highlighting and exploration. Proceedings of IEEE Conference on Visual Analytics Science and Technol-

229

ogy, pages 83–92, 2014.
[23] M. Krstajic, E. Bertini, and D. A. Keim. Cloudlines: Compact display of
event episodes in multiple time-series. IEEE Transactions on Visualization and Computer Graphics, 17(12):2432–2439, 2011.
[24] M. Krstajić, M. Najm-Araghi, F. Mansmann, and D. A. Keim. Story
tracker: Incremental visual text analytics of news story development. Information Visualization, 12(3-4):308–323, 2013.
[25] S. Liu, X. Wang, J. Chen, J. Zhu, and B. Guo. Topicpanorama: A full
picture of relevant topics. In Proceedings of IEEE Conference on Visual
Analytics Science and Technology, pages 183–192. IEEE, 2014.
[26] Y. Lu, X. Hu, F. Wang, S. Kumar, H. Liu, and R. Maciejewski. Visualizing social media sentiment in disaster scenarios. In Proceedings of the
24th international conference on World Wide Web companion. International World Wide Web Conferences Steering Committee, 2015.
[27] D. Luo, J. Yang, M. Krstajic, W. Ribarsky, and D. Keim. Eventriver: Visually exploring text collections with temporal references. IEEE Transactions on Visualization and Computer Graphics, 18(1):93–105, 2012.
[28] A. Malik, R. Maciejewski, N. Elmqvist, Y. Jang, D. S. Ebert, and
W. Huang. A correlative analysis process in a visual analytics environment. In Proceedings of IEEE Conference on Visual Analytics Science
and Technology, pages 33–42. IEEE, 2012.
[29] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and
D. McClosky. The Stanford CoreNLP natural language processing
toolkit. In Proceedings of 52nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations, pages 55–60, 2014.
[30] M. Monroe, K. Wongsuphasawat, C. Plaisant, B. Shneiderman, J. Millstein, and S. Gold. Exploring point and interval event patterns: Display
methods and interactive visual query. University of Maryland Technical
Report, 2012.
[31] D. C. Montgomery, C. L. Jennings, and M. Kulahci. Introduction to Time
Series Analysis and Forecasting. Hoboken, NJ: John Wiley & Sons, 2008.
[32] J. Neter, M. H. Kutner, C. J. Nachtsheim, and W. Wasserman. Applied
linear statistical models, 5th edition, volume 4. Irwin Chicago, 1996.
[33] D. Oelke, M. Hao, C. Rohrdantz, D. Keim, U. Dayal, L. Haug, and
H. Janetzko. Visual opinion analysis of customer feedback data. In IEEE
Symposium on Visual Analytics Science and Technology, pages 187–194,
2009.
[34] J. OLoughlin, A. M. Linke, and F. D. Witmer. Effects of temperature and precipitation variability on the risk of violence in sub-saharan
africa, 1980–2012. Proceedings of the National Academy of Sciences,
111(47):16712–16717, 2014.
[35] D. A. Scheufele. Framing as a theory of media effects. Journal of Communication, 49(1):103–122, 1999.
[36] D. Thom, H. Bosch, S. Koch, M. Worner, and T. Ertl. Spatiotemporal anomaly detection through visual analysis of geolocated twitter messages. In Pacific Visualization Symposium, pages 41–48. IEEE, 2012.
[37] J. J. Van Wijk and E. R. Van Selow. Cluster and calendar based visualization of time series data. In IEEE Symposium on Information Visualization,
pages 4–9. IEEE, 1999.
[38] F. Wanner, C. Rohrdantz, F. Mansmann, D. Oelke, and D. A. Keim. Visual sentiment analysis of RSS news feeds featuring the US presidential
election in 2008. In Workshop on Visual Interfaces to the Social and the
Semantic Web, 2009.
[39] F. Wanner, A. Stoffel, D. Jäckle, B. Kwon, A. Weiler, D. Keim, K. E.
Isaacs, A. Giménez, I. Jusufi, T. Gamblin, et al. State-of-the-art report
of visual analysis for event detection in text data streams. In Computer
Graphics Forum, volume 33, 2014.
[40] B. L. Welch. The generalization of ‘student’s’ problem when several
different population variances are involved. Biometrika, pages 28–35,
1947.
[41] K. Wongsuphasawat and D. Gotz. Exploring flow, factors, and outcomes
of temporal event sequences with the outflow visualization. IEEE Transactions on Visualization and Computer Graphics, 18(12):2659–2668,
2012.
[42] K. Wongsuphasawat, J. A. Guerra Gómez, C. Plaisant, T. D. Wang,
M. Taieb-Maimon, and B. Shneiderman. Lifeflow: Visualizing an
overview of event sequences. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, pages 1747–1756. ACM, 2011.
[43] C. Xie, W. Chen, X. Huang, Y. Hu, S. Barlowe, and J. Yang. VAET: A visual analytics approach for e-transactions time-series. IEEE Transactions
on Visualization and Computer Graphics, 20(12):1743–1752, 2014.

Evaluating the Effectiveness of Visualization Techniques for Schematic
Diagrams in Maintenance Tasks
Sungye Kim? and Insoo Woo? and Ross Maciejewski? and David S. Ebert? and Timothy D. Ropp† and Krystal Thomas‡
? Purdue University Rendering and Perceptualization Laboratory
† Aviation Technology, Purdue University
‡ Air Force Research Laboratory

Abstract

technicians often print many diagrams and highlight and annotate
them as they perform their analysis. Many of these diagrams contain multi-page spreads of complex wiring diagrams. As such, tracing paths from page-to-page and in-and-out of components can be
tedious. Furthermore, the number of diagrams needed can be cumbersome and maintaining contextual information when switching
between related diagrams is difficult.

In order to perform daily maintenance and repair tasks in complex
electrical and mechanical systems, technicians commonly utilize a
large number of diagrams and documents detailing system properties in both electronic and print formats. In electronic document
views, users typically are only provided with traditional pan and
zoom features; however, recent advances in information visualization and illustrative rendering styles should allow users to analyze
documents in a more timely and accurate fashion. In this paper,
we evaluate the effectiveness of rendering techniques focusing on
methods of document/diagram highlighting, distortion, and navigation while preserving contextual information between related diagrams. We utilize our previously developed interactive visualization system (SDViz) for technical diagrams for a series of quantitative studies and an in-field evaluation of the system in terms of
usability and usefulness. In the quantitative studies, subjects perform small tasks that are similar to actual maintenance work while
using tools provided by our system. First, the effects of highlighting within a diagram and between multiple diagrams are evaluated.
Second, we analyze the value of preserving highlighting as well as
spatial information when switching between related diagrams, and
then we present the effectiveness of distortion within a diagram. Finally, we discuss a field study of the system and report the results
of our findings.

Meanwhile, over the last decade, as technical documents have
grown in size, manufacturers have begun transferring them from
the printed page to electronic files (e.g., cgm, pdf, and raster image).
Unfortunately, given that many of these manuals consist of multipage fold-out diagrams, scrolling through pdfs and trying to maintain context amongst various components is cumbersome, leading
many technicians to still print documents. As such, the development of effective visualization techniques and systems for technical
diagrams is critical for improving daily maintenance tasks.
Previous work has analyzed how technicians interact with technical diagrams, providing insight into necessary design parameters
needed in the creation of such systems. Of key importance, Barnard
et al. [2006] and Barnard and Reiss [2006] noted that technicians
utilize technical documents in different manners, developing their
own mental models while interacting with systems. Thus, in a visualization system for technical documents, the system design needs
to be based on human perception and cognition in order to enhance the system’s effectiveness and practical relevance. Ware et
al. [2004] mentioned the pivotal role of human perception in the
determination of what visualization techniques (e.g., color, texture,
and moving pattern) should be utilized to maximize the user performance of tasks when interacting with a system. Moreover, there
have been studies [Reuter et al. 1990; Healey 2001] to answer the
question about what makes an effective visualization in terms of
human perception.

CR Categories: I.3.6 [Computer Graphics]: Methodology and
Techniques—Interaction techniques;
Keywords: Effectiveness in interactive diagram visualization,
context preservation, human factors in maintenance work

1

Introduction

In light of these issues, our previous work [Woo et al. 2009]
presented an interactive visualization system for technical diagrams (SDViz). SDViz was designed to help users explore technical diagrams interactively and incorporated many traditional diagram visualization techniques (e.g., highlighting, animation, context views). In this work, we utilize the SDViz system to quantitatively evaluate the effectiveness of a set of visualization techniques
for technical diagrams in the maintenance domain. For this evaluation, we first developed a troubleshooting scenario based on cognitive analysis of a routine tabletop maintenance exercise conducted
by experienced technicians to simulate a real world problem. This
troubleshooting scenario was broken into several key tasks primarily focusing on component identification. A quantitative evaluation
was performed utilizing a specific visualization tool for each task.
Subjects performed these given tasks after training with SDViz and
we recorded their interaction (mouse movements, clicks and task
completion time). Analysis of variance testing was performed on
data collected from each task to determine the significance in terms
of mean time and accuracy of task completion. Additionally, a field
evaluation was performed using senior students in Aviation Technology at Purdue University. They evaluated various tools provided
by our system in terms of usability and usefulness.

Effective visualization, regardless of the domain, is required to provide users with a better understanding of their data [Ware 2004;
Johnson and Hansen 2004]. In particular, visualization modalities
for 2D technical documents, such as schematic and wiring diagrams, need to be carefully designed as they can significantly affect maintenance tasks in terms of troubleshooting time and accuracy. Typical tasks in maintenance work include finding components and tracing circuit paths. Technicians begin with a broken
component and hypothesize which connections may be contributing to the fault. Next, technicians often need to find a component
in a schematic diagram, search through the manual for the related
wiring diagram, and then trace paths through the wiring diagram,
while maintaining contextual information. To facilitate these tasks,
Copyright © 2010 by the Association for Computing Machinery, Inc.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for commercial advantage and that copies bear this notice and the full citation on the
first page. Copyrights for components of this work owned by others than ACM must be
honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on
servers, or to redistribute to lists, requires prior specific permission and/or a fee.
Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail
permissions@acm.org.
APGV 2010, Los Angeles, California, July 23 – 24, 2010.
© 2010 ACM 978-1-4503-0248-7/10/0007 $10.00

33

In this article, we describe the effectiveness of context-preserving
visualization for technical diagrams while analyzing and presenting
the results from our evaluation study. Lessons learned from our
study are summarized as follows:

potential candidates that should be checked, while still maintaining
these annotations during diagram switching.

2.2

• Lesson 1: Highlighting preservation among related diagrams
enhances the contextual information resulting in a significant
performance increase in both task completion time and accuracy.

Distortion viewing is one of the most common methods for focus+context visualization. Carpendale et al. [2004] introduced distortion methods based on a fisheye lens to achieve higher magnification without eliminating any of context. However, distortion
viewing for technical diagram visualization is one of the least preferred techniques by maintenance personnel because it changes the
structural view of components within technical diagrams. Distorted
diagrams can hinder a technician’s ability to seamlessly transition
between printed and visualized diagrams. Despite this limitation,
a few distortion techniques that maintain the structural view of diagrams, such as magic lens and rectangular fisheye lens, can be
effective for the visualization of relatively large diagrams and can
enhance a user’s area of interest within the context of a diagram.

• Lesson 2: Preserving spatial information when switching between diagrams enables users to quickly find the same component in the diagrams while achieving significant improvement
in performance time.
• Lesson 3: Context-preserving visualization shows a significant difference in terms of time and accuracy.
This paper is organized as follows: first, we overview our system
(SDViz) briefly. Then, we introduce our user study design explaining the data we used, the subjects who participated in our evaluation
study, and the overall procedure of our evaluation study. Next, we
examine how highlighting and spatial information as contextual information are able to help users perform tasks when they work with
one or more technical diagrams. Then, we investigate the value of
contextual information preserved during distortion of a diagram and
transition between diagrams. Finally, we present an additional field
study conducted by senior students in the school of Aviation Technology who have experience in simulated and/or real troubleshooting using technical diagrams.

2

In our system, the magic lens [Bier et al. 1993] uses a fixedsize magnifying area occluding neighboring regions, whereas the
rectangular fisheye lens [Rauschenbach 1999; Rauschenbach et al.
2001] contracts the neighboring regions around a user-defined area.
This lens allows large diagrams to be displayed while providing
unoccluded, adjacent, contextual information.

2.3

Navigation

For navigating diagrams, we provide both single diagram navigation and inter-diagram navigation methods for exploring technical diagrams. In single diagram navigation, our system uses
the component-to-component or component-to-connection relationships to navigate within a single diagram. Our system also
allows users to center any of the connected components by selecting the component’s name. The quick identification of special
components, such as circuit breakers, is done through an interactive search feature. In inter-diagram navigation, our system allows
users to navigate to related diagrams, without explicitly searching
for these relationships, while preserving the contextual information
and maintaining user focus while switching between diagrams. Our
system also provides transition by blending two diagrams such that
it shows the part of a related diagram within the view of the current
diagram. While diagrams are blended, the highlighting as well as
spatial context of a focused component are preserved to provide a
user with consistent contextual information.

SDViz: A Context-preserving interactive visualization system for Technical diagrams

In our previous work [Woo et al. 2009], we developed a technical
diagram visualization system, which provides maintenance personnel with tools to explore technical diagrams and aid in diagnosing
faults while traversing multiple diagrams. Moreover, contextual information such as highlighting, working status, switch status, and
spatial information is preserved between related diagrams. In addition, our system provides various visualization techniques, such as
component searching within and across diagrams, connection tracing, distortion viewing, and flow animation. In this section, we
briefly explain the main functionalities of SDViz used for our evaluation study. For further details, please see [Woo et al. 2009].

2.1

Distortion Viewing

Highlighting

3

Highlighting immediately draws users’ attention by using visual effects resulting in emphasizing the information relevant to the users’
task. In our system, highlighting using color and marks is preserved
in all related diagrams as contextual information. We highlight a focused component, neighbor components directly connected to the
focused component, and connections between highlighted components in various colors indicating the status of a component. For
all highlighting, the relationship between components is generated
from the connection information. The highlighting of each component is blended with that of a parent component. Unlike components, we interpolate colors of the end points of each connection to
present a directional cue, such as a source or a sink. Our system
also uses special marks to denote whether a switch is on/off and
whether a component is working/not-working in order to provide
a symbolic abstraction when performing maintenance tasks across
multiple diagrams. One mark presents the working status of a component together with highlighting by color. The other shows the
on/off status of a switch component. The purpose of this highlighting is to provide tools to annotate users’ analysis, as well as to show

Evaluation Study Design

Using the previously described SDViz system, the goal of this work
is to evaluate the effectiveness of various visualization techniques
in terms of their ability to enhance user tasks in searching and analyzing technical diagrams. In this section, we describe the design of our experiments in order to evaluate the effectiveness of our
context-preserving methods for technical diagrams using SDViz.

3.1

Background

There has been much research (e.g., [Laskowski and Plaisant 2005;
Scholtz 2006; Plaisant et al. 2008a; Plaisant et al. 2008b; Greenberg
and Buxton 2008]) into the proper methods of evaluating the efficiency, effectiveness and user satisfaction of visualization systems.
Ferwerda et al. [2002] suggested that visualization can be evaluated based on the following criteria: i) the degree of simplicity that
the visualization can provide to the users to understand and complete their tasks; ii) whether the visualization follows simple rules

34

of perception and design; iii) how realistic the visualization resembles the real-world objects; and iv) whether the visualization allows
users to uncover something new in a dataset. Lakowski and Plaissant [2005] split the evaluation of visualization systems into three
levels: the component, system, and work levels. The evaluation
of the effectiveness of visualization is difficult due to the complex
multi-disciplinary nature of visualization. However, recent discussion in the HCI community [Greenberg and Buxton 2008] suggests
the removal of benchmark tasks in favor of qualitative insight tasks.
Moreover, controlled experiments on benchmark tasks have fundamental problems as described in North [2006]. One of the major problems is that the benchmark questions must have definitive
answers, leaving little room for deep insight, and answer correctness is treated as Boolean leaving little room for qualitative insight.
Based on previous research into visualization system analysis, our
work presents a set of quantitative tasks and a field study scenario.

3.2

Along with our quantitative benchmark tasks, we also worked with
faculty within Purdue University’s School of Aviation Technology
to develop an advanced troubleshooting scenario in which maintenance students would utilize conventional manuals alongside our
SDViz tool in a field study for evaluation. Our study here is based
upon the concept of using expert reviews (e.g., [Gabbard et al. 1999;
Jackson et al. 2003; Tory and Möller 2005]) for system evaluation.

3.3

Data

We used schematic and wiring diagrams for the Window Heat System from a Boeing 737 aircraft in our evaluation study. For training within our system we used the Warning Electronics Unit Power
Supply diagrams from the Boeing 757 aircraft, which have similar
complexity to that of the diagrams used in the actual evaluation in
terms of the structure of components and links.

Task Design
3.4

For our empirical user study, the effectiveness is defined as the measure of a technique’s ability to enhance the user’s understanding of
technical data and to reduce task completion time by maintaining
and enhancing contextual information between related schematic
and wiring diagrams. At the work environment level, evaluation
includes issues on how our system influences productivity, encourages collaborations amongst users, and promotes excitement in data
exploration. Scholtz et al. [2006] presented evaluation aspects that
should be considered as metrics and methodologies to assess visual
analytics environment developed. Plaisant et al. [2008a] introduced
the results from information visualization contests for 2003 through
2005, where a variety of visualization methods were used to analyze a given task, in order to propose methodologies to evaluate the
effectiveness of various visualization techniques. Greenberg and
Buxton [2008] described the importance of the choice of appropriate evaluation methodology in terms of usability evaluation while
mentioning how harmful if it is applied incorrectly.

Subjects

Fifty nine subjects (fifty two males and seven females) participated
in our user study. Among them, forty nine subjects who participated
in our quantitative studies were familiar with using wiring diagrams
from introductory circuit courses taken previously, but they had no
experience using schematic or wiring diagrams of aircraft. These
subjects were used in the quantitative task analysis. The others (ten
males) who participated in a field study were senior students in the
school of Aviation Technology at Purdue University. All of them
were experienced in the simulated troubleshooting of aircrafts using schematic and wiring diagrams. All subjects are classified into
separate groups for each task (except for Task 2 in the quantitative
study which had four groups): Group 1 (experimental group) and
2 (control group). For our field study, the students in Group 1 (experimental group) used the SDViz tool, and the other in Group 2
(control group) utilized only a PDF viewer.

As our system is designed for users who already have technical diagram analysis skills, we limit our subject pool to only users who
had taken undergraduate level circuit courses for the quantitative
study, and users within Purdue University’s Aviation Technology
courses for the field study. Finding a component in a diagram, as
well as identifying the relationship of components between related
diagrams, is fundamental to mechanical training and maintenance
tasks. We ask subjects to find a specific component in a schematic
or wiring diagram, and they perform repeated trials for each task.
Task-oriented performance metrics such as the speed, accuracy and
completeness with which users perform assigned tasks are some
of the adopted measurements. Psychophysical measures, such as
speed, accuracy and preference, are recorded to allow investigations
on perceptual importance and meaning of these input conditions.
Quantitative measurements in our experiments include both user’s
response time and the number of operations (e.g., mouse clicks).
Analysis of variance is used to test whether differences between
conditions are significant. Task designs were developed using information from the cognitive design principles identified by Heiser
et al. [2003; 2004] and Agrawala et al. [2003].

3.5

Procedure

Prior to the actual experiment, subjects completed a training session to familiarize themselves with the system’s functionality. Subjects who did not possess rudimentary efficiency in using schematics could not participate in the full experiment. During the training, subjects asked questions and received guidance in the use of
the system. Once the training was completed, subjects were given
a pre-test to determine their ability to use the SDViz system. Subjects who did not pass the pre-test had time for re-training until they
were satisfied with their training and were given the pre-test again.
No subjects failed after the re-training exercise was completed.
Subjects who passed the pre-test performed the actual experiments
through repeated trials and tasks randomized using a magic square
method [Farrar 1996]. Time measurements were also recorded for a
quantitative metric in this evaluation, and data collected from each
task was subjected to an analysis of variance testing to determine if
the mean time and accuracy of task completion were significantly
different between the groups. Finally, a series of exit questions were
given to participants using the SDViz tool as post-questionnaires for
feedback.

Our quantitative evaluation study is comprised of three task simulations in order to evaluate the effectiveness of the following visualization methods:

4

Case Studies

Task 1 Highlighting components and links in schematic diagrams.
In Tasks 1 and 2, we evaluated how highlighting and spatial information for technical diagrams can help users find information. In
Task 3, we evaluated distortion methods for technical diagram navigation. Each task includes several repeated subtask trials.

Task 2 Maintaining contextual information through highlighting
and spatial information.
Task 3 Distortion viewing providing contextual information.

35

Figure 1: Task 1: finding a power source connected to a specific component (with a label highlighted in red) for Group 1 (left) and 2 (right).
Subjects in Group 1 see highlighted components and links connected from the specific component with an initial highlighted label. As a result,
they easily perceive that the leftmost component on the link is a power source. Subjects in Group 2 need to read a diagram or follow links to
find a power source connected to an initial highlighted component.

4.1

Task 1

Task 1 focused on the effectiveness of highlighting within a diagram. The scenario and groups of subjects were as follows.
Scenario: This task begins with a circuit component highlighted
and requires subjects to locate the 115V AC power source connected to the component initially highlighted. When the subjects
find the power source, they need to double click the power source
component indicating that the subtask trial is completed. Subjects
perform three trials in which the power source is located in different
portions of the diagram. The time to find the component, as well
as the number of mouse clicks for each trial are recorded and used
for analysis. Our hypothesis for Task 1 is that highlighting will improve a user’s ability to find a component within a diagram. Note
that all interconnected components are highlighted for the subjects
and they are still then required to trace the path and determine which
component is the power source. Our highlighting method does not
simply provide users with the answer. See Figure 1 (left) to see the
highlighted example and Figure 1 (right) to see the control example.

Figure 2: Task 1 Completion time (left) and accuracy (right). Error
bars represent standard error on each side of the mean.

at identifying components.
Scenario: This task starts with a circuit component highlighted in
the schematic diagram and asks subjects to open the related wiring
diagram to find a specified pin connection. When the subjects find
the pin, they need to double click it denoting that this trial is completed. During three trials, timing and subjects’ mouse clicks are
recorded and used for analysis.

Groups: Subjects in Group 1 (experimental group) see the path
to power sources through the connected components highlighted
automatically, while subjects in Group 2 (control group) physically
trace the connections from the component to the power sources.
Figure 1 shows screen shots for each group in Task 1.

Groups: For Task 2, we classify subjects into Groups 1, 2, 3 and
4 because we are evaluating the combination of two techniques
(i.e., preservation of highlighting and spatial information) simultaneously. Subjects in Group 1 use highlighting preserved between
related diagrams, subjects in Group 2 use highlighting and spatial
information, subjects in Group 3 use no contextual information, and
subjects in Group 4 use spatial information maintained. Figure 3
shows screen shots for each group in Task 2.

Results and Discussion: Figure 2 shows the results for Task 1.
We find that subjects using the highlighting technique were significantly faster (p-value = 0.0118) and more accurate (p-value =
0.0335) than subjects who did not. The mean time for subjects
using our highlighting techniques was 41.0 seconds and the mean
time for subjects who had no highlighting options available was
78.8 seconds. In terms of accuracy, only one subject in Group 1
had a wrong answer for one trial amongst the three subtask trials
(96.6% mean accuracy), whereas five subjects in Group 2 answered
incorrectly for two subtask trials (81.4% mean accuracy).

4.2

Results and Discussion: The results for Task 2 are shown in Figure 4 in terms of mean time and accuracy to complete subtasks respectively. Figure 4 shows that there is significant difference among
the four groups in terms of speed of task completion. The ANOVA
tables show p-value = 0.0448 and p-value = 0.2611 for time and
accuracy respectively.

Task 2

Here, we can see that Groups 3 and 4 are significantly slower (mean
time = 54.3 seconds) at task completion than Group 1 and 2 (mean
time = 23.3 seconds). Further, the statistical significance between
Group 1 and 2, and Groups 3 and 4 is near the 5% level. This indi-

Task 2 evaluated the value of using highlighting to maintain spatial
information between related diagrams. Our hypothesis is that by
maintaining both highlighting and spatial information when navigating between diagrams, subjects will be faster and more accurate

36

Figure 3: Task 2: finding a pin number on a specific wire for Group 1 (top-left), 2 (top-right), 3 (bottom-left) and 4 (bottom-right). Images
show initial views of a wiring diagram after moving from a schematic diagram for each group. In each image, a dotted red box indicates
the position of a circuit component initially highlighted in a schematic diagram. Hence, two images in the right column provide spatial
information preserved. While finding an answer, Group 1 uses a highlighted component in a wiring diagram as contextual information
maintained, Group 2 uses highlighting as well as position maintained between schematic and wiring diagrams, subjects in Group 3 do not
have any contextual information when they move to wiring diagram, and finally Group 4 uses only spatially maintained position information.

cates that the use of highlighting actually plays the primary role in
guiding a user’s focus, and the enhanced spatial relationships built
into the diagram transition appears to go unnoticed by the users
when a target component (in this case, a component with a specific pin connection) is displayed within the current view. As such,
we can infer that spatial cues may be very dependent on whether
the information that users want to find is displayed within an initial
view when users switch from a schematic diagram to related wiring
diagram and vice versa. From Figure 4, we see there is no difference between Group 1 (highlighting) and 2 (highlighting + spatial information) because subjects in Group 2 preferred to use the
highlighting before perceiving maintained spatial information. For
Group 3 (nothing) and 4 (spatial information), subjects in Group 4
preferred to search the entire diagram using mouse interactions and
often failed to notice the spatial cue. It is possible that the resolution of the diagrams used for Task 2 was too small to determine the
usefulness of maintained spatial information between related diagrams. To summarize, we conclude that when navigating between
related diagrams, highlighting related components provides users
with a significant cue.

4.3

Figure 4: Task 2 Completion time (left) and accuracy (right). Error
bars represent standard error on each side of the mean.

the magic lens method.
Scenario: Task 3 begins with a large wiring diagram opened and
zoomed out such that subjects cannot read the words on the diagram. The component of interest is highlighted. Subjects are
asked to find a specific pin number connected to the highlighted
component in the given diagram without using a zoom-in method.
Subjects use either a rectangular fisheye lens or a magic lens view.
When subjects find the pin, they double click it indicating that a
trial is completed. During three trials, timing and subjects’ mouse
clicks are recorded and used for analysis.

Task 3

Groups: Subjects in Group 1 (experimental group) begin the task
with a wiring diagram and are asked to use rectangular fisheye lens
in the diagram to find the desired information. The rectangular

Task 3 focused on evaluating the effects of distortion within technical diagrams when using the rectangular fisheye lens compared to

37

Figure 5: Task 3: finding a pin by using the rectangular fisheye lens (left) and the magic lens (right). Subjects in Group 1 using fisheye lens
(red rectangle in the left image) see contents magnified within the lens without occluding adjacent regions while maintaining overall context
of a diagram, whereas subjects in Group 2 using magic lens (red rectangle in the right image) lose context occluded by the lens.

fisheye lens provides subjects with a magnified area of interest together with unoccluded neighbor context. While, subjects in Group
2 (control group) use the magic lens with occlusion of adjacent areas. Figure 5 shows screen shots for each group in Task 3.
Results and Discussion: The results for Task 3 show that the performance between a rectangular fisheye lens and a magic lens is
not significantly different in terms of mean time to complete subtasks (p-value = 0.3166). However, there is significance in terms
of accuracy (p-value = 0.013). Figure 6 shows that mean time and
accuracy for two groups. Two subjects in Group 1 answered wrong
for one or two trials amongst three trials resulting in 80% mean
accuracy, whereas all subjects in Group 2 answered incorrectly for
one to three subtask trials resulting in 20% mean accuracy.

Figure 6: Task 3 Completion time (left) and accuracy (right). Error
bars represent standard error on each side of the mean.

Our hypothesis for Task 3 is that distortion viewing of a large diagram on a small screen (such that the entire diagram is displayed
but only a region of interest is viewed in a readable resolution) helps
users navigate the diagram. In particular, unlike a magic lens, a
fisheye lens enables users to see more contextual information while
preserving the views of adjacent regions. The results from this
quantitative study show that our method using the fisheye lens is
significantly different in terms of task correctness. Accordingly,
we conclude the following from our analysis: To find a component
in a diagram, distortion viewing preserving adjacent regions such
as rectangular fisheye lens can provide significant cues and enable
more accurate component identification.

5

Groups: Subjects were classified into two groups; Group 1 (7 subjects) used the full functionalities of our system to perform the diagnosis after watching tutorial videos as well as self-training with the
use of our system, whereas Group 2 (3 subjects) used a PDF viewer
to see the diagrams related to the diagnosis. After completion of the
task, subjects in Group 1 were given a post-questionnaire to collect
feedback on the usability (i.e., the characteristic of being easy to
use) and usefulness (i.e., the extent to which the software actually
helps to solve user’s real, practical problems) of each functionality
of our system.
Usability and usefulness were measured using a Likert scale [Likert
1932]. Usability was ranked from “Very easy” to “Difficult”, and
usefulness was ranked from “Very useful” to “Not useful”. Figure 7
shows the results. The horizontal axis lists each function of our
system, and the vertical axis represents the number of subjects who
answered each question in a post-questionnaire.

Field Evaluation

Additionally, a field study was conducted with 10 senior students
from the school of Aviation Technology at Purdue University to
evaluate the value of our visualization techniques in diagnosing a
problem with a Boeing 737 aircraft. We reused the scenario performed in the tabletop exercise. No students in the field study participated in the tabletop exercise phase, nor had they previously performed this task in their class.

Usability: Figure 7 (left) shows the usability result for key functions of our system. Over 5 out of 7 subjects who used our system
for the troubleshooting answered that our tools were “Very easy”
or “Easy” to use (bluish bars in the graphs of Figure 7). Positively,
the component highlighting, working status, magic lens, and flow
animation were seen as “Very easy” or “Easy” to use for all subjects. For the transition by blending, two subjects answered it was
“Not easy” and another two subjects answered “No opinion” because they were not familiar with such an interface in their field
(Aviation Technology). However, they also agreed that it was certainly efficient to keep their focus by preserving contextual infor-

Scenario: This study supposes the malfunctioning of a window
heater in a Boeing 737 aircraft. Voltage is not indicated when
the left side window control switch has been turned on. The subjects were asked to troubleshoot this problem using relevant maintenance, schematic, and wiring diagram manuals. When measurements were needed, they were provided with appropriate guidance.

38

Figure 7: Usability (left) and usefulness (right) for each function as evaluated 7 aviation technology students.

mation in their feedback.

Table 1: Summary of the results for our quantitative study
Task
Time
Accuracy
1
significant (p = 0.0118)
significant (p = 0.0335)
2
significant (p = 0.0448)
not significant
3
not significant
significant (p-value = 0.013)

Usefulness: Figure 7 (right) shows the usefulness result for each
feature of our system. All subjects who used our system for the
troubleshooting agreed that the highlighting of related features was
very useful (i.e., highlighting component and highlighting links),
ranking the tools as “Very useful” or “Useful”. Additionally, 6 out
of 7 subjects answered “Very useful” or “Useful” for the working
status and maintaining the highlighting between related diagrams.
Furthermore, some of them proposed that it would be more effective
to highlight more than one component or one link at a time as well
as to turn it off. As we mentioned in Section 2.2, using the magic
lens and the fisheye lens were not preferred by subjects as shown in
Figure 7 (reddish bars in a graph).

Table 2: Summary of the results for our field study
Tool
Usability Usefulness
Highlighting
100%
100%
Switch on/off
85%
71%
Working status
100%
85%
Magic lens
100%
71%
Fisheye lens
85%
57%
Finding power sources
71%
71%
Moving to the related diagram
71%
100%
Transition by blending
42%
57%
Flow animation
100%
100%
Maintaining the highlighting
85%
Maintaining the spatial information
71%

Although the purpose of the field evaluation was to collect feedback, we also observed the troubleshooting time to complete the
scenario for both groups. Subjects in Group 1 using our system
took 9∼15 minutes to troubleshoot, whereas subjects in Group 2
who used a pdf viewer took 17∼32 minutes to troubleshoot the
component, spending most of their time in finding related diagrams
between various manuals. Our system contributed to decreasing the
task time by 46∼52%.

6

General Results and Conclusion

clude that maintaining such spatial cues will play an important role
in cases when users need to switch diagrams and then scroll and
pan a large diagram in order to search for components they want to
find. For the distortion viewing (Task 3), we find that it is significant for both quantitative analysis and field study questionnaires. In
this case, distortion viewing provides both a readable resolution for
the area of interest and an overview for adjacent regions. Table 1
summarizes the results of our quantitative study.

In this work, to evaluate the effectiveness of our visualization techniques for technical diagrams, we applied various visualization
techniques (e.g., highlighting, spatial information by locating of
a 2D viewpoint, and distortion viewing) while preserving contextual information for navigation of technical diagrams. Our evaluation consisted of the quantitative and field studies containing several scenarios, which can be occurred in real troubleshooting procedures. Further, it was focusing on assessing how the techniques
help technicians understand the diagrams as well as find the desired
information from them effectively and economically.

The results from the field evaluation performed by aviation technology students showed that the functionalities of our system are
very useful during the troubleshooting phase of aircraft maintenance. Tool features were easy for subjects to use after watching
a short tutorial video (10 minutes) and practicing with the system
(about 15 minutes), and subjects answered that our tools were very
helpful for maintenance work and asked us to provide our system as
a training tool for their class. Table 2 summarizes the results of our
field study. Additional tasks were also conducted to evaluate the
effectiveness of other techniques from SDViz as shown in Table 2.

Based on the quantitative data recorded during the evaluation studies by 49 subjects, their feedback and additional 10 aviation technology students’ field study (total 59 subjects), we analyzed our visualization techniques’ effectiveness. From both quantitative analysis and field study questionnaires, we believe that highlighting components and wire connections within a diagram (Task 1) provides
users with significant cues aiding in information retrieval. Maintaining contextual information such as highlighting (Task 2) between related diagrams shows a considerable difference although
it is not significant. For maintaining spatial information such as the
location on the screen of a selected component, we do not see a
significant difference in the analysis results. However, we can con-

Based on the analysis from the evaluation study and field study,
highlighting components and connections and preserving them between related technical diagrams are regarded as the most important
techniques among the methods we evaluated. Distortion viewing of
the diagrams was seen as less useful for users to explore techni-

39

cal diagrams and was less preferred by the maintenance personnel,
as noted during the knowledge acquisition phase and final evaluation. In conclusion, we believe that various visualization techniques
based on contextual preservation can help maintenance personnel
progress their tasks in which technical data such as schematic and
wiring diagrams are dealt with. Furthermore, we are also convinced
that such techniques will significantly affect the performance of
users in terms of either time or accuracy.

7

H EISER , J., P HAN , D., AGRAWALA , M., T VERSKY, B., AND
H ANRAHAN , P. 2004. Identification and validation of cognitive design principles for automated generation of assembly instructions. In AVI ’04: Proceedings of the working conference
on Advanced visual interfaces, ACM, 311–319.
JACKSON , C. D., ACEVEDO , D., L AIDLAW, D. H., D RURY, F.,
VOTE , E., AND K EEFE , D. 2003. Designer-critiqued comparison of 2d vector visualization methods: a pilot study. In SIGGRAPH ’03: ACM SIGGRAPH 2003 Sketches & Applications,
ACM, New York, NY, USA, 1–1.

Acknowledgements

J OHNSON , C., AND H ANSEN , C. 2004. Visualization Handbook.
Academic Press, Inc., Orlando, FL, USA.

This work has been funded by the U.S. Air Force Research Laboratory Human Effectiveness Directorate under Contract FA8560-04D-6546 and by the U.S. Department of Homeland Security’s VACCINE Center under Award Number 2009-ST-061-CI0001.

L ASKOWSKI , S., AND P LAISANT, C. 2005. Evaluation methodologies for visual analytics. In Illuminating the Path: The
Research and Development Agenda for Visual Analytics, J. J.
Thomas and K. A. Cook, Eds. IEEE Computer Society, Los
Alamitos, CA, 150–157.

References
AGRAWALA , M., P HAN , D., H EISER , J., H AYMAKER , J.,
K LINGNER , J., H ANRAHAN , P., AND T VERSKY, B. 2003.
Designing effective step-by-step assembly instructions. ACM
Trans. Graph. 22, 3, 828–837.

L IKERT, R. 1932. A technique for the measurement of attitudes.
Archives of Psychology 22, 140, 1–55.
N ORTH , C. 2006. Toward measuring insight. IEEE Computer
Graphics and Applications 26, 3, 6–9.

BARNARD , Y., AND R EISS , M. 2006. User-centred innovation of
electronic documentation for maintenance. In Developments in
Human Factors in Transportation, Design and Evaluation, 129–
142.

P LAISANT, C., F EKETE , J. D., AND G RINSTEIN , G. 2008. Promoting insight-based evaluation of visualizations: From contest
to benchmark repository. IEEE Transactions on Visualization
and Computer Graphics 14, 1 (January), 120–134.

BARNARD , Y., R EISS , M., AND M AZOYER , P. 2006. Mental
models of users of aircraft maintenance documentation. In Proceedings of the International Conference on Human-Computer
Interaction in Aeronautics, 232–239.

P LAISANT, C., G RINSTEIN , G., S CHOLTZ , J., W HITING , M.,
O’C ONNELL , T., L ASKOWSKI , S., C HIEN , L., TAT, A.,
W RIGHT, W., G ÖRG , C., L IU , Z., PAREKH , N., S INGHAL ,
K., AND S TASKO , J. 2008. Evaluating visual analytics at the
2007 vast symposium contest. IEEE Computer Graphics and
Applications 28, 2 (March), 12–21.

B IER , E. A., S TONE , M. C., P IER , K., B UXTON , W., AND
D E ROSE , T. D. 1993. Toolglass and magic lenses: the seethrough interface. In SIGGRAPH ’93: Proceedings of the 20th
annual conference on Computer graphics and interactive techniques, ACM, New York, NY, USA, 73–80.

R AUSCHENBACH , U., J ESCHKE , S., AND S CHUMANN , H. 2001.
General rectangular fisheye views for 2d graphics. Elsevier Science, vol. 24, 609–617.

C ARPENDALE , S., L IGH , J., AND PATTISON , E. 2004. Achieving
higher magnification in context. In UIST ’04: Proceedings of
the 17th annual ACM symposium on User interface software and
technology, 71–80.

R AUSCHENBACH , U. 1999. The rectangular fish eye view as an
efficient method for the transmission and display of large images.
In ICIP ’99: Proceedings of IEEE International Conference on
Image Processing, 115–119.

FARRAR , M. S. 1996. Magic Squares. BookSurge Publishing.

R EUTER , L. H., T UKEY, P., M ALONEY, L. T., PANI , J. R., AND
S MITH , S. 1990. Human perception and visualization. In
VIS ’90: Proceedings of the 1st conference on Visualization ’90,
401–406.

F ERWERDA , J., RUSHMEIER , H., AND WATSON , B., 2002. Visualization 2002 tutorial: Pschychometrics 101: How to design,
conduct, and analyze perceptual studies of computer graphics visualization techniques.

S CHOLTZ , J. 2006. Beyond usability: Evaluation aspects of visual
analytic environments. In IEEE Symposium onf Visual Analytics
Science And Technology, 145–150.

G ABBARD , J. L., H IX , D., AND S WAN , J. E. 1999. User-centered
design and evaluation of virtual environments. IEEE Computer
Graphics and Applications 19, 6, 51–59.

T ORY, M., AND M ÖLLER , T. 2005. Evaluating visualizations: Do
expert reviews work? IEEE Computer Graphics and Applications 25, 5, 8–11.

G REENBERG , S., AND B UXTON , B. 2008. Usability evaluation
considered harmful (some of the time). In Proceedings of the
twenty-sixth annual SIGCHI conference on Human factors in
computing systems, ACM, 111–120.

WARE , C. 2004. Information Visualization: Perception for Design,
vol. 2nd edition. Morgan Kaufmann.

H EALEY, C. G. 2001. Combining Perception and Impressionist
Techniques for Nonphotorealistic Visualization of Multidimensional Data. In Non-Photorelistic Rendering in Scientific Visualization, C. G. Healey, Ed., SIGGRAPH 2001 Course Notes.

W OO , I., K IM , S., M ACIEJEWSKI , R., E BERT, D. S., ROPP,
T. D., AND T HOMAS , K. 2009. Sdviz: A context-preserving interactive visualization system for technical diagrams. Computer
Graphics Forum 28, 943–950.

H EISER , J., T VERSKY, B., AGRAWALA , M., AND H ANRAHAN ,
P. 2003. Cognitive design principles for visualizations: Revealing and instantiating. In CogSci ’03: Proceedings of 25th annual
meeting of the cognitive science society, 545–550.

40

Understanding Twitter Data with TweetXplorer
Fred Morstatter, Shamanth Kumar, Huan Liu, Ross Maciejewski
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University

{fred.morstatter,shamanth.kumar,huan.liu,ross.maciejewski}@asu.edu

ABSTRACT

media data provides rich and expansive information about
the pulse of populations worldwide. However, in order to
obtain relevant information, one must know exactly what to
search for. More often than not, we do not know exactly
what we are searching for, but we know relevant search results when we see them. Using Twitter as an example2 , we
present an effective approach to understanding big data and
illustrate how one can start with a vague idea and iteratively
dive into large volumes of social media data to find stories
of interest. Our system connects human intelligence with
rich data so that human clues can inform search and guide
a user’s query to form better conclusions about his data.

In the era of big data it is increasingly difficult for an analyst
to extract meaningful knowledge from a sea of information.
We present TweetXplorer, a system for analysts with little
information about an event to gain knowledge through the
use of effective visualization techniques. Using tweets collected during Hurricane Sandy as an example, we will lead
the reader through a workflow that exhibits the functionality
of the system.

Categories and Subject Descriptors
H.2.8 [Database Applications]: Data Mining
; H.3.3 [Information Search and Retrieval]: Information
Filtering

2.

Keywords
Twitter Visualization, Retweet Network, Geospatial Analysis, Big Data

1.

EXPLORING TWITTER DATA

To tackle the challenges of big data, organizations have
evolved disciplined processes that guide the process of planning, collecting, analyzing, and curating their data. One
model is the “Data Lifecycle” [9, p. 78], which includes the
following phases: (1) “Plan & Prepare”, (2) “Collect & Process”, (3) “Analyze & Summarize”, (4) “Represent & Communicate”, and (5) “Implement & Manage”. We follow these
steps in the treatment of our social media data, outlining the
decision making process at each step of the lifecycle. Support for the first two phases is provided by a companion system, TweetTracker [6]. TweetTracker collects and processes
tweets matching the following parameters: hashtags, keywords, geographic boundary boxes, and Twitter usernames,
chosen to track events on Twitter. Collected tweets are used
by TweetXplorer. To help researchers deal with large volumes of social media data, we introduce TweetXplorer, as
shown in Figure 1, a system that takes the user through the
last three phases of the data lifecycle.

INTRODUCTION

The term “big data” describes data of a magnitude so large
that it requires a change in methodology in order to process.
Big data is often described by three “V”s: increased rate
of data flow (velocity), heterogeneous types of media and
perspective (variety), and enormous capacity (volume) [2].
These characteristics may lead one to believe that more data
is always better. However, big data presents new challenges
to existing data investigation systems and techniques. As
data accumulates, it becomes harder to separate the wheat
from the chaff. Our system addresses this “data paradox”.
Social media data is undoubtedly big data. In June of
2012, Twitter alone reported 340 million tweets per day from
170 million active users1 . In this sea of data we can find
tweets containing eyewitness discussion of events that garnered worldwide attention in past couple of years. Social

2.1

TweetXplorer

An analyst studying social media data will ask several
questions to better understand their data. To begin, the
analyst tries to find the most interesting days in the dataset
(when). Once the analyst discovers the days of interest,
they will want to enhance their understanding of the events
that occurred on these days. To do this they will want to
find important users and their tweets (who, and what), and
important locations in the dataset (where). In the following sections we show how each of these can be extracted
using TweetXplorer. For more information on our system,
including videos of TweetXplorer in action, please visit the
supplemental page3 .

1
http://blog.twitter.com/2012/03/
twitter-turns-six.html

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM or the author must be honored. To
copy otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
KDD 13, August 11–14, 2013, Chicago, IL, USA.
Copyright 2013 ACM 978-1-4503-2174-7/13/08 ...$15.00.

2
3

1482

We choose Twitter for its openness in sharing data.
http://tweettracker.fulton.asu.edu/TweetXplorer

Figure 1: TweetXplorer - A system for visualizing big social media data.
visualization toolkit4 . Each node in the network represents
a user and the edges between nodes represents a retweet relationship. The network visualization uses the force directed
layout [5] to place nodes and edges on the graph. The layout is designed to highlight nodes with high connectivity.
The computation is prohibiting, O(n3 ), where n is the number of nodes in the network, which is relaxed in D3 with
the Barnes-Hut approximation. Even with this approximation the algorithm can still involve significant computation,
so we trim the network to 3 hops away from the original
tweet. Through this visualization we are still able to find
users at an individual level while making the dataset more
manageable by showing only users and tweets which receive
approval from the community. In Figure 3, we present an
example retweet network.
Node information is encoded using two colors. The outer
color of the node represents the keyword group it is associated with, which is determined by taking a simple majority
of the group of his individual tweets. The darkness of the
inner-color reflects the number of times a user was retweeted,
with darker colors corresponding to higher retweet counts.
Additional node information can be summoned by clicking the node, which brings up an info pane, an example can
be seen in the right part of Figure 3. The top of the panel
contains the name of the user represented by the node, followed by the individual tweets authored by the user. The
background color of each tweet corresponds to a slice in the
pie chart, which summarizes the number of times each tweet
was retweeted. The global retweet network can also be filtered to show a tweet-specific network, shown in the second
row of Figure 2. This can be obtained by clicking a tweet in
the first row of the figure. The network also automatically
filters when a region is selected on the map.
Communicating Salient Locations: When a tweet is

Figure 2: Selective network filtering of a user

Creating Meaningful Queries: Traditional keyword
search is iterative and it typically allows users to obtain a
deep understanding of only one aspect of the data at a time.
In social media analysis there are often multiple avenues an
end user could investigate. We cater to this by allowing
users to select multiple groups of keywords for analysis. By
creating different groups, the user can instantly compare different pathways to see which path would best lead them to
relevant information. It also helps in simultaneous investigation of the data along different dimensions. An example
of groups of keywords a user could create is shown in the
bottom right of Figure 1.
Discovering Interesting Time Periods: TweetXplorer
offers a view into the data that shows the number of tweets
matching a user’s query occurring on each day in the dataset.
This can be used by an analyst to identify interesting time
intervals, where the importance of an interval is determined
by the number of tweets posted on that day and “zoom in”
to the specific intervals to investigate the data further.
Representing Important Users and Tweets: We use
Twitter’s “retweet” functionality to help identify prominent
users and tweets. By viewing the retweet network, the user
can see which tweets were retweeted the most. The network
visualization in TweetXplorer is implemented using the D3

4

1483

http://d3js.org/

(a) Retweet network of @humanesociety discussing pet-friendly evacuation
shelters.

(b) Heat map of tweets 1 hour before
landfall.

(c) Tag cloud of most commonly-used
words in NYC 1 hour before landfall.

Figure 4: Multi-faceted view of TweetXplorer displaying Pre-Landfall information for Hurricane Sandy.

(a)

Figure 3: A retweet network showing users from
different keyword groups. The right shows an information panel.

(b)

Figure 6: (a) Reports from @cnnbrk on flooding and
power outage on east coast. (b) A twitterer debunking the rumor about NYSE.

created, the author has the option to “geotag” their tweet.
In Figure 4(b) we see a view of the map showing a heatmap
of the tweets matching the query “evacuation”. The values
in the heatmap are generated using a 2D kernel density estimate. This visualization enables the user to immediately
see regions of interest on the map. Here, we see that the
darkest region in view lies on the Eastern Seaboard of the
U.S. We zoom in to this region, causing the heat map to
disappear and show each individual tweet on the map. To
investigate tweets from a specific region, we brush the region. Brushing the region around New York City brings up
a popup showing the tweets published from this region and a
tag cloud of the most prominent words in the dataset, shown
in Figure 4(c). This advanced view can help the user to understand the topics of discussion in specific areas during a
crisis and also assist them to improve their queries.
The map component connects network information by displaying geotagged retweets on the map. This helps to find
where individual tweets are receiving attention. The map
overlays the network information to help direct the user to
locations that retweet the initial message. This can help the
user to identify other regions of interest, but which may not
necessarily be high-traffic areas. For more information, see
the video referenced at the beginning of this paper, or the
supplemental images on our web page.
Discovering User Patterns: In addition to more macrolevel views of the data, TweetXplorer also offers a glimpse
into the patterns of individual tweeters. TweetXplorer allows the user to zoom into the tweeting behavior of users to
find characteristics of the users discussing the event. To get a
better understanding of the user’s behavior, we break their
behavior into the following categories: when, what, where,
and how, shown collectively in Figure 5. Figure 5(a) shows
the time of the day and days of the week when the user
prefers to tweet. We can also see the topics that interest

the user by viewing a tag cloud of his most common hashtags, shown in Figure 5(b). This helps us understand what
topics motivate him to tweet. To understand where the user
tweets from, we provide a map displaying the user’s geotagged tweets, shown in Figure 5(c). To show how the user
tweets, we provide a pie chart depicting his client distribution, shown in Figure 5(d). This can help to understand the
user’s tweeting conditions.

3.

CASE STUDY

The final step of the data lifecycle is to put the interpretations of the data to use. Here we will present an example of
TweetXplorer to interpret Twitter data using a dataset containing tweets from Hurricane Sandy, a massive storm that
devastated the East Coast of the United States in late October, 2012. We collected some of the discussion of this event
on TweetTracker for further investigation in TweetXplorer.

3.1

Hurricane Sandy Data

The parameters used to collect the data are shown on
our supplemental web page. We collected 5,639,643 tweets
during the period: October 25th, 2012 to November 3rd,
2012. For the purpose of case study, we partition the dataset
into three distinct epochs: pre-landfall (2012-10-29 00:00 2012-10-29 17:59), landfall (2012-10-29 18:00 - 2012-10-30
23:59), and recovery (2012-10-31 00:00 - 2012-11-01 12:00).
We select keywords to help understand the important topics
in each time period, as shown in the bottom right of Figure 1.
We will enumerate each epoch and discuss some findings.

3.2

Pre-Landfall

In the hours leading up to Hurricane Sandy’s landfall, we
see discussion along different paths. In Figure 4(a), we see

1484

(a) Times the user tweets by
hour of day and day of week.

(b) Tag cloud showing users
preferred hashtags.

(c) User’s geolocated tweets.

(d) Distribution of how the
user publishes tweets.

Figure 5: Views of the user component in TweetXplorer.
that one of the most highly-retweeted tweets is a tweet menNetwork visualizations can play an important role in the
tioning the availability of pet shelters in evacuation areas.
analysis of large datasets. Many systems have been proposed
Looking at the geotagged tweets produced during this epoch
to visualize large networks, such as Gephi [1].
we observe that tweets contain general hashtags and topics
of conversation. As the storm nears we see people move
5. FUTURE WORK
towards terms describing specific issues, such as “rumors”,
In this work we have demonstrated TweetXplorer, a vi“damage”, and “subway”, shown in Figure 4(c).
sual analytics system that can help a user gradually obtain
deeper insight into Twitter data. Going forward we will cre3.3 Landfall
ate new modules for TweetXplorer, which focus on other
Hurricane Sandy made landfall on Oct 29, 2012 at 20:00
forms of information including sentiment analysis and deEST 5 . First reports of flooding start to arrive around this
tecting dynamic communities. We will extend the keyword
time, accompanying links to images of flooding. As the
grouping interface to help the user discover keywords and
storm progresses, we observe several reports of power outhashtags central to an event, and the network component to
age from NYC and nearby areas. Due to the power outage,
show different types of networks in Twitter, such as hashtag
we discover reports of hospitals which were forced to evacco-occurrence, and Twitter friendship network.
uate their patients. In Figure 3, we can see two clusters
of users connected by common retweeters discussing this.
6. ACKNOWLEDGEMENTS
In Figure 6(a), one can observe that @CNN’s tweets claim
This work is sponsored by Office of Naval Research grants
that more than 5.5 million people were without power in
N000141010091 and N000141110527.
the region. More interestingly, most of @CNN’s retweets
came from their tweet reporting the flooded NYC subway
7. REFERENCES
tunnels, which was retweeted 678 times. Reports from the
[1] M. Bastian, S. Heymann, and M. Jacomy. Gephi: An
NYC area paint a similar picture, with top hashtags disOpen Source Software for Exploring and Manipulating
cussing power outages and flooded subway tunnels. At the
Networks. In ICWSM, volume 2, 2009.
same time, false rumors were also spreading on Twitter. As
[2] E. Dumbill. What is Big Data? http://radar.
seen in Figure 6(b), there were several reports of flooding
oreilly.com/2012/01/what-is-big-data.html,
of the NYSE building. Agencies such as @weatherchannel
January 2012.
tweeted this information and later retracted it, as it was
[3]
J.
Dykes, J. Wood, and A. Slingsby. Rethinking Map
discovered to be false.
Legends with Visualization. Visualization and
Computer Graphics, IEEE Transactions on,
3.4 Recovery
16(6):890–899, 2010.
While analyzing the Twitter activity after the storm we
[4] E. Feibush, N. Gagvani, and D. Williams. Visualization
notice that the most prominent tweet during this time disfor Situational Awareness. CG&A, IEEE, 20(5):38–45,
cusses repair resources. Also, the discussion in NYC focuses
2000.
on the words “damage”, “power”, and “flood”, indicating an
[5] T. Fruchterman and E. Reingold. Graph Drawing by
attention shift towards post-storm topics. Supporting imForce-Directed Placement. Software: Practice and
ages are available on our supplemental web page.
experience, 21(11):1129–1164, 1991.
[6] S. Kumar, G. Barbier, M. A. Abbasi, and H. Liu.
4. RELATED WORK
TweetTracker: An Analysis Tool for Humanitarian and
Visualization of events has helped bring increased situaDisaster Relief. In ICWSM, 2011.
tional awareness to first responders in crisis scenarios. [8]
[7] A. MacEachren, A. Jaiswal, A. Robinson,
uses keyword-based search to bring situational awareness to
S. Pezanowski, A. Savelyev, P. Mitra, X. Zhang, and
rescue crews in flood and fire scenarios. [4] assists highJ. Blanford. SensePlace2: GeoTwitter analytics support
ranking commanders in battlefield scenarios through the use
for situational awareness. In IEEE VAST, pages 181
of map layers and textures.
–190, oct. 2011.
Geographical visualization is central to our system. In [3]
[8] S. Vieweg, A. L. Hughes, K. Starbird, and L. Palen.
the authors’ mapping system utilizes a map’s legend to show
Microblogging During Two Natural Hazards Events:
statistical properties of the data. In [7] the authors present
What Twitter may Contribute to Situational
a tool designed to gather situational awareness from tweets.
Awareness. CHI ’10, pages 1079–1088, 2010.
5
[9]
H.
Whitney. Data Insights: New Ways to Visualize and
http://www.nhc.noaa.gov/archive/2012/al18/
Make Sense of Data. Morgan Kaufmann, 2012.
al182012

1485

440

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 17,

NO. 4,

APRIL 2011

Forecasting Hotspots—A Predictive
Analytics Approach
Ross Maciejewski, Member, IEEE, Ryan Hafen, Stephen Rudolph, Stephen G. Larew,
Michael A. Mitchell, William S. Cleveland, and David S. Ebert, Fellow, IEEE
Abstract—Current visual analytics systems provide users with the means to explore trends in their data. Linked views and interactive
displays provide insight into correlations among people, events, and places in space and time. Analysts search for events of interest
through statistical tools linked to visual displays, drill down into the data, and form hypotheses based upon the available information.
However, current systems stop short of predicting events. In spatiotemporal data, analysts are searching for regions of space and time
with unusually high incidences of events (hotspots). In the cases where hotspots are found, analysts would like to predict how these
regions may grow in order to plan resource allocation and preventative measures. Furthermore, analysts would also like to predict
where future hotspots may occur. To facilitate such forecasting, we have created a predictive visual analytics toolkit that provides
analysts with linked spatiotemporal and statistical analytic views. Our system models spatiotemporal events through the combination
of kernel density estimation for event distribution and seasonal trend decomposition by loess smoothing for temporal predictions. We
provide analysts with estimates of error in our modeling, along with spatial and temporal alerts to indicate the occurrence of statistically
significant hotspots. Spatial data are distributed based on a modeling of previous event locations, thereby maintaining a temporal
coherence with past events. Such tools allow analysts to perform real-time hypothesis testing, plan intervention strategies, and allocate
resources to correspond to perceived threats.
Index Terms—Predictive analytics, visual analytics, syndromic surveillance.

Ç
1

INTRODUCTION

V

analytics has been defined as the science of
analytical reasoning assisted by interactive visual
interfaces [1]. Recently, visual analytics systems (e.g., [2],
[3], [4]) have been developed that allow users to interactively
explore their data through linked windows, temporal
histories, document aggregations, and numerous other
views. Such systems allow users to find correlations
between events and begin forming hypotheses about what
events may be occurring in the future; however, the primary
use of such systems tends to be reactive, meaning that
analytic systems are typically used in the context of alert
generation. As event data are captured, algorithms and
analysts search for unexpected events, and these unexpected
events then trigger an alert. Analysts react by drilling down
into the data to confirm the alert, redistributing resources to
control the problem, or other such actions. Unfortunately, in
a reactive situation, events have already occurred that are
negatively affecting the population under analysis. In this
work, we propose the addition of a suite of predictive
analytics tools as a means of enhancing current analysis
systems, thereby moving from a solely reactive paradigm to
a proactive paradigm.
Our predictive analytics system focuses on categorical
spatiotemporal event data (e.g., financial data, crime reports,
and emergency department logs). In such data, events consist
ISUAL

. The authors are with Purdue University, West Lafayette, IN 47907.
E-mail: {rmacieje, rhafen, sglarew, mamitche, ebertd}@purdue.edu,
wsc@stat.purdue.edu, stephen.rudolph@gmail.com.
Manuscript received 29 Sept. 2009; revised 14 Jan. 2010; accepted 25 Mar.
2010; published online 21 May 2010.
Recommended for acceptance by M. Ward.
For information on obtaining reprints of this article, please send e-mail to:
tvcg@computer.org, and reference IEEECS Log Number TVCG-2009-09-0232.
Digital Object Identifier no. 10.1109/TVCG.2010.82.
1077-2626/11/$26.00 ß 2011 IEEE

of locations in time and/or space, and each event fits into a
hierarchical categorization structure. These categories can be
filtered by linked data, and the events may be mapped to a
particular spatial location. Data categories are typically
processed as either time series aggregated over some spatial
location (county, zip code, and collection station), or spatial
snapshots of a small time aggregate (e.g., day, week). These
aggregations are then analyzed using some control chart
method (e.g., cumulative summation, exponential weighted
moving average). There are also systems that allow for
spatiotemporal alert detection ([5]), but such systems become
intractable as the data set becomes large and current
implementations provide little to no geographical mapping
tools. As previously stated, these types of alert systems force
analysts into a reactive paradigm. As such, tools are needed
that not only perform these alert calculations based on current
events, but also perform alert calculations on predicted data.
To this end, we have developed a series of novel
predictive analytics tools. We base our extensions on the
framework developed by Maciejewski et al. [2], [6] to move
from an understanding of spatiotemporal alerts (hotspots) to
a predictive modeling of such alerts; however, it is important
to note that such methods are readily transferable to any
similar data analysis systems. In our previous work, hotspots
were noted solely as areas of the map where events are
occurring at a higher normalized magnitude with respect to
the entire area of study. As we move to predictive analytics,
the previous means of visualizing hotspots can still be
applied (depending on the chosen visualization modality);
however, we now extend the definition of hotspots to also
include areas where the current magnitude of events in an
area is larger than the predicted (or expected) number of
events. As such, our system now provides users with the
ability to compare the current time period’s event magnitude
Published by the IEEE Computer Society

MACIEJEWSKI ET AL.: FORECASTING HOTSPOTS—A PREDICTIVE ANALYTICS APPROACH

to past historical events; furthermore, the integration of our
system with the analytical capabilities of the Kulldorff’s
spatial scan statistics [5] allows users to determine the
statistical significance of the events in that area (i.e.,
determine whether or not the increased magnitude was
due to chance or not). For event prediction, we utilize
seasonal trend decomposition by loess and a scalable kernel
density estimation to aid in spatially distributing the event
prediction. Finally, we utilize texture mapping to provide
users with information on the variability of the prediction.
In our current predictive scheme, each component of the
underlying data set is treated as a separate time series
variable, and cross-correlative effects are disregarded.
Future work will focus on visual analytics schemes for
correlative analysis; however, current syndromic surveillance work focuses on separating syndromes and often
ignores correlative effects. While our time series prediction
work follows this trend, we introduce a novel visualization
scheme in which we apply a k-means clustering to the data
events and project these clusters into the geographic space,
thereby visualizing healthcare ecotopes.
To summarize, novel system features include the
following:
Linked spatial and temporal views for analysis/
forecasting.
. The application of seasonal trend decomposition by
loess for time series prediction.
. Scalable 3D kernel density estimation for spatiotemporal prediction to maintain temporal coherence.
. Integrated spatial scan statistics.
. Comparative views for temporal changes.
. Multiple spatial aggregation schemes for hotspot
analysis and forecasting.
. Texture overlays for uncertainty visualization.
. Multivariate clustering for interactive ecotope
creation.
In order to demonstrate the impact of such tools, we
focus our discussion on a representative categorical
spatiotemporal data set, syndromic surveillance data.
Syndromic surveillance is an area of healthcare monitoring
that focuses on the detection of adverse health events using
prediagnosis information from emergency departments.
Such data have long been recognized as providing meaningful measures for disease risks in populations [7], [8], and
provide a solid base for discussing the impact of our
methods. In this paper, we utilize the data provided by the
Indiana State Department of Health (ISDH) through their
Public Health Emergency Surveillance System (PHESS) [9],
which provides electronically transmitted patient data (in
the form of Emergency Department chief complaints) from 77
hospitals around the state at an average rate of 7,500 records
per day. These complaints are classified into nine categories
(respiratory, gastrointestinal, hemorrhagic, rash, fever,
neurological, botulinic, shock/coma, and other) and used
as indicators to detect public health emergencies before
such an event is confirmed by diagnosis or overt activity.
Further, to demonstrate our event detection and prediction
methods, we employ the synthetic disease injection tools
developed by Maciejewski et al. [10].
Our work focuses on advanced interactive visualization
and analysis methods providing linked environments of
.

441

geospatial data and time series graphs. Time series events
are forecast for a range of spatial aggregations, providing a
context in which to explore potential future events. Time
series alerts are generated for current and predicted event
thresholds, and analysts can explore future event bounds
for resource management and response scenarios. Alerts
generated in the temporal realm can be quickly analyzed in
the spatiotemporal interface, helping users find patterns
simultaneously in both the spatial and temporal domain.
Event distributions are generated based on all events
recorded with respect to the Emergency Department (in a
more generic sense, this can be viewed as any central data
collection location, for example, a police department or
financial institution). This event distribution function can
then be used to place predicted events, and these predicted
events are then correlated with the historical data to model
the expected geospatial density of events, thereby maintaining temporal coherence among hotspots. Such methods
can provide insight into the ongoing impact of current
events, and provide advanced warning for future events,
thereby improving interdiction and response.

2

RELATED WORK

Recently, the development of visual analytics systems for
data analysis and exploration has been rapidly growing (e.g.,
[3], [4], [11], [12], [13], [14]). These systems incorporate a
variety of visualization techniques from traditional, widely
used methods, such as scatterplots or parallel coordinate
plots, to more recently developed tools (e.g., spiral graphs
[15], theme river [16]). Techniques common across these
systems include the probing, brushing, and linking of data in
order to help analysts refine their hypotheses, and these
systems emphasize the interaction between human cognition
and computation through dynamically linked statistical
graphs and geographical representations of the data. However, while these systems allow users to explore their data
and form hypotheses, it has been only recently that visual
analytics systems have begun progressing toward predictive
analytics (e.g., [17], [18]).
Analytic systems in the realm of syndromic surveillance
include the Early Aberration Reporting System (EARS) [19]
and the Electronic Surveillance System for the Early
Notification of Community-based Epidemics (ESSENCE)
[20]. Unfortunately, all of these systems offer limited data
exploration tools and analytic capability is limited to reactive
alerts. Furthermore, these systems tend to generate a large
amount of false positives for epidemiologists to analyze.
Work in the geographical and visual analytics communities
has attempted to improve healthcare data analysis and
exploration through a variety of systems (e.g., [2], [21], [22])
using linked views and interactive plotting; however, these
systems also stop short of predicting future events.
The concept of predictive analytics is found widely
across financial services (e.g., credit scoring), retail sales,
and healthcare. With respect to the categorical spatiotemporal event data, we focus our discussion solely on time
series modeling and spatiotemporal modeling for forecasting events. A summary of time series analysis in biostatistics can be found in [23]. Common time series modeling
techniques include the use of autoregressive moving

442

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

average (ARMA) models (e.g., [24]), which describe stationary time series, and autoregressive integrated moving
average (ARIMA) models (e.g., [25]). While such models are
useful, we focus on the application of a nonparametric
method, seasonal decomposition of time series by loess
(STL) [26], [27]. This method allows flexible modeling of the
differing onsets and shapes of seasonal peaks, and allows us
to account for other components of variation also, see
Section 3.1.2 for details. Machine learning methods could
also be applied (e.g., [28]); however, we leave such methods
for future work.
Along with time series modeling and prediction, our
work also focuses on spatiotemporal predictions. A
summary of spatial modeling and geostatistical methods
can be found in [29] and [30], and the authors note that,
typically, the temporal prediction should take precedence
over the spatial. As such, we utilize temporal modeling for
predicting events and employ kernel density estimation for
creating a probability distribution of patient locations. This
probability distribution is then used to place the number of
predicted patients into geospatial locations, see Section 3.2
for more details. We also incorporate the spatial scan
statistic developed by Kulldorff [5] into our work utilizing
the batch mode interface of the SatScan system. Previous
work on visualizing the output of the SatScan has been
done by Boscoe et al. [31] using nested circles to visualize
the various cluster outputs, and Chen et al. [32], who
analyzed the sizes of the clusters output for enhancing the
resulting visualization with reliability information.
Given the sparsity of our data in certain geographical
areas, our system also provides visualizations of data
reliability/uncertainty. Due to the fact that low counts in
spatially aggregated areas can lead to skewed statistics [33],
[34], we utilize the variance of the data to represent the level
of uncertainty. Previous work in uncertainty visualization
has utilized glyphs in flow visualization [35], point-based
probabilistic surfaces [36], procedural annotations [37], and
the use of the HSI color model [38]. Our current system
follows the convention of MacEachren et al. [21]. Their work
utilized adjacent maps to show data reliability, and compared this to maps utilizing textures (hashing) overlaid on
color to display reliability. We incorporate a similar texture
overlay to represent data reliability within our system.

3

PREDICTIVE ANALYTIC ENVIRONMENT

Our current work extends the system developed by
Maciejewski et al. [2], [6]. As in many visual analytic and
information visualization systems, we utilize dually linked
interactive displays for multidomain/multivariate exploration and analysis as well as interactive filter controls for
variable selection. We extend both the spatial and temporal
viewing windows to incorporate spatiotemporal predictions for enhanced data analysis and exploration, moving
from a visual analytics environment to a predictive visual
analytics environment.
This work is motivated by the fact that in analyzing
event-driven data, analysts are often creating plans for
future resource usage and need. With regard to syndromic
surveillance, the prediction of potential disease outbreaks
allows better resource management and response; thus, the
need for analyzing and predicting spatiotemporal events

VOL. 17,

NO. 4,

APRIL 2011

necessitates the creation of such a system. The syndromic
surveillance community has done much work in categorizing and analyzing events based on emergency department
data, and we utilize this data set in order to demonstrate the
functionality of our tools.
Fig. 1 presents a screenshot of our system. In this
example, the user is exploring potential future outbreaks of
respiratory syndromes across the state of Indiana. The data
displayed in the geospatial window utilize a divergent color
map [39] illustrating the number of standard deviations the
data are above or below the expected average. Results from
Kulldorff’s spatial scan statistic [5] are overlaid on the map,
providing cues for the user to explore certain areas and are
linked in the lower right-hand window. Users may
interactively view other syndromes, or filter the data by
age, gender, and/or keyword in order to perform more
complex analyses. Selection of counties and/or hospitals is
displayed in the time series windows on the right. The time
series plots provide upper and lower bounds of prediction
through an overlaid transparent polygon. The white vertical
line serves as a reference for the geospatial date shown
above the slider. Note that predicted data are only
displayed when actual data are not available.
The contributions of our new system include methods
for spatiotemporal prediction and methods for the interactive visualization of these predictions. We employ several
time series modeling techniques for data forecasting, and
use the results of these predictions in a spatial modeling
scheme to represent event distributions. Visualization
schemes include a default view of predicted hotspots using
the EARS CUSUM2 model [19] overlaid with semitransparent circles of Kulldorf’s spatial scan statistic [5] for results
with p < :1, a view showing the magnitude of the change
across time, county level views of STL predicted hotspots,
and ecotope views for multivariate analysis. Note that the
predictive power (false positive rates) of the models is
discussed in the respective papers. The goal of this work is
to demonstrate how these methods can be augmented
through linked visuals and interactive displays, thereby
creating a predictive visual analytics environment.

3.1 Time Series Prediction
In our predictive analytics environment, time series models
are used for forecasting the future behavior of events. Our
temporal modeling is performed over a spatial aggregation
of data, meaning that the collection of all event records over
the state, county, or data collection agency make up the
time series (in the case of syndromic surveillance, the
collection agency would be the Emergency Department).
For multivariate data, we model each event category as a
separate time series signal. Future work will focus on more
robust models to capture correlations between signals. We
employ both a moving average and a seasonal trend
decomposition model [26]. These predictions can then be
used for supply management (e.g., insuring enough
antibiotics are available) and outbreak preparedness (i.e.,
if an outbreak has occurred or is expected to occur, staff
members may be informed of the predicted models and can
look for specific symptoms).
For time series prediction, we utilize both the standard
event detection algorithms utilized by the Center for

MACIEJEWSKI ET AL.: FORECASTING HOTSPOTS—A PREDICTIVE ANALYTICS APPROACH

443

Fig. 1. Our interactive predictive visual analytics environment. The interface has been customized for exploring syndromic surveillance data. Map
colors indicate the number of standard deviations an area is above the expected value. In this instance, the user is analyzing respiratory syndrome
counts across the state. Times series are being shown at the hospital and county-level aggregation. In the time series window, yellow diamonds
indicate temporal alerts, the white line represents the current day, and the translucent polygon represents the bounds of the time series prediction.
Predicted values are only displayed when data are not available, and a temporal alert threshold is provided at each prediction. The circle drawn on
the map indicates a scan statistic with p < 0:1 (yellow). Advanced details of the spatial scan statistic are found in the lower right-hand window.

Disease Control, as well as a seasonal trend decomposition
method that we have previously developed [27]. In the
work by Hafen et al., the seasonal trend decomposition
method is compared to other standard methods of
syndromic modeling in order to demonstrate the efficacy
of our techniques. While other methods can be successfully
applied to syndromic surveillance data (e.g., [40]), our
previous work in modeling syndromic surveillance data
shows that this method currently outperforms other
suggested techniques.

3.1.1 Event Detection
In terms of outbreak detection through time series analysis,
one of the standard epidemiological algorithms employed is
the EARS C2 alert algorithm [19] based on the cumulative
summation (CUSUM):


Xt  ð0 þ kxt Þ
St ¼ max 0; St1 þ
:
ð1Þ
x t
Equation (1) describes the CUSUM algorithm, where St is the
current CUSUM, St1 is the previous CUSUM, Xt is the count
at the current time, 0 is the expected value, xt is the standard
deviation, and k is the detectable shift from the mean (i.e., the
number of standard deviations the data can be from the
expected value before an alert is triggered). We apply a 28day sliding window to calculate the mean 0 and standard
deviation xt with a three-day lag, meaning that the mean and
standard deviation are calculated on a 28-day window three

days prior to the day in question. Such a lag is used to increase
sensitivity to continued outbreaks. All model parameters
utilized in our system are based on recommendations by [19]
and other syndromic surveillance literature.
Given the three-day lag, we extend the current time
series into the future by simply calculating the mean of the
sliding window. This method allows us to provide the
analyst with both an expected value for the next three days
(note that 3 could be modified depending on the chosen lag)
and an alert threshold. While this prediction is limited, the
moving average is useful for providing a quick look at the
expected average number of incoming patients, and thresholds can quickly be set to determine various alert levels. For
syndromic surveillance data, we utilize the threshold values
of the EARS CUSUM2 model [19] (approximately two
standard deviations).

3.1.2 Prediction with Seasonal Trend Decomposition
Based on Loess
Unfortunately, the moving average model fails to take into
account some important characteristics of chief complaint
count data, such as the day-of-the-week. Furthermore,
using a 28-day sliding average is not ideal for time series
with components that evolve over the course of a month. In
order to more accurately model the data, we employ a
different strategy in which the time series is viewed as the
sum of multiple components of variation [27]. Seasonal
trend decomposition based on loess (locally weighted

444

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

regression) [26] is used to separate the time series into its
various components. STL components of variation arise
from smoothing the data using moving weighted leastsquares polynomial fitting, in particular, loess [41], with a
moving window bandwidth in days. The degree of the
polynomial is 0 (locally constant), 1 (locally linear), or 2
(locally quadratic).
Here, it is important to note that in order to appropriately
model the time series using STL, the mean and variance of
the data need to be independent. To accomplish this, a
power transformation is applied to the data. In time series
analysis, the logarithm transformation is widely applied
when the mean is proportional to the standard deviation
[42]. In cases where the data consist of counts following a
Poisson distribution, a square root transformation will make
the mean independent of the standard deviation. Analysis of
the syndromic surveillance data showed that the square root
transformation stabilizes the variability and yields a more
Gaussian distribution of the time series residuals. As such,
the STL modeling is performed on the square root scale of
the original series in order to remove the dependence of a
signal’s variance on its mean.
For a given hospital, we decompose our daily patient
count data into a day-of-the-week component, a yearly
seasonal component that models seasonal fluctuations, and
an interannual component that models long-term effects,
such as hospital growth:
pﬃﬃﬃﬃﬃ
Yt ¼ Tt þ St þ Dt þ rt ;
ð2Þ
where for the t-th day, Yt is the original series, Tt is the
interannual component, St is the yearly seasonal component,
Dt is the day-of-the-week effect, and rt is the remainder.
The procedure begins by extracting the day-of-the-week
component Dt . First, a low-middle-frequency component is
fitted using locally linear fitting with a bandwidth of 39 days.
Then
pﬃﬃﬃﬃﬃ Dt is the result of means for each day-of-the-week of the
Yt minus the low-middle-frequency
pﬃﬃﬃﬃﬃcomponent. Next, the
current Dt is subtracted from the Yt and the low-middlefrequency component is recomputed. This iterative process
is continued until convergence. After removing the day-ofthe-week component from the data, we use loess smoothing
to extract the interannual component, Tt , using local linear
smoothing with a bandwidth of 1,000 days. Finally, we apply
loess smoothing to the data with the day-of-week and
interannual components removed, thereby obtaining the
yearly seasonal component, St , using local quadratic
smoothing with a bandwidth of 90 days. After removing
the day-of-week, interannual, and yearly seasonal components from the time series, the remainder is found to be
adequately modeled as independent identically distributed
Gaussian white noise, indicating that all predictable sources
of variation have been captured in the model. It is important
to note that these parameters should be modified to work
best within the confines of a given data set; however, the
application of STL for modeling and prediction is not
relegated only to syndromic surveillance data. Details of
the methodology behind the parameter choices can be found
in [27] and it is not the main focus of this work. However, the
extension of this method to include data prediction is novel.

VOL. 17,

NO. 4,

APRIL 2011

For prediction using the STL method, we rely on some
statistical properties of loess, namely that the fitted values
Y^ ¼ ðY^1 ; . . . ; Y^n Þ are a linear transformation of the observed
data, Y ¼ ðY1 ; . . . ; Yn Þ. Each step of the STL decomposition
involves a linear filter of the data. In other words, an output
time series x ¼ fx1 ; . . . xn g is produced by an input time series
w ¼ w1 ; . . . ; wn through a linear combination
xi ¼

n
X

ð3Þ

hij wj :

i¼1

If we let H be a matrix whose ði; jÞ-th element is hij , then
we have
x ¼ Hw:

ð4Þ

We will refer to H as the operator matrix of the filter.
Now, let HD , HS , and HT denote the operator matrices of
the day-of-week, yearly seasonal, and interannual filters,
respectively. All of these matrices are n  n. HS and HT are
straightforward to calculate [41], but HD is more difficult to
calculate as it is the result of an iteration of smoothing. Once
all of these have been calculated, the operator matrix for the
entire procedure H can be written as
H ¼ HD þ HT ðI  HD Þ þ HS ðI  HD  HT ðI  HD ÞÞ; ð5Þ
where I is the n  n identity matrix. Now, the fitted values
are obtained by
Y^ ¼ HY :

ð6Þ

To make better sense of (5), the day-of-the-week smoothing,
HD , is applied to the raw data, while the interannual
smoothing, HT , is applied to the raw data with the day-ofweek removed, and finally, the yearly seasonal smoothing,
HS , is applied to the raw data with the day-of-week and
interannual removed.
Now, the variance of the fitted values is easily obtained:
V arðY^i Þ ¼ ^2

n
X

Hij2 ;

ð7Þ

j¼1

where ^2 is the variance of Y and can be estimated from the
remainder term rt .
Now, if we wish to predict ahead, x days, we append the
operator matrix H with x new rows, obtained from
predicting ahead within each linear filter and use this to
obtain the predicted value and variance. For example, if we
wish to predict the value for day n þ 1, we would obtain
n
X

Y^nþ1 ¼

ð8Þ

Hnþ1;j Yj

j¼1

and
V arðY^nþ1 Þ ¼ ^

2

1þ

n
X

!
2
Hnþ1;j

;

ð9Þ

j¼1

so that an approximately 95 percent prediction interval will
be calculated as
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð10Þ
Y^nþ1  1:96 V arðY^nþ1 Þ:

MACIEJEWSKI ET AL.: FORECASTING HOTSPOTS—A PREDICTIVE ANALYTICS APPROACH

445

information in the form of a color map. On any given day,
we have either the number of events that occurred and the
predicted number of events, or only the predicted number. In
Fig. 3, the analyst is scrolling across time through the
predictive models. The analyst can visualize the data
aggregated at the county level utilizing the STL predictive
model (Fig. 3a), which provides a stronger means of
prediction at a larger computational cost, or the analyst can
utilize a finer spatial granularity view (as shown in Fig. 3b) at
the cost of a less accurate moving average prediction.

Fig. 2. Seasonal trend decomposition based on loess prediction
compared to actual measurements.

To demonstrate our prediction model, we have utilized
data from PHESS from 1 January 2006 to 31 December 2007
for a single emergency department. Our STL modeling and
prediction method is applied to this datum to predict from 1
January 2008 to 14 January 2008. We then compare this
prediction to the actual data in Fig. 2. Comparable results
were found for all other hospitals in our system.

3.2 Geospatial Prediction
While the temporal prediction provides a forecast of the
number of expected events, we are also interested in
providing analysts with a means to analyze the expected
spatial distributions. As stated above, our time series
prediction can be performed over a variety of spatial
aggregations. As such, we allow users to choose between
several granularities of the spatial prediction using various
data aggregations in the time series prediction as a basis for
our event distribution.
3.2.1 Geographically Aggregated Distribution
The simplest means for spatial prediction is to utilize the time
series counts based on an arbitrary geographic boundary
(e.g., state, county, and zip code), and visualize this

3.2.2 Spatiotemporal Distribution
In order to compute predictions at the finer spatial granularity, we expand on our previous use of density estimation [2]
and model the spatiotemporal distributions of patients based
on their emergency department visits. We employ a modified
variable kernel method [43], which scales the parameter of the
estimation by allowing the kernel width to vary based upon
the distance from Xi to the kth nearest neighbor in the set
comprising N  1 points:
!
N
X
1
1
x

X
i
^
:
ð11Þ
fðxÞ
¼
K
N i¼1 maxðh;di;k Þ
maxðh;di;k Þ
Here, the window width of the kernel placed on the point
Xi is proportional to di;k (where di;k is the distance from the
ith sample to the kth nearest neighbor) so that data points in
regions where the data are sparse will have flatter kernels,
and h is the minimum allowed kernel width.
We utilize the Epanechnikov kernel [43], (12):
3
KðuÞ ¼ ð1  u2 Þ1ðjjujj1Þ;
4

ð12Þ

where the function 1ðjjujj1Þ evaluates to 1 if the inequality is
true and 0 for all other cases.
Given the predicted number of events from the time
series modeling, we want to distribute these events given
the probability density function of event locations with

Fig. 3. Spatiotemporal prediction comparing (a) the STL prediction and (b) the moving average prediction.

446

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

respect to some shared geographic location (in the case of
syndromic surveillance, we model the population distribution served by a given Emergency Department). For each
Emergency Department, we know each patient’s home
address. These addresses are mapped to a grid centered
around the hospital, and we employ (11) to create a
distribution function representing the probability that a
patient will come to the hospital from a given (latitude,
longitude) pair. We then randomly distribute the n
predicted events according to this distribution. This is done
for each emergency department to simulate patient distributions across the state.
Once the events are distributed, we create a threedimensional array, consisting of a grid of patient locations
across the predicted day being visualized and the previous t
days. We then perform a three-dimensional kernel density
estimation to maintain the temporal coherence of previous
hotspots. This allows the analysts to determine if the
hotspots could be persistent across time under the assumption that patients will visit the Emergency Department
based only on its service area distribution. Finally, the
estimated density of the current day’s events (with the
incorporated temporal history) can be plotted as a ratio of
the number of events under analysis versus the total
number of events (also calculated to incorporate temporal
history), or each grid point can be utilized in the EARS
calculations to determine hotspots as in Fig. 3b. Examples of
the use of such modeling for hypothesis generation and
testing are provided in Section 4.

3.3 Spatial Scan Statistics
Kulldorff proposed a spatial scan statistic [5] to detect the
location and size of the most likely cluster of events in space
or space-time data. A scanning window of variable size and
shape is moved across the area of interest, and for each
window, a likelihood value is calculated based on the
contents of the window. The likelihood value is maximized
over all possible windows and this maximum likelihood is
called the scan statistic. Our system utilizes the available
batch mode settings of the SatScan system, focusing on the
Bernoulli data model [44] as patients are point-based
locations within our system. Fig. 1 shows the SatScan control
window (lower right) utilized within our system. Statistically significant events are automatically highlighted (with
yellow representing significance at the 0.1 level and red at
the 0.05 level). Semitransparent circles (also colored according to significance) for statistically significant events are
automatically drawn on the map (as seen in Fig. 1). Circles
may be toggled on and off by selecting the items from the
SatScan control window (lower right controls of Fig. 1).
3.4 Visualization Modalities
The application of various models and aggregations to
categorical spatiotemporal data allows a variety of visualization modalities to support users views. System views
include the following:
.

Percentile views using sequential color maps to
visualize which areas had the highest number of
patients with a given syndrome with respect to the
number of patients seen visiting a hospital.

.

.

.

.

VOL. 17,

NO. 4,

APRIL 2011

Temporal change views in which a divergent color
map is used to represent which areas of the map
saw an increase or decrease in the number of
syndromic patients.
Hotspot views in which a divergent color map is
used to represent the number of standard deviations
an area is above or below the expected value.
Uncertainty views in which a texture is overlaid on
any visualization modality representing areas with a
variance above some user-defined threshold.
Ecotope views in which a qualitative color map is
used to represent multivariate clusters of syndromes.

3.4.1 Percentile Views
Fig. 4a demonstrates the percentile view across both county
aggregation and the kernel density estimated data. Percentages are calculated as a function of the number of patients
with syndrome X in an area, A, divided by the total
number of patients seen in A. Such a view is useful in
providing an analyst with an overall picture of the health of
the state; however, it provides little statistical information
about events without further exploration and interaction
(such as selecting counties or hospitals and looking at the
time series view).
3.4.2 Temporal Change Views
Fig. 4b demonstrates the temporal change view across the
kernel density estimated data. The temporal change is
calculated as the current time aggregation minus the
previous time aggregation, thereby providing a magnitude
of change as the user interactively scrolls through the data.
Here, a divergent color map is used to show the percentile
change from day to day. Such a view can provide analysts
with details about the growth of previously found clusters,
allowing them to check for persistence, growth, or decay.
Analysts may lock the previous day’s clusters to the map
using a circular drawing tool or using the SatScan clusters.
Then by moving forward in time with this view, analysts
can view the predicted data change in that area. Note that
the magnitude of the illness is now known to be consistent
(or increasing) from 31 December 2007 to 1 January 2008 as
the area colored under the 12-31-2007 red cluster is either
white (meaning no increase or decrease) or red (meaning
the number of events increased).
3.4.3 Hotspot and Uncertainty Views
The hotspot view (as illustrated in Fig. 3a) is the analyst’s
default view. Here, the analyst can immediately see which
areas of the map were statistically significant. In this mode,
the analyst then forms (or generates) a hypothesis stating
that areas in red may be having a disease outbreak. The
overlay of the spatial scan statistics results can help
strengthen this hypothesis and provide a hypothesis testing
tool by returning p-values.
Furthermore, an overlay of uncertainty information
through texturing based on the variability of the data can
also help in hypothesis formation. The uncertainty information is calculated as the variance of the data in a given area
over the last week. If the variance is above some user-defined
threshold, then a texture is mapped to the area. The area
being analyzed is a single areal bin of the map. The sparser

MACIEJEWSKI ET AL.: FORECASTING HOTSPOTS—A PREDICTIVE ANALYTICS APPROACH

447

Fig. 4. (a) Percentile views showing the magnitude of a syndrome count with respect to the underlying population that visited the emergency
department in a given area on 31 December 2007. (b) Temporal change from 31 December 2007 to 1 January 2008. The SatScan cluster was found
on 31 December 2007 and was chosen to remain in persistent mode by the user, meaning that the cluster visualization will persist as the user scrolls
through time. The user can see that the magnitude of illness in that area is predicted to remain consistent or increase.

the data are, the less reliable and more uncertain the data will
tend to be. Areas of the map with high variance tend to show
up as hotspots; however, these areas may not be worth
investigating as the statistical power of the data is quite low.
In Fig. 3b, we can see that many of the hotspots correspond to
a high level of uncertainty, making those less reliable in the
analyst’s mind. This is shown through the use of the angled
line texture overlays.

3.4.4 Multivariate Clustering Ecotope Creation
Given the multivariate nature of the data, our system also
provides an ecotope view in which qualitative color maps
are utilized to represent data clusters within an n-dimensional space. Ecotopes are areas of similar conditions
relating an environment to the organisms or inhabitants of
that space. They can be thought of as the geographical
component to an ecosystem in that they denote the
particular area where a relation between habitats and their
inhabitants exists. In this paper, the idea of an ecotope is
applied to healthcare symptom rates such that the ecotope
regions are taken to mean geographical areas whose human
population exhibits similar rates for any particular symptom or set of symptoms. Instead of a broad climate zone
style stratification where all climate factors are taken into
account to produce large generalized climate zones, the
method taken in this paper is to create health ecotopes
specific to certain selected symptoms. We utilize K-means
clustering [45] as shown in the work by Hargrove and
Hoffman [46], [47] for ecotope creation.
First, we place each syndrome classification to a different
grid and use kernel density estimation to create a probability density function (pdf) for the syndrome for a given
day. Each syndrome then becomes an axis in the ndimensional space, where each cell from the pdf is mapped
into this space. We then apply K-means clustering, placing
each data cell into a group of cells with similar property

rates. By mapping the found clusters of like data points
onto a two-dimensional geographic space, health ecoregions will be delineated as colored regions. Fig. 5a
demonstrates this process on three-dimensional data. Our
system also allows the user to then sort these clusters by the
magnitude of one dimension in order to provide enhanced
visual exploration as shown in Fig. 5b.
By classifying our geography into healthcare ecotopes,
an understanding of how our environment and symptom
rates relate to each other can be found. The relationship
may be that the environment or location may be a strong
contributing factor to the syndrome rate. In this case, it is
likely that environmental anomalies would be given a
unique ecotope color. However, it is important to note that
the relationship between the environment and inhabitants
may also be weak. In the case where a region does not have
any strong contributing factor, it is likely that the ecotope
boundary will change over time.
In Fig. 5b, we can see that certain areas of the state
map to various combinations of respiratory, gastrointestinal, neurological, and constitutional syndromes. In this
view, the analyst may first have utilized the hotspot views
to find areas on the map that correspond to a significant
increase in a particular syndrome. From there, the analyst
might then move to the healthcare ecotope view and see
what syndrome groupings are occurring in that area. In
this case, Fig. 5b, we can see near Lake Michigan (the
upper West portion of the map) a pink area surrounded
by an orange area meaning that a cluster with a higher
amount of respiratory illness is in that area in comparison
to the surrounding. Further, we can see that the average
gastrointestinal value is higher than the respiratory value,
indicating that there may be a correlation between the
two syndromes.

448

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 17,

NO. 4,

APRIL 2011

Fig. 5. (a) An illustration of our ecotope generation using k-means for a three-dimensional example with k = 8. (b) Eight healthcare ecotopes
generated from a 4D data space consisting of respiratory, gastrointestinal, neurological, and constitutional syndromes with colors sorted by regions
containing the highest magnitude values of respiratory syndromes.

4

CASE STUDIES IN HOTSPOT ANALYSIS AND
FORECASTING

By using a combination of geospatial and temporal
visualization and analytics tools, our system provides
analysts with tools for real-time hypothesis generation/
testing and event prediction. In order to demonstrate the
strengths of our modeling tools, we utilize synthetic
syndromic surveillance data [10] with known outbreaks.
This section provides both a retrospective case analysis and a
prospective case analysis. We utilize two years worth of
synthetic syndromic surveillance data (1 January 2006
through 31 December 2007) with 33 emergency departments
across the state of Indiana [10] with two known outbreaks.
We utilize synthetic data as opposed to actual data in order
to remove privacy concerns; however, we have found the
results to be comparable across synthetic and actual data.

4.1 Retrospective and Reactive Analysis
In order to illustrate our alert generation using EARS C2
alerts and spatial modeling effects, an outbreak containing
patients presenting signs of respiratory illness was introduced beginning on 18 July 2007 and ending on 22 July
2007. The injection of patients followed a lognormal
distribution such that the number of excess patients
showing respiratory syndrome symptoms was 1 on 18 July,
then 18, 8, 5, 3, and 2 for each subsequent day. In Fig. 6a, the
user is looking at a typical geospatial view at a county-level
aggregation. Note that in the disease injection area, the
counties are only showing a slightly higher percentage than

their neighbor; however, if the analyst were to instead look
at a hotspot view, Figs. 6b and 6c, of the actual patient
counts versus the predicted patient counts, a different story
unfolds. The analyst can immediately identify several areas,
where a larger number of patients were seen than expected.
More information can be garnered by overlaying the
SatScan cluster output onto the maps. Now the analyst
can immediately find the general location of an outbreak
occurring on July 19th coinciding with the area where an
outbreak was injected. Furthermore, the time series alerts
from the EARS C2 algorithm, yellow diamonds in Figs. 6d
and 6e, further corroborate this outbreak.
In order to obtain a more localized view of where the
outbreak is occurring, the analyst may switch over to the
density estimate view, see Fig. 7a. The analyst can scroll
back in time prior to when the first alert was generated in
the hospital’s time series and begin looking at the estimated
patient density. Note that the estimated density remains
consistent in the northwest portion of the state until the
outbreak reaches its peak on 19 July. On that day, the spatial
model shows a higher concentration of patients across a
very specific geographic region, and the analysts may then
focus their attention on that particular region as opposed to
a multiple county alert that might have been issued if only
the views in Figs. 6a, 6b, and 6c had been utilized. However,
such a view is only applicable in a reactive manner (the
analyst is comparing what happened on 19 July to what was
expected to happen). Furthermore, the analyst could also
utilize the temporal change view and observe how the
outbreak began over time. Fig. 7b shows the magnitude of

MACIEJEWSKI ET AL.: FORECASTING HOTSPOTS—A PREDICTIVE ANALYTICS APPROACH

449

Fig. 6. An outbreak has been injected beginning on 18 July 2007 and ending on 22 July 2007. The black circle represents the injection. (a)-(c) Images
show 19 July 2007. (a) A percentile view of patients with respiratory illness. Note that no outbreak is readily observable in this view. (b) A hotspot
view of actual patients versus the number of predicted patients. Here, the analyst can quickly see which counties have exceeded the predicted
values. (c) A finer granularity hotspot view with uncertainty overlaid as a texture. Both (b) and (c) also utilize the SatScan cluster overlay feature. (d)
and (e) Time series views of the nearby hospital and counties, yellow diamonds indicate alerts. We see that on 19 July, alerts were generated for the
three dark red counties overlapped by the SatScan circle.

temporal changes. Note that the analyst can immediately
see large changes occurring in the state on any given day. It
is only through a combination of linked views and
interactive displays that more knowledge and confidence
can be gained to allow them to accurately assess an
outbreak.
In the case of retrospective and reactive analyses, our
system provides an analyst with a means to form
hypotheses. In this case, the analyst may look at the hotspot
view and note that several counties are indicating problems.
A hypothesis is then generated by the analyst of the form
“There may be a health outbreak in region X.” This
hypothesis can then be tested through the use of SatScan,
providing the analyst with a significance test, or through
the linked views and temporal models which also provide
alerts based on a significance value. Further, the analyst can
scroll back in time and watch the outbreak develop, and
through the use of finer granularity, density estimation
begins getting a picture of the exact area of infection as
opposed to the purely circular region denoted by the spatial
scan statistic.
The analyst may also then choose to look at the ecotope
view of the data. Fig. 5 shows a clustered view of the 19 July
data, clustering on respiratory, gastrointestinal, neurological, and constitutional syndromes. Here, one can see a light
pink ecotope grouping that would largely overlap the

shapes seen in the 19 July pictures of Fig. 7. This type of
mapping along with the related information can enhance
the information the analyst receives by providing them with
information on what other syndromes may be found in that
area, allowing them to refine their hypothesis from “An
outbreak is occurring in area A.” to “An outbreak of disease
X is occurring in area A.”

4.2 Forecasting and Proactive Analysis
In a proactive analysis, the analyst would be watching for
alerts, or analyzing potential future spreading of already
confirmed alerts. In order to illustrate the use of our tools
for analyzing future events, an outbreak containing
patients presenting signs of respiratory illness was introduced on 29 December 2007 and is still peaking on 31
December 2007. The injection of patients was such that six
patients were injected on 29 December, then 15 and
21 patients on each of the following days. The analyst
has no data past this point.
In Fig. 8a, the analyst is viewing 31 December 2007 for
Hospital 21 located in Montgomery County. An alert was
generated based on that hospital’s time series for three
consecutive days (29th-31st December), and the analyst is
concerned that the outbreak is not subsiding. Fig. 1 shows a
low-level SatScan alert for the area first appears on 29
December 2007. In the time series window of Fig. 8a, the

450

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 17,

NO. 4,

APRIL 2011

Fig. 7. Detecting an injected outbreak with spatiotemporal modeling. An outbreak has been injected beginning on 18 July 2007 and ending on 22 July
2007. Note that on 19 July, a rise in cases is visible in that area, providing information on a more exact cluster location. (a) A percentile view of the
data. (b) A temporal change view of the data.

analyst notes that over the following two days, the
predicted number of patients visiting the hospital is
expected to be larger than the generated alert threshold.
Further, the alert threshold within Montgomery County is
far less than the upper bounds of the prediction. This
hypothesis that the analyst has generated is further
corroborated by the fact that SatScan has also detected a
disease outbreak near Montgomery County as well as an
outbreak in northern Kentucky. Note that the Kentucky
outbreak is a false positive in this case, demonstrating the
need for investigation into these alerts.
With this information, the analyst may also wish to
corroborate these findings with the spatial models. Fig. 8b
illustrates the temporal changes for the hotspots across
Indiana from 31 December 2007 to 3 January 2008, and
Fig. 9 illustrates the hotspot view by county from 31
December 2007 to 2 January 2008. The analyst can
immediately see a hotspot in an area near the hospital.
When tracking the hotspot into the future, the levels remain

consistent or even increase on 1 January and 2 January and
begin to taper off on the 3rd, as can be seen by the color
scale legend representing the temporal change. With this
information, the analyst may assume that the outbreak
seems to be persistent in the future, and could choose to
issue a health alert localized to that area if the threat was
severe enough.

5

CONCLUSIONS AND FUTURE WORK

Our current work demonstrates the benefits of predictive
visual analytics for forecasting syndromic hotspots. By
linking a variety of data sources and models, we are able
to enhance the hypothesis generation and exploration
abilities of our state epidemiologists. Our initial results
show the benefits of linking time series prediction views
with spatiotemporal views for enhanced exploration and
data analysis through the use of traditional choropleth
maps with various granularities for more accurate event

MACIEJEWSKI ET AL.: FORECASTING HOTSPOTS—A PREDICTIVE ANALYTICS APPROACH

451

Fig. 8. In these data, an outbreak has been injected beginning on 29 December 2007 and is still peaking on 31 December 2007 (the last day of the
available data). The SatScan circle in Indiana covers the approximate area of injection. (a) The predictive visual analytics system. Note that the time
series view contains an upper and lower bound for the prediction, as well as at what range alerts would occur. Here, we can see that on 1 January
and 2 January, the predicted value would be higher than the alert threshold indicating the continued presence of an outbreak. (b) A series of
predicted geospatial data. Here, we follow the outbreak in the spatial prediction model and see it begins to subside on 3 January.

detection. Further, the extension of the kernel density
estimation to incorporate temporal modeling maintains
the temporal coherency of outbreaks, enhancing analysis.
In order to ensure system usability, the development of
our system has been done under the guidance of collaborators at the Indiana State Department of Health and the
Indiana University School of Medicine. Further, discussions
with local police departments and state law enforcement
agencies indicate that such tools are able to assist other
analysts whose data dimensions fall into the realm of
categorical spatiotemporal data.
Future work includes the introduction of advanced
control chart methods for more accurate alert generation.
Furthermore, it is important to note that the power
transformation used can change based on the time series
data under analysis. We also plan on employing spatiotemporal clustering algorithms for event detection as well
as correlative analysis views within the temporal domain.

Other needs include better modalities for uncertainty
visualization, as well as enhancements to the ecotope view
for better data analysis.

ACKNOWLEDGMENTS
The authors would like to thank the Indiana State Department of Health and the Regenstrief Institute for their input
on system development and user feedback, particularly
Dr. Shaun Grannis for his feedback on this project. The
Cancer Care Engineering Project is supported by the
Department of Defense, Congressionally Directed Medical
Research Program, Fort Detrick, MD (W81-XWH-08-1-0065)
and the Regenstrief Cancer Foundation administered jointly
through the Oncological Sciences Center at Purdue University and the Indiana University Simon Cancer Center.
This work has been funded by the US Department of

452

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 17,

NO. 4,

APRIL 2011

Fig. 9. Here, we show the county-level hotspot view for analyzing the future ramifications of the 31 December outbreak. (a) 31 December 2007. (b) 1
January 2008. (c) 2 January 2008. Note that it is now the county south of Montgomery that is showing signs of the outbreak persisting.

Homeland Security Command, Control and Interoperability
Center of Excellence.

REFERENCES
Illuminating the Path: The R&D Agenda for Visual Analytics. J.J.
Thomas and K.A. Cook, eds. IEEE Press, 2005.
[2] R. Maciejewski, S. Rudolph, R. Hafen, A. Abusalah, M. Yakout, M.
Ouzzani, W.S. Cleveland, S.J. Grannis, M. Wade, and D.S. Ebert,
“A Visual Analytics Approach to Understanding Spatiotemporal
Hotspots,” IEEE Trans. Visualization and Computer Graphics, vol. 16,
pp. 205-220, Mar./Apr. 2010.
[3] J. Stasko, C. Gorg, Z. Liu, and K. Singal, “Jigsaw: Supporting
Investigative Analysis through Interactive Visualization,” Proc.
IEEE Symp. Visual Analytics Science and Technology 2007, pp. 131138, 2007.
[4] C. Weaver, “Multidimensional Visual Analysis Using CrossFiltered Views,” IEEE Trans. Visualization and Computer Graphics,
vol. 16, pp. 192-204, Mar./Apr. 2010.
[5] M. Kulldorff, “A Spatial Scan Statistic,” Comm. Statistics: Theory
and Methods, vol. 26, pp. 1481-1496, 1997.
[6] R. Maciejewski, B. Tyner, Y. Jang, C. Zheng, R. Nehme, D.S. Ebert,
W.S. Cleveland, M. Ouzzani, S.J. Grannis, and L.T. Glickman,
“Lahva: Linked Animal-Human Health Visual Analytics,” Proc.
IEEE Symp. Visual Analytics Science and Technology (VAST), pp. 2734, Oct. 2007.
[7] A.D. Langmuir, “The Surveillance of Communicable Diseases of
National Importance,” New England J. Medicine, vol. 268, pp. 182192, 1963.
[8] S.B. Thacker, R.L. Berkelman, and D.F. Stroup, “The Science of
Public Health Surveillance,” J. Public Health Policy, vol. 10, pp. 187203, 1989.
[9] S.J. Grannis, M. Wade, J. Gibson, and J.M. Overhage, “The Indiana
Public Health Emergency Surveillance System: Ongoing Progress,
Early Findings, and Future Directions,” Proc. Ann. Symp. Am.
Medical Informatics Assoc., pp. 304-308, 2006.
[10] R. Maciejewski, R. Hafen, S. Rudolph, G. Tebbetts, W.S. Cleveland, S.J. Grannis, and D.S. Ebert, “Generating Synthetic Syndromic Surveillance Data for Evaluating Visual Analytics
Techniques,” IEEE Computer Graphics and Applications, vol. 29,
no. 3, pp. 18-28, May/June 2009.
[11] T. Butkiewicz, W. Dou, Z. Wartell, W. Ribarsky, and R. Chang,
“Multi-Focused Geospatial Analysis Using Probes,” IEEE Trans.
Visualization and Computer Graphics, vol. 14, pp. 1165-1172, Nov./
Dec. 2008.
[12] D. Guo, J. Chen, A.M. MacEachren, and K. Liao, “A Visualization
System for Space-Time and Multivariate Patterns (Vis-Stamp),”
IEEE Trans. Visualization and Computer Graphics, vol. 12, no. 6,
pp. 1461-1474, Nov. 2006.
[1]

[13] E. Hetzler and A. Turner, “Analysis Experiences Using Information Visualization,” IEEE Computer Graphics and Applications,
vol. 24, no. 5, pp. 22-26, Sept./Oct. 2004.
[14] T. Kapler and W. Wright, “Geotime Information Visualization,”
Proc. IEEE Symp. Information Visualization (INFOVIS ’04), pp. 25-32,
2004.
[15] J.V. Carlis and J.A. Konstan, “Interactive Visualization of Serial
Periodic Data,” Proc. 11th Ann. ACM Symp. User Interface Software
and Technology (UIST ’98), pp. 29-38, 1998.
[16] S. Havre, E. Hetzler, P. Whitney, and L. Nowell, “Themeriver:
Visualizing Thematic Changes in Large Document Collections,”
IEEE Trans. Visualization and Computer Graphics, vol. 8, no. 1, pp. 920, Jan.-Mar. 2002.
[17] P.C. Wong, R. Leung, N. Lu, M. Paget, J. Correia, Jr., W. Jian, P.
Mackey, T. Tayler, Y. Xie, J. Xu, S. Unwin, and A. Sanfilippo,
“Predicting the Impact of Climate Change on U.S. Power Grids
and Its Wider Implications on National Security,” Proc. AAAI
Spring Symp. Technosocial Predictive Analytics, pp. 148-153, 2009.
[18] J. Yuei, A. Raja, D. Liu, X. Wang, and W. Ribarsky, “A BlackboardBased Approach Towards Predictive Analytics,” Proc. AAAI
Spring Symp. Technosocial Predictive Analytics, pp. 154-161, 2009.
[19] L.C. Hutwagner, W.W. Thompson, and G.M. Seeman, “The
Bioterrorism Preparedness and Response Early Aberration Reporting System (EARS),” J. Urban Health, vol. 80, no. 2, pp. i89-i96, 2003.
[20] J.S. Lombardo, “A Systems Overview of the Electronic Surveillance System for the Early Notification of Community Based
Epidemics (ESSENCE II),” J. Urban Health, vol. 80, pp. 32-42, 2003.
[21] A.M. Mac Eachren, F.P. Boscoe, D. Haug, and L. Pickle,
“Geographic Visualization: Designing Manipulable Maps for
Exploring Temporally Varying Georeferenced Statistics,” Proc.
IEEE Symp. Information Visualization, p. 87, 1998.
[22] C. Tominski, P. Schulze-Wollgast, and H. Schumann, “3D
Information Visualization for Time Dependent Data on Maps,”
Proc. Ninth Int’l Conf. Information Visualisation (IV ’05), pp. 175-181,
2005.
[23] P.J. Diggle, Time Series Analysis: A Biostatistical Introduction. Oxford
Univ. Press, 1990.
[24] G. Box and G. Jenkins, Time Series Analysis: Forecasting and Control.
Holden-Day, 1970.
[25] B. Reis and K. Mandl, “Time Series Modeling for Syndromic
Surveillance,” BMC Medical Informatics and Decision Making, vol. 3,
p. 2, 2003.
[26] R.B. Cleveland, W.S. Cleveland, J. McRae, and I. Terpenning, “Stl:
A Seasonal-Trend Decomposition Procedure Based on Loess,”
J. Official Statistics, vol. 6, pp. 3-73, 1990.
[27] R.P. Hafen, D.E. Anderson, W.S. Cleveland, R. Maciejewski, D.S.
Ebert, A. Abusalah, M. Yakout, M. Ouzzani, and S. Grannis,
“Syndromic Surveillance: STL for Modeling, Visualizing, and
Monitoring Disease Counts,” BMC Medical Informatics and Decision
Making, vol. 9, 2009.

MACIEJEWSKI ET AL.: FORECASTING HOTSPOTS—A PREDICTIVE ANALYTICS APPROACH

[28] “Efficient Time Series Matching by Wavelets,” Proc. 15th Int’l Conf.
Data Eng. (ICDE ’99), p. 126, 1999.
[29] P.J. Diggle, Statistical Analysis of Spatial Point Patterns. Edward
Arnold, 2003.
[30] P.J. Diggle and P.J. Ribeiro, Model-Based Geostatistics. Springer,
2007.
[31] F.P. Boscoe, C. McLaughlin, M.J. Schymura, and C.L. Kielb,
“Visualization of the Spatial Scan Statistic Using Nested Circles,”
Health & Place, vol. 9, pp. 273-277, 2003.
[32] J. Chen, R.E. Roth, A.T. Naito, E.J. Lengerich, and A.M. Mac
Eachren, “Geovisual Analytics to Enhance Spatial Scan Statistic
Interpretation: An Analysis of U.S. Cervical Cancer Mortality,”
Int’l J. Health Geographics, vol. 7, no. 57, 2008.
[33] P. Diehr, “Small Area Statistics: Large Statistical Problems,” Am.
J. Public Health, vol. 74, pp. 313-314, 1984.
[34] K. Jones and A. Kirby, “The Use of Chi-Square Maps in the
Analysis of Census Data,” Geoforum, vol. 11, pp. 409-417, 1980.
[35] C.M. Wittenbrink, A.T. Pang, and S.K. Lodha, “Glyphs for
Visualizing Uncertainty in Vector Fields,” IEEE Trans. Visualization and Computer Graphics, vol. 2, no. 3, pp. 266-279, Sept. 1996.
[36] G. Grigoryan and P. Rheingans, “Point-Based Probabilistic
Surfaces to Show Surface Uncertainty,” IEEE Trans. Visualization
and Computer Graphics, vol. 10, no. 5, pp. 564-573, Sept. 2004.
[37] A. Cedilnik and P. Rheingans, “Procedural Annotation of
Uncertain Information,” Proc. Conf. Visualization (VIS ’00),
pp. 77-83, 2000.
[38] T. Hengl, “Visualization of Uncertainty Using the HSI Colour
Model: Computations with Colours,” Proc. Seventh Int’l Conf.
Geocomputation, 2003.
[39] C.A. Brewer, Designing Better Maps: A Guide for GIS Users. ESRI
Press, 2005.
[40] R. Hyndman, A. Koehler, and K. Ord, Forecasting with Exponential
Smoothing: The State Space Approach. Springer, 2008.
[41] W.S. Cleveland and S.J. Devlin, “Locally-Weighted Regression: An
Approach to Regression Analysis by Local Fitting,” J. Am.
Statistical Assoc., vol. 83, pp. 596-610, 1988.
[42] P.J. Brockwell and R.A. Davis, Introduction to Time Series and
Forecasting, second ed. Springer, 2003.
[43] B.W. Silverman, Density Estimation for Statistics and Data Analysis.
Chapman & Hall/CRC, 1986.
[44] M. Kulldorff and N. Nagarwalla, “Spatial Disease Clusters:
Detection and Inference,” Statistics in Medicine, vol. 14, pp. 799810, 1995.
[45] S.P. LLoyd, “Least Squares Quantization in PCM,” IEEE Trans.
Information Theory, vol. IT-28, no. 2, pp. 129-137, Mar. 1982.
[46] W.W. Hargrove and F.M. Hoffman, “Using Multivariate Clustering to Characterize Ecoregion Borders,” Computing in Science and
Eng., vol. 1. pp. 18-25, 1999.
[47] W.W. Hargrove and F.M. Hoffman, “Potential of Multivariate
Quantitative Methods for Delineation and Visualization of
Ecoregions,” Environmental Management, vol. 34, S39-S60, 2005.
Ross Maciejewski received the PhD degree in
2009 from Purdue University for his thesis
“Exploring Multivariate Data through the Application of Visual Analytics.” Currently, he is a
visiting assistant professor working as a member
of the Visual Analytics for Command, Control,
and Interoperability Environments, Department
of Homeland Security Center of Excellence. His
research interests include visual analytics, illustrative visualization, volume rendering, nonphotorealistic rendering, and geovisualization. He is a member of the IEEE
and the IEEE Computer Society.
Ryan Hafen received the master’s degree in
statistics from the University of Utah. He is
currently working toward the PhD degree in
statistics at Purdue University. His research
interests include exploratory data analysis and
visualization, massive data, computational statistics, time series modeling, and nonparametric
statistics.

453

Stephen Rudolph received the MS degree in
computer engineering from Purdue University.
He is currently working toward the master’s
degree in electrical computer engineering at
Purdue University. His research interests include casual information visualization and visual
analytics.

Stephen G. Larew is currently working toward
the undergraduate degree in electrical computer
engineering at Purdue University. His research
interests include GPGPU computing and visual
analytics. When he is not diligently researching
and studying, he likes to play sports, read, and
practice piano.

Michael A. Mitchell is currently working toward
the undergraduate degree in electrical computer
engineering at Purdue University. His research
interests include casual information visualization
and visual analytics. He is a member of the
National Society of Collegiate Scholars and the
Alpha Lambda Delta Phi Eta Sigma Honors
Society.

William S. Cleveland received the PhD
degree in statistics from Yale University. He
is the Shanti S. Gupta distinguished professor
of statistics and courtesy professor of computer science at Purdue University. His
research interests include statistics, machine
learning, and data visualization. He is the
author of The Elements of Graphing Data
(Hobart Press, 1994) and Visualizing Data
(Hobart Press, 1993).

David S. Ebert received the PhD degree in
computer science from Ohio State University.
He is a professor in the School of Electrical and
Computer Engineering at Purdue University, a
university faculty scholar, the director of the
Purdue University Rendering and Perceptualization Lab, and the director of the Purdue
University Regional Visualization and Analytics
Center. His research interests include novel
visualization techniques, visual analytics, volume rendering, information visualization, perceptually based visualization, illustrative visualization, and procedural abstraction of complex,
massive data. He is a fellow of the IEEE and a member of the IEEE
Computer Society Publications Board.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

DOI: 10.1111/cgf.12886
Eurographics Conference on Visualization (EuroVis) 2016
K.-L. Ma, G. Santucci, and J. van Wijk
(Guest Editors)

Volume 35 (2016), Number 3

Visualizing the Impact of Geographical Variations on Multivariate
Clustering
Y. Zhang1 , W. Luo2 , E. A. Mack3 , R. Maciejewski1
1 Arizona
2 University

3

State University, Tempe, USA
of California, Santa Barbara, USA

Michigan State University, East Lansing, USA

Abstract
Traditional multivariate clustering approaches are common in many geovisualization applications. These algorithms are used
to define geodemographic profiles, ecosystems and various other land use patterns that are based on multivariate measures.
Cluster labels are then projected onto a choropleth map to enable analysts to explore spatial dependencies and heterogeneity
within the multivariate attributes. However, local variations in the data and choices of clustering parameters can greatly impact
the resultant visualization. In this work, we develop a visual analytics framework for exploring and comparing the impact of
geographical variations for multivariate clustering. Our framework employs a variety of graphical configurations and summary
statistics to explore the spatial extents of clustering. It also allows users to discover patterns that can be concealed by traditional
global clustering via several interactive visualization techniques including a novel drag & drop clustering difference view.
We demonstrate the applicability of our framework over a demographics dataset containing quick facts about counties in the
continental United States and demonstrate the need for analytical tools that can enable users to explore and compare clustering
results over varying geographical features and scales.
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Applications—

1. Introduction
Traditionally, multivariate clustering has been applied to measurements aggregated over various geographical areas (e.g., counties, states, etc.) in order to identify similar and dissimilar regions. Examples include clustering vegetation measures to define regional ecosystems [MHKH11], clustering common surnames to generate cultural demographic maps [CML11], and clustering socioeconomic demographic measures to identify at risk
neighborhoods [AL05]. The most common clustering methods applied include k-means (e.g., [MHKH11]), hierarchical clustering
(e.g., [CML11]), and self-organizing maps (e.g., [CMG08]). These
methods are usually, but not always (e.g., [MJ07]), applied at a
level that is agnostic to the spatial relationships between the data
(i.e., the positions of the regions are not used as features in the
clustering method). As such, local geographic variations may be
obscured in a global clustering approach, and the quality of the
clustering results needs to be explored locally and globally.
Previous work in geographical visualization has focused on a
variety of techniques designed for analyzing multivariate relationships in spatiotemporal data. Systems in this area commonly utilize
coordinated multiple views [Rob07] in which users can observe
patterns of multivariate data in scatter plots [And72], parallel coordinate plots [Ins85] and other visual representations and then inc 2016 The Author(s)

c 2016 The Eurographics Association and John
Computer Graphics Forum 
Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

teractively select areas in those views that will be highlighted on
a map. These systems have been deployed for a variety of different application domain areas and utilize a number of different
analytical algorithms. For example, work by von Landesberger et
al. [vLBA∗ 12] presented an approach for classifying spatiotemporal categorical data supported by algorithms for the selection of
globally and focally representative time steps based on categorical
changes, and Goodwin et al. [GDST16] developed a suite of novel
interactive visualization methods to identify interdependencies in
multivariate data coupled with a series of correlation matrix views.
While there are a variety of visual analytics frameworks designed to enable the exploration of multivariate spatiotemporal
data, previous methods focus primarily on the visual inspection of
the quality of the clustering result. In this paper, we present a visual analytics framework that links users to a variety of clustering
quality measurements to enable them to develop an understanding
of global and local multivariate clustering for geographical visualization. Our framework supports the visual exploration of geographical projections of both k-means and hierarchical clustering.
Clustering comparisons and rose plots are provided to assess cluster
quality, and interactive manipulation and selection allows users to
dynamically apply multivariate clustering to different geographical
locations, features and scales. We showcase our approach using real

102

Y. Zhang & W. Luo &E. A. Mack & R. Maciejewski / Visualizing the Impact of Geographical Variations on Multivariate Clustering

world data from the US Census Bureau and demonstrate how our
approach helps in understanding geographical variations in multivariate clustering, ultimately enabling the discovery of patterns that
may be concealed.
2. Related Work
Our work aims to integrate fundamental theories from geography
into multivariate spatial clustering through a geovisual analytics
framework. In this section we review the relevant fields describing
the characteristics of spatial processes, current multivariate geovisualization techniques, and clustering comparison methods in order
to frame our contribution with respect to the current state-of-the-art.
2.1. Geographical Variation
Data generating processes associated with spatial data are often characterized as spatial dependence or spatial heterogeneity [Ans88]. Spatial dependency refers to the similarity in attribute
values of nearby spatial units [Ans88] as proposed in Tobler’s first
law of geography [Tob70]. In contrast, Spatial heterogeneity or
nonstationarity refers to variation rather than similarity in values
for a particular measures across all spatial units [BFC96]. Spatial
stationarity is often assumed in statistical analyses, but this is problematic in the presence of spatial heterogeneity where assumptions
of a global trend do not reflect the underlying data generating processes [BFC96]. As such, the diagnosis of local dependence and
heterogeneity is particularly valuable to understanding statistical
output.
Due to this persistent issue in spatial data, tools such
as the Moran scatterplot [Ans93], local indicators of spatial
association [Ans95], and geographically weighted regression
(GWR) [BFC98] are critical to diagnosing outliers which might
otherwise be obscured in global and local statistics not designed
to diagnose spatial heterogeneity. The development of the local
Moran’s I and GWR in particular were critical to analyses of spatial
data because prior local statistics including the G statistic [GO92]
and the G* statistic [OG95] are not capable of assessing spatial
heterogeneity in the form of local outliers.
The inability to diagnose spatial dependence and heterogeneity
is an issue with current aspatial multivariate clustering techniques.
Currently, the most common approaches for evaluating multivariate
clustering is to assign a label/class/category to each observation in
the dataset. Post-classification, a choropleth map can then be generated from clustering results, from which, the analyst can begin
attempt to diagnose local and global spatial dependence. For example, Turkay et al. [TSH∗ 14] have developed methods for exploring
geographically referenced multivariate data over location and scale
through a variety of linked small multiples and summary statistics.
However, research highlights that visual inspection of maps can be
misleading [Mon14] and that specific local statistics are necessary
to diagnose dependence and heterogeneity [Ans95]. Research has
also highlighted that multivariate results when mapped, can produce non-sensical results [MGK07] because closeness in multivariate space is not necessarily the same as closeness in geographic
space. Thus, the design of toolkits to examine spatial processes in
clustering results that move beyond potentially misleading visual

inspection of maps and global summary statistics that obscure important local variations in multivariate data is necessary.
2.2. Multivariate Geovisualization
Previous work in geographical visualization has focused on a variety of techniques designed for analyzing multivariate relationships
in spatial data. One well known example is the UK National Statistics Output Area Classification (OAC) which is an open geodemographic classification with a hierarchical structure of 7 supergroups, 21 groups and 52 subgroups [VR07]. This type of multivariate clustering has served as the basis for various geovisualization techniques. For example, Slingsby et al. [SDW10] developed rectangular hierarchical cartograms for mapping socioeconomic data of OAC, and also proposed a set of interactive visualization techniques to explore population profiles of areas and
how uncertainty in OAC varies geographically and by OAC category [SDW11]. Singleton and Longley [SL15] presented a London classification (LOAC) based upon the OAC methodology to
accommodate local structures that diverge from national patterns.
While Singleton and Longley compared the model of LOAC to
the 2011 OAC in a statistical and semantic manner, our work explores quantitative and visual comparison of clustering outputs.
As for localized geographical exploration, various geographically
weighted (GW) statistics have been developed (e.g., GW summary
statistics [BFC02], GWR [BFC98], GWPCA [HBC11]). Dykes and
Brunsdon [DB07] introduced geographically weighted interactive
graphics for exploring and hypothesizing the spatial relationships
under different scale-based variations. Goodwin et al. [GDST16]
developed a suite of novel interactive visualization methods to identify interdependencies in multivariate data coupled with a series of
correlation matrix views. While Goodwin et al. focus primarily on
spatial extents of pairwise correlations, our work explores spatial
extents in the multivariate clustering space and enables exploratory
analysis between clustering differences. Previous work by Lex et
al. [LSP∗ 10] also explores comparing clustering results; however,
their domain focus is on genomics which does not have issues related to spatial extent.
2.3. Clustering Comparison
For many clustering evaluation and comparison techniques, researchers assume a true cluster structure exists and use an external criteria of clustering quality, such as the Rand index [Ran71] or
NMI (Normalized Mutual Information) [MRS08] to measure the
concordance between the true structure and the output of the clustering algorithms [KF75,FH09,FLK11]. Jung et al. defined clustering gain which is based on the squared error sum as a measure for
cluster optimality [JPDD03]. Their measurements can be utilized
to estimate the desired number of clusters for partitional clustering
methods. Meilǎ [Mei05] characterized criteria for comparing two
clustering results directly by treating clusters as elements of a lattice. However, those works still remain at the arithmetic level (i.e.,
only numerical indicators have been provided and no visual information is available for illustration). Hoffman and Hargrove created
simple multivariate geographic clustering comparison according to
their state space color assignments [HHDG03], yet they do not apply a uniform comparison method. Recently, Zhou et al. [ZKG09]
c 2016 The Author(s)

c 2016 The Eurographics Association and John Wiley & Sons Ltd.
Computer Graphics Forum 

Y. Zhang & W. Luo &E. A. Mack & R. Maciejewski / Visualizing the Impact of Geographical Variations on Multivariate Clustering

103

Figure 1: An example of the clustering exploration approaches provided in our framework. (A) Choropleth clustering map with the thumbnail
plot displaying the clustering criteria. (B) Scatterplot in PCA projection mode with scope lens enabled. (C) PCP area profiler of the data
values of the local highlighted counties. (D) PCP area profiler of the data values from all counties. (E) PCP area profiler showing the data
values from counties of the blue cluster. (F) Rose plots of all five clusters.

extended parallel sets to provide the mutual comparison and evaluation of multiple partitions. Their visualization can present the
overall change between clusterings but may be not suitable for
showing the detail of changes in geographical applications. Hu et
al. [HKV12] described a heuristic to promote dynamic cluster stability and maximize stability between labels. Their approach for
visualizing multiple relationships ensures mental map preservation
but lacks the capability to show detailed local comparison. In this
work, we expand on these methods and provide a Triple-D (Drag &
Drop clustering Difference) View to interactively display the legible visual results for clustering comparison.

3. Framework and Design
In order to merge previously aspatial interactive multivariate clustering techniques with local techniques to evaluate trends in spatial
data, we have developed a framework that enables the manipulation
and exploration of spatial data in a multivariate clustering environment. To address important differences in local trends related to
either spatial dependence or spatial heterogeneity, our work characterizes space in the following four categories:
• Discrete spatial extent - Particular types of data may be reported
in such a way that they are bounded by a fixed spatial extent. Previous research tends to apply multivariate clustering to the entire
spatial extent, which can conceal local variations. The proposed
framework enables geographical selections at a constant spatial
extent and allows users to apply multivariate clustering to the selected spatial extent. The interaction and visual encoding enables
users to identify local patterns between places in order to understand the impact of the spatial heterogeneity on the multivariate
clustering procedure.
• Discrete geographical features - Different geographical features
are not always spatially continuous. Thus the proposed framework allows users to distinguish geographical features (e.g., urban vs. rural) and then apply multivariate clustering to the geographical features of interest.
• User-defined (continuous) spatial extent - While many geographic studies examine phenomenon where the spatial extent
is fixed, many other questions require an analyst to modify the
c 2016 The Author(s)

c 2016 The Eurographics Association and John Wiley & Sons Ltd.
Computer Graphics Forum 

spatial extent by zooming in to a particular set of spatial units or
zooming out to a particular extent, and then performing a cluster
analysis. In this context, the arrangement and the neighborhood
structure of the data are variable. The proposed framework allows users to adjust spatial scales around a fixed location to understand the impact of the spatial dependence on the multivariate
clustering procedure.
• Continuous geographic resolution - Another issue to consider
when evaluating multivariate clustering results in geographic
space is the impact on the results of varying the resolution of
the data. This variation in the resolution of the spatial units of
interest is otherwise known as the problem of modifiable areal
units [Ope83]. We know that using larger areal units (i.e. states as
opposed to counties) reduces the variance in the data [BBR09].
The proposed tool allows users to aggregate multivariate attributes at different spatial resolutions (e.g., county, state) to understand the impact of the spatial dependence on the multivariate
clustering procedure.
3.1. Group Selection
To enable the visual analysis of the spatial impact on multivariate
clustering, our framework extends the traditional selection operation through the concept of group operations. Our framework has
fully implemented three types of selection:
• Rubber band selection in geographic space;
• Selections from multivariate space utilizing the histogram, scatterplot, categorical view, and box plot (note that all views mentioned are fully implemented in the system);
• Automated geographical selections such as selecting based on a
boundary layer or using a neighborhood.
These three selection methods enable users to define any desired areas. The group level operations include updating all the exploratory
data analysis widgets (e.g, histogram, scatterplot), applying local
clustering, and aggregating local clustering statistics.
3.2. Clustering Exploration
Due to the often non-intuitive connection between multivariate
space and geospace, it is a challenge to simultaneously explore the

104

Y. Zhang & W. Luo &E. A. Mack & R. Maciejewski / Visualizing the Impact of Geographical Variations on Multivariate Clustering

Figure 2: Here we demonstrate the coherent clustering color mapping when both maps have five clusters. By maintaining label consistency
for generalized clustering comparison, users can quickly tell that the clustering results are similar while at the same time noting that there
exists differences in the northern part of the US (the red circle). However, it is still difficult for users to figure out exactly how many differences
there are.
clustering result in both multivariate space and geographical space.
We provide several criteria in order to help users assess the clustering results. The first criterion is the silhouette coefficient [Rou87]
for multidimensional space. The silhouette coefficient refers to a
method of interpreting clusters which allows users to know how
well each object lies within its cluster and is defined as:
S(i) =

b(i) − a(i)
,
max{a(i), b(i)}

where a(i) is the average dissimilarity of object i with all other
objects within the same cluster, and b(i) is the lowest average dissimilarity of object i to any other clusters where object i is not a
member, and S(i) lies in the range: −1 ≤ S(i) ≤ 1. When S(i) is
close to 1, it means the datum is appropriately clustered. When S(i)
is close to -1, it means object i would be more appropriate labeled
if it was clustered in one of its neighboring clusters. When S(i) is
close to zero, it means that the datum is on the border of two natural clusters. Therefore, we leverage this coefficient as a means of
assessing the goodness of a clustering result.
Users may also be interested in the relativeness of cluster labels
within a neighboring area. We use the Gini index-like [Gin12] purity indicator as the second criterion for geographic multivariate
clustering inspection. The purity indicator is defined as:
nC j 2
nC
P(i) = ( i )2 − ∑ (
) ,
N(i)
N(i)
C 6=C
i

j

where nCi is the number of units that belong to the same cluster of i
(Ci ) in i’s neighborhood, nC j is the number of units that are different
from i’s cluster in i’s neighborhood, N(i) is the total count of units
of i’s neighbors, and P(i) also lies in the range: −1 ≤ P(i) ≤ 1.
When P(i) is close to 1, it means that unit i is almost surrounded
by the neighbors within the same cluster. When P(i) is close to -1,
it means that unit i is almost surrounded by the neighbors from another cluster. When P(i) is near zero, it means its neighbors are randomly scattered in different clusters. Thus, the higher purity value
a unit has, the stronger the spatial association is around that unit.
These indicies can be displayed in the thumbnail plot when users
hover the mouse over a geographical unit (Figure 1(A)), where the

summarized value will be displayed in the group view. In addition to the numerical criteria, our framework provides three visual
analytics methods for clustering exploration: PCA (Principle Component Analysis) scatterplot, PCP (Parallel Coordinate Plots) area
profiler, and rose plot.
PCA Scatterplot: We utilize PCA to project data into 2D space
and provide users a generalized overview of how the clusters are
distributed. For instance, from the PCA scatterplot (Figure 1(B)),
we can quickly tell where the point is and on which cluster border
the point is lying on.
PCP Area Profiler: While PCA is good for visualizing the multivariate distance, it lacks consistency in the appearance as the data
change. Hence we implement a PCP area profiler that can visualize the multivariate relations of different area profiles in a simple
click. There is one customizable area profile where users can select the units of interest and three predefined area profiles: the local
neighboring area which only considers the units within the first order contiguity of the selected unit, the intra-cluster area which only
considers the units in the same cluster as the selected unit, and the
global area which considers all the units. By switching among those
area profiles, users can explore how the datum is distributed in the
multidimensional space (Figure 1(C- E)).
Rose Plot: While PCPs provide a detailed view of the data values,
they are often very cluttered. We employ a modified version of the
traditional rose plot (Figure 1(F)) akin to Schreck et al. [SOBL13].
Each variable axis owns five points which indicate lower bound,
three quartiles, and upper bound of each variable respectively.
While Gestalt principles note that humans are good at shape comparison, drawbacks of the rose plot include shape changes due to
axis ordering and the often unintuitive scaling that must be done
per axis.

3.3. Clustering comparison
To enhance the coherence and generalized cluster comparison between different clustering results, Hu et al. [HKV12] proposed the
coherent clustering color mapping that attempts to keep cluster labels of spatial units consistent between different clustering results.
c 2016 The Author(s)

c 2016 The Eurographics Association and John Wiley & Sons Ltd.
Computer Graphics Forum 

Y. Zhang & W. Luo &E. A. Mack & R. Maciejewski / Visualizing the Impact of Geographical Variations on Multivariate Clustering

105

Figure 4: An example of the Triple-D View (Drag and Drop clustering Difference View). The top three maps are three different clustering results using K-means but with different initial centroids respectively. The bottom two maps are the comparison results of the
first two and last two respectively. When users click on a certain
unit in the comparison result, indicator lines will be drawn on top
of them to mark the corresponding units from the two compared
clustering results.
Figure 3: Comparing two clustering results for the same group of
15 objects. In the top figure, the left part is a clustering result Ω
with three clusters, and the right part is a clustering result Ω0 with
four clusters. The bottom figure is the illustration of the comparison
process. The value on the arrow indicates the proportion of the subcluster in that step.

They assign the same label (color) to the clusters with the maximum number of correspondences to facilitate the comparison of
clusters. However, this method has few drawbacks: it lacks a detail
comparison capability (in Figure 2, users can not tell the exact difference between two clustering results when the amount of spatial
units or the number of clusters is large); it can only compare clustering with the exact same cluster numbers and units. To overcome
such issues, we have designed a novel visual analytics tool called
the Triple-D View (Drag and Drop clustering Difference View).
Each cluster is essentially a set, thus comparing the difference
between clusterings is equivalent to exploring the changes among
those sets. According to observations, we generalize the changes
into a combination of the splitting step and the merging step. To
keep the idea simple, consider the example in Figure 3. Here, we
have a clustering result Ω of 3 clusters A, B,C for 15 objects on the
left, and another clustering result Ω0 of 4 clusters A0 , B0 ,C0 , D0 for
those same 15 objects on the right. In the splitting step, we subdivide Ω into small clusters. For instance, for cluster A, objects 1, 3
are formed into the same cluster in Ω0 , objects 4, 6, 7, 9 are formed
into the same cluster in Ω0 , and object 14 is merged into another
cluster in Ω0 . Thus we will have three sub-clusters in Ω00 for cluster A. In the merging step, we just need to check each cluster in Ω0
to find out which small sub-clusters in Ω00 it contains. The interc 2016 The Author(s)

c 2016 The Eurographics Association and John Wiley & Sons Ltd.
Computer Graphics Forum 

mediate clusters are actually the mutual information between these
two clustering results. This process is demonstrated in Figure 3.
By dragging one clustering result and dropping it onto another
clustering result in the Triple-D view, the Triple-D view will map
the changes (i.e., the intermediate sub-clusters) under the two
clusterings being compared (Figure 4). The layout of our difference view is an inverted pyramid which is similar to the GTdiff
method [HWH∗ 11], yet GTdiff only provides comparison for temporal bins as a difference of values between time steps. Here we
utilize this layout to show the difference between different clustering results. To represent the changes, we define three criteria for the
proportion: less than 50 percent, larger than 50 percent, and equals
to 100 percent. The proportion in the splitting step refers to the ratio
between the size of the sub-cluster and the size of its original cluster where the sub-cluster splits from, in the merging step it refers
to the ratio between the size of the sub-cluster and the size of its
successive cluster which the sub-cluster merged into. As there are
three criteria for both steps, there will be 9 variables that can be
used to represent the observed changes.
The Triple-D view not only can visualize the difference between
clustering results regardless of the cluster number and coloring
scheme, but also can generate a numerical proximity metric that
obeys all the metric properties (positivity, symmetry, triangle inequality, indiscernibility) to help users assess the clustering similarity. In contrast, the Rand Index is not suitable for unlabeled
clustering comparison as it requires a ground truth, NMI (Normalized Mutual Information)/Variation of Information can not handle
the situation when mutual information is 0, and the similarity measure introduced by Torres et al. [TBS∗ 09] does not provide diversity/entropy information for the comparison which make the result

106

Y. Zhang & W. Luo &E. A. Mack & R. Maciejewski / Visualizing the Impact of Geographical Variations on Multivariate Clustering

Figure 5: An example of exploration between clusterings in different geographical locations. The top and bottom row are results based on
surroundings of King County and Harris County respectively. (A) Selections with the circle selection tool. (B) Local clustering results and
its clustering statistics. (C) Rose plots for the local clusters, the variables from the top in a clock-wise manner are: percentage of other
languages, percentage of education level above high school, mean time to work, and per capita income.

less meaningful. So our split-merge metric is defined as:
0

SM(Ω, Ω00 ) = − ∑ ∑
i

j

0

|Ci ∩C j |
|Ci ∩C j |2
log
,
0
N
|Ci ||C j |

where Ω and Ω00 are the two clustering results been compared. Ci
0
is the ith cluster in Ω, C j is the jth cluster in Ω00 , and N is the total
number of units. Larger metric values represent more dissimilarity
between clusterings. Potential future work could explore the use of
recent set visualization methods (e.g., OnSet [SMDS14]) as a novel
means of comparing clustering results.

4. Case Studies
In the following case studies, we used a demographics data set containing quick facts about counties in the continental United States
from the US Census Bureau (http://quickfacts.census.gov/
qfd/download_data.html). There are 3106 counties in this
dataset and 52 demographic variables. The counties are distinguished by their FIPS (Federal Information Processing Standards)
number, and the variables by their mnemonic identifier. Note that
variable choices here are chosen to clearly highlight observable patterns in the data and demonstrate our framework features.

4.1. Discrete spatial extent
POP815213 (Language other than English spoken percentage),
EDU635213 (Education level above high school), LFE305213
(Mean travel time to work), and INC910213 (Per capita income)
are the variables of interest in this case study. Here we explore
the surroundings of King County (Seattle) and the surroundings
of Harris County (Houston) to determine if they share any common patterns in clustering. We first choose the surrounding counties within the same radius from both counties using the circle selection tool (Figure 5(A)) and then apply hierarchical clustering using Ward’s method [War63] and Euclidean distance (Figure 5(B)).
The relatively high value of the average Silhouette coefficient from
King County’s clustering indicates the goodness of its clustering is
slightly better than Harris County’s clustering under the same clustering method. From the rose plots (Figure 5(C)), we also notice
that the characteristics of each cluster in King County’s clustering
are more distinguishable. As King County belongs to the orange
cluster, we identify that it possesses more well-educated people,
higher income, and requires more time travel to work when compared with the other clusters from the rose plots (Figure 5(C) Top).
The difference of cluster characteristics is small from King County
westward, but is large eastward as these counties have significant
less travel time to work and more non-English language speakers.
c 2016 The Author(s)

c 2016 The Eurographics Association and John Wiley & Sons Ltd.
Computer Graphics Forum 

Y. Zhang & W. Luo &E. A. Mack & R. Maciejewski / Visualizing the Impact of Geographical Variations on Multivariate Clustering

107

Figure 6: An example of partial clustering for units of different geographical feature. The left map shows the units with all positive population
change and the right map shows the units with all negative population change in 2014.

Harris County is similar to the situation in King County; however,
the difference of cluster characteristics does not shows the similar
west-east pattern. We can tell the non-English spoken percentage
drops significantly towards the north but remains towards the south,
which makes sense as Mexico boarders Harris County in the south.
Interestingly, the other two values of Shannon diversity and average purity from Harris County’s clustering suggest that the clusters
in Harris County’s clustering are more spatially associated and the
clusters in King County’s clustering are more scattered. The spatial
distribution of the clustering results in Figure 5(B) display the corresponding patterns. Though these two counties are both near the
ocean, close to the country border, and contain major metropolitan areas, the different styles in population composition, education
level and traffic level of its surroundings counties appear to contribute to different spatial patterns.
4.2. Discrete geographical features
We noticed that variable PST120214 representing the percent
change of population in 2014 has both negative and positive values, so we explore clustering on units with positive values and
negative values respectively. We first use the scatterplot (or categorical view) to select units with only positive values and then
apply hierarchical clustering using Ward’s method and Euclidean
c 2016 The Author(s)

c 2016 The Eurographics Association and John Wiley & Sons Ltd.
Computer Graphics Forum 

distance. The number of clusters chosen is 6 and the variables clustered on are SEX255213 (Percent of female in 2013), POP645213
(Percent of foreign born persons in 2013), EDU635213 (Percent
of persons with high school graduate or higher), INC910213 (Per
capita money income in 2013), and PVY020213 (Percent of persons below poverty level). Then we repeat the same process for
the units with only negative percent change of population. Figure 6
demonstrates both the clustering results for units with positive and
negative population change.
After investigating the rose plot for each clustering result, we
find a similar matching pattern based on the clusters’ characteristics. As shown in Figure 6, a matching of A ⇔ e/c, B ⇔ d, C ⇔ a,
D ⇔ b, E ⇔ f can be easily identified from the rose plot of each
cluster. This means that spatial units with positive and negative
population changes do share some similar patterns with those 5
variables. We also notice that only the F cluster does not have a
matching cluster in the other clustering result. Cluster F has the
highest distribution in education level, income level and foreign
born persons level. Also, judging from the map, we can tell that the
units from cluster F are all major metropolitan areas such as Los
Angeles, New York City, etc. We hypothesize that areas with high
income levels, education levels and foreign born population will
have positive population change, in other words, areas with nega-

108

Y. Zhang & W. Luo &E. A. Mack & R. Maciejewski / Visualizing the Impact of Geographical Variations on Multivariate Clustering

Figure 7: An example of scale effect on clustering around Cook County. (A) The Triple-D view of the clustering comparison regarding the
scale change (B) The change of scales demonstrated in 4 colors (C) PCP area profiler for scale 1 and 2 (D) PCP area profiler for scale 1
and 3 (D) PCP area profiler for scale 1 and 4.

tive population change usually do not have high income, education
and foreign born persons level.
4.3. Continuous spatial extent
Next, we explore the scale change effects in clustering. The clustering features 6 clusters with Ward’s method and Euclidean distance. It is based on the variables of age (percents of person under 5, under 18, and above 65 respectively), house living(percent
of living in same house more than 1 year), and education (percent
of person have bachelor’s degree or higher). We first choose Cook
County (the Chicago metro area). We select the start scale and end
scale as shown in the Figure 7(A-1) and (A-4) respectively. Then
the framework automatically interpolates the steps between those
two scales (Figure 7 (A-2)(A-3)). After clustering on each of the
scales and applying the comparison in the Triple-D view, we find
out that the comparison metrics which stands for dissimilarity are
slowly increasing as the scale changes (Scores circled in red in Figure 7(A)). Thus we label the change of scales in different colors
(Figure 7(B)) and visualize the difference between them with PCP
area profiler (Figure 7(C)(D)(E)). As shown in the PCP area profiler, Cook County and its neighboring counties have a higher measurement in the % of population under 18 and in the education variable (Darker Green lines as scale 1 in Figure 7(C)). When the scale
expands outward to the next contiguous set of neighbors, the outer
counties have higher percent of elderly and the education level goes
down (Light Green lines as scale 2 in Figure 7(C)). As the characteristics of these two area profilers are quite distinguishable, it
means that the scale change from A-1 to A-2 (B-2) may have less
effects on the clustering of A-1. That also explains the indiscrimination of the comparison between the scale A-1 and A-2’s clustering

results. When the spatial extent increases, more units that are similar to the units in scale A-1 have been induced (Overlapping light
green lines in Figure 7(D)), and that interferes with the clustering
results. Note that clustering results here are further confounded due
to variables being dependent proportions of the population.
4.4. Continuous geographical resolution
As shown in Figure 8, we applied the hierarchical clustering
of 6 clusters on the county level and state level respectively
across the mainland US. The clustering uses the Education level
(EDU685213), Veterans (VET605213), Mean travel time to work
(LFE305213), and Private nonfarm employment (BZA110213). We
can tell that county level rose plots (Figure 8(a)) have more variance for each of the clusters. The PCA scatterplot also shows more
overlap at the county level, but clear separation in the state level.
From the statistics of the two clustering results, the average Silhouette coefficient of clusters under state level is higher than under
county level which indicates higher intra-cluster similarity at the
state level aggregation. These observations in cooperation with the
work of Sun [SW15] demonstrate that spatial aggregation can improve data quality.
5. Conclusion and Future Work
This paper presents a geovisual analytics framework to allow users
to understand the impact of geographical variations across locations and scales for multivariate data clustering. We categorize the
space into four aspects: discrete spatial extent, discrete geographical features, continuous spatial extent, and continuous geographical resolution in order to characterize the impact of spatial dec 2016 The Author(s)

c 2016 The Eurographics Association and John Wiley & Sons Ltd.
Computer Graphics Forum 

Y. Zhang & W. Luo &E. A. Mack & R. Maciejewski / Visualizing the Impact of Geographical Variations on Multivariate Clustering

109

Figure 8: An example of clustering results under different geographical resolution. (A) Clustering result, corresponding rose plot and
PCA scatterplot in county level, (B) Clustering result, corresponding rose plot and PCA scatterplot under state level. Both use the same
hierarchical clustering with 6 clusters.

pendence and heterogeneity. A variety of visualization and interaction techniques (e.g., PCA scatterplot, PCP area profiler, Rose plot)
have been implemented to facilitate clustering exploration over geographical variations with statistical measures (e.g., Silhouette coefficient) to evaluate cluster quality. We provide methods for comparing within (k-means vs. k-means) and between (hierarchical vs.
k-means) cluster results, and demonstrate potential ways of interacting with data to explore cluster results.
While this framework enables the exploration and comparison
of clustering methods over different scales, there is still a need to
enable quick identification of similar and dissimilar regions. Currently, the comparative analysis between clusters is done in a purely
visual manner, and while humans are capable of identifying patterns, the integration of further analytical methods to help highlight
and identify statistically significant similarities and differences between clusters is critical. Furthermore, this exploration focused primarily on the spatial extent of the data; however, extensions to
the spatiotemporal domain are critical in analyzing how underlying
physical properties may develop in the data. It may also be possible to automatically explore the impact of scale simply by defining
levels of aggregation and present a summary comparison to end
users to suggest appropriate scales of analysis for the data. Future
work should explore a combination of automation with human-inthe-loop exploration and recommendations.
6. Acknowledgments
This work was supported by the NSF under Grant No. 1350573
and in part by the U.S. Department of Homeland Security’s VACc 2016 The Author(s)

c 2016 The Eurographics Association and John Wiley & Sons Ltd.
Computer Graphics Forum 

CINE Center under Award Number 2009-ST-061-CI0001. We
thank the anonymous reviewers whose comments helped improve
the manuscript.
References
[AL05] A SHBY D. I., L ONGLEY P. A.: Geocomputation, geodemographics and resource allocation for local policing. Transactions in GIS
9, 1 (2005), 53–72. 1
[And72] A NDREWS D. F.: Plots of high-dimensional data. Biometrics
(1972), 125–136. 1
[Ans88] A NSELIN L.: Spatial econometrics: Methods and models, vol. 4.
Springer Science & Business Media, 1988. 2
[Ans93] A NSELIN L.: The Moran scatterplot as an ESDA tool to assess
local instability in spatial association. Regional Research Institute, West
Virginia University Morgantown, WV, 1993. 2
[Ans95] A NSELIN L.: Local indicators of spatial association - LISA.
Geographical Analysis 27, 2 (1995), 93–115. 2
[BBR09] B URT J. E., BARBER G. M., R IGBY D. L.: Elementary statistics for geographers. Guilford Press, 2009. 3
[BFC96] B RUNSDON C., F OTHERINGHAM A. S., C HARLTON M. E.:
Geographically weighted regression: A method for exploring spatial
nonstationarity. Geographical analysis 28, 4 (1996), 281–298. 2
[BFC98] B RUNSDON C., F OTHERINGHAM S., C HARLTON M.: Geographically weighted regression. Journal of the Royal Statistical Society:
Series D (The Statistician) 47, 3 (1998), 431–443. 2
[BFC02] B RUNSDON C., F OTHERINGHAM A., C HARLTON M.: Geographically weighted summary statistics - a framework for localised exploratory data analysis. Computers, Environment and Urban Systems 26,
6 (2002), 501–524. 2

110

Y. Zhang & W. Luo &E. A. Mack & R. Maciejewski / Visualizing the Impact of Geographical Variations on Multivariate Clustering

[CMG08] C HEN J., M AC E ACHREN A. M., G UO D.: Supporting the process of exploring and interpreting space–time multivariate patterns: The
visual inquiry toolkit. Cartography and geographic information science
35, 1 (2008), 33–50. 1
[CML11] C HESHIRE J., M ATEOS P., L ONGLEY P. A.: Delineating Europe’s cultural regions: Population structure and surname clustering. Human Biology 83, 5 (2011), 573–598. 1
[DB07] DYKES J., B RUNSDON C.: Geographically weighted visualization: Interactive graphics for scale-varying exploratory analysis. IEEE
Transactions on Visualization and Computer Graphics 13, 6 (2007),
1161–1168. 2
[FH09] F ERREIRA L., H ITCHCOCK D. B.: A comparison of hierarchical
methods for clustering functional data. Communications in StatisticsSimulation and Computation 38, 9 (2009), 1925–1949. 2
[FLK11] FATTAH S. A., L IN C.-C., K UNG S.-Y.: A mutual information
based approach for evaluating the quality of clustering. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
(2011), IEEE, pp. 601–604. 2
[GDST16] G OODWIN S., DYKES J., S LINGSBY A., T URKAY C.: Visualizing multiple variables across scale and geography. IEEE Transactions on Visualization and Computer Graphics 22, 1 (2016), 599–608. 1,
2
[Gin12]

G INI C.: Variabilità e mutabilità. 1912. 4

[GO92] G ETIS A., O RD J. K.: The analysis of spatial association by use
of distance statistics. Geographical analysis 24, 3 (1992), 189–206. 2
[HBC11] H ARRIS P., B RUNSDON C., C HARLTON M.: Geographically
weighted principal components analysis. International Journal of Geographical Information Science 25, 10 (2011), 1717–1736. 2
[HHDG03] H OFFMAN F. M., H ARGROVE W. W., D EL G ENIO A. D.:
Multivariate spatio-temporal clustering of time-series data: An approach
for diagnosing cloud properties and understanding ARM site representativeness. In Thirteenth ARM Science Team Meeting Proc., Broomfield,
Colorado (2003). 2

[Mon14] M ONMONIER M.: How to lie with maps. University of Chicago
Press, 2014. 2
[MRS08] M ANNING C. D., R AGHAVAN P., S CHÜTZE H.: Introduction
to information retrieval, vol. 1. Cambridge university press, 2008. 2
[OG95] O RD J. K., G ETIS A.: Local spatial autocorrelation statistics:
distributional issues and an application. Geographical analysis 27, 4
(1995), 286–306. 2
[Ope83] O PENSHAW S.: The modifiable areal unit problem. Norwick
[Norfolk]: Geo Books, 1983. 3
[Ran71] R AND W. M.: Objective criteria for the evaluation of clustering
methods. Journal of the American Statistical association 66, 336 (1971),
846–850. 2
[Rob07] ROBERTS J. C.: State of the art: Coordinated & multiple views
in exploratory visualization. In Fifth International Conference on Coordinated and Multiple Views in Exploratory Visualization (2007), IEEE,
pp. 61–71. 1
[Rou87] ROUSSEEUW P. J.: Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of computational and
applied mathematics 20 (1987), 53–65. 4
[SDW10] S LINGSBY A., DYKES J., W OOD J.: Rectangular hierarchical
cartograms for socio-economic data. Journal of Maps 6, 1 (2010), 330–
345. 2
[SDW11] S LINGSBY A., DYKES J., W OOD J.: Exploring uncertainty
in geodemographics with interactive graphics. IEEE Transactions on
Visualization and Computer Graphics 17, 12 (2011), 2545–2554. 2
[SL15] S INGLETON A. D., L ONGLEY P.: The internal structure of
greater london: A comparison of national and regional geodemographic
models. Geo: Geography and Environment 2, 1 (2015), 69–87. 2
[SMDS14] S ADANA R., M AJOR T., D OVE A., S TASKO J.: Onset: A visualization technique for large-scale binary set data. IEEE Transactions
on Visualization and Computer Graphics 20, 12 (2014), 1993–2002. 6

[HKV12] H U Y., KOBOUROV S. G., V EERAMONI S.: Embedding, clustering and coloring for dynamic maps. In Pacific Visualization Symposium (PacificVis) (2012), IEEE, pp. 33–40. 3, 4

[SOBL13] S CHRECK T., O MER I., BAK P., L ERMAN Y.: Geographic
Information Science at the Heart of Europe. Springer International Publishing, Cham, 2013, ch. A Visual Analytics Approach for Assessing
Pedestrian Friendliness of Urban Environments, pp. 353–368. 4

[HWH∗ 11] H OEBER O., W ILSON G., H ARDING S., E NGUEHARD R.,
D EVILLERS R.: Exploring geo-temporal differences using GTdiff. In
Pacific Visualization Symposium (2011), IEEE, pp. 139–146. 5

[SW15] S UN M., W ONG D. W.: Spatial aggregation as a means to improve data quality. In Proceedings of the 13th International Conference
on GeoComputation (2015). 8

[Ins85] I NSELBERG A.: The plane with parallel coordinates. The Visual
Computer 1, 2 (1985), 69–91. 1

[TBS∗ 09] T ORRES G. J., BASNET R. B., S UNG A. H., M UKKAMALA
S., R IBEIRO B. M.: A similarity measure for clustering and its applications. Int. J. of Elec. Comput. & Syst. Eng 3 (2009), 164–170. 5

[JPDD03] J UNG Y., PARK H., D U D.-Z., D RAKE B. L.: A decision
criterion for the optimal number of clusters in hierarchical clustering.
Journal of Global Optimization 25, 1 (2003), 91–111. 2
[KF75] K UIPER F. K., F ISHER L.: 391: A monte carlo comparison of
six clustering procedures. Biometrics (1975), 777–783. 2
[LSP∗ 10]

L EX A., S TREIT M., PARTL C., K ASHOFER K., S CHMAL D.: Comparative analysis of multidimensional, quantitative
data. IEEE Transactions on Visualization and Computer Graphics 16,
6 (2010), 1027–1035. 2
STIEG

[Mei05] M EIL Ǎ M.: Comparing clusterings: An axiomatic view. In
Proceedings of the 22nd international conference on Machine learning
(2005), ACM, pp. 577–584. 2
[MGK07] M ACK E., G RUBESIC T. H., K ESSLER E.: Indices of industrial diversity and regional economic composition. Growth and Change
38, 3 (2007), 474–509. 2
[MHKH11] M ILLS R. T., H OFFMAN F. M., K UMAR J., H ARGROVE
W. W.: Cluster analysis-based approaches for geospatiotemporal data
mining of massive data sets for identification of forest threats. Procedia
Computer Science 4 (2011), 1612–1621. 1
[MJ07] M ASON G., JACOBSON R.: Fuzzy geographically weighted clustering. In Proceedings of the 9th international conference on geocomputation, Maynooth, Eire, Ireland (2007), pp. 3–5. 1

[Tob70] T OBLER W. R.: A computer movie simulating urban growth in
the detroit region. Economic geography (1970), 234–240. 2
[TSH∗ 14] T URKAY C., S LINGSBY A., H AUSER H., W OOD J., DYKES
J.: Attribute signatures: Dynamic visual summaries for analyzing multivariate geographical data. IEEE Transactions on Visualization and Computer Graphics 20, 12 (2014), 2033–2042. 2
[vLBA∗ 12] VON L ANDESBERGER T., B REMM S., A NDRIENKO N.,
A NDRIENKO G., T EKUSOVA M.: Visual analytics methods for categoric
spatio-temporal data. In IEEE Conference on Visual Analytics Science
and Technology (2012), IEEE, pp. 183–192. 1
[VR07] V ICKERS D., R EES P.: Creating the UK National Statistics 2001
output area classification. Journal of the Royal Statistical Society: Series
A (Statistics in Society) 170, 2 (2007), 379–403. 2
[War63] WARD J. H.: Hierarchical grouping to optimize an objective
function. Journal of the American statistical association 58, 301 (1963),
236–244. 6
[ZKG09] Z HOU J., KONECNI S., G RINSTEIN G.: Visually comparing
multiple partitions of data with applications to clustering. In IS&T/SPIE
Electronic Imaging (2009), International Society for Optics and Photonics, pp. 72430J–72430J. 2

c 2016 The Author(s)

c 2016 The Eurographics Association and John Wiley & Sons Ltd.
Computer Graphics Forum 

130

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 19,

NO. 1,

JANUARY 2013

Automated Box-Cox Transformations
for Improved Visual Encoding
Ross Maciejewski, Member, IEEE, Avin Pattath, Sungahn Ko,
Ryan Hafen, William S. Cleveland, and David S. Ebert, Fellow, IEEE
Abstract—The concept of preconditioning data (utilizing a power transformation as an initial step) for analysis and visualization is well
established within the statistical community and is employed as part of statistical modeling and analysis. Such transformations
condition the data to various inherent assumptions of statistical inference procedures, as well as making the data more symmetric and
easier to visualize and interpret. In this paper, we explore the use of the Box-Cox family of power transformations to semiautomatically
adjust visual parameters. We focus on time-series scaling, axis transformations, and color binning for choropleth maps. We illustrate
the usage of this transformation through various examples, and discuss the value and some issues in semiautomatically using these
transformations for more effective data visualization.
Index Terms—Data transformation, color mapping, statistical analysis, Box-Cox, normal distribution

Ç
1

I

INTRODUCTION

the visual analysis of data, the appropriate choice of
display parameters for variable comparison is a complex
issue. Under the assumptions of data normality, choices of
data binning and axes scaling for visual analysis have been
well studied [6], [7], [9], [10], [17], [23], [31]. Unfortunately,
real-world data often fail to meet any approximation of a
normality assumption. One of the most effective ways of
transforming data to a suitable approximation of normality
is to utilize a power transformation. The power transformation was introduced by Tukey [29], [30] and further
discussed as a means of visualizing data by Cleveland [8].
This concept of preconditioning data (utilizing a power
transformation as an initial step) for analysis and visualization is well established within the statistical community and
is employed as part of statistical modeling and analysis.
However, within the visualization community, the application of appropriate power transformations for data visualizations is largely ignored in favor of interactive explorations
(e.g., [16]) or default applications of logarithmic or square
root transforms (e.g., [19]). Yet, transformation is a critical
N

. R. Maciejewski is with the School of Computing, Informatics and Decision
Systems Engineering, Arizona State University, PO Box 878809, Tempe,
AZ 85287-8809. E-mail: rmacieje@asu.edu.
. A. Pattah is with the Microsoft Corporation, Potter Engineering Center,
500 Central Drive, Suite 226, West Lafayette, IN 47907.
E-mail: avin.pattah@gmail.com.
. S. Ko, W.S. Cleveleand, and D.S. Ebert are with the Purdue University,
Potter Engineering Center, 500 Central Drive, Suite 226, West Lafayette,
IN 47907. E-mail: ko@purdue.edu, wsc@stat.purdue.edu,
ebertd@ecn.purdue.edu.
. R. Hafen is with the Pacific Northwest National Laboratory, Potter
Engineering Center, 500 Central Drive, Suite 226, West Lafayette, IN
47907. E-mail: ryan.hafen@pnnl.gov.
Manuscript received 16 Nov. 2010; revised 9 Sept. 2011; accepted 13 Jan.
2012; published online 17 Feb. 2012.
Recommended for acceptance by H. Pottmann.
For information on obtaining reprints of this article, please send e-mail to:
tvcg@computer.org, and reference IEEECS Log Number TVCG-2010-11-0272.
Digital Object Identifier no. 10.1109/TVCG.2012.64.
1077-2626/13/$31.00 ß 2013 IEEE

tool for data visualization as it can substantially simplify the
structure of a data set.
Traditionally, statisticians have applied data transformations to reduce the effects of random noise, skewness,
monotone spread, etc., [8], all of which can affect the resulting
data visualizations. For example, reducing random noise can
help show global trends in the data, changing the range of
values can help fit the data on displays with small screens,
and reducing the variance can help improve comparative
analysis between multiple series of data. In approximately
normal data, methods of data fitting and probabilistic
inference are typically simple and often more powerful.
Furthermore, the description of the data is less complex,
leading to a better understanding of the data itself. As such,
by choosing an appropriate power transformation, data can
often be transformed to a normal approximation, lending
itself to more powerful visual and analytical methods.
Thus, it is clear that there is a strong need to emphasize and
explore the preconditioning of data using a power transformation for visualization and analysis. In this paper, we
explore the use of the Box-Cox transformation [5] as a means
for automatically determining an appropriate power transformation coefficient. Automatic and semiautomatic analyses
of the data are performed, and the power transform coefficient
that best normalizes the data is calculated and applied. We
demonstrate the usefulness of such data preprocessing using
examples in time-series visualization, geographical visualization, and histogram binning. This preprocessing step is
directly applicable to positively or negatively skewed data;
however, bimodal distributions or other irregular data
distributions will require different preprocessing steps.
Contributions of this work include the following:
1.
2.

An approach for applying Box-Cox transformations
to scale multiple time series at once.
A novel use of Box-Cox transformations for simplifying time series.

Published by the IEEE Computer Society

MACIEJEWSKI ET AL.: AUTOMATED BOX-COX TRANSFORMATIONS FOR IMPROVED VISUAL ENCODING

3.

2

Methods for computing color bin widths for
choropleth maps that can incorporate user analytic
interest.

RELATED WORK AND TECHNICAL CONCEPTS

The choice of an appropriate power parameter is the most
important aspect of the application of the power transform.
Power transformations help to achieve approximate symmetry, stabilize variance across multiple distributions,
promote a straight line relationship between variables and
simplify the structure of a two-way or higher dimensional
table [5], [13], [28], [29]. The power transformation [29] is a
class of rank-preserving data transformations parameterized by  (the power) defined as
 
x
ð 6¼ 0Þ
ðÞ
x ¼
ð1Þ
lnðxÞ ð ¼ 0Þ;
where x is the observed or recorded data.
Under this transformation, for  ¼ 1, the data remain
untransformed, for  ¼ 1, the data are inverted, etc. For
data skewed toward large values, powers in the range of
½1; 1 are generally explored. Powers above 1 are not
typically used if the data have large positive values because
they increases the skewness. It is also commonly observed
that as the power is reduced from 1 to 1, the data are
transformed until they are nearly symmetric and upon
further reduction they become asymmetric again [8]. This is
important for visualization as skewed data tend to result in
overly large graphs to represent the full dynamic range, or
squished graphs where outliers are visible but data near the
mean of the distribution are bunched together.
Statistically, we want to find a suitable power for the most
appropriate transformation such that the variance in the data
is stabilized. Such a value helps in conditioning the data,
enabling easier data analysis in subsequent stages. At the
same time, it also leads to desirable changes in the data that
helps to improve visualizations in 1D and 2D. Traditionally,
an appropriate power for the power transformation is chosen
through trial and error, by plotting the mean of each data
series versus its standard deviation for different powers from
a finite set of possible powers determined empirically.
Typical choices that are used by statisticians for the power
are f1;  12 ;  14 ; 0; 14 ; 12 ; 1g since they provide a representative collection of the power transformation [8]. Based on this
statistical observation, an appropriate power can be chosen
to make the distribution symmetric. Statistically, this means
that the data distribution is rid of spread variation, thus
leaving us with only location variations which are easier to
model. In many cases, the power chosen using the above
method also brings the data closer to normality which is
always a desired effect in data modeling. While this method
of interactively selecting the power transformation provides
more control over the choice of the power for each data set, it
is cumbersome and may not always result in the best possible
power as one cannot examine all the possible choices.
Therefore, we utilize an alternative to this manual procedure
using the Box-Cox family of power transformations [5].

131

2.1 The Box-Cox Power Transformation
The transformation, introduced by Box and Cox [5], is a
particular family of power transformations with advantageous properties such as conversion of data to an approximately normal distribution and stabilization of variance.
Given a vector of n observations x ¼ fx1 ; . . . ; xn g, the data are
transformed using the Box-Cox transformation given by
(
x  1 ð 6¼ 0Þ
ðÞ
x ¼
ð2Þ

lnðxÞ ð ¼ 0Þ;
where x is the vector of observed or recorded data and
the parameter  is the power. Note that both (1) and (2) are
defined only for positive data. However, any nonpositive
data can be converted to this form by adding a constant.
Given this initial transformation, Box and Cox [5] then
assumed that for some unknown , the transformed
ðÞ
observations xi ði ¼ 1; . . . ; nÞ are independently normally
distributed with constant variance 2 and with expectations
that the transformed responses xðÞ will be approximately
normal such that
xðÞ  NðA; 2 In Þ;

ð3Þ

where Nð0; 2 In Þ denotes the multivariate-normal distribution with a mean vector 0. Furthermore, xðÞ is an ðn  1Þ
matrix, A is an ðn  k þ 1Þ matrix and  is a ðk þ 1  1Þ matrix.
The likelihood in relation to the original observations, x,
is obtained by multiplying the normal density by the
Jacobian of the transformation, thus
(
)
1
ðxðÞ  AÞT ðxðÞ  AÞ
Jð; xÞ;
ð4Þ
exp 
n
22
ð2Þ2 n
where


ðÞ 
n 
Y
dxi 
Jð; xÞ ¼

:
 dxi 
i¼1
One can then maximize the logarithm of the likelihood
function. Readers of this work should refer to the work of
Box and Cox [5] for details and derivations. The final
derivation for the maximum likelihood estimator yields,
1
Lmax ðÞ ¼  log Sð; yÞ=n;
2

ð5Þ

where
T

Sð; yÞ ¼ yðÞ ar yðÞ ;

ð6Þ

Ar ¼ I  AðAT AÞ1 AT ;

ð7Þ

and
1

yðÞ ¼ xðÞ =J n :

ð8Þ

Finally,  can be maximized by taking the derivative of
Lmax with respect to  and finding the critical points. In the
special case of the one parameter power transformation,
xðÞ ¼ ðx  1Þ= (which is the focus of our work),

132

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 19,

NO. 1,

JANUARY 2013

Fig. 1. Skewness in data visualization. (Left) Visualization of the normalized number of households with income greater than $150,000 grouped by
census tracts in Indiana (using an equal interval colormap). (Middle) Statistical characteristics of the original data—1: Histogram. 2: Normal Q-Q plot
showing skewness. (Right) Statistical characteristics of power transformed data—3: Histogram with a fitted normal distribution. 4: Normal Q-Q plot
showing closeness to normality.

d
xðÞ ar uðÞ n X
Lmax ðÞ ¼ n
log xi ;
þ þ
T
d
xðÞ ar xðÞ 
T

ð9Þ

where uðÞ is the vector of components f1 xi log xi g. One
can use Newton’s Method for this maximization, and the
local maximum can be used (as is done in Matlab).

2.2 Axis Transformations for Visualization
Once an appropriate power transformation is chosen, the
data are transformed, which, in turn, means the axis on
which the data are plotted is also transformed. Such
transformations are of key significance when data are
skewed, and, despite the guidelines from a statistical
visualization viewpoint [8], few visualizations address the
issue of statistically preconditioning skewed data in practice.
Skewed data are data in which the majority of the samples
lie near the median values with outliers stretching the data
domain to large (or small) values thus increasing the range
needed for a given display axis. The plotting of this skewed
data compresses values into small regions of the graph
resulting in a lower fidelity of visual assessment of the data
[8]. One option to improve data assessment would be to
remove the outliers and focus on the range of data near the
median requiring an interactive technique such as zooming,
or users may select the data they are interested in (by
brushing) to create a new plot that focuses on the subset of
interest. Another option is to apply an appropriate choice of
power transformation as a preprocessing step and use this
power transformation to transform the axis. This transformation reduces some of the need for interaction and
massages the data into a form that is statistically more
suitable for advanced analytical techniques.
We illustrate this skewness phenomenon using an
example in Fig. 1. The map on the left shows a plot of the
normalized number of households with income greater
than $150,000, grouped by census tracts, in the state of
Indiana. The data are normalized using the total population
of the corresponding census tract and displayed using an
equal interval colormap. A quick look at the visualization
shows high values in the center (around Indianapolis), and
low values in most of the rest of the map. However, this

map hides details of the variation found within the lower
range of the data as most of the values fall into the lower
valued color bins. Plot 1 in Fig. 1 shows the histogram with
frequency counts of the data. The x-axis represents the
normalized household count and the y-axis is the number of
census tracts that fall into the corresponding histogram bin.
A quick look at the histogram shows the skewness toward
smaller values which results in the unbalanced coloring on
the map. This unwanted data characteristic not only makes
the data harder to visualize but also makes it difficult to
apply standard statistical techniques.
In Fig. 1, we illustrate the effect of applying the Box-Cox
power transformation to skewed data. The choropleth map
of Fig. 1 shows the number of households by census tract
across the state of Indiana whose income exceeds $150,000.
In order to compare distributions, we utilize the Q-Q plot (or
quantile-quantile plot). Q-Q plots can be used to compare
two data distributions by plotting their quantiles on both
axes [8]. Graphically, two distributions similar to each other
will lie close to the line y ¼ x. Linearly related distributions
will lie along a straight line, but not necessarily along y ¼ x.
Normal Q-Q plots show quantiles of given data with
standard normal quantiles. In Plot 1 of Fig. 1, we see the
original, skewed distribution of the data and the Q-Q plot of
this distribution in Plot 2. The quantiles here show a
significant departure from the straight line especially on
the right, indicating high skewness of the corresponding
map data. Plot 3 in Fig. 1 shows the frequency histogram
after application of the Box-Cox transformation with
automatic power estimation for the map on the left.
Additionally, a normal distribution is fit to the transformed
histogram data using the expectation maximization algorithm [14] in order to illustrate the transformation to an
approximately normal distribution. Plot 4 in Fig. 1, however,
shows the Q-Q plot of the transformed data that align closely
with a straight line indicating that the power transformation
converted the underlying data to a near-normal distribution.
As such, in our paper, normal Q-Q plots are used to illustrate
skewness in data distributions, since the normal distribution

MACIEJEWSKI ET AL.: AUTOMATED BOX-COX TRANSFORMATIONS FOR IMPROVED VISUAL ENCODING

is symmetrical about its mean and any skewed data cannot
produce a linear plot.
Previous work has looked at utilizing power transformations for axis transformations. For example, Cook and
Weisberg’s Arc system [11], has utilized interactive interfaces in which the user can drag sliders to change the BoxCox transformation, or simply click a button to set the
transformation to the log-likelihood-maximizing value.
Unfortunately, many current visual analysis tools still fail
to consider the underlying data distribution and instead
rely on user intuition. For example, Tableau incorporates
frequency plots and histograms and groups the data into
bins of equal width; however, the frequency plots and
binning used often results in suboptimal visual displays for
comparison and analysis and users often will resort to
interactive techniques to zoom into the data or manually
adjusting bin sizes to remove the effects of outliers [1]. Such
procedures can become tedious and often inaccurate,
especially when skewed data are involved. Thus, there is
a need for the continued exploration and application of
power transformations for enhancing both the visual
representation and underlying analytical processes.

2.3 Data Binning/Classification
While power transformations are a well studied means of
transforming data for analysis and visualization, another
important application of such statistical preconditioning of
data is the determination of appropriate color intervals for
colormaps as seen in prior research in visualization and
cartography. Monmonier [23] states that poorly chosen
intervals may convey distorted or inaccurate impressions of
data distribution and do not capture the essence of the
quantitative spatial distribution. As such several simple
class interval selection/binning methods (such as quantile,
equal interval, and standard deviation) and more complex
methods (natural breaks [18], minimum boundary error
[12], and genetic binning scheme [2]) have been used
traditionally [23]. Several researchers have reported the
comparative utility of these methods. Smith [26] reported
that quantile and standard deviation methods were most
effective with normally distributed data and were most
inaccurate with asymmetrical and/or peaked distributions.
Moreover, equal interval and natural breaks methods were
inconsistent for various data distributions. Frequency-based
plots have been used to delineate class intervals [22],
particularly for data sets with a standard normal distribution with the curve split into equal bins based on mean and
standard deviation [3]. These observations point to the fact
that normality, and hence the power transformation, can be
useful in determining an effective colormap.
Visual analysis software such as Tableau [1] provide
interactive techniques for data binning. Kidwell et al. [19]
applied power transformation-based colormaps to visualize
incomplete data. However, in all these cases, users need to be
familiar with the underlying data distribution to obtain an
effective colormap. Therefore, an automatic classification
method is favorable when data distributions change frequently, as is the case in interactive visual analysis environments. An automatic color binning/classification method
based on extracted statistical properties, including skewness,
was described by Schulze-wollgast et al. [24]. However, they

133

limited the choice of classification to just the logarithmic and
exponential mappings, which may not be the best choice for
every data set. As such, the power transformation, with an
appropriate power value that is best able to reduce skewness
and condition the data to near normality, is beneficial in
interactive environments to provide an automatic initial
visualization based on the transformed data.
Furthermore, in the case of skewed data, research has
shown that traditional methods, such as equal interval
classification, is ineffective at aiding users in identifying
clusters and rates of change in choropleth maps due to
inaccurate binning [26]. However, research showed [6], [26]
that equal interval classification is as effective as the more
sophisticated binning/classification schemes (e.g., Jenks
natural breaks [18] and minimum boundary error [12])
when the data fall under a normal distribution.

3

RANGE-SCALING AND AUTOMATIC BINNING OF
TIME-SERIES PLOTS USING POWER
TRANSFORMATION

In this section, we illustrate an application of the automated
Box-Cox transformation for improved visual encoding in
time-series plots. Time-series plots are often used for visual
analysis and are likely to be skewed due to unusually large
data values occurring as spikes in the plot. Representing
such highly varying plots causes issues in scaling and
simply changing the aspect ratio cannot solve the problem.
This issue is compounded when representing multiple
time-series plots in a single graph as some of the plots may
become compressed (e.g., Fig. 2b (left)). We use a timeseries data set representing patient visits to a hospital
within the INPC (Indiana Network for Patient Care) [4] as
an example. The recorded observations are the total number
of visits as well as visits categorized into three groups
(constitutional, respiratory and gastrointestinal complaints)
over a period of five years. The y-axis of the transformed
graphs is then in the newly transformed space; however,
the values of the labels are transformed back to the original
space in order to allow for the analysts to work with the
values in their original representation.
Results of applying the Box-Cox power transformation to
the time plot are shown in Fig. 2. The leftmost column
shows the data plotted with time on the x-axis and the
number of visits on the y-axis. The rightmost column shows
transformed data plotted with time on x-axis and the
transformed data value on y-axis. For comparison and
analysis, the middle column shows normal Q-Q plots of the
original data (top) and transformed data (bottom), allowing
us to visualize the skewness of the corresponding data.

3.1 Transformation of a Single Plot
The first column in Fig. 2a shows a plot of total patient visits
to a hospital and one can clearly see that most of the data
are compressed to the bottom of the graph as they need to
accommodate both high and low values. Its corresponding
Q-Q plot, in the middle column, confirms the skewness of
the data. However, the Box-Cox transformation can be used
to find a suitable power to transform the data that better
utilizes the space, allowing us to simultaneously see the
spike as well as the detail in the previously compressed

134

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 19,

NO. 1,

JANUARY 2013

Fig. 2. Visual and statistical conditioning in time-series plots using the Box-Cox power transformation applied to daily INPC hospital visit data. (Left)
Original data plots. (Middle) Normal Q-Q plots of original data (top) and transformed data (bottom). (Right) Transformed data plots. The x-axis
represents the day count and the y-axis represents number of hospital visits. The graph legend is as follows: blue (total number of hospital visits), red
(constitutional complaint visits), black (respiratory complaint visits) and green (gastrointestinal complaint visits). Figures show the power
transformation applied to: (a) a single skewed plot ( ¼ :3613), (b) multiple skewed plots simultaneously ( ¼ :3613), and (c) ( ¼ 1:4011). Plots
without significant skewness leading to compression. Highlighted windows indicate improvement in plot depiction after the Box-Cox transformation
leading to better data interpretation.

region. For example, in the transformed plot, the dip in the
graph to the left of the spike allows the user to visualize
details during the dip period which are compressed in
the original plot. The corresponding normal Q-Q plot shows
that the transformed data are more symmetric with a
smaller data range, indicating the statistical conditioning is
closer to normality.

3.2 Simultaneous Transformation of Multiple Plots
During visual analysis, analysts often compare multiple
plots of related data simultaneously to get a quick overview
of potential correlations or temporal trends. During such
comparisons, a spike in one of the data sets can cause other
data plots to be compressed. An example is shown in Fig. 2b
second row. The actual plot in the left column shows the total
number of visits in blue and the numbers of constitutional,

respiratory and gastrointestinal complaints in red, black, and
green, respectively. From the original plot, we cannot see the
details about individual complaint categories. In cases with
multiple data, we use the data set with the largest range to
compute an appropriate power transformation using the
Box-Cox transformation. This power is then used to transform all other data sets in order to maintain uniformity while
simultaneously maximizing the use of the display space
(since we normalize the data set with the highest variance).
The normal Q-Q plots shown in this case correspond to the
data set with the largest range (i.e., in this case, the blue plot
showing total number of visits). From the transformed plot,
we can clearly see that during the dip in total visits, the
number of constitutional complaints were greater than
gastrointestinal complaints, where as this was not the case
for the rest of the duration. Future work will focus on more

MACIEJEWSKI ET AL.: AUTOMATED BOX-COX TRANSFORMATIONS FOR IMPROVED VISUAL ENCODING

135

Fig. 3. Power transformation for automatic time-series binning. (Left) Original INPC hospital time-series data. (Middle) Original data with a
predetermined bin width of one week that still retains some noise. (Right) Original data with bin width (¼ 22 days) determined from a normal
distribution fit over the power transformed data. Most of the noise is removed, while the overall temporal trend are easily perceived.

advanced schemes where properties of all data sets being
plotted for comparison could be analyzed and incorporated
in a more sophisticated transformation.

3.3 Limitations
While the power transformation is a powerful tool, there are
a limited number of cases in which it is not appropriate to
use, particularly, in cases where the power is outside the
range of ½1; 1. In these cases, the data may be overly
exaggerated or inverted depending on the sign of the power.
Fig. 2c shows an example of this case. From the raw data, we
can see there are no clearly visible spikes in the data, and the
corresponding normal Q-Q plot suggests that the data are
already close to normal. The power computed by the loglikelihood function maximization in this case is 1.4011. Data
transformation with this power causes the plot to be further
compressed while in the Q-Q plot, there is no significant
change in the normality of the data. Therefore, the application of this procedure should be limited to data plots that
contain at least one skewed plot that is significantly
nonnormal. Normality of the given data plot can be
measured automatically by computing the correlation
coefficient of its normal probability plot and thresholding
the coefficient value. Moreover, the automatically computed
power should be checked if it is in the range ½1; 1 before
application of this procedure. Generally, transformations
will fall within this range; however, this limitation is
significant and transforming data that are already a reasonable approximation of normality add another layer of
complication to the analysis process that is unnecessary.
Automatic Time-Series Binning for Global Trend
Display
Time-series plots are typically noisy. Traditionally, either
interactive methods or bin widths based on a prior
knowledge of the data distribution are utilized to highlight
temporal trends in the data. Automatic binning is beneficial
in an interactive visual analytic environment to provide a
good initial time-series display showing global trends. As
such, using the Box-Cox power transformation, we can
convert data to an approximately normal distribution and
use the properties of the normal distribution (standard
deviation) to determine an approximate bin width. We
determine the maximum-likelihood estimates of the parameters (mean and standard deviation) of a normal
distribution fitted to the data using the expectation
maximization algorithm [14]. The standard deviation is

then used as a binning factor to smooth out local variances
while retaining global trends in the data.
Fig. 3 shows an example of applying this procedure to
the INPC hospital data shown on the left. In this case, we
obtain a result of 22 days as the bin width. The rightmost
figure shows the original plots binned by 22 days and the
middle figure shows the original plots binned by seven
days for comparison. As can be in Fig. 3, the middle figure
still retains some of the noise from the original plot where
as the rightmost figure smoothes out most of the noise
while presenting trends in all four plots of Fig. 3(Right).
Furthermore, the standard deviation of the transformed
data may provide analysts with cues as to cyclical behavior
while preserving other trends.
Other methods may also be used to show global trends
and smooth the data. If only slightly larger bins are used (say
31 days for a bin as opposed to 22 days) the results will be
similar to the point that almost no differences would be
observable. However, what this automatic binning provides
is a means of automatically approximating an appropriate
bin width by bringing the data in line with an assumption of
normality through the power transformation. In other
methods, such as weighted moving averages, or exponential
smoothing, parameter choices need to be made about the
smoothing parameter, which can affect the result depending
on if the data are normal or nonnormal. By approximating
the data as normal through the power transformation, other
methods can be applied for such smoothing. Thus, the
application of the power transformation can enhance both
the analytic and visualization tools of a system.

3.4

4

RANGE-SCALING AND SEMIAUTOMATIC COLOR
BINNING/CLASSIFICATION BASED ON INTENT

Colormaps in interactive visual analysis environments are
typically generated either manually (by adjustment of color
bins) or using preexisting methods such as quantile, equal
interval, or standard deviation-based binning. However,
each of these methods is most appropriate for specific data
distributions and not applicable in general [6], [20], [26], [27].
Moreover, in interactive environments, users perform
various operations such as zooming, panning, filtering, and
selection, and temporal browsing that constantly change the
underlying data distribution. Therefore, there is a need to
generate automatic colormaps that can provide a good initial
visualization that highlights important features in the

136

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 19,

NO. 1,

JANUARY 2013

Fig. 4. Range scaling and our intent-based semiautomatic color binning procedure using the power transformation. This power transformation is
applied after step 2. Steps 6 and 7 in the shaded rectangle are executed based on the users’ visualization intent.

underlying data. In the next section, we describe a method
for applying the power transform to the data and using the
transformed data space to determine the bin widths for
colormapping. This is a semiautomatic technique as the user
needs to provide two initial parameter values—a threshold
to filter out uninteresting data and specification of their
visualization intent. Here, the filtering step is not necessary;
however, it can be used to remove data that is of no interest to
the analyst (low ranges when looking for hotspots or outliers
when looking for trends) prior to visualization.

4.1 Procedure
Fig. 4 shows the procedure that starts with a default
histogram representing the data as illustrated in step 1. The
histogram is constructed such that the x-axis represents
the data values that are mapped to colors and the y-axis, or
the histogram counts, represents the number of entities with
a specified data value or a range of data values depending
on the histogram bin resolution. In many visualizations, a
majority of the display entities represent default values that
may not be of interest to the user. These values can interfere
with the data transformation. In step 2, we filter out such
values using a filter threshold. At the same time, we add a
constant value to the data set to eliminate the zero values
since the Box-Cox power transform requires positive data
values. The filter threshold is data dependent and is
determined based on the default values in the data set or
the range of data values that do not significantly contribute
toward understanding the visualization. Currently, this
value needs to be specified by the user before starting an
interactive visualization session.
The Box-Cox power transformation is applied, in step 3,
to the filtered histogram to convert the data into an
approximately normal distribution. The Box-Cox transformation automatically determines the power to be applied
by maximizing the log-likelihood function. In step 4, the

expectation maximization algorithm [14] is used to estimate
the parameters of a normal distribution (mean and standard
deviation) that best fit the transformed data. In step 5, we
add in all the data that was filtered out in step 2 after
applying the power transformation determined in step 3.
Steps 6 and 7 are dependent on users’ visualization intent.
In step 6, we determine the positions of histogram divisions
based on the normal distribution parameters estimated in
step 4. The estimated normal distribution curve is shown
overlaid on the histogram in red. In step 7, we determine
the number of colors to be assigned to each division based
on the visualization intent. Both steps 6 and 7 are described
and illustrated in the subsequent sections using two
visualization intents and three data sets. In step 8, we
obtain the final visualization by applying this colormap.
Note that the user intent is actually used in two phases of
this process. Initially, the user filters the data by selecting
regions that they deem as uninteresting. While still in this
space, the user can see only the untransformed space. After
the data are transformed, the intent is that more details
within the data will emerge, and the user can further refine
their intent in the transformed data space.

4.2 Displaying Complete Range of Skewed Data
Skewed data can cause issues when a user is trying to
visualize the entire range of data using predetermined
colormaps as shown in Fig. 5(left). The left map in Fig. 5
shows the census tracts in Indiana colored by the number of
households (normalized by the census tract population) with
annual income greater than $150,000. An equal interval color
binning method, in this case, makes it difficult to see the
variation of data in the lower range (all the white tracts) as
most of the data are skewed toward tracts with low incomes.
Our goal is to visualize the entire range of values simultaneously, without having to manually adjust the colormap
each time the data change as a result of user interaction.

MACIEJEWSKI ET AL.: AUTOMATED BOX-COX TRANSFORMATIONS FOR IMPROVED VISUAL ENCODING

137

Fig. 5. Color bin boundary determination and color assignment for visualizing the census data. (Left) Visualizing the original data with an equal
interval colormap showing the number of households (normalized by the census tract population) with an annual income greater than $150,000.
(Middle) Q-Q plots of the original and transformed data (top) and the transformed histogram with a fitted normal curve and color bin boundary
assignment (bottom). (Right) Visualization using the new colormap showing better variation in color throughout the data range.

4.2.1 Color Bin Boundary Determination and
Assignment
We construct a histogram for this data with the number of
households on the x-axis and the corresponding number of
census tracts on the y-axis. We do not apply a filter threshold
in this example as our intent is to visualize the entire data
range. The first five steps of our procedure are applied to
transform the filtered data. In step 6 of our procedure, we
divide the histogram into three regions at one standard
deviation from the mean on either side. The middle top plot
in Fig. 5 shows the Q-Q plots for the original and transformed
histogram. The middle bottom plot shows the transformed
data histogram, fitted normal curve and histogram divisions.
Following our goal of showing the entire range of data, we
assign colors that follow the fitted normal distribution based
on the standard deviation method. We assign more colors
near the mean of the distribution and fewer colors beyond
the standard deviation on either side of the mean in step 7 of
our procedure. As 95 percent of the transformed data will fall
within two standard deviations of the mean (as the
transformed data should be an approximately normal
distribution), this method robustly covers the data. The
colormap below shows the color assignment for this data set
with two, five, and two colors using ColorBrewer’s sequential blue colormap. The map on the right of Fig. 5 shows the
result after applying this colormap and one can now clearly
see the entire range of the data, where as, in the map on the
left side of Fig. 5, the majority of the data is mapped to two
bins (the lightest two colors in our chosen scheme).
4.2.2 Results
Further power transform results that visualize the entire data
range on the map are shown in Fig. 6. These figures show the
number of households in Indiana earning an annual income
between $45,000 and $60,000 grouped by census tracts and
normalized by the corresponding census tract population.
Here, we compare colormaps obtained using the traditional

quantile binning method (Fig. 6(Left)) and a colormap
determined using the commonly used logarithmic transformation (Fig. 6(Middle)), with our procedure using the power
(Box-Cox) transformation (Fig. 6(Right)). The normal curve
fitted after the log transform follows the same division as the
Box-Cox transform color scale division described in Section 4.2.2. Note that the Box-Cox color mapping is able to
bring out better variation when compared to the quantile
mapping and log-based mapping.

4.2.3 Limitations
Occasionally, data can still be significantly nonnormal after
the Box-Cox transformation. For example, bimodal distributions will fail to approximate normality even after the
application of a power transformation. Although such data
occur infrequently in practice, our procedure may not
generate the best classification in these cases. However, in
these situations, a goodness-of-fit value of the normal
distribution can be computed using the normal probability
plot of the transformed data. The applicability of our
procedure can be assessed by computing the correlation
coefficient of this plot and thresholding the value. Moreover, as with the time-series data, the application of this
procedure to determine colormaps is limited to power
values in the range ½1; 1, as other powers can significantly
alter the data values.
4.3

Visualizing Hotspots in Detail Using an Adaptive
Nonlinear Colormap
In geospatial visualization, density-scaled heatmaps
are often used to convey relative data densities on a map.
One particular method of density estimation uses a variable
kernel width to determine density estimates for each pixel
on the map [21], [25]. Fig. 7(left) shows hotspots indicating
criminal incidents during 2006 in the city of West Lafayette,
Indiana. An equal interval colormap ranging between the
minimum and maximum values of the density estimates is
used. The actual colors are drawn with an alpha value less

138

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 19,

NO. 1,

JANUARY 2013

Fig. 6. Comparison of various colormaps in the visualization of income data by census tracts in Indiana showing the number of households
(normalized by the corresponding population) with an annual income between $45,000 and $60,000 using Quantile binning (Left), Logarithm
transformed binning (Middle), and Power (Box-Cox) transformed binning (Right). The power transformation mapping shows better global and local
variation than both the quantile binning and the logarithmic transformation. For example, areas near the center of the state are now able to show
more local variation, whereas before, these census tracts were all binned to approximately the same color.

than 1 so as to show the underlying area map. In an
interactive visualization environment, these density maps,
and hence the underlying density histograms, change
frequently as a result of user actions (such as zooming/
panning, temporal browsing and data selection and filtering). In this situation, a predetermined colormap defined
without regard to the underlying data distribution may
wash out large areas of the map due to lack of sufficient
color resolution as shown in the highlighted boxes in the
left figure. However, our procedure from Section 4.1 can
provide a more effective initial colormap that adapts itself
based on changing density estimates. The visualization
intent in this example is to view details within high density
hotspots by allocating more colors to such areas. Using the
power transformation, we modify the histogram into a
structure with certain assumptions of normality, and then
assign a colormap based on the normal distribution
parameters as described below.

4.3.1 Color Bin Boundary Determination and
Assignment
Based on our visualization intent of finding details within
hotspots, our focus of interest in the histogram is around the
values that determine these hotspots, i.e., based on high
density estimates occurring toward the right end of the
histogram. We use a filter threshold value of 0.0005 in
step 2, for this data set, which helps to remove the default
zero values, while retaining most of the significant data
values. After performing steps 1 to 5 of our procedure, in
step 6, we divide the histogram into three unequal parts
using divisions at the mean and one standard deviation
beyond the mean of the fitted normal curve. The middle top
graphs of Fig. 7, shows Q-Q plots of the original and
transformed data (top and bottom, respectively). The
middle bottom plot shows the transformed data histogram
along with the fitted normal curve and the histogram

Fig. 7. Color bin boundary determination and color assignment for visualizing hotspots within hotspots of criminal activity density estimates in the
year 2006 in West Lafayette. (Left) Original density estimate hotspots using an equal interval color map. (Middle) Q-Q plots of the original and power
transformed data (top) and the transformed histogram, fitted normal curve and color bin boundary and assignment (bottom). (Right) Hotspot
visualization using the new color map. Notice the inner hotspots (darkest red regions) and newly visible hotspots in the highlighted rectangular
regions.

MACIEJEWSKI ET AL.: AUTOMATED BOX-COX TRANSFORMATIONS FOR IMPROVED VISUAL ENCODING

139

Fig. 8. Comparison of various colormaps in visualizing hotspots of arrests in the year 2008 in Lafayette using (Left) Equal interval, (Middle) Logarithm
transformed, and (Right) Power (Box-Cox) transformed colormaps. Both (Middle) and (Right) show more accurate hotspots. Note, however, that in
(Right), one can better see the hotspot differentiation within the highlighted rectangular regions.

divisions. The colormap below shows the color assignment
in step 7 of our procedure. Using ColorBrewer’s sequential
red colormap [15] with nine colors, we assign colors exactly
as shown in this colormap. Color bin lengths are assigned
such that they divide the normal distribution into equal
quantiles within each division. Note that while quantiles do
not change due to power transformations, the user’s
selection divides the data into user-defined chunks, which
are then divided by quantiles. Thus, this is not the same as
assigning the entire data range to a set of quantiles, which
would wash out the effects of the transformation.
The lightest color is assigned to the first histogram
division as well as to one of the bins in second division. The
next lightest color is assigned to the other bin of the second
division and the rest of the colors to the seven bins in the
third division with larger density values. The corresponding visualization is shown on the right. Note that in the
zoomed region on the bottom right we can clearly see
hotspots within hotspots as compared to the washed out
region in the left map. Moreover, we can also simultaneously see more hotspots in the rectangular highlighted
region on the top part of the right map which were missing
in the left map using equal interval color bins. The choices
are all designed by the user guided intent, where the user
chooses the number of bins to be assigned to each partition.

4.3.2 Results
Additional results of the density hotspot visualization,
using our procedure, are shown in Fig. 8, which
represents arrest data in Lafayette, Indiana in the year
2008. Here, we compare traditionally used equal interval
color binning (left) and a colormap determined using the
commonly used logarithmic transformation (middle), with
our procedure using the power (Box-Cox) transformation
(right). The normal curve is fitted after the log transform
using the same quantile division as the Box-Cox transform. While the equal interval colormap in Fig. 8(Left)
hardly produces any hotspots, logarithmic (Fig. 8(Middle))
and Box-Cox (Fig. 8(Right)) colormaps show a more
separated representation of the data. However, the BoxCox colormap is better able to differentiate regions within
the hotspots in the highlighted rectangular areas. These
examples illustrate that our procedure can automatically
generate an appropriate colormap for different data sets
with varying data distributions without any manual
parametric modifications.

5

CONCLUSIONS AND FUTURE WORK

We have demonstrated the utility of the power transformation in conditioning data for better visualization. Using the
Box-Cox class of power transformation with automatic
power estimation, we demonstrated automatic and semiautomatic procedures to determine visualization parameters
that yield good initial visualizations. We used examples of
time-series plots to illustrate the usage of this transformation
to visualize significantly skewed data as well as to
automatically bin time-series data to highlight global
temporal trends. Further, we described a procedure to use
the normality conversion property of the power transformation to determine an effective colormap based on users’
visualization intents. We presented results of this procedure
using geospatial data, compared them with commonly used
transformations and binning methods in visualization, and
discussed some limitations.
In the future, we will employ better data fitting models,
such as a normal mixture model, to more accurately fit the
transformed data, and explore histogram binning techniques based on multiple normal curves. We also plan to
automate our color binning procedure by determining an
appropriate filter threshold automatically. Further, motivated by Schulze-Wollgast et al. [24], we plan to develop
visualization methods to better represent and interpret new
color legends with significantly nonuniform bin lengths
obtained after transformation.

ACKNOWLEDGMENTS
This work is supported by the US Department of Homeland
Security’s VACCINE Center under Award Number 2009ST-061-CI0001.

REFERENCES
[1]
[2]

[3]
[4]

Tableau Binning Measures, http://www.tableausoftware.com/
public/knowledgebase/binning-measures, 2012.
M.P. Armstrong, N.C. Xiao, and D.A. Bennett, “Using Genetic
Algorithms to Create Multicriteria Class Intervals for Choropleth
Maps,” Annals Assoc. of Am. Geographers, vol. 93, no. 3, pp. 595-623,
2003.
R.W. Armstrong, “Standardized Class Intervals and Rate Computation in Statistical Maps of Mortality,” Annals Assoc. Am.
Geographers, vol. 59, no. 2, pp. 382-390, 1969.
P.G. Biondich and S.J. Grannis, “The Indiana Network for Patient
Care: An Integrated Clinical Information System Informed by over
Thirty Years of Experience,” Public Health Management Practices,
vol. 10, pp. 81-86, Nov. 2004.

140

[5]
[6]
[7]
[8]
[9]

[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]

[18]
[19]
[20]
[21]

[22]
[23]
[24]

[25]
[26]
[27]
[28]
[29]
[30]
[31]

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

G. Box and D. Cox, “An Analysis of Transformations,” J. Royal
Statistical Soc. Series B (Methodological), vol. 26, no. 2, pp. 211-252,
1964.
C.A. Brewer and L. Pickle, “Evaluation of Methods for Classifying
Epidemiological Data on Choropleth Maps in Series,” Annals
Assoc. Am. Geographers, vol. 92, no. 4, pp. 662-681, 2002.
W.S. Cleveland, The Elements of Graphing Data. Wadsworth Publ.
Co., 1985.
W.S. Cleveland, Visualizing Data. Hobart Press, 1993.
W.S. Cleveland and R. McGill, “Graphical Perception: Theory,
Experimentation, and Application to the Development of Graphical Methods,” J. Am. Statistical Assoc., vol. 79, no. 387, pp. 531-554,
1984.
W.S. Cleveland and R. McGill, “Graphical Perception and
Graphical Methods for Analyzing Scientific Data,” Science,
vol. 30, pp. 828-833, 1985.
R.D. Cook and S. Weisberg, Applied Regression Including Computing
and Graphics. John Wiley, 1999.
E.K. Cromley and R.G. Cromley, “An Analysis of Alternative
Classification Schemes for Medical Atlas Mapping,” European
J. Cancer, vol. 32A, no. 9, pp. 1551-1559, 1996.
C. Daniel, F. Wood, and J. Gorman, Fitting Equations to Data.
Wiley, 1980.
A. Dempster et al., “Maximum Likelihood from Incomplete Data
via the EM Algorithm,” J. Royal Statistical Soc. Series B (Methodological), vol. 39, no. 1, pp. 1-38, 1977.
M. Harrower and C. Brewer, “Colorbrewer. org: An Online Tool
for Selecting Colour Schemes for Maps,” The Cartographic J.,
vol. 40, no. 1, pp. 27-37, 2003.
H. Hauser, F. Ledermann, and H. Doleisch, “Angular Brushing of
Extended Parallel Coordinates,” Proc. IEEE Symp. Information
Visualization, pp. 127-130, 2002.
J. Heer, N. Kong, and M. Agrawala, “Sizing the Horizon: The
Effects of Chart Size and Layering on the Graphical Perception of
Time Series Visualizations,” Proc. 27th Int’l Conf. Human Factors in
Computing Systems, pp. 1303-1312, 2009.
G.F. Jenks, “The Data Model Concept in Statistical Mapping,” Int’l
Yearbook of Cartography, vol. 7, pp. 186-190, 1967.
P. Kidwell, G. Lebanon, and W. Cleveland, “Visualizing Incomplete and Partially Ranked Data,” IEEE Trans. Visualization and
Computer Graphics, vol. 14, no. 6, pp. 1356-1363, Nov./Dec. 2008.
A. MacEachren, Some Truth with Maps: A Primer on Symbolization
and Design. Assoc. Am. Geographers, 1994.
R. Maciejewski, S. Rudolph, R. Hafen, A.M. Abusalah, M. Yakout,
M. Ouzzani, W.S. Cleveland, S.J. Grannis, and D.S. Ebert, “A
Visual Analytics Approach to Understanding Spatiotemporal
Hotspots,” IEEE Trans. Visualization and Computer Graphics,
vol. 16, no. 2, pp. 205-220, Mar./Apr. 2010.
J.R. MacKay, “An Analysis of Isopleth and Choropleth Class
Intervals,” Economic Geography, vol. 31, pp. 71-81, 1955.
M.S. Monmonier, “Contiguity-Biased Class-Interval Selection: A
Method for Simplifying Patterns on Statistical Maps,” Geographical
Rev., vol. 62, no. 2, pp. 203-228, 1972.
P. Schulze-wollgast, C. Tominski, and H. Schumann, “Enhancing
Visual Exploration by Appropriate Color Coding,” Proc. Int’l Conf.
Central Europe on Computer Graphics, Visualization and Computer
Vision (WSCG), pp. 203-210, 2005.
B.W. Silverman, Density Estimation for Statistics and Data Analysis.
Chapman & Hall, 1986.
R.M. Smith, “Comparing Traditional Methods for Selecting Class
Intervals on Choropleth Maps,” Professional Geographer, vol. 38,
no. 1, pp. 62-67, 1986.
L. Stegena and F. Csillag, “Statistical Determination of Class
Intervals for Maps,” The Cartographic J., vol. 24, no. 2, pp. 142-146,
1987.
M.A. Stoto and J.D. Emerson, “Power Transformations for Data
Analysis,” Sociological Methodology, vol. 14, pp. 126-168, 1983.
J.W. Tukey, “On the Comparative Anatomy of Transformations,”
Annals Math. Statistics, vol. 28, pp. 602-632, 1955.
J.W. Tukey, Exploratory Data Analysis. Univ. Microfilms Int’l, 1988.
L. Wilkinson, “Algorithms for Choosing the Domain and Range
when Plotting a Function,” Computing and Graphics in Statistics,
pp. 231-237, Springer-Verlag, 1991.

VOL. 19,

NO. 1,

JANUARY 2013

Ross Maciejewski received the PhD degree in electrical and computer
engineering from Purdue University in December 2009. He is currently
an assistant professor at Arizona State University in the School of
Computing, Informatics & Decision Systems Engineering. Prior to this,
he served as a visiting assistant professor at Purdue University and
worked at the Department of Homeland Security Center of Excellence
for Command Control and Interoperability in the Visual Analytics for
Command, Control, and Interoperability Environments (VACCINE)
group. His research interests are geovisualization, visual analytics,
and nonphotorealistic rendering. He is a member of the IEEE and the
IEEE Computer Society.
Avin Pattath received the PhD degree in computer engineering from
Purdue University. He is a computer scientist with Microsoft. His
research interests include mobile visualization.
Sungahn Ko is currently working toward the PhD degree in electrical
and computer engineering from Purdue University. His research
interests include visual analytics and information visualization.
Ryan Hafen received the PhD degree in statistics at Purdue University.
His research interests include exploratory data analysis and visualization, massive data, computational statistics, time series, modeling, and
nonparametric statistics.
William S. Cleveland received the PhD degree in statistics from Yale
University. He is the Shanti S. Gupta distinguished professor of statistics
and courtesy professor of computer science at Purdue University. His
research interests include statistics, machine learning, and data
visualization. He is the author of The Elements of Graphing Data
(Hobart Press, 1994) and Visualizing Data (Hobart Press, 1993).
David S. Ebert received the PhD degree in computer science from Ohio
State University. He is a professor in the School of Electrical and
Computer Engineering at Purdue University, a University faculty scholar,
director of the Purdue University Rendering and Perceptualization Lab,
and director of the Purdue University Regional Visualization and
Analytics Center. His research interests include novel visualization
techniques, visual analytics, volume rendering, information visualization,
perceptually based visualization, illustrative visualization, and procedural abstraction of complex, massive data. He is a fellow of the IEEE and
the IEEE Computer Society, and a member of the IEEE Computer
Society’s Publications Board.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 15,

NO. 2,

MARCH/APRIL 2009

221

Visualization and Computer Graphics on
Isotropically Emissive Volumetric Displays
Benjamin Mora, Ross Maciejewski, Min Chen, Member, IEEE, and
David S. Ebert, Senior Member, IEEE
Abstract—The availability of commodity volumetric displays provides ordinary users with a new means of visualizing 3D data. Many of
these displays are in the class of isotropically emissive light devices, which are designed to directly illuminate voxels in a 3D frame buffer,
producing x-ray-like visualizations. While this technology can offer intuitive insight into a 3D object, the visualizations are perceptually
different from what a computer graphics or visualization system would render on a 2D screen. This paper formalizes rendering on
isotropically emissive displays and introduces a novel technique that emulates traditional rendering effects on isotropically emissive
volumetric displays, delivering results that are much closer to what is traditionally rendered on regular 2D screens. Such a technique can
significantly broaden the capability and usage of isotropically emissive volumetric displays. Our method takes a 3D data set or object as
the input, creates an intermediate light field, and outputs a special 3D volume data set called a lumi-volume. This lumi-volume encodes
approximated rendering effects in a form suitable for display with accumulative integrals along unobtrusive rays. When a lumi-volume is
fed directly into an isotropically emissive volumetric display, it creates a 3D visualization with surface shading effects that are familiar to
the users. The key to this technique is an algorithm for creating a 3D lumi-volume from a 4D light field. In this paper, we discuss a number
of technical issues, including transparency effects due to the dimension reduction and sampling rates for light fields and lumi-volumes.
We show the effectiveness and usability of this technique with a selection of experimental results captured from an isotropically emissive
volumetric display, and we demonstrate its potential capability and scalability with computer-simulated high-resolution results.
Index Terms—Three-dimensional displays, volume visualization, display algorithm, expectation maximization.

Ç
1

INTRODUCTION

T

HE

ultimate display may be thought of as a device that
can reproduce any given light field (LF) or plenoptic
function [1], [19], [26]. However, in order for such a
hypothetical 4D LF device to display images at an adequate
resolution, it would need a tremendous amount of bandwidth and processing power—capabilities that may not be
available for many years [33]. In the meantime, 3D displays,
which have recently become more affordable, can provide
users with a more immersive visualization experience when
compared with traditional 2D displays. While not the final
goal, these new 3D displays are an important step toward
the ultimate display.
Our work is concerned with a technique for improving
the shortcomings of such systems, focusing on the Perspecta
Display System [15] as a typical isotropically emissive
volumetric display (IEVD) [9]. This system is based on a
sweeping plane that performs 24 rotations per second,
projecting volume slices at a high refresh rate.
Due to the absence of absorbing materials on IEVDs that
would facilitate occlusion effects, the final image perceived

. B. Mora and M. Chen are with the Department of Computer Science,
University of Wales Swansea, Singleton Park, Swansea SA2 8PP, UK.
E-mail: {b.mora, m.chen}@swansea.ac.uk.
. R. Maciejewski and D.S. Ebert are with the School of Electrical and
Computer Engineering, Purdue University, 465 Northwestern Ave, West
Lafayette, IN 47907. E-mail: {rmacieje, ebertd}@purdue.edu.
Manuscript received 28 Aug. 2007; revised 26 Dec. 2007; accepted 30 June
2008; published online 22 July 2008.
Recommended for acceptance by T. Moeller.
For information on obtaining reprints of this article, please send e-mail to:
tvcg@computer.org, and reference IEEECS Log Number TVCG-2007-08-0116.
Digital Object Identifier no. 10.1109/TVCG.2008.99.
1077-2626/09/$25.00 ß 2009 IEEE

by the viewer consists of a set of ray integrals, each of which
is the sum of all emitted light along the ray. Hence, there is
no physical means for displaying a completely opaque
object in a conventional manner such that the luminance of
the occluded part is not perceived by the viewer [9]. Given a
closed surface object, without occlusion, the ray that passes
through the object will normally intersect with the surface
at two or more points. The perception of each surface point
is view dependent (see Section 3), even before taking into
account coloring and shading. Thus, the creation of shading
effects in an IEVD is a nontrivial problem, for which no
existing solution can be found yet.
Unfortunately, occlusion and shading of object surfaces
are common in real-world situations and provide optical
cues for the shape of an object and its surface properties.
As such, visualization on these IEVDs can be perceptually
less intuitive. Fig. 1a shows a typical computer-synthesized
isosurface of the UNC head data set as it would be
displayed on a 2D screen. This work aims at reproducing
such 3D rendering on IEVDs with viewpoint dependencies.
When directly displaying the original data set on an
isotropically emissive display, it results in an x-ray-like
visualization, as shown in Fig. 1b. A naive attempt to
display an isosurface by using a segmented data set would
produce Fig. 1c. Clearly, it is still difficult to perceive
surfaces in Fig. 1c. One imperative reason why Figs. 1b and
1c are less intuitive than Fig. 1a is that there is no surface
shading nor occlusion present in Figs. 1b and 1c.
One suggestion for improving renderings is to pass a
voxelized representation of a shaded surface to an isotropically emissive display. Unfortunately, as shown in Fig. 1d,
Published by the IEEE Computer Society

222

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 15,

NO. 2,

MARCH/APRIL 2009

Fig. 1. The limitations of emissive displays and the improvement made. (a) Consider a reference visualization showing an isosurface that we wish to
display on an emissive display. (b) Passing the raw volume to the Perspecta Display System results in an x-ray-like visualization. (c) Passing a
volume containing a segmented isosurface to the display enables the identification of the surface concerned, but the perception of the surface
geometry remains elusive without shading. (d) Passing a volume containing a shaded isosurface to the display produces a rather confusing
visualization. (e) With our new technique, passing a lumi-volume to the display offers a great improvement over (b), (c), and (d). (f) The simulated
result of displaying a lumi-volume on a future high-resolution emissive display system demonstrates the scalability and usability of the proposed
technique.

this does not alleviate the problem. Instead, it reduces the
surface perception because in such a representation the
shading effects are intended to be perceived correctly for an
opaque surface. When removing the opacity-based occlusion, the accumulative ray integrals aggregate the light
emitted from the shaded front surface, as well as all emitted
light behind the surface. The shading effects that capture the
continuity and curvature of the surface can no longer be
correctly preserved and perceived.
To overcome these problems, we have developed a novel
technique that enables users to visualize an isosurface with
shading effects similar to Fig. 1a on an IEVD. As a result, we
are able to generate, for the first time, a true 3D experience
of an isosurface visualization with emulated shading effects
on an IEVD, as shown in Fig. 1e, which is close to our
original simulated results (Figs. 1a and 1f).
Fig. 2 illustrates the pipeline required to solve this
rendering problem on IEVDs. We first render a 3D object or
data set into an LF using a graphics or visualization system.
This LF is then encoded inside a specific volumetric data set
with the help of a reconstruction algorithm. We call this
special volume data set a lumi-volume. When such a lumivolume is fed into an IEVD, the accumulative ray integrals
reproduce most shading effects on the viewer’s retinal
image, as exemplified in Fig. 1e. In other words, the lumivolume recreates an approximation of the original LF on the
isotropically emissive display.
This pipeline can also accommodate an arbitrary LF
captured by a plenoptic camera. Nevertheless, in this work,
we focus on reproducing volume visualization and computer graphics renderings, which is the main application
area of IEVDs. We employ an expectation-maximization
(EM) algorithm to generate a lumi-volume from an LF,

Fig. 2. The pipeline of our proposed technique. The thick lines indicate
the focus of this work.

which minimizes the difference between the original LF and
the LF rendered by the IEVDs.
To improve current imaging on isotropically emissive
displays, we must first formalize the rendering principles
behind IEVDs. Then, we examine various technical issues
and limitations of the proposed technique. We also show
that the loss of one dimension of information in approximating a 4D LF with a 3D lumi-volume will result in a
more continuous visualization of partially visible objects
through transparent surfaces. We examine the visualization
quality in relation to LFs generated using both direct
volume rendering (DVR) and nonrealistic maximum intensity projection (MIP). Our experimentation with a real
IEVD system shows that this technique is effective and
deployable, and our simulation study suggests that highquality results are attainable.

2

RELATED WORK

2.1 3D Displays
Two-dimensional displays have a limited capacity in which
to provide depth cues. Many 3D or stereo display
technologies have been developed for enriching the users’
experience of depth perception. An overview of these
developments can be found in the August 2005 issue of
Computer.
The most widely used class of techniques for improving depth perception is that of stereo parallax, which
provides each eye with separate images. This is a low-cost
technique, only requiring the creation of two images per
frame. However, wearing special glasses or goggles is
often inconvenient and may lead to the users’ fatigue.
Autostereoscopic displays [13], [28] were developed to
circumvent these issues, and some multiview systems are
able to horizontally multiplex many images. However,
with such displays the object is always at a fixed focal
depth, and the aspect ratio is only valid for a given depth.
On the other hand, volumetric displays [4], [5], [15],
which use 3D pixels that absorb or emit light, do not suffer
from the limitations of stereo parallax and autostereoscopic
displays. One such device is the Perspecta Display System
[15] (Fig. 3). This system uses a fast sweeping plane where

MORA ET AL.: VISUALIZATION AND COMPUTER GRAPHICS ON ISOTROPICALLY EMISSIVE VOLUMETRIC DISPLAYS

223

displayed by current technologies, nor is it compatible with
the input representation required by IEVDs. As such, our
technique approximates a 4D LF through a new 3D lightemitting volume representation referred to as a lumi-volume.

Fig. 3. (a) The Perspecta Display in action. (b) A close view of a
voxelized tower model. (c) The traditional rendering of the tower model
captured from a 2D computer.

slices of a 3D volume are projected onto a rotating screen
inside a hemisphere construct, providing a nearly 4
steradian field of view (FOV). Since every emission of
light on the sweeping plane will always be visible to the
user, this system is classified as an IEVD, which provides
x-ray-like renderings of the input data. This particular
display has already been deployed in several applications
[20], [35], [36].
Other types of IEVDs also exist, mostly based on
fluorescent material that is excited by electron beams or
lasers. Recently, some variants of the sweeping plane
technique, using a mirror instead of a diffuse surface, have
been proposed [9], [25]. The main advantage here is that the
imaging is not limited to the isotropically emissive model.
In the transpost system [30], the mirror is replaced by a
screen with a limited viewing angle.
Another well-known way to produce 3D displays is
holography [33]. This is an old technique that uses Fourier
optics to fully display a 4D LF. However, the technique is
limited by the huge amount of computation required
to process the data and can only be handled by today’s
supercomputers when high resolution is needed.

2.2 Light Field
An LF is a 4D function that associates a color to every
unobtrusive ray of a 3D space. The concept was introduced
in 1996 independently by Levoy and Hanrahan [26] and by
Gortler et al. [19] to facilitate interactive browsing of an
object without the need for rendering the object. For
example, the lumigraph representation [19] uses two facing
slabs to discretize the 4D LF space. A color is then associated
with every possible pair of grid points chosen from both
slabs, which actually defines a ray inside the LF space.
Fundamentally, the LF notion is similar to the plenoptic
function proposed in 1991 by Adelson and Bergen [1].
In our work, the color of a ray of light will be obtained very
differently by combining all the voxels along the ray path
according to a rendering model. Due to the fact that an LF data
set consists of samples taken from a 4D domain, a huge
amount of data is produced. Such data cannot yet be

2.3 Algorithms for Volume Reconstruction
Since IEVDs are not capable of accurately reproducing all
user-defined images, our goal is to create 3D renderings
that approximate any arbitrary LF data that the user wishes
to display. There are numerous voxel-based methods that
reconstruct a 3D volume from an LF [11], [14], [21], [31],
sometimes with the help of specific heuristics. In contrast,
our method achieves the opposite, recreating an approximation of the original LF through volume rendering (using
an isotropically emissive model in our case). Here, it is
important to note that the resultant lumi-volume is not
intended for general use and is only to be used on IEVDs
that generate x-ray-like renderings.
To create a lumi-volume, we need to define a volume
with projections that will match the given LF input as
closely as possible. This parallels medical techniques that
reconstruct 3D volumes from projections. The most prominent techniques are the filtered back-projection (FBP)
reconstruction technique, the algebraic reconstruction technique (ART), and the EM algorithm. The first technique
uses a Fourier transform, while the two others are iterative
techniques that solve a linear system.
The FBP reconstruction method was the first technique
used in computed tomography to reconstruct a volumetric
representation of an object. However, the reconstruction
equation is known only for specific cases, for example,
cone beam reconstruction [16], and cannot be used for
any arbitrarily positioned projection. The ARTs [18] utilize
an iterative process, and at every iteration, an algorithm
evaluates the difference between the synthesized projections of the current volume and the actual projections.
The algorithm then uses this difference to produce a new
refinement. The algorithm usually converges to a solution,
provided that it exists (e.g., the data is not corrupt).
However, the ART algorithm may find negative solutions,
which are not suitable for this work since the voxels in a
volumetric display cannot emit a negative amount of
light. Therefore, this work is built on the EM algorithm
[12], [32], which also employs an iterative process to solve
a linear system. For self-containment, this algorithm will
be briefly described in Section 4.
There has been a series of efforts implementing these
algorithms on graphics hardware [37], including a hardware-assisted FBP implementation by Cabral et al. [6], an
ART implementation by Mueller and Yagel [29] and
Trifonov et al. [34], and a hardware-based EM algorithm
by Chidlow and Moller [8]. More recently, there has been
interest in applying tomography reconstruction techniques
to handling LFs that capture transparent phenomena [24]. In
[3] and [23], Ihrke et al. used tomography reconstruction to
reproduce fire. More closely related to our work, [17] used
tomography to model data sets. Their work, though limited
to convex surfaces, showed that this approach is useful.
Finally, Dachille et al. [10] applied the simultaneous ART
(SART) method to recreate an LF. While the SART method
may produce negative solutions (which is undesirable in

224

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 15,

NO. 2,

MARCH/APRIL 2009

Fig. 4. Effect of the viewing angle on the luminance of a surface. The
wider the angle, the longer the length of the intersection, and thus the
brighter the surface.

this work), they managed to achieve an root-mean-square
(RMS) error close to zero for a single image reconstruction.

3

LUMI-VOLUME

A lumi-volume is a 3D volumetric data set, which specifically aims at recreating a fixed 4D LF when displayed on an
IEVD. In order to consider the usefulness of a lumi-volume,
we shall first examine how IEVDs differ from traditional
rendering displays in computer graphics and visualization.
Absorption and occlusion. IEVDs like the Perspecta have
commonly been used to display voxelized 3D models,
where the voxel values typically store the material properties of a model (e.g., a computed tomography data set) or a
discrete boundary representation of an object (e.g., an
isosurface). When such voxel values are mapped to
luminance by the IEVD device, the light emitted from
voxels arrive at the viewer’s location in a summative manner
without any absorption or occlusion events. Although the
resulting x-ray-like visualizations, as seen in Figs. 1b and 1c,
can be interpreted by viewers, it is much different from the
visualization of a real-life object on a traditional 2D
rendering system, such as Fig. 1a. As such, it is highly
desirable for a lumi-volume to encode voxel luminance in a
specific way to compensate for the lack of absorption and
occlusion. This requirement was recognized and conjectured as nonsolvable without physical modification of the
display in [9].
View-dependent rendering. With traditional rendering
of a 3D model on a 2D display, the illumination of each
visible surface element of the model can be either view
dependent (e.g., Phong, Blinn, and BRDF) or view
independent (e.g., diffuse-only and precomputed radiosity). For view-dependent shading effects (e.g., specular
highlight), the color of each visible surface element of the
model is normally recomputed every time the viewpoint is
modified. However, on an IEVD, the voxel luminance, if
any, will always be view independent because of the lack of
occlusion. The viewpoint modification occurs when the
viewer moves around; however, the 3D data set passed to
the display remains the same.
Hence, as shown in Fig. 1d, when a data set that encodes
view-independent shading effects is passed to an IEVD, the
results at different viewing positions can be very different
from what is expected. In fact, at many viewing positions,
the visualization can be quite incomprehensible. Thus, it is
necessary for a view-independent lumi-volume to encode
shading effects in an appropriate way such that the desired

Fig. 5. To create differently shaded surface elements according to
different viewpoints, the color must be spread more continuously across
the entire ray path, as opposed to being determined by a given color at
the intersection.

view-dependent shading would be partly maintained
through the different view-dependent rendering integrals
of an isotropically emissive display (Fig. 5).
Consistent brightness. A voxelized surface has a
physical thickness. As illustrated in Fig. 4, the length of a
ray-surface intersection d=cosðÞ is related to the angle of
incidence, as is the brightness of the surface due to the
accumulated light being proportional to the length of the
intersection. Therefore, the wider the angle is, the brighter
the surface appearance is, as demonstrated in Figs. 1c and
12a. This visual effect is a major obstacle when attempting to
reproduce computer graphics on the volumetric display
with voxelized scenes. Lumi-volumes address this difficulty
by considering the most correct rendering solution for this
task. To circumvent the above issues on a display with a
fixed input data set, the ideal solution would be to output a
real 4D LF that captures the visualization for all viewpoints.
Nevertheless, the practical restriction of the 3D input data set
used by IEVDs obliges us to seek an alternative solution, that
is, to approximate a 4D LF with a lumi-volume.
In order to match the lumi-volume as closely as possible
to the original LF, the problem must be first formalized in
terms of viewpoint, LF, and line integrals. The algorithm
for constructing a lumi-volume must then make use of all
the possible volume samples, not just the ones across the
surface, and work in a line integral domain, similar to
the process of volume reconstruction in medical imaging.
For instance, if a surface element is represented as a voxel
(which can only have a unique color), it is not possible
to obtain different shading properties as the viewpoint
changes (Fig. 5). If one now encodes the luminance properties along the entire light path instead of just considering the
surface elements, which results in different line integrals, it
is possible to create more independently shaded surface
elements. When the lumi-volume is passed to an IEVD, it
results in different visualizations for different viewpoints,
collectively offering the best approximation of the wanted
4D LF.
There are, however, restrictions on the reproduced LF.
For instance, the projection made according to one

MORA ET AL.: VISUALIZATION AND COMPUTER GRAPHICS ON ISOTROPICALLY EMISSIVE VOLUMETRIC DISPLAYS

225

Fig. 6. Projection and partial volume effect. bi represents an LF element,
xj represents a voxel, and Aij represents how much of xj is intersected
by bi .

viewpoint must be the same as the one made for the
viewpoint in the opposite direction.

4

EXPECTATION MAXIMIZATION

The EM algorithm was proposed by Dempster et al. [12] and
applied to medical imaging by Shepp and Vardi [32]. The
main idea is to solve a linear system in an iterative way. Let b
be an input LF (see Fig. 6), with b ¼ ðb1 ; b2 ; . . . ; bM Þ 2 RM
being the set of observed variables inside the 4D LF, and let
x ¼ ðx1 ; x2 ; . . . ; xN Þ 2 RN be the unknown voxels of a lumivolume that must be constructed as an approximation of b.
Due to the isotropic emission of the target displays, every
projection bi can be considered as a linear combination of the
voxel values in the lumi-volume. Therefore, we can write
our system as A  x ¼ b, where ðAij Þ is an M  N matrix
representing the partial volume effects. Each weight Aij
represents the volume of the intersection between bi and xj .
The EM algorithm, which iteratively converges to the
solution of our linear system, can be expressed as follows:
M
P
i¼1

xnþ1
j

¼

xnj



bi
Aij P
N

Aik xnk

k¼1

M
P

;

Aij

i¼1

where n is the iteration number.
Each iteration of the algorithm can be decomposed into
three main steps. The first step is to project the current lumivolume onto a set of the viewing planes, resulting in a
synthesized
LF r ¼ ðr1 ; r2 ; . . . ; rM Þ 2 RM ,
P intermediate
n
where ri ¼ N
A
x
,
and
r
i is initialized to one prior to
k¼1 ik k
the first iteration.
The second step is to evaluate the difference between the
input LF and the synthesized intermediate LF by calculating
the ratios between bi and ri for all i ¼ 1; 2; . . . ; M.
The final step is to update every voxel xj of the lumivolume by multiplying its value with the mean deviation
ratio of all the LF rays (or pixels) that intersect xj . The
average must also be weighted by the partial volume effects.
Unfortunately, this algorithm is rather slow, with a
complexity of Oðm3  p  lÞ, where N ¼ m3 is the total

Fig. 7. Models used in our experiments. (a) UNC Head, DVR.
(b) Aneurism, MIP. (c) Stanford Bunny, OpenGL. (d) “IEEE TVCG,”
OpenGL.

number of voxels in the lumi-volume, p is the total number
of projections in the LF, and l is the total number of iteration
steps. However, the convergence of this algorithm is linear,
and a relatively low number of steps ( 20-80) will usually
result in an adequate approximation. To compensate for the
high complexity level, numerous methods have been
proposed to improve the speed of the algorithm. One such
method is the ordered subset maximization method
(OSEM) [22] that runs the algorithm on subsets, thereby
improving the speed of convergence.
Converging to a solution usually makes sense only if
there exists a solution. In our case, we know that there is no
solution to our problem, since the input LF is a full 4D
representation in a 4D space, and a bijection cannot be
established with 3D space where the lumi-volume is
defined. However, the EM algorithm has been used in
statistics to find maximum-likelihood estimates, indicating
that the algorithm may converge to a good estimate of the
input LF by finding a minimum to ðA  x  bÞ2 .

5

EXPERIMENTAL ENVIRONMENT

In addition to the development of the pipeline shown in
Fig. 2, we have conducted a series of experiments to study a
number of technical issues in deploying our system. The
main results of these experiments will be presented and
analyzed in Section 6. In this section, we concentrate on the
data flow depicted by the solid lines in Fig. 2, describing the
environment and conditions for the experimental study,
and presenting the models and target rendering results
(Fig. 7) for benchmarking our approach using lumivolumes.

226

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

5.1 Models and Rendering Methods
Fig. 7a was rendered using the DVR method, Fig. 7b was
rendered using the MIP method, and Figs. 7c and 7d was
rendered using the projection-based mesh rendering
method. We utilized our own implementation of the DVR
and MIP methods and OpenGL for mesh rendering. These
three methods capture the characteristics of some of the
most commonly used rendering algorithms in visualization
and computer graphics.
MIP is a widely used volume rendering method in
medical visualization. Since only the maximum value
along a ray is kept, it provides selective visualization of
the internal structure of data sets. This is often used in
combination with contrast agents that aim to enhance the
signal for the specific part of the data set under investigation. Although MIP does not provide realistic renderings, it
presents an interesting challenge since it differs from the
other two methods in many respects. On one hand, an LF
resulting from MIP is more coherent since local maximums
are likely to be distributed over many projections of the LF.
For instance, the projection will always be the same for a
specific direction and the opposite direction. On the other
hand, MIP is fundamentally different from traditional
surface or x-ray-like renderings.
Our LFs are computed from different rendering methods, and the lumi-volumes constructed in this work
approximate a wide variety of rendering effects on an
IEVD. Collectively, the results can offer more conclusive
observations as to the usability of this approach and
provide indicative conjecture as to its extensibility to other
rendering methods.
5.2 Light Field Sampling
While the construction of a lumi-volume that would encode
all information in an LF is fundamentally unattainable due
to the dimension reduction, a careful selection of sampling
parameters can significantly improve the performance of
the EM algorithm in terms of both speed and accuracy. The
main unknown attribute before starting this work was how
much the RMS error will evolve with regard to the number
of projections used. Chai et al. [7] and Lin and Shum [27]
considered the efficient sampling of LFs, and their work
offered useful guidance as to the optimal sampling rate.
To generate a coherent LF, one must evenly sample the
4D LF space ð; ’; u; vÞ, as illustrated in Fig. 8. This allows
for the possibility of restricting the FOV of the LF to a
smaller portion of the 4D space, enclosed by the imagery
projections in the LF. We utilize polar coordinates to define
the projection planes in the LF. Each projection direction is
defined by two angles,  and ’, that fall respectively in the
ranges of ½max ; þmax  and ½max’ ; þmax’ . These two
intervals are uniformly sampled by taking n evenly spaced
samples for the angle ’ and then taking n  cosð’Þ evenly
spaced samples for the angle . Objects are then rendered
using an orthogonal projection for every sampled direction.
The cosine term is required to avoid a biased higher
sampling density at the poles.
5.3 Implementation of the EM Algorithm
Although the OSEM version can improve computation time
by up to an order of magnitude and is relatively easy to

VOL. 15,

NO. 2,

MARCH/APRIL 2009

Fig. 8. Images in our LFs are parameterized with two polar coordinates
angles  and ’.

implement, it makes the algorithm oscillate around the ideal
solution. Therefore, this speed acceleration technique was
not adopted by our experimental environment, and all
results presented in this paper involve only lumi-volumes
produced by the original EM algorithm. To improve the
performance, we utilized a CPU-based implementation that
has been optimized using SSE instructions. This was
preferred to a hardware-accelerated implementation because we wanted some flexibility on the implementation
side. As demonstrated by previous hardware-based implementations [6], [8], [37], it is feasible to further improve the
speed of a volume reconstruction algorithm using hardware-based algorithms, which is not the focus of this work.

5.4 Perspecta Display
We conducted extensive experimentation of displaying
constructed lumi-volume data sets on the Perspecta Display
System version 1.7 (Fig. 3). The results, as shown in Fig. 1e
and Section 6, are very encouraging when compared with
other forms of volume data sets, such as those shown in
Figs. 1b, 1c, and 1d.
The Perspecta system represents the current commercially
available technology. It has a limited dynamic range. The
sweeping plane does 24 revolutions per second, and
198 images (of resolution 7682 pixels) are displayed for each
revolution, representing a total number of 117 million voxels.
Since the Texas Instruments digital light processor can only
perform approximately 8,000 binary state changes per
second for each pixel, a color depth of approximately 2 bits
per voxel is possible. The system makes use of dithering and
halftoning to improve the color depth, which could improve
the rendering capability but also may change the properties
of our data sets. Unfortunately, we have no control over this
feature. On the API side, an 8-bit voxel color depth is used,
allowing 256 colors per voxel through the use of a palette.
With only a few distinct intensities, the display can still
produce useful visualizations (Figs. 1b and 1c) and can
support lumi-volume visualization (Fig. 1e).
On the other hand, the lumi-volumes created with our
method could require floating-point precision, with voxels
having a high dynamic range. We expect that the effectiveness and usability of our method will improve with the

MORA ET AL.: VISUALIZATION AND COMPUTER GRAPHICS ON ISOTROPICALLY EMISSIVE VOLUMETRIC DISPLAYS

227

TABLE 1
Summary of the Different Test Configurations

For tests 4 and 8 (Head 1D), Max varies in the range [10..90] with a 20-degree increment between each configuration.

gradual introduction of high-resolution IEVDs in the near
future. In addition, taking photographic pictures of the
Perspecta system was challenging because a long exposure
time must be used, which results in fuzzy images due to the
internal vibrations of the device.
We have also provided high-resolution results produced
from simulations in our evaluation in conjunction with the
direct experimentation on a Perspecta display. The simulated results offer a scalable evaluation of the capabilities of
our method.

5.5 Voxelization of 3D Meshes
In order to have a better idea of the rendering improvements brought to isotropically emissive displays, a simple
voxelization of the mesh-based models used in this paper
has been implemented. The voxelization was made with
OpenGL by rendering a specific image of our LF (e.g.,  ¼ 0
and ’ ¼ 0) as many times as there are slices in the final
voxelization. At each step, the front and back values of the
frustum are changed in order to consider only the part of
the model inside the given slice. The collection of all
obtained images gives us the voxelization. Note here that
voxel shading is always dependent on our reference
viewpoint.

6

RESULTS

6.1 Quantitative Analysis of the Results
The different tests for assessing our method and its
parameters are summarized in Table 1, including the
following:
the number of imagery projections in the rendered LF,
the resolution of each imagery projection in the
rendered LF,
. the size of the corresponding lumi-volume
constructed,
. the FOV attributes, max and max’ ,
. the sampling rate,
. the runtime in seconds per iteration, and
. the number of iterations used to construct the lumivolume.
An AMD athlon 64 X2 3800þ (2 GHz) was used for the
construction of the lumi-volumes, but only one core was
.
.

used. The UNC head data set has undergone several tests to
analyze the effect of the FOV in generating LFs. In various
“Head 1D” tests, the projections in an LF were generated by
varying the horizontal FOV ðmax Þ, while fixing the vertical
FOV ðmax Þ to zero. The “Head 2D” tests involved varying
both polar angles.
We evaluate the quality of a lumi-volume by measuring
the RMS error of the LF reconstructed by our lumi-volume.
The RMS error considers all the samples of the original LF
ðbi Þ and computes the difference between the reconstructed
LF ðri Þ at the original sampling locations using the
following formula:
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
uM
uP
u ðri  bi Þ2
ti¼1
:
M
The measurements are based on a 256-gray-level integer
scale. Fig. 9 shows the progressive changes of the RMS error
during the iterations of the EM algorithm.
The head data set has undergone several tests to analyze
the effect of the FOV on the reconstruction. The Head 1D tests
(Fig. 9a) have been conducted by adjusting the horizontal
FOV ðmax Þ, while the vertical FOV ðmax’ Þ was set to zero.
Note that even with a null vertical FOV, our problem still
remains unsolvable. The Head 2D and bunny tests (Fig. 9b)
have been realized by adjusting both angles.
First, in Fig. 9, one can easily observe that the algorithm
appears to converge to a solution as expected. Here, we see
that lumi-volumes produced using smaller FOVs tend to
result in decreasing RMS errors after each iteration of the
EM algorithm. However, for some of the input LFs created
with a wider FOV (e.g., Head 1D 70-90 degrees, Head 2D
40 degrees, Head 2D MIP 90 degrees, Aneurism MIP), the
RMS error reaches a local minimum and then begins to
increase.
In practice, halting the iteration process right after the
local minimum does not necessarily produce a lumi-volume
that will lead to the best visual results. More iteration steps
always seem to lead to sharper reconstructed LFs in our
experiments (Fig. 16, images 4e, 5a, and 5b).
Second, one can also observe that the RMS error is lower
with lumi-volumes for MIP-rendered LFs (Fig. 9c). This is
because MIP provides a more coherent rendering, as

228

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 15,

NO. 2,

MARCH/APRIL 2009

6.2 Qualitative Assessment of the Results
This section provides a visual assessment of the results in
several different aspects. The videos that accompany this
paper also support the qualitative visual analysis by
juxtaposing some of the original input and the corresponding reconstructed LFs. Some screen captures of the displays
showing the effectiveness of our technique on different
viewpoints are represented in Fig. 10. However, due to the
limitation of the display, we will mainly discuss the
synthetic results.

Fig. 9. RMS error according to the number of iterations.

discussed previously. For example, comparing the
RMS errors for the UNC Head 1D tests in the top and
bottom graphs, we can see clearly that MIP-rendered LFs are
better approximated by lumi-volumes than DVR-rendered
LFs. Third, we can also observe that the quality is affected by
the scene complexity of the original LF. For example, the
aneurism MIP data set produces a more complex LF than
those LFs of the MIP-rendered UNC Head, thus resulting in
a less accurate lumi-volume. The UNC Head 2D test has a
higher degree of freedom than those UNC Head 1D tests in
producing an LF, which also results in higher RMS errors.

6.2.1 The Transparency Effect
Because of the lack of occlusion on IEVDs, real or
simulated, one inevitable artifact is the transparency effect.
A surface that is opaque in the original input LF is likely
to become translucent in the reconstructed LF. This is
noticeable with the Head 1D example, where varying the
FOV parameter max from 10 to 180 degrees clearly
increases the transparency of the isosurface (Fig. 16). This
is also the case with the MIP LFs, and, especially, for the
aneurism MIP data set (Fig. 16, image 5d). Note that this is
more clearly visible in the comparative videos.
To emphasize this effect, we have created a 3D scene
(Fig. 7d), where the 3D text is partially hidden by a set
vertical columns. In the reconstructed LF (Fig. 11b)
obtained from a lumi-volume, the entire text is now
visible, because the columns have lost their capacity for
occlusion and appear to be translucent. In simulation, the
summation of all luminance along a ray may also result in
a saturated pixel color, due to the color quantization (e.g.,
the 256 gray scales). Nevertheless, this should not be the
case with isotropically emissive displays since the human
eyes have a higher dynamic range. Although one may
solve the saturation problem by scaling down the pixel
intensities, we have chosen not to reduce the brightness of
any images in this paper in order to make a consistent
visual comparison of the results.
The apparent translucency of an opaque surface is an
inevitable artifact of IEVDs. As demonstrated in Fig. 1, in
comparison with the naive approach for passing raw or
segmented volume data sets to an IEVD, our results have
actually shown the removal of a significant amount of
transparency. As such, the use of lumi-volumes offers a
tangible solution to applications where excessive levels of
apparent translucency is highly undesirable. This is also
supported by the direct comparison of the lumi-volumereconstructed LFs (Figs. 11 and 12) with the LFs produced
from an OpenGL voxelization of the same 3D models. In
these two cases, transparency is clearly reduced with our
technique, though not completely removed.
The MIP case is also very interesting (Fig. 16). The main
MIP structures are now transparent, which is useful for
distinguishing several overlapping objects. This could
represent a nice extension to MIP, but a user study will be
required to determine the practicality of such an extension.
Figs. 12d and 12e show two cross sections of our bunny
lumi-volume and illustrate the main difference between our
method and former volumetric reconstruction methods [29]
that produce a voxelization of the scene. Here, the lumivolume stores the required information inside the entire

MORA ET AL.: VISUALIZATION AND COMPUTER GRAPHICS ON ISOTROPICALLY EMISSIVE VOLUMETRIC DISPLAYS

229

Fig. 10. Photographic image captures of the Perspecta device. (a) Red channel of the bunny lumi-volume displayed on the Perspecta System.
(b) Head 2D (20 degrees) lumi-volume visualized from different viewpoints. (c) MIP aneurism using a full FOV.

data set, not only on the surface of the model. As such, the
data set is of no use in regular rendering applications.
In many visualization applications, a small amount of
transparency effects can actually assist viewers in their
visual interpretation of the internal structures or occluded
parts of the models being observed on IEVDs. The advantage
of our method is that the reconstructed rendering may allow
users to interpret the occluded parts from the front surface.
For instance, Fig. 11b shows the clearly discernable text,
IEEE TVCG, which is not distinguishable in the original LF.
We could, for instance, further imagine an LF acquired from
an airplane that would be processed with our technique to
discover what is hidden below trees.

6.2.2 Rendering Fidelity
The dimension reduction from a 4D LF to a 3D lumivolume also leads to some loss of high-frequency details,
as shown on the enlarged images in Fig. 12. In this
example, a maximum FOV of 40 degrees was chosen for
both  and  angles, which corresponds to an approximate
solid angle of 1.7 steradian. In this case, some highfrequency texture details, as well as some specular lighting
effects, have partially faded. This attenuation also tends to
get worse if we increase the FOV of the LF, especially for
opaque surface renderings. Increasing the size of the lumivolume does not seem to alleviate this problem (Fig. 14).
Color fading is also visible with MIP-rendered LFs but to a
lesser extent.
Despite the loss of some high-frequency details in
Fig. 12c, we can observe that the contours of the textures
are actually quite well preserved, and the overall perception

Fig. 11. The transparency effect. Surfaces become transparent inside
the reconstructed LF on emissive displays. The lumi-volume approach
(b), however, significantly reduces such an effect in comparison with
passing a voxelized volume of the scene directly to the emissive
devices (c).

of the surface with lumi-volumes is significantly superior to
a raw voxelization, as shown in Fig. 12a. To be more
general, all our examples clearly show that the reproduced
LFs create an adequate visual approximation of the original
LFs, despite the inherent limitations of such a technique. In
other words, unlike results that would be obtained from a
simple object voxelization passed to the display, results
produced from lumi-volumes are relatively close to what
would be displayed on a 2D screen, which is the main issue
this paper intended to address.
However, for realistic renderings, the FOV needs to be
restricted to a given solid angle, and it seems that a
40-degree FOV is a reasonable choice. Since these lumivolumes are created with a particular FOV, data can only
be interpreted on the Perspecta Display when users stand
at the correct viewing angle. If users move outside of the
appropriate viewing location, the volume becomes cloudy, and the visualization becomes incoherent (Fig. 13).

6.2.3 Sampling Rate Analysis
To evaluate the sampling quality of an original LF, we
rendered the UNC head data set at different sampling rates
on  (5, 10, 20, 40, and 80 images of 1922 pixels), with max
set to 40 degrees. Through simulation, we observed the
reconstructed LFs at two consecutive sampling angles, as
well as at an intermediate angle. As shown in Fig. 15 for the
lowest sampling rate (with five imagery projections, two
consecutive images differ from an angle of 20 degrees), one
can see that the reconstruction from lumi-volumes is very
good at the original sampling locations, with no visible
transparency effect. However, the reconstruction at intermediate locations is far from satisfactory. By increasing the
sampling rate, the reconstruction at intermediate locations
becomes better and better. When this phenomenon is
transferred to real displays, it means that viewers can
experience a more continuous visualization between two
consecutive sampling angles.
We also noticed that ringing artifacts appear if the
sampling rate is too low. We have found that using 80 samples
(when images differ by an angle of 1 degree) appears to be
good enough in this particular case. Finally, the reconstructed
LF converges to a more transparent rendering solution when
the sampling rate is increased.
We have made similar observations with other data
sets. From our experience, increasing the sampling rate
always seems to result in a better visualization, and the
EM algorithm also converges more consistently to a

230

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 15,

NO. 2,

MARCH/APRIL 2009

Fig. 12. The textured bunny data set. (a) An emissive-only rendering of a voxelized bunny data set. (b) An original LF projection. (c) A reconstructed
LF projection from the lumi-volume. (d) and (e) Horizontal (seen from below) and vertical slices extracted from the lumi-volume. Note that our 24-bit
color extraction produced dithering and saturation on those two images due to the high dynamic range of the lumi-volume.

specific lumi-volume. However, a high sampling rate does
require more time in the construction of a lumi-volume.

6.2.4 Lumi-Volume Sampling Rate
We have also evaluated the possible impact of using larger
lumi-volumes on the rendering by varying the size from
2563 to 5123 voxels. The same input LF (Head 1D,
max ¼ 40 degrees; 80 projections) was used, and the reconstructed LFs were observed. Four selected projections were
shown in Fig. 14.
Our observation indicates that the lumi-volume size has
relatively little impact on the rendering fidelity and the
transparency effect. The intrinsic properties of our lumivolume are still the same. The only effect we have noticed is
that the reconstructed LF appears to become smoother
when the size increases, which is as expected.

now be shown on emissive displays with almost the same
appearance as images found on a basic 2D screen.
More importantly, surface shading effects can now be
produced on IEVDs, and the apparent translucency effects
of supposedly opaque surfaces can be controlled at a
reasonable level, provided that an appropriate FOV is
chosen. As long as a lumi-volume is generated from an
LF consisting of a sufficiently large number of imagery
projections, viewers can have a seamless 3D experience
with full depth perception inside the FOV. This result can
be compared with the very recent work of Jones et al. [25]
where the diffuse surface of the display has been replaced

6.2.5 Overall Remarks on the Results
Our experimental study shows that our new technique has
managed to simulate traditional rendering effects on
isotropically emissive displays, which was once thought
to be impossible [30], [9]. Computer graphics images can

Fig. 13. Two views of the Head 1D 40-degree data set taken outside of
the FOV of the original LF.

Fig. 14. Reconstructed LFs using different lumi-volume sizes.

MORA ET AL.: VISUALIZATION AND COMPUTER GRAPHICS ON ISOTROPICALLY EMISSIVE VOLUMETRIC DISPLAYS

231

Fig. 15. Here, we present the quality of a reconstructed LF (Head 1D, max ¼ 40) according to the number of projections (columns). Rows (a) and (c)
represent the reconstructed LF at two consecutive exact sampling locations. Row (b) shows the reconstruction at the middle angle.

by a mirror covered with a holographic diffuser, similar to
[9]. Both methods have specific advantages and disadvantages. For instance, the user can experience a full 360-degree
horizontal FOV in [25] but at the cost of positioning the user
at a predefined distance d of the display and using head
tracking to ensure correct vertical parallax when required.
Shading is also very limited since this new display can only
work with binary images. In our case, the contribution of
several voxels to a ray allows having more gray levels, even
if the Perspecta display is also limited with respect to color
quantization.
Hardware modification will be needed to further improve rendering quality, increase the FOV, and reduce
transparency effects. While some recent approaches use
anisotropic spinning surfaces [30], [25], [9] to achieve this
goal, we plan to study the use of multiple spinning diffuse
surfaces. For instance, a different image could be projected
on both sides of our Perspecta Display’s spinning plane,
leading to the use of two different lumi-volumes.

problem on isotropically emissive displays and introducing
the correct rendering pipeline to produce lumi-volumes.
We have provided the first set of experimental evidence
to support the usability and scalability of this new technique.
This finding significantly broadens the application domain
of isotropically emissive displays and is expected to simulate
further research and deployment of the volumetric display
technology.
Our simulation indicates that the lumi-volume technique
can scale as the resolution and bandwidth of volumetric
display technology continues to increase. We are looking
forward to the continued enhancement of commercial
volumetric display systems such as the Perspecta system.
Our future work will include investigation into faster
reconstruction algorithms, investigation into reconstruction
algorithms that are not EM based, multiplane display
simulations, using head tracking, compression methods
for lumi-volumes, and perceptual merits of enhanced
transparency in MIP-based visualization on IEVDs.

7

ACKNOWLEDGMENTS

CONCLUSION

IEVDs offer an intuitive means for data visualization and
exploration. However, computer graphics on these displays
has been limited in the past to sending either voxelized 3D
meshes or medical data sets to the display, resulting in lowfidelity renderings. This paper has offered a way to alleviate
this issue by formalizing, for the first time, the rendering

The authors wish to thank Gregg Favalora and Joshua
Napoli (Actuality Systems) for helping them with the use of
the Perpecta Display SDK and the Purdue University
Envision Center for giving them access to a Perspecta
display. This work was supported in part by a grant from
EPSRC (EP/E001750/1).

232

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 15,

NO. 2,

MARCH/APRIL 2009

Fig. 16. The impact of the different parameters on reconstructed LFs, including the FOV angle and the number of EM steps. (1b) to (2b) Variation of
the horizontal FOV. (2c) to (2e) Variation of the horizontal and vertical FOV. (3a) X-ray of the original head data set. (3b) Original MIP LF. (3c) to (4b)
Variation of the horizontal FOV for the MIP head LF. (4c) Full FOV for the MIP head LF. (4d) to (5b) Impact of the number of EM steps on the
reconstructed LF. (5c) to (5d) MIP aneurism using a full FOV. (5e) X-ray of the original aneurism data set.

REFERENCES
[1]
[2]

[3]

[4]
[5]

E.H. Adelson and J.R. Bergen, “The Plenoptic Function and the
Elements of Early Vision,” Computation Models of Visual Processing,
M. Landy and J.A. Movshon, eds., MIT Press, 1991.
E.H. Adelson and J.Y.A. Wang, “Single Lens Stereo with a
Plenoptic Camera,” IEEE Trans. Pattern Analysis and Machine
Intelligence, vol. 14, no. 2, pp. 99-106, Feb. 1992.
L. Ahrenberg, I. Ihrke, and M.A. Magnor, “Volumetric Reconstruction, Compression and Rendering of Natural Phenomena,”
Proc. Fourth Int’l Workshop Volume Graphics, pp. 83-90, 2005.
B.G. Blundell and A.J. Schwarz, Volumetric Three-Dimensional
Display Systems. John Wiley & Sons, 2000.
B.G. Blundell and A.J. Schwarz, “The Classification of Volumetric
Display Systems: Characteristics and Predictability of the Image
Space,” IEEE Trans. Visualization and Computer Graphics, vol. 8,
no. 1, pp. 66-75, Jan. 2002.

B. Cabral, N. Cam, and J. Foran, “Accelerated Volume Rendering
and Tomographic Reconstruction Using Texture Mapping Hardware,” Proc. IEEE Symp. Volume Visualization, pp. 91-98, 1994.
[7] J.-X. Chai, S.-C. Chan, H.-Y. Shum, and X. Tong, “Plenoptic
Sampling,” Proc. ACM SIGGRAPH ’00, pp. 307-318, 2000.
[8] K. Chidlow and T. Moller, “Rapid Emission Tomography
Reconstruction,” Proc. Third Int’l Workshop Volume Graphics,
pp. 15-26, July 2003.
[9] O.S. Cossairt, J. Napoli, S.L. Hill, R.K. Dorval, and G.E. Favalora,
“Occlusion-Capable Multiview Volumetric Three-Dimensional
Display,” Applied Optics, vol. 46, no. 8, pp. 1244-1250, 2007.
[10] F. Dachille, K. Mueller, and A.E. Kaufman, “Volumetric Backprojection,” Proc. IEEE Symp. Volume Visualization and Graphics,
pp. 109-117, Oct. 2000.
[11] J.S. De Bonet and P. Viola, “Roxels: Responsibility Weighted 3D
Volume Reconstruction,” Proc. Seventh Int’l Conf. Computer Vision
(ICCV ’99), pp. 418-425, 1999.
[6]

MORA ET AL.: VISUALIZATION AND COMPUTER GRAPHICS ON ISOTROPICALLY EMISSIVE VOLUMETRIC DISPLAYS

[12] A. Dempster, N. Laird, and D. Rubin, “Maximum Likelihood from
in Complete Data via the EM Algorithm,” J. Royal Statistical Soc.,
Series B, vol. 34, pp. 1-38, 1977.
[13] N.A. Dodgson, “Autostereoscopic 3D Displays,” Computer, vol. 38,
no. 8, pp. 31-36, Aug. 2005.
[14] P. Eisert, E. Steinbach, and B. Girod, “3D Shape Reconstruction from Light Fields Using Voxel Back-Projection,”
Proc. Vision, Modeling, and Visualization Workshop (VMV ’99),
pp. 67-74, Nov. 1999.
[15] G.E. Favalora, “Volumetric 3D Displays and Application Infrastructure,” Computer, vol. 38, no. 8, pp. 37-44, Aug. 2005.
[16] L.A. Feldkamp, L.C. Davis, and J.W. Kress, “Practical Cone-Beam
Algorithm,” J. Optical Soc. of America, vol. 1, no. 6, pp. 612-619,
June 1984.
[17] D.T. Gering and M.W. Wells III, “Object Modeling Using
Tomography and Photography,” Proc. IEEE Workshop Multi-View
Modeling and Analysis of Visual Scenes (MVIEW ’99), pp. 11-18,
June 1999.
[18] R. Gordon, R. Bender, and G.T. Herman, “Algebraic Reconstruction Techniques (ART) for Three Dimensional Electron Microscopy and X-Ray Photography,” J. Theoretic Biology, vol. 29,
pp. 471-481, 1970.
[19] S.J. Gortler, R. Grzeszczuk, R. Szeliski, and M.F. Cohen, “The
Lumigraph,” Proc. ACM SIGGRAPH ’06, pp. 43-54, 2006.
[20] T. Grossman, D. Wigdor, and R. Balakrishnan, “Multi-Finger
Gestural Interaction with 3D Volumetric Displays,” ACM Trans.
Graphics, vol. 24, no. 3, p. 931, 2005.
[21] S.W. Hasinoff and K.N. Kutulakos, “Photo-Consistent 3D Fire by
Flamesheet Decomposition,” Proc. Ninth Int’l Conf. Computer
Vision (ICCV ’03), pp. 1184-1191, 2003.
[22] H.M. Hudson and R. Larkin, “Accelerated Image Reconstruction
Using Ordered Subsets of Projection Data,” IEEE Trans. Medical
Imaging, pp. 100-108, 1994.
[23] I. Ihrke and M.A. Magnor, “Image-Based Tomographic Reconstruction of Flames,” Proc. ACM SIGGRAPH/Eurographics Symp.
Computer Animation (SCA ’04), pp. 367-375, June 2004.
[24] I. Ihrke and M.A. Magnor, “Adaptive Grid Optical Tomography,”
Graphical Models, vol. 68, no. 5, pp. 484-495, 2006.
[25] A. Jones, I. McDowall, H. Yamada, M. Bolas, and P. Debevec,
“Rendering for an Interactive 360 Degrees Light Field Display,”
ACM Trans. Graphics (Proc. ACM SIGGRAPH ’07), vol. 26, no. 3,
2006.
[26] M. Levoy and P. Hanrahan, “Light Field Rendering,” Proc. ACM
SIGGRAPH ’96, pp. 31-42, 2006.
[27] Z. Lin and H.-Y. Shum, “On the Number of Samples Needed in
Light Field Rendering with Constant-Depth Assumption,” Proc.
IEEE Conf. Computer Vision and Pattern Recognition (CVPR ’00),
pp. 588-597, 2000.
[28] W. Matusik and H. Pfister, “3D TV: A Scalable System for
Real-Time Acquisition, Transmission and Autostereoscopic
Display of Dynamic Scenes,” ACM Trans. Graphics (Proc.
ACM SIGGRAPH ’04), vol. 23, no. 3, pp. 814-824, 2004.
[29] K. Mueller and R. Yagel, “Rapid 3D Cone-Beam Reconstruction
with the Simultaneous Algebraic Reconstruction Technique
(SART) Using 2D Texture Mapping Hardware,” IEEE Trans.
Medical Imaging, vol. 19, no. 2, pp. 1227-1237, 2000.
[30] R. Otsuka, T. Hoshino, and Y. Horry, “Transpost: A Novel
Approach to the Display and Transmission of 360 DegreesViewable 3D Solid Images,” IEEE Trans. Visualization and Computer
Graphics, vol. 12, no. 2, pp. 178-185, Mar. 2006.
[31] S.M. Seitz and C.R. Dyer, “Photorealistic Scene Reconstruction by
Voxel Coloring,” Int’l J. Computer Vision, vol. 35, no. 2, pp. 151-173,
1999.
[32] L.A. Shepp and Y. Vardi, “Maximum Likelihood Restoration
for Emission Tomography,” IEEE Trans. Medical Imaging, vol. 1,
pp. 113-122, 1982.
[33] C. Slinger, C. Cameron, and M. Stanley, “Computer-Generated
Holography as a Generic Display Technology,” Computer, vol. 38,
no. 8, pp. 46-53, Aug. 2005.
[34] B. Trifonov, D. Bradley, and W. Heidrich, “Tomographic
Reconstruction of Transparent Objects,” Proc. 17th Eurographics
Symp. Rendering (EGSR ’06), pp. 51-60, 2006.
[35] T. Tyler, A. Novobilski, J. Dumas, and A. Warren, “The Utility
of Perspecta 3D Volumetric Display for Completion of Tasks,”
Proc. 17th Ann. Symp. Electronic Imaging Science and Technology,
pp. 268-279, 2005.

233

[36] A.S. Wang, G. Narayan, D. Kao, and D. Liang, “An Evaluation of
Using Real-Time Volumetric Display of 3D Ultrasound Data for
Intracardiac Catheter Manipulation Tasks,” Proc. Fourth Int’l
Workshop Volume Graphics, pp. 41-45, 2005.
[37] F. Xu and K. Mueller, “Accelerating Popular Tomographic
Reconstruction Algorithms on Commodity PC Graphics Hardware,” IEEE Trans. Nuclear Science, vol. 52, no. 3, pp. 654-663, 2005.
Benjamin Mora received the DEA (MSc equivalent) degree and the PhD degree in computer
Science from Paul Sabatier University (Toulouse
3) in 1998 and 2001, respectively. Since 2004,
he has been a lecturer in the Department of
Computer Science, University of Wales Swansea. His research interests include general
computer graphics algorithms, volume visualization, ray tracing, and 3D displays.

Ross Maciejewski received the BS degree from
the University of Missouri, Columbia, and the MS
degree in electrical and computer engineering
from Purdue University. He is a PhD student in
electrical and computer engineering in the
School of Electrical and Computer Engineering,
Purdue University. His research interests include nonphotorealistic rendering and visual
analytics.

Min Chen received the BSc degree in computer
science from Fudan University in 1982 and the
PhD degree from the University of Wales in
1991. He is currently a professor in the Department of Computer Science, University of Wales
Swansea. In 1990, he took up a lectureship in
Swansea University. He became a senior
lecturer in 1998 and was awarded a personal
chair (professorship) in 2001. His main research
interests include visualization, computer graphics, and multimedia communications. He is a fellow of the British
Computer Society and a member of the IEEE, the Eurographics, and the
ACM SIGGRAPH.
David S. Ebert is a professor in the School of
Electrical and Computer Engineering, Purdue
University, as well as a university faculty
scholar, the director of the Purdue University
Rendering and Perceptualization Laboratory
(PURPL), and the Director of the Purdue
University Regional Visualization and Analytics
Center (PURVAC), which is part of the Department of Homeland Security’s Regional Visualization and Analytics Center of Excellence. He
performs research in novel visualization techniques, visual analytics,
volume rendering, information visualization, perceptually based visualization, illustrative visualization, and procedural abstraction of complex
massive data. He has been very active in the visualization community,
teaching courses, presenting papers, cochairing many conference
program committees, serving on the ACM SIGGRAPH Executive
Committee, serving as the editor in chief of IEEE Transactions on
Visualization and Computer Graphics, serving as a member of the
IEEE Computer Society’s Publications Board, serving on the National
Visualization and Analytics Center’s National Research Agenda Panel,
and successfully managing a large program in external funding to
develop more effective methods for visually communicating information.
He is a senior member of the IEEE.
. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

Visual-Analytics Evaluation

Generating Synthetic SyndromicSurveillance Data for Evaluating
Visual-Analytics Techniques
Ross Maciejewski, Ryan Hafen, Stephen Rudolph, George Tebbetts, William S. Cleveland,
and David S. Ebert ■ Purdue University
Shaun J. Grannis ■ Indiana University

V

isual analytics (VA) is often referred to as
a means for dealing with complex, large
information sources that require human
judgment to detect the expected and discover the
unexpected.1 However, the lack of readily available data sources compounds the problem of testing and evaluating tools that
fit these criteria. To effectively
This system generates
evaluate VA techniques, stansynthetic syndromicdard test data sets must be cresurveillance data for
ated that require both high- and
evaluating visualization
low-level analysis, during which
and visual-analytics
analysts sift through noise and
techniques. Modeling data
other confounding factors to
find and communicate unexfrom emergency room
pected results.
departments, the system
An ideal data source for testing
generates two years of patient
and
evaluating VA techniques is
data, into which system users
public-health
data. This data
can inject spatiotemporal
contains a low signal-to-noise
disease outbreak signals. The
ratio, making events difficult to
result is a data set with known
detect and prone to false posiseasonal trends and irregular
tives. Because VA is said to enoutbreak patterns.
hance knowledge discovery, such
data characteristics are useful in
analyzing what level of event detectability the VA
tools can enhance.
Syndromic-surveillance data contains patient
locations, derived syndrome classifications for
high-level analysis, and text-field collected patient
complaints for low-level analysis. High-level analysis includes searching for signals in the aggregated
18	

May/June 2009	

time series data from the hospital, whereas lowlevel analysis consists of looking at patient records
to verify the detected outbreaks. Furthermore, organizations must communicate to other agencies
any unexpected events that analysts find in the
data and potentially put into place quarantines.
Unfortunately, privacy concerns, the US Health Insurance Portability and Accountability Act guidelines, and a lack of electronic record availability in
many areas encumbers such data.
To circumvent these concerns, we developed a
novel system that lets users generate nonaggregated synthetic data records from emergency departments (EDs), using derived signal components
from the Indiana Public Health Emergency Surveillance System (Phess).2 ED data primarily comprises chief complaints (free text) the ED nurse
takes prior to the patient seeing a doctor. Phess
classifies these notes into eight categories (respiratory, gastrointestinal, hemorrhagic, rash, fever,
neurological, botulinic, and other) and uses them
as syndromic indicators to detect public-health
emergencies before such an event is confirmed by
diagnoses or overt activity. Our system synthesizes the daily, weekly, and seasonal syndromic
trends seen in Indiana EDs and lets users inject
outbreaks into the data. This practice creates a
data set in which analysts can be asked to solve a
problem with a known solution, allowing for standard evaluations among various techniques. Data
generated includes synthetic patient location and
demographic information (age and gender), along

Published by the IEEE Computer Society

0272-1716/09/$25.00 © 2009 IEEE

Related Work in Synthetic-Data Generation

C

ommon methods for releasing emergency department
data include aggregating, suppressing, and swapping
data, as well as adding random noise to the data.1 Analysts
could apply any of these methods to generate syndromicsurveillance data, for example, releasing the number of
patients seen per county rather than patient addresses or
longitudinal pairs or reporting patient ages by category
(under 18, 19–25, and so on) rather than individual ages.
Unfortunately, these methods compromise the data by
distorting the relationships between variables and reducing the analysis specificity. Furthermore, data aggregation can still violate privacy owing to the intersection of
overlapped data.
To overcome such issues, Donald Rubin proposed that,
instead of data reduction as a means of privacy preservation, agencies could release multiply imputed, synthetic
data sets.2 Jerome Reiter further analyzed the benefits and
limitations of this fully synthetic data approach.3 His studies found that fully synthetic data is a viable solution to
these issues, as long as analysts can estimate the models

for data imputation reasonably well from the synthetic
data. The work of Thomas Lotze and his colleagues has
characterized several methods for simulating syndromicsurveillance data.4 Their work is comparable to ours, but
the methodology and user interface we present use a different approach to syndromic surveillance.

References
	 1.	 L. Willenborg and T. De Waal, Elements of Statistical Disclosure
Control, Springer, 2001.
	 2.	 D.B. Rubin, “Discussion: Statistical Disclosure Limitation,” J.
Official Statistics, vol. 9, no. 2, 1993, pp. 461–468.
	 3.	 J.P. Reiter, “Releasing Multiply Imputed, Synthetic Public
Use Microdata: An Illustration and Empirical Study,” J. Royal
Statistical Soc.: Series A, vol. 168, part 1, 2005, pp. 185–205.
	 4.	 T. Lotze, G. Shmueli, and I. Yahav, “Simulating and Evaluating
Biosurveillance Datasets,” to be published in Biosurveillance:
A Health Protection Priority, T. Kass-Hout and X. Zhang, eds.,
Taylor & Francis, 2009.

We’ve used our system to create synthetic data
sets that closely follow real data trends seen in Phess.
Sample synthetic data sets are available for download at https://engineering.purdue.edu/PURVAC/
SyntheticData. These data sets comprise 32 hospitals spread across Indiana and various outbreak
injection scenarios for robust testing and analysis
of tools.

across syndromes, demographics, social networks,
and other factors. This data also contains text-field
descriptors and derived classifications, allowing for
low- and high-level analysis. Furthermore, analysts
can couple this data with news feeds, pharmacy
sales, and other data streams to create a rich environment for VA evaluation. Unfortunately, when
considering the release of sensitive data sets (such
as healthcare data) to the public domain, agencies
face competing objectives in terms of data quality
and breadth versus privacy. Agencies want to provide data to users with as much individual-level
data as possible while also guarding data providers’
confidentiality.
Our methods for synthetic-data creation include using seasonal decomposition of time series
by loess (locally weighted regression), an analysis
of population distribution using multiple kernel
density estimation models, and an analysis of the
age and gender of the populations and their correlation to chief complaints. (See the “Related Work
in Synthetic-Data Generation” for prior work in
this field.)

Synthetic-Data Creation

Time Series Simulation

ED data has great potential for use in developing and
evaluating novel algorithms and methods for analyzing multivariate, spatiotemporal patterns and relationships. Such data contains home addresses, age
and gender, and syndrome. The visit date provides
a temporal component coupled with spatial locations of patients, requiring a wide range of analysis

The first step in our synthetic-data creation is
choosing a model for the temporal component of
the data. The goal here is to generate a time series
of the number of patients a given ED sees daily.
The time series generated should closely reflect the
historical Phess data while maintaining privacy
about hospital business practices. The temporal

with the ED chief complaint and chief-complaint
classification.
The following are key features of our synthetic
data generation tool:
■■

■■

■■

■■

	

user-defined geographical placement of emergency departments,
adjustable population probability density control
for patient distribution (that is, the distribution
of patient’s home address in terms of latitude
and longitude),
adjustable demographic probability density controls for age and gender, and
user-defined spatiotemporal outbreak controls.

IEEE Computer Graphics and Applications

19

Visual-Analytics Evaluation
Square-root counts

Square-root daily count

12
11
10

component, an intraseasonal component that
models seasonal fluctuations, and an interannual
component that models long-term effects, such as
hospital growth:

Interannual

13
12
11
10

√
Yit = Tit + Sit + Dit + rit,

Intraseasonal
1
0
–1

Day of week
1
0
–1

Remainder
1
0
–1

2006.5

2007.0

2007.5

Time (year)
Figure 1. Decomposition of a total daily count time series. Removing the
day-of-week, interannual, and intraseasonal components from this time
series from one Indiana emergency department leaves independent,
identically distributed Gaussian white noise.

pattern of ED visits can be difficult to characterize because of variations in yearly periodicities.
Although the time series clearly exhibits yearly
periodicity, the onset, duration, and magnitude of
peaks and troughs varies from year to year.
Previous work has shown that if the data to be
simulated has a long historical basis from which to
draw, seasonal models with Arima (Autoregressive
Integrated Moving Average) can cope with yearly
quasiperiodicity. 3 However, many syndromicsurveillance systems, including Phess, contain only
a few years (or even months) of electronic data,
making such an approach difficult. Tom Burr and
his colleagues also addressed the quasiperiodicity
problem by using a Bayesian hierarchical model
with a scalable Gaussian function to model yearly
peaks, allowing the peak’s location and duration to
vary from year to year.4 This approach constrains
the shape of peaks to Gaussian. Our approach focuses on applying a nonparametric method, seasonal decomposition of time series by loess (STL).5 This
method allows for flexible modeling of the differing
onsets and shapes of seasonal peaks and lets us account for other components of variation also.
A time series can be viewed as the sum of multiple components, and STL is a method that separates a time series into these components using
the local smoothing method loess.6 We decompose
our daily patient count data into a day-of-week
20	

May/June 2009

where for the ith hospital on the tth day, Yit is the
original series, Tit is the interannual component,
Sit is the intraseasonal component, Dit is the dayof-week effect, and rit is the remainder.
We extract components on the square-root scale
of the original series to remove the dependence of
a signal’s variance on its mean. We first extract
the day-of-week seasonal component, Dit, directly
employing the method Robert Cleveland and his
colleagues described.5 We use degree-0 smoothing
for the day-of-week component with a seasonal
window of 39 days. After removing the day-ofweek component from the data, we use loess
smoothing to extract the interannual component,
Tit, using a window of 1,000 days. Finally, we apply
loess smoothing to the data with the day-of-week
and interannual components removed, thereby
obtaining the intraseasonal component, Sit, using
a window of 125 days.
After removing the day-of-week, interannual,
and intraseasonal components from the time series, the remainder is adequately modeled as independent, identically distributed Gaussian white
noise. Figure 1 shows a decomposition of squareroot daily counts for one hospital. In Figure 1, the
remainder, intraseasonal, interannual, and dayof-week terms sum to the total square-root daily
counts. Furthermore, the day-of-week and intraseasonal components are centered around a mean of
zero, so that sharing them with others wouldn’t
reveal information about the hospital they came
from. Because the square-root transformation removed the dependence of the mean and variance,
these two components can be added to any specified interannual trend component to obtain a viable syndromic time series. This serves as the basis
for our temporal simulation method.
The goal is now to synthetically recreate the
patient count series without disclosing information about the ED. As the Phess system comprises
a large range of EDs, we performed the STL decomposition on 32 hospitals, allowing us to capture slight variations across the state. For each
decomposition, we store the fitted daily intraseasonal and day-of-week component values. The interannual term contains information about the
ED growth and average number of patients. As
such, we don’t use the interannual term from our

Gastrointestinal—actual

6
5
4
3
2

Figure 2.
Real versus
simulated
syndromic time
series. The
daily counts,
broken down
by syndrome
classification,
show a slight
difference.

Gastrointestinal—simulated

6
5
4
3
2

Square-root daily count

Respiratory—actual
6
5
4
3
2

Respiratory—simulated
6
5
4
3
2

Total—actual

14
13
12
11
10
9

Total—simulated

14
13
12
11
10
9

2006.0

2006.5

2007.0

2007.5

2008.0

Time (year)

historical data, replacing it with a user-specified
mean for the average number of patients a given
ED sees per day. The simulation’s randomness
comes from the remainder term, which for each
hospital was Gaussian and had a variance close to
0.28. The remainder term has variation, but it’s
small. The mean variance is 0.28, and the first and
third quartiles are 0.270 and 0.308. Tight distribution is because of the square-root transformation,
which we found did an excellent job of stabilizing
the variance.
To create a synthetic hospital, the user inputs
the desired mean total daily cases the hospital
sees, M, from which the fixed interannual component T is calculated as
T=

∑
M−

n
j=1

2

(Sij + Dij )
n

− 0.28 .

With this, we obtain the following equation for
generating the synthetic time series data:
Zit =(T + Sit + Dit + rit)2,
where Z is the synthetically generated series and rit
is randomly generated white noise with mean 0 and
	

variance 0.28. We randomly choose day-of-week
(Dit) and intraseasonal (Sit) components from one
of the 32 historical decomposed ED signals.
This approach supplies us with only the total
number of patients the ED sees each day. The next
step is to classify these patients into their appropriate syndromic category (gastrointestinal, constitutional, respiratory, rash, neurological, hemorrhagic,
botulinic, and other). To this end, we apply a multinomial distribution to categorize patients. The
parameters of the multinomial distribution are
based on a daily average of cases seen in a given
category across the set of 32 Phess hospitals. This
preserves the differing seasonalities that occur
from one classification to another in syndromicsurveillance data.
An alternative to this approach of first generating the total counts and then classifying them
would be to simulate the data from each classification and then add them together to get the total.
We didn’t employ this method because the STL
decomposition doesn’t deal well with low counts,
and some classifications, such as botulinic, have
many 0-count days.
Figure 2 compares a real syndromic time series to our synthetically generated one, showing
IEEE Computer Graphics and Applications

21

Visual-Analytics Evaluation
Figure 3.
Kernel density
estimations.
The
distributions of
these patient
locations for
four Indiana
Public Health
Emergency
Surveillance
System (Phess)
hospitals would
be difficult
to model
parametrically.

the time series of the total daily counts and daily
counts of respiratory and gastrointestinal classifications. The simulated data set uses the fitted intraseasonal and day-of-week effects from the real
data and a constant mean, which can be set to the
overall mean of the real data’s total counts. Because
we obtained the simulated data’s respiratory and
gastrointestinal components using multinomial
distribution parameters, which we obtained by averaging over all of the hospitals, we can see a slight
difference between the actual and simulated signals. The yearly peaks correlate well, indicating that
we’ve preserved the original data’s key features.

Spatial-Distribution Estimation
The second step in our synthetic-data creation is
to assign patients an appropriate geospatial location. Previous work in syndromic-surveillance
data has focused specifically on generating time
series data for analysis.3,4,7 Our system generates
both temporal and spatial components for the
data. To simulate the patient’s location, we first
analyze the patient distribution from the 32 Phess
EDs, employing a kernel density estimation.8 The
following equation defines the multivariate kernel
density estimation:
f̂ h ( x) =

1
N

n

1

∑h

d

i=1

 x − X i 
K 
 h .

We chose to employ the Epanechnikov kernel:
K ( u) =

3
(1 − u2 )1( u ≤1).
4

In these equations, h represents the multidimensional smoothing parameter, N is the total number of
samples, d is the data dimensionality, and the function 1( u ≤1) evaluates to 1 if the inequality is true
and zero for all other cases. This estimate scales the
parameter of estimation by letting the kernel width
vary on the basis of the distance from Xi to the kth
nearest neighbor in the set comprising N - 1 points:
ˆf h ( x) = 1
N

N

1

i=1

i,k

∑d

 x − X i 

K 
 di,k .

Here, the window width of the kernel placed on
the point Xi is proportional to di,k (where di,k
22	

May/June 2009

is the distance from the ith sample to the kth
nearest neighbor), so that data points in regions
with sparse data will have flatter kernels. Such a
method lets us have various kernels that can better fit dense populations (urban) and sparse populations (rural).
Figure 3 shows the density distribution for four
Indiana hospitals. Such distributions would be extremely difficult to model parametrically because
the density distributions follow various population
groupings across the state, accounting for nearby
towns and other seemingly random population centers. As such, to model patient locations, we created
an interface that lets users define custom probability
models centered around their synthetic hospital.
When a user creates a hospital, the user can define the density window’s width and height. The
system then lays a 100 × 100 grid onto the width
and height and assigns probability density to each
square of the grid. This in effect will aggregate patients across regions of the state proportional to
the height and width. If the user employs the default width and height of 100 miles by 100 miles,
the system will aggregate the patient location to
a 1-mile block. It then uniformly distributes subjects in their associated grid cell. We chose these
parameters on the basis of observations from all
Phess EDs that patients were most likely to visit
EDs within 50 miles of their primary residence. To
manipulate the probability density on the grid, the
user can create polygons and increase or decrease
the probability density in the region the polygon
covers. The system normalizes the final probability
density to integrate to 1 and uses it to randomly
generate patient locations.
Our system also lets users import data on a
100 × 100 grid representing the density they wish
to use. The user specifies the grid’s width and
height (in miles), and the system normalizes the
values in the grid to integrate to 1. We also supply
users with sample density distributions as well as a
default Gaussian distribution. However, the sample density distributions are based on data from
Indiana and, as such, are influenced by surrounding geography. The sample distributions are based
on our kernel density estimation of the hospital’s
population distribution. But, in doing a kernel

(a)

(b)

(c)

(d)

(b)

Age

Probability
Age

density estimation, our system doesn’t take into
consideration boundary conditions of geographical
features, such as bodies of water. Thus, geographical features’ heavy influence makes it important
to let users create and manipulate their own distributions to obtain better simulations. Figure 4
illustrates the use of our system in generating a
probability distribution using an imported kernel
density estimation.

Age and Gender Distribution Estimation
The third step in our synthesis is to assign the
patient’s age and gender. In a method similar to
	

(c)

Age

Probability

(a)

Probability

Probability

Figure 4. Probability distributions. Our system generated the following distributions: (a) the default density
distribution when creating a new hospital, (b) an imported patient density distribution based on a kernel
density estimation of patient visits, (c) removing the probability that a patient will come from Lake Michigan,
and (d) one day’s worth of simulated patients overlaid on the population density distribution.

(d)

Age

that of assigning the patient’s location, we use
user-defined probability functions for generating
a patient’s age and gender. Figure 5a represents
the patient age distribution averaged across all
32 Phess EDs used in our data synthesis. This age
distribution is relatively constant across all Phess
EDs, minus the specialty hospitals such as children’s hospitals and disease-specific hospitals. Users can use this as the default probability model
for patient age distribution, or they can modify the
distribution through an interactive widget.
The plot’s probability is relative, meaning that the
area under the curve will always integrate to one.
IEEE Computer Graphics and Applications

Figure 5. Age
distribution.
The red lines
indicate a
user-defined
probability
function, and
the blue lines
indicate the
synthesized
data’s
distribution.
(a) The default
probability
function is
based on
data from
the Indiana
Phess system.
(b) Users can
customize the
distribution by
adding control
points or
manipulating
the curve.
The resulting
data curves
in (c) and (d)
show that
the resulting
simulated
distributions
match the
input model
distributions.

23

Visual-Analytics Evaluation

Figure 5 shows both a default and user-defined probability function and the resulting output data curve.
Figure 5a is the default control mechanism used in
our synthetic-data-creation system. As Figure 5c
shows, users can add control points to the plot and
manipulate the curve by dragging the points around
the screen. For defining a patient’s gender, users simply input the desired percentage breakdown. Default
values are set at 50 percent male and 50 percent female and are adjustable. Figures 5b and 5d show the
resulting data curves for age distribution.

Correlating Chief Complaints
The final step in our synthesis is to map the patient’s syndrome to an actual chief-complaint
text. Given the patient’s age, gender, and chiefcomplaint classification, we randomly assign a chief
complaint that matches the chief-complaint classification generated in the time series simulation.
The system also correlates this chief-complaint
classification to gender and age to ensure that appropriate complaints are generated. For example,
assigning a chief complaint of “vaginal bleeding”
to an 8-year-old male classified as hemorrhagic
wouldn’t make sense.
The chief complaints we use here represent a set
of the most common chief complaints for each
category correlated with age and gender across the
entire Phess. We use the most common complaints
to simulate both real-world noise in the data as
well as protect any potentially private information found in unique chief complaints. Often the
chief complaints contain typographical errors,
variations on the same complaint (“fever, cough”
versus “cough, fever”) and abbreviations (SOB for
shortness of breath). We maintain these errors to
better simulate real-world data.

Synthetic Outbreak Injection
Once we simulated the baseline data, we developed
the disease injection models so that users can create
known outbreaks in which to test their detection
tools and algorithms. Previous work in synthetic
outbreak generation falls into two main categories:
creating a multivariate mathematical model to produce the signal and defining a series of parameters
to enable the generation of a controlled feature set.
In terms of creating complex mathematical models based on population distributions, highway
travel, and spread vectors, the IBM Eclipse STEM
(Spatiotemporal Epidemiological Modeler) project
is doing significant work.9 The project focuses on
helping scientists and public-health officials create
and use models of emerging infectious diseases. It
uses built-in geographic information system (GIS)
24	

May/June 2009

data for almost every country in the world, including travel routes for modeling disease spread. The
difficulty of this type of approach is that these detailed models often require a great deal of handcrafting and fine-tuning.
Instead, we adopted the approach of parameterizing clusters using simpler mathematical models,
as Chris Cassa has done.10 In Cassa’s work, users
can create clusters for their supplied baseline data
by selecting the cluster’s center relative to the hospital, temporal duration, magnitude, and curve
type. The cluster types are based on three mathematical distribution models: random, linear, and
exponential. Our extensions to this work include
generating baseline patient data and arbitrarily
shaped clusters (as opposed to only circular) and
using seasonal data shifts to approximate appropriate cluster size. Furthermore, we let the user
create epidemiological distribution curves for the
outbreak. This epidemiological distribution cluster
lets users set the probability that the disease outbreak will affect a certain age and gender more, by
providing them with a linked probability distribution control for these parameters.
To create a cluster using our system, the user
draws a polygon on the map. The user then selects
the date that the cluster begins, the duration of the
cluster in days, and the type of cluster (respiratory,
gastrointestinal, and so on) corresponding to one
of the eight classification categories in our system.
To determine the appropriate number of patients in
the cluster, our system calculates the noise profile of
the model over the particular space-time period in
which the outbreak is occurring. By this, we mean
that if the user selects an outbreak with a duration
of five days, the system calculates the mean and
standard deviation of the baseline data on the basis
of the related syndrome counts over these five days.
The system then uses the standard deviation over
the time period to determine the total number of
points that will occur in this cluster. This is scalable
through the user’s control of cluster size, where the
minimum cluster size is .5 to 5 times the standard
deviation. This allows the user to create ranges of
clusters with variable degrees of difficulty to find.
By using the standard deviation as a means of determining the cluster size, we can account for variability among seasonal trends and create detectable
clusters in any season without the user needing a
priori knowledge of data trends.
Once the cluster size is determined, the number of
cases the system generates for each day of the cluster
is based on the user-defined distribution models. We
provide users with a baseline epidemiological distribution curve based on a log-normal plot, as Figure 6a

Patient count

(c)

Patient count

(a)

0

2

4

(b)

6
Day

8

10

12

shows. Figure 6b shows a user-defined epidemiological distribution, and Figures 6c and 6d show the resulting output patient counts. We chose log-normal
as the default distribution because of the work of Colin Goodall and his colleagues, which showed that
many natural disease outbreaks follow this type of
distribution (as opposed to Cassa’s exponential and
linear distributions).11
The system then assigns the created patients
an age and a gender on the basis of user-specified
probability controls for the model. In this way, we
move toward the more-complicated mathematical
approaches the Eclipse project is developing while
still maintaining a degree of simplicity. Through the
user-generated probability functions, the system can
now manufacture the disease clusters to affect only
certain age ranges and certain genders, allowing for
a larger model specificity. Finally, the system uniformly distributes patient locations inside the polygon. Figure 7 (next page) provides a time series view
of an injected cluster of varying magnitudes.

Scenario Creation
Using the methods we’ve described, a user can now
generate any number of EDs for locations across
the US, given some inherent knowledge of the patient distributions, or using the defaults provided.
	

Figure 6.
Outbreak
curves.
Comparing (a)
an outbreak
epidemiology
curve based on
a log-normal
distribution
and (b) its
resulting plot
of the actual
number of
patients to (c)
a user-created
outbreak curve
and (d) its
resulting plot
of patients
verifies that
the distribution
input controls
are valid.

0
(d)

2

4

6
Day

8

10

12

Once the system generates the baseline data, a
user can then consider what stories and scenarios
to create and couple the synthesized data from our
system with other systems, creating a more robust
stream of data for evaluation.
Previous work in creating synthetic data for
testing analytic tools includes the Threat Stream
Generator, which focuses on creating a particular
scenario and expressing this as data in the forms of
news articles, security agency reports, geographically referenced locations, population movement
during catastrophic events, and a variety of other
data, all linked together to tell a complete event
story.12 In terms of disease outbreaks, the Eclipse
STEM project focuses on creating data that will
simulate large-scale epidemics across populations.9
These systems let users create complex scenarios
that cover a broader set of the population or tell
a complex story and create various data sources
to corroborate and unify a given scenario. In
contrast, our synthetic-data-generation system
focuses on a distinct segment of the population:
individuals who visit EDs. This is a well-defined,
often-studied population for which data is typically not publicly available. By creating a system
in which users can generate baseline syndromicsurveillance data, we’ve enabled other systems to
IEEE Computer Graphics and Applications

25

Visual-Analytics Evaluation
14

12

Patient count

10

8

6

4

Base
Base + .5σ
Base + .2σ

2

0

29 June
2006

2 July
2006

5 July
2006

8 July
2006

11 July
2006

14 July
2006

17 July
2006

20 July
2006

23 July
2006

26 July
2006

29 July
2006

1 August
2006

4 August
2006

Date
Figure 7. A time
series view of
an emergency
department’s
respiratorysyndrome
counts.
The curves
represent the
baseline data,
baseline data
injected with
a cluster of
magnitude .5s,
and baseline
data injected
with a cluster
of magnitude
2s.

add stories to create more-robust data sets for VA
application training. Another comparable system
for generating disease outbreaks is Project Mimic,
which provides methods for describing and simulating biosurveillance data, creating realistic mimics of their own data sets.7
In Figure 8, the user created a disease outbreak
cluster representing the release of waste water into
Lake Michigan. In Figure 8a, the user has drawn
a cluster in which people living close to the lake
are appearing with gastrointestinal illnesses a few
days after the Fourth of July, a prime swimming
holiday; in Figure 8b, the system has generated the
corresponding epidemic curve. Furthermore, the
user adjusted the age distribution of this cluster
so that the syndrome is affecting only the younger
portion of the population, as Figure 8c shows.
In another sample scenario, a chemical agent is
being developed. People living near lab are getting
sick, producing clusters of disease outbreaks. A user
could correlate this data with a series of news stories and reports from the Threat Stream Generator
and produce another segment of the population to
analyze. Furthermore, in creating complex outbreaks
using the Eclipse STEM software, a user could add
these outbreaks to the baseline data our system generated and work out population distributions to see
how such outbreaks would affect ED visits.
Only our system lets users create viable spatiotemporal data for analysis and testing of any spatiotemporal techniques. A user can generate baseline data
and couple it with known outbreaks, allowing a
variety of testing from algorithm validation to expert evaluation of system tools. Multiple communi-

26	

May/June 2009

ties can benefit from the release of a standard set
of test data to evaluate various spatiotemporal algorithms. Moreover, the generated data set isn’t trivial.
The 32-hospital data scenario in Figure 8 comprises
more than 1 million patient records, averaging approximately 2,000 records per day for the two-year
span. Overall, researchers can use data our system
generates for a wide variety of testing and evaluation
applications, from large data visualizations to information visualization topics to spatiotemporal cluster
detection algorithms to VA scenarios.
It’s also important to characterize ways in which
researchers can use this data to evaluate VA tools.
In syndromic surveillance, algorithms exist to
detect spatial, temporal, and spatiotemporal outbreaks with a given level of specificity. The first
area that VA can improve is the time it takes for a
decision maker to classify an event as a false positive or as a true outbreak. An applicable scenario is
one in which several false-positive outbreaks occur
along with a true outbreak. These cases would be
identifiable only by looking further into the patient case reports. A user with a suite of VA tools
could more readily identify the true alert.
The second area in which to evaluate VA is analyzing outbreaks where the signal-to-noise ratio
leaves the outbreak undetected by conventional
syndromic-surveillance algorithms. Can VA tools
be created that would provide a user with enough
insight into the data to find such outbreaks?
Finally, such data can be useful in the strictly
analytical sense. Researchers can test and validate
new syndromic-detection algorithms against simple outbreak baselines or large, complex-shaped

(b)

(a)

(c)

outbreaks to determine which algorithms perform
best in various analytic situations.
Previous work in creating VA tools to analyze
such data can be found in the work of Ross Maciejewski and his colleagues.13 However, this work
focuses solely on methods for analyzing multivariate syndromic-surveillance data. Their system
presents a suite of tools for analyzing data similar
to what we’ve presented. Further work is needed
to test their tools with synthetic data (such as the
data presented in this work) to begin to evaluate
the usefulness of such tools.

ter generation of the baseline population distribution to look at local geographic features. We also
hope to incorporate work on space-time dependencies to model repeat visits among patients, because
our current methods fail to handle this particular
instance. Furthermore, our output doesn’t look at
spatiotemporal dependencies between the syndromes,
age, and gender. Our analysis of the data seems to indicate that seasonal trends don’t exhibit a spatiotemporal component; however, they do have dependencies
on the patient’s age and gender. Future work must be
done to better capture these correlations.

A

Acknowledgments

lthough it’s likely that the seasonal trends
seen in EDs within the Phess system are likely
to be similar across the Midwest and perhaps the
entire US, it’s not likely that these trends would
translate to other climatological regions owing to
variations among healthcare systems.
The next phase of this work is to take the synthetic data such a system generates and incorporate it into current syndromic-surveillance
systems to begin an evaluation of these tools. Scenarios created under our system can range from
small, low-signal-strength outbreaks, to high, easily detectable clusters. Researchers can use such
scenarios to test where algorithms fail in detecting
outbreaks and where VA can begin helping users
notice clusters that are difficult to detect.
Future work includes developing more-realistic
outbreak injection methods and working on a bet	

Figure 8. Our
interface for
generating
syndromic
outbreaks.
(a) We injected
a spatial cluster
drawn near the
Lake Michigan
shore,
representing
gastrointestinal
illnesses
directly
following the
Fourth of July
holiday.
(b) The system
automatically
creates the
corresponding
epidemic
curve. (c) We
then modify
the correlated
age curve to
simulate an
outbreak that
is affecting
primarily
children and
young adults.

We thank the Indiana State Department of Health for
providing the data. The US Department of Homeland
Security Regional Visualization and Analytics Center
(RVAC) Center of Excellence and the US National
Science Foundation (grants 0081581, 0121288, and
0328984) funded this research.

References
	 1.	 J.J. Thomas and K.A. Cook, eds., Illuminating the
Path: The R&D Agenda for Visual Analytics, IEEE
Press, 2005.
	 2.	 S.J. Grannis et al., “The Indiana Public Health
Emergency Surveillance System: Ongoing Progress,
Early Findings, and Future Directions,” Proc. Am.
Medical Informatics Assoc. Ann. Symp., Am. Medical
Informatics Assoc., 2006, pp. 304–308.
IEEE Computer Graphics and Applications

27

Visual-Analytics Evaluation

	 3.	 B. Reis and K. Mandl, “Time Series Modeling for
Syndromic Surveillance,” BMC Medical Informatics
and Decision Making, vol. 3, 2003, article 2; www.
biomedcentral.com/1472-6947/3/2.
	 4.	 T. Burr et al., “Accounting for Seasonal Patterns
in Syndromic Surveillance Data for Outbreak
Detection,” BMC Medical Informatics and Decision
Making, vol. 6, 2006, article 40; www.biomedcentral.
com/1472-6947/6/40.
	 5.	 R.B. Cleveland et al., “STL: A Seasonal-Trend
Decomposition Procedure Based on Loess,” J. Official
Statistics, vol. 6, no. 1, 1990, pp. 3–73.
	 6.	 W.S. Cleveland and S.J. Devlin, “Locally Weighted
Regression: An Approach to Regression Analysis by
Local Fitting,” J. Am. Statistical Assoc., vol. 83, no.
403, 1988, pp. 596–610.
	 7.	 T. Lotze, G. Shmueli, and I. Yahav, Simulating
Multivariate Syndromic Time Series and Outbreak
Signatures, research paper RHS-06-054, Robert H.
Smith School of Business, Univ. of Maryland, May
2007; http://ssrn.com/abstract=990020.
	 8.	 B.W. Silverman, Density Estimation for Statistics and
Data Analysis, Chapman & Hall/CRC, 1986.
	 9.	 D.A. Ford, J.H. Kaufman, and I. Eiron, “An Extensible
Spatial and Temporal Epidemiological Modelling
System,” Int’l J. Health Geographics, vol. 5, no. 1,
2006, p. 4.
	10.	 C.A. Cassa, “Spatial Outbreak Detection Analysis
Tool: A System to Create Sets of Semi-synthetic Geospatial Clusters,” master’s thesis, Dept. of Electrical
Eng. and Computer Science, Massachusetts Inst.
Technology, 2004.
	11.	 C.R. Goodall et al., “A System for Simulation:
Introducing Outbreaks into Time Series Data,”
Advances in Disease Surveillance, vol. 2, 2007, p. 199;
www.isdsjournal.org/article/viewArticle/933.
	12.	 M.A. Whiting, J. Haack, and C. Varley, “Creating
Realistic, Scenario-Based Synthetic Data for Test
and Evaluation of Information Analytics Software,”
Proc. Conf. Beyond Time and Errors (BELIV 08), ACM
Press, 2008, pp. 1–9.
	13.	 R. Maciejewski et al., “Understanding Syndromic
Hotspots: A Visual Analytics Approach,” Proc. IEEE
Symp. Visual Analytics Science and Technology (VAST
08), IEEE CS Press, 2008, pp. 35–42.

Ross Maciejewski is a PhD student in electrical and
computer engineering at Purdue University. His research
interests include nonphotorealistic rendering, volume
rendering, and visual analytics. Maciejewski has an MS
in electrical and computer engineering from Purdue University. Contact him at rmacieje@purdue.edu.
Ryan Hafen is a PhD student in statistics at Purdue University. His research interests include explor28	

May/June 2009

atory data analysis and visualization, massive data,
computational statistics, time series, modeling, and
nonparametric statistics. Hafen has a master’s in statistics from the University of Utah. Contact him at
rhafen@purdue.edu.
Stephen Rudolph is a master’s student in electrical computer engineering at Purdue University. His
research interests include casual information visualization and visual analytics. Rudolph has a BS in
computer systems engineering from Arizona State
University. Contact him at srudolph@purdue.edu.
George Tebbetts is an undergraduate student in computer science and mathematics at Purdue University. His
research interests include visual analytics and computer
graphics. Contact him at gtebbett@purdue.edu.
William S. Cleveland is the Shanti S. Gupta Distinguished Professor of Statistics and courtesy professor of computer science at Purdue University. His
research interests include statistics, machine learning,
and data visualization. Cleveland has a PhD in statistics from Yale University. He’s the author of The
Elements of Graphing Data (Hobart Press, 1994)
and Visualizing Data (Hobart Press, 1993). Contact
him at wsc@stat.purdue.edu.
Shaun J. Grannis is an assistant professor of family
medicine at Indiana University and a medical informatics research scientist at the Regenstrief Institute.
His research interests include developing, implementing, and studying technology to overcome the challenges of integrating data from distributed systems for
use in healthcare delivery and research. Grannis has
an MD from Michigan State University. Contact him
at sgrannis@regenstrief.org.
David S. Ebert is a professor in the School of Electrical
and Computer Engineering at Purdue University, a University Faculty Scholar, director of the Purdue University
Rendering and Perceptualization Lab, and director of the
Purdue University Regional Visualization and Analytics
Center. His research interests include novel visualization
techniques, visual analytics, volume rendering, information visualization, perceptually based visualization, illustrative visualization, and procedural abstraction of
complex, massive data. Ebert has a PhD in computer
science from Ohio State University and is a fellow of the
IEEE and member of the IEEE Computer Society’s Publications Board. Contact him at ebertd@purdue.edu.
For further information on this or any other computing
topic, please visit our Digital Library at www.computer.
org/csdl.

Article

A visual analytics process for maritime
response, resource allocation and risk
assessment

Information Visualization
2014, Vol. 13(2) 93–110
Ó The Author(s) 2012
Reprints and permissions:
sagepub.co.uk/journalsPermissions.nav
DOI: 10.1177/1473871612460991
ivi.sagepub.com

Abish Malik1, Ross Maciejewski2, Yun Jang3, Silvia Oliveros1,
Yang Yang1, Ben Maule4, Matthew White4 and David S Ebert1

Abstract
In this paper, we present our collaborative work with the U.S. Coast Guard’s Ninth District and Atlantic Area
Commands, in which we develop a visual analytics system to analyze historic response operations and assess
the potential risks in the maritime environment associated with the hypothetical allocation of Coast Guard
resources. The system includes linked views and interactive displays that enable the analysis of trends, patterns, and anomalies among the U.S. Coast Guard search and rescue (SAR) operations and their associated
sorties. Our system allows users to determine the change in risks associated with closing certain stations in
terms of response time and potential lives and property lost. It also allows users to determine which stations
are best suited to assuming control of the operations previously handled by the closed station. We provide
maritime risk assessment tools that allow analysts to explore Coast Guard coverage for SAR operations and
identify regions of high risk. The system also enables a thorough assessment of all SAR operations conducted
by each Coast Guard station in the Great Lakes region. Our system demonstrates the effectiveness of visual
analytics in analyzing risk within the maritime domain and is currently being used by analysts at the Coast
Guard Atlantic Area.

Keywords
Visual analytics, risk assessment, resource allocation, Coast Guard

Introduction
As modern datasets increase in size and complexity, it
becomes increasingly difficult for analysts and decision
makers to extract actionable information for effective
decision making. In order to better facilitate the exploration of such datasets, tool sets that allow users to
interact with their data and assist them in their analysis
are required. Furthermore, such datasets can be utilized to explore the consequences and risks associated
with making decisions, thereby providing insights to
analysts and aiding them in making informed
decisions.
Besides the sheer volume and complexity of such
datasets, analysts must also deal with data quality
issues, including uncertain, incomplete and

contradictory data. Moreover, analysts are often faced
with different decisions and are required to weigh up
all of the possible consequences of these decisions
using these datasets to arrive at a solution that minimizes the associated risks within a given time constraint. Using traditional methods of sifting through

1

Purdue University, West Lafayette, IN, USA
Arizona State University, Tempe, AZ, USA
3
Sejong University, Seoul, South Korea
4
United States Coast Guard, Cleveland, OH, USA
2

Corresponding author:
Abish Malik, Purdue University, 500 Central Drive #226, West
Lafayette, IN 47907, USA.
Email: amalik@purdue.edu

94
sheets of data to explore potential risks can be highly
inefficient and difficult because of the nature and size
of these datasets. Therefore, advanced tools are
required that enable a more timely exploration and
analysis. Our work focuses on the use of visual analytics1,2 in the realm of risk assessment and analysis, and
demonstrates the effectiveness of visual analytics in
this domain. The work described in this paper is based
on the application of visual analytics to analyze historic
response operations and assess the potential risks in
the maritime environment based on notional station
closures. Our work was done in collaboration with the
U.S. Coast Guard’s Ninth District and Atlantic Area
Commands (See Appendix 1), which are responsible
for all Coast Guard operations in the five U.S. Great
Lakes. In particular, we focused on the auxiliary stations that are staffed by Coast Guard volunteers and
civilians. These auxiliary stations assist their parent
stations with their operations and usually operate on a
seasonal basis, using a small fleet of boats for conducting their operations. However, the number of auxiliary
personnel volunteering their time at these stations has
decreased over recent years. This has required Coast
Guard analysts to develop possible courses of action
and analyze the risks and benefits associated with each
option. Several options include seasonal or weekend-only
staffing of these units, or, at worst, closure. Closure, however, may involve increased risks to the boating public
and a complete analysis of the risks associated with closing an auxiliary station needs to be undertaken. The
results of this type of analysis would assist the decision
makers in determining the optimal course of action.
In particular, the analysts are interested in determining the spatial and temporal distribution of
response cases and their associated sorties (a boat or
an aircraft deployed to respond to an incident) for all
search and rescue (SAR) operations conducted in the
Great Lakes and how closing certain auxiliary stations
affects the work load of the stations that absorb these
cases. Coast Guard policy mandates the launch of a
sortie within 30 minutes and an asset (boat or aircraft)
on scene within 2 hours of receiving a distress call.3
The closure of auxiliary stations implies longer
response times, which could potentially translate into
the loss of lives and property.
To address these option evaluation challenges, we
developed a visual analytics system that supports decision making and risk assessment and allows an interactive analysis of trends, patterns and anomalies among
the U.S. Coast Guard’s Ninth District operations
and their associated sorties. Our system, shown in
Figure 1, allows enhanced exploration of multivariate
spatiotemporal datasets. We have incorporated
enhanced tools that enable maritime risk assessment
and analysis. Our system includes linked

Information Visualization 13(2)
spatiotemporal views for multivariate data exploration
and analysis and allows users to determine the potential increase or decrease in risks associated with closing
one or more Coast Guard stations. The system
enables a thorough assessment of all operations conducted by each station. In addition, the system provides analysts with the tools to determine which Coast
Guard stations are better suited to assume control of
the operations of the closed station(s) by comparing
the distances from available stations to all SAR cases
previously handled by the closed station(s). Our system features include the following:







risk profile visualizations and interactive risk
assessment tools for exploring the impact of closing Coast Guard stations;
parameterization of Coast Guard assets for use in
risk and resource distribution analysis;
optimization algorithms that assist with the interactive exploration of case load distribution in
resource allocation;
linked filters combined with spatial and temporal
views for interactive risk analysis/exploration.

This article is an updated and expanded version of a
paper4 presented at the 2011 Visual Analytics Science
and Technology (VAST) Conference. Our work
focuses on providing analysts with interactive visual
analytics tools that equip them to deal with the risk
assessment scenarios associated with closing Coast
Guard stations. We emphasize that although our risk
assessment toolkit and the examples given in this paper
have been based in the maritime domain, these techniques apply equally well to other domains (e.g. criminal offense analysis, syndromic surveillance).

Related work
In recent years, there has been a rapid growth in the
development of new visual analytics tools and techniques for advanced data analysis and exploration (for
examples see references5 and6). From traditional scatterplots7 and parallel coordinate plots8 to tools such as
Theme River9 and spiral graphs,10 systems incorporate
different forms of visualization to provide enhanced
analytical tools to users. Although these tools allow
users to explore their data and assist them in their
decision-making process, researchers have only
recently started to employ visual analytics techniques
for risk assessment and decision-making domains that
allow users to perform a thorough analysis of the risks
associated with different decisions.
Migut and Worring11 propose an interactive
approach to risk assessment with a risk assessment

Malik et al.

95

Figure 1. A screenshot of our risk assessment visual analytics system. Here, the user is visualizing all search and
rescue (SAR) operations conducted by the U.S. Coast Guard in the Great Lakes region. The main viewing area (a) is a
map view with points showing the locations of SAR incidents in the Great Lakes that occurred in July 2008. The right
window (b) shows an interactive selection menu for distress types with SAR cases selected in blue. The top window (c) is
a time-series view of the SAR incident report data, which shows a monthly distribution of SAR incidents over 5 fiscal
years and (d) is an interactive legend of all District Nine maritime zones. The left window (e) shows a calendar view of
the SAR incidents in 2008 with the month of July highlighted. An hourly view of the incidents responded to by all stations
(to all SAR incidents occurring in July 2008) is shown by the clock view display (f). Finally, the bottom-left window (g)
shows the time slider with radio buttons that allow different temporal aggregation levels with the current time frame
selected to July 2008.

framework that integrates interactive visual exploration
with machine learning techniques to support the risk
assessment and decision-making process. They use a
series of two-dimensional visualizations including scatterplots and mosaic plots to visualize numerical and
ordinal attributes of the datasets. While the authors
demonstrate the effectiveness of using visual analytics
in the field of risk assessment, their work is mainly
focused on building classification models that can be
used interactively to classify data entities and visualize
the effects on classification. Gandhi and Lee12 also
apply visual analytics techniques to the realm of
requirements-driven risk assessment. Specifically, they
use cohesive bar and arc graphs to illustrate the risks
due to the cascading effects of non-compliance with
Certification and Accreditation requirements for the
U.S. Department of Defense. Sanusi and Mustafa13

introduce a framework to develop a visualization tool
that may be used for risk assessment in the software
development domain. Their proposed framework
allows users to identify the components of the software
system that are likely to have a high fault rate. Direct
visualizations of risk use tools such as bar graphs and
confidence interval charts to visualize measures of risk
and are usually constructed using spreadsheet programs such as Microsoft ExcelÓ.12,14 Although widely
used, these techniques fail to work for our purposes,
primarily because of the nature of the risk analysis that
is required. The Coast Guard SAR dataset is spatiotemporal in nature and the exploration of risk requires
domain knowledge that is difficult to incorporate
algorithmically.
With respect to the temporal nature of risk assessment, researchers have also developed different

96
visualization systems that allow users to explore the
risks associated with financial decisions related to
investments and mutual funds, among other financial
planning scenarios. Rudolph et al.15 propose a personal finance decision-making visual analytics tool that
allows users to analyze both short-term and long-term
risks associated with making investment decisions.
Savikhin et al.16 also demonstrate the benefits of applying visual analytics techniques to aid users in their economic decision-making and, by extension, to general
decision-making tasks. Both of the previous examples
only explore temporal datasets. In this work, we apply
visual analytics techniques to explore risks using multivariate spatiotemporal datasets that guide analysts in
making complex decisions.
As is the case with most multivariate datasets, data
tend to be inherently unreliable, incomplete and contradictory. In order to reach the correct conclusions,
analysts must take this into account in their analysis.
In this regard, Correa et al.17 describe a framework
that supports uncertainty and reliability issues in the
different stages of the visual analytics process. They
argue that, with an explicit representation of uncertainty, analysts can make informed decisions based on
the level of confidence of the data. Our system factors
data reliability issues in to the risk assessment process
and provides confidence levels at each stage of the process, which, in turn, enable analysts to better understand the underlying nature of the data and guide
them in making effective decisions.
There are also many geospatial and temporal analytical systems that enable users to explore their spatiotemporal datasets in order to find patterns and provide an
overview of the data in a visual analytics platform (for
examples see references18–21). As the needs of our end
users are unique, developing a standalone system to
address the challenges faced by the Coast Guard analysts is warranted. We plan to further examine these
robust geotemporal analysis tools and the degree to
which they can be extended to meet the Coast Guard
requirements that have been identified in this paper.
There has also been a lot of work done on the visualization of large datasets using interactive cross-filtered
and linked views that allow users to explore their datasets. Stasko et al.5 use multiple coordinated views of
documents to reveal connections between entities
across different documents. Eick and Johnson22 utilize
multiple linked views to visualize abstract, nongeometric datasets in order to reduce visual clutter
and provide users with insights into their datasets.
Eick and Wills23 also demonstrate the effectiveness of
linking and interaction techniques in the visualization
of large networks. Our system utilizes these practices
and allows users to interactively explore their

Information Visualization 13(2)
multidimensional and multiattribute datasets using a
series of multicoordinated linked views.
Researchers have also explored different methods
to address the challenges posed to maritime security
and safety. Willems et al.6 introduce a novel geographic visualization that supports coastal surveillance
systems and decision-making analysts in gaining
insights into vessel movements. They utilize densityestimated heatmaps to reveal finer details and anomalies in vessel movements. Scheepens et al.24 also
present methods to explore multivariate trajectories
with density maps, which allows the exploration of
anomalously behaving vessels. Lane et al.25 present
techniques that allow analysts to discover the potential
risks and threats to maritime safety by analyzing the
behavior of vessel movements and determining the
probability that they are anomalous. Some other models for anomaly detection in sea traffic can be found in
refereces26 and27. Researchers have also proposed several approaches to maritime domain awareness. For
example, Roy and Davenport28 present a knowledgebased categorization of maritime anomalies built on a
taxonomy of maritime situational facts involved in
maritime anomaly detection. We observe that these
methods and models may help, in risk analysis and
understanding the impact of weather and varying
speeds of Coast Guard vessels in the Great Lakes, to
identify high-risk regions.
In the maritime security domain, Orosz et al.29
developed a decision-making and planning tool that
addresses the security needs based on the port
resources. Their focus was on the tactical and strategic
tradeoffs that arise because of the different port
resource reallocation strategies. The tool provides support in visualizing the effects of the different day-byday resource allocation strategies and also allow users
to forecast the risks and impacts of expansion and
changes made by the port security officials. Their definition of risk is based on three components: threat,
vulnerability, and consequences. Their system provides users with the ability to model and visualize the
risks in specified regions. The system responds primarily to outside threats and hypothetical scenarios,
whereas our system is focused on the historic spatiotemporal distribution of SAR incidents. We note that
the authors provide an interesting research direction in
the analysis of resource allocation strategies, and we
plan to explore these further in the future. Ulusu
et al.30 also model the security risks due to maritime
traffic accidents. In their approach, they divide the
waterways into regions and compute the risks based
on potential accidents occurring within these regions.
They utilize three-dimensional graphs to visualize the
regions and their corresponding risks.

Malik et al.
There exists several geographic information systems
(GIS) that allow users to visualize the datasets associated with coastal areas. There also exists several GIS
systems that incorporate different decision-making
processes by visualizing geospatial data in their specified areas. Pelot and Plummer31 focus on traffic modeling for both small and large commercial vessels, and
use grid maps that color the regions based on the traffic density in the regions, thereby allowing the visualization of vulnerable spatial regions. Marven et al.32
analyze Canadian Coast Guard search and rescue
operations in the Pacific coast, and follow a spatial and
temporal analysis of crime (STAC) model that is based
on a clustering algorithm which identifies regions of
high spatial incident densities. The authors explore
nearest neighbor clustering and kernel density estimation techniques in their work. Abi-Zeid et al.33 also
developed a geographical decision support system for
planning aeronautical search and rescue missions
which provides support for resource allocation decisions. Our custom-built GIS system also builds upon
the concept of providing analysts with the ability to
visualize and analyze their multivariate spatiotemporal
datasets in order to make informed decisions.
There has also been a lot of work done to assess and
mitigate risks to critical infrastructure and transportation in the maritime domain. Adler and Fuller34 provide dynamic scenario- and simulation-based risk
management models to assess risks to critical maritime
infrastructure and strategies implemented for mitigating these risks. Mansouri et al.35 also propose a risk
management-based decision analysis framework that
enables decision makers to identify, analyze, and prioritize risks involved in maritime infrastructure and
transportation systems. Their framework is based on
risk analysis and management methodologies that
allow the understanding of uncertainty and enable
analysts to devise strategies to identify the vulnerabilities of the system. Furthermore, work has been done
to quantify risks in the maritime transportation
domain, a summary of which can be found in reference36. While these methods facilitate maritime infrastructure risk analysis, our work is focused on assessing
maritime risks from multivariate spatiotemporal SAR
datasets. In this paper, we present a visual analytics
approach to maritime risk assessment and provide
examples that demonstrate the advantages of applying
visual analytics in this domain.

Visual analytics risk assessment
environment
Our visual analytics system provides enhanced risk
assessment and analytical tools for users and has been

97
built to operate for SAR incident report data. Our
system has been implemented in a custom Windowsbased GIS that allows drawing on an Open
StreetMap,37 using Visual C++ , MySQL and
OpenGL. The system displays geo-referenced data on
a map and allows users to temporally scroll through
their data. We provide linked windows that facilitate
user interaction between the spatial and temporal
domains of the data. We also provide advanced filtering techniques that allow users to interactively explore
the data. In addition, we have adapted the calendar
view presented by vanWijk and Selow38 and extended
it to explore seasonal and cyclical trends of SAR operations, and also as means of filtering data to support
advanced analysis.
Figure 1 presents a screenshot of our system. The
main viewing window (Figure 1(a)) shows the map
view in which the user can explore the spatial distribution of all cases handled by the Coast Guard. We utilize density-estimated heatmaps (Section Geospatial
displays) to quickly identify hotspots. Users may draw
a bounding box over incident points on the map that
generates a summary of all incidents enclosed by the
box. We also provide tape measure tools that allow
users to measure the distance between two points on a
map. The top-most window (Figure 1(c)) shows the
time-series view of the data where multiple line graphs
can be overlaid for comparison and analysis. Users
may visualize time-series plots by department, distress
type and Coast Guard captain of the port (COTP)
zone to explore summer cyclical patterns. The leftmost window (Figure 1(e)) shows the calendar view of
the selected Coast Guard cases. The total number of
columns on the calendar may be changed as desired to
reveal seasonal trends and patterns. The bottom-right
window (Figure 1(f)) shows a clock view that allows
users to visualize the hourly distributions of SAR incidents. The bottom-left window (Figure 1(g)) shows
the time-slider widget that is used to temporally scroll
through the data while dynamically updating all other
linked windows. The radio buttons beneath the time
slider provide several temporal aggregation methods
for the data. The center-right window (Figure 1(b))
shows the distress type menu in which all SAR cases
(highlighted in blue) have been selected for visualization. Users may select multiple distress types using this
menu, which dynamically updates all linked views. We
use similar menus to filter cases by other data fields.
Users may also interactively search the menu using the
search box provided on the top of the menu. Finally,
the top-right window (Figure 1(d)) shows an interactive legend of the different Coast Guard District Nine
maritime zones. When a user clicks on one of the
zones, all cases falling in the zone are highlighted by

98
filling the circles on the map with a solid color and
dimming out the other cases displayed on the map.
A key feature of our system is the interactive distress, station and COTP zone filtering component.
Users interactively generate combinations of filters that
are applied to the data being visualized through the use
of menus (such as the one shown in Figure 1(b)) and
edit controls. The choices of filters applied affects both
the geospatial viewing region and all temporal plots.

Coast Guard SAR data
The SAR data are collected by all U.S. Coast Guard
stations and stored in a central repository. When the
Coast Guard is called into action, a response case is
generated, usually by the maritime zone that has
authority in that region and receives the distress call
(referred to by the Coast Guard as the search mission
coordinator or SMC). Upon receiving the call, this
authority will determine if resources will be applied,
including which unit will provide the resource, the
resource type (e.g. boat, aircraft) and number.
Therefore, a response case may generate zero, one or
many sorties to respond to an incident. While analyzing the risks associated with the various mitigation
options, including station closure, analysts are interested in analyzing the spatiotemporal distribution of
both the response cases and their associated sorties.
The SAR data consist of two main components: (1)
response cases and (2) response sorties. Each entry in
the response case and sortie datasets contains information that provides details of the incidents (e.g. number
of lives saved, lost, assisted) and contains the geographic location of the distress.

Uncertainty in decision making. As is the case with
most large datasets, anomalies and missing data introduce errors and uncertainty. The SAR data are no
exception. We find that many SAR cases do not have
an associated geographic location, or have the wrong
associated geographic location. These inherent errors
in data affect the spatial probability estimates and
introduce a certain amount of uncertainty in the decisions that must be considered for an effective risk analysis and assessment. As previously noted,1 visual
analytics methods help people to make informed decisions only if they are made aware of data quality problems. In this regard, we incorporate uncertainty and
confidence levels associated with the SAR dataset into
our visualizations by displaying the accuracy of the
results at each step of the risk assessment process. We
define the accuracy to be the percentage of cases in
the dataset with reliable values, which can be used in

Information Visualization 13(2)
the decision-making process (Figure 2). This percentage is calculated using the following formula:
Accuracy =

N G
3 100
N

ð1Þ

Here, N is the total number of cases and G is the
number of cases with unreliable values (e.g. unknown
geographic coordinates, swapped negative signs).
When such errors are not obvious, the data are
assumed to be correct and is displayed to the analyst
on the map. The analyst can report missed errors in
the data and contribute to the data cleaning process.

Geospatial displays
Our system provides analysts with the ability to plot
incidents as points on the map and density-estimated
heatmaps (Figure 1(a)). In addition, we provide users
with the option of filling each incident circle with a
color on a sequential color scale39 that represents its
data value. For example, users may choose to visualize
the average time taken to respond to an incident for
all SAR cases on the map and identify cases with
higher response times. Furthermore, to explore the
spatial distribution of the SAR cases and quickly identify hotspots, we employ a modified variable kernel
density estimation technique (Equation 2) that scales
the parameter of estimation by allowing the kernel
scale to vary based upon the distance from the point
Xi to the kth nearest neighbor x in the set comprising
N samples.40


N
X
1
x  Xi
^f (x) = 1
K
N i = 1 min(h, di, k )
min(h, di, k )

ð2Þ

Here, N is the total number of samples, di, k is the
distance from the ith sample to the kth nearest neighbor, and h is the maximum allowed kernel width. We
choose the maximum kernel width based on asset
speed and travel time. Furthermore, we use the
Epanechnikov kernel40 (Equation 3) to reduce calculation time:
K (u) =

3
(1  u2 )1(jjujj41)
4

ð3Þ

where the function 1(jjujj41) evaluates to 1 if the
inequality is true and 0 otherwise.

Time-series displays
Along with the graphical interface, our system provides
a variety of visualization features for both spatial and
temporal viewing. For temporal viewing, we provide
line and stacked bar graphs, calendar and clock views
to visualize time-series SAR incident report data.

Malik et al.

99

Figure 2. Average response time risk assessment when auxiliary station Y is closed. The system automatically chooses
the stations (shown in the upper-left window) that are better suited to respond to cases previously handled by station Y,
along with a count of cases that each station absorbs. The station Y cases (black circles) to be handled by stations D and
E are circled in red. The top graph shows the average response time distribution of these stations to respond to station
Y’s cases. The decision makers are provided with accuracy estimates, and, in this case, 93% of the incidents are
displayed (7% may not have valid geographic information system coordinates).

The line graph visualization allows users to overlay
multiple graphs for easy comparison and to visualize
trends. Line and stacked bar graph visualizations are
both supported and can be interchanged using the
radio buttons provided. Users may choose to visualize
SAR cases handled by individual stations or maritime
zones, or visualize them by distress types. The data are
plotted based on a temporal aggregation level that the
user selects on the time-slider widget (Figure 1(g)). In
Figure 1(c), we show the line graph display of all SAR
cases aggregated by month. We can easily observe
peaks in the number of SAR cases in the summer
months for all maritime zones in the Great Lakes
region.

The calendar view visualization was first developed
by van Wijk and Selow.38 This view allows the visualization of data over time, laid in the format of a calendar. In our implementation (Figure 1(e)), we shade
each date entry based on the overall yearly trend. Users
may interactively change the total number of columns
of the calendar, thereby changing the cycle length of
the calendar view and enabling users to explore both
seasonal and cyclical trends of their datasets. The system also draws histograms for each row and column.
This allows analysts to visualize weekday and weekly
trends of SAR incidents and further assists them in
determining an effective resource allocation scheme.
Furthermore, we have modified our calendar view to

100

Information Visualization 13(2)

Figure 3. Visualizing the hourly distribution of search and rescue incidents for three stations in the Great Lakes for
fiscal years 2007–2011.

support an interactive database querying method for
easy acquisition of summary statistics from the SAR
database.
Finally, the clock view visualization allows users to
visualize the hourly distribution of the selected SAR
incidents responded to by the Coast Guard stations.
Our implementation of the clock view organizes the
data in the form of a clock, which is a radial layout
that is divided into slices to reflect the number of incidents that occur during each hour of the clock. Each
slice of the clock view is colored based on a sequential
color scheme to reflect the corresponding number of
incidents occuring during that hour. Furthermore, we
allow users to visualize the hourly distribution of either
all selected stations in a combined clock view display
(Figure 1(f)) or individual stations that are drawn as
individual clock views over the stations on the map
(Figure 3). We note that both these clock view representations are dynamically linked to the time slider
(Figure 1(g)), and are dynamically updated as the user
interacts with the system. Moreover, many SAR cases
do not have an associated hourly time component. In
order to depict these inherent errors in the data, we
utilize Equation 1 to display the percentage of cases
with valid hourly components to the users.

Risk assessment process
In this section, we describe the different methods and
techniques that we apply in the Coast Guard risk
assessment process.

Problem description
To indentify the problem, the Coast Guard analysts
provided a series of questions for use in their analysis.
These questions are briefly summarized below.
1.

2.

3.

4.

5.

6.

Assuming a maximum transit speed of 15 nautical
miles per hour, how many cases occur per year in
which a parent station could not have a surface
asset on scene within 2 hours?
For each auxiliary station, what are the types (by
percentage) of SAR response cases occurring
every year?
For each Auxiliary station, what is the temporal
(by hour, month and day of week) distribution of
the response case load?
What is the average annual case load that would
be absorbed by each parent station in the absence
of the auxiliary station and what percentage
increase would this represent to the parent station’s annual case load?
Based on the historical data for all cases (SAR and
others), what is the expected annual response case
demand broken down by response type (e.g. person in water, vessel flooding)?
Assess the potential risks associated with closing
certain auxiliary stations in terms of additional
case load absorbed, lives potentially lost and other
available factors.

Our visual analytics system was developed to assist
the Coast Guard analysts in answering these questions

Malik et al.
and to model the potential risks of closing one or more
auxiliary stations. Furthermore, we allow analysts to
explore the effects of closing multiple stations and provide a summary of the stations that are most suitable
for absorbing the work load of the closed stations.
Analysts may restrict the stations that absorb the work
load of the closed stations to determine the stations
that provide the most effective solution, thereby
informing an operational execution for the station that
is nearest to respond to the distress case.
We perform our analysis under the assumption that
the path between a station and a distress location is a
straight line. While this assumption presents a bestcase scenario to the analyst, discussions with our
Coast Guard partners indicated this was an acceptable
approximation as using channel and waterway information would result in a large computational overhead. With this assumption in place, if a station
absorbing an auxiliary station’s cases increases the
maritime risks in the region (e.g. if the average
response time exceeds the 2 hour time limit for most
SAR incidents), then closing the auxiliary station
could prove to be dangerous for the maritime and
public safety of the region. This straight line approximation provides details on the best-case scenario.

Average response time for SAR incidents
As stated before, a Coast Guard policy mandates the
rescue resource to be on scene within 2 hours of a distress (e.g. disabled vessel, person in water). Given the
low water temperatures in the Great Lakes, even in
the summer, an increase in response time can potentially impact the success of a case. Therefore, given
the option of closing a station, the analysts need to
know the nearest available resource and calculate the
time to respond to the scene. A typical Coast Guard
vessel travels at a speed of 15 nautical miles per hour.
After an auxiliary station has closed, the parent station
should still be able to reach most of the cases handled
by the auxiliary station within the 2 hour limit. In this
section, we describe how our system can be used to
determine the average response time for cases if a parent station (or any combination of stations) absorbs an
auxiliary station’s cases.
In order to generate the average response time for
the station(s) that absorbs the work load of the closed
station, we sift through all the incidents that the closed
station handled and find the closest station (excluding
the closed station) for each incident by comparing the
distance between all stations and the incident. This
distance between the closest station and incident may
also be visualized separately to reveal more details.
Once the closest station is found, we obtain the time
for an asset to reach the incident location using the

101
distance formula time = distance/speed. Users may
also change the speed of the asset, which changes the
results dynamically.
We provide users with several filtering options while
performing average response time analysis.





Users may choose to analyze the average response
time temporal distribution of incidents by applying
any of the possible filters on distress type, department or maritime zone.
Users may analyze the distribution of only the nonSAR cases.
Users may choose to close several stations all at
once and model the resulting effects. Which stations absorb the cases of the closed stations can be
specifyed by the analyst and thus the stations best
suited for closing and the methods for reallocating
available resources can be determined.

We also note that our system can be easily modified to
incorporate other risk metrics including, for example,
normalizing SAR cases by the underlying population
density and correlating SAR incidents with other
parameters.
Figure 2 shows the output generated when the analyst opts to close auxiliary station Y. In this example,
we examine all cases responded to by station Y
between January 2004 and September 2010. The system automatically suggests the stations that should
absorb auxiliary station Y’s cases along with the total
number of cases that each station would absorb. We
find that stations C (the parent station of Y), D and E
absorb auxiliary station Y’s cases, absorbing 84, 2 and
1 cases respectively. The analyst may instead select a
specific station to absorb station Y’s cases and analyze
the results generated. In Figure 2, the map view shows
all of the cases that each of the four stations responds
to during this time period (shown as circles, with each
case color coded by its station). We have also highlighted the two cases that station D and the one case
that station E responds to in Figure 2. It may be noted
that the one case absorbed by station E appears to be
out of place (possibly because of a human error in
entering the geographic coordinates for that particular
case). The top-right bar graph shows the count of all
SAR cases handled by station Y during this time
period versus the average response time (in minutes)
taken by the resulting stations to reach these cases,
assuming a transit speed of 15 nautical miles per hour.
From this time-series plot, we observe that all cases
responded to by the auxiliary station would fall well
within the tolerance level of 120 minutes when the suggested stations take over. The system also determines
the accuracy of the results dynamically by determining
the number of cases that have no associated geographic

102
coordinates. We find that 93% of the cases responded
to by station Y in the time range January 2004 and
September 2010 have an associated geographic
coordinate (as seen from the accuracy percentage in
Figure 2, top-right). Data integrity is a necessary parameter to report to the analysts and decision makers.
Thus, the user is made aware of these uncertainties at
every step of the risk assessment process.

Temporal distribution of response case load
One important aspect of risk assessment is analyzing
the work load of the stations and the distribution of
response cases amongst the stations being analyzed
over different temporal ranges. This is necessary in
determining the feasibility of a station closure and in
determining how the available resources may be reallocated (e.g. what times of day and what months would
the stations need to have more personnel deployed).
Analysts also use their domain experience and expertise to determine whether a particular station can
absorb a closing station’s cases. In particular, the
Coast Guard officials were interested in understanding
the hourly, daily and monthly trends of SAR cases
occurring in the Great Lakes.
Using traditional methods of sifting through SAR
datasets is highly inefficient for determining the temporal distribution of the SAR cases and, as such,
advanced database querying tools are necessary to
facilitate this process. To this end, we adapt the calendar view for querying the SAR database. We provide
three different interaction methods within the calendar
view widget (Figure 1(e)) to obtain a detailed summary of response cases occurring over the selected
period of time. Users can select time periods by simply
clicking on the start and end dates, as this selects all
the dates between the two clicked dates. Users may
also select one or more columns of the calendar to
generate the summary statistics. This allows them to
query the database and acquire the summary of events
occurring, for example, only in a particular week day.
Finally, users may select any combination of individual
dates and obtain the summary of all selected response
cases on these dates. These querying methods allow
analysts to easily determine the temporal patterns of
response cases over any period of time. The system
provides summary statistics of SAR incidents for all
stations and includes the total number of lives saved,
assisted and affected; total property damaged and
saved; and the count of all cases occurring over the
selected time period. Users may select any date, row,
column or combinations thereof in the calendar view
using the mouse to access the summary statistics.
Furthermore, the system also allows users to visualize

Information Visualization 13(2)
the hourly and monthly distribution of cases for any
time period after all of the filters have been applied.

Risk profile
Our system also provides users with the ability to
interactively generate risk profiles, which can be used
to identify regions with little SAR coverage by the
Coast Guard stations in the Great Lakes. Figure 4
illustrates risk profile heatmaps that present an overview of the Coast Guard SAR coverage in the Great
Lakes. Selected filter settings affect the visual output,
and, in this case, we are looking exclusively at small
boat station coverage. When areas of low coverage
exist, resources with additional capability (e.g. aircraft)
are often provided to ensure coverage of all areas.
Figure 4 (left) shows the time (in minutes) that the
Coast Guard stations would take to respond to a specific SAR incident in the Great Lakes, assuming a
transit speed of 15 nautical miles per hour. This profile is generated assuming that the station closest to
the incident responds. The regions in the Great Lakes
that take the longest time for the Coast Guard to
respond to an SAR case can be clearly seen in this figure. Users may interactively close stations, filter on a
different resource type (e.g. boat, aircraft) or change
the asset speed, updating the risk profile interactively.
This further enables the analysts to visualize the
increase or decrease in risk when a station is closed.
Moreover, analysts can set the lower threshold of the
color scale to 120 minutes (or any arbitrary time),
thereby allowing them to easily identify regions that
may take more than 120 minutes to respond to. We
plan to incorporate contour lines into our system to
demarcate the regions that may take more than the set
threshold response time to reach.
Figure 4 (right) provides another risk profile visualization that allows officials to identify regions with low
Coast Guard coverage for SAR operations in the Great
Lakes. Regions with high SAR coverage by the Coast
Guard stations are shown by darker colors. This further allows analysts to identify stations where resources
may be reallocated without increasing maritime risk.

Dynamic assignment of station asset speeds
In order to assist analysts with the different resource
allocation strategies, we note that not all stations are
allocated resources uniformly. Stations that respond to
a high number of incidents per year are typically
assigned more assets and may be assigned assets with
higher speeds and performance. This necessitates analysts to factor in the station asset allocation strategy in
addition to the station case load when they determine
the stations that could viably absorb the incidents of a

Malik et al.

103

Figure 4. Risk profile. (left) A heatmap showing the time taken (in minutes) by the Coast Guard stations to deploy an
asset to the Great Lakes to respond to an SAR incident, assuming a speed of 15 nautical miles per hour. (right) A
heatmap showing the Coast Guard SAR coverage (i.e. the number of stations that respond to a particular region) in the
Great Lakes. The squares along the coast show the locations of the stations.

104

Information Visualization 13(2)

Figure 5. Dynamic assignment of asset speed reflected in risk profile heatmaps. (left) A heatmap showing the time taken
(in minutes) by the Coast Guard stations to deploy an asset, assuming an asset speed of 15 nautical miles per hour for all
stations. (right) Updated heatmap in which the asset speed of station J has been changed to 30 nautical miles per hour.

hypothetically closed station(s). In order to facilitate
this process, our system allows users to dynamically
assign asset speeds to different Coast Guard stations.
The system then factors these asset speed assignments
into both the average response time analysis (section
Average response time for SAR incidents) and the risk
profile heatmaps (section Risk profile). An example of
the user changing the asset speed for a station is shown
in Figure 5, in which the user has changed the asset
speed for one of the stations from 15 to 30 nautical
miles per hour. The resulting heatmap clearly shows
that the regions that previously took more than 120
minutes to respond to are now reachable within a tolerable time limit.

Exploring risk using spatiotemporal linked
views
While examining which auxiliary stations are most suitable for closure, analysts need to weigh up all of the
options and analyze the potential increases or decreases
in associated risks. They must also consider the increase
in work load of the stations that absorb the closed stations’ cases to effectively determine the optimal response
of available resources. In this section, we describe a scenario in which a group of analysts are trying to quantify
and assess the risks and effects of closing multiple stations in the Great Lakes. An example that describes a
complete analysis of a hypothetical single station closure
scenario can be found in our earlier paper.4
When analyzing the risks associated with station
closures, analysts often have to consider several
resource allocation scenarios and analyze all possible

Figure 6. Spatial distribution of cases responded to by
stations M and N in fiscal years 2007 to 2011. The
incidents are represented on the map as circles and are
color coded by their owner station.

mitigation strategies in order to reach solutions that
minimize the overall risk. This type of analysis requires
them to focus on all aspects of possible strategies and
extensively use their domain knowledge to reach these
solutions. Moreover, analysts are required to consider
the different types of resources available at each station
and the possible resource redistribution strategies once
some stations have been closed. Finding solutions to
these challenges often proves to be extremely laborious
and, as such, requires advanced techniques to aid analysts in this process. We also note that a domain expert
who can assess all these different possibilities is an
integral component in this visual analytics process.
In order to assist analysts with these challenges, we
utilize several techniques in our visual analytics framework. Here, we describe a hypothetical scenario in
which a group of analysts are assessing the risks associated with closing two stations at the same time in the
Great Lakes (stations M and N in Figure 6) and are

Malik et al.
analyzing different resource allocation strategies
among the stations that absorb the closing stations’
incidents. We note that this type of analysis can be
extended to the closures of more than two stations at
the same time. In this analysis, we assume that stations
M and N have two types of assets that must be redistributed after they are closed. These assets include
two types of boat: the first set have typical boat speeds
of 15 nautical miles per hour and the second set are
the hypothetical new generation boats with speeds of
30 nautical miles per hour. To simplify this scenario,
we assume that there are equal numbers of 15 and 30
nautical miles per hour speed boats at stations M
and N. We also assume that the analysts have a budget
of purchasing a limited number of new generation
speed boats and distributing them among the stations
that absorb the work load of the closing stations.
In order to begin the analysis process, the analysts
select the two stations for closure using the interactive
menu. The analysts first use the system’s calendar
view to determine the maximum number of cases that
the two stations respond to in a single day. They find
that the maximum number of cases responded to by
stations M and N in a day was nine, occurring on the
Independence Day weekend of the year 2007 on
July 8. They now visualize the hourly distribution of
these nine incidents using the clock view, in order to
determine a better suited resource allocation strategy,
and determine that only seven of the nine cases have
an associated time component, and that these seven
incidents are uniformly spread out over a 24 hour
period. This gives them an indication of a worst case
daily resource distribution scenario when stations M
and N are closed.

Restricting analysis to stations F, G and H
The analysts now perform an average response time
risk analysis over a period of 5 fiscal years from 2007
to 2011. They first perform a visual analysis of the spatial incident distribution of the cases responded to by
stations M and N and observe that the better suited
stations to absorb the cases of these two closing stations are stations F, G and H (Figure 6). This is indeed
confirmed when the analysts run the average response
time analysis with stations M and N closed, which
reveals that the three stated stations absorb 723 of the
total 730 cases previously handled by stations M and
N (assuming a uniform asset speed of 15 nautical miles
per hour for all stations in the Great Lakes). They
therefore restrict their analysis to these three stations
and allow only stations F, G and H to absorb the cases
previously handled by the closing stations M and N.
Now, in order to determine a good resource allocation strategy, the analysts perform a series of iterative

105
steps in which they assign different types of resources
to the three stations. They start their analysis by
assigning equal asset speeds of 15 nautical miles per
hour for stations F, G and H and visualizing the average response time redistribution of cases previously
handled by the two closing stations M and N. The
resulting average response time distribution is shown
in Figure 7(a). As can been seen from the Figure, the
analysts determine that the three absorbing stations
yeild a median average response time of around 91
minutes, with stations F, G and H absorbing 142, 407
and 181 cases respectively. The system also determines that 97% of the total cases fall within the
tolerance limit of 120 minutes (shown as a percentage
value below the graph). From a visual inspection
of the cases responded to by stations M and N
(Figure 6), the analysts note that the resulting case distribution among the three stations are valid as station
G is located in between stations M and N. This allows
them to formulate their initial hypothesis that station
G may need to be assigned more assets than stations F
and H combined after stations M and N have been
closed.

Allocating new generation boats to one station
The analysts now turn their attention to the new generation boat reallocation strategies for the absorbing
stations once stations M and N have been closed.
They first consider assigning all available new generation boats (of stations M and N) to stations F, G and
H separately. The resulting average response time and
case load distributions of the incidents previously
handled by the closing stations M and N are shown in
Figure 7(b) to (d). The analysts note that assigning all
available new generation boats to station G yield the
best average response time distribution among the
three scenarios. This can be seen from Figure 7(c),
where station G absorbs 665 cases with a median
response time of around 47 minutes. At first, they may
be concerned that an addition of 665 cases to station
G’s work load is too many. However, realizing that
these 665 cases are distributed over a period of 5 fiscal
years, with most incidents occurring during the summer months, they determine that this would constitute
an increase of only one or two additional cases a day,
which may be acceptable. To confirm this, they use
the system’s calendar view to visualize the daily temporal distribution of station G’s cases, and further
using their domain knowledge to conclude that assigning an additional one or two cases a day to station G
would indeed be acceptable. Furthermore, they note
that the average response time distribution of the other
two scenarios (Figure 7(b) and (d)) look appealing as
well, especially as the cases are distributed uniformly

106

Information Visualization 13(2)

Figure 7. Average response time analyses after stations M and N have been closed and the resources have been
reallocated equally among stations F, G and H.

over the three absorbing stations, and that most of the
cases fall within the tolerance level of 120 minutes.
However, they note that the median response time
shifts to the right in both of these cases compared with
Figure 7(c). They also note that station G yields the
best performance in terms of average response time
case distribution. This further increases their confidence in assigning the cases of the closing stations to
station G.

Allocating new generation boats to multiple
stations
The analysts now investigate the case in which they
assign the available new generation boats (of stations
M and N) to one of the three absorbing stations and,
in addition, assign new boats to the other absorbing
station utilizing the funds that they have in reserve in
this analysis scenario. They investigate the three possible scenarios separately, the results of which are shown
in Figure 7(e) to (g), with each figure showing the distribution of cases when the new generation boats are
assigned to stations F+G, G+H and H+F respectively. From the average response time distribution
plots, they find that the best-case scenario involves a

combination of station G and one of the other two stations (e.g. Figure 7(e) and (f)), with station G absorbing most of the additional cases. The third scenario
involving assigning the new generation boats to stations F and H (Figure 7(g)) also yields acceptable average response time case distributions with a relatively
more uniform station case load distribution among the
three stations than the other two scenarios. They note
that this option may be opted for if this is more desirable for the Coast Guard.
Finally, in order to see the average response time
impact of upgrading the rescue boats of all three stations to new generation boats with average speeds of
30 nautical miles per hour, the analysts assign all three
stations with the new generation boats. The results of
this action are shown in Figure 7(h). The analysts
observe that the case load redistribution of the three
stations remains very similar to that of all stations having boats with average speeds of 15 nautical miles per
hour (Figure 7(a)). However, as expected, the average
response time distribution goes down from a median
time of about 91 minutes to 46 minutes. This allows
the analysts to make a case to the Coast Guard decision makers for upgrading the assets at the stations
and their impacts on potential station closures.

Malik et al.

107

Figure 8. Station coverage risk profile heatmaps after stations M and N have been closed and new generation boats
with speeds of 30 nautical miles per hour are assigned to stations F (left), G (middle) and H (right). Note that assignment
to station G yields an optimal coverage.

Visualizing station coverage risk profile
heatmaps
Finally, the analysts decide to visualize the risk profile
heatmaps for the three cases in which the new generation boats are assigned to stations F, G and H respectively. This conforms to the average response time
case distributions shown in Figure 7(b) to (d). The
risk profile heatmaps obtained are shown in Figure 8,
with the left image showing the assignment of the new
generation boats to station F, the middle image to station G and the right image to station H. The analysts
note that assigning the new generation boats to only
stations F or H introduces a potential high risk region
in the Great Lakes, and that assigning station G the
allocation of new generation boats pays off well in
terms of station response coverage when stations M
and N have been closed. This further increases the
confidence in their hypothesis of assigning the cases of
the closed stations M and N to station G, and hence
allows them to make an informed decision.

Domain expert feedback
Our system was assessed by two analysts at the U.S.
Coast Guard’s Ninth District and Atlantic Area
Commands who are currently using the system to
determine the potential risks in the maritime domain
associated with the hypothetical allocation of Coast
Guard resources, which include both planned day-today and contingency operations. In this section, we
summarize the feedback that we received from them
after conducting several informal interviews. The analysts emphasized the need of such systems, which
allow them to quickly and easily process large datasets
in order to derive actionable results, in the maritime
domain. One of the analysts noted that processing the
desired queries took him a fraction of the time when
using our system as compared with other software
(e.g. Microsoft ExcelÓ), which he had previously been
using in his analysis. He was impressed by the fact that
the system is intuitive to use and requires little user

training. He observed that the system’s ability to process large datasets allows him to quickly filter the data
into manageable subsets while providing interactive
spatiotemporal displays that further aid him (and ultimately the senior level decision makers) in making a
decision using the best information available.
The analysts noted that the system had been utilized throughout its inception and development to
assist decision makers in making time-critical resource
allocation decisions as well as support long-term alternative analysis for force structure, coverage and allocation. As an example, they noted that the system was
used during the Coast Guard’s planning cycle for
Hurricane Irene. Forecast models had Hurricane
Irene striking the Eastern Seaboard of the United
States over the Labor Day holiday weekend in 2011.
Senior Coast Guard officials expected a need for additional resources, and wanted to pull resources from
the Ninth District to aid in the relief efforts. Our system was used, and output was provided to these senior
level decision makers that nullified the request to move
additional resources from the Ninth District. The system displayed an usually large response need in the
Great Lakes over this holiday weekend. The potential
increase in risk in the Great Lakes was deemed too
high and the resources were pulled from elsewhere.
The analysts said that to perform this analysis by leveraging Microsoft ExcelÓ would have taken hours,
whereas this system allowed analysis to be done in
minutes. They further noted that the system was built
for time critical analysis, but could be leveraged to
assist in longer term planning and development of
hypothetical alternative analysis to find efficiencies in
their current resource pool. They noted that these
efforts have been noticed by other Coast Guard
District Commanders as well, and that the system was
currently being extended for the Fifth Coast Guard
District that encompasses the Mid-Atlantic U.S.
states.
We note again that the discussion presented so far
in this section is a result of our informal discussions
with our end users to determine the merits and

108
usability of the system. We have adopted a usercentred design approach41 by involving the users in
the design process of the system. This process has
included conducting initial interviews with a focused
end user group to collect data related to their needs
and expectations, after which an initial working prototype was produced. This prototype was then delivered
to the end users and their feedback was acquired by
conducting several on-site interviews. Thereafter the
system underwent a series of iterative refinements in
which the data and results were validated and verified
throughout the development process.
We also plan to perform a formal user evaluation
of our system to assess its benefits and limitations.
Our evaluation goals include a human–computer
interaction flavored evaluation that would look at
errors using the system’s user interface. Other evaluation goals include evaluating the system in terms
of the problem that our end users intend to solve
using the system. This approach requires a comparison of the results against established ground truths
about the consequences of a decision, and, as such,
we plan to devise test scenarios after consulting with
our end users. We note that designing this type of
analysis-based evaluation strategy to evaluate the
likelihood that our end users have achieved their
objectives and minimized the risks after station closures and resource allocations is especially challenging because of the lack of solutions that act as a
control. Thus, domain knowledge and expertise
become critical in these types of decision-making
processes.

Conclusions and future work
Our current work demonstrates the benefits of visual
analytics in analyzing risk and historic resource allocation in the maritime domain. Our visual analytics
system provides analysts with a set of tools for analyzing risks and the consequences of taking major
decisions that could cause the loss of lives and property. Our results show how our system can be used as
an effective risk assessment tool when examining various mitigation strategies to a known or emergent
problem.
Before this system was developed, Coast Guard officials explored possible mitigation strategies, including
the implementation of seasonal or weekend-only auxiliary duty stations, but the sheer volume of data and
information inhibited the efficient processing of the
data. However, using our system, the decision makers
were quickly made aware of the fact that most response
cases happened on Mondays/Tuesdays at some of the

Information Visualization 13(2)
units. This further demonstrates the benefits of the use
of visual analytics in the maritime domain.
In addition to performing risk analysis on the Coast
Guard SAR cases, our system can also be used to
conduct a thorough review of the operations (i.e. nondistress cases) conducted by different Coast Guard
stations. Users may choose to visualize different datasets and analyze how each station performs in terms of
factors including average response times, average distance to target, lives saved, lives assisted and lives
affected. Hence, the officials may analyze the efficiency of each Coast Guard station and identify problem areas that may require further attention.
Future work includes deploying our system to assist
in the analysis and optimization of all operations conducted by the U.S. Coast Guard Ninth District and
expanding the use of our system to other Coast Guard
districts. Our plan includes:










implementing algorithms that factor the geography
of the coast line into the risk assessment process in
order to get accurate response times for the Coast
Guard assets;
employing prediction algorithms in the temporal
domain in order to provide analysts with insights
into the operations of the Coast Guard stations;
quantifying the impacts of reallocating different
Coast Guard resources among stations in order to
determine optimal mitigative strategies in case of
station closures;
implementing different visualization techniques
that compare the trends of different variables to
assist analysts in the resource reallocation process;
incorporating additional risk metrics to provide
insights into different risk scenarios.

Finally, we note that although the initial feedback
received so far by our end users has been positive, we
plan to conduct a formal user evaluation of our system
in the future to better understand and evaluate the
impacts and limitations of our system in the decisionmaking process of our end users. We plan to develop
several evaluation and performance metric schemes
that will consider the different aspects of our visual
analytics system (e.g. visual data cognition, system
interactivity and utility from a user perspective, validation of data and results).42 We also intend to perform
qualitative assessments of the system to assess the merits of the system in the decision-making process of our
end users by performing a qualitative video analysis
along with providing our end users with questionnaires
after user training and another after they become adept
with the system.

Malik et al.

109

Funding
This work is supported by the U.S. Department of
Homeland Security’s VACCINE Center under Award
Number 2009-ST-061-CI0002.

Acknowledgments
The authors would like to thank Capt. Eric
Vogelbacher, Steffen Koch, Zichang Liu and Dr.
Brian Fisher for their feedback.

12.

13.

14.

Disclaimer
The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies,
either expressed or implied, of the U.S. Department of
Homeland Security or the U.S. Coast Guard.

15.

16.

References
1. Keim D, Kohlhammer J, Ellis G, et al. Mastering the
information age: Solving problems with visual analytics.
EuroGraphics Association, Genéve, Switzerland, 2010.
2. Thomas JJ and Cook KA (eds) Illuminating the path: the
R&D agenda for visual analytics. IEEE Press, 2005.
3. U.S.C.G. U.S. Coast guard addendum to the United States
National Search and Rescue Supplement (NSS) to the
International Aeronautical and Maritime Search and Rescue
Manual (IAMSAR). Washington, DC, U.S. Department
of Homeland Security, September 2009.
4. Malik A, Maciejewski R, Maule B, et al. A visual analytics process for maritime resource allocation and risk
assessment. Proceedings of the IEEE conference on visual
analytics science and technology (VAST), 23–28 October,
2011, pp. 221–230.
5. Stasko J, Görg C and Liu Z. Jigsaw: supporting investigative analysis through interactive visualization. Inform
Visual 2008; 7: 118–132.
6. Willems N, Van De Wetering H and Van Wijk JJ. Visualization of vessel movements. Comput Graph Forum
2009; 28: 959–966.
7. Cleveland WS and McGill ME (eds). Dynamic graphics
for statistics. Wadsworth: Belmont, CA; Brooks/Cole:
Pacific Grove, CA, 1988.
8. Inselberg A. Parallel coordinates: visual multidimensional
geometry and its applications. Springer, New York, NY,
2009.
9. Havre S, Hetzler E, Whitney P, et al. Themeriver: visualizing thematic changes in large document collections.
IEEE T Vis Comput Gr 2002; 8: 9–20.
10. Carlis JV and Konstan JA. Interactive visualization of
serial periodic data. In: Proceedings of the symposium on
user interface software and technology, San Francisco, CA,
1–4 November 1998, pp. 29–38.
11. Migut M and Worring M. Visual exploration of classification models for risk assessment. In: Proceedings of the

17.

18.

19.
20.

21.

22.

23.

24.

25.

IEEE symposium on visual analytics science and technology,
Salt Lake City, UT, 25–26 October 2010, pp. 11–18.
Gandhi RA and Lee S-W. Visual analytics for
requirements-driven risk assessment. In: Second international workshop on requirements engineering visualization,
New Delhi, India, 15–19 October 2007, p. 6.
Sanusi NM and Mustafa N. A visualization tool for
risk assessment in software development. In: International symposium on information technology, Kuala Lumpur, Malaysia, 26–28 August 2008, pp. 1–4.
Feather M, Cornford S, Kiper J, et al. Experiences using
visualization techniques to present requirements, risks
to them, and options for risk mitigation. In: First international workshop on requirements engineering visualization,
Minneapolis/St. Paul, MN, 11–15 September 2006,
p. 10.
Rudolph S, Savikhin A and Ebert D. Finvis: applied
visual analytics for personal financial planning. In: IEEE
symposium on visual analytics science and technology, Atlantic City, NJ, 12–13 October 2009, pp. 195–202.
Savikhin A, Maciejewski R and Ebert D. Applied visual
analytics for economic decision-making. In: Proceedings
of the IEEE symposium on visual analytics science and
technology, Columbus, OH, 19–24 October 2008, pp.
107–114.
Correa C, Chan Y-H and Ma K-L. A framework for
uncertainty-aware visual analytics. In: IEEE symposium
on visual analytics science and technology, Atlantic City,
NJ, 12–13 October 2009, pp. 51–58.
Command Post of the Future (CPOF). http://www.dau.
mil/pubscats/PubsCats/AR%20Journal/arj53/Greene53.pdf.
Command Post of the Future: Successful Transition of a
Science and Technology Initiative to a Program of Record
(accessed 01 October 2012).
Esri. ArcView, http://www.esri.com/software/arcgis/arc
view/index.html (accessed 9 June 2011).
Oculus. GeoTime, http://www.oculusinfo.com/assets/
pdfs/GeoTime_Overview.pdf. GeoTime for Analysis of
Behavior in Time and Geography (accessed 01 October
2012).
Hardisty F and Robinson AC. The geoviz toolkit: using
component-oriented coordination methods for geographic visualization and analysis. Int J Geogr Inf Sci
2011; 25: 191–210.
Eick SG and Johnson BS. Interactive data visualization
at AT&T Bell laboratories. In: Conference companion on
human factors in computing systems, New York, 7–11 May
1995, pp. 17–18.
Eick SG and Wills GJ. Navigating large networks with
hierarchies. In: Proceedings of the 4th conference on visualization 1993, Washington, DC, 25–29 October 1993, pp.
204–209. IEEE Computer Society.
Scheepens R, Willems N, Van De Watering H, et al.
Interactive visualization of multivariate trajectory data
with density maps. In: Proceedings of the Pacific visualization 2011, Hong Kong, China, 1–4 March 2011, pp.
147–154.
Lane R, Nevell D, Hayward S, et al. Maritime anomaly
detection and threat assessment. In: 13th International

110

26.

27.

28.

29.

30.

31.

32.

33.

34.

35.

36.

Information Visualization 13(2)
Conference on Information Fusion, Edinburgh, UK, 26–
29, July 2010, pp. 1–8.
Laxhammar R. Anomaly detection for sea surveillance.
In: 11th International conference on information fusion,
Cologne, Germany, June 30–July 3 2008, pp. 1–8.
Ristic B, La Scala B, Morelande M, et al. Statistical analysis of motion patterns in AIS data: anomaly detection
and motion prediction. In: 11th International conference
on information fusion, Cologne, Germany, June 30–July 3
2008, pp. 1–7.
Roy J and Davenport M. Categorization of maritime
anomalies for notification and alerting purpose. In:
NATO workshop on data fusion and anomaly detection
for maritime situational awareness, La Spezia, Italy, 15–17
September 2009, pp. 15–17.
Orosz M, Southwell C, Barrett A, et al. Portsec: a port
security risk analysis and resource allocation system. In:
2010 IEEE international conference on technologies for homeland security, Waltham, MA, 8–10 November 2010, pp.
264–269.
Ulusu OS, Ozbas B, Altiok T, et al. Risk analysis of the
vessel traffic in the strait of Istanbul. J Risk Anal 2009;
29: 1454–1472.
Pelot R and Plummer L. Spatial analysis of traffic and
risks in the coastal zone. J Coast Conservat 2008; 11:
201–207.
Marven C, Canessa R and Keller P. Exploratory spatial
data analysis to support maritime search and rescue
planning. In: Geomatics solutions for disaster management
(Lecture notes in geoinformation and cartography). Berlin
and Heidelberg: Springer, 2007, pp. 271–288.
Abi-Zeid I and Frost JR. SARPlan: a decision support
system for Canadian search and rescue operations. Eur J
Oper Res 2005; 162: 630–653.
Adler R and Fuller J. An integrated framework for assessing and mitigating risks to maritime critical infrastructure. In: IEEE conference on technologies for homeland
security, Woburn, MA, 16–17 May 2007, pp. 252–257.
Mansouri M, Nilchiani R and Mostashari A. A risk
management-based decision analysis framework for resilience in maritime infrastructure and transportation systems. In: 3rd Annual IEEE systems conference, Vancouver,
BC, Canada, 23–26 March 2009, pp. 35–41.
Soares CG and Teixeira AP. Risk assessment in maritime
transportation. Reliab Eng Syst Safe 2001; 74: 299–309.

37. OpenStreetMap. http://www.openstreetmap.org (accessed 15 June 2011).
38. van Wijk JJ and Selow ERV. Cluster and calendar based
visualization of time series data. In: Proceedings of the
IEEE symposium on information visualization, San Francisco, CA, 24–29 October 1999, pp. 4–9. IEEE Computer Society.
39. Brewer CA. Designing better maps: a guide for GIS users.
ESRI Press, Redlands, CA, 2005.
40. Silverman BW. Density estimation for statistics and data
analysis. Chapman & Hall/CRC, London, UK, 1986.
41. Abras C, Maloney-Krichmar D and Preece J. Usercentered design. In: Bainbridge W (ed.) Encyclopedia of
human-computer interaction. Thousand Oaks, CA, SAGE,
2004, pp. 763–768.
42. Scholtz J. Beyond usability: evaluation aspects of visual
analytic environments. In: Proceedings of the IEEE symposium on visual analytics science and technology, Baltimore,
MD, October 31–November 2 2006, pp. 145–150.

Appendix 1
In this section, we briefly provide some domain specific terms and definitions.
Coast Guard Auxiliary: volunteers that support the
Coast Guard.
Coast Guard Ninth District: the area of Coast Guard
operations that encompasses the Great Lakes.
Atlantic Area Command: the area of Coast Guard operations east of the Rocky Mountains.
Captain of the Port (COTP) Zone: further division of
Coast Guard operations within a Coast Guard
District.
Unit or station: the operational execution arm of the
Coast Guard. For example, the small boat station provides the boat and personnel to execute the assigned
mission.
Coast Guard asset: a boat or an aircraft reserved to perform Coast Guard operations.
Coast Guard sortie: an asset that responds to an
incident.

A Visual Analytics Process for Maritime
Resource Allocation and Risk Assessment
Abish Malik ∗
∗ Purdue

Ross Maciejewski∗ , Member, IEEE

University Visualization and Analytics Center (PURVAC)

A BSTRACT
In this paper, we present our collaborative work with the U.S. Coast
Guard’s Ninth District and Atlantic Area Commands where we developed a visual analytics system to analyze historic response operations and assess the potential risks in the maritime environment associated with the hypothetical allocation of Coast Guard resources.
The system includes linked views and interactive displays that enable the analysis of trends, patterns and anomalies among the U.S.
Coast Guard search and rescue (SAR) operations and their associated sorties. Our system allows users to determine the potential
change in risks associated with closing certain stations in terms of
response time, potential lives and property lost and provides optimal direction as to the nearest available station. We provide maritime risk assessment tools that allow analysts to explore Coast
Guard coverage for SAR operations and identify regions of high
risk. The system also enables a thorough assessment of all SAR operations conducted by each Coast Guard station in the Great Lakes
region. Our system demonstrates the effectiveness of visual analytics in analyzing risk within the maritime domain and is currently
being used by analysts at the Coast Guard Atlantic Area.
Keywords: Visual analytics, risk assessment, Coast Guard
1

Ben Maule†

I NTRODUCTION

As modern datasets increase in size and complexity, it becomes increasingly difficult for analysts and decision makers to extract actionable information for making effective decisions. In order to better facilitate the exploration of such datasets, tool sets are required
that allow users to interact with their data and assist them in their
analysis. Furthermore, such datasets can be utilized to explore the
consequences and risks associated with making decisions, thereby
providing insights to analysts and aiding them in making informed
decisions.
Besides the sheer volume and complexity of such datasets, analysts must also deal with data quality issues, including uncertain,
incomplete and contradictory data. Moreover, analysts are often
faced with different decisions and are required to weigh all possible consequences of these decisions using such datasets in order
to arrive at a solution that minimizes the associated risks within a
given time constraint. Using traditional methods of sifting through
sheets of data to explore potential risks can be highly inefficient and
difficult due to the nature and size of these datasets. Therefore, advanced tools are required that enable a more timely exploration and
analysis. Our work focuses on the use of visual analytics [17, 31]
in the realm of risk assessment and analysis and demonstrates the
effectiveness of visual analytics in this domain. The work described
in this paper is based on the application of visual analytics to analyze historic response operations and assess the potential risks in
∗ e-mail:

{amalik|rmacieje|ebertd}@purdue.edu
† email: Ben.J.Maule@uscg.mil

IEEE Symposium on Visual Analytics Science and Technology
October 23 - 28, Providence, RI, USA
978-1-4673-0014-8/11/$26.00 ©2011 IEEE

David S. Ebert∗ , Fellow, IEEE
† United

States Coast Guard

the maritime environment based on notational station closures. Our
work was done in collaboration with the U.S. Coast Guard’s Ninth
District and Atlantic Area Commands that are responsible for all
Coast Guard operations in the five U.S. Great Lakes. In particular,
we focused on the Auxiliary stations that are staffed by Coast Guard
volunteers and civilians. These Auxiliary stations assist their parent
stations in their operations and usually operate on a seasonal basis
using a small fleet of boats for conducting their operations. However, the number of Auxiliary personnel that volunteer their time
at these stations has decreased over recent years. This has required
Coast Guard analysts to develop possible courses of action and analyze the risks and benefits with each option. Several options include
seasonal or weekend only staffing of these units, or at worst, closure. Closure, however, may involve increased risks to the boating
public and a complete analysis of the risks associated with closing
an Auxiliary station needs to be evaluated. The results of this type
of analysis would assist the decision makers in determining the optimal course of action.
In particular, the analysts are interested in determining the spatial and temporal distribution of response cases and their associated
sorties (a boat or an aircraft deployed to respond to an incident)
for all SAR operations conducted in the Great Lakes and how closing certain Auxiliary stations affects the workload of the stations
that absorb these cases. Coast Guard policy mandates the launch
of a sortie within 30 minutes and have an asset (boat or aircraft)
on scene within two hours of receiving a distress call [32]. Closing these stations implies a potential for longer response times that
could potentially translate into the loss of lives and property.
To address these challenges, we developed a visual analytics system that supports decision making and risk assessment and allows
an interactive analysis of trends, patterns and anomalies among the
U.S. Coast Guard’s Ninth District operations and their associated
sorties. Our system, shown in Figure 1, allows enhanced exploration of multivariate spatiotemporal datasets. We have incorporated enhanced tools that enable maritime risk assessment and analysis. Our system includes linked spatiotemporal views for multivariate data exploration and analysis and allows users to determine
the potential increase or decrease in risks associated with closing
one or more Coast Guard stations. The system enables a thorough
assessment of all operations conducted by each station. In addition, the system provides analysts with the tools to determine which
Coast Guard stations are more optimally suited to assume control
of the operations of the closed station(s) by comparing the distances
from available stations to all SAR cases previously handled by the
closed station(s). Our system features include the following:
• Risk profile visualizations and interactive risk assessment
tools for exploring the impact of closing Coast Guard stations
• Optimization algorithms that assist with the interactive exploration of case load distribution in resource allocation
• Linked filters combined with spatial and temporal views for
interactive risk analysis/exploration
Our work focuses on providing analysts with interactive visual
analytics tools that equip them to deal with risk assessment scenar-

221

Figure 1: A screenshot of our risk assessment visual analytics system. Here, the user is visualizing all search and rescue (SAR) operations
conducted by the U.S. Coast Guard in the Great Lakes region in July 2008. The main viewing area (a) shows the map view with the circles
showing the locations of SAR incidents in the Great Lakes. The right-most window (b) shows an interactive menu showing all distress types with
SAR cases selected in blue. The top window (c) shows the time-series and the left window (d) shows the calendar views of the SAR incident
report data. The bottom-left window (e) shows the time slider with radio buttons that allow different temporal aggregation levels. A legend for all
District Nine maritime zones is shown in the upper right (f).

ios associated with closing Coast Guard stations. We emphasize
that although our risk assessment toolkit and the examples given
in this paper have been based in the maritime domain, these techniques apply equally as well to other domains (e.g., criminal offense
analysis, syndromic surveillance).
2

R ELATED W ORK

In recent years, there has been a rapid growth in the development of
new visual analytics tools and techniques for advanced data analysis and exploration (e.g., [30, 34]). From traditional scatterplots
[8] and parallel coordinate plots [16] to tools like Theme River [15]
and spiral graphs [7], these systems incorporate different forms of
visualizations to provide enhanced analytical tools to users. Although these tools allow users to explore their data and assist them
in their decision making process, researchers have only recently
started to employ visual analytics techniques for risk-assessment
and decision-making domain that allow users to perform a thorough
analysis of risks associated with different decisions.
Migut and Worring [21] propose an interactive approach to risk
assessment where they demonstrate a risk assessment framework
that integrates interactive visual exploration with machine learning
techniques to support the risk assessment and decision making process. They use a series of 2D visualizations including scatterplots
and mosaic plots to visualize numerical and ordinal attributes of the
datasets. While the authors demonstrate the effectiveness of using

222

visual analytics in the field of risk assessment, their work is mainly
focused on building classification models that users may interactively use to classify their data entities and visualize the effects on
their classification. Gandhi and Lee [13] also apply visual analytics techniques to the realm of requirements-driven risk assessment.
Specifically, they use cohesive bar and arc graphs to illustrate the
risks due to the cascading effects of non-compliance with Certification and Accreditation requirements for the U.S. Department of
Defense. Sanusi and Mustafa [25] introduce a framework to develop a visualization tool that may be used for risk assessment in
the software development domain. Their proposed framework allow users to identify the components of the software system that
are likely to have a high fault rate. Direct visualizations of risk
use tools like bar graphs and confidence interval charts to visualize
measures of risk and are usually constructed using spreadsheet programs like Microsoft Excel [12, 13]. Although widely used, these
techniques fail to work for our purposes primarily due to the nature
of the risk analysis that is required. The Coast Guard SAR dataset
is spatiotemporal in nature and the exploration of risk requires domain knowledge that is difficult to incorporate algorithmically.
With respect to the temporal nature of risk assessment, researchers have also developed different visualization systems that
allow users to explore risks associated with financial decisions related with investments and mutual funds, among other financial
planning scenarios. Rudolph et al. [24] propose a personal finance

decision making visual analytics tool that allows users to analyze
both short-term and long-term risks associated with making investment decisions. Savikhin et al. [26] also demonstrate the benefits
of applying visual analytics techniques to aid users in their economic decision making and, by extension, to general decision making tasks. Both of the previous examples only explore temporal
datasets. In this work, we apply visual analytics techniques to explore risks using multivariate spatio-temporal datasets that guide
analysts in making complex decisions.
As is the case with most multivariate datasets, data tends to be inherently unreliable, incomplete and contradictory. In order to reach
to correct conclusions, analysts must take these into account in their
analysis. In this regard, Correa et. al. [9] describe a framework that
supports uncertainty and reliability issues in the different stages of
the visual analytics process. They argue that with an explicit representation of uncertainty, analysts can make informed decisions
based on the levels of confidence of the data. Our system factors
data reliability issues in the risk assessment process and provides
confidence levels at all stages of risk assessment that, in turn, enable analysts to better understand the underlying nature of the data
and guides them in making effective decisions.
There also exist many geospatial and temporal analytical systems
that provide users with the ability to explore their spatiotemporal
datasets in order to find patterns and provide an overview of the data
in a visual analytics platform (e.g., [1, 2, 3, 14]). As the needs of our
end users are unique, this warrants developing a stand alone system
to address the challenges faced by the Coast Guard analysts. We
plan on further examining these robust geo-temporal analysis tools
and the degree to which they can be extended to meet the Coast
Guard requirements that have been identified in this paper.
There has also been much work done in visualizing large datasets
using interactive cross-filtered and linked views that allow users to
explore their datasets. Stasko et. al. [30] use multiple coordinated views of documents to reveal connections between entities
across different documents. Eick and Johnson [10] utilize multiple linked views to visualize abstract, non-geometric datasets in
order to reduce visual clutter and provide users with insights into
their datasets. Eick and Wills [11] also demonstrate the effectiveness of linking and interaction techniques in the visualization of
large networks. Our system utilizes these practices and allows users
to interactively explore their multi-dimensional and multi-attribute
datasets using a series of multi-coordinated linked views.
Researchers have also explored different methods to address
the challenges posed to maritime security and safety. Willems
et. al. [34] introduce a novel geographic visualization that supports coastal surveillance systems and decision making analysts
in gaining insights into vessel movements. They utilize density
estimated heatmaps to reveal finer details and anomalies in vessel movements. Scheepens et. al. [27] also present methods to
explore multivariate trajectories with density maps and allow the
exploration of anomalously behaving vessels. Lane et. al. [18]
present techniques that allow analysts to discover potential risks
and threats to maritime safety by analyzing the behavior of vessel
movements and determining the probability that they are anomalous. Some other models for anomaly detection in sea traffic can
be found in [19, 22]. Researchers have also proposed several approaches to maritime domain awareness. For example, Roy and
Davenport [23] present a knowledge based categorization of maritime anomalies built on a taxonomy of maritime situational facts
involved in maritime anomaly detection. We observe that these
methods and models may help in risk analysis and understanding
the impact of weather and varying speeds of Coast Guard vessels in
the Great Lakes to identify high risk regions.
There has also been much work done to assess and mitigate risks
to critical infrastructure and transportation in the maritime domain.
Adler and Fuller [5] provide dynamic scenario- and simulation-

based risk management models to assess risks to critical maritime
infrastructure and strategies implemented for mitigating these risks.
Mansouri et. al. [20] also propose a risk management-based decision analysis framework that enables decision makers to identify,
analyze, and prioritize risks involved in maritime infrastructure and
transportation systems. Their framework is based on risk analysis and management methodologies that allows understanding uncertainty and enables analysts to devise strategies to identify the
vulnerabilities of the system. Furthermore, work has been done to
quantify risks in the maritime transportation domain, a summary of
which can be found in [29]. While these methods facilitate maritime infrastructure risk analysis, our work is focused on assessing
maritime risks from multivariate spatiotemporal SAR data sets. In
this paper, we present a visual analytics approach to maritime risk
assessment and provide examples that demonstrate the advantages
of applying visual analytics in this domain.
3

V ISUAL A NALYTICS R ISK A SSESSMENT E NVIRONMENT

Our visual analytics system provides enhanced risk assessment and
analytical tools to analysts and has been built to operate for SAR
incident report data. Our system has been implemented in a custom
Windows-based geographical information system that allows drawing on an OpenStreetMap map [4], using Visual C++, MySQL and
OpenGL. The system displays geo-referenced data on a map and
allows users to temporally scroll through their data. We provide
linked windows that facilitate user interaction between the spatial
and temporal domains of the data. We also provide advanced filtering techniques that allow users to interactively explore through
data. In addition, we have adapted the calendar view presented by
vanWijk and Selow [33] and extended it to explore seasonal and
cyclical trends of SAR operations and also as means to filter data to
support advanced analysis.
Figure 1 presents a screenshot of our system. The main viewing window (Figure 1 (a)) shows the map view where the user can
explore the spatial distribution of all cases handled by the Coast
Guard. We utilize density estimated heatmaps (Section 3.2) to
quickly identify hotspots. Users may draw a bounding box over
incident points on the map that generates a summary of all incidents enclosed by the box. We also provide tape measure tools that
allow users to measure the distance between two points on a map.
The top-most window (Figure 1 (c)) shows the time-series view of
the data where multiple lines graphs can be overlaid for comparison
and analysis. Users may visualize time-series plots by department,
distress type and Coast Guard Captain of the Port (COTP) zone to
explore summer cyclical patterns. The left-most window (Figure
1 (d)) shows the calendar view of the selected Coast Guard cases.
The total number of columns on the calendar may be changed as
desired to reveal seasonal trends and patterns. The bottom window
(Figure 1 (e)) shows the time-slider widget that is used to temporally scroll through the data while dynamically updating all other
linked windows. The radio buttons beneath the time slider provide several temporal aggregation methods for the data. The rightmost window (Figure 1 (b)) shows the distress type menu where
all SAR cases (highlighted in blue) have been selected for visualization. Users may select multiple distress types using this menu,
dynamically updating all linked views. We use similar menus to
filter cases by other data fields. Users may also interactively search
the menu using the search box provided on top of the menu. Finally,
the top-right window (Figure 1 (f)) shows an interactive legend of
the different Coast Guard District Nine maritime zones. This legend allows users to click on any of the zones that highlights all cases
falling in the zone by filling the circles on the map with a solid color
and dimming out the other cases being displayed on the map.
A key feature of our system is the interactive distress, station
and COTP zone filtering component. Users interactively generate
combinations of filters that are applied to the data being visualized

223

through the use of menus (like the one shown in Figure 1 (b)) and
edit controls. The choices of filters applied affects both the geospatial viewing region and all temporal plots.
3.1

Coast Guard SAR data

The SAR data is collected by all U.S. Coast Guard stations and
stored in a central repository. When the Coast Guard is called into
action, a response case is generated, usually by the maritime zone
that has authority in that region that receives the distress call (referred to by the Coast Guard as the Search Mission Coordinator
or SMC). Upon receiving the call, this authority will determine if
resources will be applied, including which unit will provide the resource, the resource type and number. Therefore, a response case
may generate zero, one, or many sorties to respond to an incident.
While analyzing risks associated with the various mitigation options, including station closure, analysts are interested in analyzing
the spatiotemporal distribution of both the response cases and their
associated sorties.
The SAR data consists of two main components: (1) response
cases and (2) response sorties. Each entry in the response case and
sortie dataset contains information that provides details of the incidents (e.g., number of lives saved, lost, assisted) and contains the
geographic location of the distress.
Uncertainty in decision making
As is the case with most large datasets, anomalies and missing data
introduce errors and uncertainty. The SAR data is no exception. We
find that many SAR cases do not have an associated geographic location, or have a wrong geographic location associated with them.
These inherent errors in data affect the spatial probability estimates
and introduce a certain amount of uncertainty in the decisions that
must be considered for an effective risk analysis and assessment. As
noted in [17], visual analytics methods help people make informed
decisions only if they are made aware of data quality problems. In
this regard, we incorporate uncertainty and confidence levels associated with the SAR dataset in our visualizations by displaying the
accuracy of the results at each step of the risk assessment process.
This is shown as a percentage that shows the total cases with reliable data that can be used in the decision making process (Figure
2). This percentage is calculated by using the following formula:
Accuracy =

N −G
× 100
N

(1)

Here, N is the total number of cases and G is the number of
cases with unreliable values (e.g., unknown geographic coordinates, swapped negative signs). When such errors are not obvious,
the data is assumed to be correct and is displayed to the analyst
on the map. The analyst can further report errors in the data and
contribute to the data cleaning process.
3.2

Geospatial displays

Our system provides analysts with the ability to plot incidents as
points on the map and as density estimated heatmaps (Figure 1 (a)).
In addition, we provide users with the option of coloring each incident circle with a color on a sequential color scale [6] that represents its data value. For example, users may choose to visualize the
average response time to respond to an incident for all SAR cases
on the map and identify cases with higher response times. Furthermore, to explore the spatial distribution of the SAR cases and
quickly identify hotspots, we employ a modified variable kernel
density estimation technique (Equation 2) that scales the parameter
of estimation by allowing the kernel scale to vary based upon the
distance from the point Xi to the kth nearest neighbor x in the set
comprising of N [28].

224

1
fˆ(x) =
N

N

1

∑ min(h,d

i=1

K
i,k )

x − Xi
min(h,di,k )

!
(2)

Here, N is the total number of samples, di,k is the distance from
the i-th sample to the k-th nearest neighbor and h is the maximum
allowed kernel width. We choose the maximum kernel width based
on asset speed and travel time. Furthermore, we use the Epanechnikov kernel [28] (equation 3) to reduce calculation time:
3
K(u) = (1 − u2 )1(||u||≤1)
4

(3)

where the function 1(||u||≤1) evaluates to 1 if the inequality is true
and to zero otherwise.
3.3

Time series displays

Along with the graphical interface, our system provides a variety of
visualization features for both spatial and temporal views. For temporal views, we provide line and stacked bar graphs and calendar
views to visualize time series SAR incident report data.
The line graph visualization allow users to overlay multiple
graphs for easy comparison and to visualize trends. Both line graph
and stacked bar graph visualizations are supported and can be interchanged using the radio buttons provided. Users may choose to visualize SAR cases handled by individual stations or maritime zones,
or visualize them by distress types. The data is plotted based on a
temporal aggregation level that the user selects on the time-slider
widget (Figure 1 (e)). In Figure 1 (c), we show the line graph display of all SAR cases aggregated by month. We can easily observe
peaks in the number of SAR cases in the summer months for all
maritime zones in the Great Lakes region.
The calendar view visualization was first developed by van Wijk
and Selow [33]. This visualization provides a means to allow the
visualization of data over time, laid in the format of a calendar. In
our implementation (Figure 1 (d)), we shade each date entry based
on the overall yearly trend. Users may interactively change the total number of columns of the calendar thereby changing the cycle
length of the calendar view, enabling users to explore both seasonal
and cyclical trends of their datasets. The system also draws histograms for each row and column. This allows analysts to visualize
weekday and weekly trends of SAR incidents and further assists
them in determining an effective resource allocation scheme. Furthermore, we have modified our calendar view to support an interactive database querying method for easily acquiring summary
statistics from the SAR database.
4

R ISK A SSESSMENT

PROCESS

In this section, we describe the different methods and techniques
that we apply in the Coast Guard risk assessment process.
4.1

Problem description

To bound the problem, the Coast Guard analysts provided a series
of questions for use in their analysis. These questions are briefly
summarized below.
1. Assuming a maximum transit speed of 15 nautical miles per
hour, how many cases occur per year in which a parent station
could not have a surface asset on scene within two hours?
2. For each Auxiliary station, what are the types (by percentage)
of SAR response cases occurring per year?
3. For each Auxiliary station, what is the temporal (by hour,
month and day of week) distribution of the response case
load?

Figure 2: Average response time risk assessment when Auxiliary station Y is closed. The system automatically chooses the stations (shown
in the upper-left window) that are optimally suited to respond to cases previously handled by station Y, along with a count of cases that each
station absorbs. The station Y cases (black circles) to be handled by stations D and E are circled in red. The top graph shows the average
response time distribution of these stations to respond to station Y’s cases. We find that only 93% of the cases responded to by station Y have
an associated geographic location.

4. What is the average annual case load that would be absorbed
by each parent station in the absence of the Auxiliary station
and what percentage increase would this represent to the parent station’s annual case load?
5. Based on the historical data for all cases (SAR and others),
what is the expected annual response case demand broken
down by response type (i.e., Person in Water, Vessel Flooding, etc.)?
6. Assess the potential risks associated with closing certain Auxiliary stations in terms of additional case load absorbed, lives
potentially lost, and other available factors.
Our visual analytics system was developed to assist the Coast
Guard analysts in answering these questions and to model the potential risks of closing one or more Auxiliary stations. Furthermore,
we allow analysts to explore the effects of closing multiple stations
and provide a summary of stations that are most optimal to absorb
the work load of the closed stations. Analysts may restrict the stations that absorb the work load of the closed stations to determine
the stations that prove most effective, thereby informing optimal
operational execution for the station that is nearest to respond to
the distress case.

We perform our analysis under the assumption that the path between a station and a distress location is a straight line. While this
assumption presents a best-case scenario to the analyst, discussions
with our Coast Guard partners indicated this was an acceptable approximation as using channel and waterway information would result in a large computational overhead. With this assumption in
place, if a station absorbing an Auxiliary station’s cases increases
the maritime risks in the region (e.g., if the average response time
exceeds the two hour time limit for most SAR incidents), then closing the Auxiliary station could prove to be dangerous for the maritime and public safety of the region. This straight line approximation provides details on the best case scenario.
4.2

Average response time for SAR incidents

As stated before, a Coast Guard policy mandates the rescue resource to be on scene within two hours of a distress (e.g., disabled
vessel, person in water). Given the cold water temperatures in the
Great Lakes, even in the summer, increase in response time can potentially impact the success of a case. Therefore, given the option
of closing a station, the analysts desire to know the nearest available
resource to respond and calculate the time to respond to the scene.
A typical Coast Guard vessel travels at a speed of 15 nautical miles

225

Figure 3: Risk profile. (Left) A heatmap showing the time taken (in minutes) by the Coast Guard stations to deploy an asset to the Great Lakes
to respond to a SAR incident, assuming a speed of 15 nautical miles per hour. (Right) A heatmap showing the Coast Guard SAR coverage (i.e.,
the number of stations that respond to a particular region) in the Great Lakes. The squares along the coast show the locations of the stations.

per hour. After an Auxiliary station is closed, the parent station
should still be able to reach most of the cases handled by the Auxiliary station within the two hour limit. In this section, we describe
how our system can be used to determine the average response time
for cases if a parent station (or any combination of stations) absorbs
an Auxiliary station’s cases.
In order to generate the average response time for the station(s)
that absorb the work load of the closed station, we sift through all
the incidents that the closed station handled and find the closest station (excluding the closed station) for each incident by comparing
the distance between all stations and the incident. This distance
between the closest station and incidents may also be visualized
separately to reveal more details. Once the closest station is found,
we obtain the time for an asset to reach the incident location using the distance formula Time = Distance/Speed. Users may also
change the speed of the asset, changing the results dynamically.
We provide users with several filtering options while performing average response-time analysis. Users may choose to analyze
the average response-time temporal distribution of incidents by applying any possible filters on distress type, department or maritime
zone. Users may also analyze the distribution of only the non-SAR
cases. Moreover, users may choose to close several stations all at
once and model the resulting effects. They may also specify which
stations absorb the cases of the closed stations and thus determine
the stations best suited for closing and the optimal methods for reallocating available resources. We also note that our system can
be easily modified to incorporate other risk metrics including, for
example, normalizing SAR cases by the underlying population density, correlating SAR incidents with other parameters, etc.
Figure 2 shows the output generated when the analyst opts to
close Auxiliary station Y. In this example, we examine all cases
responded to by station Y between January 2004 and September
2010. The system automatically suggests the stations that should
absorb Auxiliary station Y’s cases along with the total number of
cases that each station absorbs. We find that stations C (the parent
station of Y), D and E absorb Auxiliary station Y’s cases, with each
absorbing 84, 2 and 1 cases, respectively. The analyst may instead
select a specific station to absorb station Y’s cases and analyze the
results generated. In Figure 2, the map view shows all cases that
each of the four stations responds to during this time period (shown
as circles, with each case color coded by its station). We have also

226

highlighted the two cases that station D and the one case that station E responds to in Figure 2. It may be noted that the one case
absorbed by station E appears to be out of place (possibly due to a
human error in entering the geographic coordinates for that particular case). The top-right bar graph shows the count of all SAR cases
handled by station Y during this time period versus the average response time (in minutes) taken by the resulting stations to reach
these cases, assuming a transit speed of 15 nautical miles per hour.
From this time-series plot, we observe that all cases responded to
by the Auxiliary station would fall well within the tolerance level of
120 minutes when the suggested stations take over. The system also
determines the accuracy of the results dynamically by determining
the number of cases that have no associated geographic coordinates.
We find that 93% of the cases responded to by station Y in the time
range January 2004 and September 2010 have an associated geographic coordinate (as seen from the accuracy percentage in Figure
2-top-right). Data integrity is a necessary parameter to report to the
analysts and decision makers. Thus, the user is made aware of these
uncertainties at every step of the risk assessment process.
4.3 Temporal distribution of response case load
One important aspect of risk assessment is analyzing the work load
and distribution of response cases of the stations being analyzed
over different temporal ranges. This becomes necessary to determine the feasibility of a station to be closed and to determine how
the available resources may be reallocated (e.g., what times of day
and what months would the stations need to have more personnel
deployed). Analysts also use their domain experience and expertise
to determine whether a particular station can absorb a closing station’s cases. In particular, the Coast Guard officials were interested
in understanding the hourly, daily and monthly trends of SAR cases
occurring in the Great Lakes.
Using traditional methods of sifting through SAR datasets turns
out to be highly inefficient for determining the temporal distribution of the SAR cases and, as such, advanced database querying
tools are necessary to facilitate this process. To this end, we adapt
the calendar view for querying the SAR database. We provide three
different interaction methods within the calendar view widget (Figure 1 (d)) to obtain a detailed summary of response cases occurring
over the selected date-range. Users can select date ranges by simply
clicking on the start and end dates that selects all the dates between
the two clicked dates. Users may also select one or more columns

Figure 4: Risk assessment using linked views. Here, the analyst has chosen to close Auxiliary station X and is analyzing the risks associated
with this decision. Each circle on the map represents a SAR case and has been color coded by its owner station. The system shows that stations
A and B are best suited to absorb station X’s cases, with station B absorbing 144 cases and station A absorbing 15 cases.

of the calendar to generate the summary statistics. This allows them
to query the database and acquire the summary of events occurring,
for example, only on a particular week day. Finally, users may select any combination of individual dates and obtain the summary of
all selected response cases on those dates. These querying methods
allow analysts to easily determine the temporal patterns of response
cases over any date range. The system provides summary statistics
of SAR incidents for all stations and includes the total number of
lives saved, assisted, affected, total property damaged and saved
and the count of all cases occurring over the selected date range.
Users may select any date, row, column, or combinations thereof in
the calendar view using the mouse to access the summary statistics.
Furthermore, the system also allows users to visualize the hourly
and monthly distribution of cases for any time period after all filters
are applied.
4.4

Risk profile

Our system also provides users with the ability to interactively generate risk profiles that can be used to identify regions with little
SAR coverage by the Coast Guard stations in the Great Lakes. Figure 3 illustrates the risk profile heatmaps that present an overview
of the Coast Guard SAR coverage in the Great Lakes. Selected
filter settings affect the visual output, and in this case, we are looking exclusively at small boat station coverage. When areas of low
coverage exist, resources with additional capability (e.g., aircraft)
are often provided to ensure coverage of all areas. Figure 3 (Left)
shows the time (in minutes) that the Coast Guard stations would

take to respond to a SAR incident in the Great Lakes, assuming a
transit speed of 15 nautical miles per hour. This profile is generated
assuming that the station closest to a location responds to an incident in the Great Lakes. The regions in the Great Lakes that take
the longest time for the Coast Guard to respond to a SAR case can
be clearly seen in this figure. Users may interactively close stations,
filter on a different resource type (e.g., boat, aircraft), or change the
asset speed, updating the risk profile interactively. This further enables the analysts to visualize the increase or decrease in risk when
a station is closed. Moreover, analysts can set the lower threshold
of the color scale to 120 minutes (or any arbitrary time), thereby
allowing them to easily identify regions that may take more than
120 minutes to respond. We plan on incorporating contour lines
into our system to demarcate the regions that may take more than
the set threshold response time.
Figure 3 (Right) provides another risk profile visualization that
allows officials to identify regions with low Coast Guard coverage
for SAR operations in the Great Lakes. Regions with high SAR
coverage by the Coast Guard stations are shown by darker colors.
This further allows analysts to identify stations where resources
may be reallocated without increasing maritime risk.
5

E XPLORING
VIEWS

RISK

USING

SPATIOTEMPORAL

LINKED

While examining which Auxiliary stations are most suitable to
close, analysts need to weigh all options and analyze the potential
increase or decrease in associated risks. They must also consider

227

Figure 5: The average response time distribution for the additional cases when parent station A absorbs station X (Left) and when station B
absorbs station X (Right). We see a left shift in the median time indicating that station B may be a better candidate to absorb station X’s cases.

Figure 6: The change in average response time risk profile when Auxiliary station X is closed. (Left) Risk profile when station X is operational.
(Right) Risk profile when station X is closed. We see an increase in risk in station X’s area of operation when the station is closed.

the increase in workload of the stations that absorb the closed station’s cases to effectively determine the optimal response of available resources. In this section, we describe a typical scenario where
an analyst is trying to determine the risks associated with closing
an Auxiliary station in one of the sectors in the Great Lakes and the
stations that absorb the work load of this closed station.
Suppose the analyst chooses to close Auxiliary station X whose
parent station is A. Since the parent station of X is station A, the
analysts’ first inclination may be to assign all cases to station A after
station X is closed. The analyst first uses the system’s calendar view
and finds that the maximum number of cases that station X responds
to in one day is 7 in the peak boating season. He also visualizes the
hourly distribution of cases for station X and determines that the
incidents are spread out during the day. Next, the analyst uses our
risk assessment system to perform an average response time risk
analysis over a time range of January 2006 to September 2010 and
selects Auxiliary station X to be closed. Once station X is closed,
the system automatically generates the result seen in Figure 4. As
seen in the figure, the system determines stations A and B to be
the optimal stations to respond to the cases handled by Auxiliary
station X. In this figure, we see the spatial distribution of all SAR
cases that the three stations responded to during this time range
(seen as circles that are color coded by station). We observe that
station B absorbs 144 additional SAR cases as opposed to parent
station A which only absorbs 15 cases. Moreover, with this case
load distribution, we find that 154 out of the total 159 cases are
responded to within the 120 minute limit (as seen from the time
series plot in Figure 4-top-right). These results suggest that station
B is better suited to absorb most of the cases of Auxiliary station X.
In order to get a better picture, the analyst now restricts the stations that respond to the cases handled by station X to first, its
parent station A, and then to station B and analyzes the average
response time distribution of cases for each of the two stations separately. The results of this step are shown in Figure 5, with the
left graph showing the average response time taken by station A,
and the right graph corresponding to station B. As the analyst compares the two graphs, he realizes that if only station B is allowed
to absorb station X, there are 14 cases that take more than 2 hours

228

for the Coast Guard to arrive on scene. On the other hand, if station A is allowed to absorb station X, 16 cases take longer than 2
hours. However, as we can clearly see from Figure 5, station A
takes a longer time to respond to most cases than station B, with
the median time of station A being 110 minutes, and that of station B being 92 minutes. The analyst also notes that station B takes
between 264-271 minutes to respond to about nine cases, whereas
station A takes 275 minutes to respond to one case. To get a better
understanding of why this may be happening, the analyst explores
the spatial distribution of station X’s cases on the map and discovers
that some cases of station X get mapped to an inland lake (which
requires trailering the boat to the scene). In order to confirm that
these cases do not occur as a result of errors in the database, he
draws a bounding box over these incidents on the map and obtains
a summary of these incidents. The summary confirms that these
incidents do indeed occur in that particular lake. As station A is
closer to these cases, the analyst concludes that station A would be
a better candidate to absorb these cases as opposed to station B.
But for the rest of the cases, the results clearly suggest that station
B would be a better candidate to absorb station X’s cases, and that
station A would increase the maritime risks if allowed to absorb
station X’s cases alone. Thus, this analysis confirms that a combination of these two stations yields the best results. With these
results at hand, the analyst may also recommend using an aircraft
to respond to cases that take more than 120 minutes. Or, as our
preceding analysis showed, stations A and B may absorb the work
load of station X together, with station B receiving a higher share of
resources than station A. We also note that in the future, our system
could be modified to perform a real time analysis of SAR cases and
could then be used to assign each case to the correct station in real
time.
The analyst now uses the risk profile tools to observe the increase
in risks when station X is closed. This is shown in Figure 6, with
the left figure showing the average response time risk profile when
Auxiliary station X is functional whereas the right figure shows the
risk profile when station X is closed. The analyst explores the new
average response time on the map when station X is closed and determines the potential increase in maritime risks in the region. The

Figure 7: The monthly (left) and hourly (right) distributions of SAR incidents handled by Auxiliary station X between Jan. 2006 and Sept. 2010.

analyst thus identifies the regions in station X’s area of operation
that take a greater time to respond.
The analyst also visualizes the monthly and hourly distributions
of all SAR cases responded to by the closed Auxiliary station X
between January 2006 and September 2010 (shown in Figure 7,
where the left graph shows the monthly distribution and the right
graph shows the hourly distribution). We see that all activity occurs
in the summer months with a peak occurring in July and the station
responds to most cases mainly during evening hours. The analyst
also visualizes the temporal activity of stations A and B, and determines the potential work load increase for both stations. This helps
the analyst determine how the available resources must be reallocated if Auxiliary station X is to be closed. Furthermore, the analyst chooses to visualize the case distribution of Auxiliary station
X between January 2006 and September 2010 using the interactive
calendar view widget. This generates a summary dialog box that
provides the details of the SAR cases responded to by station X and
includes details including the total number of lives assisted, saved
and lost. This further helps the analyst understand the risks associated with closing this station by providing an overview of the cases
occurring in this region. With all these results at hand, the analyst
uses his domain knowledge to make an informed decision.
6

D OMAIN

EXPERT FEEDBACK

Our system was assessed by an analyst at the U.S. Coast Guard’s
Ninth District who is currently using the system to determine the
potential risks in the maritime domain associated with the hypothetical allocation of Coast Guard resources. The analyst emphasized
the need of such systems in the maritime domain that allow users to
quickly and easily process large datasets in order to derive actionable results. The analyst noted that processing the desired queries
took him a fraction of the time when using our system as compared
to using other software (e.g., Microsoft Excel) that he had been previously using in his analysis. He was impressed by the fact that the
system is intuitive to use and requires little user training. He observed that the system’s ability to process large datasets allows him
to quickly filter the data into manageable subsets while providing
interactive spatiotemporal displays that further aid him (and ultimately the senior level decision makers) in making a decision using
the best information available.
7

C ONCLUSIONS

AND

F UTURE W ORK

Our current work demonstrates the benefits of visual analytics in
analyzing risk and historic resource allocation in the maritime domain. Our visual analytics system provides analysts with a suite
of tools for analyzing risks and consequences of taking major decisions that translate into important measures including potential
lives and property lost. Our results show how our system can be
used as an effective risk assessment tool when examining various
mitigation strategies to a known or emergent problem.
Before this system was developed, Coast Guard officials explored possible mitigation strategies, including the implementation
of seasonal or weekend only Auxiliary duty stations, but the sheer
volume of data and information inhibited the efficient processing

of the data. However, using our system, the decision makers were
quickly made aware that most response cases happened on Mondays/Tuesdays at some of the units. This further asserts the benefits
of the use of visual analytics in the maritime domain.
In addition to performing risk analysis on the Coast Guard SAR
cases, our system can also be used to conduct a thorough review of
the operations (i.e. non-distress cases) conducted by different Coast
Guard stations. Users may choose to visualize different datasets
and analyze how each station performs in terms of factors including
average response times, average distance to target, lives saved, lives
assisted, lives affected, etc. Hence, the officials may analyze the
efficiency of each Coast Guard station and identify problem areas
that may require further attention.
Future work includes deploying our system to assist in the analysis and optimization of all operations conducted by the U.S. Coast
Guard Ninth District and expanding the use of our system to other
Coast Guard districts. We plan on implementing algorithms that
factor the geography of the coast line in the risk assessment process
in order to get accurate response times by the Coast Guard assets.
We also plan on employing prediction algorithms in the temporal
domain as well as spatiotemporal correlation algorithms that correlate different datasets (e.g., weather, water temperature) with the
response dataset to provide insights into the operation of the Coast
Guard stations. Furthermore, we plan on incorporating additional
risk metrics to provide insights into different risk scenarios.
8 A PPENDIX
In this section, we briefly provide some domain specific terms and
definitions:
Coast Guard Auxiliary: Volunteers that support the Coast Guard.
Coast Guard Ninth District: The area of Coast Guard operations
that encompasses the Great Lakes.
Atlantic Area Command: The area of Coast Guard operations
East of the Rocky Mountains.
Captain of the Port (COTP) Zone: Further division of Coast
Guard operations within a Coast Guard District.
Unit or Station: The operational execution arm of the Coast
Guard. For example, the small boat station provides the boat and
personnel to execute the assigned mission.
Coast Guard asset: A boat or an aircraft reserved to perform
Coast Guard operations.
Coast Guard sortie: An asset that responds to an incident.
9 ACKNOWLEDGMENTS
The authors would like to thank Capt. Eric Vogelbacher, Steffen
Koch and Zichang Liu for their feedback. This work is supported
by the U.S. Department of Homeland Security’s VACCINE Center
under Award Number 2009-ST-061-CI0002.
10 D ISCLAIMER
The views and conclusions contained in this document are those
of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the U.S.
Department of Homeland Security or the U.S. Coast Guard.

229

R EFERENCES
[1] Command Post Of The Future (CPOF). Internet: http://www.
gdc4s.com/documents/cpof_datasheet_web.pdf, [June
20, 2011].
[2] ESRI, ArcView. Internet: http://www.esri.com/software/
arcgis/arcview/index.html, [June 9, 2011].
[3] Oculus, GeoTime. Internet: http://www.oculusinfo.com/
SoftwareProducts/GeoTime.html, [June 25, 2011].
[4] OpenStreetMap. Internet: http://www.openstreetmap.org,
[June 15, 2011].
[5] R. Adler and J. Fuller. An integrated framework for assessing and mitigating risks to maritime critical infrastructure. In IEEE Conference
on Technologies for Homeland Security, pages 252 –257, May 2007.
[6] C. A. Brewer. Designing Better Maps: A Guide for GIS users. ESRI
Press, 2005.
[7] J. V. Carlis and J. A. Konstan. Interactive visualization of serial periodic data. In Proceedings of the Symposium on User Interface Software and Technology, pages 29–38, 1998.
[8] W. S. Cleveland and M. E. McGill, editors. Dynamic Graphics for
Statistics. Wadsworth and Brooks/Cole, 1988.
[9] C. Correa, Y.-H. Chan, and K.-L. Ma. A framework for uncertaintyaware visual analytics. In IEEE Symposium on Visual Analytics Science and Technology, pages 51 –58, Oct. 2009.
[10] S. G. Eick and B. S. Johnson. Interactive data visualization at AT&T
Bell laboratories. In Conference companion on human factors in computing systems, pages 17–18, New York, NY, USA, 1995.
[11] S. G. Eick and G. J. Wills. Navigating large networks with hierarchies.
In Proceedings of the 4th conference on visualization, Visualization
1993, pages 204–209, Washington, DC, USA, 1993. IEEE Computer
Society.
[12] M. Feather, S. Cornford, J. Kiper, and T. Menzies. Experiences using visualization techniques to present requirements, risks to them,
and options for risk mitigation. In First International Workshop on
Requirements Engineering Visualization, Sept. 2006.
[13] R. A. Gandhi and S.-W. Lee. Visual analytics for requirements-driven
risk assessment. In Second International Workshop on Requirements
Engineering Visualization, October 2007.
[14] F. Hardisty and A. C. Robinson. The geoviz toolkit: using componentoriented coordination methods for geographic visualization and analysis. International Journal Geographical Information Science, 25:191–
210, February 2011.
[15] S. Havre, E. Hetzler, P. Whitney, and L. Nowell. Themeriver: Visualizing thematic changes in large document collections. IEEE Transactions on Visualization and Computer Graphics, 8(1):9–20, 2002.
[16] A. Inselberg. Parallel Coordinates: Visual Multidimensional Geometry and Its Applications. Springer, September 2009.
[17] D. Keim, J. Kohlhammer, G. Ellis, and F. Mansmann, editors. Mastering the information age: Solving problems with Visual Analytics.
EuroGraphics, 2010.
[18] R. Lane, D. Nevell, S. Hayward, and T. Beaney. Maritime anomaly
detection and threat assessment. In 13th Conference on Information
Fusion, pages 1 –8, July 2010.
[19] R. Laxhammar. Anomaly detection for sea surveillance. In 11th International Conference on Information Fusion, pages 1 –8, July 2008.
[20] M. Mansouri, R. Nilchiani, and A. Mostashari. A risk managementbased decision analysis framework for resilience in maritime infrastructure and transportation systems. In 3rd Annual IEEE Systems Conference, pages 35 –41, March 2009.
[21] M. Migut and M. Worring. Visual exploration of classification models
for risk assessment. In Proceedings of the IEEE Symposium on Visual
Analytics Science and Technology, pages 11–18, October 2010.
[22] B. Ristic, B. La Scala, M. Morelande, and N. Gordon. Statistical analysis of motion patterns in AIS data: Anomaly detection and motion
prediction. In 11th International Conference on Information Fusion,
pages 1 –7, July 2008.
[23] J. Roy and M. Davenport. Categorization of maritime anomalies for
notification and alerting purpose. NATO Workshop on Data Fusion
and Anomaly Detection for Maritime Situational Awareness, pages
15–17, September 2009.
[24] S. Rudolph, A. Savikhin, and D. Ebert. Finvis: Applied visual ana-

230

[25]

[26]

[27]

[28]
[29]

[30]

[31]
[32]

[33]

[34]

lytics for personal financial planning. In IEEE Symposium on Visual
Analytics Science and Technology, pages 195 –202, Oct. 2009.
N. M. Sanusi and N. Mustafa. A visualization tool for risk assessment
in software development. In International Symposium on Information
Technology, pages 1–4, August 2008.
A. Savikhin, R. Maciejewski, and D. Ebert. Applied visual analytics for economic decision-making. In Proceedings of the IEEE Symposium on Visual Analytics Science and Technology, pages 107–114,
October 2008.
R. Scheepens, N. Willems, H. van de Watering, and J. J. van Wijk.
Interactive visualization of multivariate trajectory data with density
maps. In Proceedings of the Pacific Visualization 2011, pages 147–
154, March 2011.
B. W. Silverman. Density Estimation for Statistics and Data Analysis.
Chapman & Hall/CRC, 1986.
C. G. Soares and A. P. Teixeira. Risk assessment in maritime transportation. Reliability Engineering and System Safety, 74(3):299 – 309,
2001.
J. Stasko, C. Gorg, Z. Liu, and K. Singal. Jigsaw: Supporting investigative analysis through interactive visualization. In Proceedings
of the IEEE Symposium on Visual Analytics Science and Technology,
pages 131–138, 2007.
J. J. Thomas and K. A. Cook, editors. Illuminating the Path: The R&D
Agenda for Visual Analytics. IEEE Press, 2005.
U. S. C. G. U.S. Department of Homeland Security. U.S. Coast Guard
Addendum to the United States National Search and Rescue Supplement (NSS) to the International Aeronautical and Maritime Search
and Rescue Manual (IAMSAR). September 2009.
J. J. V. Wijk and E. R. V. Selow. Cluster and calendar based visualization of time series data. In Proceedings of the IEEE Symposium on
Information Visualization, pages 4–9, San Francisco, CA, USA, 1999.
IEEE Computer Society.
N. Willems, H. van de Wetering, and J. J. van Wijk. Visualization of
vessel movements. Computer Graphics Forum, 28(3):959–966, 2009.

VACCINATED - Visual Analytics for Characterizing a Pandemic Spread
VAST 2010 Mini Challenge 2 Award: Support for Future Detection
Abish Malik∗

Shehzad Afzal∗
∗ Purdue

Erin Hodgess†

David S. Ebert∗

Ross Maciejewski∗

Visual Analytics Center - Purdue University
of Houston - Downtown

† University

1 I NTRODUCTION
Given a set of hospital admittance and death records, the challenge
was to characterize the spread of a pandemic in terms of the attack and mortality rates, spatiotemporal patterns of onset and the
recovery time. We began the analysis by preprocessing the hospital admittance records using the University of Pittsburgh’s CoCo
classifier [1]. CoCo is a text classification software that takes hospital admittance fields and classifies them into chief complaint categories (Botulinic, Constitutional, Gastrointestinal, Hemorrhagic,
Neurological, Rash, Respiratory, and Other). The choice of the
CoCo classifier was based on its online availability as well as its
well documented classification performance metrics, see [1].
Once the data was classified, we utilized and extended work done
by the Purdue University Visual Analytics Center on healthcare
analysis [2]. Our system consists of a combination of linked views,
showing time series views of syndromes and death rates through
line graph views (Figure 1 - Top), stacked graph views showing
deaths (Figure 1 - Bottom), geographical map views showing the
impact by country (not illustrated in this paper), and summary windows providing statistical breakdowns of the data (not illustrated in
this paper). All views are linked through an interactive time slider
that allows users to explore the data over time. Extensions to our
previous work [2] include the stacked graph view, summary windows, new control chart methods, and an interactive ‘tape measure’
tool.
2 T IME S ERIES A NALYSIS
Underlying the views, we apply analytical algorithms based on
typical control charting methods for time-series anomaly detection
over the categorical chief complaints. These analytical algorithms
feed back into the visualization as glyphs within the time-series, denoting when anomalous patterns have occurred in the data. These
temporal anomalies represent deviations from the expected values
and indicate a need to investigate the current status of the data.
An Exponentially Weighted Moving Average (EWMA) control
chart with a 99% confidence interval upper bound was used for
event prediction and detection. In the line graph view (Figure 1 Top), the selected syndrome (or syndromes) is automatically passed
to the EWMA algorithm. The EWMA algorithm takes the temporal
data and first smoothes the time series:
St = (1 − α )St−1 + α yt , for t ≥ 2,
where St is the smoothed series, y is the original time series and
0 ≤ α ≤ 1. To initialize the smoothed series at t = 1, we use S0 = ȳ;
that is, the sample mean:
ȳ =

1 n
∑ yt
n t=1

IEEE Symposium on Visual Analytics Science and Technology
October 24 - 29, Salt Lake City, Utah, USA
978-1-4244-9487-3/10/$26.00 ©2010 IEEE

Given the smoothed series, we calculate an upper control limit
(UCL) as:
r
α
[1 − (1 − α )2i ]
UCL = ȳ + 3σ
2−α
where σ is

σ=

v
u n
u ∑ |yt −yt−1 |
u t=2
t
n−1
1.128

where 1.128 is a table value from the original EWMA design [3].
For this challenge, we did not use the lower control limit, since we
were only concerned with the upper limit, as this indicated high
values of the diseases. When the smoothed series was higher than
the upper control limit, then we indicated this with red dots indicating an alert. We chose not to visualize the upper control limit as
this added more clutter to the time series view when multiple series
were plotted at once.
We let the multiplier for the control limit be 3, representing
at 99% confidence interval band for the UCL. Finally, we chose
α = 0.2 based on the typical default value given in [3]. Note that
were the data to have begun being collected during the pandemic,
then the EWMA control charting would not have detected changes
in the time series as the initial rate would have already been high.
As such, it was assumed in this challenge that there would be some
period of normalcy prior to the pandemic outbreak. It is important
to note that the EWMA control chart algorithm is interactively applied to any time series view generated by through interactive filters
in the program. Thus, the user can see the alerts for not only one
syndrome, but multiple syndromes (either combined into a single
time series of separated into its components where each component
has its own control chart applied for alert generation).
For the analysis, we visually explored each of the syndromic
categories, by country, by hospital admittance records. We found
that two categories (gastrointestinal Figure 1 (Top) and hemorrhagic) have an outstanding number of temporally contiguous alerts
in eight out of the eleven given spatial locations (Aleppo, Colombia, Iran, Lebanon, Nairobi, Saudi Arabia, Venezuela and Yemen).
Only Thailand, Turkey and Karachi seem unaffected.
3 I NTERACTIVE M EASUREMENTS
Next, our goal was to characterize the parameters of the pandemic.
To facilitate this, we implemented an interactive ‘tape measure’ tool
which the user could drag across the line graph view (Figure 1 Top). The user clicks anywhere within the line graph window, and
drags the mouse. This has an effect of creating a tape measure that
provides a distance between two points with respect to the temporal
axis. The tape measure provides three outputs, the number of days
being measured, the attack rate, and the mortality rate. For a given
syndrome, the attack rate of the syndrome is calculated as the number of patients seen with syndrome X divided by the total number
of patients seen. The mortality rate is calculated as the number of

281

Figure 1: (Top) The line graph view of the gastrointestinal syndrome category for Iran. The red dots on the time series represent alerts that fall
within the 99% confidence interval of an EWMA control chart. The line connecting the first alert and the last alert is an interactive ‘tape measure’
tool which the user can use to calculate the attack and mortality rate. (Bottom) The stacked graph death records view. The black line shows the
current day the user is exploring. The overlaid red texture shows the deaths related to patients seen on the current day under exploration.

patients seen that died from syndrome X divided by the total number of patients presenting with syndrome X. The mortality rate is
an approximate rate as we ignore patients categorized as other that
may still be demonstrating signs of infection.
Use of this tool is shown in Figure 1 - Top. Here, we have extended the tape measure over the entire series of alerts and see the
approximated mortality and attack rate (with respect to gastrointestinal syndromes) for Iran, finding a mortality rate of 8.864% and
an attack rate of 23.591%. The average mortality rate over all the
countries of interest is 10%. The average attack rate over all countries of interest is 26%.
Concurrently, we wanted to characterize the time between onset
and death. For this, we utilized a stack graph view to explore patient death records linked to those patients that had presented with
syndromes. This view shows deaths, by category, totaled by all
selected countries, visualized on a single plot. As the user scrolls
through time, a texture overlay is plotted to show links between the
current days patients and the future times in which any subset of
these patients died. From this, we can determine the approximate
amount of time it would take a patient, showing signs of a given
syndrome to die. Figure 1 - Bottom shows this view for the country
selection of Iran. We find that from hospital admittance to reported
death, the average time span in which a patient may succumb to an
illness is 8 days. This was consistent across all countries of interest
in our study.
4

PANDEMIC S PREAD A NALYSIS

Finally, in comparing outbreaks across countries and cities, we utilized the map view visualization component of our system. As we
scrolled through time, we hypothesized that the disease progresses
from Nairobi City in Kenya to Aleppo in Syria, Lebanon and Iran.

282

From there, it seems to progress to other Middle Eastern countries
(Saudi Arabia and Yemen) to South America, appearing in Colombia and Venezuela indicating that air travel may have played a part
in the pandemic spread.
ACKNOWLEDGEMENTS
This work has been supported by the U.S. Department of Homeland Security’s VACCINE Center under Award Number 2009-ST061-CI0001. A portion of this research was performed under an
appointment to the U.S. Department of Homeland Security (DHS)
Summer Research Team Program for Minority Serving Institutions
under DOE contract number DE-AC05-06OR23100.
R EFERENCES
[1] W. W. Chapman, J. N. Dowling, and M. M. Wagner. Classification
of emergency department chief complaints into 7 syndromes: A retrospective analysis of 527,228 patients. Annals of Emergency Medicine,
46:445–455, November 2005.
[2] R. Maciejewski, S. Rudolph, R. Hafen, A. Abusalah, M. Yakout,
M. Ouzzani, W. S. Cleveland, S. J. Grannis, M. Wade, and D. S. Ebert.
A visual analytics approach to understanding spatiotemporal hotspots.
IEEE Transactions on Visualization and Computer Graphics, 16:205–
220, Mar. - Apr. 2010.
[3] D. C. Montgomery. Introduction to Statistical Quality Control. Wiley:
New York, 2008.

Applied Visual Analytics for Economic Decision-Making
Anya Savikhin∗
Ross Maciejewski†
David S. Ebert‡
∗ Purdue University Department of Economics
†‡ Purdue University Regional Visualization and Analytics Center (PURVAC)

A BSTRACT
This paper introduces the application of visual analytics techniques
as a novel approach for improving economic decision making. Particularly, we focus on two known problems where subjects’ behavior consistently deviates from the optimal, the Winner’s and
Loser’s Curse. According to economists, subjects fail to recognize the profit-maximizing decision strategy in both the Winner’s
and Loser’s curse because they are unable to properly consider all
the available information. As such, we have created a visual analytics tool to aid subjects in decision making under the Acquiring
a Company framework common in many economic experiments.
We demonstrate the added value of visual analytics in the decision
making process through a series of user studies comparing standard
visualization methods with interactive visual analytics techniques.
Our work presents not only a basis for development and evaluation
of economic visual analytic research, but also empirical evidence
demonstrating the added value of applying visual analytics to general decision making tasks.
1

T HE W INNER ’ S

AND

L OSER ’ S C URSE

In order to best demonstrate the applicability of visual analytics to
decision making problems, we have chosen to analyze a familiar
situation, bidding at an auction. When a person bids at an auction,
economists assume that the item under bid is of equal value to all
participants. This type of auction is known as a Common Value Auction. Here, each participant has some estimate of the item’s worth
with a degree of uncertainty, and each places a bid accordingly. The
winner of the auction is the individual who bids the highest. Unfortunately, if we assume that the average bid of all the participants
represents the actual worth of the item, then the winner of the auction has now clearly overpaid. This phenomenon is known as the
Winner’s Curse [2, 10, 14]. As the number of bidders increases, so
does the severity of the amount overpaid and the more likely it is
that some of them have overestimated the auctioned item’s value.
In technical terms, the winner’s expected estimate is the value of
the first order statistic, which increases as the number of bidders
increases. Since most auctions involve a degree of common value
and uncertainty, this deviation from the optimal bid is an important
phenomenon to study. In this case, companies must assess the value
of the market in that area and make a bid accordingly.
The sister problem to this is known as the Loser’s Curse [7],
which appears when the parameters of the experiment are changed
in such a way that the naive bid is now below the profit-maximizing
bid, and the profit-maximizing bid results in winning the company.
This is because the lower bound for the range of possible company
values is increased, reducing the bid range in which high negative profits would result (as in the Winner’s Curse setup). Subjects
∗ e-mail:

anya@purdue.edu

† e-mail:rmacieje@purdue.edu
‡ e-mail:ebertd@ecn.purdue.edu

IEEE Symposium on Visual Analytics Science and Technology
October 21 - 23, Columbus, Ohio, USA
978-1-4244-2935-6/08/$25.00 ©2008 IEEE

who do not win the company have lower profits than those who bid
higher and win the company, but subjects consistently underbid in
this setup and do not find the profit-maximizing solution.
In order to better understand subject motivations, the Winner’s
and Loser’s Curse have also been studied in a simplified framework
which is referred to as the Acquiring a Company problem, first formulated by Samuelson and Bazerman [14]. The Acquiring a Company problem is simplified because subjects no longer interact with
or bid against one another; instead, the subject bids on a company,
the value of which is randomly determined by the computer but unknown to the subject. The subject is also told that the company is
worth more to him than it is to the seller (in this case, the computer
is the seller). Even though subjects no longer compete against one
another to win the company, both the Winner’s and Loser’s Curse
are still present.
In order to compare our results with previous work, we have
chosen to employ the Acquiring a Company framework in our visual analytics application and evaluation work. The motivation for
this work is to help subjects overcome this failure by helping them
better consider the relevant information, thereby improving their
decision making abilities. We present an extensible visual analytics framework for economic decision making, and demonstrate the
added value of such techniques through the results of a user study.
Our work shows the benefit of visual analytics in economic decision making, improving subject performance in the Acquiring a
Company framework, and aiding in overcoming the Winner’s and
Loser’s Curse phenomena.
2

A PPLYING V ISUAL A NALYTICS
M AKING

TO

E CONOMIC D ECISION -

We present this study at a crucial time when many researchers are
calling for new techniques and systems to help non-expert users of
varying abilities in complex decision-making and analysis tasks [9].
According to the NIH/NSF Visualization Research Challenges report, visualization is essential to the solution of complex problems
in every field and systems must allow ordinary people to experiment with “what if” situations [9]. Our research addresses these
points through the application of a visual analytics tool that can aid
in complex decision making tasks in real world applications with
uncertainty.
2.1

Related Literature in Economics

Many studies have attempted to address the Winner’s and Loser’s
Curse problems through the Acquiring a Company framework over
the past 20 years. The goal of these studies was to find a way for
subjects to avoid the Winner’s or Loser’s Curse by improving their
decision-making abilities. Some of these studies allowed for role
reversal or modified the game, while others attempted to make the
game “easier” for subjects, or to prepare subjects better. In one
study, a training package was designed to help subjects, with training on various conditional probability problems, but this was only
marginally effective in improving learning [8]. In another study,
training was employed where subjects were given a lesson in probability and then performed four similar tasks where the Acquiring

107

Table 1: Previous Results

Ball et al, 1991
Tor and Bazerman, 2003
Bereby-Meyer et al, 2002
Charness and Levin, 2006

Baseline Learning Rate
7%
N/A
14%*
<10%*

Type of Enhancement
Role Reversal
Training
Extra Feedback
Reduced Complexity

a Company task was done third in every order [17]. In the manipulations discussed above, it was found that the Winner’s Curse
persisted for most subjects. It was found that the Loser’s Curse was
equally persistent [7].
In most of these studies, the guideline for “learning” was defined
as bidding close to the optimal bid at some point and continuing
this strategy for the duration of the study. The general result of
most studies for the baseline case is a learning rate of around 7%.
Table 1 summarizes the results of selected previous studies in order
to give the reader an understanding of typical findings in this area
of research.
2.2

Related Work in Visual Analytics

Visual analytics is the science of analytical reasoning assisted by
interactive visual interfaces, which has already been applied and
found to be effective in social sciences such as management, finance, marketing and organizational behavior to aid in decision
making [1, 5, 16].Previous work has employed visuals and found
that subjects given a problem statement in the form of a visual performed better than subjects given the same information in the form
of a written statement [5]. Also, subjects given a visual interface
outperformed those using a text-based interface in decision making in low and high complexity tasks [16]. Research in the field
of energy and management has found that a system that employs
visualization is an effective tool in planning and decision-making
for bio-energy production [1]. As such, we feel that applying visual
analytics to economic decision making is a natural extension.
Since it has been shown that visualization can extend a person’s
working memory [3], it is natural to assume that the application
of visual analytics will improve subjects’ economic decision making skills. Furthermore, economists believe that the reason subjects
succumb to the Winner’s and Loser’s Curse is that they have a limited focus of attention and therefore fail to consider all of the information needed to solve the problem correctly. The decision making
failure can be explained by the theory of bounded awareness, which
suggests that people are rational but sometimes fail to make decisions rationally because the limits of cognitive abilities do not allow
them to consider all the information available [4]. A generalized
version of this theory is the more well-known concept of bounded
rationality, which is a school of thought in economics that explains
the limits of rational behavior and has been researched by behavioral theorists and experimental economists [6, 13, 15]. We believe
that the application of visual analytics can help subjects overcome
bounded rationality because of improvement in the productivity of
their cognitive effort. Because cognitive effort is costly, the effort
expended by individuals may not be enough to solve the problem
and discover the optimal solution. We propose that subjects use
an information production function, where effort level (chosen by
the subject) and the type of visual aid (chosen externally) are inputs. Subjects maximize their earnings in the Acquiring a Company game by choosing optimal levels of effort and their bid. An
economic model is under development that will help explain this
phenomenon theoretically.
2.3

Contribution

We have evaluated the effects of various applications through the
implementation of an interactive visual analytics program and a

108

Subsequent Learning Rate
6%-9%
8.5%
26%-50%*
30%-60%*
*total percentage of optimal bids (not learning rate)

simple visual representation, as well as a baseline tabular representation of the data, under the hypothesis that such representations
will enhance subjects’ abilities to overcome the bounded awareness
issues. In particular, the interactive visual analytics program should
be most effective because subjects use this to explore the link between decisions and outcomes before, rather than after they make
their decisions as compared to the simple visual or tabular representations, in which they receive the information as feedback after the
decision has been made. The interactive visual representation is the
only way for subjects to receive information before they make their
decision, because non-interactive representations (the simple visual
and the table) do not have this capability. To test our hypothesis,
we have performed a user study in which one group of subjects was
given the data in tabular form, one group was provided a simple
visual representation of the data, and another group was given the
use of an interactive visual analytics program. Consistent with our
hypothesis, we found that the group given the visual representation
outperformed the group given the data in tabular form. Also, the
group given the interactive visual analytics program outperformed
both the group given the visual representation and the group given
the data in tabular form, where the latter was statistically significant.
This result is very promising for future work in the application of
visual analytics to various economic decision making problems.
3 E XPERIMENTAL D ESIGN AND S ETUP
We considered two problems: a Winner’s Curse treatment and a
Loser’s Curse treatment. Six treatments were run in total (Winner’s Curse-Interactive Visual Analytics (WC-IV), Winner’s CurseSimple Visual (WC-V), Winner’s Curse Table (WC-T), Loser’s
Curse-Interactive Visual Analytics (LC-IV), Loser’s Curse-Simple
Visual (LC-V) and Loser’s Curse Table (LC-T)). The treatments
and the experiment parameters are summarized in Table 2. The
baseline user study provided subjects with feedback information on
outcomes in tabular form. The second user study provided subjects with feedback information in a simple visual representation.
Finally, the third user study allowed subjects to make use of an interactive visual analytics program at all times during their decisionmaking process. Our prediction was that subjects who participated
in the interactive visual analytics study would bid closer to the profit
maximizing decision than those participating in the simple visual or
tabular studies. Furthermore, we also predicted that even those participating in the simple visual study would outperform those participating in the tabular study.
3.1 Experimental Design
Our design utilized parameters similar to previous research. Each
subject, independent of any other subject, was given the role of the
buyer. As a buyer, the subject had to make the decision of what
to bid for a company. The company could take values between X
and U, with all values in that range equally likely. In accordance
with Samuelson and Bazerman [14], the company was worth 50%
more to the buyer than to the seller for all treatments.During the
experiment, the value of the company was determined randomly by
the computer. If the bid was higher than or equal to the value of
the company, the computer always sold the company to the buyer.
If the bid was lower than the value of the company, the computer
never sold the company to the buyer. The buyers were aware that
the seller was informed about the value of the company before the

Table 2: Winner’s/Loser’s Curse treatments and parameters.

Winner’s Curse
X =4
U = 100
M = 1.5
Optimal Bid = 8
Naive Bid = 52

Loser’s Curse
X = 50
U = 100
M = 1.5
Optimal Bid = 100
Naive Bid = 75

transaction took place. The buyers were also aware that they would
not know the value of the company before they placed their bids.
Each buyer was given 350 tokens at the beginning of the experiment with which to bid. Each buyer was given 30 periods in which
to bid on the company, and informed that the company takes a new
value in each period, independent from the previous period or from
any other period. Tokens were converted to dollars and subjects
were paid in cash based on how well they performed at the end of
the experiment. After each period, subjects were shown the information screen, which was either the visual or a simple table.
We also introduced using the Surprise-Quiz methodology as described by Merlo and Schotter to quantify learning in the Acquiring
a Company game [12]. According to Merlo and Schotter, if the
experiment requires subjects to learn the properties of a static equilibrium, the surprise quiz methodology can be used. In the SurpriseQuiz, subjects were given the opportunity to use the visual (or the
table information screen, in the other treatment) to find the “optimal bid”. Subjects were told that an optimal bid exists, that they
had one chance to submit a bid they thought was close to the optimum, and that they would be paid based on how close their bid
was to the optimal bid. It was explained to subjects that the optimal
bid is one that would maximize earnings over time, regardless of
the company value. The subjects were given as much time as they
required (up to 1 hour, although no subject spent this much time) to
come up with their decisions. The bid submitted in the bonus round
is the measure of learning in the experiment and we used this bonus
round bid to compare decision making across treatments.
3.2 Theoretical Predictions
The assumption in economic theory is that a risk-neutral individual
will maximize his or her expected net earnings over time. However,
most subjects fail to take into account the fact that the value of the
company is known to the seller, and bid the expected value of the
company instead, which results in the Winner’s or Loser’s Curse.
Profit-maximizing subjects will take conditional expectations into
account and solve the maximization problem:

 


B−X
B−X
max
1.5 X +
−B
(1)
B
100 − X
2
where B is the bid. Differentiating this equation with respect to B
yields:
 


1
B−X
1.5 X +
− B − .25 (B − X)
(2)
100 − X
2
By setting Equation 2 equal to zero and solving for B, we find that
B = 2X.
The optimal bid in the Winner’s Curse treatment would be 8,
while the optimal bid in the Loser’s Curse treatment would be 100.
The naive bid of 52 for the Winner’s Curse treatment would result in an expected profit loss, while the naive bid of 75 for the
Loser’s Curse treatment would result in lower expected profit than
any higher bid. This is because raising the lower bound for the
range of company values reduces the decision space where outcomes could be negative, which is very large in the Winner’s Curse
treatment.
To understand this result intuitively, suppose that you are a buyer
bidding on a company in the Winner’s Curse treatment. The naive

approach would be to bid the expected value of the company (52),
under the assumption that your bid will be accepted by the seller
and since the company is worth 50% more to the buyer, the buyer
will benefit with higher than expected earnings. However, you must
take into account the fact that the seller knows the value of the company before accepting the offer. Therefore, at a bid of 52, the bid
will only be accepted if the company is worth less than 52. In this
case, if the company value is between 4 and 33, there will be possible negative profit for the buyer, and if the company value is between 34 and 50, there will be possible positive profit. This results
in an expected profit loss at a bid of 52; therefore, this bid is not
optimal. In fact, any bid higher than 8 will result in expected profit
loss. Bidding 8, on the other hand, will result in acceptance only if
the value of the company is between 4 and 8. However, the result
in this case is that of expected profit gain.
Suppose now that you are a buyer bidding on a company in the
Loser’s Curse treatment. The naive approach would be to bid the
expected value of the company (75), without accepting that asymmetric information plays a role here. This time, increasing the bid
at the margin would result in a larger gain than the bid of 75. For example, we know that at the bid of 75, expected profit is 9.75, where
the buyer just breaks even (gets 0) if the company value is 50, gains
positive profit if the company value is between 50 and 75, and does
not acquire the company if the company value is greater than 75.
Increasing the bid to 76 results in an expected profit of 9.99, with
small negative profit of -1 if the company value happens to be 50,
positive profit between and including company values 51 and 76,
and no profit for company values above 76. Continuing to increase
the bid, finally, at the optimal bid of 100, one finds that expected
profit is 12.75, with some small negative profit at company values
below 67, and large positive profit at company values above and
including 67. Since company values are uniformly distributed and
the bidder bids for 30 periods, it is rational to bid the value which
will result in highest expected profit - in this case, this is 100.
3.3

Visual Analytics Application

Our contribution is helping subjects improve decision making
through the use of an interactive visual analytics program. A simple
visual representation, which basically increases the quality of information feedback, is also used for comparison. Both the interactive
visual analytics program and the simple visual representation use
2D graphics to show the profit the subject would have received at
every possible company value at his/her bid amount, after the outcome of the period is determined. According to Cleveland’s graphics elements hierarchy, position along a common scale is one of
the best ways to display this type of data [18]. The interactive visual analytics tool also allows the subject to explore the dataset on
his/her own time before he/she makes a decision. First, the subject is able to move the proposed bid up and down to see how the
possible profit changes. Second, the subject is able to ‘sample’ numerous possible company values with the click of a button, and see
what he/she would have earned at the proposed bid. The simple
visual representation and the baseline tabular representation programs track the amount of time spent on each period, as well as
the subjects’ bid, company value, and profit. The interactive visual
analytics tool tracks the above but also goes further and tracks the
subjects’ changes in the proposed bid (up or down) and clicks of
the ‘sample company value’ button.
Figures 1 and 2 show screenshots of the simple visual analytics
representation at naive and optimal bids of the WC-V and LC-V
treatments. Figure 3 shows the interactive visual analytics program
for the Winner’s Curse treatment (WC-IV) during the time the subject is deliberating a decision, and after the subject has placed a
bid and the outcome has been revealed. The Loser’s Curse interactive visual analytics treatment (LC-IV) is similar but has a different
possible earnings distribution which is identical to LC-V. For com-

109

Figure 1: Visual analytics: Loser’s Curse treatment

Figure 2: Visual analytics: Winner’s Curse treatment

parison, Figure 4 is a screenshot of the information screen shown
to subjects who were not given the visual display and were instead
given the information in table format as the baseline case (WC-T
and LC-T).
Both the simple visual display and the tabular display allow subjects to input a bid and look at all possible outcomes of that bid. The
largest difference between the display shown in Figures 1 and 2 and
the display shown in Figure 4 is that the display in Figures 1 and
2 employ visualization to disseminate the information to subjects.
Both information screens show the relationship between profit and
company value. The simple visual display is thought to be more
effective because a visual can allow the user to see the relationship more easily and quickly. It can also save time, an important
economic resource. One might believe that the information screen
which is more effective is the one with the most information. It
is important to be aware that the table includes the same amount,
if not more, information than the visual display. The table depiction includes columns for the bid entered, the value of the company,

110

and the possible profit in each case, for all values of the company.
The simple visual display is slightly more complex, but does not
contain any extra information, except that color is used to draw attention.Green areas on the graph represent areas of possible positive
profit, while red areas on the graph represent areas of possible negative profit. The actual company value also appears on the graph
as a vertical blue line. In comparison, the interactive visual analytics tool is more complex than the two non-interactive treatments
described above. The interactive visual analytics tool has all the capabilities of the simple visual representation, and also allows subjects to discover possible outcomes before making their decision by
clicking “sample a company value” or moving their proposed bid
up or down.
One other difference between the interactive visual analytics tool
and the simple visual display is that the former shows information
to the user before a decision is made, while the latter shows information to the user after the decision has been made. However,
since the user makes decisions 30 periods in a row, it is likely that

Figure 3: Interactive Visual Analytics - During Decision (left) and After (right) (WC)

the user remembers the previous period’s outcome screen while he
or she is making the next period’s decision - if this is the case, then
in all periods but the first period, we can suggest that the user has
the simple visual display’s information in mind before the subjects
makes the current period’s decision. In future work, it would be
possible to test if this is the case by running a treatment where the
user is shown the outcome from the previous period at the beginning of the current period, and instructed to think about his current
decision at that time. The timing of showing this screen would be
the same, with the only difference being the additional instructions.
Because this change is so minor, we feel that the results would not
be affected.

between the company value and the profit. Most importantly, the
green positive profit and red negative profit areas were displayed in
order to effectively convince the user of the importance of the size
of these areas in making his/her decision. The relative size of the
areas represents the probability of positive versus negative profit.
Because the user chooses the bid value, and the visual looks different for different bid values, the user can change his/her bid values
to observe the differences in positive and negative profit possibilities. The interactivity of the more advanced display is even more
effective, because it allows users to view possible outcomes during
the decision making process.
3.4

Figure 4: Tabular treatment for Winner’s/Loser’s Curse

The visual analytics display was chosen because it was believed
that displaying the entirety of the information on the screen in a
visual form would allow users to overcome bounded awareness issues and be more likely to focus on all the relevant information.
The particular graph was chosen to clearly show the relationship

User Studies

Subjects in the WC-V and LC-V treatments were shown the simple
visual display after each period, while subjects in the WC-IV and
LC-IV treatments were also shown the interactive display during
each period while they were making their decisions. Subjects in the
WC-T and LC-T treatments were shown the tabular display after
each period. At the end of the experiment, all subjects were also
given the choice to use the visual analytics display again to make
their decisions in the bonus round (subjects in the WC-T and LC-T
treatments were given the choice of using the tabular tool as well.)
The choices of subjects during the first 30 periods were recorded,
and choices subjects made while using the display for the bonus
round were also recorded. In the WC-IV and LC-IV treatments,
all choices made during the deliberation process were recorded as
well.
We recorded the amount of time spent on each decision as the
time in seconds from the start of the period through the end. For the
simple table and simple visual display this included the time spent
deliberating (no visual or table during this time), plus the time spent
observing the output screen for the future period and recording the
result. For the interactive visual analytics tool, this included time
spent deliberating, which in this case was supplemented with the interactive visual analytics tool, and time spent observing and recording the result (the result screen was the same as the most recent
previous screen - the only difference being the addition of the actual company value line.) The mean amount of time spent on each
period, with all periods and subjects was between approximately
8.5 seconds and 30 seconds. Within the Winner’s Curse sessions,
the most time was spent on the WC-T and WC-V treatments (21
seconds and 30 seconds, respectively) and the least time was spent

111

Table 3: Summary of subjects.

WC
LC
Total

Tabular Treatment
11 subjects
100% passed training
8 subjects
100% passed training
19

Simple Visual Treatment
7 subjects
90% passed training
7 subjects
80% passed training
14

on the WC-IV treatment (18.7 seconds.) Within the Loser’s Curse
treatments, the most time was spent on the LC-T and LC-V treatments (19 seconds and 15 seconds, respectively), and the least time
was spent on the LC-IV treatment (8.6 seconds.) We are in the process of investigating further the reason for these differences in time
spent. In every treatment, there is a negative correlation between
period number and time spent; that is, users spent less time deliberating each decision and outcome as periods progressed. This correlation is between -0.16 and -0.33, with no systematic difference
across treatments.
From the post-experiment comments, which were submitted
anonymously, it is clear that most subjects understood how to use
both the simple visual display and the interactive visual analytics
tool. Discussion of strategies included “find a value at which loss
region is smaller so that it is more likely to gain profit”, “stay mostly
around 30s because it seemed the loss and gain areas were close to
equal” and “try to make the green [possible positive profit] area as
large as possible”. Subjects using a table instead of the visual analytics display made comments such as “guess randomly” or “try to
find the pattern of the company value.”
3.5 Experimental Setup
Inexperienced undergraduate students from Purdue University were
recruited to participate in the study - 56 subjects in total participated. Subjects earned between $5 and $25 in the experiment,
where their earnings depended on their success at learning the optimal solution to the problem. 23 subjects participated in the experiment using the interactive visual analytics display, 14 subjects
participated in the experiment using the simple visual display, and
19 subjects participated in the experiment using the table display.
The number of subjects participating in the experiment is summarized in Table 3.
3.6 Pre-Experiment Quiz & Training
Before the beginning of the experiment, subjects also received
training on reading the information screen. For all treatments, this
included a “Lemonade Stand” task which was not related to the experiment, where subjects were given an information screen which
showed possible profit for all number of customers. An example of
how to calculate possible profit was given, and then subjects were
asked to answer 5 simple questions about the information screen to
gauge how well subjects understood how to read the screen. The
“Lemonade Stand” task was not a difficult or complex problem like
the Acquiring a Company problem. Subjects who did not answer
all 5 questions correctly were asked to leave and did not participate
in the experiment. Subjects who participated in the WC-V, LC-V,
WC-IV and LC-IV treatments were shown the graphical version of
the possible profit, while subjects who participated in the WC-T
and LC-T treatments were shown the table version of the possible
profit. 100% of the subjects were able to answer the questions when
the information screen was a table, while only 90% of the subjects
were able to answer the questions when the information screen was
a graph. This assured that only subjects who understood what the
information screen was telling them were included in the study. After passing the training, subjects were explained the rules of the
Acquiring a Company game and shown one period of the bidding
process. The information screen was then explained to subjects on
a display at the front of the room.

112

4

Interactive Visual Treatment
13 subjects
100% passed training
10 subjects
90% passed training
23

Total
31
25
56

R ESULTS

Subjects given the interactive visual analytics treatment learned the
optimal solution more often than subjects who were only given the
simple visual representation or the table information screen. Subjects given the interactive visual analytics treatment outperformed
subjects given the table information screen for both the Winner’s
and Loser’s Curse treatments, and this was statistically significant.
Subjects participating in the Loser’s Curse simple visual representation treatment outperformed subjects in the Loser’s Curse tabular
treatment, and this was statistically significant. The rest of the results also show the predicted effect, but these are not statistically
significant. Also, when asked to make the optimal decision in the
bonus round, subjects in the interactive visual analytics tool were
more likely to utilize the information screen again than subjects in
the simple visual treatment, and subjects in the simple visual treatment were more likely to utilize the information screen again than
subjects in the table treatment (100% (23/23) versus 86% (9/14)
versus only 25% (5/20)).
4.1

Comparisons of Learning

Learning in the game is measured based on how close the bonus
round bids (or the surprise quiz) are to the optimal. These can be
compared across treatments to understand the effectiveness of visual analytics at improving decision making. The bonus round results should be accurate because the bonus round allowed subjects
to pause and think more carefully about their decisions. The bonus
round also gave subjects the opportunity to explore the range of the
visual display at their own pace and really “test out” different bids
without any penalty. The bonus round result was included in the
payment structure of each subject and subjects who bid closest to
optimal in the bonus round were given a large monetary bonus as
compared to the money earned in the first 30 rounds of the game.
With learning defined as bidding equal to or very close to the optimal value in the bonus round, learning rates can be reported. Here,
one could define “close to the optimal” as within 10% of the total
bid area, where the “bid area” is between 4 and 100 for WC treatments and 50 and 100 for LC treatments (since the company value
can never fall below 4 or 50, respectively, it may not be reasonable
to include those values in the total bid area - bidding these would
be dominated by bidding the lowest possible company value, as this
would increase possible earnings with zero risk - subjects seemed
to understand this as no subject bid below these values in either the
30 period game or the bonus round.) It may have been difficult for
subjects in the LC treatments to bid within just 5 values of the optimal bid (within 10% is within about 5 integers for this treatment,
and subjects were only allowed to bid integer values), therefore expanding the ‘close to the optimal’ definition to ‘just under within 10
integers’ may be reasonable, which is approximately within 10%
for the WC treatments. Learning was higher for subjects in the visual treatments versus subjects in the tabular treatment, and this is
summarized in Table 4.
4.2

Statistical Analysis

We ran a two-sample Wilcoxon (Mann-Whitney) rank-sum test to
check for differences between all possible pairings of groups within
the WC treatment and all possible pairings of groups within the LC
treatment bids in the bonus round, and these results are summarized

Table 4: Learning.

WC - defined as within 10% of area
LC - defined as within 10% of area
LC - defined as within 10% of WC area

Tabular Treatment
0%
0/9 subjects
12.5%
1/8 subjects
12.5%
1/8 subjects

Simple Visual Treatment
14.3%
1/7 subjects
28.5%
2/7 subjects
28.5%
2/7 subjects

Interactive Visual Treatment
54%
7/13 subjects
10%
1/10 subjects
30%
3/10 subjects

Table 5: Test results summary.

Test
Winner’s Curse

Wilcoxon

Loser’s Curse

Wilcoxon

Prediction
WC-V 6= WC-T
WC-V6=WC-IV
WC-IV6=WC-T
LC-V6=LC-T
LC-V6=LC-IV
LC-IV6=LC-T

in Table 5. The hypothesis would suggest that decision making for
the groups receiving the interactive visual analytics display would
be improved as compared to the groups who were shown the simple visual representation or the table, while groups receiving the
visual representation would still outperform those receiving the table. Subjects using the interactive visual analytics tool or the simple
visual display should choose to bid values closer to the optimal bid
- lower values for the Winner’s Curse treatment, and higher values
for the Loser’s Curse treatment, as compared to the group receiving
the data in table format. The Wilcoxon test gives a measure and
direction of the difference in medians between the two groups. The
WC-IV group chose bids which were lower than the WC-T group
as predicted by the hypothesis, and these results were statistically
significant (p-value 0.012). The LC-IV and LC-V groups chose
bids which were higher than the LC-T group as predicted by the
hypothesis, and these results were statistically significant as well
(p-values 0.040 and 0.055, respectively). However, no significant
results were found when comparing the other pairs of groups. It is
clear that the interactive visual analytics display was useful for both
the Winner’s and the Loser’s Curse. The simple visual display was
also useful for the Loser’s Curse.
4.3

Reasoning

While it seems that in complicated situations an interactive visual
analytics program can be very useful, we believe we may be able
to explain why the simple visual representation was useful for the
Loser’s Curse treatment but not as useful for the Winner’s Curse
treatment. One possible reason is that the relative size of the gain
region as compared to the loss region was much more pronounced
in the Loser’s Curse than in the Winner’s Curse. Thus, the Loser’s
Curse had gain and loss regions which were easier for subjects to
compare than those in the Winner’s Curse graph. That is, it was
clearer to users in the Loser’s Curse treatment that the naive bid had
a smaller relative possible positive profit region than the optimal bid
than it was to users in the Winner’s Curse treatment. This proposition is supported by previous research which suggests that graphic
information is given more weight by subjects when visual representations show sharp contrast or greater variation in size [11]. We
also strongly believe that increasing the number of subjects would
increase the significance of the results, as the level of cognitive ability is heterogeneous across subjects.
4.4

Distances to the Optimal Bid

Figure 5 shows the distances of bids from the optimal in all treatments, with WC-IV and LC-IV groups in front of WC-V and LC-V
groups, and WC-V and LC-V groups in front of WC-T and LCT groups. For both the Winner’s and Loser’s Curse experiments,

direction
WC-V<WC-T
WC-V>WC-IV
WC-IV<WC-T
LC-V>LC-T
LC-V>LC-IV
LC-IV>LC-T

Confirms With Hypothesis
Yes
Yes
Yes∗
Yes∗
No
Yes∗

p-value
0.928
0.103
0.012∗
0.055∗
0.845
0.040∗

our hypothesis was that subjects provided with the interactive visual program would bid closer to the optimal than subjects provided with a tabular representation or the simple visual representation. Also, subjects provided with the interactive visual analytics
program would outperform subjects with the simple visual representation. Clearly we can see that subjects in the visual treatments
bid closer to the optimal than subjects provided with the tabular
representation, and this was statistically significant for all interactive treatments (WC-IV and LC-IV) as well as for the simple visual
treatment in the Loser’s Curse (LC-V). Each bar in Figure 5 represents a subject’s distance to the optimal bid and we find that in general, WC-IV, LC-IV, WC-V and LC-V groups outperformed WC-T
and LC-T groups in finding the optimal bid. The only contradiction
was Subject 8 in LC-T, who found the optimal bid. In this subject’s
comments, he explained that he came to his answer by solving a
mathematical formula maximizing his profit. He also stated that he
did not use the information screen for his calculation. This is not
the typical way that subjects look at this issue, as most subjects’
comments included statements relating the information screen to
their choices. Therefore, it is probable that Subject 8 in LC-T is an
outlier and that, in a larger set of observations, he would have less
weight.
It could be argued that the reason subjects using the interactive
visual analytics tool and the simple visual display outperformed
subjects using the simple table is because graphs are easier to understand and read than tables. However, results from the “Lemonade Stand” task would suggest otherwise. First, while every user
was able to complete the task correctly in the case of the table version, only 90% of subjects were able to complete the task correctly
in the graph version. Furthermore, while those subjects who did
not correctly complete the “Lemonade Stand” task were asked to
leave, even those who stayed had more trouble in the visual analytics treatments versus the simple table treatments. During the
first few rounds of the experiment, no subjects had questions on
interpreting the table, while some subjects did ask questions about
interpreting the graph. The sessions with the table treatment took
far less time in total than the sessions with the visual treatments for
this reason. Even though it was more difficult to understand how
to read the interactive visual or simple visual analytics information
screens, users still outperformed those who had less trouble reading
the table information screen.
The improvement in cognition may be due to two factors: informational advantage and computational advantage. The information
presented in the simple visual display and the simple table was the
same; thus, all improvement here is due to computational advantage
- it was easier to interpret the information in visual form. However,
the user can obtain more information in the interactive visual ana-

113

Figure 5: Distance of bids from the optimal value (Left) Loser’s Curse (Right) Winner’s Curse. Here each bar represents the distance a single
subject’s response to the surprise quiz was from the optimal bid. The shorter the bar, the closer the bid was to optimal.

lytics tool treatment than in the other treatments by moving the proposed bid to see various outcomes. Thus, in the interactive visual
analytics treatment, increased information may also play a role. We
do not distinguish between these two factors in this paper; however,
this could be explored in future work.
The results suggest that visual analytics did help users to overcome some of the bounded rationality issues. In economic decision
making problems, considering all the information at hand is extremely important. Because of cognitive limitations, users are not
always able to consider all of the information in order to make rational decisions. Using visual analytics can improve cognitive abilities
and thus overcome some of these problems.
5

C ONCLUSIONS

AND

F UTURE W ORK

To the best of our knowledge, this is the first study that has addressed how visual analytics can improve economic decision making. It has been widely reported that competitive decision making
failures are common in Winner’s and Loser’s Curse problems in
economics. We used visual analytics as a tool to help subjects consider all the information of the problem and make a decision that is
closer to the optimal in the Acquiring a Company framework. The
interactive visual analytics display was extremely successful: subjects using the visual analytics display made improved decisions as
compared to subjects who had the same information in table form.
The simple visual display was also successful. This suggests that
showing all the information via a visual can be useful for overcoming bounded rationality issues that arise from the cognitive limitation of considering all the information at hand. This also suggests
that the addition of an interactivity element is very important to the
usefulness of visuals.
This research is the gateway for using visual analytics as a tool in
other economic decision making problems. Visual analytics would
be especially helpful in a situation where information feedback can
aid subjects, as our results are convincing evidence that visual analytics can increase the quality of the feedback. The interactivity of
the visual analytics program also increases participant involvement
and understanding, which is necessary for successful decision making. A broader situation where the Winner’s Curse is an issue is in
common value auctions. One direction for future research is to create a visual analytics display that helps subjects make profit maximizing decisions in the common value auction framework. Common value auctions are often used in the fields of real estate, commercial construction, and book publication rights. Visual analytics,
already proven to help subjects make decisions in the Acquiring a
Company framework, would be extremely valuable in the common
value auction framework as well. Visual analytics could then be ap-

114

plied by professionals in the workplace and would improve decision
making and, therefore, economic efficiency.
R EFERENCES
[1] N. Ayoub, R. Martins, K. Wang, H. Seki, and Y. Naka. Two levels
decision system for efficient planning and implementation of bioenergy production. Energy Conversion and Management, 48:709 – 723,
March 2007.
[2] E. Capen, R. Clapp, and W. Campbell. Competitive bidding in highrisk situations. Journal of Petroleum Technology, 23:641–653, 1971.
[3] S. Card, J. Mackinlay, and B. Shneiderman. Information Visualization: Perception for Design. Morgan Kaufmann Publishers, San Francisco, 1999.
[4] D. Chugh and M. Bazerman. Bounded awareness: What you fail to
see can hurt you. 2004.
[5] G. W. Dickson, G. DeSanctis, and D. McBride. Understanding the
effectiveness of computer graphics for decision support: A cumulative
experimental approach. Communications of the ACM, 29(1):40–47,
January 1986.
[6] G. Gigerenzer and R. Selten, editors. Bounded Rationality: The Adaptive Toolbox. MIT Press, 2001.
[7] C. A. Holt and R. Sherman. The loser’s curse. The American Economic Review, 84(3):642–652, June 1994.
[8] L. C. Idson, D. Chugh, Y. Bereby-Meyer, S. Moran, B. Grosskopf,
and M. Bazerman. Overcoming focusing failures in competitive environments. Journal of Behavioral Decision Making, 17:159–172, June
2004.
[9] C. Johnson, R. Moorhead, T. Munzner, H. Pfister, P. Rheingans, and
T. S. Yoo. NIH/NSF visualization research callenges report, 2005.
[10] J. H. Kagel and D. Levin. Common Value Auctions and the Winner’s
Curse. Princeton University Press, Princeton, New Jersey, 2002.
[11] N. H. Lurie and C. H. Mason. Visual representation: Implications for
decision making. Journal of Marketing, 71(1):160 – 177, Jan 2007.
[12] A. Merlo and A. Schotter. A surprise-quiz view of learning in economics experiments. Games and Economic Behavior, 28(1):25–54,
July 1999.
[13] A. Rubinstein. Modeling Bounded Rationality. MIT Press, 1998.
[14] W. F. Samuelson and M. H. Bazerman. The winner’s curse in bilateral negotiations. In V. L. Smith, editor, Research in Experimental
Economics, volume 3, pages 105–137. JAI Press, 1985.
[15] H. A. Simon. Models of Bounded Rationality. MIT Press, 1982.
[16] C. Speier and M. G. Morris. The influence of query interface design on
decision making performance. MIS Quarterly, 27(3):397–423, 2003.
[17] A. Tor and M. H. Bazerman. Focusing failures in competitive environments: Explaining decision errors in the monty hall game, the
acquiring a company game, and multiparty ultimatums. Journal of
Behavioral Decision Making, 16(5):353–374, 2003.
[18] L. Wilkinson. The Grammar of Graphics. Springer, New York, 1991.

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 20,

NO. 12 , DECEMBER 2014

1863

Proactive Spatiotemporal Resource Allocation and Predictive
Visual Analytics for Community Policing and Law Enforcement
Abish Malik, Ross Maciejewski, Member, IEEE, Sherry Towers, Sean McCullough, and David S. Ebert, Fellow, IEEE
Abstract— In this paper, we present a visual analytics approach that provides decision makers with a proactive and predictive
environment in order to assist them in making effective resource allocation and deployment decisions. The challenges involved with
such predictive analytics processes include end-users’ understanding, and the application of the underlying statistical algorithms at
the right spatiotemporal granularity levels so that good prediction estimates can be established. In our approach, we provide analysts
with a suite of natural scale templates and methods that enable them to focus and drill down to appropriate geospatial and temporal
resolution levels. Our forecasting technique is based on the Seasonal Trend decomposition based on Loess (STL) method, which
we apply in a spatiotemporal visual analytics context to provide analysts with predicted levels of future activity. We also present a
novel kernel density estimation technique we have developed, in which the prediction process is inﬂuenced by the spatial correlation
of recent incidents at nearby locations. We demonstrate our techniques by applying our methodology to Criminal, Trafﬁc and Civil
(CTC) incident datasets.
Index Terms—Visual Analytics, Natural Scales, Seasonal Trend decomposition based on Loess (STL), Law Enforcement

1

I NTRODUCTION

The increasing availability of digital data provides both opportunities
and challenges. The potential of utilizing these data for increasing effectiveness and efﬁciency of operations and decision making is vast.
Harnessing this data with effective tools can transform decision making from reactive to proactive and predictive. However, the volume,
variety, and velocity of these data can actually decrease the effectiveness of analysts and decision makers by creating cognitive overload
and paralysis by analysis, especially in fast-paced decision making environments.
Many researchers in data visualization and visual analytics [37]
have proposed interactive visual analytical techniques to aid analysts
in these tasks. Unfortunately, most work in this area has required
these casual experts (experts in domains, but not necessarily statistics experts) to carefully choose appropriate parameters from a vast
parameter space, select the proper resolution over which to perform
their analysis, apply appropriate statistical or machine learning analysis techniques, and/or understand advanced statistical signiﬁcance testing, while accounting for the different uncertainties in the data and
processes.
Moreover, the casual experts are required to adapt their decision
making process to the statistical analysis space where they need to
choose the appropriate time and space scales that give them meaningful analytical and predictive results. They need to understand the role
that data sparsity, different distribution characteristics, data variable
co-dependencies, and data variance play in the accuracy and reliability of the analytical and prediction results. In moving to this proactive
and predictive environment, scale issues become even more important.
Not only does the choice of appropriate scales help guide the users’
perception and interpretation of the data attributes, it also facilitates
gaining new insight into the dynamics of the analytical tasks [42] and
the validity of the analytical product: a spatial resolution level that is
too ﬁne may lead to zero data input values with no predictive statistical
value; whereas, a scale that is too coarse can overgeneralize the data
and introduce variation and noise, reducing the value and speciﬁcity of
• Abish Malik, Sean McCullough and David S. Ebert are with Purdue
University. E-mail: amalik|mccullo0|ebertd@purdue.edu.
• Ross Maciejewski and Sherry Towers are with the Arizona State
University. E-mail: rmacieje|smtowers@asu.edu.
Manuscript received 31 Mar. 2014; accepted 1 Aug. 2014. D ate of
publication 11 Aug. 2014; date of current version 9 Nov. 2014.
For information on obtaining reprints of this article, please send
e-mail to: tvcg@computer.org.
Digital Object Identiﬁer 10.1109/TVCG.2014.2346926

the results. Therefore, it becomes critical for forecasting and analysis
to choose statistically meaningful resolution and aggregation scales.
Utilizing basic principles from scaling theory [42], and Norman’s naturalness and appropriateness principles [26], we can both balance and
harness these cognitively meaningful natural human-centered domain
scales with meaningful statistical scales.
Therefore, in this paper, we present a visual analytics approach
that provides casual experts with a proactive and predictive environment that enables them to utilize their domain expertise while exploring their problem and making decisions and predictions at natural problem scales to increase their effectiveness and efﬁciency in
planning, resource allocation, and deployment. Our visual analytics
framework [21, 22] provides interactive exploration of multisource,
multivariate spatiotemporal datasets using linked views. The system
enables the exploration of historic datasets and examination of trends,
behaviors and interactions between the different spatiotemporal data
elements. The focus of this paper, however, is to provide a proactive
decision making environment where historic datasets are utilized at
natural geospatial and temporal scales in order to guide future decisions and resource allocation strategies.
In our predictive visual analytics process, we allow users to interactively select and reﬁne the data categories over which to perform their
analyses, explore and apply meaningful geospatial (Sections 4.1-4.3)
and temporal (Section 4.4) scales and aggregations, apply the forecasting process over geospace (Section 5), and visualize the forecasting results over their chosen geospatial domain. We utilize a Seasonal Trend
decomposition based on Loess (STL) [9] approach (Section 3) that utilizes patterns of historical data and apply it in the geospatial domain
to predict future geospatial incidence levels. Moreover, this approach
provides domain-driven reﬁnement of analysis and exploration to areas and time of signiﬁcance (e.g., high crime areas or times).
The contributions of our work include these novel natural spatial
and temporal analytical techniques, as well as a novel Dynamic Covariance Kernel Density Estimation method (DCKDE) (Section 4.2.2).
These contributions can be applied to a variety of spatiotemporal
datasets including distribution and logistics, public safety, public
health, and law enforcement. We will utilize data from Criminal, Trafﬁc, and Civil (CTC) incident law enforcement datasets in the examples throughout this paper. However, it should be noted that our technique is versatile and can be adapted for other relevant spatiotemporal
datasets that exhibit seasonality.

1077-2626 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

1864

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

2 R ELATED W ORK
In recent years, there has been much work done in utilizing historic
datasets for informing future actions and decisions of decision makers.
Below, we discuss previous work in the ﬁeld of visual analytics, and,
since our chosen example domain and implementation is focused on
crime data, we also explore previous work in criminology to provide a
breadth of the related research areas.
2.1 Predictive Visual Analytics
There have been several visual analytics systems developed in recent years that support data analysis and exploration processes, and
provide extensive data modeling and hypothesis generation tools
(e.g., [28, 35]). More recently however, researchers have also started
progressing toward creating visual analytics systems that incorporate
predictive analytics in them. For example, Wong et al. [43] provide a
visual interface and an environment that brings together research from
several different domains to predict and assess the impact of climate
change on U.S. power-grids. Muhlbacher and Piringer [25] provide a
visual analytics framework for building regression models. Monroe et
al. [23] utilize user-driven data visualizations that enable researchers
to gain insights into large healthcare datasets.
Yue et al. [45] created an artiﬁcial intelligence based tool that leverages interactive visualization techniques to leverage data in a predictive analytics processes. Their time series modeling technique includes the use of the Box-Jenkins procedure [27]. Other time series modeling techniques extensively used include the ARMA (Auto
Regressive Moving Average) [1] and ARIMA (Auto Regressive Integrated Moving Average) models. A summary of some other methods that involve geospatial modeling can be found in [11, 12]. Maciejewski et al. [20] utilize the seasonal trend decomposition by
loess smoothing for generating temporal predictions for modeling spatiotemporal healthcare events. They also use the kernel density estimation technique for creating probability distributions of patient locations for use in healthcare data. Our work builds on these ideas where
we utilize historic datasets to provide spatiotemporal forecasts into the
future. The focus of our work is to explore the issues of geospatial and
temporal scales so that casual experts can adapt their decision making process to the statistical analysis space. As such, we apply a user
assisted data analysis approach to drive future decisions that helps prevent decision makers from getting over-burdened, while, at the same
time, maximizes the utilization of their domain knowledge and perceptual capabilities.
2.2 Crime Hotspot Policing and Intervention
In recent years, there has been much research done that suggest the
beneﬁts of hot spot policing in preventing crime and disorder at these
crime hotspots (e.g., [2, 3, 4]). Weisburd et al. [41] examine the effect and impact of crime hot spots policing and their ﬁndings suggest
little negative effects and backlash among the residents of targeted areas of such policing efforts. Sherman [30] also explores the effects
of police crackdowns (sudden increase in police presence in speciﬁc
regions) among several case studies. He notes that while most of the
crackdowns appeared to demonstrate initial deterrent effects, the effects decayed after short periods of time. Our work also enables law
enforcement decision makers to identify and target crime hotspots by
forecasting high probability crime regions based on historic spatiotemporal trends. Our work also factors in the temporal variations within
the signals and, as such, provides dynamic hotspot locations for each
predicted day.
Goldkamp and Vîlcicã [15] provide insights into unanticipated negative effects of place-oriented enforcement intervention schemes on
other societal aspects. They explored an intensive targeted enforcement strategy that was focused on drug crime and its related community effects and examined the overall side effects on the society.
Sherman et al. [31] examine and provide an overview of the different
aspects of predatory criminal activity at different spatial granularities
and how these factors correlate with different aspects of the society.
Bruin et al. [7] provide a toolkit that extracts the different factors
from police datasets and creates digital proﬁles for all offenders. The

VOL. 20,

NO. 12,

DECEMBER 2014

tool then clusters the individuals against the created proﬁles by using
a distance matrix that is built around different attributes (e.g., crime
frequency, criminal history of the offenders).
2.3 Predictive Policing
There has been much work done in criminology to study criminal behaviors in order to develop models that predict various offense incidence levels at different spatial aggregation levels. Brown and Oxford [6] study methods that pertain to predicting the number of breaking and enterings in sub-cities and correlate breaking and enterings
with different factors including unemployment rates, alcohol sales and
previous incidents of crime. Yu et al. [44] also develop a crime
forecasting model by employing different data mining classiﬁcation
techniques. They employ several classiﬁcation techniques including
Nearest Neighbor, Decision Tree and Support Vector Machines. Their
experiments are run on two different data grid sizes, the 24-by-20 (approx. one-half mile square) and the 41-by-40 square grid cells (approx. one-quarter mile square). They note that the 24-by-20 grids
consistently gave them better results than the 41-by-40 grids, which
they attribute to the lack of sufﬁcient information at the coarser resolution. Our technique also allows analysts to conduct their predictive
forecasting at different spatial resolutions (e.g., over uniform spatial
grids and natural underlying spatial boundaries) and temporal granularity levels (e.g., by day, week, month). Furthermore, our system also
allows users to create spatial and temporal templates for use in the
prediction process.
Monthly and seasonal cycles and periodic properties of crime are
well known among criminologists [17]. Felson and Poulson [14] factor in the time of the day variation in the analysis of crime and provide
summary indicators that summarize the hour-of-day variations. They
provide guidelines for breaking the day into quartiles based on the
median hour of crime. We use their guidelines in our work and provide default data driven time-of-day templates over which to forecast
crime. We also utilize these techniques and incorporate the seasonality
and periodicity properties of crime in order to provide spatiotemporal
forecasts of future crime incidence levels.
3

T IME S ERIES P REDICTION USING S EASONAL -T REND D E COMPOSITION BASED ON L OESS (STL)
In order to model time series data, we employ the seasonal-trend decomposition technique based on a locally weighted regression (loess)
methodology (STL), where a time series signal is considered to consist of the sum of multiple components of variation. To accomplish
this, we ﬁrst utilize the STL method [9, 16] to desynthesize the time
series signal into its various components. An analysis of the underlying time series signal Y for CTC data reveals that a square root power
transform stabilizes the variability and yields a more Normal distribution of time series residuals, which is a requirement to appropriately model
the time series using STL. We consider the time series
√
signal
√ Y to consist of the sum of its individual components given
by Yv = Tv + Sv + Dv + Rv , where, for the v-th time step, Tv is the
inter-annual component, Sv is the yearly-seasonal component, Dv is
the day-of-the-week effect, and Rv is the remainder variation component.
To predict using the STL method, we apply the methodology described in [20], where the ﬁtted values Ŷ = (ŷ1 , ..., ŷn ) generated using
the loess operator in the STL decomposition step are considered to be
a linear transformation of the input time series Y = (y1 , ..., yn ). This is
given by ŷi = ∑ni=1 hi j y j ⇒ Ŷ = HY , where H is the operator matrix
whose (i, j)-th diagonal elements are given by hi, j . In order to predict
ahead by n days, we append the operator matrix H obtained from predicting ahead within each linear ﬁlter in the STL process with n new
rows, and use this to obtain the predicted value. The predicted value
for day n + 1 is thereby given by ŷn+1 = ∑ni=1 Hn+1,i Yi .
We use this concept of time series modeling and prediction and extend it into the spatiotemporal domain (see Section 5 for details). We
further factor in for the sparcity of data in certain geographical regions,
and devise strategies to alleviate problems resulting in prediction in
these sparse regions (Section 4).

1865

MALIK ET AL.: PROACTIVE SPATIOTEMPORAL RESOURCE ALLOCATION AND PREDICTIVE VISUAL ANALYTICS

Fig. 1. Our geospatial natural scale template signal generation process. For each geospatial sub-division, the system generates a time series of the
number of incidents, converts it into a binary signal, and processes the binary signal to generate the signal used to form the geospatial template.

4

N ATURAL S CALE T EMPLATES

In order to assist with the analysis process, we provide decision makers with natural scale templates that enable them to focus on appropriate geospatial and temporal resolution levels. These templates enable
users to analyze their data at appropriate spatiotemporal granularity
levels that help align the scale and frame of reference of the data analysis process with that of the decision making process. These templates also assist users in alleviating the impedance mismatch between
data size/complexity and the decision makers’ ability to understand
and interact with data [29]. We support the creation of both geospatial and temporal templates in our system that facilitate the decision
making process. A combination of the generated geospatial and temporal templates provide analysts with an appropriate starting point in
the analysis process; thereby, eliminating the need to examine and analyze the entire spatiotemporal parameter space and reducing it to more
manageable, appropriate scale levels. To be effective, the design of
these scale templates must follow the appropriateness, naturalness, and
matching cognitive principles [26]. As Wilkinson and Stevenson both
point out [36, 40, 42], simple scaling theory techniques are not sufﬁcient (e.g., axometric scaling theory), but provide useful guidance to
primitive scales of reference. The combinations of these design principles and the guidance from these statistical scale papers, provide the
motivation and basis for our natural scale templates described below.
4.1

Geospatial Templates

An underlying assumption with using STL to decompose time series
is that the data are Normally distributed. The model predictions can
get severely biased if this assumption is violated or if data are sparse.
To remedy this, we provide methods that help guide users in creating
geospatial scales that allow them to drill down to higher incidence
regions that may provide better prediction estimates.
4.1.1

Geospatial Natural Scale Templates based on Spatiotemporal Incident Distribution

Our system allows users to narrow down the geographic space for the
scope of analysis to regions with higher incidence counts and higher
statistical signiﬁcance for user-selected incident types. Our geospatial natural scale template methodology is shown in Figure 1. In order
to generate geospatial templates, the system ﬁrst fragments the geographic space into either uniform rectangular grids [6] or man-made
spatial demarcations (e.g., census blocks). Then, for each subregion,
the system generates a time series of the number of incidents that occurred within the subregion over time (e.g., by day, week, month).
This signal is further cached for use later in the forecasting process.
Next, we convert this time series signal into a binary signal across
time, where a 1 represents that an incident occurred on a particular
day and a 0 that no incident occurred. We then count the number
of 0’s between the 1’s and progressively sum the number of 0’s, outputting the result as another time series signal. As such, this signal is
a representation of the number of time steps over which no incidents
occurred for the given subregion.
This new time series signal is now utilized in the STL forecasting
method (Section 3) and a predicted value is computed for the next day.
It should be noted that the resulting time series for regions of lower
incidence counts will not be sparse, and consequently, will generate
higher predicted values. This process is repeated for all geospatial
subregions and a uniﬁed picture is obtained for the next day. Finally,

we ﬁlter out the regions with higher predicted values (low activity)
by thresholding for the maximum value. The resulting ﬁltered region
forms the initial geospatial template. An example of a created geospatial template using this technique is shown in Figure 4 (Left).
4.1.2

User Reﬁnement of Geospatial Template using Domain
Knowledge

The geospatial template provides regions with relatively higher incident rates. The system further allows users to use their domain knowledge and interactively reﬁne these template regions into sub-divisions.
For example, users may choose to sub-divide the formed template regions by natural or man-made boundaries (e.g., state roads, rivers,
police beats), or by underlying features (e.g., known drug hotspots).
The system also allows users to explore the predicted future counts
of the created sub-regions by generating an incidence count vs. time
signal for each disjoint region and applying our forecasting methodology (Section 3) to ﬁnd a predicted value for the next day. The results
are then shown as a choropleth map to users (e.g., Figure 4 (Right)).
These macro-level prediction estimates further assist decision makers
in formulating high-level resource allocation strategies.
4.2

Kernel Density Estimation

One of the challenges with using the spatial distribution of incidents
in a geospatial predictive analytics process is that it can exacerbate the
problem of generating signals with low or no data values. To further
reﬁne our prediction model in geospace, we utilize a Kernel Density
Estimation (KDE) technique to spread the probability of the occurrence of incidents to its neighboring regions. The rationale behind
this is that criminology research has shown evidence that occurrence
of certain types of crimes (e.g., residential burglary) at a particular
region puts neighboring regions at an elevated risk [13, 18, 32].
Furthermore, crime also tends to be clustered in certain neighborhoods, and the probability of a crime occurring at a particular location
can be highly correlated with the number of recent crimes at nearby
locations. We incorporate this concept in a novel kernel density estimation method described in Section 4.2.2, where the kernel value at
a given location depends on the locations of its k-nearest incidents.
In addition, kernel density estimation methods take into account that
crimes in low-crime or sparsely populated areas have low incidence,
but non-zero probability. We utilize two interchangeable density estimation techniques in our implementation.
4.2.1

Kernel Scale based on Distance to the k-th Nearest
Neighbor

To account for regions with variable data counts, we utilize a kernel density estimation technique and use a dynamic kernel bandwidth [33]. We scale the parameter of estimation by the distance from
the point x to its kth nearest neighbor Xi . This is shown in Equation 1.
1
fˆ(x) =
N

N

1

∑ max(h, di,k ) K

i=1



x − Xi
max(h, di,k )


(1)

Here, N is the total number of samples, di,k the distance from the
i-th sample to the k-th nearest neighbor and h is the minimum allowed
kernel width. We use the Epanechnikov kernel [33] to reduce calculation time, which is given by K(u) = 34 (1 − u2 )1(||u||≤1) . Here, the

1866

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 20,

NO. 12,

DECEMBER 2014

Fig. 2. Spatiotemporal distribution of historical CTC incidents for Tippecanoe County for (Left) 3/11/2012 through 3/10/2014, and (Right) for all
Tuesdays in March in the past 10 years.

function 1(||u||≤1) evaluates to 1 if the inequality is true and to 0 otherwise. In cases where the distance from the i-th sample to the k-th
nearest neighbor is 0 (e.g., multiple calls from the same address), we
force the variable kernel estimation to a minimum ﬁxed bandwidth h.
Making the kernel width placed at the point Xi proportional to di,k
gives regions with sparse data a ﬂatter kernel, and vice-versa.
4.2.2

Dynamic Covariance Kernel Density Estimation Technique (DCKDE)

The kernel in the previous method is based on the distance from an incident location to its k-th nearest neighbor, which provides a ﬂatter kernel for sparse regions. In a new kernel method, we use the information
from all k-nearest neighbors to calculate the width of the kernel (rather
than the most distant neighbor), thus reducing stochastic variation on
the width of the kernel. As such, we fragment the geospatial region
into rectangular grids and then utilize a Gaussian kernel at every grid
node that is based on the covariance matrix of the location of the center of each node X = {x, y} and its k-nearest neighbor incidents [39].
Therefore, the kernel value is inﬂuenced by the k-nearest neighbors
and provides a wider kernel in sparsely populated regions that enables
the model prediction to be small but non-zero and also takes into account correlations between latitude and longitude; thus, improving the
accuracy of the estimates. The value stored at each node location is
μ )T V −1 (X−μ
μ)
1
− 12 (X−μ
given by δ (X) = 2π|V
, where μ = {μx , μy } is the
|e
mean along the x and y directions of the k nearest neighbors and their
covariance matrix V is deﬁned as:
 2

σx
covx,y
V=
(2)
2
covx,y
σy
Here,

σx2

and

σy2

is the variance along the x and y dimension respec-

tively, and covx,y = ∑ki=1
tween x and y.
4.3

(xi −μx )(yi −μy )
k−1

is the sample covariance be-

Neighbors with Similar Spatio-Demographics

For regions that generate a signal of lower statistical signiﬁcance for
the user selected categories, we provide the option to explore data
in similar neighborhoods. For each census block, we utilize spatiodemographic census data to ﬁnd those census blocks that exhibit similar spatial demographics. The rationale behind ﬁnding similar neighborhoods lies in the fact that regions with similar demographics tend
to exhibit similar trends for certain types of crime [24, 34].
The process of ﬁnding similar census blocks for a given census
block X includes computing the similarity distance from X to all neighboring census blocks that lie within a d mile radius from the centroid
of X. The d mile radius constraint is imposed to factor in for Tobler’s
ﬁrst law of geography [38] that suggests that near regions are more
related to one another than distant regions. We use d = 3.0 miles
in our implementation [8]. As such, the similarity distance between
two census blocks A and B given k census data variables is given by


SA,B = ∑ki=1 (A(Vi ) − B(Vi ))2 , where A(Vi ) and B(Vi ) are the corresponding census data variable values (e.g., race, income, and age
demographic data) for census blocks A and B respectively. Finally, the
top N census blocks with the smallest similarity distance values are
chosen as the similar census blocks for the given census block X. We
use N = 5 as a default value in our implementation, but provide users
with options to change this value on demand. We note that our future
work includes extending this concept of ﬁnding similar neighborhoods
to determining similar data categories for predictive purposes.
The system now provides users with the ability to generate similar
neighborhood prediction maps where the prediction for a given census
block X depends on the historic time series data of its N similar census
blocks in addition to the past data of the census block X itself. Here,
the input time series for the census block X used in the prediction
algorithm is the per time step average of the N similar census block
signals combined with the original signal from census block X. The
resulting prediction maps incorporates the inﬂuence of incidence rates
in neighborhoods that share similar spatio-demographic data.
4.4

Temporal Natural Scale Templates

As noted previously in Section 2.3, crime trends exhibit not only
monthly and seasonal trends, but also shows day-of-the-week and
hour-of-day variations. The prediction maps produced by the methods described so far provide prediction estimates over 24-hour periods.
This information, albeit valuable to the law enforcement community in
developing resource allocation strategies for their precincts, provides
little detail of the 24-hour distribution of crime. In this section, we
describe our approach to assist users in creating temporal scales.
4.4.1

Interactive Clock Display

Figure 2 (Top-Right) shows our interactive clock view that enables a
radial display of temporal hourly data. The clock view provides a way
for users to ﬁlter the data by the hour by interactively clicking on the
desired hours, thereby ﬁltering down the data for use in the prediction process. Users may use the clock view display to obtain a visual
summary of the hourly distribution of the incidents and consequently
make informed decisions on creating temporal templates over which
good prediction estimates may be established.
4.4.2

Factoring in for Monthly and Day-of-the-Week Variations

In addition to utilizing the seasonal trend decomposition technique described in Section 3 to decompose the time series signals into its various components, we also utilize a direct approach where we allow
users to create their own custom monthly and/or daily templates. Certain crimes tend to peak on certain days of the week (e.g., alcohol
related violations tend to be higher over the weekend), whereas other
crimes tend to be lower on other days (e.g., reported burglaries drop
over the weekend). As such, we factor for these effects directly in the
system and allow users to ﬁlter data speciﬁcally by month and/or by
day-of-the-week. This further assists decision makers in developing
and reﬁning their resource allocation strategies.

MALIK ET AL.: PROACTIVE SPATIOTEMPORAL RESOURCE ALLOCATION AND PREDICTIVE VISUAL ANALYTICS

1867

Fig. 3. Geospatial prediction results for 3/11/2014 for Tippecanoe County obtained using our STL forecasting methodology. (a) Predicted choropleth
map for rectangular grids of dimension 64 × 64 using incidence count time series by day. (b) Reﬁned predicted map after removing TCPD location
from (a). (c) Predicted map using KDE based on the distance to the k -th nearest neighbor approach (Section 4.2.1). (d) Forecast map using
DCKDE method (Section 4.2.2).

4.4.3 Reﬁnement using Summary Indicators
We extend the method described in [14] to further assist users with
reﬁning and choosing appropriate hourly templates in the prediction
process. In this method, the system computes the median minute of
CTC incident for the selected 24-hour binning period that provides
information about when exactly half of the incidents for the selected
date range and offense types have occurred. Next, to get an indication of the dispersion of crime within the 24-hour period, the system
computes the ﬁrst quartile minute and third quartile minute for the selected data, which are the median times of the ﬁrst and second halves
of the 24-hour period from the median minute respectively. Finally, as
temporal data can be inaccurate with many incidents that have missing
time stamps, we provide users with an accuracy indicator to show the
percentage of cases with valid time stamps. These summary indicators, along with the temporal templates described above, enable users
to further reﬁne their selected temporal templates for use in the prediction process. Example scenarios where these indicators are used are
provided in Section 6.
5 G EOSPATIAL P REDICTION
The described visual analytics process involves a domain expert selecting appropriate data parameters, applying desired data ﬁlters and
generating spatial and temporal natural scale templates using the methods described in Section 4. Next, the system incorporates the STL
forecasting method (Section 3) and extends it to the geospatial domain
to provide prediction estimates for the next N time steps (e.g., days,
weeks, months). We now list the steps involved in our geospatial prediction methodology:
1. Dividing geospace into sub-regions: The ﬁrst step in our
methodology, just like in Section 4.1.1, involves subdividing
geospace into either uniform rectangular grids of user speciﬁed
resolutions or man-made geospatial boundaries.
2. Generating the time series signal: The system then extracts
a time series signal for each sub-division. We allow two types
of signals to be extracted for each sub-division: (a) incidence
count vs. time step, and (b) kernel value vs. time step. Note
that the signal generated in (a) is the same as that produced in
Section 4.1.1 (Figure 1 (Distribution of Incidents)). The kernel
values used in (b) are generated using any one of the methods
described in Section 4.2.
3. Forecasting: The time series signal generated for each spatial
unit is then fed through the STL process described in Section 3
where a forecast is generated for the next N time steps (e.g., days,
weeks). This process is repeated for all region sub-divisions and
prediction maps are ﬁnally obtained for the next N time steps.
4. Visualizing results: Finally, the results of our forecasting
method are provided to the user either in the form of a choropleth map or a heatmap.
When users choose to fragment the geospace into uniform rectangular grids, we provide them with the ability to select the resolution
level, or, in other words, the grid size of each grid. An incidence count

vs. time step signal is then generated for each sub-region. It is important to note here that a grid resolution that is too ﬁne may result in a
zero count vs. time step signal that has no predictive statistical value.
On the other hand, a grid resolution that is too coarse may introduce
variance and noise in the input signal, thereby over-generalizing the
data. An evaluation of our forecasting approach (Section 7) indicates
that an average input size of 10 samples per time step provide enough
samples for which our method behaves within the constraints and assumptions of our STL forecasting approach. We utilize this metric in
our system in order to determine the applicability of our forecasting
method for a particular sub-region.
Figure 3 shows a series of examples that demonstrate our geospatial
prediction results using the methods described in this section. Here,
the user has selected all CTC incidents for Tippecanoe County, IN,
and is using 10 years’ worth of historical data (3/11/2004 through
3/10/2014) to generate forecast maps for the next day (i.e., for
3/11/2014). Figure 3 (a) shows the prediction results when Tippecanoe
County, IN is fragmented into rectangular grids of dimension 64 × 64.
The input data for each sub-region consists of daily incidence count
data over the last 10 years. This method, unlike the KDE methods,
does not spread the probability to surrounding neighborhood regions
when an incident occurs at a particular place. As a result, this method
treats each region independently, and can be used when there are
no correlations between geospatial regions (e.g., commercial vs. residential neighborhoods). This method can also be useful in detecting
anomalous regions and regions of high predicted levels of activity. For
example, the user notices something peculiar from the results in Figure 3 (a): a predicted hotspot occurs prominently over the Sheriff’s
ofﬁce and county jail location (labeled as TCPD in Figure 3 (a)). This
occurs because the default geospatial location of many incidents are
logged in as the county jail, especially when arrests are associated
with cases. To remedy for this, the user can reﬁne the underlying
geospatial template (Section 4.1.2) and dynamically remove this location from the geospatial template. The reﬁned prediction map generated is shown in Figure 3 (b).
Figures 3 (c and d) show the predicted results of using the kernel
density estimation based on the distance to the k-nearest neighbor approach (Section 4.2.1) and the DCKDE technique (Section 4.2.2), respectively. The KDE method applied to generate the prediction map in
Figure 3 (c) provides a ﬂatter kernel for relatively low-crime regions.
As a result, the prediction map provides lower, but non-zero, predictions for these regions. The kernel width computed using this method
is based on the distance from a point x to its kth nearest neighbor only.
The DCKDE method, on the other hand, assumes that the probability
of the occurrence of an incident at a particular location is correlated
with the number of recent incidents at nearby locations. Accordingly,
this method utilizes information from all k-nearest neighbors in calculating the kernel value. Thus, the regions with persistently higher
incident concentrations generate focused hotspots when forecasting is
performed using the DCKDE method. Finally, it should be noted that
each method provides users with different insights into the dynamics
of the underlying processes, and users can use their domain knowledge
to further reﬁne the results to make informed decisions.

1868

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 20,

NO. 12,

DECEMBER 2014

Fig. 4. (Left) Geospatial template generated for Tippecanoe County using 10 years’ worth of historical data. (Right) Choropleth map showing the
distribution of predicted incidents for 3/11/2014 by police beats for Tippecanoe County. Users may further select regions on the map (e.g., Reg. 1-4)
to generate detailed predictions for the selected regions (Figure 5).

6

C ASE S TUDY: F ORECASTING F UTURE
AND C IVIL (CTC) I NCIDENCE L EVELS

C RIMINAL , T RAFFIC

In this section, we demonstrate our work by applying our spatiotemporal natural scale template methodology to forecast for CTC incidence
levels in Tippecanoe County, IN, U.S.A. This dataset consists of historical reports and provides several different attributes, including the
geographic location, offense type, agency, date, and time of the incident. This dataset contains an average of 31,000 incidents per year for
Tippecanoe County, and includes incidence reports for different categories of CTC incidents (e.g., crimes against person, crimes against
property, trafﬁc accidents). We use 10 years worth of historical data
for this analysis. We provide a workﬂow when using our system in the
analysis process.
Forecasting for all geospatial CTC incidents
Here, we describe a hypothetical scenario in which a law enforcement shift supervisor is using our system to develop resource allocation strategies for Tippecanoe County over the next 24 hour period
for Tuesday, March 11, 2014. The supervisor is interested in developing a high-level resource allocation strategy, in particular, by police
beats for the next 24 hour period. Law enforcement ofﬁcers are generally assigned to a particular law beat and patrol their beat during their
shift hours when not responding to a call for service. The supervisor is also interested in determining which hotspot locations to focus
on for larger police beats. Finally, he also wants to reﬁne the developed resource allocation strategy to factor in for the hourly variation
of crime. To develop an appropriate resource allocation strategy, the
shift supervisor performs several different analyses that are described
in the following subsections. Although our example uses data for all
CTC categories as inputs, users may ﬁlter their data using any combinations of CTC categories (e.g., crimes against property, person) to
further reﬁne their resource allocation strategy.
Overall daily resource allocation
The shift supervisor begins his process by visually exploring the spatiotemporal distribution of historical incidents using our system. When
working through the system, the supervisor then visualizes the geospatial and hourly distribution of the incidents that occurred over the
past 2 years, as shown in Figure 2 (Left). The supervisor notes several hotspots emerge for the selected period. The locations of these
hotspots match with his domain knowledge of the area (e.g., city
downtown regions, shopping center locations across town). The static

image of the aggregate data, however, does not factor in the inherent
spatiotemporal data variations, and basing a resource allocation decision on this image alone would be insufﬁcient. The supervisor is also
aware of the fact that police presence can act as a deterrent for certain
types of crimes, and, therefore, wants to diversify and maximize police
presence in these hotspot areas.
Next, the supervisor wants to factor for monthly and day-of-theweek patterns in his analysis. As such, he visualizes the geospatial and
hourly distribution of all CTC incidents that occurred on any Tuesday
in the month of March over the past 10 years (Section 4.4.2). The
result is shown in Figure 2 (Right). The supervisor notes a slightly
different geospatial distribution emerges as a result, with the intensity
of hotspots shifting towards the east downtown Lafayette region. In
this case, it also becomes apparent that for the 24-hour distribution,
10 AM, 1 PM and 3 PM-6 PM emerge as high activity hours.
Allocating resources by police beats
In order to narrow down the geospace and focus on relevant geographic locations, the supervisor decides to apply our geospatial template generation technique (Section 4.1) with all CTC incidents selected using 10 years’ worth of historical data (i.e., from 3/11/2004
through 3/10/2014). The resulting geospace generated is shown in
white in Figure 4 (Left). The supervisor notes that the resulting regions
correspond to highly populated areas, and exclude areas of infrequent
occurrences. Next, the system provides a total predicted number of incidents, N, for March 11, 2014 for the ﬁltered geospatial region. This
is done by generating a total incidence count vs. day time series signal
using the past 10 years’ worth of data and applying the STL forecasting method described in Section 3. Here, N is 59 incidents.
Next, the supervisor is interested in obtaining a high level overview
of the distribution of the predicted incidents over geospace, and, in particular, by police patrol routes. As such, the supervisor uses our system and fragments the generated geospatial template using the city law
beats shapeﬁle. The resulting geospace is shown in Figure 4 (Right).
In order to distribute the total predicted 59 incidents across police
beats, the system computes an incidence count vs. day time series
signal for each disjoint geospatial region and computes the predicted
number of incidents ni for each region (Section 3). Next, the probability of an incident within each disjoint region is calculated using the
formula pi = ni /N ∗ 100. The results of this operation are then shown
to the user as a choropleth map, where each disjoint region is colored
according to its value on a sequential color scale [5] (Figure 4 (Right)).

MALIK ET AL.: PROACTIVE SPATIOTEMPORAL RESOURCE ALLOCATION AND PREDICTIVE VISUAL ANALYTICS

1869

Fig. 5. User reﬁnement of geospatial resource allocation strategy. The user has chosen to visualize predicted hotspots for regions labeled in
Figure 4 (Regions 1 through 4), and for Tippecanoe County over hourly temporal templates.

Geospatial resource allocation strategy reﬁnement using domain knowledge
While the high level police beat prediction map (Figure 4 (Right)) suggests putting a heavier emphasis on the eastern police beats of the city,
the prediction results in Figure 3 indicate a more localized concentration of incidents at the city downtown locations. The shift supervisor
may use these results and allocate higher resources to the eastern police beat of the city (Reg. 4 in Figure 4), and allocate a smaller number of resources, but at more concentrated locations in the downtown
(Reg. 1 in Figure 4).
Now, the supervisor is interested in further reﬁning her geospatial
resource allocation strategy. First, she turns to the predicted hotspot
regions in the city downtown regions (Reg. 1 in Figure 4). She decides to utilize the census blocks spatial boundary information and
divides the geospace into census blocks. Next, she uses the method
described in Section 5 to create a predicted choropleth map based on
census blocks for the region. The result of this operation is shown in
Figure 5 (Reg. 1). Here, the supervisor has chosen to use the kernel
values obtained from the method described in Section 4.2.1 and spread
them across the underlying census blocks for generating these results.
To obtain detailed predictions for the eastern city police beat region
(Reg. 4 in Figure 4), the shift supervisor uses a different approach
where she draws a region around the selected beat using the mouse
and restricts the forecast to the selected region. The result of this operation is shown in Figure 5 (Reg. 4). From domain knowledge, she
knows that this area has a high concentration of shopping centers. The
hotspots obtained in Figure 5 (Reg. 4) align with these locations. Finally, the supervisor generates similar heatmaps for regions labeled
as Reg. 2 and 3 in Figure 4, the results of which are shown in Figures 5 (Reg. 2 and 3), respectively. Note that the county jail location
is once again a hotspot in Figure 5 (Reg. 3). With these detailed results in hand, the shift supervisor is able to devise an optimal resource
allocation strategy for the next 24 hour period in Tippecanoe County.
Applying temporal templates
Finally, in order to reﬁne her resource allocation strategy to different
portions of the day, the shift supervisor chooses to apply the summary
indicators method (Section 4.4.3). She ﬁnds that the ﬁrst, median,
and third quartile minutes for CTC incidents that occurred in the past
10 years were 9:25 AM, 3:11 PM and 7:28 PM respectively. She also
notes that these indicators correspond with the hourly distribution of
incidents using the clock view display in Figure 2. Therefore, the supervisor chooses two hourly templates using these summary indicators: (a) 9 AM through 3 PM, and (b) 3 PM through 7 PM. The supervisor also creates two other hourly templates: 9 PM through 3 AM to
capture night time activity, and 9 AM through 5 PM to capture working
hours of the day. She then uses the kernel density estimation method
(Section 4.2.1) and re-generates prediction maps for March 11, 2014.

These results are shown in Figure 5. As expected, the supervisor notes
the shift in hotspot locations through the 24 hour period, which further enables the reﬁnement of the resource allocation strategy for the
different portions of the 24 hour period.
7

M ODEL

EVALUATION AND VALIDATION

In order to evaluate our methodology, we conducted a series of statistical tests to understand the behavior and applicability of our approach
in the spatiotemporal domain. Our validation strategy involved testing for the empirical rule of statistics, which describes a characteristic
property of a Normal distribution: 95% of the data points are within
the range ±1.96 σ of μ, where μ and σ are the mean and standard deviation of the distribution, respectively [10]. In order to help alleviate
the challenges resulting due to the sparseness of the underlying data,
we performed our analyses over a weekly data aggregation level. Our
approach involved testing whether the 95% prediction conﬁdence interval bound acquired for the geospatial predictions using our forecasting approach holds when compared against observed data [19]. This
conﬁdence bound would be violated if the variance of the observed
data is higher (i.e., overdispersed data) or lower (i.e., underdispersed
data) than that dictated by the prediction conﬁdence bound. When the
95% prediction bounds are met as expected, and the data conforms to
the Normal regime, the applicability of our spatiotemporal STL forecasting method is established.
Building on our STL based time series prediction discussion from
Section 3, the variance of the ﬁtted values Ŷ = (ŷ1 , ..., ŷn ) using the
loess operator in the STL decomposition step is given by Var(Ŷi ) =
σ̂ 2 ∑nj=1 Hi2j [20]. Here, σ̂ 2 is the variance of the input time series
signal Y , and is estimated from the remainder term Rv . Subsequently,
the variance for the predicted value Ŷn+1 for time step n + 1 is given by
2
Var(Ŷn+1 ) = σ̂ 2 (1 + ∑nj=1 Hn+1,
j ). This provides the 95% prediction

interval as CIn+1 = Ŷn+1 ± 1.96 Var(Ŷn+1 ).
Next, we performed a series of analyses at varied geospatial and
temporal scales, and for different data categories. The geospace was
ﬁrst fragmented into sub-regions (either rectangular grids or using
man-made boundaries), and time series signals were generated for
each geospatial sub-region. In our analyses, we utilized a sliding
time window of size 3 years (i.e., 3 × 52 weeks) that provided enough
samples above the Nyquist frequency for the STL forecasting technique. Forecasting was performed using the methods described in Sections 5 and 7.1. We provide our evaluation methodology and results in
the subsequent sub-sections.
7.1

Modiﬁed STL forecasting method to factor in for
weekly data aggregation
√
As described earlier in Section 3, a time series signal Y can be
considered to consist of the sum of its inter-annual (Tv ), yearly-

1870

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 20,

NO. 12,

DECEMBER 2014

Fig. 6. 95% prediction interval accuracy vs. Avg(Y ) for different CTC offenses for Tippecanoe County, IN. Here, geospace has been fragmented
into rectangular grids of dimension k × k (∀ k ∈ [1, 128]), and by law beats and census blocks.

seasonal (Sv ), day-of-the-week (Dv ), and remainder variation (Rv )
components. However, since we used a weekly aggregation of data,
the day-of-the-week component (D√
v ) must be excluded. Therefore, the
time series signal gets modiﬁed to Yv = Tv + Sv + Rv . The prediction
step, which involves predicting the value for week n + 1, remains the
same as given in Section 3.
7.2

95% prediction interval accuracy vs. input data average (Avg(Y ))

In this method, the geospace was ﬁrst fragmented into either: (a) rectangular grid regions of dimension k × k (∀ k ∈ [1, 128], with 128 chosen as upper threshold to provide a ﬁne enough geospatial resolution),
or (b) man-made geospatial regions (e.g., census blocks, census tracts,
law beats, user-speciﬁed regions). For each geospatial region, we ﬁrst
generated the incidence count vs. week signal (denote this signal as
Y ) for a time window of n weeks beginning from the week of, e.g.,
1/1/2009. We then used the modiﬁed STL forecasting method (Section 7.1) to calculate the 95% prediction interval CI for the predicted
week n + 1, and tested whether the observed data for week n + 1 fell
within the calculated 95% prediction interval for that geospatial region. The average of the input signal Y , Avg(Y ), was also calculated.
Next, the input time window was shifted by one week to generate the corresponding incidence count vs. week signal (so, this signal
would begin from the week of 1/7/2009). We again computed Avg(Y ),
and CI for the predicted week n + 1. As before, we tested whether the
observed data for the predicted week n + 1 fell within the calculated
95% prediction interval. We repeated the process by sliding the time
window till it reached the end of available data. For each Avg(Y ) value,
we maintained two counters that kept track of the number of instances
the observed data was within the 95% prediction interval (CCorrect ),
and the total instances encountered thus far (CTotal ). Finally, Avg(Y )
values were binned, and CCorrect and CTotal were summed for each
bin. The 95% prediction interval accuracy for each Avg(Y ) bin is then
given as ∑∑bin CCCorrect
× 100%.
Total
bin

7.3

Results and discussion

Figure 6 shows the 95% prediction interval accuracy results for different CTC offenses for Tippecanoe County, IN using the method described in Section 7.2. As can be observed from these results, when
the average bin values are low (e.g., less than 10 input samples), the
accuracy levels are higher than the expected 95% conﬁdence bound.
This indicates that the data are underdispersed for lower input values.
In other words, the variance of the observed data is lower than that of
the 95% prediction bound when the underlying data are sparse. This
conforms to the expected behavior for predicting using our STL forecasting technique: the model predictions get biased if the underlying
data are too sparse.
As the input signal average (Avg(Y )) values get larger (i.e., more
than 10 samples per time step), the prediction accuracy starts to converge at around the expected 95% accuracy level. For example, the
prediction interval accuracy for all offenses converges at around 93%.
Also, note that the prediction accuracy using the DCKDE method

(Section 4.2.2) converges close to the 95% accuracy level; thereby, indicating the efﬁcacy of the technique. It should be noted that since the
underlying processes being modeled here (e.g., CTC incidents) are inherently stochastic in nature, perfect 95% conﬁdence bounds will not
be achieved (as can be seen from the results in Figure 6). Furthermore,
with an uncertain probability distribution of the underlying data, our
application of the square root power transform may not guarantee homoscedasticity (i.e., stabilization of variability). This also contributes
to our system not achieving perfect 95% conﬁdence bounds. However, even though perfect conﬁdence bounds are not achieved (as can
be observed from Figure 6), the accuracy converges close to the 95%
bounds. These results show that the underlying data are Normally distributed for higher values of Avg(Y ); thereby, satisfying the underlying
assumptions of our method used to estimate the 95% conﬁdence interval. This establishes the validity of the claims of our STL prediction
methodology in the geospatial domain that the prediction modeling
method works as expected as long as the underlying assumptions of
the method are satisﬁed by the data.
Figure 6 shows the 95% prediction interval accuracy vs. input data
average results (Section 7.2) for man-made geospatial regions (census blocks and law beats). These results show that the conﬁdence
bounds using census blocks are invariably higher than the expected
95% bound, which indicates that the underlying data are underdispersed. Census blocks are small geospatial units, typically bounded
by streets or roads (e.g., city block in a city). The smaller Avg(Y ) values for census blocks in Tippecanoe County in Figure 6 (less than 10
input samples) further highlight the sparsity of input data. The combination of higher prediction interval accuracy levels and lower Avg(Y )
values are telltale for the data sparseness issues we have described, and
suggest that the signals generated using census blocks have low predictive statistical power. This further underlines the need to intelligently
combine geospatial regions of lower statistical values to obtain a signal of higher predictive power (e.g., as was done in Section 4.3). The
95% prediction interval accuracy results obtained using law beats in
Figure 6, on the other hand, shows the accuracy converging at around
the expected 95% conﬁdence interval for higher Avg(Y ) values (more
than 10 input samples). These results provide further evidence that
as the underlying data values become larger and begin to conform to
the Normal regime, our geospatial prediction methodology provides
prediction estimates that are within the expected 95% prediction conﬁdence interval. This further bolsters the applicability and validity of
our STL prediction methodology in the geospatial domain.
We also applied the method described in Section 7.2 to all CTC incident category data and generated 95% prediction interval accuracy
vs. the input signal average value (Avg(Y )) plots for different grid resolutions k. These results are shown in Figure 7. The results indicate
that 95% prediction interval accuracy converges at or around the 95%
conﬁdence level for large enough Avg(Y ) values (i.e., for Avg(Y ) bigger than 10). The results indicate that our methodology behaves within
the constraints of the Normal regime at higher Avg(Y ) values for the
different grid dimensions. Also, note that smaller grid dimensions (k)
correspond to larger geospatial sub-divisions; and accordingly, smaller
k values generate signals of larger counts per bin (i.e., larger Avg(Y )

MALIK ET AL.: PROACTIVE SPATIOTEMPORAL RESOURCE ALLOCATION AND PREDICTIVE VISUAL ANALYTICS

1871

Fig. 7. 95% prediction interval accuracy vs. Avg(Y ) for all CTC offenses for Tippecanoe County, IN. Here, geospace has been fragmented into
rectangular grids of dimension k × k for various k values.

values), especially for regions with higher incidence rates. As can
be seen from the results in Figures 6 and 7, the accuracy for higher
Avg(Y ) values tend to be lower than the 95% prediction accuracy;
thereby, indicating that the underlying data are slightly overdispersed.
These results indicate that coarse scales can generate signals with too
much variance, or combinations of multiple signals that overgeneralize the data. Furthermore, the signals generated at coarse scales can
be affected by anomalies in underlying data (e.g., crime spikes during
unusually high weathers, holidays). These can contribute to the nonNormality of the residuals, and produce an overdispersion of underlying data as compared to the assumptions of our model. It should be
noted that although a slight data overdispersion is noticeable at coarse
scales, they are deemed to be small enough to currently not warrant
any correction. Finally, we note that further research is needed in order to determine the effects of these data overgeneralization issues at
coarse scales and to devise strategies to mitigate for their effects.
7.4

Summary

Our model evaluation and validation strategy involved testing for the
empirical rule of a Normal distribution where we tested whether the
observed data conformed with the 95% prediction interval from our
STL forecasting method at various geospatial scales. In order to cope
with data sparseness issues, we performed our analysis at a weekly aggregation of data. Our results demonstrate the validity of our approach
as long as the underlying assumptions of the underlying models are
satisﬁed by the data. The results obtained using our DCKDE method
are also promising. Our results also highlight the importance of performing analysis at appropriate scales, and demonstrate that the model
predictions get severely biased when the underlying assumptions are
violated by the data. We also explored the effects of data sparseness
issues on our model predictions at ﬁne geospatial scales. Our evaluation results show that the model predictions generated using input
signals of 10 or more counts per time step on average tend to conform
with the 95% prediction conﬁdence intervals. We also highlight the
effects of analysis performed at coarse scales, and show the data overgeneralization issues that occur at such scales. Although the results
indicate a slight data overdispersion at coarse scales, the results show
that the prediction accuracies from the model estimates still tend to
converge at around the 95% conﬁdence bounds. This further shows
the effectiveness of our forecasting methodology in the geospatial domain. We also note that although our work enables hot spot policing
and resource allocation strategy development, further evaluation is required to ascertain the efﬁcacy of our predictive analytics framework
when deployed in ﬁeld. We leave this as future work.
8

D OMAIN E XPERT F EEDBACK

Our system was assessed by a police captain who oversees the operations and resource allocation of several precincts in a mid-sized police
agency (of about 130 sworn ofﬁcers) in the United States. In this section, we summarize the initial feedback received after conducting several informal interviews with him. The captain emphasized the need
for a system that applies a data-driven approach to assist law enforcement decision makers in developing resource allocation strategies. He
was impressed by the ability of the system to interactively generate
various geospatial and temporal visualizations of historical datasets

and forecast maps in real-time. Additionally, he also appreciated having the ability to dynamically apply any desired geospatial, temporal,
and/or categorical ﬁlters on the data.
The captain stressed the need to carefully combine and aggregate
different data categories for which reliable forecast maps could be
generated. For example, he noted that a signal generated by combining two crime categories with different attributes (e.g., crimes against
property and person) might introduce variability in the forecasting process and produce unreliable results. He further suggested that crimes
of opportunity must be ﬁltered out as these exhibit no discernable patterns. He asserted that different regions within the same city can exhibit different crime patterns due to the different underlying region
dynamics. He expressed the importance for domain experts to create
data category and spatiotemporal templates so viable prediction estimates can be computed using our methodology. Finally, the captain
remarked that the predicted hotspot locations using aggregated CTC
data occur at the known problem areas in the city.
9 C ONCLUSIONS AND F UTURE W ORK
In this work, we have presented our visual analytics framework that
provides a proactive decision making environment to decision makers
and assists them in making informed future decisions using historical
datasets. Our approach provides users with a suite of natural scale templates that support analysis at multiple spatiotemporal granularity levels. Our methods are built at the conﬂuence of automated algorithms
and interactive visual design spaces that support user guided analytical
processes. We enable users to conduct their analyses over appropriate
spatiotemporal granularity levels where the scale and frame of reference of the data analysis process and forecasting matches with that of
the user’s decision making frame of reference. It should be noted that
while adjusting for the size of the geospatial and temporal scales is
necessary, it is also important to adjust for the scale of the size of the
dataset. A forecasting or analysis method that works well for one region with certain demographics and population densities may not have
the same efﬁcacy when applied to a different region. As such, our work
explores the potential of visual analytics in providing a bridge so that
different statistical and machine learning processes occur on the same
scale and frame of reference as that of the decision making process.
Our future work includes developing new kernel density estimation
techniques designed speciﬁcally for improving prediction forecasts.
We further plan on improving our designed dynamic covariance kernel
density estimation technique (DCKDE) to factor in for temporal distances to further enhance our STL based prediction algorithm. We also
plan to incorporate data-driven methods that guide users in selecting
between different choices provided by the system based on the underlying features of the data. We also plan on factoring in the inﬂuences
and correlations among different variables to further reﬁne our natural
scale template generation methodology. Finally, we plan on conducting a formal user evaluation in order to understand the efﬁcacy of our
system in aiding domain experts to understand the properties of underlying data and their effects on the workings of the different underlying
statistical processes.
ACKNOWLEDGMENTS
This work was funded by the U.S. Department of Homeland Security
VACCINE Center’s under Award Number 2009-ST-061-CI0003.

1872

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

R EFERENCES
[1] G. Box and G. Jenkins. Time series analysis: Forecasting and control.
Holden-Day, San Francisco, 1970.
[2] A. A. Braga. The effects of hot spots policing on crime. Annals of
the American Academy of Political and Social Science, 578:pp. 104–125,
2001.
[3] A. A. Braga and B. J. Bond. Policing crime and disorder hot spots: A
randomized controlled trial*. Criminology, 46(3):577–607, 2008.
[4] A. A. Braga, D. M. Hureau, and A. V. Papachristos. An ex post facto evaluation framework for place-based police interventions. Police Quarterly,
2012.
[5] C. A. Brewer. Designing Better Maps: A Guide for GIS users. ESRI
Press, 2005.
[6] D. Brown and R. Oxford. Data mining time series with applications to
crime analysis. In IEEE International Conference on Systems, Man, and
Cybernetics, volume 3, pages 1453–1458 vol.3, 2001.
[7] J. S. d. Bruin, T. K. Cocx, W. A. Kosters, J. F. J. Laros, and J. N. Kok.
Data mining approaches to criminal career analysis. In Proceedings of
the Sixth International Conference on Data Mining, ICDM ’06, pages
171–177, Washington, DC, USA, 2006. IEEE Computer Society.
[8] D. V. Canter. The environmental range of serial rapists. In D. V. Canter,
editor, Psychology in Action, Dartmouth Benchmark Series, pages 217–
230. Dartmouth Publishing Company, Hantshire, UK, January 1996.
[9] R. B. Cleveland, W. S. Cleveland, J. E. McRae, and I. Terpenning. Stl:
A seasonal-trend decomposition procedure based on loess. Journal of
Ofﬁcial Statistics, 6:3–73, 1990.
[10] G. Cowan. Statistical data analysis. Oxford University Press, 1998.
[11] P. J. Diggle and P. J. Diggle. Statistical analysis of spatial point patterns.
London: Edward Arnold, 1983.
[12] P. J. Diggle, J. Tawn, and R. Moyeed. Model-based geostatistics. Journal
of the Royal Statistical Society: Series C (Applied Statistics), 47(3):299–
350, 1998.
[13] G. Farrell and K. Pease. Repeat victimization, volume 12. Criminal Justice Press, 2001.
[14] M. Felson and E. Poulsen. Simple indicators of crime by time of day.
International Journal of Forecasting, 19(4):595–601, 00 2003.
[15] J. S. Goldkamp and E. R. Vilcica. Targeted enforcement and adverse
system side effects: The generation of fugitives in philadelphia. Criminology, 46(2):371–409, 2008.
[16] R. Hafen, D. Anderson, W. Cleveland, R. Maciejewski, D. Ebert,
A. Abusalah, M. Yakout, M. Ouzzani, and S. Grannis. Syndromic surveillance: Stl for modeling, visualizing, and monitoring disease counts. BMC
Medical Informatics and Decision Making, 9(1):21, 2009.
[17] K. Harries. Crime and the environment. American Lecture Series; No.
1033. Charles C. Thomas Publisher, Limited, 1980.
[18] S. D. Johnson, W. Bernasco, K. J. Bowers, H. Elffers, J. Ratcliffe,
G. Rengert, and M. Townsley. Space–time patterns of risk: a cross national assessment of residential burglary victimization. Journal of Quantitative Criminology, 23(3):201–219, 2007.
[19] M. H. Kutner, C. J. Nachtsheim, J. Neter, and W. Li. Applied linear
statistical models, volume 5. McGraw-Hill Irwin Chicago, 2004.
[20] R. Maciejewski, R. Hafen, S. Rudolph, S. Larew, M. Mitchell, W. Cleveland, and D. Ebert. Forecasting hotspots: A predictive analytics approach.
IEEE Transactions on Visualization and Computer Graphics, 17(4):440
–453, April 2011.
[21] A. Malik, R. Maciejewski, T. F. Collins, and D. S. Ebert. Visual analytics
law enforcement toolkit. In IEEE International Conference on Technologies for Homeland Security, pages 222–228, 2010.
[22] A. Malik, R. Maciejewski, N. Elmqvist, Y. Jang, D. Ebert, and W. Huang.
A correlative analysis process in a visual analytics environment. In IEEE
Conference on Visual Analytics Science and Technology (VAST), pages
33–42, 2012.

VOL. 20,

NO. 12,

DECEMBER 2014

[23] M. Monroe, R. Lan, H. Lee, C. Plaisant, and B. Shneiderman. Temporal
event sequence simpliﬁcation. IEEE Transactions on Visualization and
Computer Graphics, 19(12):2227–2236, 2013.
[24] J. D. Morenoff, R. J. Sampson, and S. W. Raudenbush. Neighborhood inequality, collective efﬁcacy, and the spatial dynamics of urban violence*.
Criminology, 39(3):517–558, 2001.
[25] T. Muhlbacher and H. Piringer. A partition-based framework for building
and validating regression models. IEEE Transactions on Visualization
and Computer Graphics, 19(12):1962–1971, Dec 2013.
[26] D. A. Norman. Things that make us smart: Defending human attributes
in the age of the machine. Basic Books, 1993.
[27] R. Oppenheim. Forecasting via the box-jenkins method. Journal of the
Academy of Marketing Science, 6(3):206–221, 1978.
[28] A. Rind, T. Lammarsch, W. Aigner, B. Alsallakh, and S. Miksch.
Timebench: A data model and software library for visual analytics of
time-oriented data. IEEE Transactions on Visualization and Computer
Graphics, 19(12):2247–2256, 2013.
[29] G. Robertson, D. Ebert, S. Eick, D. Keim, and K. Joy. Scale and complexity in visual analytics. Information Visualization, 8(4):247–253, 2009.
[30] L. W. Sherman. Police crackdowns: Initial and residual deterrence. Crime
and Justice, 12:pp. 1–48, 1990.
[31] L. W. Sherman, P. R. Gartin, and M. E. Buerger. Hot spots of predatory
crime: Routine activities and the criminology of place. Criminology,
27(1):27–56, 1989.
[32] M. Short, M. Dorsogna, P. Brantingham, and G. Tita. Measuring and
modeling repeat and near-repeat burglary effects. Journal of Quantitative
Criminology, 25(3):325–339, 2009.
[33] B. W. Silverman. Density Estimation for Statistics and Data Analysis.
Chapman & Hall/CRC, 1986.
[34] S. J. South and S. F. Messner. Crime and demography: Multiple linkages,
reciprocal relations. Annual Review of Sociology, 26(1):83–106, 2000.
[35] J. Stasko, C. Görg, and Z. Liu. Jigsaw: supporting investigative analysis
through interactive visualization. Information Visualization, 7(2):118–
132, 4 2008.
[36] S. S. Stevens. On the theory of scales of measurement, 1946.
[37] J. J. Thomas and K. A. Cook, editors. Illuminating the Path: The R&D
Agenda for Visual Analytics. IEEE Press, 2005.
[38] W. R. Tobler. A computer movie simulating urban growth in the detroit
region. Economic geography, pages 234–240, 1970.
[39] S. Towers. Kernel probability density estimation methods. Proceedings of
the Advanced Statistical Techniques in Particle Physics, pages 107–111,
2002.
[40] P. F. Velleman and L. Wilkinson. Nominal, ordinal, interval, and ratio
typologies are misleading. The American Statistician, 47(1):65–72, 1993.
[41] D. Weisburd, J. Hinkle, C. Famega, and J. Ready. The possible backﬁre
effects of hot spots policing: an experimental assessment of impacts on
legitimacy, fear and collective efﬁcacy. Journal of Experimental Criminology, 7(4):297–320, 2011.
[42] L. Wilkinson and G. Wills. The Grammar of Graphics. Statistics and
Computing. Springer, 2005.
[43] P. C. Wong, L. R. Leung, N. Lu, M. Paget, J. C. Jr., W. Jiang, P. Mackey,
Z. T. Taylor, Y. Xie, J. Xu, S. Unwin, and A. Sanﬁlippo. Predicting the
impact of climate change on u.s. power grids and its wider implications
on national security. In AAAI Spring Symposium: Technosocial Predictive
Analytics, pages 148–153. AAAI, 2009.
[44] C.-H. Yu, M. W. Ward, M. Morabito, and W. Ding. Crime forecasting
using data mining techniques. In Proceedings of the IEEE 11th International Conference on Data Mining Workshops, ICDMW ’11, pages 779–
786, Washington, DC, USA, 2011. IEEE Computer Society.
[45] J. Yue, A. Raja, D. Liu, X. Wang, and W. Ribarsky. A blackboard-based
approach towards predictive analytics. In AAAI Spring Symposium: Technosocial Predictive Analytics, page 154. AAAI, 2009.

Surpassing the Limit: Keyword Clustering to Improve
Twitter Sample Coverage
Justin Sampson, Fred Morstatter, Ross Maciejewski, Huan Liu
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University

{justin.sampson, fred.morstatter, ross.maciejewski, huan.liu}@asu.edu
ABSTRACT

be concise due to the 140 character limit imposed by the service, and the data gathering limits are relaxed compared to
most other similar services. The combination of these favorable traits, found almost uniquely in Twitter, have allowed it
to become organically selected as the “’model-organism’ for
research with social media data” [21]. The Twitter streaming API allows users to gather up to 1% of all tweets that
pass through the service at any time. According to Twitter, around 500 million tweets are posted every day meaning
that a single user can gather up to 5 million tweets in this
period of time.1 While this generous data rate allows for
large samples to be gathered over time, rate limiting still
poses significant challenges for any research which requires
as close to a complete data set as possible.
Recent works have shown that there is significant bias in
the sampling method used by Twitter’s filtered streaming
API. However, this sampling method remains unpublished,
making it difficult for end users to detect and correct for
the resulting bias in their data sets. Attempting to create
generalized models or make any form of measurement based
on a data set with ingrained bias is dangerous and can result in large margins of error that may not be acceptable.
Additionally, the inverse is also true. When the total population size for a data set is known bias can be minimized
making it possible to make good predictions based on principled statistical and machine learning techniques. In the
absence of the ability to measure and correct for sources of
bias, the only available recourse is to ensure that the coverage of the gathered sample is as close as possible to the total
sample population during the gathering process. However,
in order to measure the difference between a sample and the
complete set, a useful population measure is required.
The Twitter streaming API uses a mechanism called “limit
track” which informs the user of the number of tweets that
were not delivered to them due to exceeding the 1% rate
limit. The limit track data is provided periodically along
with tweets delivered in a particular stream. Unfortunately,
if the limit track value provided by Twitter is not a reliable
measurement, then it becomes significantly more difficult to
determine the overall sample population, and, as a result,
the level of bias in the sample remains unknown. In addition,
the usefulness of the limit track value is further reduced as
it does not allow for any method to retroactively obtain the
lost data. Unfortunately, since the method used for sampling
tweets, as well as how the limit track value is obtained, is not
yes published, it is imperative to know (1) whether Twitter’s

Social media services have become a prominent source of research data for both academia and corporate applications.
Data from social media services is easy to obtain, highly
structured, and comprises opinions from a large number of
extremely diverse groups. The microblogging site, Twitter,
has garnered a particularly large following from researchers
by offering a high volume of data streamed in real time.
Unfortunately, the methods in which Twitter selects data to
disseminate through the stream are either vague or unpublished. Since Twitter maintains sole control of the sampling
process, it leaves us with no knowledge of how the data that
we collect for research is selected. Additionally, past research has shown that there are sources of bias present in
Twitters dissemination process. Such bias introduces noise
into the data that can reduce the accuracy of learning models and lead to bad inferences. In this work, we take an
initial look at the efficiency of Twitter limit track as a sample population estimator. After that, we provide methods
to mitigate bias by improving sample population coverage
using clustering techniques.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Clustering;
Retrieval models

Keywords
Clustering; Text Processing; Social Media

1.

INTRODUCTION

The use of social media as a data source has allowed for
an extremely wide-reaching range of topics and phenomena
to be studied on a large scale. Highly prominent among
sources for data gathering is the microblogging site Twitter.
The popularity of Twitter as a source can be attributed to
the type of data that is produced by its users, tweets must
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
HT '15, September 1–4, 2015, Guzelyurt, Northern Cyprus.
c 2015 ACM. ISBN 978-1-4503-3395-5/15/09 ...$15.00.

DOI: http://dx.doi.org/10.1145/2700171.2791030.

1

237

https://about.twitter.com/company

limit track is accurate and, if it is not, we (2) must find
another way to decide if information is being lost.
With these limitations in mind, this work will attempt to
answer the following questions:
• Is the Twitter limit track an accurate measure of the
amount of data missing from a sample?
• How can we structure our search to reduce the volume
of missed data?
While data sets created using completely random sampling methods are known to preserve important statistical
properties that allow a smaller sample size to generalize well
to the entire set [7, 19], the introduction of deterministic processes into sample creation introduces bias that can cause
erroneous and dangerous conclusions to be drawn from the
data. Unfortunately, the incredible size and rigidity inherent to the infrastructure of these services, such as Twitter,
do not necessarily lend themselves well to producing the
type of random sampling necessary. This shifts the burden of responsibility for creating and using unbiased data
from the service to the user. However, producing unbiased
samples from inherently biased data sources is nontrivial.
Regardless of biases in the sampling method, increasing the
ratio of coverage between the sampled data and the complete dataset will reduce sample error and improve results.
While gathering the complete set of data would be optimal,
most services impose strict sampling rate limitations. In
other cases, where the complete data samples are available,
exorbitant pricing can be a roadblock. This work proposes
several methods to overcome these limitations by increasing sample coverage, thereby minimizing bias in incomplete
samples. Though the proposed methods were implemented
and tested on Twitter, the results should generalize well to
any keyword-based data gathering services.

2.

RELATED WORK

The predictive power of social media services, such as
Twitter, has been used to effectively track and predict the
spread of disease [1, 6, 8]. Other efforts have also shown
promising results by using social media to discover a wide
range of collective knowledge such as real-time political polling
[4, 22] and the potential success of movies at box-office [2,
14]. However, there are very few standards governing how
data from social media is gathered and how research and
predictions should be approached [16]. A number of studies
have shown that the method used for sampling can introduce
various forms of bias which introduce error into results and
remain largly unnoticed [9, 19]. Very little research has gone
into methods for correcting for bias. Morstatter et al. proposed using bootstrapping [5] and secondary data sources
to obtain a confidence interval for the occurrence rate of a
hashtag between the two sources. Such a statistical difference could be used as a red flag for the presence of bias in
the stream [18].
Several relevant works have attempted to uncover the underlying mechanisms with which Twitter disseminates tweets
through its various streaming APIs. In 2013, Morstatter et
al. tested the differences between the public Twitter streaming API, which is commonly used by researchers but will
only return up to 1% of all available data, and the prohibitively priced “firehose” stream, which offers all of the
available data to those willing to pay for it [19]. This work
uncovered a number of anomalies such as varying top hashtags, differing topic distributions, and varying network mea-

238

sures when using the same searches on both the paid and free
services. They explain that according to the “law of large
numbers” if the sample is truely random then it should relate well with the results from the population, in this case
the “firehose” results [19]. These differences indicate that
the streaming API introduces some form of bias when determining how to limit the results sent to a stream [18].
The possible causes of this sampling bias have been a continuing source of inquiry. Joseph et al. used five independent
streaming sources the differences between multiple streaming results taken at similar but varying time windows [10].
They used a set of keywords and usernames including stop
words such as “the” and “i” and words that they specifically
invented for the experiment. In order to determine if starting time had an impact on the results they staggered each
start by a fraction of a second. After running these tests
multiple times, they showed that, when using the same keywords across each stream, 96% of unique tweets were captured by all streams [10]. Since the experiment used stop
words that undoubtedly make up a significant portion of
English tweets, a random distribution method would have
given results that varied wildly across the separate streaming
connections. This is strong proof that the sampling method
used by Twitter is highly deterministic.
Kerg et al. found further evidence of non-random sampling. In earlier forms of the Twitter streaming API, the
three data streaming levels which provided 1%, 10%, and
100% of Twitter data were named “Spritzer”, “Gardenhose”,
and “Firehose” respectively. These streams, unlike the filterbased stream, provide data from the entire body of current
tweets. Through analysis of the unique tweet IDs provided
by Twitter, the authors discovered that the unique IDs included data, such as the timestamp when the tweet was
created, the data center that created it, and other information related to the underlying technical infrastructure of the
Twitter data centers. Analysis of the timestamp in particular showed that, in the limited streams, the timestamps only
fell over a specific interval of milliseconds directly related to
the percentage of sample coverage specified by the service
level [11]. Though no similar timestamp-based sampling
method appears to be used in the filtered search stream,
the non-random nature of the filtered data, in addition to
the use of simple deterministic methods in past dissemination schemes, indicates that there are underlying artifacts in
the filtered stream infrastructure as well that may be adding
bias to gathered samples.
The 1% boundary has proven to be a significant hindrance
for applications and research that require as close to a complete set of data as possible. These include mission critical
response situations, such as those necessary for emergency
response and monitoring applications [12, 17, 23], as well as
any form of research that is highly affected by sample bias.
As a direct result of the high cost of the “firehose” service,
many users have attempted to develop novel solutions for
gathering data to improve either the size and coverage of
the dataset [13] or the overall quality of results for a smaller
sample [7]. Li et al. proposed a system that uses “automatic
keyword selection” to determine the best search terms to use
for a given search through a keyword cost-based system. Using this method improved topic coverage from 49% of targeted tweets obtained through human-selected keywords to
90% in the automatic system [13]. Ghosh et al. took an
alternate approach and attempted to improve the quality

of their sample by gathering topic-based twitter data from
experts and comparing the “diversity, timeliness, and trustworthiness” to a larger sample of community tweets of the
same topic. While the expert-based search did show an improvement in all of these areas, they cautioned that crowdbased data could not be entirely discounted as it captured
other significant properties such as the flow of conversation
in the topic that is otherwise ignored by experts [7].

3.

KEYWORD SPLITTING APPROACH

The Twitter Streaming API allows anyone to gather realtime data from Twitter by specifying a set of parameters
that include search terms, user names, or location boxes. In
the case that the search terms specified for a stream surpass
the 1% limit, the API informs the user of the total number of tweets missed since the streaming connection began.
Ideally, this would give the user a quantitative measure of
the overall sample size for their search. The total size of
the dataset would then be the sum of the number of unique
tweets gathered added to the limit track value provided by
Twitter. Knowing the exact quantity of missing data is of
paramount importance when it is necessary to adjust the
data gathering method to account for gaps in the sample.
The Twitter limit track is designed to give a measurement of lost data for a single stream. However, our proposed
methods revolve around using multiple streams to increase
the effective sample size. In order to determine if the limit
track is a useful measure for the overall sample, when viewed
from the context of multiple streams, we ran a number of
trials simultaneously. At the first level, all keywords used in
the search were tracked using a single stream. For each level
beyond the first, the keywords were separated as evenly as
possible among the streams. In the example shown in Figure 1, all keywords are split between crawlers based on the
number of crawlers required at each level. For example, split
level two separates all 400 keywords between two crawlers.
All split levels are run simultaneously up to a maximum split
level of five which required a total of fifteen streaming clients.
After a set period of time, all crawlers terminate and any
duplicate tweet IDs are discarded from the set for each split
level. Since no keywords were added or duplicated between
the streams, the total number of possible tweets should be
equivalent to the unsplit streams number of caught tweets
as well at the reported limit value. However, in nearly every experiment, there was always a number of splits that
would result in a larger number of unique tweet IDs than
should be possible according to limit track. As shown in
Figure 1, we accumulated 107.3% of the tweets that were
indicated by the limit track, meaning that we received more
tweets than were estimated by Twitter. Furthermore, using
a four-split approach, we collected 137.2% of the tweets indicated by the limit track In order for the limit track to be
an accurate measurement of the sample population it should
not be possible to gather unique tweets much beyond 100%.
This data can also be seen in Table 1 where N/A is used
for the missing data and the totals columns when splitting
was used because each stream is only capable of indicating the number of missed tweets and not which tweet IDS
were missed. Since multiple crawlers may have overlap in
the tweets that they do not receive, it is not possible to determine the number of unique tweet IDs missed across each
crawler. Additionally, if limit track is an accurate metric,
then the number of missed tweets for a single stream with

Figure 1: Using a single crawler it is possible to
gather tweets from at most 400 keywords. As can
be seen, the rate of tweets caught remains stable for
a single crawler. Splitting the same keywords across
multiple crawlers results in immediate improvement
in the number of unique tweets caught as well as
allowing the sampled population to go beyond the
population size indicated by Twitter.

Table 1: Impact of Additional Crawlers on Sample
Coverage. Since multiple crawlers may have overlap
in the tweets that they do not receive, it is not possible to determine the number of unique tweet IDs
missed across each crawler - we use N/A when this
is the case.
Unsplit
2-split
3-split
4-split

Caught
3632
5060
8714
11143

Missed
4488
N/A
N/A
N/A

Total
8120
N/A
N/A
N/A

Coverage
44.7%
62.3%
107.3%
137.2%

all possible keywords should indicate the total population.
These figures provide strong evidence that the limit track
reports supplied by Twitter are either inaccurate or they
are an estimation.
In order to get the most tweets in a time period, we run
multiple crawlers. Given a list of keywords, the Twitter
streaming API will return every new tweet that contains any
of the words specified. Therefore, by splitting the keyword
list among multiple crawlers it becomes possible to gather
tweets beyond the 1% limitation. Under perfect conditions,
each additional stream increases the effective limit rate by
1%. Unfortunately, when partitioning keywords, it is important to keep in mind the possibility of overlap between the
streams. For example, a tweet that contains keywords that
were split between a number of crawlers will be duplicated in
each stream. Tweet duplication in this manner reduces the
overall efficiency of each stream. The stream splitting methods must be able to account for, and attempt to minimize,
the potential for overlap between keywords.
In order to gather samples closer to the population size,
we propose and evaluate three different splitting algorithms

239

- each with varying characteristics for the initial rate of
growth and growth stability as additional crawlers are added:
• Round Robin
• Spectral Clustering
• K-Means Round Robin

4.

Each streaming agent captures text data from each tweet
in order to create a word-to-word co-occurrence network.
This data takes the form of word tuples followed by the
number of times that the word pairs were observed across
each tweet. The co-occurrence can then be used to create
a network graph where each word is a graph node and the
number of observations become undirected weighted edges.
The resulting network graph G takes the standard form G =
(V, E, W ) where each v ∈ V is a word and each e ∈ E is
a co-occurrence observation with weight, W indicating the
number of times a (v, v) pair was observed.

EXPERIMENTS

Each of the following experiments is designed to test the
efficiency of the given splitting method in obtaining a sample
closer to the total sample population than is possible with
the standard single stream method. The key factors that we
will focus on include: speed of initial growth with a small
number of crawlers, how stable the splitting method is for
increasing growth as additional crawlers are added, and how
many crawlers are required before we pass the population
size estimation established by Twitter.
In each of the experiments, we drew from a pool of twentyone streams. This allows us to use a single stream with all
possible keywords as a baseline measure for standard gathering rate and population estimation with the limit track.
The remaining twenty streams are then used for performing
keyword splitting up to twenty ways. Each of these streams
was able to track up to 400 keywords, the maximum number
of keywords allowed by Twitter in any given stream. While
twenty streams could easily track more than 400 keywords,
we limit our search to all 400 keywords from a single stream
split across up to twenty streams to allow for direct comparison with the performance of a single crawler. The keywords
used were chosen by taking the most frequently occurring
keywords from the Twitter sample stream in a ten minute
period of time. These keywords contained a broad spectrum
of topical words as well as multiple source languages. Keywords were chosen in this manner to ensure a high data rate
from Twitter and represent a worst possible scenario for keyword splitting since a single keyword represents an atomic
element that can not be further split. The set of keywords
discovered in this fashion were used throughout.
For the purpose of these experiments, it was not necessary
to track specific users or geographic coordinates simply because the volume of data obtained from these sources is minuscule in comparison to the top words used on Twitter. In
cases where segmenting geoboxes is necessary, it is possible
to segment it into any number of geographic regions while
introducing no overlap between regions. In addition, the
proposed solutions can be applied to the tracking of Twitter
user names since they act as tokens in a similar manner to
keywords. Limiting the number of keywords split between
crawlers to the maximum capability of a single stream enables comparison by examining the change in sample population between each method while keeping the set of keywords constant. For example, using a single stream we know
the number of tweets obtained, n, as well at the number of
tweets left undelivered, m. Therefore, we should know the
total sample size, N , such that N = n + m. Considering
that the number of undelivered tweets is reported without
any indication as to which data was lost, this total sample
size is the only quantitative measurement given by Twitter
as to the overall volume of the data. Furthermore, in cases
where we want to employ multiple streaming accounts in a
search strategy, the number of tweets missed, reported on a
per-stream basis, becomes an unreliable measure due to the
potential of separate streams to count a single missed tweet
multiple times.

4.1

Experiment 1 - Round Robin

In order to reduce the initial overlap between crawlers,
an additional “priming” step was added before each crawling experiment. Priming a search requires running an initial
single level crawler for up to 10 minutes before creating any
additional streams. During this stage the priming crawler
observes all word pairs. Since words with a large number of
pair observations are more likely to occur together, reducing
the overlap between words will reduce the number of duplicated tweets. Though this priming step is a requirement for
each splitting technique described here, it is possible to perform a very short initial priming search and subsequently update the clusters later as the word-to-word graph improves
in quality. The priming step is not used in the results and is
instead a method to obtain a word co-occurrence graph to
be used in performing the initial splits.
The round robin method of stream keyword splitting is
an effective baseline for other splitting methods as it is a
straightforward method that requires very little additional
processing power. Sampling the amount of tweets gathered
and missed at each split level requires running one baseline
stream that contains all selected keywords as well as k additional streams that contain the keywords split between each
stream. While it is possible to sample all split levels simultaneously, the number of required accounts for a test of this
where k is the number of split levels and
type is xk = k(k+1)
2
x is the number of accounts. Sampling all splits up to a split
level of 7 would require 28 separate account which is unfeasible for our purposes. Additionally, the processing power
required to maintain the set of unique tweet IDs for each
stream becomes problematic very quickly. Alternatively, using a single baseline stream that contains all keywords and
comparing the results to each split level independently requires a much lower number of accounts, xk = k+1. It is this
latter method that we use for each stream splitting analysis.
At the completion of the priming stage, the word pairs, from
the most frequently occurring to the least frequently occurring, are assigned to streams in a round robin fashion. Each
split level runs for 30 minutes before resetting all streams,
including the baseline stream, and beginning the next split
level. Resetting the baseline stream is key to analyzing each
stream level in this method as it allows a comparison of the
difference between a single stream and multiple split streams
over a given window of time and thereby making it unnecessary to maintain all data for every level of split at once.
The graph shown in Figure 2 show that we were able to
eclipse the limit track maximum by 12 splits at which point
we were able to gather six times as many unique tweets
containing proper keywords than was possible with a single
stream. Reaching 20 split levels nearly doubled the number
of unique tweets gathered over the maximum indicated by

240

Figure 2: Round robin splitting based on word cooccurrence tends to show a steady rate of gain as
additional crawlers are added.

Figure 3: The results of Spectral Clustering do show
an increase of sample coverage overall. However,
the clustering creates unbalanced splits where one
stream, while still a good cluster, may contain significantly more words than others. The lack of balance manifests through instability in the rate of gain
from each additional crawler.

Twitter. Constructing round robin splits can be found in
Algorithm 1.
Algorithm 1: Round Robin Splits Construction
input : graph G, num clusters k
output: array of lists
for node v in G(V, E) do
keywordList += v
end
sortedList := Sort(keywordList by occurrence rate);
for word, index in sortedList do
listNum := index % k;
assign word to split list k
end

4.2

Experiment 2 - Spectral Clustering

Spectral clustering, an extension of K-Means clustering
which performs “low-dimension embedding” [20], directly leverages the word occurrence graph. Unlike K-Means, spectral
clustering requires an affinity matrix in order to create accurate clusters based on how the items relate to one another. This clustering method allows us to define a number
of clusters, k, and the spectral clustering algorithm will incorporate the nuances of the similarity between the items in
order to improve cluster results. Like most clustering algorithms, spectral clustering does not make any guarantee on
the size of each cluster. As a result, cluster size can vary to
a large degree which has implications for the usefulness of
this method.
The combined effect of these properties of spectral clustering manifest themselves as a number of interesting properties. First, as the number of requested clusters increased,
keywords in each sub cluster also became increasingly biased towards individual languages. This is a favorable trait
for reducing overlap since two or more streams gathering
keywords of differing language should have a low rate of
word overlap except in the case of multi lingually authored
tweets. Secondly, despite the ability to specify the number
of clusters, if the underlying similarity does not lend itself
well to a high number of partitions a few of the resulting

241

clusters will be very large while many will be small. Though
this behavior is preferable for most applications that employ clustering, a significant difference in the size of clusters
causes some streams to significantly over perform the 1%
limit and subsequently become severely rate limited while
streams based on smaller clusters fail to reach a limiting
rate at all. This effect can be seen in Figure 3. Clustering
based on word occurrence quickly passes the Twitter limit
with only 6 streams active but shortly thereafter struggles
to gain much ground. Wild fluctuation can be observed between each split level and while there is overall growth it is
possible to gather a smaller sample with a larger split level.
Such inconsistencies were not observed in Figure 2 further
indicating how detrimental sensitivity to cluster size is when
considering methods for gathering tweet samples. Spectral
clustering based splits can be found in Algorithm 2.

4.3

Experiment 3 - K-Means Round Robin

The K-Means Round Robin (KMRR) approach looks to
incorporate the strengths of both previous splitting methods. It borrows the near equivalently sized groups found in
the round robin method which enable stable growth across
split levels and the use of intelligent clustering from the spectral clustering method to reduce tweet duplication, or overlap. Rather than using spectral clustering, however, we use
K-Means clustering [15]. In order to accomplish this type
of non-standard clustering with K-Means we first need to
convert the network graph into a dissimilarity matrix. This
can be done by constructing the standard network graphs
weighted adjacency matrix and normalizing by the highest
occurring word pair. Next, we use the dissimilarity matrix in
a process called Multidimensional Scaling, or MDS [3]. Multidimensional Scaling uses the computed dissimilarity values
to transform data into another graph space - in this case two
dimensions. MDS leverages the dissimilarity between each
point as a measurement distance and seeks to find an embedding in the new dimensional space that maintains these

Figure 5: After performing Multidimensional Scaling on the data set, the words are clustered according to K-means. The discovered cluster centroids
are then used to create balanced groupings by assigning a centroid to the closest unassigned word in
a round robin manner. However, this process introduces intruding words as the number of remaining
assignments decreases.

Figure 4: KMRR splitting displays quick initial
gains with a low number of crawlers while eventually settling into a steady growth rate as additional
crawlers are added.
Algorithm 2: Spectral Clustering Splits Construction
input : graph G, num clusters k
output: array of lists
matrix A := [][];
for node v in G(V, E, W) do
wordIDs[v] := unique ID
end
/* create affinity matrix for spectral
clustering
*/
for word pair (v1,v2) in G(V, E, W) do
Construct symmetric matrix A for all words such
that v1,v2 := E.weight;
end
labels := the result of Spectral Clustering with k
clusters and the matrix A;
for label, cluster in labels do
splitLists[cluster].append(label)
end

any pair of nodes. The convergence process is depicted in
Figure 6. While performing the convergence step would normally be time consuming, the number of items clustered is
never larger than 400 items. Figure 7 shows the final cluster assignments after the completion of the convergence step.
These clustering assignments are completely absent of intrusion. Using this method the rate at which multiple crawlers
exceed the limit track becomes comparable to spectral clustering at 7 splits as can be seen in Figure 4. Additionally,
K-Means Round Robin does not suffer from the problem of
inconsistency between split levels showing consistency similar to that found in the round robin experiment. KMRRbased splits can be found in Algorithm 3.
A comparison of each splitting method can be seen in Figure 9. These results, shown also in 2, are the average coverage rates related to the total sample size estimated from the
limit track over 20 trials. Though the Twitter limit track is
not a perfect indicator of the total population of data available from a complete set, it is used in this comparison as
a relative measure as opposed to an absolute. Given additional crawlers, it is very likely that the limit track would
again be eclipsed. Each splitting method was able to produce a sample significantly closer to the total population as
estimated by Twitter and sometimes many times larger than
a single stream.

distances as closely as possible. With our network newly
transformed into a two dimensional graph, K-Means is run
on the data to find centroid locations. It is important to note
that we do not use the learned K-Means labels as this would
not satisfy the requirement for relatively balanced keyword
splits. Instead, keywords are assigned to clusters in a round
robin fashion based on euclidean distance from the centroid.
The combination of MDS transformation with round robin
k-means centroid assignments is shown in Figure 5.
It is obvious to see that assigning words to centroids in a
round robin style introduces intruding words into clusters.
Accounting for word intrusion is accomplished through a
convergence step. The convergence process examines the
items assigned to each centroid and swaps the centroid assignment for any pair of items where doing so would reduce
the average distance to each centroid following the inequalc1
c2
< bc2 +a
, where c1 and c2 are cluster cenity, bc1 +a
2
2
troids and the remaining variables follow the notation that
ac1 is the distance to centroid 1 from item a. This process is repeated until no improvement can be found between

5.

CLUSTER REALIGNMENT

The next step is determining if there is a difference between results when the clusters are computed at some time
variable above the reconnection limit imposed by Twitter.
To accomplish this, we use one stream that runs indefinitely
and constantly updates the observed weight for each word
occurrence pair. This stream does not maintain a tweet
set in order to keep the memory footprint from growing
too large. Every 1000 seconds the cumulative network is
clustered and new splits are designated for every stream.

242

Algorithm 3: K-Means Round Robin Splits Construction
input : graph G, num clusters k
output: array of lists
matrix dissimilarity := [][];
counter := 0;
for node v in G(V, E, W) do
wordIDs[v] := unique ID
end
max := compute highest rate of occurrence for
normalization;
/* create dissimilarity C matrix for MDS
*/
for word pair (v1,v2) in G(V, E, W) do
Construct symmetric dissimilarity matrix for all
words such that v1,v2 := | E.weight − max | ;
end
wordGraph is the result of Multidimensional Scaling of
the dissimilarity matrix to 2 dimensions;
labels := KMeans(num clusters = k, data =
wordGraph)
for i := 0 to number of keywords do
assign keyword i to the closest cluster centroid
end
/* enter convergence step
*/
while any improvement can be found do
for i := 0 to number of clusters do
for j := i to number of clusters do
if swapping a point in each cluster would
reduce the average distance to each centroid
then
swap cluster assignments
end
end
end
end
for label, cluster in labels do
splitLists[cluster].append(label)
end

Figure 6: The convergence step maintains group size
by operating on pairs of nodes with differing centroid assignments. On each pass, if a pair of nodes is
discovered for whom swapping them will reduce the
average distance between centroids then it is considered a good swap and immediately performed. The
lines between each node indicate the swap path as a
pair of cluster assignments gradually improve.

Figure 7: At the completion of the KMRR convergence step, the resulting clusters can be seen to have
no intruding assignments while maintaining the balanced group size between clusters.

gathering rate using this method can be somewhat unstable,
over a period of time the number of tweets gathered per second does become stable and shows an overall increase. Over
larger periods of time, the word occurrence network becomes
increasingly stable, and, as a result, the overall increase of
performing realignment becomes less efficient. This is especially true when considering that performing clustering,
disconnection, and reconnection can take some time and introduce small gaps into the dataset. Twitter streams also do
not return immediately to their highest potential data rate
which introduces a dip in the number of tweets obtained for a
small period of time. Overall, cluster realignment improves
the possible data rate but these factors should be considered
when implementing a realignment scheme.

This time step was chosen in order to avoid connection rate
limiting from Twitter which will block connections from a
stream if it attempts to reconnect more frequently than every 15 minutes. Unfortunately, Twitter does not supply
any method for changing the search parameters of a stream
without performing a reconnection so any method of cluster realignment will be unable to perform these steps more
frequently than the 15 minute interval.
Cluster realignment seeks to improve the performance of
the spectral clustering method as well as KMRR. Since one
stream is dedicated solely to maintaining and updating the
word-occurrence network, performing either of the clusterbased splitting methods at a later period of time should improve the resulting clusters. Using cluster realignment, it becomes possible to run a very short priming step or even eliminate the priming step altogether. This allows the streams
to begin gathering data immediately while also reducing the
overlap between each stream over time. The effect of cluster
realignment can be seen in Figure 8. While the initial data

6.

LANGUAGE CLUSTERING

In spectral clustering based on word co-occurrence, about
72% of the largest clusters were one language with a mix
of words from other languages. When k = 20, many of the
smaller clusters were completely biased towards their language to the point that they were always very close to, if
not completely from, a single language. The high rate of

243

We started our analysis by inspecting the performance of
the Twitter limit track process in single and multi-stream
situations. It has been assumed that the Twitter limit track
provides an accurate metric for the overall size of a dataset
with a given set of keywords. When using a single stream,
the limit values returned, in addition to the number of tweets
gathered, should be the size of the complete data set if
it were possible to obtain all 100%. However, the simple
addition of extra streams operating on subsections of the
same keywords were able to quickly overtake the hypothetical maximum volume of unique tweets. Such ease in producing beyond the maximum limit as calculated by Twitter cast
serious doubts on the validity of the limit track system. In
the absence of a well-defined baseline we employed a relative
method to measure improvement obtained through alternative streaming methods.
Next, we proposed a series of methods for separating keywords in order to obtain the best possible sample coverage. The simplest proposed method, Round Robin splitting, displayed stable sample growth with the addition of
each stream and was able to overtake the Twitter-calculated
complete set. The stability of growth indicated the importance of balancing keywords across streams - a property that
would later be employed in K-Means Round Robin.
The second method, Spectral Cluster splitting, again employed a word co-occurrence graph to build a similarity matrix. The clusters obtained in this manner tended to vary
significantly in size but showed interesting properties, such
as a tendency for languages to cluster together. Spectral
Cluster-based streams showed the sharpest rate of initial
growth with a smaller number of streams. However, when
the number of streams grew large the volume of increase
with each stream became unstable as a result of small clusters failing to fully populate the stream bandwidth.
Using the lessons learned from Round Robin splitting and
Spectral Cluster splitting, we proposed a balanced solution
in K-Means Round Robin. KMRR uses K-Means clustering
before following a convergence process to produce balanced
cluster sizes while maintaining as much of the original clustering properties as possible. KMRR showed the rapid initial growth displayed by Spectral Clustering as well as the
stability obtained from Round Robin splitting. While clusters still displayed a tendency to group by language, each
cluster had a higher rate of out-of-language word intrusion
than was seen in the Spectral Clustering results. Each of
these methods allow for cluster re-computation on the fly,
and improvements to the rate of tweets obtained per second
were seen when periodically realigning from an improved
word co-occurrence graph.
There are many interesting possible extensions to this
work. Since it can be shown that the Twitter limit track
is not a true indication of the overall population for a given
search, further inquiry into the methods used for calculating
the limit may reveal interesting features of the Twitter tweet
dissemination process and provide insight to the source of
bias observed in Twitter streams. While our experiments focused on a single feature of each keyword, the co-occurrence
rate, the language clustering side effect indicates that there
are potentially other features that may introduce sources of
overlap. An example of this would be the semantic meaning between each keyword where there may be potential for
further reduction of overlap by seperating keywords with
similar meaning. Identification of other such features could

Figure 8: Performing cluster realignment by maintaining and updating the word co-occurrence graph
over a long period of time can improve the number of tweets gathered per second without the need
to introduce additional crawlers. Initially, the lack
of a significant sample for the co-occurrence graph
causes fluctuations in the gathering rate but as the
sample size increases the volume of data per second
becomes stable and is still able to maintain a constant rate of growth even as the amount of data in
the population (limit maximum) reduces.
strong language clusters is a good indicator of correct cluster assignments. In all methods, a high rate of language
clustering should result in searches with less overlap. Future methods for search improvement should seek a balance
between language clusters, word co-occurrence overlap reduction, and maximum per-crawler stream utilization.
KMRR also displayed this property by produced heavily language-biased clusters but, as is likely the result of
the round robin process, few of the clusters were completely
from a single language. There is a potential for optimization
within the KMRR clustering process where by producing
slightly less balanced clusters in exchange for improved language clustering may produce better results. On the far end
of the spectrum, the round robin method seemed to produce
a completely mixed set of language keywords as is expected.
Further experiments would need to be run with attention
paid particularly to the language bias from these clustering
methods in order to obtain a better understanding of the
effect of language clusters on streamed search results.

7.

CONCLUSION

We ask whether the volume of data missed, as reported
by the Twitter limit track system, is an accurate and useful
measurement for determining sample size. Furthermore, we
ask whether standard methods used for gathering streamed
data from the Twittersphere can be improved to increase
coverage for a gathered dataset. To answer these questions,
we used a battery of twenty streams as well as a single comprehensive baseline stream to gather tweets for a large number of high volume keywords. We provide a methodology
for comparing relative improvement in sample size in the
absence of reliable reporting as well as a series of algorithms
capable of a multiplicative increase in sample coverage.

244

[8]

[9]

[10]

[11]

Figure 9: This graph shows the average sample coverage for a given set of search keywords across 20
trials. Each of our sampling methods significantly
outperform the single stream with some variation
in characteristics between each method.

[12]

[13]
Table 2: Sample Coverage of Total Population
Round Robin
Spectral Clustering
KMRR

Unsplit
19.02%
19.02%
19.02%

2-split
50.54%
28.95%
42.93%

3-split
82.58%
78.63%
77.45%

4-split
64.34%
82.08%
87.63%

[14]

[15]
further strengthen the gathering process and lead to better
and less biased samples.
[16]

8.

ACKNOWLEDGEMENTS

This work is sponsored, in part, by Office of Naval Research grant N000141410095.

[17]

9.

[18]

REFERENCES

[1] H. Achrekar, A. Gandhe, R. Lazarus, S.-H. Yu, and
B. Liu. Predicting Flu Trends using Twitter Data. In
INFOCOM, pages 702–707. IEEE, 2011.
[2] S. Asur and B. A. Huberman. Predicting the Future
with Social Media. In WI-IAT, volume 1, pages
492–499. IEEE, 2010.
[3] I. Borg and P. J. Groenen. Modern Multidimensional
Scaling: Theory and Applications. Springer Science &
Business Media, 2005.
[4] A. Ceron, L. Curini, S. M. Iacus, and G. Porro. Every
Tweet Counts? How Sentiment Analysis of Social
Media can Improve our Knowledge of Citizens’
Political Preferences with an Application to Italy and
France. New Media & Society, 16(2):340–358, 2014.
[5] B. Efron and R. J. Tibshirani. An Introduction to the
Bootstrap. CRC press, 1994.
[6] M. Garcia-Herranz, E. Moro, M. Cebrian, N. A.
Christakis, and J. H. Fowler. Using friends as sensors
to detect global-scale contagious outbreaks. PLoS
ONE, 9(4):e92413, 04 2014.
[7] S. Ghosh, M. B. Zafar, P. Bhattacharya, N. Sharma,
N. Ganguly, and K. Gummadi. On Sampling the

[19]

[20]

[21]

[22]

[23]

245

Wisdom of Crowds: Random vs. Expert Sampling of
the Twitter Stream. In CIKM, pages 1739–1744.
ACM, 2013.
J. Gomide, A. Veloso, W. Meira Jr, V. Almeida,
F. Benevenuto, F. Ferraz, and M. Teixeira. Dengue
Surveillance Based on a Computational Model of
Spatio-temporal Locality of Twitter. In WebSci, pages
1–8. ACM, 2011.
S. González-Bailón, N. Wang, A. Rivero,
J. Borge-Holthoefer, and Y. Moreno. Assessing the
Bias in Samples of Large Online Networks. Social
Networks, 38:16–27, 2014.
K. Joseph, P. M. Landwehr, and K. M. Carley. Two
1%s Don’t Make a Whole: Comparing Simultaneous
Samples from Twitter’s Streaming API. In Social
Computing, Behavioral-Cultural Modeling and
Prediction, pages 75–83. Springer, 2014.
D. Kerg, R. Roedler, and S. Seeber. On the
Endogenesis of Twitter’s Spritzer and Gardenhose
Sample Streams. In ASONAM, pages 357–364, 2014.
S. Kumar, G. Barbier, M. A. Abbasi, and H. Liu.
TweetTracker: An Analysis Tool for Humanitarian
and Disaster Relief. In ICWSM, pages 661–662, 2011.
R. Li, S. Wang, and K. C.-C. Chang. Towards Social
Data Platform: Automatic Topic-focused Monitor for
Twitter Stream. VLDB, 6(14):1966–1977, 2013.
Y. Lu, F. Wang, and R. Maciejewski. Business
Intelligence from Social Media: A Study from the
VAST Box Office Challenge. Computer Graphics and
Applications, IEEE, 34(5):58–69, Sept 2014.
J. MacQueen et al. Some Methods for Classification
and Analysis of Multivariate Observations. In BSMSP,
volume 1, pages 281–297. Oakland, CA, USA., 1967.
L. Madlberger and A. Almansour. Predictions Based
on Twitter-A Critical View on the Research Process.
In ICODSE, pages 1–6. IEEE, 2014.
F. Morstatter, N. Lubold, H. Pon-Barry, J. Pfeffer,
and H. Liu. Finding Eyewitness Tweets During Crises.
ACL, pages 23–27, 2014.
F. Morstatter, J. Pfeffer, and H. Liu. When is it
Biased?: Assessing the Representativeness of Twitter’s
Streaming API. In WWW, pages 555–556, 2014.
F. Morstatter, J. Pfeffer, H. Liu, and K. M. Carley. Is
the Sample Good Enough? Comparing Data from
Twitter’s Streaming API with Twitter’s Firehose. In
ICWSM, pages 400–408, 2013.
A. Y. Ng, M. I. Jordan, and Y. Weiss. On Spectral
Clustering: Analysis and an Algorithm. In NIPS,
pages 849–856. MIT Press, 2001.
Z. Tufekci. Big Questions for Social Media Big Data:
Representativeness, Validity and Other
Methodological Pitfalls. In ICWSM, pages 505–514,
2014.
A. Tumasjan, T. O. Sprenger, P. G. Sandner, and
I. M. Welpe. Predicting Elections with Twitter: What
140 Characters Reveal about Political Sentiment.
ICWSM, 10:178–185, 2010.
S. Vieweg, A. L. Hughes, K. Starbird, and L. Palen.
Microblogging During Two Natural Hazards Events:
What Twitter May Contribute to Situational
Awareness. In CHI, pages 1079–1088. ACM, 2010.

94

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 19,

NO. 1,

JANUARY 2013

Abstracting Attribute Space for Transfer
Function Exploration and Design
Ross Maciejewski, Member, IEEE, Yun Jang, Insoo Woo, Heike Jänicke, Member, IEEE,
Kelly P. Gaither, Member, IEEE, and David S. Ebert, Fellow, IEEE
Abstract—Currently, user centered transfer function design begins with the user interacting with a one or two-dimensional histogram
of the volumetric attribute space. The attribute space is visualized as a function of the number of voxels, allowing the user to explore
the data in terms of the attribute size/magnitude. However, such visualizations provide the user with no information on the relationship
between various attribute spaces (e.g., density, temperature, pressure, x, y, z) within the multivariate data. In this work, we propose a
modification to the attribute space visualization in which the user is no longer presented with the magnitude of the attribute; instead, the
user is presented with an information metric detailing the relationship between attributes of the multivariate volumetric data. In this way,
the user can guide their exploration based on the relationship between the attribute magnitude and user selected attribute information
as opposed to being constrained by only visualizing the magnitude of the attribute. We refer to this modification to the traditional
histogram widget as an abstract attribute space representation. Our system utilizes common one and two-dimensional histogram
widgets where the bins of the abstract attribute space now correspond to an attribute relationship in terms of the mean, standard
deviation, entropy, or skewness. In this manner, we exploit the relationships and correlations present in the underlying data with
respect to the dimension(s) under examination. These relationships are often times key to insight and allow us to guide attribute
discovery as opposed to automatic extraction schemes which try to calculate and extract distinct attributes a priori. In this way, our
system aids in the knowledge discovery of the interaction of properties within volumetric data.
Index Terms—Transfer function design, volume rendering, information theory

Ç
1

INTRODUCTION

O

common method that allows scientists to visually
query their volume rendered data is the interactive
transfer function widget. Typically, the transfer function
widgets rely on either a one-dimensional (1D) or 2D
histogram frequency plot of the volumetric data (e.g., [1],
[2], [3]), and it is often assumed that users will have an
underlying knowledge of regions of interest within the
data. By using this underlying knowledge, users will then
be able to create an appropriate transfer function that maps
voxel values to a given attribute structure. While scientists
exploring their volumetric data do have an underlying
knowledge of their data, they are often novice transfer
function designers.
NE

. R. Maciejewski is with the Arizona State University, PO Box 878809
Tempe, AZ 85287-8809. E-mail: rmacieje@asu.edu.
. Y. Jang is with the Sejong University, 98 Gunja-Dong, Gwangjin-Gu,
Seoul 143-747, South Korea. E-mail: jangy@sejong.edu.
. I. Woo and D.S. Ebert are with the School of Electrical and Computer
Engineering, Purdue University, Electrical Engineering Building, 465
Northwestern Avenue, West Lafayette, Indiana 47907-2035.
E-mail: iwoo@purdue.edu, ebertd@ecn.purdue.edu.
. H. Jänicke is with the University of Heidelberg, Im Neuenheimer Feld 368,
D-69120 Heidelberg, Germany. E-mail: heike.leitte@iwr.uni-heidelberg.de.
. K.P. Gaither is with the Texas Advanced Computing Center, University of
Texas at Austin, Research Office Complex 1.101, J.J. Pickle Research
Campus, Building 196, 10100 Burnet Road (R8700), Austin, Texas 787584497. E-mail: kelly@tacc.utexas.edu.
Manuscript received 14 Jan. 2011; revised 1 July 2011; accepted 29 Mar. 2012;
published online 10 Apr. 2012.
Recommended for acceptance by T. Möller.
For information on obtaining reprints of this article, please send e-mail to:
tvcg@computer.org, and reference IEEECS Log Number TVCG-2011-01-0012.
Digital Object Identifier no. 10.1109/TVCG.2012.105.
1077-2626/13/$31.00 ß 2013 IEEE

A typical 1D transfer function widget is shown in
Fig. 1(top). This widget was presented to five novice users
(but domain experts) as part of an informal user opinion
survey. The users were asked to interactively select an area
of interest based solely on the histogram distribution. All
users selected the singular peak of the 1D histogram.
However, as shown in the resultant volume rendering of
Fig. 1(top), the peak in this particular histogram is actually
the least interesting portion of the data. When asked to
further suggest areas of interest, the users indicated that
they would be unsure of where to explore next within this
data. Three users selected various ranges that would be of
interest to them; however, all users indicated that they were
likely to start exploring such data at peak values.
In order to overcome such issues, we propose to enhance
the conventional 1D and 2D histogram transfer function
widgets to better capture attributes and correlations that
exist in the data. To do this, we create a new mapping of the
statistical properties found within the histogram bins and
overlay this as a new visual representation within the
traditional histogram views. Essentially, an underlying
statistical property of the voxels within each histogram
bin is computed with respect to a user selected attribute of
interest. This statistical value is then used to guide
interaction within the transfer function space, providing a
visualization of the abstracted attribute space representation.
No new attribute spaces are being created; instead, the user
is presented with more information about the statistical
relationships between volumetric attributes.
The same five users were presented with the new
abstracted attribute space shown in Fig. 1(bottom). Again,
they were asked to interactively select an area of interest. In
the abstracted attribute space, the users now have a variety
Published by the IEEE Computer Society

MACIEJEWSKI ET AL.: ABSTRACTING ATTRIBUTE SPACE FOR TRANSFER FUNCTION EXPLORATION AND DESIGN

95

Fig. 1. Comparing explorations of the X38 experimental aircraft data using a traditional 1D histogram widget (top) and the pressure skewness
abstracted 1D histogram widget (bottom). In the top image, the user is presented with only the velocity histogram. By selecting the area with the
largest number of voxels, no interesting structures will be visible. By selection, we mean that the red line shown will now map opacity to the
corresponding voxels. In bottom image, the user is presented with the abstracted histogram showing the skewness of the velocity histogram bins
with respect to the pressure distribution in each bin. Here, the user explores the noticeable peak in the data and finds that it corresponds to the vortex
cores in the data. Note that some of these regions correspond to interesting isosurface values that would be less intuitive to select when looking at
the nonabstracted histogram plot. However, the same transfer function design in either widget would result in the same volume rendering.

of peaks to choose from, and many of these peaks map to
interesting features of their data. Fig. 1(bottom) illustrates
the resultant rendering of the most common peak chosen by
the users. Other areas chosen include the leftmost area of
high slope and the rightmost area of high slope.
Such an abstraction does not improve the user’s ability to
separate previously inseparable features. In fact, the traditional 1D histogram widget and the abstracted 1D histogram
widget map the exact same voxels to the same histogram bin.
Thus, if features of the volume (e.g., skin, bone) were to map
to the same density bin in the original histogram, they would
still map to the same bin in the abstracted space. The benefit
of abstracting the histogram space is that it can provide
insight to novice users with regards to the initial creation of
an appropriate transfer function. Furthermore, such an
abstraction is also beneficial for expert users as these
statistics provide more information helping target search
and analysis while exploring relationships between variables
that cannot be shown with the traditional 1D histogram.
For example, extracting, representing, and understanding
the properties and interactions within computational fluid
dynamic (CFD) simulations has generally proven to be a
difficult problem. Scientists are interested in locating critical
points, vortices, shocks, and other attributes within their
data and understanding the effects that various attributes
(temperature, pressure, velocity, etc.) have on the volumetric
flow structure. However, locating and visualizing such
attributes is extremely difficult as the criteria defining flow
features (e.g., shock) are often not well understood, imprecisely defined, and complex to extract volumetrically. For
example, Banks and Singer [4] review eight different
schemes for identifying vortices, and Ma et al. [5] listed
three data properties that indicate the presence of a shock
within the data. Thus, in extracting flow properties there is a
need for enhanced information in understanding the
interactions between the volumetric flow attribute values.

Furthermore, scientists are not only interested in the
volumetric shapes and locations of the attributes, but also
how these attributes interact and influence other attributes.
Examples of questions that scientists may want to ask of
their data are: “What effect do pressure and temperature
have on velocity, and what does this correlation reveal
about the physical domain?” By providing domain experts
with an overlay of statistical relationships between variables in the histogram space, hypotheses can be formed and
insights confirmed.
Thus, our work focuses on the mapping of statistical
properties in combination with the traditional histogram
widgets, thereby providing users with more information
and cues on where they can begin volume exploration, as
well as aiding in both transfer function design and knowledge discovery. The contribution of this work is the
algorithm for generating the abstracted attribute space as
well as the introduction of this space into the traditional 1D
and 2D histogram widgets. By using our algorithm and
modified widgets, users are able to explore the 1D
abstracted transfer function space combining a variety of
attributes (e.g., density, temperature, pressure, or x, y, and z
dimensions simultaneously). Users can also toggle between
the conventional 2D transfer function view, (in which the
entries in the 2D histogram are colored by the number of
voxels that map to a location) and the abstracted transfer
function view, (in which the entries are colored by a derived
statistical relationship). In this manner, we explore the
usefulness of abstracting statistical properties in transfer
function widgets and illustrate the effects on the exploration
of the attribute space. We discuss the benefits and drawbacks of such an approach and illustrate our results across
various volumetric data sets.

2

RELATED WORK

Interactive transfer function design has been addressed
with many different approaches, ranging from simple (yet

96

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

intuitive) 1D transfer functions (e.g., [1], [2]) in which a
scalar data value is mapped to color and opacity, to more
complex multidimensional transfer functions in which color
and opacity are mapped across multiple variables. Early
work by Kindlmann and Durkin [6] and Kniss et al. [3]
applied the idea of a multidimensional transfer function [7]
to volume rendering. This work identified key problems in
transfer function design, noting that many interactive
transfer function widgets lack the information needed to
guide users to appropriate selections, making the creation
of an appropriate transfer function essentially trial-anderror which is further complicated by the large degrees of
freedom available in transfer function editing. While many
volume rendering systems have adopted multidimensional
transfer function editing tools, the creation of an appropriate transfer function is still difficult as the user must
understand the dimensionalities of the attribute space that
they are interacting with.
Recent work on transfer function design has proposed
higher dimensional transfer functions based on mathematical properties of the volume. Examples include the
Contour Spectrum by Bajaj et al. [8] which proposed a user
interface for displaying computed contour attributes using
the surface area, volume, and the gradient integral of the
contour. Work by Kindlmann et al. [9] employed the use of
curvature information to enhance multidimensional transfer functions, and Tzeng et al. [10] focused on higher
dimensional transfer functions which use a voxel’s scalar
value, gradient magnitude, neighborhood information, and
the voxel’s spatial location. Work by Potts and Möller [11]
suggested visualizing transfer functions on a log scale in
order to better enhance attribute visibility. Lundström et al.
introduced the sorted histogram [12], the partial range
histogram [13], and the -histogram [14] as means for
incorporating spatial relations into the transfer function
design. Correa and Ma introduced size-based transfer
functions [15] which incorporate the magnitude of spatial
extents of volume attributes into the color and opacity
channels and visibility-based transfer functions [16] where
the opacity transfer function is modified to provide better
visibility of attributes. Maciejewski et al. [17] proposed a
method to structure attribute space in order to guide users
to regions of interest within the transfer function histogram,
and Bruckner and Möller [18] depicted similarities of
isosurfaces through the use of mutual information theory.
While such extensions enhance the volume rendering and
provide a larger separability of volumetric attributes, they
still fail to provide users with information about the
structures within a given attribute space. In fact, the
addition of more dimensionality into the transfer function
is often automatically incorporated into the rendering
parameters, obscuring the relationship between the volumetric properties and the volume rendering.
Other work has focused on means of better displaying
the data dimensionality to users, aiding the attribute space
exploration. Shamir [19] applied attribute-space cluster
analysis to unstructured meshes in order to automatically
incorporate spatial information for identify structures within the volume. This clustering is performed across a 5D
space (the x, y, and z components of the volume and the
value versus value gradient magnitude attribute space),
where as we only perform this on the attribute space.

VOL. 19,

NO. 1,

JANUARY 2013

However, the goal of Shamir’s work was volume segmentation as opposed to transfer function design and interactive
attribute extraction. Work by Roettger et al. [20] also
generated transfer functions through attribute space analysis. They enable the automatic setup of multidimensional
transfer functions by adding spatial information to the
histogram of the underlying data set. Work by Henze [21]
developed a system in which users can interact with a
variety of attribute space views, interactively brushing and
linking data, thereby allowing the user to define a set of
data conditions over a variety of attributes. Woodring and
Shen [22] proposed a method in which the user is presented
with several different volumetric renderings and is able to
compare values from these data sets over space and time,
and combine various rendering attributes into one volumetric rendering. Akiba et al. [23], [24] utilized parallel
coordinate plots to create a volume rendering interface for
exploring multivariate time-varying data sets. By means of
a prediction-correction process, Muelder and Ma [25]
proposed to predict the attribute regions in the previous
frame, making the attribute tracking coherent and easy to
extract the actual attribute of interest.
In all of this related work, one can note that various
statistical properties of the volumes are being used in order to
extract attributes of interest and segment properties of the
volume. Unfortunately, as the number of dimensions
increases, interaction in n-dimensional space becomes
cumbersome to the point that few systems exceed 2D transfer
functions; instead, the extra dimensionality is incorporated
automatically, somewhat limiting the user’s control. In order
to enhance the information provided in the transfer function
histogram widget, our work focuses on incorporating
statistical properties of user selected attributes into the
projected attribute space domain. Recent work on incorporating statistical and information metrics into time-varying
volumetric data includes work by Fang et al. [26] on the time
activity curve, Jänicke et al. on local statistical complexity
analysis [27], and Haidacher et al. [28] on utilizing information theory for fusing traditional transfer function space with
information enhanced transfer function spaces.
In exploring volumetric attribute space, one can think of
the 2D histogram widget as a special form of a scatterplot
where the points are binned as opposed to plotted
individually. Scatterplots have long been recognized as a
useful tool for multidimensional visualization due to their
relative simplicity and high visual clarity [29], [30], and have
been incorporated in a number of multidimensional visualization toolkits (e.g., Tableau/Polaris [31], GGobi [32],
XmdvTool [33]). While recent work explored methods to
enhance user interactions across the entire attribute space of
a multidimensional data set using scatterplot matrices [34],
little work has been done in regards to incorporating
attribute relationship visualizations into scatterplots. Some
work in that area includes methods by Wang et al. [35] that
introduces an importance-driven time-varying data approach, work in brushing moments by Kehrer et al. [36],
and the statistical transfer-function space work by Haidacher et al. [37]. In Wang et al. [35], a user is presented with
importance curves based on the temporal component of the
data. Other work on analyzing statistical properties within

MACIEJEWSKI ET AL.: ABSTRACTING ATTRIBUTE SPACE FOR TRANSFER FUNCTION EXPLORATION AND DESIGN

97

Fig. 2. The attribute space abstraction pipeline for the 1D histogram case. The algorithm is the same for the 2D histogram case.

transfer function space includes the work by Bachthaler and
Weiskopf. Their work includes a method to create a
continuous scatterplot [38] relating traditional discrete
histograms to the histograms of isosurface statistics and
they extend this work [39] to utilize subdivision within the
spatial domain for creating continuous scatterplots and
show their applications in volumetric rendering. In Kehrer et
al. [36] the authors estimate statistical moments of the data
including the mean, standard deviation, skewness, and
kurtosis and apply this to the analysis of multivariate
scientific data. In Haidacher et al. [37] the authors adaptively
estimate statistical properties including the mean and
standard deviation of the data values within a given
neighborhood. The authors perform similarity tests within
regions to determine if a material within a neighborhood is
homogenous and create a new statistical transfer function
space. Our work is similar to previous work where the user
is presented with a variety of information metrics based on
user defined specifications; however, work by Haidacher et
al. creates a new transfer function space to enable the
distinction of new materials. Our method does not enable
the distinction of new materials. Instead, our method
overlays the relationship of the current attribute being
visualized in the histogram with another user selected
attribute, thus providing fundamentally different views and
functionality to the user.

3

ABSTRACT ATTRIBUTE SPACE GENERATION

Current transfer function design widgets typically present
users with a low-dimensional projection of the volumetric
attribute space onto an interactive one or 2D histogram
widget. These projections represent the magnitude of a
given attribute (i.e., the number of voxels that map to a
given attribute). Attribute selection is then accomplished

through traditional brushing of the histogram, and the
voxels that map to a given attribute are assigned a color and
opacity by the user.
In this work, we propose a modification to the traditional
histogram transfer function design widgets. Instead of
visualizing the attribute space as a function of the
magnitude of the number of voxels, our work utilizes a
novel pipeline that presents users with information metrics
about a given attribute set. We refer to this information
enhanced attribute space as an abstract attribute space. In our
transfer function widget, a user is presented with information about the interactions between attributes within their
data set as opposed to the magnitude of the volumetric
attributes. Previous work has shown that there are links
between the geometric properties of isosurfaces and the
statistical properties of data [40], [41], and our information
enhanced transfer function widgets help aid scientists in
understanding these (and other) links. These tools aid the
users by promoting new insights into volumetric data and
enhance the knowledge discovery process.
Fig. 2 illustrates our abstraction pipeline in the 1D
histogram attribute space case (note that the 2D abstraction
process is exactly the same). Beginning with the volumetric
data, each voxel consists of a set of N measured or derived
attributes, ff1 ; . . . ; fN g. The user chooses any one of these
volumetric attributes, ffj g in Fig. 2 (for example, fj could be
density), and a histogram distribution of this attribute is
created. This histogram is typically found in the 1D transfer
function editor widget common in many volume rendering
systems.
We modify the histogram by taking the voxels that map
to a given bin in the ffj g attribute space histogram and
derive an information metric about that attribute space with
respect to one or more of the remaining attributes in the

98

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

attribute space set, ffk g (for example, fk could be
temperature). Since the voxels corresponding to each bin
in the ffj g histogram are known, we are able to map these
voxels to a 1D distribution with respect to the user defined
attribute of interest, fk . This mapping is never shown to the
user; instead, it is used to derive a set of information
metrics, this information metric can then be presented to the
user as a 1D plot, as shown in Fig. 2. Thus, the user is
presented not with the magnitude of an attribute, but with a
derived set of information about the attribute with respect
to a secondary volumetric feature.
We have also extended our process to the 2D histogram
widget as well. In the case of the 2D widget, the traditional
2D transfer function editor widget utilizes two attributes of
the volume data ffi ; fj g. The original histogram is plotted
such that each bin is colored based on the number of voxels
in each bin. The application of our algorithm will utilize a
third attribute of the volume data ffk g and calculate the
statistical properties of this feature for all voxels found in a
given bin in the 2D histogram. The original histogram is
then redrawn, where the color is now mapped to the
derived statistical property. Examples of both the 1D and
2D abstract attribute space generation are shown throughout this paper.
In this work, we survey the use of four information
metrics: mean, standard deviation, skewness, and entropy.
This section details the information metric computations
used, thereby formalizing the final (calculate information
metric) step in our abstraction pipeline (Fig. 2). This final
step in our pipeline is extensible, and future work will
explore the use of other information metrics for transfer
function design.
All information metrics are calculated as a precomputation step during volume loading and the resulting attribute
space plots can be toggled between in real time. Precomputation time for a 512  256  128 floating point data set takes
approximately 9,093 ms for the combined 2D skewness
calculations, 513 ms for the combined 2D entropy calculations, 97 ms for the combined 1D skewness calculations, and
20 ms for the combined 1D entropy calculations. Note that
the mean and standard deviation calculations are incorporated as part of the skewness calculation. Results are from a
256  256  256 data set on an Intel Xeon(R) CPU E5335 2.00
Ghz machine with 4 GB of RAM. Timings scale linearly with
respect to the number of voxels.

3.1 Mean
Given an entry m in the attribute space ffj g, we define
SV ¼ fV1 ; . . . ; VZ g to be the set of voxels that map to this
given volume attribute. There are Z voxels that will map to
this space. Within SV , we calculate the mean, ðSV Þ, with
respect to the user chosen secondary attribute ffk g
ðSV Þ ¼

1 X
fk ðxÞ:
jSV j x2S

ð1Þ

V

The calculated mean thus depicts the average value of
the fk attribute with respect to a value range for fj .
Furthermore, this mapping provides users with more
information about their data by showing them how a
certain value of attribute fj is related to values in fk .

VOL. 19,

NO. 1,

JANUARY 2013

3.2 Standard Deviation
Once the mean is calculated, we can then determine the
standard deviation, ðSV Þ, with respect to the secondary
attribute ffk g.
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
!ﬃ
u
u 1 X
ðfk ðxÞ  ðSV ÞÞ2 :
ðSV Þ ¼ t
ð2Þ
jSV j x2S
V
The standard deviation of the set, SV with respect to the fk
attribute of the voxels is then mapped back to the
corresponding bin in the ffj g histogram attribute space.
The standard deviation thus depicts the variance of the
fk attribute values corresponding to a particular value of fj .
Areas of low standard deviation typically indicate a high
correlation between the bin range of fj and the average
value of fk found with respect to that bin. Such information
can reveal relationships between data. For example, a
scientist may wish to explore temperature ranges of their
data in which the pressure is highly fluctuating. To do this,
they would interactively select the temperature ranges
where the standard deviation of the pressure is found to be
large. By using this attribute mapping, such ranges can be
immediately found, whereas in the traditional 1D histogram
users would only be unable to see such relationships.

3.3 Skewness
Skewness is a measure of the asymmetry of the distribution
of the underlying data. The skewness of a random variable
is the third standardized moment about the mean and
standard deviation of an underlying data distribution.
Given the set of voxels SV that map to the attribute space
ffj g at m in the attribute space histogram, the skewness,
ðSV Þ, of the voxels at that location with respect to an
attribute ffk g (e.g., velocity, enstrophy) is calculated.
P
3
1
x2SV ðfk ðxÞ  ðSV ÞÞ
jSV j
ðSV Þ ¼ 
ð3Þ
3 :
P
2 2
1
ðf
ðxÞ

ðS
ÞÞ
k
V
x2S
jSV j
V
Our choice of utilizing skewness as an attribute space
abstraction metric is influenced by the work of Patel et al.
[42] in which the authors demonstrate the effectiveness of
using high order statistical moments for transfer function
generation. Skewness may be either positive or negative. In
the 1D transfer function case, we add a secondary axis to
the plots to show positive and negative skewness. In the 2D
transfer function case, we utilize a blue to red color map
with blue values representing negative skewness and red
being positive. Areas with high skewness typically represent inhomogeneity in the data, while areas of low
skewness typically represent regions where the data are
normally distributed implying some underlying structural
homogeneity.

3.4 Entropy
For the entropy calculation, given the set of voxels SV that
map to the attribute space ffj g at m in the attribute space
histogram, the entropy, HðSV Þ, of the voxels at that location
with respect to a user chosen secondary attribute ffk g can
also be calculated

MACIEJEWSKI ET AL.: ABSTRACTING ATTRIBUTE SPACE FOR TRANSFER FUNCTION EXPLORATION AND DESIGN

99

Fig. 3. Exploring the effect of the histogram bin size on abstracted attribute spaces. Here, we show the 1D density gradient histogram of the Bonsai
CT data (in gray) and the skewness of the data (as a black line). As the number of bins increases, the statistical measures are smoothed.

HðSV Þ ¼ 

X

pðfk ðxÞÞ lnðpðfk ðxÞÞÞ:

ð4Þ

x2SV

Here, pðfk ðxÞÞ is the probability that a voxel in the set SV
has a value fk ¼ x. Since SV is known, we can simply use
the one-dimensional histogram of these voxels with respect
to their fk attribute values to calculate the probability
distribution. For example, if three voxels within SV map to
fk ¼ q, then pðfk ¼ qÞ ¼ 3=Z where Z is the total number of
voxels in SV . Once HS ðÞ is calculated, the entropy value is
mapped back to the ffj g histogram attribute space. The
entropy depicted at each position in the abstract attribute
space provides information about the extent of the
corresponding set of voxels.
With regards to spatial attributes (x, y, and z), spatially
isolated structures often define an attribute and low spatial
entropy is well suited to indicate them. High entropy, on the
other hand, is reached with a uniform distribution of voxels
over the physical domain. In this case, the combination of
values is likely to belong to background dynamics and
corresponding positions are often irrelevant for the user.

3.5 Histogram Binning Effects
In creating the abstracted attribute space, the results are
directly related to the voxels that map to a given histogram
bin. Thus, the bin size (or number of bins used) will directly
impact the resultant abstracted attribute space visualization.
In order to understand and explore the impact that
histogram binning will have on our proposed method, we
have modified the number of bins used and compared the
resultant abstracted attribute space plots, as shown in Fig. 3.
As the number of bins decreases, the overall shape of the
skewness curve remains intact; however, minor peaks are
smoothed out and the bin skewness values change.
We have further explored the effects of bin sizes over all
data sets presented in this paper. The shape of all abstracted
attribute space curves remains consistent for all statistical
measures used in our work (i.e., mean, standard deviation,
entropy, and skewness). Overall, the changes to the
abstracted attribute space curves seem to be minimal;
however, we plan to explore adaptive histogram binning
as a means of potentially extracting stronger statistical
correlations.
As with all histograms, there is no such thing as a “best
number” for bin size. Different choices in bin width can
reveal different features about the data; however, the size of
the bin will directly impact our underlying statistical
measures. As the bin width gets smaller, the number of
voxels that maps to each bin becomes sparser. Such sparsity
can bias the data resulting in details that may appear to be

random noise. Conversely, as the bins become larger, more
data are being placed into each bin. In this manner, more
voxels are being grouped together, and features within
the volume will be less separable. Thus, it is important to
choose a reasonable bin width when employing the use of
our abstracted feature space metrics. Preliminary work
indicates that using equal interval bins, where the number
of bins is approximately the square root of the number of
voxels in the volume is a reasonable approach.

4

ATTRIBUTE SPACE EXPLORATION TOOLS

In order to better facilitate volume exploration through
transfer function design, we have developed a small suite of
attribute space exploration tools to complement the traditional interactive transfer function design metaphors. Our
tools include an expanded 1D histogram view, linked 1D
and 2D histogram views for explorations into manual
attribute segmentation, and an opacity brushing tool for
highlighting complex attribute space structures such as arcs.

4.1 Expanded 1D Histogram View
The traditional 1D histogram view consists of the frequency
space of the voxel data with a particular volumetric
attribute (value, value gradient magnitude, etc.) being
assigned to the x-axis of the histogram and the number of
voxels this attribute maps to is assigned to the y-axis. Our
expanded 1D histogram view utilizes this convention and
plots the frequency distribution of both the value and value
gradient magnitude space in a 1D histogram widget tool.
This frequency plot is the gray background plot seen in the
histograms of Fig. 4(left). Note that the left set of histograms
show the value versus frequency and the right set of
histograms show the value gradient magnitude versus
frequency. From top to bottom, the histograms are then
overlaid with a statistical plot representing some spatial
information (in this case the standard deviation) of the
voxels with respect to their x, y, and z positions. User
interaction in any of the three windows will modify
the same transfer function applied to either the value or
value gradient magnitude attribute space. However, only
one transfer function (the value attribute space transfer
function or the value gradient attribute space transfer
function) is applied for volume rendering based on the most
recently edited transfer function.
4.2 Linked 1D-2D Exploration View
While the user may create a 1D transfer function, work has
shown that by increasing the dimensionality of the attribute
space under exploration, the separability of more attributes

100

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 19,

NO. 1,

JANUARY 2013

Fig. 4. Attribute space explorations tools. (Left) A hexa-window 1D histogram view showing the user the standard deviation spatial properties of the
Visible Male Head data set for the x, y, and z directional data (the top, middle, and bottom views, respectively) in the density (left) and density
gradient magnitude (right) attribute space. The gray area is the frequency distribution of the data. The black line is the abstracted attribute space
property. The red line is the user defined transfer function representing the opacity. (Middle-Left) A 2D histogram view of the density versus density
gradient magnitude histogram view with linked regions brushed based on the user defined 1D transfer function widget. (Middle-Right) A user defined
transfer function in the 2D histogram view where opacity brushing was applied to explore the linked region. (Right) The resultant volume rendering
from the opacity brushed transfer function. Note that the 1D transfer function was designed with respect to the z-directional skewness.

becomes possible (although this is highly influenced by the
choice of dimensions). As the user modifies the transfer
function space in the 1D histogram views, a linked 2D
histogram view is created in which the corresponding bins
are highlighted by rectangular bounding boxes so that the
user may explore further details within the 2D attribute
space. This linked view is shown in Fig. 4(Middle-Left). Here,
the white rectangular regions depict the areas of nonzero
opacity brushed by the user in the 1D histogram space.

4.3 Opacity Paintbrushing
Finally, to enhance user interaction, we utilize an opacity
paintbrush tool for the 2D histogram widget similar to the
painting tool described in Co et al. [43]. Traditional
transfer function design widgets for 2D histograms
include drawing a series of rectangular boxes and defining
a color and opacity within the selected region. However,
most structures that appear in the 2D attribute space are
unable to easily fit within a rectangular widget. To
overcome this, we utilize an opacity brushing tool in
which the user has applied some underlying color map to
the data, either automatically as done by Maciejewski et al.
[17] or manually through rectangular widget creation or

through the linked coloring from the 1D transfer function
design. Fig. 4(Middle-Right) demonstrates a user created
transfer function through the use of our opacity brushing
widget. The application of this transfer function for
volume rendering is seen in Fig. 4(Right).

4.4 Scaling by View Direction
In multivariate volumetric data, there are many attributes
that can be utilized in the attribute abstraction process. In
computational fluid dynamic simulations, users may have
variables such as spatial position, temperature, pressure,
vorticity, etc. However, in CT data, the attributes are often
limited to only the density and the spatial position. While
each coordinate of the spatial attribute could be mapped to
its own separate value, this then requires the user to
mentally integrate the three vector components. To handle
this case, our system provides the users with two options:
each component of the spatial attribute (x, y, and z) are
plotted in a separate graph and presented to the user (as
shown in Fig. 4), or the view direction is used to scale the
information vector and the scaled value is shown to the user
(Fig. 5). The scaling of the spatial attribute vector based on
the view direction is defined as follows:

Fig. 5. Exploring CT data using abstracted attribute spaces. The histogram shows the 1D density attribute space of the Engine data. Here, the user
has selected various regions of high skewness for analysis and explores the effect the view direction has on the data skewness. Note that the
underlying histogram distribution is the same in both the right and left image transfer functions; however, the overlaid skewness plot varies based on
the viewing angle. The red line in the histogram widget is the user defined opacity.

MACIEJEWSKI ET AL.: ABSTRACTING ATTRIBUTE SPACE FOR TRANSFER FUNCTION EXPLORATION AND DESIGN

101

Fig. 6. Exploring CT data using abstracted attribute spaces. The density versus density gradient magnitude attribute space of the Bonsai data.

Ax Vx þ Ay Vy þ Az Vz
S ¼ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ;
A2x Vx2 þ A2y Vy2 þ A2z Vz2

ð5Þ

where Ax , Ay , and Az are the information metrics calculated
with respect to the spatial attributes (x, y, and z), and V is
defined as the view directional vector calculated as
V ¼M

1

T

½0 0  1 ;

ð6Þ

where M is the camera matrix.
We illustrate the effect that scaling by view direction has
on the data exploration process in Fig. 5. Here, we explore
skewness as an information metric in CT data utilizing the
x, y, and z components of the voxels in the volumetric space
and calculate the skewness with respect to each direction
for each bin in the 1D histogram. The spatial skewness is
then scaled and normalized based on the view directional
vector of the volume and plotted as a black line on top of
the density histogram plot. In the density 1D attribute space
(Fig. 5(top)), we can see that the skewness provides new
peaks in the data to explore that are not evident in the
traditional frequency plot (the underlying gray histogram).
In Fig. 5(Top-Left) the user explores a peak of particular
interest in the upper range of the density values. As the
viewpoint changes, the user defined transfer function
remains constant; however, the spatial skewness changes
as is seen in Fig. 5(Top-Right). Using the same view point in
Fig. 5(Bottom-Left), we highlight all the skewness plateaus
and explore the resulting visualization. Again, we see that
changing the viewpoint results in a change in the plotted
spatial skewness Fig. 5(Bottom-Right).
Here, the user has interactively selected various areas in
the plots based on either high skewness peaks or rates of
change in the skewness. This process is similar to that
shown in Fig. 1 where the user explores various peaks in
the abstracted space. Our final rendering shows only a
subset of the peaks after exploration was done. From this
we can see that the information metric provides new cues
as to which regions in a data set may be of interest for
visualizing results. Depending on the data alignment,

different information about material boundaries and
segmentation regions can be found. Future work will look
at incorporating these various information metrics as a
means of shading parameters.

5

ATTRIBUTE SPACE EXPLORATION EXAMPLES

In this section, we survey the meaning of our abstract
attribute space representations in the context of the related
volumetric properties. First, we illustrate the effect of using
the abstract attribute space in a traditional CT data set in
order to illustrate the meaning of the attribute space
entropy. Second, we present a case study exploration of
CFD data using the derived abstract attribute space
representations. Finally, we explore the effect of noisy data
on our derived abstract attribute space representations.

5.1 Spatial Abstract Attribute Spaces
5.1.1 Entropy and Standard Deviation
A key step in utilizing the abstract attribute space
representation effectively for volume exploration is understanding the notion of entropy. As stated in Section 3.3,
locations in the attribute space that consist of high spatial
entropy are unlikely to represent structures within the data.
In Fig. 6(Top), the user is comparing the traditional voxel
magnitude-based attribute space representation to the
entropy-based representation. Here, we can quickly see that
the area of the highest entropy maps to that of the highest
number of voxels. As this is likely the air surrounding the
volumetric data, the user chooses to segment the data by
utilizing a square opacity filter in the entropy attribute
space. The underlying colors are based on the nonparametric transfer function generation of Maciejewski et al. [17].
Here, we see that the most spatially coherent volumetric
structures are able to be quickly extracted from the data.
Given the spatial randomness of the leaves, it is likely
that the spatial standard deviation will be able to guide the
user in segmenting the leaves. In Fig. 6(bottom), the user
switches to the spatial standard deviation attribute space. In
the lower right corner, a particularly high band of standard

102

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 19,

NO. 1,

JANUARY 2013

Fig. 7. Exploring the turbulent vortex data using skewness. (Left) The value attribute space of the vortex data where the user has extracted areas of
low skewness in the z spatial direction and the resultant rendering. (Right) The value versus value gradient magnitude attribute space where the user
has selected regions of high skewness within the previously selected 1D transfer function regions and the resultant rendering. The red line in each
1D histogram widget represents the user defined opacity.

deviation is evident to the user. Again, using a combination
of square opacity filters, the user is able to directly segment
the leaves. As such, one can see that by using a combination
of abstract attribute space representations, a user is able to
quickly and effectively explore their volumetric data.
While the use of abstract attribute spaces does allow the
user to find very clean segmentation areas within the
volume, in CT and MRI data, there are known properties of
the density versus density gradient attribute space that
allow users to create transfer functions by selecting the arc
like structures within the attribute space. These arc-like
structures correspond to material boundaries, and these
cues have been shown to aid users in attribute extraction.
However, CFD data do not have well-defined boundaries,
and such methods are unable to extract structures such as
shocks and vortices. Furthermore, when looking at an
arbitrary attribute space, such as Pressure versus Velocity
Magnitude, the relationships between data values may not
be apparent.

5.1.2 Skewness
Fig. 7 illustrates the application of our abstract attribute
space representation when applied to the turbulent vortex
data set. The data set is sized at 128  128  128 and is a
fluid flow simulation that has been used in many studies
[25], [44], [45]. Here, we explore regions of near zero
skewness in the value gradient magnitude attribute space of

the data and the area of high skewness change in the value
magnitude attribute space with respect to the y-directional
component, see the transfer function designs in Fig. 7(left).
Next, we change to the 2D attribute space of value versus
value gradient magnitude and highlight the regions of high
skewness within the bins chosen from the 1D attribute
space interactions, Fig. 7(right). This method allows us to
further refine the visualization that is of particular
importance with turbulent flow simulations. Areas in
turbulent flow where there is a high skew level represent
potential attribute areas and should be more closely
explored. This high skew level highlights change in the
flow. Since turbulent flow attributes can be difficult or
impossible to analytically describe, highlighting regions in
the flow that have attributes of interest is a powerful tool for
gaining insight to these data sets.

5.2

Nonspatial Abstract Attribute Spaces

5.2.1 Skewness
As previously stated, it is often that CFD data does not have
well defined boundaries. Furthermore, scientists are often
interested in relationships between not only position, but
also measured properties, such as temperature and velocity.
Fig. 8 demonstrates the abstraction of the attribute space
with respect to nonspatial properties. In Fig. 8(top), the user
is exploring the relationship between the velocity and the

Fig. 8. Exploring the convection in a box data set with respect to nonspatial attributes (Time stamp 120). (Top) The velocity histogram overlaid with
the velocity gradient skewness abstracted attribute space. (Bottom) The velocity histogram overlaid with the user defined attribute skewness (in this
case the user is exploring the temperature when compared to the velocity). The red line in each 1D histogram widget is the user defined opacity.

MACIEJEWSKI ET AL.: ABSTRACTING ATTRIBUTE SPACE FOR TRANSFER FUNCTION EXPLORATION AND DESIGN

103

Fig. 9. Exploring the formation of a cumulus cloud with respect to the water content and water vapor values. The red line in each 1D histogram widget
is the user defined opacity.

velocity gradient skewness. We create an oscillating transfer
function, the peaks of which map to the skewness peaks in
the gradient, creating an advection rendering similar to
work by Svakhine et al. [46].
In Fig. 8(bottom), the user is exploring the relationship
between the velocity and the temperature skewness. Here,
the user has selected regions of highly variable temperature
(both the highest and lowest range of skewness). The blue
line in the bottom part of Fig. 8 is the skewness of
temperature with respect to velocity. At high velocities,
we can see that the temperature is nonconstant. Visualizing
these values in abstract attribute space allows us to
highlight areas of mixing. When discussing these plots
with a domain scientist, they were able to point out that
convective heat transfer takes place through both diffusion
and advection, each of which can be highlighted in the
abstract attribute space. The domain expert indicated that
such plots were useful to her in exploring the data as it
provided her with details about the underlying data
relations as opposed to only a singular value distribution.
In this case, the domain expert was a transfer function
design expert as well. While the expert had certain value
ranges in mind to explore as part of the transfer function
design phase, the expert indicated that the enhanced
information view presented was valuable for explaining
phenomenon within the data. Furthermore, the expert
indicated that such plots would be useful for detecting
anomalies within simulations.

5.2.2 Standard Deviation
Our next data exploration example utilizes a cumulus cloud
simulation. In Fig. 9, the user is presented with a histogram
of the water content of the cloud. A conventional histogram
(Left) does not provide the user with the information
needed to quickly extract the boundary of the clouds
evolution. However, if the user chooses to overlay this
histogram with the abstracted attribute of the water vapor
(Right), it is easier to extract the precise boundary of the
cloud evolution.
The user analyzes the standard deviation of the change
in water vapor with respect to the water content distribution. From this analysis, the user selects the discontinuity
where the abrupt change in the standard deviation indicates
a movement from regions of highly varying water vapor to
regions of constant water vapor values. The resultant
rendering allows the user to visualize the boundary of the
cloud evolution, showing a boundary where there is

condensation from vapor to water droplets. The discontinuity would be somewhat near the edges of the visible
boundary of the cloud and, as the cloud further evolves, this
area may become quite turbulent. This helps show the
initial entrainment of dry air into the cloud formation.
Using the addition of the water vapor information, the user
indicates that they are better able to match their mental
model of the simulation to the rendering parameters. Here,
the user has a concept of how the water content and water
vapor will interact. By adding this information into the
histogram visualization and transfer function design phase,
the user can feel more comfortable in transfer function
exploration and explore more properties of the data
simultaneously.
Our final CFD example utilizes the X38 data set based on a
tetrahedral finite element viscous calculation computed on
geometry configured to emulate the X38 Crew Return
Vehicle. The geometry and the simulation were computed
at the Engineering Research Center at Mississippi State
University by the Simulation and Design Center. This data
set represents a single time step in the reentry process into the
atmosphere. The simulation was computed on an unstructured grid containing 1,943,483 tetrahedra at a 30 degree
angle of attack. however, for ease of testing in collaboration
with the CFD researchers to guarantee accuracy, we
resampled the data onto a 512  256  128 regular grid.
Fig. 10 illustrates the application of our abstract attribute
space representation when applied to the X38 data set.
Here, the user compares the distribution of pressure values
around the X38 to the underlying air velocity. In discussing
with the domain scientist, we chose to view the standard
deviation of the air velocity with respect to pressure. When
looking at the plots, the domain scientist was interested in
first exploring the high standard deviation of velocity in the
low pressure area. This represented the body and the
vortex cores (both rendered in blue). Next, the domain

Fig. 10. Exploring the x38 data set with respect to pressure and velocity.
The red line in each 1D histogram widget is the user defined opacity.

104

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 19,

NO. 1,

JANUARY 2013

Fig. 11. Comparing the effects of noise on the abstract attribute space. Here, the user is exploring a volumetric data set with a density that
approximates a uniform distribution; however, two small spheres are embedded in this distribution. As expected, the results for entropy, standard
deviation and skewness match that of a uniform random distribution. However, in the 2D standard deviation abstraction, areas of low standard
deviation value become visible indicating areas of interest.

expert also wanted to explore the sharp standard deviation
peak in velocity which appears almost as a discontinuity.
By interactively selecting these two regions, the scientist
can visualize the bow shock, the vortex cores, and the
secondary shocks.
While such properties could have been found while
exploring the pressure histogram alone, the domain
scientist indicated that they like the ability to overlay
secondary attributes on the data. The derived properties
allowed them to visualize the interaction between pressure
and temperature. By seeing these properties matched
together, the scientist is able to relate the transfer function
design to the physical interaction of properties and found
this to be a natural means of analysis.

5.3 Abstracting Attributes in Noisy Data
The final analysis we performed was to explore the effect
that noisy data would have on our abstracted attribute
derivations. For this, we created a 64  64  64 cube
volumetric data set. The data set contained two small
spheres; however, the voxels were distributed within the
data set such that the overall histogram distribution of the
volume density would be a uniform random distribution.
The synthetic data set is shown in Fig. 11.
Fig. 11 explores the derived entropy, standard deviation,
and skewness measures of the x, y, and z spatial attributes
of our synthetic data set. As expected, the resultant entropy
calculations showed a constant high entropy value (the
entropy of the uniform random distribution function will be
the maximum value entropy can obtain). The standard
deviation and skewness also showed properties associated
with a uniform random distribution, with the standard
deviation being a high constant and the skewness being
zero (as the data are symmetrically distributed). Based on
these results, the abstraction of the spatial volumetric
attributes provides little insight into the data; however, this
is as expected given that the underlying data should
approximate a uniform random distribution.
Of interest, though, was the 2D histogram showing the
density versus density gradient values shown in Fig. 11.

When using the standard deviation and the x, y, and z
directional components scaled by the view direction, we are
able to find small areas in the histogram with a very low
standard deviation. From previous examples, areas of low
standard deviation represent compact structure within the
volume. By selecting these areas, a reasonable rendering of
the data can be produced.
Clearly, the application of the abstracted attribute space
is sensitive to noise; however, this example was provided
to highlight the worst case scenario. As the amount of noise
in the data set increases, the amount of information that
can be extracted decreases. Assume you have a volumetric
feature, with property fk ¼ i, where i would be the
histogram bin to which the N voxels making up this
feature of interest map to, for example, a velocity, density,
etc., that is known to map to a volumetric feature. If there
exist M noisy voxels placed randomly in the volume that
also have the property fk ¼ i, then this strategy fails if M is
greater than :5N.
While the results of our abstraction were unsurprising
for the uniform random distribution, we also chose to
explore noisy CT data in order to evaluate the potential
effect of noise. In Fig. 12, we explore a noisy CT foot data
set. From our results, we see that the noise found in this
data does little to affect the resulting attribute statistics.
Moreover, using the spatial skewness close to zero in the 2D
histogram (bottom) allows us to avoid choosing noise in the
data compared to the traditional transfer function (top).
From these experiments, it is clear that noise will affect
the underlying statistical calculations in the abstract
attribute space. However, the degree to which this will
affect the calculations is based on how the noise is
distributed through the entire data set. Our calculations
are performed for each bin in the histogram. For any bin in
which the distribution is approximately normal, our
calculations will return the expected values. However, this
is not to say that such a calculation is meaningless. Our
attribute space abstraction provides insight into another
attribute distribution with respect to value ranges in the
primary attribute being analyzed. In this manner, insight

MACIEJEWSKI ET AL.: ABSTRACTING ATTRIBUTE SPACE FOR TRANSFER FUNCTION EXPLORATION AND DESIGN

105

metrics including interaction information and cross entropy
for measuring attribute complexity. Furthermore, we plan
to utilize abstracted attribute space measures for novel
volume rendering parameters in order to reduce the burden
of transfer function design on the user.

ACKNOWLEDGMENTS
This work has been supported by the US Department of
Homeland Security’s VACCINE Center under Award
Number 2009-ST-061-CI0001 and the US National Science
Foundation (NSF) under Grants 0328984 and 0121288. Yun
Jang has been supported by the Swiss National Science
Foundation under grant 200021_124642.

REFERENCES
[1]
[2]

Fig. 12. Comparing explorations of the noisy CT feet data using a
traditional 2D histogram widget and the spatial skewness abstracted 2D
histogram widget. Here, the user has used a box transfer function on the
traditional 2D transfer function (Top), whereas the user has brushed the
zero skewness to avoid the noise in the data (Bottom).

[3]

can be gleaned from the data as shown in the previous
sections.

[5]

[4]

[6]

6

CONCLUSIONS AND FUTURE WORK

Traditionally, the appropriate selection of attributes in
multidimensional transfer functions is a difficult task, often
requiring the user to have an underlying knowledge of the
data set under exploration. By providing users with
enhanced information about other attributes of their data
in an abstracted attribute space (e.g., density, value versus
value gradient magnitude) we are able to enhance the
exploration, allowing users to better discover attributes
within their data set. Users are able to quickly visualize and
explore relationships within a given attribute space,
enhancing their knowledge about interactions between the
attribute space variables.
Our interactions with domain experts showed that these
users tended to favor the mean and standard deviation
plots as they were more comfortable with those metrics.
Domain experts indicated that they liked the overlay of the
abstracted attribute space, and that the added plot enabled
them to think about transfer function design as more than a
selection of value ranges, but also as a selection of variable
interaction ranges. Experiences with the various data sets
indicates that entropy was the least used statistical measure
for exploration. Skeweness and standard deviation were
useful for showing boundary areas and changes in data
ranges, while the mean value was useful for direct
comparison between two attributes. Often, the mean value
calculation was used by domain experts in comparing
velocity and pressure, water content and water vapor, etc.
Future work will focus on deriving other information

[7]
[8]
[9]

[10]
[11]
[12]
[13]

[14]

[15]
[16]
[17]

T. He, L. Hong, A. Kaufman, and H. Pfister, “Generation of
Transfer Functions with Stochastic Search Techniques,” Proc. IEEE
Conf. Visualization, pp. 227-234, 1996.
J. Marks, B. Andalman, P.A. Beardsley, W. Freeman, S. Gibson, J.
Hodgins, T. Kang, B. Mirtich, H. Pfister, W. Ruml, K. Ryall, J.
Seims, and S. Shieber, “Design Galleries: A General Approach to
Setting Parameters for Computer Graphics and Animation,” Proc.
ACM SIGGRAPH Conf. Computer Graphics and Interactive Techniques, pp. 389-400, 1997.
J. Kniss, G. Kindlmann, and C. Hansen, “Interactive Volume
Rendering Using Multi-Dimensional Transfer Functions and
Direct Manipulation Widgets,” Proc. IEEE Conf. Visualization,
pp. 255-262, 2001.
D.C. Banks and B.A. Singer, “Vortex Tubes in Turbulent Flows:
Identification, Representation, Reconstruction,” Proc. IEEE Conf.
Visualization (VIS ’94), pp. 132-139, 1994.
K.-L. Ma, J. Van Rosendale, and W. Vermeer, “3D Shock Wave
Visualization on Unstructured Grids,” Proc. Symp. vol. Visualization, pp. 87-95, 1996.
G. Kindlmann and J.W. Durkin, “Semi-Automatic Generation of
Transfer Functions for Direct Volume Rendering,” Proc. IEEE
Symp. Vol. Visualization, pp. 79-86, 1998.
M. Levoy, “Display of Surfaces from Volume Data,” IEEE
Computer Graphics and Applications, vol. CGA-8, no. 3, pp. 29-37,
May 1988.
C.L. Bajaj, V. Pascucci, and D.R. Schikore, “The Contour
Spectrum,” Proc. IEEE Conf. Visualization, pp. 167-173, 1997.
G. Kindlmann, R. Whitaker, T. Tasdizen, and T. Möller,
“Curvature-Based Transfer Functions for Direct Volume Rendering: Methods and Applications,” Proc. IEEE Conf. Visualization,
pp. 513-520, 2003.
F.-Y. Tzeng, E.B. Lum, and K.-L. Ma, “A Novel Interface for
Higher-Dimensional Classification of Volume Data,” Proc. IEEE
Conf. Visualization, pp. 505-512, 2003.
S. Potts and T. Möller, “Transfer Functions on a Logarithmic Scale
for Volume Rendering,” Proc. Graphics Interface, pp. 57-63, 2004.
C. Lundström, P. Ljung, and A. Ynnerman, “Multi-Dimensional
Transfer Function Design Using Sorted Histograms,” Proc.
Eurographics/IEEE VGTC Workshop Vol. Graphics, pp. 1-8, July 2006.
C. Lundström, P. Ljung, and A. Ynnerman, “Local Histograms for
Design of Transfer Functions in Direct Volume Rendering,” IEEE
Trans. Visualization and Computer Graphics, vol. 12, no. 6, pp. 15701579, Nov./Dec. 2006.
C. Lundström, A. Ynnerman, P. Ljung, A. Persson, and H.
Knutsson, “The Alpha-Histogram: Using Spatial Coherence to
Enhance Histograms and Transfer Function Design,” Proc. Eurographics/IEEE-VGTC Symp. Visualization, pp. 227-234, May 2006.
C. Correa and K.-L. Ma, “Size-Based Transfer Functions: A New
Volume Exploration Technique,” IEEE Trans. Visualization and
Computer Graphics, vol. 14, no. 6, pp. 1380-1387, Oct. 2008.
C. Correa and K.-L. Ma, “Visibility-Driven Transfer Functions,”
Proc. IEEE-VGTC Pacific Visualization Symp., Apr. 2009.
R. Maciejewski, I. Woo, W. Chen, and D.S. Ebert, “Structuring
Feature Space: A Non-Parametric Method for Volumetric Transfer
Function Generation,” IEEE Trans. Visualization and Computer
Graphics, vol. 15, no. 6, pp. 1473-1480, Nov./Dec. 2009.

106

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

[18] S. Bruckner and T. Möller, “Isosurface Similarity Maps,” Computer
Graphics Forum, vol. 29, no. 3, pp. 773-782, 2010.
[19] A. Shamir, “Feature-Space Analysis of Unstructured Meshes,”
Proc. IEEE Conf. Visualization, pp. 185-192, 2003.
[20] S. Roettger, M. Bauer, and M. Stamminger, “Spatialized Transfer
Functions,” Proc. Eurographics/IEEE-VGTC Symp. Visualization,
pp. 271-278, 2005.
[21] C. Henze, “Feature Detection in Linked Derived Spaces,” Proc.
Conf. Visualization (VIS ’98), pp. 87-94, 1998.
[22] J. Woodring and H.-W. Shen, “Multi-Variate, Time Varying, and
Comparative Visualization with Contextual Cues,” IEEE Trans.
Visualization and Computer Graphics, vol. 12, no. 5, pp. 909-916,
Sept./Oct. 2006.
[23] H. Akiba and K.-L. Ma, “A Tri-Space Visualization Interface for
Analyzing Time-Varying Multivariate Volume Data,” Proc. Eurographics/IEEE VGTC Symp. Visualization, pp. 115-122, May 2007.
[24] H. Akiba, K.-L. Ma, J.H. Chen, and E.R. Hawkes, “Visualizing
Multivariate Volume Data from Turbulent Combustion Simulations,” Computing in Science and Eng., vol. 9, no. 2, pp. 76-83,
2007.
[25] C. Muelder and K.-L. Ma, “Interactive Feature Extraction and
Tracking by Utilizing Region Coherency,” Proc. IEEE Pacific
Visualization Symp., pp. 17-24, Apr. 2009.
[26] Z. Fang, T. Möller, G. Hamarneh, and A. Celler, “Visualization
and Exploration of Time-Varying Medical Image Data Sets,” Proc.
Graphics Interface (GI ’07), pp. 281-288, 2007.
[27] H. Jänicke, A. Wiebel, G. Scheuermann, and W. Kollmann,
“Multifield Visualization Using Local Statistical Complexity,”
IEEE Trans. Visualization and Computer Graphics, vol. 13, no. 6,
pp. 1384-1391, Nov./Dec. 2007.
[28] M. Haidacher, S. Bruckner, A. Kanitsar, and M.E. Gröller,
“Information-Based Transfer Functions for Multimodal Visualization,” Proc. Visual Computing for Biology and Medicine, pp. 101-108,
Oct. 2008.
[29] Dynamic Graphics for Statistics, Statistics/Probability Series,
W.S. Cleveland and M.E. McGill, eds. Wadsworth & Brooks/
Cole, 1998.
[30] The Visual Display of Quantitative Information, E.R. Tufte, ed.
Graphics Press, 1993.
[31] C. Stolte, D. Tang, and P. Hanrahan, “Polaris: A System for Query,
Analysis, and Visualization of Multidimensional Relational Databases,” IEEE Trans. Visualization and Computer Graphics, vol. 8, no. 1,
pp. 52-65, Jan.-Mar. 2002.
[32] D.F. Swayne, D.T. Lang, A. Buja, and D. Cook, “GGobi: Evolving
from XGobi into an Extensible Framework for Interactive Data
Visualization,” Computational Statistics and Data Analysis, vol. 43,
no. 4, pp. 423-444, 2003.
[33] M.O. Ward, “XmdvTool: Integrating Multiple Methods for
Visualizing Multivariate Data,” Proc. IEEE Conf. Visualization
(VIS ’94), pp. 326-333, 1994.
[34] N. Elmqvist, P. Dragicevic, and J.-D. Fekete, “Rolling the Dice:
Multidimensional Visual Exploration Using Scatterplot Matrix
Navigation,” IEEE Trans. Visualization and Computer Graphics,
vol. 14, pp. 1141-1148, Nov./Dec. 2008.
[35] C. Wang, H. Yu, and K.-L. Ma, “Importance-Driven Time-Varying
Data Visualization,” IEEE Trans. Visualization and Computer
Graphics, vol. 14, no. 6, pp. 1547-1554, Nov./Dec. 2008.
[36] J. Kehrer, P. Filzmoser, and H. Hauser, “Brushing Moments in
Interactive Visual Analysis,” Computer Graphics Forum, vol. 29,
no. 3, pp. 813-822, 2010.
[37] M. Haidacher, D. Patel, S. Bruckner, A. Kanitsar, and M.E. Gröller,
“Volume Visualization Based on Statistical Transfer-Function
Spaces,” Proc. IEEE Pacific Visualization Symp. ’10, pp. 17-24, 2010.
[38] S. Bachthaler and D. Weiskopf, “Continuous Scatterplots,” IEEE
Trans. Visualization and Computer Graphics, vol. 14, no. 6, pp. 14281435, Nov./Dec. 2008.
[39] S. Bachthaler and D. Weiskopf, “Efficient and Adpative Rendering
of 2-D Continuous Scatterplots,” Computer Graphics Forum, vol. 28,
no. 3, pp. 743-750, 2009.
[40] H. Carr, B. Duffy, and B. Denby, “On Histogram and Isosurface
Statistics,” IEEE Trans. Visualization and Computer Graphics, vol. 12,
no. 5, pp. 1259-1265, Sept./Oct. 2006.
[41] C. Scheidegger, J. Schreiner, B. Duffy, H. Carr, and C. Silva,
“Revisiting Histograms and Isosurface Statistics,” IEEE Trans.
Visualization and Computer Graphics, vol. 14, no. 6, pp. 1659-1666,
Nov./Dec. 2008.

VOL. 19,

NO. 1,

JANUARY 2013

[42] D. Patel, M. Haidacher, J.-P. Balabanian, and M.E. Gröller,
“Moment Curves,” Proc. IEEE Pacific Visualization Symp. ’09,
pp. 201-208, Apr. 2009.
[43] C.S. Co, A. Friedman, D.P. Grote, J.-L. Vay, E.W. Bethel, and K.I.
Joy, “Interactive Methods for Exploring Particle Simulation,” Proc.
Eurographics/IEEE-VGTC Symp. Visualization ’05, pp. 279-286, 2005.
[44] D. Silver and X. Wang, “Tracking and Visualizing Turbulent 3D
Features,” IEEE Trans. Visualization and Computer Graphics, vol. 3,
no. 2, pp. 129-141, Apr.-June 1997.
[45] J. Woodring, C. Wang, and H.-W. Shen, “High Dimensional Direct
Rendering of Time-Varying Volumetric Data,” Proc. IEEE 14th
Visualization (VIS ’03), p. 55, 2003.
[46] N.A. Svakhine, Y. Jang, D.S. Ebert, and K.P. Gaither, “Illustration
and Photography Inspired Visualization of Flows and Volumes,”
Proc. IEEE Conf. Visualization, pp. 687-694, 2005.
Ross Maciejewski received the PhD degree in electrical and computer
engineering from Purdue University in December, 2009. He is currently
an assistant professor at Arizona State University in the School of
Computing, Informatics & Decision Systems Engineering. Prior to this,
he served as a visiting assistant professor at Purdue University and
worked at the Department of Homeland Security Center of Excellence
for Command Control and Interoperability in the Visual Analytics for
Command, Control, and Interoperability Environments (VACCINE)
group. His research interests are geovisualization, visual analytics and
nonphotorealistic rendering. He is a member of the IEEE and the IEEE
Computer Society.
Yun Jang received the bachelor’s degree in electrical engineering from
Seoul National University, South Korea, in 2000, the master’s and
doctoral degrees in electrical and computer engineering from Purdue
University in 2002 and 2007, respectively. He is an assistant professor
of computer engineering at Sejong University, Seoul, South Korea. He
was a postdoctoral researcher at CSCS and ETH Zurich, Switzerland
from 2007-2011. His research interests include interactive visualization,
volume rendering, and data representations with functions.
Insoo Woo received the BS degree in computer engineering in 1998
from Dong-A University in South Korea and working toward the PhD
degree in the School of Electrical and Computer Engineering at Purdue
University. He is a research assistant in Purdue University Rendering
and Perception Lab. He was employed as a software engineer during
1997 to 2006. His research interest is GPU-aided Techniques for
Computer Graphics and Visualization.
Heike Jänicke received the MS degree (Diplom) in computer science
with a special focus on medical computer science in 2006 from Leipzig
University. For her research of information-theoretic methods in
visualization, she was awarded the PhD degree in 2009 from the same
university. During 2009 and 2010, she worked as a postdoctoral
researcher at Swansea University. Since March 2010, she is working as
a junior professor for computer graphics and visualization at Heidelberg
University, Germany. Her research interests include data analysis
methods from statistics and information theory, visualization of
unsteady, multivariate data with applications in biology, medicine,
astronomy, and climate research. She is a member of the IEEE.

MACIEJEWSKI ET AL.: ABSTRACTING ATTRIBUTE SPACE FOR TRANSFER FUNCTION EXPLORATION AND DESIGN

Kelly P. Gaither received the bachelor’s and master’s degrees in
computer science from Texas A&M University in 1988, and 1992,
respectively, and the doctoral degree in computational engineering from
Mississippi State University in May, 2000. While obtaining the PhD
degree, she worked full time at the Simulation and Design Center in the
National Science Foundation Engineering Research Center as the
leader of the visualization group. She is the director of Data &
Information Analysis at the Texas Advanced Computing Center (TACC)
and leads the scientific visualization, data management & collections,
and data mining & statistics programs at TACC while conducting
research in scientific visualization and data analysis. She, a research
scientist, also serves as the area director for visualization in the National
Science Foundation funded TeraGrid project. She has a number of
refereed publications in fields ranging from Computational Mechanics to
Supercomputing Applications to Scientific Visualization. She has given a
number of invited talks. Over the past 10 years, she has actively
participated in the IEEE Visualization conference, and served as the
IEEE Visualization conference general chair in 2004. She is currently
serving on the IEEE Visualization and Graphics Technical Committee.
She is a member of the IEEE and the IEEE Computer Society.

107

David S. Ebert received the PhD degree in computer science from Ohio
State University. He is a professor in the School of Electrical and
Computer Engineering at Purdue University, a University Faculty
scholar, director of the Purdue University Rendering and Perceptualization Lab, and director of the Purdue University Regional Visualization
and Analytics Center. His research interests include novel visualization
techniques, visual analytics, volume rendering, information visualization,
perceptually based visualization, illustrative visualization, and procedural abstraction of complex, massive data. He is a fellow of the IEEE and
the IEEE Computer Society, and a member of the IEEE Computer
Society’s Publications Board.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

Integrating Predictive Analytics and Social Media
Yafeng Lu, Robert Krüger, Student Member, IEEE, Dennis Thom, Feng Wang,
Steffen Koch, Member, IEEE, Thomas Ertl, Member, IEEE, and Ross Maciejewski, Member, IEEE
Abstract— A key analytical task across many domains is model building and exploration for predictive analysis. Data is collected,
parsed and analyzed for relationships, and features are selected and mapped to estimate the response of a system under exploration.
As social media data has grown more abundant, data can be captured that may potentially represent behavioral patterns in society.
In turn, this unstructured social media data can be parsed and integrated as a key factor for predictive intelligence. In this paper, we
present a framework for the development of predictive models utilizing social media data. We combine feature selection mechanisms,
similarity comparisons and model cross-validation through a variety of interactive visualizations to support analysts in model building
and prediction. In order to explore how predictions might be performed in such a framework, we present results from a user study
focusing on social media data as a predictor for movie box-ofﬁce success.
Index Terms—Social Media, Predictive Analytics, Feature Selection

1

I NTRODUCTION

Research on social media has intensiﬁed in the past few years as it is
seen as a means of garnering insight into human behaviors. The unstructured nature of social media data also provides unique challenges
and opportunities for researchers across a variety of disciplines. Businesses are tapping into social media as a rich source of information for
product design, relations management and marketing. Scientists utilize social media data as a platform for developing new algorithms for
text mining (e.g., [13]) and sentiment analysis (e.g., [45]) and focus
on social media as a sensor network for natural experimentation for
exploring social interactions and their implications (e.g.,[47]).
In using social media as a sensor network, researchers have developed methods that capture online chatters about real world events
as a means of predictive model building. For example, work by Culotta [12] explored the use of Twitter for predicting seasonal inﬂuenza.
Tumasjan et al. [43] found that the magnitude of Twitter messages
was strongly correlated to German elections. Eysenbach [15] utilized
regression modeling of Tweet counts to predict paper citations, and
Zhang et al. [48] explored mining Twitter for emotions and predicting
the opening-value of the stock market.
Currently, the visual analytics community has begun focusing on
social media analytics with respect to developing tools and frameworks to collect, monitor, analyze and visualize social media data.
Studies have ranged from geo-temporal anomaly detection (e.g., [9])
to topic extraction (e.g., [46]) to customer sentiment analysis (e.g.,
[33]). Such work focuses on capturing the incoming streams and enables the analysts to perform exploratory data analysis. However, little
work has been done on developing tools for predictive analytics using
social media. In 2013, the Visual Analytics Science and Technology
(VAST) conference ran the VAST Box Ofﬁce challenge using social
media data to predict the opening weekend gross of movies. This particular contest served as an entry point to explore how users interact
with visualization tools to develop predictions. Continuing from this
contest, our work has focused on utilizing movie data from social media to explore the promises and pitfalls of visualization for predictive

• Yafeng Lu, Feng Wang, and Ross Maciejewski, are with
Arizona State University. E-mail: {lyafeng, fwang49,
rmacieje,}@asu.edu.
• Robert Krueger, Dennis Thom, Steffen Koch, and Thomas Ertl are with
University of Stuttgart, Germany. E-mail:{Robert.Krüger, Dennis.Thom,
Steffen.Koch, thomas.ertl}@vis.uni-stuttgart.de.

IEEE Symposium on Visual Analytics Science and Technology 2014
November 9-14, Paris, France
978-1-4799-6227-3/14/$31.00 ©2014 IEEE

analytics. Unlike more specialized data sources (e.g., criminal incident reports, emergency department data, trafﬁc data, etc.), movie data
lends itself well to analyzing visual analytics modules as many casual
users think of themselves as movie domain experts.
In this paper, we present a framework for social media integration, analysis and prediction. This framework consists of tools for
extracting, analyzing and modeling trends across various social media
platforms. In order to test our framework, we focus on the speciﬁc
problem of predicting the opening weekend box-ofﬁce gross of upcoming movies. This system integrates unstructured data from Twitter and YouTube with curated data from the Internet Movie Database
(IMDB). Temporal trends and sentiment are extracted and visualized
from social media, and IMDB features can be explored through parallel coordinate plots. Speciﬁcally, this tool was developed to support
the exploration of predictive models while integrating user interaction
to iteratively update the models, compare against past models, and explore similarities between movies. To demonstrate the efﬁcacy of our
system, we tested our framework with seven subjects and evaluated
their prediction performance. We present lessons learned and future
directions for improving the user in the loop workﬂow for predictive
analytics.
2

R ELATED W ORK

This paper focuses on enabling analysts to explore, validate and ﬁlter
social media data for predictive analytics. In this section, we discuss
past work on current state-of-the-art in visual analytics surrounding
both social media data and predictive model development.
2.1

Visual Analytics of Social Media Data

Recent visual analytics systems for social media analysis include
Whisper [8], which focused on information propagation in Twitter,
SensePlace2 [28], which focused on the analysis of geographically
weighted Tweets, and TweetXplorer [31] which combined geographical visualization of Tweets along with their social networks. Other
applications have explored the use of social media analytics for improving situational awareness in emergency response. Thom et al. [42]
and Chae et al. [9] developed spatiotemporal visual analytics systems
that integrated various social media data sources for anomaly event detection and disaster management. Our proposed framework takes cues
from this previous work and is developed to integrate data from multiple sources, for our case study, we integrate Twitter, YouTube and
IMDB data.
A wide variety of work also exists with regards to social media topic
extraction and sentiment analysis of social media. Dou et al. [13] developed an algorithm for hierarchically organizing news content based
on topic modeling. Hao et al. [18] applied topic based stream analysis
techniques to detect sentiment in Tweets and created a sentiment cal-

193

Fig. 1: Front Page of the Frozen Weekend. View (a) is the Tweet and Youtube comments line. The solid lines indicate the number of Tweets
per day starting 14 days before the release (x-axis). The left y-axis indicates the number of Tweets.The dashed lines represents the number of
Youtube comments per day using the right y-axis. Each color represents one movie. Clicking the legend highlights the corresponding trend
line. View (b) is the opening weekend gross bar graph. The left bar indicates the real gross while the right bar indicates the baseline model’s
prediction. View (c) shows the Tweets and users.

endar and map. Nguyuen et al. [33] applied machine learning to Twitter to extract sentiment and compare dictionary based and machinelearning sentiment classiﬁers. Wang et al. [45] created a sentiment
analysis and visualization system called SentiView to analyze public
sentiment in Tweets and BlogPosts. Similar to previous work [24, 27],
our framework also performs sentiment analysis on the ingested social
media data. However, while previous work relies directly on automatic
algorithms, we allow the users to interactively modify the sentiment of
an item (e.g., a Tweet) as a means of correcting for classiﬁcation errors. Overall, our framework builds upon prior visual analytics work
with regards to social media analytics and expands this domain with
regards to integrating predictive analysis and model building tools.
2.2 Predictive Analytics
It is important to note that our proposed framework is not the ﬁrst
to address predictive analytics. A variety of solutions exist for both
novice and expert users (e.g., R [37], SAS [39], Weka [17], JMP
[36], Excel). These software packages and tools provide a variety of
machine learning algorithms that can be used for predictive analytics tasks, such as feature selection, parameter optimization and result
validation. Many of these systems offer basic visualizations including residual plots, scatterplots and linecharts. However, most of their
visualization are only used to display the ﬁnal results and do not provide interactive means for manipulation, feature selection or model
reﬁnement; instead, these systems often opt to show baseline models
or simple statistical measures for result validation, working as more
of a black-box system. The goal of our framework is to directly integrate the analyst into the model building loop by enabling feature
selection for model building and comparison. We include tools such
as Parallel Coordinate Plots [21] and correlation rankings for quick
comparison. Moreover, we have also created a variety of mechanisms
for automatically suggesting similar instances within a dataset to enable the analyst to identify outliers and validate models based on the
accuracy of prediction with regards to similar instances.
Recently, researchers in the visual analytics community have been
developing methods for improving model building and predictive an-

194

alytics. Berger et al. [5] used regression models for parameter space
exploration. Choo et. al. [10] provided a classiﬁcation system, iVisClassiﬁer, using linear discriminant analysis to reduce dimensionality
for improved data classiﬁcation. Brown et al. [7] designed an interactive visual analysis system to improve clustering results by updating
the distance function based on users’ feedback to the display. We also
integrate feature selection and sample ﬁltering, but our system does
not require users to be familiar with speciﬁc prediction algorithms. Instead, we focus on how much information and manipulation should be
open to the user [2].
Most closely related to our work is that of Mühlbacher et al. [32]
which developed an interactive visual framework for selecting subset
features to improve regression models. They used R2 to rank 1D features and 2D feature pairs, as well as a partition-based feature ranking.
Their goal is to approximate the local distribution of a given target,
and their visual analysis method helps to select subset features for regression models and validate the quality of models. Similar to their
measure of selecting features, we also use a goodness-of-ﬁt measure.
Furthermore, we allow users to explore the correlation between features by using Parallel Coordinate Plots (PCP) because a good subset
of features should also avoid multicorrelation [30]. Mühlbacher et al.
also provides two general partitioning methods: domain-uniform and
frequency-uniform. In our framework, local pattern detection is provided through brushing data items on any dimension from the PCP. We
also allow users to choose to only train on brushed data items. Thus
local patterns can be indicated by the goodness-of-ﬁt of the model.
Since we enable users to select different features and training sets,
we also allow for multiple model creation and comparison. This is akin
to the Delphi method [34, 38] which has multiple experts forecast and
modify their prediction iteratively by comparing to other experts’ predictions before ﬁnalizing their results. In general, the Delphi method is
used to obtain the most reliable consensus of group opinions. Our predictive analytics framework uses the concept from the Delphi method
to allow users to make their prediction after building and exploring
multiple models in multiple rounds. Similar to the Delphi method, in
our system the user evaluates results, where each model represents one

expert or one round of the expert’s prediction.
3

F RAMEWORK

FOR

P REDICTIVE S OCIAL A NALYTICS

Our framework focuses on integrating multi-source data from social
media for analysis and prediction. We combine trend analysis, sentiment analysis, similarity metrics and feature selection for model building, evaluation and prediction. In order to evaluate this framework,
we deploy our tools to the problem of weekend box-ofﬁce prediction.
We combine data from IMDB, Twitter and YouTube and explore this
data across a variety of visual analytics modalities. The system was
built using D3 [6], JSON, R [37] and WEKA [17]. The use of R
and WEKA allowed for direct integration of multivariate regression
and support vector machines, while D3 was used to create charts and
graphics for the interactive visualization. A client-server architecture
was chosen in order to allow easy portability and testing of the system across platforms, and we also explored the use of Amazon cloud
services. We used the Jersey RESTful web service [22] to enable the
interface between the web interface and backend server. Preprocessing was done for sentiment analysis and word frequency counts and
nearly-interactive rates are obtained for visualizing the data described
below. By nearly-interactive, we mean that if the data is cached, the
visualizations can be updated at greater than 10 frames per second
(FPS), if the data is not cached then the user will see a wait symbol
and typically experience a 5 second lag on the ﬁrst query, after which
the exploration of that movie’s features will be at interactive frame
rates.
3.1

Data Description

In data representation and exploration, we focused on views for social
media data sources, such as Twitter and Youtube. As Twitter data is
unstructured and dirty, it requires a deeper preprocessing and manipulation before extracting high quality features.
Twitter: We collected Tweets for 112 movies released since 2013.
Tweets are collected based on the hashtag posted by a movie’s ofﬁcial
Twitter account. In all we have 2.5 million Tweets and each Tweet
includes the posting time, retweet status, user proﬁle information and
Tweet text sentiment.
Youtube: We used a rule-based script to collect Youtube data which
contains the total viewcount and timestamps. We then calculate a
range of features such as comment volume and interpolated view
counts prior to the opening weekend. Overall we were able to collect about 7 million YouTube comments for 1104 movies.
The Internet Movie Database: The Internet Movie Database (IMDB)
has more than 2.8 million entries (Mar. 25, 2014) with each entry consisting of hundreds of features [1]. To deal with data noise and incompleteness, the available raw text IMDB data ﬁles were ﬁrst converted
into an SQL-database using JMDB [44]. The data then undergoes a
data cleaning procedure.Challenges include the data sparseness and
huge number of nominal values, such as cast names, which hamper
machine learning. To overcome the data sparseness we calculated numeric values on a per-movie basis by aggregating gross incomes and
ratings of previous movies that the cast of a new movie was involved
in. Finally, we obtained a high quality movie data set of approximately
2000 movies with up to 72 features per movie.
3.2

Social Media Visual Analytics

Our framework consists of a variety of views and analytical components. We provide an overview for quick trend analysis and exploration, detailed views for exploring tweet sentiment, and a similarity
widget for overviews on related movies and their patterns. A core component of this framework is an iterative feature selection and model
exploration module for analysis, model building and comparison.
3.2.1

Overview: Trend Analysis

When beginning analysis, users are initially presented with an
overview of the data item they are trying to predict (in the case of

(a) Bubble Plot

(b) Sentiment Wordle

Fig. 2: (a) A Tweet bubble plot where blue represents positive sentiment and red represents negative. The size of the bubble represents the
number of times a Tweet has been retweeted, the x-axis is time, and
the y-axis is the number of followers that Tweeter has. (b) A sentiment
wordle where the word size represents the number of times it was used
in a Tweet and color represents sentiment.

our example, it is an overview of the movies being released in the upcoming weekend). Figure 1 shows the initial view in our web enabled
system. Here, the weekend under exploration is from November 27,
2013. Figure 1(a) is a dual y-axis line chart showing the volume of
Tweets and YouTube comments that have been collected relating to
the movies. Users can highlight data elements by clicking on their
corresponding legend entry. Key to this view is the fact that the multiple sources of data enable cross-validation. Due to the limits of the
Twitter Streaming API, it is often the case that the Tweet stream will
consist of missing data. However, there are many instances in which
the YouTube comment trafﬁc directly tracks that of the Twitter stream
(just at different magnitudes as evidenced by the axis scales). In this
manner, the analyst can quickly validate the accuracy of a source and
determine what anomalies might be present.
In Figure 1(b), the user can also get an overview of a baseline linear
regression model prediction for that weekend. Since data for the opening box ofﬁce gross has already been collected for historical weekends, the user is also shown the actual box ofﬁce value. In this manner
the analyst can quickly gain insight into the limitations of a proposed
model. The buttons beneath the bar charts allow the user to directly
navigate to a detailed view of the movie where visualizations showing
word frequency, retweet count, Twitter followers and Tweet sentiment
can be explored.
Figure 2 shows two of the detailed views, a temporal bubble plot
and a sentiment wordle view. The bubbles in Figure 2(a) represent
an individual Tweet and are colored based on the mined sentiment,
where each Tweet has been processed using a dictionary based sentiment analysis, SentiWordNet [3]. This assigns each word in the Tweet
to a score ranging from -1 to 1, negative to positive sentiment respectively. Each Tweet is then assigned an overall sentiment score by summing the sentiment of all words in the Tweet and then normalizing the
sum. A blue color indicates positive sentiment while red indicates negative. The size of the bubble represents the number of times a Tweet
was retweeted while the height on the y-axis indicates the number of
followers the Twitter account has. The x-axis represents time. Similarly, all the Tweets related to a movie are converted into a wordle
(Figure 2(b)), where the size of each word represents the number of
times the word appears in the movie data set and the color represents
the sentiment of the word. From this view, users can quickly ﬁlter for
Tweets with particular keywords and they can modify the sentiment
value in cases where the dictionary matching is wrong (for example,
cases where the Tweet says “I want to see Frozen so bad!” will be a
negative Tweet when in reality the sentiment is positive). Future work
will deploy more machine learning techniques to allow for interactive

195

Fig. 3: Feature Selection page with Frozen as an example. View (a) is a Feature Selection table having four groups of features with their
correlation to revenue mapped to a divergent color scheme. View (b) is a Parallel Coordinate view with the ﬁve most similar movies highlighted
in red. View (c) lists the ﬁve most similar movies suggested by the system based on features in the PCP.

Tweet labeling for advanced sentiment classiﬁcation and analysis.
3.2.2

Feature Analysis and Selection

While the overview and detail visualizations enable exploratory data
analysis, the key contribution of our work is the interactive modeling
and prediction components. Feature values of movies can give insights
and hints about their box ofﬁce success. Moreover, they can be used
as predictors for a movie’s opening weekend revenue. Using Twitter,
Youtube and IMDB data sources, we extracted four groups of features
for model building with 119 features listed in the Feature Selection
Table (Figure 3). Given the large number of features, it is necessary
to provide the users with a suitable starting point for analysis. As
such, we utilized known predictive features for movie analysis from
previous work [41] (e.g., budget, number of screens the movie opens
on, etc.). Thus, when the users begin their exploration process, they
are presented with a baseline model to compare against. Other options
would include integrating automatic feature selection as an entry point
for analysis (e.g., [26, 49]).
Our goal was to augment model building by adding tools for a user
to modify and explore various features. In order to quickly enable this
exploration, our Feature Selection Table (Figure 3(a)) utilizes a variety
of interactions and visual overlays. First, for the candidate movie being predicted (in this case Frozen), features which are not available are
grayed out. Second, each of the columns in the feature selection table
provides the details of a movie. The ﬁrst three columns include information on the feature’s name, the correlation to the revenue, and the
candidate movie’s value. These columns can be automatically sorted
from high to low or low to high simply by clicking on the column
header. The Revenue Correlation column is also color coded to directly highlight correlated features. A myriad of work has been done
in feature selection [29, 35, 40] and correlation is traditionally used
as one of the major factors in feature selection. A high correlation
of a feature to the response variable (in our case the movie revenue)
indicates that this feature could greatly impact the model. We use a
green to red divergent color scale [19] where green represents a high
absolute value of correlation and red represents a low value of correlation, with .5 being the midpoint value. Although correlation here
is univariate (meaning we do not show correlations between multiple
features) and non-linear dependencies are not taken into account, it
still provides important information to users for feature detection and
analysis.

196

The ﬁnal two columns in the Feature Selection Table are associated
with the Parallel Coordinate plot visualization and the model training
data selection. The “Show in PC” column, when selected, will add
that feature as an axis of the Parallel Coordinate Plot. The “Use in
Training” column, when selected, will add all data elements that contain all of the features selected into the training set. To quickly see
what features have been selected, the analyst can sort the features by
clicking the column header. When features are selected, the footer information about the Feature Selection Table will update and tell the
user how many features have been added to the training set, as well
as the amount of movies that exist having all of these features. In this
manner, the analyst can determine how many data elements can be
used to train a model and they can quickly make decisions about the
tradeoff between the use of more features or more training samples.
For example, if a user chooses to select a Twitter feature, only 112
movies in our data set have associated Twitter data. Thus, the number
of elements in the training set decreases. However, Twitter data may
have a high correlation to the opening weekend gross. As such, the
analyst can actually build multiple models with multiple features for
training and analysis.
Another way to select the training data is through interaction with
the parallel coordinate plot view. Let us consider the case in which a
user has sorted the features by correlation to revenue, selected some
features with higher correlation to the gross, and selected features that
he/she suspects are important. These selected features can now be
further explored in the PCP view (Figure 3(b)) by simply activating
the “Show in PC” cell in the corresponding table row. Referring to
the candidate movie’s value, shown in the fourth column, the user can
further ﬁlter out movies far away from this value in the PCP view.
Figure 3(b) shows features of the movie “Frozen” with highly correlated features in different group and the movie’s genre, “Family”.
Pairwise correlations between features are explored in the PCP view.
For example, the WeekendScreens (the number of screens in which
a movie was released during its opening weekend) and the oneWeekBeforeReleeaseAVG (the daily average number of Tweets that are related to a movie one week before its opening) variables are correlated.
These axes can be dragged and dropped to explore more pairwise dimension correlations so that an analyst can choose features with low
multi-correlation in order to improve the model performance. Users
can then interactively select ranges on each axis to ﬁlter the data and
can select an option to train the model using only the selected data.

(a) Similarity by Sentiment Wordle

(b) Similarity by Youtube Trailer Trend

Fig. 4: Similarity Widget View with Frozen. (a) is the top two most similar movies’ wordle view with the Tweet sentiment wordle as the
similarity criteria. Each wordle consists of the top 200 sentiment words. (b) is the top two most similar movies’ line chart view with the 1 year
Youtube Trailer view count trend as the similarity criteria.

The PCP view can also be used to generate insight into the data.
For example, by brushing and selecting only Family movies using the
Boolean genre feature “Family,” one can deﬁne the training set to be
only those movies that are considered to be “Family” movies. Moreover, the PCP view allows the analyst to select a primary axis, this
selection deﬁnes the feature on which we base the PCP line color
scheme. For example, if we color the lines based on the genre axis
“Family” we can see that family movies rarely obtain a very high
gross. From there, the user could train the model for only Family
movies or could look for genre crossover movies such as Family and
Animation.
The ﬁnal item in our Feature Analysis and Selection widget is the
“Top 5 Similar Movies by PCP Features” view, Figure 3(c). Given the
feature vector corresponding to the features selected in the parallel coordinate plot, our system automatically calculates a Euclidean distance
metric between the candidate movie and all other movies that appear
in the PCP view. The ﬁve movies with the smallest Euclidean distance
are then summarized in a tabular view.
3.2.3

Similarity Widget

While the Feature Analysis and Selection Tools show the top 5 most
similar movies, we have also developed a series of tools for enabling
users to explore temporal and sentiment similarities with regards to
social media trends and speciﬁc feature similarities such as genre and
ratings. Figure 4 shows our similarity widget page. Items in this similarity view focus primarily on similarity across social media (as opposed to the previous widget which used a Euclidean distance metric across many features, this view is a pairwise feature similarity).

The left side of Figure 4 shows the various similarity options provided
while the center view displays line charts or wordles depending on the
selection. We have ten predeﬁned metrics and one “Make Your Own
Similarity” option. The rightmost area shows the model predictions
and the actual weekend gross for similar movies via a bar graph.
This widget enables analysts to quickly ﬁnd and compare the accuracy of predictions based on various criteria of similarity, and to perceive if the given prediction model typically underestimates, overestimates or is relatively accurate with regards to movies that the analyst
deems to be similar. In this manner, a user can further reﬁne their ﬁnal
prediction value. In this work, we have deﬁned ten similarity criteria with distance calculation methods focusing on matching temporal
trends through sequential normalization or Euclidean distance metrics
for magnitude comparisons. In all similarity matches, we show the top
ﬁve most similar movies. These views allow users to directly compare
Tweet trends and sentiment words between movies deemed to be similar in a category. Figure 4 contains snapshots from Frozen’s similarity
page cropped to the top two most similar movies by Sentiment Wordle
and Youtube Trailer Comments.
Though similarity metrics used in this page are not directly transformed into modeling features, by providing an analyst with insight
into these secondary variables, coupled with the model performance
with similar movies included in the training set, further reﬁnement of
the prediction is made possible. For example, an analyst may compare the absolute difference between Tweets/Youtube comments of
two movies, or they can inspect the trend of the Tweets through line
chart comparison using the Tweets Changing Trend similarity metric.
This tool also allows users to quickly compare the current movies un-

197

Fig. 5: Multiple Method Modeling with Frozen as the candidate movie. View (a) is a trackable Model History Table recording each model the
user built. View (b) is the scatterplot of the Actual vs. Predicted Gross showing a model’s prediction for each movie in the training set and the
prediction result together with a stable range for the candidate movie. View (c) is the bar graph of the Model Prediction Comparison having
each model’s prediction stacked. View (d) lists the ﬁve most similar movies as was done in View (c) of the Feature Selection page.

der analysis to recently released movies with the same MPAA rating
and genre. When the user builds a model involving Twitter features,
the top 5 most similar movies listed in the Feature Selection and the
Explore Models page can be compared in the similarity page.

3.3.1

Base Line Model

We used the model proposed in our VAST Boxofﬁce Challenge 2013
submission [27] as our base line model, which is described as follows:
OW = β0 + β1 T BD + β2 Budget + ε

3.3

Model Building, Analysis and Veriﬁcation

Based on recent literature and the general use of prediction models, we
support the creation of three different types of models: Support Vector
Machine (SVM) [11], Linear Regression (LIN) [30] and Multilayer
Perceptron (MLP) [20]. Using the linear regression model with the
budget and the average number of daily Tweet (TBD)s for a movie
as regressors and the opening weekend gross as response, the system
provides users with a baseline prediction result together with a 95%
conﬁdence interval for each movie. The baseline model results are
shown in both the front page (see Figure 1(b)) and the similarity page’s
right-hand bar graphs (Figure 4).
Besides exploring the baseline model, the user can build a more
complex model, bringing in domain knowledge and analytic insights.
For instance, the user is allowed to interactively set up parameters and
build models with different feature sets, training instances (movies)
and model types. We use several error measures to give the analyst
feedback about the quality of ﬁt and the prediction stability. By using
the interactive Feature Selection and Explore Models pages, the user
can iteratively change the features, training sets and model types to
improve a model’s quality. We measure the model’s accuracy using the
adjusted R2 , denoted R2ad j . Using R2ad j has the following advantage:
R2 never decreases when a regressor (feature) is added to the model,
regardless of the value of the contribution of that variable; however,
R2ad j will only increase when adding a variable to the model if the
addition of the variable reduces the residual mean square. Otherwise
R2ad j decreases when adding terms that are not helpful [30]. With a
feature set of size p and a number of instances (movies) n, R2ad j is
deﬁned as:
SSRes /(n − p)
R2ad j = 1 −
(1)
SST /(n − 1)
where SSRes is the sum of squares of the residual, and SST is the sum
of squares of total.

198

(2)

With all 110 movies in the training set, the estimation of parameters
in Equation 2 are OW = 6.878 × 106 + 1303 × T BD + 0.26 × Budget
with R2ad j ≈ 0.6 and P  0.05.
3.3.2

Advanced Models

As most of the attributes are proportional to the box ofﬁce success
(e.g. the more budget, the higher weekend gross potential) we can
even achieve good results using linear regression model. More advanced models can be built using a Support Vector Machine (SVM) or
a Neural Network, i.e. Multilayer Perceptron (MLP). To achieve good
results, these algorithms have to be ﬁnely conﬁgured by setting input
parameters based on the input data. We ran a grid search (parameter
optimization method) to ﬁnd out the best parameter settings. For SVM
we use a linear kernel and a nu-parameter of 0.4, which constrains the
inﬂuence of a single instance (movie) to the model. Considering the
relatively small number of movies when compared to the large feature
space we also tested an RBF kernel. However this did not achieve
better R2ad j results than with the linear kernel. For MLP we use the
backpropagation learning rule and use a learning rate of 0.3, 200 training epochs and a momentum rate of 0.85 to achieve good results.
3.3.3

Multiple Methods Modeling

Predictive models help to reveal relationships between the predictors
and the response variable, but no matter how good the prediction is,
no cause-effect relationship can be implied. Also, the accuracy of one
prediction can hardly be generalized to all other predictions. In statistical analysis, experts usually explore residual distributions, outliers,
inﬂuential points, and model stability. In our system, besides using
statistical methods, we apply visual analysis methods for exploring
the residual distribution.
In the page ”Explore Models” the user can select which algorithm
to use, set the number of folds for the stability test, train models to predict the movie’s revenue, and compare between models. The Explore
Models view is shown in Figure 5. For model building, the feature

and training set conﬁgurations from the Feature Selection page are applied. After the prediction is executed, the analyst can use the Actual
vs. Predicted Gross view (Figure 5(b)) to obtain an overview of the
residuals, as was presented in [24]. A diagonal referential line indicating “the perfect prediction” is also drawn. This means, the closer
the data points lie to the referential line, the better the overall ﬁt of
the model. The top 5 most similar movies are highlighted in red to
quickly guide comparison and analysis. The user can change these
similar movies based on adding/removing features in the Parallel Coordinates view (Feature Selection page). To submit a good prediction
for a particular movie, it may be more important that the model ﬁts for
similar movies than ﬁts the overall training set. In other words, if the
model predicts well for similar movies this may be an indicator that it
also gives good results for the prediction candidate.
Our tools also enable the exploration of inﬂuential points. An inﬂuential point is an outlier in both the predictor and the response domain,
and these points are known to have a noticeable impact on the model
coefﬁcients [30]. If an inﬂuential point is removed from the training
set, the ﬁt of the model will change by a relatively large degree and
usually ﬁt other points better. This fact can be used to improve prediction results. Instead of using statistic diagnostics, such as Cook’s D
and DFFITS [4], we allow the user to directly remove such instances
and only train on selected movies. In this way, inﬂuential points can be
implicitly removed via exploring differences between different models.
Finally, the Model History Table (Figure 5(a)) enables the comparison of multiple models so that the analyst can review the predictions
by re-investigating their scatterplots. In combination with the Model
Comparison view (Figure 5(c)), the user can also get an overview of
the prediction deviations, review the increase or decrease of prediction precision and select his/her ﬁnal prediction. Our goal is to build
a model which can help the analyst to better predict the upcoming
movie’s opening weekend gross, not to build an adequate model that
ﬁts all the training data very well.
To estimate the performance and to test the model’s stability, we
provide an n-fold cross-validation [16, 23]. For the cross-validation
we partition the data into n folds. Each fold includes nummovies /n
instances. The movies of each fold are predicted once, using the other
folds for training. This way, we ensure that the model generalizes and
is not overﬁt to the training data. For the prediction candidate, every
fold is used once to predict the outcome. Thus, for each prediction
we get n results. The dashed vertical line in the scatterplot shows the
range of these results. A smaller range indicates that the model is
stable. This range is also shown in the bar graph below the scatter
plot, where all predictions can be compared.
3.3.4

Auxiliary Analysis

Instead of depending totally on an automatic model, most industry
predictions also utilize an expert’s domain knowledge. For example,
if a movie is released next to an expected blockbuster, its performance
could be also impacted. With our system, analysts can query any
movie by its title to investigate features. Users can also go to previous weekends to see how much money those movies made. A user
can also investigate the Twitter and Youtube data to explore the advertising campaign and public sentiment. Usually a successful movie has
either an effective advertisement campaign, positive public reactions,
or both. From the bubble plot shown in Figure 2(a), large bubbles usually are Tweets from the movie production company and the bubble
size indicates the spread power. If the large bubbles separate along the
time line, it is likely that the company has continued advertising its
movie.
4

C ASE S TUDY: P REDICTING D ISNEY ’ S F ROZEN

This section demonstrates how an analyst would use our system to
predict Frozen’s opening weekend gross. This process consists of
multiple steps, which can be iteratively traversed in different ways.
However, we suggest the following procedure. First, the user gets an
overview of the Twitter and Youtube comments using the dual-y-axis
line chart to compare movies released together. Second, details can be

investigated using the detail pages of the candidate movie. Third, the
user can explore similar movies and compare their gross, as well as
how well the baseline model performed for them. After having a general impression of the expected revenue, the user can navigate to the
Feature Selection tab to explore and select features or ﬁlter movies to
create a model. Finally the user can build and explore different models and their prediction ranges in the Explore Models view. Step 4 and
step 5 can be iteratively applied until the user feels they can make a
conﬁdent prediction.
To illustrate these 5 steps, we will take Frozen as an example.
Starting on the overview page, the line chart in Figure 1 (a) indicates
that there are 4 movies released on the same weekend (Frozen, Black
Nativity, Homefront, and Oldboy).We quickly see that online chatter
(Tweet and YouTube comment volume) about Frozen is not dominating the other weekend movies, in fact it is trending similarly to the
movie Black Nativity. This phenomenon indicates that it is unlikely
that Frozen will obtain an anomalously large gross as the market will
be shared by competitors.
In the second step, using the detailed view of Frozen (see Figure 2)
the Tweet sentiment is analyzed. One can see frequent Tweet keywords and the sentiment polarity. Also, the retweet volumes provides
information about users’ interest in the movie and the advertisement
campaign. For example in Figure 2 we can see that Frozen does
not have a large Tweet and retweet volume compared to other blockbusters; however it does have a very positive sentiment (blueish dots).
The movie sentiment score for Frozen is approximately 0.8 which is
very high among all 112 movies having Twitter data.
In the third step the similarity widget is explored (see Figure 4).
This reveals that movies similar to Frozen were under-predicted with
the baseline model, which predicts about $44M for Frozen. The fourth
step focuses on the analysis and selection of the movies features (see
Figure 3). There are two main views for feature selection: the correlation view showing relationships between a feature and the revenue,
and; the relationship among features depicted in the PCP view. From
our baseline model we select the number of opening screens, the budget and the weekly average of Tweet counts as an initial feature selection. This gives us a model with R2ad j ≈ 0.58 (M1 in Figure 5). To
further improve the model, we add another feature, view counts of the
movie’s YouTube trailers, and built both an SVM and LIN model. R2ad j
improved to approximately 0.6 while the prediction deviations from
the different folds decreased. Next, using our background knowledge,
we explore the genre of this movie (in this case the genre is “Family”).
While adding the Family feature to the Parallel Coordinates, we ﬁnd
that the gross distribution for Family movies is signiﬁcantly different
to most non-Family genres. Thus, for our last prediction iteration, we
add the family feature to the model. We obtained an R2ad j score of
0.745. Finally, we review the Model Prediction Comparison graph
and decided to ﬁnalize our prediction between $60M to $70M based
on the best performing models.
5 E VALUATION
In order to evaluate the effectiveness of this framework for predictive
analytics, we performed a user study. On March 20th, 2014 we enlisted seven graduate students from China, India, the United States
and Germany and asked them to predict the results of four different
movies. The ﬁrst two movies predicted were to provide them with
baseline training, the next two movies were to be released on March
21st, thus having them do an actual future prediction. The movies we
had them predict included Disney’s Frozen (2013) and The Hunger
Games: Catching Fire (2013) (which were used for training) and
Divergent (March 21, 2014) and Muppets Most Wanted (March 21,
2014) (which were the movies to be predicted). For Frozen and the
Hunger Games, their weekend box ofﬁce data was removed for the
training exercise in order to simulate the prediction process.
Of the seven participants, six were male, one was female and all
were PhD students. Prior to participation, we surveyed them about
their cinema afﬁnity and data visualization knowledge on a scale from
1-5 (with 1 being the lowest). From the seven participants four claimed
to be visualization experts. Five subjects rated their movie afﬁnity

199

Table 1: Results for Frozen and Hunger Games. The opening weekend gross for Frozen is $67M and for the Hunger Games it is $158M.
subject
Prediction(Frozen)
Abs Error
Prediction(Hunger Games)
Abs Error

user1
55.9
11.1
71.1
86.9

user2
59
8
135
23

user3
50
17
NA
NA

user4
60
7
100
58

user5
57.7
9.3
95.9
62.1

user6
62.5
4.5
86
72

user7
58
9
75
83

BoxOfﬁce.com
47
20
166
8

BoxofﬁceMojo
44.7
22.3
167
9

Table 2: Results for Divergent and Muppets. The opening weekend gross for Divergent is $56M and for the Muppets it is $16.5M.
subject
Prediction(Divergent)
Abs Error
Prediction(Muppets)
Abs Error

user1
54.1
1.9
50.6
34.1

user2
53
3
21.5
5

user3
40
16
28
11.5

user4
50
6
15
1.5

as low (1-2), and two rated medium (3-4). Their machine learning
knowledge was mostly low, with only two participants claiming a basic
knowledge of machine learning and prediction related tasks (these students had all taken regression analysis and/or data mining courses, as
such we feel that they can be considered to have a relatively high level
of expertise in the modeling and analysis process). The two subjects
that rated their movie afﬁnity as low were those that rated their machine learning and predictive analytics knowledge as high. Thus, we
have three subjects that were casual users with limited domain knowledge and limited analytics experience, two subjects that had some domain knowledge and limited analytics experience, and two subjects
that had expertise in data mining and predictive analytics but limited
domain knowledge.
To introduce the system, we walked through an example analysis
of the movie After Earth and explained our proposed analytics process
(similar to the case study in section 4). Subjects were then asked to
predict Frozen and The Hunger Games. During the analysis and prediction process of these two movies, they were open to ask any questions, such as the meaning of a feature, how to use a special function
of the system, and what information could help to choose proper features and improve the model performance. After they submitted their
ﬁnal prediction about a movie, we told them the real gross so that they
could make a comparison and adjust their strategy for the next movie.
After practicing with these two movies, they used the system (unaided)
to predict the new movies Divergent and Muppets Most Wanted.
To get a deeper understanding of the users analysis processes this
study was carried out as talk-aloud [14] session. The users were asked
to speak their thoughts out loud explaining their actions. We recorded
voice and system interaction by video. After the study we summarized
the key results and classiﬁed them into System Usability, Social Media
Exploration, Feature Selection and Model Comparison.
5.1 System Usability
Key ﬁndings here indicated more details on system design. All subjects reported ease of use and interaction with the system. Furthermore, the length of the user study demonstrated the subjects’ engagement. No instructions were given on the time needed to make a prediction; however, subjects spent over 1 hour on average tuning system parameters and exploring the data. Subjects also were excited to compare
their results Monday and indicated they wanted to try this again. Design issues they faced were that they wanted even more transparency in
the data. As no subject was a self-rated expert in cinema (most indicating they had seen less than two movies in the past 6 months) many of
the subjects wanted more information about the movie features. They
suggested direct links to the IMDB pages for the movies to allow even
greater detail views. Overall, the most used views were the similarity
page and the feature selection page.
Subjects all started their analysis on the overview page, exploring
time series trends and comparing how they felt the movies on the
weekend would fare when compared to others. They typically looked

200

user5
30.1
25.9
35
18.5

user6
47.5
8.5
21.4
4.9

user7
48
8
20
3.5

BoxOfﬁce.com
66
10
25
8.5

BoxofﬁceMojo
51
5
22
5.5

at the Twitter and YouTube volumes and sentiment data. At the beginning they found it difﬁcult to interpret those visualizations as they
were unfamiliar to a user; however, by the end of the study the users
were requesting more features, wanting to create difference maps of
the movies to look for keyword differences in the sentiment analysis and also to identify what was being discussed differently between
YouTube and Twitter. As such, it is clear that more text analysis is
needed for further insight generation. A clear example of gaining insight was shown during the analysis of the movie, Divergent. No subject in our group had heard of this movie; however, when inspecting
the data they saw that The Hunger Games was often referred to in context with this movie. This grounding gave them the contextual clues
which they needed in order to analyze Divergent.
Negative comments focused on the disconnect between the similar
movies and the users’ thought process. In the Feature Selection page,
users are presented with the ﬁve most similar movies with respect to
the selected PCP features. This is calculated as a Euclidean distance
metric, and the calculation is a black-box to the user. As such, analysts were often wary of these movies and preferred to use the “create
your own similarity” option on the similarity widget page. However,
this again required more domain knowledge than some users had, with
many again requesting details about what genre, rating, etc. a particular movie had. Future work should include better views for multidimensional similarity matches and more transparency in the similarity metrics. Yet, what the process highlights is that all subjects, even
those with little self-proclaimed movie knowledge, are able to bring
some background knowledge into the prediction process, which could
be used to add value when compared to a purely automated prediction
process.
5.2 Feature Selection
All users worked with the Feature Selection table to determine which
data was available for a movie and remarked on how they felt the prediction was more reliable when they knew that the data existed. Again,
this indicates that transparency in the model training can improve an
analyst’s conﬁdence. During the feature selection process, most users
started with the baseline settings, inspected the results and then iteratively chose more features with high correlations, reinspected and then
iterated again. Other users again applied their domain expertise and
chose features that seemed interesting to them. For example, the user
that had seen 10 movies in the theater in the past six months used his
domain knowledge to select features which are not obviously highly
correlated to the revenue but these features considerably improved that
subject’s model.
Participants who decided to add Twitter related features typically
based this choice on the genre of the movie, stating that Twitter users
would be interested in Divergent but not in the Muppets. One user,
with a basic background knowledge in prediction tasks commented
on how the Parallel Coordinate view enabled her to choose features
that were independent (i.e., not multi-correlated). Other users engaged

the PCP view to ﬁlter out movies to create models based on genre or
movie ratings. Overall, they spent a large amount of time exploring
features and discussing what they felt these features meant. They also
found it extremely helpful to see how the selection of different features
impacted the amount of movies available for training.
Negative comments revolved around users’ frustration in feature selection, noting that there should be a way to provide more details on
what is likely to be a good feature. For the inspection of correlations,
one user noted that it was hard to use the PCP view and had difﬁculty
distinguishing the highlights. However, the users all liked the design of
the framework, and commented on how it would be useful to change
the domain to look at other speciﬁc problems of interest. For future
work, we plan to explore how to improve the presentation of features.
Obviously showing all features (in this case 116) is a huge amount of
information overload; however, we also want to involve the user and
allow him/her to use domain knowledge to guide the modeling and
prediction process. We plan to explore several methods of automatic
feature selection as a means of organizing information for visual presentation and exploration and performing user studies across various
feature set visualizations in order to explore this area.
5.3

Model Comparison

As for the Feature Selection view, participants found the model comparison features extremely useful. Starting with some initial predictions, they tried to improve the model to reduce the errors. Users often focused on prequel movies (particularly during the Hunger Games
prediction) and focused on developing a model that was a good ﬁt
for known prequels or known movies within a genre. One user repeated the feature inspection, selection and modeling until he was
able to create a model that strongly ﬁt to the prequel (in the case of
the Hunger Games). Others tried to inspect all outliers and then made
decisions based on their domain expertise regarding movie similarity.
This would lead to an iterative model building and reﬁnement loop.
Users also inspected the scatterplot and would then access the similarity comparison tools to explore the impact of Twitter on the model
prediction. Users noted that Twitter seemed to have an impact depending on the type of movies, and many came to the conclusion that
Twitter was relevant when predicting Science Fiction movies (such as
Divergent) but less relevant when predicting Family movies (such as
the Muppets). Again, subjects indicated a desire for even further transparency of the inner workings of the model prediction.
5.4

can all be considered more casual users and had a higher variability. In
both future prediction cases, over half the subjects were able to best the
experts over the course of a one hour training session. Furthermore,
such work indicates that visual analytics can have a direct impact on
the modeling and prediction process. As noted by Lazer et al. [25],
there is a need for tools that can improve insight into large data analytics and an increased transparency can potentially lead to improved
model efﬁcacy. Future work will look at doing a more formal evaluation where a larger subject pool is recruited and more analysis between
the three groups is performed.
6

C ONCLUSIONS

This paper presents an interactive framework integrating social media
and predictive analytics, and the presentation of a talk aloud study that
discusses design successes, pitfalls and potential future directions. Analysts can utilize the system to explore and combine information, and
underlying mechanisms for similarity matching and data ﬁltering can
help a user quickly engage in exploratory data analysis as part of the
model building process. We allow for the quick integration of structured and unstructured data sources, focusing on box ofﬁce predictions
as our example domain. In comparison to state-of-the-art in visual analytics, we have worked towards improving a user’s understanding of
the modeling and training process. Our results were validated through
case studies and user studies. We have demonstrated that such a tool
can quickly enable non-domain experts to be competitive with domain
experts in a given area. This seems to stem from a combination of a
user’s (in our case limited) domain knowledge with the interactive visualization interface. While with our system semi-professionals are
not always able to beat the expert models from boxofﬁce.com and
boxofﬁcemojo, respectable results were obtained across a majority of
users. As the industry’s models and predictive practices are not available, it is difﬁcult to comment on their workﬂow. However, talking
with experts from SAS and JMP, they recognize a need for integrating
more interactive visuals in the model building process. Overall, we believe that such a framework could be applied to a wide range of social
media data in which analysts want to locally extract information from
social media and use trend values and other metrics as input to their
modeling process. We believe that predictive analytics in general can
be improved upon by integrating human knowledge into the workﬂow
and can add more transparency to the oftentimes black-box model that
encompasses many of the current prediction methods (e.g., SVM).

Prediction Results

Table 1 and 2 show the results of our user study in both the training
trial and the actual prediction trial. For the training results (Table 1),
subjects were found to have a lower error than that of the experts for
Frozen; however, for the Hunger Games, subjects found this very difﬁcult to model. It is important to note that we went through the example
of After Earth, Frozen and the Hunger Games for training in order to
give subjects examples of a low outlier, a good ﬁt, and a high outlier
respectively. In this way they can explore all possible scenarios prior
to the actual prediction task.
For the actual results (Table 2), 5 of our 7 subjects were able to best
BoxOfﬁce.com predictions for Divergent and 2 of our 7 subjects were
able to best both expert prediction websites. Only two subjects erred
on the far low end of the spectrum for this movie (subjects 3 and 5).
For the Muppets, 4 of our 7 subjects were able to best the experts, with
one subject (subject 4) accurately predicting this would be a box ofﬁce
failure. Again, subject 5 was an outlier, and subject 1 predicted that
the Muppets would be an outlier on the positive end of the spectrum.
Overall, the results of our study are quite positive. Given our subjects self-reported lack of movie knowledge, it is clear that the integration of social media and visual analytics for model building and
prediction can quickly generate insight at a near professional prediction level. Subjects 2 and 7 had the highest self-reported domain
knowledge and (as seen in Table 2) outperformed experts from BoxOfﬁce.com (and Subject 2 outperformed the BoxofﬁceMojo results as
well). The machine learning and regression experts were subjects 4
and 6 and they also outperformed the experts. The remaining subjects

7

F UTURE

WORK

For future work we want to improve a users understanding of a feature’s impact on a model. We also want to develop methods to explore and select features according to multivariate dependencies and
feature engineering. Visualization can explain results and reveal complex dependencies. To ﬁnd such dependencies we want to integrate
and orchestrate even more data sources, such as news media and other
social media sources like bitly and Facebook, as well as weather and
seasonal information such as holidays. Moreover we expect dependencies between past and concurrent weekend releases to be highly
important. We also want to focus on the machine learning aspect of
prediction. As our models makes structure assumptions, for example,
the linear regression model only covered linear relationships, we think
we can further improve predictions by investigating the domain data
more deeply and use these insights to help analysts choose the right
algorithms and options.
8

ACKNOWLEDGMENTS

This work was supported by the U.S. Department of Homeland Securitys VACCINE Center under Award Number 2009-ST-061-CI0001. We
also wish to acknowledge to the cooperative graduate program Digital
Media of the University of Stuttgart, the University of Tuebingen, and
the Stuttgart Media University (HdM), as well as the German Federal
Ministry of Education and Research (BMBF) in context of the VASA
project and the Stuttgart Vast Challenge Team 2013.

201

R EFERENCES
[1] IMDb Database Statistics, 2014. http://www.imdb.com/stats.
[2] R. Amar and J. Stasko. A knowledge task-based framework for design
and evaluation of information visualizations. In IEEE Symposium on Information Visualization, pages 143–150. IEEE, 2004.
[3] S. Baccianella, A. Esuli, and F. Sebastiani. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In
Proceedings of the International Conference on Language Resources and
Evaluation, 2010.
[4] D. A. Belsley, E. Kuh, and R. E. Welsch. Regression diagnostics: Identifying inﬂuential data and sources of collinearity, volume 571. John Wiley
& Sons, 2005.
[5] W. Berger, H. Piringer, P. Filzmoser, and E. Gröller. Uncertainty-aware
exploration of continuous parameter spaces using multivariate prediction.
In Computer Graphics Forum, volume 30, pages 911–920. Wiley Online
Library, 2011.
[6] M. Bostock, V. Ogievetsky, and J. Heer. D3 data-driven documents. IEEE
Transactions on Visualization and Computer Graphics, 17(12):2301–
2309, Dec. 2011.
[7] E. T. Brown, J. Liu, C. E. Brodley, and R. Chang. Dis-function: Learning
distance functions interactively. In IEEE Conference on Visual Analytics
Science and Technology, pages 83–92. IEEE, 2012.
[8] N. Cao, Y.-R. Lin, X. Sun, D. Lazer, S. Liu, and H. Qu. Whisper: Tracing
the spatiotemporal process of information diffusion in real time. IEEE
Transactions on Visualization and Computer Graphics, 18(12):2649–
2658, 2012.
[9] J. Chae, D. Thom, H. Bosch, Y. Jang, R. Maciejewski, D. S. Ebert, and
T. Ertl. Spatiotemporal social media analytics for abnormal event detection and examination using seasonal-trend decomposition. In IEEE
Conference on Visual Analytics Science and Technology, pages 143–152.
IEEE, 2012.
[10] J. Choo, H. Lee, J. Kihm, and H. Park. ivisclassiﬁer: An interactive visual
analytics system for classiﬁcation based on supervised dimension reduction. In IEEE Symposium on Visual Analytics Science and Technology,
pages 27–34. IEEE, 2010.
[11] C. Cortes and V. Vapnik. Support-vector networks. Machine learning,
20(3):273–297, 1995.
[12] A. Culotta. Towards detecting inﬂuenza epidemics by analyzing twitter
messages. In Proceedings of the First Workshop on Social Media Analytics, pages 115–122, New York, NY, USA, 2010. ACM.
[13] W. Dou, L. Yu, X. Wang, Z. Ma, and W. Ribarsky. Hierarchicaltopics:
Visually exploring large text collections using topic hierarchies. IEEE
Transactions on Visualization and Computer Graphics, 19(12):2002–
2011, 2013.
[14] K. Ericsson and H. Simon. Protocol Analysis: Verbal Reports as Data.
MIT Press, Cambridge, MA, 1993.
[15] G. Eysenbach. Can tweets predict citations? Metrics of social impact
based on Twitter and correlation with traditional metrics of scientiﬁc impact. Journal of medical Internet research, 13(4), 2011.
[16] S. Geisser. Predictive inference. Chapman & Hall, New York, 1993.
[17] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H.
Witten. The WEKA Data Mining Software: An Update. SIGKDD Explorations Newsletter, 11(1):10–18, Nov. 2009.
[18] M. Hao, C. Rohrdantz, H. Janetzko, U. Dayal, D. A. Keim, L. Haug, and
M.-C. Hsu. Visual sentiment analysis on twitter data streams. In IEEE
Conference on Visual Analytics Science and Technology, pages 277–278.
IEEE, 2011.
[19] M. Harrower and C. A. Brewer. Colorbrewer. org: An online tool for
selecting colour schemes for maps. The Cartographic Journal, 40(1):27–
37, 2003.
[20] S. Haykin. Neural networks: A comprehensive foundation. Prentice Hall
PTR, 1994.
[21] A. Inselberg. Parallel Coordinates: Visual Multidimensional Geometry
and Its Applications. Springer-Verlag New York, Inc., Secaucus, NJ,
USA, 2009.
[22] Jersey Team. Jersey: Restful web services in java. https://jersey.
java.net/, 2014.
[23] R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the 14th International Joint
Conference on Artiﬁcial Intelligence, pages 1137–1143. Morgan Kaufmann, 1995.
[24] R. Krüger, H. Bosch, D. Thom, E. Püttmann, Q. Han, S. Koch, F. Heimerl,

202

[25]
[26]

[27]

[28]

[29]

[30]
[31]

[32]

[33]

[34]

[35]

[36]
[37]
[38]
[39]
[40]

[41]
[42]

[43]

[44]
[45]

[46]

[47]
[48]

[49]

and T. Ertl. Prolix - visual prediction analysis for box ofﬁce success. In
IEEE Conference on Visual Analytics Science and Technology, 2013.
D. Lazer, R. Kennedy, G. King, and A. Vespignani. The parable of google
ﬂu: Traps in big data analysis. Science, 343(6176):1203–1205, 2014.
H. Liu and L. Yu. Toward integrating feature selection algorithms for
classiﬁcation and clustering. IEEE Transactions on Knowledge and Data
Engineering, 17(4):491–502, April 2005.
Y. Lu, F. Wang, and R. Maciejewski. VAST 2013 Mini-Challenge 1: Box
Ofﬁce VAST-Team VADER. In IEEE Conference on Visual Analytics
Science and Technology, 2013.
A. M. MacEachren, A. Jaiswal, A. C. Robinson, S. Pezanowski, A. Savelyev, P. Mitra, X. Zhang, and J. Blanford. Senseplace2: Geotwitter analytics support for situational awareness. In IEEE Conference on Visual
Analytics Science and Technology, pages 181–190. IEEE, 2011.
T. May, A. Bannach, J. Davey, T. Ruppert, and J. Kohlhammer. Guiding
feature subset selection with an interactive visualization. In IEEE Conference on Visual Analytics Science and Technology, pages 111–120. IEEE,
2011.
D. C. Montgomery, E. A. Peck, and G. G. Vining. Introduction to Linear
Regression Analysis. Wiley, 2012.
F. Morstatter, S. Kumar, H. Liu, and R. Maciejewski. Understanding
twitter data with tweetxplorer. In Proceedings of the ACM SIGKDD international conference on knowledge discovery and data mining, pages
1482–1485. ACM, 2013.
T. Muhlbacher and H. Piringer. A partition-based framework for building
and validating regression models. IEEE Transactions on Visualization
and Computer Graphics, 19(12):1962–1971, 2013.
V. D. Nguyen, B. Varghese, and A. Barker. The royal birth of 2013:
Analysing and visualising public sentiment in the UK using Twitter. In
IEEE International Conference on Big Data, pages 46–54. IEEE, 2013.
C. Okoli and S. D. Pawlowski. The delphi method as a research tool: an
example, design considerations and applications. Information & Management, 42(1):15–29, 2004.
H. Piringer, W. Berger, and H. Hauser. Quantifying and comparing features in high-dimensional datasets. In 12th International Conference on
Information Visualisation, pages 240–245. IEEE, 2008.
S. Publishing et al. JMP 10 Modeling and Multivariate Methods. SAS
Institute, 2012.
R Core Team. R: A Language and Environment for Statistical Computing.
R Foundation for Statistical Computing, Vienna, Austria, 2014.
G. Rowe and G. Wright. The delphi technique as a forecasting tool: issues
and analysis. International journal of forecasting, 15(4):353–375, 1999.
SAS Institute Inc. SAS/STAT Software, Version 9.3. Cary, NC, 2011.
J. Seo and B. Shneiderman. A rank-by-feature framework for unsupervised multidimensional data exploration using low dimensional projections. In IEEE Symposium on Information Visualization, pages 65–72.
IEEE, 2004.
J. S. Simonoff and I. R. Sparrow. Predicting movie grosses: Winners and
losers, blockbusters and sleepers. Chance, 13(3):15–24, 2000.
D. Thom, H. Bosch, S. Koch, M. Worner, and T. Ertl. Spatiotemporal anomaly detection through visual analysis of geolocated twitter messages. In IEEE Paciﬁc Visualization Symposium (PaciﬁcVis), pages 41–
48. IEEE, 2012.
A. Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M. Welpe. Predicting
elections with twitter: What 140 characters reveal about political sentiment. International AAAI Conference on Weblogs and Social Media,
10:178–185, 2010.
J. Ulbts. JMDB: Java Movie Database. http://www.jmdb.de/,
2014.
C. Wang, Z. Xiao, Y. Liu, Y. Xu, A. Zhou, and K. Zhang. Sentiview:
Sentiment analysis and visualization for internet popular topics. IEEE
Transactions on Human-Machine Systems, 43(6):620–630, 2013.
P. Xu, Y. Wu, E. Wei, T.-Q. Peng, S. Liu, J. J. Zhu, and H. Qu. Visual
analysis of topic competition on social media. IEEE Transactions on
Visualization and Computer Graphics, 19(12):2012–2021, 2013.
D. Zeng, H. Chen, R. Lusch, and S.-H. Li. Social media analytics and
intelligence. Intelligent Systems, IEEE, 25(6):13–16, 2010.
X. Zhang, H. Fuehres, and P. A. Gloor. Predicting stock market indicators
through twitter “I hope it is not as bad as I fear”. Procedia-Social and
Behavioral Sciences, 26:55–62, 2011.
Z. Zhao, L. Wang, and H. Liu. Efﬁcient spectral feature selection with
minimum redundancy. In AAAI Conference on Artiﬁcial Intelligence,
2010.

ALIDA: Using Machine Learning for Intent Discernment in Visual Analytics
Interfaces
Tera Marie Green∗
∗ School

Ross Maciejewski†

Steve DiPaola∗

of Interactive Arts + Technology - Simon Fraser University
Visual Analytics Center - Purdue University

† Purdue

A BSTRACT
In this paper, we introduce ALIDA, an Active Learning Intent Discerning Agent for visual analytics interfaces. As users interact with
and explore data in a visual analytics environment they are each
developing their own unique analytic process. The goal of ALIDA
is to observe and record the human-computer interactions and utilize these observations as a means of supporting user exploration;
ALIDA does this by using interaction to make decision about user
interest. As such, ALIDA is designed to track the decision history
(interactions) of a user. This history is then utilized to enhance the
user’s decision-making process by allowing the user to return to
previously visited search states, as well as providing suggestions of
other search states that may be of interest based on past exploration
modalities. The agent passes these suggestions (or decisions) back
to an interactive visualization prototype, and these suggestions are
used to guide the user, either by suggesting searches or changes to
the visualization view. Current work has tested ALIDA under the
exploration of homonyms for users wishing to explore word linkages within a dictionary. Ongoing work includes using ALIDA to
guide users in transfer function design for volume rendering within
scientific gateways.

cognition model through use of an intelligent, autonomous agent
that sits underneath a simple data visualization prototype.
2 R ELATED W ORK
Work in determining a users interest during interaction has largely
focused on web interfaces. Recent examples include Godoy and
Amandis [1] personal agents for web experiences, which tracks user
behaviour and provides a user profile to aid in future web navigation. Another example, MASHA is a multi-agent architecture for
multi-modal web interaction [5]. In visualization related research,
machine learning has been used on user annotations to attempt to
classify user goals [6]. Our work focuses on adapting this concept
of a personal agent into the visual analytics pipeline.
3 ALIDA: ACTIVE L EARNING I NTENT D ISCERNING AGENT
This section describes our personal agent (ALIDA) and the current
application domain. The current ALIDA prototype was developed
in the Processing prototyping language. Processing does not easily
support highly interactive visualizations (such as direct interaction),
but does support the rule based decision function ALIDA uses.

Keywords: artificial intelligence, cognition, intent discernment,
volume rendering.
1

I NTRODUCTION

Intuitive interactive visualizations are designed to scaffold human
cognition. However, cognition, especially the higher processes such
as reasoning, tend to be combinatorial and dynamic. As such, these
processes are difficult to standardize. Furthermore, there are no operational models of human reasoning, thus such interactive visual
interfaces often encounter difficulties when trying scaffold what is
not known or understood. One effort to define an intuitive visualization system that would incorporate an ongoing comprehension of
holistic cognition is the Human Cognition Model (HCM) [3]. The
HCM outlines the primary interactive processes during interfaceenabled analysis.
One of the agenda items that the HCM outlined is the need to
make visual analytics more intuitive by creating interfaces that pay
attention to what information or data the user is exploring in order
to provide clues to what related information the user might be interested in [3]. This fits well with the definition of visual analytics in
general as the use of interactive interfaces to support analytical reasoning. By capturing the analyst’s focus, interfaces are able to be
automatically tailored to a user’s needs, enabling the data analysis
and visualizations to have enhanced contextual information directly
related to the analysts needs. This paper outlines ongoing research
toward the interface-human collaboration envisioned in the human

IEEE Symposium on Visual Analytics Science and Technology
October 24 - 29, Salt Lake City, Utah, USA
978-1-4244-9487-3/10/$26.00 ©2010 IEEE

Figure 1: The flow of intent discernment with ALIDA, from user interaction until ALIDA provides feedback.

ALIDA sits underneath the interactive visual interface, continuously recording and analyzing user behaviour in real-time. The
agent uses low-level interaction information, such as mouse clicks
and movements, to make decisions about current and future intent.
During this observation ALIDA does not disturb the user or ask for
direct feedback. Instead, ALIDA is designed to be autonomous; every 60 seconds, the agent makes a decision by using the captured
interactions to calculate a decision function. The results of ALIDA’s decisions are provided as feedback to the user in a variety of
ways and stored in ALIDAs memory for future use.
The decision function currently returns three categories of decision: area of interest, browsing, and inaction. (Please see Figure 1.) To return a decision about an area of interest, a comparison

223

of means of interaction behaviors and decision history are both used
to determine the current base category of interest. If no single category stands out (interaction is scattered in a statistically insignificant way), the agent decides the user is browsing. In the last case,
the user has ceased (or almost ceased) all interaction with the visualized glyphs. In this way, ALIDA uses these decisions as part of
ongoing research to make interfaces aware of what the user is exploring in order to direct the interface which context or related data
to display.
ALIDA uses a MySQL database to store a history of intent decisions for each user. This history is currently used in two ways. First,
the history is incorporated into the decision function. If past intent
discernment decisions are similar to previous decisions, the probability of accuracy is increased. Second, it allows the user to take
advantage of interface capacity to return to previous states the user
has visited. This is also part of an emerging capacity for user profiling as ALIDA becomes a learning agent. Feedback, for testing
purposes, is done through direct decision reporting in an ambient
overlay. (See Figure 2.) Other forms of feedback are less direct,
such as making areas of user intent more visible in the view. All of
ALIDA’s features are currently being user tested for accuracy and
ease of use by comparing ALIDA’s decision to the self-reported intent of the users during interaction.
4

A PPLYING ALIDA

FOR

L EXICAL O NTOLOGY

The testing and evaluation phase of ALIDA is being performed using interactive visualization tool for exploring lexical ontologies.
The interface prototype visualizes a multiple hierarchical dataset
where each level, or cluster, is visualized as a sphere within the
spatial context of the base node (or categorization) in the hierarchy,
Currently, we utilize a lexical ontology of homonyms as the test
dataset. This dataset was chosen to support more advanced semantic testing in future work. The name of each category is used to
label each spherical glyph. All category labels had fewer than three
words, and the glyphs were organized such that there was sufficient
space between glyphs to make the labels easily legible.
Interaction with the glyphs is direct, that is, users click directly
on the glyph of interest. Drill down is discrete. The clicked-on
glyph disappears to reveal all subcategory glyphs. Right-clicking
supports a discrete drilling up.
To support the “reset” and “go back” functionality, associated
icons float on the periphery of a highlighted glyph. Reset allows
the user to return to initial view. Go back takes the user back to the
glyph, or area, of last interest based on ALIDA s memory of the last
intent decision.
5

O NGOING

AND

F UTURE W ORK

This agent is only successful insofar as it “understands” and responds to the human user. To some degree, this agent will allow
us to test certain research questions that have arisen during our research of human complex cognition during analytical reasoning in
interface interaction, such as the impact of human individual differences on use of visually enabled interfaces (e.g. [2]). We will use
what we have learned about complex cognition during the agent development, and use the agent as another method of evaluating the
human cognitive models such we are currently developing, such as
aspects of the HCM.
In addition, as part of future research, we plan to extend ALIDA to be used in volumetric rendering. Multi-dimensional transfer functions have long been established as a powerful means for
interacting with volumetric data [4]. However, efficiently designing transfer functions has turned out to be a non-trivial problem.
Typically, multi-dimensional transfer functions are shown as a twodimensional (2D) scatterplot, the axes of which represent two variables of the feature space of the data. Users interact in this space,
assigning optical properties (color and opacity) to the voxel data. In

224

Figure 2: ALIDA direct reporting through ambient overlay.

this dual-domain interaction (volume data space and transfer function feature space), users are able to select data properties in the feature space and see the resultant rendering in the volume space. This
interaction improves the user’s ability to understand how the feature
space of the data is reflected in the volumetric data. However, much
of the user interaction tends to be a hunt-and- peck approach as they
modify and refine the transfer function based on their knowledge of
the dataset or general intuition. Through the addition of an intent
discerning agent where the agent unobtrusively observes the overt
behavior of the user and generates transfer functions across combinations of feature space, suggesting potential areas of exploration
to the user based upon the user’s own interactions. The goal of the
agent would be to direct the visualization with respect to underlying statistical properties that the user may or may not be utilizing
in their exploration. Thus, the agent becomes part of a continuous
feedback loop which is fed by continuing human interaction (or
lack thereof).
R EFERENCES
[1] D. Godoy and A. Amandi. User profiling for web page filtering. In
Proceedings of Internet Computing, volume 9, pages 56–64, 2005.
[2] T. M. Green, D. H. Jeong, and B. Fisher. Using personality factors to
predict interface learning performance. In Proceedings of the Hawaii
International Conference on System Sciences, volume 43, pages 1–11,
2010.
[3] T. M. Green, W. Ribarsky, and B. Fisher. Determining and building a
human cognition model for visual analytics. Information Visualization,
8(1):1–13, 2009.
[4] G. Kindlmann and J. W. Durkin. Semi-automatic generation of transfer functions for direct volume rendering. In Proceedings of the IEEE
Symposium on Volume Visualization, pages 79–86, 1998.
[5] D. Rosaci and G. M. L. Sarne. Masha: A multi-agent system handling user and device adaptivity of web sites. User Modeling and UserAdapted Interaction, 16(5):435–462, 2006.
[6] O’Brien, T.M., Laidlaw, D.H.: Toward a machine learning approach for
classifying user goals from user interactions In: Proceedings of Symposium on Interactive 3D Graphics, 2010 ACM SIGGRAPH, (2010).

Spatiotemporal Social Media Analytics for Abnormal Event Detection and
Examination using Seasonal-Trend Decomposition
Junghoon Chae∗

Dennis Thom†

Harald Bosch†

Purdue University

University of Stuttgart

University of Stuttgart

Yun Jang‡

Ross Maciejewski§

Sejong University

Arizona State University

David S. Ebert∗

Thomas Ertl†

Purdue University

University of Stuttgart

Figure 1: Social media analysis system including message plots on a map, abnormality estimation charts and tables for message content and
topic exploration. It can be seen, how the Ohio High School Shooing on February 27, 2012 is examined using the system. The selected messages,
marked as white dots on the map, show retrieved Tweets that are related to the event.

Abstract
Recent advances in technology have enabled social media
services to support space-time indexed data, and internet
users from all over the world have created a large volume of
time-stamped, geo-located data. Such spatiotemporal data
has immense value for increasing situational awareness of
local events, providing insights for investigations and understanding the extent of incidents, their severity, and consequences, as well as their time-evolving nature. In analyzing
social media data, researchers have mainly focused on ﬁnding temporal trends according to volume-based importance.
Hence, a relatively small volume of relevant messages may
easily be obscured by a huge data set indicating normal situations. In this paper, we present a visual analytics approach
that provides users with scalable and interactive social media data analysis and visualization including the exploration
and examination of abnormal topics and events within various social media data sources, such as Twitter, Flickr and
YouTube. In order to ﬁnd and understand abnormal events,
the analyst can ﬁrst extract major topics from a set of se∗ e-mail:

{jchae|ebertd}@purdue.edu
{forename.surname}@vis.uni-stuttgart.de
‡ e-mail: jangy@sejong.edu
§ e-mail: rmacieje@asu.edu
† e-mail:

IEEE Symposium on Visual Analytics Science and Technology 2012
October 14 - 19, Seattle, WA, USA
978-1-4673-4753-2/12/$31.00 ©2012 IEEE

lected messages and rank them probabilistically using Latent
Dirichlet Allocation. He can then apply seasonal trend decomposition together with traditional control chart methods
to ﬁnd unusual peaks and outliers within topic time series.
Our case studies show that situational awareness can be improved by incorporating the anomaly and trend examination
techniques into a highly interactive visual analysis process.
Index Terms: H.5.2 [Information Interfaces and Presentation]: User Interfaces—GUI; H.3.3 [Information Storage and
Retrieval]: Information Search and Retrieval—Information
ﬁltering, relevance feedback
1

Introduction

Social media services, e.g, Twitter, Youtube, Flickr, provide a rich and freely accessible database of user-generated
situation reports. As advances in technology have enabled
the widespread adoption of GPS enabled mobile communication devices, these reports are able to capture important
local events observed by an active and ubiquitous community. The diﬀerent forms of social media content provided by
the users, such as microposts, images or video footage, can
have immense value for increasing the situational awareness
of ongoing events.
However, as data volumes have increased beyond the capabilities of manual evaluation, there is a need for advanced
tools to aid understanding of the extent, severity and consequences of incidents, as well as their time-evolving nature,
and to aid in gleaning investigative insights. Due to the

143

large number of individual social media messages it is not
straightforward to analyze and extract meaningful information. For example, in Twitter, more than 200 million Tweets
are posted each day [31]. Thus, in a developing event, the relevant messages for situational awareness are usually buried
by a majority of irrelevant data. Finding and examining
these messages without smart aggregation, automated text
analysis and advanced ﬁltering strategies is almost impossible and extracting meaningful information is even more
challenging.
To address these challenges, we present an interactive spatiotemporal social media analytics approach for abnormal
topic detection and event examination. In order to ﬁnd relevant information within a user deﬁned spatiotemporal frame
we utilize Latent Dirichlet Allocation (LDA) [1], which extracts and probabilistically ranks major topics contained in
textual parts of the social media data. The ranks of the categorized topics generally provide a volume-based importance,
but this importance does not reﬂect the abnormality or criticality of the topic. In order to obtain a ranking suitable
for situational awareness tasks, we discard daily chatter by
employing a Seasonal-Trend Decomposition procedure based
on Loess smoothing (STL) [5]. In our work, globally and
seasonally trending portions of the data are considered less
important, whereas major non-seasonal elements are considered anomalous and, therefore, relevant.
However, due to the large volumes of data, the very speciﬁc syntax and semantics of microposts and the complex
needs of situational analysis, it would not be feasible to apply these techniques in the form of a fully automated system. Therefore, our whole analysis process, including the
application of automated tools, is guided and informed by
an analyst using a highly interactive visual analytics environment. It provides tight integration of semi-automated
text-analysis and probabilistic event detection tools together
with traditional zooming, ﬁltering and exploration following
the Information-Seeking Mantra [24].
The remainder of this document is structured as follows:
Section 2 is a review of related work. The automated methods to ﬁnd and examine unusual topics and events are described in Section 3. In Section 4 we brieﬂy introduce our
visual analytics system Scatterblogs, which was already featured in previous works, and explain how the automated
methods are integrated within a sophisticated iterative analysis loop. Finally we demonstrate the performance of our
system based on selected case studies in Section 5 and discuss the approach in Section 6.
2

Related Work

In recent research, social media services have become a popular and inﬂuential data source for many domains. Researchers in the ﬁelds of data mining and visual analytics
have found through studies among users and domain experts, that the analysis of such data can be essential for spatiotemporal situational awareness [15, 23]. Thus, as the size
of social media data increases, scalable computational tools
for the eﬀective analysis and discovery of critical information within the data are a vital research topic. This section
presents previous work that has focused on spatiotemporal
and event related social media analysis.
2.1

Spatiotemporal Social Media Data Analysis

As social media platforms move towards location-based social networks (LBSNs) researchers have proposed various approaches to analyze spatiotemporal document collections, in
general, and spatiotemporal social media data, in particular.

144

VisGets [7] provides linked visual ﬁlters for the space, time
and tag dimensions to allow the exploration of datasets in a
faceted way. The user is guided by weighted brushing and
linking, which denotes the co-occurrences of attributes. Further works demonstrate the value of visualizing and analyzing the spatial context information of microblogs for social
network users [9] or third parties like crime investigators [22]
and urban planners [33]. With Senseplace2, MacEachren
et al. [15] demonstrate a visualization system that denotes
the message density of actual or textually inferred Twitter
message locations. The messages are derived from a textual
query and can then be ﬁltered and sorted by space and time.
Their work also has shown that social media can be a potential source for crisis management. With ScatterBlogs [2],
our own group developed a scalable system enabling analysts to work on quantitative ﬁndings within a large set of
geolocated microblog messages. In contrast to Senseplace2,
where the analysts still have to ﬁnd and manage the appropriate keywords and ﬁlters to gather relevant messages
in the high volume of insigniﬁcant messages, we propose a
semi-automatic approach that ﬁnds possibly relevant keywords and ranks them according to their ‘abnormality’.
Special LBSN for certain domains, like Bikely1 and EveryTrail2 have an even stronger focus on the sharing and tracing of user locations. Ying et al. [37] present various location based metrics using spatial information of these LBSNs
to observe popular people who receive more attention and
relationships within the network. Similarly, there are many
related works for non-spatial temporal document collections,
for example IN-SPIRE [36], which is a general purpose document analysis system that depicts document clusters on a
visual landscape of topics.
2.2 Social Media Event Detection and Topic Extraction
One of the major challenges in analyzing social media data is
the discovery of critical information obscured by large volumes of random and unrelated daily chatter. Due to the
nature of microblogging, message streams like Twitter are
very noisy compared to other digital document collections.
Recently, many researchers have tried to solve this challenge
by means of automated and semi-automated detection and
indication of relevant data.
Sakaki et al. [23] propose a natural disaster alert system
using Twitter users as virtual sensors. In their work, they
were able to calculate the epicenter of an earthquake by analyzing the delays of the ﬁrst messages reporting the shock.
Weng and Lee [34] address the challenge by constructing a
signal for each word occurring in Twitter messages using
wavelet analysis, thereby making it easy to detect bursts
of word usage. Frequently recurring bursts can then be ﬁltered by evaluating their auto-correlation. The remaining
signals are cross correlated pairwise and clustered using a
modularity-based graph partitioning of the resulting matrix.
Due to the quadratic complexity of pairwise correlation, they
rely on heavy preprocessing and ﬁltering to reduce their test
set to approx 8k words. As a result, they detected mainly,
large sporting events, such as soccer world cup games, and
elections. Our approach, in contrast, provides a set of topics
through a probabilistic topic extraction algorithm which can
be iteratively applied to subsets and subtopics within user
selected message sets.
Lee and Sumiya [14] as well as Pozdnoukhov and
Kaiser [19] present methods to detect unusual geo-social
events by measuring the spatial and temporal regularity of
1 http://www.bikely.com/

2 http://www.everytrail.com/

Twitter streams. Lee and Sumiya propose a concept to detect unusual behavior by normalizing the Twitter usage in
regions of interests which are deﬁned by a clustering-based
space partitioning. However, their results are mainly a measurements of unusual crowd behavior and do not provide
further means for analyzing the situation. Pozdnoukhov and
Kaiser observe abnormal patterns of topics using spatial information embedded in Twitter messages. Similar to our
approach, they apply a probabilistic topic model (Online Latent Dirichlet Allocation) as a means of analyzing the document collection. A Gaussian RBF kernel density estimation
examines the geo-spatial footprint of the resulting topics for
regularities. The usual message count of identiﬁed areas
is then learned by a Markov-modulated non-homogeneous
Poisson process. The spatial patterns are shown as a static
heat map. The resulting system does not provide interactive
analytics capabilities.
Recently, researchers have applied LDA topic modeling to
social media data to summarize and categorize Tweets [39]
and ﬁnd inﬂuential users [35]. Zhao et al. [39] demonstrate characteristics of Twitter by comparing the content
of Tweets with a traditional news medium, such as the New
York Times. They discuss and adapt a Twitter-LDA model
and evaluate this model against the standard topic model
and the so-called author-topic model [25], where a document is generated by aggregating multiple tweets from a
single user, in terms of meaningfulness and coherence of topics and Twitter messages. In this work, we do not use the
author-topic model, since a users Tweet timeline is usually a
heterogeneous mixture of unrelated comments and messages
and not a homogenous framework of interrelated topics like a
traditional document. Furthermore, the evaluation of Zhao
et al. [39] shows that the standard model has quite reasonable topic modeling results on Tweets, although the TwitterLDA model outperforms the standard model. Works from
Ramage et al. [20] also show promising results in LDA based
Twitter topic modeling by evaluating another type of LDA
model (Labeled LDA) [21]. ParallelTopics [8] also extracts
meaningful topics using LDA from a collection of documents.
The visual analytics system allows users to interactively analyze temporal patterns of the multi-topic documents. The
system, however, does not not deal with spatial information,
but takes an abnormality estimation into account.
In our previous work [26], we proposed a spatiotemporal anomaly overview based on a streaming enabled clustering approach that is applied for each term in the dataset
individually. The resulting clusters can be used to generate a spatially and temporally explorable term map of large
amounts of microblog messages as an entry point for closer
examination. Even though the scalable event detection and
our current approach share the same workbench, they can
be used independently as well as complementary. The combination of LDA and STL allows for an ad-hoc analysis of a
user selected set of messages regarding the topical distribution of messages and the abnormal presence of topics. Due
to this characteristic, it provides an iterative analysis loop
for qualitative analysis and drill down operations.
3

Spatiotemporal Social Media Analytics for Event Examination

Since several social media sources recently provide spacetime indexed data, traditional techniques for spatiotemporal
zooming, ﬁltering and selection can now be applied to explore and examine the data. However, as message volumes
exceed the boundaries of human evaluation capabilities, it is
almost impossible to perform a straightforward qualitative

analysis of the data. In order to cope with the data volumes,
traditional interaction and visualization techniques have to
be enhanced with automated tools for language processing
and signal analysis, helping an analyst to ﬁnd, isolate and
examine unusual outliers and important message subsets.
To address this issue, we present an interactive analysis
process that integrates advanced techniques for automated
topic modeling and time series decomposition with a sophisticated analysis environment enabling large scale social media exploration. In part 3.1 of this Section we ﬁrst explain
how the Latent Dirichlet Allocation, a well established topic
modeling technique in the information retrieval domain, can
be used to extract the inherent topic structure from a set
of social media messages. The output of this technique is a
list of topics each given by a topic proportion and a set of
keywords prominent within the topics messages. In a subsequent step, our system then re-ranks the retrieved topic list
by identifying unusual and unexpected topics. This is done
by employing a seasonal-trend decomposition algorithm to
the historic time series data for each topic, retrieving its seasonal, trending and remainder components. Using a z-score
evaluation, we locate peaks and outliers in the remainder
component in order to ﬁnd an indicator of unusual events.
While the LDA topic extraction is done primarily for Twitter
data, the abnormality estimation is also applied to diﬀerent
social media data sources, such as Flickr and YouTube, for
each topic. This is achieved by searching matching entries
for each term of a topic and applying the same STL analysis
on the resulting time series. The results are available to the
analyst for cross validation. The details of this step are described in Subsection 3.2 and the complete detection model
is formally described in Subsection 3.3. In Section 4, we
describe how powerful tools based on these techniques are
used within our analysis environment, Scatterblogs, in order to iteratively ﬁnd, isolate and examine relevant message
sets.
3.1 Topic Extraction
Our monitoring component collects space-time indexed
Twitter messages using the Twitter-API. The received messages are preprocessed and then stored in our local database.
When users of these services witness or participate in unusual situations they often inform their friends, relatives or
the public about their observations. If enough users participate, the communication about the situation constitutes
a topic that makes up a certain proportion of all messages
within the database, or some messages within a predeﬁned
area and timespan. In most cases, however, the proportion
will be smaller than that of other prevalent topics, such as
discussions about movies, music, sports or politics. In order
to extract each of the individual topics exhibited within a
collection of social media data, we employ Latent Dirichlet
Rank
1
2
3
4
5

Proportion
0.10004
0.09717
0.09443
0.08226
0.05869

Topics
day back school today
lls bout dat wit
people make hate wanna
earthquake thought house shaking
earthquake felt quake washington

Table 1: An example of extracted topics and their proportions. We
extracted topics from Tweets written on August 23, 2011 around Virginia, where an earthquake occurred on this day. One can see that
topics consisting of ordinary and unspeciﬁc words can have high proportion values, while the earthquake related topics have a relatively
low proportion value.

145

50
foursquare pic hall brooklyn
time night day back
newyork nyc tweetmyjobs ﬁnance
york brooklyn ave street
york ave park btw

Number of Iteration Steps in the LDA process
300
1000
time back night day
time night nyc day
york ave brooklyn btw
york ave brooklyn park
pic bar food nyc
foursquare occupywallstreet mayor ousted
foursquare occupywallstreet park mayor
newyork tweetmyjobs ﬁnance citigroup
newyork tweetmyjobs ﬁnance citigroup
san gennaro street italy

Table 2: An example of topic model results depending on the number of iteration steps in the LDA process. The topics are extracted from the
Tweets posted in New York City on September 17 and 18, 2011 where the Occupy Wall Street protest movement began and a famous festival,
San Gennaro occurred. A higher number of sampling iterations provides a better topic retrieval describing the two diﬀerent events.

Allocation, a probabilistic topic model that can help organize, understand, and summarize vast amounts of information.
The LDA topic model approach, as presented by David
Blei et al. [1], is a probabilistic and unsupervised machine
learning model to identify latent topics and corresponding
document clusters from a large document collection. Basically, it uses a “bag of words”approach and assumes that
a document exhibits multiple topics distributed over words
with a Dirichlet prior. In other words, the LDA assumes
the following generative process for each document: First,
choose a distribution over topics, choose a topic from the distribution for each word, and choose a word associated with
the chosen topic. Based on this assumption one can now
apply a Bayesian inference algorithm to retrieve the topic
structure of the message set together with each topic’s statistical proportion and a list of keywords prominent within the
topic’s messages. Table 1 shows an example set of extracted
topics resulting from the application of LDA to Twitter data
ordered by the proportion ranking. The example social media data was collected from Twitter for the Virginia area on
August 23rd. On this day, the area was struck by an earthquake with a magnitude of 5.88. As seen in the table, this
earthquake event was captured as a topic within the Twitter
messages.
In our system, the MALLET toolkit [18] is used for the
topic analysis. Prior to the topic extraction, the stemming
algorithm KSTEM by Krovetz [13] is applied to every term
in the messages. The results of KSTEM are more readable
and introduce fewer ambiguities than the often used Porter
stemmer.
For the unsupervised LDA classiﬁcation and topic retrieval one has to deﬁne two parameters: the number of
expected topics and the number of iterations for the Gibbs
sampling process[10], which is used in MALLET for the topic
inference. The number of topics that should be chosen depends on the size of the document collection and the required
overview level. A small number of topics (e.g., 10) will provide a broad overview of the documents, whereas a large
number (e.g., 100) provides ﬁne-grained results. The number of sampling iterations is a trade-oﬀ between computation
time and the quality of discovered topics. To illustrate this,
Table 2 shows the experimental results of the topic model using a varying number of sampling iterations while the number of topics was set to four. The topics were extracted from
Tweets posted in New York City on September 17 and 18,
2011, where a large group of protesters occupied Wall Street
in New York City3 . A topic indicating the Occupy Wall
Street protests can be seen when using at least 300 iterations. At the time of these protests, there was also a famous
annual festival, the San Gennaro 4 , occurring in Little Italy.
3 http://occupywallst.org/

4 http://www.sangennaro.org/

146

This can only be seen when using at least 1000 iterations. As
shown in Table 2, the topics with 50 iterations do not indicate any meaningful events. The topics with 300 iterations,
on the other hand, consist of more distinguishable classes.
Finally, the topics with 1000 iterations obviously point out
individual events which happened in the city.
3.2

Abnormality Estimation using Seasonal-Trend Decomposition

Abnormal events are those that do not happen frequently
and usually they cover only a small fraction of the social
media data stream. As shown in Table 1, even during an
earthquake episode, highly ranked topics consist of ordinary
and unspeciﬁc words. The third and fourth ranked topics include words indicating the earthquake event of August 2011:
earthquake felt quake washington. From this observation in
the distributions of ordinary and unusual topics over the social media data, it is necessary to diﬀerentiate the unusual
topics from the large number of rather mundane topics. In
order to identify such abnormal topics, we utilize SeasonalTrend Decomposition based on locally-weighted regression
(Loess) known as STL [5]. For each extracted topic of the
LDA topic modeling, our algorithm retrieves messages associated with the topic and then generates a time series
consisting of daily message counts from their timestamps.
The time series can be considered as the sum of three components: a trend component, a seasonal component, and a
remainder:
Y =T +S+R

(1)

Here Y is the original time series of interest, T is the trend
component, S is the seasonal component, and R is the remainder component. STL works as an iterative nonparametric regression procedure using a series of Loess smoothers [6].
The iterative algorithm progressively reﬁnes and improves
the estimates of the trend and the seasonal components.
The resulting estimates of both components are then used to
compute the remainder: R = Y −T −S. Under normal conditions, the remainder will be identically distributed Gaussian
white noise, while a large value of R indicates substantial
variation in the time series. Thus, we can utilize the remainder values to implement control chart methods detecting anomalous outliers within the topic time series. We have
chosen to utilize a seven day moving average of the remainder values to calculate the z-scores, z = (R(d) − mean)/std,
where R(d) is the remainder value of day d, mean is the
mean remainder value for the last seven days, and std is the
standard deviation of the remainders, with respect to each
topic. If the z-score is higher than 2, events can be considered as abnormal within a 95% conﬁdence interval. The
calculated z-scores are thus used as abnormality rating and
the retrieved topics will be ranked in the analytics environment according to this estimate.

Other social
media data

Tweets

Filter messages by
relevant areas,
timespans, or
keywords

Retrieve
topics using
LDA

Top ranked
topics

Iteratively improve
LDA configuration to
further generalize or
specialize topic
extraction

Detect
Evaluate topics
anomalies
by STL and
show
anomalies in
the data

Adjust threshold of
z-score to identify
anomalies without
seasonal trends

Compare
anomalies with
other social
media data

Confirmed
anomalies

Select interesting
topics and relevant
events for analysis and
drilldown

Visualization

Interactive
data
exploration
n

Analyst
A
l
Figure 2: Overview of our iterative analysis scheme for event detection and examination.

3.3

Detection Model

To conclude this section, we formalize our abnormal event
detection model based on the probabilistic topic extraction
and time series decomposition.
An abnormal event is associated with a set of social media messages that provides its contents, location, and timestamp. To detect abnormal events for a given area and
timespan, we deﬁne a set called social spacetime as follows:
S = (T, Δtime, Δarea, msgs)

(2)

where T is a set of topics, Δtime is a time period (e.g., one
day), Δarea is a bounded geographical region, and msgs is
a set of messages. The user selected parameters Δarea and
Δtime deﬁne the analysis context for which all messages are
loaded into the analysis system. In this context, the user
selects a subset of messages (msgs) for which the LDA topic
modeling procedure (described in Section 3.1) extracts the
set of topics, ti ∈ T . Each topic is deﬁned as:
ti = (Mi , Wi , zi , Yi , pi )

(3)

where Wi is a set of words describing the topic, Mi is a set
of relevant messages, zi is an abnormality score (z-score), Yi
is a time series, and pi is a statistical proportion of the topic
in msgs.
For each topic (ti ), our algorithm searches relevant messages (Mi ) in the selected area (Δarea) and time period
(Δtime) and a predeﬁned time span of historic data preceding Δtime (e.g. one month). Messages are considered
relevant if they contain at least one word in Wi . From Mi
a daily message count time series (Yi ) is generated from the
timestamps of the messages. The algorithm decomposes Yi
to obtain a remainder component series using the STL and
calculates a z-score (zi ) from the remainder series. Lastly, it
sorts the topics based on the z-scores.
For cross validation of each topic, we search for relevant
entries in Flickr and YouTube by their meta-data that includes titles, descriptions, tags, and timestamps, using the
respective APIs. We repeat the steps for generating a time

series from the collected timestamps, applying STL to decompose the time series, and calculating the z-score from the
remainder component series.
4

Interactive Analysis Process

The complete topic extraction, abnormality estimation, and
event examination are tightly integrated into a highly interactive visual analysis workbench, that allows an analyst
to observe, supervise, and conﬁgure the method in each individual step. The following sections introduce the details
of this system and describe how the event detection is embedded within a sophisticated analysis process as shown in
Figure 2.
4.1

Social Media Retrieval and Analysis System

Our modular analysis workbench ScatterBlogs was already
featured in previous works [2, 26]. It proved itself very useful for fundamental tasks like collection, exploration and examination of individual, as well as aggregated, social media
messages. The UI of the system is composed of several interconnected views and the main view houses a zoomable openstreetmaps implementation showing message geolocations on
a world map. The system features a text search engine and
visual content selection tools that can be used to retrieve
messages, show spatial and temporal distributions and display textual message contents. Additional visualizations and
map overlays provide the analyst with powerful inspection
tools, such as a kernel-density heatmap similar to [16], to
show aggregated and normalized message distributions and
a movable lens-like exploration tool (called ‘content lens’)
that aggregates keyterm frequencies in selected map areas
[2]. To indicate spatiotemporal anomalies in the message
set, the system features a mechanism to detect spatiotemporal clusters of similar term usage, and suspicious message
clusters can be represented as Tag Clouds on the map [26].
For the real-time collection of messages using the Twitter
Streaming API the system features a scalable extraction and
preprocessing component. This component was used to collect Twitter messages since August 2011 and it currently

147

processes up to 20 Million messages per day5 , including the
almost complete volume of up to 4 million messages that
come with precise geolocation information.
4.2

• Crosscheck Validation: Each selection of messages
is accompanied by charts showing the total time series
and the remainder components for the selected message set using STL. This is true for spatiotemporal selections as well as for selections using the LDA topic
list. In addition to the geolocated Twitter messages this
STL is at the same time performed for data that has
been extracted from supplemental services like Flickr
and YouTube. Based on the multiple charts the analyst can crosscheck the importance and abnormality of
examined events and topics.

Visual Topic Exploration and Event Evaluation

Results from the topic retrieval and event detection as described in Section 3 can be iteratively reﬁned by means of
visual result presentation and interactive parameter steering.
Both, the ﬁnal result of event detection as well as intermediary ﬁndings during data ﬁltering and topic extraction can be
used by the analyst to adjust the process in order to identify
interesting topics and keyterms as well as relevant map areas
and timespans for a given analysis task. New insights can be
generated on each of four individual analysis layers which, in
conclusion form an iterative analysis loop from data ﬁltering
to result visualization:
• Spatiotemporal Data Filtering: The analyst selects
an initial spatiotemporal context of Twitter messages
to be represented in the visualization and to serve as
a basis for analysis. He can do so by using textual
as well as spatiotemporal query and ﬁlter mechanisms
that load the relevant base message set from a larger
database into active memory. The analyst can further
ﬁlter the base set and remove unimportant parts by using a time-slider, depicting temporal message densities,
or polygon and brush selection tools. Using these tools
the analyst can gain an initial impression of the spatial and temporal distribution and location of messages
that could be relevant for his analysis task.
• LDA Topic Examination: In the subsequent step
the analyst can choose to start the topic extraction either on the whole analysis context or on some subset
of selected messages. At this stage he can utilize the
conﬁguration parameters of LDA extraction to interactively explore available topics by generalization and
specialization. In this regard the most important parameter is the number of topics that have to be deﬁned
for the topic model inference. If the analyst decreases
the number using the provided tools, the extracted topics will be more general. If he increases it, they will be
more speciﬁc and thus candidates for small but possible
important events. Once topics are generated from the
data they will be presented to the analyst through a list
of small tag clouds for each topic. He can now select
the topics from the list to see their individual message
distribution on the map and the temporal distribution
in the time-slider.

In our system, the analyst is supposed to iteratively use these
means of semi-automated processing, visualization and interaction to reﬁne the selection of messages up to a point
where he can begin to examine individual message details.
For this task, he can then utilize tools like the content lens
for small scale aggregation or the table view to read the
messages textual content. The application of these tools is
shown in Figure 3. Usually the most valuable messages will
be reports from local eyewitnesses of an important event or
from insiders for a given topic. Thus, to retrieve large quantities of such messages helping to understand an ongoing
event or situation will be the ﬁnal goal of the iterative process. Unusual topics, suspicious keyword distributions and
events with high STL abnormality discovered on the repeatedly traversed analysis layers can guide the analysis from a
very broad and general overview to very speciﬁc topics and
a relatively small message set suitable for detailed examination.
5

Case Study

In this section, we present three case studies for our system covering diﬀerent types of events including the Chardon
High School Shooting, the Occupy Wall Street protests in
New York, and the 2011 Virginia Earthquake. The ﬁrst case
shows how analysts can use our system eﬃciently to ﬁnd and

• STL Evaluation: Depending on the analyst’s choice,
the topics can be evaluated and ordered based either
on absolute topic frequency or based on abnormality
estimates that have been computed using STL. As described in Section 3.2, a valid estimate of abnormality
depends on the computation of z-scores from data seven
days prior to the observed time frame. Therefore, the
STL evaluation will extend the data examination to a
range prior to the selected spatiotemporal context, if
data is available. Once abnormality is computed for
each topic, the topic list will be ordered according to
the values and the topics with most outstanding abnormality are highlighted.
5 This

is just a 10% sample of the total 200 million Twitter
messages due to API rate limitations.

148

Figure 3: Examining the location of the Chardon high school shooting
with a text aggregating content lens.

Figure 4: Cross validation of an event using Twitter, Flickr, and YouTube data for the Occupy Wall Street Protests. The protests occurred on
Sep. 17 and 30, Oct. 5 and 15. The line charts show the remainder components R (blue) and the original data volumes Y (red) for the STL
evaluation. The scales on the right and left side of each chart view are adapted to the maximum values.

explore an abnormal event. The second case highlights the
diﬀerences between social media types by cross validation
of a planned event. Finally, the last example showcases the
eﬀects of an abrupt, unexpected, natural disaster.
5.1

Ohio High School Shooting

On February 27, 2012, a student opened ﬁre inside the
Chardon High School cafeteria in the early morning. The
gunman killed one student and injured four, from which two
eventually died after the incident.
To examine this incident we ﬁrst locate and select the
broader Cleveland area on the map and select a time frame
covering three days from February 26 to February 28. Using the text search engine and a wildcard query (‘*’) we can
establish an exploration context showing all messages plotted on the map with their respective contents and meta data
listed in a separate table view. First, we want to get a broad
overview of the topics discussed in the region and thus we
select all messages in the area and apply the LDA extraction
tool to the current selection. In order to see the most general topics, we chose a low parameter value for the number
of topics and a high iteration count to achieve good separation. At this level of semantic detail, the extracted topics
indicate messages about the NBA all-star game (February 26
in Orlando) with keywords like kobe, game, dunk and lebron
as well as the showing of the movie ‘The Lion King’on TV
with keywords king, lion, tv. If we look at the STL-Diagrams
of these topics and the computed z-scores, we also see a peak
for these events. By clicking on the retrieved topic representations the associated messages are highlighted in each view.
By reading some of the message contents (e.g. ’Watching my
fav. Movie on ABC family..... Lion King!!!!’, ’Can’t wait till
the dunk contest starts!’ ), the analyst can easily disqualify
these from further analysis.
To get a higher semantic resolution we can now increase
the number of topics and slightly decrease the iteration count
in order to achieve a fast computation. By selecting 20
topics, the topic indicating the shooting event is extracted
and indicated by keyterms like shooting, chardon and school,
alongside the other topics. Although the proportion of the
topic is not very high compared to the others, the topic receives a very high z-score (i.e., 3.77) and is ranked among
the top ﬁve topics (highlighted in orange). Figure 1 demonstrates the system view of this observation. An analyst can
now select the incident topic to see the spatial distribution

of associated messages on the maps as well as the temporal distribution in the timeslider histogram. By examining
messages using the content lens to aggregate topics over map
areas as well as the tools for reading individual message contents, we can easily distinguish between messages informed
by media reaction and messages of actual observers in the
Chardon High School area. In this case, after isolating the
messages from local observers, we ﬁnd messages like ‘Omg
shooting at Chardon High School?!?!’ and ‘Helicopter overhead. We are on scene. Message from school says students
moved to middle school’.
5.2 Occupy Wall Sreet
Starting on September 17, 2011 in the Wall Street ﬁnancial
district in New York City, people have been gathering for
the Occupy Wall Street protest movement. The movement
against economic inequality has since spread to other major
cities throughout the world. Various social media services
including Twitter, Facebook, Flickr and Youtube have been
utilized both by the participants and the global media for
communication and reports about the movement in forms

Figure 5: Abnormality and correlation on multiple social media
sources. As a result of high z-scores around the same time periods,
we found a strong correlation between the three social media sources.
Marked regions correspond to periods where at least 2 providers received scores over 2.0.

149

Figure 6: Virginia earthquake on August 23rd, 2011. Our abnormal event detection system detects the earthquake event using our STL based
anomaly detection algorithm. The abnormality degree is extremely high on August 23rd, 2011 (times are given in UTC).

of text, images and videos. For the related extracted topic
(occupywallstreet, wall, takewallstreet, takewallst, park ), Figure 4 shows the results of our abnormality estimation for the
three social media services Twitter, Flickr, and YouTube
over the course of one month. As shown in Figure 5, in each
of the marked regions, at least two of the services show zscores over 2.0 and they correspond to actual events during
the Occupy Wall Street protests. From this experimental
result, one can derive a strong correlation between the three
social media data sources. The related data volumes and
remainder (R) are shown in Figure 4 for all three providers.
As shown in Figure 5, on September 17 (the ﬁrst day of the
protests with approximately 1,000 participants [30]), only
the Twitter stream received an abnormal score while the
Flickr and YouTube data artifacts are delayed by 1-3 days.
We attribute this initial delay to the simple nature of Twitter usage compared to Flickr and YouTube where the data
potentially has to be recorded, edited, and uploaded and is
thus more labor intensive. Additionally, eighty protesters
where arrested while marching uptown on September 24,
but even though Flickr and YouTube reaction on this event
created higher z-scores in the following days, they were not
signiﬁcant enough to register an event. The following spikes
of high z-scores overlap with a march across the Brooklyn
Bridge (Oct. 1 [29]), a large demonstration (Oct. 5 [27]),
and globally coordinated protests (Oct. 15 [28]).
5.3

2011 Virginia Earthquake

For the last use case we examine a magnitude 5.8 earthquake that occurred on the afternoon of August 23rd 2011
in Mineral, Virginia [32]. Starting with the minute of the
earthquakes occurrence, Twitter users posted more than
40.000 earthquake-related Tweets reporting tremors they
felt along the East Coast [11]. Among these were messages like: ‘EARTHQUAKE!!!!!!!’ ; ‘Whoa!!!! Just experienced an earthquake here in Virginia!!!!’ ; and ‘Omg I just
felt an earthquake’. Figure 6 gives an impression how our
system is applied to examine this event.
For the analysis we begin with selecting the Virginia area
from Baltimore to Virginia Beach and three days around the
23th. A topic extraction with 5 topics and just 100 iterations already retrieves two earthquake related topics showing

150

that this event is very prominent within the selection. By
clicking these topics one can observe that the highest density of earthquake messages can be found in the Washington,
Baltimore and Richmond areas.
To observe the areas in more detail we combine the topic
selection with a spatial selection of the three cities and reapply the topic extraction. This time we use 20 topics with 500
iterations. Since we are now operating only on earthquake
related messages, the retrieved topics all contain earthquake
as a dominant keyword. On this level of detail we can see
topics indicating that buildings have been evacuated due to
the earthquake (earthquake, people, evacuated, early, building) and that damage has been caused (earthquake, building,
shake, damage). The z-scores for all top ranked topics are
now very high (often above 8.0) and thus indicate the high
abnormality of this event.
Finally, when going into even higher detail with 100 topics and 1000 iterations we can see smaller events within
the big earthquake event. For example, one topic indicates
that damage was caused to the Washington Monument and
by clicking on the topic we can see messages like ‘damage
to Washington Monument’ ; ‘Washington Monument is tilting?!? ’ ; and ‘Helicopter just landed next to Washington
Moniment, west side. #DCearthquake ’. There are also misleading messages, indicating that the damage to the Washington Monument was just false rumors: ‘the Washington
monument was not damaged in any way from the earthquake. #rumor’. However, media crosschecks show that
visible damages did in fact happen and will probably cost
the city 15 million dollars to repair6 .
At this point, it is important to note, that while several earthquake topics produced signiﬁcant z-values in Twitter, the event did not produce high z-scores in Flickr and
YouTube. This is probably due to fact, that many people will write a quick message after a shock has been felt
by themselves, but it takes quite some time until images or
videos are uploaded from cameras to Flickr and YouTube.
The event also demonstrates that large and unexpected
events will produce immediate and signiﬁcant reactions in
6 http://www.huffingtonpost.com/2012/03/14/washington-

monument-did-e_n_1344422.html

services like Twitter and they can thus easily be detected by
using our system.
6

Discussion

In this section we want to discuss four important notes and
observations relevant to the presented approach.
Event Types: As was demonstrated with the three case
studies, events in social media can be categorized into two
diﬀerent types. The 2011 Virginia Earthquake and the Ohio
High School Shooting can be categorized as abrupt or disaster events, while Occupy Wall Street can be considered a
social and planned event. The two types of events have quite
distinguishable features. For the abrupt events, there is a
strong change in daily counts mainly in the text based Twitter messages. For the planned event, the Twitter signal may
still be faster, but due to the gradual increase and decrease,
it is less pronounced. In contrast, Flickr and YouTube have
delayed, but very prominent changes, for planned events;
however, we could not ﬁnd signiﬁcant signals for abrupt
events. This reﬂects that video and photo recording happen rarely during abrupt events. Social events, e.g., Occupy
Wall Street or election debates, however, have a high impact
on such multimedia based social media; Relevant videos,
photos, and even meta-data (e.g., descriptions, tags) allow
analysts to ﬁnd additional information about them. We,
therefore, think that cross validating events among multiple
social media types is important in order to establish situational awareness.
Base Data: Regarding the base data, it is important
to note, that our approach depends on geo-located Twitter
messages with precise coordinates, which are only a fraction
of the whole Twitter stream. While this fraction still consists
of several million messages per day, it is not a representative
sample of the population, because it mainly covers mobile
users equipped with GPS enabled devices. We think, however, that mobile users, who share their daily experiences
freely, are the most relevant group for situational awareness
scenarios. Some studies [4, 17] tried to overcome the problem
of location information scarcity in Twitter messages, which
adds another source of uncertainty. First, the user’s self reported locations can be outdated. Second, the geo-coding of
the location can be considerably wrong due to place name
ambiguities. Furthermore, we have just shown the feasibility
of the approach for Twitter, Flickr, and YouTube data, but
it can easily be adapted to other social media providers like
Facebook or Forsquare as well, in order to widen the sample
of the population.
Probabilistic Models: In this work, we use STL to
decompose time series of topic streams. There are many alternative statistical models for this task, such as DHR (Dynamic Harmonics Regression) [38] and SARIMA (Seasonal
AutoRegressive Intergrated Moving Average) [3]. DHR and
SARIMA models are particularly useful for forecasting and
STL can also be used for prediction based on seasonal (periodic) time series [12]. Our main reasons for choosing STL
was the fact that it is non-parametric, can be computed
faster than SARIMA [12] and needs less training data for
equally good results.
End User Feedback: We requested informal feedback
from users within our institutes and received comments and
suggestions. To compare the LDA topic modeling plus the
seasonal-decomposition based abnormality analysis versus
only the LDA topic modeling, we enabled our system to
switch between these modes. The users were impressed by
the fact that both results (two lists of topics) from two diﬀerent modes were quite diﬀerent. Highly ranked topics by LDA

topic modeling consisted of ordinary words, while the combined analysis was indicating unusual events. They noted
that the tightly integrated visual analysis workbench was
useful to apply the automated methods. Furthermore, they
suggested a function allowing people to see a pattern of abnormality for a user-deﬁned topic.
7

Conclusion

In this paper, we presented an interactive abnormal event
detection and examination system for the analysis of multiple social media data sources. The system uses an abnormality estimation scheme based on probabilistic topic
modeling and seasonal-trend decomposition to ﬁnd and examine relevant message subsets. This scheme is tightly integrated into an highly interactive visual analytics system,
which supplements tools based on automated message evaluation with sophisticated means for parameter steering, ﬁltering and aggregated result set exploration. Three use cases
demonstrated the visualization and user interaction within
the system and its capabilities to detect and examine several diﬀerent event types from social media data. The ability
to crosscheck ﬁndings based on three distinct social media
sources revealed the kinds of correlations that can be expected from various event types.
For future work, we will further investigate context-based
analysis and improve the current detection algorithm to allow for a faster analysis. Due to the fast-paced and low
quality nature of microblogging, we will also investigate the
eﬀects of additional preprocessing options like automated
spell-checking or synonym recognition under the constraint
of preventing ambiguities. Furthermore, we want to supplement the system with real-time monitoring features, demanding additional means for adaptive attention guiding as
well as interaction techniques for use in high pressure environments. For the ﬁnal system we are currently preparing
a thorough evaluation to test it in cooperation with crisis
management personnel and other domain experts.
Acknowledgements
This work was partially funded by the U.S. Department of
Homeland Securitys VACCINE Center under Award Number 2009-ST-061-CI0003, the German Federal Ministry for
Education and Research (BMBF) as part of the VASA
project, and the European Commission as part of the FP7project PESCaDO (FP7-248594). We would like to thank
the reviewers for their valuable suggestions and comments,
which helped to improve the presentation of this work.
References
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet
allocation. J. Mach. Learn. Res., 3:993–1022, Mar. 2003.
[2] H. Bosch, D. Thom, M. Worner, S. Koch, E. Puttmann,
D. Jackle, and T. Ertl. Scatterblogs: Geo-spatial document analysis. In Visual Analytics Science and Technology
(VAST), 2011 IEEE Conference on, pages 309 –310, 2011.
[3] G. E. P. Box and G. Jenkins. Time Series Analysis, Forecasting and Control. Holden-Day, Incorporated, 1990.
[4] Z. Cheng, J. Caverlee, and K. Lee. You are where you tweet:
a content-based approach to geo-locating twitter users. In
Proceedings of the 19th ACM international conference on
Information and knowledge management, CIKM ’10, pages
759–768, New York, NY, USA, 2010. ACM.
[5] R. B. Cleveland, W. S. Cleveland, J. E. McRae, and I. Terpenning. Stl: A seasonal-trend decomposition procedure
based on loess (with discussion). Journal of Oﬃcial Statistics, 6:3–73, 1990.

151

[6] W. S. Cleveland. Robust locally weighted regression and
smoothing scatterplots. Journal of the American Statistical
Association, 74(368):829–836, 1979.
[7] M. Dörk, S. Carpendale, C. Collins, and C. Williamson. VisGets: Coordinated visualizations for web-based information
exploration and discovery. IEEE Transactions on Visualization and Computer Graphics (Proceedings Information Visualization 2008), 14(6):1205–1212, 2008.
[8] W. Dou, X. Wang, R. Chang, and W. Ribarsky. Paralleltopics: A probabilistic approach to exploring document
collections. In Visual Analytics Science and Technology
(VAST), 2011 IEEE Conference on, pages 231 –240, oct.
2011.
[9] K. Field and J. O’Brien. Cartoblography: Experiments in
using and organising the spatial context of micro-blogging.
Transactions in GIS, 14:5–23, 2010.
[10] T. Griﬃths and M. Steyvers. Finding scientiﬁc topics. In
Proceedings of the National Academy of Sciences, volume
101, pages 5228–5235, 2004.
[11] L. Indvik. East coasters turn to twitter during virginia earthquake. Retrieved March 30, 2012, http://mashable.com/
2011/08/23/virginia-earthquake/, 2011.
[12] B. Jiang, S. Liang, J. Wang, and Z. Xiao. Modeling modis lai
time series using three statistical methods. Remote Sensing
of Environment, 114(7):1432–1444, 2010.
[13] R. Krovetz. Viewing morphology as an inference process.
In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’93, pages 191–202, New York, NY,
USA, 1993. ACM.
[14] R. Lee and K. Sumiya. Measuring geographical regularities of
crowd behaviors for twitter-based geo-social event detection.
In Proceedings of the 2nd ACM SIGSPATIAL International
Workshop on Location Based Social Networks, LBSN ’10,
pages 1–10, New York, NY, USA, 2010. ACM.
[15] A. MacEachren, A. Jaiswal, A. Robinson, S. Pezanowski,
A. Savelyev, P. Mitra, X. Zhang, and J. Blanford. Senseplace2: Geotwitter analytics support for situational awareness. In Visual Analytics Science and Technology (VAST),
2011 IEEE Conference on, pages 181 –190, oct. 2011.
[16] R. Maciejewski, S. Rudolph, R. Hafen, A. Abusalah, M. Yakout, M. Ouzzani, W. S. Cleveland, S. J. Grannis, and D. S.
Ebert. A visual analytics approach to understanding spatiotemporal hotspots. IEEE Transactions on Visualization
and Computer Graphics, 16(2):205–220, Mar. 2010.
[17] J. Mahmud, J. Nichols, and C. Drews. Where is this tweet
from? Inferring home locations of twitter users. In International AAAI Conference on Weblogs and Social Media,
2012.
[18] A. K. McCallum. Mallet: A machine learning for language
toolkit. http://mallet.cs.umass.edu, 2002.
[19] A. Pozdnoukhov and C. Kaiser. Space-time dynamics of
topics in streaming text. In Proceedings of the 3rd ACM
SIGSPATIAL International Workshop on Location-Based
Social Networks, LBSN ’11, pages 1–8, New York, NY, USA,
2011. ACM.
[20] D. Ramage, S. Dumais, and D. Liebling. Characterizing microblogs with topic models. In ICWSM, 2010.
[21] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. Labeled lda: a supervised topic model for credit attribution in
multi-labeled corpora. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, pages 248–256, Stroudsburg,
PA, USA, 2009. Association for Computational Linguistics.
[22] E. Roth and J. White. Twitterhitter: Geovisual analytics for
harvesting insight from volunteered geographic information.
In Proceedings of GIScience, 2010.
[23] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake shakes
twitter users: real-time event detection by social sensors. In
Proceedings of the 19th international conference on World

152

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

wide web, WWW ’10, pages 851–860, New York, NY, USA,
2010. ACM.
B. Shneiderman. The eyes have it: a task by data type taxonomy for information visualizations. In Visual Languages,
1996. Proceedings., IEEE Symposium on, pages 336 –343,
sep 1996.
M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. Griﬃths. Probabilistic author-topic models for information discovery. In
Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04,
pages 306–315, New York, NY, USA, 2004. ACM.
D. Thom, H. Bosch, S. Koch, M. Woerner, and T. Ertl. Spatiotemporal anomaly detection through visual analysis of geolocated twitter messages. In IEEE Paciﬁc Visualization
Symposium (PaciﬁcVis), 2012.
N. Times. Major unions join occupy wall street protest. Retrieved June 25, 2012, http://www.nytimes.com/2011/10/
06/nyregion/major-unions-join-occupy-wall-streetprotest.html, 2011.
N. Times. Occupy wall street protests worldwide. Retrieved
June 25, 2012, http://www.nytimes.com/2011/10/16/
world/occupy-wall-street-protests-worldwide.html,
2011.
N. Times. Police arresting protesters on brooklyn bridge.
Retrieved June 25, 2012, http://cityroom.blogs.nytimes.
com/2011/10/01/police-arresting-protesters-onbrooklyn-bridge, 2011.
N. Times.
Wall street protest begins with demonstrators blocked.
Retrieved June 25, 2012, http:
//cityroom.blogs.nytimes.com/2011/09/17/wall-streetprotest-begins-with-demonstrators-blocked, 2011.
Twitter. 200 million tweets per day. Retrieved March
1, 2012, http://blog.twitter.com/2011/06/200-milliontweets-per-day.html, 2011.
United States Geological Survey (USGS).
Magnitude 5.8 - virginia.
Retrieved March 30, 2012,
http://earthquake.usgs.gov/earthquakes/recenteqsww/
Quakes/se082311a.php, 2011.
S. Wakamiya, R. Lee, and K. Sumiya. Crowd-based urban
characterization: extracting crowd behavioral patterns in urban areas from twitter. In Proceedings of the 3rd ACM
SIGSPATIAL International Workshop on Location-Based
Social Networks, LBSN ’11, pages 77–84, New York, NY,
USA, 2011. ACM.
J. Weng and B.-S. Lee. Event detection in twitter. In International AAAI Conference on Weblogs and Social Media,
2011.
J. Weng, E.-P. Lim, J. Jiang, and Q. He. Twitterrank: Finding topic-sensitive inﬂuential twitterers. In Proceedings of
the third ACM international conference on Web search and
data mining, WSDM ’10, pages 261–270, New York, NY,
USA, 2010. ACM.
P. C. Wong, B. Hetzler, C. Posse, M. Whiting, S. Havre,
N. Cramer, A. Shah, M. Singhal, A. Turner, and J. Thomas.
IN-SPIRE Infovis 2004 contest entry. In IEEE Symposium
on Information Visualization, Oct. 2004.
J. J.-C. Ying, W.-C. Lee, M. Ye, C.-Y. Chen, and V. S.
Tseng. User association analysis of locales on location
based social networks. In Proceedings of the 3rd ACM
SIGSPATIAL International Workshop on Location-Based
Social Networks, LBSN ’11, pages 69–76, New York, NY,
USA, 2011. ACM.
P. C. Young, D. J. Pedregal, and W. Tych. Dynamic harmonic regression. Journal of Forecasting, 18(6):369–394,
1999.
W. X. Zhao, J. Jiang, J. Weng, J. He, E.-P. Lim, H. Yan, and
X. Li. Comparing twitter and traditional media using topic
models. In Proceedings of the 33rd European conference on
Advances in information retrieval, ECIR’11, pages 338–349.
Springer-Verlag, Berlin, Heidelberg, 2011.

DOI: 10.1111/j.1467-8659.2008.01300.x

COMPUTER GRAPHICS

forum

Volume 28 (2009), number 1 pp. 114–126

Shape Context Preserving Deformation of 2D Anatomical
Illustrations
Wei Chen∗1 , Xiao Liang2 , Ross Maciejewski3 and David S. Ebert3
1 State

Key Lab of CAD&CG, Zhejiang University, Hangzhou, China
chenwei@cad.zju.edu.cn
2 Microsoft Research Asia, Beijing, China
xiao.liang@microsoft.com
3 Purdue University, West Lafayette, Indiana, USA
rmacieje@ecn.purdue.edu, ebertd@purdue.edu

Abstract
In this paper, we present a novel two-dimensional (2D) shape context preserving image manipulation approach
which constructs and manipulates a 2D mesh with a new differential mesh editing algorithm. We introduce a
novel shape context descriptor and integrate it into the deformation framework, facilitating shape-preserving
deformation for 2D anatomical illustrations. Our new scheme utilizes an analogy based shape transfer technique
in order to learn shape styles from reference images. Experimental results show that visually plausible deformation
can be quickly generated from an existing example at interactive frame rates. An experienced artist has evaluated
our approach and his feedback is quite encouraging.
Keywords: Anatomical illustration, shape context, shape deformation, illustrative visualization
ACM CCS: I.3.3 [Computer Graphics]: Picture/Image Generation

terns, textures, media and shading [BG05, GG01, ONOI04].
Unfortunately, many computer-generated illustration techniques concentrate solely on illustrative shading effects,
while neglecting the influence of the shape of the illustrated
objects.

1. Motivation
In illustration, the proper depiction of shape is fundamental
to the human vision system’s ability to recognize an object
or process. The expressiveness of a drawing is greatly influenced by the manner in which shape is rendered and varied
amongst objects within a scene. In order to efficiently enhance human perception, reduce occlusion or even express
complex dynamic procedures, hand-drawn illustrations have
exploited various methods for shape and shape style depiction [Cla99, Hod88, ST90].

Mathematically, shape is defined as an equivalence class
under a group of transformations. However, this definition only judges whether two shapes are exactly the same
[BMP02]. In the context of computer-generated illustration,
an appropriate definition of shape perception would be necessary to create faithful and expressive effects. This is analogous to shape coordinates in morphometrics [Boo92], which
is used to investigate biological forms by comparing shape
and shape change. For instance, the two objects shown in
Figure 1a and b have similar biological forms because their
shapes could be related by certain mathematical transformations [Tho17]. To distinguish various biological forms, a set
of shape descriptors have been derived based on the analysis of these transformations [BMP02, Boo92, WAA∗ 05].
However, little attention has been paid to how to iden-

Such illustrative techniques have recently been introduced
into the computer graphics community. Various approaches
for illustrative rendering [ER02, GG01] have been proposed for simulating artistic styles and concepts, including
the variation, emphasis and subordination of colours, pat-

∗

This work was done while Wei Chen was visiting Purdue university.


c

2008 The Authors
c 2008 The Eurographics Association and
Journal compilation 
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

114

Submitted February 2008
Revised May 2008
Accepted July 2008

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

115

Figure 2: (a) A volume rendering of a foot dataset [SDS05];
(b) a hand-drawn image showing the bending of a foot; (c)
our result that simulates the shape style of (b), by performing
shape deformation to (a). The total user time is 1 minute.

Figure 1: The fishes (Pomacanthus and Scarus) shown in
(a) and (b) can be linked with region-based linear or nonlinear coordinate transformations [Tho17]. Our approach
captures the shape context difference between (a) and (b),
and performs shape context based analogy from (a) and (b)
onto (c), another illustration of Pomacanthus [Ito], yielding
the result (d).

tify the shape dissimilarity between two objects. The shape
description that can be used to depict the shape difference
is called the shape context, which abstracts the geometric
details and measures the shape styles. Until now, it has remained challenging to capture and learn shape styles for
the purpose of depiction, though there are many attempts to
achieve the automatic or semi-automatic learning of rendering styles [HJO∗ 01, HOCS02, HS99].
As such, consideration of an object’s shape is key to the
proper rendering of not only still objects, but also deformable
objects. For instance, in medical education there are various
terms describing the movements of the limbs and other parts
of the human body. These movements normally take place
at joints where two or more bones and cartilage articulate
with one another [MD99]. Currently, to illustrate deformations, artists typically acquire the models in key frames by
using measurement instruments [Hod88], which is a laborious process. In contrast, a common method in the graphics community is to display a sequence of individual model
deformations to form an animation sequence. This is quite
challenging when the scene complexity and the requirements
of visual reality are high, as shown in Figures 2a and 3a.
In this paper, we present a novel deformation algorithm
that uses a two-dimensional (2D) input image. One immediate benefit of this scheme is that a visually pleasing simulation of deformations can easily be created by applying simple
2D image manipulations. Our solution is a tool for an illustrator to quickly generate deformations for 2D anatomical

illustrations. It can also be used by non-illustrators to freely
deform existing images and check the validity of the results.
Figure 3b–d show our results.
Our solution reformulates the 2D image deformation as a
user-guided differential mesh manipulation that operates on
the differential to absolute geometry and distributes detail
distortions across the entire domain by means of a leastsquare minimization reconstruction scheme. One main difference between our approach and previous 2D image deformation techniques is that our approach pays special attention
to the expressiveness of the deformation and the preservation of shape styles. Specifically, we integrate shape context
into the interactive deformation framework. We also introduce an analogy-based shape transfer technique to efficiently
mimic the shape styles from reference images. One example
is shown in Figure 2.
The rest of this paper is organized as follows. We briefly
summarize the related work in Section 2. After describing
our deformation approach in Section 3, we introduce the
analogy-based shape style transfer technique in Section 4.
The experimental results are reported in Section 5. Finally,
we conclude this paper in Section 6.

2. Background and Related Work
2.1. Image deformation and manipulation
Previous image deformation and manipulation approaches
typically deform the shape space in which the image is embedded. For instance, Barrett and Cheney used space-warp
deformation for object-based image editing [BC02]. Bookstein employed thin-plate splines to find a space deformation
that is defined by several feature points [Boo89]. A potential drawback of these approaches is that they model deformations as smooth global transformations. Thus, the results undergo local non-uniform scaling and shearing, which
is undesirable in many applications. Although the moving
least squares algorithm [SMW06] provides a point-based


c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

116

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

Figure 3: (a) The input illustration; (b) the constructed mesh; (c) the head is rotated; (d) the lower arm is moved. Image
source: http://www.lib.uiowa.edu/hardin/rbr/imaging/mascagni/zoom/zfront1.htm. This example was selected from the exhibit
‘So Divinely Built a Mansion: Six Centuries of Human Anatomical Illustration’, by Paolo Mascagni (1755–1815).
closed-form solution using affine, similarity and rigid transformations, it is still an image warping technique.
By triangulating the image into a 2D mesh and building
deformations dependent on the specified topology, Igarashi
et al. presented an interactive, as-rigid-as-possible, shape manipulation system [IMH05]. Because this algorithm employs
a two-step approximation optimization to achieve interactive
performance, it may create unnatural effects by solely considering rigid transformations. To overcome this problem,

Weng et al. introduced an algorithm based on a non-linear
least squares optimization [WXW∗ 06]. The preservation of
the Laplacian coordinates of the shape boundary and the region area yields visually plausible deformation. However,
this approach concentrates on the preservation of the shape
outline and is probably not effective for handling the deformations of internal structures. More recent work by Fang
and Hart [FH07] describes an image editing system that decouples the feature position from pixel colour generation by
resynthesizing textures from the source image in order to


c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

117

preserve its detail and orientation around a new feature curve
location.

2.2. Computer-generated illustration
Illustrators [Cla99], [ST90] strive to create perceptually effective images by amplifying features and exploiting artistic
abstractions. Except for pen or paintbrush [Hod88], modern
illustrators often employ commercial software (e.g. Adobe
Illustrator, Corel PhotoPaint, Corel Painter, JASC PaintShop
Pro ) to manually create still 2D illustrations. For animated
2D images, illustrators commonly use Adobe PhotoShop,
After Effects, Macromedia Flash, Adobe PhotoShop, Image
Ready and Corel Painter [And].
In the computer graphics and visualization community,
scenes are modelled as three-dimensional (3D) surfaces or
volumes. Comprehensive approaches have been introduced
for point, line, surface and volume drawings [BKR∗ 05, ER02,
GG01, LEM∗ 02]. Furthermore, by applying traditional illustrative techniques, new algorithms have been developed that
work directly on 2D images [BSM∗ 07, KML99].

2.3. Example based modelling and rendering
Example-based approaches create new effects by simulating
the patterns or styles of specified examples. One representative work is texture synthesis, which has been applied in
volume illustration [ONOI04]. Another category of examplebased approaches is the analogy-based scheme, including
image analogy [HJO∗ 01] and rendition analogy [HS99]. The
curve analogy algorithm [HOCS02] generalizes the idea of
image analogy by transferring the position offset from one
curve to another.

2.4. Differential coordinates based mesh manipulation
Inspired by the success of partial differential equations (PDEs) in the digital image processing community [PGB03], differential mesh manipulation techniques
[SLCO∗ 04, YZX∗ 04] transform differential surface properties directly and reconstruct the results with a global optimization. This scheme not only preserves the geometric
feature, but also avoids artefacts due to per-vertex editing by
distributing errors globally. As the differential coordinates
are related to geometric details or features, the global optimization scheme favours detail-preserving results.

Figure 4: The three stages of our approach. The items with
dashed frames are optional.

We deform the constructed 2D triangular mesh within the
differential mesh editing framework. Poisson-based mesh
manipulation is briefly described in Section 3.1. Our approach improves previous image deformation approaches in
two aspects. First, we integrate the properties of different
regions into the deformation to produce an effective illustration. We will explain the enhanced Poisson mesh deformation
technique in Section 3.2. Second, we introduce a shape context descriptor that offers a globally discriminative characterization of the objects in the illustrations. The preservation of
the shape context during deformation imposes an additional
deformation constraint. The issues involving shape context
are discussed in Section 3.3. Finally, we summarize the user
interface and deformation pipeline in Section 3.4. Figure 4
shows the conceptual pipeline of our approach.

3.1. Preliminaries
Let us consider an arbitrary non-degenerate planar triangular mesh S = (K, V), where V is the set of 2D vertex coordinates and K describes its vertex connectivity. A scalar
field f on
 S can be defined as a piecewise linear function
f (v) = i fi φi (v), where f i is a scalar, φ i (·) is a piecewise
linear basis function with value 1 at vertex v i and 0 at other
vertices. The discrete gradient operator of an arbitrary scalar
field f on S is defined as [TLHD03]:
∇f (v) :=



fi ∇φi (v).

(1)

i

It yields the discrete Laplacian operator at each vertex v i :
f (vi ) :=



1
(cot Bj + cot Cj )(fi − fj ). (2)
2A
j
(v )

vj ∈Nv i

3. Our Approach

Here, N v (v i ) denotes the set of 1-ring neighbouring vertices
of v i . A j is the area of the jth triangle, and B j and C j are
two angles opposite to the edge (v i , v j ).

We construct triangulations to outline the boundary of object regions and use this topological information to create
deformations by separating parts of the images.

If we set f i for two vertex coordinates, vm
i (m = x, y),
separately, S can be viewed as two discrete scalar fields (S x ,
S y ). By applying the discrete gradient operator to S m (m =


c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

118

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

x, y) separately, we get two gradient vector fields:

∇S m (v) =
vm
i ∇φi (v) (m = x, y).

m
where k m
Bj and k Cj are the rigidities of two triangles that share
the edge (v i , v j ).

(3)
Finally, we have the rigidity-related Poisson equation:

i

Each ∇S m is a piecewise constant vector field which can
be regarded as the guidance vector field w involved in solving
the discrete Poisson equation:
f = div · w, f |∂ = f ∗ |∂

(4)

where f is the scalar field to be computed and f ∗ provides
the desired value on the boundary ∂. Manipulating this
guidance vector field w will result in varied reconstructed
vertex coordinates.
By specifying a set of boundary vertices ∂ and f ∗ |∂
to construct the boundary condition, we have two equations
from Equation (4) for solving the vertex coordinates (x and
y) in the form of f . The user can modify the position and orientation of a set of vertices, resulting in two altered guidance
vector fields ∇Sx and ∇S y . The new vertex positions (S x , S y )
can be reconstructed by solving Equation (4) with modified
∇S x and ∇S y .
3.2. Property-related deformation
The basic idea for achieving expressive deformation is to
define appropriate properties and use them to guide the
mesh manipulation. An intuitive choice is the rigidity or
elasticity that describes the stretching degree under various
forces. With the triangulation representation, we regard the
rigidity distribution of the illustrated objects as two piecewise constant scalar fields and assign each triangle, T, two
constant rigidities kTm (m = x, y) for (x, y). This definition
yields two new piecewise linear basis functions kTm φ i (·) (m
= x, y). Intuitively, kTm (m = x, y) scales the discrete gradient
vector along the x and
 and can be represented with a
 xy axes,
kT 0
. If we consider the deformation
2D transformation
y
0 kT
intention imposed on the triangle T as an affine transformation, kTm (m = x, y) defines the response coefficients of T
to the deformation intention along the x and y axes, respectively. Accordingly, we denote the property-related discrete
divergence operator div km as:
divkm · ∇f (vi )

kTm AT ∇φi |T · ∇f (vi ) (m = x, y).
:=
T ∈NT (vi )

(5)

It also induces the new discrete Laplacian operator:
km f (vi )

:=

1
2A
j
(v )

 

 2
2
kBmj cot Bj + kCmj cot Cj (fi − fj )

vj ∈Nv i

(6)

km f = divkm · w, f |∂ = f ∗ |∂.

(7)

In practice, the rigidities are determined by the user and
are attached to each region of the illustrated objects. A typical
range of the rigidities are from 1.0 to 10.0. The smallest value,
1.0, corresponds to softly deformed regions, such as the fat
in the human body. For fully rigid objects, such as the bones,
the rigidities are set to be the largest value, i.e. k x = 10.0,
k y = 10.0. Beyond distinguishing between elastic and rigid,
this technique simulates the resiliency, i.e. the degree to
which an elastic object resists deformation. Thus, at one
end of the spectrum we could have an elastic material and
at the other a rigid material, and somewhere in the middle
would be user-selectable resilient materials. In the biomedical arena, these resiliency factors could be based on actual
tissue properties. Our approach simulates this effect. In addition, non-uniform deformation in the 2D plane can be simulated by setting different rigidities for any direction (x, y).
One example is the muscle on the legs which exhibits a directional deformation. Figure 5 depicts different behaviours
under three types of rigidity configurations of the selected
region.
3.3. Shape context preserving deformation
By shape context, we mean any measurement of the configuration of shape features that does not change when the global
shape stretches or shrinks. Figure 6a and b show two faces
with 16 landmarks. In each example, the set of feature points
constructs a recognizable identification and a special shape
context that can differentiate itself from others.
Although there have been several definitions for shape descriptors, we find that most of them are unsuitable for the purpose of shape-preserving image manipulation. For instance,
the shape descriptor introduced in [BMP02] describes the
coarse distribution of the rest of the shape with respect to a
given point on the shape. It is basically a statistical property
and cannot be quantized and manipulated. A less statistical
characterization of geometrical shape would be the properties of a figure that are not changed by translation, rotation or
scaling [Boo92]. However, the shape coordinates representation proposed in [Boo92] only considers the distance ratios
and measures the triangular shape that maintains the distinctions among three vertices. In our approach, we define the
shape context as an inter-dependency of a group of feature
points, i.e. a 2D mesh structure C = (K̂, V̂ ) strictly built on
these points. All vertices of C are selected from V, meaning
V̂ ⊂ V . To make each triangle of C as canonical as possible,
the mesh connectivity K̂ is constructed with constrained Delaunay triangulation algorithm [She05] and is not necessarily
a subset of K.


c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

Figure 5: The deformation results with different rigidity settings for a bone with a red star. (a) The input image; (b)
uniform strong elasticities: (k x , k y ) = (1.0, 1.0); (c) nonuniform rigidities: (k x , k y ) = (10.0, 1.0); (d) uniform strong
rigidities: (k x , k y ) = (10.0, 10.0). The rigidities for other two
bones are set as (k x , k y ) = (10.0, 10.0).
We perform shape context preserving deformation by imposing an additional constraint on the discrete Poisson equation. The goal is to transform the discrete guidance field on
C in a uniform way. Thus, we reformulate Equation (7) as
∗

km f |S + λkm f |C = divkm · w + λdivkm · u , f |∂
∗

= f |∂.

(8)

In Equation (8), λ is an adjustable parameter that weights
the preservation of the shape context. The definitions of ∇C m
and u∗ are analogous to those of ∇S m and w∗ (see Section
3.1). During mesh manipulation, w∗ is interactively altered
by users, i.e. each triangle of S may be modified with different
rotation, scale or translation transformations. Whereas all triangles in C are transformed with the same rotation and scale
transformations for the purpose of preserving
the shape con-⎞
⎛
cos θ − sin θ ⎟
text. Therefore, u∗ = R(θ)S(s) u with R(θ )=⎝⎜
⎠
sin θ
cos θ


s 0
and S(s) =
. The parameters θ and s are unknown
0s

119

Figure 6: (a, b) Two line-drawing images and their shape
context points [Boo92]; (c) the face of an old man; (d) our
result by replacing the shape context of (c) with that of (b).
and have to be determined in each deformation step. To find
an optimal solution, we calculate the centroid v c of C and
seek to keep the relative distances and angles between each
vertex v i in C and v c unchanged. This yields the following
minimization problem:

argminθ,s
(v∗i − v∗c − R(θ)S(s)(vi − vc ))2 (9)
vi ∈C

where v∗i denotes the ith transformed vertex.
With the assumption that the deformation is adequately
stable, we employ an iterative technique to progressively approximate the optimized solution. In the first step, we solve
Equation (7) to compute the modified vertex positions, without considering the influences of shape context. Based on
these vertices, we calculate an average scale s and use it to
estimate an optimized θ in Equation (9). Subsequently, we
solve Equation (8) with the new R (θ) and S (s) to get an initial estimation of the new vertices. In the next iteration, the
computed vertices can be used to recover a better estimation
of θ and s, and so on. The iteration ends when either the iteration number exceeds a given maximum, or the difference
between θ and s between two successive steps is smaller than
a given threshold.


c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

120

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

toolkits, including a region constructor, a mesh constructor, a
region of interest (ROI) selector and a mesh manipulator, to
interactively deform the illustration in an intuitive fashion. In
the context of solving the property-related Poisson equation,
all interactions are intended to alter the guidance gradient
vector field and specify the boundary conditions. As a result, the efficiency of our system is greatly influenced by the
modes of how to capture the user intentions and then change
the guidance vector field and boundary conditions for the
specified manipulation tasks. Below, we describe each operation step that is sequentially demonstrated in Figure 7.
3.4.1. Region construction
Beginning from an input illustration, the user manually constructs a polygonal contour for each region and specifies its
rigidities. This process is analogous to the segmentation of
images and could be accelerated by directly adjusting the
boundaries formed from interactive image segmentation. In
general, the mesh construction may be not accurate. The average case can be constructed in several minutes. Figure 7b
shows the constructed region contours on an input illustration
(Figure 7a). For the sake of demonstration, the constructed
contours are rough.
3.4.2. Mesh construction

Figure 7: The pipeline of our system: (a) the input image
(MS-0101, Muscular System @1989 John M. Daugherty,
Highlight Studios); (b) the constructed region contours done
in 2 minutes. For the sake of demonstration, they are rough.
(c) The constructed mesh and the sets of controlling vertices,
constrained vertices and free vertices, which are shown in
yellow, red and other colours; (d) after rotating the chest, we
specify a new set of controlling vertices, constrained vertices
and free vertices, for the purpose of rotating the head; (e) the
final result without distance weighting; (f) with the Gaussian
weighting scheme, a more natural effect is achieved. We enlarge the head parts to show the difference between (e) and
(f).
It is worth mentioning that the weight λ is set to be a small
value at the first iteration. This is automatically enlarged in
the next iterations because the influence of the shape context
would gradually become dominant when approaching the
optimized solutions. Figure 6d shows the deformation result
by entirely replacing the shape context of Figure 6c with that
of Figure 6b.
3.4. Processing stages
We have built an interactive 2D deformation system for 2D
anatomical illustrations. It allows the user to employ a set of

We apply a constrained conforming Delaunay triangulation
algorithm [She05] to the constructed region contours. Note
that this algorithm does not result in a true Delaunay triangulation, i.e. some triangles might not be Delaunay, but all
triangles are ensured to be constrained Delaunay. To achieve
optimized mesh structures, two additional requirements for
triangulation are set. First, the vertices and edges of the region contours are completely preserved in the final mesh.
Second, a minimum on the triangle inner angles and a maximum on the triangle area are set. The constructed mesh based
on Figure 7b is shown in Figure 7c.
3.4.3. ROI selection
By region of interest (ROI), we mean the region where
the user’s intentions are imposed. It consists of two parts: the
operation handles and the free region. The former denotes
the actual user-controlled vertices and is used to determine
the modification of the discrete guidance vector field in Equation (8). The free region includes the vertices whose positions
are freely changed during deformation. Other vertices that do
not belong to the ROI are kept unchanged and are called constrained vertices. Our system allows the user to manually
select a sequence of vertices or simply choose the vertices
of one region as the controlling vertices or the constrained
vertices. The remaining vertices are by default free vertices.
Additionally, the user can determine a group of shape context vertices and construct a shape context mesh. Figure 7c


c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

121

depicts the determined controlling vertices (in yellow), constrained vertices (in red) and free vertices in other colours.
After deforming the illustration by rotating the chest, we have
the deformed mesh shown in Figure 7d. To further modify
the illustration, we fix the body trunk and let the head be the
controlling region (see Figure 7d).

3.4.4. Mesh mmanipulation
During mesh manipulation, the user can either freely drag
one or several controlling vertices, or modify the controlling
vertices with specified 2D transformations. Another commonly used interaction mode is to simultaneously translate,
rotate or scale all vertices of a selected region. Thereafter, the
modifications on the controlling vertices are gradually propagated to the remaining vertices. The propagation is dependent
on the distance between vertex pairs. Our system supports
three distance-dependent functions which correspond to the
nearest, linear and Gaussian weighting schemes. After the
propagation of the transformations to all vertices, a new constant gradient vector is computed in each triangle, yielding
two altered guidance vector fields. Finally, the discrete Poisson equation is formulated as a sparse linear system and
solved with a direct solver [Tol03]. In Figure 7e and f, we
show two results with the same manipulation settings and
different weighting schemes.

Figure 8: (a) A volume rendering by Stefan Bruckner
[BG05]; (b) our result; (c) another 2D image; (d) our result by simulating the shape styles of (c).
The Laplacian operator on each vertex v i is defined as
(vi ) = vi − (vi−1 + vi+1 )/2.

(11)

The shape transfer from R 0 to R 1 is accomplished by replacing the gradient vectors of R 1 to those of R 0 and specifying constrained vertices in Equation (4). Note that, R 0 and
R 1 have to be aligned and resampled with arc-length parameterization before shape transfer, so that their vertices are a
one-to-one correspondence.
In the second step, the modification of the region contour
is integrated into the Poisson equation, Equation (7) or (8),
to alter the guidance vector field and the boundary condition.
Figures 8d and 9 provide examples.

4. Analogy Based Shape Style Transfer
Transferring shape styles between illustrations requires appropriate representations of shape styles. In our system, we
consider two types of shape styles.

4.1. Shape transfer with region contours
Object contours are important features for shape recognition
[BMP02]. Every region boundary of the underlying image
can be regarded as a formulation of its shape style. The
transfer of region contours can be fulfilled by a simple curve
analogy driven shape deformation scheme.
For a selected region, we first deform its boundary with a
differential curve deformation technique. Suppose we have
two illustrations whose meshes are M 0 and M 1 . The boundary R 0 of one region in M 0 is modified to mimic another
region boundary R 1 in M 1 . R 0 and R 1 are represented with
two polygons, which can be regarded as two piece-wise linear curves. Analogous to the definitions of 2D scalar fields
and the discrete differential operators in Section 3.1, we construct a one-dimensional differential representation for a linear curve R = (K, V). For the sake of simplicity, the discrete
gradient operator on an arbitrary scalar field f defined in R
can be rewritten as
∇f (v) = vi − vi−1

−→
s.t. v ∈ −
v−
i−1 vi .

(10)

4.2. Shape transfer with shape context
In terms of shape context, its transfer should be performed between two illustrations that share a similar context. Two sets
of shape context vertices with identical vertex numbers are
first specified in the source illustration and the destination.
The pairwise correspondences between vertices are determined by the user. We regard the transfer of shape context as
solving the following Poisson equation (m = x, y):
km f |S + λkm f |C = divkm · w̄ + λdivkm · u∗ , f |∂
= f ∗ |∂.

(12)

Here, S and C are the mesh and shape context mesh of
the destination illustration. w̄ denotes the altered guidance
vector field after introducing the new shape context. u∗ is the
guidance vector field of the shape context mesh in the source
illustration. Similar to the shape context preserving deformation, a global rotation and a global scale transformation have
to be determined before the shape transfer in order to eliminate the problem caused by the rotation-variant property of
the discrete Laplacian operator.
5. Experimental Results and Discussions
We have tested our approach on a PC with an Intel P4 3.2G
HZ CPU. For a typical image at the resolution of 380 × 380,


c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

122

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

Figure 9: Shape styles of (a) are transferred to (b) and (d), yielding (c) and (e), respectively.
the constructed mesh normally contains 300 ∼ 1500 vertices
and 500 ∼ 2000 triangles. The user’s time spent on mesh
construction and ROI specification is typically 60–120 and
15–30 seconds, respectively. With a non-optimized Poisson
solver, the average times for the transformation propagation
and mesh reconstructions are 10 milliseconds and 15 milliseconds. In each example we tested, the deformation and
shape transfer is performed in real-time. For a fine mesh
with more than 50 000 vertices, our system can still perform
at interactive frame rates.
In our implementation, we regard the 2D illustration as a
2D texture and map it to a 2D triangular mesh. The image
deformation is driven by the mesh manipulation, in a similar
mode to previous mesh-based image deformation techniques
∗
(e.g. [IMH05] and [WXW 06]). We use OpenGL to implement the texture mapping for high efficiency. All input illustrations are rescaled to a fixed window resolution (e.g. 380 ×
380). Due to the rescaling of the input illustrations, there are
some differences between the input illustrations and the deformed results. In addition, some fuzziness may appear in the
boundary of the shape, due to the deformation of the mesh
and the use of texture mapping (such as the line drawings
shown in Figure 9c and e).
5.1. Results
We tested our approach on the rendering results shown in
Figures 2a and 8a. By setting appropriate rigidities to the
skin, bone and swimming bladder, satisfying deformations
are easily achieved (see Figures 2c and 8b and d). Specifically, shape styles of Figures 2b and 8c are transferred to

Figures 2c and 8c. The user interaction time for each example
is approximately 1 minute.
There are various terms describing the movements of the
limbs and other parts of the human body. They can be classified into about eleven categories, namely, flexion, extension, abduction, adduction, rotation, circumduction, opposition, retrusion and protrusion, elevation, eversion, pronation
[MD99]. We tested our system on several 2D hand-drawn illustrations of these anatomical movement types. Figure 10a–
c shows three hand-drawn illustrations that depict the hyperflexion and hyperextension of the cervical tissues of a human
head. Our approach requires 10 minutes to interactively illustrate both deformations (see Figure 10e and f) based on
Figure 10d. Figure 11a and c demonstrates our results that
simulate the depression and elevation movements of a human
body.
Anatomical shape and its variation, play important roles in
medical research. A particular disorder or aging may cause
anatomical changes or differences. In medical illustration,
the abnormality of certain human anatomy can be illustrated
using a set of comparison images. By referring to two sets of
2D hand-drawn illustrations, we performed two experiments.
The first one illustrates the systolic (Figure 11e) and diastolic
(Figure 11f) dysfunctions of a human heart. Compared to the
normal status of filling blood (Figure 11d), the systolic case
presents an over-enlarged ventricle because too much blood
is filled and the diastolic one fills with less blood because the
material of the ventricle is too stiff.
Introducing properties into different regions results in expressive deformation effects. Figures 5 and 7 show two such


c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

123

Figure 10: Three hand-drawn images (a, c) that depict the hyperflexion and hyperextension of the cervical tissues of a human
head. With the input image (d), our approach allows a user to interactively simulate this dynamic procedure in 10 minutes,
resulting in (e) and (f).
examples. Note that our solution is not physically based, and
its results do not strictly conform to results generated by 3D
deformation approaches as 3D transformations and deformations may be distorted after perspective projection. Nevertheless, satisfying results can still be achieved by interactively
adjusting the engaged 2D transformations. Shape context
preserving deformation and shape transfer are very useful
operations for content recreation in computer-generated illustration. The examples shown in Figures 1, 6, 9 and 12
demonstrate the efficiency of our approach.
5.2. Failure cases
In terms of limitations, our approach cannot handle the case
when self-collision occurs (see Figure 13a and b). One pos-

sible solution is to add an additional self-collision detection
constraint on the discrete Poisson Equation (7), in a similar
way as the shape context preserving Poisson Equation (8).
In addition, our approach is suitable for simulating continuous deformations because the topology information has
to be preserved during manipulation. In the case of object
splitting or fractures, as shown in Figure 13c, an objectbased image editing mechanism [BC02] may be introduced
for illustrating the separations of individual objects.
Another failure case is caused by object occlusion during object animation or deformation (see Figure 13d and e).
Although it is a common problem for 2D image editing techniques, we hope that our approach can overcome this obstacle
through combinations with 3D algorithms. For example, we


c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

124

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

cial 2D deformation toolkits, such as Avid ‘Elastic Reality’
program [AVI], are available for artists to use. In any case,
our approach could be regarded as a reliable addition, or
complementary technique, to existing 3D approaches.
Our scheme works in these situations because it reformulates complicated 3D transformations as a set of 2D
affine, similarity and rigid transformations. The degree of
2D transformations can be interactively modulated by users.
Additionally, our approach would be a very useful tool
to post-process rendered images under selected viewpoint,
cutting-plane and lighting conditions.

Figure 11: Two results with our approach: (a) an input abdomen image (Image Courtesy: Deutscher Infografik Dienst:
http://www.infografikdienst.de); (b–c) the images showing
the depression and elevation movements; (d) an input heart
image; (e, f) our results depicting the systolic and diastolic
dysfunctions when the ventricles fill with blood.
can represent the results by a 3D approach with a set of depth
layers and manipulate these layers on a per-object basis.

We have received encouraging feedback from an experienced medical illustrator [And]. His evaluation states that
‘You have come up with a remarkable and useful program.
I have used Avid’s “Elastic Reality” program some in the
past, but your program appears much superior. My first and
foremost concern was bending soft, plastic tissues without
distorting rigid ones. You have allayed my concerns completely, and in a very solid manner’.
6. Conclusions and Future Work
In this paper, we address the problem of 2D shape deformation by means of a differential mesh manipulation approach.
Our system allows the users to flexibly design a sequence of
dynamic effects in an intuitive fashion. We also introduce a
new description to shape context that is amenable for shape
context preserving deformation and shape transfer.

5.3. Discussion and evaluation
Our approach is basically a 2D solution and is not physically accurate. This disadvantage is mainly caused by the
infeasibility of image manipulation for 3D transformation
and lighting. However, in many cases, we do believe that our
approach is more efficient than existing 3D methods due to
the simplicity of 2D image manipulation. Indeed, commer-

Shape depiction depends on not only the shape styles, but
also the lighting and rendering configurations. Our current
approach does not consider the influences from perspective
projection, shadow and lighting change during deformation.
Exploring lighting with regard to deformation is important,
and could greatly enhance the value of the proposed method.
We plan to exploit a more efficient strategy based on previous

Figure 12: (a) An input image; (b) a reference image with 16 landmarks; (c) the constructed mesh based on (a); (d) our result
by transferring the shape context from (b) to (a).

c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

125

[AVI] Available at http://www.avid.com/. Elastic reality.
Software release.
[BC02] BARRETT W. A., CHENEY A. S.: Object-based image
editing. In Proceedings of ACM SIGGRAPH (2002), pp.
777–784.
[BG05] BRUCKNER S., GRÖLLER M. E.: VolumeShop: An
interactive system for direct volume illustration. In Proceedings of IEEE Visualization (2005), pp. 671–678.
[BKR∗ 05] BURNS M., KLAWE J., RUSINKIEWICZ S.,
FINKELSTEIN A., DECARLO D.: Line drawings from volume
data. ACM Transactions on Graphics 24, 3 (2005), 512–
518.
[BMP02] BELONGIE S., MALIK J., PUZICHA J.: Shape matching and object recognition using shape contexts. IEEE
Transactions on Pattern Analysis Machine Intelligence
24, 4 (2002), 509–522.
[Boo89] BOOKSTEIN F. L.: Principal warps: Thin-plate
splines and the decomposition of deformations. IEEE
Transactions on Pattern Analysis and Machine Intelligence 11, 6 (1989), 567–585.
Figure 13: Failure cases: (a) a knee; (b) our approach may
cause self-collision when moving one bone; (c) the compound
fracture of the bone; (d) a heart with a saphenous vein graft;
(e) a heart with the left internal thoracic artery grafted to the
anterior descending coronary artery.

image-based rendering approaches to simulate the shading
variations during deformation. Another potential issue is to
extend the property-related differential mesh manipulation
algorithm to 3D tetrahedral meshes. More vivid and complicated 3D deformation can be simulated in a shape-preserving
mode. The main challenge for this is the high computational
cost caused by the large number of tetrahedron primitives.
Moving the computations of the linear system solution and
volume rendering onto graphics processing units is a promising solution.
Acknowledgments
The authors would like to thank Stefan Bruckner and
Nikolai Svakhine for providing images, and thank Nvidia
for equipment donations. This work is partially supported
by NSFC (No. 60873123) 863 program of China (No.
2006AA01Z314), NSF Grants 0081581, 0121288, 0328984,
and the U.S. Department of Homeland Security.

[Boo92] BOOKSTEIN F. L.: Morphometric tools for landmark
data. Cambridge University Press, Cambridge, UK, 1992.
[BSM∗ 07] BRESLAV S., SZERSZEN K., MARKOSIAN L., BARLA
P., THOLLOT J.: Dynamic 2D patterns for shading 3D
scenes. ACM Transactions on Graphics 26, 3 (2007), 366–
373.
[Cla99] CLARK J. O.: A Visual Guide to the Human Body.
Barnes and Noble Books, New York, USA, 1999.
[ER02] EBERT D., RHEINGANS P.: Volume illustration: Nonphotorealistic rendering of volume models. In Proceedings of IEEE Visualization (2002), pp. 253–264.
[FH07] FANG H., HART J. C.: Detail preserving shape deformation in image editing. ACM Transactions on Graphics
26, 3 (2007), 12–16.
[GG01] GOOCH B., GOOCH A.: Non-Photorealistic Rendering. A.K. Peters, USA, 2001.
∗

[HJO 01] HERTZMANN A., JACOBS C. E., OLIVER N., CURLESS
B., SALESIN D. H.: Image analogies. In Proceedings of
ACM SIGGRAPH (2001), pp. 327–340.

References

[HOCS02] HERTZMANN A., OLIVER N., CURLESS B., SEITZ
S. M.: Curve analogies. In Proceedings of Eurographics
Workshop on Rendering (2002), pp. 233–245.

[And] ANDREWS W. M.: Personal Communication. William
M. Andrews, Available at http://www.mcg.edu/medart/
MI-Faculty.html.

[Hod88] HODGES E.: The Guild Handbook of Scientific Illustration. John Wiley and Sons, Inc, Hoboken, USA,
1988.


c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

126

W. Chen et al. / Shape Context Preserving Deformation of 2D Anatomical Illustrations

[HS99] HAMEL J., STROTHOTTE T.: Capturing and re-using
rendition styles for non-photorealistic rendering. Computer Graphics Forum 18, 3 (1999), 173–182.

http://www.cs.cmu.edu/q̃uake/triangle.html, last visited in
July 2005.
∗

[IMH05] IGARASHI T., MOSCOVICH T., HUGHES J. F.: As-rigidas-possible shape manipulation. In Proceedings of ACM
SIGGRAPH (2005), pp. 327–340.

[SLCO 04] SORKINE O., LIPMAN Y., COHEN-OR D., ALEXA
M., RÖSSL C., SEIDEL H.-P.: Laplacian surface editing. In
Proceedings of the Eurographics/ACM SIGGRAPH symposium on Geometry processing (2004), pp. 179–188.

[Ito] ITO K.: Annotated list of fish illustrations. Available at http://www.hrw.com, last visited date: March
2007.

[SMW06] SCHAEFER S., MCPHAIL T., WARREN J.: Image deformation using moving least squares. ACM Transactions
on Graphics 25, 3 (2006), 533–540.

[KML99] KIRBY R., MARMANIS H., LAIDLAW D.: Visualizing
multivalued data from 2D incompressible flows using concepts from painting. In Proceedings of IEEE Visualization
(1999), pp. 333–340.

[ST90] STAUBESAND J., TAYLOR A. N.: Sobotta Atlas of
Human Anatomy. Urban and Schwarzenberg BaltimoreMunich, 1990.

∗

[LEM 02] LU A., EBERT D., MORRIS C., RHEINGANS P.,
HANSEN C.: Non-photorealistic volume rendering using
stippling techniques. In Proceedings of IEEE Visualization (2002), pp. 211–218.
[MD99] MORE K. L., DALLEY A. F.: Clinically Oriented Anatomy. Lippincott Williams and Wilkins Corp,
Philadephia, USA, 1999.
[ONOI04] OWADA S., NIELSEN F., OKABE M., IGARASHI T.:
Volume illustration: Designing 3D models with internal
textures. In Proceedings of ACM SIGGRAPH (2004), pp.
322–328.
[PGB03] PEREZ P., GANGNET M., BLAKE A.: Poisson image
editing. In Proceedings of ACM SIGGRAPH (2003), pp.
313–318.
[SDS05] SVAKHINE N., D. EBERT, STREDNEY D.: Illustration
motifs for effective medical volume illustration. IEEE
Computer Graphics and Applications 25, 3 (2005).
[She05] SHEWCHUK J. R.: A two-dimensional quality
mesh generator and delaunay triangulator. Available at

[Tho17] THOMPSON D.: On Growth and Form. Cambridge
University Press, Cambridge, UK, 1917.
[TLHD03] TONG Y., LOMBEYDA S., HIRANI A. N., DESBRUN
M.: Discrete multiscale vector field decomposition. ACM
Transactions on Graphics 22, 3 (2003), 445–452.
[Tol03] TOLEDO S.: TAUCS: A Library of Sparse Linear
Solvers, version 2.2. Tel-Aviv University, Available at
http://www.tau.ac.il/s̃toledo/taucs/, 2003.
∗

[WAA 05] WILEY D. F., AMENTA N., ALCANTARA D. A.,
GHOSH D., KIL Y. J., DELSON E., HARCOURT-SMITH W., ROHLF
F. J., JOHN K. S., HAMANN B.: Evolutionary morphing. In
Proceedings of IEEE Visualization 2005 (2005), pp. 431–
438.
∗

[WXW 06] WENG Y., XU W., WU Y., ZHOU K., GUO B.: 2D
shape deformation using nonlinear least squares optimization. The Visual Computer 22, 9 (2006), 653–660.
∗

[YZX 04] YU Y., ZHOU K., XU D., SHI X., BAO H., GUO B.,
SHUM H.-Y.: Mesh editing with poisson-based gradient
field manipulation. ACM Transactions on Graphics 23, 3
(2004), 644–651.


c 2008 The Authors
c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

Understanding Syndromic Hotspots - A Visual Analytics Approach
Ross Maciejewski∗
Stephen Rudolph∗
Ryan Hafen∗
Ahmad Abusalah∗
Mohamed Yakout∗
∗
∗
†
Mourad Ouzzani
William S. Cleveland
Shaun J. Grannis
Michael Wade‡
∗
David S. Ebert
∗ Purdue

University Regional Visualization and Analytics Center (PURVAC)
Institute and Indiana University School of Medicine
‡ Indiana State Department of Health

† Regenstrief

A BSTRACT
When analyzing syndromic surveillance data, health care officials
look for areas with unusually high cases of syndromes. Unfortunately, many outbreaks are difficult to detect because their signal is
obscured by the statistical noise. Consequently, many detection algorithms have a high false positive rate. While many false alerts can
be easily filtered by trained epidemiologists, others require health
officials to drill down into the data, analyzing specific segments
of the population and historical trends over time and space. Furthermore, the ability to accurately recognize meaningful patterns in
the data becomes more challenging as these data sources increase
in volume and complexity. To facilitate more accurate and efficient event detection, we have created a visual analytics tool that
provides analysts with linked geo-spatiotemporal and statistical analytic views. We model syndromic hotspots by applying a kernel
density estimation on the population sample. When an analyst selects a syndromic hotspot, temporal statistical graphs of the hotspot
are created. Similarly, regions in the statistical plots may be selected to generate geospatial features specific to the current time
period. Demographic filtering can then be combined to determine
if certain populations are more affected than others. These tools
allow analysts to perform real-time hypothesis testing and evaluation.
1

M OTIVATION

Recently, the detection of adverse health events has focused on
pre-diagnosis information to improve response time. This type of
detection is more largely termed syndromic surveillance and involves the collection and analysis of statistical health trend data,
most notably symptoms reported by individuals seeking care in
emergency departments. Currently, the Indiana State Department
of Health (ISDH) employs a state syndromic surveillance system called PHESS (Public Health Emergency Surveillance System)
[9], which receives electronically transmitted patient data (in the
form of emergency department chief complaints) from 73 hospitals
around the state at an average rate of 7000 records per day.
These complaints are then classified into nine categories (respiratory, gastro-intestinal, hemorrhagic, rash, fever, neurological,
botulinic, shock/coma, and other) [4] and used as indicators to detect public health emergencies before such an event is confirmed
by diagnoses or overt activity. Unfortunately, detection of events
from these indicators is an extremely challenging issue. Figure 1
shows a typical month of emergency department visits for those
complaints classified as neurological syndromes. During this time
period, there was one event of carbon monoxide poisoning which
happened to coincide with the largest peak on December 21st; however, this peak is not significantly higher than any other peak during

IEEE Symposium on Visual Analytics Science and Technology
October 21 - 23, Columbus, Ohio, USA
978-1-4244-2935-6/08/$25.00 ©2008 IEEE

this month. Obviously, the detection of such a small signal deviation can be extremely difficult.

Figure 1: A sample syndromic surveillance signal containing a carbon monoxide poisoning event.

In order to facilitate enhanced syndromic surveillance and
improve signal detection, we have developed a linked geospatiotemporal visual analytics tool designed for advanced data exploration for epidemiologists and other healthcare officials. The
system was designed from its inception in collaboration with health
surveillance experts, state healthcare officials and epidemiologists
to address their needs. Our system features include:
• A new kernel density estimation that works for both urban and
rural populations
• Dually linked interactive displays for multi-domain/multivariate exploration and analysis
• Novel data aggregation for effective visualization and privacy
preservation
• Control charts for identifying temporal signal alerts
• Demographic filter controls that enable database querying and
analysis through a simple graphical interface
Our work focuses on advanced interactive visualization and analysis methods providing linked environments of geospatial data
and time series graphs. Syndromic hotspots found in one display
method can be selected and immediately analyzed in the corresponding linked view. Furthermore, our work focuses on the early
detection and analysis of syndromic hotspots facilitated through the
use of control charts for outbreak detection. Alerts generated in the
temporal realm can be quickly analyzed in the geo-spatiotemporal
interface, helping users find patterns simultaneously in the spatial

35

and temporal domains. Concurrently, we have also applied statistical modeling techniques to estimate syndrome distributions in the
spatial realm. Users may select syndromic hotspots from the generated heatmaps and analyze historical time series data in the area to
look for unusual trends or potential outbreaks. Such doubly linked
views allow users to quickly form and test hypotheses, thereby reducing the time needed to reject false positives and confirm true
outbreaks.
2

P REVIOUS W ORK

Data from public health surveillance systems has long been recognized as providing meaningful measures for disease risks in populations [12, 23]. As such, many disease modeling packages, outbreak
alert algorithms and data exploration systems have been developed
to aid epidemiologists in identifying outbreaks within their data.
Some of the most popular of these systems are the Early Aberration Reporting System (EARS) [10], the Electronic Surveillance
System for the Early Notification of Community based Epidemics
ESSENCE [14], and Biosense [15]. Unfortunately, all of these systems offer limited data exploration tools and little-to-no interactive
geospatial support. Furthermore, many detection algorithms employed by these systems generate a large amount of false positives
for epidemiologists to analyze. While creating algorithms to reduce
false positives is important, our work focuses on creating advanced
visual analytics tools for more efficiently exploring these alerts and
hypotheses.
Our work employs a variety of methods from information visualization and geographical visualization as a basis for creating
an advanced visualization and analytics environment. This section
briefly describes components from related work and addresses their
applicability to syndromic surveillance.
A key component that must be addressed in syndromic surveillance data is the geo-spatiotemporal nature of the data. Geographic
visualization is a field focused on displaying data with a geographic
context such as a map. In more recent years, it has ballooned to
include increasingly complex data, other spatial contexts, and information with a temporal component.
Several current systems exist that leverage advanced geographical visualization techniques for various health data. MacEachren
et al. [17] presented a system designed to facilitate the exploration
of time series, multivariate, geo-referenced health statistics. Their
system employed linked brushing and time series animation to help
domain experts locate spatiotemporal patterns. Further work in analyzing health statistics was done by Edsall et al. [6]. Here, the use
of interactive parallel coordinate plots was used to explore mortality data as it relates to socio-economic factors. Schulze-Wollgast et
al. [25] developed a system for visualizing health data for the German state Mecklenburg-Vorpommern. This system allowed users to
interactively select diseases and their parameters and view the data
over a specific time interval at different temporal resolutions. Further work in this system [26] employed the use of intuitive 3D pencil and helix icons for visualizing multiple dependent data attributes
and emphasizing the type of underlying temporal dependency.
Many of these previous systems provided useful visualization
and exploration of data, but did not support interactive analysis. To
address this gap, visual analytics has emerged as a relatively new
field formed at the intersection of analytical reasoning and interactive visual interfaces [24]. It is primarily concerned with presenting
large amounts of information in a comprehensive and interactive
manner. By doing so, it is hoped that the end user will be able to
quickly assess important data and, if required, investigate points of
interest in detail. The branch of visual analytics with which we are
most concerned for this paper is that of geospatial and temporal analytics, which applies the concepts of visual analytics to problems
rooted in space and time.
A few examples of recent work in the spatiotemporal branch

36

of visual analytics include VIST-STAMP by Liao et al. [13], FemaRepViz by Pan and Mitra [20], and LAHVA by Maciejewski et
al. [18]. VIS-STAMP supports the overview of complex patterns
through a variety of user interactions. Specifically, this work focuses on visualizing multivariate patterns using parallel coordinate
plots and self organizing maps. FemaRepViz provides a display
of Federal Emergency Management Agency (FEMA) reports on
a globe and dynamically determines where each report should be
placed based on the text of the report. It also allows the user to
navigate through time; displaying only the relevant reports for that
period. And finally, LAHVA looked at using multiple datasets (pet
and human health data) with similar properties to enhance disease
surveillance. This system provided a geo-spatiotemporal interface
with limited interaction amongst different view windows.
3

V ISUAL A NALYTIC E NVIRONMENT

Our system adopts the common method of displaying georeferenced data on a map and allowing users to temporally scroll
through their data. However, such exploration only provides slices
of spatial data at a given time or an aggregate thereof. In order
to understand these slices, users need to know the trends of previous data (and, if possible, model future data trends). Furthermore, a limiting factor in using mapping as a tool for syndromic
surveillance is that aggregation of data can lead to unreliable estimates of the true measure of infection. Fortunately, the PHESS
data used in our visual analytics system provides geo-referenced
patient locations, allowing us to either aggregate the data on a spatial level, or employ statistical methods to model the data over arbitrarily sized geo-regions. As such, our system employs advanced
statistical models for data exploration, enabling new visualizations,
analyses, and enhanced detection methods.
3.1

System Features

Figure 2 (Left) provides a conceptual overview of our visual analytics system, and Figure 2 (Right) provides a screenshot of the system. Data entering our system first undergoes a cleaning and transformation process. This process fills in missing patient information
from past visit information, normally distributes patients with unknown addresses to locations within their county, and aggregates
patient visits that occur with similar syndromes multiple times on
a given day. This process is then refined through feedback from
our visual analytics system. Furthermore, the user may report data
errors as well, allowing for data correction. Finally, frequently accessed time series models of the data are also stored in the database
for future use after initial modeling is done via our visual analytics
system.
Further interaction is performed within the different viewing and
modeling modalities of the system. As shown in Figure 2 (Right),
the main viewing area is the geo-spatiotemporal view, and the three
windows on the right allow users to view a variety of data sources
simultaneously for a quick comparison of trends across varying
hospitals or data aggregated over spatial regions. Both the geospatial and time series viewing windows are linked to the time slider
at the lower portion of the screen. This allows users to view the
spatial changes in the data as they scroll across time. Additionally
temporal controls are also employed. These controls are denoted
as “aggregate” and “increment” in the scroll bar window. The aggregate function allows the user to show all data over a period of
x days. The increment function allows the user to step through the
data by increments of 1, 2, 3, ... days. All temporal views also provide a locking mechanism in which the user can choose to freeze
the data window(s) while exploring changes across time in other
views. This allows users to explore data while keeping a reference
point to the time-varying trend(s) under inspection.
Another key feature of our system is the interactive demographic
and syndrome filtering. Users interactively generate database

Figure 2: The visual analytics system. (Left) The conceptual diagram of our visual analytics system. Observe the interaction between the
analyst and the system as well as the modeling components of the system. (Right) Our visual analytics system. The left portion of the screen
represents the interactive database querying tools. We include checkboxes for classified syndromes, keyword searches for chief complaint text,
and demographic filtering for age and gender. The main viewing area is a geo-spatial temporal view that has pan and zoom controls in the upper
left corner. Hospitals and regions of the map may be selected with a circular query tool for interactive time series generation. The rightmost
windows are the temporal views, showing selected time series plots. Users may select points or regions of time to interactively manipulate the
geo-spatial temporal window. Finally, a time slider is included on the bottom portion of the screen, allowing users to move through time on all
unlocked screens.

search queries through the use of check boxes and edit controls
to find specific syndromes, keywords, and gender and age demographics amongst patients. Such work furthers hypothesis testing
as users can now quickly filter signals by demographic constraints
in order to see if adverse health conditions are targeting a particular segment of the population. The choices of filters affect both the
geo-spatiotemporal viewing area and all unlocked temporal plots.
3.2

Data Aggregation and Privacy Preservation

Our system also provides multiple views for enhanced visualization
and analysis. One simple, yet key view for this data set is showing georeferenced patient locations on the map in order to provide
health officials with a quick overview of health statistics across the
state. Unfortunately, showing exact patient locations on a map is
encumbered by privacy issues. Previous work in visualizing health
statistics bypasses these concerns by showing data spatially aggregated over geographical areas such as zip code or county. While
such visualizations are useful, there are times when it may be of interest to health officials to simply see a plot of patient locations on
a smaller level of data aggregation. Unfortunately, not all software
users have the same level of permissions for viewing this data.
A naive visualization method would be to zoom out of the map
at such a level that a pixel would represent a large enough region
that it would be difficult to extract any private information about
patients mapped on a transformed geolocation to pixel basis. Unfortunately, as the data set becomes arbitrarily large, the visual clutter can not be reduced in such a manner, see Figure 3 (Top-Left),
and it becomes clear that a visualization of every patient record at
a high spatial zoom level is not effective for analysis. Furthermore,
simple methods, such as using additive opacity to demonstrate patient density, Figure 3 (Top-Left), are inadequate as the number of
patients makes it impossible to readily distinguish density levels
between areas. This is further complicated when the syndromic patients are then highlighted with regard to their locations. Figure 3
(Top-Middle) shows the syndromic patients mapped in red. In order
to alleviate this problem, we have employed a method of data aggregation for enhanced visualization at low resolution views, which
also acts as a privacy preserving technique at low zooms.
Our data aggregation method finds sets of patient locations
where each member is at most a set distance from at least one other

member. The group is then represented by a circle at the set’s geographic center that has an area proportionate to the size of the
set. This allows us to successfully aggregate data around major
cities while preserving the autonomy of smaller sets in rural areas.
This method is derived from the idea of connected components in
graph theory, where patients are connected if and only if they are
within the threshold distance from another patient in the graph [5].
The generated circles are then colored using a sequential colormap
[3] where the color represents the percent of patients with a given
syndrome found within this geographical centroid. This method
operates under the assumption that the data is clumped in certain
locations, otherwise it is possible to have an aggregation that hides
too much of the actual data. Furthermore, as this method groups
data at its geographic center of mass, it preserves the data context
and helps alleviate privacy concerns.
Figure 3 (Top-Right) shows the low resolution aggregation of our
data across the state of Indiana. Figure 3 (Bottom-Left) shows the
zoomed in region, and Figure 3 (Bottom-Right) represents where
the actual patient locations would be with respect to their representation as a geographic centroid.
3.3

Heatmaps

While such data aggregation can be useful for an overall view of
patient distribution, it is also useful to model the population distribution across the state in order to approximate trends where little
or no data values exist. Therefore, our system provides a geospatial heatmap [7] view which employs a diverging color map [3] to
represent the percentage of a given syndrome over the total patients
seen on a given day.
As previously discussed, the healthcare data provided by PHESS
contains a set of observations in which an individual from location
Xi arrives at time t to a hospital and is diagnosed with a particular
syndrome. Such data is often aggregated by county or zip code and
then shown to the user. This type of aggregation can be thought of
as a histogram or box-plot of the data, and while a spatial histogram
can be useful, such a visualization does not provide any hints as to
what may be occurring in areas with little to no patient visits. Furthermore, areas with a small number of patients may stray towards
a high percentage of the population seen reporting the syndrome in
question. In those cases, visual alerts may be triggered that would

37

Figure 3: Data aggregation and privacy preservation. (Top-Left) Georeferenced patient data as small additive opacity circles. (Top-Middle)
Georeferenced patient data overlaid with red circles representing syndromic patients. (Top-Right) Data aggregation for enhanced visualization.
(Bottom-Left) High-resolution zoom of an area of interest. (Bottom-Right) Actual patient locations at a high-resolution zoom overlayed with our
data aggregation method.

clearly appear as false positives once the individual records were
analyzed. Figure 4 (Left) demonstrates the problems with visualizing such histogram distributions. The national baseline influenzalike-illness (ILI) percentage during flu season is 2.1% [1] for the
2006-07 season. Note in Figure 4 (Left) that many counties seem
to be visually displaying an extremely high level of ILI, where if we
compare this to the overlaid data aggregation circles, these counties
actually have very few patients contributing to the aggregations’
center of mass.
To overcome these issues, our system estimates the probability
density function of the entire population using the known patient
locations and produces a heatmap visualization of the entire state.
To this end, we employ a kernel density estimation [22]. Equation 1
defines the multivariate kernel density estimation, and this method
has been used in other works [11, 16, 8]. To reduce the calculation
time, we have chosen to employ the Epanechnikov kernel, Equation 2.


1 N 1
x − Xi
fˆh (x) = ∑ d K
(1)
N i=1 h
h

the last seven aggregate time periods. The density estimation for
the ill patients is then divided by the density estimation for the total
patients to provide a percentage count for the expected number ill
of the population.
Unfortunately, a fixed bandwidth kernel turns out to be inappropriate for our data due to sparse data counts in rural counties and
high data counts in large urban areas. A large fixed bandwidth over
smoothes the data (as shown in Figure 4 (Middle)) while trying to
accommodate for the sparse data regions, and a small fixed bandwidth is unable to handle data in sparse regions, creating visual
alerts in a similar fashion as Figure 4 (Left).
To overcome these issues, we employ the use of a variable kernel
method [22], Equation 3. This estimate scales the parameter of
the estimation by allowing the kernel scale to vary based upon the
distance from Xi to the kth nearest neighbor in the set comprising
N − 1 points.


x − Xi
1 N 1
K
(3)
fˆh (x) = ∑
N i=1 di,k
di,k

3
K(u) = (1 − u2 )1(||u||≤1)
4

Here, the window width of the kernel placed on the point Xi is proportional to di,k (where di,k is the distance from the ith sample to the
kth nearest neighbor) so that data points in regions where the data
is sparse will have flatter kernels. Unfortunately, our data set also
exhibits problems with this method. In health care data, a primary
recipient of emergency care are patients of long-term health care
facilities (for example, nursing homes). As such, the use of the k
nearest neighbors may result in a di,k of 1 as many patients visiting
emergency rooms may report the same address. This concept can be
extended to large apartment complexes, as well as data uncertainty
(for example, many hospitals report unknown patient addresses as
the hospital address). To overcome these issues, we slightly modify

(2)

Here, h represents the multi-dimensional smoothing parameter, N
is the total number of samples, d is the data dimensionality, and the
function 1(||u||≤1) evaluates to 1 if the inequality is true and zero for
all other cases. We calculate both the density estimation for the ill
patients as well as the density estimation of all patients that visited
a hospital in our system using an appropriately chosen h for each
data set. Density estimation for the ill patients is done only in two
dimensions for the given time period aggregation. Density estimation for the total patients is done both spatially and temporally over

38

Figure 4: A variety of heatmaps. (Left) Heatmap showing percentage of patients with ILI aggregated by county and overlaid with the patient data
aggregation. (Middle) Heatmap generated using a fixed bandwidth kernel density estimation. (Right) Heatmap generated using our modified
variable kernel density estimation.

Figure 5: Heatmap plots of the 2006-2007 flu season. Time increments from left to right with exact time periods shown on each figure.

the variable kernel estimation to force it to have a minimum fixed
bandwidth of h as shown in Equation 4.
!
N
1
x − Xi
ˆfh (x) = 1 ∑
K
(4)
N i=1 max(h,di,k )
max(h,di,k )
In the case of our modified variable kernel estimation, we calculate
the kernel only spatially as opposed to both spatially and temporally as was done in the fixed bandwidth method. Future work will
include extending our modified density estimation into the temporal domain. Results from our variable kernel estimation can be seen
in Figure 4 (Right). Slight problems in the estimation can be found
near the state borders due to the abrupt cut of data in those areas.
Future work will address these issues through more advanced spatial modeling. Presently, we feel that our results show that our modified variable kernel method provides a better estimate than either
the histogram approximation of Figure 4 (Left) or the fixed bandwidth kernel of Figure 4 (Middle).
Figure 5 further demonstrates our heatmap ability, showing a
progressive set of geospatial time series snap shots from our system. Note that in the summer months the estimated statewide level
of influenza falls into the lower regions of our scale. As we move
into winter and towards flu season, we can visually see the percent
of ILI across the state growing.
We find that in localized areas near the hospitals, the %ILI shows
clear peaks of ILI, most likely due to geo- coding errors in those
hospitals. Note in June that the predominant colors of the state

are the two darker blue colors showing that Indiana is residing at
approximately the national average or below. As we move into December, the predominant color shifts to light blue and white, indicating flu season has started as the state exceeds the baseline percentages, moving towards the 3-8% range. Our future work in this
area will address uncertainty visualization in density estimation as
part of a more effective analysis.
3.4 Time Series Analysis
While the spatial visualizations employed in our system are useful
for detecting syndromic hotspots, it is also helpful for an analytics
system to provide hints as to where outbreaks may be occurring.
To this end, we have employed the use of a standard epidemiological algorithm for time series analysis, the cumulative summation (CUSUM) [10]. The CUSUM algorithms provide alerts for
potential outbreaks in the temporal domain, and users of our system may then select these alerts for further exploration in the geospatiotemporal viewing window.


Xt − (µ0 + kσxt )
(5)
St = max 0, St−1 +
σxt
Equation 5 describes the CUSUM algorithm, where St is the current
CUSUM, St−1 is the previous CUSUM, Xt is the count at the current time, µ0 is the expected value, σxt is the standard deviation, and
k is the detectable shift from the mean (i.e. the number of standard
deviations the data can be from the expected value before an alert is
triggered). We apply a 28 day sliding window to calculate the mean,

39

Figure 6: Exploration using linked views. (Left) Images taken from our system illustrating the linked temporal analysis to the geospatial filtering.
Here, a user has selected the alert occurring on 01-01-2007. The geospatial viewing window then opens that day’s data corresponding to the
alert allowing for further investigation. (Right) Images taken from our system illustrating the linked views in selecting geospatial areas and seeing
temporal plots. Here, a user has selected an area in north-west Indiana (the green circle). This selection brings up the time series graph and
our alert detection algorithm finds an unusual event in that area on that day.

µ0 , and standard deviation, σxt , with a 3 day lag, meaning that the
mean and standard deviation are calculated on a 28 day window 3
days prior to the day in question. Such a lag is used to increase
sensitivity to continued outbreaks. Figure 6 shows the application
of the CUSUM algorithm to the temporal plot of ILI counts during
peak flu season. An alert is represented by a large red circle, which
is generated if St exceeds the threshold (for a point of reference the
threshold is typically set at three standard deviations from the mean
in the Early Aberration Reporting System and is shown as the green
line in Figure 6).

In Figure 6 (Right), we see a heat map of the state. In this figure,
note that the circled area represents a user selection. Here, the user
has chosen a region of the state that appears to currently be a syndromic hotspot. A linked time series analysis view plots the data
from that area in the lower right window. Here, we see that an alert
(small red circle) is found for that area on the day in question. A
user can then further explore these alerts by clicking on the alerts in
the time series window to find the patients associated with this alert
in the geospatial window.
4

3.5

Exploration with Linked Views

While the alerts provided from aberration detection algorithms may
provide a useful starting point for exploration, they may also be
providing false alarms. Furthermore, epidemiologists may want to
explore areas where information may be unknown, for example, visual hotspots generated in our heatmap approach may contain only
sparse data points. Ideally, epidemiologists would like to dynamically query and select elements on the visual display in order to see
how selections update related views. This type of selection is commonly referred to as brushing [2] and it is used in many interactive
visualization environments [19, 21].
For our implementation, we use only the highlight operation over
the time dimension of our temporal view and the spatial region of
our main viewing window. In the temporal view, the highlighted
region is shown in red and once the mouse button is released, all
other information displays are updated to reflect the selection. Because the individual plots are interrelated, only one may be brushed
at a time. The principal purpose of this feature is to allow selection of the current day and the number of days being aggregated
together from the plot windows based on a region of interest in the
plotted data. In Figure 6 (Left), we see a series of hospital generated alerts in the middle temporal viewing window. In this figure,
a user has clicked on an alert, causing the temporal window to lock
in place, while scrolling the geospatial window back in time to the
alert on that day. Notice that the patients who are associated with
that hospital and syndrome are now exclusively shown on the map.
In the geospatial view, highlighting is performed through a circular selection of an area. This circular selection allows users to
select multiple geographic regions and view their temporal history.

40

U NDERSTANDING H OTSPOTS

By using a combination of geospatial and temporal visualization
and analytics tools, our system provides epidemiologists with tools
for real-time hypothesis testing. To better illustrate the hypothesis testing phase, we conducted an informal interview with an Indiana State Department of Health (ISDH) syndromic surveillance epidemiologist. During this interview, we discussed how he searches
for syndromic hotspots, creates an initial hypothesis, and what steps
are taken in an attempt to confirm or deny this hypothesis.
Traditionally, the first items examined when identifying potential
syndromic problem areas are the spatial alerts generated for a given
syndrome. Based on his experience, certain alerts will be immediately resolved as false positives, and others will be moved to the top
of the queue. From the alerts he identifies as potential problems, a
hypothesis is formulated stating that a problem with syndrome X
is occurring in patients found at location Y. These alerts are aggregated by zip code level, meaning that zip codes A, B, C, etc.
contribute to the alert. From this step, the epidemiologist would
look at the time series data for all zip codes contributing to the alert
in order to gain a better understanding of where the baseline lies.
In contrast, our visual analytics tool allows users to select an arbitrary region to view the time series data, providing a baseline for
the overall area, potentially allowing quicker comparison.
Often, the next step taken would be to further corroborate the
geospatial area of the alert by looking at the counties involved and
pulling up county level alerts, their corresponding time series plots,
and county maps down to the zip code level. Similarly, our tool
provides both heatmaps at the reduced levels of granularity, as well
as a finer, smoother granularity heatmap option that the epidemiologist thought may add value. If, from the heatmap, the hypothesis

Figure 7: Using visual analytics for hypothesis testing in syndromic surveillance. (Top-Left) The user observe a heatmap for a given syndrome,
in this case, gastro-intestinal. (Top-Right) Next, the user selects an area of interest, generating a time series plot for that region. Note that in the
time series plot generated, an alert is occurring on the day of interest. (Bottom-Left) The user then drills down to the hospital level by selecting
the neighboring hospital and generating a time series plot for that emergency department. Here, we see that there is no hospital level alert for
gastro-intestinal syndromes. (Bottom-Right) Finally, the user looks for correlating symptoms and filters by the keyword fever. New time series
plots are generated. While an alert still exists for the selected area, the user can now see that this alert was generated by only one individual,
meaning an outbreak is unlikely.

can not be rejected, the next step is to drill down to patient level
data in order to assess the actual chief complaints. For example, if
(in the case of a gastro-intestinal problem) a patient’s “vomiting” is
related to pregnancy, then it is less likely to be part of the gastrointestinal outbreak being considered in this hypothesis. As such,
sometimes potential clusters then fall apart. Next, the epidemiologist would look at the patient level data to assess timestamps and
actual chief complaints for clustering which may lead to filtering by
ages for clustering and gender for skew if clues exist that lead the
hypothesis refinement in those directions. If there was a string of
elevated days, then he would group these elevated days and do the
same type of descriptive analysis. Our dual linked views provide
advanced tools for such an operation, aiding in the overall hypothesis testing.
During this process, he also searches for potential “cosyndromes” in the same geography, such as fever, to see if it is
somehow linked to the gastro-intestinal problem. Again, the linked
views and filter options of our system allow the user to easily look
at multi-variate time series components. If concurrent syndromes
are found, this potentially strengthens the hypothesis and may lead
to a follow-up with the actual emergency department(s) involved.
Figure 7 illustrates the use of our system during the hypothesis testing phase.
First, in Figure 7 (Top-Left), the user has selected the syndrome
he/she is interested in analyzing, in this case, gastro-intestinal. This

generates a query to the database, and the epidemiologist can now
look at the patient distribution with either an additive opacity for all
patients that visited an emergency department, or as an aggregate
of the data. Next, the user visually searches for unusual hotspots
using a combination of the kernel density estimation and the patient
overlay. The user may select multiple areas for testing; however, if
the area selected shows no temporal alert for the day in question,
then it is likely that the hypothesis of area X being problematic is
rejected.
In Figure 7 (Top-Right), the user has selected an area of the map
in central Indiana, and the corresponding time series graph that was
generated indicates that the selected area is showing an alert on the
day in question. The next step in analyzing this alert is to look at
data from the nearby emergency departments. In this case, there is
only a single emergency department. The user clicks on the hospital glyph on the map, and the time series plot for this emergency
department is generated, see Figure 7 (Bottom-Left). In this time
series plot, there is no alert generated for this emergency department for the day in question. This weakens the hypothesis that
there is an outbreak in the area; however, the user may still want to
take further steps to confirm/deny the hypothesis.
The next step taken is to look for corresponding symptoms. In
this case, the user looks for patients with gastro-intestinal syndromes that also reported signs of fever. Figure 7 (Bottom-Right)
shows this filter query. Note that the heatmap and time series plots

41

are automatically updated from the query. We can see now that
there are no visual hotspots occurring on the map; however, there
is still a time series alert for that area. Further investigation of the
time series alert shows that the expected number of patients was
slightly less than one, and one patient came in on that day, thereby
generating an alert. It is now unlikely that an outbreak is occurring
in this area, and the hypothesis can be denied after a brief analysis
of the patient record.
While it may seem odd that one case can cause an outbreak alert,
this is quite a common occurrence in all current systems. For example, the carbon monoxide case shown in Figure 1 contains only
three emergency department complaints. Therefore, the high sensitivity is necessary to avoid missing small cluster cases.
5

C ONCLUSIONS

AND

[8]

[9]

[10]

F UTURE W ORK

Our current work demonstrates the benefits of visual analytics for
understanding syndromic hotspots. By linking a variety of data
sources and models, we are able to enhance the hypothesis generation and exploration abilities of our state epidemiologists. Our
initial results show the benefits of linking traditional time-series
epidemiological views with geo-spatiotemporal views for enhanced
exploration and data analysis. Our system also moves away from
traditional spatial histogram visualizations, providing a finer granularity of heatmap for more accurate syndromic detection.
Unfortunately, database query performance currently reduces the
interactivity of several functions of our system, most notably the
keyword filtering. These queries can take seconds to minutes depending on the length of the time series being visualized. We plan
to enhance our database system to optimize keyword queries on our
ten million record database and achieve interactive system rates for
all components in the near future.
Other future work includes advanced modeling of geospatiotemporal data for enhanced data exploration and hotspot detection. Furthermore, we plan to include a suite of aberration detection algorithms and their corresponding control charts for enhanced
alert detection in the temporal domain. We also plan on employing
spatiotemporal clustering algorithms for syndromic event detection
as well as correlative analysis views within the temporal domain.
Once these features are implemented, we plan to deploy our system
with our state health partners for further evaluation.

[11]

6 ACKNOWLEDGMENTS
The authors would like to thank the Purdue University Student
Health Center and the Indiana State Department of Health for providing the data. This work has been funded by the US Department
of Homeland Security Regional Visualization and Analytics Center
(RVAC) Center of Excellence and the US National Science Foundation (NSF) under Grants 0328984 and 0121288.

[19]

R EFERENCES
[1] Update: Influenza activity — United States and Worldwide, 2007–07
season, and composition of the 2007–08 influenza vaccine. MMWR
Morb Mortal Wkly Rep, 56:789–794, 2007.
[2] R. A. Becker and W. S. Cleveland. Brushing scatterplots. Technometrics, 29(2):127–142, 1987.
[3] C. A. Brewer. Designing better Maps: A Guide for GIS users. ESRI
Press, 2005.
[4] W. W. Chapman, J. N. Dowling, and M. M. Wagner. Classification
of emergency department chief complaints into 7 syndromes: A retrospective analysis of 527,228 patients. Annals of Emergency Medicine,
46:445–455, November 2005.
[5] T. Cormen, C. Leiserson, R. Rivest, and C. Stein. Introduction to
Algorithms. The MIT Press, 2001.
[6] R. M. Edsall, A. M. MacEachren, and L. Pickle. Case study: Design and assessment of an enhanced geographic information system
for exploration of multivariate health statistics. In INFOVIS ’01: Proceedings of the IEEE Symposium on Information Visualization 2001

42

[7]

[12]

[13]

[14]

[15]

[16]
[17]

[18]

[20]

[21]

[22]
[23]

[24]
[25]
[26]

(INFOVIS’01), page 159, Washington, DC, USA, 2001. IEEE Computer Society.
U. Fayyad, G. G. Grinstein, and A. Wierse, editors. Information visualization in data mining and knowledge discovery. Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA, 2002.
M. Gibin, P. Longley, and P. Atkinson. Kernel density estimation and
percent volume contours in general practice catchment area analysis
in urban areas. In Geographical infomration science research conference, 2007.
S. J. Grannis, M. Wade, J. Gibson, and J. M. Overhage. The Indiana public health emergency surveillance system: Ongoing progress,
early findings, and future directions. In American Medical Informatics
Association, 2006.
L. C. Hutwagner, W. W. Thompson, and G. M. Seeman. The bioterrorism preparedness and response early aberration reporting system
(ears). Journal of Urban Health, 80(2):i89 – i96, 2003.
D. Kao, A. Luo, J. L. Dungan, and A. Pang. Visualizing spatially
varying distribution data. In Proceedings of the sixth internationl conference on information visualization, pages 219–225, 2002.
A. D. Langmuir. The surveillance of communicable diseases of national importance. New England Journal of Medicine, 268:182 – 192,
1963.
K. Liao. A visualization system for space-time and multivariate patterns (vis-stamp). IEEE Transactions on Visualization and Computer
Graphics, 12(6):1461–1474, 2006. Member-Diansheng Guo and Student Member-Jin Chen and Member-Alan M. MacEachren.
J. S. Lombardo. A systems overview of the electronic surveillance system for the early notification of community based epidemics
(ESSENCE II). Journal of Urban Health, 80:32 – 42, 2003.
J. W. Loonsk. Biosense - a national initiative for early detection and
quantification of public health emergencies. MMWR, 53:53 – 55,
2004.
A. L. Love, A. Pang, and D. L. Kao. Visualizing spatial multivalue
data. IEEE Comput. Graph. Appl., 25(3):69–79, 2005.
A. M. MacEachren, F. P. Boscoe, D. Haug, and L. Pickle. Geographic
visualization: Designing manipulable maps for exploring temporally
varying georeferenced statistics. In INFOVIS ’98: Proceedings of the
1998 IEEE Symposium on Information Visualization, page 87, Washington, DC, USA, 1998. IEEE Computer Society.
R. Maciejewski, B. Tyner, Y. Jang, C. Zheng, R. Nehme, D. S. Ebert,
W. S. Cleveland, M. Ouzzani, S. J. Grannis, and L. T. Glickman.
Lahva: Linked animal-human health visual analytics. In Proceedings
of the IEEE Symposium on Visual Analytics Science and Technology,
October 2007.
A. R. Martin and M. O. Ward. High dimensional brushing for interactive exploration of multivariate data. In VIS ’95: Proceedings of the
6th conference on Visualization ’95, page 271, Washington, DC, USA,
1995. IEEE Computer Society.
C.-C. Pan and P. Mitra. Femarepviz: Automatic extraction and geotemporal visualization of fema national situation updates. Visual Analytics Science and Technology, 2007. VAST 2007. IEEE Symposium
on, pages 11–18, Oct. 30 2007-Nov. 1 2007.
J. C. Roberts and M. A. E. Wright. Towards ubiquitous brushing for
information visualization. In IV ’06: Proceedings of the conference
on Information Visualization, pages 151–156, Washington, DC, USA,
2006. IEEE Computer Society.
B. W. Silverman. Density Estimation for Statistica and Data Analysis.
Chapman & Hall/CRC, 1986.
S. B. Thacker, R. L. Berkelman, and D. F. Stroup. The science of
public health surveillance. Journal of Public Health Policy, 10:187 –
203, 1989.
J. J. Thomas and K. A. Cook, editors. Illuminating the Path: The R&D
Agenda for Visual Analytics. IEEE Press, 2005.
C. Tominski, P. Schulze-Wollgast, and H. Schumann. Visual analysis
of human health data. In 2003 IRMA International Conference, 2003.
C. Tominski, P. Schulze-Wollgast, and H. Schumann. 3d information visualization for time dependent data on maps. In International
Conference on Infomation Visualization (IV), 2005.

2012 45th Hawaii International Conference on System Sciences

Applied visual analytics for exploring the National Health and Nutrition
Examination Survey
Silvia Oliveros Torres*, Heather Eicher-Miller+, Carol Boushey+, David Ebert*, Ross Maciejewski*
* Purdue University Regional Visualization and Analytics Center, + Dept. of Nutrition, Purdue University

Abstract

Visual data exploration techniques have been
shown to be an effective tool in aiding analysts in
exploring and understanding these types of large,
multivariate datasets. Our main contribution in this
paper is the development of a visual analytics system
for NHANES. The system will help researchers
explore patterns, form hypotheses, understand the
underlying structure of the dataset, and will also
provide the researchers with means of presenting
their findings. The visual analytics system relies on
an interactive scatterplot matrix to visualize the
different dimensions of the data set. The scatterplot
matrix was chosen because it provides the user with
an easy way to interpret relationships between
different pairs of dimensions within the data.
Distinguishing correlation between variables allows
the user to understand how one variable affects the
other. Additionally, we have incorporated advanced
analytics tools for exploring this scatterplot matrix,
including clustering and dimensional ordering that
provides a more guided exploration of this large
dataset. Our primary analytic tool is clustering.
Cluster analysis is a powerful tool for the exploration
of high dimensional data. Clustering can be used to
discover hidden associations without a prior
hypothesis, therefore, we included automatic
clustering via k-means [11]. We have also included
two different types of automatic dimension ordering
based on cluster density and dimension similarity.
Dimensional ordering has proven to reduce the clutter
that obscures the underlying structures in high
dimensional data [5]. By automatically ordering the
way the scatterplots are presented, we can enhance
the process of exploring the data for the users by
ordering the data by its information content.
Along with automatic analysis, the user has the
ability to filter the data depending on the problem
being analyzed. The system provides users with the
ability to filter the data based on age, gender, and
ethnicity of the participants. The user also has full
control over the number of dimensions displayed in
the scatterplot matrix, providing both global and local
analysis options. The size of the matrix can be

The National Health and Nutrition Examination
Survey (NHANES) is a research program to assess
the health and nutritional status of the population in
the United States. In this work, we present a visual
analytics system designed to help researchers explore
patterns and form hypotheses within the NHANES
dataset. The visualization component of the
environment is an extension of traditional scatterplot
matrices. Since the upper portion of the scatterplot
matrix is a redundant encoding, we utilize this space,
to show the projected N-dimensional clustering of
points. The rows and columns of the matrix are
automatically ordered using information about the
cluster projection in each space as a means of
showing the most meaningful dimensions. A
comparison module has also been included that
allows the user to compare groupings of people to the
2010 Dietary Guidelines for Americans. This tool
enhances the analysis work by aiding discovery and
hypothesis formation.
Keywords: visual analytics, scatterplot, NHANES,
diet records.

1. Introduction
The United States health sector has deployed
many survey programs that produce large datasets
with increasing complexity and dimensionality. One
such survey program is the National Health and
Nutrition Examination Survey (NHANES) [1], which
is a population-based survey designed to collect
information on the health and nutrition of the U.S.
household population. NHANES collects data from
physical examinations along with surveys where they
ask the responder to recall their ingestion of food for
the past 48 hours. The 2-day food recollection survey
includes demographic questions such as gender, age
and race/ethnicity.

978-0-7695-4525-7/12 $26.00 © 2012 IEEE
DOI 10.1109/HICSS.2012.116

1855

increased or decreased, and users can modify which
dimensions are being shown. All these features
provide the user with a fully customizable experience
while navigating and analyzing the NHANES dataset.
Finally, the system also includes a familiar
graphing/comparison model to aid the user in
understanding the overall dataset while focusing on a
specific variable. The system transforms the data
points presented in the scatterplot matrix into bar
graphs that are similar in style to the ones presented
in publications by the U.S. Department of Health and
Human Services [10]. These bar graphs can also be
used to present the findings of the data exploration.
Feedback from nutrition experts indicated that
researchers would be familiar with these graphs, and
find them to be easily understood.
The remainder of this paper is organized as
follows. Section 2 provides a summary of related
work. Section 3 provides an overview of the structure
of the NHANES dataset. Section 4 discusses the
visual analytics environment and its principal
components such as the scatterplot matrix, clustering
and dimension ordering. Section 5 introduces the
graphing/comparison model being used with this
dataset. Section 6 provides the reader with a usage
scenario. Finally, conclusions and future work are
discussed in Section 7.

proposed by Ankerst et al. For one of the dimension
reordering methods used in our system, the notion of
a pair wise correlation is used to compute final
scatterplot arrangement.
Recent work by Peng et al. [5], shows that by
reordering the dimensions, clutter in a representation
can be reduced without reducing the information
content. Clutter is considered to be anything that
interferes with the process of finding structures. For
scatterplot displays Peng et al., proposed arranging
the matrix based on scatterplot cardinality. For the
high cardinality dimensions, the Pearson correlation
coefficient is used to calculate the clutter measure
and re-arrange accordingly. [We incorporate Peng et
al.’s concept of using the Pearson correlation
coefficient as one way to re-arrange the scatterplots
in our matrix.] Tatu et al. [8] presented a ranking
method for scatterplots that uses rotating variance,
class density, and histogram density measures. We
use their notion of density and apply it to the
clustering of data points within the scatterplots.
Yang et al. [6,7] presented a framework that
provides a multi-resolution view of the data via
hierarchical clustering where the user can
interactively explore the desired focus region at
different levels of detail. The framework works upon
hierarchical cluster trees of data sets, and makes use
of proximity-based color to help identify and link
relationships between clusters. Our system uses some
of the clustering concepts and interactive exploration
without the dimension reduction that Yang presented.
Sips et al. [9] presented the concept of class
consistency in classes of n-Data shown in 2D
scatterplots. Their quantitative methods of
consistency are based on clusters center of gravity
and on entropies of the spatial distributions of
classes. We did not compute any measure of class
consistency, nor do we have a method to qualify the
placement of the scatterplots. However it is a topic
that we are interested in and will leave this as part of
the system’s future work.

2. Related work
Many multi-dimensional visualization tools exist
that utilize scatterplots including XmdvTool [2],
which supports many interaction modes and tools,
and Polaris [3], which takes a database and projects
the data into a scatterplot matrix. Other tools, such as
ScatterDice[13], have also included interactive
techniques for navigation within the scatterplot
matrix and methods for dimension reordering
designed to show correlation and differences between
individual dimensions. Our work integrates some of
the features seen in these previous tools, such as,
brushing, linking, zooming, panning, and reordering
of dimensions. We extend these features through the
addition of clustering into a new tool to visualize this
specific multidimensional dataset.
Furthermore our work also expands approaches
for ordering and filtering the dimensions of multidimensional datasets. Ankerst et al. [4] explored a
variety of clutter reduction metrics, along with some
work in dimension reduction. Ankerst et al. proposed
a method for arranging dimensions using pair wise
similarity measures that are used to calculate a global
optimization method. The similarity measure in this
work was based on the Euclidean distance function

3. Structure of NHANES Data
The NHANES interview includes demographic,
socioeconomic, dietary, and health-related questions.
The examination component consists of medical,
dental, and physiological measurements, as well as
laboratory tests administered by medical personnel.
In this study, we focus exclusively on the dietary
survey information collected.
NHANES collects data from a 2-day food
recollection survey. The survey collects the types and

1856

Upper portion
shows k-means
clustering

Figure 1. General overview of the visuaal analytics system for NHANES exploration. One of the clusterrs has been selected
in the upper portion and all the paarticipants that belong to that cluster have been highlighted acro
oss the matrix.

quantities of food ingested during 48 hhours. After the
survey, health experts separate the foodd into different
groups and calculate their caloric and nutrient
components.
The nutritional content is classified into 12
different Healthy Eating Indexes (HEII) as described

he index has different
in Table 1. Each component of th
maximum scores and a minimum
m score of zero. High
component scores indicate inttakes close to the
recommend ranges or amountts; low component
scores indicate less comp
pliance with the
recommended ranges or amountts. Low compliance

1857

means a HEI score of zero. The HEI scores indicate
levels, thus the scales are not normalized. Finally, all
the components can be compounded and an overall
HEI score is calculated and viewed as a measure of
diet quality that assesses conformance to the federal
dietary guidance [10].
HEI
1
2
3
4
5
6
7
8
9
10
11
12

Description
Total Fruit
Whole Fruit
Total Vegetables
Dark green and orange vegetables
and legumes
Total grains
Whole grains
Milk
Meat and beans
Oils
Saturated fat
Sodium
Calories from solid fats, alcoholic
beverages, and added sugars

this in mind, we set the grid to be 6x6 which is half
of the maximum number of HEI variables. The initial
grid size was determined based on aesthetics. The
chosen size displays all the necessary labels and fits
multiple monitor sizes. As mentioned before, the user
can choose to increase or decrease the number of
dimensions shown at any time as he/she sees
appropriate.
The matrix is divided diagonally into an upper
and lower portion as shown in Figure 1. The lower
diagonal portion shows the regular scatterplots of the
components plotted against each other. The upper
diagonal portion shows the clusters projected into
each of the plots. In Figure 1, one of the clusters has
been selected and all of the participants who belong
to that cluster are highlighted across the entire matrix.
We discuss the upper diagonal of the scatterplot
matrix below.

Range
0-5
0-5
0-5
0-5
0-5
0-5
0-10
0-10
0-10
0-10
0-10

4.2 Upper diagonal clustering

0-20

Clustering is one of the most frequently used
methods to improve the perception and recognition of
patterns in multivariate datasets. In our system the kmeans [11] clustering algorithm is applied and a
Euclidean distance metric between the points in the
N-dimensional space is used.
In k-means, we are given a set of n data points in
a d-dimensional space (in the NHANES case d=12)
Rd , and an integer k. The problem is to determine a
set of k points in Rd, called centers, which will
minimize the mean square distance from each point
to its nearest center. Our k-means clustering was
computed using an enhanced form of a kd tree as
specified by Kanugo et al. [11].
The clusters computed at the beginning of the
program are projected into each of the 2D plots on
the upper diagonal matrix. Each cluster is
differentiated with the aid of different colors and
occlusion is addressed by using transparency when
we plot the different points. We revised the work of
Chiang and Mirkin [14] about an intelligent choice of
the number of clusters which described Hartigan’s
rule as one of the best methods to calculate the initial
number of clusters. Hartigan’s rule suggests using 25
different clusters for this specific dataset, however
the initial number was reduced in order to not
overwhelm the users and distract them from seeing
general trends. A large number of clusters can
compromise the legibility of the scatterplots. We
leave the decision of the number of clusters to the
user, who can modify it at any time as part of the
exploration process.
Although there is no guarantee that the 2D
projections will show a clear distinction between the

Table 1. HEI Components

4. Visual analytics environment
The visual analytics environment consists of a
scatterplot matrix of selected variables that can be
modified at any time. The upper diagonal portion of
the matrix displays the same data as the lower
diagonal but in a k-means clustered form as shown in
Figure 1. The initial dimensions chosen are the result
of dimension ordering.
This ordering can be
implemented using two different methods: one based
on cluster density and the other on dimension
correlation. The different user tools implemented in
the system are described at the end of this Section.

4.1 Basic scatterplot matrix
The main component of the application consists
of a traditional scatterplot matrix set up initially as a
6x6 grid of default variables as shown in Figure 1.
These default variables are determined based on two
different methods of dimension ordering that are
discussed in Section 4.3. In Figure 1, the variables
are HEI 5, 7, 12, 10, 8, and 4. One of the main
concerns while designing the system was the user.
We do not want the user to become overwhelmed by
the high dimensionality of the dataset. Instead, we
want the user to be able to focus on a given
scatterplot and explore it freely and completely. With

1858

(a)

(b)
Figure 2. (a) Cluster density ordering. (b) Pearson’s correlation ordering.

clusters, the dimension ordering described in the
following subsection can help in further separating
the clusters. These computed clusters also provide a
reference group of participants, which can be further
explored with the aid of brushing and linking.

∑

∑

where s represents the variance, n is the size of the
sample, and x is the data point being used.
Welford’s method computes the variance as the x’s
arrive, one at a time, and gives more arithmetic
precision. Once the algorithm is initialized with
1
1
1
0 the subsequent x’s use the
recurrence formulas:

4.3 Dimension ordering
Our system automatically generates a default
ordering scheme that the user can modify later. There
are two different measures employed for reordering
the scatterplot matrix.
The first measure arranges the dimensions
according to their cluster density. This measure
simply allows the user to view the scatterplots that
present the most defined clusters calculated with kmeans as explained in the previous section. Cluster
density may be defined on a local or global scale
within the N-dimensional space.
Global cluster density is a comparison across all
the N-dimensions. Local cluster density compares
only the projected dimensions for any given
scatterplot. The cluster density, the variance, and
standard deviation for each cluster are computed
using Welford’s method [12].

1

1

1

1 /

.

Welford's method is more efficient because it
requires looping through all the data points only
once. Welford's method is also more accurate since it
does not use abnormally large numbers in
calculation, compared to the sum of squares method.
Using these global metrics, we order the
scatterplots such that the dimensions with the tightest
clusters will be displayed to the left of the
dimensional ordering lineup. These plots should
potentially contain the most relative information as
the projected clusters are the most compact in this
space. The local scale is then used during the
rendering of the clusters within a given 2D plot.
Clusters are rendered in order of their local density,
from smallest to largest. By rendering the clusters in

Traditionally, the mathematical formula for
computing the sample variance is:

1859

Highlighted participants in a scattterplot are shown as
red points and this provides an
a easy method to
observe these participants behaavior across all the
other categories. The highlightin
ng ability across the
matrix allows for a quick discov
very of relationships
between different components. Finally,
F
the user has
the ability of further narrowing down
d
the dataset by
filtering the population being displayed in the
scatterplots. There are filters avaailable based on age,
gender, race and ethnicity and caalculated body mass
index (BMI).

this manner, we also reduce some off the issues in
over-plotting.
The second measure is based on Pearson’s
correlation coefficient, which is definned to be the
measure of the correlation (linearr dependence)
between two variables. The formula ussed to calculate
the coefficient is the following:
∑
1
In the equation above the variables are represented as
and , with means
and
respectively, and
and respectivvely.
standard deviations,
The coefficient is calculated ffor each plot
individually and plots are arranged bbased on their
coefficients. This measure also allows tthe scatterplots
with the most similar features to be pplotted close to
each other.
In Figure 2, we show how the two different
methods reordered the given dimensiions in a 4x4
matrix. Figure 2(a) shows the re-ordeering based on
cluster density. Since it presents the sccatterplots with
the most compact clusters, the userr can visually
identify the different clusters in some of the
scatterplots as clearly defined color areeas. Figure 2(b)
shows the re-arrangement based on Pearson’s
correlation. In Figure 2(b), the clusters do not play an
important role when the correlatiion is being
computed. In Figure 2(a) cluster ddensity shows
defined clusters in the upper diaggonal portion,
whereas in Figure 2(b), Pearson's corrrelation shows
more defined areas in the bottom diagonnal portion.

5. Comparison graphs
Every five years, the Diettary Guidelines for
Americans (DGA) are published
d jointly by the U.S.
Department of Health and Hum
man Services (HHS)
and the U.S. Department of Agricculture (USDA) [10]
with the last one being publish
hed in 2010. These
guidelines provide authoritaative advice for
Americans, ages two and olderr, about consuming
fewer calories, making informed
d food choices, and
being physically active to attaain and maintain a
healthy weight, reduce risk of chronic
c
disease, and
promote overall health. Since these
t
guidelines are
followed and understood by mostt of the health sector
in the country, our system has th
he ability to generate
two different graphs to transform
m the NHANES data
into familiar graphs and standarrds presented in the
DGA.

4.4 User tools and interaction
Besides the normal interactive capab
abilities such as
zooming and panning, our system allso includes a
selection tool that allows the user to chhoose a subset
of specific participants in the dataseet that will be
highlighted across the scatterplot matrix. The
selection tool allows for regular brushinng and linking.

Figure 4. Scatterplot matrix highlights the participants with high
dairy consumption in red.

Figure 3. Filtering tool.

1860

Figure 5. Comparison graphs for participants with high milk consumption (HEI 7). Hei graph on the left and DGA on the right.

Figure 6. Comparison graphs for participants with high vegetable and legume consumption. (HEI 4)

Using the selection tool described in the previous
section, the user can select a region, or any given
cluster, to update the two given graphs. The first
graph transforms the selected points into the most
representative DGA graph. The graph adds up all the
selected participants data, averages it and then
converts it into a percentage based on the guideline
for consumption. The bottom of Figure 1 presents the
newly created graphs at the bottom. The DGA graph
on the right includes two separate markers, the top
one indicates the amount of foods that the Americans
needs to consume in order to fulfill the 100%
recommended daily intake. The bottom marker
indicates the foods that need to be consumed in
moderation, showing a red line when the participants
have consumed more than the recommended intake.

The left side graph at the bottom of Figure 1
provides the user with an overview of all the HEI
within the selected group. Since the user might only
be looking at fewer scatterplots, the summary graph
provides insight into which HEI should be observed
next. A 100% score in this graph indicates the
maximum score given in Table 1 for any given HEI.
For example, if a participant’s HEI 5 score is 4.5 the
percentage shown in the graph for HEI 5 will be
90%. The percentage HEI graph was implemented
for the purpose of representing all HEI scores in a
common scale.

1861

Figure 7. Comparison graphs for participants with whole grains consumption. (HEI 6)

6. Usage scenario

diagonal on the HEI 4 vs HEI 4 scatterplot. The
resulting graphs for this selection can be seen in
Figure 6.
Based on the DGA generated graph on the left of
Figure 6 we notice that these participants perform
better in other categories except for whole grains,
which seems to remain the same. This hypothesis is
confirmed by the HEI graph that shows an increase in
most of the categories. The remaining question is to
examine the whole grain category since it seemed to
remain constant during the last two examples.
This time around, we select participants with a
high whole grain intake that is represented by HEI 6.
After looking at the generated graphs (Figure 7), we
can observe that these participants maintain a more
balanced diet since their graphs demonstrate an
overall increase as well as being more leveled. The
participants also show a lower consumption of
sodium and lower consumption of any extra added
calories. Based on the past three examples, we
hypothesize that the best indicator of a good diet in
teenagers comes from their whole grains intake.
Teenagers who consume a high quantity of whole
grains may lead a more balanced diet compared to
those who consume high quantities of milk or
vegetables and legumes.

We begin our usage scenario with a 6x6
scatterplot matrix that shows the plots arranged based
on higher cluster density. First, we select only the
population that we are interested in working with
(i.e., males in the age group 14-18). After we have
added this filter, the clusters are recomputed and the
scatterplots are re-arranged (see Figures 3-4). Our
goal is to determine which single HEI category might
prove to be the best indicator of healthy eating habits.
Since HEI 7 (Milk) was one of the first two
dimensions shown in the matrix, we make use of the
scatterplots in the diagonal of the matrix and select
the participants with a high HEI 7. We are able to
select those participants by selecting the upper right
corner of the diagonal line of HEI 7. After
performing the selection we can see the adjacent
plots being highlighted with the same participants as
shown in Figure 4.
Most of the plots do not show any correlation
with HEI 7 since all the points highlighted appear
across all the scatterplots. However, we can also see
the two updated graphs. With the aid of the HEI
graph (upper graph in Figure 5) we can visualize the
other dimensions that we do not see in the scatterplot
matrix. We can see that teenagers who consume an
adequate amount of milk do not necessarily consume
an adequate amount of other foods; however, they do
relatively well on consuming low quantities of
sodium and. We also notice that participants who
show a high dairy intake do not do perform well in
HEI 4 (vegetables and legumes). Since we know that
participants with high milk consumption have a low
vegetable consumption, we now select patients with
high consumption in HEI 4 by selecting the upper

7. Conclusions and future Work
We have described a visual analytics system
developed to explore the multidimensional NHANES
dataset. The system makes use of an interactive
scatterplot with automatic clustering that is
rearranged to present the user with the variables that
show the most correlation from the beginning. The
system also displays familiar graphs that condense
1862

Science and Technology (IEEE VAST), Atlantic City, New
Jersey, USA (2009)
[9] Mike Sips, Boris Neubert, John P. Lewis, and Pat
Hanrahan. Selecting good views of high-dimensional data
using class consistency. Computer Graphics Forum,
28(3):831–838, June 2009.
[10] U.S. Department of Agriculture and U.S. Department
of Health and Human Services. Dietary Guidelines for
Americans, 2010. 7th Edition, Washington, DC: U.S.
Government Printing Office, December 2010.
[11] Tapas Kanungo, David M. Mount, Nathan S.
Netanyahu, Christine D. Piatko, Ruth Silverman, and
Angela Y. Wu. A local search approximation algorithm for
k-means clustering. Comput. Geom. Theory Appl., 28:89–
112, June 2004.
[12] Donald E. Knuth. The art of computer programming,
volume 2 (3rd ed.): seminumerical algorithms. AddisonWesley Longman Publishing Co., Inc., Boston, MA, USA,
1997.
[13] N. Elmqvist, P. Dragicevic, J.-D. Fekete. Rolling the
Dice: Multidimensional Visual Exploration using
Scatterplot Matrix Navigation. In IEEE Transactions on
Visualization and Computer Graphics (Proc. InfoVis 2008),
14(6):1141-1148, 2008.
[14] Mark Ming-Tso Chiang and Boris Mirkin. 2010.
Intelligent Choice of the Number of Clusters in K-Means
Clustering: An Experimental Study with Different Cluster
Spreads. J. Classif. 27, 1 (March 2010), 3-40.

the information presented to the user. The system
aids the user in developing and testing new
hypotheses. We showed a usage scenario that
demonstrates the use of the comparison/graphing
system implemented to navigate the data.
We recognize there is still work that could be
done to improve the system. We would like to
experiment with new layouts to present the dietary
elements that seem to have the most impact in the
participants diet, possibly a combination of the
methods used in this paper, as well as an extension
exploring dimensionality reduction. In order to
further differentiate the clusters, we are planning on
adding an interactive exploration of the scatterplot
matrix. This visualization system could also be used
to analyze other types of complex data sets. Finally,
future work will include modifying the system to fit
similar datasets.

8. References
[1] Centers for Disease Control and Prevention. National
Health
and
Nutrition
Examination
Survey:
http://www.cdc.gov/nchs/nhanes.htm May 27, 2011 [May
30, 2011]
[2] Matthew O. Ward, "XmdvTool: Integrating Multiple
Methods for Visualizing Multivariate Data," IEEE Conf. on
Visualization '94, pp 326 - 333, Oct. 1994.
[3] C. Stolte, D. Tang, and P. Hanrahan. Polaris: A system
for query, analysis, and visualization of multidimensional
relational databases. IEEE Transactions on Visualization
and Computer Graphics, 8(1):52–65, 2002.
[4] M. Ankerst, S. Berchtold, and D.A. Keim. Similarity
clustering of dimensions for an enhanced visualization of
multidimensional data. In Information Visualization, 1998.
Proceedings. IEEE Symposium on, pages 52 –60, 153,
October 1998.
[5] Wei Peng, Matthew O. Ward, and Elke A.
Rundensteiner. Clutter reduction in multi-dimensional data
visualization using dimension reordering. In Proceedings of
the IEEE Symposium on Information Visualization, pages
89–96, Washington, DC, USA, 2004. IEEE Computer
Society.
[6] J. Yang, M. O. Ward, E. A. Rundensteiner, and S.
Huang. Visual hierarchical dimension reduction for
exploration of high dimensional datasets Eurographics /
IEEE TCVG Symposium on Visualization, pages 19–28,
May 2003.
[7] Jing Yang, Matthew O. Ward, and Elke A.
Rundensteiner. Interactive hierarchical displays: a general
framework for visualization and exploration of large
multivariate data sets. Computers & Graphics, 27(2):265–
283, April 2003.
[8] A. Tatu, G. Albuquerque, M. Eisemann, J.
Schneidewind, H. Theisel, M. Magnor, D. Keim.
Combining automated analysis and visualization techniques
for effective exploration of high-dimensional data. In
Proceedings of IEEE Symposium on Visual Analytics

1863

LAHVA: Linked Animal-Human Health Visual Analytics
Ross Maciejewski∗
David S. Ebert∗

Benjamin Tyner∗
Yun Jang∗
Cheng Zheng∗
Rimma V. Nehme∗
∗
∗
William S. Cleveland
Mourad Ouzzani
Shaun J. Grannis‡
†
Lawrence T. Glickman

∗ Purdue

University Regional Visualization and Analytics Center (PURVAC)
University School of Veterinary Medicine Clinical Epidemiology Group
‡ Regenstrief Institute and Indiana University School of Medicine

† Purdue

A BSTRACT
Coordinated animal-human health monitoring can provide an early
warning system with fewer false alarms for naturally occurring disease outbreaks, as well as biological, chemical and environmental incidents. This monitoring requires the integration and analysis of multi-field, multi-scale and multi-source data sets. In order
to better understand these data sets, models and measurements at
different resolutions must be analyzed. To facilitate these investigations, we have created an application to provide a visual analytics framework for analyzing both human emergency room data
and veterinary hospital data. Our integrated visual analytic tool
links temporally varying geospatial visualization of animal and human patient health information with advanced statistical analysis of
these multi-source data. Various statistical analysis techniques have
been applied in conjunction with a spatio-temporal viewing window. Such an application provides researchers with the ability to
visually search the data for clusters in both a statistical model view
and a spatio-temporal view. Our interface provides a factor specification/filtering component to allow exploration of causal factors
and spread patterns. In this paper, we will discuss the application
of our linked animal-human visual analytics (LAHVA) tool to two
specific case studies. The first case study is the effect of seasonal
influenza and its correlation with different companion animals (e.g.,
cats, dogs) syndromes. Here we use data from the Indiana Network
for Patient Care (INPC) and Banfield Pet Hospitals in an attempt to
determine if there are correlations between respiratory syndromes
representing the onset of seasonal influenza in humans and general respiratory syndromes in cats and dogs. Our second case study
examines the effect of the release of industrial wastewater in a community through companion animal surveillance.
1

I NTRODUCTION

The role of public health surveillance is to collect, analyze and interpret data about biological agents, diseases, risk factors and other
health events in order to provide timely dissemination of collected
information to decision makers. Surveillance activities share several common practices in the way data are collected, managed,
transmitted, analyzed, accessed and disseminated. Surveillance
methods that can detect disease at a pre-diagnostic stage are generally referred to as syndromic because they have the ability to recognize outbreaks based on the symptoms and human behavior, sometimes prior to first contact with the healthcare system. As such,
syndromic surveillance can be defined as the systematic and ongoing collection, analysis and interpretation of data that precedes
diagnosis.
In order to create better surveillance systems, it is important to
know that an estimated 73% of emerging infectious diseases are

IEEE Symposium on Visual Analytics Science and Technology 2007
October 30 - November 1, Sacramento, CA, USA
978-1-4244-1659-2/07/$25.00 ©2007 IEEE

zoonotic in origin[19, 26]. Thus, monitoring the companion animal population of a society (e.g. dogs, cats) can provide early
warning signs for emerging diseases. In conjunction, exposures to
many substances, such as pollutants, chemicals, allergens and natural toxins, originate from the environment and can have a detrimental effect on health. Companion animals are exposed to the same
substances as humans and monitoring their health can function as a
“canary in a coal mine” [27]. It has long been the goal of healthcare
officials to identify and prevent hazardous exposures; however, lack
of infrastructure and reportability in human health monitoring has
hindered progress in this area. As such, we present a visual analytics environment that uses companion animal data in conjunction
with human emergency room data as a detection system for emerging disease outbreaks and public health incidents.
Our application provides a framework for analyzing both human
emergency room data and veterinary hospital data. Various statistical analysis techniques have been applied in conjunction with a
spatio-temporal visualization system. Such an application provides
researchers with the ability to visually search the data for clusters
in both a statistical model view and a spatio-temporal view. By providing linked graphical and statistical analysis views for health care
researchers and public health officials, we hope to improve event
detection and response, while reducing false positives.
Our system uses emergency room data from the Indiana Network
for Patient Care (INPC) and all general visits to the Banfield Pet
Hospitals. The Indiana Network for Patient Care consists of five
major hospital systems that serve more than 390,000 emergency
room visits per year [1]. The Banfield Pet Hospitals provide nationwide coverage with demographics distributed according to human
population density. Coverage of Banfield Pet Hospitals is one location for every 5-mile radius containing 100,000 pet owners, and
currently has greater than 600 veterinary hospitals located in 42
states that service approximately 70,000 pets per week. Hence, our
system has nationwide syndromic coverage by using companion animals as sentinel surveillance, as well as a strong localized coverage
in a major metropolitan area.
Currently, our work has focused on two case studies: 1) seasonal
influenza and its correlation to general companion animal health,
and 2) the effects of an industrial wastewater release on companion
animals and the correlation to potential human health issues. In the
case of seasonal influenza, early findings indicate that there may
be a correlation between general dog respiratory symptoms and the
onset of human influenza. In the case of the industrial wastewater
release, several syndromes for both cats and dogs were analyzed
and preliminary results indicated that the industrial wastewater release negatively influenced the health of companion animals in this
region. Ongoing analysis is being performed in both cases before
any definitive confirmations can be made.
Section 2 describes the motivation and necessity of improved
syndromic surveillance while Section 3 discusses previous work in
this area. Section 4 provides the details of the individual components of LAHVA. Section 5 outlines the details of the particular

27

case studies we use to showcase our system , and Section 6 shows
the application of our system to these case studies. Finally, we discuss conclusions and plans for future work in Section 7.
2

M OTIVATION

Timely and accurate detection of unusual population health trends
is a challenging problem requiring the analysis of data collected
from disparate sources over time. These data sources vary widely in
accuracy and reliability, and it is often the case that unusual health
trends, such as outbreaks or poisonings, often have an incidence
profile (signal) that is obscured by the statistical noise. For instance, the Indiana Public Health Emergency Surveillance System
(PHESS) [9, 10] generates several daily potential outbreak alerts.
However, only a handful of these alerts have proven to be significant events. Current systems, including those described in Section
3, are not capable of both high true positive rates (precision) and
low false positive rates (recall).
In addition to suboptimal accuracy, current population monitoring systems face other challenges. Many existing systems do
not leverage existing messaging and vocabulary standards such as
Health Level 7 (HL7) and LOINC. Further, many systems require
manual data input which further encumbers already overburdened
public health and health care workers, and is infeasible as a long
term solution. Other challenges include the lack of timely data acquisition, data quality concerns (e.g., duplicate records, typographical errors), and accurate data linkage.
Our system attempts to overcome many of these problems
through the use of the Banfield Pet Hospital database. Banfield
is a nationwide system with a geographical coverage similar to the
human population. It captures veterinary visits in real-time for all
Banfield practices, and this data can augment existing human syndromic surveillance efforts. Furthermore, we link to the Indiana
Network for Patient Care (INPC) [1] database and monitor human
health events in the Indianapolis metropolitan region.
3

P REVIOUS W ORK

Data from public health surveillance systems has long been recognized as providing meaningful measures for disease risks in populations [16, 21, 22]. In light of this, many systems have been developed to analyze this data and provide syndromic surveillance
to epidemiologists. Some of the most popular of these systems are
the Early Aberration Reporting System (EARS) [14], the Electronic
Surveillance System for the Early Notification of Communitybased Epidemics (ESSENCE) [17], and Biosense [18].
EARS was developed through the Centers for Disease Control
and Prevention (CDC) and provides epidemiologists with several
aberration detection methods. This system has been implemented
in multiple state and local health departments throughout the United
States and in several other countries. ESSENCE relies on both
syndromic and nontraditional health information to provide early
warnings of abnormal health conditions. This system is implemented in the national capital area, as well as many state health
departments, and utilizes military and civilian healthcare information as the means of identifying abnormal outbreaks. Biosense is
part of a national initiative to detect bio-terrorism. BioSense’s main
goal is to facilitate the sharing of automated detection and visualization algorithms through the creation of national standards. This
implementation will include an internet-based software-system that
includes both spatio-temporal and temporal analysis and currently
operates in more than 20 cities.
Work has also been done in applying interactive visualization
techniques to analyzing human health data. Schulze-Wollgast et al.
[23] developed a system for visualizing health data for the German
state Mecklenburg-Vorpommern. This system allowed users to interactively select diseases and their parameters and view the data

28

over a specific time interval at different temporal resolutions. Further work in this system [24] employed the use of intuitive 3D pencil and helix icons for visualizing multiple dependent data attributes
and emphasizing the type of underlying temporal dependency.
These systems focus specifically on data collected on human
health; however, this data is often encumbered by privacy concerns.
Furthermore, many emergency rooms are not yet collecting electronic records, and those that do collect records often only do data
analysis on the zip code level. In contrast, data collected at the Banfield Pet Hospitals is entered into a national database in real-time,
allowing instant access for analysis. There are no privacy concerns
for pets, so the exact location may be used for analysis instead of
aggregation to the zip code level. As such, our work focuses on
syndromic surveillance by using companion animals as predictors
to increase sensitivity and specificity.
The need for such companion animal monitoring has been outlined in presidential panels [7]; however, little work has been done
in this area. Our system addresses this need by combining data from
Banfield Pet Hospitals with INPC data. Unfortunately, though, not
all methods used for syndromic surveillance in human data are appropriate for syndromic surveillance in companion animals. Due
to the sparsity of pet visits with comparable syndromes, these data
sources exhibit statistically different signal characteristics.
For human data, syndromic surveillance is done through means
of aberration detection. Aberration detection is the change in the
distribution or frequency of important health-related events when
compared with historical data, and can be divided into two broad
categories: case definition methods and pattern recognition methods. Case definition methods employ epidemiological experience
to define syndromes of interest that would indicate an event. For
pattern recognition methods, we employ the use of SatScan [15]
which employs spatial, temporal, and spatio-temporal scan statistics to identify unusual disease clusters in a given population.
For aberration detection, most surveillance systems use long
term data, three or more years, to calculate the expected historical value. However, historical data is not always available. As an
approach for short term aberration detection, many systems employ the use of the CUSUM model (cumulative sum) [14, 13, 12].
CUSUM can be used for a short term (approximately 21 days)
surveillance method and due to the short length, seasonality factors
are less important in the assessment of daily aberrations.
For companion animal data, we have tested several different
aberration detection methods and report on both their benefits and
shortcomings in the following sections.
4

L INKED A NIMAL -H UMAN V ISUAL A NALYTICS S YSTEM

We have developed a system (LAHVA) that combines both human
and animal health data for syndromic surveillance and aberration
detection. Our system consists of three components: a data management component, a statistical analysis component and a visual
analytics component as seen in Figure 1. Our system directly accesses data from INPC and Banfield Pet Hospitals. The INPC data
is updated daily in our database and the Banfield data is updated at
regular intervals of 1 - 3 weeks. Currently, statistical models are
pre-computed in R [20] and S-plus in order to evaluate their potential use. Future versions of the system will directly analyze the data
through direct implementations of these methods.
4.1 Data Management
To support efficient and effective visualization analysis, we have
built a data integration system that supports the transformation,
management, and integration of raw human and animal health data.
In the process, several data management issues were required: (1)
cleaning and transformation of the data arriving from different data
sources, (2) integration and correlation of data (e.g., hospitals and

Figure 1: System infrastructure.

veterinary clinics), and (3) assurance that the data is used in a secure
and privacy preserving way.
4.1.1 Data Preparation
Raw data arriving from emergency departments and Banfield Pet
Hospitals is not directly usable. As such, several data preparation
steps are applied, including data cleaning and transformation. Data
cleaning is used for detecting and removing errors and inconsistencies from the raw data in order to improve the quality of data, and
all data transformations are tracked and recorded. This preparation
also allows us to provide feedback to our data providers in terms of
how well their systems are being managed. Through this, several
previously undetected data management issues have been resolved
in their systems.
4.1.2 Data and Information Integration
Since the data comes from disparate sources stored in different formats, seamless and uniform querying and manipulating of this data
is required. A critical challenge is matching and correlating the human and animal data coming from disparate sources using different
naming conventions, relational schemas, and values that semantically may represent the same symptoms. While at this stage of the
project, most of these issues are resolved in an ad hoc fashion, we
are currently conducting research into different directions to solve
these issues. Possible solutions include using the query logs from
these different databases to automatically match their schemas.
4.1.3 Privacy-Preserving Data Sharing and Analysis
Once the data is processed and stored, data privacy and sharing
concerns needed to be addressed. Since we are dealing with sensitive medical data, we may not make the assumption that access
to this data can be granted without restrictions. In order to ensure
that this data is protected from actions that violate the privacy of
individuals, restrictions have been put into place. However, these
restrictions need to also allow data extracted to be useful for our
visual analytics system. We have to strike a balance between the
need to preserve privacy and our capacity to enable rapid, accurate,
comprehensible, and communicable analyses. Our current system
uses traditional de-identification techniques to address this issue.
We also are working on visual abstractions of the data where the
information being visualized is transformed in such a way that it
does not reveal any private information. This will complement our
privacy preservation techniques applied at lower levels to the raw
data.

4.2 Statistical Modeling
Once the data management system was created, it was necessary to
address the statistical modeling problems of both human and companion animal data. As explained in Section 3, much work has
been done on aberration detection in emergency room data. Unfortunately, many of these techniques are not easily applied to veterinary hospital data. In emergency room data, there are typically
9 to 11 chief complaints, most commonly consisting of: respiratory, gastro-intestinal, hemorrhagic, rash, fever, neurological, botulinic, shock/coma, and other. Multiple cases of these syndromes
are present in emergency room data every day.
In contrast, the Banfield Pet Hospital data is more robust in that
it contains detailed examination records of each pet that visited the
hospital. These records may be searched for syndromes that are
equivalent to emergency room chief complaints; however, the number of cases per day will often be zero. As such, common EARS
analysis methods are not always applicable. In the following sections, we will discuss the statistical methods applied to the companion animal data and potential problems within.
4.2.1 Power Transformation
One method we applied to simplify our analysis was the application
of a logarithm or power transformation to bring the data more in line
with model assumptions [6]. In time series analysis, the logarithm
transformation is widely applied when the mean is proportional to
the standard deviation [3], and in cases where the data consists of
counts following a Poisson distribution a square root transformation
will approximately make the mean independent of the standard deviation. In each case, the transformations are necessary to simplify
the modeling procedure.
Due to the zeros in the animal hospital data, a logarithm is not directly applicable. Naturally, log(x + 1) was tried, but failed to eliminate the skewness on the right tail of the distribution for the number
of observations. A square root transformation did not work either
due to the skewness on the√left tail caused by the zeros. Our experimental results suggest x + 1 gives good performance in terms
of stabilizing the variability and yielding a skew-free distribution in
most cases.
4.2.2 Data Normalization
While a power transformation is useful for some analysis, others
require data normalization to pull out the underlying trend. In the
INPC and Pet Hospital data, daily counts are stored in our database
and the daily counts can vary according to seasonal effects and increases in data collection capacity. Regular daily count plots tend

29

to be very noisy and it is hard to identify abnormal characteristics.
In order to analyze patterns of data over time, we apply a normalization to capture the aberrations in the data. To reduce the noisy
patterns and to compensate for the different scaling in counts over
time we typically use counts per week. For the denominator of our
normalization, we use the sum of the daily counts for the past six
months. This six month sliding window then allows us to observe
the seasonal effects and larger trends while removing day of the
week effects and smaller aberrations. As such, data normalization
of this manner will not be applied when looking for short-term effects.
4.2.3 Aberration Detection for Sparse, Dependent Data
For short term abberation detection, one statistical approach we applied was the use of CUSUM [14, 13, 12]. CUSUM is defined as
the following.


Xt − (µ0 + kσxt )
(1)
St = max 0, St−1 +
σxt
where St is the current CUSUM, St−1 is the previous CUSUM,
Xt is the count at the current time, µ0 is the expected value, σxt
is the standard deviation, and k is the detectable shift from the
mean. µ0 and σxt are computed according to the degree of sensitivity. We use three different models (C1, C2, C3) and each model
uses different time period for the µ0 and σxt computations. For
C1, the baseline period is Day−7 , . . . , Day−1 and a flag is noted on
Day0 . For C2, Day−9 , . . . , Day−3 are used as the baseline and similarly, C3 uses Day−9 , . . . , Day−3 as the baseline but an average of
Day−2 , . . . , Day0 is used to detect the aberration. However, our Pet
Hospital data has a relatively small number of counts and we use
doubled baselines in order to avoid zero count for the baseline period. Here, we see the problems in analyzing our veterinary data
using common human syndromic surveillance methods. The sparsity of the data requires a modification of the CUSUM, and may
produce undesirable false positives.
As previously mentioned, zeros are common among daily counts
of clinical signs among the Banfield pets within a given area (a
radius of a few miles). Consequently, detection of aberrations must
proceed over a large distance, or over longer time periods than a
single day.
While it is common for this kind of data to exhibit both spatial
and temporal variation, some variations may be uninteresting. For
example, there may be temporal dynamics associated with a changing population that are not associated with a particular syndrome.
To achieve reasonable sensitivity and specificity on important signals, it is necessary to first adequately model the unimportant effects. The problem is compounded by the fact that only local estimates of animal population are available.
Bootstrapping is a general-purpose robust alternative to parametric inference used when the analyst does not wish to make strong
parametric assumptions about the data. In the words of its inventor
[8], it “can by applied to complicated situations where parametric
modeling and/or theoretical analysis is hopeless.” The idea is to
sample the data with replacement in order to simulate the distribution of the data and functions thereof. When bootstrapping dependent data, care must be taken to preserve as much of the dependence
structure as possible when doing the resampling. Typically this is
done via a blocked approach; for a univariate time series the sampling units are then contiguous subseries drawn from the original
data. Such a scheme is described by Carlstein et al.[4], with Hanna
et al.[11] among the first applications.
For detection of unusually high levels of symptomatic cases, one
statistic whose distribution can be bootstrapped is the quantile. For
all pets within a radius of incidence, we identify all symptomatic
encounters over a time window of tw days after the alleged release at

30

time t. Over the window [t,t +tw ), there is a distance to the epicenter associated with each symptomatic encounter, and our detection
statistic St⋆ is the radius inside which x% of the window’s symptomatic cases occur. One imagines that an adverse event near the
epicenter will cause the distribution of these distances to be shifted
downward, and our approach seeks to detect such shifts over time.
Our reasons for using the quantile as a measure of location are
severalfold. First, it seems important to choose a statistic not dominated by animals far from the epicenter; a small quantile is likely to
be more sensitive to aberrations close to the epicenter than would an
arithmetic average, for example. Moreover, the distribution of the
average distance is highly influenced by the choice of the radius,
whereas the quantile should be less so. Of course, it is important
not to choose a quantile so small that the bootstrap no longer applies; as an extreme case, the minimum is an example of a quantile
whose distribution cannot be bootstrapped.
Though computationally intensive, the actual bootstrapping
technique is rather straightforward: to obtain R null replicates of
the statistic, one may resample R windows of length tw days corresponding to null data and compute the statistic there, resulting
(1) (2)
(R)
in bootstrap replicates St , St , . . . , St . In prospective mode, the
null data occurs prior to the window under investigation; in retrospective mode, one may opt to include data from after the window
as well. In any case the bootstrap significance associated with S⋆ is
then


(i)
(2)
pt = 1 + number of {St } exceeding St⋆ / (1 + R)
If the mild assumptions underlying the bootstrap hold, the
null distribution of pt is approximately discrete uniform over
{1/R, 2/R, . . . , R/R}. Consequently, if there is no signal in the window under investigation, rejecting the null hypothesis when pt⋆ ≤ α
will result in a false alarm rate of α × 100%. For prospective mode,
one will need to update pt⋆ with the passage of time, and in this
case a plot of pt⋆ versus t is appropriate. In this case the {pt }
are themselves correlated; moreover, the probability of at least one
false alarm grows with t for fixed α . If the number of null windows is less than R (common for our analyses), then bootstrapping
is unnecessary when only a p-value is required, since the bootstrap
p-value will have expectation equal to the fraction of null windows
with statistic at least as extreme as the observed value. However,
statistics such as standard deviation can still benefit from the bootstrap in this situation.
The resampling of different null windows within the same radius
assumes a stationary distribution across time. Of course this cannot
be literally true due to effects such as a changing at-risk population;
nevertheless, by not going too far back in time one may be able to
minimize such temporal effects without needing to incorporate estimates of the population itself. If one is willing to assume the null
distribution does not vary much with local geography, another strategy is to use a second epicenter as a control denominator, though
this introduces another source of variability. For example, for a 20mile radius, one may choose the second epicenter at least 40 miles
away so that there is no overlap.
4.2.4 Seasonal-Trend Decomposition Based on Loess
The previous method discusses the identification of small signals;
however, we are also interested in signal correlation. Our time series signals can be viewed as the sum of multiple trend components: a seasonal component and remainders. For each data signal,
“trend components” are extracted to represent the long term trend
and yearly seasonality using a seasonal decomposition of time series by loess (STL) [5]. Here, the “seasonal component” would
represent the day-of-the-week effect.
Yit = Tit + Sit + Dit + rit

(3)

Figure 2: LAHVA screen shots. (Left) Geospatial temporal view. (Right) Statistical view.

where for the ith series, Yit is the original series, Tit is the long term
trend, Sit is the yearly seasonality, Dit is the day-of-the-week effect,
and rit is the remainder. We can then look at the correlation between
the extracted components to see if they have any potential effects on
each other.

versions of this system will include more robust mapping features
and interactive statistical analysis components.
5

C ASE S TUDIES

4.3 Visual Analytics
Our visual analytics system, LAHVA, takes advantage of both the
data-management and statistical modeling components presented
above. An initial direct access query to the database is done, and
human hospitals, veterinary hospitals and individual animal locations are displayed on an interactive map. Statistical plots are precomputed and linked to the factor specification and filtering components in the system.
In Figure 2, we see the typical LAHVA viewing windows. Emergency rooms are represented by crosses, veterinary hospitals are
represented by the large V’s, cats are triangles, and dogs are circles. For the emergency rooms and veterinary hospitals, the size
and color are determined by the number of cases seen on that given
time period, normalized by either the six-month sliding window
previously discussed, or modified by a power transformation. As
more cases of a particular syndrome are encountered on the specified time period, the colors change from green to red and the glyph
area increases proportionally to the number of cases. Glyph scaling
in the images is also enlarged to help preserve privacy and the scaling during use can be set smaller for higher specificity or larger to
help signal alerts. The time period can be specified as daily, weekly
or monthly using the controls on the bottom right near the slider,
and the slider allows users to move forward and backwards in time.
The case selection and factors are determined by the check boxes
in the upper left corner and more factors are in the process of being
added. Further information can be obtained by left clicking on a
human or animal hospital glyph. This opens an information screen
that details the patient records for the specified time period, see
Figure 2 (Left).
For the cats and dogs, red represents respiratory syndromes, blue
would represent gastro-intestinal syndromes and green would represent eye-inflammation syndromes. For prototyping purposes, the
lower left window contains pre-computed plots of the data for varying factors. The main window contains the time-varying geo-spatial
interface. Time is controlled by the slider on the lower portion of
the window. By clicking on the statistical window plot, the main
window and lower-left window of the system will switch allowing
for different types of analysis as seen in Figure 2 (Right). Future

Figure 3: Banfield Pet Hospital visits in the release area.

In order to evaluate our system and test different aberration detection methods, two case studies were chosen. The first case study
uses both the human and companion animal data for enhanced syndromic surveillance, while the second case uses only the companion animal data to demonstrate the benefits of this population in
syndromic surveillance.
5.1 The Effects of Seasonal Influenza
Our first case study focuses on correlations between companion
animal and human illnesses. Particularly, we analyze seasonal
influenza through emergency room department chief complaints.
Much work has already been done on identifying seasonal influenza
via chief complaint (e.g., [25, 2]). However, little has been done
in comparing equivalent flu-like syndromes in companion animals.
For our work, we are using eighteen emergency rooms based in
the Indianapolis metropolitan region. Trends of cat and dog illnesses in Indiana and bordering metropolitan areas were analyzed.
For comparison, we focused on cats and dogs reporting respiratory
syndromes and compare how these would correlate to emergency
room chief complaints of respiratory syndromes.

31

Figure 4: Using LAHVA to identify seasonal influenza.

5.2 Assessing Effects of a Chemical Release
Our second case study focuses on using pets as sentinels to detect
unusual events. Here, we focus on the release of industrial wastewater. The site in question has been anonymized and is shown in
Figure 3. The release center is denoted as a red diamond.
In order to examine the effects of this release, the local Department of Health led an investigation in the region. This region
has a human population of approximately 8,500; and the combined human population of the nearby communities is approximately 28,000. Unfortunately, lack of human health data sources
led the local Department of Health to assess these effects through
a self-reported survey. In contrast, our study focuses on pets in
a twenty-mile radius surrounding the site using data from Banfield,
the pet hospital. We have patient records for 74,660 dog and 21,202
cat visitations in this area spanning the time period prior to and following the release dates. Distributions of these patients can be seen
in Figure 3.
6

R ESULTS

In order to test the functionality of our system, LAHVA was applied to the case studies described in Section 5. Various statistical
methods were used to test their functionality in conjunction with
the geospatial temporal viewing window.
6.1 Seasonal Influenza Analysis
Our first case study was an analysis of seasonal influenza using
LAHVA. In Figure 4 we show the temporally varying window centered over the Indianapolis metropolitan area. The factor specification is showing cases of human and companion animals showing
signs of respiratory illnesses. From LAHVA, one can easily identify the onset of seasonal influenza as the hospitals begin showing
signs of increased respiratory cases. Viewing the statistical plot
coupled with this allows us to see the overlying trend of respiratory syndromes in this area over a multi-year period. The blue line

32

Figure 5: Yearly pattern for human and dog respiratory syndromes.

in the plot represents the INPC hospitals, the magenta line represents dogs with respiratory syndromes and the yellow represents
cats with respiratory syndromes.
We also applied the STL analysis to see if there were correlations between dog respiratory syndromes and human respiratory
syndromes. The yearly seasonal components for these two series
are overlaid in Figure 5. Here, we can see the similarity between
the two. The data are standardized by subtracting the mean and dividing by standard deviation for visualization and comparison purposes. The grey bars are used to roughly illustrate the local maximum values over time providing evidence that respiratory symptoms in dogs occur approximately 10 days earlier than that of the

Figure 6: Dogs (circles) and cats (triangles) showing eye-inflammation (green), respiratory (red), and gastro-intestinal (blue) syndromes near the
release. (Left) June 22 - 28. (Right) June 29 - July 5.

Dog Eye Inflammation

Cat Eye Inflammation

15

60

50

12

40
9
30
6
20
3

10

9/22/2006

11/22/2006

7/22/2006

5/22/2006

3/22/2006

1/22/2006

9/22/2005

11/22/2005

7/22/2005

5/22/2005

3/22/2005

1/22/2005

9/22/2004

11/22/2004

7/22/2004

5/22/2004

3/22/2004

1/22/2004

9/22/2006

11/22/2006

7/22/2006

5/22/2006

3/22/2006

1/22/2006

9/22/2005

11/22/2005

7/22/2005

5/22/2005

3/22/2005

1/22/2005

9/22/2004

11/22/2004

7/22/2004

5/22/2004

3/22/2004

0
1/22/2004

0

Figure 7: CUSUM for pets Showing Eye-inflammation Syndromes.(Left) Dogs , (Right) Cats

humans in regular years.
6.2 Industrial Wastewater Release Analysis
Our second case study analyzes the effects of an industrial wastewater release through companion animal surveillance. Three syndromes were identified as being potential indicators of adverse effects due to a release: eye inflammation, respiratory, and gastrointestinal. In Figure 6 we see an area within a 20 mile radius of the
spill. Cats are triangles and dogs are circles. In the week following
the spill, what seems to be an unusual amount of eye-inflammation
cases appear near the source. Figure 6 (Left) is one week prior to
the spill (June 22 - 28). Figure 6 (Right) is the week starting the day
of the spill (June 29 - July 5). The green glyphs represent animals
with eye-inflammation.
Once a problem is visually identified in our system, different statistical analyses can be run to confirm or deny problems in that area.
CUSUM was applied to the data to determine if any alerts would be
generated for eye-inflammation in this area. Figure 7 shows the resultant CUSUM plots using CUSUM2. Due to the small number
of eye-inflammation cases seen over the course of a year, it is difficult to determine any direct information from applying CUSUM
directly to the pet syndrome data. Current work is being done to
find ways to potentially better apply CUSUM to the data.
Due to the data sparsity, the application of CUSUM was not ef-

fective in this case. In order to further verify that problems with
eye-inflammation occurred, the bootstrapping method discussed in
Section 5.2.3 was applied. To illustrate the procedure and effect
size Figure 8 shows a plot of distance to the alleged release point
versus time, with horizontal bars indicating the 10% quantiles for
each 21-day window. This results are shown in Table 1, and indicate that eye-inflammation in dogs was significant near the release
in our time period of interest.
species
canine
feline

statistic
mean 10% quantile before
10% quantile during
1-sided bootstrap p-value
mean 10% quantile before
10% quantile during
1-sided bootstrap p-value

eye inflammation
8.035
2.365
0.006
11.195
17.531
0.909

Table 1: Summary of the bootstrap analysis findings.

7 C ONCLUSIONS AND F UTURE W ORK
Our work has demonstrated the benefits of creating a linked visualstatistical analysis system for health surveillance, and our methodologies are currently being applied to other case studies. It is clear

33

symptom = eye inflammation, p = 0.006
20

15

10

5

07
n
Ja

N

ov

20

20

06

06
20
Se

Ju

p

l2

00

6

06
ay
M

ar
M

n
Ja

20

20

06

06

0

20

miles from location of alleged release

canine

date

Figure 8: Visualization of the bootstrap analysis for eye inflammation
in dogs.

that using companion animals for syndromic surveillance has great
potential for early aberration detection; however, more work is
needed to determine appropriate methodologies for using companion animals as sentinels. Our system has demonstrated the use of
applied visual analytics through two different case studies. In both
cases, the visuals allow users to easily locate potential problems in
a region and then apply further statistical analyses to confirm their
suspicions.
In the case of the effects of human influenza on general dog respiratory symptoms, we were able to find early signs indicating that
there may be correlations between these events. In the case of the
industrial wastewater spill, we were able do identify problem areas.
From these problem areas, statistical tests were generated and we
were able to verify what was seen visually.
While our current work has been retrospective, we intend to
modify the system and integrate our statistical models for better
interactivity. By doing this, we can provide health care officials and
epidemiologists with tools to monitor varying regions of the country and provide better detection for potential disease outbreaks and
health incidents.
Future work will focus on verification of these case study results, as well as others, and system enhancements to LAHVA. Current plans include adding the statistical analysis features directly to
LAHVA and allowing users to interactively select areas of the map
to analyze for potential health issues. Also, given the discreteness
of illness data, i.e., records only exist on the day pets visit, we also
plan to add time ghosting for an approximated contagious period.
This period will be based on syndrome and interactively modifiable.
8 ACKNOWLEDGMENTS
We would like to thank the Department of Homeland Security and
the Lily Endowment Fund for funding this work as well as our partners at Banfield the Pet Hospital and the Indiana Network for Patient Care for providing the datasets.
R EFERENCES
[1] P. G. Biondich and S. J. Grannis. The Indiana network for patient care:
An integrated clinical information system informed by over thirty
years of experience. Public Health Management Practices, pages 81
– 86, Nov 2004.
[2] F. Bourgeois, K. Olson, J. Brownsten, A. McAdam, and K. Mandl.
Validation of syndromic surveillance for respiratory infections. Annals
of Emergency Medicine, 47:265 – 271, 2006.
[3] P. J. Brockwell and R. A. Davis. Introduction to Time Series and
Forecasting (2nd edition). Springer, 2003.
[4] E. Carlstein. The use of subseries values for estimating the variance
of a general statistic from a stationary sequence. Annals of Statistics,
14:1171–1179, 1986.

34

[5] R. B. Cleveland, W. S. Cleveland, J. McRae, and I. Terpenning. Stl:
A seasonal-trend decomposition procedure based on loess. Journal of
Official Statistics, 6:3–73, 1990.
[6] W. S. Cleveland. Visualizing Data. Hobart Press, 1993.
[7] N. R. Council. Animals as Sentinels of Environmental Health Hazards.
National Academy Press, Washington, DC, 1991. Library of Congress
Catalog NO.91-61734.
[8] B. Efron. The Jackknife, the Bootstrap and Other Resampling Plans.
SIAM, Philadelphia, 1982.
[9] S. J. Grannis, P. G. Biondich, B. W. Mamlin, G. Wilson, L. Jones,
and J. M. Overhage. How disease surveillance systems can serve as
practical building blocks for a health information infrastructure: the
Indiana experience. In AMIA Annual Symposium, pages 286 – 290,
2005.
[10] S. J. Grannis, M. Wade, J. Gibson, and J. M. Overhage. The Indiana public health emergency surveillance system: Ongoing progress,
early findings, and future directions. In American Medical Informatics
Association, 2006.
[11] S. R. Hanna. Confidence limits for air quality model evaluations, as estimated by bootstrap and jackknife resampling methods. Atmospheric
Environment, 23:1385–1398, 1989.
[12] L. Hutwagner, T. Browne, G. M. Seeman, and A. T. Fleischauer. Comparing aberration detection methods with simulated data. Emerging
Infectious Diseases, 11(2):314 – 316, February 2005.
[13] L. C. Hutwagner, W. W. Thompsom, G. M. Seeman, and T. Treadwell.
A simulation model for assessing aberration detection methods used in
public health surveillance for systems with limited baselines. Statistics
in Medicine, 24(4):543 – 550, February 2005.
[14] L. C. Hutwagner, W. W. Thompson, and G. M. Seeman. The bioterrorism preparedness and response early aberration reporting system
(ears). Journal of Urban Health, 80(2):i89 – i96, 2003.
[15] M. Kulldorff. A spatial scan statistic. Communications in Statistics:
Theory and Methods, 26, 1997.
[16] A. D. Langmuir. The surveillance of communicable diseases of national importance. New England Journal of Medicine, 268:182 – 192,
1963.
[17] J. S. Lombardo. A systems overview of the electronic surveillance system for the early notification of community based epidemics
(ESSENCE II). Journal of Urban Health, 80:32 – 42, 2003.
[18] J. W. Loonsk. Biosense - a national initiative for early detection and
quantification of public health emergencies. MMWR, 53:53 – 55,
2004.
[19] M. Pappaioanou, T. Gomez, and C. Drenzek. New and emerging
zoonoses. Emerging Infectious Diseases, 10(11), Nov 2004.
[20] R Development Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna,
Austria, 2007. ISBN 3-900051-07-0.
[21] S. B. Thacker and R. L. Berkelman. Public health surveillance in the
united states. Epidemiology Review, 10:164 – 190, 1988.
[22] S. B. Thacker, R. L. Berkelman, and D. F. Stroup. The science of
public health surveillance. Journal of Public Health Policy, 10:187 –
203, 1989.
[23] C. Tominski, P. Schulze-Wollgast, and H. Schumann. Visual analysis
of health data. In 2003 IRMA International Conference, 2003.
[24] C. Tominski, P. Schulze-Wollgast, and H. Schumann. 3d information visualization for time dependent data on maps. In International
Conference on Infomation Visualization (IV), 2005.
[25] F.-C. Tsui, M. M. Wagner, V. Dato, and C.-C. H. Chang. Value of
ICD-9-Coded Chief Complaints for Detection of Epidemics. J Am
Med Inform Assoc, 9(90061):S41–47, 2002.
[26] M. E. J. Woolhouse and S. Gowtage-Sequeria. Host range and
emerging and reemerging pathogens. Emerging Infectious Diseases,
11(0997), Nov 2005.
[27] R. D. Zane. Syndromic surveillance: A canary in the coal mine? Journal Watch Emergency Medicine, pages 265 – 271, April 2006.

J Geogr Syst (2014) 16:183–209
DOI 10.1007/s10109-013-0193-4
ORIGINAL ARTICLE

Spatio-temporal analysis of industrial composition
with IVIID: an interactive visual analytics interface
for industrial diversity
Elizabeth A. Mack • Yifan Zhang • Sergio Rey
Ross Maciejewski

•

Received: 20 December 2012 / Accepted: 6 September 2013 / Published online: 8 October 2013
Ó Springer-Verlag Berlin Heidelberg 2013

Abstract The industrial composition of places has received considerable attention
because of the widespread belief that industrial diversity buffers regional economies
from economic shocks. Subsequently, a variety of toolkits and indices have been
developed with the goal of better capturing the compositional dynamics of regions.
Although useful, a key drawback of these indices is their static nature, which limits the
utility of these indices in a space–time context. This paper provides an overview of and
applications of an interface called interactive visualization tool for indices of industrial
diversity, which is a visual analytics tool developed specifically for the analysis and
visualization of local measures of industrial composition for areal data. This overview
will include a discussion of its key features, as well as a demonstration of the utility of
the interface in exploring questions surrounding diversity and the dynamic nature of
composition through space and time. A focus of this demonstration is to highlight how
the interactivity and query functionality of this interface overcome several of the
obstacles to understanding composition through space and time that prior toolkits and
comparative static approaches have been unable to address.
Keywords Space–time  Visual analytics  Industrial composition 
Indices of industrial diversity
JEL Classification

C8  O10  R10

1 Introduction
The ability to analyze and display spatio-temporal data is a topic that has
preoccupied researchers across multiple disciplines for some time, with a more
E. A. Mack (&)  Y. Zhang  S. Rey  R. Maciejewski
Tempe, AZ, USA
e-mail: eamack1@asu.edu

123

184

E. A. Mack et al.

recent focus on space–time dynamics relating to economic growth and development
(Rey and Janikas 2006; Bode and Rey 2006; Ye and Rey 2011; Ye and Carroll
2011). This recent focus is related to our expanding ability to store and analyze large
datasets and the ongoing quest to develop better tools for spatio-temporal analysis,
particularly in a GIS context (Kwan 1999; MacEachren et al. 1999; Peuquet 1994).
The ability to effectively analyze data over space and time has also become
increasingly necessary to better understand the vulnerability and stability of places
in an integrated and volatile global economy (Ye and Rey 2011).
With respect to global economic dynamics, an ongoing area of research is the
search for the link between industrial composition and economic stability (Duranton
and Overman 2008; Ellison et al. 2010; Marcon and Puech 2010; Drucker 2011;
Drucker and Feser 2012), which began in the era of the Great Depression (Dissart
2003). Past research has produced several measures of industrial composition which
leverage different aspects of economic theory and portfolio theory to quantify and
test our intuition that diversity is related to economic stability (Siegel et al. 1995a).
To date, however, users of these metrics must rely on comparative statics in the
form of maps and tables to portray diversity through space and time.
Although these metrics are important to understanding diversity, the static nature
of these measures makes it difficult to understand changes in industrial diversity.
Some persistent questions about compositional dynamics include: How dynamic is
composition overtime and how is this impacted by recessions and boom periods in
economic history? Are there places with similar compositions to Detroit and Silicon
Valley? How robust are these economies to shocks? These are important questions
to answer to better understand the dynamics of lesser known regions, which may be
particularly robust to economic shocks or that suffer disproportionately from natural
disasters or national business cycles.
Since the development of these metrics began, advances in geographic
information systems (GIS) and related visualization and analytical toolkits such
as GeoDa (Anselin 2003b) and STARS (Rey and Janikas 2006) have greatly
advanced our ability to understand trends in data through space and time. However,
these programs are not tailored to deal explicitly with some of the unique aspects of
indices of industrial diversity. In particular, these and other toolkits have limited
logical operands and similarity measures, which would permit the identification of
geographically distant areas with similar compositional dynamics.
In this regard, the spatial nature of these toolkits is a double-edged sword: space
permits the evaluation of spillover effects to the exclusion of considering trends in
non-neighboring areas. Subsequently, key questions about diversity remain
unanswered that are crucial to understand the impact of composition on the growth
dynamics of regional economies. For example, are particular regions of the country
rapidly specializing or diversifying or is this a less universal, place-specific trend
that evolves slowly overtime? What are the best ways to identify geographically
distant areas that may have similar compositional trends?
Given these important questions surrounding industrial composition, this study
provides an overview of and applications of an interface called interactive
visualization tool for indices of industrial diversity (IVIID), which is a visual
analytics tool developed specifically for the analysis and visualization of local

123

Spatio-temporal analysis of industrial composition with IVIID

185

measures of industrial composition for areal data. This overview will include a
discussion of its key features, as well as a demonstration of the utility of the
interface in exploring questions surrounding diversity and the dynamic nature of
composition through space and time. A focus of this demonstration is to highlight
how the interactivity and query functionality of this interface overcome several of
the obstacles to understanding composition through space and time that prior
toolkits and comparative static approaches have been unable to address.

2 Metrics of industrial diversity
Ever since the economic crisis associated with the Great Depression, researchers
have been searching for ways to quantify diversity and evaluate the impact of
diversity on economic growth (Dissart 2003). This search has taken on a variety of
foci through the years. In the 1960s, research efforts focused on the development of
measures of diversity and their linkage with the stability of regional economies
(Dissart 2003). This focus on indices evolved in the mid-1970s into an interest in the
application of portfolio theory to evaluate the link between diversity and stability
(Dissart 2003). More current research has focused on input-output-based measures
of diversity (Siegel et al. 1993; Alwang and Siegel 1994; Siegel et al. 1995a, b;
Wagner and Deller 1998) and linking input-output techniques with econometrics
(Isard et al. 1998; Rey 1998, 2000). In this section, a brief overview of indices of
industrial diversity will be provided. For a more thorough treatment of these indices,
readers are referred to review pieces written by Siegel et al. (1995a, b), Wagner
(2000), and Dissart (2003).
Indices of industrial diversity measure the relative concentration of a particular
economic variable across a specified number of industries for each spatial unit of
interest for a given time interval. Economic variables used in these measures vary
widely and may include employment data, earnings data, and firm data. Industries
typically pertain to a defined classification system, such as the standard industrial
classification (SIC) system or the North American industrial classification system
(NAICS). Wagner (2000) categorizes the multitude of diversity metrics developed
overtime into four categories: equiproportional measures, industry measures,
portfolio-theoretic measures, and input-output-based measures. The intuition behind
equiproportional measures of diversity is that maximum diversity is achieved when
all sectors contain equal proportions of economic activity, as specified by
employment, firms, or any other variable of interest (Wagner 2000)
Industry-based measures of diversity examine the relative concentration of
economic activity in a particular sector relative to all regional activity. Examples of
these metrics include the percent of goods classified as durable goods, location
quotients, and shift-share-based numbers (Wagner 2000). Portfolio-based measures
leverage both Markowitz (1959) and Sharpe (1970) portfolio theory to consider the
impact of diversity on economic stability (Wagner 2000). Essentially, the sectors of
the economy are treated like a portfolio of assets, and the goal is to combine a
number of sectors to minimize the variance or the fluctuation of this portfolio (as
measured in jobs or income) overtime (Dissart 2003) while maximizing growth.

123

186

E. A. Mack et al.

Input-output (IO)-based approaches use the direct requirements table in the I-A
matrix and the total requirements table (the Leontief inverse or the inverse of the
I-A matrix) to examine linkages between industrial diversity and economic stability
(Wagner 2000). Use of IO information is typically used in one of two ways: it is
either incorporated into portfolio-based measures of diversity or is used to construct
a summary measure of stability (Wagner and Deller 1998). For example, Wundt and
Martin (1993) incorporate the I-A matrix as a constraint in a portfolio-theoreticbased optimization problem to analyze how inter-industry linkages impact
fluctuations in regional employment.
A common feature of these four types of industrial diversity measures is their
static nature. Each of these approaches are computed with cross-sectional data and
are thus a snapshot of economic activity at one point in time. This presents both
analytical and visualization challenges for exploring these metrics through space
and time. Given this limitation, indices are often ranked, placed in table format, and/
or mapped (Conroy 1975; Attaran and Zwick 1987; Mack et al. 2007a).
Computation of these measures is typically done offline using traditional data
analysis tools (Excel, SAS, etc.). The data are then imported into a geographic
information system (GIS) for visualization and display purposes.
While the calculation of these indices is straightforward and relatively fast, it is
the identification of places with similar levels of industrial diversity and similar
trends in diversity that is quite time-consuming and labor intensive for researchers
given the current set of analytical toolkits available. The functionality of IVIID
helps overcome some of these mechanical analytical approaches to exploring
industrial diversity. Before describing some of the key components of IVIID that
help explore diversity through space and time, the next section will discuss some
key antecedents of this visual analytics interface.

3 Software packages for data analysis
The limitation of comparative statics, which persists despite the evolution of
geographic information systems (GIS), has prompted researchers to develop new
tools to allow analysts to interactively explore data. In fact, since Hagerstrand’s
seminal work on time geography (Hägerstrand 1970), the issue of analyzing
phenomena through both space and time continues to challenge geographers and
researchers in related fields. This issue became especially prevalent due to the
evolution of the data storage capacity and analytical potential of geographic
information systems (Harvey 1991; Openshaw 1994; Peuquet 1994; Peuquet and
Duan 1995; Peuquet 2001; Kwan 1998). For example, Openshaw (1994) noted that
of all the tools available for exploring and analyzing data, only space–time statistics
are capable of handling data with three dimensions. Thus, more sophisticated tools
are required to handle the proliferation of GIS data that are capable of separating
meaningful patterns from noise (Openshaw 1994).
Since the early 1990s, a number of approaches for dealing with spatio-temporal
data have been proposed by the GIS community. These approaches may be grouped
into five categories (Peuquet 2001):

123

Spatio-temporal analysis of industrial composition with IVIID

1.
2.
3.
4.
5.

187

Ontological issues
The development of appropriate database models and languages
Scaling and accuracy issues
Queries and graphical user interfaces
Indexing databases with spatio-temporal information

More recent work on toolkits for exploratory data analysis is closely related to
category four: queries and graphical user interfaces. The proliferation of these
toolkits began in the early 1990s (Anselin and Bao 1997) and is wide ranging in
their application areas and functionality. Table 1 provides an overview of some of
these toolkits including their key features, their spatio-temporal capability, and their
respective programming languages. This table is not meant to be a complete review
of all possible toolkits created since work in this area is quite extensive. In fact,
since the early 1990s, the range of tools for exploratory data analysis has expanded
to include statistical calculations that may be performed in a GIS such as the
Moran’s I, local Moran, and Getis and Ord G-statistics, as well as Python libraries
for spatial constrained cluster analysis or analytical regionalization such as
ClusterPy (Duque et al. 2011). Instead, this table is meant to provide an overview
of toolkits that were foundational to the development of IVIID, which will be
described in a later section of this paper.
A common feature of these toolkits is their one-directional integration with GIS
(Anselin et al. 1993). One-directional integration means data are taken from a
geographic information system and imported into a separate toolkit for analysis, or
results are taken from a toolkit and placed back into a GIS for display and mapping
(Anselin and Bao 1997). Some efforts have been made to link these packages to a
GIS and/or statistical software to minimize the required transfer of data, but many of
the toolkits in Table 1 remain stand alone packages. For example, Symanzik et al.
(1996) linked ArcView to the data visualization software XGobi. Symanzik et al.
(1997) expanded on this initial work to also link the statistical software XploRe to
ArcView and XGobi.
Other common features of the toolkits in the table are their interactivity, via
techniques such as linking and brushing, for exploratory spatial data analysis
(ESDA). For example, in GeoVISTA Studio, selected counties on a map are also
highlighted in a corresponding parallel coordinate plot (PCP) window (Takatsuka
and Gahegan 2002). In terms of ESDA, Crimestat is an example of a program that
leverages classic EDA and ESDA tools for crime analysis. The hotspot functionality
in particular uses several techniques including the local Moran (Anselin 1995) and
k-means clustering (Everett 1974; McBratney and Gruijter 2006) to identify
elevated areas of crime in an exploratory fashion.
A final item of note regarding Table 1 is the functionality of the interfaces for
visualization, spatial, temporal, spatio-temporal analysis, and nonspatial analysis
specified in the table as Vis, S, T, ST, and NS respectively. Vis corresponds to
systems that are designed for visualization purposes and provide interactive
graphics. These systems may or may not handle spatial or temporal attributes;
however, their commonality is an interactive graphical display of data (as opposed
to text only reports). The cartographic data visualizer (CDV) is an example of this

123

Analytical toolkit

REGARD (radical effective
graphical analysis of
regional data)

SPIDER (spatial interactive
data explorer)

XGobi

XmdvTool

SaTScan

SAGE (spatial analysis in a
GIS environment)

CDV (cartographic data
visualizer)

CrimeStat

GeoVista studio

SpDep (spatial dependence)

GeoDa (geographic data
analysis)

Dcluster

Improvise

No.

1.

123

2.

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

Weaver (2004)

Gomez-Rubio et al.
(2003)

Anselin (2003b)

Bivand and Gebhardt
(2000)

Gahegan et al. (2000)

Levine (1999)

Dykes (1995)

Wise et al. (1997)

Kulldorff (1997)

Ward (1994)

Swayne et al. (1991)

Haslett et al. (1990)

Unwin et al. (1990)

First citation

Table 1 Overview of relevant analytical toolkits

Weaver (2006)

N/A

Anselin et al. (2006)

Bivand (2002), Anselin
(2003a)

Takatsuka and Gahegan
(2002), Takasuka and
Gahegan (2001)

Levine (2006), (2010)

Dykes (1998)

Haining et al. (2000), Wise
et al. (2001)

Kulldorff et al. (1998, 2005)

Rundensteiner et al. (2007)

Symanzik et al. (1997, 2000)

N/A

Wills et al. (1991)

Related citations

2007

2004

2003

2002

2002

1999

Not released

1997

1997

1994

1996

Not released

Not released

Year first released

2011

V0.2–5 released in
2012

Shifted to
OpenGeoDa
V1.2.0

V0.5–53 released in
2012

V1.2 released in
2007

V3.3 released in
2010

N/A

No updates

V9.1.1 released in
2011

V8.0 released in
2010

Shifted to GGobi
V2.0

N/A

N/A

Updates

Demographic data
(2007)

Sudden infant death
syndrome (2003)

Homicide counts and
rates (2006)

African conflict data
(2003)

Forest habitat (2002)

Crime data (2006)

Educational
attainment(1998)

Health data (1997)

Epidemiological data
(1997)

Census data (2007)

Corn yield measurements
(1997)

Geochemical analyses
(1991)

Irish socioeconomic data
(1991)

Example application

Y

N

Y

N

Y

Y

Y

Y

N

Y

Y

Y

Y

Visa
(Y/ N)

188
E. A. Mack et al.

N

Y

N

N

2.

3.

4.

IVIID (interactive
visualization tool for
indices of industrial
diversity)

18.

1.

STAMP (spatial-temporal
analysis of moving
polygons)

17.

N

N

N

N

N

N

N

N

Y

N

N

N

NSe (Y/ N)

N/A

C/C??

C

Pascal

Pascal

Non-geographic

Point

Point

Point, areal

Data type

McIntosh and Yuan (2005),
Sadahiro and Umemura
(2001)

N/A

Janikas and Rey (2005)

Liu and LeSage (2010)

Related citations

Programming language

Robertson et al. (2007)

Guo et al. (2006)

Rey and Janikas (2006)

LeSage and Pace (2004)

First citation

STd (Y/ N)

VIS-STAMP (Visualization
system for space–time and
multivariate patterns)

16.

Tc (Y/ N)

STARS (space–time analysis
of regional systems)

15.

Sb (Y/ N)

Arc_Mat

14.

No.

Analytical toolkit

No.

Table 1 continued

N/A

No updates

No updates

V0.8.2 released in
2006

V1.0 in 2010

Updates

N-dimensional brushing, four methods for
displaying multivariate data in both flat
and hierarchical approach

Visualization engine, high-dimensional
drawing, handle missing value,
manipulation, and display of the scatter
plot

Multiwindow, dynamic linking, and
ability of layers manipulation

Regional analysis, network analysis,
animation, cross-layer linking, and
interactive graphics

Key features

Not released

2007

2006

2004

2004

Year first released

Y

Y

Y

Y

Y

Visa
(Y/ N)

http://davis.wpi.edu/xmdv/index.html

http://www2.research.att.com/areas/
stat/xgobi/#xgobi-paper

No webpage link

http://www.statlab.uni-heidelberg.de/
projects/workshop/Regardinfo.html

Link

Economy data

Wildfire spread (2007)

Company data (2006)

Regional income (2006)

Population growth data
(2004)

Example application

Spatio-temporal analysis of industrial composition with IVIID
189

123

Sb (Y/ N)

Y

Y

Y

Y

Y

Y

Y

Y

N

No.

5.

6.

123

7.

8.

9.

10.

11.

12.

13.

N

N

N

N

N

N

N

N

Y

Tc (Y/ N)

Table 1 continued

N

N

N

N

N

N

N

N

Y

STd (Y/ N)

N

N

N

N

Y

Y

N

Y

N

NSe (Y/ N)

Java

R

C??

R

Java

C??

Tcl/Tk

C

Java

Programming language

Areal

Areal

Point, areal

Point, areal

Point, areal

Point

Areal

Areal

Point, areal

Data type

Declarative visual query language,
multiple coordinated views, and
integrated metavisualization

A set of functions for the detection of
spatial clusters

Interactive environment that combines
maps with statistical graphics and
methods of descriptive spatial data
analysis, such as spatial autocorrelation
statistics, spatial regression

A collection of various spatial analysis
functions such as regional aggregation,
spatial autocorrelation, spatial
regression model.

Modular nature, codeless environment,
combining computational clustering and
sorting with cartographic and
information visualization methods

Analyze the distribution, identify hot
spots, indicate spatial autocorrelation,
monitor the interaction of events, and
have specific crime analysis tools

Interactive cartographic visualization,
comprises interpreted scripts for
extension

Exploratory data analysis and exploratory
spatial data analysis by utilizing GIS

a flexible user interface for computing
scan statistics for a variety of
distributions to detect statistically
significant clusters

Key features

http://www.cs.ou.edu/weaver/
improvise/index.html

http://www.uv.es/geeitema/Virgilio/
Rpackages/DCluster/index.shtml

https://geodacenter.asu.edu/projects/
opengeoda

http://cran.r-project.org/web/
packages/spdep/index.html

http://www.geovistastudio.psu.edu/
jsp/index.jsp

http://www.icpsr.umich.edu/
CrimeStat/about.html

http://www.spatial-modelling.info/
CDV-cartographic-data

ftp://ftp.shef.ac.uk/pub/uni/academic/
D-H/g.old/sage/sagehtm/sage.htm

http://www.satscan.org/

Link

190
E. A. Mack et al.

Y

N

N

N

15.

16.

17.

18.

e

d

c

b

Y

N

Y

Y

N

Tc (Y/ N)

Nonspatial functionality

Spatial-temporal functionality

Temporal functionality

Spatial functionality

Visualization

Y

14.

a

Sb (Y/ N)

No.

Table 1 continued

N

Y

N

Y

N

STd (Y/ N)

Y

N

Y

N

Y

NSe (Y/ N)

C??

VB.Net

Java

Python

Matlab

Programming language

Areal

Point

Areal

Areal

Areal

Data type

All interactive linked views, dynamic
analytic filter, similarity computing, and
indices calculation

Analyzing changes in multiple polygon
layers inside ArcGIS, such as
phenomena that change spatially
through time

Self-organizing map, combine
visualization with clustering, sorting

A number of recently developed methods
of space–time analysis with an array of
dynamically linked graphical views

Basic choropleth mapping and linked
exploratory graphs combined with
spatial data modeling

Key features

No webpage link

http://www.geog.uvic.ca/spar/stamp/
help/index.html

http://www.spatialdatamining.org/
software/visstamp

http://regionalanalysislab.org/index.
php/Main/STARS

http://www.spatial-econometrics.com

Link

Spatio-temporal analysis of industrial composition with IVIID
191

123

192

E. A. Mack et al.

kind of toolkit. The abbreviation S corresponds to systems with an explicitly spatial
component where spatial analysis is defined as techniques that account for spatial
autocorrelation or analyze the underlying processes behind data with a locational
component. This functionality includes variograms, Moran’s I (Moran 1948),
Geary’s C (Geary 1954), the Getis and Ord G statistic (Getis and Ord 1992), and the
local Moran (Anselin 1995).T refers to toolkits capable of analyzing temporal data
via techniques such as similarity metrics, control charts, ARIMA modeling, and
time series plots.
ST in the table corresponds to systems that are able to import and analyze data
with both spatial and temporal components. Although this definition does not
correspond to true spatio-temporal analysis, where space and time are analyzed
simultaneously, these systems represent great strides in overcoming the analytical
challenges associated with spatio-temporal data mentioned previously. Of the
toolkits highlighted in this table, just three were designed with original spatiotemporal functionality: STAMP, STARS, and SaTScan.
STAMP is a toolkit that examines geometric changes for polygons where
association through space and time is defined as the union between layers in
consecutive time periods. Change is characterized as a series of events: generation,
disappearance, expansion, or contraction, and information about these events is
stored as a field in a GIS layer (Robertson et al. 2007). Global and local change
metrics are also computed prior to the creation of the polygon change layer to
characterize changes in polygons through space and time (Robertson et al. 2007).
STARS is a Python-based toolkit for the spatio-temporal analysis of areal data and
is comprised of two parts, a geocomputational module and a visualization module,
which may be used together or separately (Rey and Janikas 2006). Key features of
this toolkit include the Gini and Theil inequality measures as well as the capability
of performing traditional and spatial Markov analyses (Rey and Janikas 2006).
Interestingly, both GeoDa and Crimestat were not originally space–time capable,
but have been revised to include this functionality in later releases. Version 1.2 of
OpenGeoDa is now capable of analyzing spatio-temporal data via map animation and
comparative static box plots (Anselin 2012). Users merely need to create variables in a
.dbf for each time period of interest and then convert the table to a space–time project
(Anselin 2012). More current releases of Crimestat also make spatio-temporal
analyses possible. Space–time tools within Crimestat include the STAC or the Spatial
and Temporal Analysis of Crime routine (Block and Block 1995), as well as the Knox
and Bartlett (1964) and the Mantel (1967) tests for space–time interaction. Finally, the
NS column defines capabilities of toolkits that analyze data but may not use the spatial
component of data directly. Functionalities that support this type of analysis include
k-means clustering, principal component analysis, and other data mining algorithms
that do not explicitly look at the geographic space of the data.

4 Interactive visualization tool for indices of industrial diversity (IVIID)
Although these tools contain a diverse array of analytical tools, their respective
functionalities are not necessarily appropriate for the analysis of industrial diversity.

123

Spatio-temporal analysis of industrial composition with IVIID

193

While spatial trends may be prevalent in specialization, it is also likely that
geographically distant areas may have similar industrial trends. Currently, it is
difficult to detect the presence of such trends via a visual inspection of maps. It is
also not possible to detect such trends via spatial analytical tools that define a
neighborhood in terms of geographic proximity. Thus, the functionality of these
existing toolkits is not necessarily conducive to analyzing similarities in time series
trends in geographically disparate areas, particularly in a multivariate context. It is
this times series analytical capability that differentiates IVIID from its antecedents
highlighted in Table 1.
IVIID is a visual analytics interface that allows for the interactive visualization of
the composition of regional economies through space and time. It is written in C??
and uses QT GUI widgets. Although it was developed in a Windows environment,
the tool is multiplatform and may be accessed at https://github.com/yifantastic/
IVIID. The IVIID interface is flexible and can accept all data placed in shapefile
format. Space–time data should be placed in distinct columns where each column
represents a variable at different points in time. Data can be for any time period of
interest; however, the user must identify the temporal variables when the data are
first imported into the interface via an interactive selection menu.
Figure 1 highlights the layout of IVIID. Users are presented with an interactive
choropleth map in the center, a scatterplot view in the upper right, and a histogram
view of the data in the lower right. Along the top of the interface are four menus:
file, setting, function, and view. The function menu is where the indices and location
quotients are calculated. This menu also contains three transformations for applying
an automatic power transformation (Maciejewski et al. 2013) as a means of creating
better separations in the choropleth map classification: power, log, and equal
interval.

Fig. 1 The interactive visualization tool for indices of industrial diversity

123

194

E. A. Mack et al.

These transformations help visualize data while accounting for the underlying
distribution, whether it is skewed or more normally distributed. Next to the
transformations in this same toolbar are three normalization options: individual,
data, and no normalization. The individual normalization option normalizes an
attribute value of interest by its minimum and maximum values over the whole time
series. The calculation of the normalized value is as follows:
Normalized ðVik Þ ¼

Vik  mint ðVit Þ
maxt ðVit Þ  mint ðVit Þ

ð1Þ

where i stands for the ith region, t stands for the time period which is from 1 to
m, k is the current time step, then Vik is the value of interest for region i at time
k that we want to normalize. The data normalization option normalizes an attribute
value of interest by the minimum and maximum values across the entire temporal
data range of that attribute. The calculation for this kind of normalization is as
follows:
Normalized ðVik Þ ¼

Vik  minj ðVjk Þ
maxj ðVjk Þ  minj ðVjk Þ

ð2Þ

where i and j stand for the ith and jth region, respectively, and k is the current time
step. Equation 1 normalizes each individual region by the maximum value a region
has over the entire time period. Equation 2 normalizes all regions in a given time
step by the maximum value found overall regions at time k.
The interface highlighted in Fig. 1 above is actually comprised of a series of
visualization widgets. Each of these widgets are designed so that the user may
interactively explore space–time composition data. Users may dock and undock
each widget as well as close and open different views in order to conserve screen
space and customize their data analysis experience. This section will describe the
key widgets of the system, and the remainder of the paper will demonstrate the
utility of these widgets.
4.1 Scatterplot and histogram widget
The scatterplot and histogram views contain linked brushing to highlight areal units
on the map. When points in the scatterplot are selected with a rectangular widget, or
when a histogram bin is selected, the corresponding areas on the map are
highlighted through a blurring filter (Robinson 2011). The histogram view
represents the distribution of counties for a user selected variable at a given point
in time. Users can interactively search for temporal changes within a particular
index variable by manipulating the vertical line or threshold widget in the histogram
view. This flexibility in the histogram recognizes that industrial diversity is both
dynamic and represented by a spectrum of index values. The interactivity of the
widget allows the user to specify the degree of specialization or diversity of interest
and analyze corresponding counties by adjusting the position of the vertical line.
This vertical line represents a threshold such that if a county’s selected index value
was to cross this threshold at any point in time, the border of the county would be

123

Spatio-temporal analysis of industrial composition with IVIID

195

assigned a color from a qualitative color scheme (Brewer 2005). Based on the
position of the line, counties are assigned to three classes: green (low to high),
purple (high to low), and orange (oscillation). The corresponding points (representing spatial areas) in the scatterplot are also colored in this manner, as well as the
border color of the corresponding areas. The choropleth map is also rendered such
that areas that did not cross the threshold of interest are blurred via a focus ?
highlight construct while the other areas remain in focus.
The green class indicates that at some point in time, the county’s industrial index
increased in specialization and crossed from below to above the value set by the
vertical line. The purple class indicates that at some point in time, the county’s
industrial index reflected a decrease in specialization and crossed from above the
threshold to below the set threshold value. Counties in the orange class did not
experience a consistent trend in their index values and oscillated around the index
value set by the threshold widget. The flexibility of the threshold widget allows the
user to search for spatial clusters or geographically distant counties with similar
index or location quotient values without requiring much a prior knowledge of the
user. In addition to this time series thresholding widget, the time series similarity
and logic widgets of IVIID provide more resolution on regions with similar time
series trajectories.
4.2 Similarity analysis widgets
In IVIID, similarity refers to the relative similarity (or dissimilarity) in time series
trends for a particular variable between different economic regions. For a given
region p, there is a time series p denoted as ðp1 ; p2 ; p3 ; . . .; pm Þ. For any other region
q in the dataset, there exists another time series q denoted as ðq1 ; q2 ; q3 ; . . .; qm Þ. In
order to determine whether other regions follow a similar temporal pattern as the
region of interest, IVIID utilizes the time series similarity metrics from the data
mining community listed below (Tan et al. 2006):
Manhattan distance
m
X
distMan ðp; qÞ ¼
ð3Þ
jqt  pt j;
t¼1

Euclidean distance
sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
m
X
distEuc ðp; qÞ ¼
ðqt  pt Þ2 ;

ð4Þ

t¼1

and Cosine distance
Pm
t¼1 qt pt
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
distCos ðp; qÞ ¼ pP
Pm 2ﬃ :
m
2
t¼1 qt
t¼1 pt

ð5Þ

Given that the range and magnitude of the time series may vary while the
underlying pattern remains the same, IVIID also uses two other metrics that are less
sensitive to magnitude differences within the data. The first is the Sequential

123

196

E. A. Mack et al.

Normalized Euclidean Distance, which locally normalizes each individual time
series for comparison. In other words, the time series of each region in the dataset is
normalized based on its time series mean before computing the distance metric. This
similarity metric is specified as follows:
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u m 
uX qt  lq pt  lp 2
distSNE ðp; qÞ ¼ t

:
ð6Þ
rq
rp
t¼1
here, l and r represent the mean and the standard derivation of the time series,
respectively. The second normalized similarity metric is the Mahalanobis distance,
which globally normalizes each regional time series according to the distribution of
all regional time series in the dataset:
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
distMah ðp; qÞ ¼ ðp  qÞT R1 ðp  qÞ:
ð7Þ
here, R is the covariance matrix of the time series group. It is calculated in the
program and not visible to users.
Combined, these metrics summarize the similarity of one region to all other
regions in the dataset in a single number. The larger the number, the less similar the
region is to all other regions. The smaller the number, the more similar the region is
to all other regions. These metrics are flexible enough that the user may specify the
other regions of interest. At present, IVIID uses all other observations in the dataset.
4.3 Logic widget
The logic widget functionality expands the univariate prospective provided by the
similarity analysis widget to a multivariate context. The similarity widget is limited
to time series correspondence for a particular variable, while the logic widget can be
used to locate regions with similarities or dissimilarities across multiple variables.
This approach is a more direct alternative to the use of animated choropleth maps to
search for patterns and spatial trends.
For example, the logic widget is capable of identifying regions with similar levels
of specialization and similar time series trends in their overall level of diversity, as
specified by a particular index. Users could also leverage the logic widget to identify
regions that have manufacturing trends similar to those in a location of interest. This
would be particularly useful for identifying key industry players in lesser known
regions. For example, what are regions that have a similar specialization in semiconductor manufacturing as Santa Clara County California (where Silicon Valley is
located) AND a similar time series trend in their level of specialization?
In order to find similar economic regions, the user interactively chooses a region
of interest on the map. From there, the user can access the similarity logic widget.
Once the temporal variables and similarity metric are chosen, we define the
similarity between the time series to be (Tan et al. 2006):

123

Spatio-temporal analysis of industrial composition with IVIID

s¼

1
1 þ dist

197

ð8Þ

or
s¼1

dist  distmin
distmax  distmin

ð9Þ

depending on the chosen distance metric. This value will range from 0 to 1 with 1
indicating a high degree of similarity and 0 indicating a low degree of similarity.
Given the large number of variables and spatial locations, the similarity logic
widget allows for the creation of logic trees. In this manner, the user can refine
queries based on a variety of operations (greater than, less than, AND, OR). The logic
tree also allows users to dynamically specify a series of filter rules. For example, a
user could look for regions with similar temporal trends in their Herfindahl Index at a
specified similarity level greater than 0.5 AND regions that have dissimilar temporal
trends in their manufacturing index at a similarity value less than 0.2.
As an example, let St(i) denote the similarity measurements from region i with
respect to temporal variable t. Given the dynamic filters, one could compare
Herfindahl Index similarity scores larger than 0.4 with Ogive Index similarity scores
larger than 0.6, and National Average Index similarity scores greater than or equal
to 0.7 or Herfindahl Index scores greater than or equal to 0.6 and Ogive Index
similarity scores less than 0.8. Such a query would result in the following logic tree:

Each branch of the tree can be interactively visualized through choropleth map
highlighting so that users can quickly explore different combinations. From there,
they can animate the map which has now applied the previously mentioned focus ?
context blurring modality to the choropleth maps so that attention can directly be
focused on the filtered areas, which are known to have similar temporal trends.
Also, each tree is directly associated with a single areal unit in the map and is stored
in memory so that if users select a different region of interest and then return to the
original region of interest, the previously created logic tree is still associated with
that original region until deleted by the user.
5 Data
While the tools and techniques presented in this paper are applicable to any
geographically referenced spatiotemporal dataset, this particular application of

123

198
Table 2 NAICS Industries

E. A. Mack et al.

2-digit NAICS

Industry description (%)

11

Agriculture, forestry, fishing, and hunting

21

Mining

22

Utilities

23

Construction

31

Manufacturing

42

Wholesale trade

44

Retail trade

48

Transportation and warehousing

51

Information

52

Finance and insurance

53

Real estate and rental and leasing

54

Professional, scientific, and technical services

55

Management of companies and enterprises

56

Administrative waste services

61

Educational services

62

Health care and social assistance

71

Arts, entertainment, and recreation

72

Accommodation and food services

81

Other services (except public administration)

92

Public administration

IVIID is specific to questions related to regional specialization and industrial
diversity. Data were obtained from the US Census Bureau’s County Business
Patterns (U.S. Department of Commerce 2012) database, which contains detailed
industrial information about establishments for all US counties dating back to 1986.
For this particular study, the years 1998–2009 are examined to restrict the dataset to
years for which data are reported by NAICS industries. There are 19 two-digit
NAICS industries in this study, which are specified in Table 2 and 3,106 counties in
the continental USA for which data are available.
5.1 Measures of industrial diversity
Specialization of regions is indicated via the popular location quotient method
where values less than one represent no specialization and values greater than one
represent specialization in an industry. Three indices of industrial diversity are
also highlighted in this application of IVIID: the Ogive Index, the Herfindahl
Index, and the National Averages Index. The specification of these three indices is
as follows:
h 
iq
esit
1
sit
X
eit  sit
Oit ¼
ð10Þ
1
s¼1

123

sit

Spatio-temporal analysis of industrial composition with IVIID

Hit ¼


sit 
X
esit 2
s¼1

Nit ¼

sit
X

h 

s¼1

esit
eit

eit

est
et

199

ð11Þ

 iq
est
et

ð12Þ

where sit = total number of industries in the ith region at time t, et = total activity
within the reference economy (usually the nation) at time t, eit = total activity in the
ith region at time t, est = activity in industry s for the reference economy at time t,
esit = activity in the ith region in industry s at time t, q = a scalar value (either one or
two).
As mentioned previously, these indices are equiproportional measures of
industrial concentration (Siegel et al. 1995a, b; Wagner 2000), where maximum
diversity is achieved when all industries contain equal amounts of activity (Conroy
1975; Jackson 1984; Wagner 2000). Low index values for these indices are an
indication of diversity and high index values are an indication of specialization
(Dissart 2003; Jackson 1984; Siegel et al. 1995a, b).
Although location quotients and these indices of industrial diversity are used
widely, they are not without their shortcomings. A key drawback of these metrics is
their relative definition of specialization or diversity. For the three indices of
industrial diversity described above, as well as others not discussed in this paper,
there is no distinct threshold above or below which a county is considered either
specialized or diversified. The only thing that determines the diversity or
specialization of a locale is the index values of other counties.
A second shortcoming of these indices is their inability to highlight the industries
in which a spatial unit is specialized. The indices merely indicate the relative
diversity or specialization of a place but obscure the specifics of its industrial
specialization. The use of location quotients in combination with these indices helps
overcome their lack of specificity about specialization. However, here too, the
threshold value of 1 for specialization is problematic because the range for location
quotients, and thus, the upper bound on these metrics is dependent on the dataset of
interest. In some cases for example, it is not uncommon to see location quotient
values as high as 8 or 13 for industries in particular areas.
Two alternatives to the three equiproportional indices described above are the
Krugman Similarity Index and the CS-Index. Krugman’s Similarity Index (KSI)
(Bernat and Repice 2000) is specified as follows:
"
!#
n
X
SIs ¼ 1 
jSi;s  Si;n j
 100
ð13Þ
i¼1

where Si,s is industry i’s share in region s, Si,n is the nation’s share of activity in
industry i and n is the number of industries. The CS-Index (Mack et al. 2007b) is
specified as follows:

123

200

E. A. Mack et al.

CSi ¼

n 
X
j¼1

x
Pn ij


ð14Þ

i¼1 xij

where xij stands for the value of variable j in region i. For the purpose of legibility
and consistency, we formalize the above two formulas using the same notations as
for the other indices:
"
!#
sit
X
esit est
SIit ¼ 1 
j
 j
 100
ð15Þ
eit
et
s¼1

CSit ¼

sit 
X
s¼1

esit

Pn

i¼1 esit


ð16Þ

where sit is the number of industries for region i at time t; eesitit is the share of region i’s
activity in industry s at time t and eestt is the nation’s share of activity in industry s at
time t.
The lower bound of the KSI is 0, and the upper bound is 100 with larger values,
indicating that the industrial composition of a region is similar to that of the
reference region. As for other indices, the reference region is user defined, and in
this case, it is the nation. An index value of 85, for example, indicates that the
compositional profile of a region is 85 % similar to the compositional profile of the
nation. A key advantage of this index is that the values are more intuitive than those
of other indices. A disadvantage of this index, which is similar to the disadvantages
associated with the equiproportional measures described above, is its spatial
inaccuracy when mapped, particularly in multivariate extensions (Mack et al.
2007b).
In this regard, the CS-Index has an advantage over the other indices included in
this paper. When mapped, particularly in a multivariate context, it produces a
spatially accurate picture of economic activity (Mack et al. 2007b). The range of
this index is 0 to 1. A value of zero means a region contains no firms in any of the
industries of interest and a value of 1 means the region contains all of the
establishments in a particular industry of interest. Although it is highly unlikely that
a region would have 100 % of a variable of interest, this method of computing the
index provides several advantages over more traditional indices. First, it resolves the
division by zero issue in the equiproportional measures (Mack et al. 2007b).
Second, it is flexible with respect to the number of variables that may be included in
the index and weighting schemes that may be used or not used (Mack et al. 2007b).
This is particularly important if one wanted to emphasize growing sectors of the
economy more than others. Figure 2 presents a visual summary of the five different
indices calculated in IVIID. A unifying feature of all of these indices, regardless of
their specification, is the amount of time required to analyze the index results over
space and through time. In this respect, the interactivity of IVIID described above
saves computational time and effort and allows the user to identify trends in
industrial data in minutes rather than hours or days. Three features of this interface

123

Spatio-temporal analysis of industrial composition with IVIID

201

Fig. 2 A demonstration of five different indices in the same year of 2007

provide evaluations of industrial diversity that are not possible via static spreadsheet
and geographic information system tools. First, the information gleaned via the
threshold widget substitutes for explorations of varied cutoff values for location
quotients via static choropleth map comparisons, where each map is produced with
a different threshold value. Second, the dynamic linking of the scatterplot and map
to the threshold widget allows the user to quickly determine which counties moved
above and below a given threshold value in seconds in place of more timeconsuming iterative tables and counts of county movements. The third feature of
IVIID that allows for unique evaluations of specialization is the logic operator,
which quickly identifies similar trajectories in industrial specialization for a
specified county of interest. In the results section of this piece, the value added of
these features will be demonstrated specifically.

6 Results
Figure 3 illustrates the traditional way of analyzing industrial diversity via small
multiples, one for each year in the study period. It also illustrates some of the
drawbacks associated with this comparative static method. While general patterns
are evident along the coasts, it is difficult to see any changes in the Southeast and
the Midwest, particularly given the use of county data in this example. In
comparison with this traditional approach, IVIID dynamically generates these maps
and allows the users to zoom and scroll through the maps to look for subtle changes
in areas of interest.
Another advantage of IVIID is that it allows one to explore the impact of
different threshold values for these indices where no true benchmark value for

123

202

E. A. Mack et al.

Fig. 3 Small multiples of choropleth maps and scatterplots showing manufacturing trends overtime

Fig. 4 Illustration of variations in analytical results for the Herfindahl Index with the threshold widget

diversity or specialization exists, other than the minimum and maximum values.
Figure 4 displays two maps with different threshold values for the Herfindahl Index.
The histogram shows the number of counties above and below the value set by the
threshold widget, as well as the movement of county index values above and below
this benchmark. The panel on the left shows a lower threshold value for the
Herfindahl Index and highlights more counties with changes in their index value
than the panel on the right, which places the threshold value at a higher level and
highlights fewer counties with changes in their index values. Aside from these
general benefits to the use of IVIID, more complex analyses can be performed to
uncover important trends in critical industries and to identify places with similar
compositional profiles. The next two examples will demonstrate IVIID applications
in each of these contexts: one example for the manufacturing industry and one
example that looks for places with similar compositional profiles
The scatterplot feature of IVIID can be used to look at the link between diversity
via the Herfindahl Index and specialization in the manufacturing sector, which is
considered a vital industry to continued economic growth in the US. However, this
industry also faces an incredible amount of international competition from low-cost
producers abroad. Given this duality associated with the industry, manufacturing
growth trends have received a considerable amount of research attention. Figure 3
shows a series of scatterplots for different years with the Herfindahl index on the
x-axis and the location quotient for manufacturing on the y-axis. If there is a strong
link between the level of diversity and specialization in the manufacturing sector,

123

Spatio-temporal analysis of industrial composition with IVIID

203

either negative or positive, then the scatterplots are expected to form a 45 degree
line. The majority of counties form a vertical line. This indicates that places that are
more specialized are not necessarily more specialized in the manufacturing sector.
There are a few counties in the 1998 scatterplot that are very diverse and also
display a high level of specialization in the manufacturing sector. By interactively
selecting these counties and looking at the corresponding counties highlighted on
the map, one can see these places are clustered primarily in Indiana, Ohio, Missouri,
Mississippi, Alabama, and Georgia. Other outliers in the 1998 scatterplot include
counties with no specialization in the manufacturing sector and a low level of
diversity. These counties are located in the middle of the country in states such as
North Dakota, South Dakota, Nebraska, Kansas, Oklahoma, and Texas.
The information sector (NAICS 51), which is comprised of several subindustries
including software publishers, telecommunications, and Internet service providers,
has seen a surge in growth over the last fifteen years that is related to growth in
information and communications technologies (ICTs). This New Economy industry
was especially important in the dot.com bubble in the late 1990s related to the
popularity of web browsers and e-commerce. Unfortunately, very little is known
about trends in this sector outside of Silicon Valley, which is recognized as a bastion
of New Economy growth. Therefore, it is important to uncover and analyze trends in
this sector beyond the boundaries of Santa Clara County (where Silicon Valley is
located).
Figure 5 shows the results of an analysis of NAICS sector 51 using IVIID. First,
the histogram widget is used to evaluate spatial trends in this sector in the initial
year of the dataset (1998), where the data are displayed with no power
transformation. The histogram shows the widget placed at a location quotient
value of 3.2 and the map shows places with corresponding location quotient values
using the focus ? context visualization. Several counties are outlined in orange,
indicating their location quotient values oscillated around this threshold value of
3.2. Just six counties are outlined in purple and green, which correspond to a
decrease and increase in location quotient values from this threshold at some point

Fig. 5 A pipeline view of user interactions with IVIID. In the left image, the user moves the threshold
widget line and observes the pattern in the choropleth map. In the right image, the user hovers over a
county whose location quotient value has increased overtime to observe the time series plot of its location
quotient value for industry 51: information

123

204

E. A. Mack et al.

in the time series. If the location quotient value is also specified as a temporal
attribute, it is possible to hover the mouse over a county to see its time series
trajectory.
Finally, the user chooses to focus on Costilla County Colorado, which is outlined
in green on the map indicating its location quotient value increased beyond the
threshold value of 3.2 between 1998 and 2009. The corresponding time series plot
for this county (shown in Fig. 5) highlights a dip in the location quotient value
between 1998 and 2001, and then a steady increase in sector specialization from
2001 onward. This identification of such a trend in Costilla County is interesting,
given its small size. According to the U.S Census Bureau, this county has only 3,
556 inhabitants (U.S. Census Bureau 2012). An examination of the Ogive Index for
this same county reveals rising specialization over a large portion of the same time
period (2000–2006). These two trends suggest that the increase in information
specialization might have made the industrial base of Costilla County less diverse
than it had been previously.
If one wanted to find other counties with similar trends in their information
specialization and industrial diversity as Costilla County, the similarity logic widget
may be used. Figure 6—left shows the use of the similarity logic widget. To
evaluate similar location quotient trends, the user specifies LQ51 as the current and
comparing variable. Next, the user specifies sequence normalized in the metrics
drop down menu and greater than 75 % similarity on a sliding scale of 0–100 where
100 represents complete similarity. The results of this query are highlighted in
green.
The user could take this analysis one step further and also look for trends in
county Ogive indices given the apparent link between specialization in the
information sector and county industrial composition in Costilla County. Figure 6—
right shows this portion of the analysis. The Ogive index is selected as the current
and comparing variable. Sequence normalized is specified in the drop down menu to
look for counties that are 50 % similar to Costilla County with respect to their

Fig. 6 Logic widget combined with similarity comparing and dynamic filtering. Left The user selects
Costillo County and interactively queries for all counties have a time series trend in Index 51 similar to
Costillo County. Right The user looks for counties that have a time series trend in Index 51 and in the
OGive Index similar to Costillo County.)

123

Spatio-temporal analysis of industrial composition with IVIID

205

Ogive index trends. By clicking AND, the user can combine the queries to look for
counties that have similar location quotient trends AND similar Ogive index trends.
The result of this logical operand is also shown in Fig. 6—right.

7 Discussion and conclusion
The preceding discussion demonstrated the application of a recently developed
toolkit called IVIID. This toolkit is preceded by several other packages developed to
facilitate the exploration and analysis of data. These toolkits are wide ranging in
their analytical capabilities. Some possess spatial and/or space–time functionality,
while others are limited to nonspatial techniques such as k-means clustering and
principal components analysis.
This particular application of IVIID highlighted the combination of features
within this interface that are helpful in understanding the dynamic and not
necessarily spatial trends in industrial composition. The discussion of Costilla
County above highlights how the tool may be used to identify and analyze trends in
smaller, less frequently discussed regions. These locales tend to be more specialized
than larger areas and are subsequently more vulnerable to business cycles. This
example also highlighted the utility of the threshold and logic widgets to identify
trends in industries of interest.
Prior to the development of IVIID, the highlighted analysis would have take
much longer via small multiples and tables. While programs such as STARS have
some dynamic functionality in their spatial traveling and time roaming, the query
functionality of the logic widget allows for a more targeted investigation of
particular regions. It also permits one to specify the degree of similarity or
dissimilarity between regions, which is not possible in other toolkits currently
available for researchers. Further, the threshold widget allows one to dynamically
change the level of specialization and the level of diversity, which is key for
investigating relative concepts such as diversity where a set value is not indicative
of either diversity or specialization.
Of course, there are several extensions to the core visual analytics functionality
of IVIID. These extensions include summary measures of concentration such as the
Gini Index and the Ellison and Glaeser (1997) model-based measure of geographic
concentration. Other extensions of IVIID could expand on the largely temporal
analytical toolkits that identify similar trends in spatial units overtime. These
expansions could focus on adding statistical measures that further highlight the
spatio-temporal behavior of diversity and concentration, and the distribution of
these indices. In this regard, autocorrelation measures could also be added to the
interface to examine spatial trends through time or spatio-temporal autocorrelation
trends simultaneously. Right now, indices computed in IVIID must be analyzed in
software packages external to the interface. An example of these statistical measures
that may be incorporated are the spatio-temporal autocorrelation measures
developed by López et al. (2011).
Given this range of extensions, it is important to note that IVIID represents a step
in the right direction of developing more comprehensive spatio-temporal toolkits,

123

206

E. A. Mack et al.

with a particular focus on compositional questions. As noted above, the range of
extensions to IVIID reflects the need for more research at the intersection of spatiotemporal methods and industrial composition. Global integration has heightened
foreign competition and the volatility of growth dynamics, and the integration of
these concepts and relevant toolkits is increasingly important for helping researchers
and policymakers understand regional trends in industrial composition.

References
Alwang J, Siegel PB (1994) Portfolio models and planning for export diversification: Malawi, Tanzania,
and Zimbabwe. J Dev Stud 30(2):405–422
Anselin L (1995) Local indicators of spatial association–LISA. Geogr Anal 27(2):93–115
Anselin L (2003a) An introduction to spatial regression analysis in R. University of Illinois, UrbanaChampaign
Anselin L (2003b) An introduction to EDA with GeoDa. Spatial Analysis Laboratory, Department of
Agricultural and Consumer Economics, UIUC
Anselin L (2012) Space-time mapping. Spatial Autocorrelation, Chicago
Anselin L, Bao S (1997) Exploratory spatial data analysis linking SpaceStat and ArcView. Recent
developments in spatial analysis—spatial statistics, behavioural modelling and computational
intelligence pp 35–59
Anselin L, Dodson R, Hudak S (1993) Linking GIS and spatial data analysis in practice. Geogr Syst
1:3–23
Anselin L, Syabri I, Kho Y (2006) GeoDa: an introduction to spatial data analysis. Geogr Anal
38(1):5–22
Attaran M, Zwick M (1987) Entropy and other measures of industrial diversification. Q J Bus Econ
26(4):17–34
Bernat GA Jr, Repice ES (2000) Industrial composition of state earnings in 1958–1998. Surv Curr Bus
80(2):70
Bivand R (2002) Spatial econometrics functions in R: classes and methods. J Geogr Syst 4:405–421.
doi:10.1007/s101090300096
Bivand R, Gebhardt A (2000) Implementing functions for spatial statistical analysis using the R language.
J Geogr Syst 2:307–317
Block R, Block C (1995) Space, place and crime: hot spot areas and hot places of liquor-related crime.
Crime Place 4(2):145–184
Bode E, Rey SJ (2006) The spatial dimension of economic growth and convergence. J Reg Sci
85(2):171–176
Brewer CA (2005) Designing better maps: a guide for GIS users. ESRI Press, California
Conroy M (1975) The concept and measurement of regional industrial diversification. South Econ J
pp 492–505
Dissart J (2003) Regional economic diversity and regional economic stability: research results and
agenda. Int Reg Sci Rev 26:423–446
Drucker J (2011) Regional industrial structure concentration in the United States: trends and implications.
Econ Geogr 87(4):421–452
Drucker J, Feser E (2012) Regional industrial structure and agglomeration economies: an analysis of
productivity in three manufacturing industries. Reg Sci Urban Econ 42(1-2):1–14
Duque J, Dev B, Betancourt A, Franco J (2011) ClusterPy: library of spatially constrained clustering
algorithms. Version 0.9.9. RiSE-group (Research in Spatial Economics). EAFIT University.
Duranton G, Overman H (2008) Exploring the detailed location patterns of UK manufacturing industries
using microgeographic data. J Reg Sci 48(1):213–243
Dykes J (1995) Pushing maps past their established limits: a unified approach to cartographic
visualization. Innov GIS 3:177–87
Dykes J (1998) Cartographic visualization: exploratory spatial data analysis with local indicators of
spatial association using TcI/Tk and cdv. J Roy Stat Soc Ser D (The Statistician) 47(3):485–497

123

Spatio-temporal analysis of industrial composition with IVIID

207

Ellison G, Glaeser EL (1997) Geographic concentration in U.S. manufacturing industries: a Dartboard
approach. J Polit Econ 105(5):889–927
Ellison G, Glaeser EL, Kerr W (2010) What causes industry agglomeration? Evidence from coagglomeration patterns. Am Econ Rev 100:1195–1213
Everett B (1974) Cluster analysis. Heinemann Educational Books Ltd, London
Gahegan M, Takatsuka M, Wheeler M, Hardisty F (2000) Abstract GeoVISTA Studio: a geocomputational workbench. In: The Proceedings of the 5th international conference on GeoComputation
Geary R (1954) The contiguity ratio and statistical mapping. Inc Stat 5(3):115–146
Getis A, Ord JK (1992) The analysis of spatial association by use of distance statistics. Geogr Anal
24(3):189–206
Gomez-Rubio V, Ferrandiz J, Lopez A (2003) Detecting clusters of diseases with R. J Geogr Syst
7:189–206
Guo D, Chen J, MacEachren AM, Liao K (2006) A visualization system for space-time and multivariate
patterns (VIS-STAMP). IEEE Trans Vis Comput Graph 12(6):1461–1474
Hägerstrand T (1970) What about people in regional science?. Papers Reg Sci 24(1):6–21
Haining R, Wise S, Ma J (2000) Designing and implementing software for spatial statistical analysis in a
GIS environment. J Geogr Syst 2:257–286
Harvey J (1991) Modelling accessibility using space-time prism concepts within geographical
information systems. Int J Geogr Inf Syst 5(3):287–301
Haslett J, Wills G, Unwin A (1990) SPIDER–interactive statistical tool for the analysis of spatially
distributed data. Int J Geogr Inf Syst 4(3):285–296
Isard W, Azis IJ, Drennan MP, Miller RE, Saltzman S, Thorbecke E (1998) Methods of interregional and
regional analysis. Ashgate, Aldershot
Jackson RW (1984) An evaluation of alternative measures of regional industrial diversification. Reg Stud
18(2):103–112
Janikas MV, Rey SJ (2005) Spatial clustering, inequality and income convergence. Reg Dev 21:45–64
Knox E, Bartlett M (1964) The detection of space-time interactions. J Roy Stat Soc Ser C (Appl Stat)
13(1):25–30
Kulldorff M (1997) A spatial scan statistic. Commun Stat Theory Methods 26(6):1481–1496
Kulldorff M, Rand K, Gherman G, Williams G, DeFrancesco D (1998) SaTScan v 2.1: software for the
spatial and space-time scan statistics. National Cancer Institute, Bethesda
Kulldorff M, Heffernan R, Hartman J, Assunção R, Mostashari F (2005) A space–time permutation scan
statistic for disease outbreak detection. PLoS Med 2(3):e59
Kwan M (1998) Space-time and integral measures of individual accessibility: a comparative analysis
using a point-based framework. Geogr Anal 30(3):191–216
Kwan M (1999) Gender and individual access to urban opportunities: a study using space–time measures.
Prof Geogr 51(2):211–227
LeSage J, Pace R (2004) Arc_mat, a toolbox for using arcview shape files for spatial econometrics and
statistics. In: Egenhofer M, Freksa C, Miller H (eds) Geographic information science, lecture notes
in computer science, vol 3234. Springer, Berlin pp 179–190. doi:10.1007/978-3-540-30231-5_12
Levine N (1999) CrimeStat: a spatial statistics program for the analysis of crime incident locations. In:
The IV international conference on GeoComputation, Fredericksburg
Levine N (2006) Crime mapping and the Crimestat program. Geogr Anal 38(1):41–56
Levine N (2010) CrimeStat: a spatial statistics program for the analysis of crime incident locations(V
3.3). National Institute of Justice, Washington
Liu X, LeSage J (2010) Arc_mat: a matlab-based spatial data analysis toolbox. J Geogr Syst 12(1):69–87
López FA, Matilla-Garcı́a M, Mur J, Marı́n MR (2011) Four tests of independence in spatiotemporal data.
J Reg Sci 90(3):663–685
MacEachren A, Wachowicz M, Edsall R, Haug D, Masters R (1999) Constructing knowledge from
multivariate spatiotemporal data: integrating geographical visualization with knowledge discovery
in database methods. Int J Geogr Inf Sci 13(4):311–334
Maciejewski R, Pattah A, Ko S, Hafen R, Cleveland WS, Ebert DS (2013) Automated box-cox
transformations for improved visual encoding. IEEE Trans Vis Comput Graph 19(1):130–140
Mack E, Grubesic TH, Kessler E (2007a) Indices of industrial diversity and regional economic
composition. Growth Change 38(3):474–509
Mack E, Grubesic TH, Kessler E (2007b) Indices of industrial diversity and regional economic
composition. Growth Change 38:474–509

123

208

E. A. Mack et al.

Mantel N (1967) The detection of disease clustering and a generalized regression approach. Cancer Res
27(2 Part 1):209–220
Marcon E, Puech F (2010) Measures of the geographic concentration of industries: improving distancebased methods. J Econ Geogr 10(5):745–762
Markowitz H (1959) Portfolio selection: efficient diversification of investments. Wiley, New York
McBratney A, Gruijter J (2006) A continuum approach to soil classification by modified fuzzy k-means
with extragrades. J Soil Sci 43(1):159–175
McIntosh J, Yuan M (2005) A framework to enhance semantic flexibility for analysis of distributed
phenomena. Int J Geogr Inf Sci 19(10):999–1018
Moran P (1948) The interpretation of statistical maps. J Roy Stat Soc Ser B (Methodol) 10(2):243–251
Openshaw S (1994) Two exploratory space-time-attribute pattern analysers relevant to GIS. Spatial Anal
GIS pp 83–104
Peuquet D (1994) It’s about time: A conceptual framework for the representation of temporal dynamics in
geographic information systems. Ann Assoc Am Geogr 84(3):441–461
Peuquet D (2001) Making space for time: issues in space-time data representation. GeoInformatica
5(1):11–32
Peuquet D, Duan N (1995) An event-based spatiotemporal data model (ESTDM) for temporal analysis of
geographical data. Int J Geogr Inf Syst 9(1):7–24
Rey S (1998) Technician needs assessment survey. Technical representative, San Diego Regional
Economic Development Corporation
Rey S (2000) Identifying regional industrial clusters in the california economy: volume I conceptual
design. Technical representative California Employment Development Department, Sacramento
Rey SJ, Janikas MV (2006) STARS: space–time analysis of regional systems. Geogr Anal 38(1):67–86
Robertson C, Nelson T, Boots B, Wulder M (2007) STAMP: spatial–temporal analysis of moving
polygons. J Geogr Syst 9(3):207–227
Robinson AC (2011) Highlighting in geovisualization. Cartogr Geogr Inf Scis 38(4):373–383
Rundensteiner EA, Ward MO, Xie Z, Cui Q, Wad CV, Yang D, Huang S (2007) XmdvtoolQ: : qualityaware interactive data exploration. In: SIGMOD conference, pp 1109–1112
Sadahiro Y, Umemura M (2001) A computational approach for the analysis of changes in polygon
distributions. J Geogr Syst 3(2):137–154
Sharpe WF (1970) Portfolio theory and capital markets. McGraw-Hill, New York
Siegel PB, Johnson TG, Alwang J (1993) Diversification of production agriculture across individual
states: comment. J Prod Agric 6(3):445–446
Siegel PB, Alwang J, Johnson TG (1995a) A structural decomposition of regional economic instability: a
conceptual framework*. J Reg Sci 35(3):457–470
Siegel PB, Johnson TG, Alwang J (1995b) Regional economic diversity and diversification. Growth
Change 26(2):261–284
Swayne DF, Cook D, Buja A (1991) Xgobi: interactive dynamic graphics. In: The X window system with
a link To S
Symanzik J, Majure J, Cook D (1996) Dynamic graphics in a GIS: a bidirectional link between ArcView
2.0 and XGobi. Comput Sci Stat 27:299–303
Symanzik J, Kötter T, Schmelzer S, Klinke S, Cook D, Swayne DF (1997) Spatial data analysis in the
dynamically linked ArcView/XGobi/XploRe environment. Comput Sci Stat 29:561–569
Symanzik J, Cook D, Lewin-Koh N, Majure JJ, Megretskaia I (2000) Linking ArcViewTM and XGobi:
insight behind the front end. J Comput Graph Stat 9(3):470–490
Takasuka M, Gahegan M (2001) Sharing exploratory geospatial analysis and decision making using
GeoVISTA studio: from a desktop to the web. J Geogr Inf Decis Anal 5:129–139
Takatsuka M, Gahegan M (2002) GeoVISTA studio: a codeless visual programming environment for
geoscientific data analysis and visualization. Comput Geosci 28:1131–1144
Tan PN, Steinbach M, Kumar V (2006) Introduction to data mining. Addison-Wesley, Reading
Unwin A, Wills G, Haslett J (1990) REGARD—graphical analysis of regional data. In: Proceedings of the
section on statistical graphics, American Statistical Association, Alexandria, pp 36–41
US Census Bureau (2012) ACS demographic and housing estimates 2007–2011. American Community
Survey 5-year Estimates. http://factfinder2.census.gov/faces/tableservices/jsf/pages/productview.
xhtml?pid=ACS_11_5YR_DP05&prodType=table
US Department of Commerce (2012) County Business Patterns (CBP). http://www.census.gov/econ/cbp/
Wagner J (2000) Regional economic diversity: action, concept, or state of confusion. J Reg Anal Policy
30:1–22

123

Spatio-temporal analysis of industrial composition with IVIID

209

Wagner JE, Deller S (1998) Measuring the effects of economic diversity on growth and stability. Land
Econ 74(4):541–556
Ward MO (1994) XmdvTool: integrating multiple methods for visualizing multivariate data. In:
Proceedings of the conference on visualization ’94, IEEE Computer Society Press, Los Alamitos,
pp 326–333
Weaver C (2004) Building highly-coordinated visualizations in improvise. In: Proceedings of the IEEE
symposium on information visualization, IEEE Computer Society, Washington, pp 159–166
Weaver CE (2006) Improvise: a user interface for interactive construction of highly-coordinated
visualizations. PhD thesis, University of Wisconsin at Madison, Madison
Wills G, Unwin AR, Haslett J (1991) Spatial interactive graphics applied to Irish socioeconomic data. In:
Proceedings of the ASA statistical graphics section, American Statistical Association, Atlanta,
pp 37–41
Wise S, Haining R, Ma J (1997) Regionalization tools for exploratory spatial analysis of health data.
Recent developments in spatial analysis: spatial statistics, behavioural modelling, and computational
intelligence pp 83–100
Wise S, Haining R, Ma J (2001) Providing spatial statistical data analysis functionality for the GIS user:
the SAGE project. Int J Geogr Inf Sci 15(3):239–254
Wundt B, Martin L (1993) Minimizing employment instability: a model of industrial expansion with
input-output considerations. Reg Sci Perspect 23(1):81–93
Ye X, Carroll MC (2011) Exploratory space-time analysis of local economic development. Appl Geogr
31(3):1049–1058
Ye X, Rey S (2011) A framework for exploratory space-time analysis of economic data. Ann Reg Sci
pp 1–25

123

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 15, NO. 6, NOVEMBER/DECEMBER 2009

1473

Structuring Feature Space: A Non-Parametric Method for
Volumetric Transfer Function Generation
Ross Maciejewski, Member, IEEE , Insoo Woo, Wei Chen, Member, IEEE, and David S. Ebert, Fellow, IEEE

Fig. 1. An application of non-parametric clustering to the value vs. value gradient magnitude feature space using the CT visible
woman feet dataset.
Abstract—The use of multi-dimensional transfer functions for direct volume rendering has been shown to be an effective means
of extracting materials and their boundaries for both scalar and multivariate data. The most common multi-dimensional transfer
function consists of a two-dimensional (2D) histogram with axes representing a subset of the feature space (e.g., value vs. value
gradient magnitude), with each entry in the 2D histogram being the number of voxels at a given feature space pair. Users then
assign color and opacity to the voxel distributions within the given feature space through the use of interactive widgets (e.g., box,
circular, triangular selection). Unfortunately, such tools lead users through a trial-and-error approach as they assess which data
values within the feature space map to a given area of interest within the volumetric space. In this work, we propose the addition of
non-parametric clustering within the transfer function feature space in order to extract patterns and guide transfer function generation.
We apply a non-parametric kernel density estimation to group voxels of similar features within the 2D histogram. These groups are
then binned and colored based on their estimated density, and the user may interactively grow and shrink the binned regions to
explore feature boundaries and extract regions of interest. We also extend this scheme to temporal volumetric data in which time
steps of 2D histograms are composited into a histogram volume. A three-dimensional (3D) density estimation is then applied, and
users can explore regions within the feature space across time without adjusting the transfer function at each time step. Our work
enables users to effectively explore the structures found within a feature space of the volume and provide a context in which the user
can understand how these structures relate to their volumetric data. We provide tools for enhanced exploration and manipulation of
the transfer function, and we show that the initial transfer function generation serves as a reasonable base for volumetric rendering,
reducing the trial-and-error overhead typically found in transfer function design.
Index Terms—Volume rendering, kernel density estimation, transfer function design, temporal volume rendering.

1

I NTRODUCTION

A common method for direct volume rendering is to employ the use of
interactive transfer functions as a means of assigning color and opacity to the voxel data. One of the most popular transfer function design
tools is the interactive 2D histogram widget introduced by Kniss et al.
[10]. In this widget, the user is presented with a 2D histogram (the
axes of which represent a feature space of the data) and various selec• Ross Maciejewski, Insoo Woo and David S. Ebert are with the Purdue
University Rendering and Perceptualization Laboaratoy, E-mail:
{rmacieje|iwoo|ebertd}@purdue.edu respectively.
• Wei Chen is with the State Key Lab of CAD&CG, Zhejiang University
E-mail: chenwei@cad.zju.edu.cn.
Manuscript received 31 March 2009; accepted 27 July 2009; posted online
11 October 2009; mailed on 5 October 2009.
For information on obtaining reprints of this article, please send
email to: tvcg@computer.org .
1077-2626/09/$25.00 © 2009 IEEE

tion tools are used to assign color and opacity to the voxels through
an interactive brushing of the feature space. However, the appropriate
selection of features in multi-dimensional transfer functions is a difﬁcult task, often requiring the user to have an underlying knowledge
of the data set under exploration. Moreover, normal transfer function
design widgets typically include rectangular bounding boxes or other
linearly deﬁned area speciﬁcation tools for transfer function deﬁnition. Unfortunately, features located in the multi-dimensional space
may have complex shapes requiring extensive interaction to extract local boundaries. Also problematic is the fact that within the volume
feature space, the features of interest are often not clearly visible as
peaks or valleys. Furthermore, in temporal volumetric data sets, the
feature space is changing across time, meaning that the structures captured with a transfer function in one time step may not be captured by
the same transfer function in the subsequent time step.
In this work, we propose enhancing the conventional 2D histogram
transfer function editing scheme through the application of a nonparametric density estimation in order to visualize and extract arbiPublished by the IEEE Computer Society

1474

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 15, NO. 6, NOVEMBER/DECEMBER 2009

trarily sized and shaped clusters within the given feature space. We
apply a nearest neighbor kernel density estimation technique, utilizing the Epanechnikov kernel [21], to the feature space histogram. The
estimated density is then used to assign a color and/or opacity to the
data by placing each estimated density value into a 1D histogram bin.
Each bin corresponds to a given color, and the opacity can then be
assigned across the density values. As users manipulate the density
binning, regions within the feature space expand and contract, providing contextual information about the structures hidden within the
feature space. Users can then interactively select arbitrarily shaped regions within the data feature space and those selected regions form the
transfer function used for volume rendering. In this manner, users are
able to easily extract regions found within the given feature space that
correspond to their regions of interest within the volume. A sample
result of our method is shown in Figure 1.
We further extend this scheme to temporal volumetric datasets. In
this case, we extract the corresponding feature space histogram of each
volumetric dataset from each time step, and create a volume of the feature space, with one axis of the volume representing time and the other
two axes representing the feature space dimensions. Kernel density estimation is then applied within this 3D space. At each time step, the
user is presented with only the single slice of the resultant histogram
volume that corresponds to this time step; however, from the density
estimation, temporal coherence is now embedded within this slice. A
second volume rendering of the histogram volume is also presented to
the user, and as a user selects a feature within the 2D histogram slice,
the temporal structure across the feature space volume is revealed. As
such, the selection within a slice of the feature space volume propagates a transfer function across the entire temporal feature space, creating a 3D transfer function that will maintain temporal coherence. The
user may now step through time and watch the selected feature evolve
in both the temporal feature space and the volumetric space. In this
manner, once a feature of interest is identiﬁed in one time step of the
volume, the transfer function will be extended temporally and track
this feature as at it changes.
2

R ELATED W ORK

Early work by Kindlmann et al. [8] and Kniss et al. [10] applied the
idea of a multi-dimensional transfer function [12] to volume rendering.
This work identiﬁed key problems in transfer function design, noting
that many interactive transfer function widgets lack the information
needed to guide users to appropriate selections, making the creation of
an appropriate transfer function essentially trial-and-error which is further complicated by the large degrees of freedom available in transfer
function editing. While many volume rendering systems have adopted
multi-dimensional transfer function editing tools, the creation of an
appropriate transfer function is still difﬁcult as the user must understand the dimensionalities of the feature space that they are interacting
within.
As such, much work has been done on providing users with a
simpliﬁed means of transfer function creation. Fang et al. [7] deﬁned transfer function design as a set of 3D image processing tools.
Takanashi et al. [24] utilized independent component analysis and
multi-dimensional histograms to classify areas in the volume domain.
By interactively clipping regions in the component space, users were
able to effectively generate opacity transfer functions in the data. Work
on statistically quantitative volume visualization by Kniss et al. [11]
noted that nearly all image data share the characteristic that samples
are spatially correlated, and focused on a general statistical classiﬁcation for feature extraction. Rezk-Salama et al. [17] utilized principal
component analysis for transfer function design, and Weber et al. [27]
utilized topology-based methods to extract features from volumetric
data and applied separate transfer functions for contour viewing. Wu
and Qu [29] proposed an intelligent scheme that allows users to create
multiple transfer functions and use the resultant volume renderings to
add or subtract regions to create enhanced focus plus context views.
Similarly, an interactive feature brushing interface was proposed by
Ropinski et al. [19] to facilitate automatic generation of a 1D transfer
function.

Unfortunately, the projection of the volumetric data properties
down to a 2D space will obscure features, making it difﬁcult to separate regions of interest within the volume with only a 2D transfer
function. In order to overcome these difﬁculties, much research has
focused on enhancing transfer function design through the addition of
other data properties. Examples include work by Kindlmann et al. [9],
which employed the use of curvature information to enhance multidimensional transfer functions, and Tzeng et al. [25], which focused
on higher dimensional transfer functions which use a voxel’s scalar
value, gradient magnitude, neighborhood information and the voxel’s
spatial location. Work by Potts et. al [16] suggested visualizing transfer functions on a log scale in order to better enhance feature visibility.
Lundstrom et al. introduced the partial range histogram [13] and the
α -histogram [14] as means for incorporating spatial relations into the
transfer function design. Correa et al. introduced size based transfer
functions [5] which incorporate the magnitude of spatial extents of volume features into the color and opacity channels and visibility based
transfer functions [6] where the opacity transfer function is modiﬁed
to provide better visibility of features.
While such extensions enhance the volume rendering and provide a
larger separability of volumetric features, they still fail to provide users
with information about the structures within a given feature space.
In fact, the addition of more dimensionality into the transfer function is often automatically incorporated into the rendering parameters,
obscuring the relationship between the volumetric properties and the
volume rendering. However, the addition of more dimensions should
provide more coherency between transfer functions, particularly in the
case of temporal volumes. Recent work by Akiba et al. [1, 2] utilized
parallel coordinate plots to create a volume rendering interface for exploring multivariate time-varying datasets. By means of a predictioncorrection process, Muelder and Ma [15] proposed to predict the feature regions in the previous frame, making the feature tracking coherent and easy to extract the actual feature of interest.
3 N ON -PARAMETRIC T RANSFER F UNCTION G ENERATION
Our work focuses on providing the user with information about the
volume data within the conﬁnes of the transfer function feature space.
Given a 2D histogram representing some subset of an n-dimensional
space, not all features can be separated. In fact, the projection of n
dimensions to a 2D space ensures that some features will be obscured.
However, one can identify the neighborhood features within this two
dimensional projection, providing the users a context in which to explore their data. These identiﬁed features can be represented as clusters of color based on the neighborhood properties, and users may then
explore the effects that growing or shrinking these neighborhoods (as
applied transfer functions) will have on the volume rendering.
3.1 Kernel Density Estimation
In order to create these neighborhoods and provide contextual information about the feature space to the user, we employ the use of a variable kernel method [21, 26] formed in Equation 1. A non-parametric
clustering approach is chosen in order to remove the need for a priori
knowledge about the number of clusters within a dataset (as opposed
to k-means clustering where the user speciﬁes the number of clusters).
Furthermore, we utilize an adaptive kernel which scales the parameter
of the density estimation by allowing the kernel radius to vary based
upon the distance from each point, Xi , to the kth nearest neighbor in
the set comprising the N −1 data points of the histogram feature space.
1
fˆh (x) =
N

N

1
∑ di,k K
i=1



x − Xi
di,k


(1)

Here, fˆh (x) is the probability density estimate of the histogram (h)
at a given location, x, in the feature space, di,k represents the multidimensional smoothing parameter and N is the total number of samples in the histogram (i.e., the number of voxels in the volume). The
window width of the kernel placed on the point Xi is proportional to
di,k (where di,k is the distance from the i-th sample to the k-th nearest
neighbor) so that data points in regions where the data is sparse will

MACIEJEWSKI ET AL: STRUCTURING FEATURE SPACE: A NON-PARAMETRIC METHOD FOR VOLUMETRIC…

√
have ﬂatter kernels. We choose k =  N as this tends to approximate
the optimal density estimation ﬁtting (this is a rule of thumb approximation [21]). Such a method groups the data based on their neighborhood information, allowing us to visualize the underlying structure of
the data.
In order to reduce the calculation time, we have chosen to employ
the Epanechnikov kernel, Equation 2.
3
K(u) = (1 − u2 )1(||u||≤1)
4

(2)

The function 1(||u||≤1) evaluates to 1 if the inequality is true and zero
for all other cases. Other kernels choices (i.e., Gaussian) could be utilized; however, the Epanechnikov kernel is the kernel of choice for fast
calculations as it is less computationally expensive than a comparable
Gaussian kernel [21]. Future work will allow users to choose from a
variety of kernels including uniform, Gaussian, etc.
Currently, For a 256 × 256 histogram, showing the feature space
of a 2563 volume (i.e., N = 2563 ) interactions with the kernel density
feature space tool results in a volume rendering frame rate of approximately 10 fps on an Intel Xeon 3.00 GHz processor with 2.0 GB or ram
using an Nvidia 8800 card. Note that the size of the histogram bins will
directly affect the kernel density estimation results. As the histogram
bins become larger (i.e., the histogram dimensions get smaller), more
voxel values will map to the same histogram bin. This results in the
adaptive kernel radius tending towards a value of one, and the resultant
density estimation becomes more discrete. In general, the smaller the
histogram bins in a 2D transfer function window, the easier it will be
to separate values as less voxels will map to the same coordinate.
Figure 1 illustrates this process (in the 2D case) using the CT visible woman feet. Here, the user is presented with the typical 2D histogram widget in the value vs. value gradient magnitude feature space.
Each entry in the feature space histogram contains information on the
number of voxels that map to these features. Given this information,
we use the variable kernel method (Equation 1), to create a continuous density distribution function across the entire feature space. This
method would produce comparable results to the continuous scatterplots work by Bachthaler and Weiskopf [3] if a logarithmic color scale
were to be applied and a ﬁner grid was used in the density estimation;
however, our method provides information about clustered structures
within the 2D histogram feature space where as the continuous scatterplot method does not. The values obtained in this procedure now represent a probability that a voxel will be found at a location in feature
space. We then create a 1D histogram of the probability distribution
and color the feature space based on the chosen binning, thereby generating our transfer function. The transfer function is then applied to
color the volume, and boundary enhancement (see Section 3.2) is applied for the opacity. Note that the arc-like structures (which represent
material boundaries) are preserved, indicating that they share similar
distributions of points within the feature space. By creating an automatic coloring of the image, we not only provide information about the
underlying feature space structures, but we also provide the user with
a reasonable starting transfer function, reducing the trial-and-error exploration time typically required for transfer function creation. The
resultant rendering is able to effectively color the various structures
within that feature space (note that this method cannot separate overlapping features as it still only uses two-dimensions and the projection
of the data down to two-dimensions can result in a combination of features). From there, the user may interact within the clustered transfer
function space, reﬁning the number, size and color of the clusters. As
such, this initial transfer function generation is able to provide users
with a solid starting point for feature space exploration, and users are
then able to quickly navigate through the feature space to segment out
regions of interest, greatly reducing the time taken to create a transfer
function.
Note that the choice of the number of histogram bins for the KDE
coloring is not the same as choosing the number of clusters in a knearest neighbor approach. In the case of the non-parametric clustering, we have simply mapped a range of density values to a discrete
color. Notice that in Figure 2 the red bin corresponds to two unique

1475

clusters. However, this discretization is also a limitation in the system
as the user must individually selected each red cluster for visualization in order to determine which feature(s) of the volume the cluster
maps too. Examples of this selection and exploration can be seen in
Figure 4.
Related work has been done by Shamir [20] where feature-space
analysis was applied to unstructured meshes in order to automatically
identify structures within the volume. This work utilized a mean shift
procedure for clustering which is similar to kernel density estimation.
This clustering is performed across a ﬁve dimensional space (the x, y
and z components of the volume and the value vs. value gradient magnitude feature space), where as we only perform this on the feature
space. However, the goal of Shamir’s work was volume segmentation
as opposed to transfer function design and interactive feature extraction. Our work provides users with interactive tools to segment the
feature space of a volume, and provides a means in which to explore
and understand the relationships between a given feature space group
and the related volumetric data.
Work by Roettger et al. [18] also generated transfer functions
through feature space analysis. Automatic setup of multi-dimensional
transfer functions is enabled by adding spatial information to the histogram of the underlying dataset. The approach taken in this paper
differs as we focus on the neighborhoods within a feature space to ﬁnd
natural borders of the feature space point densities and project these
feature clusters back to the volumetric domain. While our method is
currently only applied to the 2D feature space (or 3D temporal feature
space), an extension to n-dimensions is minor, and the incorporation
of variable neighborhood information for clustering should improve
upon the work by Roettger et al. Furthermore, our procedure also
smoothes the noise in the data, interpolating the 2D histogram at all
positions through the kernel density estimation procedure. In medical
datasets, Roettger et al. noted that their clusters have added noise and
segmentation ﬁltering is necessary. In contrast, our interactive widgets allow users to adjust the cluster boundaries, allowing for greater
ﬁltering precision as demonstrated in Section 4.
3.2 Coloring and Opacity
As previously stated, the coloring of the transfer function is based on
1D histogram binning of the probability values calculated during the
kernel density estimation procedure. Figure 2 illustrates the 1D histogram created based on the probability density estimation method deﬁned in Section 3.1. Values on the left of the histogram represent low
density areas, while values on the right represent high density areas.
Users are able to interactively select the number of bins (and their
color). The default color scheme utilizes a qualitative color scheme
[4] with a default of twelve color bins (although other schemes could
also be applied). In Figure 2, we can observe the effect of increasing
or decreasing the number of bins. Users may also increase or decrease
the number of bins by entering into a ‘join’ mode in which they can
click on any two histogram bins and they will merge together, with
the resultant color being that of the ﬁrst bin clicked. In joining bins,
the effect is to increase the width of a histogram bin, thereby increasing the number of voxels mapped to a given color. This will result in
neighboring rings joining together into a single area. Users may also
separate bins by right clicking within a bin. The two resultant bins will
remain the same color until the user chooses to assign a new color.
Allowing the users to interactively change the number of bins can
create more meaningful clusters. For example, in the Feet dataset, one
may expect to ﬁnd four materials, namely, air, bone, skin and soft tissue. By starting with four bins (as opposed to twelve), structures will
group together in a more intuitive manner. However, it is important to
note that the number of clusters seen can be greater than the number
of bins selected. These clusters represent peaks and valleys within the
feature space; therefore, clusters of colors appear in various regions
across the feature space as the point density changes.
In order to enhance data exploration within the feature space, our
histogram widget also allows users to interactively adjust the size of
the bins. Users may choose from initial settings of either equally
spaced or exponentially sized bins, or users may use the mouse to

1476

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 15, NO. 6, NOVEMBER/DECEMBER 2009

Fig. 2. Two examples of clustering done by binning the probability density estimates of the histogram data. This example uses the value versus the
value gradient magnitude feature space of the visible woman feet.

Fig. 3. Temporally coherent transfer function volume: (Left) A single time step rendering of the velocity ﬁelds in the tornado volume, (Left-Center)
The transfer function volume, (Right-Center) A clustered 2D histogram extracted from the volume at time step t, (Right) The 1D histogram of the
transfer function volume showing various opacity applications in the color bins.

expand the boundary of any histogram bin. As the bin grows or
shrinks, regions in the transfer function also grow and shrink, and these
changes are automatically reﬂected in an interactive volume rendering
window.
While the color selection is done semi-automatically through the
non-parametric clustering, we utilize a boundary enhancement [22]
method for modulating the opacity. This choice was made based on
the fact that areas with high gradients often represent boundaries between materials. In order to more effectively illustrate these areas, one
may simply increase the opacity proportional to the gradient magnitude such that
→
−
oe = oo · | (P)|α
(3)
→
−
where (P) is the gradient at the sampled point P, oo is the original
sample opacity, oe is the enhanced sample opacity, and α is the boundary exponent which serves to change the sensitivity of the magnitude
of the rendered boundary. The application of our density estimation
transfer function coupled with boundary enhancement typically leads
to high-quality renderings with no user interaction. To clarify, the rendering in Figure 1 was generated completely automatically. Once an
initial rendering is generated, the user may begin exploring their data
in the feature space; meanwhile, by providing such a starting location,
we have vastly increased the user’s ability to understand the relationships between the feature space domain and the volumetric domain.
Along with boundary enhancement, our control widgets also provide several schemes for manually adjusting the opacity values. In the
1D density estimation histogram distribution, the user may enter the
opacity selection mode and modify how the opacity is mapped across
the color bin. The red line found in the histogram bins indicates the
opacity value across the histogram space. The top of each bin represents an opacity of one, and the bottom zero. Users may select from

one of seven default opacity curves (variations of linear, triangular, and
sinusoidal) by clicking in the histogram bin. Figure 3 (Right) shows
the modiﬁed opacity curves in the histogram bin. Future work will
contain an interface for user deﬁned opacity curves.

3.3 Feature Space Exploration
In order to explore the data feature space, we provide users with the
ability to select any color region in the given 2D histogram window.
Area selection will effectively create a mask for the transfer function,
and only voxels located within the region selected will be rendered.
Users may select/deselect a region simply by clicking in the feature
space, and if the user chooses to expand these areas through the use
of the density estimation histogram controls, the selection will follow
this expansion. Users may also combine areas in the feature space by
entering a combination mode. In this mode, the user clicks on one
area in the feature space, and then another. This will result in the second area being colored the same as the ﬁrst area. However, such a
combination disables the density estimation histogram mapping. For
example, Figure 2 has two distinct red areas in the density estimated
transfer function. If one of those areas were combined with the purple area, there is no guarantee that such a combination equally splits
the red bin. In fact, it is more likely that both red areas share a combination of values across the entire red bin. As such, one histogram
color bin does not necessarily represent a single neighborhood in the
feature space. These features allow for interactive region exploration
and combination, letting the user quickly search for neighborhoods of
interest within the feature space. In depth examples of the use of these
modes can be found in Section 4.

MACIEJEWSKI ET AL: STRUCTURING FEATURE SPACE: A NON-PARAMETRIC METHOD FOR VOLUMETRIC…

1477

Fig. 4. Feature segmentation and transfer function design in the CT Bonsai dataset.

3.4 Temporal Coherency in Transfer Function Design
In this work, we extend our previously described feature space clustering into the temporal dimension. In the case of temporal volumetric
data, each time step consists of a volume data set, from which a 2D
feature space can be extracted. We create a volume of these feature
space histograms (the third dimension being time), and, as in the 2D
histogram case, we apply the variable kernel density estimation to extract features within this new volume. The feature space volume is
then colored based on the 3D probability density binning (as was done
in the 2D case). Now at each time step, the transfer function is the
slice of the feature space volume that corresponds to the current time
step.
Figure 3 illustrates this procedure on a time step of the tornado
dataset. Figure 3 (Left) shows the volume rendering of the tornado
data set based on a single transfer function time step (Figure 3 (LeftCenter)) extracted from the temporal transfer function volume (Figure 3 (Right-Center)) at time step t and binned according to the 1D data
histogram (Figure 3 (Right)). Here, the user can see the feature space
clusters propogated over the entire temporal space. The green-red axis
plane of Figure 3 (Left-Center) represents the feature space domain,
and the blue axis represents the temporal domain. All previously described widgets also operate in this space; however, modiﬁcations to
the 1D density histogram now affects the entire transfer function volume, and selections, as described in Section 3.3, also propagate across
the transfer function volume.
Such a method is a vast improvement over current transfer function

design as data clusters are able to evolve over time, these data clusters then become part of the transfer function, and features of interest
are able to be visualized over time, without manual adjustment of the
transfer function. Unfortunately, should features leave and re-enter or
somehow become obscured by other features due to the projection into
the 2D feature space, our current implementation would be unable to
account for these changes. However, as shown in the results of Section 4.4 and 4.5, our method is able to provide high-quality results
even under this limitation.
4

E XPLORING T HE F EATURE S PACE

In this section, we illustrate the beneﬁts and limitations of our system
using common volumetric datasets as examples. Here, we discuss the
potential for feature segmentation, fast transfer function creation, and
temporally coherent transfer function design.
4.1 Feature Segmentation
When provided with a 2D histogram of the feature space, novice users
have little to no intuition as to what data points in the feature space
map to volumetric features. By utilizing our non-parametric clustering
technique, we are able to quickly provide the users with information
on how the feature space relates to the volumetric space. In Figure 4,
the user begins with the value vs. value gradient magnitude feature
space, and then automatically applies our method to generate a color
transfer function. While the initial volume rendering is less than desirable, the user is able to immediately understand which clusters in

1478

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 15, NO. 6, NOVEMBER/DECEMBER 2009

Fig. 5. Feature segmentation and transfer function design comparing traditional transfer function design methods to our non-parametric clustering
method in the Cayley’s cubic data set using the value versus value gradient magnitude feature space.

Fig. 6. The electron orbitals of a two million atom Indium-Arsenic quantum dot simulation in the second excited state: (Left) the p orbital rendered
using non-parametric clustering of the p-value versus p-value gradient magnitude feature space, (Right) the p orbital rendered using non-parametric
clustering of the p-value verse s-value feature space.

the feature space map to volumetric features (for example, the purple
color obviously maps to the leaves and the blue color to the earth).
Using this knowledge, the user can quickly modify the cluster colors
to a more natural set (green for leaves, brown for earth, etc.).
However, while the initial volume rendering is able to provide information about the relation between feature space clusters and volumetric properties, the user may still not be clear on the exact relationship
of each feature cluster in the volume rendering. To better explore the
feature space, the user may interactively select one (or more) clusters
in the feature space using the mouse and a new transfer function is
applied to the volumetric data, representing only the selected feature
space clusters. In the example of Figure 4, the user ﬁnds that the middle green cluster is actually also a portion of the earth. The user can
then join that cluster to the brown grouping, and a ﬁnal rendering is
created. Note that fuzzy artifacts still exist in the ﬁnal rendering as
the leaf air boundary is indistinguishable in our technique due to the
fact that the 1D histogram binning that is mapping colors to areas of
similar density. Notice how in the bottom left cluster of the Generated
Transfer Function in Figure 4, the purple cluster encompasses the red
cluster. The data nearest the red cluster is most likely the source of the
noise; however, even by adjusting the bin width the noise can only be
reduced, not eliminated.
4.2 Arbitrarily Shaped Transfer Functions
As shown in the previous examples, our technique provides an intuitive method for quick transfer function generation. However, the argument could be made that were novice users informed that the arcshaped structures represent material boundaries; they could simply se-

lect those areas with rectangular transfer function widgets and produce
a reasonably high-quality image. Such an argument is valid; however,
if the user is then provided with a dataset with no discernable arcshapes, where should exploration begin? Further, as was shown in
Figure 4, clusters within feature space do not typically follow simple
square clustering.
To illustrate the beneﬁt of arbitrarily shaped clusters, our next example utilizes the Cayley’s cubic data [28] shown in Figure 5. The
Cayley’s cubic is the unique cubic surface satisfying the equation:
4(x3 + y3 + z3 + w3 ) − (x + y + z + w)3 = 0

(4)

Here, the user has rendered a portion of the Cayley cubic through our
transfer function generation widgets by selecting the purple region of
Figure 5 (Left). In order to create a similar rendering using common
box widgets, the user needs multiple interactions. Furthermore, the
knowledge of where to draw such a grouping might never be realized from merely analyzing the distribution within the feature space.
As such, it becomes clear that while powerful, rectangular transfer
function editing widgets can pose severe limitations on the data exploration; moreover, providing a user with a starting point as to where to
draw a transfer function within the feature space is extremely useful.
4.3 Applications Across Arbitrary Feature Spaces
In the previous examples, we applied non-parametric transfer function
generation to the value versus value gradient magnitude feature space
of data; however, the choice of the feature space dimensions is arbitrary. In fact, by choosing a more meaningful feature space, users may
be able to better extract particular features of interest. In Figure 6 we

MACIEJEWSKI ET AL: STRUCTURING FEATURE SPACE: A NON-PARAMETRIC METHOD FOR VOLUMETRIC…

1479

Fig. 7. Oriented structural ﬂow visualizations of temperature advection in a convection dataset with numerical labels corresponding to the time-step.
(Top) The volumetric rendering and (Bottom) their associated transfer function.

illustrate the difference that feature space choices make in our methods output by utilizing the s-orbital and p-orbital electron probability
density functions from a two million atom Indium-Arsenic quantum
dot simulation in the second excited state.
In Figure 6 (Left) we apply non-parametric transfer function generation to the p-value versus p-value gradient magnitude feature space
(where p is the orbital), and in Figure 6 (Right) we apply nonparametric transfer function generation to the p-value versus s-value
feature space. By comparing the resultant renderings, scientists can
gain a better understanding of how various feature space dimensions
interact in the physical volumetric space. For instance, Figure 6
(Right) is useful to understand areas and structures within the quantum dot where density relationships between p-orbital and s-orbital
electron density are maintained.
4.4 Temporally Coherent Transfer Functions
As previously stated, our transfer function generation method also is
able to readily capture structures in n-dimensional space. By applying
non-parametric density estimation across a volume of feature spaces,
we are able to create a transfer function at each feature space time
step that encodes information about the growth of volumetric features.
Figure 7 demonstrates the application of our temporal transfer function
generation method across a series of time steps of the convection in a
box data set.
We apply a sine function to map the scalar data value to opacity,
creating alternating low and high opacity contours in order to create
the advection results [23]. In each of the transfer function time steps,
we can see the cluster shapes grow tracking regions of like density
distributions across time. As the user scrolls through time, the transfer
function is extracted from the transfer function volume (described in
Section 3.4). As such a new transfer function is automatically applied
to the volume at each time interval, eliminating the need for transfer
function modiﬁcation at each step. Here, researchers are able to see
the mixing and obtain a feel for the time varying properties in a single
image with the transfer function showing both the internal and external
structures.
Furthermore, Figure 8 demonstrates how a static transfer function
may fail to follow features of the volume. From time step 4380 to
4620 in the cloud simulation, one can observe that the static transfer
function seems to lose the upper cloud features which are prevalent
when using the temporally coherent transfer function.
4.5 Enhanced Feature Space Knowledge
While the temporal transfer function is a powerful tool for feature
tracking, it also is able to enhance a user’s knowledge about their data
set by providing an analysis in the change of density distributions at
each time interval. In many simulation models, researchers want to

look at the frequency in which data variable relationships occur within
a data set. For example, in the simulated cloud generation of Figure 8, the transfer function is generated on the turbulent kinetic energy
(TKE) versus water content. At each time step, the atmospheric scientist is interested in understanding the density of voxels for a given
(TKE, water content) pair and the change of this relationship over time
during the cloud formation process. By analyzing the transfer function
clusters at each time step, the researcher can see how the voxel distribution changes over time, and future work will track the voxels over
time for enhanced analytic capabilities.
5

C ONCLUSIONS

AND

F UTURE W ORK

In this work, we have shown that the application of non-parametric
density estimation for clustering in feature space can provide the basis
for semi-automatic transfer function generation. Volumetric structures
are quickly separated in the feature space (when possible), and users
are able to effectively render individual features by using our interactive widgets. Our work enables users to more effectively explore the
volumetric feature space by automatically grouping neighborhoods of
similar distributions, and these neighborhoods then tend to correlate
to volumetric features. Furthermore, the extension of this scheme to
temporal volumetric data provides a means for creating temporally coherent transfer functions, allowing users to track changing volumetric
features without the need to redeﬁne the transfer function at each stage
of the process. Our scheme has proven to be effective over various
types of volumetric data, ranging from scalar data (such as CT and
MRI) to ﬂow data.
However, our scheme does have several limitations. First, areas
with similar density distributions will map to the same color bin of the
KDE histogram, and the resulting clusters will need to be explored in
order to determine the feature space cluster, volumetric feature correspondence. Second, the growth of KDE bins will not always allow for
100There are instances where this could cause features and noise to
blend together.
Future work will incorporate advanced edge dragging tools for the
clusters to allow for better editing as well as the extension of our
scheme to n-dimensional feature space, as well as enhanced widgets
for the exploration of higher data dimensionalities. By incorporating
this method across a higher number of dimensions, features that were
not separable in the current system should become separable.
ACKNOWLEDGMENTS
T his work has been funded by the US Department of Homeland Security Regional Visualization and Analytics Center (RVAC) Center
of Excellence and the US National Science Foundation (NSF) under
Grants 0328984 and 0121288 and NSF China (No. 60873123) and
973 program of China (2010CB732504).

1480

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 15, NO. 6, NOVEMBER/DECEMBER 2009

Fig. 8. The use of non-parametric two-dimensional transfer functions to visualize time steps of a simulated cloud formation in the feature space
of TKE versus water content. (Top) The volumetric rendering using the red portion of our temporal transfer function. (Bottom) The volumetric
rendering using a static 2D transfer function. .

R EFERENCES
[1] H. Akiba and K.-L. Ma. A tri-space visualization interface for analyzing
time-varying multivariate volume data. In Proceedings of Eurographics/IEEE VGTC Symposium on Visualization, pages 115–122, May 2007.
[2] H. Akiba, K.-L. Ma, J. H. Chen, and E. R. Hawkes. Visualizing multivariate volume data from turbulent combustion simulations. Computing
in Science and Engineering, 9(2):76–83, 2007.
[3] S. Bachthaler and D. Weiskopf. Continuous scatterplots. IEEE Transactions on Visualization and Computer Graphics, 14(6):1428–1435, 2008.
[4] C. A. Brewer. Designing better Maps: A Guide for GIS users. ESRI
Press, 2005.
[5] C. Correa and K.-L. Ma. Size-based transfer functions: A new volume
exploration technique. IEEE Transactions on Visualization and Computer
Graphics, 14(6):1380–1387, October 2008.
[6] C. Correa and K.-L. Ma. Visibility-driven transfer functions. In Proceedings IEEE-VGTC Paciﬁc Visualization Symposium, Beijing, China, April
2009.
[7] S. Fang, T. Biddlecome, and M. Tuceryan. Image-based transfer function
design for data exploration in volume visualization. In Proceedings of the
IEEE Conference on Visualization, pages 319–326, 1998.
[8] G. Kindlmann and J. W. Durkin. Semi-automatic generation of transfer
functions for direct volume rendering. In Proceedings of the IEEE Symposium on Volume Visualization, pages 79–86, 1998.
[9] G. Kindlmann, R. Whitaker, T. Tasdizen, and T. Moller. Curvature-based
transfer functions for direct volume rendering: Methods and applications.
In Proceedings of the IEEE Conference on Visualization, pages 513–520,
2003.
[10] J. Kniss, G. Kindlmann, and C. Hansen. Interactive volume rendering using multi-dimensional transfer functions and direct manipulation widgets.
In Proceedings of the IEEE Conference on Visualization, pages 255–262,
2001.
[11] J. M. Kniss, R. V. Uitert, A. Stephens, G. S. Li, T. Tasdizen, and
C. Hansen. Statistically quantitative volume visualization. In Proceedings of the IEEE Conference on Visualization, pages 287–294, 2005.
[12] M. Levoy. Display of surfaces from volume data. IEEE Computer Graphics & Applications, 8(3):29–37, 1988.
[13] C. L. P. Ljung and A. Ynnerman. Local histograms for design of transfer
functions in direct volume rendering. IEEE Transactions on Visualization
and Computer Graphics, 12(6):1570–1579, 2006.
[14] C. Lundström, A. Ynnerman, P. Ljung, A. Persson, and H. Knutsson.
The alpha-histogram: Using spatial coherence to enhance histograms and

[15]

[16]
[17]

[18]

[19]

[20]
[21]
[22]

[23]

[24]

[25]

[26]
[27]

[28]
[29]

transfer function design. In Proceedings Eurographics/IEEE-VGTC Symposium on Visualization 2006, May 2006.
C. Muelder and K.-L. Ma. Interactive feature extraction and tracking by
utilizing region coherency. In Proceedings of IEEE Paciﬁc Visualization
Symposium, April 2009.
S. Potts and T. Moller. Transfer functions on a logarithmic scale for volume rendering. In Proceedings of Graphics Interface, pages 57–63, 2004.
C. Rezk-Salama, M. Keller, and P. Kohlmann. High-level user interfaces
for transfer function design with semantics. IEEE Transactions on Visualization and Computer Graphics, 12(5):1021–1028, 2006.
S. Roettger, M. Bauer, and M. Stamminger. Spatialized transfer functions.
In Proceedings Eurographics/IEEE-VGTC Symposium on Visualization
2005, 2005.
T. Ropinski, J.-S. Prani, F. Steinicke, and K. H. Hinrichs. Stroke-based
transfer function design. In IEEE/EG International Symposium on Volume and Point-Based Graphics, pages 41–48, 2008.
A. Shamir. Feature-space analysis of unstructured meshes. In Proceedings of the IEEE Conference on Visualization, pages 185–192, 2003.
B. W. Silverman. Density Estimation for Statistics and Data Analysis.
Chapman & Hall/CRC, 1986.
N. Svakhine and D. S. Ebert. Interactive volume illustration and feature
halos. In Proceedings IEEE-VGTC Paciﬁc Visualization Symposium, October 2003.
N. A. Svakhine, Y. Jang, D. S. Ebert, and K. P. Gaither. Illustration and
photography inspired visualization of ﬂows and volumes. In Proceedings
of the IEEE Conference on Visualization, pages 687–694, 2005.
I. Takanashi, E. B. Lum, K.-L. Ma, and S. Muraki. ISpace: Interactive
volume data classiﬁcation techniques using independent component analysis. In Proceedings of Paciﬁc Graphics, pages 366–374, 2002.
F.-Y. Tzeng, E. B. Lum, and K.-L. Ma. A novel interface for higherdimensional classiﬁcation of volume data. In Proceedings of the IEEE
Conference on Visualization, pages 505–512, 2003.
M. P. Wand. Kernel Smoothing. Chapman & Hall/CRC, 1995.
G. H. Weber, S. E. Dillard, H. Carr, V. Pascucci, and B. Hamann.
Topology-controlled volume rendering. IEEE Transactions on Visualization and Computer Graphics, 13(2):330–341, 2007.
E. W. Weisstein. “The Cayley Cubic.” From MathWorld—A Wolfram
Web Resource. http://mathworld.wolfram.com/GreensIdentities.html .
Y. Wu and H. Qu. Interactive transfer function design based on editing
direct volume rendered images. IEEE Transactions on Visualization and
Computer Graphics, 13(5):1027–1040, 2007.

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS  VOL. 23,  NO. 1,  JANUARY 2017371

Quantifying the Visual Impact of Classification Boundaries in
Choropleth Maps
Yifan Zhang and Ross Maciejewski

Fig. 1. An example of the visual effect of shifting classification boundaries. In this example, we create a choropleth map of housing
factors in the Tokyo metro area (data captured in 1990 as part of a mortality study [40]). A two-dimensional k-means classification
scheme was applied and elements near the classification boundaries are highlighted on the map and the scatterplot. The x-axis is the
proportion of professional workers in an area and the y-axis is the proportion of the population that owns a house. The spatial units
highlighted in the original classification result are all within some threshold value (−.03 ≤ τ ≤ 0.0) of the cluster boundary. If the cluster
boundaries were to be modified, the result (after modification) would show different amounts of visual clustering of spatial units as
demonstrated when comparing the two maps.
Abstract—One critical visual task when using choropleth maps is to identify spatial clusters in the data. If spatial units have the same
color and are in the same neighborhood, this region can be visually identified as a spatial cluster. However, the choice of classification
method used to create the choropleth map determines the visual output. The critical map elements in the classification scheme are
those that lie near the classification boundary as those elements could potentially belong to different classes with a slight adjustment
of the classification boundary. Thus, these elements have the most potential to impact the visual features (i.e., spatial clusters) that
occur in the choropleth map. We present a methodology to enable analysts and designers to identify spatial regions where the visual
appearance may be the result of spurious data artifacts. The proposed methodology automatically detects the critical boundary cases
that can impact the overall visual presentation of the choropleth map using a classification metric of cluster stability. The map elements
that belong to a critical boundary case are then automatically assessed to quantify the visual impact of classification edge effects. Our
results demonstrate the impact of boundary elements on the resulting visualization and suggest that special attention should be given
to these elements during map design.
Index Terms—Choropleth, Classification, Visualization, Geodemographics, Geovisualization

1

I NTRODUCTION

One of the most common methods of visualizing spatially referenced
data is the choropleth map. Choropleth maps are based on data aggregated over defined areal units (country, ZIP code, etc.), and one of the
first design choices in creating a choropleth map is determining which
range of data values should be associated with which color. This step
is typically referred to as classification, and a variety of class interval
selection/binning methods (e.g., quantile, equal interval, standard deviation, natural breaks [30], minimum boundary error [15], and genetic
binning [5]) have been developed. The selection of the class interval
has a major impact on the visual appearance of the map [18]. Ideally,
regions that are alike under a given statistical measure will appear as the
same color on a map; however, due to the nature of interval selection,
there can be map elements where the statistical measure falls near the

• Yifan Zhang and Ross Maciejewski are with Arizona State University. For
questions, e-mail: rmacieje@asu.edu.

Manuscript received 31 Mar. 2016; accepted 1 Aug. 2016. Date of publication
15 Aug. 2016; date of current version 23 Oct. 2016.
For information on obtaining reprints of this article, please send e-mail to:
reprints@ieee.org, and reference the Digital Object Identifier below.
Digital Object Identifier no. 10.1109/TVCG.2016.2598541

classification boundary. Depending on the spatial positioning in the
map, switching a map element from one class to another could result
in large changes in the appearance of visual clustering in the map. For
example, in Fig. 1, several elements are on the boundary between two
classes (Yellow and Purple) as well as others. Here, we show that if
the classification boundary was slightly shifted, the area under analysis
would visually appear to have a larger amount of spatial clustering than
if the boundary were to remain as chosen. Given unit i and j’s spatial
proximity to other elements in the Yellow class, shifting the PurpleYellow classification boundary slightly to incorporate these units into
the Yellow class may not be unreasonable.
This issue is further compounded in more complicated maps where
the classification is not being done for one or two variables but rather
as a combination of multiple variables. Such multivariate classification
is common practice in many areas, such as geodemographic profiling [6, 46], ecological area selection [25], and epidemiology [7], and
involves creating intervals based on multiple statistical measures. This
multivariate classification typically employs various data mining and
machine learning classification methods (e.g., k-means [36, 43], selforganizing maps [23], hierarchical clustering [13, 22]), and the class
intervals in the choropleth map no longer belong to a single data range
but rather they belong to a more complex combination of relationships

1077-2626 © 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
  See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

372  	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,  VOL. 23,  NO. 1,  JANUARY 2017

between variables. Thus, the issue of boundary cases (i.e., map elements that could potentially belong to multiple classes) is critical due to
the fact that small changes to the classification of a boundary element
can result in a large change in the visual appearance of the choropleth
map.
Here, it is critical to note that the concept of map classification is
a matter of reducing precision in order to provide a compact visual
representation. What is being done is to group similar data elements
into classes and project these on the map. Spatial analysis will be done
on the actual data values in order to explore relationships in the data.
However, the visual display of the data still depends on this reduced
precision. As such, a map classification can imply to a viewer that
spatial relationships exist (whether intentionally or unintentionally).
Thus, we need to fully understand the visual implications that such
classifiers have. In this paper, we utilize a cluster measure to identify
map elements that could belong to multiple class intervals as a result
of multivariate classification as done in prior studies [8, 50, 52]. Once
these elements are identified, we develop a novel metric based on a
global measurement of spatial clustering (Moran’s I [39]) to quantify
the visual change that will occur if a boundary element is relabeled.
Based on this metric, we identify map elements that warrant a closer
inspection as a reclassification of the elements will result in more (or
less) spatial association being visually present in the map. We believe
that the identification of such map elements will better inform map
design by enabling analysts or designers to identify spatial regions
where the visual appearance may be the result of spurious data artifacts,
or to identify spatial regions that should potentially be reclassified due
to their regional spatial association. Demos, datasets and code related
to the proposed methodology can be found at: https://vader.dtn.
asu.edu:8443/BoundaryElements/.
2

R ELATED W ORK

Our work focuses on quantifying the effect of the changes caused
by modifying the class labels of boundary elements. In this section
we review the work in map classification, spatial association, map
comparison and direct manipulation related to classification boundaries.
2.1

Map Classification

The goal of a classification scheme is to group similar observations
and split dissimilar observations to simplify and clarify the message
of the map. For univariate data, the simplest methods include quantile,
equal interval, and standard deviation [35]. More complex methods
have been proposed since the early 1960s. For example, Jenks developed natural breaks, which seeks to reduce the variance within classes
and maximize the variance between classes [30]. Scripter presented
nested means [45] that calculates intervals for statistical maps by repeatedly deriving and using the arithmetic mean to divide a numerical
array. Cromley [15] proposed a minimum boundary error method that
maximizes spatial similarity among contiguous units in the same class
interval, and Armstrong [5] developed a genetic binning scheme that
creates optimal classifications with respect to multiple criteria (e.g.,
number-line relationships, fragmentation).
The most important part of map classification is how to choose
the breaks or class boundaries. Evans [18] categorized sixteen classinterval systems and suggested that class intervals should be selected
according to the overall shape of the data distribution. Brewer et al. [10]
compared seven map classification methods with fifty-six subjects in a
two-part experiment to determine which classifications are most suitable for epidemiological rate maps. Sun et al. [48] proposed a heuristic
classification approach that utilizes the class separability concept and
other classification criteria. They compared their approach to other
classification methods based on element separability; however, visual
changes in the map appearance that could occur due to slight shifts
in classification boundaries have not, to our knowledge, been fully
addressed. In fact, most of the previously mentioned studies focus on
single variable classification methods where the statistical distributions
can be easily plotted and explored.
Multivariate map classification, on the other hand, typically involves
classification over several variables using various data mining and ma-

chine learning classification methods (e.g., k-means and self-organizing
maps) that are often used as black box methods. By assigning a color
to each label/ class/ category in the clustering result, a choropleth map
is generated. Here, the multivariate statistical distributions become
visually complex, and designers may simply default to the base parameters. Such multivariate classification methods are heavily utilized in
demographics classification. For example, Vickers and Rees created
the United Kingdom National Statistics Output Area Classification
(OAC) [49], which is an open geodemographic classification with a
hierarchical structure of 7 super-groups, 21 groups and 52 subgroups.
Because of the efficiency and simplicity [28], k-means clustering remains the core algorithm for the computation of geodemographic classifications [27]. Therefore we use k-means as our default multivariate
classification method; however, our findings can easily be extended to
other classification methods.
2.2

Spatial Association

One of the main uses of choropleth maps is enabling analysts to mentally assess spatial associations. In this paper, we focus on how a
modification of elements near the classification boundaries could potentially change the observed spatial association. To quantify this, we
focus on statistical measures of spatial association, particularly spatial autocorrelation, which is a statistical measure of how spatial units
are associated. A variety of methods for spatial autocorrelation have
been developed over the past decades. The most popular metrics for
global spatial autocorrelation include the Join count statistic, which
was developed mainly for binary variables based on the probability of
a unit having neighboring units of the same class [14], Moran’s I [39],
which considers pairwise products of deviations, Geary’s C [19], which
considers pairwise squared differences, and Getis-Ord General G [20],
which is mostly used for hotspot detection. Global Moran’s I, Geary’s
C and Getis-Ord G are developed for measuring autocorrelation in continuous variables, and the main difference between these methods is the
measures of value similarity used. These methods can be generalized
into cross-product statistics [29], and local versions of these metrics
have been further developed (e.g., LISA [4]).
While methods for measuring autocorrelation in continuous variables are critical, our focus is on the autocorrelation between class
labels, which are categorical variables. In order to compute categorical
spatial association, Join count statistics have been used and are often
applied to the k-color cases [16]. For example, Boots [9] developed a
procedure for extending local statistics to categorical spatial data based
on the composition and configuration characteristics of categorical data.
Our work leverages these spatial autocorrelation metrics as a means of
identifying critical points with regards to the change of visual appearance. In this way, we can quantify the visual change that will occur if
classification boundaries are changed.
2.3

Map Comparison

The major metric of assessing changes based on re-classification of
data in a choropleth map is through visual inspection and comparison. Research in comparison of choropleth maps has focused on using
statistics (e.g., intercorrelation [33], relative blackness [34]) to visually
compare spatial distributions. Olson [41] examines the effects of class
interval systems via the visual judgment of the correlation between
pairs of choropleth maps. Lloyd et al. [34] shows that decisions on
the similarity of maps appears to be influenced by both the similarity
of the spatial distributions and the relative blackness of the maps. Olson [42] also explores the issue of map pattern complexity regarding
several statistics including rank autocorrelation, average differences,
and weighted proportions. Xiao and Armstrong [51] developed an
evolutionary algorithm that allows users to explore spatial patterns in
terms of their visual correlation. Our work differs from previous research in that we focus on the effect of spatial association changes due
to boundary modifications rather than conventional visual correlation;
however, these measures that link similarity to appearance are directly
applicable and could be extended in future work as yet another quantification method for assessing the impact of boundary cases in map
design. Based on previous literature, it is clear that the size of a region

ZHANG AND MACIEJEWSKI: QUANTIFYING THE VISUAL IMPACT OF CLASSIFICATION BOUNDARIES IN CHOROPLETH MAPS373

Fig. 2. An example of k-means clustering (k = 5) using the US Census data variables “Education above bachelor’s degree,” “Mean time to travel to
work,” and “Foreign born person.” Boundary elements with a silhouette coefficient from −.1 ≤ τ ≤ 0 are highlighted. A projection in the geographical
space is shown on the left, and a principle component projection of the first two principle components is on the right. We annotate one boundary
element with a red circle and show the relationship between the two views.

(number of units in a region) directly impacts the implied spatial associations [31] of a map. Thus, as a contiguous region adds more elements
on the map, the size of the region may also imply more importance, and
work by Haroz and Whitney [26] illustrated that grouped arrangements
of elements directly influence visual search and subitizing tasks. As
such, our metric directly ties to the contiguity of units as well as the
change in the size of a region; however, other metrics such as shape
and overall proximity should be considered for future exploration.
2.4 Direct Manipulation & Classification Boundaries
Given that multivariate map classification utilizes a variety of data
mining and machine learning algorithms, it is critical to note that many
methods have been developed to help analysts interactively explore and
manipulate cluster structure. The goal is to utilize domain knowledge
to refine classifications and reduce classification errors. Such work is
directly applicable to classification methods for generating choropleth
maps. Work in this area includes the VISTA system [11], which was
developed to help domain experts validate and refine cluster structures
through interactive feedback. VISTA allows users to mark the visual
boundaries between clusters and refine the algorithmic result if applicable. Chen and Liu [12] developed iVIBRATE as an interactive machine
learning tool, which allows users to iteratively modify the clustering
process. iVIBRATE consists of a visual cluster rendering component
and an adaptive labeling subsystem, and Andrienko et al. [2] developed
methods for analyst-guided clustering of large collections of trajectories
by combining clustering and classification together through an interactive interface. In this paper, we focus on the identification of class
elements near boundaries and enable direct manipulation for relabeling
class elements. While our focus is more on the resulting changes in the
visual output, the identification of elements that impact the visual output can be used as measures of importance to direct analysts’ attention
to elements that require further inspection.
3 T HE V ISUAL I MPACT OF B OUNDARY E LEMENTS
The analysis and understanding of spatial patterns is essential to all
subfields of geography, and the visual representation of spatial patterns
is greatly affected by the choice of classification boundaries. Applying an inappropriate classification may create false patterns or lead
to misinterpretation of the resulting map and choosing an appropriate
data classification scheme for map generation can be difficult [32]. In
fact, Klippel et al. [31] found that subjects (both expert and non-expert)
seemed to base their notion of spatial significance of a map on the number of cells of a particular color. Thus, the choice of class for boundary
elements can directly impact the perceived spatial significance as these
classes directly lend themselves to the cell count. Given these known

issues, it is critical to evaluate the map classification design prior to
presentation. By identifying elements that lie near a classification
boundary, we can quantify the visual impact that shifting the boundary
will have on the map. In this way, users can explore and modify their
classification design scheme or highlight problematic elements that
may be contributing to spurious visual effects.
Previous work has also focused on exploring elements near classification boundaries, for example, Egbert and Slocum created ExploreMap [17], which showed zones that are near classification boundaries in the univariate case. Andrienko et al. [3] developed the
Descartes [1] system to provide additional statistics (min, max, etc.)
about available attributes for each class in order to help analysts understand the classification relationship, and recent work by Slingsby et
al. [47] shows class and distance to class centroid for the results of a
multivariate classification for all locations concurrently. While distance
to the centroid can provide insight into the compactness of a cluster,
it is not necessarily a measure of the stability of the classification of a
map element. Our work extends beyond previous work and explores
the use of other indicators of cluster stability (as opposed to distance
to the centroid) and defines the visual impact of class boundaries. By
identifying elements that are potentially unstable in a cluster, we are
then able to assess how a change of the element’s classification will
impact the resultant visualization. As such, a discrete method for identifying only elements that impact the visual output provides designers
with new information not found in previous methods.
3.1 Identification of Boundary Elements
A variety of metrics exist for characterizing the stability of the results
of multivariate clustering. One such metric is the silhouette coefficient [44], which is used to define the separation distance of elements
between the resulting clusters. The silhouette coefficient is defined as:
S(i) =

b(i) − a(i)
,
max{a(i), b(i)}

where a(i) is the average dissimilarity of object i with all other objects
within the same cluster, and b(i) is the lowest average dissimilarity of
object i to any other cluster in which object i is not a member. S(i)
is bounded by −1 ≤ S(i) ≤ 1. S(i) = 1 indicates that element i is
very far away from all other clusters and so is most likely classified
correctly. S(i) = 0 indicates that element i is near (or on) the decision
boundary of a cluster (meaning it could potentially be reclassified), and
S(i) < 0 indicates that element i is likely misclassified. We leverage
this coefficient as a means of assessing the boundary elements between
clusters. A range, τ for the silhouette coefficient is interactively chosen,

374  	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,  VOL. 23,  NO. 1,  JANUARY 2017

Fig. 3. Four spatial cases and the effects of changing a single unit.

and map elements with values satisfying τ are highlighted on the map.
For example, Fig. 2 shows a demographic clustering of three US Census
variables (“Education above bachelor’s degree”, “Mean time travel to
work”, and “Foreign born person”) with k = 5.
In order to identify which boundary an element i is associated with
during the computation of the silhouette value, the proximity of i to
each cluster is stored. Then, an ordered list of classes are assigned to i
based on the chosen silhouette range, τ. The ordered list represents all
the potential classes that i could be reclassified as.
In addition to the numerical metrics for identifying the points near
boundaries, we also provide a principle component analysis (PCA) scatterplot view (Fig. 2 right) to enable users to visually inspect boundary
conditions of projected clusters. By projecting the k-means clusters
into a 2D space, users can have a generalized overview of how the
clusters are distributed. As the silhouette range τ is changed, boundary
elements are indicated by changing shape from circles to squares in the
PCA scatterplot.
3.2

Indicators of Spatial Association

While the identification of boundary elements is critical, the silhouette
coefficient does not provide information on whether changing these
elements will impact that visual spatial associations on the map. In
order to identify the visual impact of shifting an element class, we first
define the types of spatial association that can be observed:
• Clustered: Map elements with the same class are contiguous in
geographic space, as indicated by positive measures of spatial
autocorrelation in Moran’s I.
• Dispersed: Map elements with different classes (but with a repeated pattern) are contiguous in geographic space, as indicated
by negative spatial autocorrelation in Moran’s I, an example of
such a pattern would be a checkerboard.
• Random: Map element classes are randomly distributed on the
map, as is indicated by a Moran’s I near zero, i.e., the distribution of regions with similar properties is unspecified/random in
geographic space.
Each type of pattern is associated with a description of the visual appearance of the map, and these spatial association patterns are typically
defined and tested using spatial autocorrelation. Spatial autocorrelation
is often used with p-value, z-score, and resampling methods to indicate
the significance level of the tendency of spatial clustering in a map.
Our goal is to adapt an indicator of spatial association to quantify the
visual change that may occur in a choropleth map as an element’s class
is altered.
Many indicators for spatial association exist (e.g., join count statistics [14], Geary’s C [19], Moran’s I [39], Getis-Ord General G [20]).

However, these statistics are all special cases of cross-product statistics [14,29]. Moran’s I [39] is perhaps the most well-known and widely
used measure of spatial autocorrelation. Moran’s I is defined as:
I=

Σi Σ j wi j (xi − X̄)(x j − X̄)
N
,
Σi Σ j wi j
Σi (xi − X̄)2

(1)

where N is the number of spatial units indexed by i and j, x is the
variable of interest, X̄ is the mean of x, and wi j is an element of a
matrix of spatial weights.
Unfortunately, Moran’s I is designed for continuous variables. Since
the visual appearance of the map relates solely to the final class, we
need a metric that can be applied to categorical data values. As such,
we modify the Moran’s I measure to provide a metric of spatial autocorrelation based on the class. To do this, we need to redefine the variables
in Equation (1). xi is now defined as a vector (c1 , c2 , . . . , cn ), where n
is the number of clusters and cn is a binary value, 0 or 1, such that if
element i belongs to cluster 1, then c1 = 1 otherwise, c1 = 0. Then X̄
will be the average of all vectors xi , and a modified global Moran’s I can
be calculated to evaluate the spatial association of classes. In this paper,
we utilize the Queen contiguity for defining the spatial weights matrix.
And wi, j = 1 for all Queen contiguous neighbors in our implementation.
While the choice of the spatial weights matrix will impact the calculation, the application is generalizable to any spatial weights choice.
Note that the change of the definition of Xi is due to the fact that we are
applying Moran’s I over the results of the map classification (instead
of using the statistical measure of a county, we are using the class of
the county). Since the county value is categorical, we cannot use the
continuous formulation for Xi . While Join counts are appropriate for
categorical data, our choice of using Moran’s I is due to the fact that it
is one of the most commonly used measures of spatial autocorrelation.
Furthermore, an application of join count would require treating the
K-color case as a binary case (1 color and K-1 colors). This would
result in multiple computations of the join count and added complexity
to the proposed methodology.
3.3

Categorizing the Effects of Reclassifying

Once a measure for the spatial association of the classes is defined,
the next step is to determine the cases in which altering a class will
impact the visual spatial association. We identify four potential spatial
arrangements for elements on a choropleth map, Fig. 3. Based on these
arrangements, we then define the value change in our modified Moran’s
I that would result in a change of the classification.
Case 1: The spatial unit under analysis, i, is spatially contiguous only
to units with different classes. The position of i in the classification space is such that it lies near the class boundary of one or
more spatially contiguous units. In this case, if i was reclassified,
the spatial association will increase. This is illustrated in Fig. 3

ZHANG AND MACIEJEWSKI: QUANTIFYING THE VISUAL IMPACT OF CLASSIFICATION BOUNDARIES IN CHOROPLETH MAPS375

Fig. 4. An example of adjacent changeable regions. Dashed lines represent the contiguity and solid lines represent the co-effect. Non-negative node
weight indicates that class k is reachable by i. From the initial states to the terminal states, three co-effect connections have been established.

(Case 1). Here, unit i is the red square and belongs to class 2.
This element lies near the boundary of class 2 and class 3. If
the class of i were to change from 2 to 3, an increase in visual
clustering could be observed and the spatial association value
would increase.
Case 2: The spatial unit under analysis, i, is spatially contiguous only
to units with different classes. The position of i in the classification space is such that it does not lie near the class boundary of
any spatially contiguous units. In this case, if i was reclassified,
there would be no change in the spatial association. This is illustrated in Fig. 3 (Case 2). Here, unit i is the red square and belongs
to class 3. None of its neighbors share the same class, thus i does
not add to any visual cluster. i lies on the boundary of class 3
and class 4; however, changing i’s class to 4 does not result in i
visually combining with other spatially contiguous regions, thus
there is no change in the spatial association metric.
Case 3: The spatial unit under analysis, i, is spatially contiguous to
some (or all) units that share the same class. The position of i in
the classification space is such that it does not lie near the class
boundary of any other spatially contiguous units. In this case,
if i was reclassified, the spatial association will decrease. This
is illustrated in Fig. 3 (Case 3). Here, unit i is the red square
and belongs to class 2. Several of its neighbors share the same
class, thus forming a small region that will visually appear to
be clustered. While i does lie near the boundary of class 2 and
class 4, there are no spatially contiguous elements belonging to
class 4. As such, if i were to be reclassified, the size of the region
containing elements belonging to class 2 would decrease, and no
other region in this scenario would add i to their spatial grouping.
As such, the visual clustering would decrease, resulting in a lower
spatial association value.
Case 4: The spatial unit under analysis, i, has a class that lies near a
classification boundary and is spatially contiguous to some units
that share the same class. The position of i in the classification
space is such that it does lie near the class boundary of other
spatially contiguous units. In this case, if i were to be reclassified,
the change in spatial association could be positive, negative, or
neutral dependent on the number of contiguous units (and their
contiguous units) that have the same class as i. This is illustrated
in Fig. 3 (Case 4). Here, unit i is the red square and belongs
to class 2. Several of its neighbors share the same class, thus
forming a small region that will visually appear to be clustered.
However, i lies on the boundary of class 2 and class 4 and is
spatially contiguous to other regions belonging to class 4. If i
were to be reclassified, the size of the region containing elements
belonging to class 2 would decrease; however, the size of the
region containing elements belong to class 4 would increase. As

such, the modified Moran’s I would need to be recalculated for
the entire map to determine the net change in spatial association.
While Cases 1-3 are straightforward to identify, Case 4 is perhaps
the more common case in choropleth map design. Thus, for a unit i
in Case 4, we define the number of regions that belong to the same
cluster as i in its surrounding area as pi . The number of regions that
belong to a cluster that i can change to in its surrounding area as qi .
The effect on the spatial association after i is changed is based on the
number of surrounding units that i can change to and is proportional
to qi − pi . Fig. 3 only considers the effect of a single changeable
unit, we extend this to more complex situations (Fig. 4(A)) in which
several contiguous regions could change, resulting in a cascade of
visual clustering patterns.
Theorem 1. If the potential changeable regions are not adjacent, then
their effects on the spatial association are separate/independent.
By inspection, one can observe that if spatial units that are identified
as being near class boundaries are non-adjacent, then the effect of
modifying their classes will be independent. This can be observed in
Equation (1) where units that are not adjacent will have an entry in
the spatial weights matrix wi j = 0 making the resulting calculations
independent from one another.
Once independence is established, we can identify all spatial units
that fall into Cases 1-4. Then, we can consider the situation where
several changeable regions are adjacent, meaning that a change of class
in one region will affect the visual clustering (i.e., the value of p and q)
of another changeable region. In this case, we have:
Theorem 2. The effect of the change (EOC) only depends on the initial
states and the terminal states of the changeable regions.
Thus, the measurement of spatial association remains the same as
long as the final states of those changeable regions stay the same. We
generalize the effect of the changes as:
EOCξ =

it & jt =

1

∑ (qi − pi ) + 2 ∑ ∑

i∈ξ







A

 

1 if it = jt
0 if it = jt

i∈ξ j∈ξ , j=i

wi j (it & jt − is & js ) ,

B



(2a)

(2b)

where ξ is the set of changeable units, qi , pi are similar to qi , pi but
exclude the other changeable units. wi, j is the spatial weight between
spatial units i and j. it and jt are the terminal states (classes) of regions
i and j respectively, and is and js are the starting classes of regions i
and j respectively. Here the effect of the changes can be broken into
the total separated effect caused by all of the changeable regions (Equation (2a) A) and the total co-effect among those changeable regions

376  	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,  VOL. 23,  NO. 1,  JANUARY 2017

Fig. 5. A k-means classification of US census variables illustrates boundary elements and their corresponding cases from Fig. 3. Here the Red
outline represents Case 1, the Black outline represents Case 2, the Green outline represents Case 3, and the Blue outline represents Case 4.

(Equation (2a) B). Note that the co-effect is divided by 2 because i and
j are symmetric and would double the effect.
We can maximize EOC in Equation (2a) to determine the set of
classes that will create the largest visual clustering in the map. This
problem can be solved by maximizing the modified Moran’s I in the
terminal class state of a unit. We note that this may not be a desirable
effect as this could introduce spurious patterns into the map; however,
elements near classification boundaries need to be inspected and the
EOC can be used as a metric for defining which elements could have the
largest potential change on the visual output (which is the overall goal
of this work). First, it is assumed that there exists a group of contiguous
spatial units that lie near classification boundaries (Fig. 4(A)). Each
unit can be altered to a certain class with a known weight. The weight
is set to be the number of neighboring units that share the same class. In
practice, for the class that a unit i cannot change to, the weight is set to
− ∑ j∈ξ , j=i wi, j (see the red circle in Fig. 4(C)). By setting the weight to
− ∑ j∈ξ , j=i wi, j , we neutralize the possible co-effects and guarantee that
a unit cannot change into an unreachable class. If the two adjacent units
have the same class, an edge will be established with a given weight.
For simplicity, the weight of the edge is unified to 1 when the spatial
weight wi, j is 1. Finally, this can be formulated as a maximization
problem where the nodes need to be classified such that the overall
weight of the nodes and edges is maximized. This can be further
defined as an integer linear programming (ILP) problem. Given a graph
G = (V, E) with n nodes and each node has m choices of classes, we
introduce binary variables xik (i = 1, . . . , n, and k = 1, . . . , m) to indicate
whether node i has been classified as class k. The weights cik ∈ R are
given for each xik , and variables ye , e ∈ E indicate whether edge e is
valid based on if its two nodes have been classified in the same class
(Fig. 4(D)). The resulting ILP can be formulated as:
max
s.t.

n

m

∑ ∑ cik xik + ∑ ye

i=1 k=1
m

(3a)

e∈E

∑ xik = 1

i = 1, . . . , n

(3b)

2ye − xik − x jk ≤ 0

e = (i, j) ∈ E, k = 1, . . . , m

(3c)

k=1

xik + x jk − ye ≤ 1
0 ≤ xik , ye ≤ 1
xik , ye ∈ Z .

e = (i, j) ∈ E, k = 1, . . . , m

(3d)
(3e)
(3f)

Here Equation (3c) constrains two nodes of a valid edge to be in the
same class and Equation (3d) constrains an invalid edge to not have

two nodes in the same class. By solving this ILP we can identify the
terminal states that maximize the Moran’s I, the same formulation can
also be used to minimize the Moran’s I. The problem of finding the maximum possible value is similar to the Maximum Edge-Weighted Clique
Problem (MEWCP) [37], which is a known NP-Hard problem. Therefore the problem of calculating the maximum EOC is also an NP-Hard
problem (i.e., there is no general solution that can find the optimized
value in polynomial time). Efficient algorithms for the MEWCP, such
as heuristics approximation, may be modified and applied to solve this
ILP. However, in practice, we find the number of adjacent nodes and
the number of class choices are relatively small (traditional choropleth
map design rules of thumb limit the number of classes to be less than
9). Thus, we implement a brute force solution to compute all possible
values in our framework. During this computation, our framework
stores the configuration of the classes that would maximize or minimize the current spatial association. Fig. 5 shows a multi-dimensional
classification of demographic data in the United States and we highlight
county boundaries based on their correspondence to the cases of Fig. 3
for illustrative purpose.
Note that for a connected component with n spatial regions and
m possible class choices, the time complexity is O(mn ). As such,
our computations are heavily dependent on the number of boundary
elements that have been identified. In practice, the largest dataset
we have tested is the county map for the continental United States,
which contains over 3100 spatial regions. A 6-class map classification
with Silhouette value between -0.4 to 0.1 identifies approximately 600
regions that lie on class boundaries. Among these units, there are
about 50 connected components and the largest connected component
contains 20 regions. Our experiments used an Intel Core I7-3630QM
Quadcore 2.40 Gz, and we found that the Min or Max EOC calculation
time was less than 1 second.
3.4

Unit Size as a Function of Visual Change

While our proposed metric for quantifying the impact of visual change
takes into account classes, perceptual studies have also shown that
the size of the map units is a primary driver behind the patterns that
users observe. As noted by Haklay [24], a thematic map created using
spatial units that vary in shape and size leads the user into thinking
that the larger areas are more significant because they have a bigger
visual impact than the smaller areas. Seonggook [38] proposed the
concept of gross change detection and verified that different spatial
distributions between two adjacent choropleth maps may lead users to
under- or over-estimate the gross change in the map, which implied
that the spatial distribution of change should be considered. As such,

ZHANG AND MACIEJEWSKI: QUANTIFYING THE VISUAL IMPACT OF CLASSIFICATION BOUNDARIES IN CHOROPLETH MAPS377

Fig. 7. Minimizing and maximizing the EOC of elements near the classification boundary using 2014 socio-economic data from the African
Development Bank. 1- Minimizing the EOC, thus creating more visual
heterogeneity. 2 - The initial k-means classification. 3 - Maximizing the
EOC, thus creating more visual spatial clustering.

Fig. 6. Minimizing and maximizing the EOC of elements near the classification boundary using criminal incident reports in Chicago, IL. 1 Minimizing the EOC, thus creating more visual heterogeneity, highlighted
elements are those near the classification boundary. 2 - The initial kmeans classification with only the changeable elements highlighted. 3 Maximizing the EOC, thus creating more visual spatial clustering.

the size of the region should be considered when quantifying the visual
impact of the classification changes. Goldsberry and Battersby [21]
introduced the magnitude of change (MOC) to quantify the graphical
change between choropleth map pairs for animated choropleth maps.
MOC is applicable to both object-oriented and pixel-based measures,
and we extend our EOC measure to consider the size of the map element
with a final metric for quantifying the impact of boundary effects on the
visual spatial association in choropleth maps. The metric is a simple
multiplication to derive the visual impact of changes (VIOC) and is
defined as:
si EOCi
V IOCξ = ∑ (
),
(4)
S
i∈ξ
where si is the area size of the ith region (in pixels) and S is the overall
area size of the map (in pixels). This accounts for the proportional
physical change of the choropleth map under different resolutions.
3.5

Summarizing the Visual Impact

Once these metrics are defined, we can now identify units on the map
that could potentially be modified to change the visual appearance of
spatial association. While there are methods for specifically identifying
statistically significant spatial associations on a map, the majority of
choropleth maps are presented with no underlying analysis of spatial
association. Instead, they are presented in the wild and left solely for
visual interpretation. By being able to quantify potentially spurious
elements on a map, new designs could be considered where the elements
could be blurred, highlighted or reclassified to another separate class in
order to try and insure that patterns being seen are what was intended by
the map designer (of course we recognize that the intent of the designer
could have been to mislead). Thus, our method could be summarized
into the following steps:
1. Choose a classification method and classify the dataset of interest
2. Calculate the silhouette value for all elements in the dataset
3. For all elements whose silhouette value is within a user defined
range τ calculate the EOC/ VIOC
4. Render the classified choropleth map and visually highlight all
units with a VIOC value within a user defined range γ

After the map is rendered, the designer can inspect the marked units,
create a map that will minimize or maximize the EOC/ VIOC, manually change units near classification boundaries to obtain the desired
rendering effect, or embed the EOC/ VIOC measures as uncertainty
information in the map design.
4

VALIDATION

In this section, we demonstrate by example that our proposed metrics
are able to identify map elements that lie near classification boundaries,
whereby a small change in the boundary would impact the perceived
visual spatial association.
4.1

Applying EOC for Visual Clustering

The first dataset used here is the Chicago crime data of 2014 (https:
//data.cityofchicago.org). There are 77 regions and 26 types of
crime variables in this dataset. For classification, k-means clustering
has been applied with k = 5 for three variables “Liquor Violation”,
“Sex Offense” and “Robbery”. The resulting classification and the PCA
scatterplot are shown in Fig. 6.2, and spatial units that may be near
the cluster boundary having an impact on the EOC are highlighted by
setting the silhouette value to the range of -.2 to .2. Fig. 6.1 shows the
results of minimizing the EOC to reduce the spatial clustering that is
visually observed, and Fig. 6.3 shows the results of maximizing the
EOC to increase the spatial clustering that is visually observed.
To further summarize, our modified Moran’s I in Fig. 6.2 (the initial
k-means clustering) is .14836. By shifting the classes as in Fig. 6.1,
Moran’s I can be reduced to .08185 and more dispersion in the regions
is seen. For example, the large purple region in the middle is dispersed
as the unit on the Purple/Blue boundary shifts to Blue. Such an effect
may be desirable in map design as this may help eliminate the potential
for users to identify spurious patterns if the result of the visualization
is designed to be as disperse as warranted by the data. Note the shift
of the convex hulls in the scatterplot as well when the units are reclassified. In Fig. 6.1, we see the Purple-Blue border now overlaps as
does the Red-Teal. Similarly, in Fig. 6.3, Moran’s I can be increased to
.20510 and we see larger red regions form in the North and South. The
Red-Teal border now overlaps in the scatterplot as well.
The most interesting boundary in the PCA projection is the Teal-Red
boundary. In the Teal group, measures of the three crimes are all quite
low; however, in the Red group, the data is clustered around mid-level
rates. Rates are normalized by the total count of crimes, and in the
Red group, liquor violations and sex offense have normalized values
ranging from .19 to .43 and .21 to .47 respectively. In the Teal group
these rates are 0 to .19 and 0 to .047 respectively with robbery rates
in both groups being less than .21. Thus, if one were to provide a
label to the Teal cluster, it could reasonably called the “low risk group”
and red could be a “mid-to-high risk group”. What we see in Fig. 6
is that there are regions in the North and South of Chicago with a
Teal unit surrounded by Red. When we maximize the EOC, the Red
clusters become visually larger indicating more areas in the “mid-tohigh risk group.” Given that the units that were changed are near the
classification border, the change from Teal to Red could be warranted,
and the designer’s goal could be to show that crime is a problem in

378  	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,  VOL. 23,  NO. 1,  JANUARY 2017

Fig. 8. Maximizing the EOC based on the VIOC near the classification boundary using US indices of industrial diversity from the western United
States. 1 - The PCA scatterplot for the initial k-means classification. 2 - A choropleth map of the k-means classification, highlighted elements are
those near the classification boundary. 3 - The VIOC measure of elements near the classification boundary. Darker elements will have a larger visual
impact if their class changes. 4 - Maximizing the EOC of all units near the classification boundary. 5 - Maximizing the EOC of all units with VIOC in
between .46 and 1.

Chicago and larger visual clusters could help sell that point. Again,
the goal of this work is not on the ethical implications of such design
choices, but the focus is on the fact that elements near classification
borders may need to be identified to capture a holistic picture of the
multivariate classification scheme. In fact, the elements that shift from
Teal to Red may be some of the most interesting elements as these
represent local outliers with the characteristics of more distant places.
While hiding these on the map may help motivate a story, the stronger
implication may be that such elements need to be highlighted to call
attention to the analyst.
To further demonstrate the impact of minimizing or maximizing
the EOC we explore 2014 socio-economic data from the African Development Bank Group (http://dataportal.afdb.org/Default.
aspx). We perform k-means clustering with k = 6 on seven variables:
the annual % of inflation; the central government’s fiscal balance as a
% of the GDP; the central government’s total revenue and grants as a %
of the GDP; the total outstanding debt as a % of the GDP; the real per
capita GDP growth rate as an annual %; the gross capital formation as
a % of the GDP, and; the real GDP growth as an annual %. Results of
the clustering are shown in Fig. 7. Fig. 7.2 highlights all the units that
can impact the EOC calculation. Fig. 7.1 is the result of minimizing
the EOC. When the EOC is minimized (resulting in less visual spatial
clustering), changes can be identified in Northeast Africa (the red region that was previously there has been dispersed), Northwest Africa,
and the South of Africa. Fig. 7.3 is the result of maximizing the EOC.
Here, more visual clustering can be observed particularly in the central
blue cluster now adding contiguous members.
4.2

Combining EOC and VIOC

In Fig. 6 and Fig. 7, what becomes obvious is that the size of the spatial
units plays a large role in the visual output. This is completely expected
as documented in the related work [24]. Thus, while Fig. 6 and Fig. 7
focus on highlighting all units that can change with the EOC measure,
the proposed VIOC measure can provide information about which units
can be changed and, if changed, will have the largest visual impact.
In this example, we explore measures of industrial diversity in the
Western United States using data from the US Census Bureau (http:
//quickfacts.census.gov/qfd/download_data.html). These
measures represent the relative concentration of industries for a given
spatial unit of interest at a particular point in time. Fig. 8 shows the
result of applying k-means clustering (k = 6) to the indices of healthcare (N62), finance and insurance (N52), and professional and science
services (N54). We use a silhouette value range of -.25 to .05 and
Fig. 8.1 and Fig. 8.2 show the result of the classification with the units
on a classification boundary highlighted. Note that while other units
may be initially highlighted with the silhouette coefficient, by using the
EOC case criteria we reduce the highlights to only those units that will
have a visual impact on the map. Note that there are approximately 30
counties highlighted on the map and we want to explore which units
will have the most visual impact. We use a sequential color scheme to

shade the highlighted units based on their VIOC measurement, which
is directly proportional to the percent of the screen space that the spatial
unit occupies. The result is shown in Fig. 8.3. As expected, the larger
the county, the darker the highlighting. The reason we show this is that
by simply applying silhouette filtering and EOC metrics to highlight
the boundary regions, many units will be selected. If we only want
to focus on the most visually salient units, the units could be further
filtered based on their VIOC values. In Fig. 8.5, we set a VIOC range
from .46 to 1.0 leaving only 7 of the initial 30 counties highlighted. We
then modify the classification to maximize EOC. Filtering by VIOC can
be thought of as another tool for the map designers’ toolbox in which
they can consider modifications to classes and boundaries. However,
it is important to note that filtering only by size may limit the overall
design space. In Fig. 8.5, we can see that in the Northeast region, the
small orange county that was reclassified in Fig. 8.4 is now unchanged
during the maximization of the VIOC. While the size is small, the
placement creates a very strong visual break (a hole) in the cluster. As
such, measures that include shape, size, distance and contiguity for
filtering should also be explored as future work.
5

R ECLASSIFICATION

VERSUS

B OUNDARY M ODIFICATION

Throughout the discussion, we have primarily discussed the impact of
reclassifying elements that are on classification boundaries; however,
simply reclassifying an element may not be the most appropriate means
of adjusting the classification. In multivariate schemes, such as kmeans, recent work has focused on incorporating user feedback into
the classification model [11]. Thus, if a user changes an element
class, the classification model will update the weights and reassign the
classification boundaries. Recent work on this topic was discussed in
Sect. 2.4, and we extend our work to incorporate a modification for
flexible direct manipulation.
Fig. 1 to Fig. 8 rely on what is known as result manipulation, which
means the modification will only effect on the class index of the user
selected units. Model manipulation, on the other hand, will affect the
weights in the clustering processing and eventually the classes of other
data points. Each element in the dataset will have an associated weight
that can be modified through user interaction. Suppose there are n
units u1 , u2 , . . . , un and their weights are formed as w1 , w2 , . . . , wn , such
that initially each spatial unit ui will have the same instance weight
wi = 1 influencing the placement of the centroids. After the initial
clustering, analysts may assign unit ui to specific cluster C j , then ui ’s
weight will be modified to be either based on the cluster C j ’s size
√
s j such as wi = 1 + s j or a predefined constant value larger than 1.
During each iteration, ui ’s proximity to C j ’s centroid c j is increased
by multiplying that weight wi . Thus ui is more likely to be assigned to
cluster C j . When calculating the cluster centroid, ui will only contribute
its weight wi to the cluster it belongs to. The new centroid of C j will
be c j = ΣΣi∈Ji∈Jwwi ui i where J is the set of units that have been assigned to
C j . Eventually, this result in cluster C j ’s centroid c j moves towards ui
and this unit will likely belong to that cluster.

ZHANG AND MACIEJEWSKI: QUANTIFYING THE VISUAL IMPACT OF CLASSIFICATION BOUNDARIES IN CHOROPLETH MAPS379

Fig. 9. The effects of model manipulation on choropleth map classification. 1 - A k-means classification of criminal incident reports in Chicago, IL. 2 Maximizing the EOC through result manipulation (i.e., changing the unit classification does not affect the k-means weight). 3 - Maximizing the EOC
through model manipulation (i.e., changing the unit classification updates the k-means weights). 4 - The difference between result manipulation and
model manipulation. Note that units that were not originally highlighted as being near the classification boundary are now reclassified due to the
updated weights used in k-means.

We can also apply a model manipulation scheme. First, we identify
elements to reclassify using the silhouette range τ. We can then maximize (or minimize) the EOC that forces the reclassifying of elements.
This reclassifying will automatically update the weights of the k-means
clustering, and a new classification based on the updated weights will
be generated. This result is shown in Fig. 9. Here, we revisit the data
and classification scheme applied in Fig. 6 (the Chicago crime data).
Fig. 9.1 and Fig. 9.2 are the same k-means and maximized EOC
results from Fig. 6.1 and Fig. 6.4 respectively. What is interesting is that
by changing the k-means weights, the classification boundaries shift and
units that were not marked as boundary candidates are now subsumed
by a new class. In Fig. 9.3, the same units that were reclassified in
Fig. 9.2 are reclassified in Fig. 9.3. This causes the weights in the
k-means clustering to update, and then a new k-means classification
is performed, resulting in the map classification of Fig. 9.3. If we
take a difference between Fig. 9.2 and Fig. 9.3, we can see what other
units were shifted as a result of updating the weights of the k-means
classification (Fig. 9.4), and we notice that an even larger amount of
visual spatial clustering can be seen in Fig. 9.3 than in Fig. 9.2.
6 C ONCLUSION AND F UTURE W ORK
As previously stated, a critical step in designing choropleth maps is
the choice of classification method. How a map is classified directly
impacts the resultant visual output and can lead to misinformation
about the underlying data. In order to assess the visual impact of
such choices, we have developed a methodology for quantifying the
visual impact of adjusting classification boundaries in a choropleth map
and present a scheme for maximizing or minimizing the amount of
visual clustering present in the map and demonstrated the results using
several datasets. What is important to note is that the goal of choosing
classification boundaries is to achieve a reasonable split in the data, and
this is often left up to the designer. By providing designers with new
ways to assess the visual impact of small classification changes, the
designer can further refine and assess their map message.
Perhaps the most intriguing part of our results is the output when
applying a reweighting scheme to the classification. While the reweighting of units in Fig. 9 resulted in more changes (and arguably more visual
clustering) than a naı̈ve reclassifying of elements, it is likely that this
reweighting could also introduce a reduction of visual clustering. Future research should explore the sensitivity of such an application across
various clustering schemes. Furthermore, research into what pattern
changes result in an increased perception of visual clustering should
also be undertaken. In this paper, we rely solely on the fact that colors
are changing and regions are becoming larger. Past research [24] has

shown that the larger a patch of color becomes in a choropleth map, the
more likely that it will be identified as a cluster. However, there may be
particular patches that could be changed that may have a greater impact
on the perception. Of course, the size of the spatial unit matters a great
deal, but what if changing the classification of a spatial unit fills in a
donut hole? Is this perceived as resulting in more spatial clustering than
if we change a spatial unit’s classification such that it just adds to the
edge of the donut? What if a spatial unit acts as a bridge? For example,
if there are two spatial groupings with the same class separated by a
narrow band of other classes, how is the spatial clustering perceived if
one unit is changed to create a bridge? Understanding the impact of
these patterns would allow us to computationally identify them and use
these types of patterns to create a more perceptually rigorous VIOC
metric. Future work will focus on the use of such metrics for highlighting uncertainty within the map, as well as exploring boundary elements
with respect to statistical measures of spatial clustering. Specifically,
if a region is found to be a statistically significantly spatially cluster,
should boundary elements contiguous to this region be adjusted to
highlight the significance?
While our study presents an initial methodology, there is a wide
range of future research that needs to be undertaken to further explore
the visual impact of classification boundaries in choropleth maps. First,
the class stability metric applied here was designed to be relatively
agnostic to the underlying clustering algorithm used; however, different
clustering algorithms have different properties and constraints. For
example, in the k-means approach showcased here, the choice of seed
points may greatly impact the boundary elements, as would the choice
of single-linkage versus complete-linkage in hierarchical clustering. It
is likely that clustering specific methods might better capture the underlying uncertainties associated with the resulting visual representation.
Second, our VIOC and EOC measures focused only on neighboring
spatial associations and size. Future work could explore the resultant
shape of a cluster or the area of an element as well as metrics that take
spatial distances into account too. Finally, this research points to the
need for a comprehensive study on this issue where the underlying data
distributions vary along with the proposed clustering algorithm in order
to generate comprehensive guidelines for how to approach boundary
issues given a known data distribution and clustering methodology.
ACKNOWLEDGMENTS
We would like to thank the reviewers for their suggestions that greatly
improved the manuscript as well as Michael Steptoe for his work in
video and demo production. This work was supported by the NSF
under Grant No. 1350573.

380  	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,  VOL. 23,  NO. 1,  JANUARY 2017

R EFERENCES
[1] G. Andrienko and N. Andrienko. Interactive maps for visual data exploration. International Journal of Geographical Information Science,
13(4):355–374, 1999.
[2] G. Andrienko, N. Andrienko, S. Rinzivillo, M. Nanni, D. Pedreschi, and
F. Giannotti. Interactive visual clustering of large collections of trajectories.
In IEEE Symposium on Visual Analytics Science and Technology, pp. 3–10.
IEEE, 2009.
[3] G. Andrienko, N. Andrienko, and A. Savinov. Choropleth maps: classification revisited. In Proceedings of ICC, pp. 6–10, 2001.
[4] L. Anselin. Local indicators of spatial association - LISA. Geographical
Analysis, 27(2):93–115, 1995.
[5] M. P. Armstrong, N. Xiao, and D. A. Bennett. Using genetic algorithms
to create multicriteria class intervals for choropleth maps. Annals of the
Association of American Geographers, 93(3):595–623, 2003.
[6] D. I. Ashby and P. A. Longley. Geocomputation, geodemographics and
resource allocation for local policing. Transactions in GIS, 9(1):53–72,
2005.
[7] H. G. Basara and M. Yuan. Community health assessment using selforganizing maps and geographic information systems. International journal of health geographics, 7(1):1, 2008.
[8] A. Ben-Hur, D. Horn, H. T. Siegelmann, and V. Vapnik. Support vector
clustering. The Journal of Machine Learning Research, 2:125–137, 2002.
[9] B. Boots. Developing local measures of spatial association for categorical
data. Journal of Geographical Systems, 5(2):139–160, 2003.
[10] C. A. Brewer and L. Pickle. Evaluation of methods for classifying epidemiological data on choropleth maps in series. Annals of the Association
of American Geographers, 92(4):662–681, 2002.
[11] K. Chen and L. Liu. VISTA: Validating and refining clusters via visualization. Information Visualization, 3(4):257–270, 2004.
[12] K. Chen and L. Liu. iVIBRATE: Interactive visualization-based framework
for clustering large datasets. ACM Transactions on Information Systems
(TOIS), 24(2):245–294, 2006.
[13] J. Cheshire, P. Mateos, and P. A. Longley. Delineating europe’s cultural
regions: Population structure and surname clustering. Human Biology,
83(5):573–598, 2011.
[14] A. D. Cliff and K. Ord. Spatial autocorrelation: A review of existing
and new measures with applications. Economic Geography, 46:269–292,
1970.
[15] E. Cromley and R. Cromley. An analysis of alternative classification
schemes for medical atlas mapping. European Journal of Cancer,
32(9):1551–1559, 1996.
[16] M. F. Dacey. A review on measures of contiguity for two and k-color maps.
Dept. of Geography, Northwestern University, Evanston, Illinois, 1965.
[17] S. L. Egbert and T. A. Slocum. EXPLOREMAP: An exploration system
for choropleth maps. Annals of the Association of American Geographers,
82(2):275–288, 1992.
[18] I. S. Evans. The selection of class intervals. Transactions of the Institute
of British Geographers, pp. 98–124, 1977.
[19] R. Geary. The contiguity ratio and statistical mapping. The incorporated
statistician, 5(3):115–146, 1954.
[20] A. Getis and J. K. Ord. The analysis of spatial association by use of
distance statistics. Geographical Analysis, 24(3):189–206, 1992.
[21] K. Goldsberry and S. Battersby. Issues of change detection in animated
choropleth maps. Cartographica: The International Journal for Geographic Information and Geovisualization, 44(3):201–215, 2009.
[22] D. Guo. Regionalization with dynamically constrained agglomerative
clustering and partitioning (redcap). International Journal of Geographical
Information Science, 22(7):801–823, 2008.
[23] D. Guo, J. Chen, A. M. MacEachren, and K. Liao. A visualization system
for space-time and multivariate patterns (VIS-STAMP). IEEE Transactions
on Visualization and Computer Graphics, 12(6):1461–1474, Nov. 2006.
[24] M. Haklay. Interacting with geospatial technologies. Wiley Online Library,
2010.
[25] W. W. Hargrove and F. M. Hoffman. Potential of multivariate quantitative
methods for delineation and visualization of ecoregions. Environmental
Management, 34:S39–S60, 2005.
[26] S. Haroz and D. Whitney. How capacity limits of attention influence information visualization effectiveness. IEEE Transactions on Visualization
and Computer Graphics, 18(12):2402–2410, 2012.
[27] R. Harris, P. Sleight, and R. Webber. Geodemographics, GIS and neighbourhood targeting, vol. 7. John Wiley and Sons, 2005.

[28] J. A. Hartigan and M. A. Wong. Algorithm AS 136: A k-means clustering
algorithm. Applied statistics, pp. 100–108, 1979.
[29] L. J. Hubert, R. G. Golledge, and C. M. Costanzo. Generalized procedures
for evaluating spatial autocorrelation. Geographical Analysis, 13(3):224–
233, 1981.
[30] G. F. Jenks. The data model concept in statistical mapping. International
yearbook of cartography, 7(1):186–190, 1967.
[31] A. Klippel, F. Hardisty, and R. Li. Interpreting spatial patterns: An inquiry
into formal and cognitive aspects of tobler’s first law of geography. Annals
of the Association of American Geographers, 101(5):1011–1031, 2011.
[32] J. Krygier and D. Wood. Making maps: A visual guide to map design for
GIS. Guilford Press, 2011.
[33] R. Lloyd and T. Steinke. Visual and statistical comparison of choropleth
maps. Annals of the Association of American Geographers, 67(3):429–436,
1977.
[34] R. E. Lloyd and T. Steinke. The decisionmaking process for judging the
similarity of choropleth maps. The American Cartographer, 3(2):177–184,
1976.
[35] P. Longley. Geographic information systems and science. John Wiley &
Sons, 2005.
[36] P. A. Longley. Geodemographics and the practices of geographic information science. International Journal of Geographical Information Science,
26(12):2227–2237, 2012.
[37] E. M. Macambira and C. C. de Souza. The edge-weighted clique problem:
Valid inequalities, facets and polyhedral computations. European Journal
of Operational Research, 123(2):346–371, 2000.
[38] S. Moon, E.-K. Kim, and C.-S. Hwang. Effects of spatial distribution
on change detection in animated choropleth maps. Journal of the Korean Society of Surveying, Geodesy, Photogrammetry and Cartography,
32(6):571–580, 2014.
[39] P. A. Moran. Notes on continuous stochastic phenomena. Biometrika, pp.
17–23, 1950.
[40] T. Nakaya, A. S. Fotheringham, C. Brunsdon, and M. Charlton. Geographically weighted poisson regression for disease association mapping.
Statistics in medicine, 24(17):2695–2717, 2005.
[41] J. Olson. The Effects of Class Interval Systems on the Visual Correlation
of Choropleth Maps. PhD thesis, University of Wisconsin–Madison, 1970.
[42] J. M. Olson. Autocorrelation and visual map complexity. Annals of the
Association of American Geographers, 65(2):189–204, 1975.
[43] M. Polczynski and M. Polczynski. Using the k-means clustering algorithm
to classify features for choropleth maps. Cartographica: The International
Journal for Geographic Information and Geovisualization, 49(1):69–75,
2014.
[44] P. J. Rousseeuw. Silhouettes: A graphical aid to the interpretation and
validation of cluster analysis. Journal of computational and applied
mathematics, 20:53–65, 1987.
[45] M. W. Scripter. Nested-means map classes for statistical maps. Annals of
the Association of American Geographers, 60(2):385–392, 1970.
[46] A. D. Singleton and P. A. Longley. Geodemographics, visualisation, and
social networks in applied geography. Applied Geography, 29(3):289–298,
2009.
[47] A. Slingsby, J. Dykes, and J. Wood. Exploring uncertainty in geodemographics with interactive graphics. IEEE Transactions on Visualization
and Computer Graphics, 17(12):2545–2554, 2011.
[48] M. Sun, D. Wong, and B. Kronenfeld. A heuristic multi-criteria classification approach incorporating data quality information for choropleth
mapping. Cartography and Geographic Information Science, pp. 1–13,
2016.
[49] D. Vickers and P. Rees. Creating the UK National Statistics 2001 output
area classification. Journal of the Royal Statistical Society: Series A
(Statistics in Society), 170(2):379–403, 2007.
[50] C. Xia, W. Hsu, M. L. Lee, and B. C. Ooi. Border: Efficient computation of
boundary points. IEEE Transactions on Knowledge and Data Engineering,
18(3):289–303, 2006.
[51] N. Xiao and M. P. Armstrong. Supporting the comparison of choropleth
maps using an evolutionary algorithm. Cartography and Geographic
Information Science, 32(4):347–358, 2005.
[52] Y. Zhang, W. Luo, E. A. Mack, and R. Maciejewski. Visualizing the
impact of geographical variations on multivariate clustering. Computer
Graphics Forum, 35(3):101–110, 2016.

DOI: 10.1111/j.1467-8659.2012.03117.x
Eurographics Conference on Visualization (EuroVis) 2012
S. Bruckner, S. Miksch, and H. Pfister
(Guest Editors)

Volume 31 (2012), Number 3

MarketAnalyzer: An Interactive Visual Analytics System for
Analyzing Competitive Advantage Using Point of Sale Data
S. Ko†1 , R. Maciejewski‡2 , Y. Jang§3 , and D. S. Ebert¶1
1 Purdue

University
State University
3 Sejong University

2 Arizona

Abstract
Competitive intelligence is a systematic approach for gathering, analyzing, and managing information to make informed business decisions. Many companies use competitive intelligence to identify risks and opportunities within
markets. Point of sale data that retailers share with vendors is of critical importance in developing competitive
intelligence. However, existing tools do not easily enable the analysis of such large and complex data. therefore,
new approaches are needed in order to facilitate better analysis and decision making. In this paper, we present
MarketAnalyzer, an interactive visual analytics system designed to allow vendors to increase their competitive
intelligence. MarketAnalyzer utilizes pixel-based matrices to present sale data, trends, and market share growths
of products of the entire market within a single display. These matrices are augmented by advanced underlying
analytical methods to enable the quick evaluation of growth and risk within market sectors. Furthermore, our
system enables the aggregation of point of sale data in geographical views that provide analysts with the ability
to explore the impact of regional demographics and trends. Additionally, overview and detailed information is
provided through a series of coordinated multiple views. In order to demonstrate the effectiveness of our system,
we provide two use-case scenarios as well as feedback from market analysts.
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Pixel-based visualization, Linked Views, Geospatial, Temporal, Multi-variate, Business Intelligence, Competitive Intelligence, Market
Analysis—

1. Introduction
The underlying goal of a business is to increase (or at least
maintain) its current market share and to maximize its profits
within the market. In order to pursue this goal, analysts must
constantly explore and analyze market share data changes
that are relevant to their current business sector. Their goal
is to forecast changes in the market as a means of controlling and expanding the company’s current market share. This
exploration, analysis, and prediction of the market share is
termed competitive intelligence (CI) [Kah98]. Companies
use CI to compare themselves to other companies, to identify
market risks and opportunities and to evaluate the potential
impact of new sales strategies.

†
‡
§
¶

ko@purdue.edu
rmacieje@asu.edu
jangy@sejong.edu: corresponding author
ebertd@purdue.edu

c 2012 The Author(s)

c 2012 The Eurographics Association and Blackwell PublishComputer Graphics Forum 
ing Ltd. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ,
UK and 350 Main Street, Malden, MA 02148, USA.

In order to generate intelligence reports, many companies extract information from a variety of sources using various methods of data collection and analysis (e.g., networking with company rivals, examining security filings, patent
application analysis). One key data source is point of sale
data that retailers share with vendors. This point of sale
data is temporal, multivariate, and spatial in nature; therefore, it is well suited for analysis in a visual analytics environment. However, it is difficult to find systems that manage the characteristics of point of sale data effectively. In
this paper, we present MarketAnalyzer, a visual analytics
system for exploring, comparing, analyzing, and predicting
trends of point of sale data. We have worked directly with
analysts to provide proper and accurate analysis of their
point of sale data (e.g., 288 stores with 36 different products) to increase their understanding and improve their market insight. We use an enhanced pixel-based visualization
approach [KK94, Kei00] in MarketAnalyzer to efficiently
utilize limited screen space for the large store and product

1246

Ko et al. / MarketAnalyzer

information. Our system allows analysts to explore current
sales volume, trend, and temporal market share growth rates
using a series of linked views including pixel-based visualization matrices, line graphs, stacked bar graphs, and choropleth maps.

block resolutions. Oelke et al. [OJS∗ 11] studies visual boosting techniques for pixel-based visualization such as halos and distortion. Ziegler et al. [ZNK08] presents how
the pixel-based visualization helps analysts gain insight for
long-term investments.

MarketAnalyzer has several benefits compared to other
tools for performing market analysis tasks. MarketAnalyzer
enables analysts to investigate the status of the market by
observing all the characteristics of point of sale data at
the same time. In addition, the status of the competition
in point of sales, trends and growth rates is projected onto
a map for regional market analysis. MarketAnalyzer provides forecasts for both individual products and different
stores utilizing statistical models, such as linear trend estimation [DS98] and ARIMA (Auto-Regressive Integrated
Moving Average) [BJ76]. In order to reduce the perceptual difficulties inherent in pixel-based visualizations, a local
magnification lens is also provided for focus + context analysis. Additionally, CUSUM [Pag54] and normalized trend
filtering are provided for data filtering. For evaluation, we
provide two case studies that describe how sales data can
be analyzed with MarketAnalyzer. Although the case studies
are business domain specific, it is easy to extend our system
to other multivariate, spatial, temporal datasets such as property sales, crime and disease data to provide comparisons,
insight, and new intuition.

Many systems have been developed for visually exploring multivariate data (e.g., Xmdv [War94], Spotfire [Ahl96],
XGobi [SCB98], GGobi [SLBC03], Comvis [MFGH08],
Polaris [STH08], Tableau [Tab]). Common amongst these
systems is the extensive use of interactive techniques (brushing, linking, zooming, filtering) to refine the user’s queries.
However, such systems often do not support market forecasting or geographical analysis. In discussing design strategies
with our market analysts, it was noted that forecasting future trends and understanding outperforming geographical
locations are important in market analysis. Of the systems
previously listed, Tableau software [Tab] allows analysts to
easily access and analyze their data by offering flexible operations. Although multivariate and time-series data analysis
is possible in the tool, comparison between multivariate attributes with geographical information is not well supported
by Tableau. In MarketAnalyzer, all attributes of the data is
visualized in multiple linked views for simultaneous comparison, and analysts can investigate future sale trends based
on statistical models and market share growth rates.

2. Related Work
Traditionally various tasks, such as discovering market
trends and predicting future prices of assets have been addressed with charts and line graphs in the financial data
domain [Mur99, EM01]. While charts and line graphs provide useful visualizations of univariate data, they quickly
become clutter as new dimensions are added. Analysts
often use tree map visualizations to represent the market [Sma, VvWvdL06]. Unfortunately, these maps only provide a snapshot of the current market value whereas analysts
often wish to explore short or long term trajectories within
the market [TA03, STKF07].
In order to display the maximum amount of data relative to the screen space, Keim et al. introduce pixelbased [OJS∗ 11] or pixel-oriented [KK94, Kei00] visualization techniques. In these techniques, each data element is
assigned to a pixel. Then, a predefined color map is used to
shade the pixel to represent the range of the data attribute.
Thus, the amount of information in the visualization is theoretically limited only by the resolution of the screen. In the
context of our work, pixel-based visualizations are visualizations that utilize small areas of the screen to encode one
data item. Note that the areas may not necessarily be pixels,
as the use of small rectangles also falls under the accepted
classification of pixel-based visualizations [OJS∗ 11].
Borgo et al. [BPC∗ 10] present how the usability of the
pixel-based visualization varies over different tasks and

3. Visual Analytic Environment
Analysts tend to easily understand competition within a market and quickly draw conclusions when maximal information is presented. In order to present the most information for
analysis, coordinated and multiple linked views have been
used in various applications [SFOL04, WFR∗ 07, CGK∗ 07,
Rob07, SGL08]. In this work, we also employ coordinated
multiple linked views to visualize attributes from point of
sale data. Figure 1 shows how MarketAnalyzer provides
complete information in multiple linked views. Note that all
color maps are chosen to fit perceptually with the data being analyzed. Both sequential and divergent color maps from
ColorBrewer (which have been previously tested and evaluated [HB03]) are used. Sequential color scales are chosen to
show ordered data, while divergent color maps are chosen to
show differences in data values with respect to some point
of interest.
Companies are displayed in a selectable list in (a), stores
in (b), and products in (c). Note that in window (b), there
are two selectable lists. The leftmost store list is used to select multiple stores for computing the sales average, while
the rightmost list is used to select a single store, whose sales
will be compared to the computed sales average. This single
store selection is the anchor for view (h) and (i) that shows
the sales of the primary company (h) or competitor company
(i). In (h) and (i), the selected store’s sales of the products
chosen in the list view (c) are plotted in green, and the average sales of the products across a group of stores selected
c 2012 The Author(s)

c 2012 The Eurographics Association and Blackwell Publishing Ltd.


1247

Ko et al. / MarketAnalyzer

(h)

(l)

(i)

Total sales

Trends
(m)

(f)
P1
P2
P3
P4
P5

Growth Rate

(j)

(k)

M6
M1

Forecasts

Figure 1: The MarketAnalyzer interface. MarketAnalyzer consists of multiple coordinated views linked with interactive filters:
(a) Company filter, (b) Store filter, (c) Products filter, (d) Legend view, (e) (Sorted) Matrix view for sales, trends, and growth
rates. (f) Stacked bar view, (g) Geographical view, (h) and (i) Line graph small multiples views, (j) and (k) Time slider widgets
and aggregation tools for temporal comparison. (l) Tooltip. (m) Filter. In the legends, the blue indicates positive and the red
represents negative measurements in sales, trends, or growth rates.
in the leftmost list of (b) are plotted in purple. At the end
of the line graph, a four week forecast for sales based on an
ARIMA model is plotted as a blue line bounded by two red
dotted lines representing the upper and lower error bounds.
Note that the ARIMA forecasts are calculated in R [R D06],
which is integrated directly into our system.
3.1. Pixel-Oriented Display Matrix
A fundamental challenge in the visualization of large multivariate data is that screen space limits the amount of information that can be simultaneously presented to a user.
This scalability problem causes various difficulties in analysis, such as inefficiency in comparison and tedious jumping back and forth to adjust different parameters. In order
to alleviate this problem, we incorporate a pixel-based visualization [KK94, Kei00] that is effective when the screen
space is limited. Sales, trends, and market share growth rates
for stores and products (e.g., 288 stores, 36 products) are effectively presented in MarketAnalyzer as shown in Figure 1
(e). Note that we place different stores and products in different columns and rows, respectively. The matrix view in
(e) consists of three views: sales, trends, and growth views.
Each view has two matrices for a primary company and its
c 2012 The Author(s)

c 2012 The Eurographics Association and Blackwell Publishing Ltd.


competitor that are chosen in Figure 1 (a) by users. We place
small squares side-by-side in each matrix with all matrices
positioned vertically. The rationale behind this arrangement
is that its conceptual simplicity makes the comparison and
discovery of trends easier. Also comparing data side-by-side
is more efficient than jumping back and forth and memorizing previously shown data, based on the principle of small
multiples [Tuf90].
In the sales view, we use Equation 1 to define the sales
(Si, j ) for each square as a sum of the sales during the userselected time interval.
n

Si, j =

∑ Sales(t, i, j),

(1)

t=m

where i and j indicate the ith row (product) and the jth column (store) while m and n are the first and the last month in
the time interval. The darker the blue, the more units are being sold. One frequent question that decision-makers might
have is "Are sales increasing in this specific time period?".
In order to answer this question, we present the user with
a trend view in which the slope (variable b in Equation 2)
of our linear trend estimation is visualized using a divergent
color scale. Positive slopes (indicating that sales are trend-

1248

Ko et al. / MarketAnalyzer

Figure 2: (top-left) Magnification is applied for detailed comparison, (top-right) the CUSUM filtering method with the strict
option is applied on Feb 2010, (bottom-left) SimulSort is applied to the sales view, (bottom-right) Proportional legend.
ing upwards) are mapped towards the blue values, negative
slopes (indicating sales are trending downward) towards the
red values, with a slope of zero being white (indicating that
sales are stable).
y = a + bx,

(2)

where x, y are variables, a is the intercept point of the regression line and the y axis, b is the slope from the linear
trend estimation for a certain time interval. b is computed in
Equation 3,
n

n

n

n( ∑ XmYm ) − ( ∑ Xm )( ∑ Ym )
b=

m=1

m=1
m=1
n
2
n(
Xm ) − (
Ym )2
m=1
m=1
n

∑

(3)

∑

Here, n represents the number of months in the specified
time interval, Ym is the index of each month (e.g., 1, 2, ...,
n), and Xm is the sales for the month.
Along with sales trends, we also define the growth rate of
a business using Equation 4.
Gi, j =

Si, j,cur − Si, j,past
× 100,
Si, j,past

(4)

where Si, j,cur is defined as the sum of the sales between the
first (M f ) and the last (Ml ) month using the red time slider
(Figure 1-(k)), as shown in Equation 5.
Ml

Si, j,cur =

∑

Sales(Tred , i, j).

(5)

Tred =M f

Si, j,past is similarly defined for a past time interval (Tblue )
from the blue time slider (Figure 1-(j)). A divergent color
scale is applied to represent the growth rate. When Si, j,past is
zero, the growth rate is set to the maximum rate in the range.
This indicates a case where a product was not supplied at the
selected time but was provided later at the time specified by
the red time slider.
We calculate the competitive advantage for the primary
company (CPi, j ) using Equation 6 when the analyst activates
the comparative mode, as shown in Figure 5.
CPi, j = MPi, j − MCi, j ,

(6)

where MPi, j is a measurement of the ith product from the
jth store for the primary company and MCi, j is that of the
competitor. The measurement can be any input data attribute,
such as sales, trends, and growth rates used in these examples. This calculates a new matrix representing the competitive advantage of the primary company. Analysts should be
alerted about a red row or a red column because it explicitly
indicates the primary company is losing sales in comparison
to its competitor as shown in Figure 5.
In order to provide more detailed information, we use a
tooltip and magnification lens. A tooltip, as shown in Figure 1 (l), provides numerical information of the product in
the store where a mouse is hovering. Squares are gray when
no product is supplied to the stores. In order to help recognition and comparison, MarketAnalyzer provides a magnification lens as shown in Figure 2 (top-left).
We reorder rows in the matrix to emphasize the importance of data where squares near the top-left locations are
more important than squares near the bottom-right. For the
sales matrix, rows and columns are sorted based on sales
sums. Products (rows) are sorted by the sales sum across the
entire stores and then stores (columns) are re-sorted by the
sales sum across the entire products. The topmost row indicates the top-selling product, and the leftmost column represents the top-selling store in the market as shown in Figure 1
(e). Note that matrices in the trend and growth views are also
sorted corresponding to the sorting of the sales matrix. Negative measurements in trends and growth rates represent adverse situations and analysts want to find out the adverseness
promptly in their analysis. In this case we use adverseness
for the importance.
We also incorporate SimulSort [HY09] to sort columns
independently as an auxiliary approach. In each store, products are sorted by sales. Thus, squares with higher sales tend
to go upward in matrices. This enables analysts to quickly
evaluate sales performance of products in a store. In order
to see the performance of a product across all stores, MarketAnalyzer highlights the product in orange for all stores
as presented in Figure 2 (bottom-left) where the sales performance of the product is good in most stores with some
minor variations.
c 2012 The Author(s)

c 2012 The Eurographics Association and Blackwell Publishing Ltd.


1249

Ko et al. / MarketAnalyzer

Forecasts

(a) Stacked bar view

(b) Geographical view

Figure 3: (a) The stacked bars represent trends in individual products. Analysts can see sudden increase in sales of product
P3 in May. Note that M1 and M6 indicate company names. (b) Competitive advantages in February–May 2010 are linked on
the maps to represent regional competitions for sales (left), trends (middle), and growth rates (right) compared to the past time
interval, March–June 2009.
3.2. Filtering Methods
MarketAnalyzer helps analysts filter out uninteresting stores
by providing two filtering methods: the cumulative summation method (CUSUM) [Pag54] applied to sales and the
trend filtering method based on the computed trends. The
CUSUM filtering method has two modes: default and strict.
In the default mode, the CUSUM method in MarketAnalyzer
filters out stores during the visualization process that have a
negative Sst,n from Equation 7.
n

Sst,n =

∑ (Xst,m − µst )

(7)

m=1

Here, Sst,n is the CUSUM of the st store in the nth month
(where the nth month is the first month of the red time slider).
Xst,m is the sum of all products of store st at the mth month
(where the mth month is started from the first month of our
data set–January 2009). µst is the mean sales of st over the
time period n-m. The CUSUM method can also be changed
from the default parameter to a strict mode. In the strict
mode, stores are filtered out only when the CUSUM of the
nth month and the n − 1th month are negative. If the CUSUM
of the nth month is negative but that of the n − 1th month is
positive, then the store will still be visualized.
When the trend filtering method is used instead of
CUSUM, MarketAnlayzer maps trends from -1 to 1. Then,
MarketAnalyzer visualizes stores whose trends are below
the threshold specified in the filter (Figure 1 (m)). The normalized trend Nst for store st is computed in Equation 8.
Nst =

Tst − Tmean
√
Tvar

(8)

Here, Tst is the sum of the trends of all products in the store
st, Tmean and Tvar are the mean and variance of trends across
all stores. Then, Nst is mapped to Mst in Equation 9.


Nst − Nmin
− 0.5 ∗ 2,
(9)
Mst =
Nmax − Nmin
where Nmax and Nmin are the maximum and minimum in normalized trends.
3.3. Proportional Legends
The legend view in Figure 1 (d) provides numeric and color
map information for each view. The scale in MarketAnac 2012 The Author(s)

c 2012 The Eurographics Association and Blackwell Publishing Ltd.


lyzer can be adjusted to have the denominator represent either measurements (sales, trends, and growth rates) at the
local level (comparing measurements within a company) or
at the global level (comparing measurements across the primary company and its competitor). In the local scale mode,
six legends (two for each view) are provided, while three
legends (one for each view) are provided in the global scale
mode.
Generally a legend consists of evenly divided intervals.
However, analysts often need to walk through a huge volume of data within a relatively short period to check for abnormalities. For the market analysis, the legend can be as
important as the data itself [Cle94, Wil05, TLH10]. In order to enhance understanding abnormalities of the data, the
intervals for the legends in MarketAnalyzer can be nonlinearly mapped according to importance as shown in Figure 2
(bottom-right). Analysts need to be alerted when red intervals are much wider than blue intervals. For instance, the
change of the width for negative measurements during investigation explicitly presents the worst growth rates.
3.4. Geographical View
The geographical view as shown in Figure1 (g) supports the
visual analysis of spatiotemporal patterns. The regional status of competition is a measure of the difference between
two selected companies within the selected geographical
area and is represented by colors. For instance, analysts are
able to see that the primary company has lower sales (red)
compared to its competitor in the left most image in Figure 3
(b). However, the market looks optimistic (blue) for the primary company because the sales trend is increasing (middle
image in Figure 3 (b)). Note that when a mouse hovers on
a store in the matrix view, the corresponding region on the
map is highlighted.
Dynamic querying for specifying time intervals also plays
an important role in geographic analysis because sales patterns evolve over time. This querying helps analysts identify complicated spatiotemporal patterns in the competition
and facilitate appropriate strategies. For instance, analysts
can easily find the time when the leading competitor started
losing its competitive advantage. Then, the analyst can start
investigating reasons for the loss by investigating informa-

1250

Ko et al. / MarketAnalyzer
AUG-NOV 2009

SEP-DEC 2009

FEB-MAY 2010

(g)

Sales

(a)

(c)

(e)
(h)

Trends

(b)

(d)

(f)

M6th

Mwin

Figure 4: (a)–(f) Analysis from the geographical view. The sales row shows the process of losing competitive advantage while
the trends row presents forecasts for each column interval (August 2009–May 2010). The blue color represents good performance while the red color represents bad performance for the primary company compared to its competitor. (g) The trend
view helps an analyst design short term tactics such as promotions. This example shows a decreasing overall sales trend of the
competitor in some stores. The analyst notes that the competitor has the worst downward trend in products P7 and P12 . (h) The
red box represents possible new markets for the new company M6th but its competitor Mwin has already started its business in
various products.
tion on the matrices and other views. These processes during
analysis are described in Section 4.
3.5. Stacked Bar View and Time Sliders
Well-designed stacked graphs are popular because of their
aesthetics and ease of perception [HHWN02, BW08]. Since
it is important to verify combinatorial trends of multiple
products, we employ stacked bar graphs to investigate these
trends as shown in Figure 1 (f) whose products and stores are
chosen from (b) and (c). When a person buys two products
together, there could be various assumptions to explain why
these two are chosen. It may be because of complementary
relationship between two products (e.g., ketchup and mustard), similar purchase cycles (e.g., beer and diapers), impulse purchases of a product (e.g., candy bar), or undiscovered reasons, such as happenstance [MAG99]. Figure 3 (a)
shows an example where one can see that as the sales of P1
increases and reaches a certain level, the sales of P3 rapidly
increase as well.
Draggable and length-changeable time sliders are used to
select time intervals for the stacked bar view, as shown in
Figure 1 (j) and (k), respectively. The red slider (k) is for
specifying current time interval. For the growth rate computation, a past time interval is required that is selected using
the blue slider (j). When the slider is dragged or the range
of the slider is changed, the time interval in the analysis also
changes, updating all other views with the new time interval.
4. Case Studies
We describe two scenarios with anonymous manufacturers,
stores, and products for privacy. The first scenario presents

the process of designing a strategy for a young company that
wants to increase its market share. The second scenario illustrates the analysis of a defending champion that needs to
verify and understand its competitive advantage compared
to 94 other competitors in the market.

4.1. Analysis to Step into a New Market
An analyst in a two year old company (M6th ) is asked to look
for potential areas in which the leading competitor (Mwin )
may currently be showing signs of weakness. By finding
these weaknesses, the new company can begin making inroads into the market as it expands its base. The exploration
begins in the pixel-based matrix views, as shown in Figure 1 (e). In the top matrix view, the y-axis represents all
36 products that are currently sold in the market. The x-axis
represents all 288 stores that sell its products. The analyst
chooses to sort the matrix by sales. Here, the darker the blue,
the more units are being sold. In the local mode, the analyst can see which stores and products are the top sellers for
M6th and Mwin . On the other hand, in the global mode, as
shown in Figure 4 (h), the analyst sees that the competitor
has more blue squares, meaning it is outperforming M6th .
In the red box in Figure 4 (h), the analyst easily verifies the
products and stores in which the new company is not supplying any product while the competitor has been earning
additional profit.
Before deciding on which market opportunities might
be the most profitable, the analyst needs to understand the
weakness and trends within the market. In the trend view,
the analyst sees the rate of growth or decline over the last
sales period. First, the sales period is selected using one time
slider (e.g., February–May 2010) in the lower portion of the
c 2012 The Author(s)

c 2012 The Eurographics Association and Blackwell Publishing Ltd.


1251

Ko et al. / MarketAnalyzer

P1
P7
P12

P17
<MAG1>

P23

<MAG4>

S1
P1
P7

P7

<MAG5>

P12 <MAG2>

P5

<MAG6>

<MAG3>

Figure 5: Our pixel-based matrix views using the comparative mode. Each row of the matrix represents a product, and each
column represents the store selling the product. The three views present three types of information (from top to bottom): sales,
trend, and growth rate comparisons between two selected companies. Note that gray color indicates zero. The blue color
presents positive and the red color represents negative in difference of the two measurements.
stacked bar view as shown in Figure 1 (k) (red slider). Here,
the analyst sees that many of the stores that the new company supplies are showing a downward growth trend. While
alarming, it is important to note that the leading competitor
is also displaying negative trends for similar products during
this time period. Looking at the trend analysis, the analyst
notes that the P7 and P12 products of the competitor have the
worst trends, which might indicate they are becoming less
popular. In that case, they could be the targets for the new
company to take the market share as shown in Figure 4 (g).
Of primary interest to the analyst is the matrix view under
the comparative mode, where the analyst can explore how
the current product sales is performing with respect to Mwin .
If a square is blue, then M6th is outperforming Mwin , and if
red, then Mwin is still outperforming the new company. Variations in the red and blue hue show the degree of sales performance. The analyst notices the following from Figure 5.
• P1 is the best-selling product, outperforming its competitor (MAG1 ),
• P7 records the worst sales performance in almost all stores
(MAG1 ). The sales of the P7 in two thirds of stores is
expected to increase (MAG2 ) while those in other stores
will still keep decreasing (MAG5 ),
• P5 has not been sold much in one third of the stores
(MAG6 ),
In Figure 3 (a), the analyst sees that the sales of P3 suddenly increased in May 2010 while those of P1 has grown
gradually. Thanks to increasing sales in these two products,
M6th has outperformed its competitor since May 2010 in
those selected products and it is predicted to outperform the
competitor with those products in June 2010 according to
the ARIMA forecast.
The analyst further wants to explore the distribution of its
c 2012 The Author(s)

c 2012 The Eurographics Association and Blackwell Publishing Ltd.


performance across stores and chooses to use the geographical view for regional competition analysis. Competition in
the last quarter (February–May 2010), as presented in Figure 3, can be summarized as follows:
• Sales (Figure 3 (b) Left): M6th has difficulty in overcoming its competitor in all regions. It is notable that the company is losing in the most important region including the
capital city that is colored in the darkest red.
• Trends (Figure 3 (b) Middle): The trends are optimistic in
all regions and the most important region has the highest
upward trend.
• Growth rates (Figure 3 (b) Right): Sales in some regions
have grown compared to sales in March–June 2009 although there are regions where growth rates are negative.
Therefore, the analyst will propose attacking the weaker
points of the leading competitor (Mwin ). For instance, P7 and
P12 of the competitor in the trend view in Figure 4 (g) are
expected to keep losing competitive advantage. Thus, they
might be proper targets for launching additional new products, to take over the market. Conversely, to keep or increase
the sales, a local promotion focusing on the capital city or a
global advertisement should be designed for the P7 product,
as shown in MAG1 in Figure 5. Giving up P5 in MAG6 might
not be a good strategy since it still remains in 5th place in
sales for the company. Watching other companies’ strategies
when they launch a new product, could be an effective solution at the moment. In addition, the analyst notices that there
are many gray products where Mwin does not supply its products. These would be the secondary locations where M6th
could launch new products for earning more profit. Lastly,
finding reasons for the abnormal sales pattern between P1
and P3 in Figure 3 (a) is important as well since they are in
the 1st and 3rd places in sales.

1252

Ko et al. / MarketAnalyzer
<MAG7>
P18

P1
P1

<MAG8>

P18
P18

<MAG9>

Figure 6: The matrix view under comparative mode with sorting provides direct information for the competition. These matrices
imply a possible strategic failure that could cause the loss of a big market.
4.2. Analysis of a Defending Champion
Often, the leading competitor’s analyst (company Mwin ) is
asked to analyze market share competition that requires a
performance comparison of all companies in all regions. The
analyst might begin exploration in the geographical view
that quickly shows the regional competition status for a
given time interval. In Figure 4 (a), the analyst sees that Mwin
had market share higher than 50% before November 2009
but the trend during the period decreased in all regions (Figure 4 (b)). The analyst notes that the most important region,
colored in the darkest red, was expected to have the most
severe decrease in sales. From December 2009, as shown in
Figure 4 (b), Mwin started losing competitive advantage in
three regions but still had three winning regions (Figure 4
(c)). It is notable that the regions that had moderate downward trends (Figure 4 (b)) still kept a competitive advantage,
while others with darker red colors were losing the competitive advantage. At the same time, forecasts kept warning of downward trends in all regions (Figure 4 (d)). In the
end, Mwin did not have any winning region during February–
May 2010 (Figure 4 (e)). However, Figure 4 (f) shows two
regions having upward trends in sales (darker blue colors).
During the analysis, the analyst sees that the proportional
legends keep changing. For instance, the red area representing an adverse situation, only occupied a small portion of the
growth legends in the February–May 2009 time period but
it ends up taking almost 80% of the area in the February–
May 2010 period, as shown in Figure 2 (bottom-right). The
strict CUSUM filtering also suggests deeper investigation
in February 2010 because the number of visualized stores
is largest (93 stores) while 28 stores are visualized in January 2010. This implies the largest number of stores turned

profit into loss in cumulated sales in February 2010 as shown
in Figure 2 (top-right). The default CUSUM filtering also
effectively reduces the number of stores, which, for example, visualizes 150 stores on average during October 2009–
January 2010 (48% reduction).
In order to find the causes for the loss of competitive advantage, a view of primary interest to the analyst is the matrix view under the comparative mode, as shown in Figure 6.
In this view, the analyst sees one of the problematic products. From October 2009, the company started losing competitive advantage in P18 . The effect of the loss was not significant at that time. However, during its sharpest decrease
between December 2009 and March 2010, the sales of product P18 decreased in January–April 2009. Even worse, P18
had the worst growth rate in about 40% of the stores severely
(MAG9 ). This implies a strategic failure that could cause the
loss of a big market. On the other hand, the analyst sees additional important information in MAG8 meaning that the
three top-selling products (P1 –P3 ) will severely lose competitive advantage in the next month. Through the analysis,
the analyst verified how the leading company has been losing its market share due to the decreasing sales performance.
The analyst also verified that as a product became less popular in the market, the trend, growth, and proportional legend
view reflected the adverse situation while the geographical
view highlighted the region where the sales performance is
lowest. Although discoveries should be interpreted from various perspectives in management strategies, we believe that
the two use-case examples show the effectiveness of the system for competitive advantage analysis using point of sale
data.
For our analysis, we collaborated with two groups of proc 2012 The Author(s)

c 2012 The Eurographics Association and Blackwell Publishing Ltd.


Ko et al. / MarketAnalyzer

1253

Figure 7: 288x36 resized pixels with assumptions of 63 (left), 126 (middle), and 252 (right) products in 1000 stores.
fessionally employed analysts. The first group consisted of
four analysts that had not previously used automated tools
for analyzing CI. Working in a supervised learning environment in our lab, they described that they required tools
for report generation and trend discovery. They noted that
they usually compare singular data aspects one at a time
using conventional tools such as Excel. The second group
consisted of three analysts who were working with custommade tools for CI analysis. Their tools utilized line charts,
bar graphs and data aggregation schemes. The analysts in
group 2 noted that the support of geographical visualization and the coupling of advanced analytical methods were a
marked improvement over their current tools. Both groups
utilized MarketAnalyzer and reported on their increased
ability to assess sales performance against their competitors. The greatest benefit in the analysis is that analysts can
recognize the overall competition status at a glance without
tedious selections. This enables time-efficient investigation
and easy comparison in various dimensions that can lead
to discovery of unexpected trends. The system is also very
helpful for people who are responsible for analyzing a competitive advantage in wide areas such as countries. In addition, the system is easy to understand and use. A manager
from the company’s finance department immediately gained
new insight without difficulty by using the system for few
minutes.
5. Discussion
In this work, we discussed a relatively small data set using
288 stores and 36 products. In this section we discuss some
of the potential limitations of our chosen techniques when
scaling to larger datasets. One key scaling issue is the limitation of the pixel-based visualization. Figure 7 illustrates the
effects of resizing the matrix. We show results using 63, 126
and 252 products (from left to right) in 1000 stores. Here,
we see that we are still able to distinguish between blue and
red stores (P1 – P3 and P18 ) as well as discrete patterns of
light red pixels (left, middle). As the number of products
increases, the ability to distinguish between such groupings
becomes more difficult. As such, intelligent filter controls
are necessary to help show the most important aspects of
the data (thus the motivation for the CUSUM and trend filtering). In our current data set, of the 288 stores, approximately 157 stores (std: 4.74) would be removed utilizing
the de f ault mode, and 229 stores (std: 23.86) would be
removed utilizing the strict mode. Future work will focus
on other importance metrics for guiding the data analysis.
Another method for overcoming the scaling issue is to use
focus+context interaction methods. Our current system provides a single-level zoom magnification lens. Future iterac 2012 The Author(s)

c 2012 The Eurographics Association and Blackwell Publishing Ltd.


tions of this tool will employ further interaction techniques
to better facilitate problems of scale. Finally, the stacked bar
chart method employed also becomes less effective as the
data size increases. However, the purpose of this particular view is to look at a small combination of products. Furthermore, future iterations will utilize more statistical methods and machine learning algorithms for directing users to
particular products of interest. In ARIMA, blue and red are
chosen due to their maximum distance in a diverging color
scheme [HB03] to contrast with each other.

6. Conclusions and Future Work
We have introduced a new visual analytics tool for market and competitive business analysis incorporating multiple tightly integrated interactive visualizations with integrated trend analytics. Our zoomable and sortable matrix
displays presents sales, trends, and growth rates with enhanced pixel-based visualization, while linked line graph
views and stacked bar views aid analysis and awareness acquisition of global and specific product/store information.
The linked choropleth maps enable geospatial, temporal, and
regional competition analysis. We presented two use-case
examples using real point of sale data to illustrate the use
and potential of the system. Our system can be easily applied to analysis with any other multivariate spatiotemporal
data. In the future, we plan to improve the geographical view
with advanced selecting and filtering to investigate correlations between point of sale data, demography, and geolocations for more advanced business analysis. To find strength
and weakness, a user study comparing to other alternative
techniques such as small multiple horizon graphs is needed.
We also plan to deploy our system with our corporate partner
and start a longitudinal study.

Acknowledgements
This work was supported in part by the U.S. Department of
Homeland Security’s VACCINE Center under Award Number 2009-ST-061-CI0001. The authors would like to thank
the analysts and corporate partners for feedback, advice and
evaluation during all stages of the design process.

References
[Ahl96] A HLBERG C.: Spotfire: An information exploration environment. ACM Special Interest Group on Management of Data
Record 25, 4 (1996), 25–29. 2
[BJ76] B OX G., J ENKINS G.: Time Series Analysis: Forecasting
and Control. Holden–Day Press, San Francisco, U.S.A., 1976. 2

1254

Ko et al. / MarketAnalyzer

[BPC∗ 10] B ORGO R., P ROCTOR K., C HEN M., JANICKE H.,
M URRAY T., T HORNTON I.: Evaluating the impact of task demands and block resolution on the effectiveness of pixel-based
visualization. IEEE Transactions on Visualization and Computer
Graphics 16, 6 (2010), 963–972. 2
[BW08] B YRON L., WATTENBERG M.: Stacked graphs - geometry & aesthetics. IEEE Transactions on Visualization and Computer Graphics 14, 6 (2008), 1245–1252. 6
[CGK∗ 07] C HANG R., G HONIEM M., KOSARA R., R IBARSKY
W., YANG J., S UMA E., Z IEMKIEWICZ C., K ERN D., S UD JIANTO A.: WireVis: Visualization of categorical, time-varying
data from financial transactions. In Proceedings of IEEE Symposium on Visual Analytics Science and Technology (2007),
pp. 155–162. 2
[Cle94] C LEVELAND W. S.: Visualizing Data. Hobart Press, New
Jersey, U.S.A., 1994. 5
[DS98] D RAPER N. R., S MITH H.: Applied Regression Analysis.
Wiley-Interscience, New Jersey, U.S.A., 1998. 2
[EM01] E DWARDS R., M AGEE J.: Technical Analysis of Stock
Trends. Amacom Press, New York, U.S.A., 2001. 2
[HB03] H ARROWER M. A., B REWER C. A.: Colorbrewer.org:
An online tool for selecting color schemes for maps. Cartographic Journal 40, 1 (2003), 27–37. 2, 9
[HHWN02] H AVRE S., H ETZLER E., W HITNEY P., N OWELL
L.: ThemeRiver: Visualizing thematic changes in large document
collections. IEEE Transactions on Visualization and Computer
Graphics 8, 1 (Jan. 2002), 9–20. 6
[HY09] H UR I., Y I J. S.: Simulsort: Multivariate data exploration
through an enhanced sorting technique. In Human-Computer Interaction. Novel Interaction Methods and Techniques, 13th International Conference, HCI International 2009, San Diego, CA,
USA, July 19-24, 2009, Proceedings, Part II (2009), vol. 5611 of
Lecture Notes in Computer Science, Springer, pp. 684–693. 4
[Kah98] K AHANER L.: Competitive Intelligence: How to Gather
Analyze and Use Information to Move Your Business to the Top.
Touchstone Press, New York, U.S.A., 1998. 1
[Kei00] K EIM D. A.: Designing pixel-oriented visualization techniques: Theory and applications. IEEE Transactions on Visualization and Computer Graphics 6, 1 (2000), 59–78. 1, 2, 3

[Rob07] ROBERTS J. C.: State of the art: Coordinated & multiple views in exploratory visualization. In Proceedings of Fifth
International Conference on Coordinated and Multiple Views in
Exploratory Visualization (2007), pp. 61–71. 2
[SCB98] S WAYNE D. F., C OOK D., B UJA A.: XGobi: Interactive
dynamic data visualization in the X window system. Journal of
Computational and Graphical Statistics 7, 1 (1998), 113–130. 2
[SFOL04] S HIMABUKURO M., F LORES E., O LIVEIRA M.,
L EVKOWITZ H.: Coordinated views to assist exploration of
spatio-temporal data: A case study. In Proceedings of Second
International Conference on Coordinated and Multiple Views in
Exploratory Visualization (2004), pp. 107–117. 2
[SGL08] S TASKO J. T., G ÖRG C., L IU Z.: Jigsaw: Supporting
investigative analysis through interactive visualization. Information Visualization 7, 2 (2008), 118–132. 2
[SLBC03] S WAYNE D. F., L ANG D. T., B UJA A., C OOK D.:
GGobi: Evolving from XGobi into an extensible framework for
interactive data visualization. Journal of Computational Statistics & Data Analysis 43, 4 (2003), 423–444. 2
[Sma] SmartMoney. http://www.smartmoney.com/marketmap/.
Accessed by 12 Aug 2011. 2
[STH08] S TOLTE C., TANG D., H ANRAHAN P.: Polaris: A system for query, analysis, and visualization of multidimensional
databases. ACM Communications 51, 11 (2008), 75–84. 2
[STKF07] S CHRECK T., T EKUSOVA T., KOHLHAMMER J.,
F ELLNER D. W.: Trajectory-based visual analysis of large financial time series data. ACM Special Interest Group on Knowledge
Discovery and Data Mining Explorer Newsletter 9, 2 (2007), 30–
37. 2
[TA03] TASKAYA T., A HMAD K.: Bimodal visualisation: A financial trading case study. In Proceedings of Seventh International Conference on Information Visualization (2003), pp. 320–
326. 2
[Tab] Tableau.
Aug 2011. 2

http://www.tableausoftware.com, Accessed on

[TLH10] TALBOT J., L IN S., H ANRAHAN P.: An extension
of Wilkinson’s algorithm for positioning tick labels on axes.
IEEE Transactions on Visualization and Computer Graphics 16,
6 (2010), 1036–1043. 5

[KK94] K EIM D. A., K RIEGEL H.: VisDB: Database exploration
using multidimensional visualization. IEEE Computer Graphics
and Applications 14, 5 (1994), 40–49. 1, 2, 3

[Tuf90] T UFTE E. R.: Envisioning Information. Graphics Press,
CT, U.S.A., 1990. 3

[MAG99] M ANCHANDA P., A NSARI A., G UPTA S.: The shopping basket: A model for multicategory purchase incidence decisions. Journal of Marketing Science 18, 2 (1999), 95–114. 6

[VvWvdL06] V LIEGEN R., VAN W IJK J. J., VAN DER L IN DEN E.-J.: Visualizing business data with generalized treemaps.
IEEE Transactions on Visualization and Computer Graphics 12,
5 (2006), 789–796. 2

[MFGH08] M ATKOVIC K., F REILER W., G RACANIN D.,
H AUSER H.: ComVis: A coordinated multiple views system for
prototyping new visualization technology. In Proceedings of 13th
International Conference on Information Visualisation (2008),
pp. 215–220. 2
[Mur99] M URPHY J. J.: Technical Analysis of the Financial Markets: A Comprehensive Guide to Trading Methods and Applications. Prentice Hall Press, New Jersey, U.S.A., 1999. 2
[OJS∗ 11] O ELKE D., JANETZKO H., S IMON S., N EUHAUS K.,
K EIM D. A.: Visual boosting in pixel-based visualizations. Computer Graphics Forum 30, 3 (2011), 871–880. 2
[Pag54] PAGE E. S.: Continuous inspection scheme. Biometrika
41 (1954), 100–115. 2, 5
[R D06] R D EVELOPMENT C ORE T EAM: R: A Language and
Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2006. ISBN 3-900051-07-0.
URL: http://www.R-project.org. 3

[War94] WARD M. O.: XmdvTool: Integrating multiple methods
for visualizing multivariate data. In Proceedings of IEEE conference on Visualization (1994), pp. 326–333. 2
[WFR∗ 07] W EAVER C., F YFE D., ROBINSON A.,
H OLDSWORTH D., P EUQUET D., M AC E ACHREN A. M.:
Visual exploration and analysis of historic hotel visits. Information Visualization 6, 1 (2007), 89–103. 2
[Wil05] W ILKINSON L.: The Grammar of Graphics (Statistics
and Computing). Springer-Verlag, New Jersey, U.S.A., 2005. 5
[ZNK08] Z IEGLER H., N IETZSCHMANN T., K EIM D. A.: Visual analytics on the financial market: Pixel-based analysis and
comparison of long-term investments. In Proceedings of International Conference on Information Visualisation (2008), IEEE
Computer Society, pp. 287–295. 2

c 2012 The Author(s)

c 2012 The Eurographics Association and Blackwell Publishing Ltd.


IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 16,

NO. 2,

MARCH/APRIL 2010

205

A Visual Analytics Approach to Understanding
Spatiotemporal Hotspots
Ross Maciejewski, Student Member, IEEE, Stephen Rudolph, Ryan Hafen,
Ahmad M. Abusalah, Mohamed Yakout, Mourad Ouzzani, William S. Cleveland,
Shaun J. Grannis, and David S. Ebert, Fellow, IEEE
Abstract—As data sources become larger and more complex, the ability to effectively explore and analyze patterns among varying
sources becomes a critical bottleneck in analytic reasoning. Incoming data contain multiple variables, high signal-to-noise ratio, and a
degree of uncertainty, all of which hinder exploration, hypothesis generation/exploration, and decision making. To facilitate the
exploration of such data, advanced tool sets are needed that allow the user to interact with their data in a visual environment that
provides direct analytic capability for finding data aberrations or hotspots. In this paper, we present a suite of tools designed to facilitate
the exploration of spatiotemporal data sets. Our system allows users to search for hotspots in both space and time, combining linked
views and interactive filtering to provide users with contextual information about their data and allow the user to develop and explore
their hypotheses. Statistical data models and alert detection algorithms are provided to help draw user attention to critical areas.
Demographic filtering can then be further applied as hypotheses generated become fine tuned. This paper demonstrates the use of
such tools on multiple geospatiotemporal data sets.
Index Terms—Geovisualization, kernel density estimation, syndromic surveillance, hypothesis exploration.

Ç
1

INTRODUCTION

H

reports, terrorism alerts, criminal activities, and
numerous other incidents need to be analyzed and
evaluated, often within the context of related data sets. It is
no longer efficient for a single analyst to pull files, take
notes, form hypotheses, and request data from different
sources. Instead, tools need to be developed that bring
varying data sources into a unified framework assisting
analysis and exploration. These needs are being addressed
by the emergence of a new scientific field, visual analytics.
Visual analytics is the science of analytical reasoning
assisted by interactive visual interfaces [40]. Major challenges
EALTH

. R. Maciejewski and D.S. Ebert are with the Purdue University Regional
Visualization and Analytics Center, Purdue University, Potter Engineering Center, Room 134, 500 Central Drive, West Lafayette, IN 47906.
E-mail: {rmacieje, ebertd}@purdue.edu.
. S. Rudolph is with the Purdue University Regional Visualization and
Analytics Center, 8550 E McDowell Rd., Apt. 279, Scottsdale, AZ 852573902. E-mail: stephen.rudolph@gmail.com.
. R. Hafen and W.S. Cleveland are with the Department of Statistics, Purdue
University Regional Visualization and Analytics Center, Purdue University, 150 N. University Street, West Lafayette, IN 47906.
E-mail: rhafen@purdue.edu.
. A.M. Abusalah is with Purdue University, 1160 Cushing Cir, Apt 315,
Saint Paul, MN 55108. E-mail: aabusala@purdue.edu.
. M. Yakout is with the Department of Computer Sciences, Indiana Center
for Database Systems (ICDS), Purdue University, 305 N. University
Street, West Lafayette, IN 47907-2107. E-mail: myakout@cs.purdue.edu.
. M. Ouzzani is with the Cyber Center, Purdue University, 155 S. Grant
St., West Lafayette, IN 47907. E-mail: mourad@cs.purdue.edu.
. S.J. Grannis is with the Regenstrief Institute, Inc., Indiana University
School of Medicine, 410 W. 10th St., Suite 2000, Indianapolis, IN 46202.
E-mail: sgrannis@regenstrief.org.
Manuscript received 30 Jan. 2009; revised 8 May 2009; accepted 16 July 2009;
published online 19 Aug. 2009.
Recommended for acceptance by T. Ertl.
For information on obtaining reprints of this article, please send e-mail to:
tvcg@computer.org, and reference IEEECS Log Number
TVCGSI-2009-01-0020.
Digital Object Identifier no. 10.1109/TVCG.2009.100.
1077-2626/10/$26.00 ß 2010 IEEE

in this field include the representation and linkage of largescale multivariate data sets. In order to facilitate enhanced
data exploration and improve signal detection, we have
developed a linked geospatiotemporal visual analytics tool
designed for advanced data exploration. This paper presents
a set of extensions to our previous suite of visual analytics
tools [31] for the enhanced exploration of multivariate
geospatiotemporal data. Our system features include
a new kernel density estimation that works for both
urban and rural populations;
. dually linked interactive displays for multidomain/
multivariate exploration and analysis;
. novel data aggregation for effective visualization
and privacy preservation;
. control charts for identifying temporal signal alerts;
. demographic filter controls that enable database
querying and analysis through a simple graphical
interface;
. spatiotemporal history via contour line ghosting;
. bivariate exploration combining contours and color;
. multivariate exploration combining height maps,
contours, and color;
. thresholding data to analyze specific trends;
. interactive color mapping tools for enhanced data
contextualization;
. region selection tools for analyzing area specific
hotspots.
Our work focuses on advanced interactive visualization
and analysis methods providing linked environments of
geospatial data and time series graphs. Hotspots found in
one display method can be selected and immediately
analyzed in the corresponding linked view. Furthermore,
our work focuses on the early detection and analysis of
hotspots facilitated through the use of control charts for
.

Published by the IEEE Computer Society

206

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

alert detection. Alerts generated in the temporal realm can
be quickly analyzed in the geospatiotemporal interface,
helping users find patterns simultaneously in the spatial
and temporal domains. Concurrently, we have also applied
statistical modeling techniques to estimate event distributions in the spatial realm. Users may select hotspots from
the generated heatmaps and analyze historical time series
data in the area to look for unusual trends or potential areas
of interest. Such doubly linked views allow users to quickly
form and test hypotheses, thereby reducing the time needed
to reject false positives and confirm true alerts.
We have also extended our previous system [31] to
include the spatiotemporal history, bivariate and multivariate exploration, thresholding, and color mapping tools.
Contour histories provide users with geospatiotemporal
views of current and past data trends, allowing them to
track hotspot movement across time, or look for correlations
between multiple variables. We also allow interactive range
selection and thresholding to allow users to focus directly
on hypothesis specific information. Furthermore, we demonstrate the flexibility of such tools by providing example
applications in the domain of law enforcement data analysis
and syndromic surveillance.

1.1 Law Enforcement Data
One data source we focus on is law enforcement data. These
data come in the form of traffic violations, misdemeanors,
criminal activities, etc. These data are typically spatiotemporal, containing the location of the incident, the time, and
some description allowing the data to be classified into
various categories. Such data can be analyzed for trends,
enabling agencies to better manage their resources and
deploy officers to potential problem areas. Our work
utilizes data from the West Lafayette Police Department,
and through contacts with former State Highway Patrol
officers, we are able to tailor our tools toward officer
specific needs.
1.2 Syndromic Surveillance Data
Another data source that we explore is syndromic surveillance data. Recently, the detection of adverse health events
has focused on prediagnosis information to improve
response time. This type of detection is more largely termed
syndromic surveillance and involves the collection and
analysis of statistical health trend data, most notably
symptoms reported by individuals seeking care in emergency departments. Currently, the Indiana State Department of Health (ISDH) employs a state syndromic
surveillance system called Public Health Emergency Surveillance System (PHESS) [19], which receives electronically
transmitted patient data (in the form of emergency
department chief complaints) from 73 hospitals around the
state at an average rate of 7,000 records per day.
These complaints are then classified into nine categories
(respiratory, gastrointestinal, hemorrhagic, rash, fever,
neurological, botulinic, shock/coma, and other) [11] and
used as indicators to detect public health emergencies
before such an event is confirmed by diagnoses or overt
activity. Unfortunately, detection of events from these
indicators is an extremely challenging issue. Fig. 1 shows
a typical month of emergency department visits for those

VOL. 16,

NO. 2,

MARCH/APRIL 2010

Fig. 1. A sample syndromic surveillance signal containing a carbon
monoxide poisoning event.

complaints classified as neurological syndromes. During
this time period, there was one event of carbon monoxide
poisoning which happened to coincide with the largest
peak on December 21; however, this peak is not significantly higher than any other peak during this month.
Obviously, the detection of such a small signal deviation
can be extremely difficult.
Again, it is important to note that the tools being
developed are supervised by our partners in the Indiana
State Department of Health. Prior to tool development, we
meet with our partners and discuss the needs and
functionalities of the tools and work to build them in such
a manner as to enhance their work flow. We have found
that tools developed for syndromic surveillance have
translated well to the analysis of law enforcement data,
and feedback from both agencies has been valuable in
creating appropriate tools.

2

RELATED WORK

As previously stated, visual analytics is the science of
analytic reasoning facilitated by interactive visual interfaces
[40]. In order for these interfaces to be effective, they need to
integrate not only data exploration and visualization tools,
but also human factors such as interaction, cognition,
perception, collaboration, presentation, and dissemination.
In order to create an effective visual analytics systems,
methods from a variety of backgrounds must be merged
together in a simple, yet effective framework. This section
covers relevant topics in the areas of crime analysis,
syndromic surveillance, multivariate interaction techniques,
time series visualization, and geographical visualization.

2.1 Crime Analysis
In order to improve public safety and prevent crimes, law
enforcement agencies need to analyze the volumes of data
from multiple systems, search for trends, and deploy
services appropriately. As such, many packages exist for
studying spatial relationships between crime and area
demographics. Work by Messner and Anselin [34] uses
exploratory spatial data analysis to visualize spatial
distributions and suggest clusters and hotspots. Specifically, they look at spatial autocorrelation and box maps.
Other work includes WebCAT by Calhoun et al. [10] which

MACIEJEWSKI ET AL.: A VISUAL ANALYTICS APPROACH TO UNDERSTANDING SPATIOTEMPORAL HOTSPOTS

focuses on enhanced data sharing and crime data analysis
tools via the web. Their tools include chloropleth mapping
and capabilities to export records to excel. Our work
presents similar capabilities to both Messner and Anselin
[34] and Calhoun et al. [10]; however, we also include
dynamically linked views and advanced hotspot detection
tools not found in either of these works.

2.2 Syndromic Surveillance Systems
Data from public health surveillance systems have long
been recognized as providing meaningful measures for
disease risks in populations [25], [39]. As such, many
disease modeling packages, outbreak alert algorithms and
data exploration systems have been developed to aid
epidemiologists in identifying outbreaks within their data.
Some of the most popular of these systems are the Early
Aberration Reporting System (EARS) [22], the Electronic
Surveillance System for the Early Notification of Community-based Epidemics ESSENCE [27], and Biosense [28].
Unfortunately, all of these systems offer limited data
exploration tools and little-to-no interactive geospatial
support. Furthermore, many detection algorithms employed by these systems generate a large amount of false
positives for epidemiologists to analyze. While creating
algorithms to reduce false positives is important, our work
focuses on creating advanced visual analytics tools for more
efficiently exploring these alerts and hypotheses.
2.3 Multivariate Interaction Techniques
When creating an interactive framework for data exploration and hypothesis testing/generation there are a variety
of interactive techniques that can be applied. The majority
of techniques utilized in our work focus on the probing,
brushing, and linking of data in order to help analysts refine
their hypotheses. These methods emphasize the interaction
between human cognition and computation through dynamically linked statistical graphs and geographical representations of the data (e.g., [13], [7], [4]).
Examples of recent work in spatiotemporal interaction
include VIST-STAMP by Liao [26], FemaRepViz by Pan and
Mitra [35], and LAHVA by Maciejewski et al. [32]. VISSTAMP supports the overview of complex patterns through
a variety of user interactions. Specifically, this work focuses
on visualizing multivariate patterns using parallel coordinate plots and self-organizing maps. FemaRepViz provides
a display of Federal Emergency Management Agency
(FEMA) reports on a globe and dynamically determines
where each report should be placed based on the text of the
report. It also allows the user to navigate through time;
displaying only the relevant reports for that period. And
finally, LAHVA looked at using multiple data sets (pet and
human health data) with similar properties to enhance
disease surveillance. This system provided a geospatiotemporal interface with limited interaction among different
view windows. Our current system is similar in that it
allows users to explore both spatially (through panning and
zooming) and temporally through interactive time sliders
and history filter/aggregation controls.
Examples of recent work in multivariate data exploration
through linked views and probing includes work by
Weaver [44], who created a system for interactively
expressing sequences of multidimensional set queries by

207

cross-filtering data values across pairs of views. Another
example is work by Stasko et al. [38] which introduced the
Jigsaw system which provides a series of visual interfaces
that deal with identifying linkages between entities within a
data set. Other work includes the use of data probes by
Butkiewicz et al. [9]. This work noted that when analysts are
zoomed out of their data, local trends are suppressed, and
when zoomed in, spatial awareness and comparison
between regions is limited. Our current system uses similar
modalities in that users can selectively filter data through
query command and through interaction between the
linked interfaces. Furthermore, when zooming into the
data, we allow users to manipulate rendering parameters
(such as color) in order to help better contextualize and
explore hotspots in their local surroundings.

2.4 Time Series Visualization
One of our key linked views is a time series visualization
view. The analysis of time series data is one of the most
common problems in any data domain, and the most
common techniques of visualizing time series data (sequence charts, point charts, bar charts, line graphs, and
circle graphs) have existed for hundreds of years. Recent
work in time series visualization has produced a variety of
techniques, an overview of which can be found in [2]. Of the
more modern techniques, some of the most commonly
applied are the theme river [21], the spiral graph [45], and
the time wheel [41].
Work on event prediction [8] and pattern recognition [8]
was done by Buono et al. for time series data. This work
presented users with a tool to explore multivariate time
series for common patterns, and extended this approach for
predicting future events. Other techniques of interest
include the visualization of queries on databases of
temporal histories [12], and novel glyphs for representing
temporal uncertainties [3]. Unfortunately, most temporally
oriented visualization techniques are not suited to represent
branching time, or time with multiple perspectives. As
such, the modification of existing techniques is necessary in
order to more adequately analyze multivariate data from
varying sources.
Our work focuses primarily on line graphs showing
event counts. These graphs are then statistically analyzed
and plotted as control charts in order to quickly provide the
analysts with contextual information about the significance
on an event. We allow for the plotting of multiple series on
a single graph, as well as interactive selection tools for area/
region specific plots.
2.5 Geographical Visualization
Another key linked view is the geographical visualization
component. Geographic visualization is a field focused on
displaying data with a geographic context such as a map. In
more recent years, it has ballooned to include increasingly
complex data, other spatial contexts, and information with a
temporal component.
Several current systems exist that leverage advanced
geographical visualization techniques for various health
data. MacEachren et al. [30] presented a system designed to
facilitate the exploration of time series, multivariate, and
georeferenced health statistics. Their system employed

208

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 16,

NO. 2,

MARCH/APRIL 2010

Fig. 2. The visual analytics system. (a) The conceptual diagram of our visual analytics system. Observe the interaction between the analyst and the
system as well as the modeling components of the system. (b) Our visual analytics system. The left portion of the screen represents the interactive
temporal tools. We include time aggregation tools, pull down menus for data classifier selections, keyword searches for chief complaint text, and
demographic filtering for age and gender. The main viewing area is a geospatiotemporal view that has pan and zoom controls in the upper left
corner. Hospitals and regions of the map may be selected with a circular query tool for interactive time series generation. The rightmost windows are
the temporal views, showing selected time series plots broken down into their relevant components. Users may select points or regions of time to
interactively manipulate the geospatial temporal window. For analyzing crime data, the interface is modified only slightly to reflect the relevant
categories.

linked brushing and time series animation to help domain
experts locate spatiotemporal patterns. Further work in
analyzing health statistics was done by Edsall et al. [16].
Here, the use of interactive parallel coordinate plots was
used to explore mortality data as it relates to socioeconomic
factors. Other work includes Dang et al. [15] and Zhao et al.
[46] which utilized dynamic queries and brushing for
creating choropleth map views, and Tominski et al. [42]
developed a system for visualizing health data for the
German state Mecklenburg-Vorpommern. This system
allowed users to interactively select diseases and their
parameters and view the data over a specific time interval at
different temporal resolutions. Further work in this system
[43] employed the use of intuitive 3D pencil and helix icons
for visualizing multiple dependent data attributes and
emphasizing the type of underlying temporal dependency.
Work by Hargrove and Hoffmann [20] used multivariate
clustering to characterize ecoregion borders. Here, the
authors select environmental conditions in a map’s individual raster cells as coordinates that specify the cell’s
position in environmental data space. The number of
dimensions in data space equals the number of environmental characteristics. Cells with similar environmental
characteristics will appear near each other in data space.

3

VISUAL ANALYTIC ENVIRONMENT

Our system adopts the common method of displaying
georeferenced data events on a map and allowing users to
temporally scroll through their data. However, such
exploration only provides slices of spatial data at a given
time or an aggregate thereof. In order to understand these
slices, users need to know the trends of previous data (and,
if possible, model future data trends). Furthermore, a
limiting factor in using mapping as a tool for syndromic
surveillance and crime analysis is that aggregation of data

can lead to unreliable estimates of the true measure of the
event impact. Fortunately, the data used in our visual
analytics system provide georeferenced locations, allowing
us to either aggregate the data on a spatial level, or employ
statistical methods to model the data over arbitrarily sized
georegions. As such, our system employs advanced
statistical models for data exploration, enabling new
visualizations, analyses, and enhanced detection methods.

3.1 System Features
Fig. 2a provides a conceptual overview of our visual
analytics system, and Fig. 2b provides a screenshot of the
system modified specifically for syndromic surveillance
data. Note that all features described in this section are
available for both crime analysis and syndromic surveillance. Data entering our system first undergo a cleaning and
transformation process. This process is then refined through
feedback from our visual analytics system. Furthermore, the
user may report data errors as well, allowing for data
correction. Finally, frequently accessed time series models of
the data are also stored in the database for future use after
initial modeling is done via our visual analytics system.
Further interaction is performed within the different
viewing and modeling modalities of the system. As shown
in Fig. 2b, the main viewing area is the geospatiotemporal
view, and the three graphs on the right allow users to view
a variety of data sources simultaneously for a quick
comparison of trends across varying hospitals/precincts
or data aggregated over spatial regions. Both the geospatial
and time series viewing windows are linked to the time
slider at the left side of the screen. This allows users to view
the spatial changes in the data as they scroll across time.
Additionally, temporal controls are also employed. These
controls are denoted as “aggregate” and “increment” in the
scroll bar window. The aggregate function allows the user
to show all data over a period of x days. The increment

MACIEJEWSKI ET AL.: A VISUAL ANALYTICS APPROACH TO UNDERSTANDING SPATIOTEMPORAL HOTSPOTS

209

Fig. 3. Data aggregation and privacy preservation. (a) Georeferenced syndromic surveillance data as small additive opacity circles.
(b) Georeferenced data overlaid with red circles representing syndromic patients. (c) Data aggregation for enhanced visualization. (d) Highresolution zoom of an area of interest. (e) Actual patient locations at a high-resolution zoom overlayed with our data aggregation method.

function allows the user to step through the data by
increments of 1, 2, 3, . . . days. All temporal views also
provide a locking mechanism in which the user can choose
to freeze the data window(s) while exploring changes
across time in other views. This allows users to explore data
while keeping a reference point to the time-varying trend(s)
under inspection.
Another key feature of our system is the interactive
demographic and category filtering. Users interactively
generate database search queries through the use of check
boxes and edit controls to find specific categories, keywords, and gender and age demographics from the data.
Such work furthers hypothesis generation and exploration
as users can now quickly filter signals by demographic
constraints in order to search for correlations. The choices of
filters affect both the geospatiotemporal viewing area and
all unlocked temporal plots.

3.2 Data Aggregation and Privacy Preservation
Our system also provides multiple views for enhanced
visualization and analysis. One simple, yet key view for this
data set is showing georeferenced data locations on the map
in order to provide analysts with a quick overview of
statistics across the area of interest. Unfortunately, in both
syndromic surveillance and crime data, showing exact
event locations on a map is encumbered by privacy issues.
Previous work in visualizing health statistics bypasses these
concerns by showing data spatially aggregated over
geographical areas such as zip code or county. While such

visualizations are useful, there are times when it may be of
interest to analysts to simply see a plot of event locations on
a smaller level of data aggregation. Unfortunately, not all
software users have the same level of permissions for
viewing this data.
A naive visualization method would be to zoom out of
the map at such a level that a pixel would represent a large
enough region that it would be difficult to extract any
private information about event mapped on a transformed
geolocation to pixel basis. Unfortunately, as the data set
becomes arbitrarily large, the visual clutter cannot be
reduced in such a manner, see Fig. 3a, and it becomes clear
that a visualization of every record at a high spatial zoom
level is not effective for analysis. Furthermore, simple
methods, such as using additive opacity to demonstrate
event density, Fig. 3c, are inadequate as the number of
events makes it impossible to readily distinguish density
levels between areas. This is further complicated when the
events are then highlighted with regard to their locations.
Fig. 3b shows the syndromic patients mapped in red. In
order to alleviate this problem, we have employed a
method of data aggregation for enhanced visualization at
low resolution views, which also acts as a privacy
preserving technique at low zooms.
Our data aggregation method finds sets of event
locations where each member is at most a set distance from
at least one other member. The group is then represented by
a circle at the set’s geographic center that has an area
proportionate to the size of the set. This allows us to

210

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 16,

NO. 2,

MARCH/APRIL 2010

Fig. 4. Data aggregation and privacy preservation visualized as a percentage of syndromic population over the total population seen. (a) Data
aggregated by county. (b) Data aggregated through nearest neighbor groupings. (c) A combination of data aggregation to enhance contextual
visualization.

successfully aggregate data around major cities while
preserving the autonomy of smaller sets in rural areas.
This method is derived from the idea of connected
components in graph theory, where patients are connected
if and only if they are within the threshold distance from
another patient in the graph [14]. The generated circles are
then colored using a color map [6], where the color
represents the percent of events within a given category
found within this geographical centroid. This method
operates under the assumption that the data are clumped
in certain locations, otherwise it is possible to have an
aggregation that hides too much of the actual data.
Furthermore, as this method groups data at its geographic
center of mass, it preserves the data context and helps
alleviate privacy concerns.
Fig. 3c shows the low resolution aggregation of our
syndromic surveillance data across the state of Indiana.
Fig. 3d shows the zoomed in region, and Fig. 3e represents
where the actual patient locations would be with respect to
their representation as a geographic centroid.

3.3 Heatmaps
While such data aggregation can be useful for an overall view
of event distribution, it is also useful to model the event
distribution across the entire area of interest in order to
approximate trends where little or no data values exist.
Therefore, our system provides a geospatial heatmap [17]
view which employs a diverging color map (or any other
Color Brewer scheme) [6] to represent the percentage of a
given event category over the total events seen on a given day.
The georeferenced data contain a set of observations in
which an event (crime/syndrome) occurs at location Xi
associate at time t with a hospital/precinct and is classified
with a particular category. Such data are often aggregated
by county or zip code and then shown to the user. This type
of aggregation can be thought of as a histogram or box-plot
of the data, and while a spatial histogram can be useful,
such a visualization does not provide any hints as to what
may be occurring in areas with little-to-no event data.
Furthermore, areas with a small number of events may
stray toward a high percentage of the total events in the
category under question. In those cases, visual alerts may be

triggered that would clearly appear as false positives once
the individual records were analyzed. Fig. 4a demonstrates
the problems with visualizing such histogram distributions.
The national baseline influenza-like-illness (ILI) percentage
during flu season is 2.1 percent [1] for the 2006-2007 season.
Note in Fig. 4a that many counties seem to be visually
displaying an extremely high level of ILI, where if we
compare this to the overlaid data aggregation circles, these
counties actually have very few patients contributing to the
aggregations’ center of mass as seen in Figs. 4b and 4c.
To overcome these issues, our system estimates the
probability density function of all the recorded events using
the georeferenced locations and produces a heatmap
visualization of the area. To this end, we employ a kernel
density estimation [37]. Kernel density estimation takes a
collection of sample points and fits a weighting function at
each point. A kernel is a nonnegative real-valued integrable
function that integrates to one over all real values, and is
symmetrical about the origin. The bandwidth of the kernel
can be fixed or dynamic depending on the method
employed. The bandwidth of the kernel influences the
magnitude of the kernel, i.e., kernels with large bandwidths
have a smaller height.
Equation (1) defines the multivariate kernel density
estimation, and this method has been used in other works
[23], [29], [18]. To reduce the calculation time, we have
chosen to employ the Epanechnikov kernel, (2).


N
1X
1
x  Xi
K
;
f^h ðxÞ ¼
N i¼1 hd
h

ð1Þ

3
KðuÞ ¼ ð1  u2 Þ1ðkuk1Þ :
4

ð2Þ

Here, h represents the multidimensional smoothing parameter, N is the total number of samples, d is the data
dimensionality, and the function 1ðkuk1Þ evaluates to 1 if the
inequality is true and zero for all other cases. We calculate
both the density estimation for the event category of interest
as well as the density estimation of all categories in our
system using an appropriately chosen h for each data set.

MACIEJEWSKI ET AL.: A VISUAL ANALYTICS APPROACH TO UNDERSTANDING SPATIOTEMPORAL HOTSPOTS

211

Fig. 5. Kernel density estimate (KDE) heatmaps visualized as a percentage of syndromic population over the total population seen. (a) KDE
heatmap. (b) Contextualizing the KDE heatmap by overlaying patient data aggregated through nearest neighbor groupings. (c) A zoomed in view of a
local hotspot. (d) Contextualizing a hotspot through interactive coloring.

The density estimation for the event category of interest is
then divided by the density estimation for the total events to
provide a percentage count for the expected number of
events within a given area.
Unfortunately, a fixed bandwidth kernel turns out to be
inappropriate for our data due to sparse data counts in rural
counties and high data counts in large urban areas. A large
fixed bandwidth over smoothes the data while trying to
accommodate for the sparse data regions, and a small fixed
bandwidth is unable to handle data in sparse regions,
creating visual alerts in a similar fashion.
To overcome these issues, we employ the use of a
variable kernel method [37], (3). This estimate scales the
parameter of the estimation by allowing the kernel scale to
vary based upon the distance from Xi to the kth nearest
neighbor in the set comprising N  1 points.


N
1X
1
x  Xi
^
:
K
fh ðxÞ ¼
N i¼1 di;k
di;k

ð3Þ

Here, the window width of the kernel placed on the point
Xi is proportional to di;k (where di;k is the distance from the
ith sample to the kth nearest neighbor) so that data points in
regions where the data are sparse will have flatter kernels.
Unfortunately, our data sets also exhibit problems with this
method. In healthcare data, a primary recipient of emergency care are patients of long-term healthcare facilities (for
example, nursing homes). As such, the use of the k nearest
neighbors may result in a di;k of 0 as many patients visiting
emergency rooms may report the same address. This
concept can be extended to large apartment complexes
generating many noise complaints in the crime data, as well
as data uncertainty (for example, many hospitals report
unknown patient addresses as the hospital address). To
overcome these issues, we slightly modify the variable
kernel estimation to force it to have a minimum fixed
bandwidth of h as shown in (4).
!
N
1X
1
x  Xi
^
fh ðxÞ ¼
:
ð4Þ
K
N i¼1 maxðh;di;k Þ
maxðh;di;k Þ
In the case of our modified variable kernel estimation, we
calculate the kernel only spatially as opposed to both
spatially and temporally as was done in the fixed
bandwidth method. Future work will include extending
our modified density estimation into the temporal domain.
Results from our variable kernel estimation can be seen in

Fig. 5. Slight problems in the estimation can be found near
the state borders due to the abrupt cutoff of data in those
areas. Future work will address these issues through more
advanced spatial modeling.

3.4 Context through Color Exploration
Of key importance in all the previously presented data
aggregation methods is the choice of coloring. In coloring
our maps, data ranges get binned to a certain color. Clearly,
the choice of bins can be based on model assumptions of the
expected percentage of events within an area. However,
each category of event will have varying model assumptions. Furthermore, the distribution of the data can also play
a key role in placing hotspots into the proper context. For
example, if the data are binned such that the maximum
value covers a large range of variation, it is possible that
such a mapping could hide hotspots within hotspots.
As such, we have created an interactive color widget for
exploring data ranges. This widget allows users to modify
the color scale either interactively or through a set of
mathematical binning functions. We provide functions for
linear, ramp, exponential, and logarithmic binning.
In linear binning, the points on the map are first binned
across a large histogram. The histogram is then divided
such that each color represents an equal number of points
within the data. For ramp binning, the histogram is divided
such that each color represents an increasingly larger
number of points, following along the line y ¼ x. This idea
is then extended for both exponential and logarithmic
curves. Future work will include binning the data to a
Gaussian distribution. The distribution functions were
based on observations that the mathematical distributions
are often able to automatically highlight various properties
of the data with little user interaction. Further research into
this connection is left to future work.
In exploring the data through contextual color clues, our
domain experts typically employed the use of either a
default mathematical binning, or arranged the color bins
manually such that the last bin began at some data
threshold of interest. In Fig. 5, the data have been mapped
using a logarithmic binning. Both the data aggregation and
the kernel density estimation tools can be used in conjunction for contextualizing hotspots. Here, we find several
hotspots in the state. When placed in the context of the data
aggregation overlay (Fig. 5b), we begin to develop
hypotheses of key places that need further exploration.
These places are marked by the black squares in Fig. 5b.

212

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 16,

NO. 2,

MARCH/APRIL 2010

Fig. 6. Exploration using linked views. (a) Images taken from our system illustrating the linked temporal analysis to the geospatial filtering. Here, a
user has selected the alert occurring on 01-01-2007. The geospatial viewing window then opens that day’s data corresponding to the alert allowing
for further investigation. (b) Images taken from our system illustrating the linked views in selecting geospatial areas and seeing temporal plots. Here,
a user has selected an area in north-west Indiana (the green circle). This selection brings up the time series graph and our alert detection algorithm
finds an unusual event in that area on that day.

Further, we see the dense hotspot centered in the middle
of the state. To further explore this hotspot, users may zoom
into the map. The zoom results in a recalculation of the
kernel density estimate as the latitude/longitude point
space mapping to the grid changes. Fig. 5c provides a
zoomed in view of the state’s central hotspot. Notice that
this heatmap is dominated by a singular range of red. In
Fig. 5b, the user interactively adjusts the color scale to
provide more binning across that particular data range.
Through this interaction, the user is now able to find several
previously undetectable peaks within this region that may
warrant further investigation.

3.5 Time Series Analysis
While the spatial visualizations employed in our system are
useful for detecting hotspots, it is also helpful for an
analytics system to provide hints as to where outbreaks
may be occurring. To this end, we have employed the use of
a standard epidemiological algorithm for time series
analysis, the cumulative summation (CUSUM) [22]. The
CUSUM algorithms provide alerts for potential outbreaks in
the temporal domain, and users of our system may then
select these alerts for further exploration in the geospatiotemporal viewing window.



Xt  0 þ kxt
:
ð5Þ
St ¼ max 0; St1 þ
x t
Equation (5) describes the CUSUM algorithm, where St is
the current CUSUM, St1 is the previous CUSUM, Xt is the
count at the current time, 0 is the expected value, xt is the
standard deviation, and k is the detectable shift from the
mean (i.e., the number of standard deviations the data can
be from the expected value before an alert is triggered). We
apply a 28 day sliding window to calculate the mean, 0 ,
and standard deviation, xt , with a 3 day lag, meaning that
the mean and standard deviation are calculated on a 28 day
window 3 days prior to the day in question. Values chosen
were based on the EARS C2 alert method detailed in [22].
Such a lag is used to increase sensitivity to continued

outbreaks while the 28 days provides a month worth of
baseline data to test against while minimizing long-term
historical effects. Fig. 6 shows the application of the
CUSUM algorithm to the temporal plot of ILI counts
during peak flu season. An alert is represented by a large
red circle, which is generated if St exceeds the threshold (for
a point of reference the threshold is typically set at three
standard deviations from the mean in the Early Aberration
Reporting System and is shown as the green line in Fig. 6).

3.6 Exploration with Linked Views
While the alerts generated from aberration detection
algorithms may produce a useful starting point for exploration, they may also be eliciting false alarms. Furthermore,
analysts may want to explore areas where information may
be unknown, for example, visual hotspots generated in our
heatmap approach may contain only sparse data points.
Ideally, analysts would like to dynamically query and select
elements on the visual display in order to see how selections
update related views. This type of selection is commonly
referred to as brushing [5] and it is used in many interactive
visualization environments [33], [36].
For our implementation, we use only the highlight
operation over the time dimension of our temporal view
and the spatial region of our main viewing window. In the
temporal view, the highlighted region is shown in red and
once the mouse button is released, all other information
displays are updated to reflect the selection. Because the
individual plots are interrelated, only one may be brushed at a
time. The principal purpose of this feature is to allow selection
of the current day and the number of days being aggregated
together from the plot windows based on a region of interest
in the plotted data. In Fig. 6a, we see a series of hospital
generated alerts (the red marks) in the middle temporal
viewing window. In this figure, a user has clicked on an alert,
causing the temporal window to lock in place, while scrolling
the geospatial window back in time to the alert on that day.
Notice that the events associated with the hospital/precinct
and category are now exclusively shown on the map.

MACIEJEWSKI ET AL.: A VISUAL ANALYTICS APPROACH TO UNDERSTANDING SPATIOTEMPORAL HOTSPOTS

213

Fig. 7. Contour mapping for contextual cues. (a) The analyst has created a heatmap of one year’s worth of crime data in West Lafayette Indiana with
the yellow polygon representing Purdue University Campus. The contours overlaid represent the noise complaints aggregated over the past
100 days. (b) The analyst plots only the noise complaints aggregated over the past 50 days as a heatmap. (c) The analyst plots the noise complaints
aggregated over the next 50 day period. The contours in this figure represent the previous noise complaint map outline.

In the geospatial view, highlighting is performed
through a circular selection of an area. This circular
selection allows users to select multiple geographic regions
and view their temporal history. In Fig. 6b, we see a
heatmap of the state. In this figure, note that the circled area
represents a user selection. Here, the user has chosen a
region of the state that appears to currently be a syndromic
hotspot. A linked time series analysis view plots the data
from that area in the lower right window. Here, we see that
an alert (small red circle) is found for that area on the day in
question. A user can then further explore these alerts by
clicking on the alerts in the time series window to find the
patients associated with this alert in the geospatial window.

3.7 Temporal Contours
Along with exploring data context through color adjustments, we also provide contour line options for preserving
the temporal history of the data. In the case of using
temporal contours, data shown in the heatmap mode can be
overlaid with the past x-days worth of contour lines. The xdays is a user defined parameter, and the color date
aggregation is shown as a label on the maps, providing
users with the appropriate temporal context. This allows
users to view shifting hotspots across time and analyze the
movements of trends and patterns over days.
In Fig. 7a, the analyst has visualized a heatmap of all
criminal reports in West Lafayette, Indiana, over the past
year. The analyst then overlays a 100 day aggregate of the
noise complaint data from the beginning of the semester as a
contour in order to see which hotspots on the map are most
related to noise. Here, we can see that the noise complaints are
directly correlated with an area of West Lafayette nearest
Purdue’s Campus (the yellow polygon). Next, the analyst
changes mode to only view the heatmap noise complaints
from this time period (Fig. 7b), and then uses the contour
history mode to compare the next 50 day period aggregate of
noise complaints to the previous map (Fig. 7b).
3.8 Multivariate Views
Our system also provides a series of complex viewing
modalities for searching for correlations between multiple
variables. In a two-dimensional view of the geographic area,
one can map the density estimated heatmap color to
variable x, and then create another heatmap for variable y,

thus providing a multivariate view of the geographical
location and the x and y variables. Variable y can then be
displayed as contours overlaid on the heatmap of variable x.
Users can then look for places of high contours and high
colors to search for correlations between data variables.
Furthermore, one can create a view for multiple variables by
assigning a third variable as height. The data can then be
viewed in three dimensions, and users can search for
correlations between three variables simultaneously. The
heatmaps, contours, and height are all calculated based on
the kernel density estimation described earlier in Section 3.3.
In Fig. 8a, the analyst is searching to see if there are any
hotspots showing a correlation between rash cases (the
contours) and shock/coma cases (the color). Here, the
analyst finds that there are large concentrations of cases in
several overlapping areas. The analyst then also chooses to
look for cases associated with respiratory illness, and enters
3D mode. In 3D mode, height now represents the
magnitude of respiratory illness cases. Fig. 8 shows the
3D view with color, contour, and height mapping.

3.9 Interactive Thresholding
Our system also provides users the ability to interactively
select data threshold values that they are interested in. In
order to better focus attention on areas of interest, users
may choose to only look at event values, where the
percentage of events occurring in an area is greater than
some threshold, t. Fig. 9a illustrates an analyst searching for
gastrointestinal hotspots in Indiana. In Fig. 9b, the user has
thresholded the data such that only higher values will
appear. The user then moves forward in time by 10 days
(Fig. 9b) using the temporal contour ghosting to track the
movement of hotspots across the state. Contours are
displayed such that the most recent days are drawn with
a higher opacity, thus, creating the temporal contour
ghosting. However, as the number of historical days being
viewed increases, the number of colors that can be
distinguished in this manner reaches its maximum. As
such, contour ghosting is only effective for a limited
historical basis. Note that the thresholding is also applied
to the contour history as well, where again, the lighter the
contours, the further in the past they have occurred.

214

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 16,

NO. 2,

MARCH/APRIL 2010

Fig. 8. Multivariate views. (a) The analyst has created a heatmap of shock/coma cases overlaid with contours for rash. (b) The analyst adds the
category respiratory as the height dimension.

Fig. 9. Interactive thresholding. (a) The analyst searches for gastrointestinal hotspots. (b) The analyst uses the thresholding capability to filter the
data. (c) The analyst moves forward in time viewing movement trends among the data.

4

UNDERSTANDING HOTSPOTS

By using a combination of geospatial and temporal
visualization and analytics tools, our system provides
analysts with tools for real-time hypothesis generation
and exploration. Here, we present two example cases of
using such a system to analyze data.

4.1 Syndromic Hotspots
To better illustrate the hypothesis generation/exploration
phase, we conducted an informal interview with an Indiana
State Department of Health (ISDH) syndromic surveillance

epidemiologist. During this interview, we discussed how an
epidemiologist would search for syndromic hotspots,
creates an initial hypothesis, and what steps are taken in
an attempt to confirm or deny this hypothesis.
Traditionally, the first items examined when identifying
potential syndromic problem areas are the spatial alerts
generated for a given syndrome. Based on the epidemiologist’s experience, certain alerts will be immediately
resolved as false positives, and others will be moved to
the top of the queue. From the alerts the epidemiologist
identifies as potential problems, a hypothesis is formulated

MACIEJEWSKI ET AL.: A VISUAL ANALYTICS APPROACH TO UNDERSTANDING SPATIOTEMPORAL HOTSPOTS

215

Fig. 10. Using visual analytics for hypothesis exploration in syndromic surveillance. (a) The user observes a heatmap for a given syndrome, in this
case, gastrointestinal. (b) Next, the user selects an area of interest, generating a time series plot for that region. Note that in the time series plot
generated, an alert is occurring on the day of interest. (c) The user then drills down to the hospital level by selecting the neighboring hospital and
generating a time series plot for that emergency department. Here, we see that there is no hospital level alert for gastrointestinal syndromes.
(d) Finally, the user looks for correlating symptoms and filters by the keyword fever. New time series plots are generated. While an alert still exists for
the selected area, the user can now see that this alert was generated by only one individual, meaning an outbreak is unlikely.

stating that a problem with syndrome X is occurring in
patients found at location Y. These alerts are aggregated by
zip code level, meaning that zip codes A, B, C, etc.,
contribute to the alert. From this step, the epidemiologist
would look at the time series data for all zip codes
contributing to the alert in order to gain a better understanding of where the baseline lies. In contrast, our visual
analytics tool allows users to select an arbitrary region to
view the time series data, providing a baseline for the
overall area, potentially allowing quicker comparison.
Often, the next step taken would be to further corroborate the geospatial area of the alert by looking at the
counties involved and pulling up county level alerts, their
corresponding time series plots, and county maps down to
the zip code level. Similarly, our tool provides both
heatmaps at the reduced levels of granularity, as well as a
finer, smoother granularity heatmap option that the
epidemiologist thought may add value. If, from the
heatmap, the hypothesis cannot be rejected, the next step
is to drill down to patient level data in order to assess the
actual chief complaints. For example, if (in the case of a
gastro-intestinal problem) a patient’s “vomiting” is related
to pregnancy, then it is less likely to be part of the
gastrointestinal outbreak being considered in this hypothesis. As such, sometimes potential clusters then fall apart.
Next, the epidemiologist would look at the patient level

data to assess time stamps and actual chief complaints for
clustering which may lead to filtering by ages for clustering
and gender for skew if clues exist that lead the hypothesis
refinement in those directions. If there was a string of
elevated days, then the analyst would group these elevated
days and do the same type of descriptive analysis. Our dual
linked views provide advanced tools for such an operation,
aiding in the overall hypothesis exploration.
During this process, the epidemiologist also searches for
potential “cosyndromes” in the same geography, such as
fever, to see if it is somehow linked to the gastrointestinal
problem. Again, the linked views and filter options of our
system allow the user to easily look at multivariate time
series components. If concurrent syndromes are found, this
potentially strengthens the hypothesis and may lead to a
follow-up with the actual emergency department(s) involved. Fig. 10 illustrates the use of our system during the
hypothesis exploration phase.
First, in Fig. 10a, the user has selected the syndrome he/
she is interested in analyzing, in this case, gastrointestinal.
This generates a query to the database, and the epidemiologist can now look at the patient distribution with either an
additive opacity for all patients that visited an emergency
department, or as an aggregate of the data. Next, the user
visually searches for unusual hotspots using a combination
of the kernel density estimation and the patient overlay. The

216

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

Fig. 11. Sample ESSENCE output showing daily values for a single
Indiana county.

user may select multiple areas for testing; however, if the
area selected shows no temporal alert for the day in
question, then it is likely that the hypothesis of area X being
problematic is rejected.
In Fig. 10b, the user has selected an area of the map in
central Indiana, and the corresponding time series graph
that was generated indicates that the selected area is
showing an alert on the day in question. The next step in
analyzing this alert is to look at data from the nearby
emergency departments. In this case, there is only a
single emergency department. The user clicks on the
hospital glyph on the map, and the time series plot for
this emergency department is generated, see Fig. 10c. In
this time series plot, there is no alert generated for this
emergency department for the day in question. This
weakens the hypothesis that there is an outbreak in the
area; however, the user may still want to take further
steps to confirm/deny the hypothesis.
The next step taken is to look for corresponding
symptoms. In this case, the user looks for patients with
gastrointestinal syndromes that also reported signs of fever.
Fig. 10d shows this filter query. Note that the heatmap and
time series plots are automatically updated from the query.
We can see now that there are no visual hotspots occurring
on the map; however, there is still a time series alert for that
area. Further investigation of the time series alert shows
that the expected number of patients was slightly less than
one, and one patient came in on that day, thereby
generating an alert. It is now unlikely that an outbreak is
occurring in this area, and the hypothesis can be denied
after a brief analysis of the patient record.
While it may seem odd that one case can cause an
outbreak alert, this is quite a common occurrence in all
current systems. For example, the carbon monoxide case
shown in Fig. 1 contains only three emergency department
complaints. Therefore, the high sensitivity is necessary to
avoid missing small cluster cases.

4.2 Syndromic Hotspots in Current Systems
In order to further evaluate our system, we would need to
include experts within the field of syndromic surveillance in
order to reduce training time and develop meaningful test
scenarios. Currently, the Indiana State Department of Health
employs only a few epidemiologists (who we have consulted). While it is possible that other counties in the state
employ epidemiologists with knowledge of syndromic
surveillance, the pool of available subjects is quite limited.

VOL. 16,

NO. 2,

MARCH/APRIL 2010

In regards to system evaluation, we provide a comparison to
the current tools employed at the State Department of Health.
The current tool used by the Indiana State Department of
Health is ESSENCE [27]. The alerts are displayed in a line
listing (Fig. 11) that is reviewed every day with color codes
representing serious and mild alerts. There are region alerts
for counties, hospital alerts, and spatial alerts for detecting
clusters. Selecting an alert from the line listing will bring up
a time series of 90 days, from here you can drill down to the
details of the alert, including the ability to map the patients
by zip code.
As discussed in the previous section, our program is also
able to generate temporal alerts based on any level of spatial
aggregation for counties and hospitals. Future work will
include the introduction of spatial alerts through the use of
SatScan [24]. Our advantages over the ESSENCE system
includes the ability to interactively aggregate data over a
variety of temporal ranges, as well as providing a variety of
spatial aggregation methods (as opposed to only plotting
data by zip code). We also provide enhanced interactive
filtering for multivariate data exploration as well bivariate
views through coloring and contouring and multivariate
views through the addition of height fields.

4.3 Crime Hotspots
Our second expert is an independent evaluator with
experience as an Indiana State Police Commanding Officer.
He is an accomplished security professional having a
25 year operations background in public safety, corporate
security, and the military. He is retired from the Indiana
State Police reaching the rank of Regional Commander. In
this capacity, he was responsible for overseeing the
investigative mission within a 12,000 square mile, 21 county
region in northwest Indiana. His authority covered all
criminal and civil investigations, including death investigations, violent crimes, thefts, burglaries, public corruption,
and internal investigations. Again, many police departments do not employ crime analysts as their budgets are
limited. As such, the use of a single expert for feedback and
evaluation seems to be the most appropriate. The scenario
presented in this section is based on questions the evaluator
wished to ask of the data.
In the case of our crime data, events occur less frequently
than in the case of syndromic surveillance data. As such,
analysts often wish to look at the last x-days or weeks of data
and begin planning new patrol routes based on previous
trouble locations. For our example, the analyst first pulls up
all criminal activities for the 2007-2008 Purdue University
school year (Fig. 12a), and finds two major areas of activities,
one nearby campus, and one near the intersection of two
major cross-streets. In this case, two major hotspots are
evident, one near campus, and one located at an intersection
some distance from the main campus.
Here, the analyst chooses to investigate the causes
leading to the secondary hotspot. First, the analyst searches
for what types of crimes are occurring near the secondary
hotspot by filtering the region by various crime categories
and finds that theft is the leading crime in that area. Next,
the analyst compares the thefts from fall semester in West
Lafayette to the overall criminal activities of the 2007-2008
school year. Fig. 12a shows the overall crimes from the

MACIEJEWSKI ET AL.: A VISUAL ANALYTICS APPROACH TO UNDERSTANDING SPATIOTEMPORAL HOTSPOTS

217

Fig. 12. Using visual analytics for hypothesis testing in crime analysis. The user is analyzing thefts (as contours) versus all crimes (as color) for a
given school year (2007-2008). (a) The user analyzes fall semester thefts (contours) compared with the over all school year crimes. (b) The user
analyzes the last 20 days of the fall semester for thefts. (c) The user analyzes spring semester thefts. (d) The user analyzes the last 20 days of spring
semester.

2007-2008 school year overlayed with contours from the
2007 Fall semester theft reports. Next, the analyst investigates if the end of the fall semester (the week of finals and
the week directly after finals) indicates a high rate of thefts,
and shifts the contours to only map to the last 20 days of the
fall semester (Fig. 12b). The analyst finds that in the fall
semester, no thefts are occurring during this time period.
Next, the analyst chooses to compare with events from
the spring semester. Fig. 12c shows the overall crimes from
the 2007-2008 school year overlayed with contours from the
2008 Spring semester thefts. Next, the analyst investigates if
the end of the Spring semester (finals week and the week
after finals) indicates a high rate of thefts, and overlays the
theft contours from the last 20 days of the semester, Fig. 12d.
Here, the analyst finds that a large number of thefts are
taking place during this time period. The analyst may then
begin forming hypotheses about why this occurs at the end
of the Spring semester (more students moving out of town,
warmer weather) as opposed to at the end of Fall semester
when more houses are empty for the holidays.

Based on this tool, the analyst found that he was easily able
to test hypotheses that he had about the nature of criminal
activities in the West Lafayette community. Currently, with
departments that have no crime analysts, he felt that our tool
provided a simple and intuitive means of analyzing that data
the department would benefit from. Being able to visually
understand what the data represent is important at the
tactical level for the street officer to anticipate problems
before they occur. For the Police Chief, visualization provides
a strategic advantage in deploying resources, managing
budgets, and developing strategies for crime reduction and
predictive analytics. Our expert found this to be the next
generation of crime mapping technology that along with its
mobile capabilities and ease of use, could be easily integrated
into current law enforcement products and techniques.

5

CONCLUSIONS AND FUTURE WORK

Our current work demonstrates the benefits of visual
analytics for understanding syndromic hotspots. By linking
a variety of data sources and models, we are able to enhance

218

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

the hypothesis generation and exploration abilities of our
state partners. Our initial results show the benefits of
linking traditional time series views with geospatiotemporal views for enhanced exploration and data analysis. Our
system also moves away from traditional spatial histogram
visualizations, providing a finer granularity of heatmap for
more accurate hotspot detection.
Other future work includes advanced modeling of
geospatiotemporal data for enhanced data exploration and
hotspot detection. Furthermore, we plan to include a suite of
aberration detection algorithms and their corresponding
control charts for enhanced alert detection in the temporal
domain. We also plan on employing spatiotemporal clustering algorithms (such as SatScan [24]) for hotspot detection as
well as other correlative analysis views within the temporal
domain (scatter plots, calendar view, etc.). Furthermore, we
plan to enhance our system from a visual analytics system to
a predictive analytics system, creating views to allow for
event planning, prediction and interdiction. Once these
features are implemented, we plan to deploy our system
with our state partners for further evaluation.

ACKNOWLEDGMENTS
The authors would like to thank the Purdue University
Student Health Center, the Indiana State Department of
Health, and the Police Department of West Lafayette,
Indiana, for providing the data. This work has been funded
by the US Department of Homeland Security Regional
Visualization and Analytics Center (RVAC) Center of
Excellence and the US National Science Foundation (NSF)
under Grants 0811954, 0328984, and 0121288.

REFERENCES
[1]

L. Blanton et al., “Update: Influenza Activity—United States and
Worldwide, 2006-07 Season, and Composition of the 2007-08
Influenza Vaccine,” Morbidity and Mortality Weekly Report, vol. 56,
pp. 789-794, 2007.
[2] W. Aigner, S. Miksch, W. Muller, H. Schumann, and C. Tominski,
“Visual Methods for Analyzing Time-Oriented Data,” IEEE Trans.
Visualization and Computer Graphics, vol. 14, no. 1, pp. 47-60, Jan./
Feb. 2008.
[3] W. Aigner, S. Miksch, B. Thurnher, and S. Biffl, “Planninglines:
Novel Glyphs for Representing Temporal Uncertainties and Their
Evaluation,” Proc. Ninth Int’l Conf. Information Visualization (IV ’05)
2005.
[4] L. Anselin, I. Syabri, and O. Smirnov, “Visualizing Multivariate
Spatial Correlation with Dynamically Linked Windows,” Proc.
Workshop New Tools for Spatial Data Analysis, CD-ROM, 2002.
[5] R.A. Becker and W.S. Cleveland, “Brushing Scatterplots,” Technometrics, vol. 29, no. 2, pp. 127-142, 1987.
[6] C.A. Brewer, Designing Better Maps: A Guide for GIS Users. ESRI
Press, 2005.
[7] A. Buja, D. Cook, and D. Swayne, “Interactive High Dimensional
Data Visualization,” J. Computational and Graphical Statistics, vol. 5,
pp. 78-99, 1996.
[8] P. Buono, A. Aris, C. Plaisant, A. Khella, and B. Shneiderman,
“Interactive Pattern Search in Time Series,” Proc. Conf. Visualization and Data Analysis, pp. 175-186, 2005.
[9] T. Butkiewicz, W. Dou, Z. Wartell, W. Ribarsky, and R. Chang,
“Multi-Focused Geospatial Analysis Using Probes,” IEEE Trans.
Visualization and Computer Graphics, vol. 14, pp. 1165-1172, Nov./
Dec. 2008.
[10] C.C. Calhoun, C.E. Stobbart, D.M. Thomas, J.A. Villarrubia, D.E.
Brown, and J.H. Conklin, “Improving Crime Data Sharing and
Analysis Tools for a Web-Based Crime Analysis Toolkit: Webcat
2.2,” Proc. 2008 IEEE Systems and Information Eng. Design Symp.,
2008.

VOL. 16,

NO. 2,

MARCH/APRIL 2010

[11] W.W. Chapman, J.N. Dowling, and M.M. Wagner, “Classification
of Emergency Department Chief Complaints into 7 Syndromes: A
Retrospective Analysis of 527,228 Patients,” Annals of Emergency
Medicine, vol. 46, pp. 445-455, Nov. 2005.
[12] L. Chittaro and C. Combi, “Visualizing Queries on Databases of
Temporal Histories: New Metaphors and Their Evaluation,” Proc.
IEEE Symp. Information Visualization (INFOVIS ’01), p. 159, 2001.
[13] W.S. Cleveland, Visualizing Data. Hobart Press, 1993.
[14] T. Cormen, C. Leiserson, R. Rivest, and C. Stein, Introduction to
Algorithms. The MIT Press, 2001.
[15] G. Dang, C. North, and B. Shneiderman, “Dynamic Queries and
Brushing on Choropleth Maps,” Proc. Fifth Int’l Conf. Information
Visualization (IV ’01), pp. 757-764, 2001.
[16] R.M. Edsall, A.M. MacEachren, and L. Pickle, “Case Study: Design
and Assessment of an Enhanced Geographic Information System
for Exploration of Multivariate Health Statistics,” Proc. IEEE Symp.
Information Visualization (INFOVIS ’01), pp. 159-162, 2001.
[17] Information Visualization in Data Mining and Knowledge Discovery,
U. Fayyad, G.G. Grinstein, and A. Wierse, eds. Morgan Kaufmann
Publishers, Inc., 2002.
[18] M. Gibin, P. Longley, and P. Atkinson, “Kernel Density Estimation
and Percent Volume Contours in General Practice Catchment
Area Analysis in Urban Areas,” Proc. Geographical Information
Science Research Conf., 2007.
[19] S.J. Grannis, M. Wade, J. Gibson, and J.M. Overhage, “The Indiana
Public Health Emergency Surveillance System: Ongoing Progress,
Early Findings, and Future Directions,” Proc. Am. Medical
Informatics Assoc. Ann. Symp., 2006.
[20] Hargrove and Hoffman, “Using Multivariate Clustering to
Characterize Ecoregion Borders,” Proc. Computing in Science &
Eng., the AIP and the IEEE Computer Soc., vol. 1, pp. 18-25, 1999.
[21] S. Havre, E. Hetzler, P. Whitney, and L. Nowell, “Themeriver:
Visualizing Thematic Changes in Large Document Collections,”
IEEE Trans. Visualization and Computer Graphics, vol. 8, no. 1, pp. 920, Jan.-Mar. 2002.
[22] L.C. Hutwagner, W.W. Thompson, and G.M. Seeman, “The
Bioterrorism Preparedness and Response Early Aberration Reporting System (EARS),” J. Urban Health, vol. 80, no. 2, pp. i89-i96,
2003.
[23] D. Kao, A. Luo, J.L. Dungan, and A. Pang, “Visualizing Spatially
Varying Distribution Data,” Proc. Sixth Int’l Conf. Information
Visualization, pp. 219-225, 2002.
[24] M. Kulldorff, “A Spatial Scan Statistic,” Comm. Statistics: Theory
and Methods, vol. 26, pp. 1481-1496, 1997.
[25] A.D. Langmuir, “The Surveillance of Communicable Diseases of
National Importance,” New England J. Medicine, vol. 268, pp. 182192, 1963.
[26] K. Liao, “A Visualization System for Space-Time and Multivariate
Patterns (VIS-STAMP),” IEEE Trans. Visualization and Computer
Graphics, vol. 12, no. 6, pp. 1461-1474, Nov./Dec. 2006.
[27] J.S. Lombardo, “A Systems Overview of the Electronic Surveillance System for the Early Notification of Community Based
Epidemics (ESSENCE II),” J. Urban Health, vol. 80, pp. 32-42, 2003.
[28] J.W. Loonsk, “Biosense—A National Initiative for Early Detection
and Quantification of Public Health Emergencies,” Morbidity and
Mortality Weekly Report, vol. 53, pp. 53-55, 2004.
[29] A.L. Love, A. Pang, and D.L. Kao, “Visualizing Spatial Multivalue
Data,” IEEE Computer Graphics and Applications, vol. 25, no. 3,
pp. 69-79, May/June 2005.
[30] A.M. MacEachren, F.P. Boscoe, D. Haug, and L. Pickle,
“Geographic Visualization: Designing Manipulable Maps for
Exploring Temporally Varying Georeferenced Statistics,” Proc.
IEEE Symp. Information Visualization (INFOVIS ’98), p. 87, 1998.
[31] R. Maciejewski, S. Rudolph, R. Hafen, A. Abusalah, M. Yakout, M.
Ouzzani, W.S. Cleveland, S.J. Grannis, M. Wade, and D.S. Ebert,
“Understanding Syndromic Hotspots—A Visual Analytics Approach,” Proc. IEEE Symp. Visual Analytics Science and Technology
(VAST), Oct. 2008.
[32] R. Maciejewski, B. Tyner, Y. Jang, C. Zheng, R. Nehme, D.S. Ebert,
W.S. Cleveland, M. Ouzzani, S.J. Grannis, and L.T. Glickman,
“Lahva: Linked Animal-Human Health Visual Analytics,” Proc.
IEEE Symp. Visual Analytics Science and Technology, Oct. 2007.
[33] A.R. Martin and M.O. Ward, “High Dimensional Brushing for
Interactive Exploration of Multivariate Data,” Proc. Sixth Conf.
Visualization (VIS ’95), pp. 271-278, 1995.

MACIEJEWSKI ET AL.: A VISUAL ANALYTICS APPROACH TO UNDERSTANDING SPATIOTEMPORAL HOTSPOTS

[34] S.F. Messner and L. Anselin, “Spatial Analyses of Homicide with
Areal Data,” Spatially Integrated Social Science CD-ROM, pp. 127144, Oxford Univ. Press, 2002.
[35] C.-C. Pan and P. Mitra, “Femarepviz: Automatic Extraction and
Geo-Temporal Visualization of Fema National Situation Updates,”
Proc. IEEE Symp. Visual Analytics Science and Technology (VAST
’07), pp. 11-18, Oct./Nov. 2007.
[36] J.C. Roberts and M.A.E. Wright, “Towards Ubiquitous Brushing
for Information Visualization,” Proc. Int’l Conf. Information
Visualization (IV ’06), pp. 151-156, 2006.
[37] B.W. Silverman, Density Estimation for Statistics and Data Analysis.
Chapman & Hall/CRC, 1986.
[38] J. Stasko, C. Gorg, Z. Liu, and K. Singal, “Jigsaw: Supporting
Investigative Analysis through Interactive Visualization,” Proc.
IEEE Symp. Visual Analytics Science and Technology, pp. 131-138,
2007.
[39] S.B. Thacker, R.L. Berkelman, and D.F. Stroup, “The Science of
Public Health Surveillance,” J. Public Health Policy, vol. 10, pp. 187203, 1989.
[40] Illuminating the Path: The R&D Agenda for Visual Analytics.
J.J. Thomas and K.A. Cook, eds. IEEE Press, 2005.
[41] C. Tominski, J. Abello, and H. Schumann, “Axes-Based Visualizations with Radial Layouts,” Proc. ACM Symp. Applied Computing
(SAC ’04), pp. 1242-1247, 2004.
[42] C. Tominski, P. Schulze-Wollgast, and H. Schumann, “Visual
Analysis of Health Data,” Proc. Int’l Information Resource Management Assoc. (IRMA) Conf., 2003.
[43] C. Tominski, P. Schulze-Wollgast, and H. Schumann, “3D
Information Visualization for Time Dependent Data on Maps,”
Proc. Int’l Conf. Information Visualization (IV), 2005.
[44] C. Weaver, “Multidimensional Visual Analysis Using CrossFiltered Views,” Proc. IEEE Symp. Visual Analytics Science and
Technology (VAST), Oct. 2008.
[45] M. Weber, M. Alexa, and W. Muller, “Visualizing Time-Series on
Spirals,” Proc. IEEE Symp. Information Visualization (INFOVIS ’01),
pp. 7-14, Oct. 2001.
[46] H. Zhao, B. Shneiderman, and C. Plaisant, “Improving Accessibility and Usability of Geo-Referenced Statistical Data,” Proc. 2003
Ann. Nat’l Conf. Digital Govt. Research (DGO ’03), p. 1, 2003.
Ross Maciejewski received the MS degree in
electrical and computer engineering from Purdue
University and the BS degree from the University
of Missouri, Columbia. He is a PhD student in
electrical and computer engineering at Purdue
University. His research interests include nonphotorealistic rendering, volume rendering, and
visual analytics. He is a student member of the
IEEE and the IEEE Computer Society.

Stephen Rudolph received the BS degree in
computer systems engineering from Arizona
State University. He is a master’s student in
electrical computer engineering at Purdue University. His research interests include casual
information visualization and visual analytics.

Ryan Hafen received the MStat degree in
mathematics from the University of Utah and
the BS degree in statistics from Utah State
University. He is a PhD student in Statistics at
Purdue University. His research interests include exploratory data analysis and visualization, massive data, computational statistics, time
series, modeling, and nonparametric statistics.

219

Ahmad M. Abusalah received the BS degree in
mechanical engineering from Jordan University
of Science and Technology, the BS degree in
computer sciences and information systems
from Philadelphia University-Jordan, the MS
degree in computer sciences from the University
of Illinois, and the MS degree in computer
sciences from Purdue University in 2000, 2003,
2006, and 2009, respectively. Currently, he is
pursuing the PhD degree in health informatics at
the University of Minnesota. He is involved in many healthcare-related
applications as database administrator or engineer. For example, health
surveillance systems, cancer care engineering hub, cardiac patient
management system, and chemical database. His main interests are
databases and scientific data management within healthcare domain.
Recently, he was awarded the Graduate School Fellowship in Health
Informatics from the University of Minnesota.

Mohamed Yakout is a member of Indiana
Center of Database Systems (ICDS). His research areas of interests are data quality and
integration, privacy preserving data integration,
data mining, and digital libraries.

Mourad Ouzzani received the PhD degree in computer science from
Virginia Tech. He is a research assistant professor in the Cyber Center,
Discovery Park, at Purdue University. His research interests cut across
databases, Web services, and Semantic Web with an emphasis on data
integration, data quality, service querying and optimization, and data/
service support for life sciences. He is involved in several projects for
enabling discovery in life sciences, accessing biological databases using
Web services, and providing database support for public health
surveillance. He has published several papers in international journals
and conferences including the IEEE TKDE, the IEEE Internet Computing, the IEEE Computer, Bioinformatics, VLDB, CIDR, ICDE, and
SIGMOD. He was selected as an outstanding reviewer by the IEEE
Internet Computing in 2002. He was a recipient of the USENIX
scholarship in 2000.

William S. Cleveland is the Shanti S. Gupta
distinguished professor of statistics and courtesy
professor of computer science at Purdue University. His areas of methodological research are
in statistics, machine learning, and data visualization. He has analyzed data sets ranging from
very small to very large in his research in
computer networking, homeland security, visual
perception, environmental science, healthcare
engineering, and customer opinion polling. In the
course of this work, he has developed many new methods that are widely
throughout engineering, science, medicine, and business; in 2002, he
was selected as a highly cited researcher by the American Society for
Information Science and Technology in the newly formed mathematics
category. He has carried out fundamental work in data visualization. His
two visualization books, The Elements of Graphing Data and Visualizing
Data, are widely read and reviewed. In graphical perception, he set out
basic theory and carried out many experiments. With Richard A. Becker,
he developed the trellis display framework for visualization, used by a
worldwide community of data analysts via its implementation in two
software systems based on the S language: S-Plus (commercial) and R
(open source).

220

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

Shaun J. Grannis received the BS degree in
aerospace engineering from the Massachusetts
Institute of Technology (MIT), the MD degree
from Michigan State University, and the MS in
clinical research and informatics from Indiana
University. He is an assistant professor of family
medicine at Indiana University and medical
informatics research scientist at the Regenstrief
Institute in Indianapolis, where his interests
include developing, implementing, and studying
technology to overcome the challenges of integrating data from
distributed systems for use in healthcare delivery and research.

VOL. 16,

NO. 2,

MARCH/APRIL 2010

David S. Ebert is a professor in the School of
Electrical and Computer Engineering at Purdue
University, a university faculty scholar, a fellow
of the IEEE and the IEEE Computer Society, the
director of Purdue University Rendering and
Perceptualization Lab (PURPL), and the director
of Purdue University Regional Visualization and
Analytics Center (PURVAC), which is a part of
the Department of Homeland Security’s Regional Visualization and Analytics Center of Excellence. He performs research in novel visualization techniques, visual
analytics, volume rendering, information visualization, perceptually
based visualization, illustrative visualization, and procedural abstraction
of complex, massive data. He has been very active in the visualization
community, teaching courses, presenting papers, cochairing many
conference program committees, serving on the ACM SIGGRAPH
Executive Committee, serving as an editor in chief of the IEEE
Transactions on Visualization and Computer Graphics, serving as a
member of the IEEE Computer Society’s Publications Board, serving on
the IEEE Computer Society Board of Governors, and successfully
managing a large program in external funding to develop more effective
methods for visually communicating information.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

Visualizing Social Media Sentiment in Disaster Scenarios
Yafeng Lu

Xia Hu

Feng Wang

Arizona State University

Arizona State University

Arizona State University

lu.yafeng@asu.edu
Shamanth Kumar

xia.hu@asu.edu
Huan Liu

fwang49@asu.edu
Ross Maciejewski

Arizona State University

Arizona State University

Arizona State University

huan.liu@asu.edu

rmacieje@asu.edu

shamanth.kumar@asu.edu
ABSTRACT

manitarian activities [21]. For instance, in an emergency
situation [19], some users generate information either by
providing first-person observations or by bringing relevant
knowledge from external sources. Twitter, with its real-time
nature, has been successfully used as a sensor of earthquakes
[12] and wildfires [17]. Furthermore geo-located Twitter
data has been shown to be a reliable source for detecting
disasters and investigating response [7]. While social media
mining has been widely used in different disaster scenarios,
one of the most important aspects to understand social responses is to gauge people’s opinion for improved disaster
management [10, 13, 20].
To understand public sentiment during disasters, an accurate sentiment classifier is required. While sentiment analysis has been extensively studied for some domains, such as
product reviews [8, 11], the performance on social media
data is still unsatisfactory due to the distinct data characteristics [5, 6]. First, social media posts are always short
and unstructured. For example, Twitter allows no more
than 140 characters and uses many informal words such as
“cooool” and “OMG” . The short texts can hardly provide
sufficient statistical information for learning based models.
Second, it is labor intensive and time consuming to obtain
ground truth for training data, which is needed to build an
effective supervised learning model. In this paper, we study
this problem from a novel aspect with visual analytics.
Visual analytics is widely used in social media data analysis and contributes in many areas of exploratory data analysis, such as geographical analysis [2], information diffusion
[22] and business prediction [9]. Besides showing the data
intuitively, visual analytics enables users to navigate through
the data, compare different metrics or datasets, and interactively explore patterns. In this paper, we propose a visual
analytics framework to explore geo-located Twitter data in
disaster scenarios specifically focusing on sentiment. This
framework enables us to observe the distribution of Tweets,
compare between positive and negative sentiment, and study
the sentiment predictions from multiple models. The research questions motivating our visual analytics framework
can be described as follows.

Recently, social media, such as Twitter, has been successfully used as a proxy to gauge the impacts of disasters in real
time. However, most previous analyses of social media during disaster response focus on the magnitude and location of
social media discussion. In this work, we explore the impact
that disasters have on the underlying sentiment of social media streams. During disasters, people may assume negative
sentiments discussing lives lost and property damage, other
people may assume encouraging responses to inspire and
spread hope. Our goal is to explore the underlying trends in
positive and negative sentiment with respect to disasters and
geographically related sentiment. In this paper, we propose
a novel visual analytics framework for sentiment visualization of geo-located Twitter data. The proposed framework
consists of two components, sentiment modeling and geographic visualization. In particular, we provide an entropybased metric to model sentiment contained in social media
data. The extracted sentiment is further integrated into a
visualization framework to explore the uncertainty of public
opinion. We explored Ebola Twitter dataset to show how visual analytics techniques and sentiment modeling can reveal
interesting patterns in disaster scenarios.

Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous;
H.5 [Information Interfaces and Presentation]: User
Interfaces; I.2 [Artificial Intelligence]: Natural Language
Processing

General Terms
Sentiment Analysis, Social Media Visual Analytics

1.

INTRODUCTION

Social media data, encapsulating knowledge chunks about
events and people’s opinion, is sensitive to disasters and hu-

RQ1 How can we reveal disagreements among multiple sentiment classifiers?
Copyright is held by the International World Wide Web Conference Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink to the
author’s site if the Material is used in electronic media.
WWW 2015 Companion, May 18–22, 2015, Florence, Italy.
ACM 978-1-4503-3473-0/15/05.
http://dx.doi.org/10.1145/2740908.2741720.

RQ2 Does positive sentiment exist in a disaster scenario? If
yes, can we compare the patterns between the distribution of positive and negative sentiment.

1211

Figure 1: Sentiment analysis and visualization overview of sentiment analysis on our Ebola Twitter dataset.
The two maps make up the geo-comparison view. The list on the right contains the top Tweets ordered by
their retweet count. The bottom view shows our entropy sentiment river.

Table 1: Statistics of the ebola dataset
Classifier #Positive #Negative #Neutral
CoreNLP 24089
529848
138630
SentiStrength 67506
247643
377418
51243
601989
SentiWordNet 39335
Committee Vote 25673
261500
405394

2.

Table 2: Statistics of geo-located ebola dataset
Classifier #Positive #Negative #Neutral
CoreNLP 849
10002
3474
4750
7128
SentiStrength 2447
SentiWordNet 1018
1512
11795
5517
7873
Committee Vote 935

MULTI-CLASSIFIER SENTIMENT ANALYSIS AND VISUALIZATION

The goal of our work is to develop a visual analytics framework for sentiment analysis on social media data relating to
disasters, particularly Twitter data, so that users can find
disagreements among multiple sentiment classifiers, observe
the uncertainty of sentiment predictions, and investigate interesting sentiment distribution patterns, as well as compare between the distributions. To achieve this goal and
answer RQ1 and RQ2, we propose a visual analytics framework consisting of an entropy-based sentiment model and a
geographical visualization.
To test our sentiment analysis method and visual analytics
framework, we carried out an experiment on Ebola Twitter
dataset, which has been collected using the Twitter Search
API with keyword “ebola”. From September 1st to September 8th, this dataset has 567,015 Tweets among which 5,338
have geographical locations. The statistics of this dataset
in regard to sentiment classifiers are shown in Table 1 (the
whole dataset) and Table 2 (only geo-located Tweets).

2.1

Table 1 and Table 2 show the supporting statistical results.
To answer the first research question (RQ1), we propose
a metric to evaluate the inconsistency between sentiment
classes and then use a committee vote method to decide on
a Tweet’s sentiment class.
Since entropy is a well defined metric for measuring the
level of disagreement, we define an uncertainty measure using vote entropy [3] to gauge the disagreement among multiple classifiers. Our uncertainty is defined as:
U C = 1 − normalize(M ax Entropy − Entropy)

(1)

where Entropy is defined as follows.
Entropy = −

K
X
V (yi )
V (yi )
log
C
C
i=1

(2)

Here V (yi ) is the number of “votes” that a class (Ki ) receives from among the committee members’ prediction, K
denotes the number of classes, and C is the committee size.
Max Entropy is the highest possible entropy given C and
K. There are two situations, C ≤ K and C > K. When
C ≤ K, the entropy is maximized when no two votes go to
the same class, thus M ax Entropy = log C. When C > K,
the entropy is maximized when the difference of the number
of votes between any two classes is no larger than 1. Without loss of generality, assume C = Kt + d with t and d being

Sentiment Modeling

The disagreement among multiple sentiment classifiers is
shown from a primary study on the Ebola dataset using
three well-know sentiment analysis classifiers [1, 16, 18].

1212

and neutral sentiment is colored white. While setting opacity, the dense area can be identified by the deeper color. A
Tweet whose U C is above a threshold will be represented
by the blurred glyph.
Regarding RQ2, this view shows a kernel density estimation (KDE) map and implements a sentiment comparison
lens. The sentiment KDE is obtained by first splitting the
sample Tweets into positive and negative groups and then
calculating the fixed bandwidth KDE [15].


N
|x − xi |
1 X 1
K
(3)
fˆh (x) =
N i=1 hd
h

Figure 2: Glyph of sentiment representation for
Tweets. The left most one is a Tweet with high uncertainty represented by a blurred circle (threshold
= 0.6). Others are relatively certain Tweets classified into negative. The deep red area indicates multiple negative Tweets that overlap each other (opacity = 0.5).

Here, h is the bandwidth, d is the data dimension, in our
case d = 2 for spatial data, N is the total number of samples.
|x − xi | is the Sphere Mercator projection distance between
two locations, and the kernel function is:
K(x) =

i=d+1

Having uncertainty described with regard to entropy, we
can reveal the disagreement among multiple classifiers. Previous works have shown that even a small committee can
improve the performance of prediction in practice [4, 14].
In this paper,we take the majority of the committee’s predictions as the final label to increase confidence.
Regarding RQ1, in our visualization design, we create a
Tweet sentiment glyph and an entropy sentiment river to
represent the uncertainty. On the map view, the sentiment
of Tweets is labeled as the majority vote from the committee
for a confident sentiment class representation. In our pilot
experiment, we used the following three sentiment classifiers
for our committee: SentiWordNet [1], SentiStrength [18]
and CoreNLP [16]. SentiWordNet generates a decimal score
from -1 to 1, with -1 being the most negative, 1 being the
most positive and 0 being neutral. SentiStrength (trinary)
assigns integer scores from -4 to 4 to each Tweet, with 0
being the neutral. And CoreNLP classifies each Tweet into
5 classes scored from 0 to 4, with 2 being neutral.

2.2

(4)

where the indicator function I(x2 ≤1) is evaluated as 1 when
(x2 ≤ 1), and 0 otherwise.
When a user clicks on the “Combine KDE” button on the
top-right side of the overview, a kernel density estimation
based on positive Tweets and negative Tweets will be calculated and visualized on the dual map, as shown in the left
two maps in Figure 3. The density map pair shows the
distribution patterns of the sentiment, as well as the similar
and different hot spots.
To enable quick density distribution comparison, we propose a novel sentiment comparison lens to show the contrast
of a positive sentiment distribution and a negative sentiment
distributions by alpha blending the images (the right map
in Figure 3). In blending the KDE images, we blend the
S(source) over the D(destination), e.g. positive KDE over
negative KDE. The alpha blending algorithm used in our
system can be described as:

positive integers and d < K, then the entropy is maximized
when there are d labels having t + 1 votes for each while
the other K − d labels
for each, and
 d having t votes K
 thus
P t+1
P t
t+1
t
log C +
log C .
M ax Entropy = −
C
C
i=1

2
(1 − (x2 ))I(x2 ≤1)
π

OA = SA + DA (1 − SA )

0,
ORGB =
(SRGB SA + DRGB DA (1 − SA ))/OA ,

Visual Analytics Framework

To explore the underlying sentiment of our Twitter dataset,
we have developed a visual analytics framework, in which
our sentiment model is used to show the uncertainty of sentiment prediction among multiple classifiers and enable sentiment distribution analysis. The proposed framework consists of three linked views: the geo-comparison dual map,
the top Tweets list, and the entropy sentiment river(Figure
1). The top Tweet list is linked with time and area selection
to show the most popular Tweets.
The geo-comparison dual map is designed for displaying
the geographical sentiment distribution of Tweets. It has
two maps centering on the same region and displaying positive and negative Tweets synchronously. Tweets are displayed as translucent color coded circle glyphs (Figure 2)
to show the sentiment, uncertainty and density. Positive
sentiment is colored blue, negative sentiment is colored red,

1213

if OA = 0
otherwise.

In this expression, O is the output color, S is the source color
and D is the destination color with subscript A representing
the alpha channel and RGB representing the RGB color
channel. Users can move the lens to investigate both the
positive and negative sentiment on one map to find overlaps,
exclusions, and differences in distribution patterns.
Our geo-comparison view also supports circle, rectangle
and polygon selection by which only Tweets in the user defined area are displayed. This selection is linked with the top
Tweets list so that the list will update to the most retweeted
Tweets posted in the given time range from the selected area.
Our third view, the entropy sentiment river, is designed to
reveal the uncertainty of sentiment classification over time.
Additionally, when setting the class label by a single classifier, it shows the prediction bias of the classifier under analysis. The entropy sentiment river is developed based on sentiment river [9] by adding uncertainty information. In Figure
1, the bottom view shows the entropy sentiment river with
blue representing the volume of positive Tweets and red representing the volume of negative Tweets. A lower opacity is
used when the Tweets have a high average uncertainty in a
particular time chunk. The volume in each polarity refers to

Figure 3: Geo-comparison view with kernel density estimation on positive and negative Tweet sentiment and
the sentiment comparison lens blending negative sentiment distribution over positive sentiment distribution.
This KDE is calculated using the Ebola dataset with fixed bandwidth of 55 miles.

SentiWordNet in Figure 1. It can change to other classifiers
or the majority vote from a committee.

3.

in the negative distribution is denser than the one in the
positive distribution. By comparing with the Boston hot
spots, Washington DC has a higher percentage of positive
Tweets posted because the color code in the combined image contains more blue in that hot spot. From exploring the
sentiment distribution patterns, we may assume that there
are some positive opinions related to Ebola starting from
Washington DC. This may provide a clue for the users to
compare the effects of local activities in different places.

CASE STUDY - SENTIMENT IN EBOLA
DATASET

In our experiment, we loaded the Ebola dataset, described
in section 2. The geo-comparison dual map shows the positive Tweets vs. negative Tweets. It is obvious that negative
Tweets (represented by red) has a higher volume than positive Tweets (represented by blue). However, from this view,
users can find some positive hot spots in the disaster scenario with non-negligible magnitude. The sentiment label
for this view is decided by the majority of the committee.
In contrast, the entropy sentiment river uses a single classifier, SentiWordNet, in this example. From the entropy
sentiment river, the magnitude of positive and negative sentiment trends similarly, which is different from the impression gained from the maps. It also shows many low opacity
chunks along the river, especially on the positive side. This
indicates that SentiWordNet is likely to provide a positivebiased label for the Ebola disaster dataset. This result can
also be confirmed by evaluating the polarity proportion from
Table 1 and Table 2. The inconsistency in the conveyed
sentiment volume from the map and the entropy sentiment
river and the uncertainty visualization both reveal the disagreement problem among multiple sentiment classifiers.
The geo-comparison dual map also shows that in general,
the east coast cities, such as the areas around Washington
DC and Boston, have more Tweets. For negative Tweets,
we observe other possible hot spots, such as Chicago and
Atlanta. To see the density distribution accurately, we generated the density maps shown in Figure 3. Now the magnitude is normalized and the distribution is more clear. From
the density maps, we confirmed our hypothesis of Washington DC and Boston being hot spots of negative Tweets;
however it shows that these two cities are the hot spots for
positive Tweets too. Additionally, we can see that there is
a slight hot spot around New York City. To compare these
two density distributions, we used our sentiment comparison lens to look at the mixture of these two maps, and
this is shown on the right map in Figure 3. Now with the
lens, we can clearly see that the New York City hot spot

4.

CONCLUSION AND FUTURE WORK

We have presented a sentiment visual analytics framework for social media. This framework consists of sentiment
modeling and geographical visualization components, and
answers our research questions. The uncertainty under multiple classifiers’ prediction is measured by means of entropy
and further visualized in the Tweets’ glyphs on the map and
the chunks of the entropy sentiment river. Through this visualization design, users are able to detect places with high
or low sentiment confidence and the change of sentiment polarity and uncertainty. The sentiment distributions can be
analyzed through our KDE maps, and compared via using
the sentiment comparison lens. By analyzing the sentiment
distribution, users can locate hot spots and reveal similarities and differences between the distributions. From our
Ebola case study, we demonstrated the usage of the framework and explained the sentiment investigation.
Our future work includes (1) extending the committee
vote method by involving more classifiers and evaluating the
accuracy between voted prediction and single classifier’s prediction, (2) generalizing our density map comparison lens to
other differential metrics and evaluating the effects on users’
perception, and (3) applying the comparison lens to other
kind of data measures besides sentiment.

5.

ACKNOWLEDGMENTS

This material is based upon work supported by, or in
part by, the U.S. Army Research Office (ARO) under contract/grant number 025071, the National Science Foundation (NSF) under grant number IIS-1217466 and 1350573,
and the Office of Naval Research (ONR) under grant number
N000141410095.

1214

6.

REFERENCES

[12] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake
shakes twitter users: real-time event detection by
social sensors. In Proceedings of the 19th international
conference on World wide web, pages 851–860, 2010.
[13] A. Schulz, T. Thanh, H. Paulheim, and I. Schweizer. A
fine-grained sentiment analysis approach for detecting
crisis related microposts. ISCRAM 2013, 2013.
[14] B. Settles. Active learning literature survey. Computer
Sciences Technical Report 1648, University of
Wisconsin, Madison, 52:55–66, 2010.
[15] B. W. Silverman. Density estimation for statistics and
data analysis, volume 26. CRC press, 1986.
[16] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D.
Manning, A. Y. Ng, and C. Potts. Recursive deep
models for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642, 2013.
[17] J. Sutton, L. Palen, and I. Shklovski. Backchannels on
the front lines: Emergent uses of social media in the
2007 southern california wildfires. In Proceedings of
the 5th International ISCRAM Conference, pages
624–632. Washington, DC, 2008.
[18] M. Thelwall, K. Buckley, and G. Paltoglou. Sentiment
strength detection for the social web. Journal of the
American Society for Information Science and
Technology, 63(1):163–173, 2012.
[19] S. Vieweg. Microblogged contributions to the
emergency arena: Discovery, interpretation and
implications. Computer Supported Collaborative Work,
pages 515–516, 2010.
[20] B. Vo and N. Collier. Twitter emotion analysis in
earthquake situations. International Journal of
Computational Linguistics and Applications,
4(1):159–173, 2013.
[21] D. Yates and S. Paquette. Emergency knowledge
management and social media technologies: A case
study of the 2010 haitian earthquake. International
Journal of Information Management, 31(1):6–13, 2011.
[22] J. Zhao, N. Cao, Z. Wen, Y. Song, Y.-R. Lin, and
C. Collins. # fluxflow: Visual analysis of anomalous
information spreading on social media. In 2014 IEEE
Conference on Visual Analytics Science and
Technology (VAST), 2014.

[1] S. Baccianella, A. Esuli, and F. Sebastiani.
Sentiwordnet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Proceedings
of the International Conference on Language
Resources and Evaluation, volume 10, pages
2200–2204, 2010.
[2] J. Chae, D. Thom, H. Bosch, Y. Jang, R. Maciejewski,
D. S. Ebert, and T. Ertl. Spatiotemporal social media
analytics for abnormal event detection and
examination using seasonal-trend decomposition. In
2012 IEEE Conference on Visual Analytics Science
and Technology (VAST), pages 143–152, 2012.
[3] I. Dagan and S. P. Engelson. Committee-based
sampling for training probabilistic classifiers. In
ICML, volume 95, pages 150–157, 1995.
[4] X. Hu, J. Tang, H. Gao, and H. Liu. Actnet: Active
learning for networked texts in microblogging. In
SDM, pages 306–314, 2013.
[5] X. Hu, J. Tang, H. Gao, and H. Liu. Unsupervised
sentiment analysis with emotional signals. In
Proceedings of the 22nd international conference on
World Wide Web, pages 607–618, 2013.
[6] X. Hu, L. Tang, J. Tang, and H. Liu. Exploiting social
relations for sentiment analysis in microblogging. In
Proceedings of the sixth ACM international conference
on Web search and data mining, pages 537–546, 2013.
[7] S. Kumar, X. Hu, and H. Liu. A behavior analytics
approach to identifying tweets from crisis regions. In
Proceedings of the 25th ACM conference on Hypertext
and social media, pages 255–260, 2014.
[8] B. Liu. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
5(1):1–167, 2012.
[9] Y. Lu, F. Wang, and R. Maciejewski. Business
intelligence from social media: A study from the vast
box office challenge. IEEE Computer Graphics and
Applications, pages 58–69, 2014.
[10] B. Mandel, A. Culotta, J. Boulahanis, D. Stark,
B. Lewis, and J. Rodrigue. A demographic analysis of
online sentiment during hurricane irene. In
Proceedings of the Second Workshop on Language in
Social Media, LSM ’12, pages 27–36, Stroudsburg, PA,
USA, 2012. Association for Computational Linguistics.
[11] B. Pang and L. Lee. Opinion mining and sentiment
analysis. Foundations and trends in information
retrieval, 2(1-2):1–135, 2008.

1215

Information Visualization (2008) 7, 77 -- 88
© 2008 Palgr ave Macmillan Ltd. All r ights r eser ved 1473-8716 $30.00

www.palgrave-journals.com/ivs

Mobile analytics for emergency response
and training
SungYe Kim1
Ross Maciejewski1
Karl Ostmo1
Edward J. Delp1
Timothy F. Collins1
David S. Ebert1
1

Purdue University Regional Visualization
and Analytics Center, Purdue University,
West Lafayette, IN, U.S.A.
Correspondence:
SungYe Kim, Potter Engineering Center
#134, Purdue University, 500 Central Dr.,
West Lafayette, IN47907-2022, U.S.A.
Tel: +765 494 5943;
E-mail: inside@purdue.edu

Abstract
During emergency response events, situational awareness is critical in effectively managing and safe-guarding civilians and in-field personnel. To better
support both command center controllers and in-field operators, we have
developed a mobile visual analytics tool to help enhance situational awareness and support rapid decision making. Our mobile visual analytics tool
consists of a 2D/3D visualization component, which shows personnel-related
information, situational and static scene-related information, integrated multi
media playback functionality for personnel outfitted with cameras, and fastforward/rewind capabilities for reviewing events. Our current system has been
employed in the evaluation of two different scenarios: a simulated evacuation
of The Station nightclub fire that occurred in Rhode Island during 2003 and
a testing exercise for a rescue operation in an elementary school. Our system
has been deployed on a Dell Axim X51v PDA, an OQO 02, and on a Sprint
PCS VisionSM smart device PPC-6700.
Information Visualization (2008) 7, 77 -- 88. doi:10.1057/palgrave.ivs.9500168
Keywords: Mobile visualization; visual analytics; emergency response

Introduction

Received: 1 December 2007
Accepted: 5 January 2008
Online publication date: 14 February 2008

Visual analytics is defined as the science of analytical reasoning facilitated
by interactive visual interfaces.1 Mobile visual analytics extends the visual
analytics process using state-of-the-art mobile devices to increase the effectiveness and interactivity of on-site analysis. These types of solutions can
provide advanced analytical insight to first responders and public safety
command personnel by allowing them to analyze and understand on-scene,
active emergency situations through interactive, integrated data analysis,
and visualization.
Such mobile analytics solutions can benefit first responders in a variety
of ways. First, the mobility of the handheld devices using wireless connectivity can minimize the ‘fog of war’ effect allowing first responders to better
carry out rapid and actionable on-site decision making. Furthermore, these
tools can provide improved situational awareness and support first responders in planning immediate life-saving responses and prioritizing actions
in emergency situations. Second, the rapidly growing capabilities of mobile
devices (e.g., PDAs and cell phones) provide a ubiquitous environment for
deployment within a variety of fields. However, most mobile devices still
have many limitations including small screens, limited user interfaces, a
short battery life, low bandwidth of the system bus, slow CPU clock speed,
limited storage capacity, and a lack of advanced graphics hardware.
Our goal is to make mobile devices valuable tools for emergency response
by effectively visualizing relevant, selected information (e.g., images,
videos, 3D models, and sensor data streams) on devices with varying capabilities and resolutions. To this end, we have developed a mobile visual
analytics system that processes and displays sensor, location, and video

78

Mobile analytics for emergency response

data for first responders to enhance situational awareness
and enable more effective decision making. Previously,
we introduced a prototype visual analytics system for
emergency response on mobile devices.2 In this paper, we
extend our previous work and present an enhanced visual
analytics system for emergency response and training
with congestion visualization, video playback, and reinforced visualization of 3D personnel and scene models.
This paper is organized as follows: The following
sections discusses the background and then summarize
visual analytics for emergency response. The further
sections describes the design of our system and presents
visualization and analytics on client mobile devices. The
next further section gives a brief summary of implementation and results of our system. The penultimate section
discusses the capabilities and potential of our system as
a visual analytic tool for emergency response. The final
section presents conclusions and discusses some possible
extensions for visual analytics.

Background
The related work is classified into three categories: visualization of sensor data, visualization on mobile devices,
and visual analytics on mobile devices.

Visualization of sensor data
With the increase in applications for sensor networks,
manipulation and visualization of sensor data streams
have become crucial components in the effective analysis
of network data. Fan and Biagioni3 described approaches
to process and interpret data gathered by sensor networks
for geographic information systems. These approaches
combine database management technology, geographic
information systems, web development technology, and
human computer interactive design to visualize the
data gathered by wireless sensor networks. Koo et al.4
designed software to analyze multi-sensor data for
pipeline inspection of gas transmission. The information gathered by sensors is parsed and converted before
it is saved in a database. Gunnarsson et al.5 introduced
a mobile augmented reality prototype for visual inspection of hidden structures on mobile devices using data
collected from a mobile wireless sensor network. Pattath
et al.6 implemented an interactive visual analytic system
to visualize sensor network data during football games
on PDAs.
Visualization on mobile devices
Many researchers have examined ways to effectively
display complex 3D models on mobile devices. We can
divide this research into two categories: the simplification
of representations and the effective transmission of data.
Simplification of representations7–11 makes it possible
to visualize complex 3D models with a limited graphical
capability while effective transmission12,13 between a

Information Visualization

SungYe Kim et al

server and a client enables mobile devices to visualize
more complex models.
Hekmatzada et al.7 described non-photorealistic rendering of 3D models based on a server and client environment. Their work provided transmission of meshes from
a server progressively, as well as level of detail (LOD)
rendering, allowing clients to navigate through the data
in nearly real-time. Diepstraten et al.9 proposed a remote
line rendering technique between the server and client.
The server extracts feature lines of 3D models and transmits them. The clients then draw the transmitted results.
Thus, the clients do not need to have high computational
capabilities since they only draw 2D lines. Duguet et
al.10 displayed complex geometry on mobile devices by
using a point-based rendering technique. Quillet et al.11
presented two optimization methods to visualize an urban
environment on mobile devices interactively. One optimization method extracts feature lines and then changes
the lines into vector lines. The other splits the urban environment into cells in order to transmit them as a stream.
Their work also provides an efficient LOD solution. Pouderoux and Marvie12 proposed two levels of adaptivity to
display a large amount of terrain data regardless of the
device. The terrain data are partitioned into regular tiles
and the tiles around a viewer are transmitted as a stream.
The tiles are rendered using a pre-computed triangle strip
path. Zhou et al.13 introduced a client-oblivious framework, which integrates volumes and iso-surfaces into
one hierarchical structure to visualize volumes on mobile
devices.
While much work on visualization on mobile devices
has focused on 3D rendering, 2D graphics and visualization can be just as efficient in the case of information
visualization. In fact, there are many applications that
utilize 2D capabilities of mobile devices in fields such
as geographic information systems,14,15 entertainment,
education, business, and industry. Moreover, OpenVG16
and Mobile 2D Graphics (M2G) are boosting the development of more 2D applications that are scalable across any
screen size.

Visual analytics on mobile devices
The application of visual analytics to mobile devices has
several challenges. It is different from visual analytics
on common desktop systems because of the restricted
display space and computing resources of mobile devices.
Sanfilipa et al.17 introduced InfoStar,18 an adaptive
visual analytics platform for mobile devices. Their work
was demonstrated at Super Computing 2004 (SC2004)
providing information such as maps, schedules, exhibitor
lists, and visual exploration tools to conference attendees.
Similarly, the work by Pattath et al.6 provided a visual
analytic tool for the visualization of network and sensor
data gathered from Purdue Ross-Ade Stadium during football games. Finally, Herrero et al.19 presented an intrusion detection system for visual analysis of high-volume
network traffic data streams on mobile devices.

Mobile analytics for emergency response

Visual analytics for emergency response
For emergency response, a well-designed visual analytics
system is necessary. The display capability must be
tailored to the responders and their roles, and provide
a succinct, quickly understood display of relevant information extracted from all information acquired. For
example, Special Weapons And Tactics (SWAT) teams are
highly trained groups of police officers whose missions
include hostage rescue, dignitary protection, and highrisk warrant services. These missions all require successful
coordinated information collection and exchange. In the
case of the SWAT team responding to an active shooter
in a school, the first and most critical requirement of the
team leader, as well as all responders, is to have the most
accurate sense of situational awareness as possible. In
order to do this, the responder must answer the following
questions:
• Where are all team members located?
• Where are the locations of responding personnel?
• Where are the secure, neutral, and hot zones of the
incident?
• What locations provide opportunity or threat information?
The goal of our system is to efficiently provide answers
to these questions in order to enhance situational awareness.
In addition, the capability to provide information
back to the emergency operation center, such as indicating rooms cleared or information contradictory to
current situational assessment, is also vital. As previously
indicated, relevant information is specialty-dependent.
A firefighter responding to a fire at the same building
would need some of the above information, as well as
task-specific information, such as fire spread, potential
toxic gases, or locations of dangerous goods stored.

Figure 1

SungYe Kim et al

79

Moreover, the first generation of mobile analytics
should target readily available technology, such as PDAs
and smartphones. This display system will be useful in
decision support during emergency response, as well
as planning for event response. The system will allow
responders to reduce the time spent on information
gathering and instead focus on response actions, such as
asset dispersal. These actions will be assisted with a visual
display of current information while after-action reviews
(AAR) will also be enhanced by providing users with the
ability to better see potentially unknown information
such as evacuation routes that are not used efficiently.
After-action review methods are also aided through realtime analysis of actions taken during response time. This
system will enhance training for response to many unique
emergency situations, and we have deployed a version of
this system at our in-field emergency response training
site. The results seen here show its application to both
simulation data and previously recorded first-responder
training exercises.

System design
Figure 1 shows the abstraction of our system structure as a server–client architecture. Our system focuses
on the utilization of various types of data sets such as
images/videos, 3D models, sensor data, and text data. All
of the streaming data are received from and preprocessed
by each server in the server group. In Figure 1, the data
converter in a server group converts all input data into
the appropriate types for the mobile visual analytic client.
This conversion is necessary for visualization on mobile
devices and involves determining the appropriate representation of the data for rapid, in-field cognition on a
small-screen mobile device. The data created for desktop
systems cannot be used for mobile devices without further
preprocessing because of the limits of mobile devices in
terms of memory, bandwidth, and screen resolution.

System overview.

Information Visualization

80

Mobile analytics for emergency response

The converter preprocessing has components that are
customized for each type of input data stream, but in
general it uses a flexible structure to allow the input of
a variety of data based on the given response situation.
Moreover, the structure is designed to allow tailored
processing of the same input data for different response
situations and different roles in the response.
As our previous work,2 we are using pre-generated/
recorded data so that all of the information can be
initially transmitted to the client. However, our system
could be modified to get real-time feeds.
Our mobile visual analytics tool consists of a 2D/3D
visualization component that shows personnel-related
information, situational and static scene related information, integrated multimedia playback functionality
for personnel outfitted with cameras, and fast-forward/
rewind capability for reviewing events.

SungYe Kim et al

Input data
We categorize the input data into three types in terms
of their functionalities. One is personnel-related information for moving entities. Each moving entity has identification, position, and time stamp information. Second is
situational information. Examples of this include temperature, dispersion of toxic gases, and water contamination.
The third is static scene-related information such as a
2D map or a 3D model that is used for representing the
environment.

location data) are time varying. Therefore, our data
converters are designed to process both time varying data
and static data.
For the static data (e.g., images/blueprints, 3D models,
text files), conversion to a format that enables execution
on a mobile device in real-time under the management
of the server is performed. Using vector images gives our
system scalability across screen size and resolution. Hence,
all images are converted into vector path data in scalable
vector graphics format.20
For the conversion of time-varying data (e.g., sensor/
video data, location data), special processing is needed
to provide proper synchronization of the temporal data
streams in a networked environment. Moreover, filtering
and selection of large data streams is necessary to enable
real-time use on processor- and memory-limited mobile
devices.
To effectively utilize large streaming data on mobile
devices, we employ simple compression/transformations
of the data to reduce both network bandwidth and
local storage requirements. We also use data importance
characteristics to determine update rates, data items to
skip, and interpolation methods to maintain our performance requirements. Finally, level of detail, level of
abstraction, and level of data aggregation are chosen
not only to enable interactive performance but also to
reduce visual clutter and enable effective visualization
and analysis on the mobile device.

Preprocessing in a data converter
In our current system, input datasets can also be roughly
categorized by their real-time properties. The abstracted/
simplified background images as well as the 3D models
are immutable data during processing for visual analytics,
whereas the other data sources (e.g., sensor/video data,

Management of streaming data on the client
To deal with the large size of time-varying data streams,
we need to utilize an appropriate data structure for
storage. We use a circular queue, as shown in Figure 2,
in order to minimize memory consumption. A circular
queue is a particular implementation of a queue in

Figure 2

Information Visualization

Data structure for streaming data.

Mobile analytics for emergency response

which insertion and deletion are totally independent.
Although our system focuses on client-based visualization and analytics, such queuing structures can be used
for processing of streaming data in server–client architectures. In our work, the size of the circular queue is set
to 30 to provide short-term reference data for visualization and still fit in the memory of PocketPC phones and
PDAs. In Figure 2, the simulation data manager serves as
a communication handler in a server–client system and
can be composed of several types of data managers. The
data manager is responsible for updating the appropriate
entry type in each element of the queue (e.g., sensor, location). The application in a main thread can then take the
data from the queue by time stamp without suspension
occurring due to any network traffic.

SungYe Kim et al

81

appropriate data aggregation/abstraction level to enable
effective decision making. In our work, the device memory
limitations are primarily solved on the server component
during data conversion to an appropriate representation.
Hence, our mobile visualization client mainly deals with
the visual representation to enable visual analytics on
mobile devices for emergency response and training.

The main concerns for visual analytics on mobile devices
are the device limitations (screen size, memory) and the

Case studies
To demonstrate our system capabilities we have employed
two case studies: a simulated evacuation of The Station
nightclub fire that occurred in Rhode Island in 2003 and
a testing exercise for a rescue operation in an elementary
school.
Case 1: The first scenario has been completed from an
investigation and a computer simulation by the National
Institute of Standards and Technology (NIST)21 after the
fire. We used two simulated data sets including fire data
and evacuation data of 419 personnel. The fire was simulated by the Fire Dynamics Simulator (FDS4)22 developed
by NIST. FDS4 is a computational fluid dynamics model of
fire-driven fluid flow and provides time-resolved temperature, carbon monoxide, carbon dioxide, and soot distribution throughout the building for the duration of the fire.
These calculations show how the fire and smoke propagate through the building and the results were used in the
evacuation model for movement of personnel. Personnel
start to move towards the exits to find the nearest known
exit at the time fire started. All simulation and 3D model
data that we used for this study were provided by the
Purdue Homeland Security Institute (PHSI).23 Figure 3
shows the floor plan of The Station nightclub. The 3D
model and the background image we used were generated
with the same scale and locale as in using the document21
from NIST.
Case 2: The second scenario is data recorded at the
emergency response training testbed facility that has been
developed at Purdue University in a local decommissioned
elementary school (Burtsfield Elementary). This scenario

Figure 3 Floor plan of the Station nightclub. Image courtesy:
National Institute of Standards and Technology.

Figure 4 Floor plan of the north wing of the Burtsfield Elementary School, West Lafayette, IN.

Visualization of data
In our system, we consider several visualization issues
to represent various data types effectively. Dynamic and
static categories can be applied to moving entities and
stationary scenes, respectively. Temporal and spatial categories describe information changed at each time and
position. Aggregation and non-aggregation categories
highlight personnel-related information in terms of a
group or specific individual information.
From the point of view of input data for the visualization of personnel-related information, we show the
position, path traveled, health/activity level, evacuation status, congestion, area traveled, and corresponding
video data. For situational information, our system visualizes sensor data, examples of which are the distribution
of temperature and toxic gases. In addition, our system
provides a perspective view of the 3D environment. These
functions can be utilized for situational awareness and
assessment.

Visualization on mobile devices

Information Visualization

82

Mobile analytics for emergency response

SungYe Kim et al

is a training exercise in which first responders are called
to an in-school shooting incident. Two teams enter the
building and attempt to find and apprehend the subject.
This data set includes real-time agent location as well as
video feeds from both stationary and on-agent cameras.
Agents were also equipped with sensor boards providing
activity level and directional information. Figure 4 shows
the floor plan of the testbed facility.

Visualization for emergency response
Our input data sets have different characteristics that
are representative of most emergency-related information. The personnel-related data include moving
entity-centered information (e.g., id, location, activity,
health level, video), whereas the situational data is
global, time-varying information (e.g., fire spread, carbon
monoxide distribution). Scene-related data corresponds
to a 2D background map and a 3D model.

Personnel-related information The personnel-related
information, including data of moving entities, is
displayed using 2D vector graphics. The current position
is shown by a circle and the time-evolving path is drawn
by line segments. Based on the size of data contained in
the circular queue, the paths of the movement is visualized using fading as a method of temporal visualization.
The color of each entity’s path can be pre-assigned based
on entity/team designation, it can be changed based on
the entity’s health/activity level, or it can be randomly
assigned.
To help responders better understand the personnel
locations, we have created visual representations of the
congestion information and area traveled. Congestion
represents how many personnel stay at any position at the
same time making it time-based information whereas area
traveled is location-based information. Figure 5 shows
the visualization of congestion from the fire simulation
of case study 1. Congestion is calculated by the number
of personnel per unit area (m2 ) per second. To compute
the area traveled, we use the number of personnel that
are accumulated at every frame. We divide our environment into a set of cells with each cell representing
approximately 1 m2 for this scenario. Each moving entity
can partially affect congestion based on how much of
the entity covers a given cell (e.g., half, quarter). Figure 6
provides a simple example of our congestion calculation
for a set of cells and agents. This calculation is based
on three propagation areas (e.g., zero propagation area,
half propagation area, quarter propagation area), and
this example demonstrates how the degree of congestion can propagate to its neighbors. While this is not
a completely accurate model, it is reasonable considering the small resolutions and capabilities of mobile
devices.
Further personnel-related visualizations include a
health or activity-level indicator. In first case study,

Information Visualization

Figure 5 Congestion visualization. (A) Congestion visualization with personnel. (B) Congestion visualization without
personnel.

Figure 6 Approximate propagation for congestion (9 cells);
(red) zero propagation area, (blue) half propagation area,
(green) quarter propagation area.

each entity has two health indicators based on the
fractional effective doses (FED) for heat exposure and for
gas concentration. Personnel become unconscious and

Mobile analytics for emergency response

cannot move anymore when either of these values reaches
a pre-defined threshold (e.g., threshold = 0.5). We nominally visualize healthy personnel as green and display
unconscious personnel using red. A health level between
0.0 and 0.5 is visualized with yellow to orangish colors
pre-defined in a health level table. Figure 7 (left) shows the
evacuation of personnel visualized in a 2D environment
for global evacuation analysis. During the visualization,
the number of personnel with a given health status is
displayed in the information window. After analyzing, it
is apparent that 102 of 419 personnel have become incapacitated by the end of the simulation. Figure 7 (right)
shows the visualization of moving (circle) and stationary
(triangle) entities for second case study.

Figure 7 Visualization of personnel in 2D environment; (left)
419 people in alive status (green) and their movement path by
fading line, (right) 6 moving entities (circle) and 5 stationary
entities (triangle).

Figure 8

SungYe Kim et al

83

In the second case study, each entity has a single
activity-level indicator based on their amount of movement. As personnel change from a walk to a run, we
nominally visualize the slow movements as green and
the fast movements as red.

Situation-related information Situation-related information includes data about the surrounding environment.
Our first case study allows to visualize fire simulation data
including the temperature, heat release rate (HRR), smoke,
CO2 , and CO during the fire as emergency situationrelated information. To visualize this information, we use
separate 11-element color and gray-level tables. Temperature and HRR are displayed using colors, while smokes,
CO2 , and CO are drawn using gray levels. The visualization of each is overlaid on the 2D or 3D environment.
Figures 9 and 10 show the results of the visualization of
temperature and CO data at different time steps. In Figure
9 and Figure 10, we use a grid of 7 × 7 pixel squares to
interpolate and visualize temperature and CO since the
data was transformed on a coarse grid for performance.
In the second case study, situational information
includes the mobile and stationary camera data streams.
Users can select an agent or stationary camera from the
application and play the corresponding, synchronized
video stream. Figure 8 shows the playback of video data
corresponding to a selected agent.
Static scene-related information Scene-related information includes the building outlines and models.
This information plays a serious role in understanding
environment, analyzing, and planning for emergency
situation. All our visualizations are based on 2D

Video data playback from stationary cameras and agents.

Information Visualization

Mobile analytics for emergency response

84

visualization using 2D background map as orthogonal
view. In the 2D view, responders can see exit areas and
annotation for each exit.
In addition, we provide a 3D perspective view to better
understand emergency environment and factors that may
have determined the evacuation paths chosen. In the 3D
view, all personnel and their environment are visualized
as 3D objects. Similarly, 3D navigation and observation
can help train first responders by enhancing their recognition of potential evacuation routes and visual building
characteristics that may lead responders to probable alternative paths taken by people missing during an actual
emergency incident. Particularly, the transparent and
wireframe views help responders see personnel behind
obstacles. Figures 11 and 12 show the movement of
3D personnel in different shading modes. The transform applied in the 3D view is likewise applied to the
2D view.

Mobile visual analytics
There is general analytics information that is commonly
required for most emergency response situations. Such
information helps first responders suggest response priorities and plan actions based on their evaluation of effective situational awareness common operating picture
data. Table 1 classifies and lists analytic questions for our
emergency cases in terms of personnel and the environment. The most basic information is the location and
movement of people and assets (see Figures 7 and 11). In
our first scenario, the number of personnel in each health
condition (alive, unconscious, dead) and the number
of personnel who used each exit are displayed on an
information window to analyze the effectiveness of the
evacuation. Figure 13 shows a few visual analytics results
with such numerical values. When the fire started, most
personnel started to run for the exit that was most familiar
to them or was closest to them based on the algorithm
for personnel’s movement used for our first scenario.

Table 1

Questions for emergency analysis

Object

Questions

Personnel

1. Who is he/she?
2. Where is he/she?
3. What about his/her movement pattern for
evacuation?
4. How is his/her health condition?
5. How does their health condition change?
6. Did he/she succeed in evacuating?
7. How many entities succeed in evacuating?

Environment

1. What is the condition? (e.g., temperature,
toxic gas)
2. How does the condition change?
3. What is the structure? (e.g., exits)

Information Visualization

SungYe Kim et al

However, the main exit that might be the most familiar
to most people is not most heavily used exit because of
congestion nearby the main exit: most personnel ran
towards the bar exit. In Figures 9, 10, and 13, we also
obtain an unexpected result from the analysis. Although
the kitchen area was safer than others were in terms of
temperature and carbon monoxide, only a small number
of personnel used this area for the evacuation.
Figure 14 shows the rate (the number of personnel per
second) of evacuation during the fire. The slope of the
data line decreased when the main exit became blocked
by the crowd. Thus, many personnel chose the other
exit (bar exit) for evacuation instead of the main exit.
This happened 90 s after the fire occurred. The kitchen
exit was not used as an efficient evacuation exit because
of its unfamiliarity. Figure 14 also shows the congestion
near the main exit that caused the heavy usage of the
bar exit.
Figure 15 shows the information of a specific entity
selected by a user. The selected entity is displayed in
magenta. In Figure 15, the entity with ID = 127 and
ID = 103 became unconscious before evacuation (left).
Their FED of CO of the entity is over 0.5, whereas the
entity with ID = 206 who is still healthy shows the FED
of heat and gases at low levels. Moreover, obtaining the
change in health levels of each entity at each time can
help first responders (e.g., fire fighters) to establish rescue
priorities.
In the second case study, we can analyze the paths used
by the agents and their corresponding video components,
and the activity value of each entity is displayed. In Figure
16, we see two agents at the upper right entrance and the
corresponding video stream from a stationary camera and
an agent in that location. Later in the exercise, we see
the agents identifying an unknown entity in the second
classroom (Figure 17).

Implementation and results
We have implemented and tested our tool on a Dell Axim
X51v PDA that uses the Intel 2700G graphics processor
and 16 MB of video RAM, an OQO 02 that uses a 1.5 GHz
VIA Esther processor with 1 GB of RAM, and a Sprint PCS
VisionSM smart device PPC-6700, which uses Windows
Mobile 5.0 and a 416 MHz Intel processor. However, our
tool runs on any PDA using Pocket PC with sufficient
processing capabilities. We use the Hybrid Rasteroid3 for
OpenGL|ES and the OpenVG library provided by Hybrid
Graphics, which is a reference implementation and
provides functionality through OpenGL|ES 1.1, OpenVG
1.0, and EGL 1.3 specification as announced by the
Khronos16 group. All images in this paper were captured
with the Win32 version of our system. Figure 18 shows
our system running on mobile devices.
We set the main screen as a 2D orthogonal projection
of a building model for global situational awareness since
the visualized entities are all in the same 2D plane. In
addition, our system does provide 3D perspective views

Mobile analytics for emergency response

Figure 9

Temperature distribution at different time steps.

Figure 11

Visualization of personnel and 3D environment.

of all the data within the 3D building model. All of the
user interfaces are represented with transparency in order
to provide a non-invasive interface. The main menu and
information window can be also be hidden to not interfere with situational awareness visualizations. As such,
this interface can always guarantee a full main view of
the situation to the user. Due to the small screen size of
mobile devices, the problem of an efficient user interface
is another challenge for visualization on mobile devices.
GLUT|ES24 has been developed for WinCE and Win32
systems based on OpenGL|ES as the open source implementation. However, it can be space consuming for the
visualization of information. Therefore, we implement the
API for a user interface based on OpenVG. Currently, it
provides a button, a check box, a radio button, a text box,
a time slider, a hiding window, a line graph, and vector
fonts.
Buttons for play, pause, stop, speed-up/down, and selection mode are provided. The time slider shows the progress
of the overall simulation. Menu windows are opened with
their own button. Our tool has two menu windows: one

SungYe Kim et al

85

Figure 10

CO distribution at different time steps.

Figure 12

Transparent 3D view.

is used for displaying text information and the other is
used for visualizing additional information, such as rate of
evacuation graphs. There are personnel-related sub-menus
where a user can choose visualization options related to
personnel, and a situation-related menu where a user can
select visualization options related to emergency situations, such as viewing a fire spread (e.g., temperature, HRR,
CO2 , and CO). In the option menu, a user can toggle
2D/3D views, annotation, and shading modes.
As a prototype mobile visual analytic tools for emergency response and training, our tool presents efficient
and interactive visual analytic methods and provides visualization of various types of data. For situations requiring
rapid decisions, such as emergency response analysis
and services, our system can be used as an efficient
testbed.
Based on the overall visualization and analysis for our
test datasets, we observed that some personnel evacuated
using the stage exit at the beginning of the fire. Most
personnel ran about in confusion while they moved to
the exits located opposite to the source of the fire near the

Information Visualization

86

Mobile analytics for emergency response

Figure 13 Visualization of personnel-related information: (left)
the number of personnel in each health condition, (right) the
number of personnel at each exit.

Figure 14 Visualization of the rates for evacuation and crowd
(total personnel = 30).

stage. Some personnel who tried to evacuate out of the
main exit failed because of congestion near the corridor
between the main exit and the main bar. Hence, many
personnel moved to the bar exit. As a result, 102 out of
419 personnel became incapacitated and 97 personnel
did not find an evacuation exit. Most of the personnel
who became unconscious were found near the exits (e.g.,
main exit) that may have been familiar with them. In
the first case study, at the completion of the simulation,
23% exited via the bar exit, 2% via the kitchen exit, 16%
via the main exit, 10% via the stage exit, 15% via the
window, and 10% of personnel used a sun-room for their
evacuation.
For our second scenario, users felt that the ability to see
the coordinated movement of all responding personnel
was a great asset in increasing situational awareness
and provided needed information for effective decision
making. The ability to see video streams from the other
team also provides the ability to determine the status and

Information Visualization

SungYe Kim et al

Figure 15 Information of selected entities: (left) for first case
study and (right) for second case study.

Figure 16 Agents entering a building: (bottom left) video from
stationary camera and (upper left) video from an agent in the
location.

Figure 17
building.

Agents encountering an unknown entity in a

circumstances of unusual events encountered when there
is a lack of radio contact and can more efficiently provide
situational awareness over general radio traffic.

Mobile analytics for emergency response

SungYe Kim et al

87

The capabilities of visual analytics needed for the
hotwash include providing integrated visual analytics of
additional data. Visual analytics of correlated 2D, 3D,
video, and audio data is extremely beneficial in creating
lessons learned from exercises and enabling new insight
from the interactive exploration and analysis of all information captured during the exercise. Replaying via the
time slider and location, by exercise plan as chapters, and
at increased/decreased speeds are required. Evaluators
and analysts need the ability to display various videos,
2D/3D scene reconstruction, and statistical results of the
exercise as they are reviewing performance.
To evaluate the effectiveness and capacity of our system
to be used in real emergency situation or emergency
training, we received informal feedback from emergency
responders in Purdue University fire department and
PHSI. Through this feedback, we have learned that our
system could be useful in real emergency situations if
it is equipped with a real-time tracking capability, since
accurate situational awareness is a crucial issue in real
situations. Responders also felt that our system will be
useful for training such as pre-planning scenarios and
site inspections. To this end, we have deployed a version
of our system at the Burtsfield Elementary school for use
during training sessions.

Conclusion and future work

Figure 18 Photos of our system running on mobile devices:
(A) Running on PDA, (B) Running on (left) smartphone and
(right) OQO device.

Capabilities and possibilities for mobile visual
analytics for emergency response
Visual analytic systems for emergency response can be
used not only during actual emergency events, but also
during training, and for hotwash25 and AAR of exercises
and incidents. The hotwash is a debriefing and critique
conducted immediately after the exercise and incident.
The AAR is a detailed and extensive assessment and
comment on the exercise with written evaluations that
takes place several days or weeks after the exercise. The
AAR does not judge success or failure but rather focuses
on learning what happened, why things happened, and
what tasks and goals were or were not accomplished.
Mobile visual analytics adds mobility to common visual
analytics and can provide enhanced situational awareness
on-site during the hotwash. Such rapid and appropriate
awareness leads to ‘lessons learned,’ which is intended to
guide future response direction in order to avoid repeating
errors made in the past. Hence, the effectiveness of analysis and evaluation to identify strengths and weaknesses
of the response to a given situation can be enhanced.

We have shown a flexible prototype of our mobile visual
analytics system for emergency response and demonstrated its use for a building fire evacuation and an exercise for a rescue operation. For situations requiring rapid
decisions such as placement and location of public safety
assets during a critical incident, our system can be used
as an efficient prototype and testbed.
In the future, we will extend this work to include
more analytic functions to enhance emergency situational awareness and support rapid decision making.
For example, a tool for interactively selecting specific
personnel groups and comparing information within and
between them can improve analysis of response asset allocation and training effectiveness. Moreover, our system
can be extended for actual first responder 3D tracking
and visualization for training and in-field deployment
support. The integration of RSS data and social network
data (e.g., family, friend group, local community, police,
fire station, hospital, and government department) will
provide interesting visual representation and interrogation challenges, and will further increase the usefulness
of our system for emergency response.

Acknowledgements
We thank the Purdue Homeland Security Institute (PHSI)
for supplying the simulation data and Army CERDEC for
releasing the data used in our second scenario. This work
has been funded by the US National Science Foundation
(NSF) under Grants 328984 and 0121288, and by the US

Information Visualization

88

Mobile analytics for emergency response

Department of Homeland Security Regional Visualization
and Analytics Center (RVAC) Center of Excellence. We also
thank Nicholas Klosterman, Deen King-Smith, and Aravind
Mikkilineni for their input.

References
1 Thomas JJ, Cook KA (Ed). Illuminating the Path: The R&D Agenda
for Visual Analytics, IEEE Press: New York, 2005.
2 Kim S, Jang Y, Mellema A, Ebert DS, Collins T. Visual analytics on
mobile devices for emergency response. In: VAST ’07: Proceedings
of IEEE Symposium on Visual Analytics Science and Technology
(Sacramento, CA, USA), IEEE Computer Society: Los Alamitos,
CA, USA, 2007; 35–42.
3 Fan F, Biagioni ES. An approach to data visualization and
interpretation for sensor networks. In: HICSS ’04: Proceedings of
the 37th Annual Hawaii International Conference on System Sciences
(HICSS’04) -- Track 3, IEEE Computer Society, Washington, DC,
USA, 2004; 30063.1.
4 Koo SO, Kwon HD, Yoon CG, Seo WS, Jung SK. Visualization
for a multi-sensor data analysis. In: Proceedings of International
Conference on Computer Graphics, Imaging and Visualization
(Sydney, Australia), IEEE Computer Society: Washington, DC,
USA, 2006; 57–63.
5 Gunnarsson A-S, Rauhala M, Henrysson A, Ynnerman A.
Visualization of sensor data using mobile phone augmented
reality. In: ISMAR ’06: IEEE/ACM International Symposium on Mixed
and Augmented Reality (Santa Barbara, USA), IEEE Computer
Society: Los Alamitos, CA, USA, 2006; 233–234.
6 Pattath A, Bue B, Jang Y, Ebert DS, Zhong X, Ault A, Coyle E.
Interactive visualization and analysis of network and sensor data
on mobile devices. In: VAST ’06: Proceedings of IEEE Symposium on
Visual Analytics Science and Technology (Baltimore, Maryland, USA),
IEEE Computer Society: Los Alamitos, CA, USA, 2006; 83–90.
7 Hekmatzada D, Meseth J, Klein R. Non-photorealistic rendering
of complex 3d models on mobile devices. In: Proceedings
of 8th Annual Conference of the International Association for
Mathematical Geology (Berlin, Germany), Alfred-Wegener-Stiftung:
Berlin, Germany, vol. 2, 2002; 93–98.
8 Dollner J, Walther M. Real-time expressive rendering of city
models. In: Proceedings of the 7th International Conference on
Information Visualization (London, UK), IEEE Computer Society:
Washington, DC, USA, 2003; 245–250.
9 Diepstraten J, Gorke M, Ertl T. Remote line rendering for mobile
devices. In: Proceedings of the Computer Graphics International
(Crete, Greece), IEEE Computer Society: Washington, DC, USA,
2004; 454–461.
10 Duguet F, Drettakis G. Flexible point-based rendering on mobile
devices. IEEE Computer Graphics and Applications (Los Alamitos,
CA, USA), IEEE Computer Society: Los Alamitos, CA, USA, 2004;
24: 57–63.

Information Visualization

SungYe Kim et al

11 Quillet J-C, Thomas G, Marvie J-E. Client–server visualization of
city models through non photorealistic rendering, INRIA Technical
Report, September 2005.
12 Pouderoux J, Marvie J-E. Adaptive streaming and rendering of
large terrains using strip masks. In: GRAPHITE ’05: Proceedings of
the 3rd International Conference on Computer Graphics and Interactive
Techniques in Australasia and South East Asia, ACM Press: New
York, NY, USA, 2005; 299–306.
13 Zhou H, Qu H, Wu Y, Chan M-Y. Volume visualization on mobile
devices, PG ’06: The 14th Pacific Conference on Computer Graphics
and Applications (Taipe, Taiwan), National Taiwan University Press:
Taipe, Taiwan, 2006; 76–84.
14 Masoodian M, Budd D. Visualization of travel itinerary
information on pdas. In: AUIC ’04: Proceedings of the Fifth
Conference on Australasian User Interface, (Darlinghurst, Australia),
Australian Computer Society, Inc.: Australia, 2004; 65–71.
15 Chittaro L. Visualizing information on mobile devices. Computer
2006; 39: 40–45.
16 Khronos Group. http://www.khronos.org/ (accessed 30 November
2007).
17 Sanfilippo A, May R, Danielson G, Baddeley B, Riensche
R, Kirby S, Collins S, Thornton S, Washington K, Schrager
M, Randwyk JV, Borchers B, Gatchell D. An adaptive visual
analytics platform for mobile devices. In: SC ’05: Proceedings of
the 2005 ACM/IEEE Conference on Supercomputing (Washington,
DC, USA), IEEE Computer Society: Washington, DC, USA,
2005; 74.
18 Infostar.
http://www . sc - conference . org / sc2004/infostar.html
(accessed 30 November 2007).
19 Herrero Á, Corchado E, Sáiz JM. Movicab-ids: Visual analysis
of network traffic data streams for intrusion detection. In:
IDEAL (Burgos, Spain), Springer-Verlag: Berlin, Heidelberg, 2006;
1424–1433.
20 W3C: Scalable Vector Graphics (SVG) XML Graphics for the Web.
http://www.w3.org/Graphics/SVG/ (accessed 30 November 2007).
21 Grosshandler WL, Bryner NP, Madrzykowski D, Kuntz K.
Report of the technical investigation of the station nightclub
fire. [NIST NCSTAR 2: Volume 1] http://www.nist.gov/
public_affairs/releases/RI_finalreport_june2905.htm/ (accessed 30
November 2007), June 2005.
22 NIST Fire Dynamics Simulator (FDS) and Smokeview.
http://fire.nist.gov/fds/ (accessed 30 November 2007).
23 Chaturvedi A, Mellema A, Filatyev S, Gore J. Dddas for fire and
agent evacuation modeling of the rhode island nightclub fire.
ICCS ’06: Workshop on Dynamic Data-Driven Application Systems
(University of Reading, UK), Springer-Verlag: Berlin, Heidelberg,
2006; 433–439.
24 GLUT|ES–The OpenGL|ES Utility Toolkit. http://glutes.source
forge.net/ (accessed 30 November 2007).
25 Johnson L. Katrina: Hotwash vs. whitewash, [TPMCafe],
http://www.tpmcafe.com/story/2005/9/4/111442/8437 (accessed
30 November 2007), September 2005.

2014 IEEE Pacific Visualization Symposium

A Mobile Visual Analytics Approach
for Law Enforcement Situation Awareness
Ahmad M. M. Razip, Abish Malik,
Shehzad Afzal, Matthew Potrawski∗

Ross Maciejewski†

Yun Jang‡

Niklas Elmqvist,
David S. Ebert§

Purdue University

Arizona State University

Sejong University

Purdue University

(a) Risk profile tools give time- and location-aware assessment of public safety using law enforcement data.

(b) The mobile system can be used anywhere with cellular network
coverage to visualize and analyze law enforcement data. Here, the system is being used as the user walks down a street.

Figure 1: Mobile crime analytics system gives ubiquitous and context-aware analysis of law enforcement data to users.
1

A BSTRACT

In 2010, the number of handheld devices reached a staggering volume of 4 billion devices globally [9]. With a large and diverse user
base, it is the only truly universal computational platform today.
With this global explosion in the usage of modern smartphones and
handheld devices, users now have more connectivity to the digital
world and the ability to ubiquitously ingest data and transform it
into knowledge that enables them to comprehend a situation better
and make more effective decisions. However, challenges associated with the data processing, exploration and analysis on a mobile platform are becoming prominent due to the increasing scale
and complexity of modern datasets and limited screen space of mobile devices. These challenges are being addressed by the emerging
field of visual analytics [40]. Visual analytics in the mobile domain
utilizes state-of-the-art mobile devices and provides users with the
ability to effectively and interactively analyze large and complex
datasets on-the-go; thereby, providing analysts, in-field workers, responders, decision makers and other users insights into any emerging or emergent situation in real-time (Figure 1).
In this paper, we present a mobile visual analytics approach for
solving one such problem in the public safety domain. Our work
leverages the ubiquity of the mobile platform and focuses on creating an effective, interactive, client-server-based situational analysis
and analytics system for the geotemporal exploration of criminal,
civil and traffic (CTC) incidents. Our mobile system (Figure 1b)
provides on-the-go situational awareness tools to law enforcement
officers and, in the future, to citizens. Our system has been designed
in collaboration with a consortium of law enforcement agencies and
first responder groups and has been developed using a user-centered
approach. Designing a mobile visual analytics system in this domain provides a unique set of challenges that range from identifying
the in-field needs of a diverse group of end-users to understanding
the applicability of a mobile solution to improve the day-to-day operations of law enforcement officers. We discuss these challenges

The advent of modern smartphones and handheld devices has given
analysts, decision-makers, and even the general public the ability
to rapidly ingest data and translate it into actionable information
on-the-go. In this paper, we explore the design and use of a mobile visual analytics toolkit for public safety data that equips law
enforcement agencies with effective situation awareness and risk
assessment tools. Our system provides users with a suite of interactive tools that allow them to perform analysis and detect trends,
patterns and anomalies among criminal, traffic and civil (CTC) incidents. The system also provides interactive risk assessment tools
that allow users to identify regions of potential high risk and determine the risk at any user-specified location and time. Our system
has been designed for the iPhone/iPad environment and is currently
being used and evaluated by a consortium of law enforcement agencies. We report their use of the system and some initial feedback.
Keywords:
safety

Mobile visual analytics, situation awareness, public

Index Terms: I.3.6 [Computer Graphics]: Methodology and
Techniques—Interaction techniques; I.3.8 [Computer Graphics]:
Applications—Mobile Visual Analytics

∗ e-mail:

{mohammea|amalik|safzal|mpotraws}@purdue.edu
rmacieje@asu.edu
‡ e-mail: jangy@sejong.edu
§ e-mail: {elm|ebertd}@purdue.edu
† e-mail:

978-1-4799-2873-6/14 $31.00 © 2014 IEEE
DOI 10.1109/PacificVis.2014.54

I NTRODUCTION

169

and present our mobile solution that has the following main contributions:

[4, 8]). Our system is designed to enable domain experts and, in the
future, ordinary citizens explore and analyze spatiotemporal CTC
incidents. Our work uses the notion of casual visualization [34],
but further provides statistical tools that help users identify trends,
patterns and relations within the data in the EDA process.
The ability to have situational awareness in law enforcement is
essential in maintaining public safety. Situational awareness by definition [18] enables one to perceive, comprehend, and project into
the future to make more effective decisions. Law enforcement officers make decisions in resource allocation and patrol planning to
reduce crime, while citizens may make decisions to be at a place
at any particular time. Critical to gaining awareness of a situation are the fundamental questions of where, what, and when an
event/incident occurs [7]. Our mobile system, similar to many GIS
tools, presents the law enforcement information in a place-timeobject organization to allow for information exploration and sensemaking using the different tailored views that we have developed
based on the user-driven requirements of different situations.

• Identifying the needs of first responders and law enforcement
agencies in the mobile domain. Our collaboration with the
law enforcement agencies enables us to explore and discuss
the use of our system in their day-to-day tasks at different
organizational levels in the agencies.
• Discussion of the design of a public safety mobile visual analytics solution for the first responder community. We adapt
several methods and techniques to work effectively in a mobile environment that has unique interaction methods, use
cases, and objectives (e.g., risk profile, hotspot alerts, plume
visualization).
2

R ELATED W ORK

The use of mobile devices in visual analytics has proliferated
greatly over the past few years. Mobile devices should not be
thought of merely as auxiliary devices for use while on the road but
they are the new personal computers [9]. With the explosion of the
number of users of smartphone, tablets and other mobile devices,
the interest to develop visual analytic systems on the mobile platform has greatly increased. For example, work by Kim et al. [23]
presents a mobile visual analytics system for emergency response
cases and highlights the use of such systems in time-critical applications. However, these systems face a unique set of challenges
compared to the commonly used desktop systems. Some of the
main constraints of mobile systems include limited performance,
small displays, and different usage environments [30, 37]. Novel
methods that deal with these unique constraints are thus a must.
Pattath et al. [31], for example, focus their work mainly on addressing the display and interaction area size constraint by utilizing the
focus+context technique. Additionally, the development of mobile
visual analytics systems provides novel use cases that traditional
desktop systems cannot provide.
For a dataset of geospatiotemporal nature (data with information
to geographical location and time) such as public safety data, geographic information systems (GIS) play an important role in the exploration and analysis processes in decision support environments
[3]. There exist many GIS systems that provide tools to support the
Exploratory Data Analysis (EDA) [41] process. Many of these systems provide a geospatial interface along with statistical tools for
analysis and are usually designed to enable users to explore trends,
patterns and relations among such datasets [6, 16]. For example,
Andrienko et al. developed the Descartes system [5] that automates
the presentation of data on interactive maps. The GeoDa system [8]
also provides an interactive environment for performing statistical
analysis with graphics. Many systems are also tailored to focus on
specific applications in a given domain. Examples of these include
GeoTime [21] and Flow Map Layout [32] that focus on tracking
movements of objects in time. Other domain specific applications
include traffic evacuation management [19] and urban risk assessment [24].
Similarly, GIS systems provide important support in the public
safety domain [12]. In analyzing and modeling the spatiotemporal
behavior of criminal incidents, Chen et al. [13], developed a crime
analysis system with spatiotemporal and criminal relationship visualization tools. However, in contrast to our focus on risk assessment and situational awareness in the mobile domain, their system
focuses on finding relations between datasets and is developed for a
desktop environment. Moreover, there exist many web-based crime
GIS tools (e.g., [14, 15, 28, 29, 33, 36, 39]). However, most of these
tools offer only basic crime mapping and filtering functionalities
and provide basic analytical tools to allow users to perform EDA.
Also, many of these systems target casual users only. Similarly in
the GIS domain, some systems also target non-GIS specialists (e.g.,

3

R EQUIREMENTS AND C HALLENGES

Our system’s design was driven by the task, environment, and device factors gathered from our end users through a user-centered
design approach.
3.1 Domain Analysis of Requirements
Our collaboration with local law agencies started with our work on
an earlier desktop-based system [27]. Having seen the benefits of a
desktop based visual analytics system in their operations, the local
law agencies approached us with the idea to develop a system for
the mobile platform that addresses their mobile needs. As such,
we had several meetings and informal discussions about the use of
such a mobile system in their day-to-day tasks to derive the system
requirements.
Below, we identify the requirements of the law enforcement
agencies and explore the use of such a mobile system based on our
formative engagements with officers ranging from shift supervisors,
patrol officers, detectives, and crime analysts.
R1: Easy operation — Mobile systems are often used ubiquitously, sometimes in less than ideal conditions. Additionally, using
a mobile system in such conditions often requires users to divide attention between the system, the task they are performing, and their
surroundings. Thus, the visualizations need to be easily comprehensible in a short glance and interaction should be simple and easy.
R2: On-the-go risk assessment and emergency management — Our end users emphasized the need for a mobile analytics system that would provide on-the-go risk assessment to in-field
officers. Crime trends are affected by the hour-of-the-day and dayof-the-week effects, and, as such, our risk profile system factors in
the current location and time of the officers to provide them with a
situational awareness of their surroundings. Additionally, because
our end user group included first responders, there was a need for
emergency management tools in case of accidents that potentially
affected a large population. As such, we provide tools that allow
them to visualize the impacts of chemical spills for use in emergency and evacuation situations.
R3: Near real-time data — The shift supervisor is primarily
responsible for resource planning and allocation. He needs to have
access to the most recent data and look at all the CTC incidents from
the prior and current day to prepare for the shift change briefing
and roll call. Thus, we design the mobile system on a server-client
architecture that centralizes the entire data on the server, which is
always kept up to date. Based on recommendations made by our
end-users, the CTC data is currently acquired four times a day and
put in to our database server which allows the data to be up to date
before a patrol shift change and during patrol.

170

Figure 2: A screenshot of our mobile visual analytics law enforcement system. (left) Visualizing all CTC incidents for the city of Seattle,
WA, USA on February 20, 2011. The map view (a) plots the incidents as color-coded points on a map (the legend for the points has been
shown in the top-right window). The interactive time series view (b) plots the incident count over time with the estimated weighted moving
average (EWMA) control chart overlaid. The bottom-left image (c) shows an overview+detail calendar view of the CTC incidents. (right)
Visualizing all CTC incidents for Tippecanoe County, IN, for the month of February 2011. The county’s census tracts have been overlaid on
the map. The bottom right image (d) shows the interactive clock view to provide an hourly view of CTC incidents for the month selected.
The interactive time slider (e) allows users to scroll through time and offers various temporal aggregation levels.
R4: Trend analysis and visualization — During shift briefings, shift supervisors discuss the current crime trends with patrol
officers and review, for example, what has been happening over last
week, day, this day last week, and so far today. They use this information for tasks including planning their patrols and deciding on
what they should be on the lookout for. This is also important for
detectives and crime analysts to be able to see the patterns of specific crimes over space and time. Interactive visualization of CTC
trends over time is thus an important task required for the mobile
system.

analytics. On the other hand, mobile devices are often used when
officers are away from their workstations, or when they want to get
a quick access to the data and focus on rapid situation assessments.
C2: Different usage environments — As briefly discussed
above, the usage conditions of these systems also differs in that
desktop systems are often used in a controlled office environment;
whereas, mobile systems are more geared for use on-the-go. For
law enforcement officers working in the field, our system is highly
beneficial in providing them with tools to increase their situational
awareness within their areas of responsibility [17]. Additionally,
using such a mobile system out in the field introduces new challenges in that it requires the officers to have divided attention between their surroundings and the system, thereby stressing the need
of having views that are intuitive and easy-to-read. As such, our
system makes use of casual visualization [34] concepts in addressing these issues.

R5: Mobile information — The shift supervisors typically use
paper print outs of the incidents that happened the previous day on
a map during shift briefings to point out certain incidents of interest and show their geospatial trends. We identify that the need to
offload information from a workstation to something a person can
carry around (e.g., paper) is essential and that the person must be
able to explore the information (they may carry multiple print outs
of the map showing different information).

C3: Limited computing resources — The limited physical
size imposed on mobile devices, which restricts their hardware capability such as computing power, memory, display area, and input capabilities due to miniaturization also limits the device performance. Speed is a key feature that users look for when performing
EDA, especially because mobile systems are often used in timecritical situations. For example, officers may use this system as
they respond to dispatch or emergency calls; and citizens may want
to get a quick check of the safety around their area as they walk
to their office. With our mobile system, we choose to determine a
default data size limit (the number of incidents to load) depending
on the device used and the network to guarantee an interactive performance. In this case, the most recent incidents are chosen and
shown using the system.

3.2 Mobile Challenges
Today, tasks that were once only done commonly using desktop computers can now be done using devices that can fit into a
pocket [9]. But developing such a system brings about unique foci,
tasks, goals, and constraints. In the following paragraphs, we discuss how some of these constraints affected the final design of our
mobile system.
C1: Different task and user intent — Due to the form factor of mobile devices, certain tasks are better done on mobile devices than on desktop workstations. For example, long-term risk
assessment and resource allocation are usually done using desktop
workstations due to better hardware and screen space for advanced

171

C4: Limited/varying display and interaction area — Device fragmentation in terms of display area was another key factor in the system design process. The screen of a smartphone can
only display a fraction of what is displayable on a tablet’s screen
(e.g., compare the iPhone’s 3.5-inch screen to the iPad Mini’s 7.9inch and iPad’s 9.7-inch screens). Additionally, interaction also becomes an issue where users would get a richer experience using the
iPad’s bigger interaction area on its touchscreen as opposed to the
small touchscreen on the iPhone [20]. We thus designed the system to be context-aware and behave differently based on the device
used.

provide multiple aggregation levels to group the different crime incidents. Our system provides support for the Uniform Crime Code
(UCR) categorization of CTC offenses utilized by the Federal Bureau of Investigation [42] that helps increase familiarity with the
system (R4).
4.2

System Design

Our system consists of two main components, a server back-end for
processing and computation, and a client front-end composed of our
interactive mobile visual analytics system. The server back-end
consists of a database that enables querying and provides data to the
client. The data going into the server undergoes a pre-processing
and data cleaning stage so it becomes ingestible to our client system. The front-end consists of the mobile device that provides a
user interface for the visualization, exploration, and analysis of the
spatiotemporal public safety dataset. The exploration and analysis
of data is done per user on his/her device and our end users have indicated that it will be advantageous to have the ability to share this
visualization or data exploration state with another user. However,
since this is not in our initial list of requirements (Section 3.1), we
leave this for future work.

C5: Security — One of the main benefits of having a risk assessment system on the mobile platform is its ubiquity. The system
can be used by law enforcement agencies in situ while responding
to dispatch or emergency calls. This increases the risk of having the
device misplaced or stolen and thus the risk of having sensitive data
falling into the wrong hands. Being used in the public, the system
is also more susceptible to being a target of data sniffing. To ensure
data confidentiality, we use a secure protocol to transmit encrypted
data to the end-devices. We also utilize secure authentication services to authenticate the user logging on to the database in order to
ensure the data is protected in case the device is misplaced.

4.3

4

M OBILE V ISUAL A NALYTICS E NVIRONMENT FOR F IRST
R ESPONDERS AND L AW E NFORCEMENT
Our mobile visual analytics system provides users with an overview
of public safety data in the form of criminal, traffic and civil (CTC)
incidents. It comprises a suite of tools that enables a thorough analysis and detection of trends and patterns within these incident reports. Our system has been developed for visualizing multivariate
spatiotemporal datasets, displaying geo-referenced data on a map,
and providing tools that allow users to explore these datasets over
space and time (R2). We further provide filtering tools that allow
users to dynamically filter their datasets. Our system also incorporates linked spatiotemporal views that enhance user interaction with
their datasets.
Figure 2 shows two snapshots of our system. Figure 2 (left)
shows all CTC incidents for the city of Seattle, WA, USA occurring on February 20, 2011, and Figure 2 (right) shows the CTC
incidents for Tippecanoe County, IN, USA occurring in the month
of February 2011. The main view of the system is the map view
(Figure 2(a)) that provides users with the ability to plot the CTC
incidents as color-coded points on the map (Section 4.3).
With temporal data, the system allows for visualization using
several views, namely the time series graph view (Figure 2(b)), the
calendar view (Figure 2(c)), and the clock view (Figure 2(d)). The
time series graph view allows users to visualize the temporal aspect
of the incident data using line graphs and model the data for abnormal event detection. The calendar view [43] lays the temporal data
in the format of a calendar, allowing users to visualize the weekly
and seasonal trends among the CTC incidents. The clock view, on
the other hand, allows users to visualize the hourly distribution of
the CTC incidents. A time series slider (Figure 2(e)) is provided to
allow users to scroll in time, updating all linked views dynamically.
Furthermore, our system also provides users with risk profile tools
that allow them to dynamically assess the risks associated with their
neighborhoods and surroundings.

Geospatial Displays

Our visual analytics system provides multiple views to visualize
the spatial component of the datasets. We allow users to plot the
incidents as points on the map that are color coded [10] to represent
the different parameters of the datasets (e.g., agencies responding
to the incident, offense type). The map has been dimmed so as to
distinguish these color-coded incident points from the map colors.
The radius scaling of the points is dependent on the zoom level.
Moreover, in order to tackle the issue of over-plotting the incidents
on the map, we provide interaction methods where the users can
zoom in and tap on incidents and drill down to his or her level of
interest. A better approach in dealing with the over-plotting issue is
to show the aggregate sum of the overlapping incidents on the map
at different zoom levels, and provide interaction techniques to show
details on demand. We leave this as future work.
Furthermore, we utilize a kernel density estimation technique
[25] to allow a quick exploration of the incidents on the map and to
identify hotspots. The system also allows users to overlay different
layers on the map (e.g., law beats, census tracts, bus routes) [44]
and allows them to place custom placemarks. The users can also
overlay driving and walking routes on the map, enabling them to
visualize the CTC spatial distributions along their intended routes.
An example of this has been shown in Figure 2(a).
Furthermore, as is the case with most multivariate datasets, the
CTC dataset is often incomplete. For example, many of the incidents do not contain valid geolocation data, causing uncertainty in
the analysis process. In order to account for the uncertainty caused
by this incompleteness in the dataset, we provide the ratio of correct incidents as a percentage value to show the accuracy of the visualization. This becomes important for users to accurately extract
information from their dataset [22].
4.4

Temporal Displays and Analysis

Our system provides users with three temporal displays that allow
them to visualize the temporal distribution of their datasets. We
provide a time series display that presents the distribution of the
incidents over time as a line graph, a calendar view visualization
that lays the temporal data in the format of a calendar and a clock
view that visualizes the hourly distribution of incidents.

4.1 Public Safety Data
Our system was developed using CTC data collected by a consortium of law enforcement agencies in our local county and from publicly available data [38, 39]. Each report entered into the database
consists of, among other fields, the date and time of when the incident was reported, the time range between which the incident was
thought to have occurred (e.g., in case of burglaries), the geolocation and the charges associated with the incident. Additionally, we

4.4.1

Time Series View

The system allows users to simultaneously select multiple offenses
and displays them as time series line graphs highlighting the trends
between multiple datasets (Figure 2(b)). Furthermore, we provide

172

(a) Risk profile at 9:00 A.M..

(b) Risk profile at 1:00 P.M..

(c) Risk profile at the user’s current time (2:00 P.M.).

(d) Risk profile at 5:00 P.M..

Figure 3: Risk profile visualizations at different times of day. The selected hour is shown by the dark green colored histogram bin, and the
current time is shown as the orange colored bin. The ±3 window from the current selected hour are shown as the green colored histogram
bins, and the rest of the histogram bins is colored as blue.
4.4.3 Clock View
users with several temporal aggregation options to aggregate their
In order to visualize the hourly distribution of the CTC incidents,
we implement a clock view (Figure 2(d)) that organizes the data in
the format of a clock. The clock view is a radial layout divided into
24 slices that are colored on a sequential color scale [10] to reflect
the number of incidents that occur during each hour of a day.
As is the case with geospatial data (Section 4.3), many of the
incidents do not contain a valid time field, causing uncertainty in
the analysis process. Thus, we use the same method of displaying uncertainty for geospatial data in this domain, by showing the
accuracy of the clock view visualization.

datasets. For example, users may choose to aggregate the incidents
by day, week, month or year, and visualize the results dynamically.
We further note that the time series display is interactive, allowing
users to touch the screen to get the incident count at any particular
time. A time series tape measure tool [26] in the graph view allows
users to determine the temporal distance between any two points
on the graph. We also provide users with tools that allow them to
accurately model the data using an Exponentially Weighted Moving
Average (EWMA) control chart for event prediction [26].
4.4.2

Calendar View

4.5

We adopt the calendar view visualization developed by van Wijk
and Selow [43] to provide a temporal overview of the data in the
format of a calendar, allowing users to visualize the data over time.
Each date entry is colored on a sequential color scale that is based
on the overall yearly trend to show the relative count of incidents
for each day with respect to the maximum daily count over that
calendar year.
To account for the smaller screen space of mobile devices (C4),
we provide an overview+detail view. This is shown in Figure 2(c),
where the left portion of the calendar view shows the weekly
overview of the entire calendar. The overview display draws the
individual rows based on the selected aggregation level (e.g., week,
month) and are colored on a sequential color scale to reflect the
weekly count of incidents. Users may tap on any portion of the calendar overview, updating the calendar view visualization dynamically to the position touched by the user. The overview+detail calendar view allows users to quickly identify weeks of high activity,
and provides an easy way to scroll to them.

Risk Profile

The risk profile visualization shows the spatial and temporal distribution of incidents with respect to the current location and time of
the user.
Our system utilizes the GPS feature of the mobile device and
factors in the geospatial location of the user, the current time, and
the historic CTC incidents occurring within their neighborhoods to
provide estimates of CTC activity in their surroundings. These provide users with an overview of all the incidents and allows them
to increase their level of situational awareness of the safety risks
involved in their surroundings. Now, in order to show the spatial
distribution of historic incidents, we utilize the map view and plot
the incidents as points on the map. When users enable the risk
profile feature, the system shows the current location of the user
as a green colored pin on the map, and draws a circle (of a usercontrolled radius) around their current location. The system then
performs a query to the server and acquires all incidents that occur
within this circle within a ±3 hour offset (adjustable by the user)

173

Figure 5: Plume visualization. (left) The plume visualization displaying the different levels of danger zones. (right) The plume visualization showing census tracts colored on a sequential color scale
to encode the number of people affected in each census tract, with
respect to the maximum people affected in any tract.

Figure 4: Incidents with nearby localized high-frequency activity
are highlighted in yellow to alert users of areas with suspicious
criminal activity.

In the case of a hazardous chemical release, our system can display the threat zones (plume) as a geospatial visualization. Each
threat zone defines an area where the hazard level exceeds some
‘Levels of Concern’ (LOC) [2]. The concentration of chemicals is
measured in ppm (parts per million) and each LOC specifies an area
with a certain range of the released chemical concentrations in ppm.
The innermost zone shows the area of most hazard and the outer
zone shows the area of least hazard. The number of zones shown
on the map can be changed by changing the number of LOCs, but
is set to three LOCs by default [2].
The threat zones are plotted on top of a geographic map to help
first responders quickly identify the regions under the most threat
and also the safest roads for travel and evacuation. The threat zones
are dynamic in nature due to changing weather conditions, and their
visualization on the mobile system can be updated regularly by the
user. Our system allows two types of plume threat zone visualizations to be shown. Figure 5 (left) shows the first visualization
where the plume is visualized as three threat zones, colored red, orange and yellow to encode the levels of danger from most danger to
least danger. The visualization also takes into account the changing
weather conditions and shows the plume model confidence interval,
represented by the outermost threat zone boundary (dark outline).
By default, only the confidence line of the outermost threat zone is
shown, but the user can also choose to show confidence lines for
each individual threat zone.
In the second visualization (Figure 5 (right)), we display colorcoded census tracts to show the distribution of people affected by
the chemical plume in addition to the plume LOC visualization.
For the color encoding, we first calculate the ratio of the census
tract area covered by the plume. By assuming uniform population
distribution, we calculate the expected number of people affected
by the chemical release in each census tract by multiplying this ratio with the census population data. We take the number of people
affected in each census tract and normalize this number by the maximum number of people affected in any tract. This obtained value
is used to pick a color from a sequential color scale to provide information about the census tracts that have larger percentages of
people in danger due to the chemical release. Detailed statistics of
the affected people can also be shown.

with respect to the current time, for a date range specified by the
user. The resulting incidents are then displayed as points on the
map. Also, the temporal distribution of all incidents falling within
the circle is shown in an interactive bar graph (Figure 3a (top)). We
highlight the hours on this graph to reflect the hours on which the
incidents are being displayed. Note that in addition to visualizing
the risk profile for the current location and time, users may also
choose to generate risk profiles for any desired spatial locations (by
dragging the pin provided on the map) and for any hour of the day
(by selecting a time by tapping on the risk profile time series graph
(Figure 3a (top))). The users can then get a geotemporal overview
of risk at any location and time.
In order to show which incidents have happened nearer in time
(with respect to the current time of day), we modify the kernel
density estimated heatmap (as described in Section 4.3) to encode
the temporal distance from the current time. So in this case, the
heatmap gives more weight to those incidents that fall closer to the
current time within the ±3 hour window, than to those that are farther away from the current time. The hotspots that so emerge provide an estimate of those incidents that happen closest in time with
respect to the current time. An example of this approach is shown
in Figure 3, where the user is visualizing all offenses against person
and property for the month of February 2013. We can see hotspots
emerging in different locations that show the distribution of incidents that happened closest to the current selected time.
4.6 Hotspot Alert
Our system also provides a feature to help users identify unusual
localized high-frequency patterns of crimes and identify crime
hotspot locations. Each data entry in the database is checked for
other crimes with similar properties (defined by the user). This is
done within a 200 meter block radius of the incident location, and
for a 14 day period that extends from the day the incident occurred
backwards in time. The system then highlights the incidents with
the most number of similar incidents within the space-time window
(Figure 4) which is updated dynamically as new data is entered into
the database by the user. This alerts law enforcement officials of
higher probability regions with nearby localized suspicious criminal activity and allows for more effective resource allocation and
patrol planning.

4.8 Implementation Notes
Our mobile system front-end has been developed for the
iPad/iPhone environment on the iOS platform. The system requires
iOS version 5.0 onwards and is developed in the Objective-C environment using XCode 4. The PROJ.4 Cartographic Projections
Library [35] is used to switch between map projection systems and
the Google Maps library is used for routing.
On the back-end of our server-client architecture, the database is
managed by MySQL that enables data querying and the web service
that handles data requests by the client uses PHP. Data is transmitted securely (C5) using HTTP over SSL/TLS in XML or plain text
format to keep the data format simple and generic for other uses.

4.7 Chemical Plume Modeling
Our system also provides a dynamic chemical plume modeling tool
that provides law enforcement officers and first responders with
better situational awareness in emergency situations resulting from
chemical releases. Our system uses the ALOHA (Areal Locations
of Hazardous Atmospheres) [1] software for chemical dispersion
modeling and generating threat zones to assess the potential hazards caused by chemical releases. ALOHA is part of the CAMEO
software suite [11] and can be used as a standalone desktop program.

174

5

U SER E VALUATION

list generating, selection/filtering tool. Geospatial mapping also
seems to appear to be prominently used at the present time as an
analysis tool for performing hotspot analysis. The overall graphical
user interface of our system looks promising for this role because of
the ease of use and the widely known mobile application selection,
zoom, and positioning conventions. One feature that appears to be
the most promising is the ability to dynamically create regions or
utilize shape files for geographic boundary definition. These features allow the crime analyst and/or detective to zero in on specific
geographic areas of interest, like neighborhoods or law beats, without having to manually sort through data. These features also assist
in generating historical reports for use in detailed crime analysis or
investigation. The detective/crime analyst also found the selection
capability within the calendar tool useful for seasonal crime analysis.
From our engagements, an intelligence analyst responsible for
providing intelligence and planning for strategic, operational or tactical situations also saw value in the system for his day-to-day tasks.
He particularly liked the ability of the system to apply any desired
filters on-the-fly in a visual interface in order to narrow down his
investigative analysis and to observe any spatial or temporal patterns. He was also interested in the crime monitoring process with
the system and the potential to quickly identify incidents that were
viewable by security cameras and pull up live and historical feeds
by those cameras. Additionally, he suggested adding a layer of
surveillance camera locations along with their range and angle of
view information in the system for use in identifying incidents that
may have been captured by any of the cameras for investigative
purposes.

In this section, we provide user feedback of our mobile visual analytics law enforcement system from domain experts from our local law enforcement agencies. For the purpose of evaluating our
system, we formed a focus group of 15 domain experts consisting of several police departments in our local county as well as
the county legal offices. We deployed our mobile system to this
group in December 2012 and have made continuous modifications
to the system based on their responses and feedback obtained from
our meetings which include several demonstrations, structured interview questionnaires, and informal in-person discussions. Here,
we report some of the group’s responses over the past 9 months.
We have received generally very positive feedback from this
group. At a high level, officers, especially the chief of police in our
local community, saw great value and potential of the system for
certain daily tasks. He cited one use of this system to increase public awareness of their safety from the criminal activities that occur
in their surroundings during safety campaigns. He also mentioned
its use to visualize the impacts of active crime prevention actions
(e.g., resource allocation for patrol, public safety campaigns).
At another level, the shift supervisor responsible for overseeing
the patrol officer assignments during their shift periods and communicating information of the previous shifts to the incoming officers viewed the application as a major improvement to their existing tools used for briefings and roll call (e.g., print outs of incident reports from the previous 24 hour period). The features within
our system that allow the display of the heat map of incidents displayed provided an added level of focus for incident location. The
feedback provided by the supervisor indicated that they utilized the
temporal reporting features in conjunction with geospatial features
to assist in resource allocation planning. They also found the convenience of having a mobile version useful for their roll call briefings
that allows them to look at the past incidents and have discussions
outside of the office space for instance while having coffee in the
lounge.
Patrol officers saw the value of using the system while on the
field to get CTC incidents data in a visual form to have an increased
situation awareness. Furthermore, the officers particularly liked the
risk profile feature that provides the regions with historically higher
incident levels based on the patrol officers current position and time.
They indicated that this feature was especially useful as it factored
in the current time-of-day information while they were patrolling
their area of responsibility. The Mobile Computer Terminal (MCT)
systems in their patrol cars are currently more geared towards reactive policing for reacting to an emergent situation, and as such, our
application provides them with tools that enable proactive policing.
The officers, however, also indicated that our system should be better integrated with the functions provided by their MCTs (e.g., dispatch calls, ability to enter detailed reports to be stored directly in
their database) for a more effective tool. They also indicated a need
for voice controlled commands for performing common functions.
During discussions with law enforcement personnel, additional
information would be useful to officers responding to incidents
(e.g., protective orders, non-contact orders). However, this information does not currently reside within their systems and, as such,
the officers do not get access to such data while responding to incidents. We plan on incorporating these datasets in our system to assist the law enforcement personnel with operational decisions when
responding to service calls.
At the role level of detectives and crime analysts, the usage patterns begin to shift more towards the predictive end of the spectrum
as well as an increased usage of the temporal reporting features.
Detectives are responsible for, among other things, solving crimes
by, for example, investigating crime patterns; whereas, crime analysts provide insights and analysis into patterns and trends in crime
to their police departments. Existing analysis tools tend toward a

6

C ONCLUSIONS AND F UTURE W ORK

In this paper, we have presented our mobile visual analytics system
that has been designed to equip law enforcement personnel and,
in the future, citizens with effective situation awareness and risk
assessment tools. Our current work demonstrates the benefits of
visual analytics in the mobile domain, and shows the effectiveness
of providing users with on-the-go tools that allow them to make
effective decisions. We have learned that the use of a mobile risk
assessment system differs slightly in providing real-time situation
awareness from a full, thorough analysis of CTC incidents with a
full-fledged system on a desktop. From our interaction with our end
users, we know that they do not have the time to tinker with a technology: it has to just work. Also with law enforcement agencies,
they are trained to handle risky situations and would not rely on the
system to gauge risk and determine if backup is needed. The use
of the system is also limited in tactical situations where attention in
certain actions takes precedence (e.g., focusing on the road while
driving). In this case, a technology like speech recognition would
significantly help but this remains a research direction we have not
yet explored. Finally, as has been shown in this paper, we argue
that there is great potential for the use of mobile visual analytics
solutions to serve multiple role levels in the law enforcement and
other related domains.
Future work includes adding advanced analytical and predictive
capabilities into the system. Furthermore, we plan on incorporating
image-based glyphs to represent the incidents on the map for easier
identification of the charges associated with the incidents. We also
plan on enhancing our routing methodology to factor in the risk and
other parameters in order to allow users to plan safer routes. In addition, we think that incorporating multiple datasets (e.g., census
data, street light locations, court records, weather) in the system
would be an interesting research direction to explore the correlations between CTC incidents and other datasets. We also plan on
modifying the kernel density estimation technique to further incorporate road network information to factor in for incident types that
are road bound (e.g., traffic accidents). Finally, we plan on investi-

175

gating porting the system to other mobile platforms (e.g., Android,
Windows RT).

[21] T. Kapler and W. Wright. GeoTime information visualization. Information Visualization, 4(2):136–146, Jun. 2005.
[22] D. Keim, J. Kohlhammer, G. Ellis, and F. Mansmann, editors. Mastering the information age: Solving problems with Visual Analytics.
EuroGraphics, 2010.
[23] S. Kim, R. Maciejewski, K. Ostmo, E. J. Delp, T. F. Collins, and D. S.
Ebert. Mobile analytics for emergency response and training. Information Visualization, 7(1):77–88, 2008.
[24] O. Linda and M. Manic. Online spatio-temporal risk assessment for
intelligent transportation systems. IEEE Transactions on Intelligent
Transportation Systems, 12(1):194–200, Mar. 2011.
[25] R. Maciejewski, R. Hafen, S. Rudolph, S. G. Larew, M. A. Mitchell,
W. S. Cleveland, and D. S. Ebert. Forecasting hotspots – a predictive
analytics approach. IEEE Transactions on Visualization and Computer Graphics, 17(4):440–453, April 2011.
[26] A. Malik, S. Afzal, E. Hodgess, D. Ebert, and R. Maciejewski. VACCINATED - visual analytics for characterizing a pandemic spread
VAST 2010 mini challenge 2 award: Support for future detection.
In Proceedings of IEEE Symposium on Visual Analytics Science and
Technology, pages 281–282, Oct. 2010.
[27] A. Malik, R. Maciejewski, T. F. Collins, and D. S. Ebert. Visual analytics law enforcement toolkit. In Proceedings of IEEE Conference
on Technologies for Homeland Security, pages 222–228, 2010.
[28] Metropolis police service crime mapping. Internet: http://maps.
met.police.uk, [Mar. 23, 2012].
[29] Oakland crimespotting.
Internet: http://oakland.
crimespotting.org, [Mar. 23, 2012].
[30] A. Pattath, D. S. Ebert, R. A. May, T. F. Collins, and W. Pike. Realtime scalable visual analysis on mobile devices. Multimedia on Mobile
Devices, 6821(1):682102, 2008.
[31] A. Pattath, D. S. Ebert, W. Pike, and R. A. May. Contextual interaction for geospatial visual analytics on mobile devices. Multimedia on
Mobile Devices, 7256:72560H, 2009.
[32] D. Phan, L. Xiao, R. Yeh, and P. Hanrahan. Flow map layout. In
Proceedings of IEEE Symposium on Information Visualization, pages
219–224, 2005.
[33] Police.uk. Internet: http://www.police.uk, [Mar. 23, 2012].
[34] Z. Pousman, J. Stasko, and M. Mateas. Casual information visualization: Depictions of data in everyday life. IEEE Transactions on
Visualization and Computer Graphics, 13(6):1145–1152, Nov.-Dec.
2007.
[35] PROJ.4 cartographic projections library. Internet: http://trac.
osgeo.org/proj/, [Mar. 23, 2012].
[36] R. Roth, K. Ross, B. Finch, W. Luo, and A. MacEachren. A usercentered approach for designing and developing spatiotemporal crime
analysis tools. In Proceedings of GIScience, Zurich, Switzerland,
2010.
[37] A. Sanfilippo, R. May, G. Danielson, B. Baddeley, R. Riensche,
S. Kirby, S. Collins, S. Thornton, K. Washington, M. Schrager,
J. Van Randwyk, B. Borchers, and D. Gatchell. InfoStar: An adaptive visual analytics platform for mobile devices. In Proceedings of
the ACM/IEEE conference on Supercomputing, pages 74–83. IEEE
Computer Society, Nov. 2005.
[38] Seattle data. Internet: https://data.seattle.gov/, [Nov. 5,
2013].
[39] Spotcrime. Internet: http://www.spotcrime.com, [Nov. 4,
2013].
[40] J. J. Thomas and K. A. Cook, editors. Illuminating the Path: The R&D
Agenda for Visual Analytics. IEEE Press, 2005.
[41] J. W. Tukey. Exploratory Data Analysis. Addison-Wesley, 1977.
[42] Uniform Crime Reports, Federal Bureau of Investigation. Internet: http://www.fbi.gov/about-us/cjis/ucr/ucr,
[November 28, 2011].
[43] J. J. van Wijk and E. R. van Selow. Cluster and calendar based visualization of time series data. In Proceedings of IEEE Symposium on
Information Visualization, pages 4–9, 1999.
[44] J. Wood, J. Dykes, A. Slingsby, and K. Clarke. Interactive visual exploration of a large spatio-temporal dataset: Reflections on a geovisualization mashup. IEEE Transactions on Visualization and Computer
Graphics, 13(6):1176–1183, 2007.

ACKNOWLEDGEMENTS
This work was partially funded by the U.S. Department of Homeland Security’s VACCINE Center under Award Number 2009-ST061-CI0003. Jang’s work was supported in part by Basic Science
Research Program through the National Research Foundation of
Korea (NRF) funded by the Ministry of Education, Science and
Technology (NRF-2013R1A1A1011170). We would like to thank
the reviewers for their valuable suggestions and comments, which
helped to improve the presentation of this work. We also would like
to thank Shantanu Joshi for his contribution in the project.
R EFERENCES
[1] ALOHA. Internet: http://response.restoration.noaa.
gov/aloha, [Mar. 23, 2012].
[2] Ask Dr. ALOHA: Choosing toxic levels of concern.
Internet:
http://archive.orr.noaa.gov/book_shelf/
1475_ToxicLOCs.pdf, [Mar. 23, 2012].
[3] G. Andrienko, N. Andrienko, P. Jankowski, D. Keim, M. Kraak,
A. MacEachren, and S. Wrobel. Geovisual analytics for spatial decision support: Setting the research agenda. International Journal of
Geographical Information Science, 21(8):839–857, 2007.
[4] G. Andrienko, N. Andrienko, and H. Voss. GIS for everyone: the
CommonGIS project and beyond. In M.Peterson, editor, Maps and
the Internet, pages 131–146. Elsevier Science, 2003.
[5] G. L. Andrienko and N. V. Andrienko. Interactive maps for visual
data exploration. International Journal of Geographic Information
Science, 13(4):355–374, 1999.
[6] N. Andrienko and G. Andrienko. Exploratory Analysis of Spatial and
Temporal Data: A Systematic Approach. Springer - Verlag, 2006.
[7] N. Andrienko, G. Andrienko, and P. Gatalsky. Exploratory spatiotemporal visualization: an analytical review. Journal of Visual Languages & Computing, 14(6):503–541, 2003.
[8] L. Anselin, I. Syabri, and Y. Kho. GeoDa: An introduction to spatial
data analysis. Geographical Analysis, 38(1):5–22, Jan. 2006.
[9] P. Baudisch and C. Holz. My new PC is a mobile phone. ACM XRDS,
16(4):36–41, June 2010.
[10] C. A. Brewer. Designing Better Maps: A Guide for GIS users. ESRI
Press, 2005.
[11] CAMEO software suite.
Internet: http://response.
restoration.noaa.gov/oil-and-chemical-spills/
chemical-spills/response-tools/
cameo-software-suite.html, [Mar. 23, 2012].
[12] S. Chainey and J. Ratcliffe. GIS and Crime Mapping. Mastering GIS:
Technology, Applications & Management. John Wiley & Sons, 2006.
[13] H. Chen, H. Atabakhsh, T. Petersen, J. Schroeder, T. Buetow,
L. Chaboya, C. O’Toole, M. Chau, T. Cushna, D. Casey, and Z. Huang.
Coplink: visualization for crime analysis. In Proceedings of the Conference on Digital Government Research, pages 1–6, 2003.
[14] CrimeMapping. Internet: http://www.crimemapping.com,
[Mar. 23, 2012].
[15] CrimeReports.
Internet: http://www.crimereports.com,
[Mar. 23, 2012].
[16] J. Dykes, A. MacEachren, and M.-J. Kraak. Exploring Geovisualization. Elsevier, 2005.
[17] M. Endsley and D. Garland. Situation awareness: analysis and measurement. Lawrence Erlbaum Associates, 2000.
[18] M. R. Endsley. Toward a theory of situation awareness in dynamic
systems. Human Factors: The Journal of the Human Factors and
Ergonomics Society, 37(1):32–64, 1995.
[19] G. Hamza-Lup, K. Hua, M. Le, and R. Peng. Dynamic plan generation and real-time management techniques for traffic evacuation.
IEEE Transactions on Intelligent Transportation Systems, 9(4):615–
624, Dec. 2008.
[20] C. Harrison. Appropriated interaction surfaces. Computer, 43(6):86–
89, 2010.

176

ShotVis: Smartphone-Based Visualization of OCR Information
from Images
BIAO ZHU, HONGXIN ZHANG, and WEI CHEN, Zhejiang University
FENG XIA, Dalian University of Technology
ROSS MACIEJEWSKI, Arizona State University

While visualization has been widely used as a data presentation tool in both desktop and mobile devices, the
rapid visualization of information from images is still underexplored. In this work, we present a smartphone
image acquisition and visualization approach for text-based data. Our prototype, ShotVis, takes images of
text captured from mobile devices and extracts information for visualization. First, scattered characters
in the text are processed and interactively reformulated to be stored as structured data (i.e., tables of
numbers, lists of words, sentences). From there, ShotVis allows users to interactively bind visual forms to
the underlying data and produce visualizations of the selected forms through touch-based interactions. In
this manner, ShotVis can quickly summarize text from images into word clouds, scatterplots, and various
other visualizations all through a simple click of the camera. In this way, ShotVis facilitates the interactive
exploration of text data captured via cameras in smartphone devices. To demonstrate our prototype, several
case studies are presented along with one user study to demonstrate the effectiveness of our approach.
CCS Concepts: • Information systems → Data scans; Mobile information processing systems;
• Human-centered computing → User interface design; Information visualization
Additional Key Words and Phrases: Cyber-physical interaction, data visualization, touch-based interface,
interaction, smartphone, wearable computing
ACM Reference Format:
Biao Zhu, Hongxin Zhang, Wei Chen, Feng Xia, and Ross Maciejewski. 2015. ShotVis: Smartphone-based
visualization of OCR information from images. ACM Trans. Multimedia Comput. Commun. Appl. 12, 1s,
Article 12 (October 2015), 17 pages.
DOI: http://dx.doi.org/10.1145/2808210

1. INTRODUCTION

Imagine a menu at McDonalds, labels on grocery store shelves, data tables in text books.
Underlying each of these items is a massive quantity on data that can be captured and
explored. While visualization is perhaps one of the quickest means of comparing the
price of twenty cans of tomato soup or plotting trends from data tables, visualization
only truly lends itself to digitized information that can have structure imposed upon it.
The fact is, it is difficult to transform information found in the real world into digital
form. While tools for turning McDonald’s menus into a data file exist, solutions typically
This work is supported by the Major Program of the National Natural Science Foundation of China (under
grant 61232012), the National Natural Science Foundation of China (under grant 61422211), the Fundamental Research Funds for the Central Universities, the Major Program of the Natural Science Foundation of
Zhejiang province, China (under grant LZ12F02002) and the US National Science Foundation (under grant
1350573).
Authors’ addresses: B. Zhu, H. Zhang, and W. Chen, State Key Lab of CAD&CG, Zhejiang University,
China; email: arthurbzhu@gmail.com; {zhx, chenwei}@cad.zju.edu.cn; F. Xia, School of Software, Dalian
University of Technology, China; email: f.xia@ieee.org; R. Maciejewski, Arizona State University; email:
rmacieje@asu.edu.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by
others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
c 2015 ACM 1551-6857/2015/10-ART12 $15.00

DOI: http://dx.doi.org/10.1145/2808210
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

12

12:2

B. Zhu et al.

require professional hardware and toolkits from computer vision [Szeliski 2010], video
processing [Wang et al. 2001], text analysis [Miner et al. 2012], and signal processing [Lyons 2010]. These tools are simply not readily available to nonprofessional/
casual users. As such, it is uncommon to find ubiquitous visualization tools for dailylife information analysis. However, the global adoption of smartphones has led to the
rise of ubiquitous computing [Weiser 1999; Oulasvirta et al. 2012]. What smartphones
provide is a handheld camera and universal means of capturing data through images
or video. With access to high-resolution cameras and high-speed wireless networks,
smartphones are naturally suited for the capturing, processing and analysis of information, and are being increasingly used as multimedia computation and illustration
platforms [Munzner 2014].
While the ubiquity of smartphones can provide a means of capturing and processing
data, there is a distinct need for understanding data. Traditionally, visualization is
one of the most important, and commonly used, methods of generating insight into
large scale data. Particularly for textual or tabular data, visualization is used to create
summaries that take advantage of the power of the human perception [Roberts et al.
2014]. Even though visualization has been widely used as a data presentation tool in
both desktop and mobile devices, the rapid visualization of information from visual
media (e.g., images, videos) is still underexplored. In addition, most previous works
focus on the visualization of datasets that are recorded as data tables or formatted
data files. To the best of our knowledge, there is little work dedicated to visualizing
information contained in images and videos.
The presented work is motivated by the fact that smartphones are the most generally
available computational devices. By utilizing the ubiquity of smartphones and the
ability of smartphones to capture and process data, our goal is to enable visualization
of information that is not readily available as a data file. In doing so, we will provide
user with a means of generating insight into nontraditional data sources. Our work
focuses on the integration of a suite of data processing techniques, including cameras,
touch-based interfaces and optical character recognition (OCR) [Microsoft 2014] within
an interactive data acquisition and visualization pipeline. The unique feature of this
pipeline is that it retranslates text information contained in images into visual forms,
thereby providing users with a viable means of generating insight from large quantities
of (potentially) nondigitized data.
While the data capture and processing portion of the pipeline is challenging, it is important to note the inherent challenges that visualization faces on smartphones. These
devices have limited screen space and computing power. Furthermore, interaction traditionally found through a keyboard and mouse is replaced with touch and gesture
interactions. Our goal is to develop an interactive system for data capture, processing
and analysis that is both visually expressive and intuitive to use.
In this article, we describe a novel mobile visualization scheme that builds upon the
latest OCR, touch interaction and visual design techniques, and leads to a new portable
visual design tool. Our contributions include:
—a new visual transformation mode that reformulates camera-captured images into
expressive visual representations;
—an efficient touch-based interaction scheme that allows for direct manipulation and
structuring of scattered characters into visualizable data forms;
—a smartphone-based visualization system that supports the recognition, structuring
and design of visualizations based on camera-captured images.
Previous work has primarily focused on designing visualizations that are appropriate
for lower screen resolutions and proper interaction techniques in the context of mobile
devices (e.g., Chittaro [2006] and Burigat et al. [2006]). Specific layout and interaction
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

ShotVis: Smartphone-Based Visualization of OCR Information from Images

12:3

principles for mobile devices are described in Yoo and Cheon [2006], and design
principles from past work were used during our prototype development. Our approach
advances work in this area by designing an easy-to-use means of data acquisition and
visualization for mobile devices. While the popular usage (e.g., indoor navigation) of
camera-captured images is for photo sharing, our approach allows users to seamlessly
turn images that contain text into visualizations. Examples include understanding a
large textual paragraph with tag-cloud visualizations, reading data form hand-written
charts, and comparing multiple product specifications with graphs, as demonstrated
in the result section. Most similar to our proposed work is ReVison [Savva et al. 2011],
which seeks to scan and automatically convert existing charts (data visualizations)
back to a tabular data format. Once the data is acquired in ReVision, it can then be
transformed into new visualizations using different visual designs, for example, changing pie charts to bar charts. The pipeline in ReVision specifically focuses on extracting
information from existing visualizations using advanced pattern recognition and data
mining techniques. Our works differs from ReVision in that we focus specifically on
characters rather than rendered charts. Furthermore, our work leverages smartphones
and user interactions to make it a practical solution for novices and experts alike.
2. RELATED WORK

As previously stated, the most similar work to our proposed pipeline is the ReVision
system by Savva et al. [2011]. ReVison was developed to take images of existing charts,
turn these charts back into the original structured data set and then allow users to turn
them into new visualizations. In this manner, data that was only previously available
as bitmap images can now be extracted, edited, and revisualized. Unfortunately, the
data extraction rate of bitmap-image charts is relatively low even with state-of-the-art
techniques. In some cases, this is caused by poor image quality, while in other cases it
is caused by fidelity issues such as tiny sectors within a pie chart or low resolution axis
labels on a grid. While the goal of ReVision was to extract and remap data from charts,
our proposed pipeline is to take images of textual data found in the physical world,
digitize the data and then allow users to visualize the captured data. While tools such as
Office Lens Office Lens [Microsoft 2014], provide text recognition for mobile devices, our
goal is to expand from text recognition to visualization. Although our pipeline shares
similar factors to that of OfficeLens [Microsoft 2014] with regards to data capture and
recognition, ShotVis is unique in that our pipeline encompasses an all-in-one targeted
visualization solution. Moreover, our approach is focused on the data manipulation
and visual design aspects which are key issues for visualization authoring directly on
smartphones. While OfficeLens can be employed as an OCR backend of our approach, its
functionality would only serve part of the proposed pipeline. Thus, our article presents
a pipeline for capturing data in the physical world, transforming this data into digital
information and then providing multiple visualization options for viewing the newly
captured data.
It is important to note that our work is not the first to propose using smartphones
for data capture and visualization. Chittaro [2006] and Burigat et al. [2006] discussed
different aspects of visualizing information on mobile devices, such as how to visualize
locations of off-screen objects. Yoo and Cheon [2006] proposed different visualization
methods for different data types and found that sequential layouts are suitable for
data with less relational information while radial layouts are more suitable for hierarchical information. Hao and Zhang [2007] proposed an interface for hierarchical
information, which takes advantage of connection and enclosure approaches within the
limits of screen resolution on mobile devices. Work by Kim et al. [2008] and Razip et al.
[2014] explored using mobile devices for improved situational awareness, and several
research projects (e.g., Zhou et al. [2006] and Lamberti and Sanna [2007]) explored 3D
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

12:4

B. Zhu et al.

visualizations for mobile devices. However, all of the aforementioned works utilize data
that is preprocessed and store either on the mobile phone or through servers available
to the client. In these cases, the visualizations are designed based on a known data
structure, format, and visualization type. In ShotVis, we want to enable users to capture more free-form data; however, since the data type being captured is unknown,
our visualization needs to handle data capture, data manipulation and visualization of
unstructured data. To the best of our knowledge, our work is the first time a full data
capture to visualization pipeline has been proposed for a mobile device.
With regards to data capture, smartphones are equipped with a large number of
sensors (e.g., microphones, motion sensors, cameras, etc.) which can be used to capture
information in a user’s environment. These devices can be further augmented with
external sensors, and their portable form factor and ubiquity has led many researchers
to begin exploring how one can utilize the data acquisition abilities of a smartphone to
improve peoples’ day-to-day lifestyle. For example, Buttussi and Chittaro [2008] and
Kanjo et al. [2008] both utilized mobile devices to collect and visualize geolocation
and movement data, respectively. Macı́as et al. [2011, 2012] tagged videos with data
collected by sensors embedded in the phones. Vasilev et al. [2012] visualized context
captured in smart space with HiveMind. Girod et al. [2011] developed a tool where a
picture snapped with a handheld device becomes a search query, allowing things like
comparison shopping.
While previous researchers have used mobile sensors for the capture and visualization of data, our work moves past the type of passive data collection as seen in Buttussi
and Chittaro [2008] and Kanjo et al. [2008] and brings the user into the data capture
process in a manner similar to Girod et al. [2011]. The difference here is that our data
capture is not intended for search queries, but it is intended for sense making and
knowledge acquisition, taking free-form data found in the physical world and turning
this into digital information that can be visualized. In order to capture free-form data
(specifically text data), we utilize optical character recognition tools (OCR) [Wikipedia
2014], which is used for the conversion from image text to machine-encoded text. These
tools are a key part of our proposed pipeline in that they transfer image blocks containing scattered characters into numbers and words. Once the data is captured, the
data then needs to be organized and structured for visualization purposes. Given that
the data we are proposing to capture is free-form, we need to incorporate data wrangling techniques for labeling and manipulating the data to make it appropriate for
visualization. Data wrangling is the process of manually converting data from one
form into another format that allows for more convenient consumption of the data with
automatic tools.
Recent work in the visualization community has explored the development of tools
for aiding the data wrangling process. Specifically, Wrangler [Kandel et al. 2011] was
developed to combine the direct manipulation of data with automatic inference of relevant data transforms, thus relieving some of the manual burden from the user and
making data wrangling more efficient and effective. However, Wrangler focuses on
structured but incomplete or faulty data. In our application, the data encountered in
daily life is typically unstructured (and often incomplete). To manipulate such unstructured data, the traditional approach is to write scripts in Python, Perl, and R to
transform the data, or to manually edit the data with tools like Microsoft Excel. However, such massive editing and script writing is not transferable to mobile devices. As
such, our work explores designing data wrangling operations specific for smartphones
to enable intuitive and efficient data manipulation on mobile devices.
Different from the “mouse-and-keyboard” interaction in desktop PC environments,
interaction design on smartphones is crucial for visualizing data on small screens. A
natural solution is to support sketch gestures for smartphone applications instead of a
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

ShotVis: Smartphone-Based Visualization of OCR Information from Images

12:5

Fig. 1. The ShotVis pipeline.

button-based interface. Furthermore, sketching often conveys information that can be
hard to explain using text or button inputs, while at the same time sketching does not
require a tremendous amount of effort from the user. As human-computer interaction
moves towards higher level languages, sketching will certainly continue to have a place
in mobile applications and visualization. Rather than employing complex sketch shape
classifications in the command recognition stage [Schneider and Tuytelaars 2014], we
apply a relatively simple way to recognize users’ input [Wobbrock et al. 2007] using a
2D single stroke [Girod et al. 2011].
3. SHOTVIS

In this section, we describe our pipeline for capturing data in the physical world,
transforming this data into digital information and then providing multiple visualization options for viewing the newly captured data. Our approach is applicable for
scenarios where textual information (recognizable characters or numbers) is physically presented, for example in a booklet, a poster, or a hand written whiteboard. While
such data is readily available for human consumption, our goal is to automatically
synthesize this data and transform this into visualizations. In this way, we hope to
enable users to quickly gain insight into large amount of physical data. Our approach
combines automatic text recognition, data manipulation/wrangling and visualization
design with requirements of a modest amount of user interactions. Figure 1 presents
the ShotVis pipeline together with the flow of user interaction.
First, a user takes a photograph of text found in the physical world with the smartphone. Automatic image alignment and character recognition are performed on the
photo. Then, the recognized characters are interactively organized and manipulated
into structured data tables or forms. An initial organization is suggested, and, subsequently, the user manually maps the data into their desired visual forms by selecting
visual transformations and encodings.
3.1. Data Recognition

Suppose that a user captures one image I with a smartphone, and the image contains information (handwritten characters or numbers which may be freely placed or
organized as tables, forms, or lists) that the user wants to visualize. For the sake of
simplicity, we denote such images as raw data R. Conventionally, visualization is applied to datasets that are stored as data tables or data forms. If the targeted data is
displayed in physical media, such as books or slides, a data scanning process must be
performed to digitize the data. The process includes three steps.
—First, a captured image I (Figure 2(a)) is cleaned and registered with a state-of-theart image warping technique [Lee et al. 2014]. Figure 2(b) shows the warped result
of Figure 2(a).
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

12:6

B. Zhu et al.

Fig. 2. Illustrating the process of data scanning.

—Second, an OCR process is performed on the calibrated image, resulting in a list of N
scattered characters or strings Si (i = 1, 2, . . . , N) (Figure 2(c)). An additional check
is then performed to classify the data into a category, such as tabular, paragraph,
unrecognized image, etc.
—Third, for each Si , an additional attribute set Ai is automatically recognized for the
purpose of preserving consistency between the raw data and the final visualization.
The attribute set Ai is actually a triple < fi , pi , li > which consists of a font color
of the physical characters fi , a location center coordinate pi of string Si , and a
set of category labels li . Currently, several content category label are supported in
our implementation including text, numbers, and geographical locations. However,
the definition of the category labels can be dynamically extended for future data
acquisition and visualization purposes.
Finally, the raw data R = {(Si , Ai )|i = 1, 2, . . . , N} is generated.
The output of OCR is unstructured data which consists of a set of strings Si with
corresponding 2D coordinates pi = (xi , yi ). If the data R is categorized as tabular (as
in Figure 2(c)), only a single preprocessing step needs to be performed to construct
a table-like structure from the image. To achieve this, we utilize integral-projection
row-column relationships for all Si . We project all positions pi to the X- and Y-axis.
After this transformation, coordinates are clustered into several groups and a simple
median thresholding is applied to distinguish columns and rows.
3.2. Data Manipulation

As shown in Figures 3(a)-(b), raw textual data is directly generated from an image.
However, the generated data only consists of characters and their corresponding locations within the image. Without other contextual information, these data are not
ready for visualization. The resultant data are unstructured, in other words, there are
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

ShotVis: Smartphone-Based Visualization of OCR Information from Images

12:7

Fig. 3. (a) A camera image of the physical print data. (b) Optical character recognition has been performed.
(c) Data has been selected by the user. (d-e) The data is then mapped to data attributes via the user interface.
(f) The final mapped data table is then viewed by the user.

Fig. 4. Sketch gestures for data manipulation: (a) Select data for X axis, (b) Select data for Y axis, (c) Clear
data selection, (d) Add a data group.

no rows, columns, or subtables. Along with the missing structure problems, the generated data may also contain errors due to poor character recognition. Thus, in order to
visualize the data, users must clean, select, and label dimensions from the raw data
through a series of interactions.
3.2.1. Gestures. The interaction for data manipulation consists of two simple steps:

(1) the user selects data via sketching gestures;
(2) the user assigns data labels to the set of selected data.
ShotVis recognizes several gestures which can be sketched by the user and automatically manipulates the data according to the supported gestures. Figure 4 shows the
four sketching gestures that can be used to label and assign data variables to axes. To
create a chart (bar chart, tally chart, line chart, and scatterplot) , the user will create
an arrow sketch; the clear operation is invoked with a circle gesture, and; adding a
data group is invoked with a one touch “+” sketch.
Sketching operations are processed via a lightweight gesture recognition library,
$1 Unistroke Recognizer [Wobbrock et al. 2007]. This library is a 2D single-stroke
recognizer designed for the rapid prototyping of gesture-based user interfaces. To fully
utilize the touch interface of a smartphone, a two-finger touch gesture is applied as an
on/off switch to change between the gesture command mode and the true editing-andselection mode. Alternatively, a row of five buttons has been added to the bottom of the
screen. Each button, when activated, corresponds to one of the five sketching gestures
(including the switch gesture), and each button is rendered with the corresponding
gesture symbol for ease of use.
These operations have been specifically designed to help structure the data for visualization. Typically, for visualization, each data attribute describes a single aspect of a
record, and multiple records form a data set. To visualize such a data set, we need to
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

12:8

B. Zhu et al.

Fig. 5. Algebraic operation illustration: (a) Two dimensions are selected. (b) Initial interface. (c) After user
interaction. (d) A new dimension, U nit cost = Cost/Net f low, is generated.

map each data attribute to a separate visual channel. For example, mapping attribute
A to the X-axis and attribute B to the Y-axis creates scatter plot.
3.2.2. Labeling. On the bottom of Figure 3(b), there exists two visual channel buttons
corresponding to the X and Y axes. To select and map an attribute to a data axis, users
need to first tap on either the axes-Y-button or axes-X-button. After that, users can
then select data by tapping on or swiping over each data entry. Once the data for an
attribute is selected, users can then tap on the corresponding visual channel button
again to further edit the data attribute. The editing interface is shown in Figures 3(d)
and 3(e). This interface allows users to perform editing operations including assigning
a name/type/unit to the attribute.
As part of data labeling, users may also wish to derive data characteristics from the
data set. For example, Figure 3(a) contains information about the price, net flow, and
call time of each package. However, a user may wish to visualize the unit cost of net flow
or call time. This would require algebraic manipulation of the data. ShotVis enables
users to perform algebraic operations with existing data, as illustrated in Figure 5.
First, in order to get the unit cost dimension, two dimensions, net flow and call time,
are selected (Figure 5(a)). Then a long-tap interaction is performed upon the blank
area to activate the algebraic operation. Next, in Figure 5(b), the name of each selected
dimension and the available algebraic operators are all shown as draggable labels.
Users can drag these labels into the white area to construct an algebraic expression
and derive new data dimensions, Figure 5(c). Finally, after a user confirms the algebraic
operation, the new dimension will be added to the original data form, Figure 5(d).
3.2.3. Cleaning. While gestures allow users to label and manipulate data, it is possible
that the OCR algorithms used will cause the scanned data to contain errors. As such,
it is often necessary to clean the data. To improve data cleaning, the captured data is
labeled using a specific color coding for all recognized string blocks. Blocks filled with
red indicate that there is likely to be an unrecognized character string in that element.
Such data can be corrected in the value editing mode. While users can manually
adjust the data, this process would be burdensome, and we have automated many
data cleaning tasks in order to make ShotVis easy to use.
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

ShotVis: Smartphone-Based Visualization of OCR Information from Images

12:9

Our first automated option involves automatic data matching which allows users
to make efficient data selections and extractions by modifying two parameters, From
and Every. This option is necessary when data attributes are co-located in a column.
For example, in Figure 3(a), two dimensions, net flow and call time, are co-located in
the same column. Conventionally, instead of swiping, the user would need to select,
by tapping each item, and then label each data value as part of a data dimension.
This process, as described, can be very slow and inaccurate. However, adjusting From
and Every can make this process quite efficient. First, the user selects the whole
column by swiping. Then, by setting From to 0 and Every to 2, the 1st, 3rd, 5th· · ·
items, which consist of the net flow dimension are grouped, and setting From to 1
and Every to 2 will select the 2nd, 4th, 6th· · · items, which consist of the call time
dimension. Thus, by using our automatic data matching tool, the ith items can be
selected, where i ∈ From + n × Every, n = (0, 1, 2 . . .). Furthermore, ShotVis is designed
to automatically recommend the value of From and Every. Again, taking Figure 3 as
an example, if we choose the right column first and then the left column, ShotVis will
detect that the number of rows in the left column is twice the number of rows in the
right column. ShotVis will then recommend Every to be 2 and leave From to 0, which
automatically selects the net flow dimension.
Our next automated option includes content-aware editing. When the data table
contains information about an attribute, but the data is recorded differently (part of
the data is metric units and part is British units), we need to make unit conversions
in order to have an appropriate visualization. In this case, the user must choose the
default unit and define the conversion ratio. We have predefined the most common
units and their conversion ratios, thus making this operation automatic in many cases.
Next, data extracted needs an automatic data type assignment. In ShotVis, we define
three data types, category, numeric and geography. Different combinations of data
types lead to different types of visualization. For example, a combination of categorical
and numeric attributes will result in a bar chart or a line chart, while a combination of
purely numeric attributes can result in a scatter-plot. Furthermore, when geographical
attributes are detected, maps can be produced. Attribute data starting with a letter
(A-Z character) are automatically labeled as a categorical attribute, except when the
data are exact geography locations (e.g., names of cities, countries, and continents), in
that case, the data will be labeled as a geography attribute. Attribute data starting
with a number (0-9) are automatically labeled as a numeric attribute. Note that the
type assigned by ShotVis is a recommendation which the user can change in the case
of an error.
Since multiple attributes may be captured, we have developed tools for automatic
data group recommendations. In ShotVis, we employ an “add” operation to realize this
functionality. Each time a user finishes a selection from a data group, the user can press
the “add” button to confirm this selection and start a new one. With the knowledge that
data groups usually appear one by one in regular tabulation (e.g., the data shown in
Figure 7(a)), we can automatically recommend the selection of the next data group
after user finishes the selection of a data group.
3.3. Data Visualization Design

Once data is cleaned, labeled, and selected, the next goal is to visualize the data. After
data processing, an initial visualization is automatically generated according to the
combination of data types. However, users all have personal preferences and may want
to make adjustments to the automatically generated visualization.
When creating an information visualization solution, a major challenge is selecting
the best visual option to represent the data. There are two main approaches for designing a visualization, namely exploratory and explanatory visualization. Choosing
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

12:10

B. Zhu et al.

Fig. 6. (a) The initial visualization. (b) Changed to line chart. (c) Changed to a new palette. (d) Title modified.

one depends on the context of use, the preset goal, the intention of the user, and the
potential audience.
Exploratory visualizations are used to discover patterns, trends, or subproblems in a
captured data set. They are useful for performing analysis on data sets without having
a detailed understanding of the data content. Explanatory visualizations are used to
transmit information or a point of view from the user (in this case, the visualization
designer) to an audience. Unlike exploratory visualizations, explanatory visualizations
are employed when the user knows the story behind the data and would like to communicate it to an audience through visualization.
In order to enable customizable visualizations, ShotVis allows users to make interactive and content-aware revisions to a visualization. For example, users can try
different visualization methods for the same data as shown in Figure 6. Users can also
change the title and the string colors as demonstrated in Figures 6(c)-6(d). For the colors, ShotVis includes palettes designed for perceptually discriminable colors [Harrower
and Brewer 2003] and also for the colors collected from the original bitmap-image data,
as discussed in Section 3.1. While color schemes and line styles can drastically alter
how a visualization is perceived and received by an audience, the focus on our work
has been on the visualization pipeline of ShotVis. Future work will focus on automatically recommending appropriate color schemes and line styles based on data types
(sequential data should use a sequential color scheme, etc.).
4. CASE STUDIES

ShotVis aims to enable the visualization of printed textual data. Traditionally, when
data is not available in digital form, we need to input it into a computer first, which can
be very time consuming. With ShotVis, users only need to take a photo, then the data
can be seamlessly translated into a digital datafile. In this section, several examples of
how ShotVis works are presented to demonstrate its effectiveness.
4.1. Anscombe’s Data

In this case, we take Anscombe’s Data (see Figure 7(a)) as an example case to demonstrate ShotVis. This data contains four datasets, the mean, variance, correlation and
linear regression of each dataset are nearly identical; however, their distributions in
a Cartesian coordinate system are quite different. Here, visualization becomes a very
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

ShotVis: Smartphone-Based Visualization of OCR Information from Images

12:11

Fig. 7. (a) Anscombe’s data. (b) Choose multiple data groups in ShotVis. (c) Anscombe’s data visualized with
ShotVis.

useful tool for revealing such differences and helping people understand the data better. With ShotVis, we can visualize Anscombe’s data simply by taking a picture of
our textbook as shown in Figure 7(a). Then four groups of data are selected with the
“+” operation (Figure 7(b), which is discussed in Section 3.2.1). Finally, the four data
groups are visualized side by side. As we can see in Figure 7(c), the data distributions
are clearly revealed by visualization where simply looking at the chart alone would be
insufficient.
4.2. Paper Data

Many people read every day, they read newspapers, articles, and books. However, reading is time consuming. Even for experienced researchers, it takes several minutes for
them to summarize what an article is about. Word clouds are an effective visualization
for summarizing the most frequent terms in an article. Word clouds first determine
simple statistics about the frequency of the words in an article and then adjust the
words’ size and position according to their frequency.
However, in the real world, we often only have access to reading material in printed
form, not digital. Creating word clouds from print data would require a user to transcribe the print material back into digital form. However, with ShotVis, the user can
skip the transcription step entirely, replacing it with the simple act of photographing
the physical document. As an example, we took a photo of the (1st draft) abstract and
introduction of this article with ShotVis (Figures 8(a) and 8(b)) and visualized it using
a word cloud (Figures 8(c)-8(e)). From the results of the word cloud, we can get an
overview of the keywords of this article, including “ShotVis”, “media”, and “Visual”.
4.3. Map Data

The target of ShotVis is to assist users in visualizing all kinds of printed information.
This requires ShotVis to provide multiple kinds of visualizations. Here we demonstrate the visualization of geographic information (Figure 9). ShotVis can recognize
the “Continent” attribute as a categorical variable and the “Population” attribute as a
numeric variable automatically. From there, ShotVis recommends visualizing the data
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

12:12

B. Zhu et al.

Fig. 8. (a) and (b) photos of the abstract and introduction in this article. (c) Word cloud visualization with
orientation modification in (d) and with font modification in (e).

Fig. 9. (a) A photo with a tabulation of the world’s population. (b) After data manipulation. (c) Bar chart
visualization of the data. (d) Map visualization of the data.

as a bar chart or line chart as shown at the bottom of Figures 9(c)-9(d). However, since
the names of the continents are predefined inside ShotVis, our system also recommends
the map-based visualization.
5. USER STUDY

ShotVis is designed to be used in mobile environments rather than on the desktop. We
have conducted a user study to evaluate both the usability and effectiveness of ShotVis.
Each participant was given three distinct pieces of printed textual data and asked to
create two visualizations for each piece of data with ShotVis and with visualization
tools they are familiar with on the PC. After completing the tasks, a questionnaire was
given to evaluate the user experience of ShotVis. Subjective feedback is also collected
from the participants through an interview at the end of the experiment.
5.1. Task and Questionnaire Design

In the evaluation, four participants (two male and two female) participated in this user
study, with ages ranging from 25 to 27. All participants are skilled in using smartphones
and familiar with touch interactions. The majors of the participants include Computer
Science, Software Engineering, and Aerospace Engineering.
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

ShotVis: Smartphone-Based Visualization of OCR Information from Images

12:13

No subject had previous experience with ShotVis. A short training session was
provided to teach them how to use ShotVis. The training consisted of a set of demo
videos of the datasets used in Section 4 together with a real-time explanation from the
instructor. After that, the participants were permitted to play with the datasets first
and to ask the instructor any questions in order to make sure that they all fully understand ShotVis and were familiar with its interactions.
The task for the participants was to visualize the three printed documents used in
Section 4. First, they were asked to visualize the data with any tool they are familiar
with on a PC and then with ShotVis on a mobile Device. All the datasets were provided
as printed documents, except the “paper” one. For the “paper” dataset, they were free
to choose either the printed or digital edition.
With ShotVis, participants were asked to examine whether there were mistakes in
the recognized data and fix the mistakes (if any were found). After that, they can freely
manipulate and visualize the data. With the PC, there was no constraint and they were
allowed to use any visualization tool they were familiar with. However, for tasks that
required using printed documents as input, participants were asked to transfer the
data from the printed documents to digital ones using either OCR software or by hand.
After finishing the tasks, the participants were asked to answer a questionnaire with
five questions (Q1-Q5) on a 5-point scale (1 = very positive, 5 = very negative). The
questions are listed here.
Q1.
Q2.
Q3.
Q4.
Q5.

Is it easy to learn how to use ShotVis?
Are the interactions in ShotVis intuitive?
Is it more efficient to make visualizations with ShotVis than with a PC?
Is it easy to make a visualization you are satisfied with in ShotVis?
Will ShotVis be useful in daily life and for common users?

5.2. Results

Completion time for the visualization of each case was collected for evaluation for both
the ShotVis and PC tools. We also recorded the time the participants spent entering the
data into their computer. As we can see in Figure 10(a), for the first two tasks, the time
spent in ShotVis is much shorter than that spent in entering the printed documents
into PC, let alone the whole time spent with PC. While the result is not surprising as
ShotVis was designed for this task, it further demonstrates the need for such a tool
in the case where digital data is not readily available. For the last case, the “paper”
one, no participant choose to input the data by hand, all of them used the digital data,
but the time spent in visualizing this data is still much longer with the PC than with
ShotVis.
Besides the time, we also collected the visualizations that participants made both
with a PC and with ShotVis. In Figure 11, each column corresponds to one case, the
last row is the visualization made with ShotVis while other rows are those made with
a PC. We have labeled the time cost of each visualization. From the results, we can see
that compared to the conventional way we make visualizations (i.e., with PC), ShotVis
can usually achieve equal or better results faster.
The result of the questionnaire is shown in Figure 10(b). As we can see, the average
ratings are all below 3. For the first two questions, the average ratings are both below
2, demonstrating that ShotVis is intuitive and easy to learn. For the third question
about the efficiency of ShotVis compared to PC, all participants thought that it is quite
efficient to complete the visualization with ShotVis. Good efficiency implies a strong
usability of ShotVis. The rating for question 5 also implies that ShotVis was easy to
use as most participants think it will be quite helpful in their daily life. For question 4,
it was the highest rating, indicating relatively low satisfaction with the visualizations
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

12:14

B. Zhu et al.

Fig. 10. (a) Time spent in each visualization task. The orange bar shows the average time spent when
visualizing with ShotVis. The blue and purple bars show the average time spent entering data and making
visualizations with ShotVis and the PC, respectively. (b) The rating of the five questions in the questionnaire.
The number of responses is encoded in the radius of the circle. The orange triangles represent the average
rating for each question. We chose this representation of the results due to the low number of subjects in our
study.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

ShotVis: Smartphone-Based Visualization of OCR Information from Images

12:15

Fig. 11. Visualizations made by participants. Each column corresponding to one case. First two rows are
made with PC, last row is made by ShotVis. Each visualization is labeled with the tool by which it is made
and the time cost to make it.

created. This implies that more work can be done in the visualization template
design.
5.3. Limitations and Lessons Learned

ShotVis bridges the data acquisition gap present in print media to enable visualization
in everyday situations. However, ShotVis is still in the prototype phase and has several
limitations. ShotVis can handle tabular data and purely text data (for the word cloud
example), but for more unstructured data, like numbers from different parts of an
article, it will fail. Even in the case of tabular data, when the table cannot be captured
by a single image, it will be difficult for users to manipulate the data into an appropriate
digital form. In order to make ShotVis more usable, we need to introduce more efficient
interactions for smartphones, thus making it easier to handle unstructured and large
data. It will also be helpful to leverage advanced techniques in the field of machine
learning to reduce OCR errors, especially when capturing handwritten tables from
whiteboards.
ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

12:16

B. Zhu et al.

6. CONCLUSIONS AND FUTURE WORK

ShotVis is a touch-based data manipulation and visualization design system focusing on smartphones. ShotVis facilitates the construction of expressive and customized
visualization from data derived from camera-captured images. As smartphones are
becoming all-in-one mobile devices possessed by common users, our approach integrates data accquisition and visualization together that leverages the capabilities of
smartphones with the representation efficiency of visualization.
ShotVis provides a short pipeline from unorganized image data to well-organized
visual forms. Moreover, it allows users to interactively bind visual forms to the underlying data and specify visual attributes of selected forms with touch-based interactions. ShotVis facilitates understanding and interactive exploration of print media
using camera-captured images via smartphone devices. Several case studies have been
presented to demonstrate the effectiveness of ShotVis. According to our user study,
most users provided positive feedback about ShotVis and they were eager to use this
new solution.
Currently, our prototype is just a first step towards the ideal model of mobile visual
computing. Our approach is still limited by the recognition error rate and response
speed of OCR. In the future, we will explore the possibility of enhancing the OCR portion
and providing more content-aware factors. We also think it could be very interesting
to integrate our approach with augmented reality applications such as with Google
glasses and Facebook oculus. In conclusion, we believe that the way to author visual
media is changing, not only because of the nature of smartphones, but also due to the
everyday needs of users.
REFERENCES
Stefano Burigat, Luca Chittaro, and Silvia Gabrielli. 2006. Visualizing locations of off-screen objects on
mobile devices: A comparative evaluation of three approaches. In Proceedings of the 8th Conference on
Human-Computer Interaction with Mobile Devices and Services. ACM, New York, 239–246.
Fabio Buttussi and Luca Chittaro. 2008. MOPET: A context-aware and user-adaptive wearable system for
fitness training. Artif. Intelli. Med. 42, 2, 153–163.
Luca Chittaro. 2006. Visualizing information on mobile devices. Computer 39, 3, 40–45.
Bernd Girod, Vijay Chandrasekhar, David M. Chen, Ngai-Man Cheung, Radek Grzeszczuk, Yuriy Reznik,
Gabriel Takacs, Sam S. Tsai, and Ramakrishna Vedantham. 2011. Mobile visual search. IEEE Signal
Processing Mag. 28, 4, 61–76.
Jie Hao and Kang Zhang. 2007. A mobile interface for hierarchical information visualization and navigation.
In Proceedings of the IEEE International Symposium on Consumer Electronics, 2007 (ISCE’07). IEEE,
1–7.
Mark Harrower and Cynthia A. Brewer. 2003. Colorbrewer. Org: An online tool for selecting colour schemes
for maps. Cartograph. J. 40, 1, 27–37.
Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer. 2011. Wrangler: Interactive visual
specification of data transformation scripts. In Proceedings of the SIGCHI Conference on Human Factors
in Computing Systems. ACM, New York, 3363–3372.
Eiman Kanjo, Steve Benford, Mark Paxton, Alan Chamberlain, Danae Stanton Fraser, Dawn Woodgate,
David Crellin, and Adrain Woolard. 2008. MobGeoSen: Facilitating personal geosensor data collection
and visualization using mobile phones. Pers. Ubiq. Comput. 12, 8, 599–607.
SungYe Kim, Ross Maciejewski, Karl Ostmo, Edward J. Delp, Timothy F. Collins, and David S. Ebert. 2008.
Mobile analytics for emergency response and training. J. Inf. Visuali. 7, 1, 77–88.
Fabrizio Lamberti and Andrea Sanna. 2007. A streaming-based solution for remote visualization of 3D
graphics on mobile devices. IEEE Trans. Visuali. Comput. Graph. 13, 2, 247–260.
Hyunjoon Lee, Eli Shechtman, Jue Wang, and Seungyong Lee. 2014. Automatic upright adjustment of
photographs with robust camera calibration. IEEE Trans. Pattern Anal. Mach. Intell. 36, 5, 833–844.
Richard G. Lyons. 2010. Understanding Digital Signal Processing (3rd Ed.). Prentice-Hall.
Elsa Macı́as, Hanna Abdelfatah, Alvaro Suárez, and Alejandro Cánovas. 2011. Full geo-localized mobile
video in Android mobile telephones. Netw. Protocols Algor. 3, 1, 64–81.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

ShotVis: Smartphone-Based Visualization of OCR Information from Images

12:17

Elsa Macı́as, Jaime Lloret, Alvaro Suarez, and Miguel Garcia. 2012. Architecture and protocol of a semantic
system designed for video tagging with sensor data in mobile devices. Sensors 12, 2, 2062–2087.
Microsoft. 2014. Office lens: A OneNote scanner for your pocket. http://blogs.office.com/2014/03/17/
office-lens-a-onenote-scanner-for-your-pocket/ (March 2014).
Gary Miner, John Elder, Andrew Fast, Thomas Hill, Robert Nisbet, and Dursun Delen. 2012. Practical Text
Mining and Statistical Analysis for Non-structured Text Data Applications. Academic Press.
Tamara Munzner. 2014. Visualization Analysis and Design. A K Peters/CRC Press.
Antti Oulasvirta, Tye Rattenbury, Lingyi Ma, and Eeva Raita. 2012. Habits make smartphone use more
pervasive. Pers. Ubiq. Comput. 16, 1 (Jan. 2012), 105–114.
A. Razip, A. Malik, S. Afzal, S. Joshi, R. Maciejewski, Y. Jang, N. Elmqvist, and D. S. Ebert. 2014. A
mobile visual analytics approach for situational awareness and risk assessment. In Proceedings of IEEE
PacificVis.
Jonathan C. Roberts, Panagiotis D. Ritsos, Sriram Karthik Badam, Dominique Brodbeck, Jessie Kennedy,
and Niklas Elmqvist. 2014. Visualization beyond the desktop-the next big thing. IEEE Comput. Graphi.
Appl. 34, 6, 26–34.
Manolis Savva, Nicholas Kong, Arti Chhajta, Li Fei-Fei, Maneesh Agrawala, and Jeffrey Heer. 2011. Revision:
Automated classification, analysis and redesign of chart images. In Proceedings of the 24th Annual ACM
Symposium on User Interface Software and Technology. ACM, 393–402.
Rosália G. Schneider and Tinne Tuytelaars. 2014. Sketch classification and classification-driven analysis
using Fisher vectors. ACM Trans. Graph. 33, 6, Article 174.
Richard Szeliski. 2010. Computer Vision: Algorithms and Applications. Springer.
Andrey Vasilev, Ilya Paramonov, Sergey Balandin, Ekaterina Dashkova, and Yevgeni Koucheryavy. 2012.
Context capturing in smart space applications. Netw. Protocols Algorith. 4, 4, 84–100.
Yao Wang, Jeorn Ostermann, and Ya-Qin Zhang. 2001. Video Processing and Communications. Prentice Hall.
Mark Weiser. 1999. The computer for the 21st century. SIGMOBILE Mob. Comput. Commun. Rev. 3, 3 (July
1999), 3–11.
Wikipedia. 2014. Optical character recognition. http://en.wikipedia.org/wiki/Optical character recognition
(Nov. 2014).
J. O. Wobbrock, A. D. Wilson, and Y. Li. 2007. Gestures without libraries, toolkits or training: A $1 recognizer
for user interface prototypes. In Proceedings of the ACM Symposium on User Interface Software and
Technology (UIST’07). ACM, 159–168.
Hee Yong Yoo and Suh Hyun Cheon. 2006. Visualization by information type on mobile device. In Proceedings
of the Asia-Pacific Symposium on Information Visualisation. Vol. 60, Australian Computer Society, Inc.,
143–146.
Hong Zhou, Huamin Qu, Yingcai Wu, and Ming-Yuen Chan. 2006. Volume visualization on mobile devices.
In Proceedings of the 14th Pacific Conference on Computer Graphics and Applications. 76–84.
Received January 2015; revised April 2015; accepted June 2015

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 12, Publication date: October 2015.

Big Data Intelligent Visualization Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

TopoGroups: Context-Preserving Visual Illustration
of Multi-Scale Spatial Aggregates
Jiawei Zhang,1 Abish Malik,1 Benjamin Ahlbrand,1
Niklas Elmqvist,2 Ross Maciejewski,3 and David S. Ebert1
1 Purdue

University
{zhan1486|amalik|bahlbran|ebertd}@purdue.edu

2 University

of Maryland
elm@umd.edu

3 Arizona

State University
rmacieje@asu.edu

Topics
(boundary)
• Drinking
• RNC
• Traffic

Scale (area)
High Level

Low Level

Figure 1. The TopoGroups technique showing multiple levels of spatial aggregation of social media posts around Cleveland, OH, during the 2016
Republican National Convention. TopoGroups supports effective comparison, correlation and analysis of multi-scale aggregates by combining them
into the same display, thereby helping users to understand the spatial distribution as well as identify trends and anomalies at different granularity levels.

ABSTRACT

Author Keywords

Spatial datasets, such as tweets in a geographic area, often exhibit different distribution patterns at multiple levels of scale,
such as live updates about events occurring in very speciﬁc
locations on the map. Navigating in such multi-scale data-rich
spaces is often inefﬁcient, requires users to choose between
overview or detail information, and does not support identifying spatial patterns at varying scales. In this paper, we
propose TopoGroups, a novel context-preserving technique
that aggregates spatial data into hierarchical clusters to improve exploration and navigation at multiple spatial scales.
The technique uses a boundary distortion algorithm to minimize the visual clutter caused by overlapping aggregates. Our
user study explores multiple visual encoding strategies for TopoGroups including color, transparency, shading, and shapes
in order to convey the hierarchical and statistical information
of the geographical aggregates at different scales.

Context preservation; social media; multi-scale analysis;
geospatial visualization.

ACM Classiﬁcation Keywords

H.5.m. Information Interfaces and Presentation (e.g. HCI):
Miscellaneous

INTRODUCTION

In geography, Tobler [35] tells us that “everything is related to
everything else, but near things are more related than distant
things.” Practically speaking, this means that spatial datasets—
such as geotagged tweets in an area, weather station measurements across a region, or Yelp reviews for tourist spots in a
city—often exhibit different patterns at multiple different levels of scale. For example, while tweets in a particular city may
be dominated by the visit of a foreign dignitary (say, the Pope
visiting Philadelphia in September 2015; #popeinphilly),
there may be other, more local patterns in the same geographic
region (such as a Black Lives Matter protest in the same city;
#BLM). Using traditional visualization approaches to navigate
such geospatial datasets often causes global patterns to overshadow local ones, can be tedious and inefﬁcient [24], and
does not provide an overview when focusing on speciﬁc details (and vice versa). What’s more, choosing the proper scale
and interpreting the results become non-trivial tasks [28].
We propose TopoGroups, a multi-scale visual analytics technique based on automatic hierarchical spatial clustering of
data. TopoGroups models multi-scale spatial clusters as a hierarchical tree structure where each node in the tree represents a
speciﬁc cluster, and each edge indicates a parent-child relationship of clusters at adjacent scales. The technique visualizes
multiple levels of the hierarchy at the same time to provide

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from permissions@acm.org.
CHI 2017, May 06-11, 2017, Denver, CO, USA.
Copyright © 2017 ACM ISBN 978-1-4503-4655-9/17/05 ...$15.00.
http://dx.doi.org/10.1145/3025453.3025801

2940

Big Data Intelligent Visualization Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

information about patterns at multiple scales of aggregation
(Figure 1). The boundary of each cluster is modeled using
an implicit curve that is distorted to reduce overlap between
clusters at adjacent scales. The TopoGroups technique also
allows for coupling navigation to the visual representation;
double-clicking on a speciﬁc cluster automatically zooms and
pans the viewport to ﬁt the entire viewport to its extents.
The design space of the TopoGroups technique includes multiple visual encoding choices using color, transparency, shading,
and shape for representing aggregation level, cluster contents,
and statistical aspects of the spatial data. To determine the
strengths and weaknesses of each visual encoding strategy, we
conducted several controlled laboratory experiments where
participants are asked to perform spatial analysis tasks under
different visual encodings. Our results yield guidelines on
which visual encodings to use depending on the user, task, and
application. We also discuss ideas for how TopoGroups can
be extended with text visualization encoding to show terms,
phrases, and topics for each cluster. The practical applications for TopoGroups include geographic information systems
(GIS), geospatial visual analytics, and online geographic services such as Google Maps, Bing Maps, and OpenStreetMap.
BACKGROUND

To facilitate effective exploration of geospatial datasets at
multiple spatial scales, researchers in the visual analysis ﬁeld
have proposed various visual and interaction methods. Below
we discuss work related to multi-scale interactive navigation,
multi-scale visual summary, and context-preserving design.
Multi-Scale Navigation

Interactive maps allow users to explore geospatial datasets at
multiple scales by directly zooming in and out of a region of
interest. Although this technique has been applied in various
visual analytics frameworks [4, 29, 5], there exists several limitations: First, users need to switch between different spatial
scales in order to observe the corresponding results, which
adds heavy interaction overload. Moreover, since the map
typically only visualizes the results at the current scale, users
can easily lose the semantic context of the previous scales as
they interact across multiple scales.
In order to reduce the interaction overload and maintain the
semantic context, researchers have proposed several frameworks that juxtapose multiple maps at different scales, which
allow users to visually compare the multi-scale analysis results
without the need to perform numerous zooming operations.
Ferreira et al. [14] develop an interactive system to visualize
spatiotemporal distributions of birds. Their system provides
multiple coordinated geographical map views to facilitate the
effective visual comparison of different spatial regions across
multiple scales. Javed et al. [23] propose a novel visual design named stack zooming. As users navigate on the map
from higher to lower scales, the corresponding geographical
views stack on each other in order to indicate the hierarchical
relationships across multiple scales. Delort [9] establishes a
hierarchical tree based on the spatial clustering results that
enables users to interactively select different cluster nodes
at different scales and visualizes the clusters using a Voronoi

2941

partition. Zhang et al. [42] propose a two-staged animated transition technique in order to provide a smooth visual transition
as users navigate through multiple spatial scales.
Similarly to the juxtaposition approach, our method aims to
summarize multi-scale visual results in a single, compact visualization to reduce the interaction overload and further facilitate effective correlation and analysis across different scales.
Multi-Scale Visual Summary

Besides the aforementioned interaction-based approaches,
there also exist visual approaches that create summaries of
the analysis results at multiple scales in a single visualization.
This approach effectively maintains the context of exploration
and reduces the overload caused by jumping across different
views for visual comparison. Dykes and Brunsdon [11] use
multiple line charts to encode the relationships between the statistical results and the geographical scales. Turkay et al. [37]
propose attribute signature, which summarizes the multi-scale
statistical results in a static visualization that avoids the tedious
zooming operations and meanwhile maintains the context of
different scales. Goodwin et al. [18] propose a novel glyph design named Scale Mosaic, in which they use a set of concentric
rectangular rings to encode the correlations of the statistical
variables from global to local scales.
Although these solutions manage to facilitate effective multiscale analysis and context preservation, most of them are oriented toward statistical analysis (such as correlation analysis)
of multivariate geographical data, where the analysis result at
one scale can be represented as numeric values. In contrast,
our work aim to provide a visual summary of multi-scale spatial clusters. To the best of our knowledge, little work has been
done in the visual analytics ﬁeld to focus on combining the
multi-scale spatial aggregates in the same visual space.
Context-Preserving Visual Design

Overview+Detail and Focus+Context techniques [6] have
been widely applied in the visualization ﬁeld to provide efﬁcient context preservation. Overview+detail separates the
focus and context into separate views, while focus+context
seamlessly integrates the focus within the context, often by
applying distortion such as ﬁsheye distortion [16, 17]. For
overview+detail, the stack zooming technique [21] has been
applied to both time-series exploration as well as geographical
navigation [23]. In terms of focus+context, Gutwin [19] improves ﬁsheye views by dynamically adjusting the distortion
effect based on the movement of the cursor to allow users
to more effectively target objects. Pietriga and Appert [30]
explore several dimensions including transparency and time
to control the transition between the context of focus. Variants of ﬁsheye techniques have been applied to various usage
scenarios, such as the system diagrams [7, 40], word cloud
layout [8], and road visualization [33]. Other related examples
can be found in the survey paper by Tominski et al. [36].
Similar to focus+context, our approach combines the visualization of different scales in the same display. We design
an overlap minimization method that distorts the contour of
spatial clusters to reduce visual clutter, making it easier for
users to identify aggregates at different scales.

Big Data Intelligent Visualization Systems

a

CHI 2017, May 6–11, 2017, Denver, CO, USA

c

b

Figure 2. Examples of multi-scale clustering: (a) Keywords are aggregated and displayed on the map (Tag Map). Zooming into the map shows lowerlevel sub-events [34]; (b) Spatial clusters are visualized at consecutive zoom levels. Large clusters at a higher zoom level split into multiple small ones at
a lower level [42]; (c) Clustering results of demographic statistics under different geographical resolutions (left: state level, right: county level) [43].

and mentally correlating those results further increases their
cognitive overload in the analysis process.

CHALLENGES AND TASK CHARACTERIZATION

Spatial clustering is an important component within spatial
data mining [2], which generally refers to approaches that
groups spatial data points into classes based on their spatial
proximity. Spatial clustering provides valuable insights into
the spatial data distribution, characteristics of the individual
groups, as well as trends and anomalies within the dataset. Creating these clusters and exploring their characteristics across
multiple spatial scales is an important but challenging task.
Varying scale is an inherent property in multi-scale data analysis and spatial clustering analysis (e.g., [3, 28, 22, 25]). Spatial
datasets can be aggregated by varying granularity levels that
are determined by a distance measure between pairwise data
points in the clustering process. Accordingly, clustering results
often vary signiﬁcantly across different scales, as Figure 2
shows. Although the variation in scale provides a unique
perspective to characterize the spatial data attributes [28],
compared to a single scale, the size of analysis space in the
multi-scale scenario exhibits an approximate quadratic growth.
Moreover, navigating and correlating across scales remains a
non-trivial task to domain experts in various ﬁelds where multiscale analysis is critical in their domain-speciﬁc tasks. The
challenges faced can be characterized into two major aspects,
interaction overload and cognitive overload.

We characterize a set of representative analytical tasks that
are involved in the navigation and exploration of multi-scale
spatial aggregates [1, 27, 31, 32]. The task characterization
guides the design of our context-preserving technique and
provides motivation for research in similar areas.
T1 Establish an overview of the geographical distribution of
aggregates at multiple scales.
T2 Distinguish different aggregates at the same and/or across
different scales in the geographical space and identify their
potential relationships.
T3 Locate aggregates of interest at the same and/or across different scales. Determine the spatial extent of the aggregates.
T4 Measure and compare the volume of domain-speciﬁc attributes for the aggregates at the same and/or across scales.
T5 Access raw data items (e.g., geographical location, domain
attributes) associated with a speciﬁc aggregate on demand.

Spatial scales typically range from an overview (the global
view) to low level details (individual data points). From the
global perspective, the entire dataset is aggregated as a single
object, which may provide overall summary information, but
is too coarse-grained to reveal potential spatial patterns. From
the detail perspective, each individual data point is regarded
as one cluster, where no actual aggregation exists for analysis. Therefore, users have to identify the appropriate scales
between these polar extremes that can best characterize the
hidden spatial patterns. Conventional navigation paradigms
such as the zooming operation require the users to switch to
each individual scale in order to understand the analysis result at that scale, adding signiﬁcant interaction overload to
the analysis process. Moreover, in most multi-scale analysis
scenarios, understanding how the spatial attributes and patterns evolve across scales is critical. For example, crime in
certain regions may be unnaturally high; however, this may
be explained if local geospatial patterns (e.g., petty thefts at
the mall) are analyzed. Hence, users require the ability to
effectively correlate analytical results between different scales.
With conventional navigation paradigms, users have to remember the analysis result at different scales during the navigation

2942

TOPOGROUPS: MULTI-SCALE SPATIAL AGGREGATES

In order to tackle the aforementioned challenges, we propose
TopoGroups, a visual analytics approach that enables effective context-preserving navigation and exploration of spatial
clusters at multiple scales. Motivated by past work [18, 37,
39], TopoGroups aims to superimpose clusters of multiple
scales into a single visual display (T1, T2, T3, T4). With such
a design, the user can easily understand the structure of the
space at several levels of scale, reducing cognitive load. Furthermore, this also minimizes the need for navigating across
multiple scales, thus reducing interaction overload as well.
TopoGroups consists of two major steps, following the hierarchical aggregation model proposed by Elmqvist and
Fekete [12]. First, we model the multi-scale spatial clusters
as a hierarchical representation where each scale (zoom level)
maps to a speciﬁc layer in the hierarchical structure. This step
has been described as data space aggregation [12]. Second,
we design our visual interface that allows users to explore the
spatial clusters both hierarchically and spatially, while maintaining the context of navigation at different spatial scales.
This step has been described as simpliﬁed visual representations of the aggregates in visual space [12].
Generating the Spatial Aggregate Hierarchy

Geospatial datasets are typically represented by latitude and
longitude in a geographical coordinate system, and can be
transformed into planar coordinates based on map projection

Big Data Intelligent Visualization Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

non-leaf node, the algorithm repositions the control points of
the boundary that overlap with its children for the sake of both
an aesthetic visual result and performance efﬁciency. The algorithm ensures an optimal amount of distance between adjacent
boundaries to provide a visual budget for the boundary-based
visual encodings and avoid visual clutter.
Figure 3. The hierarchical (left) and the corresponding geospatial (right)
representations of the multi-scale aggregates

methods. In TopoGroups, the geo-spatial data points are projected into 2D screen space coordinates, where the clustering
is performed. TopoGroups utilizes the common algorithm
where each data point only belongs to one cluster at a single
scale (e.g., the DBSCAN [13] algorithm). We also note that
the clustering process maintains a consistent distance measure
(Euclidean distance in screen space) across different spatial
scales (zoom level). Under such conditions, geospatial data
clustering highly depends on the spatial scales (zoom levels)
of the geographical space. As the spatial scale varies from a
higher (abstract) level to a lower (detailed) level, the screen
space distance of any pair of geo-spatial points increases accordingly. Intuitively, the clusters at a higher level split into
smaller ones at a lower level. Hence, the multi-scale nature of
spatial clustering is consistent with a hierarchical representation that can be represented as a dendrogram (Figure 3).
We represent the multi-scale aggregates using a tree structure that naturally depicts the hierarchical relationships of the
clusters at different scales. In this hierarchy, nodes represent
individual spatial clusters, while the edges represent the parentchild relationships of clusters at adjacent spatial scales. The
clusters that are formed at the same spatial scales correspond
to the nodes that have the same depth in the tree.
Context-Preserving Visualization

In order to visualize the multi-scale hierarchy, TopoGroups creates a boundary-based visual representation using an implicit
curve for each aggregate in the hierarchy (Figure 4(a)). The
beneﬁts of this particular representation lie in three aspects,
referring to the six guidelines G1 through G6 by Elmqvist
and Fekete [12]. First, the boundary within the context of a
geographical space naturally depicts the spatial scope of the
aggregate, which is intuitive and interpretable to the users (G2,
G6). Second, while the data points are typically represented as
small circles, the implicit curve is easily distinguishable from
the data items (G4). Third, since the visual space inside the
boundary of the higher level clusters can be utilized to visualize the lower level clusters, the boundary-based representation
produces minimum visual clutter (G3).
We note that the overlapping of the boundaries at different
scales may exist, as shown in Figure 4(b). To this end, we propose a bottom-up distortion algorithm (Figure 5) toward effective overlap minimization of the multi-scale spatial boundaries
(G3, G5). This is inspired by the nested treemap design that
adds padding to adjacent rectangles in order to highlight the
parent nodes in the hierarchy more effectively [10]. Motivated
by the force-directed drawing algorithm [15], our algorithm
traverses the hierarchy from the bottom level and for each

2943

In order to facilitate effective visual perception of the hierarchical and statistical information of the geographical aggregates, TopoGroups provides a set of visual encoding strategies
combining different perceptual dimensions including color,
transparency, shading, and shapes. The strategies have been
applied to the inner area of the aggregates as well as the boundary, which is inspired by Bristle Maps [20, 26] where map
features (roads, subway line, city blocks, etc.) are associated
with visual elements—bristles—in order to visually encode
the multivariate information in the geographical region of interest. In TopoGroups, we aim to convey both univariate and
multivariate attributes of the spatial aggregates through our
visual encoding strategies. In the rest of this section, we illustrate our visual designs based on a practical usage scenario of
multi-scale spatial aggregates generated from location-based
social media data (Twitter) in Cleveland, OH during the 2016
Republican National Convention.
Univariate Attributes

Aggregates’ univariate attributes typically include the volume
of data points, size of the geographical area, scale of aggregation (zoom level), etc. This type of attributes can be encoded
using either the color of the inner area of the cluster, or the
width/color of the boundary. For example, the analyst is interested in investigating the scale of aggregation at which
different clusters have formed in Cleveland during the RNC
event, and learning the relationships of different clusters across
scales. To this end, TopoGroups encodes the scale of the individual clusters by rendering the inner area using speciﬁc
color schemes. Figure 6 illustrates the encoding strategy based
on a blue-red color scheme where dark blue represents the
abstract level while dark red represents the detailed level (T2,
T3). Clusters of the same color indicate that they are generated
at the same level. Different color schemes such as sequential
or qualitative schemes can also be applied here. In order to
evaluate whether color encoding can enhance the understanding of hierarchical relationships of multi-scale aggregates, and
which color scheme achieves the best result, we conducted a
user study (Details are discussed in the evaluation section).
In addition to ﬁlling the color, TopoGroups applies the halo
effect on the boundary of the clusters (Figure 6(b)) in order
to visually indicate the sidedness of the boundary [38]. The
halo is only rendered at one side of the boundary (outer side of
the cluster) in order to provide a visual cue in terms of which
side of the boundary belongs to the cluster. The halo effect
is especially helpful when the user zooms into a certain level
where only the partial cluster is visualized in the viewport.
Multivariate Attributes

Aggregates’ multivariate attributes are typically generated
from classiﬁcation or categorization of the data items, such as
different topics extracted from social media message content.
Assuming the analyst is curious about the major topics during

Big Data Intelligent Visualization Systems

a

CHI 2017, May 6–11, 2017, Denver, CO, USA

c

b

d

Figure 4. Spatial aggregates are generated (a) at multiple scales and coupled into one visual display using a boundary-based representation (b). The
boundaries are properly distorted and smoothed to avoid visual clutter (c). Visual encodings and interactions are designed to facilitate effective exploration of multi-scale aggregates (d).

a

b

a

c

Figure 5. Boundary overlap minimization: (a) The boundaries of the
parent and child overlap (highlighted in the dashed box); (b) The parent’s boundary is inﬂated in order to avoid the overlap; (c) Part of the
inﬂation result is used to form the new boundary of the parent.

the RNC event, and wishes to further examine the prominent
topics in different clusters at different scales, TopoGroups automatically extracts major topics (e.g., RNC-related, trafﬁc
and accident, drinking and entertainment) and provides several
boundary-based encoding strategies to convey the quantitative
information of different topics for the individual aggregates
(T4) as Figure 7 shows. In all three designs, each color corresponds to a speciﬁc category:
• Continuous colored segments: Figure 7(a) shows line segments being used as the major visual element to convey the
quantity of categories. The length of the colored segment is
in proportion to the quantity of the corresponding category.
Segments repeat to ﬁll the entire boundary.
• Discrete colored dashes: Figure 7(b) shows a sequence of
dashes being used to convey the quantity of categories. The
number of colored dashes in each sequence is in proportion
to the quantity of the corresponding category. Sequences
repeat to ﬁll the entire boundary. We choose the dash as
the visual element in this design instead of circle or other
similar shapes for the sake of visual discrimination between
aggregates and data items (G4).
• Stacked lines: Figure 7(c) shows the entire boundary line
of the cluster being used to convey the quantity of categories. The width of the boundary lines is in proportion to
the quantity of the corresponding category. The lines for
different categories are stacked next to each other.
We have conducted a user study to assess the efﬁcacy of these
techniques in conveying the categorical distribution, the details

2944

b

Figure 6. The area inside the multi-scale clusters is ﬁlled based on a
blue-red color scheme from blue (abstract level) to red (detailed level)
to indicate the aggregation level. The comparison of the visualization
without halo (a) and with halo (b) is shown. With halo, it is easier for the
user to determine which side of the boundary belongs to this cluster.

of which are discussed in the evaluation section. We also note
that for both the continuous segments and discrete dashes, we
ﬁll the entire boundary of the clusters by concatenating the
segments or dash sequences repetitively along the boundary.
The rationale behind such a design is that the repetitive patterns avoid misleading the users in terms of interpreting the
categorical information (Figure 8(b)). Without the repetitive
patterns, the visualization can convey the wrong categorical
information particularly when only a partial cluster is shown
in the viewport (Figure 8(a)).
As Figure 1(left) shows, with the effective visual encodings
provided in TopoGroups, the analyst clearly notices that while
most of the tweets are related to RNC at an abstract scale (the
major color in the outward cluster is red), as she investigates
lower levels, the clusters within Cleveland are more related
to RNC, while in the nearby cities more tweets relate to trafﬁc (green) and drinking (blue). As she further zooms into
the city of Cleveland, she identiﬁes that drinking and trafﬁc
related tweets form several clusters around the suburban regions (Figure 1(right)). Therefore, TopoGroups provides a
comprehensive picture in terms of how the different topics are
correlated with the clusters at different spatial scales, and how
they evolve from the overview level to the detailed level.
Interaction and Interface Design

TopoGroups consists of two visual and interactive dialogs: an
interactive map view that visualizes the multi-scale aggregates
within the same geographical display, and a tree view that

Big Data Intelligent Visualization Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

a

a

b

b

c

Figure 7. Design alternatives for encoding categorical information on
the boundary: (a) Continuous colored segments; (b) Discrete colored
dashes; (c) Stacked Lines.

illustrates the hierarchical relationships of the multi-scale aggregates. These two dialogs are coordinated and seamlessly
integrated when the users navigate across different scales.
General Navigation: The interactive map view allows the
users to navigate across different spatial scales through common zooming operations (T2, T3). Each time the user zooms
in or out, the map navigates to either the higher or lower adjacent level, respectively. TopoGroups visualizes the multi-scale
spatial aggregates that are visible or partially visible in the map
viewport. Aggregates that occupy too small screen space (i.e.,
less than 100 pixels) are not rendered (G1). TopoGroups also
provides a conﬁgurable parameter S that restricts the number
of adjacent scales to visualize (G1) in order to avoid computational performance issue and potential overload on the
user. For example, if the user navigates to zoom level 10, with
S = 2, then only the levels from 8 to 12 are visualized. After
a visual inspection of the results, we found S = 2 to produce
reasonable results with respect to performance and readability.
Muti-Scale Navigation through Selecting Targets: TopoGroups supports simple yet intuitive interactions that allow
users to navigate across multiple levels. As the user doubleclicks on a target aggregate, the map automatically pans to
the target aggregate and zooms in to ﬁt its extent (T3). With
this design, the conventional navigation paradigm that requires
multiple panning and zooming operations is simpliﬁed by a
single and intuitive interaction that signiﬁcantly alleviates the
interaction overload. Furthermore, by double-clicking on the
region outside the target aggregate, TopoGroups automatically
resets the view back to the previous geographical space.
Exploring the Hierarchy — The Tree View: The tree view
in TopoGroups illustrates the multi-scale hierarchy using both
a dendrogram and a node-link diagram [41] (T1, T2). The
dendrogram illustrates the scale of aggregation, as the nodes
of the same scale are aligned based on the same vertical offset
(Figure 4(d)). However, this may cause signiﬁcant visual
clutter when the number of nodes is large. Hence, TopoGroups
provides a complementary node-link representation that fully
utilizes the two dimensional space. The node-link diagram
simply regards the hierarchical structure as a graph rendered
using a force-directed layout. The user can toggle between the
two views in the control panel.
The tree view is coordinated with the map view through the
brushing and linking paradigm. As the user navigates on the
map, the aggregates visible in the geographical space are highlighted in the tree view. With this design, while the user may
focus on exploring the detailed levels on the map, the tree view
provides a context of the entire structure to the user. When

2945

Figure 8. Without the repeating patterns (a), the visualization may convey the wrong categorical information when only the partial cluster is in
the viewport. This visual confusion can be avoided by introducing the
repeating patterns (b).

the user selects one or more nodes in the tree view, the corresponding aggregates on the map view highlight accordingly.
The tree view supports ﬁltering based on the domain-speciﬁc
attributes of the aggregates such as geographical size, data
volume and density. The user can also ﬁlter to show only a
subtree by specifying a node as the root of that subtree. The
view supports sorting the children (from left to right) of each
tree node based on the aforementioned attributes.
Details-on-Demand: TopoGroups supports easy access to
details-on-demand of the raw data items (T5). When the user
right-clicks on the speciﬁc aggregate and selects the relevant
option, the data items that belong to this aggregate are shown
on the map as circles. Simultaneously, a separate message
table shows the semantic content of those data items in a list
and highlights the keywords relevant to different categories
based on the same color scheme.
Implementation Details

TopoGroups consists of a multi-layered SVG canvas. The
map layer stays at the bottom of the hierarchy and provides
an interactive map visualization. On top of the map layer is
the visualization layer, which is the primary workspace for
rendering various visual elements including boundaries, halos,
categorical encodings, etc. The toolbox layer stays on top of
the hierarchy, showing interactive menus and the toolbar.
TopoGroups applies cardinal spline interpolation to smooth
the boundary of the spatial aggregates. In order to ﬁll
color inside the boundary, the SVG <mask> command is
used to create masks according to the boundary of the inner children aggregates, thus avoid rendering those areas.
TopoGroups achieves the shadow (halo) effect by initiating
an SVG ﬁlter (<filter>) and associates a Gaussian blur
(<feGaussianBlur>) to the ﬁlter. The size of the shadow
is controlled by the standard deviation (SD) of the Gaussian
blur (<stdDeviation>). A higher SD value results in a
larger shadow in screen space. The SD value in TopoGroups
is set as 5, which achieves a satisfactory visual effect.
TopoGroups utilizes SVG dash styling to render colored line
segments (each line segment is regarded as a long dash)
and dash sequences along the boundary. Speciﬁcally, the
<stroke-dasharray> attribute deﬁnes the patterns and
gaps of the dash styling, and the <stroke-dashoffset>
attribute controls the offset where the pattern begins. In order
to visualize multiple categories, TopoGroups pre-calculates
the dash patterns and offset for each category based on the
categorical distribution, and then renders them iteratively.

Big Data Intelligent Visualization Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

EVALUATION

TopoGroups visualizes the hierarchical spatial aggregates and
provides visual encoding strategies to indicate domain-speciﬁc
attributes associated with individual aggregates. Our evaluation focuses on design alternatives for TopoGroups that convey
hierarchical and categorical information, as we believe these
low-level visual encodings are critical factors for multi-scale
perception and context preservation. In order to focus on
the task primitives, we break down the experiment into two
aspects (Study 1: structural/hierarchical; Study 2: semantic/categorical) rather than introduce more complexity in the
tasks, which may obfuscate the results.
Participants and Apparatus

We recruited 20 participants (age range of 19 to 28, 7 female,
13 male) for Study 1, and 20 participants (age range of 22 to
36, 6 female, 14 male) for Study 2. Most participants were
students and staffs from our college of engineering, who have
some basic understanding of the concepts being tested (e.g.,
spatial clustering, hierarchical structures). The participants
were paid $5 for participation in one study. The experiments
were conducted on a windows-based computer with a 30-inch
Dell monitor. The interface for the main visualization occupied
an area of 1600x1600 pixels.
Procedure

The two studies were conducted independently and had similar
procedures. At the start of the study, the investigator asked
the participants to sign a consent form and introduced the
research background and the different visualization designs.
Then the investigator provided a training session and presented
sample questions covering major visual designs and task types
to familiarize the participants with the tasks. In order to ensure
that they did not have any difﬁculty or misunderstanding, the
participants were provided with the correct answer and were
asked to raise any questions or concerns to the investigator
during the training. The accuracy and the completion time for
each trial were recorded. After each study, the participants
were asked to complete an online demographic survey.
User Study 1: Encoding the Hierarchical Information

This experiment evaluated the efﬁcacy of color and different
color schemes in terms of conveying the hierarchical structure
of the spatial aggregates within the geographical space.
Techniques and Task Design

In this experiment, we utilized four different visual encoding
strategies (visualization technique V) in the experiment:
NoC Only the boundaries of the clusters are visualized. No color
is rendered inside the cluster.
SEQ A sequential color scheme is used to indicate the scale of
aggregates. In TopoGroups blue is used as the main hue.
Lighter colors represent higher scales (abstract level), and
darker colors represent lower scales (detailed level).
B-R A blue-red color scheme is used to indicate the scale of aggregates, which starts from blue (higher scales), transitions
to yellow (middle scales), and ends at red (lower scales).
QT A qualitative color scheme is used to indicate the scale.

2946

a

b

Figure 9. Task design in Study 1: (a) Comparing scales of aggregation
(TSC); (b) Identifying parent-children relationships (TPC).

We developed two classes of typical tasks. The ﬁrst class
investigated the participants’ performance in terms of visual
comparison between scales of individual aggregates (TSC). A
typical task of this class highlights two aggregates denoted as
A and B, and the participants are asked to decide which one
is at a higher (or lower) level (Figure 9(a)). The second class
of tasks evaluated the participants’ understanding in terms of
parent-child relationships among aggregates at different scales
(TPC). A typical task of this class speciﬁed a cluster denoted
as T , and highlighted a set of clusters denoted as X, Y , Z. The
participants were asked to decide which cluster among X, Y
and Z contains T in the visualization (Figure 9(b)).
We controlled the difﬁculty level D of each trial based on the
complexity of the cluster hierarchy. The hierarchy complexity
is deﬁned based on two parameters: the height (or depth) of
the hierarchy (L), and the average number of children for each
non-leaf node (C). Moreover, we deﬁne three difﬁculty levels: easy (L ∈ {3, 4};C = 2), middle (L ∈ {6, 7};C = 4), hard
(L ∈ {9, 10};C = 6). Each trial consists of a multiple-choice
question along with the visualization. The four techniques
were presented in a counter-balanced order. The whole study
consisted of 4 (technique) × 2 (task type) × 3 (complexity of
hierarchy) × 2 (repetition) = 48 trials.
Results and Observations

The accuracy was quite high (average of 96.04%) across all
visualization techniques for both tasks. Since there was no
time limit for the tasks, the users were able to correctly identify
the hierarchy of the spatial aggregates shown.
The completion time for the type 1 task (TSC) is shown in Figure 10(a). The results have been analyzed based on a repeatedmeasure analysis of variance (assumptions met). Visualization
technique V had a signiﬁcant main effect on completion time
(F(3, 57) = 27.12, p < .0001). Pairwise comparison between
visualization techniques using a Tukey HSD showed that all
pairs have statistical signiﬁcance (p < .05), except for the
pair of no color (NoC) and sequential scheme (SEQ). As expected, the difﬁculty level D had a signiﬁcant main effect on
completion time as well (F(2, 38) = 48.54, p < .0001). Furthermore, there was a signiﬁcant interaction effect between
visualization technique V and difﬁculty level D on completion
time (F(6, 114) = 6.60, p < .0001) As shown in Figure 10(a),
the sequential scheme (SEQ) has the highest completion time
(mean: 19.70 seconds), followed by no color (NoC) (mean:
18.14 seconds), then the qualitative scheme (QT) (mean: 14.49

50

Big Data Intelligent Visualization Systems

50
30

40

NoC
U1A
SEQ
U1B
B-R
U1C
U1D
QT

0

0

10

20

Completion Time (sec)

30
20
10

Completion Time (sec)

40

NoC
NC
U1A
SEQ
SE
U1B
BR
B-R
U1C
S
QT
U1D
QT

CHI 2017, May 6–11, 2017, Denver, CO, USA

U1A
NoC

U1B
SEQ

U1C
B-R

U1D
QT

(a) Completion Time (TSC)

NoC
U1A

SEQ
U1B

B-R
U1C

QT
U1D

a

(b) Completion Time (TPC)

Figure 10. Completion time for the two types of tasks in Study 1. As
the result indicates, color encoding the areas helps identify the scale of
aggregation (a), but not the parent-child relationships (b).

seconds), and ﬁnally the blue-red scheme (B-R) (mean: 10.18
seconds). The blue-red color scheme had the lowest completion time, which can be explained by noting that this scheme
consists of different hues diverging from the middle, making
the encoding space a bit larger while retaining a step to step
relationship between the shades at each level. The qualitative
color scheme follows the blue-red scheme in completion time.
This scheme facilitates the user tracing across multiple scales
since equal levels are quickly identiﬁable by their color, and
adjacent levels are also easily distinguishable. The sequential
color scheme and no color scheme had the longest completion
time. This can be explained by noting the sequential color
scheme is based on a single hue, and requires a higher cognitive load for a user to identify the equal levels between very
similar shades of the same color. Similarly, without rendering
color in the aggregates, users have to identify the scales purely
based on the nested boundaries, which adds to the cognitive
overload. Based on the post-experiment survey, users seem
to prefer the blue-red color scheme: one user commented: "I
liked the contours with the blue-red color as it is the easiest to
view and decreases my response time to answer."
The completion time for the type 2 task (TPC) is shown in
Figure 10(b). Based on a repeated-measure analysis of variance, visualization type V had a signiﬁcant main effect on
completion time (F(3, 57) = 2.87, p < .05). However, for pairwise comparisons using a Tukey HSD, only no color (NoC) vs
qualitative scheme (QT) and sequential scheme (SEQ) vs qualitative scheme (QT) were marginally signiﬁcant (p < .05). The
difference between the completion time for each technique
(Figure 10(b)) was relatively small (QT: 11.92 seconds, B-R:
11.31 seconds, SEQ: 10.06 seconds and NoC: 10.04 seconds).
This can be explained by the fact that although color changes
across scale, there is little color diversity among different
sub-groups of aggregates. As users are not able to intuitively
identify these differences with the help of color, color may be
of limited beneﬁt in identifying the parent-child relationship.
Our guidelines for color encoding the multi-scale aggregates
in order to convey hierarchical information are summarized
in two aspects. First, color encoding the areas of multi-scale
aggregates helps to identify the aggregation level. We found
that a blue-red (or similar) color scheme is most effective toward this end. Second, while encoding the areas of multi-scale
aggregates can assist identiﬁcation of the aggregation level,
it does not fully convey the parent-child relationships. Addi-

2947

b

Figure 11. Task design in Study 2: Comparing categories within one
aggregate (a) and across multiple aggregates (b).

tional encoding or interaction designs are required, such as
providing different sub-clusters with individualized color encoding or highlighting sub-clusters when a parent is selected.
User Study 2: Encoding Categories on the Boundary

This section describes the experiment that evaluates the design
alternatives in TopoGroups that encodes categorical information at individual aggregates.
Techniques and Task Design

In this experiment, we evaluated three design choices (visualization technique V) (Figure 7) for encoding categorical
information on the boundary of the spatial aggregates: continuous colored segments (CS), discrete colored dashes (DD)
and stacked lines (SL). Two types of tasks were involved in
the experiment. For the ﬁrst type of task, the participants were
shown a single aggregate on the map, with the boundary being
visualized according to an underlying categorical distribution
(category set denoted as S = {C1,C2,C3...}). The participants
were asked to identify the category that has the highest/lowest
volume within category set S in the visualization (Figure 11(a)).
For the second type of task, the participants were shown two
aggregates denoted as A and B on the map, with the boundaries
being visualized according to two different categorical distribution of the same category set: with one category denoted
as C1 highlighted, the participants were asked to determine in
which cluster (A or B) the category C1 is more prominent (has
a higher proportion among all categories) (Figure 11(b)). For
each type of task, the same visual design was applied to all
the aggregates. The different categories were visualized based
on a qualitative color scheme, appropriately adjusted so that
when concatenating segments of different colors or stacking
lines of different colors, the adjacent colors were easily distinguishable. Although the proposed designs are applied to
multi-scale aggregates, the scale itself has a minimum effect
on the visual perception of categories. Hence, we limit this
study to a single scale to emphasize the impact of comparing
categories within and across different aggregates.
We controlled the difﬁculty level D of each trial based on the
size of the category set (2 and 4). Each trial consisted of a
multiple-choice question along with the visualization. The
three techniques were presented in a counter-balanced order.
The whole study consisted of 3 (technique) × 2 (task type) ×
2 (difﬁculty level) × 3 (repetition) = 36 trials.

Experimental Results in Practice

CS
U2A
DD
U2B
U2C
SL

U2B
DD

U2C
SL

(a) Accuracy

20

30

40

50

60
U2A
CS

10

Completion Time (sec)

0.0

U2A
CS
DD
U2B
SL
U2C

0

0.8
0.6
0.4
0.2

Correctness

CHI 2017, May 6–11, 2017, Denver, CO, USA

70

1.0

Big Data Intelligent Visualization Systems

U2A
CS

U2B
DD

U2C
SL

(b) Completion Time

Figure 12. Accuracy and completion time for Study 2. Among the three
design alternatives, the discrete dash design achieves the highest accuracy (a) and meanwhile requires the longest time (b).

Results and Observations

The results of accuracy (Figure 12(a)) show that the discrete
dashes (DD) had the highest accuracy (average: 85.42%), followed by continuous segments (CS) (average: 76.67%) and
stacked lines (SL) (average: 67.50%). The results have been
analyzed based on the linear regression (glimmix) with the
assumptions satisﬁed. Visualization technique V had a signiﬁcant main effect on completion time (F(2, 38) = 10.76, p
< .0001). Pairwise comparison between visualization techniques using a Tukey HSD showed that all pairs had statistical
signiﬁcance (p < .05). As expected, difﬁculty level D had a
signiﬁcant main effect on completion time (F(1, 19) = 45.11,
p < .0001). The results reﬂect that calculating the number of
dashes is more accurate than visually comparing the length
of different segments, especially when the difference between
the values is small. Furthermore, stacked lines was the least effective as the visualization budget (the entire width of lines) is
too limited to visually reﬂect the variation of different values.
In terms of the completion time, the participants spent a relatively long time on the discrete dashes (DD) (20.46 seconds),
followed by continuous segments (CS) (12.57 seconds) and
stacked lines (SL) (9.00 seconds). Visualization technique V
had a signiﬁcant main effect on completion time (F(2, 38) =
47.08, p < .0001). Pairwise comparison between visualization
techniques using a Tukey HSD showed that all pairs have statistical signiﬁcance (p < .05). As expected, difﬁculty level D
had a signiﬁcant main effect on completion time as well (F(1,
19) = 45.88, p < .0001). The results indicate that although
DD achieves the highest accuracy, it requires a longer time
for the users to count the number of dashes in each category
for comparison. In terms of visual perception, the number of
visual units in this design is the largest, requiring a longer time
for the users to perceive. When the length of the boundary
is large, or the size of the dash is small, this can potentially
result in a larger number of dashes and overload the users.
Our guidelines for encoding categories on the boundary are
summarized in the following two aspects. First, the discrete
dash design is the most effective in terms of the accuracy. This
is useful in analyses where comparison accuracy is critical, and
the quantitative difference between categories are potentially
not obvious. Second, the continuous segment design should
be used for analyses where speed is favored over accuracy, as
the discrete dash design may overload users in extreme cases.

2948

Analyzing criminal, trafﬁc and civil (CTC) incident data collected in the cities of West Lafayette and Lafayette, Indiana
(more than 170000 incidents from 2010 to 2016) illustrates the
use and beneﬁt of the TopoGroups technique. Assume a police
ofﬁcer is interested in several crime types in these regions,
including liquor law violation, robbery and theft, he starts the
analysis by visualizing the multi-scale clusters at an overview
level (Figure 13(left)). The user observes that while a huge
cluster is formed at an abstract scale (county level) that covers
the two cities, as the ofﬁcer investigates lower scales (city
level), the cluster splits into two smaller ones that are located
around the downtown area of the two cities, indicating a high
frequency of incidents. Furthermore, the two clusters are visually separated by the Wabash River, which is consistent with
the fact that the river is a natural boundary between the two
cities. With the continuous segment design applied to show
the distribution of different crimes, the user also notices that
while most of the incidents are related to theft at a higher scale
(the major color in the outward cluster is green), the cluster
within downtown West Lafayette is more related to liquor law
violation (blue), while most incidents related to theft come
from Lafayette and suburban areas in West Lafayette. Since
Purdue University is located in downtown West Lafayette, this
indicates the campus is safer than other regions (little robbery
or theft), although many liquor law violations occur.
The user further zooms in to lower scales (street level) to
examine the different patterns in the two cities. Figure 13(middle) shows the downtown West Lafayette where the campus
is located. Interestingly, the user identiﬁes several clusters
around the campus malls and student activity centers where
the liquor law violation is prominent. The mall on the east side
of the campus also has a considerable number of robbery and
theft incidents. The ofﬁcer then navigates to Lafayette where
the visual result indicates active robbery and theft activities
around major shopping malls and supermarkets. Therefore
with TopoGroups, while the user navigates across different
scales or targets a speciﬁc scale, the visualization effectively
preserves the context at other adjacent scales, thus reducing
interaction and cognitive overload during the analysis process.
DISCUSSION

TopoGroups distorts the boundary of aggregates and couples
multi-scale results in a single display in order to preserve the
context of multiple scales. Our distortion method (Fig 5) has
a minimal effect in terms of lowering the ﬁdelity (G5) and
interpretability (G6) of the boundary representation, since the
proposed distortion approach enlarges the parent boundary
that overlaps with its children. Hence, the data points that
belong to a speciﬁc aggregate are guaranteed to stay within the
boundary of the same aggregate after the distortion is applied.
However, we note that the boundary itself is an approximate
representation for aggregates, since the boundary merely depicts the spatial coverage of the corresponding data items
instead of their accurate spatial distribution, and the distortion
may further exaggerate the boundary and provide misleading
results to the users. Compared to TopoGroups, conventional
techniques (e.g., [14, 23, 42]) typically visualize a single-scale
result in one visualization. Although they need interaction and

Big Data Intelligent Visualization Systems

Crime Type
(boundary)
•
•
•

Liquor Law
Violation
Robbery
Theft

West
Lafayette

CHI 2017, May 6–11, 2017, Denver, CO, USA

Wabash River

Campus

A
B

Mall and
Supermarket

Scale (area)

High Level

Low Level

Lafayette

B: Lafayette

A: Campus

Activity
Center and
Dormitory

Figure 13. Multi-scale spatial aggregates of CTC incident data in West Lafayette and Lafayette, IN. Left: The campus has a high frequency of liquor
violations while other regions show more incidents related to robbery and theft. Middle: Most incidents related to liquor violations occur around the
campus malls and student activity centers. Right: Robbery and theft occur frequently around the shopping malls and supermarkets.

may create cognitive overload, these techniques present the
original (accurate) analysis results that are intuitive for users
to understand. Hence, conventional techniques are preferred
in analysis tasks where geographical accuracy is required.

one another. For example, a semi-transparent sedimentation
layer as a background would allow for users to quickly understand the categorical distribution while still being able to add
other information relevant to the analysis space.

Although TopoGroups provides users with a conﬁgurable parameter S to restrict the number of adjacent scales visible from
the current scale, further evaluations are required to explore
the scalability of our approach. First, as the number of scales
visible on the map increases, the amount of multi-scale information presented in the visual space increases accordingly,
which can potentially burden or distract users especially when
focusing on a speciﬁc level. Second, too many levels visible
on the map may make it difﬁcult for users to perceive the underlying map due to occlusion. Hence, TopoGroups is favored
in analysis tasks that mainly require comparison or correlation
of the analysis results across scales. In contrast, conventional
techniques are more suitable for scenarios where users target several individual (discrete) scales of interest within the
multi-scale analysis space.

As a future extension, we would like to extend TopoGroups to
visualize the semantic knowledge underlying the multi-scale
aggregates. The prominent terms or phrases extracted from
the content associated with the data items can be embedded
within the aggregates, in order to maintain the semantic context
across different scales. A potential issue associated with this
text-based visualization is that some keywords that are of
lower signiﬁcance may have a longer length; thus, occupy
more space and unduly draw the users’ attention. A potential
solution for this might be to dynamically adjust the font weight
(thickness) in order to make the important words stand out.

Since TopoGroups summarizes the categorical information
along the boundary within a geographical context, the users
may associate the categorical information with the geographic
information in the background. Unfortunately, the users may
have the wrong interpretation that the visualization represents
the local statistics near the boundary. We note that this is a
limitation of this current design, and preventing this requires
clear explanation or training to the users before they use the
system. Future work could address this design limitation by
encoding the locality of information into the boundary itself.
For example, categorical data points could be projected to
the nearest point on the cluster boundary, which would reduce potential errors over larger areas, and indicate the spatial
distribution of the categorical information contained within.
Although TopoGroups visualizes multiple scales in the same
display, the user may only focus on a speciﬁc scale (i.e., the
current zoom level) while other scales are used to provide
contextual information. A potential improvement might be to
allocate more visualization budget (screen space) to the level
on which one is focusing. This can be achieved by adding a
weight parameter to the distortion algorithm so that boundaries
of adjacent scales are shifted with larger offsets. This could
provide an opportunity to encode more information within the
chosen scale, perhaps layering different techniques on top of

2949

CONCLUSION AND FUTURE WORK

Our primary contribution in this paper is a novel contextpreserving visualization and navigation technique called TopoGroups for representing discrete spatial data as hierarchically clustered shapes. In terms of visual representation, we
have explored the design space of different visual encodings
for the boundaries and contents of each shape using color,
transparency, and labels. In terms of interaction, we have described appropriate interactions for manipulating TopoGroups,
including smoothly navigating in the cluster hierarchy. Results from a user study yielded guidelines on optimal visual
encodings and interactions for the technique.
Spatially distributed data is ubiquitous, particularly in the
domain of geolocated social media posts, so we see many
potential future research directions in this area. Our future
work includes integrating TopoGroups with text visualization,
exploring the use of advanced text analytics techniques such
as topic modeling, and applying the TopoGroups technique to
full-ﬂedged spatial visualization systems.
ACKNOWLEDGEMENTS

This work was funded by the U.S. Department of Homeland
Security VACCINE Center under Award Number 2009-ST061-CI0003.
REFERENCES

1. Robert Amar, James Eagan, and John Stasko. 2005.
Low-level components of analytic activity in information

Big Data Intelligent Visualization Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

visualization. In IEEE Symposium on Information
Visualization. 111–117.

13. Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei
Xu, and others. 1996. A density-based algorithm for
discovering clusters in large spatial databases with noise..
In KDD, Vol. 96. 226–231.

2. Natalia Andrienko and Gennady Andrienko. 2006.
Exploratory analysis of spatial and temporal data: a
systematic approach. Springer Science & Business
Media.

14. Nivan Ferreira, Lauro Lins, Daniel Fink, Steve Kelling,
Chris Wood, Juliana Freire, and Claudio Silva. 2011.
Birdvis: Visualizing and understanding bird populations.
IEEE Transactions on Visualization and Computer
Graphics 17, 12 (2011), 2374–2383.

3. Caroline Appert and Jean-Daniel Fekete. 2006.
OrthoZoom scroller: 1D multi-scale navigation. In
Proceedings of the SIGCHI conference on Human
Factors in computing systems. ACM, 21–30.
4. Harald Bosch, Dennis Thom, Florian Heimerl, Edwin
Puttmann, Stephan Koch, Robert Kruger, Michael
Worner, and Thomas Ertl. 2013. Scatterblogs2: Real-time
monitoring of microblog messages through user-guided
ﬁltering. IEEE Transactions on Visualization and
Computer Graphics 19, 12 (2013), 2022–2031.

15. Thomas MJ Fruchterman and Edward M Reingold. 1991.
Graph drawing by force-directed placement. Software:
Practice and Experience 21, 11 (1991), 1129–1164.
16. George W. Furnas. 1986. Generalized ﬁsheye views. In
Proceedings of the ACM Conference on Human Factors
in Computing Systems. 16–23.
17. George W. Furnas. 2006. A ﬁsheye follow-up: further
reﬂections on focus + context. In Proceedings of the ACM
Conference on Human Factors in Computing Systems.
999–1008.

5. Isaac Cho, Wewnen Dou, Derek Xiaoyu Wang, Eric
Sauda, and William Ribarsky. 2016. VAiRoma: a visual
analytics system for making sense of places, times, and
events in Roman history. IEEE Transactions on
Visualization and Computer Graphics 22, 1 (2016),
210–219.

18. Sarah Goodwin, Jason Dykes, Aidan Slingsby, and
Cagatay Turkay. 2016. Visualizing multiple variables
across scale and geography. IEEE Transactions on
Visualization and Computer Graphics 22, 1 (2016),
599–608.

6. Andy Cockburn, Amy Karlson, and Benjamin B
Bederson. 2009. A review of overview+detail, zooming,
and focus+context interfaces. Comput. Surveys 41, 1
(2009), 1–31.
7. Aurélie Cohé, Bastien Liutkus, Gilles Bailly, James
Eagan, and Eric Lecolinet. 2016. SchemeLens: A
content-aware vector-based ﬁsheye technique for
navigating large systems diagrams. IEEE Transactions on
Visualization and Computer Graphics 22, 1 (2016),
330–338.

19. Carl Gutwin. 2002. Improving focus targeting in
interactive ﬁsheye views. In Proceedings of the ACM
Conference on Human Factors in Computing Systems.
267–274.
20. Tobias Isenberg. 2013. Visual abstraction and stylisation
of maps. The Cartographic Journal 50, 1 (2013), 8–18.
21. Waqas Javed and Niklas Elmqvist. 2010. Stack zooming
for multi-focus interaction in time-series data
visualization. In Proceedings of the IEEE Paciﬁc
Symposium on Visualization. 33–40.

8. Weiwei Cui, Yingcai Wu, Shixia Liu, Furu Wei,
Michelle X Zhou, and Huamin Qu. 2010. Context
preserving dynamic word cloud visualization. In
Proceedings of the IEEE Paciﬁc Symposium on
Visualization. 121–128.
9. Jean-Yves Delort. 2010. Vizualizing large spatial datasets
in interactive maps. In Proceedings of the International
Conference on Advanced Geographic Information
Systems, Applications, and Services. IEEE, 33–38.

22. Waqas Javed, Sohaib Ghani, and Niklas Elmqvist. 2012a.
GravNav: Using a gravity model for multi-scale
navigation. In Proceedings of the International Working
Conference on Advanced Visual Interfaces. ACM,
217–224.

10. Peter Demian and Renate Fruchter. 2006. Finding and
understanding reusable designs from large hierarchical
repositories. Information Visualization 5, 1 (2006),
28–46.

23. Waqas Javed, Sohaib Ghani, and Niklas Elmqvist. 2012b.
PolyZoom: multiscale and multifocus exploration in 2d
visual spaces. In Proceedings of the ACM Conference on
Human Factors in Computing Systems. 287–296.

11. Jason Dykes and Chris Brunsdon. 2007. Geographically
weighted visualization: interactive graphics for
scale-varying exploratory analysis. IEEE Transactions on
Visualization and Computer Graphics 13, 6 (2007),
1161–1168.

24. Susanne Jul and George W. Furnas. 1998a. Critical zones
in desert fog: aids to multiscale navigation. In
Proceedings of the ACM Symposium on User Interface
Software and Technology. 97–106.

12. Niklas Elmqvist and Jean-Daniel Fekete. 2010.
Hierarchical aggregation for information visualization:
Overview, techniques, and design guidelines. IEEE
Transactions on Visualization and Computer Graphics 16,
3 (2010), 439–454.

2950

25. Susanne Jul and George W. Furnas. 1998b. Critical Zones
in Desert Fog: Aids to Multiscale Navigation. In
Proceedings of the ACM Symposium on User Interface
Software and Technology. 97–106.

Big Data Intelligent Visualization Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

26. SungYe Kim, Ross Maciejewski, Abish Malik, Yun Jang,
David S Ebert, and Tobias Isenberg. 2013. Bristle Maps:
A multivariate abstraction technique for geovisualization.
IEEE Transactions on Visualization and Computer
Graphics 19, 9 (2013), 1438–1454.
27. Etien L Koua, Alan MacEachren, and M-J Kraak. 2006.
Evaluating the usability of visualization methods in an
exploratory geovisualization environment. International
Journal of Geographical Information Science 20, 4
(2006), 425–448.

Transactions on Visualization and Computer Graphics 22,
7 (2016), 1816–1829.
35. Waldo Tobler. 1970. A computer movie simulating urban
growth in the Detroit region. Economic Geography 46, 2
(1970), 234–240.
36. Christian Tominski, Stefan Gladisch, Ulrike Kister,
Raimund Dachselt, and Heidrun Schumann. 2016.
Interactive Lenses for Visualization: An Extended Survey.
In Computer Graphics Forum. Wiley Online Library,
1–28.

28. Nina Siu-Ngan Lam and Dale A Quattrochi. 1992. On the
issues of scale, resolution, and fractal analysis in the
mapping sciences. The Professional Geographer 44, 1
(1992), 88–98.
29. Abish Malik, Ross Maciejewski, Sherry Towers, Sean
McCullough, and David S Ebert. 2014. Proactive
spatiotemporal resource allocation and predictive visual
analytics for community policing and law enforcement.
IEEE Transactions on Visualization and Computer
Graphics 20, 12 (2014), 1863–1872.

37. Cagatay Turkay, Aidan Slingsby, Helwig Hauser, Jo
Wood, and Jason Dykes. 2014. Attribute signatures:
Dynamic visual summaries for analyzing multivariate
geographical data. IEEE Transactions on Visualization
and Computer Graphics 20, 12 (2014), 2033–2042.
38. Jarke J Van Wijk and Huub Van de Wetering. 1999.
Cushion treemaps: Visualization of hierarchical
information. In Proceedings of the IEEE Symposium on
Information Visualization. 73–78.

30. Emmanuel Pietriga and Caroline Appert. 2008. Sigma
lenses: focus-context transitions combining space, time
and translucence. In Proceedings of the ACM Conference
on Human Factors in Computing Systems. 1343–1352.
31. Victoria Rautenbach, Serena Coetzee, and Arzu Çöltekin.
2016. Development and evaluation of a specialized task
taxonomy for spatial planning–A map literacy experiment
with topographic maps. ISPRS Journal of
Photogrammetry and Remote Sensing (2016).
32. Ben Shneiderman. 1996. The eyes have it: A task by data
type taxonomy for information visualizations. In
Proceedings of the IEEE Symposium on Visual
Languages. 336–343.
33. Guodao Sun, Yang Liu, Wenbin Wu, Ronghua Liang, and
Huamin Qu. 2014. Embedding temporal display into
maps for occlusion-free visualization of spatio-temporal
data. In Proceedings of the IEEE Paciﬁc Symposium on
Visualization. 185–192.
34. Dennis Thom, Robert Krüger, and Thomas Ertl. 2016.
Can Twitter Save Lives? A Broad-scale Study on Visual
Social Media Analytics for Public Safety. IEEE

2951

39. Leland Wilkinson. 2006. The Grammar of Graphics.
Springer Science & Business Media.
40. Insoo Woo, SungYe Kim, Ross Maciejewski, David S
Ebert, Timothy D Ropp, and Krystal Thomas. 2009.
SDViz: A context-preserving interactive visualization
system for technical diagrams. In Computer Graphics
Forum, Vol. 28. 943–950.
41. Jing Xia, Wei Chen, Yumeng Hou, Wanqi Hu, Xinxin
Huang, and David S Ebert. 2016. DimScanner: A
Relation-based Visual Exploration Approach Towards
Data Dimension Inspection. In Proceedings of the IEEE
Conference on Visual Analytics Science and Technology.
181–190.
42. Jiawei Zhang, Benjamin Ahlbrand, Abish Malik,
Junghoon Chae, Zhiyu Min, Sungahn Ko, and David S
Ebert. 2016a. A visual Analytics framework for
microblog data analysis at multiple scales of aggregation.
In Computer Graphics Forum, Vol. 35. 441–450.
43. Yifan Zhang, Wei Luo, Elizabeth A. Mack, and Ross
Maciejewski. 2016b. Visualizing the impact of
geographical vriations on multivariate clustering. In
Computer Graphics Forum, Vol. 35. 101–110.

Business Intelligence Analytics

Business Intelligence
from Social Media
A Study from the VAST Box Office Challenge
Yafeng Lu, Feng Wang, and Ross Maciejewski ■ Arizona State University

S

ocial media presents a promising, albeit
challenging, source of data for business intelligence. Customers voluntarily discuss
products and companies, giving a real-time pulse
of brand sentiment and adoption. Unfortunately,
such data is noisy and unstructured, making it
difficult to easily extract real-time intelligence. So,
using such data can be time-consuming and cost
prohibitive for businesses.
One promising direction is to apply visual analytics (VA). Recently, the VA community has begun
focusing on extracting knowledge from unstructured social
This visual-analytics toolkit
media data.1 Studies have ranged
extracts data from Twitter and
from geotemporal anomaly deBitly to predict movie revenue
tection2,3 to topic extraction4
and ratings. Its interactivity
to customer sentiment analyprovides benefits that a purely
sis.5 The development of tools
for such analyses now lets users
statistical approach can’t. The
explore this rich information
approach is generalizable
source and mine it for business
to other domains involving
social media data, such as sales intelligence.
One key area for business
forecasting and advertisement
intelligence is revenue predicanalysis.
tion. In particular, owing to
the abundance of social media
discussions on movies, movie revenue prediction
has drawn much attention from both the movie
industry and academia. Prediction methods have
employed movie metadata, social media data,
and Google search volumes (for some examples,
see the “Related Work” sidebar). Such methods
have demonstrated the benefits of extracting business intelligence from social media for predicting
movie revenue. However, they’ve relied solely on
58	

g5mac.indd 58

September/October 2014	

automated extraction and knowledge prediction.
We’ve developed a VA toolkit for predicting opening-weekend revenue and viewer-rating
scores of upcoming movies. It consists of a Webdeployable series of linked visualization views that
combine data mining with statistical techniques.
To demonstrate our toolkit’s effectiveness, we
report on the results of the 2013 Visual Analytics Science and Technology (VAST) Box Office
Challenge (www.boxofficevast.org/vast-welcome.
html). These results also let us explore the hypothesis that VA can help users develop better
movie revenue predictions, compared to a purely
statistical solution.
Such a VA approach for social media analysis and
forecasting is directly applicable to a wide range
of business intelligence problems. Understanding
how information spreads, as well as the underlying
sentiment of the messages being spread, can give
analysts critical insight into the general pulse
of their brand or product. Developing a set of
quick-look visualization tools for an overview of
such social media data and linking these tools
to models that business analysts generate for
deploying new products, advertising campaigns,
and sales forecasts can be crucial. Our toolkit
can also be used to explore other business-related
social media data—for example, to see how well an
ad campaign did and the pattern of information
spreading. Some exploration can help adjust
business decisions.

Tools for Movie Predictions
Our toolkit lets users quickly extract, visualize,
and clean information from social media sources.

Published by the IEEE Computer Society

0272-1716/14/$31.00 © 2014 IEEE

8/21/14 4:11 PM

To create predictions, it integrates visual analytics
with linear regression, temporal modeling, and
sentiment analysis.

Tweet Mining
For tweet mining, we focused on structured data
from the Internet Movie Database (IMDb) (for
example, the genre, budget, and review rating)
and unstructured data from social media (for
example, movie-related tweets and blog posts).
Whereas extracting structured data is relatively
straightforward, unstructured data requires much
preprocessing and manipulation. We collected
tweets during the two weeks before the release
date, on the basis of the hashtag provided by a
movie’s official Twitter account.
We wanted tools that can extract a variety
of metrics from IMDb and Twitter. Table 1
summarizes the metrics we found most useful.
Several of them require data mining and cleaning.
To facilitate this, we developed tools to
■■

■■

■■

present the volume of tweets at various levels of
temporal aggregation (see Figure 1a),
let users remove unrelated tweets from the aggregate metrics, and
let users extract and manually adjust a tweet’s
sentiment (see Figures 1b through 1d).

To approximate the popular sentiment of a
movie, we process each tweet using SentiWordNet, a dictionary-based classifier.6 First, we assign
each word in the tweet a score from –1 to 1, with
–1 being the most negative sentiment and 1 being the most positive sentiment. Next, we assign
each tweet a sentiment score (TSS) by summing
the sentiment score of all the words in the tweet
and scaling the range from –0.5 to 0.5. Finally, we
calculate the movie sentiment score (MSS):
MSS =

Positive Score
,
Positive Score + Negative Score

Related Work in Predicting
Movie Revenue

A

n early study by Jeffrey Simonoff and Ilana Sparrow predicted
movie revenue with a logged response regression model
using metadata features (for example, the time of year, genre,
and Motion Picture Association of America rating) as categorical regressors.1 Wenbin Zhang and Steven Skiena enhanced
regression models based on metadata features by using variables
extracted from news sources.2 Mahesh Joshi and his colleagues
explored the relationship between film critic reviews and movie
revenue.3 Sitaram Asur and Bernardo Huberman found that the
rate of tweets per day explained nearly 80 percent of the variance
in movie revenue prediction.4 Finally, a recent Google white paper
claimed 94 percent accuracy in movie revenue prediction, using
the volume of Internet trailer searches for a given movie title.5

References
	 1.	 J.S. Simonoff and I.R. Sparrow, “Predicting Movie Grosses: Winners
and Losers, Blockbusters and Sleepers,” Chance, vol. 13, no. 3, 2000,
pp. 15–24.
	 2.	 W. Zhang and S. Skiena, “Improving Movie Gross Prediction
through News Analysis,” Proc. IEEE/WIC/ACM Int’l Joint Conf. Web
Intelligence and Intelligent Agent Technology, 2009, pp. 301–304.
	 3.	 M. Joshi et al., “Movie Reviews and Revenues: An Experiment in
Text Regression,” Human Language Technologies: The 2010 Ann. Conf.
North Am. Chapter of the Assoc. for Computational Linguistics, 2010,
pp. 293–296.
	 4.	 S. Asur and B.A. Huberman, “Predicting the Future with Social Media,”
Proc. IEEE/WIC/ACM Int’l Conf. Web Intelligence and Intelligent Agent
Technology, 2010, pp. 492–499.
	 5.	 R. Panaligan and A. Chen, Quantifying Movie Magic with Google Search,
white paper, Google, 2013.

where Positive Score is the sum of all tweets for a
given movie with TSS > 0 and Negative Score is the
absolute value of the sum of all tweets for a given
movie with TSS < 0.
Our toolkit visualizes the extracted TSSs for the
users. Figures 1b through 1d show the bubble plot,

Table 1. Metrics we found useful.
Metric

Description

OW

The three-day opening-weekend revenue.

Budget

The approximate movie budget (in US$ millions) according to the Internet Movie Database (IMDb).

Genre

The movie’s genre according to IMDb.

TUser

The number of unique users who tweeted about a movie.

TBD

The average daily number of tweets during the two weeks before the movie’s release.

TSS

Tweet sentiment score—a summation of each word’s sentiment polarity as calculated with SentiWordNet.6

MSS

Movie sentiment score—a derivation of a movie’s overall sentiment.

MSP

Movie star power—a summation of the Twitter followers of the three highest-billed movie stars (as listed by IMDb).

	

g5mac.indd 59

IEEE Computer Graphics and Applications

59

8/21/14 4:11 PM

Business Intelligence Analytics

(a)

(b)

(c)

(d)
Figure 1. Tweet trend and sentiment views for the movie Despicable Me 2. (a) Line charts and bar graphs showing how many
tweets per day and the predictions. (b) A tweet bubble plot in which blue represents positive sentiment and red represents
negative sentiment. A bubble’s size represents how many times a tweet has been retweeted; the x-axis is time, and the y-axis
is how many followers the person who submitted the tweet has. (c) A sentiment river view that aggregates sentiment over
four-hour intervals. Positive sentiment is red; negative sentiment is blue. Users can select an area on the river to see the ratio of
positive to negative sentiment. (d) A sentiment wordle in which a word’s size represents how many times it was used in a tweet
and in which its color represents sentiment. Users can click on a word to view the tweets containing it.
60	

g5mac.indd 60

September/October 2014

8/21/14 4:11 PM

Figure 2. Our interactive Bitly classification widget. In the center are the unclassified links, which the user can click and classify, as
seen in the floating window. The upper left is a plot of review scores by click counts, with a line for the average review score.

the sentiment river, and the sentiment wordle.
The sentiment wordle visualizes the 200 most
frequently mentioned words. Both the bubble plot
and wordle enable interactive searching and filtering by keywords and users. Users can remove irrelevant tweets from the tweet count and modify
mismatched sentiment.
The primary use we found for the views in
Figure 1 was data cleaning. The primary lesson
learned was that visualization tools are a necessity
for data cleaning owing to the noisiness of social
media data and the problems inherent in sentiment matching using a sentiment dictionary. (For
example, phrases such as “I want to see this movie
so bad” are marked as negative because of the word
“bad,” and words such as “Despicable” are marked
as negative even though they’re merely references
to a movie title.) The wordle provides a quick way
to assess the sentiment of popular words. However, to fully explore a tweet’s context, users must
hover over the bubble plot or open a tweet list view
through the search bar.
Our implementation of the toolkit (which we
describe later) demonstrated that these views were
more effective for cleaning and overview than for
model analysis. The need for tools to extract the
correct metrics for regression modeling is a major
hurdle for using social media data for business intelligence. The bubble plot and wordle plot helped
us deal with the challenges of sentiment analysis
and cleaning the noise from social media data.

Bitly Mining
Here, we explored long-form text by extracting
Bitly links containing movie keywords. These links
typically consisted of review articles or news reports about the movies (or in many cases unrelated news—for example, when the movie The Heat
was released, the Miami Heat basketball team
had just won the National Basketball Association
championship).
	

g5mac.indd 61

We developed an interactive tool for extracting
prescreening review scores embedded in Bitly links
(see Figure 2). Initially, each Bitly link is unclassified and represented in a pixel matrix (the color
saturation corresponds to how many times a link
was clicked). When users click on an unclassified
square, a pop-up box appears with a brief bit of
text from the article. Users can follow the link to
scan the article for review scores and manually
assign a score to an article or classify it as news
or unrelated. For analysis, the tool provides a plot
of review scores from articles versus how many
times an article was accessed (see the upper-left
graph in Figure 2). The predicted review score is
an average of extracted review scores normalized
into one scale.
This tool allows for quick data filtering and extraction. For example, users can easily separate
reviews of the Star Trek video game from reviews
of the Star Trek movie, which would be difficult
to automatically encode. Furthermore, the pixel
matrix’s color coding can serve as a metric for
classifying only those articles with a substantial
number of views.
Similarly to our experiences with tweet mining,
we learned here that extracting information from
Bitly can be difficult to fully automate. As in the
Star Trek example, multiple products related to a
movie might be released and reviewed at the same
time. Furthermore, review scores might vary, from
“two thumbs up” to “4 out of 5 stars” to “6 out of
10.” With the user in the loop, these scores can be
mapped to the user’s own base system (in the case
of our contest entry, our metric was “x out of 10”).

Regression Modeling
Once we completed data cleaning and variable extraction, we used the social media metrics to develop a model to predict movie revenue and review
scores. Traditional variables used in movie revenue
prediction models include structured variables (for
IEEE Computer Graphics and Applications

61

8/21/14 4:11 PM

Business Intelligence Analytics

(b)
(a)

(c)

(d)

Figure 3. The weekend prediction view for newly released movies and the prediction adjustment widget. This view shows
the weekend when Despicable Me 2 and The Lone Ranger were released. (a) A bar graph showing the actual value, submitted
prediction, and model prediction. (b) A stacked bar graph showing the predicted weekend revenue overlaid with the upcoming
movie’s regression model prediction. (c) The prediction adjustment widget, for modifying the total weekend revenue prediction.
(The predicted values for the new movies remain proportional.) (d) The adjustment widget, for changing individual predictions.
The gray box represents the total weekend revenue.

example, the Motion Picture Association of America [MPAA] rating and movie budget) and derived
measures (for example, movie stars’ popularity
and popular sentiment regarding the movie).
On the basis of our initial literature search,
we used multiple linear regression for an initial
prediction range for the opening-weekend movie
revenue (OW). (For a brief introduction to multiple linear-regression modeling, see the related
sidebar.) We explored a variety of variables that
could be mined from the contest (see Table 1). After initial model fitting and evaluation using R,7
we found our best fit to be
OW = b 0 + b1TBD + b2Budget + e,
where b is a coefficient parameter and e is the
error term.
We updated the model weekly as new movies
entered the dataset. We fit the parameters using
movie data beginning in January 2013. Our first
prediction, for the 17 May weekend, used data
from 39 movies for training. Our weekly models
reported an adjusted R 2 of approximately 0.60,
with p < 0.5. Our final parameters were b0 ≈ 4.9 ×
103, b1 ≈ 4,462, and b2 ≈ 2.3 × 105.
Unfortunately, this model doesn’t fit the data
overly well, and predictions have a large variance. For
comparison, a linear-regression model using Google
search volumes explained more than 90 percent of
the variance on movie revenue performance.8 Also,
models by Sitaram Asur and Bernardo Huberman
produced an adjusted R 2 of over 90 percent with
the number of theaters as a regressor.9
However, we hypothesized that a VA toolkit could
partly help users overcome poor data (due partly to
noise in social media data and partly to the closedworld nature of the contest). To facilitate better
model prediction, we created a simple bar graph
view (see Figure 3a). For past movies, it shows
62	

g5mac.indd 62

the model prediction, its 95 percent confidence
interval error range, the submitted prediction,
and the actual movie revenue. For new movies, it
shows only the model prediction and submitted
prediction. This view was critical in our analysis.
The primary view of the data consists of an
overview of the tweets per day and the predictions
for the selected movies (see Figure 1a).

Temporal Modeling
The regression model provides one point for
analysis; we wanted to also provide a big-picture
overview. For any given weekend, there’s likely
a maximum amount of money available in the
market. To approximate the total available money,
we employed a simple moving-average model.
Limitations here included access to data (historical
weekend revenues weren’t available, and after a
movie opened, further weekend revenues were no
longer reported in the contest). To compensate
for this, we approximated subsequent weekend
revenues for movies, assuming that movies would
run for three weeks following their opening
weekend and that each weekend their revenue
would decrease by 50 percent.
So, for any given weekend, we approximated the
revenue as
Weekend Revenue (t ) =

j=3

∑ OW (t) + ∑ 0.5 OW (t − j),
i

j

i

∀ i , j=1

∀i

where t is the current weekend and i is the index
to a movie that exists at t. Then, for the weekend
revenue prediction, we used a moving average:
Weekend Revenue (t + 1) =

1
3

j=2

∑ Weekend Revenue (t − j) .
j=0

Finally, we approximated the available revenue for
new movies as

September/October 2014

8/21/14 4:11 PM

Linear-Regression Model Construction and Evaluation
egression analysis is one of the most common methods
of pattern detection and multifactor analysis.1 With
a proper regression model, analysts can better describe,
interpret, and predict data.

R

The solution takes the form b̂ = (XT X ) XT Y , and the
prediction function is Y = HY, where H = X(XTX) –1XT. In oneorder multiple linear regression, the predicted response is
a linear combination of observations.

The Linear-Regression Model

Model Selection

A k-variable linear-regression model has this basic form:

In a multiple-variable dataset with a single response variable, analysts traditionally face a large set of potential
linear-regression models consisting of various regressors
and orders. For example, in movie revenue prediction,
the response could be related to the number of tweets
per day, the number of theaters the movie is released in,
or any combination of variables.
To decide which model to use in prediction, analysts
typically consider four principles:

y = b0 + b1x1 + b2x2 + … + bkxk + e,
where y is the response; b is an unknown parameter; xi, i =
1, 2, …, k, are the regressors; and e is the error term. The
goal is to define a relationship between the response and
regressors by solving for the linear coefficients that best
map the regressors to the response. The linear-regression
model is most often written as a matrix, such that

■■ Don’t

Y = X β + ε,



Y = 




y 1 

y 2 
 ,
 

y n 




X = 






β = 




1 x11  x1k
1 x 21  x 2k
 

1 xn1  xnk




 ,







 .

 

βk 

β0
β1

For multiple regression models, you can use higherorder terms to model the response (for example, secondorder variables are of the form xi2 and xixj). However, for
the research described in the main article, we focused on
the simple linear-regression model.

Parameter Estimation
To solve for bi, the ordinary least squares (OLS) solution is
most often employed. This assumes normality for the data.
However, if this assumption isn’t valid, a maximum-likelihood
estimation would be employed (which is equivalent to
OLS under the assumption of normality).
For OLS, we wish to minimize

S (β ) =

n

∑ε
i =1

2
i

= εT ε = (y − X β T )(y − X β ) ,

where S indicates the least-squares function and ∂ indicates a partial derivative, by satisfying

∂S
= −2XT y + 2XT X bˆ = 0 .
∂b bˆ

	

g5mac.indd 63

−1

violate the scientific principle, if one exists, behind the dataset.
■■ Maintain a sense of parsimony to keep the order of
the model and the number of regressors as low as
possible.
■■ Keep an eye on extrapolation. Regression fits data
in a given regressor space; there’s no guarantee that
the same model applies to other data outside this
space.
■■ Always check evaluation plots more than the statistics.
Residual plots and normal plots help show outliers and
lack of fit.
To verify a model’s efficacy, analysts typically rely
on a variety of statistical graphics to determine the
critical variables in the model—those that explain the
most variation with the simplest form. 2 Evaluation of a
model’s effective fit usually involves three statistics. The
p-value shows a regression model’s significance, where
p < 0.05 indicates the model is significant with a 95
percent confidence interval. R 2 and the adjusted R 2 generally describe the percentage of variance explained by
a given model. The adjusted R 2 takes into consideration
the degrees of freedom and should be used in multiple
regression to compensate for the increased variance
when adding regressors. A model is typically selected
when its p-value is small, its R 2 or adjusted R 2 is high,
and it has a relatively simple form with reasonable residual distributions.

References
	 1.	 D.C. Montgomery, E.A. Peck, and G.G. Vining, Introduction
to Linear Regression Analysis, John Wiley & Sons, 2012.
	 2.	 T. Muhlbacher and H. Piringer, “A Partition-Based Framework
for Building and Validating Regression Models,” IEEE Trans.
Visualization and Computer Graphics, vol. 19, no. 12, 2013,
pp. 1962–1971.
IEEE Computer Graphics and Applications

63

8/21/14 4:11 PM

Business Intelligence Analytics

New Movie Revenue (t + 1) =
Weekend Revenue (t + 1) −

j=3

∑ 0.5 OW (t + 1 − j) .
j

i

∀ i , j=1



(1)
Although this prediction is crude, it gives users a
valuable bound in which to explore the revenue
predictions.
Our toolkit provides two views of the results
from the weekend revenue prediction and the
linear-regression model. The first view combines
a linked bar graph with stacked bars (see Figure
3b). The graph’s primary portion consists of gray
bars indicating the predicted total weekend revenue for the new movies. The short dark-gray line
indicates the actual weekend revenue for each calendar week shown on the x-axis. The stacked bar
graph appears only for the analyzed weekend; the
colors are the same as in the prediction bar graph.
The second view (see Figures 3c and 3d) lets
users interactively adjust predictions while visualizing the bounds of the total weekend revenue
prediction. A gray rectangle’s area is scaled linearly
to the total weekend revenue prediction. Colored
rectangles are superimposed onto the gray rectangle; each colored rectangle’s area represents
the linear-regression prediction for each movie
released on that weekend. If the sum of the individual predictions is equal to the total prediction,
the colored rectangles will fit exactly into the gray
rectangle. The colors are the same as in the bar
graph; modifying a bar’s size in any view modifies
the size across all views.
Users can perform three types of prediction adjustments:
■■

■■

■■

They can change the total weekend revenue prediction, but the ratio between the movies will
remain consistent.
They can change an individual movie revenue
prediction, but the total weekend revenue prediction will remain consistent.
They can arbitrarily change each movie’s revenue
prediction and ignore the total weekend revenue.

By implementing and integrating multiple comparison methods, we could quickly bound our analysis. Although flexible, these bounds provided an
early estimate of the total weekend revenue with
which to compare the predictions of our linearregression models.
Although our temporal predictions were of low
quality, the combination of predictions and bounding of the problem space provided critical informa64	

g5mac.indd 64

tion for comparison and analysis. Overall, adding
multiple models predicting similar information
can help guide users to a better ground truth. Like
the Delphi method, which solicits predictions from
multiple experts and uses them to come to a common conclusion,10 our toolkit lets users solicit predictions from multiple models to aid their analysis.
Users can employ this bounded adjustment widget
for other hierarchical predictions that have both
individual and total predictions, such as subtopic
trend prediction in a time period.

Similarity Visualization
The similarity widget lets users quickly find and
compare predictions’ accuracy on the basis of
various similarity criteria. They can determine
whether the given prediction model typically underestimates, overestimates, or is relatively accurate regarding movies they deem similar. So, they
can further refine their final prediction for both
revenue and review scores.
We’ve defined eight similarity criteria; Table 2
shows them and their distance measurements. In
all similarity matches, our toolkit shows the top
five most similar movies. These views let users directly compare tweet trends and sentiment words
between movies deemed similar in a category.
Figure 4 contains snapshots from the Despicable
Me 2 similarity page, showing line charts using
the MPAA criterion, a wordle using the sentiment
wordle criterion, and a theme river using the sentiment river criterion.
Although all the variables used in our similarity
metrics could also be used in the linear-regression
model, the modeling results indicated that these
variables weren’t significant in altering the model.
However, by providing users with insight into these
secondary variables, coupled with the weekend
modeling, our toolkit lets them further refine predictions. For example, users might compare the absolute difference between tweets of two movies or
inspect the trend of the tweets through line chart
comparison using the tweet-changing-trend criterion. Users can also quickly compare the selected
movies to recently released movies with the same
MPAA rating or genre. In addition, they can compare the popularity of the movies’ stars, which is
based on how many Twitter followers the stars have.

Implementing the Toolkit
In the VAST 2013 Box Office Challenge, we used
our toolkit to predict 23 movies over three months.
Here, we give an example based on the July 4th
holiday in the US, when Despicable Me 2 and The
Lone Ranger were released.

September/October 2014

8/21/14 4:11 PM

Table 2. Calculations of similarity criteria.*
Similarity criteria

Distance measurement

Number of tweets
by day

Dis (v , s ) =

∑

Dis (v , s ) =

∑

14

Dis (v , s ) =

∑

14

Tweet changing
trend

Sentiment river

14
i =1

i =1

i =1

TBDi (v ) − TBDi (s )
TBDi (v )

Max (TBD j (v ) , j = 1, 2, … , 14)
MSSi (v )

Max (MSS j (v ) , j = 1, 2, … , 14)

−

−

TBDi (s )

Max (TBD j (s ) , j = 1, 2, … , 14)
MSSi (s )

Max (MSS j (s ) , j = 1, 2, … , 14)

MSS

Dis (v , s ) = MSS (v ) − MSS (s )

MPAA

The same Motion Picture Association of America rating and close release dates

Genre

Dis (v , s ) = 1−
MSP

card (Genre (v ) ∩ Genre (s )) × 2

card (Genre (v )) + card (Genre (s ))

Dis (v , s ) = MSP (v ) − MSP (s )

Sentiment wordle

Dis (v , s ) = 1−

card (SWordle (v ) ∩ SWordle (s ))
card (SWordle (v ))

*v and s are the two movies being compared; card is the cardinality.

Figure 4. User-defined similarity views cropped to show the most similar movies. On the top in the middle
are graphs using the MPAA criterion. On the top right are graphs of the actual opening-weekend revenue,
our final prediction, and the prediction range. The circled star shows the review score. On the bottom left is
a wordle using the sentiment wordle criterion; on the bottom right is a theme river using the sentiment river
criterion. (For an explanation of these criteria, see Table 2.)

Predicting Review Scores
To predict IMDb review scores, we first entered the
Bitly view for each movie. We manually extracted
review scores from Bitly users who had attended
a prescreening of the movie (see Figure 2). For
	

g5mac.indd 65

Despicable Me 2, the analysts manually classified
the most-clicked Bitly reviews; the average value
of the extracted review scores was 7.8.
Once we recorded the selected movie’s average
value, we used the similarity view to compare it to
IEEE Computer Graphics and Applications

65

8/21/14 4:11 PM

Business Intelligence Analytics

Table 3. Competitors’ performance in the 2013 VAST Box Office Challenge. The average error is in millions of dollars.
Revenue predictions
No. of
predictions

Team

Average
error

Viewer-rating predictions

Standard
deviation

MRAE*

No. of
predictions

Average
error

Standard
deviation

MRAE*

Our team (VADER)

23

11.213

9.416

0.467

23

0.487

0.460

0.075

Team Prolix

23

16.466

15.195

0.424

20

0.820

0.640

0.129

Uni Konstanz Boxoffice

14

17.056

15.743

3.929

21

0.905

1.519

0.095

CinemAviz

21

17.219

17.677

1.970

21

0.738

0.559

0.114

Team Turboknopf

8

21.900

15.606

0.685

18

0.514

0.426

0.079

elvertoncf—UFMG

3

12.677

9.806

3.009

3

1.323

0.328

0.259

Philipp Omentisch

5

30.657

38.028

0.678

5

0.500

0.324

0.071

CDE IIIT

2

60.600

62.084

0.537

2

0

0

0

*Mean relative absolute error.

other movies. The movie review score appeared as
a star highlighting the review value in the corner
of the bar graphs (see Figure 4). Typically, we compared across genre, movie rating, and sentiment to
determine whether we felt the average value extracted from Bitly links was a reasonable prediction.
We compared Despicable Me 2 to Monsters University because both were animated sequels. Monsters
University’s IMDb rating was 7.8, giving us confidence that our predicted value of 7.8 for Despicable
Me 2 was reasonable. We then performed this process for the Lone Ranger, which received a predicted
rating of 6.4. The actual IMDb ratings were 7.9 for
Despicable Me 2 and 6.8 for The Lone Ranger.

Predicting Revenue
Predicting revenue for the July 4th weekend was
challenging for two reasons. First, the data stream
from the contest was broken, providing only six
days’ worth of tweets. Second, the predictions were
for a five-day weekend instead of the typical threeday weekend. Using the available data, we obtained
rough estimates of US$76M (±$13M) for Despicable
Me 2 and $85M (±$13M) for The Lone Ranger.
For the three-day weekend, the New Movie Revenue (see Equation 1) estimated that $124M was
available for the two movies. A quick look at Figure 3 shows that our regression predictions were
well outside the bounds of the time series model
prediction.
Given the misalignment between the two models, we explored the similarity views to determine
the movies most similar to Despicable Me 2 and The
Lone Ranger, on the basis of the predicted review
scores and various other metrics. We compared
Despicable Me 2 to a variety of animated movies;
the predicted $73M was actually low compared
to animated movies such as Monsters University.
Next, we explored various similarity views for The
Lone Ranger. It was likely similar to World War Z,
which had a weekend revenue of $66M. However,
66	

g5mac.indd 66

World War Z’s viewer rating was 7.4, much higher
than the predicted 6.4 for The Lone Ranger.
We determined that Despicable Me 2 should perform similarly to Monsters University, and we predicted a three-day revenue of $85M. On the basis
of our temporal prediction, this left only $39M for
The Lone Ranger. However, given the other evidence,
The Lone Ranger seemed likely to underperform.
Finally, we took our three-day prediction values
and linearly scaled them, resulting in a five-day
prediction of $116.5M for Despicable Me 2 and
$55.45M for The Lone Ranger. The actual three-day
revenue was $83.5M for Despicable Me 2 and $29M
for The Lone Ranger. The actual five-day revenue
was $143M for Despicable Me 2 and $48.7M for
The Lone Ranger.

VAST Challenge Results
Eight teams from various research institutes participated in the 2013 VAST Box Office Challenge.
Our team was Team VADER (Visual Analytics and
Data Exploration Research Lab; http://vader.lab.
asu.edu). Here, we compare our performance with
that of our VAST competitors and four professional
movie prediction websites.

Comparison with Peer Teams
Table 3 summarizes each team’s performance. For
the revenue predictions, we report the average error (in terms of millions of dollars), the standard
deviation of the average error, and the mean relative absolute error (MRAE), which is the percentage of bias deviating from the real value:

MRAE =

1
N

N

∑
i=1

Predictioni − Real Valuei
Real Valuei

.

We report similar values for predicting the IMDb
rating (which ranged from 1 to 10). For these statistics, smaller values indicate more accurate pre-

September/October 2014

8/21/14 4:11 PM

MRAE

1.5

Our prediction
boxoffice.com
filmgo.net
hsx.com
boxofficemojo.com

1.0
0.5
0
rek
rT

Sta

l
c
e
e
p
d
h
3
6
Epi Fast over Eart ee M rnshi Purg f Stee e En
r
S nte
o
g
th
e
t
n
s
u
n
f
i
I
o
A
Ha
Ma This
wY
No

r
t
e
o
MU WWZ Hea Down DM2 ange Turb juring Red 2 RIPD verin
l
e
R
n
Th ouse
Wo
Co
H
ite
Wh

MRAE

Figure 5. The mean relative absolute error (MRAE) of weekend revenue predictions. We clearly outperformed the experts for
three movies (Epic, The Hangover Part III, and Fast & Furious 6). Where we had the largest error (After Earth), we relied heavily on
the analytical component, with no interaction.

0.25
0.20
0.15
0.10
0.05
0
rek
rT

Sta

l
c
e
e
p
d
h
3
6
Epi Fast over Eart ee M rnshi Purg f Stee e En
r
S nte
o
g
th
e
t
n
s
u
n
f
i
I
o
A
Ha
Ma This
wY
No

r
t
e
o
MU WWZ Hea Down DM2 ange Turb juring Red 2 RIPD verin
l
e
R
n
Th ouse
Wo
Co
H
ite
Wh

Figure 6. The MRAE of our viewer-rating predictions. Sixteen out of 21 predictions had an error below 10 percent, and 11 had an
error below 5 percent.

dictions. The data in Table 3 was provided to all
challenge participants after the contest closed.
Regarding the average error and standard deviation for revenue predictions, our team reported
the lowest values. Regarding the MRAE for revenue predictions and viewer-rating predictions, our
results were slightly worse than Team Prolix and
similar to Philipp Omentisch, CDE IIIT, and Team
Turboknopf. However, Team Prolix’s average error
and standard deviation were much larger than
ours, indicating more inconsistent predictions.
Regarding the average error and MRAE for
viewer-rating predictions, our team had the lowest values of all teams that submitted more than
five predictions. CDE IIIT submitted two perfect
predictions; however, it submitted only those
two predictions, making it difficult to determine
whether its methods would produce consistent
results. Regarding the average error and standard
deviation for viewer-rating predictions, our team
performed similarly to Team Turboknopf, but with
a slightly lower average error and a slightly higher
standard deviation.

Comparison with Professional Predictions
In this comparison, we used our predictions for
only 21 of the 23 movies. Two of the 23, The Bling
Ring and The To Do List, were limited-release movies that opened in only five and 591 theaters, respectively. Most expert prediction sites don’t provide predictions for limited-release movies.
	

g5mac.indd 67

For each prediction, we followed the same general process we described in the section “Implementing the Toolkit.” As we stated before, the
underlying linear-regression model used in our
toolkit was significant, with an adjusted R 2 of approximately 0.60.
Figure 5 compares our MRAE with that of the
four websites for the opening-weekend revenue. We
clearly outperformed the experts on the weekend
when Epic, The Hangover Part III, and Fast & Furious 6 were released. On the weekend when we had
the largest error (for After Earth), we relied heavily
on the analytical component, with no interaction.
Figure 6 plots the MRAE for the review scores.
Approximately half of our predictions were within
a 5 percent error of the real review score. The four
websites had no published review score predictions.
The predictions with our toolkit were a dramatic
improvement over using just our model without
interaction (see the first two rows of Table 4). This
strongly indicates that our hypothesis (that VA
will help users develop better predictions than a
purely statistical solution will) is valid. However,
we don’t wish to overstate our claims. The contest
provided only a single data point for exploring how
one group of analysts in a closed-world setting
could use a VA toolkit for improved prediction.
The need exists for further controlled studies in
which a group of analysts performs similar model
predictions both with a VA platform and with only
a given regression model.
IEEE Computer Graphics and Applications

67

8/21/14 4:11 PM

Business Intelligence Analytics

Table 4. Comparing our toolkit with professional predictions.
Prediction source

No. of predictions

Average error

21

12.729

9.425

0.285

VADER, no interaction

21

23.051

22.011

0.501

boxoffice.com

21

8.538

7.466

0.191

6

12.750

7.409

0.297

hsx.com

20

9.060

7.397

0.205

boxofficemojo.com

14

9.864

7.527

0.224

Table 4 shows that our average error and average MRAE were slightly lower than those of filmgo
.net. This indicates that our approach enabled our
group of novice analysts to be competitive with
experts. The significance of this relies on three
major assumptions:
■■

■■

■■

The professional prediction websites had more
experience in movie revenue prediction than
our team.
The professional prediction websites had access
to more data than our team was allowed in the
closed-world contest.
Access to more data can enable better predictive
models.8,9,11,12

First, it seems reasonable that a professional prediction website would have much more experience
than a computer science team who had never previously attempted to predict movie revenue. Second, there’s no restriction on what data a professional website’s predictions can use. For example,
boxoffice.com uses Facebook tracking and Twitter
tracking, and hsx.com uses the Hollywood Stock
index. Third, it’s clear that using more data (specifically, the number of theaters a movie is released
in) will produce a better prediction model (a larger
R 2). From these assumptions, it becomes clear that
(in this instance) a VA toolkit can enable individuals who are knowledgeable about data analysis to
quickly understand information being presented
to them in new domains and make predictions
that are in line with expert predictions.
Our MRAE (0.285) was slightly lower than
that of filmgo.net (0.297) but approximately 50
percent worse than that of boxoffice.com (0.191).
However, if we remove the After Earth and Now
You See Me weekend (during which we relied
heavily on the model and little on the interactive
visuals), our MRAE drops to 0.239, which puts
us near boxofficemojo.com (0.224). Other error
sources can be accounted for in disrupted Twitter
and Bitly data feeds. These interruptions were
pronounced for The Heat, White House Down,
Monsters University, and World War Z. However,
even with those interruptions, our predictive
analysis was still quite robust, with only The Heat

g5mac.indd 68

Average MRAE

VADER, interactive

filmgo.net

68	

Standard deviation

obtaining a significantly worse prediction than the
professional sites.

The Challenges Ahead
Overall, applying VA for social media analysis has
proven relatively effective. However, four main
challenges exist in applying this to all domains of
business intelligence. First, social media data is extremely noisy. Movie predictions work well because
you can track ad campaigns’ effectiveness by following the specific hashtags promoted by a brand.
As the analysis gets farther afield from Twitter (for
example, when trying to mine Bitly data), choosing effective keywords becomes difficult.
Second, owing to the ever-changing stream of
social media sources and users, any automated
system for data collection and prediction will
likely eventually be steered off course. So, it’s critical to link the human into the loop. However, as
is evidenced by the issues in sentiment analysis,
data cleaning shouldn’t overburden analysts. The
sentiment analysis and cleaning employed in our
research places an overly large burden on the user.
A more effective solution could be a system for
sentiment model training that has users label a
subset of tweets.
Third, it’s imperative to link highly curated small
datasets with this big data. Although social media
data can serve as a proxy for many signals, we find
that linking multiple data sources with varying
reliability levels (for instance, the total weekend
revenue for all movies and regression modeling)
can enhance a system’s predictive abilities. For example, doing focus groups and linking their data
with results from social media could enhance the
analysis of a proposed new product release.
Finally, this research demonstrates the need for
interactive tools to mine social media data. From
the examples of movie revenue prediction, it’s clear
that such data contains a wealth of information.
However, extracting knowledge from this data and
effectively communicating it remain a challenge.
The need clearly exists for effective data-cleaning
tools to improve the filtering of unrelated social
media signals and for improving the results of
challenging analytical tasks (such as sentiment
analysis). Our results demonstrate that using VA

September/October 2014

8/21/14 4:11 PM

tools can significantly affect knowledge discovery
for business intelligence.

A

lthough our results demonstrate only a single
data point, we feel this is significant in that
the contest provisions let us directly compare analysts using a VA toolkit to experts in a particular
modeling domain. We recognize that this is a far
cry from definitively validating our hypothesis
that the use of VA will enable users to develop better box-office predictions than a purely statistical
solution would.
This research points to the need for better
methods for evaluating the impact of VA used for
complex problems such as prediction. A variety of
factors and variables must be addressed and controlled, including the level of expertise and the
types of visualizations provided. Using our toolkit,
we’ve been collecting streaming movie data in a
manner similar to the VAST Box Office Challenge
and plan to run a variety of controlled experiments. Of primary interest is exploring levels of
expertise and VA’s impact on predictions. We feel
that the results we reported here are an important
starting point for such explorations.

Acknowledgments
This research was supported partly by the US Department of Homeland Security’s VACCINE (Visual
Analytics for Command, Control, and Interoperability Environments) Center under award 2009-ST061-CI0001. We thank the 2013 Visual Analytics
Science and Technology Box Office Challenge organizers and participants for their help in data collection,
evaluation, and discussions.

References
	 1.	T. Schreck and D. Keim, “Visual Analysis of Social
Media Data,” Computer, vol. 46, no. 5, 2013, pp. 68–75.
	 2.	 H. Bosch et al., “Scatterblogs2: Real-Time Monitoring
of Microblog Messages through User-Guided
Filtering,” IEEE Trans. Visualization and Computer
Graphics, vol. 19, no. 12, 2013, pp. 2022–2031.
	 3.	 J. Chae et al., “Spatiotemporal Social Media Analytics
for Abnormal Event Detection and Examination
Using Seasonal-Trend Decomposition,” Proc. 2012
IEEE Conf. Visual Analytics Science and Technology
(VAST 12), 2012, pp. 143–152.
	4.	X. Wang et al., “I-SI: Scalable Architecture for
Analyzing Latent Topical Level Information from
Social Media Data,” Computer Graphics Forum, vol.
31, no. 3, part 4, 2012, pp. 1275–1284.
	

g5mac.indd 69

	5.	M.C. Hao et al., “Visual Sentiment Analysis of
Customer Feedback Streams Using Geo-temporal
Term Associations,” Information Visualization, vol.
12, nos. 3–4, 2013, pp. 273–290.
	
6.	
S. Baccianella, A. Esuli, and F. Sebastiani,
“SentiWordNet 3.0: An Enhanced Lexical Resource
for Sentiment Analysis and Opinion Mining,” Proc.
Int’l Conf. Language Resources and Evaluation, 2010,
pp. 2200–2204.
	7.	R Development Core Team, R: A Language and
Environment for Statistical Computing, R Foundation
for Statistical Computing, 2008.
	 8.	 R. Panaligan and A. Chen, Quantifying Movie Magic
with Google Search, white paper, Google, 2013.
	 9.	 S. Asur and B.A. Huberman, “Predicting the Future
with Social Media,” Proc. IEEE/WIC/ACM Int’l Conf.
Web Intelligence and Intelligent Agent Technology, 2010,
pp. 492–499.
10.	G. Rowe and G. Wright, “The Delphi Technique
	
as a Forecasting Tool: Issues and Analysis,” Int’l J.
Forecasting, vol. 15, no. 4, 1999, pp. 353–375.
	11.	W. Zhang and S. Skiena, “Improving Movie Gross
Prediction through News Analysis,” Proc. IEEE/
WIC/ACM Int’l Joint Conf. Web Intelligence and
Intelligent Agent Technology, 2009, pp. 301–304.
	12.	M. Joshi et al., “Movie Reviews and Revenues: An
Experiment in Text Regression,” Human Language
Technologies: The 2010 Ann. Conf. North Am. Chapter
of the Assoc. for Computational Linguistics, 2010, pp.
293–296.
Yafeng Lu is a PhD student working for Ross Maciejewski
in Arizona State University’s School of Computing, Informatics, and Decision Systems Engineering. Her research interests are data analysis and visualization. Lu received her
master’s in computer science and theory from Northeastern
University, China. Contact her at lyafeng@asu.edu.
Feng Wang is a PhD student working for Ross Maciejewski
in Arizona State University’s School of Computing, Informatics, and Decision Systems Engineering. His research interests
include data visualization and data mining. He received his
master’s in computer science from the University of Science
and Technology of China. Contact him at fwang49@asu.edu.
Ross Maciejewski is an assistant professor in Arizona State
University’s School of Computing, Informatics, and Decision
Systems Engineering. His research interests are geographical
visualization and visual analytics focusing on public health,
social media, criminal incident reports, and dietary analysis.
He received his PhD in computer engineering from Purdue
University. Contact him at rmacieje@asu.edu.
Selected CS articles and columns are also available
for free at http://ComputingNow.computer.org.
IEEE Computer Graphics and Applications

69

8/21/14 4:11 PM

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

Describing Temporal Correlation Spatially in a Visual Analytics
Environment
Abish Malik ∗

Ross Maciejewski∗
∗ Purdue

A BSTRACT

Keywords: Visual analytics, temporal correlation, spatial aggregation, crime analysis.
M OTIVATION

In multivariate spatiotemporal data, the relationships between temporal trends and spatial locality become an important factor in analysis. Time series of variables are often generated and analyzed as
a precursor to creating predictive models of the data. However,
when analyzing temporal trends of spatiotemporal data, it is often most mathematically tractable to aggregate the data over a particular collection repository (i.e., a financial institution, hospital,
police station, etc.). As the number of variables in a dataset increases, the number of linear correlations available to calculate and
explore quickly becomes intractable. Given a multivariate dataset
with N variables, the number of pairwise correlative factors to be
N×(N−1)
. Thus, the amount of information regarding
explored are
2
correlative properties between variables is of the order of O(n2 ).
However, understanding the relationships between variables within
a dataset can provide analysts with valuable insights and aid them
in hypothesis generation and exploration. As the analysis of competing hypotheses depends on complete (i.e., sufficiently complete
to be reasonably accurate) mental models of considered concepts,
∗ e-mail:
† e-mail:

{amalik|rmacieje|ebertd}@purdue.edu
HodgessE@uhd.edu

David S. Ebert∗

University Visualization and Analytics Center (PURVAC)
† University of Houston - Downtown

In generating and exploring hypotheses, analysts often want to
know about the relationship between data values across time and
space. Often, the analysis begins at a world level view in which
the overall temporal trend of the data is analyzed and linear correlations between various factors are explored. However, such an
analysis often fails to take into account the underlying spatial structure within the data. In this work, we present an interactive visual
analytics system for exploring temporal linear correlations across a
variety of spatial aggregations. Users can interactively select temporal regions of interest within a calendar view window. The correlation coefficient between the selected time series is automatically
calculated and the resultant value is displayed to the user. Simultaneously, a linked geospatial viewing window of the data provides
information on the temporal linear correlations of the selected spatial aggregation level. Linear correlation values between time series
are displayed as a choropleth map using a divergent color scheme.
Furthermore, the statistical significance of each linear correlation
value is calculated and regions in which the correlation value falls
within the 95% confidence interval are highlighted. In this manner, analysts are able to explore both the global temporal linear
correlations, as well as the underlying spatial factors that may be
influencing the overall trend.

1

Erin Hodgess†

and that those models are wholly dependent on all pertinent information being considered, new ways of disseminating and exploring
information are needed.
This need provides an impetus to the field of visual analytics.
Visual analytics is the science of analytical reasoning assisted by
interactive visual interfaces[16, 22]. This interactivity, especially
in large and semantically complex sets, provides an immersive
technique for exploring information relevant to the mental models
[13, 14] developed by analysts. By combining this interactivity with
intuitive visual displays, analysts can ingest information, form, explore, and validate hypotheses, as well as generate supporting data
for reports.
In this paper, we present a visual analytics system for exploring
temporal correlations within a given data set over various levels of
spatial aggregation. Our system, shown in Figure 1, provides interactive filters and linked views in which analysts can test their mental models by exploring only the relevant data components. Furthermore, given a spatiotemporal dataset, various levels of spatial
aggregation exist at which one may analyze the data. Our system
presents an overall summary aggregation of the data and plots the
temporal history of the dataset. Users are able to interactively select
a set of time series data and using their specific domain knowledge,
interactively explore correlations in the temporal domain and visualize the correlations at various spatial resolutions on a choropleth
map (a map in which area are colored in proportion to the measured
variable being displayed).. Such a system provides the user with an
insight into the predictive value of a given set of time series data as
well as an insight into which spatial areas in the data show strong
signs of temporal correlation.
Our correlative analysis system focuses on categorical spatiotemporal event data (e.g., financial data, crime reports, emergency department logs). In such data, events consist of locations
in time and/or space, and each event fits into a hierarchical categorization structure. These categories can be filtered by linked
data, and the events may be mapped to a particular spatial location. Data categories are typically processed as either time series
aggregated over some spatial location (county, zip code, collection
station), or spatial snapshots of a small time aggregate (e.g., day,
week). In order to facilitate the discussion of our visual analytics
system, we frame our work around an application of exploring correlations within criminal incident reports from West Lafayette, Indiana, USA. Such a dataset is representative of a variety of spatiotemporal data that consists of fields including date and time of occurance, crime types, geographic location, street address, description
of crime events, etc. Currently, our work has focused on four case
studies: 1) an analysis of the correlative factors between criminal
incidents and large events such as football and basketball games, 2)
the spatial exploration of temporal correlations between semester
criminal incidents, 3) the spatial exploration of the aggregate of
noise and vandalism incidents between semester weekends, and 4)
a correlative analysis between criminal incidents over semester holidays.

1530-1605/11 $26.00 © 2011 IEEE

1

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

Figure 1: A screenshot of our system showing an interactive user date selection in the temporal calendar view, the resultant Pearson’s correlation
choropleth map, and a color legend showing the range of correlation values from -1.0 to 1.0. In the calendar view on the left, the user has
interactively selected one temporal region (in blue) and is analyzing the correlation to the equal length temporal selection (in green). The
correlation coefficient is displayed on the top of the calendar view. In this case, it is -.236. On the Right, we show the geospatial map view
illustrating the temporal correlations with respect to census tract areas. The choropleth colors represent the correlation coefficients for each
region. If regions are statistically significantly correlated, their boundaries will appear as thicker black lines.

2

R elated work

In recent years, researchers have explored different methods of analyzing temporal data to discover and analyze trends and correlations. The analysis of time series data is one of the most common
problems in any data domain, and the most common techniques
of visualizing time series data (sequence charts, point charts, bar
charts, line graphs, and circle graphs) have existed for hundreds of
years. Recent work in time series visualization has produced a variety of techniques, an overview of which can be found in [1]. Early
work by van Wijk and Selow [24] looked at using calendar view visualizations to enable users to identify patterns and trends on multiple time scales simultaneously and provided a simple cluster analysis method. More recent techniques include the theme river [15],
the spiral graph [26], and the time wheel [23]. These techniques
looked at new ways to visualize temporal data to try and emphasize
repeating patterns.
While such visualizations are able to provide insight into temporal data sets, many temporal data streams also contain spatial components. In order to facilitate the exploration of the spatial components of data, geographical visualization tools and techniques were
developed. Geographic visualization utilizes sophisticated, interactive maps to explore information, guiding users through their
data and providing context and information with which to generate and explore hypotheses. In more recent years, it has expanded
to include increasingly complex data, new spatial contexts, and
information with a temporal component. Relevant summaries on
work in the field can be found in texts by Peuquet [20], Dykes and
MacEachren [9], and Andrienko and Andrienko [4]. These books

detail thoughts on knowledge discovery and the exploratory analysis of spatial and temporal data. Other reviews on spatiotemporal
data mining and analysis can be found in [2, 5, 11].
Many of the geovisualization methods described in these textbooks [4, 9, 20] have been leveraged to create systems for data exploration combining both interactive mapping techniques and statistical methods. Dykes et al. [10] utilized web services to deliver
interactive graphics to users, and introduced an approach to dynamic cartography supporting brushing and dynamic comparisons
between data views in a geographical visualization system. Andrienko et al. [3] developed the Descartes system in which users
were able to select suitable data presentation methods in an automated map construction environment. Edsall et al. [12] created a
system that decomposed geographic time series data using a threedimensional Fourier transformation of the geographic time series,
allowing users to explore the three-dimensional representation of
the physical and spectral spaces. Carr et al. [8] utilized a two-way
layout of choropleth maps to enhance a user’s ability to compare
dataset and explore data hypotheses.
Such systems have paved the way for the emergence of visual analytics as a field formed at the intersection of analytical reasoning
and interactive visual interfaces [22]. Visual analytics is concerned
with presenting large amounts of information in a comprehensive
and interactive manner. By doing so, the end user will be able to
quickly assess important data and, if required, investigate points of
interest in detail. While the formulation of the term visual analytics is relatively new, there has been much work in the realm of
exploratory data analysis, particularly with regards to geographi-

2

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

Figure 2: Screenshots of the user selection process in the calendar view mode. Our system allows for contiguous selection by row (Left) or
column (Right) in the calendar and the creation of a disjoint time series pattern (Middle).

cally located data. Work by MacEachren et al. [18] emphasized
the use of visual techniques in areas of decision making and argued
that a use-based approach was needed in developing information
processing environments for a given user, which is of fundamental concern under the concept of visual analytics. These concepts
were further explored in work by MacEachren et al. [19] in which
a conceptual framework for the integration of knowledge discovery using geographical visualization was proposed in order to enable users to explore their data in the context of a spatiotemporal
environment. More recently, the development of visual analytics
systems for data analysis and exploration has been rapidly growing
(e.g., [7, 17, 21, 25]). Techniques common across these systems
include the probing, brushing and linking of data in order to help
analysts refine and explore their hypotheses.
3

V isual analytics environment

Our system utilizes many of the aforementioned techniques to create an interactive environment for exploring temporal correlations
spatially. By using domain knowledge of the data, analysts can
form hypotheses about correlative relationships between variables
in the dataset. Our work adopts the calendar view idea presented by
van Wijk and Selow [24] and extends this work for interactive multivariate data correlation analysis. We incorporate linked views and
brushing, thereby facilitating user interaction between the spatial
and temporal domains of the data.
Figure 1 shows a snapshot of our system. The main window
(Figure 1 - Right) of the system shows the geospatial view that

supports the overlay of different maps and criminal incidents along
with interactive panning and zooming tools. The left-most window
is the calendar view of the selected CTC incidents that shows the
sum of crime incidents for each day of a calendar year. The calendar view enables the users to visualize special events like football
and basketball games on the calendar further allowing them to make
a connection between the reported CTC activities and these events.
The bottom-rightmost window contains the time slider that is used
to temporally scroll through the data while dynamically updating
all the other linked windows to reflect the change.
3.1

Temporal correlation exploration

For the temporal correlation exploration, we provide several interaction methods within the calendar view window widget of Figure 2. The users can select date ranges by simply clicking on the
start and end dates of the first date range and then choosing the start
date of the second range using a mouse. To ensure that the length
of the two selected time series remains the same (a requirement
for calculating the correlation coefficient), the system automatically
calculates the ending date of the second range. This method selects
all the dates between the selected starting and ending dates. This
mode of selection can be seen in Figure 2 (Left). The users can
also select individual dates and form two arbitrary date ranges for
performing correlative analysis. Figure 2 (Middle) shows this mode
of selection. This method can be used, for example, to analyze the
spatial and temporal correlations between the crime occurences on
major calendar events like football and basketball games. Finally,

3

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

Figure 3: Our temporal correlation analysis with respect to spatial aggregation. The user has selected a time series (A and B) and for each
temporal unit, the number of events in areal unit gi are calculated. Then for each gi , there exists two time series giA and giB which are used
to calculate Pearson’s correlation. The correlation value is mapped to a color and the resulting temporal correlations with respect to spatial
aggregates are plotted as a choropleth map.

we also include a feature by which users can select an arbitrary
range of dates by clicking on a start date and choosing an end date
which selects all the dates between the columns or rows of the starting and ending dates. Figure 2 (Right) shows this mode of selection
where the analyst intends to perform correlative analysis between
the two selected range of weekends. This type of selection is especially useful if, for example, the analyst wants to analyze the correlation between only the weekends of two different times of a year
(or of different years). In each of the selection methods, the system colors each calendar day with a color shade that corresponds to
the relative count with respect to the maximum crime count for that
particular calendar year. This further allows the users to observe
trends in their selected date ranges. After selecting the two date
ranges, the system provides the users with the option of selecting
the time aggregation (by day, week or month) over which it automatically aggregates the data. Moreover, to facilitate the selection
of dates, we allow the users to change the width of the calendar to
range from 7 to 30 days. This action further provides the users with
more options of selecting the dates for correlative analysis by using
any of the described methods above.
Once the temporal ranges to be compared are selected, the correlation calculation is performed. Correlation is a single number
used to describe the linear relationship between two variables. In
the case of time series analysis, we use correlation to describe the
degree in which certain periods of time can be used to predict future
events. For example, a police official may be interested in knowing
whether crime activity levels at a certain period of a year are related
to those of a different year thus indicating a pattern in criminal activity. As such, we choose to apply the Peasron product-moment
correlation coefficient (Pearson’s correlation - rxy ) as part of our
hypothesis analysis which is defined as
N x N y
N ∑N
i ∑i=1 i
i=1 xi yi − ∑
qi=1
rxy = q
N
2
2 N ∑N y2 − (∑N y )2
N ∑N
i=1 xi − (∑i=1 xi )
i=1 i
i=1 i

(1)

where N is the length of time series x and y (note that to calculate
rxy x and y must be of equal length), and xi and yi are the values of
the respective time series at time i. Once rxy is calculated, we then

apply a two sided t-test:
q
2 )/(N − 2)
t = rxy / (1 − rxy

(2)

We then use the t-distribution with N − 2 as the degrees of freedom to determine if the calculated correlation coefficient is significant within the 95% confidence interval. The correlation coefficient
for the temporal selection is then displayed at the top of the calendar view dialog box and we correspondingly show whether the two
sided t-test implies temporal significance or not.
3.2

Spatial correlation exploration

Given the temporal correlation for a given time series, the next
question that could be asked is, what influence do the spatial constraints in the data have on this correlation? To that end, our system
automatically splits the time series into a set of k new time series,
where each of these new k time series is based on the aggregate
number of samples within a given areal unit. The system then calculates the Pearson’s correlation coefficient for each areal unit and
colors it based on its calculated correlation coefficient using a divergent color scale [6]. This generates a choropleth map, which
is a thematic map with each geographic unit colored in proportion
to the resulting correlation coefficient value mapped between the
range of the minimum (-1.0) and maximum (1.0) attainable values.
We also apply the two sided t-test to each areal unit and highlight
the statistically significant regions by increasing the thickness of
their boundary outlines.
This process of generating the spatial correlation map is explained in Figure 3. In this example, the analyst has chosen to
see the correlation of the aggregate of noise, vandalism and theft
crime incidents in West Lafayette, Indiana, USA over the two selected time series (A and B) of N weeks each from the calendar
view by selecting a range of dates from which the system generates
the data for the selected weeks automatically. After selecting the
two desired date ranges on the calendar, the system calculates the
Pearson’s correlation coefficient for each areal unit and maps this
value to a color with which the corresponding geographical unit is
then colored. Moreover, for each areal unit, the system determines
whether the calculated correlation coefficient is significant within

4

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

Figure 4: A comparison of temporal correlations in combined noise and vandalism incidents for West Lafayette, Indiana, USA between football
weekends over the years. The spatial regions with statistically significant correlations are highlighted by increasing the thickness of their borders.
No consistent correlation patterns are observed between football weekends over the years.

Figure 5: Temporal correlation comparison of an aggregate of noise and vandalism complaints between semester weekends.

the 95% confidence interval using a two sided t-test. If the test indicates that the spatial unit is significant, the system highlights it by
increasing the thickness of its boundary outlines.
4

Exploring correlations in crime data

To demonstrate the functionality of our system, we utilize the criminal incident reports from West Lafayette, Indiana, USA for the
years 2000 to 2008. These crime reports have been categorized into
different crime categories and as such we generate a calendar view
of the crime incidents to facilitate the selection of the date ranges
for correlative analysis. Our system allows users to interactively
filter by crime category, and in this section, we cover four examples
of using our system to explore correlations in West Lafayette, IN,
USA.
4.1

Correlations between football weekends

The first example looks at noise and vandalism reports with respect
to home football games. Here, the analyst believes that noise and
vandalism reports during home football games may be consistent
from year to year. The analyst selects the home football games from
2004 and those from 2005. This process is repeated to compare
2004 to 2006 and 2004 to 2007. Results of the analysis are shown
in Figure 4.

The overall temporal correlation is found to be insignificant with
respect to the two-sided t-test. In the 2004 to 2005 image, we can
see that the two dark brown areas are spatially correlated, and the
dark border indicates that this is a statistically significant correlation. Unfortunately, this correlation pattern does not hold when
comparing 2004 to 2006. Furthermore, a different trend set emerges
when comparing 2004 to 2007. This indicates that football game
schedules will not be a good predictor of noise and vandalism complaints.
4.2 Correlations between semester weekends
Our second example demonstrates the correlative trends of the aggregates of noise and vandalism reports for West Lafayette, Indiana,
USA occuring on the weekends (Friday to Sunday) between two
semesters. As can be seen from the results (Figure 5), we observe
that none of the areas show consistent significant correlative trends
over the semesters. We also find that one region is highly positively
correlated and statistically significant when comparing the weekends between the spring and fall 2004 semesters (Figure 5-Left),
even though the temporal correlation for this case is not significant.
This analysis indicates that even though noise and vandalism complaints are more frequent on the weekends (as can be observed from
the weekly histogram plots of Figure 2-Right), semester weekends
are not good predictors of noise and vandalism complaints over the

5

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

Figure 6: A semester to semester comparison of temporal correlations in combined noise and vandalism reports for West Lafayette, Indiana,
USA. (Top) Comparison of correlations between fall semesters. (Middle) Comparison of correlations between spring semesters. (Bottom)
Comparison of correlations between fall and spring semesters.

subsequent semesters.
4.3

Semester to semester analysis

Our third example looks at the weekly correlation between combined noise and vandalism complaints. We compare yearly across
semesters and between semesters across years. The results of the
analysis are shown in Figure 6. Here we find that the temporal correlation between semesters is not significant; however, in both the
Fall (Figure 6-Top) and Spring (Figure 6-Middle) semesters, there
are areas of West Lafayette, IN, USA that show considerably high
correlation values from Fall to Fall and from Spring to Spring. Unfortunately, in the three year period chosen, there seems to be no
area that is consistently statistically significantly correlated across
all three years. Finally, when comparing Fall semester to Spring
semester (Figure 6-Bottom), we see that the correlation values are
negative for two of the three comparisons, with one having a global
significance. Again, however, there are no significant correlative
trends that can be extracted between these components consistently.

4.4

Correlations between aggregate of theft and burglary crimes over semester breaks

In our final example, we compare the correlations between an aggregate of theft and burglary crime reports for West Lafayette, Indiana, USA occuring during the semester breaks when most of the
university students leave for their homes for the holidays. In particular, we compare the correlative trends between crime occurances
occuring on spring breaks and thanksgiving breaks over different
years. The results of the analysis are shown in Figure 7. These
results highlight regions that are spatially significant, even though
their corresponding temporal correlations are not. We also find certain geographic regions that have consistently high spatial correlation over the years for these crime types. For example, the regions
maked by arrows in the figure show positive spatial correlation over
their respective time periods, and their highlighted borders indicate
statistical significance. So the regions marked by arrows across the
years over spring break (and correspondingly over fall break) show
high positive correlation indicating an increase in crime incidents

6

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

Figure 7: A comparison between an aggregate of thefts and burglaries over spring and thanksgiving breaks over the years.

over the years. This allows the analysts to point out the spatial regions that influence the overall global temporal trend. This example
further shows that semester breaks are a good predictor of theft and
burglary crimes.
5

Conclusions and future work

In this work, we presented a method of exploring temporal correlations spatially through an interactive visual analytics environment.
Our system provides analysts with both a means of generating and
exploring hypotheses along with tools to provide statistical tests of
these hypotheses. We have demonstrated the functionality of our
system using criminal reports from West Lafayette, Indiana, USA.
Future work will include the incorporation of temporal lags and
allow analysts to automatically find the optimal correlation coefficients within a given time range.
ACKNOWLEDGMENT
The authors would like to thank the Tippecanoe County Police Department. This work has been supported by the U.S. Department
of Homeland Security’s VACCINE Center under Award Number
2009-ST-061-CI0001. A portion of this research was performed
under an appointment to the U.S. Department of Homeland Security (DHS) Summer Research Team Program for Minority Serving
Institutions under DOE contract number DE-AC05-06OR23100.
R EFERENCES
[1] W. Aigner, S. Miksch, W. Muller, H. Schumann, and C. Tominski. Visual methods for analyzing time-oriented data. IEEE Transactions on
Visualization and Computer Graphics, 14(1), January/February 2008.
[2] G. Andrienko, D. Malerba, M. May, and M. Teisseire. Mining spatiotemporal data. Journal of Intelligent Information Systems, 27:187 –
190, 2006.

[3] G. L. Andrienko and N. V. Andrienko. Interactive maps for visual
data exploration. International Journal of Geographic Information
Science, 13(4):355–374, 1999.
[4] N. Andrienko and G. Andrienko. Exploratory Analysis of Spatial and
Temporal Data: A Systematic Approach. Springer - Verlag, 2006.
[5] N. Andrienko, G. Andrienko, and P. Gatalsky. Exploratory spatiotemporal visualization: an analytical review. Journal of Visual Languages & Computing, 14(6):503 – 541, 2003.
[6] C. A. Brewer. Designing better Maps: A Guide for GIS users. ESRI
Press, 2005.
[7] T. Butkiewicz, R. Chang, Z. Wartell, and W. Ribarsky. Alleviating the
modifiable areal unit problem with probe-based geospatial analyses.
Computer Graphics Forum, To appear.
[8] D. B. Carr, D. White, and A. M. Maceachren. Conditioned choropleth maps and hypothesis generation. Annals of the Association of
American Geographers, 95(1):32–53, March 2005.
[9] J. Dykes and A. M. MacEachren. Exploring Geovisualization. Elsevier, 2005.
[10] J. A. Dykes. Exploring spatial data representation with dynamic
graphics. Comput. Geosci., 23(4):345–370, 1997.
[11] J. A. Dykes and D. M. Mountain. Seeking structure in records of
spatio-temporal behaviour: visualization issues, efforts and applications. Comput. Stat. Data Anal., 43(4):581–603, 2003.
[12] R. M. Edsall, M. Harrower, and J. L. Mennis. Tools for visualizing properties of spatial and temporal periodicity in geographic data.
Comput. Geosci., 26(1):109–118, 2000.
[13] T. Green and W. Ribarsky. Using a human cognition model in the
creation of collaborative knowledge visualizations. In Proceedings
of SPIE (Defense & Security Conference 2008), volume 6983, pages
C1–C10, 2008.
[14] T. Green, W. Ribarsky, and B. Fisher. Visual analytics for complex
concepts using a human cognition model. In Proceedings of the IEEE
Symposium on Visual Analytics Science and Technology (VAST), pages
91–98, 2008.

7

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

[15] S. Havre, E. Hetzler, P. Whitney, and L. Nowell. Themeriver: Visualizing thematic changes in large document collections. IEEE Transactions on Visualization and Computer Graphics, 8(1):9–20, 2002.
[16] D. Keim, G. Andrienko, J.-D. Fekete, C. Görg, J. Kohlhammer, and
G. Melançon. Visual analytics: Definition, process, and challenges.
Information Visualization, pages 154–175, 2008.
[17] K. Liao. A visualization system for space-time and multivariate patterns (vis-stamp). IEEE Transactions on Visualization and Computer
Graphics, 12(6):1461–1474, 2006. Member-Diansheng Guo and Student Member-Jin Chen and Member-Alan M. MacEachren.
[18] A. M. MacEachren and M.-J. Kraak. Exploratory cartographic visualization: advancing the agenda. Comput. Geosci., 23(4):335–343,
1997.
[19] A. M. Maceachren, M. Wachowicz, R. Edsall, D. Haug, and R. Masters. Constructing knowledge from multivariate spatiotemporal data:
Integrating geographic visualization (gvis) with knowledge discovery
in database (kdd) methods. International Journal of Geographical Information Science, 13:311–334, 1999.
[20] D. Peuquet. Representations of Space and Time. Guilford Press., 2002.
[21] J. Stasko, C. Gorg, Z. Liu, and K. Singal. Jigsaw: Supporting investigative analysis through interactive visualization. In Proceedings
of the IEEE Symposium on Visual Analytics Science and Technology
2007, pages 131–138, 2007.
[22] J. J. Thomas and K. A. Cook, editors. Illuminating the Path: The R&D
Agenda for Visual Analytics. IEEE Press, 2005.
[23] C. Tominski, J. Abello, and H. Schumann. Axes-based visualizations
with radial layouts. In SAC ’04: Proceedings of the 2004 ACM symposium on Applied computing, pages 1242–1247, New York, NY, USA,
2004. ACM.
[24] J. J. Van Wijk and E. R. Van Selow. Cluster and calendar based visualization of time series data. In INFOVIS ’99: Proceedings of the 1999
IEEE Symposium on Information Visualization, page 4, Washington,
DC, USA, 1999. IEEE Computer Society.
[25] C. Weaver. Cross-filtered views for multidimensional visual analysis.
IEEE Transactions on Visualization and Computer Graphics, 16:192–
204, Mar. - Apr. 2010.
[26] M. Weber, M. Alexa, and W. Muller. Visualizing time-series on spirals. In INFOVIS ’01: Proceedings of the IEEE Symposium on Information Visualization 2001 (INFOVIS’01), pages 7–14, Washington,
DC, USA, Oct. 2001. IEEE Computer Society.

8

VAST Challenge 2015: Grand Challenge - Team VADER/VIS
Award for Outstanding Comprehensive Submission
Michael Steptoe, Robert Krueger, Yifan Zhang, Xing Liang, Rolando Garcia, Sagarika Kadambi,
Wei Luo, Thomas Ertl, Ross Maciejewski ∗

A BSTRACT
The VAST 2015 Grand Challenge focused on a fictional park called
DinoFun World where visitors are provided with a handheld device
for communicating with park patrons and registering their visits to
rides. Our goal was to develop a visual analytics system to help
explore the spatiotemporal movement and communication data of
the park patrons. Specifically, we wanted was to determine which
patrons were involved in a theft that occurred at a park pavilion.
In order to facilitate this data exploration, we have created a webdeployable system that combines data wrangling, trajectory analysis, network analysis, and interactive visualizations for discovering
movement and communication patterns of users and their networks.
We earned the Award for Outstanding Comprehensive Submission.
1

I NTRODUCTION

Understanding the spatiotemporal movement of individuals and
groups is necessary to characterize their behaviors and observe
anomalies or unusual patterns. By combining this with communication information we can see the way people act and react based
on information received within their network. Several transformations that can be applied to spatiotemporal data to achieve this include trajectory extraction, aggregation, and simplification [1]. In
this work, we explore spatiotemporal movement analysis and communications through the use of visual analytics.
2

DATA P RE -P ROCESSING

The movement and communication data as provided gave little insight into the parks operations and its visitors. Our approach for
processing the movement data was to create a spatiotemporal trajectory structure. The trajectory structure would be defined by attraction check-in information around the park. A large majority
of attractions did not have check-in information, but one can infer check-in/out time based on how long a person was idle near an
attraction. Inferred check-ins were calculated as follows:
• If a person’s location remains within a distance, d, from attraction, a, for more than a temporal threshold, t, then we
consider this to be an inferred check-in at a. We used d =5
pixels, t = 5 minutes (analysts can vary these parameters).
Our observation is that if users visit attractions in the same order at
the same time, then they are likely traveling the park together. We
then chose to aggregate the check-ins to 5 minute intervals (user
adjustable). A spatiotemporal trajectory also consists of the location that a user last checked into. For example, if the user checks
in at the park gate (attraction 84) and then took fifteen minutes to
go to the Flying TyrAndrienkos(12) and stayed at that ride for 15
minutes, their trajectory would be: 84−84−84−12−12−12−. . .
For a full day at the park 8AM-12AM, we have a trajectory of
length 192. We combine all three days together for our trajectory
∗ E-mail:
{msteptoe,yzhan212,xliang22,rsgarci1,skadambi,wluo23,
rmacieje}@asu.edu; {robert.krueger,thomas.ertl}@vis.uni-stuttgart.de.

IEEE Conference on Visual Analytics Science and Technology 2015
October 25–30, Chicago, Il, USA
978-1-4673-9783-4/15/$31.00 ©2015 IEEE

Figure 1: System overview. View (1) shows the analytics interface
which allows the user to adjust the system’s parameters. View (2)
shows a map of DinoFun World and the trajectories of selected visitors. View (3) shows a calendar which represents each attraction
in the park and displays the count of visitors in 30 minute intervals. View (4) shows a histogram which displays the number of
calls made during a time period. View (5) shows a pixel based representation of a visitor’s trajectory in 5 minute intervals.
and a not-in-park value is stored when the user is out of the park.
The trajectory structure can be adjusted to represent park regions
(five park regions) and attraction categories (thrill ride, etc. - 10
categories total). After creating the structure, patrons with similar trajectories were clustered together using Levenshtein distances
(user adjustable). These clusters enabled the exploration and classification of various group types and sizes within the park visitors.
Next we processed the communication data using a spatiotemporal communication structure. The communication structure would
be defined by sender/receiver information such as ID and location.
From this structure park visitors could then be clustered using their
communication networks. Using visitor movement data in combination with their communication data visitors could be aggregated
using a blend of network and trajectory clustering. By blending the
aggregation techniques the user is able to discover networks of visitor that may or may not spend any time together, but communicate
or vice versa. The user can also identify when the visitors within
these networks meet, how much time they spend together, and the
amount that they communicated.
3

V ISUAL A NALYTICS FOR D INO F UN W ORLD

For spatiotemporal movement analysis, the trajectory structure previously described is used to visualize, cluster, and map the park
visitors’ movements through several views. The goal was to enable
users to explore individual and group movements interactively to
find interesting patterns and behaviors. The system consists of a
series of coordinated multiple views and an analytic interface.
3.1

Analytics Interface

The analytics interface allows the user to adjust the system’s parameters depending on the objective of their exploration. The interface
is made up of five subcontrols.

119

ID selection: A user can input a list of visitor IDs and view their
trajectories on the map view (2). Pressing play will animate the trajectories. The sequence of check-ins for each visitor is displayed in
the trajectory view (5). The communications data is also visualized
as points on the map that appear at the time of the call.
Visual query: A user can select time and location intervals on the
calendar view (2) and create a visual query with logic operators
(AND/OR/NOT). This query will return, for example, the IDs of
all patron that were at attraction 38 at 4PM and at attraction 45 at
9PM on Friday. This is our primary feature for finding users that
were at locations of interest at particular times. IDs and trajectories
that return from the query are plotted in the trajectory view (5).
Cluster: All visitor trajectories can be clustered using a Levenshtein distance function and hierarchical clustering. If a tolerance
of 0 is selected, the resultant clusters consist of the IDs with identical trajectories (in terms of locations visited at the same time). Reducing the tolerance provides fuzzier clusters (i.e., they have visited
mostly the same locations at the same time during their stay). The
groups found are plotted in the trajectory view where the trajectory
is shown to be the most representative trajectory of the group.
Outliers: The larger the smallest Levenshtein distance is, the more
unique a trajectory is. This slider returns the top n-IDs with the
largest distance. Results are plotted in the trajectory view.
Calendar Aggregation: This controls how the rows in the calendar
view are sorted (by region, attraction or ride type) as well as the data
plotted in the cells of the calendar view (data can be the number of
visitors at a ride, the number of sent/received/external/unique calls
sent from a ride at time t).
3.2

DinoFun World Visualization

The DinoFun World visualization enables the user to examine and
interact with the spatiotemporal movement and communication
data. Each view manipulates a different aspect of the data and allows for coordinated exploration between each view.
Map View: This view shows the trajectories of selected IDs and
also animates their movements over time showing communications
during animation and is linked to the trajectory view. The link enables brushing over a section of the trajectory to plot the movement
segment on the map. A heat map view coloring each pixel by the
number of times a visitor stepped there can also be displayed.
Calendar View: Each row represents an attraction in the park and
each cell is colored based on the calendar aggregation control. Data
can be viewed for each day, or all three days are aggregated in the
any day view. The every day view shows the counts of IDs that were
at the same place at the same time every day. Each cell represents
30 minutes of time.
Distribution View: This provides a histogram view of the number of sent/received/external/unique calls made during a time period. The y-axis is the number of IDs and the x-axis is the number
of communications made (a histogram of call distribution by ID).
Users can click a bin to see all the IDs in a bin, in this way we can
find those IDs with unusually large amounts of communications.
Trajectory View: This is a pixel based representation of a visitor’s
trajectory. Each cell is a 5 minute time interval that is colored based
on the location a user is at in the park. By hovering over a cell the
attraction or path associated with a patron highlighted on the map.
4

N ETWORK A NALYSIS V ISUALIZATION

Analysis of the spatiotemporal communication data required the examination of visitor communications with respect to location, time,
and frequency. This visualization allows the user to see when and
where visitors communicate, assemble/disband, and the frequency
of user interactions. Input to the view is a list of visitor IDs and the
day to examine. The x axis represents the time from 08:00 to 24:00
and the Y axis represents the attractions that the visitors are near.
Circles represent the movement of the visitors from one attraction

120

Figure 2: Overview of the network analysis visualization.
to another and the circle size encodes the number of visitors. A circle consists only of visitors that have a link in the communication
data. In the lower left, a small black circle can be seen at attraction
84. The small black circle represents multiple people simultaneously checking-in at that time. Later, part of the group at attraction
84 moved to attraction 1. A green line is then drawn to emphasize this movement. By hovering over the line we show if visitors
joined or left this group. If the circle changes to red, this means
that a person left the group at this attraction. However, if the circle
changes to blue a person joined the group. Blue lines represent the
internal communications among the visitors and red lines represent
their external communications.
5 C ONTEST R ESULTS
A famous soccer player, Scott Jones, had his trophies on display
for the weekend at the park. During that weekend a crime was
committed involving a figure from Scotts past. The goal of the
Grand Challenge was to determine which patrons were involved
in a theft using a combination of spatiotemporal movement data
and spatiotemporal communication data. The challenge asked participants to determine when and where the crime occurred and to
identify the possible suspects. Using the calendar view to look at
patterns from previous days, we could determine that the crime occurred between 9:45AM and 11:30AM at the Creighton Pavilion.
With this information we can identify one group of suspects by creating a visual query that returns all IDs that were in the pavilion
during 9:45-11:30AM, for more than 5 minutes (inferred check-ins
are 5 minutes). The result of this query was finding several users
with inferred check-ins and a single user with a hard check-in.
Next these IDs were used in the Network Analysis Visualization
to analyze group movement and communication. From the IDs, we
see that they are spatially clustered into three groups. One group
waits by an attraction for Scott to pass and ensure he enters the stage
at about 9:30. During this time, another group leaves the pavilion
and waits at attraction 7 for the group that will enter the pavilion.
Between 10AM and 10:55AM a third group is seen at the pavilion
while it is supposed to be closed. During this time there is a lot of
communication from the pavilion group to the others two groups.
Due to this interaction, we hypothesized that the crime takes place
between 10AM and 10:55AM. For a demonstration of the system,
please see the video at https://youtu.be/LUZr3qEt7Qo.
ACKNOWLEDGEMENTS
Some of the material presented here was supported by the NSF
under Grant No. 1350573 and in part by the U.S. Department of
Homeland Securitys VACCINE Center under Award Number 2009ST-061-CI0001. The authors would like to thank the VAST challenge organizers and reviewers for organizing the contest.
R EFERENCES
[1] G. Andrienko, N. Andrienko, P. Bak, D. Keim, and S. Wrobel. Visual
Analytics of Movement. Springer, 2013.

Visualizing the Time-varying Crowd Mobility
Feiran Wu∗, Minfeng Zhu†, Xin Zhao‡, Qi Wang§, Wei Chen¶
State Key Lab of CAD&CG, Innovation Joint Research Center for Cyber-Physical-Society System, Zhejiang University
Ross Maciejewski∥
The School of Computing, Informatics and Decision Systems Engineering, Arizona State University

Abstract
Modeling human mobility is a critical task in fields such as urban
planning, ecology, and epidemiology. Given the current use of mobile phones, there is an abundance of data that can be used to create models of high reliability. Existing techniques can reveal the
macro-patterns of crowd movement, or analyze the trajectory of an
individual object; however, they focus on geographical characteristics. In this paper, we employ a novel data representation, the mobility transition graph, which is generated from a citywide human
mobility dataset by defining the temporal trends of crowd mobility
and the interleaved transitions between different mobility patterns.
We describe the design, creation and manipulation of the mobility
transition graph and demonstrate the efficiency of our approach by
case study.
CR Categories:
Visual Analysis;
Keywords:
tion

1

I.3.8 [Computer Graphics]: Applications—

storyline, timeline, mobility, spatio-temporal transi-

Motivation

Understanding and exploring human mobility patterns is essential
in domains such as urban planning, transportation optimization, and
epidemiology [Gonzalez et al. 2008; Schneider et al. 2013]. Understanding where and how people move provides a window into
how they interact with their built environment and can provide insight for planners to improve transportation routes, prepare for disasters, or various other concerns. Given the current use of mobile
devices, researchers can now, more than ever, study how humans
move. As such there are emerging research trends focusing on the
development of data-driven pattern discovery for human mobility
patterns [Barabasi 2005]. While there has been much recent work
on the visual exploration of traffic patterns based on trajectory data
(e.g., [Wang et al. 2013; Zeng et al. 2014]), Visualizing the mobility
patterns from a citywide population is still a challenging task.
Given the ubiquity of mobile phones, measurements of human location and travel via cell signals and GPS locations are becoming
readily available. Such data can be mined as a proxy to describe
the daily life of citizens. Previous studies based on mobile phone
∗ e-mail:tomwfr007@gmail.com
† e-mail:zhuminfeng@live.cn

‡ e-mail:zhaoxinitachi@gmail.com
§ e-mail:wqstu1@163.com

¶ e-mail:chenwei@cad.zju.edu.cn
∥ e-mail:rmacieje@asu.edu

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for commercial advantage and that copies bear this notice and the full citation on the
first page. Copyrights for third-party components of this work must be honored. For all
other uses, contact the Owner/Author.
Copyright is held by the owner/author(s).
SA’15 Symposium on VisHPC, November 02 – 06, 2015, Kobe, Japan.
ACM 978-1-4503-3929-2/15/11.
http://dx.doi.org/10.1145/2818517.2818540

data have made significant progress in extracting human behavior
and mobility patterns by leveraging various approaches, including
statistical physics, mobility modeling, and data mining. Though
effective, these approaches focus primarily on discovering macroscale patterns. However, studies on mid-level or micro-scale mobility patterns are missing. Such studies are of critical importance as
recent work from a study of fifteen months of human mobility data
for 1.5 million individuals revealed that human mobility traces are
highly unique [de Montjoye et al. 2013].
In order to better explain the extracted mobility patterns, many of
the previous approaches incorporate classical visualization techniques, including parallel coordinates, heat-maps, and glyphs.
However, there are rarely integrated works that involve intertwined
statistical methods, data mining and visualization techniques. In
this paper, we contribute the design and implementation of a novel
visual exploration approach for depicting the temporal evolution
of human mobility patterns of 500,000 mobile phone users in a
medium-sized city (3 districts, 2 county-level cities and 6 county
areas, 919.7 million resident population in 2013). This dataset contains the location information of one million individuals over the
course of 30 days. The location of an individual is specified at a
temporal resolution of less than one minute and with a spatial resolution equal to the distribution of the mobile cell towers. This
makes it feasible to characterize the mobility patterns of an individual over the 30 day period at a fine scale resolution.
The main contribution of our study is to structure and depict mobility patterns and how persons transition from one mobility pattern
to another by a hybrid timeline-graph representation, called mobility transition graph (MTG), in which the vertices represent clusters
of similar mobility patterns and the edges represent transitions between patterns. The advantage of mobility transition graph is its
ability to simultaneously characterize the temporal evolutions and
interleaved transitions of mobility patterns (i.e., nodes may represent a concept such as walking to the downtown, and then transition
to a node representing riding the downtown). Besides, we incorporate the other assistant views to enable users to visually locate the
patterns, analyze the mobility features and groups.

2 Data Preprocess
The dataset employed in our study is provided by a mobile phone
service company. It consists of 14 billion records of 7 million phone
users across 25,000 cell towers over January and February of 2014.
The dataset is collected on the basis of cell towers: a record is generated whenever the following activity happens: a user enters or
leaves a cell tower, a user makes a call or sends a short message,
or a user stays in a cell tower for more than a given duration (e.g.,3
minutes). Each record contains multiple items: a phone ID, a cell
tower ID, the activity type, a time stamp.
Because of the dirtiness, geographical inaccuracy and temporal sparsity of this dataset, several preprocesses are taken to the raw
data:
• Removing Ping-pong Effects The ping-pong effect [Xiong
et al. 2012] is caused by the frequent handoffs between two
nearby cell towers, yielding fabricated records. We encode a

(a)
(b)

(c)
(d)

Figure 1: The mobility patterns extracted from the mobile phone location records of 500,000 users in 7 days (Jan.21 - Jan.27, 2014). (a) The
mobility patterns (clusters) are depicted with a multiple coordinate style in the mobility pattern view. (b) A map view shows the geographical
scene and trajectories. In this case, the selected mobility pattern describes the inactive behavior in the downtown. (c) A mobility transition
graph (MTG) is built upon the dataset. (d) Controlling widgets. When the user selects a pattern, the corresponding trajectories are shown in
the map view and the statistical information are depicted in the multiple coordinate view.

cell tower with a word, and employ an N-gram model [Gao
et al. 2002] to detect and eliminate frequent changes between
multiple cell towers.

temporal-correlated entropy S tc , which is similar to S unc despite that p(j) is temporal-correlated. If the ith mobile phone
stays a long time in the jth cell tower, p(j) will be high.

• Removing Invalid Records The records whose the activity
type or the cell tower id is unknown are removed.

• Average Speed is defined as the average speed of valid movement records. We set the speed limit to be 200km per hour,
and clean the records whose speed is higher than the limit.

• Removing Duplicate Records Several consecutive records of
a mobile phone may be generated due to the signal intensity
fluctuations or reconnections. We remove duplicated records.
• Compensating Missed or Sparse Records To compensate
missed records of a mobile phone, we add records by interpolating the values from known records.

3

The Mobility Transition Graph

Before designing a visual representation for studying citywide mobility patterns, we firstly represent the mobility of a mobile phone
user with an array of features that is derived from the movement and
geographical context. Considering the commonly used and the representative features proposed in recent years, 8 feature descriptors
(10 dimensions) are employed as follows:
• Temporal-uncorrelated Entropy An entropy describes the
degree of predictability. Many definitions concerning entropy have been proposed [Song et al. 2010] to measure the
predictability in human mobility. We adopt the temporaluncorrelated entropy because of its capability of distinguishing different movements. Usually, high entropy suggests the movements with dramatic changes. The temporaluncorrelated entropy S unc = −ΣN
j=1 p(j)log2 p(j) describes
the heterogeneity of visitation patterns, where p(j) denotes
the historical probability of the visit from the mobile phone
user to the jth cell tower.
• Temporal-correlated Entropy We additionally employ a

• Activity Mileage is defined as the distance the mobile phone
moves.
• Centroid Location is the geographic centroid (the longitude,
the latitude) of the sequence of records in a frame.
• Radius of Gyration is the typical distance traveled by a mobile phone user around the centroid of mass of the trajectory [Gonzalez et al. 2008]. Which is a synthetic and easy to
compute parameter leveraged to describe the movement span
of the user.
• Residence Location is the estimated location of the home of
a mobile phone user. It is defined as the most frequently appeared location (the longitude, the latitude) from 00:00 a.m.
to 6:00 a.m.
• Activity Radius is the average distance a mobile phone user
travels from his residence location. It measures the activity
scope in ti .
Second, the human mobility in a period of time can be regarded as
a composition of multiple segments, and each segment encodes a
specific event or activity in a time interval. The transitions among
consecutive segments may indicate temporal variations, periodicity
or abnormality of the mobility. Third, the mobility in a segment
is reasonable if and only if the result has a statistical significance.
Consequently, we divide the record sequence of each mobile phone
user into segments with a fixed time interval, then extract the features of each segment and cluster all segments of all mobile phone
users by K-means. Each cluster encodes the average mobility of a

group of mobile phone users can be regarded as a mobility pattern.
By assuming that the mobility pattern transition can be characterized as the transition between the consecutive segments, we incorporate the k-th (k = 1) order Markovian assumption and calculate
the transition likelihood between the mobility patterns in two consecutive periods of time by the method in [Song et al. 2009].

3.1

Visualization

Visualizing the constructed mobility transition graph is straightforward. Essentially, a mobility transition graph (MTG) is a hybrid
timeline-graph representation that characterizes the interconnected
mobility transition in a sequential way. To emphasize on the temporal transitions, we pack the mobility patterns vertically as a stacked
bar chart sorted by the value of a descriptor or the size (Figure 2
(a)). Each pattern is encoded as a grey rectangle, whose darkness
encodes the count of segments belong to the mobility pattern. The
user may filter and display the necessary patterns (e.g., according to
the likelihood of the transitions which connects to these patterns).
Then we sequentially place the stacked bars from left to right to
represent the flow of mobility over time. The transitions between
two consecutive time periods are encoded by ribbons that connect
pairs of mobility pattern. The ribbons are colored with respect to
the transition likelihood. To highlight the transition in the identical
pattern, the edge of the ribbon is colored in black while the edges in
green represents the transitions between different patterns. Normally, the width of a ribbon is set to be identical. In cases that multiple
transitions start or end in a node, the width of each ribbon is set
to be proportional to the transition likelihood of the corresponding
transition (see the region in red rectangle of Figure 2 (a)).
Further, an enhanced bar chart is employed to summarize the mobility transition graph as an overview. Each bar in dark grey corresponds to a time period in the MTG, and its height encodes the
number of the displayed patterns in the period. Each bar in light
grey encodes the transitions between consecutive time periods. A
semitransparent rectangle mask in blue indicates the interval of the
overview that represents the shown portion of the underlying MTG
(Figure 2 (c)).
pattern A

...

pattern B
pattern C
pattern D
pattern E
pattern F

pattern A
pattern B
pattern D
pattern G

pattern C
pattern B

...

pattern F

concentrates to demonstrate the transitions between the mobility
patterns and supports the user to explore with interactive operations.
• The Map View employs OpenStreetMap as the map, and
shows the geographic-related information in the map (Figure 1 (b)). The trajectories of mobility phone users are shown
in the map to help study the mobility descriptors and mobility
patterns. To avoid heavy visual clutter, trajectories of top-k
mobile phone records that are the closest to the cluster centroid are shown. k is a user-adjustable parameter. The stroke
weight of trajectory encodes the distance to the cluster centroid.
• The Mobility Pattern View displays the detailed information of 10 dimensions of 8 mobility descriptors by means of
a multiple coordinates plot (Figure 1 (a)). The histogram on
each coordinate axis characterizes the statistical distribution
of each dimension of the selected mobility pattern. A pattern list panel (Figure 1 (d)) employs a matrix view to show
the dimensions of each mobility pattern. The color of each
matrix cell encodes the numeric value of the corresponding
dimension. The user can either select a row in the list panel
or choose the ribbons in the mobility pattern view to specify and study a mobility pattern. The user can also filter each
dimension by adjusting the range slider on the associated axis.
• The MTG View The MTG view visualizes the constructed
mobility transition graph. The user can navigate, reorder, filter and manipulate with the patterns, links and axes of the
representation (Figure 1 (c)).

4 Implementation
The backend takes the responsibility of the data preprocessing and
MTG model constructing. We store and calculate the data with 12
computing nodes. Each node drives 8 core and 20GB memory. We
employ the Apache Spark as the data processing engine which takes
about 1h to fulfill all preprocess steps. The model constructing is
faster which spends about 12m clustering the mobility patterns and
2m calculating the transition between them. The frontend runs on a
PC with 3.4GHz dual core, 16GB memory, which is implemented
in JAVA and uses Processing for rendering. Because the construction of MTGs is pre-computed, exploring and analyzing the mobility patterns and their transitions can be performed in real-time.

5 Case Study

pattern H

pattern F

(a)
time

(a)

low

5.1 Case 1: The General Exploration of The Mobility
Patterns

high

(b)

Figure 2: (a) The stacked flow view of a MTG. The color bar at the
right is used to color the links based on the transition likelihood;
(b) A bar chart overviews the MTG.

3.2

(a)

(b)

(c)

(d)

Figure 3: (a)(b): The patterns with high speed and entropy are
distributed along the highway and railway; (c)(d) The patterns with
low speed and entropy may distribute in the residential zone.

The Interface

Figure 1 illustrates the interface of our system. The map view and
the mobility pattern view are designed to display the corresponding
information of a selected mobility pattern. While the MTG view

The first case study is designed to study the general citywide mobility patterns. We select the records collected from Feb. 4th 2014 to
Feb. 10th 2014. By the mobility pattern view, we may investigate
the distribution of each mobility description and find the radius of

(g)

(d)

(a)

(b)

(e)

(i)
(f)

(c)
(h)

Figure 4: The mobility transition graph of case 2. The mobility pattern transition are highlighted and can be traced from (a) to (i).

gyration follows the power law which is consistent with the conclusion in [Gonzalez et al. 2008]. By selecting the patterns, the map
view shows their geographic representation. The patterns with high
speed and entropy are distributed along the highway and railway
(Figure 3 (a)(b)) while the low speed and entropy may distribute
in the residential zone (Figure 3 (c)(d)). Meanwhile, the MTG and
its overview exhibit a periodicity over time (Figure 1 (c)). That is,
there is routinely a peak from 0:00 a.m. to 2:00 a.m. of the transition likelihood. The transitions among these patterns are quite
stable from 00:00 am to 04:00 am. In the map view, these mobility patterns are slow and inactive. We deduce that these transition
patterns describe the people sleep in this time period.

5.2

Case 2: Visualizing The Interregional Movement

In this case, we show a movement pattern between two main districts selected by our system in Figure 4. Figure 4 (a) describes the
movement in the downtown from 8:00 a.m. to 10:00 a.m. Then
in the next 2 hour, this mobility pattern transfers to the pattern in
Figure 4 (b), which describes the movement from the downtown to
the development zone of this city. Next, this pattern splits into two patterns depicted in Figure 4 (c) and Figure 4 (d), respectively.
From 2:00 p.m. to 4:00 p.m. the pattern describes the movement between the downtown and the development zone appears again
and connects the previous pattern in Figure 4 (c). In this way, the
user may trace the transition of the mobility pattern and observe the
relevant information on the map view.

6

Conclusion

We present a novel visual representation that characterizes the statistical transitions of mobility patterns of crowd in daily life. The
implemented visualization scheme not only allows for intuitive understanding of mobility patterns, but also provides a mechanism for
studying the transition modes in a situation-aware fashion. We exemplify our approach with case studies on a real dataset and demonstrate the efficiency of our approach.

Acknowledgements
Supported by National 973 Program of China (2015CB352503), the
Fundamental Research Funds for the Central Universities.

References
BARABASI , A.-L. 2005. The origin of bursts and heavy tails in
human dynamics. Nature 435, 7039, 207–211.
M ONTJOYE , Y.-A., H IDALGO , C. A., V ERLEYSEN , M., AND
B LONDEL , V. D. 2013. Unique in the crowd: The privacy
bounds of human mobility. Scientific reports 3.

DE

G AO , J., G OODMAN , J., C AO , G., AND L I , H. 2002. Exploring asymmetric clustering for statistical language modeling. In
Proceedings of the Association for Computational Linguistics,
183–190.
G ONZALEZ , M. C., H IDALGO , C. A., AND BARABASI , A.-L.
2008. Understanding individual human mobility patterns. Nature
453, 7196, 779–782.
S CHNEIDER , C. M., B ELIK , V., C OURONN É , T., S MOREDA , Z.,
AND G ONZ ÁLEZ , M. C. 2013. Unravelling daily human mobility motifs. Journal of The Royal Society Interface 10, 84,
2013–2046.
S ONG , L., KOLAR , M., AND X ING , E. P. 2009. Time-varying
dynamic bayesian networks. In Advances in Neural Information
Processing Systems, 1732–1740.
S ONG , C., Q U , Z., B LUMM , N., AND BARAB ÁSI , A.-L. 2010.
Limits of predictability in human mobility. Science 327, 5968,
1018–1021.
WANG , Z., L U , M., Y UAN , X., Z HANG , J., AND V. D . W ETER ING , H. 2013. Visual traffic jam analysis based on trajectory
data. IEEE Transactions on Visualization and Computer Graphics 19, 12, 2159–2168.
X IONG , H., Z HANG , D., AND G AUTHIER , V. 2012. Predicting
mobile phone user locations by exploiting collective behavioral
patterns. In IEEE Conference on Ubiquitous Intelligence and
Computing, 164–171.
Z ENG , W., F U , C.-W., A RISONA , S. M., E RATH , A., AND Q U ,
H. 2014. Visualizing mobility of public transportation system.
IEEE Transactions on Visualization and Computer Graphics 20,
12.

Journal of Visual Languages and Computing 22 (2011) 268–278

Contents lists available at ScienceDirect

Journal of Visual Languages and Computing
journal homepage: www.elsevier.com/locate/jvlc

A pandemic inﬂuenza modeling and visualization tool$
Ross Maciejewski a,, Philip Livengood a, Stephen Rudolph a, Timothy F. Collins a,
David S. Ebert a, Robert T. Brigantic b, Courtney D. Corley b, George A. Muller b,
Stephen W. Sanders b
a
b

Purdue University Visualization and Analytics Center, United States
Paciﬁc Northwest National Laboratory, United States

a r t i c l e in f o

abstract

Available online 7 May 2011

The National Strategy for Pandemic Inﬂuenza outlines a plan for community response to
a potential pandemic. In this outline, state and local communities are charged with
enhancing their preparedness. In order to help public health ofﬁcials better understand
these charges, we have developed a visual analytics toolkit (PanViz) for analyzing the
effect of decision measures implemented during a simulated pandemic inﬂuenza
scenario. Spread vectors based on the point of origin and distance traveled over time
are calculated and the factors of age distribution and population density are taken into
effect. Healthcare ofﬁcials are able to explore the effects of the pandemic on the
population through a geographical spatiotemporal view, moving forward and backward
through time and inserting decision points at various days to determine the impact.
Linked statistical displays are also shown, providing county level summaries of data in
terms of the number of sick, hospitalized and dead as a result of the outbreak. Currently,
this tool has been deployed in Indiana State Department of Health planning and
preparedness exercises, and as an educational tool for demonstrating the impact of
social distancing strategies during the recent H1N1 (swine ﬂu) outbreak.
& 2011 Elsevier Ltd. All rights reserved.

Keywords:
Pandemic inﬂuenza
Visual analytics
Risk assessment
Geovisualization

1. Introduction
Federal, state, and local community public health ofﬁcials
must prepare and exercise complex plans to deal with a
variety of potential mass casualty events [13,16,22]. In
recent years, one of the most notable potential mass casualty
events that require appropriate planning is pandemic
inﬂuenza. However, ofﬁcials responsible for developing
such plans must often rely on information and trends
provided via very complex modeling (requiring supercomputers so that only a few cases can be considered due to
resource constraints) or, at the opposite extreme, modeling
that has incorporated very drastic simplifying assumptions

$
This paper has been recommended for acceptance by Gennady
Andrienko, Natalia Andrienko, Daniel Keim, Alan M. MacEachren and
Stefan Wrobel.
 Corresponding author.
E-mail address: rmacieje@purdue.edu (R. Maciejewski).

1045-926X/$ - see front matter & 2011 Elsevier Ltd. All rights reserved.
doi:10.1016/j.jvlc.2011.04.002

so as to be computationally practical. Moreover, such plans
are developed with only a few speciﬁc scenarios or preevent concepts in mind and often ignore the fact that the
solutions dealing with a pandemic are very dependent on its
underlying traits and actual characteristics, which cannot be
known with any certainty a priori. Thus, there is a critical
need to better equip public health ofﬁcials responsible for
pandemic inﬂuenza planning, or planning for other mass
casualty events, with sophisticated yet easy to use tools that
capture the complex elements, especially individual social
behaviors, of traumatic events and that can also adjust as
additional information is obtained and conditions evolve
over time.
While desktop pandemic inﬂuenza modeling tools do
exist (e.g., FluAid [1], FluSurge [12]), these tools are often
restrictive in their scope and provide little to no spatiotemporal support to allow users to observe conditions
evolving over time and space. To address this gap, visual
analytics has emerged as a relatively new ﬁeld formed at

R. Maciejewski et al. / Journal of Visual Languages and Computing 22 (2011) 268–278

the intersection of analytical reasoning and interactive
visual interfaces [38]. It is primarily concerned with
presenting large amounts of information in a comprehensive and interactive manner. By doing so, the end user is
able to quickly assess important data and, if required,
investigate points of interest in detail. As such, we have
developed a visual analytics toolkit (PanViz – Fig. 1) to aid
in the modeling, analysis and exploration of pandemic
inﬂuenza. Our interface utilizes linked views for displaying statistical information about populations under study,
ﬁltering controls for age and demographic data, and
detailed bed capacity information at the county level.
We provide end users with the means to interactively
explore the model, make parameter changes, and engage
in a variety of user created scenarios. As such, PanViz is
able to provide healthcare ofﬁcials with training and
education scenarios for a variety of pandemic situations.
Model parameters such as spread origin, mortality rate,
etc. are all modiﬁable through a graphical user interface
designed to support and enhance training exercises. Our
toolkit was most recently deployed as a portion of the
Indiana State Department of Health pandemic readiness
training exercises, and was utilized as an educational tool
for illustrating the potential impact of social distancing
measures during the recent H1N1 outbreak [39].
Furthermore, the U.S. National Strategy for Pandemic
Inﬂuenza [22] outlines three pillars of strategic intent: (1)
preparedness and communication; (2) surveillance and
detection; and (3) response and containment. PanViz is an
effective method of communicating information between
healthcare ofﬁcials, ﬁrst responders and the media, as
well as providing insight into the impact of various
responses and containment. The rationale for such a
system is that mass casualty event response planning
can be done with higher accuracy and more realism so
that if an event happens, mitigation strategies can be
invoked to minimize casualties as the event evolves. The

269

potential impact of such a system is a reduction in the
number of casualties associated with a mass casualty
event and the overall health of U.S. citizens. This can be
achieved via a mechanism to delineate decisions on
mitigative measures, especially as additional information
is gathered during the course of a pandemic or other mass
casualty event. Thus, public health ofﬁcials can use our
tool as an operational research to both plan and rapidly
assess health impacts, required community level
resources, and the effect of potential decision strategies
associated with a pandemic. The main thrust of our work
is not creating an advanced model of inﬂuenza; instead,
we provide a visual analytics environment in which users
may effectively explore decision points and potential
scenarios.
2. Related work
Previous work in pandemic inﬂuenza modeling has
focused on understanding the spread of the disease at the
micro-level (person-to-person contact) in order to help
ofﬁcials evaluate the effectiveness that their mitigation
policies might have. This includes analyzing how viruses
change and adapt [8], modeling the effects of immunization on inﬂuenza transmission [14], forecasting the economic impact of an inﬂuenza pandemic [31], and
evaluating the general preparedness of the emergency
response community [18,19]. Other research focuses on
containment and control of possible outbreaks [27–29].
Work by Guo focused down to the level of individual daily
movements, studying pandemic spread as a spatial interaction problem [20]. Inglesby et al. [23] present an overview of a variety of mitigation measures and discuss their
potential effectiveness. Nuno et al. [35] modeled the
effects of antivirals and community transmission controls
on the spread of inﬂuenza, and Simchi-Levi et al. [11]
modeled the transportation of antivirals as a supply chain

Fig. 1. PanViz – a visual analytics environment for the modeling and exploration of pandemic inﬂuenza. In this image, an outbreak which began in
Chicago, IL has quickly spread as a result of heavy air travel across major US airports.

270

R. Maciejewski et al. / Journal of Visual Languages and Computing 22 (2011) 268–278

problem demonstrating how current production risks will
lead to insufﬁcient vaccine supplies. Larson et al. [25]
studied how a population’s heterogeneity affects the
disease progression and analyzed how social distancing
could effectively reduce disease spread. Nigmatulina et al.
[34] utilized a series of spreadsheet models to analyze the
effects of infection spread between several linked heterogeneous communities in order to evaluate the use of nonpharmaceutical intervention strategies. Atkinson and
Wein modeled modes of inﬂuenza transmission routes
[3] and studied the efﬁcacy of other forms of control such
as face masks and ventilators [4].
In all of the above modeling work, it is clear that there
are a variety of strategies that can be implemented to
reduce the impact of a pandemic, if the strategies are
implemented early during the decision making process.
However, these models are typically self-contained, unavailable to health care ofﬁcials, or unlinked to any interactive visual interfaces. What is needed now are tools to
help health ofﬁcials prepare for an outbreak and disseminate information to the public in the event of a possible
pandemic situation. Various modeling tools do exist
which are meant to aid in pandemic preparation. FluAid
is provided by the United States Department of Health
and Human Services in order to assist state and local level
planners in preparing for the next inﬂuenza pandemic [1].
This software provides a range of estimates of impact in
terms of deaths, hospitalizations, and outpatients visits.
FluSurge is a spreadsheet-based model which provides
hospital administrators and public health ofﬁcials estimates of the surge in demand for hospital-based services
during the next inﬂuenza pandemic [12]. While these
simulation tools provide excellent statistical support to
planners, they are fairly restrictive in their scope, and do
not provide any spatiotemporal support. Work done by
Germann et al. [19] begins addressing this shortcoming.
They developed a simulation which allows various intervention strategies to be set and simulations to be run.
Results are displayed via a heat map displaying illness
attack rates over the entire nation, and charts of incidence
rates. However, this is a complex, large scale simulation
which requires a supercomputing platform in order to
run. User interaction is largely absent, and investigating
the effect of decision changes requires a re-running of the
entire simulation. In response, our application provides a
desktop pandemic modeling tool with interactive, spatiotemporal support. This allows users to observe conditions
evolving over time and space. Decision changes can be
made interactively, and results modeled immediately as
users simulate each time step.
In terms of creating complex mathematical models
based on population distributions, highway travel, and
spread vectors, much work is being done by the IBM
Eclipse STEM project [17]. This work focuses on helping
scientist and public health ofﬁcials create and use models
of emerging infectious diseases. It uses built-in Geographical Information System (GIS) data for almost every
country in the world, including travel routes for modeling
disease spread. The difﬁculty of this type of approach is
that these detailed models often require a great deal of
hand-crafting and ﬁne tuning. This work currently

provides an interface to Google Earth in which users can
visualize their data. With respect to our current work, our
tool provides a system in which a pandemic simulation
has been developed and is directly linked to an interactive
visualization tool, allowing for easier use by general
public health ofﬁcials at all level of government.
3. PanViz
Our pandemic inﬂuenza modeling and visualization
system (PanViz) adopts the common method of displaying geo-referenced data on a map and allowing users to
temporally scroll through their data. However, such
exploration only provides slices of spatial data at a given
time or an aggregate thereof. In order to understand these
slices, users need to know the trends of previous data
(and, if possible, model future data trends). Fig. 1 shows
our visual analytics system. Population, demographic
[40], and hospital bed [2] data are provided as input to
the back-end modeling functions. The modeling functions
output information on the number of sick, dead and
hospitalized individuals by county and PanViz provides
color coded geographical representations of the data.
Users may interact with the system through a variety of
viewing and modeling modalities. As shown in Fig. 1, the
main viewing area is the spatiotemporal view, and the
three windows on the right provide a time series view of
the population statistics (number of people sick, hospitalized or dead due to the modeled pandemic) of any county
selected (county selections are indicated by a darker
border) in the main viewing area. These linked views
allow for a quick comparison of trends across various
spatial regions. Both the geospatial and time series viewing windows are linked to the time control at the upper
left portion of the screen. This allows users to view the
spatial changes in the data as they scroll across time.
3.1. Epidemic model
The PanViz visualization framework uses a mathematical epidemic model to calculate population dynamics
and infection rate data. Speciﬁcally, disease dynamics are
calculated per county (z ¼92 counties in Indiana) by a
system of non-linear difference equations derived from
traditional compartment epidemic models with homogeneous population mixing. Model parameters are deﬁned
and equations for disease dynamics are presented in
Tables 1 and 2. Individuals in the population are assigned
to a compartment by disease state (susceptible (S), infectious/sick (I), recovered(R)). The population is demographically stratiﬁed by age into three groups: infant to 18,
18–64 and over 64 years old. Population numbers for each
age group are taken from the 2000 U.S. census.
We also track infection severity by tabulating those
hospitalized (H) and mortality as deceased (D) because of
infection by pandemic inﬂuenza. The mortality rate (km )
is the percentage infected that will ultimately die due to
complications from pandemic inﬂuenza with average
time to death of dm . Conversely, the recovery rate
(1km ) is the percentage of those infected with pandemic
inﬂuenza that will ultimately recover, with average time

R. Maciejewski et al. / Journal of Visual Languages and Computing 22 (2011) 268–278

271

Table 1
Pandemic inﬂuenza model.
Model parameters

Zy

Population of county y¼1 to z counties. Indiana: z ¼ 92

t

time index from the ﬁrst day of a disease outbreak (integer value)
mortality rate: percentage of those infected with pandemic inﬂuenza that will ultimately die
recovery rate: percentage of those infected with pandemic inﬂuenza that will ultimately recover
hospitalization rate: percentage of those infected with pandemic inﬂuenza that will ultimately require hospitalization
time, in days, to recover once infectious, at rate 1km (integer value)
time, in days, until death once infectious, at rate km (integer value)
time, in days, of hospitalization duration due to disease, at rate kh (integer value)
disease spread rate modiﬁer in county y, by county density c of l

km
1km

kh
dr
dm
dh

ry,c
c¼1
c¼2
c¼3

oj
fj
j ¼1
j ¼2
j ¼3

ck
kmod

dkstart
dkfull
k ¼1
k ¼2
k ¼3

y0
yc
iy,j,t
dy,j,t
ry,j,t
hy,j,t
Iy,j,t
Dy,j,t
Hy,j,t
Iy,t
Dy,t
Hy,t
Inﬂuenza dynamics
delay
p

y0
ck
yc

Rural: density o 100

people
mi2

Small towns: 100r densityr 250
Urban: density 4250

people
mi2

people
2

mi
Proportion of county population in the age group, j, of m age groups (m ¼3 in initial model setup)
disease prevalence modiﬁer for age group j

0 o age o 18
18 r age r 64
64 o age
Preventative measure reduction (%) in baseline prevalence due to decision measure, k, of n measures
Decision measure, k, prevalence modiﬁer (in %)
Time between the beginning of the outbreak until decision measure, k, is initiated, measured in days
Time until decision measure, k, reaches full efﬁcacy, measured in days
Decision measure: school closures
Decision measure: media alerts
Decision measure: strategic national stockpile deployment
Baseline prevalence (y0 ) derived from polynomial ﬁtting of epidemic data reported in [7]
Prevalence (yc ) after decision measures are implemented
incidence of infectious in county y, age group j, at time t
incidence of deceased in county y, age group j, at time t
incidence of recovered in county y, age group j, at time t
incidence of hospitalized y, age group j, at time t
Number of individuals who were infectious or are currently infectious in county y, age group j, at time t
Number of deceased individuals in county y, age group j, at time t
Number of individuals who have been hospitalized or are currently hospitalized in county y, age group j, at time t
Number of individuals who were infectious or are currently infectious in county y, at time t
Number of deceased individuals in county y, at time t
Number of individuals who have been hospitalized or are currently hospitalized in county y, at time t
distance ðmilesÞ between outbreak origin & county centroid
outbreak spread speed measured in miles per hour
ðtoutbreak peak daydelayÞ=epidemic curve spread [7]
1
4  peak amplitude 
[7]
p2
 10p
1
þ
10


t þ dkstart
kmod y0 min 1,
dkfull
P
y0  k2n ck

Table 2
Population dynamics.
iy,j,t
dy,j,t
ry,j,t
hy,j,t
Iy,j,t
Dy,j,t
Hy,j,t
Iy,t
Dy,t
Hy,t

Zy ry,c oj fj yc
iy,j,ðtdm Þ km
iy,j,ðtdr Þ ð1km Þ
iy,j,t kh
Pt

v ¼ 0 iy,j,v ðry,j,v þ dy,j,v Þ

Pt

v¼0

Pt

dy,j,v

v ¼ ðtdh Þ hy,j,v
P
Pt
v ¼ 0 sy,j,v
j2m
P
Pt
v ¼ 0 dy,j,v
j2m
P
Pt
v ¼ 0 hy,j,v
j2m

to recover of dr . Individuals are hospitalized at rate kh for
an average of dh days. In the model, the user is asked to
specify typical hospital capacity which is set to a default
value of 70% (research indicates that typical hospital
capacities may be as large as 80–90%). Hospital capacity
is the percentage of total beds in use out of the total beds
available.
We use the following terminology and deﬁnitions
throughout our models. The functional form used to
assign an individual’s probability of being infected with
pandemic inﬂuenza is listed in Table 1. This function is
parameterized with user entered approximations for the
center of the epidemic curve (default 70), the measure of
the spread of the epidemic curve (default 11) and the

272

R. Maciejewski et al. / Journal of Visual Languages and Computing 22 (2011) 268–278

peak amplitude of the epidemic curve (default 2.0%), see
[30] for details. The form of this curve is based on
epidemic curves experienced during the 1918 inﬂuenza
pandemic and presented by (though not necessarily
endorsed by) M. Cetron (DGMQ, CDC) which originated
with S. Barrett and MIDAS.
The baseline prevalence (y0 ) of an individual being
infected with pandemic inﬂuenza, no decision measures
yet implemented, can be approximated from the gross
attack rate (GAR). The percent gross attack rate (GAR) is the
percentage of the entire U.S. population that will have a
clinical case of inﬂuenza. GAR is closely related to the mean
number of secondary cases a typical single infected case
will cause in a population with no immunity to the disease
in the absence of interventions to control the infection
[37], called the basic reproduction number (R0). In the
initial setup and default values for the model, we did
assume a GAR of  30% and R0  2:0 as indicated. This
leads us to an analytic expression for a prevalence curve
(or the baseline probability mentioned above) that is used
to drive the model and compute the daily number of new
infected individuals as provided in Table 1. The speciﬁc
parameter values for this expression are deﬁned by
Brigantic et al. [7]. A rough way to calculate R0 for a simple
single population is to take the product of the attack rate
and the duration of infection (in this case the sum of the
incubation and shedding periods). The incubation period is
the time elapsed between exposure to a pathogenic organism and when symptoms and signs are ﬁrst apparent [37]
and the shedding period is the time that infected person
can expel virus particles from the body, important routes
include respiratory tract. These parameter values are based
on a literature review that was further vetted by subject
matter experts to arrive at appropriate values representative of pandemic inﬂuenza. These values are not necessarily speciﬁc to only air travel/airports, but are completely
appropriate for cities or small towns. Moreover, these
parameter values can be modiﬁed by the user of the PanViz
tool, either to mimic alternate assumptions for pandemic
inﬂuenza as desired and/or to model other potential
infectious disease (e.g., smallpox) as well.
The user speciﬁes coordinates and time of the ﬁrst
clinical pandemic inﬂuenza case in the state. These
coordinates might correspond to a major city center or
an airport, such as Indianapolis International Airport
(IND). The time (day) at which pandemic inﬂuenza enters
in a county (delay) is determined by the distance between
the index case and target county centroid, and the
approximate outbreak spread speed. Population density
effects contact rates and thus disease spread rate. Counties are labeled as one of three categories: rural (less than
100 people per square mile), small (100–250 people per
square mile), and urban (over 250 people per square
mile). The disease spread rate modiﬁer for population
density (c of three categories in county y) is ry,c . Moreover, the model allows for variability in prevalence in the
age groups, to account for population speciﬁc susceptibility or lack of immunity. The disease prevalence modiﬁer due to age stratiﬁcation (age group j) is given by the
parameter fj . A future version of the tool will include
additional options to establish associated impact

parameters (e.g., hospitalization rates) by demographic
group as found in the literature, such as provided by
Meltzer et al. [31].
In the model, the user can evaluate diverse what-if
scenarios for a 60-day period by varying decision measure
k’s efﬁcacy (kmod), where the decision measures are school
closures, media alerts, and strategic national stockpile
deployment. Speciﬁcally, the modiﬁcation to pandemic inﬂuenza prevalence due to a decision measure (ck ) is dependent
on the baseline prevalence (y0 ), decision measure efﬁcacy
(kmod), time the measure is implemented (dkstart ) and the time
at which the decision measure is fully effective (dkfull ). The
resulting pandemic inﬂuenza prevalence (yc ) is the baseline
prevalence (y0 ) minus the sum of all prevalence modiﬁcaP
tions due to decision measures ( k2n ck ).
Disease dynamics are evaluated by combining the user
supplied values for county demographics, population density, mortality and recovery rate, hospitalization rate, baseline and modiﬁed pandemic inﬂuenza prevalence. The
number of infectious/sick in county y, age group j and at
time t (iy,j,t ) is the product of the total county population
(Zy ), county density modiﬁer (ry,c ), proportion of population in target age group (oj ), age group disease modiﬁer
(fj ) and the decision measure modiﬁed pandemic inﬂuenza prevalence (yc ). The number of deaths due to pandemic inﬂuenza in county y, age group j, at time t (dy,j,t ) is
the product of the mortality rate (km ) and the number of
sick at time t  dm . The number of individuals recovered
from pandemic inﬂuenza in county y, age group y, at time t
(ry,j,t ) is the product of the recovery rate (1km ) and the
number of sick at time tdr . The number of hospitalizations due to pandemic inﬂuenza in county y, age group j, at
time t (hy,j,t ) is the product of the hospitalization rate (kh )
and the number of sick at time t. The total number of
individuals who have become sick due to pandemic inﬂuenza in county y, age group j, and at time t (Iy,j,t ) is the
sum of the sick minus the recovered and deceased
each day, from the start of the pandemic to time t
P
( tv ¼ 0 iy,j,v ðry,j,v þ dy,j,v Þ). The total sick population in
county y at time t (Iy,t ) is the sum of the sick per age
P
P
group in the county ( j2m tv ¼ 0 iy,j,v ). The total number of
deceased, recovered, and hospitalized are calculated similarly, the exact equations are listed in Table 1.
Default parameters to our model are based on information from the U.S. National Strategic Plan [22]. In this
plan, states are charged with the task of preparing for a
pandemic inﬂuenza wave under the prediction that up to
35% of the population could be infected, 50% of the
infected population will seek medical care, 20% of those
seeking care will require hospitalization, and up to 2% of
the infected population will die. These numbers are based
on rates from the 1918 inﬂuenza pandemic [5,14]. Unless
otherwise speciﬁed, all images generated in this document use the default parameters found in Table 3.
3.2. Pandemic inﬂuenza visualization
The PanViz toolkit makes use of a person-to-person
contact model spread with a constant rate of diffusion in
order to simulate a spatiotemporal outbreak. The model
employed by PanViz was designed to determine the

R. Maciejewski et al. / Journal of Visual Languages and Computing 22 (2011) 268–278

number of inﬂuenza outbreak infections, hospitalizations,
and deaths on a daily basis. As input, it requires the
pandemic inﬂuenza characteristics, county data such as
population, demographics [40], and hospital beds [2], and
potential decision measures like closing schools. Spread
vectors based on the point of origin and distance traveled
per day are calculated, and effects on different age groups
and population densities are taken into account.
Fig. 2 (Left) illustrates the infection probability model
utilized by PanViz. In this case, users may vary the
magnitude of the pandemic through a simple graphical
user interface, Fig. 2 (Right). Users can directly control the
Table 3
Default parameter settings.

km ¼ 0:02
ry,1 ¼ 0:8
ry,2 ¼ 1:0
ry,3 ¼ 1:2

kh ¼ 0:30
f1 ¼ 1:1
f2 ¼ 1:0
f3 ¼ 0:8

dr ¼ 10
c1 ¼ 0:1
d1start ¼ 2
d1full ¼ 2

Outbreak origin ¼ ð41:879536,87:624333Þ
Outbreak speed ¼ 25.00

dm ¼ 6
c2 ¼ 0:15
d2start ¼ 4
d2full ¼ 5

dh ¼ 6
c3 ¼ 0:25
d3start ¼ 6
d3full ¼ 7

273

mortality and infection rates, allowing for the creation of
multiple scenarios and the ability to adapt this model to
various ranges of pandemics. As the pandemic spreads over
time, the peak wave hits various counties at different days
as shown in Fig. 2 (Middle). In this case, the left curve is for
a higher population density county that is also closer to the
origin county than the right curve. Under the person-toperson contact model, the pandemic spreads diffusely from
a single point source location at a constant rate. Fig. 3
shows the effects of modifying the spread origin. Previous
work in disease modeling has looked at other means of
spread vectors such as generation interval [24] in which a
transmission delay is introduced between the host and
those agents the host infects, and at the transmission of
severe acute respiratory syndrome in household contacts
[36]. In order to more realistically demonstrate the speed
with which inﬂuenza can travel, we have also included
travel between the 15 largest airports as part of the model.
For a given location, the amount of time required for it to
be affected is now determined by the minimum of the
distance either from that point to the pandemic origin, or
from that point to the nearest airport plus the distance

Fig. 2. This ﬁgure (Left) illustrates the probability of infection for a variety of attack scenarios and (Middle) the impact that the spread factor and
population density (which is controllable in the user interface) has on the time of the peak infection based on distance from the source. Note the lag
between the two curves and the difference in magnitude. The smaller magnitude curve is due to a more rural population. (Right) shows the user interface
for modifying the infection curve magnitude and duration parameters.

Fig. 3. This ﬁgure (Left) showsDay 20 of a spread originating in Chicago, IL and (Middle) shows Day 20 of a spread originating in Indianapolis, IN. (Right)
shows the user interface for modifying the spread center and rate.

274

R. Maciejewski et al. / Journal of Visual Languages and Computing 22 (2011) 268–278

from the pandemic origin to the airport closest to the
pandemic origin. Once the disease reaches the nearest
airport, it will begin spreading to all other cities with
airport hubs on the subsequent day. Fig. 4 illustrates the
difference between a single point source spread and the
utilization of air travel routes for spread. Future work will
focus on better parameterizing the airport spread models
based on typical hub transportation.
Along with modeling the spread from a given point of
origin, our model also allows users to input an estimate of the
number of days a person will remain sick, how many days a
hospitalized person will remain in the hospital, and, if a
person is going to die from the pandemic, how many days it
will take the person to succumb. Fig. 5 provides a quick
overview of the a simulated pandemic in a single county in
Indiana. Here, we can observe the number of sick, hospitalized and dead individuals and note the lag between the sick
and dead curves due to the user speciﬁed parameter. Again,
many inﬂuenza models have been tested, from looking at the

transmissibility of swine ﬂu at Fort Dix in 1976 [26], to
simulating pandemic inﬂuenza in San Antonio, Texas [32].
Our system enhances these modeling capabilities by allowing
users to interactively adjust parameters and then visualize
the result in an interactive environment.
Our system also allows for interactive ﬁltering based
on population demographics. Fig. 6 shows the number of
people affected by the pandemic as a percentage of their
given age range. Here we can observe which counties are
hit the hardest for a given population. Furthermore, users
may also modify the infection probability model of the
pandemic based on the age ranges and population density
of a county. In our system, we classify ages into three
ranges (under 18, 18–65, and 65 plus) and counties into
three ranges (rural, small town or major metropolitan).
Users may interactively adjust the model parameters to
deﬁne a magniﬁcation factor which will increase/decrease
the probability of infection for a given age and/or county
type. The impact of this can be seen in Fig. 2 (Middle). Note

Fig. 4. Modeling a pandemic spread originating in Chicago, IL. (Left) The effects of an outbreak after 40 days using a single source point spread model.
(Right) The effects of an outbreak after 40 days including air travel between the 15 largest United States airports.

Fig. 5. This ﬁgure shows our model of patients who have become ill, need hospitalization, or have died from the pandemic. Note the lag in deaths from
time of infection as speciﬁed by the user.

Fig. 6. This ﬁgure shows the use of our ﬁltering tools to analyze the population of ill patients for a given age range on Day 25 of a pandemic originating in
Chicago, IL.

R. Maciejewski et al. / Journal of Visual Languages and Computing 22 (2011) 268–278

that each curve in that image represents a county; however,
the magnitude of the pandemic is less in one county as
compared to the other. This is due to the effects of modeling
counties as different types. A similar result would be
achieved by modifying the age parameters. Note that
studies have been done on the distribution of inﬂuenza
vaccine to high-risk groups (e.g., [28]), and future work will
incorporate these factors into a more robust parameter set.
3.3. Decision measures
Within our modeling tool, we also account for various
decision measures. These decision measures were decided
on based on requirements from the Indiana State Department of Health in order to best accommodate their
training exercises. In our system, we focus on three
decision measures: (1) school closures; (2) media alerts;
and (2) strategic national stockpile deployment.
The choice of these decision measures is also inﬂuenced
by previous work. Historical records of past pandemics
illustrate the efﬁcacy of social distancing with regards to
lessening the impact of a pandemic [6,21]. Furthermore,
other researchers have noted the expected reduction of
inﬂuenza transmission based on school closures [9] or
quarantines [15], and the effects of containing pandemic
inﬂuenza through the use of antiviral agents and stockpiles

275

have been well documented [27,29]. However, other work
suggests that for multiple outbreak sites, the idea of
quarantines will prove ineffectual [33]. Detailed descriptions of the effects of various decision measure strategies
can also be found in [19,32], along with others.
Fig. 7 shows how an user can simply toggle on and off
decision points within PanViz to see their effects on the
pandemic impact.Fig. 7 (Left) shows the model on Day 40
with no decision measures employed. Using the controls
on the lower left portion of the screen, the analyst chooses
to deploy the strategic national stockpile (SNS) antivirals.
The control widget shown in Fig. 9 allows the user to set

Fig. 9. This ﬁgure shows the interactive widget for modifying the
decision measure impacts on the probability of infection model.

Fig. 7. Here we illustrate the effects of utilizing decision measures within the conﬁnes of PanViz. In the left image, the analyst has used no decision
measures. In the right image, the analyst has decided to see what effects deploying the strategic national stockpile on Day 3 would have had on the
pandemic.

Fig. 8. Here we illustrate the potential impact that a pandemic may have on the available health care facilities. In this case, each county is assumed to
have 70% of all beds ﬁlled in a hospital on a given day. On Day 1 of the simulated pandemic, it is projected that Hamilton County will required 32
additional hospital beds over its baseline capacity usage to support the pandemic. By Day 10, Hamilton County has 762 patients needing hospitalization;
however, the county resources are approximately 144 beds.

276

R. Maciejewski et al. / Journal of Visual Languages and Computing 22 (2011) 268–278

the day of the simulation on which the decision measure
was enacted, the number of days it will take the decision
measure to reach full effect, and the impact the decision
measure is expected to have in reducing the infection. In
the graphs of Fig. 7 (Right), the user can immediately see
how the use of the (SNS) has helped mitigate the magnitude of the pandemic. Through these controls, the user
can interactively toggle decision points on and off and
explore the effects that decisions taking place in the past
would have on the current situation. Interactive toggling
allows the user to understand the magnitude of the
change by watching both the graphs and map display
colors change for a given day as decision measures are
implemented. Future work will include the use of more
advanced decision measures and allow for both local and
national measures. Note that this software is available
online at http://pixel.ecn.purdue.edu:8080/rmacieje/Pan
Viz/ and can be freely downloaded for experimentation.
4. Pandemic preparedness exercises
The main thrust of our work is to provide a means for
enhancing pandemic preparedness exercises and providing
tools for public education through easy to understand
visuals. In 2008, the Indiana State Department of Health
tasked its 10 districts to increase their level of preparedness and response through a series of functional exercises
designed to test their readiness for a pandemic inﬂuenza.
Here it was noted that we would not have a vaccine during
the ﬁrst wave of the pandemic [41] and that antivirals
would be insufﬁcient in supply and potential ineffective
[22]. Hospitals would be overwhelmed and the public
health community would be urging home care. In the
absence of pharmaceutical measures, the general populace
will need to rely on infection control measures (school
closures and enhanced hygiene practices). As part of these
functional exercises, four objectives were identiﬁed:
1. Participants will determine the ability of their County
Emergency Operations Center to establish and implement an order of command succession during an
inﬂuenza pandemic.
2. Participants will utilize their existing plans, policies
and procedures to develop, coordinate, disseminate
and manage public information during an inﬂuenza
pandemic.
3. Participants will utilize their existing plans, policies and
procedures to manage Strategic National Stockpile (SNS)
Pandemic Countermeasures including receipt, storage,
security, distribution, dispensing and monitoring.
4. Participants will determine existing medical surge
capacity within their county and identify alternate
care site needs during an inﬂuenza pandemic.
As a portion of these objectives, the PanViz tool kit
was utilized as a means of providing situational awareness during the functional injects. The functional exercise assumed a 30% attack rate with a 2% mortality rate
with the point of origin of the outbreak being Chicago,
Illinois. Participants were able to input decision

measures (such as when to deploy their SNS countermeasures) and observe the impact of their decisions. All
scenarios utilized the default parameter settings documented in Table 3.
This tool was utilized as a demonstration of decisions
taken during the tabletop exercise. Participants were able
to provide input to the model as part of a web seminar. A
single controller then modiﬁed the input parameters to the
tool, and the resultant changes were visualized and shown
within the webinar. PanViz was able to actively engage
participants in discussions on issues with the medical
surge capacity. Fig. 8 was used as an educational component of the functional exercises to illustrate the importance
of advanced surge capacity plans. In Fig. 8 the number of
available hospital beds (as noted in the Emergency Preparedness Atlas: U.S. Nursing Home and Hospital Facilities
[2]) is displayed for each county. In our model, it is
assumed that 70% of all beds are full due to general
medical needs. As an example, on Day 1 of the pandemic,
our model estimates that Hamilton County will need 32 of
its 144 beds for patients as a direct result of the pandemic
inﬂuenza. By Day 10, Hamilton County will need 762 of its
144 beds for patients as a direct result of the pandemic.
One can quickly observe (by color) that all counties across
the state have quickly reached their bed capacity. These
striking visuals created wide spread discussion amongst
participants and provided greater gravitas for the exercises.
5. Public awareness and education
More recently, PanViz has been used as a means of
providing educational information about the impact of
implementing social distancing measures during the recent
H1N1 outbreak. Utilizing attack and mortality rates similar
to the 1918 pandemic, we created a series of graphics
illustrating the impact that social distancing could have on
reducing the pandemic’s magnitude. Fig. 10 illustrates the
spread of the pandemic when no decision measures are
employed with that of the spread of the pandemic when
social distancing and vaccinations have been employed
early in the outbreak stages. Note the signiﬁcant reduction
of the magnitude of the outbreak. These educational
materials were distributed through Purdue University’s
pandemic education website and details were also
reported on by the United Press International [39].
In a situation similar to the recent H1N1 outbreak, PanViz
could be deployed as an operational research tool in which
ofﬁcials could input the current known attack and mortality
rates of the given pandemic. As data comes in, analysts can
quickly adjust model parameters and settings within the
PanViz framework in order to gain a rough prediction of the
potential magnitude and spread. In this way, PanViz can
provide ofﬁcials with a means of communicating information
amongst agencies, and providing public service announcements similar to our current press release.
6. Conclusions and future work
The interactive approach and ease of use of our visualization modeling methodology makes complex modeling
and simulation tools available directly to public health

R. Maciejewski et al. / Journal of Visual Languages and Computing 22 (2011) 268–278

277

Fig. 10. Here we illustrate the potential impact that social distancing and early vaccination could have on magnitude of a pandemic inﬂuenza. For Days
19 and 37 we present a comparison of the effects of a pandemic when no social distancing or vaccinations have been employed (the left map for each
day) with the effect of an application of social distancing and vaccinations (the right map for each day). One can immediately see that the magnitude of
the pandemic is substantially lessened.

ofﬁcials and decision makers for their own use. Moreover,
these tools and techniques have the potential to be
updated in near real-time as actual data and observations
are made during the course of a pandemic or epidemic
such as the current Swine Flu outbreak that ﬁrst appeared
in April, 2009 [10]. In future work, we intend to reﬁne the
underlying modeling algorithms to be more sophisticated
and accurate via detailed simulations and agent based
modeling driven by basic input parameters from the user.
These simulations can run underneath the top level model
structure via a simple button click and can be transparent
to the user, but returned results will have greater robustness increasing their power and overall effectiveness.
Furthermore, we plan to incorporate more advanced
temporal and spatiotemporal analytics tools into future
versions of the framework. Currently, the model does
allow users to scroll through time as well as adjust the
timing of different mitigation measures and the time it
takes for these to reach full effect. Our plan is to include
side-by-side temporal comparison and/or potentially
include difference map views so that users can better
ascertain temporal differences.
Our partners at the Indiana State Department of Health
have shown immense interest in expanding their use of
this tool, and current steps are underway to deploy this to
all 92 county health ofﬁcials in Indiana. While our tool’s
use cannot be directly quantiﬁed in terms of its impact in
raising Indiana’s preparedness rating, our contribution
was a major component of the training and preparedness
exercise program. Furthermore, the educational value of
easy to understand visuals as a means for conveying
information to the public cannot be overstated. As such,
our PanViz tool provides an easy to use interface for both
the modeling and exploration of pandemics for use in
both training and operational research. We plan to further
pursue our collaborations to port this into a fully functional emergency response tool where more detailed
critical tasks can be solved.

Acknowledgments
This project was conducted by Purdue University
under contract with the Indiana State Department of

Health and was supported by Grant Award No.
5U90TP517024-08 from the Centers for Disease Control
& Prevention (CDC) as well as the U.S. Department of
Homeland Security’s VACCINE Center under Award Number 2009-ST-061-CI0001. Its contents are solely the
responsibility of the authors and do not necessarily
represent the ofﬁcial views of the CDC.
References
[1] Availability of inﬂuenza pandemic preparedness planning FluAid,
2.0, Journal of the American Medical Association 284 (14) (2000)
1782.
[2] Agency for Healthcare Research and Quality, Emergency preparedness atlas: U.S. nursing home and hospital facilities, AHRQ Publication No. 07-0029-2, April 2007.
[3] M.P. Atkinson, L.M. Wein, Quantifying the routes of transmission
for pandemic inﬂuenza, Bulletin of Mathematical Biology 70 (2008)
820–867.
[4] M.P. Atkinson, L.M. Wein, Assessing infection control measures for
pandemic inﬂuenza, Risk Analysis 29 (2009) 949–962.
[5] M. Billings, The Inﬂuenza Pandemic of 1918, 1997.
[6] M.C. Bootsma, N.M. Ferguson, The effect of public health measures
on the 1918 inﬂuenza pandemic in US cities, Proceedings of the
National Academy of Science 104 (18) (2007) 7588–7593.
[7] R. Brigantic, J. Malone, G. Muller, R. Lee, J. Kulesz, W. Delp,
B. McMahon, Simulation to assess the efﬁcacy of U.S. airport entry
screening of passengers for pandemic inﬂuenza, International Journal of Risk Assessment & Management 12 (2–4) (2009) 290–310.
[8] R. Bush, C. Bender, K. Subbarao, N. Cox, W. Fitch, Predicting the
evolution of human inﬂuenza A, Science 286 (5446) (1999)
1921–1925.
[9] S. Cauchemez, A.J. Valleron, P.Y. Boëlle, A. Flahault, N.M. Ferguson,
Estimating the impact of school closure on inﬂuenza transmission
from sentinel data, Nature 452 (7188) (2008) 750–754.
[10] Centers for Disease Control and Prevention, H1N1 Flu (Swine Flu),
May 2009.
[11] S.E. Chick, H. Mamani, D. Simchi-Levi, Supply chain coordination
and inﬂuenza vaccination, Operations Research 56 (6) (2008)
1493–1506.
[12] M.D. Christian, D. Kollek, B. Schwartz, Emergency preparedness:
what every healthcare worker needs to know, Canadian Journal of
Emergency Medicine 7 (5) (2005) 330–337.
[13] D. Dausey, J. Aledort, N. Lurie, Tabletop exercises for pandemic
inﬂuenza preparedness in local public health agencies, TR-319DHHS, prepared for the U.S. Department of Health and Human
Services Ofﬁce of the Assistant Secretary for Public Health Emergency Preparedness, 2005.
[14] L.R. Elveback, J.P. Fox, E. Ackerman, A. Langworthy, M. Boyd,
L. Gatewood, An inﬂuenza simulation model for immunization
studies, American Journal of Epidemiology 103 (2) (1976) 152–165.
[15] N.M. Ferguson, D.A. Cummings, C. Fraser, J.C. Cajka, P.C. Cooley,
D.S. Burke, Strategies for mitigating an inﬂuenza pandemic, Nature
442 (7101) (2006) 448–452.

278

R. Maciejewski et al. / Journal of Visual Languages and Computing 22 (2011) 268–278

[16] A.C. for Disease Control and Prevention, Community strategy for
pandemic inﬂuenza mitigation in the United States—early, targeted, layered use of nonpharmaceutical interventions, Centers for
Disease Control and Prevention, Atlanta, 2007.
[17] D.A. Ford, J.H. Kaufman, I. Eiron, An extensible spatial and temporal
epidemiological modeling system, International Journal of Health
Geographics 5 (4) (2006).
[18] K.F. Gensheimer, M.I. Meltzer, A.S. Postema, R. Strikas, Inﬂuenza
pandemic preparedness, Emerging Infectious Diseases 9 (12)
(2003) 1645–1648.
[19] T.C. Germann, K. Kadau, I.M. Longini, C.A. Macken, Mitigation
strategies for pandemic inﬂuenza in the United States, Proceedings
of the National Academy of Sciences 103 (15) (2006) 5935–5940.
[20] D. Guo, Visual analytics of spatial interaction patterns for pandemic
decision support, International Journal of Geographical Information
Science 21 (8) (2007) 859–878.
[21] R.J. Hatchett, C.E. Mecher, M. Lipsitch, Public health interventions
and epidemic intensity during the 1918 inﬂuenza pandemic,
Proceedings of the National Academy of Sciences 104 (18) (2007)
7582–7587.
[22] Homeland Security Council, National strategy for pandemic inﬂuenza, The White House website, November 2005.
[23] T.V. Inglesby, J.B. Nuzzo, D.A. Henderson, Disease mitigation measures in the control of pandemic inﬂuenza, Biosecurity and Bioterrorism 4 (4) (2006) 366–375.
[24] E. Kenah, M. Lipsitch, J. Robins, Generation interval contraction and
epidemic data analysis, Mathematical Biosciences 213 (1) (2008)
71–79.
[25] R.C. Larson, Simple models of inﬂuenza progression within a heterogeneous population, Operations Research 55 (3) (2007) 339–412.
[26] J. Lessler, D.A. Cummings, S. Fishman, A. Vora, D.S. Burke, Transmissibility of swine ﬂu at Fort Dix, 1976, Journal of The Royal
Society Interface 4 (15) (2007) 755.
[27] M. Lipsitch, T. Cohen, M. Murray, B.R. Levin, Antiviral resistance and
the control of pandemic inﬂuenza, PLoS Medicine 4 (1) (2007) 111.
[28] I.M. Longini, M.E. Halloran, Strategy for distribution of inﬂuenza
vaccine to high-risk groups and children, American Journal of
Epidemiology 161 (4) (2005) 303–306.

[29] I.M. Longini, M.E. Halloran, A. Nizam, Y. Yang, Containing pandemic
inﬂuenza with antiviral agents, American Journal of Epidemiology
159 (7) (2004) 623–633.
[30] J. Malone, R. Brigantic, G. Muller, A. Gadgil, W. Delp, B. McMahon,
R. Lee, J. Kulesz, F. Mihelic, U.S. airport entry screening in response
to pandemic inﬂuenza: modeling and analysis, Travel Medicine and
Infectious Disease 7 (4) (2009) 181–191.
[31] M. Meltzer, N. Cox, K. Fukuda, The economic impact of pandemic
inﬂuenza in the United States: priorities for intervention, Emerging
Infectious Diseases 5 (5) (1999) 659–671.
[32] G. Miller, S. Randolph, J. Patterson, Responding to simulated
pandemic inﬂuenza in San Antonio, Texas, Infection Control and
Hospital Epidemiology 29 (4) (2008) 320–326.
[33] C.E. Mills, J.M. Robins, C.T. Bergstrom, M. Lipsitch, Pandemic
inﬂuenza: risk of multiple introductions and the need to prepare
for them, PLoS Medicine 3 (6) (2006).
[34] K.R. Nigmatulina, R.C. Larson, Living with inﬂuenza: impacts of
government imposed and voluntarily selected interventions, European Journal of Operational Research 195 (2) (2009) 613–627.
[35] M. Nuno, G. Chowell, A.B. Gumel, Assessing the role of basic control
measures, antivirals and vaccine in curtailing pandemic inﬂuenza:
scenarios for the US, UK and the Netherlands, Journal of the Royal
Society Interface 4 (14) (2007) 505–521.
[36] I.R. Douglas Scott, E. Gregg, M.I. Meltzer, Collecting data to assess
SARS interventions, Emerging Infectious Diseases, May 2009.
[37] R. Scott, E. Gregg, M. Meltzer, Collecting data to assess SARS
interventions, Emerging Infectious Diseases 10 (7) (2004)
1290–1292.
[38] J.J. Thomas, K.A. Cook (Eds.), Illuminating the Path: The R&D
Agenda for Visual Analytics, IEEE Press, 2005.
[39] United Press International, Program simulates spread of pandemic
ﬂu, UPI.com Science News, 2009.
[40] United States Census Bureau, Population demographics, 2000.
[41] US Department of Health and Human Services, HHS pandemic
inﬂuenza plan, US Department of Health and Human Services,
Washington, DC, November 2005.

A Visual Analysis System for Metabolomics Data
Ross Maciejewski†

Philip Livengood∗

Wei Chen‡

David S. Ebert§

Purdue University Visualization and Analytics Cente∗
Arizona State University†
State Key Lab of CAD&CG, Zhejiang University‡

A BSTRACT
When analyzing metabolomics data, cancer care researchers are
searching for differences between known healthy samples and
unhealthy samples. By analyzing and understanding these differences, researchers hope to identify cancer biomarkers. In
this work we present a novel system that enables interactive
comparative visualization and analysis of metabolomics data obtained by two-dimensional gas chromatography-mass spectrometry (GCxGC-MS). Our system allows the user to produce, and
interactively explore, visualizations of multiple GCxGC-MS data
sets, thereby allowing a user to discover differences and features
in real time. Our system provides statistical support in the form of
mean and standard deviation calculations to aid users in identifying meaningful differences between sample groups. We combine
these with multiform, linked visualizations in order to provide researchers with a powerful new tool for GCxGC-MS exploration and
bio-marker discovery.
Keywords: Metabolomics, visual analysis, TIC, GCXGC-MS.
1

I NTRODUCTION

In recent years, GCxGC-MS has become an invaluable laboratory
analysis tool. However, this procedure produces large (gigabytes
of data per sample), four dimensional datasets (retention time one,
retention time two, mass and intensity). Such data is cumbersome,
and researchers must spend time formatting and processing the data
in order to remove acquisition artifacts, and quantify and identify
chemical compounds [9]. Furthermore, while statistical analysis
has played an important role in this work (because of the need to
reduce the thousands of acquired spectral features to a more manageable size), the large data size, inherent biological variability and
measurement noise makes the identification of bio-markers through
purely statistical processes extremely difficult and time consuming.
As such, our work focuses on the development of a visual analysis suite for exploring differences across samples and groups of
samples. In this work, we have created a system that utilizes a series
of multiform linked visualizations to enable interactive exploration,
filtering and comparison of multiple samples simultaneously. This
allows users to quickly locate feature similarities and differences
across samples and drill down into the mass spectrum data for detailed analysis. These techniques allow researchers to form and explore hypotheses about sample alignment, acquisition artifacts, and
(most importantly) cancer bio-marker sites. For example, Figure 1
(top) illustrates a cancerous and non-cancerous data sample visualized with total ion count (TIC) images. In this example, differences
between the two samples are not obvious, and, once differences are
∗ e-mail:

plivengo@purdue.edu
rmacieje@asu.edu
‡ e-mail:chenwei@cad.zju.edu
§ e-mail:chenwei@cad.zju.edu
† e-mail:

IEEE Symposium on Biological Data Visualization
October 23 - 24, Providence, Rhode Island, USA
978-1-4673-0004-9/11/$26.00 ©2011 IEEE

found, it can be difficult to determine whether or not those differences are meaningful. By using our tools, researchers are able to
quickly identify and explore sample differences as seen in Figure 1
(bottom).
Our work is being developed in collaboration with analytical
chemists and biology researchers, and is designed to provide an
interactive comparative visual analysis environment for GCxGCMS data exploration. While this work is similar to previous work
[1, 2, 7] in that we provide linked views that support the interactive
visual exploration of mass spectrometry data, our system provides
several features not currently available in other GCxGC-MS analysis systems:
1. a comparative visualization window that allows multiple samples (and multiple views of individual samples) to be displayed simultaneously,
2. data exploration tools for exploring mass spectra and filtering
and comparing TIC images in real-time,
3. grouping of samples and calculation of group means for comparison and difference calculation,
4. the application of mean and standard deviation TICs to the
color-mapping of difference measures,
5. a dynamic color scale adaptation tool for discovering differences in low-intensity peaks.
As automated analysis tools take days to run, our system serves
as a front end in order to make the calculation tractable by specifying a region for evaluation. With our tool, users may visualize
multiple samples of GCxGC-MS data, interactively search for sample differences, probe areas of interest to bring up mass spectrum
plots, and compare regions where the mass spectra deviate the most.
These features provide valuable data analysis for bio-marker discovery, and make an excellent complement to existing workflows.
2 GC X GC-MS
In cancer care engineering, researchers collect GCxGC-MS data in
order to search for biomarker differences between cancerous and
non-cancerous samples. In GCxGC-MS, a sample to be analyzed is
first mixed with a carrier gas that transports it through the machine.
The device has two columns through which a sample passes before
being analyzed by a mass spectrometer. Different components of
the sample move through the columns at different rates, resulting
in two levels of separation according to how long it takes each of
the components to move through each of the columns. The mass
spectrometer gives an additional level of separation as the mixture
is ionized when it exits the second column. This process results in
a four-dimensional dataset with two time axes (retention time one
(RT1) from the first column and retention time two (RT2) from the
second column), mass, and intensity.
Up to this point we have referred to mass as one of the dimensions of data produced by the mass spectrometer. More correctly,
we should be referring to this as the mass-to-charge ratio, designated m/z. A mass spectrometer breaks molecules up into ionized

71

Figure 2: A color mapped height field (left). Using the high contrast
option for height field rendering (right), small peaks and background
noise, which might otherwise be hidden, are easily seen.

Figure 1: GCxGC-MS data sets rendered as 2D TIC images (top).
Our system provides visual analytic tools to help researchers identify meaningful differences (bottom). Here we show a canine cancer
sample (left) compared with a healthy canine sample (right).

fragments. It is then able to determine the mass-to-charge ratio by
using the fact that two particles with the same m/z will move in the
same path in a vacuum when subjected to the same electrical and
magnetic fields. A particular chemical compound will have a very
specific mass spectral signature. The spectral signature of unknown
compounds can be compared to a library of known compounds to
help identify the compound or its composition. This type of identification can be difficult because different chemical compounds may
have the same retention times, and thus their spectral peaks may
have significant overlap, a problem known as co-elution.
GCxGC-MS is marked by its large peak capacity, an order of
magnitude increase in chemical separation ability [1], and its improved speed [3]. However, the data obtained exhibits several complexities.Samples must be mixed with a carrier gas as part of the acquisition process. This carrier gas shows up in the resulting data set,
and can obscure sample peaks of interest. Inconsistencies in sample
amounts will lead to differences in peak intensities. Peak retention
times and shapes exhibit slight differences that are uncontrollable,
yet are unrelated to actual chemical differences in the samples [1].
Data samples exhibit background noise that can vary from sample
to sample, and make accurate peak quantification difficult. Finally,
despite the improved separation ability over one-dimensional GCMS, GCxGC-MS still exhibits some peak overlap due to coelution.
Despite these difficulties, GCxGC-MS is used in a wide array of
applications, such as quality control, chemical identification, and
biomarker detection. As a result, much research has been done to
determine effective ways to analyze the large amounts of data produced and overcome the previously mentioned pitfalls. Alignment
algorithms [14], background removal algorithms [8], and compound identification [10] are common areas of study. Other researchers have worked on identifying potentially significant compounds, such as the orthogonal partial least-square (OPLS) approach used by Wiklund et al. [13].
3 R ELATED W ORK
The thrust of this work is the development and deployment of an
interactive visual analysis tool for analyzing data captured through
the GCxGC-MS process. Kincaid and Dejgaard [6] developed a

72

system to explore protein complexes in tandem mass spectrometry
data through modifications to a scatter plot visualization. Jourdan et
al. [5] also utilized scatterplot matrices and parallel coordinate plots
as a means of exploring metabonomics data. Work by de Corral and
Pfister [2] presented a system for the three-dimensional visualization of Liquid Chromatography - Mass Spectrometry (LCMS) data.
The goal of this system was to provide LCMS practitioners with an
overview of their data set and enable them to identify features that
can vary with mass and time, and this research primarily focused
on rendering speeds of large scale LCMS data. Work by Linsen et
al. [7] also focused on LCMS data looking at characteristic patterns of isotopes. Their visual exploration tools allowed users to
click on labeled peaks to explore mass spectrum data similar to our
proposed system. However, the key difference lies in the ability of
the user to compare multiple samples simultaneously through difference mappings and linked views.
Perhaps the closest is the work done by Hollingsworth et al. [1],
in which they utilize image processing based techniques based on
the total ion count image. Prior to visualization and comparison
each image must undergo background removal and peak detection.
Once this pre-processing is complete, a difference image is computed. This difference can be visualized using tabular data, a 2D
image, or 3D height field visualization in either grayscale or color.
Their main contribution is the calculation of a fuzzy difference that
compares each pixel value in one image with a small neighborhood
of pixels in the other, rather than doing a pixel-by-pixel comparison. This technique helps to reduce the incidental differences in
peak shape and retention time that may still exist even after alignment. While these techniques represent a good first step, they are
ill-suited for biomarker detection as they rely solely on images produced from the total ion count data, meaning that peaks of interest could easily become obscured. Since only the magnitude of
the difference is considered, the results obtained can be misleading. Large, yet unimportant differences may be emphasized while
small, yet meaningful differences may not be noticeable. Additionally, there is only limited user interaction available for exploring
peaks and mass spectra.
4

C OMPARATIVE V ISUAL A NALYTICS S YSTEM

Our work provides a comparative visual analytics system for
GCxGC-MS data. The primary goal is to increase data exploration
and analysis in order to help researchers determine meaningful differences between groups of samples. Finding such differences is
the first step in bio-marker identification. We provide both new and
traditional visualization techniques and couple them with linked,
interactive exploration. By incorporating mean and standard deviation calculations we are able to visualize the significance of differences in peak intensity rather than merely the magnitude of those

Figure 3: Examples of the different color mappings that can be applied to total ion count images. Intensity (left), difference (middle), and standard
deviations (right).

differences.
In this section, we first discuss our interactive visualization methods. We describe our scheme for total ion count visualization, mass
spectrum visualization and the various applied color mapping functionalities. We then discuss the linked views and data exploration
tools including the area selection tool, mass filtering and TIC lens.
4.1

Visualization Methods

Our system allows researchers to view multiple samples, as well
as display multiple views of individual samples. Each view can be
configured with a different visualization that provides researchers
with a different perspective on the data. The visualization methods
include 2D total ion count visualization, mass spectrum visualization, and comparative visualization.
4.1.1

Total Ion Count (TIC) Visualization

The total ion count is a reduction of the data set from four dimensions down to three. For each retention time coordinate, the intensities of the entire mass spectrum at that point are summed together
to obtain the total intensity, or total ion count. Once the TIC data
has been computed, it can be visualized either in 2D, or as a 3D
height rendering.
2D TIC: The two-dimensional total ion count image is one of
the most common visualization techniques for GCxGC-MS data.
Ion count values are mapped to a color using the specified transfer function and rendered to the screen as a flat, two-dimensional
image, as in Figure 1.
3D TIC: A total ion count height rendering is nearly identical to
the two-dimensional total ion count image. For the height rendering, the intensity is used as the z-coordinate in a polygonal mesh,
and can be scaled linearly or logarithmically to fit within a reasonable dimension. The x and y coordinates are evenly spaced points
corresponding to retention time 1 and retention time 2. When used
in conjunction with a normal color mapping, this does not actually
convey any more information than a two-dimensional TIC image
(Figure 2, left). However, this is still a useful technique as data can
often be portrayed more effectively by mapping the data values to
multiple display parameters, in this case color and height. Not only
do the two parameters serve to reinforce each other, but one may
overcome deficiencies in the other.
Additionally, as a new application, the height field can be used
with alternative color mapping schemes, similar to work done by
Linsen et al. with LC-MS data [7]. In this case, the height of the
peaks is a useful method for communicating peak intensity compared with the color mapped attribute. Alternative color mapping
schemes are discussed in more detail in Section 4.1.2.

As an alternative to color mapping, we also provide a ‘high contrast’ rendering option for the height field. For this technique we
enable OpenGL lighting and create a single light source positioned
along the positive z-axis with ambient, diffuse, and specular components. We apply diffuse, specular, and shininess material properties to the polygons. Vertex normals are calculated at each vertex
in the mesh, corresponding to each RT1/RT2 coordinate. The end
result is a high-gloss, metallic looking rendering with high contrast.
With this technique, even small peaks are highlighted and readily
noticeable, as seen in Figure 2 (right). Background noise is also
highly visible in this view, as it produces a large number of small
peaks and valleys.
4.1.2

TIC Color Maps

The total ion count visualizations both support color mapping based
on intensity, difference, and standard deviations away from a mean.
Each of these methods can be configured to use either a continuous
or discrete color scheme. For the continuous color scheme, the system uses a set of three curves that allow independent control of the
hue, saturation, and brightness.
For the discrete color scheme, the system presents the user with
a histogram that displays bin colors and data distribution. The user
can modify the number of bins, data range for the bins, and bin
colors interactively. The color for each bin can be specified by the
user, or the system can automatically generate a color mapping. In
each case, values are initially mapped to a logarithmic scale where
a larger color range is used to represent small intensities, and the
scale of large peaks is greatly reduced.
We provide three different types of visualization modes for the
TIC color maps: intensity, difference and standard deviation.
Intensity: The intensity mode is a simple mapping of the peak
intensity to color. This technique is useful for providing a highlevel view of the data. It reveals the location and relative intensity
of peaks, and can be useful in helping a user identify any samples
that may contain data collection errors. An example of the intensity
mode mapping is shown in Figure 3 (left).
Difference: The difference mode calculates the difference between two samples. The result is then displayed by simply mapping
the difference to a color. The system uses a separate set of HSV
curves for positive and negative differences. By default, the hue for
negative differences is set to pure green, and the hue for positive
differences is set to pure red. An example of the difference mode is
shown in Figure 3 (middle). Here, the user can quickly find areas
of high positive or negative differences between two samples.
Standard Deviation: The standard deviation mode also calculates the difference between samples and renders an image based

73

Figure 4: The point selection tool is used for comparing TIC values,
as well as interactively exploring mass spectra.

on that difference. However, a color mapping based solely on the
magnitude of the difference in intensities may not be what is most
interesting. Even using the mean of two sets of samples, the difference in intensity between two large peaks could be relatively high
in magnitude compared to two smaller peaks, but this does not necessarily mean that difference is meaningful. By analyzing the standard deviation within user specified groups of samples, differences
can be visualized in more certain terms. We create a standard deviation color mapping from a sample group by first calculating a mean
TIC for all the samples within that group. Note that the samples
are chosen by the user such that they have been pre-normalized as
input to the system.
Next a standard deviation TIC is calculated as:
s
1 n
σA =
(1)
∑ (xi − µA )2
nA i=1
Here, nA is the number of samples in group A, µA is the mean TIC
of group A, and the xi are the TICs of the ith sample. Once we
have computed the standard deviation TIC, it is stored to use for
color mapping. This color mapping can then be applied to a sample
visualization. Generally, it would be applied to a sample that is part
of another group. In order to determine the color at a particular
retention time coordinate, the system calculates the corresponding
z-value for each point b in the new sample, as shown in Equation
2. The z-value is simply how many standard deviations different a
value is than the calculated mean. We then use that difference to
determine the appropriate color.
z=

µA − b
σA

(2)

This can help a user to determine whether an observed difference
is truly meaningful. Additionally, this technique may effectively reveal areas of difference in smaller peaks that are significant in terms
of standard deviations, but were not previously noticed simply because the peaks themselves are smaller. An example is shown in
Figure 3 (right), note the green streaks that are not seen in Figure
3 (middle). This helps the user explore regions in the image that
are statistically different in a sample when compared to a group of
samples.

74

Figure 5: The mass filter tool allows a user to compare specific mass
values using TIC visualizations. In this image, everything except
mass 100 and 141 have been filtered out for the TIC visualization.

4.1.3

Mass Spectrum Visualization

A mass spectrum view is simply a plot of intensity on the y-axis vs.
the mass-to-charge ratio on the x-axis, as seen in Figure 4 (bottom).
The user is given the option to plot this spectrum as a bar graph, or
as a connected line.
4.2

Linked Views and Data Exploration

Linked views are a common technique for comparative visualization. This type of interaction aids a user in quickly, and intuitively
exploring data. Within the context of our system, we provide a
user with multiple tools he may use in order to interact with the
views, including scale, pan, rotate, point selection, area selection,
mass filter, and tic lens. Such linked views are not scalable as the
number of samples to be compared grows; however, by using the
difference and standard deviation views, users can explore samples
within groups for comparison, thereby reducing the need for numerous side-by-side comparison.
4.2.1

Point Selection

The point selection tool is used on 2D TIC visualizations. The
tool generates point selection messages that can then be handled by
other 2D TIC visualization, and by mass spectrum visualizations.
As the user interactively moves the cursor over a 2D TIC visualization, a cursor is displayed at the same location on all 2D TIC visualizations. The total ion count value at that location is displayed
in the lower left corner of the window, and mass spectrum views
are updated with the corresponding mass spectrum at that location,
as shown in Figure 4. All of this happens interactively, providing
real-time TIC and mass spectra exploration and comparison for the
data sets.
4.2.2

Area Selection

Rather than selecting a single point, a user may choose to select
an area in a TIC image. This tool allows a user to draw a rectangular region on the TIC image. The aggregated TIC value is then
displayed in the lower left, and mass spectrum visualization are updated with the total sum of intensity values for each mass in the
entire region. In combination with linked views, this can be used to
select a region containing a specific peak. The total integrated value

Figure 6: Here we use arrows to highlight the specific four differences discussed in the results. Visualized are the mean of the healthy and
cancer samples (left top, left bottom), the difference between them (middle top), cancer mean compared to mean and standard deviation of
healthy (middle bottom), and the healthy and cancer means during exploration using the TIC lens (right top, right bottom).

of the region can be compared across samples, as well as individual mass values for any spectra that are displayed. The selected
area can be made large enough so that the peak will be completely
within the region across all samples, even if there is a slight variation in retention time across some of the samples. This is illustrated
in Figures 8 and 9.
4.2.3

Mass Filter

When a user is looking for bio-markers and meaningful differences
between samples, the user will often identify a few mass values
that make up some interesting compound. The mass filter tool allows users to select a unique mass or set of masses from the mass
spectrum display. Total ion count data for active TIC visualizations
is then re-acquired using only the selected masses. The color mapping is then rescaled to match the new range of values across all of
the TIC displays, and the TIC displays are refreshed. This allows a
user to quickly determine differences among samples for a selected
set of mass values. While other applications provide mass filtering
via dialog options, none currently support real-time updating of the
mass filter in an interactive manner. Mass values can quickly be
added to or removed from the filter as the data sets are being interactively explored for differences in a unique set of mass values.
Mass filtering is demonstrated in Figure 5.
4.2.4

Figure 7: The mass spectra of the cancer mean (top) and healthy
mean (bottom) for the second difference.

TIC Lens

In any data visualization, color mapping may lead to a loss of visual features. This is due both to the limited color depth of the
display, as well as to the limited perceptual ability of human vision. As a result of these properties, it can be difficult to compare
peaks across separate TIC visualizations, and small peaks may not
be noticeable at all. The TIC lens implements a dynamic colorscale
adaption technique that was described by Elmqvist et. al [4]. In
order to more closely examine a particular region of the dataset, the

TIC lens tool allows the user to optimize the TIC color mapping
for a particular region of interest. When the user selects a region
using the TIC lens tool, the color mapping is rescaled to match the
range of values that lie within the lens. This is accomplished via linear transformation from the old intensity range to the new intensity
range. This technique is similar to Magic Lens filters [11]. However, while those techniques employed a local lens, our TIC lens
tool is a global lens that rescales the color mapping over the entire

75

image. This is demonstrated in Figure 6 (right). Our TIC lens tool
has two modes of operation that are activated by the left or right
mouse button. The first mode causes the color mapping for each visualization to be rescaled using the range of values within the lens
of the image it was activated on. This allows absolute intensity values to be compared across samples. The second mode will cause
each visualization to rescale based on its individual range of values
within the lens region. This allows relative intensity values to be
compared across samples. Using this tool with the 2D TIC visualization, the intensities of peaks can be compared, and small peaks
that may have been previously hidden can be seen and compared.
5

R ESULTS

To evaluate the benefits of our system, we have worked with biological and chemical researchers on several datasets. In order to better
illustrate the use of this system, we describe below one session of
our working side-by-side with a GCxGC-MS researcher to analyze
one of his data sets. This particular data set consisted of canine
serum samples. There were five samples each from healthy canines
and canines with cancer. No preprocessing was performed.
He created ‘Cancer’ and ‘Healthy’ groups in order to classify the
samples. After averages for the two groups were calculated, he then
calculated the difference of the two averages. In order to get a quick
overview of the data, the researcher first displayed 2D TIC visualizations of the means and difference side-by-side.In the difference
image we observed several red peaks that indicated higher intensities in the cancer mean sample. However, there was a bright green
peak about one-third of the way up on the difference image toward
the left side, as seen in Figure 6 (top middle) labeled as Arrow 1.
As this seemed interesting, he then used the region select tool in
order to get a feeling for the relative peak sizes. He created two
more visualizations in order to simultaneously show the mass spectra of these peaks for the mean samples. The first thing we noticed
was that the mean of the healthy samples had a much higher background level than the mean of the cancer samples. Unfortunately,
this makes comparison using the TIC values problematic. However,
individual mass values can still be compared from the mass spectra views, and using this we see that the healthy mean sample has
about twice the intensity of the cancer mean sample for that peak.
He next wanted to verify that this difference was consistent. All
ten samples were visualized simultaneously using 2D TIC views.
This allowed him to verify that this peak was consistently bigger
in the healthy samples. We also noticed a single sample among
the cancer group that had significantly less background level than
the other samples. This one outlier was responsible for most of the
difference in background levels between the two means, and was
therefore moved into a new group so that it would not be included
in future comparisons between the calculated means.
In order to dig deeper into the differences between the samples,
he next created standard deviation color mappings from the healthy
and cancer samples. These color mappings indicated that several
peaks showed higher than normal intensities in the cancer samples.
After adjusting the color mapping to highlight only the largest of
those differences, he chose to investigate one particularly noticeable
difference near the center of the image, which is shown in Figure
6 (bottom, middle). After visualizing all of the samples simultaneously, use of the region select tool showed a consistent difference
between the two groups. Again, differences in background levels
reduce the accuracy of using the displayed TIC values as difference
measures, but by visually inspecting the spectra we could see about
a six-fold difference in intensities between the groups, Figure 7.
He next began exploring this area by using the TIC lens tool to
compare the two mean samples. He quickly noticed a peak in the
cancer sample that barely showed up in the healthy samples, Figure
6 (middle) labeled as Arrow 2. Because this peak was small, it
is barely noticeable using the standard color mapping. We zoomed

76

into that area and immediately noticed two peaks in this area, Figure
6 (right) labeled as Arrow 3 and 4. The zoom and differences can
be seen in Figure 6 (right). We again visualized all the samples
simultaneously to look for consistency. The first peak (Arrow 3)
was very prominent in 4 out of the 5 cancer samples. We saw no
trace of it in 3 out of 5 healthy samples, and very slight presence
in the other 2 healthy samples, as shown in Figure 8. The second
peak (Arrow 4) was even more consistent, showing a significant
presence in all 5 cancer samples, and little to no presence in all 5
healthy samples, as shown in Figure 9. By noting the retention time
coordinates, he was then able to quickly look up and confirm these
differences. These consistent differences across samples mean that
these peaks are potential biomarkers.
6

D ISCUSSION

Initial feedback from researchers working with GCxGC-MS data
has been very enthusiastic. This system has provided them with
their first opportunity to visualize multiple samples simultaneously.
Enthusiasm has also been expressed about mean, standard deviation, and difference calculation for a set of samples. By visualizing
these calculated data, the human eye can quickly identify differences. This system can be used to identify a particular peak or
region of difference, and then the mass spectra can be explored to
provide validation of differences, and hypotheses about the compounds involved. With this information, their existing tools can be
used to obtain information about compound identification and intensity details much more quickly than was previously possible.
The researchers also frequently mentioned how pleased they
were with the speed of the software. Other commercial systems
will often take tens of seconds or minutes to even display a TIC
image. Additionally, these software systems allow masses to be filtered, and individual spectra to be visualized, however, it is a slow
and cumbersome process to change parameters and redisplay a new
spectra or filter out different mass values. No other system currently used by this group was able to provide the fast, interactive
filtering and mass spectra exploration of our system. As an example, the researchers can now quickly change the mass filter for a
specific value, and slowly move through the entire range of mass
values. Multiple samples can be visualized, and as the mass filter is
updated the researchers can very quickly visually identify cases in
which a unique mass has an unusual abundance in some samples.
Currently, a version of this system is deployed for use on the Cancer
Care Engineering Hub at Purdue University, http://ccehub.org.
7

C ONCLUSIONS AND F UTURE W ORK

As our system was evaluated, we also received several suggestion
for future work. Plans for future work involve producing output that
can be used by other tools. For example, we could allow the user
to select a single peak (or what appears to be a single peak) from
a TIC image. This could be used to reconstruct a one-dimensional
chromatogram for that region, and input that data into other existing
tools that would then perform peak deconvolution (if necessary) and
identification. Finally, many of these features could benefit from
incorporating gradient based value mapping into the display. This
was applied to GCxGC datasets in [12], and could be very effective
when used with the types of comparative visualization techniques
provided by this system.
ACKNOWLEDGEMENTS
The Cancer Care Engineering project is supported by the Department of Defense, Congressionally Directed Medical Research Program, Fort Detrick, MD (W81-XWH-08-1-0065) and the Regenstrief Cancer Foundation administered jointly through the Oncological Sciences Center at Purdue University and the Indiana University Simon Cancer Center. This work is supported by the U.S. Department of Homeland Security’s VACCINE Center under Award

(a) Healthy samples

(b) Cancer samples
Figure 8: Examining the third difference. The TIC lens tool was used to rescale the color mapping in the TIC images, and the region select tool
was used to highlight the peak and display the corresponding mass spectra.

77

(a) Healthy samples

(b) Cancer samples
Figure 9: Examining the fourth difference. The TIC lens tool was used to rescale the color mapping in the TIC images, and the region select tool
was used to highlight the peak and display the corresponding mass spectra.

Number 2009-ST-061-CI0001 and under the 973 program of China
(2010CB732504), NSFC 60873123, and NSF of Zhejiang Province
(N0. Y1080618). Thanks to Amber Jannasch and Bruce Cooper for
their input, feedback, data sets.
R EFERENCES
[1] Comparative visualization for comprehensive two-dimensional gas
chromatography. Journal of Chromatography A, 1105(1-2):51–58,
2006.
[2] J. de Corral and H. Pfister. Hardware-accelerated 3D visualization
of mass spectrometry data. In Proceedings IEEE Conf. Visualization,
pages 439–446, 2005.
[3] J. Dimandja. A new tool for the optimized analysis of complex volatile
mixtures: Comprehensive two-dimensional gas chromatography /
time-of-flight mass spectrometry. American Laboratory, 35(3):42–53,
2003.
[4] N. Elmqvist, P. Dragicevic, and J.-D. Fekete. Color Lens: Adaptive
Color Scale Optimization for Visual Exploration. IEEE Transactions
on Visualization and Computer Graphics, 17(6):795–807, 2011.
[5] F. Jourdan, A. Paris, P.-Y. Koenig, and G. Melançon. Multiscale scatterplot matrix for visual and interactive exploration of metabonomic
data. In VIEW, pages 202–215, 2006.
[6] R. Kincaid and K. Dejgaard. MassVis: Visual Analysis of Protein
Complexes Using Mass Spectometry. In Proceedings of the IEEE
Symposium on Visual Analytics Science and Technology, 2009.
[7] L. Linsen, J. Löcherbach, M. Berth, D. Becher, and J. Bernhardt. Visual analysis of gel-free proteome data. IEEE Transactions on Visualization and Computer Graphics, pages 497–508, 2006.

78

[8] S. Reichenbach, M. Ni, D. Zhang, and E. Ledford. Image background removal in comprehensive two-dimensional gas chromatography. Journal of Chromatography A, 985(1-2):47–56, 2003.
[9] S. E. Reichenbach, M. Ni, V. Kottapalli, and A. Visvanathan. Information technologies for comprehensive two-dimensional gas chromatography. Chemometrics and intelligent laboratory systems, 71(2):107–
120, 2004.
[10] A. E. Sinha, J. L. Hope, B. J. Prazen, E. J. Nilsson, R. M. Jack, and
R. E. Synovec. Algorithm for locating analytes of interest based
on mass spectral similarity in GCxGC-TOF-MS data: analysis of
metabolites in human infant urine. Journal of Chromatography,
1058:209–215, 2004.
[11] M. C. Stone, K. Fishkin, and E. A. Bier. The movable filter as a user
interface tool. In CHI ’94: Proceedings of the SIGCHI conference on
Human factors in computing systems, pages 306–312, New York, NY,
USA, 1994. ACM.
[12] A. Visvanathan, S. Reichenbach, and Q. Tao. Gradient-based value
mapping for pseudocolor images. Journal of Electronic Imaging,
16:033004, 2007.
[13] S. Wiklund, E. Johansson, L. Sjostrom, E. Mellerowicz, U. Edlund,
J. Shockcor, J. Gottfries, T. Moritz, and J. Trygg. Visualization of
GC/TOF-MS-based metabolomics data for identification of biochemically interesting compounds using OPLS class models. Anal. Chem,
80(1):115–122, 2008.
[14] W. Yao, X. Yin, and Y. Hu. A new algorithm of piecewise automated beam search for peak alignment of chromatographic fingerprints. Journal of Chromatography, 1160:254–262, 2007.

Understanding Hotspots:
A Topological Visual Analytics Approach
Jonas Lukasczyk

Ross Maciejewski

Christoph Garth

Hans Hagen

TU Kaiserslautern

Arizona State University

TU Kaiserslautern

TU Kaiserslautern

jl@jluk.de

rmacieje@asu.edu

ABSTRACT
Analysis of spatio-temporal event data is of central importance in many domains of science and policy making. Current visualization methods rely on animation, small multiples, and space-time cubes to enable spatio-temporal data
exploration. These methods require the user to remember
state spaces or deal with layout occlusions when exploring
their data. To overcome such issues, we propose a novel
visualization technique for such data that applies the topological notion of Reeb graphs to identify hotspots as areas of
relatively high event density within kernel density estimates.
We illustrate that the topological identification of hotspots
proposed in this paper is able to elucidate lifetime, properties, and relationships of hotspots by visualizing their temporal evolution based on the spatio-temporal Reeb graph.
To validate our approach, we demonstrate our method on
an epidemiological and a crime dataset. The resulting visualizations assist users in quickly identifying and comprehending important dates, events, hotspot properties, and
relationships between hotspots.

Categories and Subject Descriptors
G.3. [Mathematics of Computing]: Probability and
Statistics—Statistical computing; J.2 [Computer Applications]: Physical Sciences and Engineering—Mathematics
and Statistics

Keywords
Spatio-Temporal Event Data, Hotspots, Geovisualization,
Density Estimation, Topology, Reeb Graph

1.

INTRODUCTION

Spatio-temporal datasets are common measurements and
appear in many research fields. In general, a spatio-temporal
dataset is a finite collection of points, where each point
ei = (si , ti ) represents the location si ∈ R2 and time t ∈ R
of an event ei . For instance, Geoscience researchers often
work with measurements at latitude-longitude points over
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGSPATIAL’15, November 03 - 06, 2015, Bellevue, WA, USA
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3967-4/15/11...$15.00.
DOI: http://dx.doi.org/10.1145/2820783.2820817

garth@cs.uni-kl.de

hagen@cs.uni-kl.de

time. Common examples of such datasets are occurrences of
diseases, fires, earthquakes, tsunamis, robberies, and emergency calls. Given such a dataset, analysts are interested
in information hidden within the data, e.g., where are areas with a high rate of events, how do those areas behave
over time, and what is the relationship between them? Such
areas are commonly referred to as hotspots, and their properties are essential for understanding spatio-temporal event
data. Spatio-temporal analysis becomes even more difficult
if the datasets are a large collection of events and contain
a complex evolution of hotspots. In those cases, visualizing
the data points directly is prone to change blindness, ambiguity, and occlusion [13, 31]. Therefore, a common approach
is to visualize an aggregation of the events, such as a spatiotemporal density estimate [22, 27]. Yet, to understand the
evolution of hotspots within these estimates, analysts have
to manually compare density images for different time steps
without additional visual support, e.g., analysts have to repeatedly watch animations until they are convinced that
they have recognized all of the important information. However, if the animation contains a huge amount of information
and if the analysts have to keep track of several areas simultaneously, then it is likely that the analysts may miss or
misinterpret important information. Thus, the repeated examination of animations can be exhausting, unreliable, and
leads to a high cognitive load [14, 17].
In this work, we present a method for extracting and visualizing the spatio-temporal trajectories and relationships of
hotspots. While there has been much recent work on trajectory analysis, clustering and visualization, e.g. [1, 30], our
work differs in that we are not explicitly working with trajectory or origin-destination data. Instead, we are calculating
a spatio-temporal density estimate of the input data, and,
subsequently, we are utilizing topological methods to understand the underlying relationship between hotspots and
their movements. For example, emergency room records are
routinely collected and such records may contain an underlying notion of disease spread, but the records themselves have
no explicit definition of movements. Instead, each record
has only a patient’s address, time of visit, and visit reason.
Looking at some measure of movement within the spatiotemporal events could provide analysts with insights into
various spread patterns. We visualize these movements by
depicting the evolution of areas within the density estimate
with a high event rate, which we will further refer to as
hotspots. Although this hotspot definition does not consider additional information, such as which infected persons

were in direct contact with each other, our method is capable
of visualizing the underlying pattern of disease spread that
is contained in the density estimate. Such spread patterns
are not limited only to the emergency room events. We can
expand this idea to any spatio-temporal event data, such as
criminal incident reports, economics, and social trends.
In order to find hotspots within the statistical data, both
spatial and temporal patterns need to be explored during
the analysis phase. Our technique visualizes trajectories and
relations of hotspots within event-based data sources by deriving a spatio-temporal Reeb graph. For an introduction
to the notion of Reeb graphs we refer the reader to Edelsbrunner et al. [11]. In general, Reeb graphs are used to
illustrate the evolution of level sets of a real-valued function
on a manifold. In the case of our approach, the manifold is a
subset of the spatio-temporal density estimate, and the level
sets are the extracted hotspots. These graphs enable analysts to quickly identify important topological events, such
as when hotspots appear, disappear, merge, or split. As a
first step, our technique approximates the underlying data
distribution over time through the application of a spatiotemporal kernel density estimator. This provides us with a
continuous functional representation of the data. Next, we
extract and track areas within the estimates with a density
value above a user specified threshold. Then, our method
derives topological features of these extracted hotspots and
visualizes them as spatio-temporal Reeb graphs. Furthermore, the thickness and location of edges encode the size
and location of their associated hotspots, respectively. We
developed a web-based visualization software to demonstrate
the applicability of our method. The density estimates and
graphs are calculated on the fly and users are able to interactively adjust algorithm parameters, such as the threshold
value and kernel bandwidths. In this way, we can explore
the spread patterns of spatio-temporal event data.
The major contributions of our paper are as follows:
1) The algorithmic extraction of hotspot trajectories
and hotspot evolution within event-based datasets
2) The visualization of those features by Reeb graphs
3) An automatic Minard inspired flow map generation

2.

RELATED WORK

Since spatio-temporal datasets are common measurements
and appear in many research fields, several techniques have
been proposed which derive and illustrate properties of those
datasets. Typically, analysts have to manually explore temporal features of spatio-temporal data by either evaluating
abstract mathematical properties, repeatably observing animations, or examining and comparing time-dependent colorcoded static images, which are likely to be cluttered and ambiguous as the datasets become larger. This work proposes
a new method to analyze and visualize features of spatiotemporal data by combining spatio-temporal kernel density
estimations and topological based methods for time-varying
scalar functions.

2.1

Kernel Density Estimations

One important feature of spatio-temporal datasets are areas
with a high rate of events, commonly referred to as hotspots.
Specifically, analysts want to explore how those areas behave over time and what their relationship is between each

other. The works of Malik et al. [22], Peters et al. [27], and
Maciejewski et al. [21] describe the problem and present
solutions of tracking and visualizing hotspots inside spatiotemporal data. A common approach for this task is to use a
kernel density estimate of the observed point patterns. Such
a density estimate extrapolates from single point events to
the entire spatial domain by blurring the points in space and
time, thus yielding a scalar field over the spatial domain. In
contrast to simply plotting the observed points, visualizing
the scalar fields with heatmaps and iso-contours makes it
easy for analysts to quickly identify and examine areas of
interest. For instance, in the work of Malik et al. [22] a visualization method is presented which uses such kernel density estimates of recorded incidents to highlight areas where
future incidents may occur. It was shown that this approach
supports decision makers in their task of gaining insight into
the data and allocating resources. Also the methods proposed in Maciejewski et al. [21] deal with the problem of
identifying and visualizing hotspots. Again, a density estimate was used to highlight areas with a high frequency of
events. They propose a system which enables analysts to
calculate the density estimates for different points in time,
visualizes the estimates in linked views, and allows the users
to search for hotspots in space and time. Once more, in the
work of Peters et al. [27] a kernel density estimate was used
to examine temporal trends of hotspots. They propose a
technique which color codes the density estimate according
to the temporal trend, i.e., projecting the temporal component of the dataset via colors onto the density estimate.
This approach yields good results as long as the dataset has
a relatively simple temporal trend, but the color coding fails
as soon as clusters overlap in time. Furthermore, to extract
topological features of hotspots, all presented approaches
require the user to manually compare several static images
for different time steps and recognize patterns without additional visual support.

2.2

Spatio-Temporal Visualizations

In recent years, many spatio-temporal geo-visualization techniques have been developed for exploratory data analysis.
Many of them have been facilitated to inspire creative thinking and provide new insights into the previously unknown
characteristics of original data [2, 4, 28]. Some of the techniques focus on data with spatial and temporal information [10, 16, 18]. In the case of spatio-temporal datasets,
most tools display the events as markers at points in a
three-dimensional space-time cube [13, 15], and offer the
user various techniques to project the data onto the spatial and temporal domain, thus creating a two-dimensional
representation. For instance, the toolkit GeoTime [10] is
capable of projecting graphs, paths, and scatter plots inside
the space-time cube onto the geographical map or the timeline. Although the space-time cube is a useful tool in geovisualizations [20], it is still hard for users to recognize the
location and time of certain events. Moreover, it becomes
more difficult to analyze the temporal trends for frequently
occurring events. For example, Nakaya and Yano [25] use
spatio-temporal kernel density estimations of crime events to
render crime density by volume rendering techniques inside
the space-cube. However, it is still a problem to comprehend temporal trends and volume rendering also introduces
the problem of occlusion. In this work, we extract volumes
inside a spatio-temporal density estimate and use topolog-

ical based methods to represent those volumes as graphs,
which in turn can be easily projected onto the spatial and
temporal domain.

2.3

Topological Methods for Time-Varying
Scalar Fields

Previous methods lack a visualization which supports analysts in keeping track of the relationship between hotspots
over time. To track these topological features, this work proposes to visualize the evolution of hotspots within the timedependent kernel density estimates using Reeb graphs. The
concept of representing the relationship between specific areas of time-varying scalar fields through graphs has already
been proposed in several topology related work [11, 19, 23,
26, 29, 34]. The work of Mascarenhas et al. [23], Samtaney
et al. [29], and Ji et al. [19] propose iso-contour based
visualization techniques to track changes of time-varying
scalar fields. After extracting an iso-contour for two adjacent time steps, they present methods to examine the deviation of the contours and construct Reeb graphs accordingly.
These visualizations have been proven to be useful tools in
understanding real-valued space-time data from computational simulations of physical processes, especially since the
graph structures make it easy for analysts to comprehend
the evolution of level sets and detect important iso-values
[11]. Similar methods can even be applied in higher dimensions as proposed in Doraiswamy et al. [35] and Weber et
al. [34]. In both works the researches extract iso-volumes of
four-dimensional temperature fields and subsequently construct Reeb graphs of the extracted iso-volumes. The resulting Reeb graphs, which they call tracking graphs, support users in tracking the evolution of burning regions in
combustion simulations. These tracking graphs are similar
to our timeline projections, but our graphs are augmented
with additional information and are also projected onto the
spatial domain to illustrate hotspot location, trajectory, and
size, i.e., in contrast to tracking graphs, our graphs also enable geographical analysis. In our method, a hotspot is a
subset of scalar field that exceeds a user specified threshold.
This is comparable to the work of Doraiswamy et al. [9] in
which such areas are detected clouds. They also propose a
method to illustrate the trajectory of these areas by visualizing the optical flow between to time steps. On the other
hand, our approach uses topological features of the density
estimate to visualize hotspot trajectories by projecting the
spatio-temporal Reeb graph onto the spatial domain. The
approach of Doraiswamy et al. [8] also extracts areas with
a high density from a kernel density estimate. In their case,
the density estimate represents the availability of taxis at a
spatial location during a fixed time period and uses Reeb
graphs to visualize the topology of these areas. However,
they derive Reeb graphs for a fixed point in time, thus visualizing only the spatial evolution of these areas, whereas
our approach aims to represent the evolution of hotspots in
space and time. Oesterling et al. [26] also apply topological analysis to density estimates of multidimensional point
clouds. Specifically, they compute and visualize the nesting
of clusters identified as density maxima. In contrast, our
method aims to describe, extract, and depict the evolution
and trajectories of density clusters (i.e. hotspots).

3.

METHOD

The proposed method consists of two steps. First we calculate a spatio-temporal density estimate of the input data.
Then we extract hotspots from this estimate and derive
their topological features via the newly introduced spatiotemporal Reeb graphs. Finally, the results are visualized in
our web-based spatio-temporal analysis toolbox (STAT).

3.1

Spatio-Temporal Kernel Density
Estimation

In the case of spatio-temporal datasets, we propose the use
of a multivariate kernel density estimation in space and time.
In particular, the proposed approach performs separate density estimations for the spatial and temporal coordinates of
sample points. The spatio-temporal kernel density estimate
formula is given in Definition 1.
Definition 1. (Spatio-Temporal Kernel Density Estimate).
The spatio-temporal kernel density estimate of a dataset
{e1 , ..., en } with ei = ( (xi , yi ) , ti ) ∈ R2 × R is given by
λ(x, y, t) =





n
xi − x yi − y
ti − t
1 X
K
,
, (1)
K
S
T
nht h2s i=1
ht
hs
hs

where ht is the temporal bandwidth, hs is the spatial bandwidth, and KT and KS are the temporal and spatial kernel
functions, respectively.
Definition 1 allows analysts to use different kernels for the
spatial and temporal domain, i.e., Equation 1 can be interpreted as an ordinary multivariate spatial kernel density
estimate, where the temporal kernel additionally calculates
a weight for each sample point. The temporal bandwidth
ht is thereby used to model already known properties of the
dataset, e.g., if it is known that an infectious disease has
an incubation period of approximately one month, then the
likelihood of a new outbreak should only depend on the previous and upcoming thirty days. Nevertheless, it is a known
problem of spatio-temporal kernel density estimators that
the density estimation near the boundaries is not consistent. There are boundary correction methods available to
correct for those boundary effects, but this is not the scope
of this work.
For the case studies presented in this work, we use a one
dimensional triangular kernel
KT (u) = (1 − |u|)1{|u|<1}

(2)

as a temporal kernel function, where 1 is the indicator function. Since we do not intend to predict new outbreaks, but
rather want to gain insight into collected historic data, we
choose a symmetric temporal kernel. This incorporates the
fact that the probability of an event occurring at location s
at time t is also higher if events occurred in the near future
of this location. However, one could also choose an asymmetric temporal kernel to explore the data from a predictive
point of view [5]. In fact, Equation 1 allows analysts to use
any kernels they think models the underlying scenario best.
As for the spatial kernel function, we use a multivariate multiplicative Epanechnikov kernel:
KS (u, v) =

9
(1 − u2 )(1 − v 2 )1{ |u|<1 ∧ |v|<1 } .
16

(3)

After the analyst has chosen a spatial bandwidth, temporal
bandwidth, and appropriate kernels, Equation 1 is discretely
evaluated on a grid over the spatial domain. Thus, each evaluation of Equation 1 for a constant point in time t over the
gird points yields a two dimensional matrix λt . To allow
analysts to explore the effect of different kernels and bandwidths on the fly, we implemented the density estimation
on the GPU through a custom WebGL shader. Specifically,
the kernel parameters are passed to the shader as uniforms,
and the dataset as a floating point texture. Therefore, the
number of points is limited by the largest supported texture
size, which is for most devices at least a resolution of 20482
points. Our current implementation supports a resolution
of 40962 points up to 1000 time steps over a grid of 1282
cells and is still able to achieve interactive frame rates due
to various optimizations.

3.2

Spatio-Temporal Reeb Graph Derivation

This work proposes the use of Reeb graphs to represent the
topological relationship between hotspots within the spatiotemporal denstiy estimate. Hence, this section formally defines hotspots and describes a construction algorithm for
spatio-temporal Reeb graphs.
In this work, hotspots are subsets of the domain of the
spatio-temporal density estimate λ : S × T → R with a
density value above a user specified threshold τ (see Definition 2). Each connected component of such a subset D is a
single hotspot, i.e., two points belong to the same hotspot
if and only if both points are contained in the same connected component of D. The choice of τ thereby influences
the significance of hotspots. For instance, a value of τ near
maxs∈S,t∈T λ(s, t) would only extract areas with a very high
density. The proposed approach aims to visualize the evolution of those hotpots given by the volume D ⊂ S × T .
Therefore, we define an auxiliary function φ : D → T which
returns the temporal coordinate of a point inside the volume.
In other words, φ is the height function of this volume, where
the temporal coordinate of the volume is interpreted as its
height. It is possible to extract for t ∈ T level sets φ−1 (t)
of D, and hence an ordinary Reeb graph can be constructed
that represents the topology of the evolution of these level
sets (see Definition 3).
Definition 2. (Hotspot). Let D be a subset of the domain
of the spatio-temporal density estimate λ : S × T → R
such that (s, t) ∈ D iff λ(s, t) ≥ τ for some fixed value
τ ∈ [0, maxs∈S,t∈T λ(s, t)]. Then each connected component
of D is called a hotspot of λ.
Definition 3. (Spatio-Temporal Reeb Graph). For some
value τ , let the subset D be given as in Definition 2, and the
function φ : D → T be given as φ(s, t) = t. Then the Reeb
graph for φ is called the spatio-temporal Reeb graph.
However, in most cases the spatio-temporal density estimate
λ is evaluated on a grid for several points in time, and therefore the estimate is given as a three-dimensional lookup
table λ̂[x, y, t]. In this case, D is the set of all cells with
λ̂[x, y, t] ≥ τ , which can be interpreted as a voxel volume.
Algorithm 1 is capable of extracting hotspots from such a
spatio-temporal lookup table. The algorithm requires four
input parameters: the discretized spatio-temporal density

estimate λ̂, the temporal boundary points t0 and tn , and the
threshold value τ for each cell to be part of a hotspot (see
Definition 2). The algorithm constructs an approximation
of the spatio-temporal Reeb graph in a bottom-up approach.
In particular, it uses a data structure called the level set matrix (LSM), which is a regular matrix of the same dimension
as the spatial grid of the discretized density estimate. An
entry of this matrix for time t is either 0 if the corresponding
cell is not part of the voxel volume D, or the value is the
unique identifier (ID) of the component the corresponding
cell is part of. The procedure calculateLSM computes, for a
given time slice of the spatio-temporal density estimate, the
LSM for this slice. The algorithm initializes the LSM with
zero and, subsequently, all cells with a value larger than τ
are marked by −1. So far, a binary partition according to τ
has been obtained, but it is not possible to differentiate between connected components. Thus, for each cell with value
−1 a common flood fill algorithm is used to assign to each 8adjacent cell with the same value a unique ID. After all flood
fill procedures have been performed, each connected component has a unique ID for this time slice. Figure 1a and 1b
display two exemplary LSMs for adjacent time steps, where
the first matrix contains three components and the second
matrix two components.
Algorithm 1 Calcualte Spatio-Temporal Reeb Graph
1: procedure SpatioTemporalReebGraph(λ̂, t0 , tn , τ )
2:
3:

// initialize sets of vertices, edges, and components
V = ∅, E = ∅, C = ∅

4:
5:

// M1 is the LSM of the previous time step
M1 = null

6:
7:

// M2 is the LSM of the initial time step
M2 = calculateLSM (λ̂[:, :, t0 ], τ )

8:
9:
10:
11:

// create nodes for initial LSM
createN odes(V, M2 )
// for each subsequent time step...
for (t = t0 + 1 : tn ) do

12:
13:

// M1 becomes LSM of last time step
M1 = M2

14:
15:

// M2 is LSM of the current time step
M2 = calculateLSM (λ̂[:, :, t], τ )

16:
17:

// create nodes for M2
createN odes(V, M2 )

18:
19:

// connect nodes to preceding nodes
connectN odes(V, E, M1 , M2 )

20:

end for

21:
22:

// mark critical nodes of the graph (V, E)
markCriticalN odes(V, E)

23:
24:

// compute components of the graph (V, E)
C = computeComponents(V, E)

25:

return(V, E, C)

26: end procedure
Algorithm 1 initializes in line 3 the sets of vertices, edges,
and components as empty sets. Since there is no previous

(a)

(b)

(c)

Figure 1: Example level set matrices for two time steps: (a)
t1 and (b) t2 . (c) shows the overlap of the level set matrices
t1 and t2 marked in red.

time step, the LSM of the previous time step is set to null
in line 5. Then the LSM of the initial time step is calculated
in line 7 with the previously described procedure calculateLSM. Afterwards, the procedure createNodes creates, for
each connected component of the LSM, a new node located
at the center of mass of that component. Each node also
stores the size of its associated component. Subsequently,
for each following time step, an LSM and new nodes are
calculated in line 15 and 17, respectively. The procedure
connectNodes in line 19 is used to test the level set matrices
of the current and previous time step for overlaps of their
connected components, which is similar to the methods proposed in G. Ji et al. [19] and D. Silver et al. [32]. Although
more sophisticated approaches could be used, the simple test
for overlaps suffices to demonstrate the concept. If two components overlap, then a new edge between their associated
nodes is created. Both IDs just have to be unequal to zero to
qualify for an overlap, hence the identifiers themselves can
be different. In the example of Figure 1, the components of
time slice t1 with ID 1 and 3 overlap with the component
of time slice t2 with ID 1. Hence, the nodes associated with
component 1 and 3 of time slice t1 are connected to the node
associated to the component with ID 1 of time slice t2 . Also
the component with ID 2 of time slice t1 has an overlap with
component 2 of time slice t2 , so their associated nodes are
connected as well. After the for loop from line 11 to 20,
the intermediate graph (V, E) does not fulfill the definition
of a Reeb graph since there also exist nodes at uncritical
points. Thus, in line 22 the method markCriticalN odes
marks those nodes which have no predecessor, no successor,
or more than one predecessor or successor. As a final step,
the procedure computeComponents parses the graph (V, E)
and groups all edges togehter that belong to a path that
starts and ends at a critical node, and contains only noncritical nodes in between. Thus, each component represents
an edge of the Reeb graph.

3.3

Spatio-Temporal Reeb Graph
Visualization

To visualize the spatio-temporal Reeb graphs, we implemented the previously described theory in our web-based
Spatio-Temporal Analysis Toolbox (STAT ). Since the application is web-based, the visualization is easily accessible,
cross-platform compatible, and does not require the installation of additional software or plug-ins. The input of the
software is a CSV-File, which contains a list of latitudelongitude positions and dates. First, the software calculates a bounding box of the spatial locations and normalizes

Figure 2: Map projection of the spatio-temporal Reeb graph
for τ = 0.12max(λ) of the FMD dataset until April 26. Solid
lines indicate continues movement of hotspots, and dashed
lines merge/split events of hotspots.
them according to the bounding box such that all points
are contained in the unit-square. Then, the software uses
the method described in Section 3.1 to calculate a spatiotemporal density estimate of the normalized data, where the
user can select the spatial bandwidth, temporal bandwidth,
and the used kernels in real-time. Afterwards, users can
also vary the threshold τ at interactive frame rates to extract hotspots from the density estimate and derive spatiotemporal Reeb graphs as described in Section 3.2. The output of the proposed method is highly sensitive to the selected threshold. In fact, there are no τ values that suit
every dataset, thus, the analyst has to vary the threshold to
gain insight into the data by pealing through the density estimate. On the other hand, this enables analysts to use the
threshold as a tool to explore the data from different angles,
e.g., high thresholds would filter out insignificant hotspots,
while lower thresholds bundle hotspots which are close to
each other in the spatial domain. We allow users to adjust
the threshold and bandwidths in real time, which enables
them to analyze hotspot properties and nesting. To further support the exploration of different thresholds, we also
display the density maxima of the extracted hotspots by a
simple line chart. This chart supports the user in comparing
and filtering the extracted hotspots.
We visualize the spatio-temporal Reeb graphs in two ways:
as a projection onto the map, and; as a projection onto
the timeline. Critical nodes of the graphs are displayed by
different symbols, i.e., birth nodes are visualized by disks,
death nodes with squares, merge nodes with crosses, and
split nodes with diamonds as shown in Figure 2. Besides the
graph connectivity, which represents the topological features
of the hotspots, the thickness of an edge at time t encodes
the size of the associated hotspot at this time. Furthermore,
each edge has the color of its associated hotspot. If a new
hotspot appears, i.e., an edge starts with a birth node, then

it gets a new color. When hotspots merge, then the resulting
hotspot inherits the color of the largest merging hotspot. If
a hotspot splits, then the largest new hotspot inherits the
color of the preceding hotspot, and all the other hotspots
get new colors.
To create a Minard inspired flow map visualization [24], we
project the spatio-temporal Reeb graph onto a map. In particular, we use the Leaflet Javascript API to generate the
map in the background, which provides geo-spatial context.
For the projection, we simply discard the temporal coordinate of the graph and add a date caption at each critical
node. The solid lines of the graph represent movement of
their associated hotspots, while the dashed lines indicate
discontinuities of the hotspot centers (see Figure 2). For instance, consider the green hotspot of Figure 3b that splits
up from the orange hotspot of Figure 3a. In this particular
case, the green hotspot did not move away from the orange hotspot, instead their areas are simply not connected
anymore, causing the center of mass of the green hotspot
to jump to a new location. These jumps can also occur
during merge events, i.e., in this case the centers of mass
of the merging hotspots jump to the center of the resulting hotspot (see Figure 3c-d). Therefore, we use solid and
dashed lines to differentiate between these jumps and actual
movement. To read the graph, users could start at a birth
node and follow the edges until they reach another critical
node. Reading the graph this way enables users to follow
the history of a hotspot, which is similar to Minard’s flow
map visualization [24]. The map also contains a WebGLcanvas to render the density estimate, the extracted level
sets, and the point data. The density estimate can be rendered by a user specified number of contour lines through a
custom WebGL-shader, and the areas of the level sets are
rendered in the color of their associated Reeb graph edge.
Their boundaries are not rendered with a fixed width; instead, we draw a pseudo border by rendering the density
with a value near the threshold value black. This has the
additional advantage that we can visualize these borderline
areas.

(a)

(b)

(c)

(d)

Figure 3: Examples for jumps of hotspot centers during split
events (a-b) and merge events (c-d). Black crosses indicate
hotspot centers.
To support users in comprehending the temporal evolution
of hotspots and navigating through time, we also project
the spatio-temporal Reeb graph onto the timeline. Since the
spatio-temporal Reeb graph is a part of the spatio-temporal
domain, i.e., it has a latitude, longitude, and a time coordinate, it is possible to project the spatio-temporal Reeb
graph onto the timeline by discarding one spatial dimension. We chose to discard the longitude dimension, so that
the timeline and the map share the same y-domain, which
makes it easier for users to compare the graphs between both
projections (compare Figure 2 and 5a).

4.

CASE STUDIES

In this section, the application of the proposed method is
demonstrated on two datasets. Section 4.1 analyzes an outbreak of foot-and-mouth disease (FMD) during the UK 2001
FMD epidemic, and Section 4.2 analyzes reported crimes in
the City of Chicago (US) from 2013 to 2015.

4.1

Foot-and-Mouth Disease Dataset

The foot-and-mouth disease dataset is part of the R package stpp [12] and consists of 648 reported outbreaks in the
northern part of Cumbria county (UK). FMD is a highly infectious and severe viral disease of farm livestock that affects
cloven-hoofed animals by causing fever and blisters on the
feet and mouth. The disease can be transmitted by direct
contact with infected animals or through airborne particles
[33]. The UK 2001 FMD epidemic was one of the largest
outbreaks in history and lasted from February till September. Besides the massive environmental fallout, the disease
also had serious economical consequences, such as costs for
countermeasures and compensations for farmers [6].
This case study focuses on the time period starting at the
beginning of the outbreaks in end of February until the disease was brought under control in the end of April. To calculate the spatio-temporal density estimate λ according to
Section 3.1, one has to choose appropriate kernels and bandwidths KT , KS , ht and hs . In this case study, we model the
FMD outbreak according to official reports [6, 7], university
reports [3], and guidelines [33]. FMD has an approximate
incubation period between two and fourteen days [7, 33],
therefore we set the temporal bandwidth to th = 14 days
and use the triangular temporal kernel given by Equation 2.
In order to quarantine infected farms, the US Department
of Agriculture suggests in their FMD response plan to establish control areas with a radius of approximately 10km
around the infected premises [33]. Thus, we use this radius
as the spatial bandwidth, i.e., hs = 10km. The spatial kernel used in this case study is a multivariate Epanechnikov
kernel as given in Equation 3. Finally, the lattice on which
Equation 1 is evaluated has a spatial resolution of 128 × 128
grid points for each day. The resulting density estimates for
representative dates are shown in Figure 4.
Before we analyze the density estimate with our proposed
technique, we evaluate the estimate by comparing it to statements made in official records and reports. Figure 4a shows
that the first hotspot of the epidemic emerged north of
Carlisle on February 28. In fact, the indicated area around
Longtown is considered to be ground zero of the FMD epidemic in Cumbria, since many already infected animals have
unwittingly been distributed through the Longtown cattle
market, which is the largest sheep market in England [3,
6, 7]. This lead to outbreaks at close susceptible farms,
which caused the first hotspot to grow. At the same time,
a new hot spot emerged in the south east of Carlisle (see
Figure 4b). Both hotspots grew further and merged during
March (see Figure 4c). Beginning in April, the epidemic
was slowly brought under control by effective and organized
countermeasures such as vaccinations and culling of animals,
causing the main infection to shrink and split into smaller
hotspots until April 28 [6] (see Figure 4d-f).

(a) Feb 28

(b) Mar 5

(c) Mar 22

(d) Apr 08

(e) Apr 23

(f) Apr 28

Figure 4: Representative plots of the spatio-temporal kernel density estimate λ of the FMD dataset.

a)

b)

c)

Figure 5: Projections of spatio-temporal Reeb graphs of the FMD dataset onto the timeline for different τ values:
(a) τ = 0.12max(λ), (b) τ = 0.07max(λ), and (c) τ = 0.9max(λ).

The major contribution of this work is the spatio-temporal
Reeb graph, which is capable of visualizing such stories algorithmically in a single figure. Thus, analysts are no longer
required to manually compare static density images for different points in time. For instance, we can calculate the
spatio-temporal Reeb graph for the threshold equal to the
first contour line of Figure 4, i.e., τ = 0.12max(λ). Figure 2
and 5a show the projection of this graph onto the map and
the timeline, respectively. The timeline projection makes
it very easy to identify important days and to understand
the underlying hotspot structure, while the spatial projection additionally provides geographic references. Note, both
projections directly visualize the statements made in the previous paragraph. The graphs in both projections show that
the first hotspot emerged on February 28, while the spatial
projection also indicates the correct origin near Longtown.
Then, a new hotspot emerged on the March 2, which merged
with the first hotspot around March 8. The thickness of the
lines also encode the size of the associated hotspots, e.g.,
Figure 5a shows that the blue hotspot reached its maximum at the end of March, coinciding with the official reports which state that the disease reached its peak around
that time [3, 6]. On April 6, April 22, and April 26, small
hotspots split from the main hotspot, indicated by the diamond symbols and the green, red, and purple lines, respectively. The square symbols mark the disappearance of these
small hotspots on April 10, April 24, and April 27. However,
Figure 4 does not show the existence of the purple hotspot
due to the low temporal sampling rate. To compensate for
that, analysts would have to choose a finer temporal resolution and thus manually compare a much higher number
of density images. The spatio-temporal Reeb graph, on the
other hand, is capable of extracting these events algorithmically and visualizing them in graphs, which are easy to read
and contain all the important information in just one static
image. This shows that the spatio-temporal Reeb graph is
capable of telling the same story as the official reports.

As mentioned in Section 3.3, our method requires the user
to vary the threshold to analyze the data. For example,
a smaller τ value causes Algorithm 1 to additionally consider areas which did not exceed the previous threshold, but
exceed the smaller one. This can be used for anomaly detection, e.g., Figure 5b shows the timeline projection for
τ = 0.07max(λ). For this threshold, the graph consists of
two trees, i.e., the main infection and a remote hotspot.
Note, for the threshold τ = 0.12max(λ) the green hotspot is
discarded for being considered insignificant. By choosing a
low threshold, we smooth out features of the main hotspot,
such as the small hotspots of Figure 5a, but we are now
able to detect hotspots with lower density, such as the green
hotspot of Figure 5b. Conversely, choosing a high threshold
results in a graph that only represents intense hotspots, e.g.,
Figure 5c shows the timeline projection for τ = 0.9max(λ).
This graph shows that the outbreak was most intense from
March 24 through April 4, which is again consistent with
the official reports [3, 6].

4.2

Chicago Crime Dataset

Next we analyze a crime dataset that is available on the
City of Chicago’s Data Portal website. Specifically, we investigate a collection of 1,680,393 reported incidents of thefts
and robberies in Chicago from 1 January 2001 until 22 June
2015.
Similar to the first case study, we start by choosing appropriate parameters for Equation 1. Again, a triangular and
a multivariate Epanechnikov kernel are used as a temporal
and spatial kernel, respectively, and the used lattice has a
resolution of 128 × 128 grid points for each day. To detect
small temporal variances in the data, the temporal bandwidth has to be small enough not to smooth out those variances. Therefore, we use a temporal bandwidth of three
days. To explore the spatial distribution of crimes, we vary
the spatial bandwidth between three and seven kilometers.

a)
b)
c)
d)

Figure 6: Timeline projections of the spatio-temporal Reeb graphs of the Chicago crime dataset in 2015 for different bandwidths
and thresholds: (a) (τ, hs ) = (0.36max(λ), 7 km), (b) (τ, hs ) = (0.36max(λ), 3 km), (c) (τ, hs ) = (0.05max(λ), 3 km), and
(d) (τ, hs ) = (0.86max(λ), 3 km), where λ is the density estimate with (ht , hs ) = (3 days, 3 km).

(a)

(b)

Figure 7: Extracted hotspots of the Chicaco crime dataset
at 19 June 2015 for (a) hs = 3 km and (b) hs = 7 km.

Using a larger spatial bandwidth causes more smoothing, resulting in simple graphs which can be used to get an overview
of the data. For instance, Figure 6a shows a timeline projection for hs = 7 km. The projection shows that there exists a
permanent crime hotspot and another hotspot that exceeds
the current selected density threshold only in intervals. To
further examine the spatial distribution of these hotspots,
one can compare these projections with the graphs for a
lower spatial bandwidth, since a lower bandwidth represents
the spatial distribution in more detail. By comparing 6a
and 6b one can see that the hotspots of Figure 6a split into
smaller components, e.g., at 19 June 2015 the permanent
crime hotspot of Figure 6a consists of two separate hotspots
as shown in Figure 6b and 7. Another example is the cyan
hotspot from June 16 to June 21 of Figure 6a which consists
for a lower bandwidth also of two components as shown in
Figure 6b. Just like the spatial bandwidth can be used to explore the spatial distribution of hotspots, analysts can vary
the temporal bandwidth to explore the temporal distribution of hotspots. Since there are no optimal bandwidths, it
is crucial that analysts can modify bandwidths and observe
the resulting density estimates in real-time, which is possible
with our current implementation.
As shown in the previous paragraph, the bandwidths can
be used to explore the distribution of hotspots in space and
time. The threshold, however, can be used to to explore the
intensity of hotspots. For example, choosing a low threshold
bundles hotspots that are spatially close to each other. This

Figure 8: Map projection of the spatio-temporal Reeb graph
and extracted hotspots for (τ, hs ) = (0.05max(λ), 3 km) of
the Chicago crime dataset at 3 June 2015.

can be used to identify hotspots which are located remotely
to the bundle, e.g., the large blue hotspot of Figure 6c bundles all hotspots of Figure 6b and thus represents them in
one line. At the same time, new hotspots are detected which
are not connected to the bundle and therefore are located
remotely to it. In fact, for τ = 0.05max(λ) it was possible to identify the O’Hare airport as a crime hotspot, where
the timeline projection shows for which intervals the rate
of crimes was above the threshold. The timeline projection
also shows an anomaly between June 1 and June 3. In particular, on June 1 a hotspot appeared close to the bundle
and grew until the next day, causing the hotspot to be considered as part of the bundle on June 2. On June 3 the
hotspot starts to shrink, and therefore is separated from the
bundle again. Figure 8 shows the hotspot bundle (blue), the
airport hotspot (cyan), and the hotspot anomaly (orange).
The map projection of the spatio-temporal Reeb graph does
not contain solid lines, which represents the fact that the
hotspots remain stationary. The dashed, orange line indicates that the orange hotspot was once considered part of
the bundle until June 2.

If one selects a high threshold, most hotspots will be filtered
out and only areas with a very high density are detected
as hotspots. This can be used to search for time intervals
and locations with a relatively high rate of crime. Figure 7d
shows such time intervals for τ = 0.86max(λ). The spatial
projections of these hotspots are all located around the city
center, indicating that most of the crimes are committed
in this area, which intuitively coincides with reality. Each
year form 2001 to 2015 a hotspot is detected in the middle of
March, which could be related to the city’s St. Patrick’s Day
Parade. The timeline projections of all years also show that
the crime rate is most intense from June to August. Again,
this could be related to the increased number of festivals and
other large events during summer.
The benefit of the interactive τ exploration is that the choice
of bins for coloring maps is an extremely difficult task in
spatio-temporal data. If an analyst changes the bin range
each day, they lose coherence between the colors; however,
if the bins are designed to match the entire range, extreme
hotspots, as seen in this case, may be obfuscated by the binning technique. Given that the cartographic rule of thumb
is 5-7 color intervals on a map, the use of an interactive
threshold combined with the projected Reeb graph can improve the spatio-temporal analysis process.

5.

CONCLUSION AND FUTURE WORK

This work proposes a novel technique for visualizing hotspots
within spatio-temporal datasets. In particular, it was shown
how hotspots can be extracted from spatio-temporal kernel density estimates, and how they can be used to derive spatio-temporal Reeb graphs. These graphs elucidate
lifetime, properties, and relationships of hotspots by visualizing their temporal evolution in static images. Moreover, the graphs are easy to read, algorithmically computed,
and visualize trajectory information of event-based datasets.
We demonstrated the application of the method on two
real datasets in our WebGL-based Spatio-Temporal Analysis
Toolbox. With this software, users can explore the density
estimates, the extracted hotspots, and the spatio-temporal
Reeb graphs in real-time. In both case studies, the results
are consistent with the ground truth, yield further insight
into the data, enable reliable data extraction, and visualize information in static images that analysts before had to
extract manually from animations. As such, the proposed
method can be used by analysts as an additional tool to easily explore hotspots and their properties within event-based
spatio-temporal datasets.
While we have not currently engaged domain experts, we
believe that this work provides benefits not found in current
geovisualization methods. As mentioned previously, current
methods for spatio-temporal analysis rely on either small
multiple, animations, or space time cubes for exploratory
data analysis. Our visual analytics solution extracts hotspots
and directly plots their growth and decay in a single image.
While our approach cannot answer every question an analyst
may ask, it does provide a succinct means of summarizing
important characteristics in such datasets. As such, analysts
can immediately answer questions about when, where, and
for which threshold hotspots emerge, disappear, merge, or
split, while in previous techniques this would require mentally combining small multiples, replaying animations, or

rotating the space-time cube. By providing these methods
with a suite of interactive linked views, we are able to support a variety of analytical processes.
In future work, we plan to extend our approach in several
ways. First, we are going to explore a way to visualize the
underlying hotspot hierarchy for varying bandwidths and
thus provide the user with an interface to effectively select bandwidths and track their effects. We will also investigate the applicability of asymmetric kernels and if we
can use them to actually predict hotspots and their properties. Furthermore, we aim to derive a list of data specific
threshold values to support the analysts in exploring critical
events, such as when hotspots are filtered out. Although the
threshold value τ enables analysts to filter hotspots, we also
plan to extend our technique to track the actual topology
of the density estimate over time, i.e., to explicitly track
the topology of the underlying, time-varying scalar function. Our method also has some limitations that have to be
solved. For instance, if a hotspot does not contain its center of mass, which can be the case for non-convex hotspots,
then the edges of the spatio-temporal Reeb graph might not
correctly indicate the position of the hotspots. Another limitation is that the method only analyzes event-based spatiotemporal data in its most basic form, i.e., a location-time
pair. It would be possible to adapt the density estimate to
also support event data with associated scalar values, such
as the severity of a reported outbreak, or, more interestingly, datasets that already contain information about the
relationship between events. For example, epidemiological
datasets that contain contact information between infected
persons, or crime datasets that contain the identity of criminals and links between incidents.

6.

ACKNOWLEDGMENTS

Some of the material presented here was sponsored by Department of Defense and is approved for public release, case
number 15-383 and upon work supported by the NSF under
Grant No. 1350573.

7.

REFERENCES

[1] G. Andrienko, N. Andrienko, H. Schumann, and
C. Tominski. Visualization of Trajectory Attributes in
Space-Time Cube and Trajectory Wall. In
Cartography from Pole to Pole, pages 157–163.
Springer Berlin Heidelberg, 2014.
[2] N. Andrienko, G. Andrienko, and P. Gatalsky.
Exploratory spatio-temporal visualization: an
analytical review. Journal of Visual Languages &
Computing, 14(6):503–541, 2003.
[3] K. Bennett, T. Carroll, P. Lowe, and J. Phillipson.
Coping with Crisis in Cumbria: the Consequences of
Foot and Mouth Disease. Technical report, Centre for
Rural Economy, Newcastle University, 2002.
[4] C. A. Brewer. Designing Better Maps: A Guide for
GIS Users. Environmental Systems Research Institute
Inc.,U.S., 2005.
[5] C. Brunsdon, J. Corcoran, and G. Higgs. Visualising
space and time in crime patterns: A comparison of
methods . Computers, Environment and Urban
Systems, pages 52–75, 2007.

[6] Comptroller and Auditor General. The 2001 Outbreak
of Foot and Mouth Disease. Technical report, National
Audit Office (NAO), 2002.
[7] Department for Environment, Food and Rural Affairs
(DEFRA). Origin of the UK Foot and Mouth Disease
epidemic in 2001. Technical report, 2002.
[8] H. Doraiswamy, N. Ferreira, T. Damoulas, J. Freire,
and C. Silva. Using Topological Analysis to Support
Event-Guided Exploration in Urban Data. IEEE
Symposium on Visualization and Computer Graphics,
20(12):2634–2643, 2014.
[9] H. Doraiswamy, V. Natarajan, and R. Nanjundiah. An
Exploration Framework to Identify and Track
Movement of Cloud Systems. IEEE Transactions on
Visualization and Computer Graphics,
19(12):2896–2905, 2013.
[10] R. Eccles, T. Kapler, R. Harper, and W. Wright.
Stories in GeoTime. In IEEE Symposium on Visual
Analytics Science and Technology, pages 19–26, 2007.
[11] H. Edelsbrunner, J. Harer, A. Mascarenhas,
V. Pascucci, and J. Snoeyink. Time-varying Reeb
Graphs for Continuous Space-Time Data.
Computational Geometry, 41(3):149–166, 2008.
[12] E. Gabriel, B. Rowlingson, and P. J. Diggle. stpp: An
R Package for Plotting, Simulating and Analysing
Spatio-Temporal Point Patterns. Journal of Statistical
Software, 53(2), 2013.
[13] P. Gatalsky, N. Andrienko, and G. Andrienko.
Interactive analysis of event data using space-time
cube. In Eighth International Conference on
Information Visualisation, pages 145–152, 2004.
[14] K. Goldsberry and S. Battersby. Issues of change
detection in animated choropleth maps. Cartographica:
The International Journal for Geographic Information
and Geovisualization, 2009.
[15] T. Hagerstrand. What about people in Regional
Science? Papers of the Regional Science Association,
24(1):6–21, 1970.
[16] F. Hardisty and A. C. Robinson. The geoviz toolkit:
using component-oriented coordination methods for
geographic visualization and analysis. International
Journal of Geographical Information Science, pages
191–210, 2011.
[17] M. Harrower. The cognitive limits of animated maps.
Cartographica: The International Journal for
Geographic Information and Geovisualization, 2009.
[18] T. Hengl, P. Roudier, D. Beaudette, and E. Pebesma.
plotKML: Scientific Visualization of Spatio-temporal
Data. Journal of Statistical Software, pages 1–23, 2014.
[19] G. Ji, H.-W. Shen, and R. Wenger. Volume Tracking
Using Higher Dimensional Isosurfacing. In Proceedings
of the 14th IEEE Visualization, pages 28–,
Washington, DC, USA, 2003. IEEE Computer Society.
[20] M. Kraak. The space-time cube revisited from a
geovisualization perspective. pages 1988–1996, 2003.
[21] R. Maciejewski, S. Rudolph, R. Hafen, A. Abusalah,
M. Yakout, M. Ouzzani, W. S. Cleveland, S. J.
Grannist, and D. S. Ebert. A Visual Analytics
Approach to Understanding Spatiotemporal Hotspots.
IEEE Transactions on Visualization and Computer
Graphics, 2009.

[22] A. Malik, R. Maciejewski, S. Towers, S. McCullough,
and D. S. Ebert. Proactive Spatiotemporal Resource
Allocation and Predictive Visual Analytics for
Community Policing and Law Enforcement. IEEE
Transactions on Visualization and Computer
Graphics, 2014.
[23] A. Mascarenhas and J. Snoeyink. Isocontour based
Visualization of Time-varying Scalar Fields. In
Mathematical Foundations of Scientific Visualization,
Computer Graphics, and Massive Data Exploration,
Mathematics and Visualization, pages 41–68. Springer
Berlin Heidelberg, 2009.
[24] C. Minard. Des tableaux graphiques et des cartes
figuratives. Thunot, 1862.
[25] T. Nakaya and K. Yano. Visualising Crime Clusters in
a Space-time Cube: An Exploratory Data-analysis
Approach Using Space-time Kernel Density
Estimation and Scan Statistics. Transactions in GIS,
14(3):223–239, 2010.
[26] P. Oesterling, C. Heine, H. Janicke, G. Scheuermann,
and G. Heyer. Visualization of High-Dimensional
Point Clouds Using Their Density Distribution’s
Topology. IEEE Transactions on Visualization and
Computer Graphics, 17(11):1547–1559, 2011.
[27] S. Peters and L. Meng. Spatio Temporal Density
Mapping of a Dynamic Phenomenon. In
GEOProcessing 2014. Department of Cartography Technische Universitaet Muenchen, 2014.
[28] D. J. Peuquet and M.-J. Kraak. Geobrowsing:
Creative Thinking and Knowledge Discovery Using
Geographic Visualization. Information Visualization,
1(1):80–91, 2002.
[29] R. Samtaney, D. Silver, N. Zabusky, and J. Cao.
Visualizing Features and Tracking Their Evolution.
Computer, 27(7):20–27, 1994.
[30] R. Scheepens, N. Willems, H. van de Wetering, and
J. van Wijk. Interactive density maps for moving
objects. Computer Graphics and Applications, IEEE,
32(1):56–66, 2012.
[31] A. Shrestha, B. Miller, Y. Zhu, and Y. Zhao.
Storygraph: Extracting Patterns from
Spatio-temporal Data. In Proceedings of the ACM
SIGKDD Workshop on Interactive Data Exploration
and Analytics, pages 95–103. ACM, 2013.
[32] D. Silver and X. Wang. Tracking and visualizing
turbulent 3D features. IEEE Transactions on
Visualization and Computer Graphics, 3(2):129–141,
1997.
[33] United States Department of Agriculture (USDA).
Foot-and-Mouth Disease Response Plan, 2014.
[34] G. Weber, P.-T. Bremer, M. Day, J. Bell, and
V. Pascucci. Feature Tracking Using Reeb Graphs. In
Topological Methods in Data Analysis and
Visualization, pages 241–253. Springer Berlin
Heidelberg, 2011.
[35] W. Widanagamaachchi, C. Christensen, P.-T. Bremer,
and V. Pascucci. Interactive exploration of large-scale
time-varying data using dynamic tracking graphs. In
IEEE Symposium on Large Data Analysis and
Visualization, pages 9–17, 2012.

Abstract Feature Space Representation for
Volumetric Transfer Function Exploration
Ross Maciejewski1 , Yun Jang2 , David S. Ebert3 , and
Kelly P. Gaither4
1,3 Purdue University Visual Analytics Center – West Lafayette, IN, USA
{rmacieje|ebertd}@purdue.edu
2
ETH Zurich – Zurich, Switzerland
jangy@inf.ethz.ch
4
University of Texas – Austin, TX, USA
kelly@tacc.utexas.edu

Abstract
The application of n-dimensional transfer functions for feature segmentation has become increasingly popular in volume rendering. Recent work has focused on the utilization of higher order
dimensional transfer functions incorporating spatial dimensions (x, y, and z) along with traditional feature space dimensions (value and value gradient). However, as the dimensionality
increases, it becomes exceedingly difficult to abstract the transfer function into an intuitive and
interactive workspace. In this work we focus on populating the traditional two-dimensional histogram with a set of derived metrics from the spatial (x, y and z) and feature space (value, value
gradient, etc.) domain to create a set of abstract feature space transfer function domains. Current two-dimensional transfer function widgets typically consist of a two-dimensional histogram
where each entry in the histogram represents the number of voxels that maps to that entry. In
the case of an abstract transfer function design, the amount of spatial variance at that histogram
coordinate is mapped instead, thereby relating additional information about the data abstraction in the projected space. Finally, a non-parametric kernel density estimation approach for
feature space clustering is applied in the abstracted space, and the resultant transfer functions
are discussed with respect to the space abstraction.
1998 ACM Subject Classification I.3 Computer graphics, I.3.5 Computational Geometry and
Object Modeling, I.3.6 Methodology and Techniques, I.3.7 Three-Dimensional Graphics and Realism
Keywords and phrases Volumetric Transfer Function, Abstract Feature Space
Digital Object Identifier 10.4230/DFU.Vol2.SciViz.2011.212

1

Introduction

A common method for direct volume rendering is to employ the use of interactive transfer
functions as a means of assigning color and opacity to the voxel data. One of the most
popular transfer function design tools is the interactive 2D histogram widget introduced by
Kniss et al. [8]. In this widget, the user is presented with a 2D histogram (the axes of which
represent a feature space of the data) and various selection tools are used to assign color
and opacity to the voxels through an interactive brushing of the feature space. This widget
typically displays each entry in the histogram as a gray scale color with white representing
the entry that maps to the largest number of voxels within a given data set, Figure 1 (a).
While such a tool has been shown to be extremely effective at advanced transfer function
© Ross Maciejewski, Yun Jang, David S. Ebert, and Kelly P. Gaither;
licensed under Creative Commons License NC-ND
Scientific Visualization: Interactions, Features, Metaphors. Dagstuhl Follow-Ups, Vol. 2.
Editor: Hans Hagen; pp. 212–221
Dagstuhl Publishing
Schloss Dagstuhl – Leibniz Zentrum für, Germany

R. Maciejewski, Y. Jang, D.S. Ebert, and K.P. Gaither

213

Figure 1 The value vs. value gradient magnitude feature space with the intensity as a function
of a) the number of voxels, b) the magnitude of spatial variance of x, c) the magnitude of spatial
variance of y, and d) the magnitude of spatial variance of z for the CT bonsai dataset.

creation, this type of histogram provides no information about the spatial relationships
between the voxels at the histogram entry.
In this work we propose to enhance the conventional 2D histogram transfer function by
mapping the entries of the histogram to statistical properties related to the spatial locations
of the voxels, specifically, the magnitude of the spatial variance, Figure 1 (b-d). By doing
so, this new abstracted feature space now illustrates the areas in which the voxels have a
higher spatial relationship as opposed to simply providing the user with a view of where
in the feature space the majority of their voxels lie. Users are able to toggle between the
conventional transfer function view, Figure 1 (a), (in which the entries in the 2D histogram
are colored by the number of voxels that map to a location) and the abstracted transfer
function views, Figure 1 (b-d), (in which the entries are colored by the spatial variance in
the voxels that map to a location).
Traditionally, the appropriate selection of features in multi-dimensional transfer functions
is a difficult task, often requiring the user to have an underlying knowledge of the data set
under exploration. By providing users with information about the spatial domain (x, y, and
z) of their data in an abstracted feature space (e.g., value versus value gradient magnitude)
we are able to enhance the exploration, allowing users to better discover features within their
dataset. Finally, we utilize the non-parametric transfer function design method proposed by
Maciejewski et al. [12] clustering the feature space in both the conventional and abstracted
transfer function views. In this manner, we explore the usefulness of abstracting statistical
properties into the transfer function widget and illustrate the effects on the exploration of
the feature space.

2

Related Work

Interactive transfer function design has been addressed with many different approaches,
ranging from simple (yet intuitive) one-dimensional (1D) transfer functions (e.g., [5, 13]) in
which a scalar data value is mapped to color and opacity, to more complex multi-dimensional
transfer functions in which color and opacity are mapped across multiple variables. Early
work by Kindlmann et al. [6] and Kniss et al. [8] applied the idea of a multi-dimensional
transfer function [9] to volume rendering. This work identified key problems in transfer
function design, noting that many interactive transfer function widgets lack the information
needed to guide users to appropriate selections, making the creation of an appropriate
transfer function essentially trial-and-error which is further complicated by the large degrees
of freedom available in transfer function editing. While many volume rendering systems have

Chapter 15

214

Abstract Feature Space

Figure 2 The value vs. value gradient magnitude feature space as a function of the number of
voxels and the rendering from the denoted segments of the transfer function for the CT bonsai
dataset.

adopted multi-dimensional transfer function editing tools, the creation of an appropriate
transfer function is still difficult as the user must understand the dimensionalities of the
feature space that they are interacting with.
Recent works on transfer function design have proposed higher-dimensional transfer
functions based on mathematical properties of the volume. Examples include work by
Kindlmann et al. [7], which employed the use of curvature information to enhance multidimensional transfer functions, and Tzeng et al. [18], which focused on higher dimensional
transfer functions which use a voxel’s scalar value, gradient magnitude, neighborhood
information and the voxel’s spatial location. Work by Potts et. al [15] suggested visualizing
transfer functions on a log scale in order to better enhance feature visibility. Lundstrom
et al. introduced the partial range histogram [10] and the α-histogram [11] as means for
incorporating spatial relations into the transfer function design. Correa et al. introduced size
based transfer functions [3] which incorporate the magnitude of spatial extents of volume
features into the color and opacity channels and visibility based transfer functions [4] where
the opacity transfer function is modified to provide better visibility of features.
While such extensions enhance the volume rendering and provide a larger separability
of volumetric features, they still fail to provide users with information about feature space
structures. In fact, the addition of more dimensionality into the transfer function is often
automatically incorporated into the rendering parameters, obscuring the relationship between
the volumetric properties and the volume rendering. Work by Roettger et al. [16] incorporates
similar ideas of using spatial features of the volume to enhance transfer function design.
They enable the automatic setup of multi-dimensional transfer functions by adding spatial

R. Maciejewski, Y. Jang, D.S. Ebert, and K.P. Gaither

215

Figure 3 The value vs. value gradient magnitude feature space as a function of the magnitude of
the spatial variance and the rendering from the denoted segments of the transfer function for the
CT bonsai dataset.

information to the histogram of the underlying dataset, where as our work proposes a means
for the direct analysis of spatial properties. Other work focuses on the addition of more
dimensions to provide coherency between transfer functions. Recent work by Akiba et al.
[1, 2] utilized parallel coordinate plots to create a volume rendering interface for exploring
multivariate time-varying datasets. Muelder and Ma [14] proposed a prediction-correction
process to aid in creating coherent feature tracking.
In all of these related works, one can note that various statistical properties of the volumes
are being used in order to extract features of interest and segment properties of the volume.
Unfortunately, as the number of dimensions increases, interaction in n-dimensional space
becomes cumbersome to the point that few systems exceed two dimensional transfer functions;
instead, the extra dimensionality is incorporated automatically, somewhat limiting the user’s
control. In order to enhance the information provided in the transfer function histogram
widget, our work incorporates some statistical properties of the spatial domain (x, y and z)
into the projected feature space domain (e.g., value versus value gradient magnitude).

3

Abstract Feature Space Representation

Given any two-dimensional feature space for a given volume data set, the user is presented
with a representation illustrating the number of voxels at a given location within that feature
space. In order to enhance this feature space, we propose the use of an abstract feature
space representation. By this, we mean that the 2D histogram feature space is no longer
representing the number of voxels as a given location. Instead, the 2D histogram feature space

Chapter 15

216

Abstract Feature Space

is representing statistical properties of the volumetric (x, y and z) space with relationship to
a particular feature. The key property we have chosen to look at is spatial variance with
respect to a given feature set. Thus, given any two feature properties of the volumetric
data set (e.g., value vs. value gradient norm, temperature vs. pressure, x vs. pressure) we
compute the spatial variance of all the points that map to a given entry in the feature space
histogram.
As such, at location (i, j) in the feature space histogram, we have N voxels, Vk , that map
to this given feature space pair. For all the N voxels that map to the feature space pair (i, j)
we calculate the mean position within the (x, y, z) space of the volume.
V̄(i,j) =

N
1 X
Vk
N

(1)

k=1

Once the mean is calculated, we can then determine the magnitude of the standard deviation
within the volumetric space at the feature space pair (i, j).
v
u
N
u1 X
|σi,j | = t
(Vk − V̄(i,j) )2
(2)
N
k=1

Figure 1 illustrates the difference between the traditional histogram mapping the magnitude of the voxels found at a histogram entry (a) to the magnitude of the spatial variance
found at a histogram entry (b-d). Furthermore, we can see that the magnitude of the spatial
variance in the z-direction is relatively constant; however, there is an obvious clustering of
high spatial variance in the y-direction. The magnitude of the standard deviation vector can
also be employed for analyzing the spatial variance found within a given feature space, and
the utility of the creation of this abstract feature space is further discussed in Section 5.

4

Non-Parametric Transfer Function Generation

Once the abstract feature space is defined in the 2D histogram, we can use the non-parametric
transfer function generation approach described in Maciejewski et al. [12] to provide users
with a means to group data in the 2D histogram by areas of similar value (in terms of the
number of voxels or in terms of the abstracted standard deviation metric described in Section
3). In order to create these clusters we employ the use of a variable kernel method [17, 19]
formed in Equation 3. Furthermore, we utilize an adaptive kernel which scales the parameter
of the density estimation by allowing the kernel radius to vary based upon the distance from
each point, Xi , to the kth nearest neighbor in the set comprising the N − 1 data points of
the histogram feature space.


N
1 X 1
x − Xi
ˆ
fh (x) =
K
N i=1 di,k
di,k

(3)

Here, fˆh (x) is the probability density estimate of the histogram (h) at a given location, x, in
the feature space, di,k represents the multi-dimensional smoothing parameter and N is the
total number of samples in the histogram (i.e., the number of voxels in the volume). The
window width of the kernel placed on the point Xi is proportional to di,k (where di,k is the
distance from the i-th sample to the k-th nearest neighbor) so that data
√ points in regions
where the data is sparse will have flatter kernels. We choose k = b N c as this tends to
approximate the optimal density estimation fitting (this is a rule of thumb approximation

R. Maciejewski, Y. Jang, D.S. Ebert, and K.P. Gaither

217

[17]). Such a method groups the data based on their neighborhood information, allowing us
to visualize the underlying structure of the data.
In order to reduce the calculation time, we have chosen to employ the Epanechnikov
kernel, Equation 4.
K(u) =

3
(1 − u2 )1(||u||≤1)
4

(4)

The function 1(||u||≤1) evaluates to 1 if the inequality is true and zero for all other cases.

5

Exploring Abstracted Feature Spaces

The focus of this work is to determine if statistical properties derived from the make-up
of a given feature space can be utilized to extract more information and aid in transfer
function creation and data exploration. In this section we describe the impact of utilizing
the proposed abstract feature space for feature segmentation and data exploration.

5.1

Feature Segmentation

Figure 2 illustrates the application of the non-parametric transfer function generation
technique on the conventional transfer function view of the CT bonsai data set, and Figure 3
illustrates the application of the non-parametric transfer function generation technique on
the abstracted transfer function. Here we can see which portions of the clustered transfer
function map to regions of the volume. Note that the cluster pattern is completely different
between Figure 2 and Figure 3. Here we can see that the clustering in the conventional
transfer function space winds up with a larger portion of the background noise in each feature
segmentation than the abstract transfer function view.
Furthermore, in Figure 2 we see that in segmenting out the leaves (the red cluster), values
representing air are still being included with the transfer function; however, by utilizing the
spatial variance, those values are not included in the segmentation of the leaves as seen in
Figure 3 (again the red cluster). However, other features wind up being combined when
using the spatial variance. We can see that the tree trunk and root ball has more separation
when utilizing the voxel magnitude for clustering than when using the spatial deviation.
We can further explore the effects of using spatial deviation in transfer function generation
by utilizing the x, y and z spatial components separately, thus creating an abstract transfer
function space representing the standard deviation . Figure 4 (middle) illustrates the same
transfer function generation technique on each of the components of the spatial deviation
mapped to the transfer function. What we find in the case of the CT bonsai data is that the
area representing air (the red circled area in Figure 4 (middle)) in the feature space is not
found as a separate cluster portion in any application of the non-parametric transfer function
generation; however, when utilizing the y spatial variance component in the transfer function,
the area representing air in the transfer function is associated with another data grouping
within the transfer function. This grouping allows us to segment out the air component
(as seen in the resultant volume rendering (Figure 4 (bottom)) where as this component
is directly related to other important features in the volume when utilizing the x and z
spatial components. The segmentation of the air component is also what leads to the noise
in Figures 2 and 3
Based on these observations, it seems likely that utilizing information on the spatial
variance can help in transfer function design; however, it seems unlikely that this feature
alone can create the desired segmentation. Another option for creating abstract feature space

Chapter 15

218

Abstract Feature Space

Figure 4 (Top) The value vs. value gradient magnitude feature space as a function of the spatial
variance in the x (left), y (middle) and z (right) component direction. (Middle) The resultant
non-parametric transfer function. (Bottom) The resultant volume rendering from applying various
segments of the transfer function. Note in the x and z spatial variance histograms, segmentation of
the noise can not be accomplished.

representations could be utilizing bivariate color mapping schemes to simultaneously relay
information about the magnitude of change at a transfer function location and the number
of voxels that map to that location.

5.2

Data Exploration

While such tools have their use in traditional volumetric datasets (CT and MRI), the
application of these novel analytic methods to more complex simulation data also proves to
be very interesting. With the maturation of computational power, simulations are capable of
modeling physical phenomena at increasingly more realistic scales. Analysis tools, however,
are struggling to keep up with the explosion of data that has resulted from the increase
in computational horse-power. This is particularly evident in computational fluid dynamic

R. Maciejewski, Y. Jang, D.S. Ebert, and K.P. Gaither

219

(CFD) simulations modeling time-dependent complex flow phenomena. Because the contents
of the simulation are not known exactly, and because computational simulations are still
evolving and improving their ability to correctly and accurately model physical behavior, the
output from these simulations must be analyzed using data mining, feature detection, and
feature extraction techniques to provide useful, pertinent information.
Analytical 3D feature detection methods employ well-formed mathematical descriptions
of features often unique to a given domain, thus demanding a priori knowledge of the nature
and location of any potential “areas of interest.” This is a cumbersome and tedious task,
resulting in a set of methods incapable of scaling with the size of the data. Statistical
feature detection tools, on the other hand, provide an automatic way to provide general
characteristics.
Flow data sets are ideal candidates for statistically based feature detection techniques
because we can leverage physical properties to apply segmentation algorithms. For example,
using similarity allows us to segment those regions of the flow data that contain like properties
for a given variable. Conversely, using dissimilarity allows us to pinpoint regions in which
the flow changes drastically or perhaps discontinuously. This discontinuity is of interest in all
flow data sets and often indicates an important region of interest. As such, the utilization of
abstract feature space for exploring CFD data should allow researchers to discover interesting
regions hidden within their datasets. By utilizing a mapping of the spatial deviation, analysts
should be able to quickly explore areas in which flow feature properties (temperature, pressure,
velocity, etc.) quickly change from a consistent amount of spatial deviation to a (relatively)
larger or smaller amount.

6

Conclusions

From our preliminary work on creating abstract feature space, it seems like utilizing the
underlying statistical properties of the spatial volumetric features that map to a given feature
space histogram pair will provide analysts with another tool in which to explore data. Based
on our exploration of the abstract feature space, it seems likely that enhancing transfer
function design through the use of statistical information about the spatial relationships
between voxels will aid in feature segmentation and exploration. Future work will focus on
utilizing the spatial variation as an uncertainty metric, and looking at other derived data
properties (entropy, etc.) as a means for enhancing transfer function exploration and design.
Furthermore, we plan to utilize abstracted feature space measures for novel volume rendering
parameters in order to reduce the burden of transfer function design on the user. In this
manner, we hope to match statistical properties of the data to visual properties thereby being
able to semi-automatically create effective transfer functions. Finally, we plan to extend
this to the analysis of complex fluid dynamic simulations and map a variety of statistical
properties to the transfer function domain, thereby providing researchers with a tools that
provide both a fast statistical analysis of data properties and a means in which to filter the
data on said properties.

Acknowledgements
This work is supported by the U.S. Department of Homeland Security’s VACCINE Center
under Award Number 2009-ST-061-CI0001 and by the National Science Foundation OCI0906379. Yun Jang is supported by the Swiss National Science Foundation under grant
200021_124642.

Chapter 15

220

Abstract Feature Space

References
1

2

3

4
5

6

7

8

9
10

11

12

13

14
15
16
17

Hiroshi Akiba and Kwan-Liu Ma. A tri-space visualization interface for analyzing timevarying multivariate volume data. In Proceedings of Eurographics/IEEE VGTC Symposium
on Visualization, pages 115–122, May 2007.
Hiroshi Akiba, Kwan-Liu Ma, Jacqueline H. Chen, and Evatt R. Hawkes. Visualizing
multivariate volume data from turbulent combustion simulations. Computing in Science
and Engineering, 9(2):76–83, 2007.
Carlos Correa and Kwan-Liu Ma. Size-based transfer functions: A new volume exploration
technique. IEEE Transactions on Visualization and Computer Graphics, 14(6):1380–1387,
October 2008.
Carlos Correa and Kwan-Liu Ma. Visibility-driven transfer functions. In Proceedings IEEEVGTC Pacific Visualization Symposium, Beijing, China, April 2009.
Taosong He, Lichan Hong, Arie Kaufman, and Hanspeter Pfister. Generation of transfer
functions with stochastic search techniques. In Proceedings of the IEEE Conference on
Visualization, pages 227–234, 1996.
Gordon Kindlmann and J. W. Durkin. Semi-automatic generation of transfer functions for
direct volume rendering. In Proceedings of the IEEE Symposium on Volume Visualization,
pages 79–86, 1998.
Gordon Kindlmann, Ross Whitaker, Tolga Tasdizen, and Torsten Moller. Curvature-based
transfer functions for direct volume rendering: Methods and applications. In Proceedings
of the IEEE Conference on Visualization, pages 513–520, 2003.
Joe Kniss, Gordon Kindlmann, and Charles Hansen. Interactive volume rendering using
multi-dimensional transfer functions and direct manipulation widgets. In Proceedings of
the IEEE Conference on Visualization, pages 255–262, 2001.
Marc Levoy. Display of surfaces from volume data. IEEE Computer Graphics & Applications, 8(3):29–37, 1988.
Claes Lundstrom Patric Ljung and Anders Ynnerman. Local histograms for design of
transfer functions in direct volume rendering. IEEE Transactions on Visualization and
Computer Graphics, 12(6):1570–1579, 2006.
C. Lundström, A. Ynnerman, P Ljung, A. Persson, and H. Knutsson. The alpha-histogram:
Using spatial coherence to enhance histograms and transfer function design. In Proceedings
Eurographics/IEEE-VGTC Symposium on Visualization 2006, May 2006.
Ross Maciejewski, Insoo Woo, Wei Chen, and David S. Ebert. Structuring feature space:
A non-parametric method for volumetric transfer function generation. IEEE Transactions
on Visualization and Computer Graphics, (To appear) 2009.
J. Marks, B. Andalman, P. A. Beardsley, W. Freeman, S. Gibson, J. Hodgins, T. Kang,
B. Mirtich, H. Pfister, W. Ruml, K. Ryall, J. Seims, and S. Shieber. Design galleries: a
general approach to setting parameters for computer graphics and animation. In Proceedings
of the ACM SIGGRAPH Conference on Computer graphics and Interactive Techniques,
pages 389–400, 1997.
Chris Muelder and Kwan-Liu Ma. Interactive feature extraction and tracking by utilizing
region coherency. In Proceedings of IEEE Pacific Visualization Symposium, April 2009.
S. Potts and Torsten Moller. Transfer functions on a logarithmic scale for volume rendering.
In Proceedings of Graphics Interface, pages 57–63, 2004.
Stefan Roettger, Michael Bauer, and Marc Stamminger. Spatialized transfer functions. In
Proceedings Eurographics/IEEE-VGTC Symposium on Visualization 2005, 2005.
B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman &
Hall/CRC, 1986.

R. Maciejewski, Y. Jang, D.S. Ebert, and K.P. Gaither

18

19

221

Fan-Yin Tzeng, Eric B. Lum, and Kwan-Liu Ma. A novel interface for higher-dimensional
classification of volume data. In Proceedings of the IEEE Conference on Visualization,
pages 505–512, 2003.
M. P. Wand. Kernel Smoothing. Chapman & Hall/CRC, 1995.

Chapter 15

Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010

Data Aggregation and Analysis for Cancer Statistics - A Visual Analytics
Approach
Ross Maciejewski ∗

Travis Drake∗
∗ Purdue

Stephen Rudolph∗

Keywords: Spatial aggregation, cancer statistics, AMOEBA, visual analytics.

and see reports displayed on an interactive map. Users may scroll
through time and use a novel dual time slider control to compare
changes in rates with a variable lag time. Spatial clustering is done
through the AMOEBA algorithm developed by Aldstadt and Getis
[1].
In this approach, we hope to reduce the issues associated with
small areas by enlarging the area base over which the summary
statistic may be calculated. Further, the clustering algorithm threshold can be user defined in order to enlarge clusters where the data
aggregation would still fail to be sufficient. This work does not
propose to seek out statistically significant cancer clusters within a
population. Instead, our goal is to cluster spatial data based on relationships between underlying factors (age, demographics, income,
etc.). Once a set of counties are grouped by this underlying factor,
age adjusted cancer statistics may be calculated for this new region
that is statistically homogenous with respect to a given population
factor. Then, the counties can be clustered, and inferences can be
made. In such a way, we are able to reduce the uncertainty found
within small area statistics by increasing the area size (when possible).
Furthermore, our system introduces a novel dual time slider interaction technique in which users may explore various lag times
and temporal aggregations amongst data resources. In this way,
we encourage hypothesis generation. Linked views of the temporal domain can provide a static reference for exploration while interactive comparative maps can animate changes over time during
exploration.
2

M OTIVATION

Relative grouping of cancer statistics for analysis and summary reporting is an important task for public health officials. Unfortunately, summary statistics of cancer data are either provided only by
geographic unit (county, state, etc.), or by population demographic
unit (age, ethnicity, etc.). In the case of summarizing cancer statistics on a county by county basis, the disparity between data collected in rural and urban counties is often detrimental in the appropriate analysis of cancer care statistics. Low counts drastically affect the incidence and mortality rates of the data, leading to skewed
statistics. This problem is often referred to as the small area problem [5] or the small number problem [10]. One common method of
handling this is to simply summarize the cancer data by population
demographics within a state, ignoring the spatial data components.
In this work, we present a system that allows analysts to create demographic clusters of their data while maintaining the spatial data constraints. Our scheme increases the stability of the reported cancer rates by aggregating areas with similar demographics
through interactive spatial clustering. Interactive selection of demographic groupings allows analysts to ask questions of their data
∗ e-mail:

David S. Ebert∗

University Regional Visualization and Analytics Center (PURVAC)

A BSTRACT
The disparity between data collected in rural and urban counties is
often detrimental in the appropriate analysis of cancer care statistics. Low counts drastically affect the incidence and mortality rates
of the data, leading to skewed statistics. In order to more accurately report the data, various levels of aggregation have been used
(grouping counties by population, age percentages, etc.); however,
such data aggregation methods have often been ad hoc and/or time
consuming. Such groupings are performed on a user defined basis; however, grouping based purely on population demographics
does not take into account the spatial relationships between data.
Furthermore, researchers want to search for spatiotemporal correlations within their data domain. In this work, we introduce a visual
analytics system for exploring cancer care statistics in a series of
linked views and interactive user interface queries. We also apply
the AMOEBA algorithm [1] for clustering counties based on population demographics in a visual analytics environment. Users select
the population demographics field on which they wish to cluster,
and these county clusters then form the basis for the data aggregation. Such a system allows the user to group their data by fields
(age, gender, income) while maintaining spatial structure and provides and interactive mapping system in which to compare and explore such groupings. By utilizing such geographical groupings, we
hope to better enhance the underlying structure of the data and help
alleviate reporting problems associated with small area statistics.

1

Abish Malik∗

{rmacieje|tdrake|srudolph|amalik|ebertd}@purdue.edu

R ELATED W ORK

In recent years, much research has been done on determining effective means of exploring and disseminating health and cancer statistics. Previous work in the visual exploration and analysis of cancer
statistics has looked at ways to visualized the uncertainty in data
due to small area statistics. MacEachren et al. [13] utilized adjacent maps to show data reliability, and compared this to maps
utilizing textures (hashing) overlaid on color to display reliability.
MacEachren et al. [12] also presented a system designed to facilitate the exploration of time series, multivariate, geo-referenced
health statistics. Their system employed linked brushing and time
series animation to help domain experts locate spatiotemporal patterns. Further work in analyzing health statistics was done by Edsall et al. [6]. Here, the use of interactive parallel coordinate plots
was used to explore mortality data as it relates to socio-economic
factors. Other work includes Dang et al. [4] and Zhao et al. [20]
which utilized dynamic queries and brushing for creating choropleth map views, and Schulze-Wollgast et al. [17] developed a system for visualizing health data for the German state MecklenburgVorpommern. This system allowed users to interactively select diseases and their parameters and view the data over a specific time
interval at different temporal resolutions. Further work in this system [18] employed the use of intuitive 3D pencil and helix icons for
visualizing multiple dependent data attributes and emphasizing the
type of underlying temporal dependency.
All of these systems fall into the realm of visual analytics. Visual

978-0-7695-3869-3/10 $26.00 © 2010 IEEE

1

Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010

analytics is the science of analytic reasoning facilitated by interactive visual interfaces [16]. It is primarily concerned with presenting
large amounts of information in a comprehensive and interactive
manner. By doing so, it is hoped that the end user will be able
to quickly assess important data and, if required, investigate points
of interest in detail. The branch of visual analytics with which this
work is most concerned is that of geospatial and temporal analytics,
which applies the concepts of visual analytics to problems rooted in
space and time.
Recently, the development of visual analytics systems for data
analysis and exploration has been rapidly growing (e.g., [2, 7, 15,
19]). These systems incorporate a variety of visualization techniques from traditional, widely used methods, such as scatterplots
or parallel coordinate plots to more recently developed tools (e.g.,
spiral graphs [3], theme river [8]). Techniques common across these
systems include the probing, brushing and linking of data in order to
help analysts refine their hypotheses, and these systems emphasize
the interaction between human cognition and computation through
dynamically linked statistical graphs and geographical representations of the data.
To this end, the goal of the system presented in this paper is
to provide the user with a system capable of performing advanced
spatial analysis methods in an interactive environment, thereby enabling hypothesis generation and data exploration. An important
aspect of analyzing these cancer datasets is comparative and correlative analysis of different data sources and different time intervals.
If ones wants to find correlation among incidence rates and mortality rates, what time intervals of each dataset should they compare?
Should an aggregate of 5 years of incidence data be compared to
one or two years of mortality data? What is the best alignment of
these time windows? These are important questions to cancer care
researchers, and our system provides researchers with an interactive tool that allows flexible comparison and aggregation of various
time scales and factors to enable the generation of new correlative
hypotheses and enable them to be tested and validated.
3

V ISUAL A NALYTICS E NVIRONMENT

Figure 1 provides a screenshot of the system. cancer data is collected from the Indiana State Cancer Registry [14], details of which
are summarized in Section 3.1. Our system provides a simple GUI
in which users can query cancer data collected in Indiana from 1997
- 2004. Users can choose to cluster data by the 2000 census statistics data. Various visualizations are shown across four maps, allowing users to compare various cluster groupings and population
statistics. Users can see the unclustered cancer statistics (incidence
and mortality rate), the census statistics, the county groupings based
on AMOEBA clustering, and then the age adjusted combined cancer statistics for the resultant clusters. Linked views provide historical context to view the changes of rates over time.
3.1

Cancer Statistics

Cancer data is collected by county agencies and reported to the Indiana State Department of health. In order to insure confidentiality
and stability of rates, counts by gender are suppressed if fewer than
16 cases were reported in a specific area-sex-race category. Incidence and mortality rates are calculated based on a per 100,000
population per year metric and are age-adjusted to the 2000 US
standard population by 5-year age groups. When interpreting the
data provided, it is important to understand that observed groupings may be subject to ecological fallacy (correlation does not imply causation). Furthermore, data collection is such that when the
population size for a denominator is small, the rates may be unstable. A rate is unstable when a small change in the numerator (e.g.,
only one or two additional cases) has a dramatic effect on the calculated rate. Suppression is used to avoid misinterpretation when
rates are unstable, and visualization methods for expressing such

Figure 1: A screenshot of our system showing comparative views of
mortality rates and linked temporal history plots of selected counties.

instances in geographical visualization systems have been explored
by MacEachren et al. [13].
In this work, we utilize the available rates as data for a proof
of concept demonstration. All collected data is available, and falls
under various privacy concerns due to small area statistics. Our system provides researchers with a potential means of reporting statistics over a larger areal aggregation, allowing for less suppression
and higher rate stability. Further, our system could be modified
to include a privacy threshold such that if data were available to
the backend of the system for calculations, data reports could be
restricted by the system if the questions being asked by the user
would violate established privacy rules (i.e., fewer than 16 cases
reported in a given area).
3.2 AMOEBA Clustering
In order to group data based on spatially similar population statistics, we utilize the AMOEBA algorithm for creating spatial weights
matrix developed by Aldstadt and Getis [1]. AMOEBA (A Multidirectional Optimum Ecotope-Based Algorithm) procedure is designed to identify hot and cold spots in mapped data by assessing
the spatial association of a particular mapped unit to its surrounding units. It is able to aid in the demarcation of clusters of related
spatial units, and we utilize this fact to group counties based on
population statistics.
AMOEBA maps clusters of high and low values by creating a
spatial weights matrix based on the Getis-Ord G∗i statistic. For a
given location i, G∗i is defined as
∑Nj=1 wi j x j − x̄ ∑Nj=1 wi j
G∗i = 
S

(1)

[∑ N j=1 w2i f −(∑Nj=1 wi j )2 ]
N−1

Here, N is the number of spatial units, x j is the value of interest
within the areal unit at location j, x̄ is the mean of all values, and

∑Nj=1 x2j
− (x̄2 )
(2)
S=
N
wi j is used as an indicator function that is one if j is a neighbor of i
and zero otherwise.
The AMOEBA algorithm develops a cluster from a selected seed
location by evaluating G∗i at all locations surrounding this seed location, and if the addition of a neighbor to the cluster increases the

2

Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010

Figure 2: Aggregating data on population groups under 18. (Left) Choropleth map of the percent of population under 18. (Right) AMOEBA
clustering of population based on percent under 18.

G∗i value, then the neighbor is added. Details of this algorithm and
the use of it in other visualization applications can be found in [1]
and [9].
Figure 2 illustrates the application of AMOEBA clustering in our
system. In Figure 2 (Left) we have the chlorpleth map of the percentage of the population under 18 in Indiana counties. In Figure 2
(Right) we show the results of an AMOEBA clustering. Groups are
colored based on their G∗i values, and counties that connect to other
counties of the same color are considered to be a cluster. These
clusters make up the basis of our summary statistic groups for the
cancer analysis. Furthermore, users may also utilize this clustering
mechanism to group counties specifically by cancer types in order to see which counties have similar cancer statistics in a general
sense.

derlying cluster structure. The resulting summary statistics are then
displayed for the cluster to provide advanced analysis and exploration. By doing this, the user can begin to see patterns emerging in
the data that may be otherwise obscured. However, it is important
not to fall prey to ecological fallacies in such an analysis.
3.3

4

Figure 3: Aggregating data on population groups over 65 averaging the colon cancer incidence rates. Here we see the selection of
a county reveals the underlying cluster structure and provides summary statistics about the cluster.

Figure 3 shows an aggregate grouping of the percentage of population over 65 colored by their average colon cancer incidence
rates. The user can select a county in the state to reveal the un-

Temporal Comparison Controls

Along with our clustering tools, we also provide users with a unique
temporal control system in the form of a dual slider system for comparative visualization. Users may interactively scroll through time
in the normal mode, using the leftmost slider (Figure 1). Time can
also be aggregated by year.
In comparative mode, the user interactively chooses the time period of interest using the leftmost slider and compares it to the period selected by the rightmost slider. In this case, the choropleth
maps displayed show the difference in rates between the two time
periods.
Users may also select counties in this mode and display the cancer statistics for each choropleth map on a linked statistical view.
Here, raw rates are shown as a time series plot for each county.
This allows users to view the stability of the signal over time and
jointly investigate other aspects of their data.
E XAMPLES

IN

AGGREGATION

AND

E XPLORATION

Our cancer analytics system provides researchers with a mean to
interactively compare cancer rates for various cancer types. These
comparisons can be used to influence policy decisions and guide
health care spending (for example, targeting areas with high lung
cancer rates with more ads, or increasing free screenings in areas
with high colon cancer mortality rates. Figure 4 illustrates a way in
which researchers can search for large changes in mortality rates. In
Figure 4, the upper left choropleth map shows the change in mortality rate in 2004 compared to 1999 for the aggregation of all cancer
sites. Here, the analyst may begin exploring the various red groups
(indicating that the rate is higher in 2004 than it was in 1999). For
example, a group of four counties with higher mortality rates is
found in the central east portion of the state. The analyst can select
the counties, bringing up the temporal history for each choropleth
map in the corresponding linked window to the right. Here, the
topmost window is linked to the map in the upper left, and the following time series windows are linked in order of the maps going
from left to right, top to bottom. In each window, the analyst can
select the variable by which to color the map. Here, the analyst has

3

Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010

Figure 4: This example illustrates how a user might user such a system to explore groups of increased mortality rates across the state.

chosen to separate the cancers by site in each of the other three windows in order to determine which types of cancers are most likely
to be influencing the high rate changes. After interactively searching through the cancer types, the analyst finds that three of the four
counties rates have increased due to an increase in lung cancer, and
the fourth county is due to colon cancer. In this case, policy makers
could use this information to warrant more spending on an antismoking campaign, or increase free prostate examinations.
In Figure 5, the analyst has chosen to look at colon cancer rates.
Further, the analyst has chosen to cluster the counties by those with
similar populations under the age of 18. In Figure 5(Left), we see
the choropleth map of colon cancer rates by county for 1999. In
Figure 5(Right), a new picture emerges where the data has been
smoothed due to the grouping, and new patterns emerge. The analyst can then begin analyzing these new patterns and forming hypotheses by pulling up other data attribute maps in other linked windows of the system.
5

C ONCLUSIONS

AND

F UTURE W ORK

Our current work demonstrates the benefits of visual analytics for
helping analysts ask questions of their data and begin generating hypotheses. Our initial results show the benefits of linking time-series
views with geo-spatiotemporal views for enhanced exploration and
data analysis through the use of traditional choropleth map visual-

izations and advanced temporal controls allow for finer tuning of
map comparisons.
Future work includes the introduction of SatScan [11] and the
use of AMOEBA as means to search for statistically significant
cancer clusters under user controlled guidance. We hope this preliminary work demonstrates the potential power a visual analytics
system could have for cancer care engineering, and we hope that
this will prompt researchers to release more detailed data to be used
in this setting under a refined set of privacy controls.
R EFERENCES
[1] J. Aldstadt and A. Getis. Using amoeba to create a spatial weights
matrix and identify spatial clusters. Geographical Analysis, 38, 2006.
[2] T. Butkiewicz, W. Dou, Z. Wartell, W. Ribarsky, and R. Chang. Multifocused geospatial analysis using probes. IEEE Transactions on Visualization and Computer Graphics, 14:1165–1172, Nov. - Dec. 2008.
[3] J. V. Carlis and J. A. Konstan. Interactive visualization of serial periodic data. In Proceedings of the Symposium on User Interface Software and Technology, 1998.
[4] G. Dang, C. North, and B. Shneiderman. Dynamic queries and brushing on choropleth maps. In IV ’01: Proceedings of the Fifth International Conference on Information Visualisation, page 757, Washington, DC, USA, 2001. IEEE Computer Society.
[5] P. Diehr. Small area statistics: large statistical problems. American
Journal of Public Health, 74, 1984.

4

Proceedings of the 43rd Hawaii International Conference on System Sciences - 2010

Figure 5: Aggregating data on percentage of population under 18. (Left) Choropleth map of the incidence rates of colon cancer. (Right) Incidence
rates of colon cancer aggregated by population groups under 18.

[6] R. M. Edsall, A. M. MacEachren, and L. Pickle. Case study: Design
and assessment of an enhanced geographic information system for exploration of multivariate health statistics. In Proceedings of the IEEE
Symposium on Information Visualization 2001, page 159, 2001.
[7] D. Guo, J. Chen, A. M. MacEachren, and K. Liao. A visualization
system for space-time and multivariate patterns (vis-stamp). IEEE
Transactions on Visualization and Computer Graphics, 12(6):1461–
1474, 2006.
[8] S. Havre, E. Hetzler, P. Whitney, and L. Nowell. Themeriver: Visualizing thematic changes in large document collections. IEEE Transactions on Visualization and Computer Graphics, 8(1):9–20, 2002.
[9] M. Jankowska, J. Aldstadt, A. Getis, J. Weeks, and G. Fraley. An
amoeba procedure for visualizing clusters. In Proceedings of GIScience 2008, 2008.
[10] K. Jones and A. Kirby. The use of chi-square maps in the analysis of
census data. Geoforum, 11, 1980.
[11] M. Kulldorff. A spatial scan statistic. Communications in Statistics:
Theory and Methods, 26:1481–1496, 1997.
[12] A. M. MacEachren, F. P. Boscoe, D. Haug, and L. Pickle. Geographic
visualization: Designing manipulable maps for exploring temporally
varying georeferenced statistics. In Proceedings of the IEEE Symposium on Information Visualization, page 87, 1998.
[13] A. M. MacEachren, C. A. Brewer, and L. W. Pickle. Visualizing georeferenced data: representing reliability of health statistics. Environment and Planning, 30:1547 – 1561, 1998.
[14] I. S. D. of Health. Isdh: Cancer reports. WWW page.
[15] J. Stasko, C. Gorg, Z. Liu, and K. Singal. Jigsaw: Supporting investigative analysis through interactive visualization. In Proceedings
of the IEEE Symposium on Visual Analytics Science and Technology
2007, pages 131–138, 2007.
[16] J. J. Thomas and K. A. Cook, editors. Illuminating the Path: The R&D
Agenda for Visual Analytics. IEEE Press, 2005.
[17] C. Tominski, P. Schulze-Wollgast, and H. Schumann. Visual analysis
of health data. In 2003 IRMA International Conference, 2003.
[18] C. Tominski, P. Schulze-Wollgast, and H. Schumann. 3d information visualization for time dependent data on maps. In International
Conference on Infomation Visualization (IV), 2005.
[19] C. Weaver. Multidimensional visual analysis using cross-filtered
views. In Proceedings of the IEEE Symposium on Visual Analytics
Science and Technology (VAST), October 2008.
[20] H. Zhao, B. Shneiderman, and C. Plaisant. Improving accessibility
and usability of geo-referenced statistical data. In dg.o ’03: Proceedings of the 2003 annual national conference on Digital government
research, pages 1–1. Digital Government Research Center, 2003.

5

1438

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 19,

NO. 9,

SEPTEMBER 2013

Bristle Maps: A Multivariate Abstraction
Technique for Geovisualization
SungYe Kim, Ross Maciejewski, Member, IEEE, Abish Malik, Yun Jang, Member, IEEE,
David S. Ebert, Fellow, IEEE, and Tobias Isenberg, Member, IEEE
Abstract—We present Bristle Maps, a novel method for the aggregation, abstraction, and stylization of spatiotemporal data that enables
multiattribute visualization, exploration, and analysis. This visualization technique supports the display of multidimensional data by
providing users with a multiparameter encoding scheme within a single visual encoding paradigm. Given a set of geographically located
spatiotemporal events, we approximate the data as a continuous function using kernel density estimation. The density estimation
encodes the probability that an event will occur within the space over a given temporal aggregation. These probability values, for one or
more set of events, are then encoded into a bristle map. A bristle map consists of a series of straight lines that extend from, and are
connected to, linear map elements such as roads, train, subway lines, and so on. These lines vary in length, density, color, orientation,
and transparency—creating the multivariate attribute encoding scheme where event magnitude, change, and uncertainty can be
mapped as various bristle parameters. This approach increases the amount of information displayed in a single plot and allows for unique
designs for various information schemes. We show the application of our bristle map encoding scheme using categorical spatiotemporal
police reports. Our examples demonstrate the use of our technique for visualizing data magnitude, variable comparisons, and a variety of
multivariate attribute combinations. To evaluate the effectiveness of our bristle map, we have conducted quantitative and qualitative
evaluations in which we compare our bristle map to conventional geovisualization techniques. Our results show that bristle maps are
competitive in completion time and accuracy of tasks with various levels of complexity.
Index Terms—Data transformation and representation, data abstraction, illustrative visualization, geovisualization

Ç
1

INTRODUCTION

A

data dimensionality increases, the encoding of
variables and their relationships is often abstracted
down to a representative subset for analysis in a single
display, or dispersed across a series of coordinated multiple
views [1], [2], [3]. Moreover, many techniques have been
developed to visually encode multiple data attributes/
variables for each data sample to enable interactive analysis,
ranging from discrete glyph attribute encoding [4] to more
spatially continuous color, transparency, and shading
encodings [5], [6], [7]. As the number of visualized variables
increases, the amount of information that can be effectively
displayed becomes limited due to overplotting and cluttering [8]. This is especially a problem in geographical
visualization as a key attribute of the data is the location
within the two-dimensional map space.
In geographical visualization, data can be described at
any given location on a map. The data being described can
come from an aggregated measurement, a direct event
S

. S. Kim, A. Malik, and D.S. Ebert are with the School of Electrical and
Computer Engineering, Purdue University, 465 Northwestern Avenue,
West Lafayette, IN 47907. E-mail: {inside, amalik, ebertd}@purdue.edu.
. R. Maciejewski is with School of Computing, Informatics, and Decision
Systems Engineering, Arizona State University, Mail Code 8809, Tempe,
AZ 85287-8809. E-mail: rmacieje@asu.edu.
. Y. Jang is with Department of Computer Engineering, Sejong University,
98 Gunja-dong Gwangjin-gu, Seoul, 143-747, South Korea.
E-mail: jangy@sejong.edu.
. T. Isenberg is with Team Aviz, INRIA-Saclay, Bât 650, Université ParisSud, 91405 Orsay Cedex, France. E-mail: tobias.isenberg@inria.fr.
Manuscript received 14 Feb. 2012; revised 7 Nov. 2012; accepted 2 Mar. 2013;
published online 20 Mar. 2013.
Recommended for acceptance by M. Agrawala.
For information on obtaining reprints of this article, please send e-mail to:
tvcg@computer.org, and reference IEEECS Log Number TVCG-2012-02-0030.
Digital Object Identifier no. 10.1109/TVCG.2013.66.
1077-2626/13/$31.00 ß 2013 IEEE

occurrence, or various other means. In dense data sets,
plotting events as symbols on the map (e.g., Fig. 1a) leads to
cluttering and is often unable to convey a meaningful sense
of event magnitude within the data. Aggregation of the data
by defined boundaries, such as county or census tract
boundaries (e.g., Fig. 1b), leads to a loss of specificity in data
location and runs afoul of the Modifiable Areal Unit
Problem [9]. Furthermore, it is known that the level of data
aggregation can affect aspects of task complexity such as
information load and the user’s ability to recognize patterns
within the data [10]. To combat problems associated with
areal aggregation, dasymetric mapping focuses on using
zonal boundaries that are based on sharp changes in the
statistical surface being mapped [11]. However, even when
grouping data into small spatial quadrats, data can either be
overaggregated or underaggregated. A third option is to
estimate the discrete event points as a continuous function
(e.g., Fig. 1c); such a mapping, however, only allows for the
use of color as a means of representing data variables. As an
encoding based on underlying network data, Fig. 1d shows
a traditional line map. However, its representation is still
restrained by the color and thickness of the lines.
To increase the amount of information that can be
visualized within the constraints of a thematic map, this
paper explores a novel method of multivariate encoding.
Inspired by ideas of symbolic encoding from Spence [12]
and choices of visual encodings by Wilkinson [13], we have
developed the bristle map (Fig. 1e), a novel method for the
aggregation, abstraction, and stylization of geographically
located spatiotemporal data. The bristle map consists of a
series of straight lines extended from and connected to
linear map elements (roads, train lines, subway lines, etc.)
that have some contextual relationship with the data being
Published by the IEEE Computer Society

KIM ET AL.: BRISTLE MAPS: A MULTIVARIATE ABSTRACTION TECHNIQUE FOR GEOVISUALIZATION

1439

Fig. 1. Data abstraction in geovisualization. In this image, we show crimes in West Lafayette and Lafayette, Indiana, where the blue line
represents the Wabash River. (a) Plotting events as points. (b) Aggregation of points by areal units. (c) Approximation of a continuous domain
from point sampling. (d) Approximation of a continuous domain using solid lines applied to roads. (e) Our abstraction using a series of bristle
lines applied to roads.

visualized. We vary these lines with respect to their color,
length, density, and orientation to allow for a unique
encoding scheme that can be used to create informative
maps. With respect to the other representations shown in
Fig. 1, our technique utilizes the underlying geographical
context as a part of its symbology, thereby directly
incorporating geographical elements within its encoding
scheme. One of the major advantages of the bristle map
technique is that the basis domain of the data (e.g., street
network) remains highly visible regardless of the color scale
being used. If one compares Figs. 1c and 1e, the street
network in Fig. 1e is clearly visible because the lines only
“bristle off” to one side, whereas in Fig. 1c some streets are
hardly discernible due to the dark colors.
To demonstrate our technique, we focus on categorical
spatiotemporal event data (e.g., emergency department
logs, crime reports). In such data, events consist of locations
in time and space where each event fits into a hierarchical
categorization structure. These categories are typically
processed as time series and snapshots of time are
aggregated and typically visualized on a choropleth map
[14]. Past work [6], [15] has shown that the use of kernel
density estimation (KDE) [16] is highly suitable in the
spatial analysis of such data. Thus, our approach incorporates kernel density estimation as a means of estimating the
underlying distribution of spatiotemporal events. Using the
estimated distribution in an area for a given category (or
categories) and temporal unit, we incorporate the underlying geographical network structure into the visual
encoding. Bristles are extended from this underlying
structure, and the color, length, density, transparency, and
orientation of each bristle are mapped to a particular
variable (or set of variables). Schemes presented in this
paper include combinations of the following mappings:
.
.
.
.
.

length, density, and color as data magnitude,
orientation and coloring for bivariate mapping,
color and length for bivariate mapping,
color and density for bivariate mapping, and
length and transparency for temporal variance.

Given the available parameters for visual encoding
within the bristle map, other encodings also exist, which
illustrate the flexibility and power of our technique. Our
work focuses on showing how bristle maps can be used to
show spatial and temporal correlations between variables,
encode uncertainty in a unique and aesthetically informative
way, and maintain geographical context through linking our
visual encoding directly to geographical components. As
such, the bristle map is a powerful multivariate encoding
scheme that is adaptable to various attribute encodings to
create richly informative visualizations.

2

RELATED WORK

Many techniques in multivariate data visualization focus on
a means of reducing clutter and highlighting information
through a variety of approaches including filtering (e.g.,
[17]), clustering (e.g., [18]), and sampling (e.g., [19]). In this
section, we focus particularly on techniques within geographical visualization for improving the understanding of
thematic/statistical maps, as Wilkinson [13] noted that the
problem of multivariate thematic symbology for maps is
that they are not only challenging to make, but also
challenging to read.
In geographical visualization, the most common means
of data representation is the choropleth map in which areas
are shaded or patterned in proportion to a measured
variable. Such maps are typically used to display only one
variable, which is mapped to a given color scale. Other
research has focused on encoding multivariate information
into choropleth maps (such as uncertainty) with textures
and patterns [20], creating bivariate color schemes for
visualizing interactions between two variables [21], [22], or
animating choropleth maps to enhance the exploration of
temporal patterns and changes [23]. We present bristle
maps as a robust alternative to these schemes in which
multivariate attributes are instead mapped to a variety of
graphical properties of a line (length, density, color, and
orientation), as opposed to utilizing a bivariate color
scheme, texture overlays, or animation.

1440

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

More recent geographical visualization techniques have
included extensions to choropleth mapping ideas. HaghShenas et al. [24] compared the effectiveness of visualizing
geographically referenced data through the use color
blending (in which a single composite color conveys the
values of multiple color encoded quantities) and color
weaving methods (in which colors of multiple variables are
separately woven to form a fine grained texture pattern).
The results from their study indicate color weaving to be
more effective than color blending for conveying individual
distributions in a multivariate setting. Saito et al. [25]
proposed a two-tone pseudo coloring method for visualizing precise details in an overview display. Under this
scheme, each scalar value is represented by two discrete
colors. Sips et al. [26] focused on revealing clusters and
other relationships between geospatial data points by their
statistical values through the overplotting of points. This
work was later extended [27] to combine a cartogram-based
layout to provide users with insight to the relative
geospatial positioning of the data set while preserving
cluster information and avoiding overplotting. Other cartogram techniques include the WorldMapper Project [28]
which is used to represent social and economic data of the
countries of the world. In each of these, novel data
visualization techniques are created; however, the distortion of spatial features (country boundaries, roads) is often
undesirable. While these techniques focus on displaying
large amounts of aggregate data on small screens, our
technique focuses on enhancing details of geographical
context within the data. A similar concept of preserving
data context is found in Wong et al.’s [29] GreenGrid in
which they visualize both the physics of the power grids in
conjunction with the geographical relationships using
graph-based techniques.
Along with the previously described map schemes and
cartogram distortions, there has been work in the use of
heatmaps based on spatial data. Fisher [30] applied
heatmaps to visualize the trends of the interactions of users
with interactive maps that are based on their view of the
geographic areas. Maciejewski et al. [6] used heatmaps as
one of the tools to find aberrations or hotspots that facilitate
the exploration of geo-spatial temporal data sets. Work by
Chainey et al. [15] illustrated a number of different mapping
techniques for identifying hotspots of crime and demonstrated that kernel density estimation provides analysts with
an excellent means of predicting future criminal activities.
In conjunction with previous visualizations, other research has focused on expanding the dimensionality of the
data being displayed by utilizing three-dimensional visuals.
Van Wijk and Telea [7] utilized color and heightfields to
visualize scalar functions of two variables. Tominski et al.
[31] explored embedding 3D icons into a map display as a
means of representing spatiotemporal data. In contrast, our
work focuses on a two-dimensional encoding scheme that
incorporates a variety of the visual variables described by
Bertin [32] and Wilkinson [13] as a means of representing
multivariate data.
Finally, it is important to note that our technique is akin
to traditional traffic flow maps (e.g., Fig. 1d) seen in a
variety of atlases; however, provides more generalized

VOL. 19,

NO. 9,

SEPTEMBER 2013

schemes. In traffic flow maps, the amount of data that can
be displayed is restrained by the color and the width of the
line representing linear elements (i.e., roads) on the map.
Our work is similar to that of the traffic flow maps in that
we utilize width (specifically, matched to the length in our
bristle maps) and color as underlying visual variables of our
encoding. However, our work also incorporates bristle
density as a means of further encoding parameters. In the
following sections, we compare our encodings to a variety
of methods including the point, color, and flow line maps.

3

BRISTLE MAP GENERATION

In Fig. 1, we developed our motivation for the need to
directly incorporate geographic features to the underlying
data to better preserve contextual information. It is clear
that the aggregation of data into arbitrary geographical
areas obscures data, while the continuous approximation of
an underlying data source can lead to incongruent mappings with respect to geographic features. Furthermore,
both these mappings are limited in the fact that only color
and texture are available for variable encoding, limiting the
amount of data that can be displayed to either a single
variable or possibly two variables in the case of a bivariate
color map. The goal of this work is to create visual
encodings for higher order structures.
The bristle map was inspired by the Substrate simulation
of Tarbell [33] and abstract renderings of map scenes in
work by Isenberg [34]. Given these images, our work
focuses on using the underlying visual properties
to intelligently encode information for display. In The
Grammar of Graphics [13], Wilkinson discusses the combination of several perceptual scales into a single display. Here,
he notes the idea of separable dimensions of the data is a
key issue, where discriminations between stimuli are of key
importance in the visualization. The Substrate aesthetic
directly lends itself to this approach as color, line length,
and orientation are distinct classes within Wilkinson’s table
of aesthetic attributes and each of these visual parameters
directly contributes to the substrate aesthetic.
Fig. 2 illustrates the bristle map generation pipeline.
Given underlying data events, we compute a continuous
distribution. We also create a topology graph from given
geographically relevant linear content for clutter reduction
described in Section 5. As an example of geographical
content, if the underlying data was water pollution we
could use a city sewage map for the geographic components, for our crime data examples we use roadways. Each
linear geographic component consists of a series of line
segments, and we extend bristle lines from these line
segments. These bristle lines emerge perpendicularly from
the underlying geographical line segment and are allowed
to vary in length, density, color, transparency, and orientation, to facilitate multivariate data encoding. The third stage
of the bristle map generation pipeline (Fig. 2) illustrates the
bristle line concept for each geographical line segment, SE,
and P1 P2 defines our generated bristle line. Each bristle line
is created using the vector equation of a line as shown in
!
!
P2 ¼ P1 þ Vl Ll ¼ P1 þ Vl ðt  Llmax Þ:

ð1Þ

KIM ET AL.: BRISTLE MAPS: A MULTIVARIATE ABSTRACTION TECHNIQUE FOR GEOVISUALIZATION

1441

Fig. 2. The bristle map generation pipeline. Beginning with data events, a continuous abstraction is created. We also create a topology graph from
contextually important linear features (in this case roads). Next, bristles are extended from these features based on the continuous abstraction and
the topology. Clutter reduction is performed when generating each bristle, and finally the resultant bristle map is generated.

Here, P1 is a point on the contextually relevant geographic
!
line segment, SE, Vl is a unit vector perpendicular to the
line SE, and Llmax is the maximum length of the P1 P2 . Ll is
the length of the P1 P2 determined by a parameter t.
Each line from P1 to P2 is drawn in such a manner that it
will either encode different properties of a multivariate data
set, or use a data reinforcement technique where properties
are encoded to the same variable to provide redundant
cues. We utilize three encoding properties for each bristle:
length, color, and orientation. The length of a line P1 P2 is
separated into two portions: a constant component, which is
proportional to the magnitude of the variable being
encoded, and a variance component. It can also capture
other properties such as level of certainty. The color of a
bristle P1 P2 is proportional to the underlying variable
distribution to be encoded at point P1 . When the variance
component is used, its transparency is adjusted as a means
of visually distinguishing it from the constant component.
Orientation of the bristle line is always perpendicular to SE
and is utilized for bivariate comparison (i.e., day/night, two
data types) and/or clutter reduction. To summarize, length
and color represent a local data magnitude property at
point P1 . We also choose to encode redundant information
into the density of the number of bristles placed on a given
line segment, where the density of the bristles along SE is
decided by an average data value on a line segment SE.
For each visual encoding, the underlying data is assumed
to be continuous over a given geographical segment, such
that for all points between any two nodes on the underlying
contextual geographic structure, a data distribution value is
associated with the point. In the case of a discrete data set
(e.g., crime locations), the choice of an appropriate means of
data interpolation with regards to the underlying geographic information is dependent on the data analysis being
performed. Based on the recommendations of Chainey et al.
[15], we apply a kernel density estimation [16] to approximate the underlying distribution of crimes over the
geographic features. The kernel density estimation procedure used is defined by the following equation:


N
1X
1
x  Xi
^
K
:
fðxÞ ¼
N i¼1 h
h

ð2Þ

Here, the window width of the kernel placed on point x is
proportional to a window bandwidth, h, and the total
number of samples, N. We utilize the Epanechnikov
kernel [16]:
3
KðuÞ ¼ ð1  u2 Þ1ðjjujj1Þ ;
4

ð3Þ

where the function 1ðjjujj1Þ evaluates to 1 if the inequality is
true and zero for all other cases.
Thus, given a multivariate data set where locations in
space and time correspond to a series of categorized events,
we can create bristle maps that encode various properties of
the data. Note that this technique relies on the data being
contextually relevant to an underlying geographical network. For example, crime event data with its 2D geographical coordinates is recorded and hence defined by
addresses on streets; thus, it is contextually relevant to a
street network. Data sets in which this contextual relationship does not exist should utilize other visual encoding
schemes. Table 1 shows the parameters in our bristle map
and their corresponding potential variables being encoded
to each parameter. In the following section, we present a
series of potential parameter combinations for various
bristle map encodings and discuss the various results.
TABLE 1
Parameters, Corresponding Variables, and Ranges

1442

4

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

ENCODING SCHEMES

The bristle map is a powerful visual encoding scheme that
lends itself to a variety of data encodings, examples of
which we present next. For demonstration purposes, we
employ categorical spatiotemporal police reports collected
in Tippecanoe County (specifically West Lafayette and
Lafayette, IN, USA), from 1999 to 2010. The data set
contains the date, time, crime type (e.g., armed/unarmed
aggravated assault, armed robbery, burglary, homicide,
noise, other assaults, rape, rape attempted, residential entry,
robbery, theft, vandalism, and vehicle theft), and the
address of each recorded criminal event. Note that other
data sets can be easily encoded with bristle maps, and our
choice of data was only made to illustrate the technique.
Utilizing this multivariate crime data set, we discuss
potential encoding schemes for multivariate spatiotemporal
data. We then provide illustrations of each described
encoding scheme with respect to our crime data set.
Encoding schemes presented in this section include the
use of bristle color, length, and density to encode data
magnitude, the use of bristle orientation to inform temporal
comparison, and the encoding of temporal variance in the
bristle lengths.

4.1 Color, Length, and Density as Data Magnitude
Here, we discuss our technique for encoding the color,
length, and density of the bristles into two separate variable
groups. As both color and length (size) fall into two distinct
categories of aesthetics according to Wilkinson [13], the use
of separate variables for both categories allows for a
distinguishable visual data encoding. In both cases, we
assign data magnitude to both a color scale and a length
scale. We note that such an encoding scheme has the
potential to portray data more effectively than visualizations that map each data variable to a single display
parameter. As noted in the arguments for the use of
redundant color scales by Rheingans [35], the use of
different display parameters is able to convey different
types of information. Furthermore, by combining encodings
in a redundant manner, it is possible to reinforce the
encoding scheme. The utility of redundant color scales was
confirmed by Ware [36].
In our encoding scheme, each bristle line’s length, Ll , is
calculated using (4) based on a parameter, t, and the
maximum length, Llmax :
Ll ¼ t  Llmax ¼ ð  P1 þ   P1 Þ  Llmax :

ð4Þ

For this visual encoding of the bristles, the parameter t is
defined by the ratio of the data value at P1 , which we call
P1 , the ratio of the temporal variance at P1 , P1 , and a set
of tuning parameters ( and ) that provide weights to the
constant and variance components as shown in Fig. 2. In
this work, we use  ¼ 1:0 and  ¼ 0:3. Note that the
choice of encoding the variance at a 30 percent value was
chosen through trial and error by generating visualizations
that the authors found to be the most useful and
aesthetically pleasing. For problems where determining
exact data values from the visual encoding is required (as
opposed to approximating high and low rates), the
variance portion is removed from the equation entirely
by using  ¼ 0:0. As such, by creating the encoding
scheme with diverse parameters, we are able to generate

VOL. 19,

NO. 9,

SEPTEMBER 2013

more aesthetic choices and visualizations. It is important
to note that not all encodings will be appropriate and are
most likely task dependent.
The Llmax portion of (4) is defined in
!
r 1
1 NX
L
Llmax ¼   logb
:
ð5Þ
Nr i¼0 SE
In this equation, we take the average length of all line
segments (where Nr is the total number of line segments in
the map) and calculate Llmax using a nonlinear function
such that the length of bristle lines does not grow in an
unbounded manner when zooming in. Moreover, Llmax is
modified by the parameter , where  is the ratio of the
current zoom level to the initial zoom level, to decouple our
technique with the zoom level. In this work, we use b ¼ 15
for the base of a log function.
Next, we determine the number (or density) of bristles,
Nl , to be drawn on each line segment SE using



ð6Þ
Nl ¼  LSE SE :

Here, Nl is calculated using two user-defined constants  and
, where  is the unit geographical length (distance) and  is
the number of bristle lines per unit geographical length. We
use  ¼ 0:0009 and  ¼ 3-15 in our current visualization. As
the bristle density may also be used to encode data magnitude
parameters in bristle map generation, Nl should be proportional to the ratio of average data value on SE, SE . Moreover,
we also apply  such that Nl will be independent of the zoom
level to preserve the extent of density.
For color, we allow users to choose either a continuous or
a sequential color scheme from Color Brewer [37]. Then,
data are linearly mapped to a probability that a crime of type
A will occur at geographic point B, where the probability is
estimated from the underlying data distribution using
kernel density estimation as described in Section 3.
Fig. 3 illustrates our length, density, and color encoding
using the previously described crime data set. Burglary is
encoded with the red color scheme, and color is proportional to the probability (calculated from the underlying
point distribution using kernel density estimation) that a
burglary occurred at a given location. Fig. 3(left) shows our
bristle map encoding for burglary rates with a color scheme
and bristle density, and Fig. 3(right) shows a line map
encoding the same information with a color scheme and line
thickness for comparison to our bristle map. Compared to
this line map, our bristle map provides the advantages of
additional dimensionality through the density of bristle
lines. In this scheme, one is able to easily encode two
variables in different combinations of bristle map parameters (i.e., color and density with a constant length, color
and length with a constant density), and provide users with
distinguishable visual parameters that seem to focus
attention to various details.

4.2

Multivariate Encoding: Separating Length,
Density and Color, and Using Orientation
In the previous section, we illustrated how our method can
be utilized for univariate encoding by using a redundant
encoding scheme. However, a major benefit of bristle maps

KIM ET AL.: BRISTLE MAPS: A MULTIVARIATE ABSTRACTION TECHNIQUE FOR GEOVISUALIZATION

1443

Fig. 3. (Left) Our bristle map encodes burglary rates with both bristle color and bristle density. (Right) A line map encoding burglary rates as both line
color and line thickness. Compared to the line map, our bristle map provides a distinguishable visualization by incorporating bristle density. For
example, bristle lines on the right top area are easily identified, whereas thickness in the line map on the same area is too small to clearly be perceived.

Fig. 4. Encoding daytime versus nighttime variations. (Left column) From top to bottom, color maps showing day, night, and the difference of day and
night burglary rates. (Right) Our bristle map separating the burglary rates into their day and night components with opposite orientation along roads.
Note that a color map cannot present two components (i.e., both daytime and nighttime burglary rates) at the same location, hence three color maps
are needed to see day and night variations simultaneously. Our bristle map can present such information within one bristle map by using different
orientations of bristle lines.

is the ability to encode multivariate attributes. One example
of this is seen in day versus night time comparison.
Here, one can utilize the orientation to separate two
temporal components of a single variable by mapping the
temporal components to different orientations of the
geographic feature. For instance, it is likely that the rates
of data variable will be different with respect to day and
night occurrences. We illustrate this visual encoding in
Fig. 4. We separate the events into day (6:00 am-6:00 pm)

and night (6:01 pm-5:59 am) and map the daytime rates to
red and one orientation, and nighttime rates to blue and the
other orientation. In Fig. 4(right), we illustrate a bristle map
encoding of one variable (burglary) during 2009 where
length, density, and color represent the magnitude of the
burglary as well as the encoding of day and night
parameters is explored as line orientation.
In Fig. 4(right), we show areas of high/low nighttime
crime, high/low daytime crime, and combinations there

1444

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

within. In contrast, a traditional heatmap using a univariate
color scheme can only show either daytime crime (Fig. 4(left
top)), or nighttime crime (Fig. 4(left middle)). Hence,
several heatmaps are needed to see day and night variations
as shown in Fig. 4(left column). Viewers must mentally
combine the images to locate regions of the map that have
high crime levels at daytime and nighttime, thereby
increasing their cognitive load.
Another means of reducing the cognitive burden would
be to create a heatmap of the difference between night and
day. Fig. 4(left bottom) shows the difference of day and
night data, and the divergent color scheme shows where
high daytime or high nighttime crimes occur. For instance,
in Fig. 4(left bottom) the right area indicates higher rates
during day, the left area shows higher rates during night,
and the border area between the blue and red color schemes
only indicates that day and night rates were approximately
equal, regardless of them being low or high. Moreover, you
need other color maps to explore areas, where one occurs
similarly high or low during day and night time.
Bristle map encodings have benefits in this situation.
When we explore a daytime versus nighttime bristle map in
Fig. 4(right), we see that there exists distinct temporal
profiles along the road lines, where we see exclusively
dominant areas during either day or night. For instance, see
the diagonal road from the top center to the right center
(Main Street, Lafayette, IN) showing that daytime burglary
dominates along this road. Another observation is made on
the horizontal road at the center of the map (Central Street,
Lafayette, IN). Along this road, daytime burglary rates
increase from left (west) to right (east), whereas nighttime
burglary rates decrease from left to right. For the center area
in Fig. 4(left bottom), where the blue and red color schemes
meet, we also see in Fig. 4(right) that it has relatively
equally high rates during both day and night. Such a
comparison allows people to understand the differences
between the data; however, when subtracting, areas of
nearly equal daytime and nighttime crimes will be colored
the same. Thus, areas that are safe during both day and
night, and areas that are highly dangerous during both day
and night will appear the same in the difference color map.
In contrast, bristle maps allow viewers to quickly observe
trends related to both day and night.
Another example of multivariate encoding using our
bristle map is done by separating and/or combining bristle
parameters. For instance, bristle density (or length) encodes
a variable A, and color encodes a variable B while being
presented on one orientation. Similarly, another two
variables (C and D) could be encoded and presented on
the other orientation. However, this type of parameter
combination should be determined carefully so as not to
increase viewers’ cognitive load. Its effectiveness would
depend on several factors such as data type and analysis
purpose. In Section 6, we conduct experiments to explore
the effectiveness of different parameter combinations.

4.3 Encoding Data Variance
As introduced in Fig. 2, each bristle can include a portion
generated for temporal variance of data, see (4). To present
the temporal variance of the data over time, we compute
both the monthly and yearly mean and variance values. For

VOL. 19,

NO. 9,

SEPTEMBER 2013

a given discrete data set during time periods NT , we first
calculate continuous distributions over time. Then, we
determine mean and standard deviation values with respect
to the underlying data distribution for the entire data set
over a given temporal aggregation. Thus, we calculate the
mean 	 and variance 
 values from time varying data Ki ,
where i 2 ½0; NT  1. Note that 	 and 
 are computed only
once as they represent constant values for a given data set.
Mean and variance values for each grid point j are
calculated using (7) and (8), respectively. Variance is then
used to weight the parameter  in (4) such that given the
data magnitude at the current time Kcur , we compute the
ratio of variance at the current time, 
~ as shown in (9). As
such, the parameter t in (4) can be detailed as shown in (10)
to represent the length of bristle lines with respect to
temporal variance:
	½j ¼

T 1
1 NX
Ki ½j
NT i¼0

ð7Þ

vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u
T 1
u 1 NX
ð	½j  Ki ½jÞ2

½j ¼ t
NT i¼0

~½j ¼

1
j	½j  Kcur ½jj

½j

ð8Þ

ð9Þ


t ¼   P1 þ   P1 ¼   P1 þ  



~P1
:

~max

ð10Þ

Furthermore, the variance term, P1 , in parameter t in (4)
can also be revised to encode an uncertainty factor by using
randomness. We may also encode an uncertainty factor by
using color and transparency to enhance the variance
component. When using color and transparency, we use a
highlight color for the variance component, and then fade
out the variance component over the bristle length with a
full alpha value for one end point and an alpha value
weighted by the variance for the other end point. The
constant portion of the bristle is assigned an alpha value of
1 to both end points as it represents an exact data value.
Hence, according to the data type and analysis purpose, the
encoding of parameter t and the use of the variance portion
can be different and should be assessed with respect to the
visual message trying to be conveyed. Fig. 5 illustrates the
application of encoding the data variance of vandalism with
the uncertainty factor. In Figs. 5a and 5c, we use the same
color scheme for the constant and variance portions of
bristle lines. To enhance the variance component in Figs. 5b
and 5d, we highlighted the variance portion in a different
color and assigned full alpha values for the constant portion
of bristle lines. Figs. 5a and 5b show the same area. In this
area, the bristle length shows large fluctuations, indicating a
high yearly variance Figs. 5c and 5d show another area. In
this area, the bristles are of a nearly constant length,
indicating low yearly variance. When considering that the
area in Figs. 5a and 5b includes residential areas, while the
area in Figs. 5c and 5d includes the downtown Main street,
an art theater, and the City Hall in Lafayette, IN, our bristle
map shows that the residential areas have higher yearly

KIM ET AL.: BRISTLE MAPS: A MULTIVARIATE ABSTRACTION TECHNIQUE FOR GEOVISUALIZATION

1445

Fig. 5. Encoding data variance of vandalism-graffiti in Lafayette, IN, USA, in 2010 creating an uncertainty aesthetic. Yearly variance of vandalismgraffiti is represented in (a) a residential area and (c) a commercial area without distinguishing the variance component in the bristle length. Parts (b)
and (d) show the results using a highlight color for the variance portion and full alpha values for the constant portion of bristle lines. Here, we clearly
see that our bristle map can encode the temporal variance and create an uncertainty aesthetic using the variance component.

variance of vandalism (graffiti) when compared to commercial areas.

5

BRISTLE CLUTTER REDUCTION

Although our bristle map can encode various characteristics
from multivariate data, it often suffers from clutter around
the intersections of road lines. To minimize cluttering, we
employ two strategies in our bristle map generation
pipeline (Fig. 2): 1) using topology among road lines
to determine bristle orientation to minimize clutter and
2) cutting bristle lines crossing neighbor road lines.

5.1 Using Topology
Each bristle map contains an underlying topology of the
contextual geographic network that the data re mapped to.
In the topology graph, each node is defined as either
“outward” or “inward” as illustrated in Fig. 6. Using the
topology graph, we choose each segment’s bristle line
orientation such that the overlap of the bristles at intersections will be minimized, thereby reducing the clutter. If the
encoding scheme requires both sides of the edge to contain
bristles, then clutter at each intersection is inevitable.
However, in cases where bristles map to only one side of
an edge, we use the right-hand rule to decide the orientation.
Hence, bristle lines on edges connected to neighboring

Fig. 6. To minimize clutter, a topology graph consisting of directed edges
as road lines and outward (red) and inward (blue) nodes on the
intersection of lines is used to decide the bristle line orientation.

outward and inward nodes are generated in a manner that
provides a reasonable reduction in clutter (Fig. 6).
Choosing the orientation of bristle lines to minimize
overlap can be considered as a 2-coloring problem in vertex
coloring; one color presents “outward” while the other
presents “inward.” Vertex coloring is a well-known graph
problem, where no two adjacent nodes share the same
color. Moreover, coloring a general graph with the minimum number of colors is known to be an NP-complete
problem. In our case, the minimum number of colors
should always be 2 but such 2-colorability is not guaranteed
for general road lines. While deciding the orientation of
bristle lines, we often have undesirable topology generating
inevitable overlap of bristle lines. Fig. 7 (upper row) shows
such a bad topology example and our strategy to solve this
issue. In Fig. 7a, we see two clutter areas caused by an
undesirable configuration of neighbor nodes, which guarantee bristle overlap. To solve this, we consider the addition
of a virtual node in a topology graph as shown in Fig. 7b,
thereby allowing for an orientation switch midway across

Fig. 7. Two pairs of the cluttering cases and our methods to minimize
clutter. Colored box areas on a side of each edge line indicate the
orientation for bristle lines. (a) Case 1: bad topology, where two inward
nodes (blue) share a line and two outward nodes (red) share a line,
generates inevitable clutter. (b) Virtual nodes (dotted circles) are added
to split an edge line. (c) Case 2: a small angle between edge lines
causes a clutter area. (d) Bristle lines crossing a neighbor edge line are
cut on the neighbor line.

1446

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

VOL. 19,

NO. 9,

SEPTEMBER 2013

Fig. 8. Before/after image pairs of our clutter reduction. Each pair shows a case of (a) changing bristle orientation using topology, (b) cutting bristle
lines crossing neighbor road lines, (c) circular roads, and (d) curved roads.

the edge and reducing the clutter. For neighboring two
inward nodes (blue), we add a virtual outward node (red
dotted circle) at the road line connecting two inward nodes
resulting in splitting bristle lines on the road line. Similarly,
a virtual inward node (blue dotted circle) is added for
neighboring two outward nodes (red).

5.2 Avoid Crossing Neighbors
Another cluttering case is illustrated in Fig. 7c. When two
road lines intersecting with less than a 90 degree angle have
bristle lines, some of the bristle lines overlap as illustrated
in Fig. 7c. For this case, we forbid bristle lines to cross
neighbor road lines by placing the end point of a bristle line
on the neighbor road line as shown in Fig. 7d. We first check
the intersection of bristle blocks (colored boxes in Fig. 7) for
the current road line on which we are generating bristle
lines and its neighboring road lines by using the topology
graph. If the blocks are intersected, we then check if a bristle
line crosses the neighbor road lines by utilizing the
intersection algorithm of 2D line segments [38]. This idea
is based on the theory of amodal completion (or amodal
perception) [39] in psychology that describes how the
human visual system completes parts of an object even
when it is only partially visible. Although the length of a
bristle line represents data magnitude, benefits from cutting
the length to avoid clutter dominate the side effects from
data misunderstanding that could be caused by clutter.
Moreover, when using redundant encoding utilizing bristle
length and density as data magnitude, bristle density could
help viewers complete parts of the bristle lines. Fig. 8 shows
four image pairs before and after applying our clutter
reduction strategies. Some improvements could also be
considered in the future. For instance, our strategies still
generate cluttered bristle lines in cases where road lines are
very dense or close to others. We perform experiments in
Section 6 to see how people understand the differences
before and after clutter reduction. Here, we note that the
experiments performed were for comparison and identification tasks. In these task types, line direction (as will be
shown in the experiments) had little impact on the user
results. However, in a cluster/delineate task in which users
are asked to segment the data, the splitting of direction may
influence the user’s perception of cluster boundaries. As
such, we recommend that map designers take caution in
employing this scheme and use it only in appropriate map
contexts. Future work will explore other schemes and
design issues to handle neighbor crossings and influence
on map design.

6

training session. In the first study, five tasks were
conducted to evaluate the efficiency of bristle maps
compared to existing visualization methods (point, color
(kernel density estimated—KDE), and line maps as shown
in Figs. 1a, 1c, and 1d) and post-task questionnaires for
qualitative feedback. In the second study, two tasks were
conducted to evaluate the accuracy of users in estimating
values from each of the map types (point, KDE, bristle, and
line) as well as evaluating the perceived aesthetics of each
image. Prior to each study, a pilot study was also conducted
to ensure that each task contains a fair comparison among
the techniques.
Participants. In the first study, thirty graduate students
(23 males, seven females) in engineering, science, and
statistics from our university participated in the study. All
participants reported that they had experience in visualizing data on geographical maps using colors or icons
(e.g., paper maps, online map services). The experience
varied from almost daily (11 participants), 1-2 times a week
(17 participants) to 1-3 times a month (two participants). For
the identification/accuracy tasks and aesthetic comparisons
(Tasks 6 and 7), a secondary study was run on 26 undergraduate students in engineering from our university.
Apparatus. The experiment was performed on a 3000
monitor using our experimental application running on
Windows XP, as shown in Fig. 9, where all visualizations
were generated with 2,2281,478 resolution. Each visualization was overlayed with numbered circles as shown in
Fig. 9. Participants selected one of the numbers to answer
the question in each trial using buttons in the interface
panel on the top of the screen. Criminal incident reports
collected in West Lafayette and Lafayette, Indiana from
1999 to 2010 were used in each trial, but different types of
crimes were selected to generate visualizations in the
training phase and in the actual study.
Design. We employed a repeated measure design of
tasks incorporating variations of the images shown in
Figs. 1a, 1c, and 1d and line maps similar to those of

EVALUATION

To evaluate the effectiveness of our bristle maps, we
conducted two quantitative controlled experiments. These
studies are both comprised of an introductory session, and a

Fig. 9. Example setup for our experiment.

KIM ET AL.: BRISTLE MAPS: A MULTIVARIATE ABSTRACTION TECHNIQUE FOR GEOVISUALIZATION

TABLE 2
The Number of Data Sets, Techniques
(Cases in Fig. 8 for Task 5), and Trials

Fig. 3(right). Table 2 shows the number of data sets,
techniques (cases as shown in Fig. 8 for Task 5), and trials
in each task. For example, in Task 1 we utilized 18 different
data sets to compare five different techniques (i.e., point
map, color map, line map, and bristle maps using two
different encoding schemes). Hence, each participant
performed 185=90 trials in Task 1. In Task 3, we
compared six different techniques (i.e., point map, bivariate
color map, line maps in two different encoding schemes,
and bristle maps in two different encoding schemes) with
15 data sets, resulting in 90 trials. Due to the difficulty of
creating good examples to be used from our real crime
data, we used fewer data sets in Tasks 4 and 5. In
summary, each participant performed a total of 374 trials in
Tasks 1 to 5, and it generally took 90 minutes.
Since the design of Tasks 1-5 focused on questions of
comparing regions, a secondary study was also conducted.
This study was again a repeated measure design of tasks
incorporating variations of the images shown in Figs. 1a, 1c,
and 1d and line maps similar to those of Fig. 3(right).
However, here the subjects were asked to identify the values
of regions in the image. Areas of homogeneous visual
variables were circled in each image and the subjects were
asked to approximate the amount of crime per region. As a
final task, the subjects were simultaneously presented with
a point map, color map, bristle map, and line map and asked
to rank order the images based on their aesthetic values.
For all Tasks, trial order was varied using a magic square
method [40] in each task. Completion time and participants’
answers were recorded for a quantitative metric. The
collected data from each task was subjected to an analysis
of variance (ANOVA) test to determine if the average time
and accuracy of task completion were significantly different
among techniques. A Post-Hoc Tukey HSD test was then
performed to determine significance between the techniques. P-values reported in this study come from the
resultant Tukey HSD test. Before the study, participants
were introduced to our experiment application and the
techniques through an introductory session and a training
session. During the training session, participants could ask
questions and receive guidance in the use of the experiment
application and analysis of each visualization. Once the
training was completed, participants moved to the actual
study. After completing each task (Tasks 1 to 4) participants
were asked to answer the questionnaire to rate the
efficiency of the techniques using a five-point Likert scale
[41]. After completing Task 5, participants were also
asked to describe their impression with regards to visual

1447

complexity for before and after image pairs applying our
clutter reduction. In the questionnaire, we stated that the
visual complexity is high if a participant felt any kind of
difficulty or confusion in understanding the density, length,
and color of bristle lines that encode the underlying data.
Finally, after finishing all tasks, participants were asked to
rate the overall efficiency among techniques.
Hypotheses. In this experiment, we hypothesized that our
bristle maps would be better than or equally as good when
compared to the other techniques in terms of task
completion time and accuracy. Specifically, we hypothesized that our bristle maps would be better than other
techniques as the complexity level of tasks increased from
univariate to multivariate. The rationale of this assumption
is that the line map and bivariate color map use at most two
variables, whereas the several encoding parameters in our
bristle map have the potential to create effective encoding
combinations. We also hypothesized that our clutter
reduction strategies would be useful to minimize cluttering
on areas where a large number of bristle lines are created. In
our follow-on experiment exploring identification of values,
we hypothesized that bristle maps would be as accurate as
all other representations in determining values. We also
hypothesized that bristle maps would be ranked higher in
terms of their aesthetics.
Tasks. We tested seven tasks: three for univariate,
bivariate, and multivariate data encoding, respectively,
one for temporal variance encoding, one for the clutter
reduction, one for accuracy comparisons among the rendering styles, and one for aesthetic comparisons.
In Task 1, when given four regions highlighted in circles
on the map, participants were asked to “find the region
with the highest crime rate” in different visualizations
representing spatiotemporal crime data using point, color,
line-T (data encoded in the line (T)hickness), bristle-CLD (a
redundant data encoding using (C)olor, (L)ength, and
(D)ensity), and bristle-LD (a redundant data encoding
using (L)ength and (D)ensity).
In Task 2, four regions were highlighted in circles on the
map. Participants were asked to “find the region with the
highest crime rates at both (or either) day and night time,”
using point (encoding day/night time crime rates in
different colors), color, line-TO (data encoded as line
(T)hickness and using (O)rientation for day/night crime
rates), bristle-CLDO (redundant data encoding using
(C)olor, (L)ength ,and (D)ensity, and using (O)rientation
to indicate day/night crime rates), and bristle-LDO (data
encoded using (L)length and (D)density, but in a constant
color, using (O)rientation to indicate day/night crime rates).
The point map had differently colored points for day and
night time crime rates, and two maps (day and night time
color maps) were given in different colors for the color map.
In Task 3, four regions were highlighted in circles on the
map. Participants were asked to “find the region with the
highest crime rates for both (or either) two crimes (crime 1
and 2),” using point map (encoding two crimes in different
colors), bivariate color map (Color-B), line-TO (a data
encoding using (T)hickness in different colors, and using
(O)rientation to indicate crime types), line-CT (encoding
crime 1 using (C)olor and crime 2 using (T)hickness),

1448

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

bristle-LDO (a redundant data encoding using (L)ength and
(D)ensity, and using (O)rientation to indicate crime types),
and bristle-CD (an encoding using (C)olor to indicate crime
1 and (D)ensity to indicate crime 2, with constant length).
In Task 4, participants were given two regions highlighted in circles on the map. Then, they were asked to “find
the region with the highest temporal variance” in different
visualizations using point maps, color maps, line maps, and
bristle-LDV (a redundant data encoding using (L)ength and
(D)ensity, and representing (V)ariance in the variance part
of a bristle line). For the point, color, and line maps, multiple
images were displayed on the screen to provide visualizations during several years. Our bristle map embedded the
variance in the variance part of the bristle length as shown
in Fig. 2 (third stage) and Fig. 5 (right column).
In Task 5, given two regions predefined in circles on
bristle maps, participants were asked to “answer if crime
rates on this given two regions look either different or the
same as each other.” Fig. 8 shows representative image pairs
before and after applying our clutter reduction method. In
trials, participants compared each case in Fig. 8 to a base
case (i.e., bristle lines on a single straight road).
In Task 6, subjects were presented with a series of images
with a single predefined circle, which covered an area
consisting of homogeneous visual variables (i.e., identical
color, bristle length, thickness, etc.). A univariate encoding
was explored, and the Bristle-CLD settings were utilized for
the bristle map. Participants were asked to estimate the
amount of crime in the area using the provided scale (or
scales in the case of bristle and line maps). Time and
accuracy of the results were measured.
In Task 7, subjects were presented simultaneously with
four images representing the same data set. These images
consisted of a point map, a color map, a bristle map, and a
line map. Subjects were asked to rank order the images in
order of most to least aesthetically pleasing.

7

RESULTS AND DISCUSSION

After all tasks were completed, times and answers
collected during the study were analyzed using a singlefactor ANOVA. A Post-Hoc Tukey HSD test was then
performed to determine significance between the techniques. P-values reported in this study come from the
resultant Tukey HSD test. For accuracy, the percentage of
correct answers was computed.
Task 1. A one-way between-subjects ANOVA was
conducted to compare the effect of different map visualizations on a subject’s time and accuracy in determining areas
with highest crime rates within a given visualization.
Conditions varied based on the given visualization, point
maps, kernel density estimated color maps, line maps, and
bristle maps. There was a significant effect of visualization
type on time at the p < 0:05 level for the conditions ½F ð4;
145Þ ¼ 35:366; p ¼ 0:0000001 and a significant effect of
visualization type on accuracy at the p < 0:05 level for the
conditions ½F ð4; 145Þ ¼ 3266:782; p ¼ 0:0000000006. Because
statistically significant results were found, we computed
a Tukey posthoc test with results reported in Table 3. In
Table 3, p-values < 0:05 indicate that groups were statistically different from one another.

VOL. 19,

NO. 9,

SEPTEMBER 2013

TABLE 3
Tukey HSD Results for Task 1

The result showed that the bristle maps groups were
both significantly different than the point, color, and line
maps in terms of speed (at the p < 0:05 level). Specifically,
the bristle map groups average times were 50.7 and
56.6 seconds for the CLD and LD conditions, respectively,
which was slightly faster than the Line-T condition at
69 seconds and much faster than the point map condition
at 102.6 seconds. However, the color map group was the
fastest at 34.6 seconds.
For accuracy, the bristle maps groups were both
significantly different than the point map group in terms
of accuracy (at the p < 0:05 level). Specifically, the bristle
map groups accuracy ratings were 99.6 and 99.8 percent for
the CLD and LD conditions, respectively, which was much
higher than the point map condition with accuracy of
41.4 percent. No accuracy differences were found when
compared to the other groups. See Table 8 for more
specific results.
The comparison between color maps and bristle maps
showed that color maps were better than the bristle map in
terms of average time, and were not significantly different
in terms of accuracy. This shows that bristle maps as a
redundant encoding scheme has the same potential to
convey data as single parameter encoding schemes; however, traditional schemes such as color maps may allow for
a quicker comparison in the univariate case.
Comparing Bristle-LD and Line-T, we saw that the
length of the bristle map matches the thickness of the line
map. Hence, the bristle density was useful to find answers
in Task 1 in terms of completion time and accuracy. Some
participants also mentioned bristle density in their qualitative feedback as “Bristle map is especially good when density of
the bristles is also used” and “In bristle map, length, and density
were more noticeable than color difference.” In this univariate
encoding test, the point map showed the worst results and
the color map was the best results in terms of time and
accuracy as shown in Table 8.
Task 2. A one-way between-subjects ANOVA was
conducted to compare the effect of different map visualizations on a subject’s time and accuracy in determining areas
with highest crime rates at both day and nighttime within a
given visualization. Conditions varied based on the given
visualization, point maps, kernel density estimated color
maps, line maps and bristle maps. There was a significant
effect of visualization type on time at the p < 0:05 level for
the conditions ½F ð4; 145Þ ¼ 2:717; p ¼ 0:032 and a significant effect of visualization type on accuracy at the p < 0:05
level for the conditions ½F ð4; 145Þ ¼ 89:89; p ¼ 0:0000002.
Because statistically significant results were found, we

KIM ET AL.: BRISTLE MAPS: A MULTIVARIATE ABSTRACTION TECHNIQUE FOR GEOVISUALIZATION

1449

TABLE 4
Tukey HSD Results for Task 2

TABLE 5
Tukey HSD Results for Task 3

computed a Tukey posthoc test with results reported in
Table 4. In Table 4, p-values < 0.05 indicate that groups
were statistically different from one another.
As we hypothesized, the result showed that the bristle
maps groups were both significantly different than the
point maps in terms of speed (at the p < 0:05 level).
Specifically, the bristle map groups average times were
86.3 and 87.2 seconds for the CLDO and LDO conditions,
respectively, which was slightly faster than the point map
condition at 106.2 seconds.
For accuracy, the bristle maps groups were both
significantly different than the point map group in terms
of accuracy (at the p < 0:05 level). Specifically, the bristle
map groups accuracy ratings were 90.5 and 93.3 percent for
the CLDO and LDO conditions, respectively, which was
much higher than the point map condition with accuracy of
63.1 percent. See Table 8 for more specific results.
The comparison between color maps and bristle maps
showed that color maps were better than the bristle map in
terms of average time, and were not significantly different
in terms of accuracy. This shows that bristle maps as a
reduandant encoding scheme has the same potential to
convey data as single parameter encoding schemes; however, traditional schemes such as color maps may allow for
a quicker comparison in the univariate case.
Findings also indicated that Bristle-LDO was better than
Line-TO in terms of accuracy, whereas Bristle-CLDO was
not significantly different from Line-TO in terms of
accuracy. This indicated that the bristle density seems to
be useful in finding correct answers in Bristle-LDO, but it
was not in Bristle-CLDO. Further testing in combinations of
visual variables and the ability to determine levels of
sparseness will be done in the future.
Task 3. A one-way between-subjects ANOVA was
conducted to compare the effect of different map visualizations on a subject’s time and accuracy in determining areas
with highest crime rates in two types of crimes within a
given visualization. Conditions varied based on the given
visualization, point maps, kernel density estimated color
maps, line maps, and bristle maps. There was a significant
effect of visualization type on time at the p < 0:05 level for
the conditions ½F ð5; 174Þ ¼ 6:655; p ¼ 0:00001 and a significant effect of visualization type on accuracy at the p < 0:05
level for the conditions ½F ð5; 175Þ ¼ 144:24; p ¼ 0:00000001.
Because statistically significant results were found, we
computed a Tukey posthoc test with results reported in
Table 5. In Table 5, p-values < 0.05 indicate that groups
were statistically different from one another.

The result showed that the bristle maps groups were
both significantly different than the point maps and color
maps in terms of speed (at the p < 0:05 level). Specifically,
the bristle map groups average times were 88.2 and
94.5 seconds for the LDO and CD conditions, respectively,
which was faster than the point map condition at 118.3 seconds and the color map condition at 115.3 seconds.
For accuracy, the bristle maps groups were both
significantly different than the point map group and the
color map group in terms of accuracy (at the p < 0:05 level).
Specifically, the bristle map groups accuracy ratings were
94.4 and 90.4 percent for the LDO and CD conditions,
respectively, which was much higher than the point map
condition with accuracy of 26.6 percent and the color map
condition with accuracy of 72.6 percent. See Table 8 for
more specific results.
Note that we separated parameters for different crime
types in Bristle-CD: (C)olor encodes crime 1 and (D)ensity
encodes crime 2. Bristle-CD showed a significant effect
compared to the bivariate color map as shown in Table 5.
However, generation on this type of bristle maps should be
selected carefully because one parameter could dominate
the other. For instance, when we use color and length to
separate two crime data, short bristle length for low crime
rates in crime 2 removes bristle lines in dark color for high
crime rates in crime 1. In our experiment, we selected color
and density for two crimes, with constant length of bristles.
Task 4. A one-way between-subjects ANOVA was
conducted to compare the effect of different map visualizations on a subject’s time and accuracy in determining areas
with high temporal variance within a given visualization.
Conditions varied based on the given visualization, point
maps, kernel density estimated color maps, line maps, and
bristle maps. There was a significant effect of visualization
type on time at the p < 0:05 level for the conditions
½F ð3; 116Þ ¼ 42:051; p ¼ 0:00001 and a significant effect of
visualization type on accuracy at the p < 0:05 level for the
conditions ½F ð3; 116Þ ¼ 42:33; p ¼ 0:00001. Because statistically significant results were found, we computed a
Tukey posthoc test with results reported in Table 6. In
Table 6, p-values < 0:05 indicate that groups were
statistically different from one another.
The result showed that the bristle maps groups were
both significantly different than the point maps, line maps
and color maps in terms of speed (at the p < 0:05 level).
Specifically, the bristle map groups average time was
48.4 seconds for the LDV condition, which was faster than
the point map condition at 194 seconds, the color map

1450

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

TABLE 6
Tukey HSD Results for Task 4

VOL. 19,

NO. 9,

SEPTEMBER 2013

TABLE 8
Average Time and Accuracy

TABLE 7
Average Rank Ordering by Aesthetics

condition at 171.8 seconds, and the line map condition at
178.9 seconds.
For accuracy, the bristle maps groups were both
significantly different than the point maps, line maps, and
color maps in terms of speed (at the p < 0:05 level).
Specifically, the bristle map groups accuracy rating was
94.7 percent for the LDV condition, which was much higher
than the point map condition with accuracy of 53.6 percent,
the color map condition with accuracy of 72.6 percent, and
the line map condition with accuracy of 75.5 percent.
See Table 8 for more specific results.
As we hypothesized, we found that the representation of
temporal variance in bristle maps was significantly faster
and accurate in terms of both average time and accuracy
compared to providing several images of the point, color,
and line maps. Moreover, we found that techniques showed
the increasing pattern from the point maps to Bristle-LDV
as shown in Table 8. This indicates that changes among
several images would be better perceived in line patterns
than in points or colors.
Task 5. A one-way between-subjects ANOVA was
conducted to compare the effect of different map visualizations on a subject’s time and accuracy in determining areas
with high temporal variance within a given visualization.
Conditions varied based on the given visualization, point
maps, kernel density estimated color maps, line maps, and
bristle maps. There was no significant effect of visualization
type on time at the p < 0:05 level for the conditions
½F ð1; 56Þ ¼ 0:328; p ¼ 0:569 and no significant effect of
visualization type on accuracy at the p < 0:05 level for the
conditions ½F ð1; 56Þ ¼ 0:315; p ¼ 0:315. In Task 5, we found
that bristle lines with and without clutter reduction did not
differ significantly w.r.t. both average time and accuracy for
all cases (Fig. 8). This means that the base bristle lines and
bristle lines before applying clutter reduction and the base
and bristle lines after applying our clutter reduction are
perceived similarly by participants. Moreover, when told
that the bristle line orientation does not encode data, the
opposite orientations of bristle lines on a single straight
road caused by virtual nodes (Fig. 7b) did not affect
accuracy (87.7 percent). Other cases showed 42-58 percent
of accuracy.
Task 6. For Task 6, we hypothesized that subjects would
be as accurate as all other representations in determining

values. In Task 6, we found that bristle maps did not differ
significantly w.r.t. accuracy when compared with point
map, color map, and line map identification (ANOVA
results of p-value ¼ 0:18093, F ¼ 1:63). However, we found
that bristle maps did differ significantly w.r.t. time when
compared with point map, color map and line map
(ANOVA results of p-value ¼ 0:0314, F ¼ 2:622). Particularly, we found line maps and heat maps to both be
significantly faster than point maps and bristle maps in
identifying values (Tukey HSD test value of p < 0:05).
Overall, these results indicate that in terms of accuracy, all
geographical representations were equally useful; however,
participants were (on average) over 1 second quicker in
value judgments on both line maps and colors maps. This is
most likely due to the fact that participants were quicker at
making color judgments as compared to counting points
and mentally linking multiple variables for the bristle maps.
Task 7. In Task 7, we found that users had a highly
variable rating of which image appeared to be more
aesthetically pleasing. The average positions and standard
deviations are summarized in Table 7. Here, we find that
while bristle maps have a slightly higher average ranking,
there is no significant difference between the aesthetic
ordering. A one-way between-subjects ANOVA was conducted to compare the rankings of map visualizations by
subject in determining which visualization was ranked
highest in aesthetics. There was no significant effect of
visualization type on aesthetics at the p < 0:05 level for the
conditions ½F ð3; 183Þ ¼ 1:79; p ¼ 0:149.
Qualitative evaluation. Fig. 10 shows the results from
qualitative feedback. Among the 30 participants, 27 participants (90 percent) agreed or strongly agreed that the
bristle map was efficient for day and night time comparison
in Task 2, 26 for two color maps and 23 for line map.
Twenty-four participants (80 percent) agreed or strongly
agreed that the bristle map was efficient for the comparison
of two crimes in Task 3, 26 for the line map and 19 for the

KIM ET AL.: BRISTLE MAPS: A MULTIVARIATE ABSTRACTION TECHNIQUE FOR GEOVISUALIZATION

1451

Fig. 10. Results from qualitative feedback for Tasks 2, 3, 4, and overall efficiency.

bivariate map. Twenty-nine participants (96.6 percent)
agreed or strongly agreed that the bristle map was efficient
for temporal variance representation. In the question for
overall efficiency, 27 participants (90 percent) agreed or
strongly agreed that bristle maps and color maps were
overall efficient, and 19 (63.3 percent) for line maps. For
point maps, 25 participants (83.3 percent) disagreed or
strongly disagreed.
Participants were also asked to answer visual complexity
and preference questions regarding the before (NCR) and
after (CR) image pairs applying our clutter reduction. For
the circular case (Fig. 8c), 96.5 percent of participants felt
that NCR has higher visual complexity and 78.5 percent
preferred CR. For the curved case (Fig. 8d), 65.5 percent of
participants answered that CR has a higher visual complexity and 64 percent preferred NCR. While both cases use a
technically identical clutter reduction algorithm, participants reported different visual complexity and preference
for them. This indicates that our clutter reduction could be
improved by considering the complexity of the underlying
network structure.
Summary and Limitations. As a univariate encoding, the
bristle maps were significantly different (in terms of speed
and accuracy) than the point, color, and line maps. In the
case of the point and line maps, bristle maps use resulted
in a higher average correctness and speed; however, the
color map for the univariate case had the fastest response
and accuracy totals. This seems to indicate that the
redundant encoding scheme is actually not beneficial in
these cases. As such, use of bristle maps for single variable
encoding is not recommended.
With regards to bivariate and multivariate encoding,
bristle maps and line maps outperformed color and point
maps. This is not surprising as bristle and line maps are able
to combine variables into a single image, whereas in the case
of point and color maps, the user must mentally combine
the two images together. Bristle-(C)LD also showed a
significant effect of the bristle density compared to Line-T.
As a bivariate encoding, using orientation in bristle maps
was not significant compared to two color maps. However,
in the comparison with the bivariate color map, Bristle-LDO
showed a significant effect in terms of average time and
accuracy. As such, we have that Bristle-(C)LDO as a

bivariate encoding scheme created a middle level of
cognitive load in-between two color maps and a bivariate
color map. Bristle maps also showed potential as a multivariate encoding technique in a single view. Based on the
results in Task 3, a point map using various colors and a
multivariate color map would considerably increase users’
cognitive load. In Tasks 1-3, we also observed that there is
no significant effect between the bristle maps using the
different encodings. The representation of temporal variance in the bristle map was significantly different from
other methods. Our results also showed the differences
among point, color, and line maps. Participants could better
find the region with higher temporal variance when using
line maps than using point and color maps. In the
qualitative evaluation, 90 percent of the participants agreed
or strongly agreed the overall efficiency of bristle maps to
find answers. However, users also strongly preferred the
color map in these cases as well.
Finally, we found that with regards to accuracy in
identifying values, no technique outperformed any others.
However, users were significantly faster in identifying
values in both the color and line map scenarios. We
hypothesize that in both cases the user focused only on
the color, whereas in the point map case they needed to
count the points and in the bristle map case they needed to
reconfirm the univariate value by double checking several
of the encoding legends.
Overall, this technique would be recommended when
encoding large amounts of multivariate spatiotemporal
point data. As the number of point samples increase,
aggregation techniques are need to allow for quick
summaries of the data, and, as is evidenced by our studies,
pure spatial location representation by glyphs results in too
much overlap for accurate measurement and evaluation. As
the number of variables increase, color map representations
allow for the encoding of variables only along a single
visual variable (resulting in bivariate color maps or small
multiple plots).
In using multivariate encodings, it is extremely important to understand the interaction effects that the visual
variables will introduce in one another. Research into the
perceptual interactions among different visual variables
was performed by Acevedo and Laidlaw [42]. They

1452

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

measured the perceptual interference of icon size, spacing,
and brightness, noting that brightness outperforms spacing
and size while being subject to interferences from both
spacing and size. Acevedo and Laidlaw also noted that
spacing also outperformed size, which contradicted some
previous results; however, this result seems to align with
our participants noting that the bristle spacing was a useful
cue. Their results were reportedly due to the spacing
sampling along a sinusoidal curve. The sampling of our
bristles follows a uniform pattern within classification bins.
Thus, there seems to be sufficient scientific evidence to
justify using sparsity as a discriminating variable in the
case of the bristle maps; however, further studies on this
are warranted. Stone [43] has also studied the effect of size
in color perception, noting that color appearance changed
dramatically with the size being viewed. As such, it may be
better to utilize fewer map classifications (color bins) when
using bristle maps to increase the perceptual distance
between each color being visualized.
The main limitations of the bristle map technique is that
the combinations of data encoding can potentially prove
overwhelming for the designer, and a poor choice on
variable encoding can result in a suboptimal visualization.
In particular, previous studies have provided results that
can be used to predict that certain combinations of visual
variables will either enhance or impede map reading. For
example, the combination of length and density form
an emergent property akin to Bertin’s definition of grain.
Such effects cannot be ignored; however, bristle maps can
be encoded to take advantage of such combinations, as
shown in Tasks 3 and 4.
Finally, with regards to scalability of the bristle map
technique, in areas of dense roadways, different aggregation methods would need to be considered. As the roads
become dense, the ability to plot lines of perceptually
different length would become untenable. However, a
solution to this would be to draw only the most important
roads, thereby removing smaller roads from the analysis, or
utilizing bristle maps in a focus+context manner.

8

CONCLUSIONS

In this work, we have described our novel multivariate data
encoding scheme, the Bristle Map. This scheme provides a
novel approach for encoding color, length, density, and
orientation as data variables and allowing the user to explore
correlations within and between variables on a single view.
Given the number of parameters available within this
encoding, this article has presented only a subset of potential
encodings and examples. Here, we have shown the use
of encoding bristle lines with redundant information,
multivariate attributes for variable comparison, and temporal variance. We also showed a means of potentially encoding
data uncertainty. To minimize overlap of bristle lines, we
generated a topology graph from underlying geographical
line features and employed strategies for clutter reduction.
Then, to evaluate the effectiveness of bristle maps, we
performed an evaluation study, where we explored different
visual encoding combinations within the bristle maps and
compared with existing techniques in several tasks. Based on
our experiment results, we believe that our bristle map
technique has much potential to increase the amount

VOL. 19,

NO. 9,

SEPTEMBER 2013

of information that can be visualized on a single map for
geovisualization.

ACKNOWLEDGMENTS
The authors would like to thank Ahmad M. Razip for his
help in setting up a web-based user study environment.
This work was supported by the US Department of
Homeland Security’s VACCINE Center under Award
Number 2009-ST-061-CI0001. Jang’s work was supported
in part by the Industrial Strategic technology development
program, 10041772, funded by the Ministry of Knowledge
Economy (MKE, Korea). Isenberg’s work was supported in
part by a French DIGITEO chair of excellence.

REFERENCES
[1]

[2]

[3]
[4]

[5]
[6]

[7]
[8]
[9]
[10]

[11]

[12]
[13]
[14]
[15]

[16]
[17]

A. MacEachren, D. Xiping, F. Hardisty, D. Guo, and G. Lengerich,
“Exploring High-D Spaces with Multiform Matrices and Small
Multiples,” Proc. IEEE Symp. Information Visualization (InfoVis),
pp. 31-38, 2003, IEEE CS, doi: 10.1109/INFVIS.2003.1249006.
C. North and B. Shneiderman, “Snap-Together Visualization: A
User Interface for Coordinating Visualizations via Relational
Schemata,” Proc. Working Conf. Advanced Visual Interfaces (AVI),
pp. 128-135, 2000, ACM, doi: 10.1145/345513.345282.
C. Weaver, “Cross-Filtered Views for Multidimensional Visual
Analysis,” IEEE Trans. Visualization and Computer Graphics, vol. 16,
no. 2, pp. 192-204, Mar./Apr. 2010, doi: 10.1109/TVCG.2009.94.
D.S. Ebert, R.M. Rohrer, C.D. Shaw, P. Panda, J.M. Kukla, and
D.A. Roberts, “Procedural Shape Generation for MultiDimensional Data Visualization,” Computers & Graphics, vol. 24,
no. 3, pp. 375-384, June 2000, doi: 10.1016/S0097-8493(00)00033-9.
S. Bachthaler and D. Weiskopf, “Continuous Scatterplots,” IEEE
Trans. Visualization and Computer Graphics, vol. 14, no. 6, pp. 14281435, Nov./Dec. 2008, doi: 10.1109/TVCG.2008.119.
R. Maciejewski, S. Rudolph, R. Hafen, A.M. Abusalah, M. Yakout,
M. Ouzzani, W.S. Cleveland, S.J. Grannis, and D.S. Ebert, “A
Visual Analytics Approach to Understanding Spatiotemporal
Hotspots,” IEEE Trans. Visualization and Computer Graphics,
vol. 16, no. 2, pp. 205-220, Mar./Apr. 2010, doi: 10.1109/
TVCG.2009.100.
J.J. van Wijk and A. Telea, “Enridged Contour Maps,” Proc.
Conf. Visualization (VIS), pp. 69-74, 2001, IEEE CS, doi: 10.1109/
VISUAL.2001.964495.
R.J. Phillips and L. Noyes, “An Investigation of Visual Clutter in
the Topographic Base of a Geological Map,” Cartographic J., vol. 19,
no. 2, pp. 122-132, Dec. 1982, doi: 10.1179/000870482787073225.
S. Openshaw, “The Modifiable Areal Unit Problem,” Concepts and
Techniques in Modern Geography, vol. 38, Geo Books, 1984.
M. Swink and C. Speier, “Presenting Geographic Information:
Effects of Data Aggregation, Dispersion, and Users’ Spatial
Orientation,” Decision Sciences, vol. 30, no. 1, pp. 169-195, Jan.
1999, doi: 10.1111/j.1540-5915.1999.tb01605.x.
C.L. Eicher and C.A. Brewer, “Dasymetric Mapping and Areal
Interpolation: Implementation and Evaluation,” Cartography and
Geographic Information Science, vol. 28, no. 2, pp. 125-138, Apr. 2001,
doi: 10.1559/152304001782173727.
R. Spence, Information Visualization. Addison-Wesley, 2001.
L. Wilkinson, The Grammar of Graphics, second ed. SpringerVerlag, 2005.
A.M. MacEachren, How Maps Work: Representation, Visualization,
and Design. Guilford Press, 1995.
S. Chainey, L. Tompson, and S. Uhlig, “The Utility of Hotspot
Mapping for Predicting Spatial Patterns of Crime,” Security J.,
vol. 21, no. 1/2, pp. 4-28, Feb.-Apr. 2008,
doi: 10.1057/
palgrave.sj.8350066.
B.W. Silverman, Density Estimation for Statistics and Data Analysis
(Monographs on Statistics and Applied Probability), vol. 26, Chapman
and Hall, 1986.
C. Ahlberg and B. Shneiderman, “Visual Information Seeking
Using the FilmFinder,” Proc. Conf. Companion Human Factors in
Computing Systems (CHI), pp. 433-434, 1994, ACM, doi: 10.1145/
259963.260431.

KIM ET AL.: BRISTLE MAPS: A MULTIVARIATE ABSTRACTION TECHNIQUE FOR GEOVISUALIZATION

[18] Y.-H. Fua, M.O. Ward, and E.A. Rundensteiner, “Structure-Based
Brushes: A Mechanism for Navigating Hierarchically Organized
Data and Information Spaces,” IEEE Trans. Visualization and
Computer Graphics, vol. 6, no. 2, pp. 150-159, Apr.-June 2000,
doi: 10.1109/2945.856996.
[19] A. Dix and G. Ellis, “By Chance: Enhancing Interaction with Large
Data Sets through Statistical Sampling,” Proc. Working Conf.
Advanced Visual Interfaces (AVI), pp. 167-176, 2002, ACM, doi:
10.1145/1556262.1556289.
[20] A. MacEachren, “Visualizing Uncertain Information,” Cartographic
Perspectives, vol. 13, pp. 10-19, 1992.
[21] R. Dunn, “A Dynamic Approach to Two-Variable Color Mapping,” The Am. Statistician, vol. 43, no. 4, pp. 245-252, Nov. 1989,
doi: 10.1080/00031305.1989.10475669.
[22] J. Olson, “Spectrally Encoded Two-Variable Maps,” Annals
Assoc. Am. Geographers, vol. 71, no. 2, pp. 259-276, June 1981,
doi: 10.1111/j.1467-8306.1981.tb01352.x.
[23] A. MacEachren and D. DiBiase, “Animated Maps of Aggregate
Data: Conceptual and Practical Problems,” Cartography and
Geographic Information Systems, vol. 18, no. 4, pp. 221-229, Oct.
1991, doi: 10.1559/152304091783786790.
[24] H. Hagh-Shenas, S. Kim, V. Interrante, and C. Healey, “Weaving
Versus Blending: A Quantitative Assessment of the Information
Carrying Capacities of Two Alternative Methods for Conveying
Multivariate Data with Color,” IEEE Trans. Visualization and
Computer Graphics, vol. 13, no. 6, pp. 1270-1277, Nov./Dec. 2007,
doi: 10.1109/TVCG.2007.70623.
[25] T. Saito, H.N. Miyamura, M. Yamamoto, H. Saito, Y. Hoshiya, and
T. Kaseda, “Two-Tone Pseudo Coloring: Compact Visualization
for One-Dimensional Data,” Proc. IEEE Symp. Information Visualization (InfoVis), pp. 173-180, 2005, IEEE CS, doi: 10.1109/
INFOVIS.2005.35.
[26] M. Sips, J. Schneidewind, D.A. Keim, and H. Schumann,
“Scalable Pixel-Based Visual Interfaces: Challenges and Solutions,” Proc. 10th Int’l Conf. Information Visualization (IV), pp. 3238, 2006, IEEE CS, doi: 10.1109/IV.2006.95.
[27] C. Panse, M. Sips, D. Keim, and S. North, “Visualization of GeoSpatial Point Sets via Global Shape Transformation and Local
Pixel Placement,” IEEE Trans. Visualization and Computer Graphics,
vol. 12, no. 5, pp. 749-756, Sep./Oct. 2006, doi: 10.1109/
TVCG.2006.198.
[28] D. Dorling, A. Barford, and M. Newman, “Worldmapper: The
World as You’ve Never Seen It Before,” IEEE Trans. Visualization
and Computer Graphics, vol. 12, no. 5, pp. 757-764, Sep./Oct. 2006,
doi: 10.1109/TVCG.2006.202.
[29] P.C. Wong, K. Schneider, P. Mackey, H. Foote, G. Chin, R.
Guttromson, and J. Thomas, “A Novel Visualization Technique
for Electric Power Grid Analytics,” IEEE Trans. Visualization and
Computer Graphics, vol. 15, no. 3, pp. 410-423, May/June 2009,
doi: 10.1109/TVCG.2008.197.
[30] D. Fisher, “Hotmap: Looking at Geographic Attention,” IEEE
Trans. Visualization and Computer Graphics, vol. 13, no. 6, pp. 11841191, Nov./Dec. 2007, doi: 10.1109/TVCG.2007.70561.
[31] C. Tominski, P. Schulze-Wollgast, and H. Schumann, “3D
Information Visualization for Time Dependent Data on Maps,”
Proc. Ninth Int’l Conf. Information Visualization (InfoVis), pp. 175181, 2005, IEEE CS, doi: 10.1109/IV.2005.3.
[32] J. Bertin, Semiology of Graphics. ESRI Press, 2011.
[33] J. Tarbell, “Substrate,” Web Site & Simulation, http://www.
complexification.net/gallery/machines/substrate/, 2003, Feb.
2012.
[34] T. Isenberg, “Visual Abstraction and Stylisation of Maps,”
Cartographic J., vol. 50, no. 1, pp. 8-18, Feb. 2013, doi: 10.1179/
1743277412Y.0000000007.
[35] P. Rheingans, “Task-Based Color Scale Design,” Proc. SPIE,
vol. 3905, pp. 35-43, 2000, SPIE, doi: 10.1117/12.384882.
[36] C. Ware, “Color Sequences for Univariate Maps: Theory, Experiments and Principles,” IEEE Computer Graphics and Applications,
vol. 19, no. 5, pp. 41-49, Sep./Oct. 1988, doi: 10.1109/38.7760.
[37] C.A. Brewer, Designing Better Maps: A Guide for GIS Users. ESRI
Press, 2005.
[38] M. Prasad, “Intersection of Line Segments,” Graphics Gems II,
J. Arvo, ed., pp. 7-9, Academic Press, 1991.
[39] A. Michotte, G. Thinès, and G. Crabbé, Les Complements Amodeux
des Structures Perceptives (Amodal Completions of Perceptual Structures), Louvain: Institut de Psychologie del’Université de Louvain,
France: Studia Psychologica, 1964.

1453

[40] M.S. Farrar, Magic Squares. BookSurge Publishing, 1996.
[41] R.A. Likert, “A Technique for the Measurement of Attitudes,”
Archives of Psychology, vol. 22, no. 140, pp. 5-55, 1932.
[42] D. Acevedo and D. Laidlaw, “Subjective Quantification of
Perceptual Interactions among Some 2D Scientific Visualization
Methods,” IEEE Trans. Visualization and Computer Graphics, vol. 12,
no. 5, pp. 1133-1140, Sept. 2006, doi: 10.1109/TVCG.2006.180.
[43] M. Stone, “In Color Perception, Size Matters,” IEEE Computer
Graphics & Applications, vol. 32, no. 2, pp. 8-13, Mar./Apr. 2012,
doi: 10.1109/MCG.2012.37.
SungYe Kim received the master’s degree in
computer science and engineering from ChungAng University, South Korea in 2000, and the
PhD degree in electrical and computer engineering from Purdue University in May, 2012. She is
currently a graphics software engineer at Intel
Corporation. Prior to this, she was employed as
a research engineer at the Electronics and
Telecommunications Research Institute from
2000 to 2006. Her research interests include
computer graphics, illustrative visualization, visual analytics, and
information visualization.
Ross Maciejewski received the PhD degree
in electrical and computer engineering from
Purdue University in December, 2009. He is
currently an assistant professor at Arizona State
University in the School of Computing, Informatics & Decision Systems Engineering. Prior to
this, he served as a visiting assistant professor
at Purdue University and was at the Department
of Homeland Security Center of Excellence for
Command Control and Interoperability in the
Visual Analytics for Command, Control, and Interoperability Environments (VACCINE) group. His research interests include geovisualization, visual analytics, and nonphotorealistic rendering. He is a member
of the IEEE.
Abish Malik received the BS degree in electrical
engineering from Purdue University in 2009, and
is currently working toward the PhD degree in
the School of Electrical and Computer Engineering at Purdue University. He is a research
assistant at the Purdue University Rendering
and Perception Lab. His research interests
include visual analytics, correlation, and predictive data analytics.

Yun Jang received the bachelor’s degree
in electrical engineering from Seoul National
University, South Korea in 2000, and the
master’s and doctoral degree in electrical and
computer engineering from Purdue University in
2002 and 2007, respectively. He is an assistant
professor of computer engineering at Sejong
University, Seoul, South Korea. He was a
postdoctoral researcher at CSCS and ETH
Zürich, Switzerland from 2007 to 2011. His
research interests include interactive visualization, volume rendering,
visual analytics, and data representations with functions. He is a
member of the IEEE.

1454

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,

David S. Ebert received the PhD degree in
computer science from Ohio State University.
He is a professor in the School of Electrical and
Computer Engineering at Purdue University, the
University Faculty scholar, the director of the
Purdue University Rendering and Perceptualization Lab, and the director of the Purdue
University Regional Visualization and Analytics
Center. His research interests include novel
visualization techniques, visual analytics, volume rendering, information visualization, perceptually based visualization, illustrative visualization, and procedural abstraction of complex,
massive data. He is a fellow of the IEEE and a member of the IEEE
Computer Society’s Publications Board.

VOL. 19,

NO. 9,

SEPTEMBER 2013

Tobias Isenberg received the doctoral degree
from the University of Magdeburg, Germany. He
is a senior research scientist with INRIA in
France. Previously, he held positions as assistant professor for computer graphics and interactive systems at the University of Groningen,
the Netherlands, and as a postdoctoral fellow
at the University of Calgary, Canada. He works
on topics in interactive nonphotorealistic and
illustrative rendering as well as computational
aesthetics and explores applications in scientific visualization. He is a
member of the IEEE.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

