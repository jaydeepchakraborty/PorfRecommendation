University / Industry Collaboration in Developing A Simulation Based
Software Project Management Training Course
James S. Collofello
Department of Computer Science and Engineering
Arizona State University
Tempe, Arizona 85287-5406
(480) 965-3733
collofello@asu.edu

Abstract

A significant factor in the success of a software project is the management skill of the project
leader. The ability to effectively plan and track a software project utilizing appropriate
techniques and tools requires training, mentoring and experience. This paper describes a
collaborative effort between Arizona State University and Motorola University to develop a
software project management training course. Although many such courses exist in academia
and industry, this course incorporates a system dynamics simulator of the software development
process. The use of this simulator to train future software project managers is analogous to the
use of a flight simulator to train pilots. This paper describes the software project simulator and
how it is utilized in the software project management training course. Feedback from the
training course participants is also shared and discussed.

Introduction
A successful software project is one that meets the customer’s requirements, possesses high
quality and is delivered on time and on budget. Unfortunately, many software development
efforts do not meet these success criteria and lead to project failures. Many of these projects may
not, however, have been a failure if common project management pitfalls had been avoided.
Barry Boehm notes in [1], “Poor management can increase software costs more rapidly than any
other factor.” Moreover, both Capers Jones and Joyce Statz note inadequate project management
training and inadequate standards with which to judge project management performance as the
root causes of poor project management [2,3].
Software project management involves, among other tasks: planning, tracking, and control of a
software development project. These abilities require two things: knowledge, such as knowing
common project management pitfalls, and skills, such as the ability to recognize those pitfalls and
formulate a new project plan. Some of the methods by which software project management
skills can be improved include:
• reading relevant texts and papers
• studying appropriate case studies
• classroom instruction

• working with a mentor
• trial and error
• practicing with a simulator

The motivation for exploring computer simulation as a software project management
training vehicle is best introduced by analogy. Software project management is a highly
complex, on-going process similar to flying a plane in rough conditions. A pilot must plan
the course, continually track progress, monitor the condition of the plane, and react to
problems as they are encountered. Pilots are often trained in flight simulators which provide
the pilot with experience in many different scenarios without the high risk and expense of

learning using a real plane. A pilot could not be expected to learn piloting skills from a text
or instructor alone, because a pilot needs hands-on, active experience to become effective.
Similarly, hands-on, active experience is needed to build effective project management skills.
The basis for this flight simulation analogy was given in [4] which was built upon the system
dynamics modeling of the software development process documented in [5].

Software process simulation background
Simulation has been used in engineering disciplines for many years to great advantage. In
recent years, an increasing amount of attention has been paid to using simulation to advance
software engineering. According to Christie [6], from the Software Engineering Institute, there
are a number of areas in which simulation can benefit software engineering, including: assessing
the costs of software development, supporting metric collection, building consensus and
communication, requirements management, project management, training, process improvement,
risk management, and acquisition management. This paper will focus on the training benefits of
simulation. In particular, simulation based on system dynamics models will be addressed.
System dynamics models are based on cause-effect relationships that are observable in a real
system. Figure 1 provides an example of a cause-effect relationship that might be observed
during software development.
time spent
on quality
activities

O

undiscovered
defects in the
product

Figure 1. A Cause-Effect Relationship
The cause-effect relationship indicates that the time developers spend on quality activities
impacts the number of undiscovered defects remaining in the software product. The “O” on the
arrow indicates that an inverse or opposite relationship exists between the two entities. An
increase in the first entity leads to a decrease in the second entity, and a decrease in the first entity
leads to an increase in the second entity. In this example, when “time spent on quality activities”
increases, the number of “undiscovered defects in the product” decreases. When the “time spent
on quality activities” decreases, the number of “undiscovered defects in the product” increases. A
second type of cause-effect relationship exists, in which a similar relationship exists between the
two entities. An increase in the first entity leads to an increase in the second entity, and a
decrease in the first entity leads to a decrease in the second entity. These relationships are
indicated by an “S” label on the arrow.
Cause-effect relationships in a simulation model are constantly interacting during a simulation.
The most powerful feature of this type of simulation model is realized when multiple cause-effect
relationships are connected forming a circular relationship, known as a feedback loop. The
concept of a feedback loop reveals that any entity in a system will eventually be affected by its
own action. Figure 2 provides a simple example of the two types of feedback loops.
The first type of feedback loop is called a reinforcing loop. Reinforcing loops are often
characterized as modeling a downward spiral. In Figure 2., the reinforcing loop is labeled with a
“+” sign. In the example, as the developers perceive that their “ability to meet the schedule” is
lessened, they spend less “time on quality activities”, which results in an increase in
“undiscovered defects in the product”, which further lessens their “ability to meet the schedule”,
and so on. Without anything to stop it, this loop will spiral out of control. A second type of
ability to
meet
schedule

o

time spent
on quality
activities

-

o

s
+
o

undiscovered
defects in the
product

Figure 2. Feedback Loops
feedback loop, called a balancing loop, has the purpose of reigning in reinforcing loops.
Balancing loops bring the system back into balance. In Figure 2., the balancing loop is labeled
with a “-“ sign. In the example, as the developers perceive that their “ability to meet the
schedule” is lessened, they spend less “time on quality activities”, which they perceive to increase
their “ability to meet the schedule”.
The literature contains numerous descriptions of successful applications of simulation models
of the software development process demonstrating the growing maturity of this discipline [4, 715]. Although the use of these simulators for management training is not unique, our model
provides a low-level concurrent incremental development process modeling simulation capability
typically not found in other models.
Integration of simulator into training course
ASU and Motorola began their work together with the objective of developing a system dynamics
model of the software development process at a level suitable for performing software process
improvement activities. Several such models were developed over a period of about 4 years.
One of the models was brought into a Motorola University software project management training
course to pilot its capabilities in support of software project management training. The
demonstration was successful and lead to the updating of the course to fully integrate the
management process simulator. The current version of the software project management training
consists of 3 days of training providing instruction on software project planning and tracking
activities. In the remainder of this section, the simulator will be described along with the class
exercises which utilize it.
Description of software project management simulator
The software project management simulator is implemented in iThink software from High
Performance Systems Inc. The highest level input panel is shown in Figure 3. Additional layers
of menus provide more detailed inputs. User inputs to the simulator are clustered as follows:
Model Initialization
Contains underlying metrics driving the simulator such as productivity rates, defect rates and
cost to repair defects.

Figure 3. High Level Simulator Input Panel
Project Information
Contains information specific to the project to be simulated including:
- planned completion time
- project complexity
- time allocated to rework
- time allocated to inspections
- time allocated to regression testing
- degree of inter-increment communication overhead applicable to the project
Increment Scheduling Information
Enables estimates to be entered for each development phase of each increment. Dependencies
among increments can also be entered.
Staffing
Enables staff to be assigned to each phase of each increment. Three levels of staffing
expertise are provided.
Once all of the inputs are entered, the simulator can be started and paused at any time. The
simulator can also be set to stop at predetermined points such as monthly or completion of
milestones. Once the simulator has stopped, the various input parameters can be adjusted. While
the simulator is running various output panels can be observed. An example of a high level
output panel is shown in Figure 4. As can be seen from Fig. 4, this screen has monitoring
devices, some in the form of number boxes while others in the form of devices that resemble a
speedometer in appearance. Also placed on the control panel are two push buttons that would take

the user to two different sets of graphs.
below.

Each of the variables showed in Fig. 4 are explained

Current staff Load
The number displayed here at any instance is the sum total of all the engineers working on the
project at the time this value is monitored.
Elapsed Man Hours
This variable displays the total number of man hours that have been consumed since the
project has started.
Elapsed Days:
The number of days the project has taken so far.
Remaining Hours Boxes
This section depicts the estimated number of hours left for each increment.

Figure 4. High Level Simulator Output Panel
Schedule Pressure Gauge
If the project is lagging behind schedule, schedule pressure will increase. The current
schedule pressure on the engineers is monitored here. A value of 0 indicates no schedule
pressure and 6 indicates maximum schedule pressure.
Exhaustion Rate Gauge

The exhaustion rate is an indication of how stressed the engineers are. The exhaustion rate is
depicted on a scale of 0 to 160. Once the team reaches maximum exhaustion, the productivity
of the project decreases until the team is de-exhausted.
Percent Complete Button
The percent complete button takes the user to a graph that plots the perceived percentage
completion of the project.
Remaining Hours Button
This group displays data stating the number of hours left to be completed in each of the phases
of all the increments.
Outputs for Earned Value Project Management
On another panel, outputs are provided to support earned value project management. The
values for the Budgeted Cost Work Scheduled (BCWS), Budgeted Cost Work Planned
(BCWP) and the Actual Cost Work Performed (ACWP) are plotted as graphs and also
displayed in the monitoring devices.
Simulation Exercises
In this section, each of the exercises utilizing the simulator will be described. The exercises
follow lecture material and examples presenting the concepts addressed by the exercises.
1. Life Cycle Model Comparison
The objective of this exercise is to illustrate the differences of various software development
life cycle models and factors that govern their selection. The first part of the exercise has
students simulating a project using both the waterfall and sequential incremental development
life cycles. Students observe the additional overhead associated with incremental
development but also the additional benefits derived by a smaller development effort and
earlier quality feedback. The students then re-execute both life cycle models on the same
project varying inspection effectiveness. Students observe that incremental development will
mitigate the impact of weak inspections better than waterfall development. The students then
re-execute both life cycle models varying project complexity. Students observe that
incremental development is advantageous as complexity levels increase. Finally, students are
asked to create a concurrent incremental project varying staffing levels. Students observe
that concurrent incremental development can lead to a shorter development life cycle if
additional staffing is available.
2. Risk Management
The objective of this exercise is to illustrate the impact of various project risk factors and
contingency plans on a project. Students initially create a concurrent incremental
development project and assess the risk of underestimating the complexity of the project.
Various contingency plans are then simulated to assess their impact at different points in
time. The students then simulate the impact of losing key personnel at various points of time
along with the impact of specified contingency plans.

3. Software Inspections

The objective of this exercise is to illustrate the impact of software inspections on software
development and testing schedules. Students create a software project and simulate the
impact of varying levels of inspection commitment on testing time and overall project
completion date.
4. Critical Path Scheduling
The objective of this exercise is to illustrate the importance of managing the critical path.
Students are given a set of tasks, task durations and dependencies and asked to identify the
critical path. Students then simulate the project varying the experience level of staff assigned
to critical path activities to assess the impact.
5. Overall Planning and Tracking
The final exercise serves as the capstone project for the course. Students are given the
responsibility of planning and tracking a simulated project from inception to completion.
Information available for planning includes:
- features to be implemented
- estimated development time per feature
- feature dependencies
- available staff
Students develop an an overall schedule and plan for the project including:
- planned start and end date for each phase of each increment
- planned staffing for each phase of each increment
- planned rework
- percent of effort allocated to inspections
- allocation for regression testing
Once the plan is completed students simulate the project based on their plan and track the
project’s performance. Periodic reports are written for “upper management” and decisions
must be made concerning adding staff, reducing functionality or delaying the delivery date.

Evaluation and future work
The updated course integrating the simulator was taught in the fall of 1999 at Motorola
University to a class of about 16 students. Students completed a written course evaluation at the
end of the course and were also invited back for a more detailed critique of the class a few weeks
later. All participants found the use of the simulator added significantly to the value of the course
and its exercises. The participants also learned the value of software process simulations in
software process improvement and management activities and inquired about how they might
utilize software process simulation models in the workplace.
We are continuing to expand upon and refine the modeling capabilities of the project
management simulator. A web-based graduate level software project management course will be
taught at Arizona State University in Fall 2000 utilizing the simulator.

References
[1] Boehm, B., Software Engineering Economics, Prentice-Hall, Englewood Cliffs, New Jersey,
1981.

[2] Jones, C., Applied Software Measurement: Assuring Productivity and Quality, McGrawHill, New York, 1991.
[3] Statz, J., "Training Effective Project Managers," American Programmer, June 1994, pp. 4348.
[4] Howard A. Rubin, Margaret Johnson, and Ed Yourdon, "SOFTWARE PROCESS FLIGHT
SIMULATION Dynamic Modeling Tools and Metrics," Information Systems Management,
summer 1995.
[5] Tarek Abdel-Hamid and Stuart E. Madnick, SOFTWARE PROJECT DYNAMICS
Integrated Approach, Prentice-Hall, Englewood Cliffs, New Jersey, 1991.

An

[6] Alan M. Christie, “Simulation: An Enabling Technology in Software Engineering”, CrossTalk, April
1999.
[7] Bradley J. Smith, Nghia Nguyen, and Richard F. Vidale, "DEATH OF A SOFTWARE MANAGER:
HOW TO AVOID CAREER SUICIDE THROUGH DYNAMIC SOFTWARE PROCESS MODELING,"
American Programmer, May 1993, pp. 10-17.

[8] R. Rembert Aranda, Thomas Fiddaman, and Rogelio Oliva, "QUALITY MICROWORLDS:
MODELING THE IMPACT OF QUALITY INITIATIVES OVER THE SOFTWARE
PRODUCT LIFE CYCLE," American Programmer, May 1993, pp. 52-61.
[9] Karim J Chichakly, "THE BIFOCAL VANTAGE POINT: Managing Software Projects
From a Systems Thinking Perspective," American Programmer, May 1993, pp. 18-25.
[10] Kenneth G. Cooper and Thomas W. Mullen, "Swords and Plowshares: The Rework Cycles
of Defense and Commercial Software Development Projects," American Programmer, May 1993,
pp. 41-51.
[11] Chi Y. Lin, "WALKING ON BATTLEFIELDS: TOOLS FOR STRATEGIC SOFTWARE
MANAGEMENT," American Programmer, May 1993, pp. 33-40.
[12] Raymond J. Madachy, “System Dynamics Modeling of an Inspection-Based Process,”
Proceedings of the Eighteenth International Conference on Software Engineering, Berlin,
Germany, March 1996.
[13] Rubin, H., Johnson, M., Yourdon, E., "With the SEI as My Copilot Using Software Process
'Flight Simulation' to Predict the Impact of Improvements in Process Maturity," American
Programmer, September 1994, pp. 50-57.
[14] Proceedings of the Software Process Simulation Modeling Workshop, Oregon Center for
Advanced Technology Education, 1998.
[15] Proceedings of the Software Process Simulation Modeling Workshop, Oregon Center for
Advanced Technology Education, 1999.

Assessing the Software Process Maturity
Software Engineering Courses

of

James S. Collofello
Department of Computer Science and Engineering
Arizona State University, Tempe, AZ
collofello@asu.edu
Manmahesh Kantipudi

Mark A. Kanko, Capt, USAF (AFIT)

Intel Corporation
Chandler, AZ
kanktipud@enuxsa. eas.asu.edu

Department of Computer Science and Engineering
Arizona State University,Tempe,
AZ
kanko@suvax.eas.

instmctor managing the projects.
We define successful
projects as projects that complete on time and meet their
functional,
performance and quality objectives.
Different
instructor
attempting to use the same course model are
likely to obtain different results. Even the same instructor

Abstract

Since its introduction by the Software Engineering Institute
(SEI) in 1987, the SEI Software Process Maturity Model
has gained wide visibility and acceptance by industry. Its
overall goal is to enable an organization to assess its
software process maturity in order that it might be able to
improve its software development process. Inspired by the
successof the SEI model, this paper describes an attempt to
develop an analogous, but highly simplified
and
invalidated process maturity framework applicable to
software engineering project courses. Our goal is to enable
an instructor of a software engineering project course to
assessthe software development processes utilized in the
course and to improve these processes leading to better
managed and more successful projects.

using the same course model may encounter
results

for

different

suggest that many

In or&r

to identify

engineering

universities.
teach

The

software

providing
apply

general

the students

has

organization

and

engineering

their newly

Much

project

been

with

acquired

written

courses

are regularly

graduate

objective
concepts

levels

of these
while

a project
knowledge

concerning

courses
they

content

from

tion’s

courses exist ~airley87,
Ford88,
One problem that remains, howe~er,

these
have

industrial

been

utilizing

experiences.
a Software

by the Software

process into one of five maturity

questionnaire and evaluation
prvven statistical
techniques

These
Process

Enginca-ing

levels.

The SEI

methods attempt to utilize
to ensure reliability.
Ta

increase its reliability and usefulness, the SEI questionnaire
has recently undergone revisions based on experience
gained from its initial years of use. ~aumert91]
Thorough

Perr-nkion to copy without fee all or part of this material is
granted provided that the copiee are not made or distributed for
direct commercial advantage, the ACM copyright notice and the
title of the publication s+ndits date appear, and notioe ia given
that copying ie by perrnseion of the Aeeociation for Computing
Machinery.
To copy other wiee, or to republish, requiree a fee
at-dor epecific permission.
SIGSCE 94- 3/94, Phoenix, Arizor@JSA
1994 ACM 0-89791 4&#MKW3..s3.50

have

development process. This is accomplished via a complex
questionnaire that maps a software development organiza-

and

is that the success of the team projects in these courses is
largely dependent upon the skill and experience of the

~

with software development

over the last few years and much can be

The overall objective of the SEI Software Process Maturity
Model is to assess an organization’s
software process
maturity as a first step toward impmving
the software

might

of these courses and many reasonable models

for teaching these
Gibbfi89, Deimc490].

it is

is to

and skills.

the

processes utilized

to assess their current approach to

Maturity Model developed
Institute in 1987.

many

simultaneously

in which

problems

course projects

these software process deficiencies,

many companies involved

offered
in

engineering

process management in light of advances in the software
area. This is exactly the same activity that

learned

undergraduate

inconsistent

These

engineering

organizations
the

offerings.

software

necessary for instructors

I. Introduction
Software

course

suffer from deficiencies in the software
by the instructor to manage the projects.

been performing

at

asu.edu

training is provided to aid art organization in this appraisal
process.
The SEI
maturity
levels as described by
~umphrey89]
are summarized below
Level
stage,

.

16

1, Initial:
the

Usually
organization

called

ad hoc or chaotic.

typically

operates

At this
without

formalized procedures, cost estimates, or project plans.
Tool use is not integrated or uniform. Change control is
lacking and higher management has little exposure or
understanding of the problems and issuti.

established Likewise, before improvements can be ma(de
in the management of a software engineering project
course, a measure of “where you stand” must first be
determined. In the spirit of the SEI software process
framework,
our
matttrit y
maturity
classification
accomplishes that goal. By assessing oneself against the
following maturity levels, an instructor can easily seewhich
areasneed strengthening for increased maturity.

Level 2, Repeatable: Control is provided over the way the
organization establishes its plans and commitments via
project management, management oversight, quality
assurance, and change control techniqua. A degree of
statistical control has been achieved through learning to
make and meet estimates and plans. A quality assurance
group has been established.

Each maturity level is defined by a number of
characteristics and software engineering practices that serve
to classify the relative process maturity of a sotlware
engineering project comse. As such, it is possible for a
course to possess some upper-level characteristics even
though it’s overall maturity is at a low level. For example,
if a particular course utilizes a state-of-the-ati software
engineering environment for projects but has r10
documented development process (a level 3 requirement) or
doesn’t collect process metrics (a level 4 requirement), then
the maturity cannot rise above level 2. With these
considerations in mind, we now describe our maturily
levels.

A process group, as well as a
Level 3, Defined
development process architecture have been established. A
family of software engineering methods and technologies
have been introduced for use. The foundation has now
been established for examining the process and deciding
how to improve it.
Level 4, Managed: Quantitative process measuresto gauge
the quality and cost parameters of each process step have
been put in place. A process databasehas been established
and is managed full-time. Each product is assessed for
relative quality and management is informed when quality
targets are not being met. Historical data is used to
measure and assessprogress.

LEVEL

Level 5, Optimized: Process metric &ta collection is
automated. This data is used to dynamically analyze and
modify the software development process to prevent
problems and improve efficiency. The organization now
has the means to identify the weakest elements of the
process and to fii them.
In the remainder of this paper, we will describe our
proposed approach for software engineering project course
instructors to assessthe software process maturity utilized
in their coumes. This approach is derived from the SEI
Software Process Maturity Model and adapted to an
academic environment. Our approach is much more
infortnrd than that of the SEI since it is our intention to only
mise the consciousness level of instructors to their software
process maturity. Thus, our approach does not incorporate
reliability evaluation considerations. In section II our
proposed academic maturity levels will be presented. In
section III we describe our questionnaire and evaluation
scheme. Finally, section IV describes initial results and
future plans, as well as a summary of the paper.
II. Academic Maturity

1

does not embrace software engineering as a
discipline. Although some individual software engineering
concepts are discussed, they are neither applied to tours e
projects, nor are they shown to be a part of a larger software
engineering development process. Since no integmted view
of the software development process is in place, software
project development teams are not required to follow any
particular process model. The instructor assigns students
into teams and provides little to no project direction.
Project evaluation is strictly ad hoc within teams. As such,
the course project is merely an exercise in group
programming rather than software engineering.
The

course

LEVEL

2

A software development model is in place. The instructor
has achieved a stable process that enables most projects to
be successful. When a software project goes awry, the
instructor knows why, based on comparisons to the
informal model he follows.
A group of softwane
engineering methods and technologies are formally
introduced to students.
LEVEL

3

ma~r difference between level 2 and 3 is
documentation. Although a reasonable process may have
existed in level 2, it depended completely on the presence
of one or two specific instructors. If these key individuals
were to leave, there would be no known structure to fall
back on. The instructor now has a defined, documenta3
process as a basis for consistent implementation and better
understanding. Level 3 formalizes the software engineer@

The

Levels

Humphrey ~umphrey89] stressesthe need to determine an
organization-s status in the SJ51 maturity ~amework before
an appropriate plan of action for improvement can be

17

process, making it instructor-independent. At this point,
automated tools are introduced for use by the instructor and
students. Additionally,
quantitative software product
metrics begin to be used. Software process metrics, if Used
are only qualitative in nature. It should be noted that there
is wide variation in software engineering sophistication
possible within level 3. The key emphasis in level 3 is
having a defined, documented process. Numerous
questions in our questionnaire have been written to assist
the instmctom in assessing their relative sophistication
within this level.
LEVEL

level. The questions and the maturity levels evolved in
parallel. As the questionnaire stands presently, it consists
of 74, course-related questions. There are 9 additional
experience and background-related questions which are not
evaluated, but are included for informational purposes. All
the questions that are evaluated are based on ‘YeNNo’
responsesfmm the respondent.
The questionnaire is designed to be more of a selfassessment of courses rather than an evaluation of the
best/womt courses that exist. Each question is considered
to apply to a single academic maturity level. The pettinent
level is shown in brackets following each question.
Thirteen (13) of the questions contribute to Level 2, forty
(40) to Level 3, seventeen (17) to Level 4, and four (4) to
Level 5. While answering each question, kep track (by
level) of how many questions can be answered in the
positive. After all questions have been answered, compare
the number of ‘yes’ responses for each level to the total
number of questions contributing to each level. This fairly
simple evaluation scheme gives respondents an indication
of the percentage of questions that they have answered
‘yes’ and hence provides them with an overall
understanding of where they stand in terms of the maturity
levels. One advantage of this scheme is that a course is not
placed into a single, overall maturity level. Instead, the
respondent is made aware of the degree to which a course
supports each level.

4

instructor has initiated comprehensive process
Process metrics are
measurements and fUldJWS,
quantitative in nature. Not only does the instructor know
why a project is successful or unsuccessful, but the
instructor usea this information to re-focus the course.
Measurements on student projects are collected and
analyzed Instruction and team process management are
consistently modified to implement improvements pointed
to by the analyses.

The

LEVEL

5

instructor now haa a complete foundation for
continuing improvement and optimization of the software
engineering process used in course projects. The software
engineering process is presented as a complete, integrated
discipline.
Process metrics collection is automated.
Metrics databases are maintained and routinely analyzed
with the results being used to improve instruction areas,
methods, and technology. The major difference between
level 4 and level 5 is that the process measurement activity
in level 5 is fully automated and, thus, totally integrated
into the software development process.

The

IV. Conclusion and Future Plans
This paper has described an approach for software
engineering instructor to assess the process maturity
utilized in their course projects. The approach is basedon a
self-assessment utilizing a questionnaire modelled after the
SEI process maturity questionnaire. A few inatructora have
used the questionnaire and found it to be useful in giving
them an idea of maturity strengths and weaknesses in their
comes.
Our future
plans are to distribute the
questionnaire to instructors offering software engineering
project courses and compile the results. We hope that our
efforts will help individual instructotx identify areas where
their courses can be improved. Through continuous process
improvements these instructors can help bridge the gap
between academic software projects and industry projeds,
thus improving the quality of software engineering
educatiom

III. The Questionnaire and its Evaluation
To evaluate software process maturity, a questionnaire is
imperative. We have developed a questionnaire that wvers
a wide range of topics related to the academic environment.
Our work is basedon the original SEI questionnaire that we
have alluded to in the previous sections. The present
questionnaire evolved by initially trying to reword the SEI
questions to fit the academic environment. It was found
that many questiona were only related to industry and were
not applicable to the acadmnk envknnnmt and hence had
to be removed Each of the authors, who all have
experience with software engineering courses, added
questions they thought were relevant to the questionnaire.
Although the questionnaire haa been customized to
academic environment, some questions may still seem
inappropriate for particular classes and should be ignored.
In developing the questionnaire, thought was also put into
the fact that the questions should fit a particular maturity

Acknowledgements
We would like to thank Dr. Ken Collier of Northern
Arizona University for his evaluations of and suggestions
about the questionnaire.

18

References
D3aumert91] Johu Baumert, “New SEI Maturity Model
Targets Key Pmctices”, IEEE Software, Nov 1991,
pp78-79.
~eime190] Lionel Deimel, cd., Software Emjneering
Education SEI: Conference 1990, Springer-Verlag,
1990.
~airley87] Richard Fairley and Peter Freeman, cd., Issues
in Software Etwineerin ~ Education Proceedings of
the 1987 SEI Conference, Sprirtger-Verlag, 1987.
~ord88] Gard Ford, cd,, Software Engineering Education:
SEI Conference 1988, Springer-Verlag, 1988.
[Gibbs89] Norman Gibbs, cd., Software Engineering
Education SEI: Conference 1989, Springer-Verlag,
1989.
~umphrey89]
Watts Humphrey, Manazing the Software
Process Addison-Wesley
Publishing
Company,
—>
Reading, Mass., 1989.
Appendix

Academic Maturity

Questionnaire

Note: In the following questions, the terms stumiardized,
mean that the methods used are
repeatable
procedures.
Additionally,
the
terms
stanoldized
and fbrmd rnethock imply that the procechues
must be documented.

mecham”sm, and iixmal

1.

Are students shown examples of how phases of the
software development cycle taught in the course, are
applied in the coxporatesector? [2]
2. AIE students required to assess the development
process and suggest improvements? [4]
3. Does the class use a designated software engineering
text book? [2]
4. Is a standardized software development process
(documented) used in the course (e.g, waterfall, spiral,
prototyping)? [3]
5. Does the documentation describe the use of tools
(compilers, debuggem) and techniques (requirements
analysis, design)? [3]
6. Does the instructor train the students to do
inspectiomdreviews? [3]
7. Are students required to use unit development folders
8. Does the instmctor check for completion of milestones
at each stageof the software development process? [3]
9. Am students required to apply coding standardsto each
software development project? [3]
10. Are students required to apply standards to the
preparation of unit test cases? [3]
code
provided
documented
students
11. Ate

maintainability standards? [3]
12. Ate students provided documented design review
standards? [3]
13. Are students provided documented code review
standards? [3]
14. Are students provided documented test case review
standards? [3]
15. Are students required to estimate project size using a
formal procedure? [3]
16. Are students required to use a formal prvcedure to
produce software development schtiules (e.g., PERT
charts)? [3]
17. Are studenta required to use a model to estimate
software development costs (e.g., COCOMO)? [3]
18. Does the instmctor use a mechanism, based cm
(to
previous projects, to assign studentsbwns
individual software development tasks? [4]
19. Are students required to produce a users guide? [2]
20. Are students required to produce a specifications
document? [2]
21. Must students produce a design document? [2]
22. Are students required to produce a maintenance
document? [2]
23. Are students required to report the final size of, and the
time required to develop each sot?vvarsproduct? [4]
24. Are students required to formally track (e.g, using a
software deficiency report) software design, code, and
test errors? [3]
25. Ate students required to estimate errors (e.g., design,
code, test errors) and compare them to actual? [4]
26. Are students required to maintain time profiles c~f
actual versus planned software units designed? [4]
27. Are students required to maintain time profiles clf
actual versus planned software units tested? [4]
28. Are students required to maintain time profiles of
actual versus planned software units integmted? [4]
29. Are design and code review coveragea measured and
recorded? [4]
30. Are test coverages measured and tecorded for each
phaseof fictional testing? [4]
31. Are students required to track to closure the action
items that result fmm design and code reviews? [3]
32. Are students required to track each deliverable sofiwam
component against a submitted schedule? [4]
33. Has a managed and controlled process database been
established by the instructor for process metrics data
across projects? [4]
34. Are students required to analyze the error data from
code reviews and test execution, to determine the likely
distribution and characteristics of errors remaining in
the product? [4]
35. Are students required to conduct analyses of the errotx
found, to determine their process-related causes? [4]
36. Does the instructor review error causes and determine
the process changes required to prevent them? [4]

19

or data decomposition? [2]
62. Are students required to use test wverage analyzers
(automated or not) for functional testing? [3]
63. Does the instructor provide automated test input data
generator(s) for students to use? [3]
64. Are students required to analyze their software for
component structure and interrelationships? [3]
65. Does the instructor provide students with any computer
tools that transform design specifications directly into
functional software components? [3]
66. Are students required to use a formal specification
language prior to preliminary design? [3]
67. Does the instructor provide an automated library
management system for use by students during
software development, integration, and testing to
manage change? [3]
68. Does the instructor provide an automated configuration
control for use by students to control and track
implementation and change activity throughout the
development process? [3]
69. Is student software required to possessquality attributes
that encourage reuse, such as modularity, low coupling,
and minimal global data? [3]
70. Does the instructor provide computer tools that allow
the students to analyze their software for component
structure and interrelationships? [3]
71. Does the instructor provide students with any computer
tools that allow them to measwe their softsvare for
product metrics data? [3]
72. Does the instructor provide students with any computer
tools that allow them to measure their software for
process metrics data? [5]
73. Are students provided with any computer tools that
allow them to implement full tmceability between
~quirements, design, and implementation? [3]
74. Are students provided with an integmted software
engrg. environment as a development platform? [3]

37. Are the students required to use a mechanism for
implementing error prevention actions? [4]
38. Does the instructor analyze productivity of students by
process step? [4]
39. Does instructor have a mechanism for regular review
of the status of s/w development team projects? [3]
40. Does the instructor use a periodic mechanism for
assessing and impmving the software engineering
process? [4]
41. Does the instructor use a mechanism for ensuring
compliance to software engineering standards? [3]
42. Are students required to use a mechanism for tmsuring
traceability between the software products (e.g.,
IWvveen requirements and desigm between design and
code)? [3]
43. Are students required to use ~ mechanism for
controlling changes to the software products (e.g.,
requirements, design, and code)? [3]
44. Are students required to conduct specifications, design,
code, and test casereviews? [2]
45. Does the instructor use a mechanism for Configuration
Management of the software tools used in the
development process? [3]
46. Are studenta required to use a mechanism for assuring
the adequacy of regression testing? [3]
47. Does the instructor use a mechanism to evaluate
sofiware quality of products? [3]
48. Are teams taught about reasonsfor group failures? [2]
49. Are students taught any leadership/mgmt. skills? [2]
50. Does the instructor introduce students to the subject of
ethics? [2]
51. Are process metrics tools and techniques available to
the students? [5]
52. h
students required to use tools that ensure
traceability of software products? [3]
53. Are studenta required to use tools for process
management? [5]
54. Are students required to use tools to measure test
coverage? [3]
55. Are students required to use tools to do testing? [3]
56. Does the instructor maintain a databaseof projects as a
potential source for software reuse? [3]
57. Does the instructor use computer tools to gather and/or
analyze process metrics data? [5]
58. Does the instructor provide computer tools that allow
the students to demonstrate that every function is
testedherified for all code produced? [3]
59. Does the instructor provide students the opportunity to
employ prototyping methods in designing the critical
performance elements of their software projects? [2]
60. Am students required to use prototyping methods in
designing the critical elements of the man-machine
interface? [2]
61. Are students required to use a design method that
emphasizes structured problem analysis by functional

Background Questions
1.
2.
3.
4.

5.
6.
7.
8.
9.

20

How many times has the instructor taught this course?
How many years of industry experience does the
instructor have?
What percentage of students are seniors or graduate
students?
What
is the median number of years of industry
experience of the student?
On the average, what percentage of students have used
a high level language before taking this course?
What is the median size of the project in SLOC (source
lines of code)?
What is the median size of the class?
What is the approximate size of each team?
What is the length of the course (quarter, semester)?

Teaching

Practical

Software Maintenance
Engineering
Course

Skills

in a Software

Dr. James S. Collofetlo
Computer Science Depnrunent
Arizona State University
Tempe, Arizona 85258
(602) 965-3733

should be stressed [I].

Abstract
The typical one-semester software engineering course is normally geared
towards new software development.
Unfortunately, most new computer
science graduates do not find themselves in a position where they are
developing new software but instead in a position where they are maintaming an existing product. This paper describes some current practical
software maintenance approaches which can be taught as a part of a
software engineering course.

These activities

can be classihed into:

-

corrective

-

adaptive maintenance (deals with enb,ancements)

maintenance (deals with failures)

-

perfective

maintenance (deals with optimization)

The student should also be made aware of the phases in a typical
software maintenance process such as:

Background

-

understanding the software

Although software maintenance is widely regarded as being the dominant activity performed in most software development organizations,
very little attention has been paid to sofiware maintenance in university
courses. Even in software engineering courses, most of the emphasis is
on developing new software as opposed to approaches to maintaining it.
Those software engineering texts which address softw‘are maintenance
normally only provide a broad overview of the importance of maintenance with very little in the way of practical approaches to actually performing maintenance tasks.

-

generating maintenance proposals

-

accounting for ripple effect

-

retesting the modified code [3].

Common problems in performing these tasks as well as their causes
should also be described [4]. Finally a set of maintenance myths should
be presented such as those discussed in ]S] and summarized below:

State University, software maintenance has been a part of
At Arizona
software engineering education for the last four years. At the graduate
level an entire semester is devoted towards an examination of state-ofthe-art software maintenance theories, techniques and tools. At the
undergraduate level, sevexd lectures ate incorporated in our onesemester software engineering course.

This paper focuses on our undergraduate software maintenance education
approach. Several lecture topics will be described along with appropriate exercises which serve as the basis of introducing software engineering students to practical sotiware maintenance skills. In the remainder
of this paper the content of each of these lectures as well as references
will be presented. Although the length of each lecture may vary, a reasonable guideline is approximately SO minutes.

Lecture Topics and Exercises
Lecture I:

Maintenunce

[2].

I.

“A special department for maintenance? Ridiculous.
Our development programmers can maintain a system
or two each in their spare time.”

2.

“We can’t really control maintenance or how welt the
crew is working.
After ah, every hx or change is
different.”

3.

“You don’t get anywhere doing maintenance”.

4.

“Any of my progmmmers can maintain any program”.

5.

“Maintenance is all 3 A.M. tiies and frantic hysterics.
It’s nothing we can anticipate and it doesn’t take up
that much time anyway”.

6.

“Your just can’t tind anyone who wants to do software
maintenance”.

7.

“Maintenance
burnouts and
hexadecimal
hurt anything

8.

“Why bother to provide maintenance programmers
with new software tools? AtIer all, they’re just patching up.”

Background and MFtlls

The intent of this lecture is for students to realize the importance of
software maintenance and to be aware of common myths surrounding
maintenance practices, The high cost of software maintenance activities

is the place to dump your trainees, your
Joe, the boss’ nephew, who dunks that
is a trendy new disco. How can they
there?”

This lecture should present students with some practical approaches to
understanding existing software and documenting this understanding.
There are two basic approaches to understanding existing sohware: Top
Down and Bottom Up. Top Down understanding assumes a well documented program in which it is possible to proceed from an overall
speciticiation and design of the program through levels of documentation
until the actual code is reached. This, of course, requires clear traceability of functionality throughout the software documentation. An important
message for software engineering students is that they develop software

Permission to copy without fee all or part of this matetia1 is granted provided
that the copies are not made or distributed for direct commercial advantage,
the ACM copyright notice and the title of the publication and its date appear,
and notice is given that copying is by permission of the Association for
Computing Machinery. To copy otherwise, or to republish, requires a fee and/
or specific permission.
0 1989 ACM 0-89791-298-5/89/0002/0182
$1.50

182

in a manner such that Top Down understanding can later be utilized by
the individunls who will perform maintenance on their product. Bottom
Up understanding is necessary for poorly documented softwllre. It is
also sometimes referred to as “stepwise abstraction” since the goal is to
proceed from the code and try to discover intermediate abstractions
which facilitate program understrmding.
Regardless of the undenmnding approach followed, there are several
b:eic questions which must be answered inorder to comprehend what a
progam is doing. The students must be made aware of these questions
as well as approaches to answering them. The basic questions are summarized below and explained in more detail in [6].
I.
2.
3.
4.

5.
6.

Whd actions ‘are performed by the program?
What data is utilized?
Where does information come from?
Where does infomration go?
How are actions performed?
Why me actions performed?

This lecture attempts to provide students with two practical approaches
for ensuring that their software ch‘angesare correct. The two approaches
consist of software reviews and software testing. Software reviews must
be performed for each change to ensure that:

Once understanding is acquired, it is essential that it be documented to
facilitate future maintenance activities. This documentation normally
takes the form of inhne comments and data dictionaries. Students must
be made aware of the types of comments which aid software maintenance and those which do not. An important pmctical approach is
defming the role and goal of variables where the role is a description of
the variable and the goal is the reason why the variable exists [7].
The ideas presented in this lecture must be reinforced through one or
more exercises. A good exercise is to provide the students with an
undocumented program and have them attempt to understand it utilizing
the guidelines presented in the lecture. This seems to work well as a
group project. Their understanding can then be recorded by their documenting the program. A test of how well they have documented the
program is submitting their documented program along with a set of previously written questions concerning the program to another group of
students. If the program was adequately documented, the students seeing the program for the first time should have little problem addressing
the questions.

The objective of this lecture is to sensitize students to the extremely
error prone nature of changing software and the imponnnce of carefully
and thoroughly documenting all software changes. Statistics should be
presented which show that every 3 hxes introduce a new error and that
there is a very high probability that adding a new feature to a program
will break an existing one [8].
The students should lllso be made aware of the information necessary to
effectively diagnose an error and the basic approach to follow in defect
removal. An effective teaching technique is to acquire and bring into
class some actual defect description forms utilized by companies. Some
typical information usually found on those forms includes:
-

Special attention should be paid to steps 2 and 4 since students often do
not adequately organize information about errors and conclude prematurely that they have found the source of an error.
The last important topic in this lecture is documenti& the changed
software. This requires documenting what was changed, why it was
changed and ensuring that alt documentation has been updated to reflect
the changes. Some examples and further guidelines are presented in [9].
A good exercise to support this lecture involves providing the students
with a program Landa documented error in the program. The student’s
task is to diagnose the problem. ti the error ,anddocument the changes
which were made.

a description of the problem
a description of the scenario in which the problem
occurred
an identification of when the problem occurred
location of where the problem occurred
technical contact if more infomration is needed

The students should also be made aware of the “Scientific Method for
Debugging” which includes the following steps:
1. collect data
2. organize the data
3. develop a hypothesis
4.
prove the hypothesis

183

-

the change is correct
the change does not introduce side effects
the change does not degrade the quality of the software
all documentation is updated

Some practical review checklists and guidelines are contained in [lo].
It is .also important to stress to the students that all software changes
must be tested. This testing must ensure that the software has been
changed correctly without adverse side effects. Normally there are
several levels of testing beginning with a unit test of the changed
modules and ending with an overall regression test. Some basic testing
guidelines applicable during software maintenance are included in [II,
.

..1

IL,.

An obvious exercise to complement this lecture is to provide the students with a sohware program and a proposed change to the program.
The students CNI then work in groups to attempt to review the proposed
change and develop a testing strategy for ensuring that the change is
correct and does not have any side effects. If group presentations are
utilized the learning experience can be enhanced as students become
aware of some of their own oversights as they observe other teams.
Conclusion
This paper has described a set of lectures for a one-semester software
engineering course which provide students with some practical software
maintenance skills. This approach to sottwate maintenance education
has been followed successhdly several times in both academic and
industrial training settings with the students acquiring basic maintenance
skilis, but more importantly a positive attitude toward the maintenance
process. Our future plans are to eventually develop a separate undergraduate software maintenance course due to the importance of this area.

Bibliography
1.

Lehman, M.M., “Survey of Software Maintenance Issues”. Proc.
1983 Sof’&ure Muir~trnurrce Workshop, IEEE Computer Society
Press, 1984, pp. 2X-242.

2.

Swcmson, E.B.. “The Dimensions of Mxiuntenance”, Proc. Zd
Inrerrrutionul Conf on Sojkwur-e Eqinrering.
1976, pp. 494497.

3.

YXI. S.S. and CoUofello
J.S., “Ripple Effect Analysis of
78. 1978, pp.
Software Maintenance”, Proc. IEEE COMPMC
60-65.

4.

of Application
Licntz, B.P. and Swanson, E.B.. “Charxteristics
Software Maintenance”, Corr~munic~~tions 4 tlte ACM. 1 I, 6, June
1978, pp. 466-47 I.

5.

Schwartz, B., “Eight Myths about Software Maintenance”,
motion. 28, 9. Aug. 1982, pp. 125-128.

Durti-

6.

zvcgintzov,
iv., “The Eureka Countdown”,
April 1982. pp. 172-178.

28, 4,

7.

Letovsky, S. and Soloway, E., “Deloclllized Plan5 and Program
Comprehension”. IEEE Sofhwrc. May 1986, pp. 41-49.

8.

Collofello, J.S. nnd Buck, J.J., “Software Quality Assurance for
Maintenance”, IEEE Soj??vure, Sept. 1987. pp. 46-53.

9.

Fay, S.D. and D.G. Holmes, “Help! I have to Update an Undocumented Program”, Proc. 198.5 Sofhcure Moititerrunce Workshop.
IEEE Computer Society Press, 1986, pp. 194-202.

LO.

Freedman, D.P. and G.M. Weinberg. Hundhook of Wulkrlrroqits.
Inspec-tiom clnd Technical Reviews. Little Brown and Company,
1982.

II.

Verification
and Testing of
Wallace, D.R., “The Validation.
Software: An Enhancement to Sotiware Maintainability”,
Proc.
I9X~ S&t~re
Muintmunce Workshop. IEEE Computer Society
Press, 1986, pp. 69-77.

12.

Fischer, K.F., “A Test Case Selection Method for the Validation
of Software Maintenance Modifications”,
Proc. IEEE COMPSAC
1977. pp. 421-426.

Dor~mution.

184

The Journal of Systems and Software 59 (2001) 259±270

www.elsevier.com/locate/jss

Behavioral characterization: ®nding and using the in¯uential
factors in software process simulation models
Dan X. Houston a,*, Susan Ferreira b,1, James S. Collofello c,2,
Douglas C. Montgomery d,3, Gerald T. Mackulak d,4, Dan L. Shunk d,5
a

b

Honeywell Inc., 817 W. Northview Avenue, Phoenix, AZ 85021-8042, USA
Arizona State University and Motorola IISG, 8201 E. McDowell Rd, MD H8175, Scottsdale, AZ 85257, USA
c
Computer Science & Engineering Department, Arizona State University, Tempe, AZ 85287-5406, USA
d
Industrial Engineering Department, Arizona State University, Tempe, AZ 85287-5906, USA
Received 12 July 2000; received in revised form 1 December 2000; accepted 22 March 2001

Abstract
Most software process simulation work has focused on the roles and uses of software process simulators, on the scope of models,
and on simulation approaches. Consequently, the literature re¯ects a growing body of models that have recently been characterized
by modeling purpose, scope, key result variables, and simulation method. While the software process simulation arena is maturing,
little eort appears to have been given to statistical evaluation of model behavior through sensitivity analysis. Rather, most of
software process simulation experimentation has examined selected factors for the sake of understanding their eects with regard to
particular issues, such as the economics of quality assurance or the impact of inspections practice. In a broad sense, sensitivity
analysis assesses the eect of each input on model outputs. Here, we discuss its use for behaviorally characterizing software process
simulators. This paper discusses the bene®ts of using sensitivity analysis to characterize model behavior; the use of experimental
design for this purpose; our procedure for using designed experiments to analyze deterministic simulation models; the application of
this procedure to four published software process simulators; the results of our analysis; and the merits of this approach. Ó 2001
Elsevier Science Inc. All rights reserved.
Keywords: Sensitivity analysis; Design of experiments; Model characterization; Software process modeling

1. Introduction
The ®eld of software process simulation has been
growing, particularly since the 1991 publication of Abdel-Hamid and Madnick's Software Project Dynamics:
An Integrated Approach (Abdel-Hamid and Madnick,
1991) focused attention on the application of dynamic
simulation to software project management. Most of the

*

Corresponding author. Tel.: +1-602-313-4231.
E-mail addresses: dxlsh@uswest.net (D.X. Houston), susanf1@
uswest.net (S. Ferreira), collofello@asu.edu (J.S. Collofello), doug.
montgomery@asu.edu (D.C. Montgomery), mackulak@asu.edu
(G.T. Mackulak), dan.shunk@asu.edu (D.L. Shunk).
1
Tel.: +1-480-441-2139.
2
Tel.: +1-480-965-3733.
3
Tel.: +1-480-965-3836.
4
Tel.: +1-480-965-6904.
5
Tel.: +1-480-965-6330.

work in this developing ®eld has focused on the roles
and uses of software process simulators, on the scope of
models, and on simulation approaches. Consequently,
the literature re¯ects a growing body of models that
have recently been characterized by modeling purpose,
scope, key result variables, and simulation method
(Kellner et al., 1999).
While the software process simulation arena is maturing, little eort appears to have been given to statistical evaluation of model behavior through sensitivity
analysis. (Perhaps this lack of interest re¯ects the same
lack in the ®eld of systems dynamics modeling (Clemson
et al., 1995), the venue in which a large stream of software process simulation research was initiated.) Rather,
most of software process simulation experimentation
has examined selected factors for the sake of understanding their eects with regard to particular issues,
such as the economics of quality assurance (Abdel-Hamid and Madnick, 1991) or the impact of inspections
practice (Madachy, 1994).

0164-1212/01/$ - see front matter Ó 2001 Elsevier Science Inc. All rights reserved.
PII: S 0 1 6 4 - 1 2 1 2 ( 0 1 ) 0 0 0 6 7 - X

260

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

In a broad sense, sensitivity analysis assesses the eect
of each input on model outputs. Here, we discuss its use
for behaviorally characterizing software process simulators. A behavioral characterization provides basic information about a simulation model, information as
important as the static characteristics of purpose, scope,
and so forth.
The remainder of this paper discusses the bene®ts of
using sensitivity analysis to characterize model behavior;
the use of experimental design for this purpose; our
procedure for using designed experiments to analyze
deterministic simulation models; the application of this
procedure to four published software process simulators; the results of our analysis; and the merits of this
approach.

2. Bene®ts of sensitivity analysis for characterizing model
behavior
With the continuing maturity of software process
simulation, modelers are able to re¯ect more on the
modeling process, improving it through evaluation of its
models. Sensitivity analysis is one of the better-known
statistical techniques for evaluating a simulation model.
For the purpose of this discussion, it involves the systematic variation of a model's exogenous parameters,
and analysis of the outputs of interest so as to estimate
the eects of each parameter. Most systems exhibit a
sparsity of eects, meaning their behavior is dominated
by only some (often a minority) of the factors. The
identi®cation of these in¯uential factors in a model
provides a characterization of the model's behavior,
which can be used to improve the modeling process, and
to aid in the selection of a model, either for a particular
usage or for enhancement.
 Improved modeling. A major trend in software
process modeling has been the development of system
dynamics models (SDM) that are validated against the
data of an actual software project (hereafter referred to
as a ``base case''). In a single experiment, sensitivity
analysis can readily identify the signi®cant factors for a
base case, thereby determining which ones have to be
modeled correctly. A modeler may then make informed
decisions regarding further level of detail in modeling,
degree of eort spent in data collection, and speci®cation of parameter values (Law and Kelton, 2000).
 Model selection. Rather than investing in the construction of a new simulation model, prospective model
users may want to choose from among various existing
software process simulators. An understanding of a
model's behavior with respect to project outcomes (cost,
quality, and schedule) provides an additional criterion
for selecting a model either for use as-is or for enhancement. Later in this paper, we discuss examples of

using behavioral characterization to support model selection for these purposes.
3. A procedure for sensitivity analysis
A variety of techniques, including many from design
of experiments (DOE), are available for designing a set
of input values such that the output values can be analyzed for their sensitivity to the inputs. Although DOE
was developed in applications to physical systems, many
of its techniques, due to their eciency, have been applied to the sensitivity analysis of computer models.
Comparisons of input selection techniques for computer
model sensitivity analysis have been oered by McKay
et al. (1979), Morris (1991), and Clemson et al. (1995).
Among the techniques available are 2-level fractional
factorial designs, widely used for screening in¯uential
from unimportant factors (Montgomery, 1997).
A fractional factorial design is fraction of a full factorial design. In a two-level factorial design, designated
2k , k  number of factors, every combination of two
values for each input is run. An advantage of full factorials is that factor eects and the eects of all the interactions between factors can be measured. With
fractional factorials, the number of experimental runs is
traded-o with the ability to distinguish factor interactions. As a factorial design is more highly fractionated
(1/2 fraction, 1/4 fraction, . . .), more interactions are
aliased by (measured with) main factor eects. The degree of aliasing, or resolution, allows an analyst to
choose an experimental design that provides an optimal
trade-o between the cost of experimentation and the
resolution required. If interactions are not expected to
be signi®cant, then a low resolution design is usually
sucient.
Another bene®t of fractional factorial designs is the
ability to accommodate any number of factors. In most
physical experiments, the number of factors is on the
order of 1±10 factors, but in computer experiments,
having 10±100 factors is not uncommon. Consequently,
highly fractionated two-level factorials (designated 2k p ,
where p is the power of the 1/2 fraction or 1=2p ) provide
an ecient means of identifying signi®cant factors. If
interactions are indicated and cannot be clearly identi®ed, an additional fraction of the full factorial can be
run to ``de-alias'' eects. Factorial designs can also be
sequenced, selecting fewer factors in successive experiments as insigni®cant factors are eliminated.
Analysis of a factorial experiment produces leastsquares estimates of the coecients of a linear model of
the form
y  bx  e;
where e is a random error term, ordinarily estimated
from replications of the experiment. However, when

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

working with deterministic computer models, the error
term is eliminated, 6 so the usual parametric statistics do
not apply. Nonetheless, the partitioned sums of squares
provide a convenient means for measuring the relative
contribution of each factor to variations in the response
variables.
The steps for performing a 2k p experiment are:
1. Choose the factors.
2. Choose the response variable(s).
3. Choose the levels of the factors.
4. Generate the experimental design.
5. Run the experiment using the combinations in the design and measuring the responses.
6. Analyze the eects of the factors. For each response
variable, calculate each factor's contribution to the
total variation in the response: divide each factor's
sum of squares by the total sum of squares.
4. Case studies
4.1. Selection of models
The above procedure was applied to four published
simulation models of software development (AbdelHamid and Madnick, 1991; Madachy, 1994; Sycamore,
1996; Tvedt, 1996). These models were selected because
(a) the documentation and source code was readily
available for them; (b) they are dierent models with
dierent purposes and scopes, but employ some of the
same modeling constructs, thereby allowing comparisons between experimental results; (c) they have been
validated (albeit to varying degrees); and (d) the availability of an ithinkâ implementation of each model facilitated experimentation. Even though these four are
system dynamics models, the procedure is applicable to
any simulation model.
It is helpful to summarize the emphasis of each
model.
· Abdel-Hamid and Madnick (1991) (A-H&M) modeled an integrated approach to project management
of software development for the purpose of learning
about the management process. In addition to production, the scope of his model includes project staing, personnel ¯ow, eort allocation, project planning
and control, and an error ¯ow.
· Madachy's model (1994) focused on the production
stream with particular attention to work product inspections. He introduced inputs from COCOMO
for risk assessment. He includes stang, calculated

from COCOMO, and an error stream, ignoring the
managerial aspects.
· Tvedt's model (1996) encompassed the scope of AH&M, reusing many of its modeling constructs in
an incremental development process. It elaborated
work product inspections in signi®cant detail.
· Sycamore (1996) also modeled an incremental development process, but in a very reduced scope. He does
not have a production ¯ow and instead models eort
consumption for the sake of studying trade-os between cost and duration. 7
4.2. Applying the procedure
The exogenous factors in each model were selected by
identifying any element having a modi®able value: inputs, constants, and tables. 8 To facilitate experimentation, inputs were added for manipulating the values of
constants and table outputs.
Sensitivity analysis, though de®ned variously across
disciplines, is commonly understood as the investigation
of model responses either to extreme input values or to
drastic changes in input values (Kleijnen, 1995). This
de®nition usually presumes that the analysis is for the
purpose of model validation, optimization, or related
activity. However, for purpose of characterizing model
behavior around a base case, we focused the analysis
(local sensitivity analysis) using marginal variations in
parameter values. Speci®cally, factor levels were selected
by choosing values 10% above and below the values
speci®ed for each model's base case. Local sensitivity
analysis and the degree of common factor variation were
chosen for a number of reasons.
· Job size and stang were expected to dominate, particularly for duration and eort, if all factors were
varied across normal operational ranges. By varying
all factors the same degree from values in a validated
baseline case, the relative signi®cance of the factors in
the baseline case could be assessed.
· It is often dicult to identify normal operating ranges
for many factors, especially in a computer model that
is highly abstracted from the system it represents. The
use of a ®xed percent variation, about baseline case
values, provides an objective and consistent means
of choosing factor levels.
· Using 10% provided adequate input variation to
measure signi®cant eects on responses. Varying
some factors to larger degrees (say, 15% or 20%)
7

6
Least-squares analysis of deterministic output does produce a sum
of squares for error, but this represents linear model bias rather than
experimental variance. It is desirable to reduce this bias to an
insigni®cant level. A footnote in the next section explains further.

261

Although the Tvedt and Sycamore models are incremental, only the
®rst increment was used for experimentation so that all experiments
would re¯ect a simple waterfall process.
8
An exception to this approach was the COCOMO calculations in
two of the models: A-H&M and Madachy. These equations were
considered integral to the structure of these models, therefore they
were not treated as experimental factors.

262

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

Table 1
Experimental designs
Model
A-H&M
Madachy
Tvedt
Sycamore

No. of factors
65
21
103
30

Table 2
Number and percent of in¯uential factors
Design
65 57

2
221 15
2103 95
230 24

No. of runs
256
64
256
64

may create extreme values that would not allow model runs to ®nish. (In fact, this was found to be the case
for the one model on which it was tried.)
· This approach would better accommodate serial experimentation. If factors are to be screened for subsequent experiments, screening of locally insigni®cant
factors is much less likely to produce errors than
screening of globally insigni®cant factors. In other
words, a factor may be insigni®cant globally but locally in¯uential, while the converse (globally signi®cant but locally insigni®cant) is improbable.
· Using the same variation to choose levels for all factors in all models facilitates comparison of results
across models.
A 2k p design of Resolution IV was produced for each
of the four models (Table 1) using SAS PROC FACTEX. Resolution IV designs were chosen because twoway interactions could be clearly distinguished from
main eects. 9 Response variables included measures of
cost (total eort expended on the project), of project
duration, and of quality (number of defects in the last
project phase).
Each experiment was performed by copying the factor values from the design in a spreadsheet and pasting
them into the ithinkâ Sensitivity-Spec.
SAS PROC GLM was used to calculate partitioned
sums of squares, though a spreadsheet program also
proved very workable for calculating them from formulas provided by Montgomery (1997). For each response variable, the percent contribution to its variation
was calculated, as described earlier, and the model factors were sorted by this percentage. 10 Each list of sorted
factors was categorized into very in¯uential, marginally
in¯uential, and non-in¯uential factors. The distinction
between very in¯uential and marginally in¯uential was
usually 10% contribution to response variation, though
9

In many cases, Resolution IV designs also have the advantage of
minimizing the bias error (Myers and Montgomery, 1995). In these
experiments, the use of Resolution IV designs reduced the percentage
of the error sum of squares to insigni®cant levels, well below 1%.
10
The calculation of percentage contribution to variation from sums
of squares is not commonly used in DOE, so it was employed in
conjunction with normal probability plots, a widely accepted technique
for analyzing unreplicated factorial experiments. In every experiment
discussed here, the two techniques produced the same ordering of
factors. The former is used here because it oers the advantage of
quanti®ed results.

Model

No. of factors
having P1%
in¯uence

Percentage
of factors
tested

Percent
variation
explained

A-H&M
Madachy
Tvedt
Sycamore

15
9
21
16

23
43
20
53

58±79
97±99
87±88
95±96

in one case 15% provided a better line of demarcation.
The distinction between marginally in¯uential and nonin¯uential factors was arbitrarily chosen at 1%.
Appendix A contains the lists of very in¯uential and
marginally in¯uential factors for each of the four models
studied. These two categories of in¯uence capture most
of the variation in responses (see Table 2). Interactions
were found to be signi®cant only in the A-H&M model.
4.3. Results
Table 2 shows the number of factors that exhibited a
base case in¯uence P1% of the variation in any of the
three response variables. This table also shows the percentage of in¯uential factors, and the percent of variation in the responses explained by the in¯uential factors.
(The ``percent variation explained'' is a range for the
multiple responses.)
The A-H&M and Tvedt models are each dominated
by roughly 20% of their exogenous factors, whereas the
Madachy and Sycamore models appear to be much
more ecient. In all but the A-H&M model, the in¯uential factors explain a very high percentage of the
variation in all responses; for A-H&M, approximately
40% of the variation in two of the responses is diused
across many factors.
The results of each experiment can be used to characterize the behavior of the model within the context of
a given job size (10%) and stang (10%). In order to
re®ne the behavioral characterization of each model, the
results in Appendix A were summarized into factor
categories: each factor listed in Appendix A was categorized using the factor categories listed in Table 3 (due
to model dierences, the categories are similar but not
the same for each model). For example, the factors listed
for A-H&M in Table 4 were summarized into four
categories: error generation/regeneration, stang level,
schedule, and QA eort. The percentage contribution to
outcome variation for each factor (shown in parentheses
in Tables 4±7 of Appendix A) was summed for the respective category. Again using the A-H&M model as an
example, the total contribution to variation in Project
Duration by the Error generation/regeneration factors
(Nominal errors per KDSI, Error multiplier for schedule
pressure, Time to smooth active error density, Multi-

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270
Table 3
Percent contribution to variation of in¯uential factor categories
Model
Factor category
A-H&M
Error generation/
regeneration
Stang level
Schedule
QA eort
Madachy
Stang/schedule/
productivitya
Job size
Inspection eectiveness
Inspection practice
Error generation
Tvedt
Work rate/
productivity
Rework
Inspection eort
Job size
Inspection eectiveness
Technical risk
Error generation/
regeneration
Stang level (by
interval)
Inspection bene®ts

Response
Duration

Cost

Quality

49

45

54

7
2
3

5
2
3

88

52

0

12
0
0
0

44
1
0
0

16
40
31
11

39

26

0

16
13
11
0
8
0

20
18
13
0
10
0

0
0
31
49
1.5
4

20
8
1.5

0

1.5

0

0

0

2

61
10
14
3
8
0
0

54
19
4
12
1.4
3
2

b

Sycamore
Work rate/productivity
Budget
Schedule
Reviews eectiveness
Stang level
Error ®x cost
Error generation

a
These factors are aggregated in the model into two inputs from
COCOMO, a schedule constraint multiplier and a constant calibrated
to the organization.
b
Sycamore modeled eort consumption rather than production and
error processing, therefore, it has no response variable for quality.

plier to active error regeneration rate due to error density, and Fraction of escaping errors that will be active)
is 49% (21  20  3:7  2  1:8).
Verbal descriptions of behavioral characterizations,
based on both Appendix A and Table 3, are presented in
this section, followed by a graphical display of the behavioral characterizations.
4.3.1. A-H&M model
Two error generation factors account for approximately 40% of the variation in all three of the major
response variables for cost, duration, and quality. This
dominance of two factors in the same function, error
generation, across all responses may be interpreted as
both a lack of behavioral discrimination and a very tight

263

integration within the model. Speci®cally, this dominance is explained by the biological modeling of error
regeneration, a unique construct of this simulator. (In
fact, the error regeneration time, ``time to smooth active
error density'', contributes 4±5% variation for each response.) Aside from the model's focus on error generation, the remainder of the behavior is quite diused.
Only a stang factor, willingness to change workforce,
contributes a variation (to duration) on the same order
as the two error generation factors. Another 13 factors
account for an additional 12±20% of variation in responses, leaving 22±46% of variation accounted for by
the remaining 49 factors.
4.3.2. Madachy model
This model exhibits discrimination of factor in¯uence
across the responses. The COCOMO schedule constraint dominates the duration; both COCOMO inputs,
plus the job size, dominate the cost; and the quality
response is governed by a combination of job size, inspection eort, inspection eectiveness, and error generation. Factor in¯uence is undiused: one or two
factors account for 70% or more of variation in each
response. The simulation model is very focused, due
largely to the COCOMO-based rule engine that serves
as its front end with inputs to stang, productivity, and
schedule. The model is sensitive to small changes in job
size: it is a dominant factor for eort and in¯uential in
the other two responses. The dominance of the inspection factors for quality clearly re¯ects one of the author's two purposes for the model; the dominance of the
COCOMO factors re¯ects the other purpose, risk assessment.
4.3.3. Tvedt model
This model exhibits a moderate degree of factor in¯uence diusion. No single factor accounts for the majority of variation in a response, but 87% of the
variation for each response is produced by 20 of the 103
factors. Re¯ecting its attention to detail in the inspection
process, the most in¯uential factor for duration and cost
is the percent of work needing only minor changes after
inspection. However, factors for productivity, job size,
and inspection eort are signi®cant in¯uences on duration and cost. This model shares two behavioral patterns
with Madachy's model:
1. all three responses are sensitive to job size;
2. quality is most sensitive to the fraction of errors
found during inspections, congruent with research
that supports the bene®ts of inspections by reducing
defect leakage.
(This is the only one of the four models that counts
defects left in the product after testing.) This model also
contains a heuristic technical risk factor (not calibrated
to any of the usual development metrics), in¯uential for

264

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

all responses because it directly modi®es productivity
and error generation.
4.3.4. Sycamore model
Sycamore modeled eort consumption rather than
production and error processing, therefore, it has no
response variable for quality. This model also exhibits a
moderate degree of factor in¯uence diusion. No single
factor accounts for a majority of the variation in either
cost or duration, but 8 of the 30 factors contribute approximately 85% of the variation for both responses.
Two work rate/productivity factors, nominal work rate
and communication overhead (a productivity modi®er),
account for 50% of each response's variation. Due to the
model's eort consumption approach, the eort budgeted for each development phase is a factor. These
budgets are marginally in¯uential, with one exception:
the eort budgeted for design is signi®cant for project
cost.
4.3.5. Graphical display
In addition to the verbal descriptions above, the data
in Table 3 can be plotted to display a graphical characterization of each model's base case behavior. These are
presented in Figs. 1(a)±(d). This ®gure clearly displays the
contrasts between the behavioral characterizations of the
four models. (Each factor category is colored the same in
all four plots.) Because all four models are intended to
represent the same process (waterfall software develop-

ment) and have very similar scopes, one might expect the
behavioral characterizations to be similar. Some agreements are apparent. Madachy and Tvedt agree that QA
eectiveness is important for Quality; Tvedt and Sycamore agree that work rate/productivity is important for
duration and cost; Madachy and Tvedt agree that job size
is important for all three response variables. Yet the
patterns for the four models are so dierent that one
might wonder whether they represent the same process,
or whether the modeling dierences account for these
dierent behavior patterns.

5. Uses of behavioral characterizations
5.1. Improved modeling
Simulation modeling is an expensive undertaking,
particularly for modeling the abstractions of human
organizations, such as software development projects.
Extensive analysis of the target system is required to
ascertain its structure and represent it in a model. A
substantial amount of eort is usually necessary to
identify, specify, and collect data for model parameterization. Veri®cation and validation of the simulation
model also take a large amount of work. Given these
high costs, eciency in the modeling process is very
desirable. Once a tentative model has been constructed
and populated with values, either estimated or for a

Fig. 1. Percentage contribution of each factor category to variation in response variables for four models (%).

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

known case, local sensitivity analysis can be used for
behavioral characterization. If the characterization
matches observations of the real system, then the modeling process can proceed, but a mismatch may indicate
the need to revisit the system analysis. This early assessment may prevent wasted eort in the modeling
process while providing con®dence in intermediate results.
The use of behavioral characterization in speci®cation of parameter values and in data collection and
analysis is straightforward. Knowing which parameters
must be speci®ed with a degree of precision, a modeler
can direct the expense of data collection and analysis
accordingly. On the other hand, insigni®cant factors do
not warrant the expense of data collection and analysis;
they can be speci®ed with rough estimates. For example, most of the delays common in system dynamics
models fall into this latter category (only one delay
factor is listed in Appendix A). As another example,
despite the detailed modeling of exhaustion due to
overwork in both A-H&M and Tvedt, only one of the

265

six factors in this construct is listed in Tables 4±7 of
Appendix A (for A-H&M, exhaustion depletion delay
time is among the marginally in¯uential for project
eort), and with a very low percent contribution to
variation (1.3). If either of these two models were to be
reused to simulate projects on the order of the base
cases, only cursory eort would be warranted for calibrating these factors.
Taking a longer view, behavioral characterization
also supports modeling evolution. Whether one starts
with a simple model and adds detail (top±down approach), or starts with a detailed model and simpli®es it
(bottom±up approach), behavioral characterizations can
guide or explain modeling decisions over generations of
a model.
The model of A-H&M provides a bottom±up example in which behavioral characterization explains a lack
of construct reuse. The base case behavior of this model
is dominated by its error generation and regeneration
factors. This may explain why its biological modeling of
error reproduction has not been widely reused. This

Table 4
A-H&M model
Project duration

Project eort (cost)

Product qualitya

Very in¯uential
(>10% contribution
to variation)

 Nominal errors per KDSI (21)b
 Error multiplier for schedule
pressure (20)b
 Willingness to change
workforce (14)

 Nominal errors per KDSI (19)b
 Error multiplier for schedule
pressure (18)b

 Nominal errors per KDSI (23)b
 Error multiplier for schedule
pressure (22)b

Marginally
in¯uential (1±10%
contribution to
variation)

 Schedule compression factor
(8.2)
 Time to smooth active error
density (3.7)
 Nominal fraction of man-day on
project (2.9)

 Time to smooth active error
density (3.7)
 Nominal fraction of man-day
on project (3.6)
 Fraction of escaping errors that
will be active (2.2)

 Initial understang factor (2.5)

 Schedule compression factor
(1.7)
 Initial understang factor (1.7)

 Time to smooth active error
density (5.4)
 Nominal fraction of man-day on
project (2.4)
 Multiplier to active error
regeneration rate due to error
density (2.4)
 Schedule compression factor
(2.3)
 Initial understang factor (2.1)

 Multiplier to active error
regeneration rate due to error
density (2.0)
 Fraction of escaping errors that
will be active (1.8)
 Planned fraction of
manpower for QA (1.5)

 Multiplier to active error
regeneration rate due to error
density (1.6)
 Testing manpower needed per
error (1.5)

 Real job size (1.2)

 Willing to change workforce (1.4)
 Exhaustion depletion delay time
(1.3)
 Nominal QA manpower needed
to detect average
error (1.1)
 Multiplier to productivity weight
due to development (1.1)

a

 Fraction of escaping errors that
will be active (1.6)
 Nominal QA manpower needed
to detect average
error (1.5)
 Planned fraction of
manpower for QA (1.1)

Measured by the number of errors ®xed in test.
The interaction between these two factors was signi®cant and it's percentage is allocated between them. A number of other interactions appeared in
the marginally in¯uential list, but they are ignored in this analysis because each was <3%.
b

266

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

Table 5
Madachy model
Very in¯uential
(>15% contribution
to variation)

Marginally
in¯uential (1±15%
contribution to
variation)

a

Project duration

Project eort (cost)

Product qualitya

 Relative schedule constraint
eort multiplier from
COCOMO (79)

 Job size (44)

 Inspection eciency (40)

 Relative schedule constraint
eort multiplier from
COCOMO (28)
 Calibrated organizational
COCOMO constant (24)

 Fraction of code tasks that are
inspected (28)

 Inspection eciency (1.1)

 Code error density (7.0)
 Fraction of design tasks that
are inspected (3.1)
 Design error density (1.9)
 Design error to code errors
ampli®cation (1.7)

 Job size (12)
 Calibrated organizational
COCOMO constant (8.1)

 Job size (16)

Measured by number of errors ®xed in test.

Table 6
Sycamore modela
Project duration

Project eort (cost)

Very in¯uential (>10%
contribution to variation)

 Nominal work rate (27)
 Communication overhead (22)
 Scheduled completion date (13.6)






Communication overhead (37)
Nominal work rate (17)
Eort budgeted for design (14)
Fraction of errors found in reviews (12)

Marginally in¯uential (1±10%
contribution to variation)

















Scheduled completion date (3.5)
Eort budgeted for coding (2.7)
Eort budgeted for integration (2.3)
Errors generated per hour (2.1)
Time to ®x an error (1.7)
Number of level 2 engineers (1.4)
Relative cost of ®xing errors found after the
integration (1.3)

Productivity for each level 2 engineers (8.1)
Eort budgeted for design (6.9)
Number of level 2 engineer (4.8)
Fraction of errors found in reviews (3.3)
Productivity for each level 1 engineers (2.5)
Number of level 3 engineers (2.2)
Eort budgeted for coding (1.6)

 Eort budgeted for integration (1.6)
 Productivity for each level 3 engineers (1.6)
 Number of level 1 engineers (1.0)
a

This model has no error stream, so it has no response for quality.

modeling construct does not support a behavioral
characterization that matches those of many modeled
software production environments. (Lack of reuse of a
modeling construct may re¯ect process advances in
software engineering. The last 15 years has seen a
surging interest in defect prevention, exhibited particularly in an emphasis on inspections and other forms of
peer review.)
Taking a top±down approach, on the other hand,
behavioral characterization provides information for
making decisions about which aspects of a model warrant more detailed consideration. For example, Tvedt
modeled technical risk as a factor aecting productivity
and error generation. This factor simply uses an ordinal
scale (0±2). However, it makes an important contribution to the base case variation in the cost (10%) and
duration (8%) responses, suggesting that a technical risk
factor warrants more detailed modeling, such as development of an interval scale.

5.2. Model selection
Someone who wants to use a software process simulator, but lacks either the expertise or the resources to
develop a model, may want to select an existing model
for use. After ascertaining the requirements for such a
model, the acquirer must decide which model best suits
the intended use. The behavioral characterization, if
available, can be considered in addition to each model's
scope, purpose, responses of interest, and implementation. Following are examples that draw upon the preceding base case behavioral characterizations.
5.2.1. Training
When selecting a model for training new project
managers in software project trade-os, one might
choose a simple model, one that is relatively easy to
understand, but represents basic concepts such as staing, experience mix, production cost, error management

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

267

Table 7
Tvedt model
Project duration

Project eort (Cost)

Product qualitya

Very in¯uential
(>10% contribution
to variation)

 Percent of work needing
only minor changes after
inspection (16)
 Daily manpower per engineer
(15.5)
 Potential productivity for
experienced engineers (12)
 Initial perceived size of the
development phase (11)

 Percent of work needing
only minor changes after
inspection (20)
 Potential productivity for
experienced engineers (15)
 Initial perceived size of the
development phase (13)
 Planned fraction of daily
eort for review (11)
 Level of technical risk (10)

 Fraction of errors found during
inspection (41)

Marginally
in¯uential (1±10%
contribution to
variation)

 Planned fraction of daily
manpower for reviews (8.8)
 Level of technical risk (7.9)

 Inspection team size (5.4)

 Fraction of a man-day spent on
the project (work rate) (7.0)
 Inspection team size (4.3)
 Typical level of work
intensity (2.5)
 Maximum level of potential
work intensity (2.4)

 Fraction of a man-day spent on
the project (work rate)
(3.3)
 Typical level of work
intensity (3.1)
 Maximum level of potential
work intensity (3)
 Number of experienced
engineers working during
last development interval
(1.5)
 Inspection meeting eort
(1.2)
 Communication overhead
(1.1)

a

 Initial perceived size of the
development phase (22)

 Initial perceived size of test
phase (8.7)
 Percentage of defects found
during test (8)
 Defect regeneration (2.6)
 Error generation rate for
experienced (1.7)
 Level of technical risk (1.5)

 Inspection eectiveness in
decreasing future error
generation (1.2)
 Maximum decrease in future
error generation due to
inspections (1.1)

Measured by the number of defects left in product after testing and ®xing.

cost, and project duration. Sycamore's model addresses
these factors and its behavioral characterization supports its use for training. Work rate and productivity
factors dominate the trade-os between cost and
schedule, but these are modi®ed by the budgeting of
eort for each phase, the schedule, the eectiveness of
reviews, and stang levels. In the base case, error generation and management play a small role in the outcomes, but armed with this characterization knowledge,
trainers could make the cost/schedule trade-o more
interesting by increasing both the error generation rates
and the error ®nd and ®x costs.
5.2.2. Risk analysis
A model used for risk analysis should incorporate a
variety of factors. In Madachy's model, inputs produced
by a COCOMO-based rule engine are used to propagate
measures of risk into the simulator. Since the COCOMO
inputs dominate the cost and duration outcomes, the
model is very well suited to its stated purpose of risk
analysis.
5.2.3. Project planning
Project managers often face stang issues regarding
manpower build-up and the levels of experience. Tvedt's

model takes into account both timing and experience of
stang by providing separate stang pro®le inputs for
two levels of experience. The combinations of experience
aect error generation and productivity, including effects of learning, communication overhead, work rate,
and training. The dominance of work rate/productivity
and rework factors, for cost and duration, indicate that
it responds well to ``what-if'' simulation analysis of these
stang issues.
5.2.4. Process improvement
Tvedt modeled an inspections improvement that includes factors for representing the bene®ts of inspections
when supplemented with causal analysis of defects
(``inspection eectiveness in decreasing future error
generation'' and ``maximum decrease in future error
generation due to inspections''). The behavioral characterization shows that these factors have a low degree
of in¯uence on quality as calibrated for the base case
(though May et al., 1990 report a signi®cant in¯uence in
practice). Nonetheless, one can reasonably expect the
model to illustrate the trade-os for this improvement
once it is calibrated to include the additional eort for
causal analysis and the likely inspection bene®ts for a
target organization.

268

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

5.3. Model enhancement
Someone lacking the resources for developing a new
model may also select an existing one for the purpose of
enhancing it. This situation can easily occur when an
organization is considering improvements to its development process. Again, the static characteristics of
models will play an important role in such a decision, but
the behavioral characterization may be as important.
Consider, for example, a process quality manager
who wants to use simulation to study the potential effects of two improvements to work product inspections.
The ®rst improvement is a policy requiring all design
and code to be inspected in an environment where inspections are often neglected as schedule pressure increases. The second improvement is the assignment of
product-oriented roles in addition to process roles.
Madachy and Tvedt both modeled inspection processes. Madachy's model takes constant inputs for
specifying how much inspections are used, but these
inputs could be enhanced as variables dependent on
schedule pressure (the schedule pressure calculation
would need to be added). This enhancement would allow the manager to compare the simulated quality
outcome for runs in which inspection practice is a
function of schedule pressure against those for which a
project adheres to a 100% inspection. The behavioral
characterization for this model indicates he could certainly see the eect of inspection practice on defect
leakage. However, the characterization also shows that
the model is not designed to show the eects of inspection practice on cost and duration. If the manager
wanted to see this trade-o between inspection practice
and cost or duration, he would either have to modify the
model further or turn to another model.
Tvedt modeled inspections in much more detail. The
behavioral characterization of this model shows that
cost and duration are sensitive to inspection eort, while
the quality metric is sensitive to the eectiveness of inspections. Thus this model is a good candidate for
studying the aforementioned trade-o.
For the same reason, Tvedt's model is also a good
candidate for enhancement to study the second improvement, assignment of product-oriented roles in addition to process roles for inspections. In this case, the
required enhancement is minor, adding curves to the
model to represent training and learning costs, and increasing inspection eectiveness. Again, the characterization shows that the eects of these changes would be
readily discernible in the simulated project outcomes.
6. Summary and conclusions
This paper has examined the use of sensitivity analysis in characterizing model behavior and has explained

how fractional factorial experiments can be used for
ecient sensitivity analysis. The procedure was applied
to local sensitivity analysis of four published software
process simulators. Behavioral characterizations for the
base cases were produced from the experimental results.
These characterizations were applied in discussions of
improving modeling and of selecting a model, either for
unaltered use or for enhancement.
Though the experimentation described herein was
originally undertaken with the idea that it might reveal
something about the software production systems
modeled, the results do not support conclusions about
software development. Therefore, we refrained from
making inferences about software development and
drew conclusions only about the models. Since our
®ndings pertain only to the models, no particular level
of model validation has been assumed. In fact, behavioral characterization might be used as a validation tool:
if a new model can reproduce the behavioral characterization of a highly validated model, then the characterization builds con®dence in the new model.
The experimental results reported here apply only to
the base case for each model. However, it is not unreasonable to expect that each model's characterization
may also apply well beyond the base case to the degree
that a characterization may be explained by model
structure. For example, the A-H&M characterization is
readily explained by the biological modeling of errors, a
distinct feature of this model. As another example, the
Calibrated COCOMO constant is a unique feature of
Madachy's model that can be expected to dominate its
behavior in all cases. One who wishes to employ a model
for a set of operational values signi®cantly dierent
from those of the base case should consider characterizing its behavior in the region of interest.
Systematic knowledge of a model's behavior is requisite for its appropriate use, but also guides subsequent
modeling decisions. Thus, sensitivity analysis for behavioral characterization can be expected both to enhance current model characterizations and to contribute
to the maturing of software process modeling practices.
Therefore, the authors encourage its use when reporting
work on software process models.
Software process simulation is a young, emerging
technology. At this time it provides more bene®ts for
process understanding than for process prediction and
design. But, just as the use of simulation in manufacturing matured into a process design tool, simulation of
software processes will also. To achieve this level of
maturity, software process simulators must continue to
explore, test, and validate the signi®cant factors in various software processes. As software process models
evolve, they can be expected to become more accurate
(in the correctly modeling the most important factors),
more balanced in their treatment of factors, and more
diverse (representing software process diversity).

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

7. Further research
The experiments described in this paper were limited
to varying the exogenous factors about the base case for
each model. One may reasonably question the validity
of these results for other cases within the scope of each
model. Would the same behavioral characterizations be
exhibited for other experiments, such as larger projects?
The authors expect the same behavior would be demonstrated for each model insofar as the behavioral
characterization can be explained by the model's structure. However, further experimentation is required to
validate these characterizations beyond the base cases.
This paper did not discuss ``pruning'' of insigni®cant
factors from models because the reported experiments
were limited to sensitivity analysis about each model's
base case. However, further experimentation and sensitivity over the complete ranges of factors will provide
data for identifying those factors that can be eliminated
without signi®cantly aecting modeled project outcomes. More ecient modeling would be accompanied
by the bene®ts of making a model easier to understand,
simplifying training in its use, and improving model
maintainability and performance.
The four models in this study all represent software
projects, but their behavioral characterizations are quite
dierent. This raises questions about the nature of
software development processes and their representations. Do the characterizations dier due system dierences or due to modeling dierences? To the extent that
there exist ``natural laws'' governing the development of
software, commonality in behavioral characterizations
should aid in their discovery and de®nition. Statistical
analysis of more software process models may help to
distinguish systemic variations from representational
dierences.
Sensitivity analysis is one of a number of statistical
techniques available for evaluating software process
simulation models. Other techniques that can be applied
in future research include response surface analysis for
process optimization, uncertainty analysis for quantifying output probabilities based on uncertain inputs, and
decision analysis for evaluating and ranking potential
model improvements.
Appendix A. Tables of base case in¯uential factors for
each model studied, listed by response variable
See Tables 4±7.
References
Abdel-Hamid, T., Madnick, S., 1991. Software Project Dynamics An
Integrated Approach. Prentice-Hall, Englewood Clis, NJ.

269

Clemson, B., Tang, Y., Pyne, J., Unal, R., 1995. Ecient methods for
sensitivity analysis. System Dynamics Review 11 (1), 31±49.
Kellner, M.I., Madachy, R.J., Rao, D.M., 1999. Software process
simulation modeling: Why? What? How? Journal of Systems and
Software 46, 91±105.
Kleijnen, J.P.C., 1995. Sensitivity analysis and optimization of system
dynamics models: regression analysis and statistical design of
experiments. System Dynamics Review 11 (4), 275±288.
Law, A.M., Kelton, W.D., 2000. Simulation Modeling and Analysis,
third ed. McGraw-Hill, Boston, MA.
Madachy, R.J., 1994. A software project dynamics model for process
cost, schedule, and risk assessment. Ph.D. Dissertation, University
of Southern California.
May, R.G., Jones, C.L., Hathaway, G.J., Studinski, D.P., 1990.
Experiences with defect prevention. IBM Systems Journal 29 (1), 4±
32.
McKay, M.D., Beckman, R.J., Conover, W.J., 1979. A comparison of
three methods for selecting values of input variables in the
analysis of output from a computer code. Technometrics 21 (2),
239±245.
Montgomery, D.C., 1997. Design and Analysis of Experiments, fourth
ed. Wiley, New York.
Morris, M.D., 1991. Factorial sampling plans for preliminary computational experiments. Technometrics 33 (2), 161±174.
Myers, R.H., Montgomery, D.C., 1995. Response Surface Methodology. Wiley, New York.
Sycamore, D.M., 1996. Improving software project management
through system dynamics modeling. Master of Science Thesis,
Arizona State University, Tempe, Arizona.
Tvedt, J., 1996. An extensible model for evaluating the impact of
process improvements on software development cycle time. Ph.D.
Dissertation, Arizona State University, Tempe, Arizona.
Dan X. Houston received Ph.D. and M.S. degrees in Industrial Engineering at Arizona State University, Master of Divinity at St. Thomas
University (Houston, Texas), and B.S. Mechanical Engineering at the
University of Texas at Austin. He leads a global software development
team at Honeywell. His research interests include software project
management and economics, software risk and quality management,
and statistical modeling of software processes. He is a member of the
IEEE Computer Society, the Association for Computing Machinery,
and the American Society for Quality.
Susan Ferreira works as a systems engineer in the engineering group at
Motorola's Integrated Systems Division while completing her doctoral
studies. Her previous work experience includes full time positions in
both information systems and manufacturing. Susan earned her
Master of Science in Industrial Engineering from Arizona Statue
University in 1990 after receiving a fellowship to Motorola's Industrial
Fellows program and while working part-time. She also holds a
Bachelor of Science in Industrial Engineering from ASU. Her current
research interests include requirements engineering, software systems
engineering process improvement, and software process modeling.
Susan is a member of IEEE-Computer Society and Institute of Industrial Engineers.
James S. Collofello is a professor in the Department of Computer
Science and Engineering at Arizona State University. He received his
Doctor of Philosophy degree in Computer Science from Northwestern
University, Master of Science in Mathematics and Computer Science
from Northern Illinois University, and Bachelors of Science in
Mathematics and Computer Science from Northern Illinois University.
He is a member of the IEEE Computer Society and Association for
Computing Machinery. His research and teaching interests are in the
software engineering area with an emphasis on software project
management, software process improvement and software quality assurance.
Douglas C. Montgomery is Professor of Engineering at Arizona State
University. He is an author of 12 books and over 100 technical papers.
He is a recipient of the Shewhart Medal, the Brumbaugh Award, the
Hunter Award, and the Shewell Award from the American Society for
Quality Control. He is also a recipient of the Ellis R. Ott Award. He is

270

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 259±270

the one of the Chief Editors of Quality & Reliability Engineering International and a former Editor of the Journal of Quality Technology.
He is a member of the editorial boards of JQT, Quality Engineering,
and the International Journal of Production Research. Dr. Montgomery is a Fellow of the American Statistical Association, a Fellow of
the American Society for Quality Control, a Fellow of the Royal
Statistical Society, and a Fellow of the Institute of Industrial Engineers. He also serves on the Technical Advisory Board of the US Golf
Association.
Gerald T. Mackulak received his B.Sc., M.Sc., and Ph.D. degrees from
the Department of Industrial Engineering at Purdue University. He is
currently the Executive Associate Chair Undergraduate Studies in the
Department of Industrial Engineering at Arizona State University. Dr.
Mackulak has held positions with US Steel, Burroughs Corporation,
and Pritsker and Associates as well as consulting for over 75 national
companies. His primary areas of research interest are in the modeling

of manufacturing systems (particularly semiconductor AMHS), simulation methodology and production control.
Dan L. Shunk is an Associate Professor of Industrial Engineering at
Arizona State University and former Director of the CIM Systems
Research Center. He received his Ph.D. in Industrial Engineering from
Purdue. He is former manager of Industrial Engineering at Rockwell,
former manager of manufacturing systems at International Harvester,
and former VP-GM of the multi-million dollar Integrated Systems
Division of GCA Corporation. He is on the Editorial Board of the
Agility and Global Competition Journal and the International Journal
of Flexible Automation and Integrated Manufacturing. He is an active
member of the International Federation of Information Processors
(IFIP) Committee 5.3 on CIM and is a senior member of SME and
IIE. He is currently pursuing research into global new product development, model-based enterprises and global supply chain. His latest
book is Integrated Process Design and Development.

The Journal of Systems and Software 59 (2001) 247±257

www.elsevier.com/locate/jss

Stochastic simulation of risk factor potential eects for software
development risk management
Dan X. Houston a,*, Gerald T. Mackulak b,1, James S. Collofello c,2
a
Honeywell Inc., 817 W. Northview Ave., Phoenix, AZ 85021, USA
Industrial Engineering Department, Arizona State University, Tempe, AZ 85287-5906, USA
Computer Science & Engineering Department, Arizona State University, Tempe, AZ 85287-5406, USA
b

c

Received 12 July 2000; received in revised form 1 December 2000; accepted 22 March 2001

Abstract
One of the proposed purposes for software process simulation is the management of software development risks, usually discussed within the category of project planning/management. However, modeling and simulation primarily for the purpose of
software development risk management has been quite limited. This paper describes an approach to modeling risk factors and
simulating their eects as a means of supporting certain software development risk management activities. The eects of six common
and signi®cant software development risk factors were studied. A base model was then produced for stochastically simulating the
eects of the selected factors. This simulator is a tool designed speci®cally for the risk management activities of assessment, mitigation, contingency planning, and intervention. Ó 2001 Elsevier Science Inc. All rights reserved.
Keywords: Software development risk; Risk factors; Software risk management; Stochastic simulation; Software process simulation

1. Introduction
One of the proposed purposes for software process
simulation is the management of software development
risks, usually discussed within the category of project
planning/management (Kellner et al., 1999). However,
modeling and simulation primarily for the purpose of
software development risk management has been quite
limited. A notable exception is Madachy's model
(Madachy, 1994), designed partially for the purpose of
risk assessment. It uses an expert system front-end to
provide two simulation inputs that represented combinations of COCOMO drivers heuristically graded for
degrees of risk. This paper describes a dierent approach
to simulation for managing software development risks,
that of researching common and signi®cant software
development risk factors and their eects, then adapting
a base model for stochastically simulating the eects of
the selected factors. This simulator is a tool designed
speci®cally for the risk management activities of as*

Corresponding author. Tel.: +1-602-313-4231.
E-mail addresses: dxlsh@uswest.net (D.X. Houston), mackulak
@asu.edu (G.T. Mackulak), collofello@asu.edu (J.S. Collofello).
1
Tel.: +1-480-965-6904.
2
Tel.: +1-480-965-3733.

sessment, mitigation, contingency planning, and intervention.
This paper is organized to provide an overview of a
project in software development risk factor (SDRF)
research, modeling, and simulation. The basis for the
risk factor approach to modeling is discussed ®rst
(Section 2), followed by results of a literature search that
identi®ed software development risk factors (Section 3).
Modeling SDRFs required new research into their potential eects (Section 4), the results of which were used
to extend a base model (Section 5) with modeling constructs for SDRFs (Section 6). After discussing model
validation (Section 7), a risk management framework is
described as a context for model usage (Section 8). Results from risk assessment runs compare the eects of
selected SDRFs (Section 9).

2. Assessing uncertainty through risk factors
Modeling for risk management requires that one both
understand risk and de®ne a means of characterizing it.
This section discusses the nature of risk using categories
and examples, then discusses the approach of using risk
factors in system modeling.

0164-1212/01/$ - see front matter Ó 2001 Elsevier Science Inc. All rights reserved.
PII: S 0 1 6 4 - 1 2 1 2 ( 0 1 ) 0 0 0 6 6 - 8

248

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 247±257

2.1. Describing uncertainty and risk
Uncertainty gives rise to risk, the potential of loss.
Whether they think of themselves as risk managers or
not, those who manage projects are managing (or failing
to manage) the many uncertainties attendant to a project. The categories of Gemmer (1997) (Table 1) are
helpful for thinking about the many and various uncertainties in managing software projects.
Project managers confront all three of these types of
uncertainty. For example, a manager may not know
when his lead developer will leave the project (uncertainty in time); he may exhibit a lack of in¯uence over a
peer review process (uncertainty in control); or, he may
lack accurate information about customer expectations
for product performance (uncertainty in information).
One means of managing the risks arising from uncertainty is to characterize risky scenarios and identify
the factors in those scenarios. Each scenario can then be
associated with a probability of occurrence, a potential
cost, and its value (utility) to the stakeholders. However,
analysis of scenarios begins with identi®cation and
quantitative description of the factors composing scenarios. These risk factors can then be arranged in various scenarios and, using a vehicle for propagating their
uncertainties, can be related to system outcomes.
2.2. A generic process for identifying and assessing system
risk
When risk factors and their associated probabilities
of adversely aecting an outcome can be speci®ed, then
a variety of methods can be employed to synthesize the
factors in a model for estimating the system risk. These
methods can be generally characterized by a generic
process for estimating the inherent uncertainty for each
risk factor, then propagating the uncertainties from the
factor level to the system level. This process is composed
of ®ve steps that are aligned with the de®nition of risk.
· Identify the risk factors. A system must be analyzed
for the purpose of identifying the signi®cant risk factors.
· Model the system to incorporate the risk factors. A
model of the system describes the relationships between the signi®cant risk factors.
Table 1
Managerial categories for uncertainty
Type of
uncertainty

De®nition

In time

Uncertainty about when certain events
may occur or the ability to react to them
Inadequate authority to make or in¯uence
decisions or inconsistency in processes
Inadequate or inaccurate information on
which to base decisions

In control
In information

· Quantify risk factor uncertainties. The uncertainty
characterizing the risk factors is described by assigning each a probability of occurrence.
· Propagate the uncertainties. In this step the model is
exercised to generate a probability of system success/failure.
· Sensitivity analysis. Once the probability of system
success/failure is calculated, the model can be used
to study alternatives that decrease the probability of
system failure. This step, or the whole process, can
be iterated until a solution is achieved.
This generic process for identifying and assessing
system risk can be implemented in a variety of methods.
Three of these are regression analysis, expert systems,
and stochastic modeling. Software cost estimation tools
are typically developed by single regression across job
size and multiple regression across in¯uential, or cost
driver, variables that can be treated as risk factors. Input
variances are propagated arithmetically. These tools can
be used for rudimentary risk assessment through sensitivity analysis, particularly when measures of statistical
uncertainty for the inputs are provided.
Madachy's model, mentioned in Section 1, provides a
good example of an expert system for assessment of
scenarios composed of risk factors in the form of combinations of COCOMO cost drivers (cf. Madachy,
1997). This expert system produces a constant that
represents the software development risk. The risk (rather than the uncertainty) is propagated by using the
constant as a simulation input.
PERT (Project Evaluation and Review Technique)
charts can be modeled stochastically by assigning a
probability distribution (usually triangular) to each task
duration, thereby treating the task durations as risk
factors. The uncertainty is propagated by calculating a
project schedule for a sample from each distribution. A
sucient number of these calculations provides a distribution describing the project schedule.
This paper demonstrates dynamic stochastic simulation as a ¯exible vehicle for eectively propagating risk
factor uncertainties of all types (Table 1). A simulation
model supports risk management to the extent that it
supports the generic process described above.
3. Software development risk factors (SDRFs)
The software engineering literature on risk has focused on three topics: (a) the appropriation of techniques from other disciplines; (b) the development of
risk management approaches for software development;
and (c) the identi®cation of software development risk
factors and their relationship to project outcomes. Although the second topic, risk management, has received
the most attention, considerable attention has been and
is being given to understanding the sources of risk in

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 247±257
Table 2
Most important SDRFs































Creeping user requirements
Unavailability of key sta
Reliance on a few key personnel
Project manager unavailable
Instability and lack of continuity in project stang
Lack of sta commitment, low morale
Excessive paperwork
Large and complex project
Lack of experience with the software product type
Inexperience with project's platform/environment/
Methods
Lack of senior management commitment
Lack of client support
Lack of quantitative historical data
Lack of contact person's competence
Inaccurate metrics
Lack of organizational maturity
Complex application
Incapable project management
Inaccurate cost estimation
Excessive schedule pressure
Unnecessary features
Inadequate con®guration control
Lack of user support
Inexperience with the user environment/operations
Excessive reliance on a single development improvement
Large number of complex external interfaces
Immature technology
Low productivity
Unreliable subproject delivery

software development, particularly through identi®cation of risk factors.
A variety of approaches have been used to investigate
SDRFs. From them have emerged collections of risk
factors, including prioritized lists, taxonomies, questionnaires, and matrices for assessing software development risk. Some investigators have produced SDRF
lists numbering on the order of 150 or more factors. In a
study of this literature, Houston (2000) found twentynine (29) of these factors were cited most often in studies
intended to identify the most important SRDFs. This set
of common and important SDRFs, listed in Table 2,
was used to study the potential eects of SDRFs.
4. Identifying risk factor potential eects
The identi®cation of SDRFs provides a starting point
for modeling them in a software process simulator.
However, their potential eects must also be known and
the probabilities of occurrence understood. While a
small but important body of work identifying SDRFs is
accumulating, little work has been undertaken on the
potential eects of these factors. To address this
knowledge gap, a study was performed in two stages to
identify and quantify the potential eects. In the ®rst
stage, a qualitative survey based on causal mapping, a

249

technique commonly employed in system dynamics
modeling (Richardson and Pugh, 1981), was used to
identify potential eects. To facilitate the survey process,
a set of preliminary eects diagrams was drawn using
software development problem eects suggested in the
literature, particularly in Jones (1994). Software project
managers were invited to review these diagrams and
revise them, adding missing eects, deleting insigni®cant
or nonexistent eects, and indicating the relative importance of the eects.
Thirty-six respondents, representing 22 software organizations, each reviewed and marked diagrams for
approximately a third of the factors. Since the quality of
the survey results depended on the knowledge of the
reviewers, the results were validated by gathering data
on the reviewers, using years of experience as a surrogate variable for knowledge of software development
risk factor eects. Reviewers were asked for their years
of experience in each software development role, for the
major software domains in which they had experience,
and for the project sizes with which they had experience.
Statistics calculated for the respondents to each diagram
veri®ed that distribution of the survey packages was
unbiased relative to reviewer experience. Tables 3 and 4
provide summary statistics on the experience of the 36
respondents with regard to roles and domains. All ranges of project sizes were also represented in the experience of these reviewers. Both the depth and breadth of
the reviewers' experience were viewed as supporting the
assumption that they constituted a knowledgeable
group for this survey.
The results were compiled by producing a merged set
of diagrams that showed added elements, deleted elements, and degree of agreement on existing elements. A
third set of diagrams was then produced by extracting
from the second set only those elements upon which
respondents generally agreed. For example, an element
Table 3
Reviewers' average years of experience by software development role
Role

Average years of experience

Individual contributor
Project manager
Other roles
Total experience in all roles

10.1
9.6
7.0
21.0

Table 4
Number of reviewers experienced in each software domain
Software domain

Number of
reviewers

Management information systems
Systems software (for control of physical
devices)
Commercial
Military/DoD
Other

21
27
14
19
8

250

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 247±257

new to a preliminary diagram was used only when it was
independently added by two or more respondents. One
of the ®nal diagrams for which agreement was particularly strong, ``excessive schedule pressure'', (Fig. 1 3) is
shown here with the description provided in the survey
instrument.
Excessive schedule pressure is de®ned by multiple
direct eects on loss of process discipline, productivity,
morale, and exhaustion. The survey results showed that
participants usually had little diculty agreeing to the
description provided with each SDRP, but oered considerable modi®cations to the eects diagram for each
factor.
In the second stage of the study, six of the 29 SDRFs
were selected for further investigation, based on their
importance (both the frequency with which they were
cited as problematic and their potential for adversely
aecting projects) and the interrelationships found in the
®rst survey. Table 5 lists the selected factors. 4
Based on the results of the ®rst survey, these six
factors were hypothesized to form a network of in¯uence as shown in Fig. 2. One of the reasons for selecting
these six factors for further study and modeling is that
the network in Fig. 2 was at the center of a larger network of factors, so inclusion of these six factors can
serve as a basis for later modeling eorts.
A quantitative survey was developed to measure the
occurrence and eects of each factor. This survey asked
project managers to estimate the degree to which each
factor was present as a problem on their most recently
completed project, and the degree to which each of the
hypothesized eects occurred. The survey was announced through e-mail and hosted on a website. Over
an eighteen-day period, 458 people participated.
Regarding the project roles of the survey respondents,
89% were either the project manager (73%) or the team
leader (16%) of the project about which they reported.
Most of the respondents were experienced as both an
individual developer and as a team leader/project manager. Table 6 pro®les the experience of the respondent
population in each of these two role categories. The
respondents worked mostly on custom software projects, either for clients internal or external to their
3

Regarding notation, positive and negative signs indicate the
direction, same (+) or opposite ()), of a relationship between a cause
and its eect. A positive sign indicates that the eect increases as the
cause increases; a negative sign indicates that the eect decreases as the
cause increases. Multiple signs indicate a stronger causal in¯uence.
Also, in an enhancement to the usual method of causal diagramming,
it was necessary to introduce conditions as quali®ers for some of the
eects. Eects in bold are also risk factors.
4
These six factors exhibit all three managerial categories in Table 1.
Requirements creep and stang instability manifests uncertainty in
time. Inaccurate cost estimation is a result of information uncertainty.
The other three factors in Table 3 are conditions that produce
uncertainty in control.

companies (Table 7), and the projects ranged from very
small (64 person-months of eort) to very large (>8000
person-months), but a third of them were of intermediate size (13±70 person-months).
The survey results validated the common occurrence
of the six problems, as well as their potential impacts.
Table 8 shows the percentage of surveyed projects that
exhibited each problem.
Analysis of this data was used to validate or deny the
results of the ®rst survey. Most eects were validated,
but a few were not. For example, ``Lack of senior
management commitment'' was expected to produce an
unwillingness to extend a project schedule as new work
was discovered and requirements changed. However,
only a minor statistical relationship was found between
these two variables. On the other hand, senior management commitment was found to condition the eects
of other factors, such as changes in morale due to excessive schedule pressure.
The data was also analyzed to produce distributions
for random variables that described the eects of each of
the six factors. For example, the percentage of additional work due to requirements creep and the percentage of rework due to requirements creep were both
modeled as exponential distributions.

5. A base model
Software development projects have been modeled in
a number of simulators, but usually these have not been
designed speci®cally for risk management activities.
However, due to their scopes, they can be considered,
individually or in combination, as a basis for a model to
which SDRFs can be added. Thus, the modeling eort
described here proceeds with the notion of a base model
extended with modeling of risk factors and signi®cant
eects.
From among four published, system dynamics models of software projects, two ± Abdel-Hamid and Madnick (1991) (A-H&M) and Tvedt (1996) ± were selected
for constructing a base model. A-H&M was selected for
its comprehensively modeled scope of software project
management and development. It also models the widely
used waterfall process. 5 However, it is limited by its
reliance on COCOMO for stang and scheduling; by
its de®nition of experience as project-speci®c; and by its
aggregation of defect management. Also, its treatment
of reviews has been outdated by advances in work
product inspection techniques and practice, and research has not supported its biological modeling of er5
In the quantitative survey conducted for this research, 29% of
respondents reported using a waterfall process. Other processes
reported used were incremental (23%), prototype-driven (20%), code
and ®x (16%), spiral (7%), and combination or other (5%).

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 247±257

251

Risk Factor: Excessive schedule pressure
Description: Medium size and larger projects are susceptible to the expectation that deliverables can be
completed in shorter durations than are technically feasible. The source may be internal to the project (for
example,project leadership), internal to the organization (for example, senior management or a marketing
group), or external (for example, a client or a user group).

Fig. 1. Description and eects diagram for ``Excessive schedule pressure''.
Table 5
Six SDRFs selected for quantitative study
Creeping user requirements
Inaccurate cost estimation
Excessive schedule pressure

Table 7
Types of software projects about which respondents reported

Lack of sta commitment,
low morale
Instability and lack of continuity in project stang
Lack of senior management
commitment

Project type

Percent of respondents

Custom, internal
Custom, external
Systems
Commercial
Military/DoD
Outsourced or third party
Other

28%
27%
4%
16%
4%
11%
10%

Table 8
Occurrence of six problems studied
Inaccurate cost estimationa
Requirements creepb
Stang attrition and turnoverc
Lack of sta commitment, low moraled
Excessive schedule pressuree
Lack of senior management commitmentf
a

Fig. 2. Hypothesized in¯uence diagram of six selected SDRFs.

Table 6
Experience pro®les of quantitative survey respondents by software
development roles
Experience
(years)

Percent of respondents
Individual
developer role

Team leader/Project
manager roles

0
1±5
6±10
11±15
16±20
>20

21%
28%
27%
15%
5%
4%

5%
43%
31%
14%
5%
2%

57%
60%
58%
50%
58%
38%

Inaccurate cost estimation was considered a problem when the actual
eort expended on the project, after subtracting for requirements
creep, varied from the project's estimated eort by more then 20%
(more than 20% higher or less than 20% lower than the estimated eort
for the project).
b
Requirements creep was considered a problem when requirements
additions and changes either added more than 10% to the product size,
or caused more than 10% of the developed work product to be reworked.
c
Stang attrition was de®ned as unplanned departures. In other
words, stang attrition and turnover was considered a problem if any
of the project sta members left the project prior to the time agreed
upon when they joined the project.
d
Lack of commitment, low morale was considered a problem when a
survey respondent reported the sta's lowest morale during the project
was lower than a neutral point described as ``Neither `up' nor `down'.''
e
Excessive schedule pressure was considered a problem when a survey
respondent described the highest schedule pressure during the project
as greater than or equal to ``High pressure (1.3)'', where 1.3 is the ratio
of ``the eort required for the remaining work'' to ``the eort available
for the remaining work''.
f
Lack of senior management commitment was considered a problem
when a survey respondent disagreed with or was neutral regarding the
statement, ``Senior management was committed to the success of this
project and supported it with adequate resources (sta, tools, time,
etc.) and interest''.

252

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 247±257

rors (May et al., 1990). Tvedt was selected because it
reused many of the modeling constructs employed in AH&M and addressed these limitations. In addition to
combining features of these two models, the base
model was extended to include a rework, or test-and-®x,
cycle.
Composing a base model from existing models has
several advantages. Although a composition from existing models must be validated, it does reuse concepts
and constructs of validated models. To the extent that
the selected models are the same, modeling is facilitated.
When dierent constructs are used to model the same
phenomenon, the more attractive one can usually be
selected. And, the purpose of the new model may be
partially addressed. In this case, some risk factors are
already partially incorporated by both of the source
models simply due to their scope. The largest gain in this
reuse eort was the modeling of schedule pressure. Excessive schedule pressure and two of its eects, on productivity and error generation, were already modeled by
A-H&M and reused in Tvedt.
The organization of the base model is similar to that
of its predecessors. It has sectors for Planned stang,
Actual stang, Eort allocation, Project planning,
Project control, Adjustment of job eort, Productivity,
Work ¯ow, and Quality management. A brief description of each sector follows:
Planned stang. The base model borrows Tvedt's
stang pro®les, one for each combination of experienced/new and developer/tester. The pro®les are used (1)
for summing cumulative planned manpower, (2) for
calculating planned daily manpower, and (3) for calculating remaining planned manpower as it is used.
Actual stang. A-H&M's personnel ¯ow into and out
of a project was adapted to provide two de®nitions of
experience: experienced in the project (A-H&M) and
experienced in platform and processes of the software
development organization (Tvedt).
Eort allocation. This sector controls the priority of
eort usage as in A-H&M, with the following modi®cations: (1) eort allocated for reviews is based on
nominal eort required rather than a fraction of total
available eort; (2) review backlog tolerated, depending
on schedule pressure, is added; (3) eort allocations for
development and testing are treated independently since
they are staed separately; (4) an allocation for rework
after testing is added because this model represents a
project in which testing rework is done by developers;
(5) desired rework delay is a function of time, per Tvedt,
so that the delay is shortened as the project completion
date approaches; (6) the testing eort allocation divides
the total testing eort between planned testing and
testing of rework (new in this model), such that the
planned testing is all done ®rst.
Project planning. This sector, taken from A-H&M,
adjusts the schedule according to what is needed to

®nish the project, subject to the policies for adding sta.
(1) The workforce_level_needed was modi®ed such that
it cannot be less than the planned sta; (2) a switch that
turns o the willingness to add personnel was added so
that usage of a contractor pool can be selected/deselected.
Productivity. Adapted from A-H&M, this sector
contains all the calculations for modifying sta productivity, both developers and testers. The modi®ers are
workforce experience mix, learning as the project progresses, communication overhead, motivation (morale),
and the work rate. Motivation is modi®ed by attrition
and by schedule pressure. The work rate is modi®ed by
exhaustion and the ability to increase the work rate.
Project control. This sector, taken from A-H&M, is
the tracking and reporting sector. It tracks productivity,
perceived completeness, eort still needed, and available
eort remaining. This sector also has calculations for the
start of testing, schedule pressure, and ending the simulation.
Adjustment of job eort. This sector updates the
planned project eort as new work is discovered. AH&M had this function as part of the Control sector,
but Tvedt broke it out into a separate sector. Two eort
amounts are updated: total job eort and eort for
testing. Total job eort is updated by three ¯ows: increases in development work due to additional work,
increases in testing work due to additional work, and
increases due to underestimating the job. Eort for
testing is updated with increases in testing work due to
additional work and, once testing has started, with the
increases due to job underestimation.
Work ¯ow. This sector combines modeling ideas
from A-H&M with those of Tvedt and then extends
them. This sector is based on the ¯ow of work products during a project, from high level design through
rework for defects and retesting. Work can enter the
project four ways: as originally estimated work, as
undiscovered work due to underestimation, as new
work due to requirements creep, and as rework due to
requirements changes. Although a single new requirement may produce both new work and rework of
existing products, these are modeled as two distinct
eects and the inputs are taken as percentages of the
initial perceived job size. Following both A-H&M and
Tvedt, the generation of the software product is highly
aggregated: design and coding are not distinguished.
The remainder of the work ¯ow is adapted from
Tvedt's model and extended to include a rework
phase.
Quality management. This sector is based on the ¯ow
of errors and defects. Errors are generated during
product generation. They may be detected during peer
reviews and then reworked. A bad ®x rate is associated
with this rework because some of the errors may not be
®xed, or they may have undesirable side eects. Un-

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 247±257

detected errors become defects and may be found in
testing. Those defects not found in testing are presumed to escape in the product as released beyond the
scope of the project. Defects detected in testing can be
reworked, with an associated bad ®x rate. The explicit
modeling of peer reviews and lack of attention to unit
testing re¯ects an industrial environment in which peer
reviews are emphasized organizationally and unit testing is left to the discretion of the individual developer.
The type of testing is not speci®ed in the model, except
that it is not unit testing. It is performed by an independent testing sta, so may include product testing,
integration testing, and system testing ± the scope of
the testing represented by the model depends on the
scope of the project simulated and how the model is
calibrated.
6. Modeling SDRFs
Though the base model is deterministic, it provides a
sound vehicle for propagating the uncertainties of risk
factor eects characterized by random variates. The

253

project outcomes, re¯ecting the variation of the random
distributions, can be estimated as con®dence intervals
rather than as point estimates. Specifying outcomes in
terms of con®dence intervals, calculated speci®cally
from selected risk factors, provides a strong statistical
tool for supporting risk management.
Risk factor eects in this model have been implemented stochastically, with the exception of eects already modeled. When eects of a risk factor were
incorporated in the base model, the existing deterministic modeling construct for the eect was retained so as
to limit the expense of data collection. For new eects
modeled, analysis of the survey data generally provided
both the probability of each eect's occurrence and the
distribution of a random variable for the degree of each
eect.
Table 9 lists the potential eects modeled for each risk
factor and the variables used to model each eect. Each
random variable is implemented with a distinct random
distribution. Some distributions in the model are sampled once per run and others are sampled continuously
throughout each run. For example, requirements creep is
calculated for each run, so the distributions for its vari-

Table 9
Six SDRFs selected for simulation
Risk factor

Potential eects modeled

Random variables

Requirements creep

Increased job size

Percent new work due to requirements creep
Time of maximum requirements creep
Percent of requirements creep after the
maximum
Percent rework due to requirements creep
Relative rework productivity

Rework
Inaccurate cost estimation

Actual job eort more or less than eort
provided by stang and schedule

Size estimation inaccuracya

Excessive schedule pressure

Fluctuating productivity, exhaustion, and
higher error generation
Morale change
Weaker reviews

(In existing models)
Change to morale for schedule pressure
(In existing models)
(Review backlog tolerated)

Lack of sta Commitment,
low Morale

Lower productivity
Increase in error generation
Attrition

Multiplier to productivity for morale
Multiplier to error generation for morale
Multiplier to attrition for morale

Instability and lack of continuity
in stang

Attrition

Attrition
Replacement delay
Change to morale for attrition
(In existing models)

Lack of senior Management
Commitment

Initial schedule compression
Understang
Lower morale after periods of excessive
schedule pressure and attrition

Morale change
Lower productivity due to loss of expertise

Willingness to extend schedule

Multiplier to schedule
Multiplier to stang
Change to morale for schedule pressure
Change to morale for attrition
Maximum schedule extension

a
Data were collected for calculating inaccuracy of both size and eort estimates. Analysis revealed that (1) the distribution of cost estimation inaccuracy was the same for projects that estimated size as for those that did not; and (2) the distributions for size estimation inaccuracy and cost
estimation inaccuracy were nearly the same. For these reasons and because the model production stream is driven by job size, size estimation inaccuracy is used as a surrogate for cost estimation inaccuracy.

254

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 247±257

ables are sampled at the beginning of each run. The eect
of low morale on productivity can vary during a project
and it can be continuously recalculated, so the distributions for its variables are sampled continuously
throughout each run.
6.1. Stochastic modeling construct examples
An explanation of every stochastic modeling construct employed in the model is beyond the scope of this
paper, but two examples, requirements creep and the
eect of low morale on productivity, are presented here.
These two examples were chosen because they demonstrate two dierent kinds of stochastic modeling, one for
a risk modeled as characteristic of an entire project
(requirements creep), and the other modeled as a risk
that may vary throughout a project (eect of low morale
on productivity).
6.1.1. Requirements creep
Requirements creep was modeled as a continuous
¯ow of requirements additions and changes into a project. The ¯ow increases linearly until it reaches a maximum, then decreases linearly (Fig. 3).
To parameterize this construct for new work due to
requirements creep, the survey asked for the amount of
this work as a percentage of the estimated job size, for
tmax , and for the percentage of the requirements creep
that occurred after tmax . Analysis of the data produced
exponential distributions for the new work as a percentage of the estimated job size and for the percentage
of requirements creep after the maximum; the time of
maximum requirements creep is approximated by a
Weibull distribution. These distributions are sampled
once at the beginning of each run.
6.1.2. Eect of low morale on productivity
Morale is modeled as stock having an initial level set
by the user. This starting level, a value between 1 (lowest
morale) and 11 (highest morale), typically re¯ects good
morale, for example 7 (slightly ``up'') or 8 (fairly satis®ed team). The morale level can be aected by high
schedule pressure and by attrition. It may, in turn,
produce eects on productivity, error generation, and
attrition.

Fig. 3. A continuous model of requirements creep.

Fig. 4. Logic for modeling the eect of low morale on productivity.

With regard to the eect of sta morale on productivity, the survey results showed that (a) when low
morale aected productivity, productivity decreased,
(b) the probability of an eect increased from 0.46 to
0.95 as morale decreased from ``slightly down'' to
``lowest: open rebellion'', and (c) the degree of decreased productivity can be characterized by a dierent
random distributions dependent on the level low morale. Consequently, the productivity decrease due to
low morale is modeled conditionally, as shown in the
listing of Fig. 4.
In Fig. 4, U 0; 1 is a random number generated from
a uniform distribution 0; 1 to represent the probability
of the productivity eect at each level of low morale.
lognormal_distribution and Weibull_distribution are inverse transforms for random distributions of the fractional decrease in productivity. These distributions are
sampled continuously during each run, producing stochastic productivity decreases throughout the periods
when morale is low.

7. Model veri®cation and validation
The completed model, including both the base
model and risk factors, was veri®ed and validated
using the framework proposed by Richardson and
Pugh (1981). This framework allocates con®dencebuilding activities into a three-by-two matrix, having
veri®cation, validation, and evaluation activities on
the one axis, and structure and behavior on the
other. These activities were performed by three individuals, the ®rst author and two experts in software
process simulation who both have software development experience. The two experts reviewed the model
for structural adequacy, structural sensitivity, face
validity, parameter validity, and the appropriateness
of model characteristics for the intended audience.
The remaining exercises were performed by the ®rst
author.
The most interesting of the exercises was a case study
in which the model's behavior was validated by using it
to replicate an actual project. In the actual project, an
average full-time equivalent sta of slightly less than

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 247±257

255

Table 10
Actual project replication results
Actual
Simulated
% Dierence

Cost (person-days)

Duration (days)

Defect density (defects/KLOC)

Size (KLOC)

836
889
6.3

220
231
5.0

1.06
1.05
)0.9

32
32
0

four people produced a forms processing application in
10 months. The application had been underestimated
about 18% at 26 KLOC. The project experienced a
nominal amount of requirements creep (5% of the estimated product size). The project sta had very high
morale and experienced no attrition, despite very high
schedule pressure in the later stages. Senior management
was very committed to the project.
The simulation model was calibrated to the actual
project using values for 37 variables, including the
amounts of requirements creep and estimation inaccuracy. The actual project had no risk exposure, aside
from these two factors. Consequently, no variation is
re¯ected in the simulation results (Table 10). The simulation results closely approximated the actual project
outcomes, cost and schedule being higher in the simulation (6.3% and 5.0%, respectively) than for the actual
project.

8. Model usage
Software project risk management is usually described in terms of six activities in two major categories,
risk assessment and risk control. Boehm's framework
(Fig. 5) is illustrative.
Within this framework, the present model, called
Software Project Actualized Risk Simulator (SPARS), is
designed to support risk analysis and risk management
planning for the six risk factors studied. Two types of
simulation runs can be used in analyzing risks: (1) running the simulator without treating any of the risks as
actualized provides a baseline for the project outcomes;
and (2) running the simulator with risk factor(s) actualized provides potential outcome measures for comparison with the baseline.

Fig. 5. Boehm's risk management framework.

SPARS is also designed to support three kinds of risk
management planning activities: risk mitigation, risk
contingency planning, and risk intervention. For
risk mitigation study, various risk reduction measures
may be simulated by setting inputs at the outset of a run.
For risk contingency planning, triggers have been added
to the model so as to simulate invocation of a mitigation
activity upon the occurrence of a predetermined condition. For risk intervention, outputs provide the user
with simulated project states so that a ``project'' may be
paused and project parameters may be manipulated,
thereby simulating an unplanned change for reducing
the eects of an actualizing risk.

9. Model results
To illustrate some results of SPARS, it was ®rst parameterized similarly to A-H&M's EXAMPLE project
(1991), their prototype project for experimentation.
(SPARS could not be calibrated exactly to imitate EXAMPLE due to dierent means of inputting stang and
schedule, calculating attrition, and processing defects
found in testing.) The EXAMPLE project plan was to
produce 42.88 thousand lines of code (KLOC) in 296
days with an average stang level of eight full-timeequivalent personnel, for an estimated cost of 2359
person-days. Due to underestimated product size, EXAMPLE produced 64 KLOC in 430 days at a cost of
3795 person-days. The SPARS base case had an average
full-time-equivalent sta of 10 people scheduled for 348
days. It produced 64 KLOC in 383 days at a cost of 3606
person-days. Although the cost of SPARS baseline case
was within 5% of EXAMPLE, the SPARS base case
includes the cost of ®xing defects found in testing and
testing the ®xes, a cost not included in EXAMPLE. The
duration of the SPARS case was 11% shorter than
EXAMPLE because EXAMPLE uses a COCOMOcalculated stang rampup and rampdown, whereas the
SPARS base case was fully staed at the start with
constant stang levels, did not sta from a contractor
pool, and had no attrition. Lack of rampup and rampdown, as well as separation of developer and test personnel, account for the higher average stang level in
SPARS. The lower cost of the SPARS base case despite
a higher average stang level is also explained by the
fact that SPARS sums the applied costs rather than the
cost of available sta.

256

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 247±257

After establishing baseline outcomes, the risk factors
were actualized. Figs. 6 and 7, which illustrate results of
risk assessment using survey data with the SPARS base
case, display six sets of results, the baseline run and a
90% con®dence interval on the mean (calculated from
100 runs) for each of ®ve actualized risk factors. Fig. 6
displays the results for eort consumed and Fig. 7 displays the results for project duration. These con®dence
intervals indicate, with 90% con®dence, that a mean
project outcome falls within the respective interval.
The con®dence intervals ordinarily indicate variance
above the baseline case outcomes, but the results for
inaccurate estimation are an exception because overestimation can occur. Usually the presence of a risk is
indicative of probable higher project cost and duration.
Simulation results re¯ect this tendency by being very
positively skewed. Consequently, the con®dence intervals were not calculated in the usual manner using a tstatistic; instead, a distribution, such as a Weibull, was
®t to the outcome data and an inverse transform of the
®tted distribution was used to obtain the 1 a values
for a  0:05; 0:95.
The model and distributions derived from the survey
show that requirements creep is the most signi®cant risk
factor modeled. Its con®dence intervals are the longest,
indicating a very high degree of unpredictability in
outcomes for projects subject to this risk. Its con®dence
intervals also achieve the highest magnitude, indicating
the largest impact on project outcomes.
Inaccurate estimation also has long con®dence intervals with high magnitudes, making it the second most
signi®cant risk of this group. Lack of senior management commitment is also a major risk, more to project
duration than to project cost because one of its eects
can be cutting sta. The potential eects of attrition and
to low morale are relatively unimportant next to the
previous three factors.

Fig. 8 displays 90% con®dence intervals on the mean
project duration for each degree of senior management
commitment (without the eect of cutting sta). Recall
that the nature of this factor's in¯uence is dierent from
others: aside from sometimes in¯uencing project stang
and schedule, it does not have direct eects. Rather it
tends to act in concert with other risk factors. This
factor is a subtle one and its in¯uence on project outcomes exhibits an unusual pattern. It demonstrates a
tendency to have less impact but be more unpredictable
with increasing commitment.
Low morale can also occur at varying levels and the
model provides for setting the starting level for morale.
Fig. 9 shows the project cost in response to the starting
level of sta morale. These results are shown as points
because the runs showed little variation within each
starting level. The interesting trend in this graph is that
project cost is aected measurably as morale decreases,
but the cost increases dramatically when the sta morale
decreases from low to very unhappy.
In addition to studying the eects of individual risk
factors, the model oers the bene®t of simulating any
combination of the modeled risk factors, as well as a
wide variety of risk management plans. Furthermore,
the distributions in the model are based on industry
data, but may be treated as a starting point for risk
assessment because they can be re®ned to re¯ect the
experience of a particular organization. This usage was
demonstrated by producing an extensive ®ctional risk
management scenario in which the mitigated eects of
the risk factors were simulated in a series of twelve steps
that produced (a) a list of 11 items for a risk management plan, and (b) numbers that illustrated the project
planning trade-os between released defect density, duration, and cost (Houston, 2000).

Fig. 6. Con®dence intervals on mean cost for actualized risk factors.

Fig. 8. Con®dence interval on mean project duration for each degree
of senior management commitment.

Fig. 7. Con®dence intervals on mean duration for actualized risk
factors.

Fig. 9. Variation in project cost due to starting level of low sta morale.

D.X. Houston et al. / The Journal of Systems and Software 59 (2001) 247±257

257

10. Summary and conclusions

References

The study of SDRFs was undertaken to identify
common and signi®cant risk factors for modeling in
simulation. Qualitative and quantitative surveys were
used to study the factors and their potential eects.
These survey results provided the basis for developing
stochastic modeling constructs and statistical relationships incorporated into a software process simulator.
This simulator, with its ability to model development
project outcomes probabilistically, demonstrates that
explicit modeling of risk factors and their potential effects provides an additional tool for software development risk management, particularly in the assessment
and planning phases. Software project managers can use
such a model to study the potential impact of various
combinations of risk factors on project outcomes, then
run simulations for risk mitigation plans, risk contingency plans, and interventions, all as means of elucidating their experience and supporting project
management decisions.

Abdel-Hamid, T., Madnick, S., 1991. Software Project Dynamics: An
Integrated Approach. Prentice-Hall, Englewood Clis, NJ.
Gemmer, A., 1997. Risk management: moving beyond process. IEEE
Software 14 (3), 33±43.
Houston, D., 2000. A software project simulation model for risk
management. Ph.D. Dissertation, Arizona State University,
Tempe, AZ.
Jones, C., 1994. Assessment and Control of Software Risks. PrenticeHall, Englewood Clis, NJ.
Kellner, M.I., Madachy, R.J., Rao, D.M., 1999. Software process
simulation modeling: why? what? how? Journal of Systems and
Software 46, 91±105.
Madachy, R.J., 1994. A software project dynamics model for process
cost, schedule, and risk assessment. Ph.D. Dissertation, University
of Southern California.
Madachy, R.J., 1997. Heuristic risk assessment using cost factors.
IEEE Software 14 (3), 51±59.
May, R.G., Jones, C.L., Hathaway, G.J., Studinski, D.P., 1990. Experiences with defect prevention. IBM Systems Journal 29 (1), 4±32.
Richardson, G.P., Pugh III, A.L., 1981. Introduction to System
Dynamics Modeling with DYNAMO. MIT Press, Cambridge, MA.
Tvedt, J., 1996. An extensible model for evaluating the impact of
process improvements on software development cycle time. Ph.D.
Dissertation, Arizona State University, Tempe, AZ.

11. Further research
Research into SDRFs is ongoing and research into
their potential eects has begun. Continued research in
this area is expected to provide further insights into and
data for the development of models for software project
risk management. Modeling constructs for risk factors
are also evolving. Experimentation with risk-oriented
simulators may provide insights into the relative in¯uence of various risk factors and into best practices for
project risk management. Though some SDRFs are
quite common, some appear to be speci®c to certain
software development domains (Jones, 1994), which
suggests domain-speci®c simulation models for risk
management.
Finally, this research was oriented toward collection,
analysis, and use of industrial data from a wide range of
software projects. While use of such data provides a
good starting point for risk management in organizations lacking such data, results can be expected to improve by re®ning probability distributions using local
data.

Dan Houston received his Ph.D. and M.S. degrees in Industrial Engineering at Arizona State University, Master of Divinity at St. Thomas
University (Houston, Texas), and B.S. Mechanical Engineering at the
University of Texas at Austin. He leads a global software development
team at Honeywell. His research interests include software project
management and economics, software risk and quality management,
and statistical modeling of software processes. He is a member of the
IEEE Computer Society, the Association for Computing Machinery,
and the American Society for Quality.
Gerald T. Mackulak received his B.Sc., M.Sc., and Ph.D. degrees from
the Department of Industrial Engineering at Purdue University. He is
currently the Executive Associate Chair Undergraduate Studies in the
Department of Industrial Engineering at Arizona State University. Dr.
Mackulak has held positions with US Steel, Burroughs Corporation,
and Pritsker and Associates as well as consulting for over 75 national
companies. His primary areas of research interest are in the modeling
of manufacturing systems (particularly semiconductor AMHS), simulation methodology and production control.
James S. Collofello is a professor in the Department of Computer
Science and Engineering at Arizona State University. He received his
Doctor of Philosophy degree in Computer Science from Northwestern
University, Master of Science in Mathematics and Computer Science
from Northern Illinois University, and Bachelors of Science in
Mathematics and Computer Science from Northern Illinois University.
He is a member of the IEEE Computer Society and Association for
Computing Machinery. His research and teaching interests are in the
software engineering area with an emphasis on software project
management, software process improvement and software quality assurance.

2011 IEEE World Congress on Services

Transaction Level Economics of Cloud Applications
Kevin Buell and James Collofello
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
Tempe, Arizona, USA
Email: kevin.buell@asu.edu
costs: processing, storage, bandwidth, and outside services [1],
[2]. Hence, we are considering cloud application costs in the
context of Software as a Service (SaaS) and Platform as a
Service (PaaS).
We define the cost of a transaction with a cloud application
to be the cost of each of the constituent parts. That is, for a
given transaction T with constituents Tp for processing, Ts for
storage, Tb for bandwidth, and Tv for services, we define

Abstract—Cloud computing provides many benefits to users
including the elimination of up front infrastructure investment
plus lower overall infrastructure costs due to increased resource
usage efficiency. Cloud applications can also make use of outside
services on demand. Since cloud computing operates on a payas-you-go model, application developers must understand the
economics of their software in finer detail. They must understand
how much their software costs to run and how much revenue
they earn as their software runs. This understanding comes from
knowing how the application uses infrastructure and services for
which costs accumulate. We explore some fundamental concepts
of cloud application economics at the transaction level.

Cost(T )

=

I. I NTRODUCTION

Cost(Tb ) + Cost(Tv )

In a traditional computing model where software is hosted
on in-house servers, economics are often considered at a
relatively high level. In addition to software development
costs, considerations include a large up front infrastructure
investment, third party software purchases or contracts, and
bulk bandwidth service agreements. An expected overall level
of revenue is forecast, and the business attempts to recoup initial costs over time. During system planning, various levels of
resource usage would be accounted for by allocating resources
with slack, and only in the worst case would unforeseen and
excessive resource usage cause system unavailability [1].
Cloud users are not concerned with resource or system
unavailability, nor are they as concerned with the economics of
large initial investments (though they still need a business plan
that recoups initial development costs). Instead, what follows
naturally from a pay-as-you-go environment is an earn-asyou-go environment. The more they understand about the fine
economic details of their software (i.e. at the transaction level),
the more cloud application developers will be able to maximize
profit and/or minimize the costs they pass on to customers.
This work is particularly beneficial to transaction oriented
applications and services, especially those that might aggregate
other services. Possible examples include middleware for
processing credit card purchases, online brokerage transaction
software, and real estate research engines.

We have consulted the pricing policies of Amazon Web
Services [3], Google App Engine [4], and Microsoft’s Windows Azure [5]. Note that cloud providers may charge for bulk
purchases of resources, much more than a single transaction
requires. We scale the costs down to the transaction level, but
ongoing research may help overcome obstacles to more fine
grained cost accounting and billing [6]. These fine grained
costs can be associated with source code which enables static
analysis to help us understand how cloud applications incur
costs.
A. Processing Costs
We measure processing in terms of time [3], [4], [5].
Ultimately, our goal is to determine the clock time required for
a transaction (T IM E) and have in hand a cost per nanosecond
(Cost(ns)). Furthermore, we would need to know the number
of bytes of memory (M EM ) reserved for the duration of the
transaction and a corresponding cost per byte per nanosecond
(Cost(mem)). This yields:
Cost(Tp )

=

Cost(ns) × T IM E(T ) +
Cost(mem) × M EM (T )

Of course, cloud providers/vendors are free to produce
pricing mechanisms as they see fit, and processing cost per
hour appears to be common. So for example, if a cloud
provider charges $0.10 for an hour of processing, we must
obtain the cost per nanosecond by dividing that number by
60 × 60 × 109 .
Memory costs are currently not well defined by vendors or
not separated from processing costs. Amazon Web Services
charges more for memory intensive instances [3], so the

II. C LOUD A PPLICATION C OSTS
Within this section, we discuss a framework for determining
the cost of a single transaction with a cloud application. Note
that when we refer to a transaction in this work, we mean a
single usage of a cloud application, especially one for which a
specific fee is charged or for which a specific cost is budgeted.
We consider four basic areas in which cloud applications incur
978-0-7695-4461-8/11 $26.00 © 2011 IEEE
DOI 10.1109/SERVICES.2011.12

Cost(Tp ) + Cost(Ts ) +

515

application is provided to clients as SaaS, we may charge a
specific amount per use of the service.
We should clarify that our cloud application may be SaaS,
but our application may also use SaaS. Of course, our clients
may instead purchase the services we use directly from our
providers. However, we ostensibly are providing some added
value in composing the services we use in such a way that
would be more difficult for the clients of our application
to do themselves. They decide instead to pay for use of
our service which provides a complete package of logic and
outside services that fits what our clients require.
Paying for a single use is not the only pricing scheme that
produces direct revenue. We may charge clients only after
some number of uses, or we may allow unlimited usage for
a single fee. However, we can attempt to estimate the per
use (transaction) revenue derived from our customers, perhaps
based on usage data we collect or usage patterns we observe.

difference in price from a standard instance can be scaled down
to represent a memory unit cost.
B. Storage Costs
Storage costs are comprised of two main components: the
cost of interfacing with the storage medium and the cost of
retaining data on the storage medium [3], [5]. The former
is similar to other costs we have calculated and can be
determined by taking the vendor’s rate and scaling it down
to the level which we are using for our transaction.
The cost of retaining data on the storage medium is more
difficult to associate with a transaction. To deal with persistent
storage, we introduce a variable into our calculations representing the length of time for which the data we store will be
persisted in the storage medium.
The storage cost of a transaction is based on the number
of bytes stored (B), the cost per byte for interfacing with the
storage medium (Cost(intf store)), the cost per second per
byte for persistent storage (Cost(store)), and the persistence
duration of the data stored during the transaction (ST AY ).
We obtain
Cost(Ts ) =

B. Indirect Revenue or Budgeted Cost
Many products and services may use a cloud application in
part, though it may not be the direct source of revenue. For
example, when an individual purchases a book online, she may
use a vendor’s website to read about the book, access reviews,
and find associated products. The software to provide all of
these ancillary services may run on the cloud, but the book
is the actual product that is purchased and provides the direct
revenue. The services were important and certainly supported
the purchase. Since the services do not produce a specific
amount of revenue, they instead may be allocated a certain
budget.
Other products and particularly services may not produce
any revenue at all, but may use cloud applications and may
benefit from our cost calculations. These include information
technology infrastructure for private sector companies as well
as services provided by governmental and non profit organizations. For example, a local municipality may allow residents
to pay their bills online. The services to do this may run on
the cloud. Though the municipality may not generate revenue
from this service, it is likely saving money since they do not
have to pay the cost of opening envelopes and cashing checks.
Therefore, they allocate a certain budget to the services and as
long as the services stay within the budget, the municipality
is pleased with the results.

Cost(intf store) × B(T ) +
Cost(store) × B(T ) × ST AY (T )

C. Bandwidth Costs
Bandwidth costs are generally separated into those for
incoming and outgoing transfers [3], [4], [5]. If our transaction
uses a certain bandwidth in bytes (IN , OU T ) and our vendor
charges a certain amount per byte (Cost(in), Cost(out)), or
if the charges can be scaled down to this level, we have
Cost(Tb )

= Cost(in) × IN (T ) +
Cost(out) × OU T (T )

D. Cost of Services
As we make use of outside services in the form of SaaS, we
incur costs according to the pricing mechanisms established
by the service vendors and/or our individual service level
agreement (SLA). Let’s assume that we have these costs and
that we have scaled them down to a single use. Further, let us
determine for our transaction the set of service calls we make
{sc0 , . . . , scn }. Then our total cost for using outside services
for our transaction is

Cost(Tv )

=

n
X

IV. C LOUD A PPLICATION P ROFIT
Our ultimate goal is to use our cloud applications to either
produce profit or stay at or below budgeted cost. Our work
focuses on technologies that allow us to understand and
minimize costs and/or ensure costs stay within tolerances.
Maximizing revenue is not part of the current work.
We may attempt to fix a single fee for a single usage of a
cloud application. However, it should be recognized that for all
but the simplest of applications, our actual costs will vary to
service a transaction, sometimes widely. As we strive to ensure
our cloud applications turn a profit, we look for answers to
the following questions:

Cost(sci )

i=0

III. C LOUD A PPLICATION R EVENUE
A. Direct Revenue
Our work is more naturally applied to cloud applications
that directly produce revenue and are made up of one or more
transactions which perform a service that can be associated
directly with some amount of revenue. For example, if our

516

•
•
•
•
•
•

as the amount of data received (and possibly stored later in
a database). These annotations would help a tool perform a
static analysis of source code and estimate an upper bound on
monetary costs for a transaction.

How much does it cost to service a transaction?
Does the cost per transaction vary?
What does the cost of a transaction depend on?
What is the maximum cost of a transaction?
What is the average cost of a transaction?
Could the average cost change significantly?

V. A N E XAMPLE
Suppose we have developed software that analyzes images
and identifies real estate (homes, apartments, lots) in the image
either from a street or aerial view. We decide to use this
software to provide a service that accepts an address as input
and produces an image of the real estate at that address as
output. We charge a flat fee per address and deliver a high
quality image of the real estate, one that uses the latest imagery
and frames the real estate accurately no matter what type of
property it is or where it is located.
We don’t have expertise in hosting web services, and we
don’t want to incur the initial cost of the necessary infrastructure, so we decide to turn to the cloud for a pay-as-yougo platform. Furthermore, since our expertise is in real estate
recognition software, we rely on outside services for other
parts of our system.
First, we use an outside service to convert an address to a
geo location (latitude and longitude). We use a second outside
service to find a street level image for the location. If the
service has such an image, we may post process it with our
algorithm and send it to our customer. Or, we may request
another image at a different scale/width. If we are not satisfied
with the street view, or if the service does not have an image
for the location, we use a third service to retrieve an aerial
image for the location. This service allows us to specify a fine
zoom level, and depending on the type or extent of the real
estate we detect, we may request the image at multiple zoom
levels to obtain an accurate image. We may compare it to the
street view to determine which would give the best detail in
a single image.
After we produce an image for a given location, we store
it in a database for one month. If it is requested again, we
access the database and provide it directly. After one month,
we remove it from the database.
Now let’s work through the cost calculations for a sample
transaction. This particular transaction is processed as follows:
1) Receive street address from customer
2) Determine if we already have an image for this address
and find that we don’t
3) Request geo location for street address
4) Receive geo location.
5) Request street level image
6) Receive street level image
7) Run real estate recognition algorithm and determine
closer image is needed
8) Request closer street level image
9) Receive closer street level image
10) Run real estate recognition algorithm and determine size
and rotation for final image
11) Send final image to customer
12) Store image in database

In addition, we would like to answer the following important
questions:
•
•

Could we make changes to our software to make it
cheaper to service a transaction?
How can we determine (before deployment) if our software will produce more revenue per transaction than it
will cost per transaction?

The first set of questions fall broadly under the topic of
cloud application cost analysis, while the last two questions are
specifically optimization and verification. It is likely that the
cloud computing optimization problem is largely dependent
on the particular application. However, some types of general
optimizations may be possible. For example, an approach to
workflow cost minimization is given in [7].
Cloud application cost verification, on the other hand, might
be accomplished in a largely application independent way.
Let’s state the problem this way: given a cost tolerance for a
specific transaction, can we verify that the software completes
the transaction (under various conditions) using resources with
a total cost less than or equal to the tolerance? Furthermore,
can we characterize cost excessive paths through our software
to convince ourselves that they are so unlikely as to be negligible? These questions position cloud application monetary
costs as a quality attribute or part of a requirement that should
be verified during the software development process.
One method for determining the amount of processing
resources used is to borrow from the field of Worst-Case
Execution Time (WCET) analysis. WCET research makes
very precise calculations about cycle time, taking into account
method caching and even using data flow analysis to give a
tight upper bound on the cycles used for a given function [8].
For cloud application cost analysis, this could provide direct
results or promising tools for calculating processing cost.
However, cloud computing generally abstracts away from the
user (hosted application) the details of the resources it is using.
WCET analysis generally relies on intimate knowledge of
the host processor to determine a tight and accurate bound.
Therefore, in order to adapt WCET to cloud application cost
analysis, we may need use some measured results to find
averages and/or make some assumptions about host hardware.
Some aspects of WCET analysis are difficult to calculate
through static analysis (e.g. loop and recursion bounds). In
these cases, the analysis may rely on user inserted annotations
into source code [8]. Such annotations might also be used for
calculating a bound on monetary costs using tools borrowed
from WCET analysis. Annotations could account for fees
charged for the use of outside services. Cloud application
developers could also plan for specific bounds on the amount
of time an outside service will take to return results as well

517

We now determine the total cost for this transaction. The
calculations use ' to denote rounded values. We use unrounded values to achieve exact values when possible, and
we avoid compounding imprecisions due to rounding.
Our cloud provider charges $0.10 per hour for processing costs and provides a standard set of memory resources. Therefore, the cost per nanosecond comes to about
$0.000000000000027778. The processing duration for the
transaction is 500 milliseconds (500000000 ns), so the processing cost of the transaction is

recoup our costs and make some profit on each transaction.
A viable business plan built on this cloud application would
likely anticipate several million transactions per month.
VI. R ELATED W ORK
Truong presents an approach for estimating and monitoring
costs incurred by scientific cloud applications. The approach
focuses on workflows and measurements rather than transactions and static analysis [9].
Deelman provides cost calculations (measurements) on a
per transaction basis for a scientific application running on
the cloud. Costs are also scaled down to some extent [10].
Henzinger identifies WCET as a method for determining
task duration, but from the cloud provider’s perspective [11].

Cost(Tp ) ' $0.000000000000027778 ×
500000000
' $0.0000138889

VII. C ONCLUDING R EMARKS
We have presented a model for calculating transaction level
costs of cloud applications, and we have applied that model
to a sample transaction with an example application. Note,
however, that our application could handle transactions with
significant cost variations based on the paths taken in our
algorithm and the associated service, bandwidth, and storage
variations. If we attempt to charge our users a single price per
transaction, what amount do we choose? Can we determine,
before deployment, if/when our own costs will exceed the
amount we charge? We leave these issues for future research.

Our cloud provider charges $0.01 to interface with our
storage medium 10,000 times, and $0.15 per gigabyte per
month for persistent storage. Therefore, the cost per interface
is $0.000001 and the cost to store one byte per month is
about $0.0000000001397. Our transaction interfaces with the
database twice and we store an image for one month that is 2
megabytes (2097152 bytes), so the storage cost is
Cost(Ts ) ' $0.000001 × 2 +
$0.0000000001397 × 2097152
=

R EFERENCES

$0.00029496875

[1] M. Armbrust, A. Fox, R. Griffith, A. D. Joseph, R. Katz, A. Konwinski,
G. Lee, D. Patterson, A. Rabkin, I. Stoica, and M. Zaharia, “A view of
cloud computing,” Commun. ACM, vol. 53, pp. 50–58, April 2010.
[2] Y. C. Lee, C. Wang, A. Y. Zomaya, and B. B. Zhou, “Profit-driven
service request scheduling in clouds,” in Cluster, Cloud and Grid
Computing (CCGrid), 2010 10th IEEE/ACM International Conference
on, May 2010, pp. 15 –24.
[3] Amazon. (2010, Dec.) Amazon web services. [Online]. Available:
http://aws.amazon.com/
[4] Google. (2010, Dec.) Google app engine. [Online]. Available:
http://code.google.com/appengine/
[5] Microsoft. (2010, Dec.) Windows azure platform. [Online]. Available:
http://www.microsoft.com/windowsazure/
[6] K.-W. Park, S. K. Park, J. Han, and K. H. Park, “Themis: Towards
mutually verifiable billing transactions in the cloud computing environment,” in Cloud Computing (CLOUD), 2010 IEEE 3rd International
Conference on, 2010, pp. 139 –147.
[7] S. Pandey, A. Barker, K. Gupta, and R. Buyya, “Minimizing execution
costs when using globally distributed cloud services,” in Advanced
Information Networking and Applications (AINA), 2010 24th IEEE
International Conference on, 2010, pp. 222 –229.
[8] R. Wilhelm, J. Engblom, A. Ermedahl, N. Holsti, S. Thesing, D. Whalley, G. Bernat, C. Ferdinand, R. Heckmann, T. Mitra, F. Mueller,
I. Puaut, P. Puschner, J. Staschulat, and P. Stenström, “The worst-case
execution-time problem–overview of methods and survey of tools,” ACM
Trans. Embed. Comput. Syst., vol. 7, pp. 36:1–36:53, May 2008.
[9] H.-L. Truong and S. Dustdar, “Composable cost estimation and monitoring for computational applications in cloud computing environments,”
Procedia Computer Science, vol. 1, no. 1, pp. 2175 – 2184, 2010, iCCS
2010.
[10] E. Deelman, G. Singh, M. Livny, B. Berriman, and J. Good, “The
cost of doing science on the cloud: The montage example,” in High
Performance Computing, Networking, Storage and Analysis, 2008. SC
2008. International Conference for, 2008, pp. 1 –12.
[11] T. A. Henzinger, A. V. Singh, V. Singh, T. Wies, and D. Zufferey, “A
marketplace for cloud resources,” in Proceedings of the tenth ACM
international conference on Embedded software, ser. EMSOFT ’10.
New York, NY, USA: ACM, 2010, pp. 1–8.

Our cloud provider charges $0.10 per gigabyte transferred
for both incoming and outgoing data. Therefore, our bandwidth cost for each direction is about $0.00000000009313 per
byte. Our transaction receives 4 megabytes (4194304 bytes)
of data and sends 2 megabytes (2097152 bytes) of data, so
the bandwidth cost for the transaction is
Cost(Tb ) ' $0.00000000009313 × 2097152 +
$0.00000000009313 × 4194304
=

$0.0005859375

The service converting a street address to a geo location is
free. The service to get street level images costs $0.01 for 10
images ($0.001 per image). Our transaction gets two images,
so total service costs are
Cost(Tv )

=

$0.001 + $0.001

=

$0.002

The total cost for the entire transaction comes to
Cost(T )

= Cost(Tp ) + Cost(Ts ) +
Cost(Tb ) + Cost(Tv )
' $0.0028948

If we knew the costs of all our transactions were close to
this one, we might decide to charge $0.01 per transaction to

518

A Decision Support System for
Software Reliability Engineering Strategy Selection
Ioana Rus
Fraunhofer Centerfor Experimental Somare Engineering, Maryland, irus@fc-md.umd.edu
James S . Collofello
Arizona State University, Computer Science and Engineering Department, collofello@ asu.edu
quality can be controlled and achieved. Getting the
balance right between software quality and other
project drivers such as budget and delivery time is a
real challenge for any project manager. What
practices to use in the project to achieve the required
reliability? What will work and what will not in the
given environment? What methods are applicable?
What is their effect on cost and schedule? These are
some of the questions that must be answered by a
manager when planning a software development
project. The system presented here offers guidance
with finding answers, based on previous experience
with developing reliable software, allowing
knowledge transfer from experts to less experienced
software engineers.
The next section introduces the concepts of
software reliability engineering and reliability
strategy and how to select one appropriate for a
specific project. Then the heuristics and
implementation of the decision support system are
presented together with an example of how the
strategy selection component can be used as an aid to
a manager in the project planning phase.

Abstract
This decision support system was built to assist
project managers to design and tailor the software
development process to their specific projects for
achieving required software reliability values. The
decision support system has two subsystems: a
knowledge-based expert system for reliability
engineering strategy selection and a process
simulator for strategy assessment. We present here
the knowledge-based component for strategy
selection assistance. The paper describes the
framework for identifying software reliability
engineering practices and the decision factors for
selecting a subset of practices appropriate for the
project, taking into account various project
constraints. After identifying heuristics for making
the selection decisions, we have developed a
knowledge base and implemented an expert system to
help a less experienced project manager with
engineering the process in a product quality driven
manner. The expert system is a rule basedfuzzy logic
system, developed by using the off-the-shelf
FuzzyCLIPS shell.

2. Software reliability engineering
There are two main approaches for improving the
development process in order to increase reliability:
by avoiding having defects in the final product and
by using fault tolerance methods. Fault avoidance
consists of fault prevention and fault detection and
correction methods. Fault tolerance allows the
software to continue to operate in the presence of
latent faults, such that the whole system will function
as required. Software reliability engineering ( S E )
consists of applying engineering practices
(sometimes called “best practices”) to the following
tasks: defining the reliability objective of the
software, supporting the development of a system
that achieves this objective, and assessing the
reliability of the product through testing and analysis.

1. Introduction
Although software quality is not easy to define
and measure, quantitative product goals have to be
defined and their achievement must be assessed.
Quality can and must be decomposed into more
measurable and tractable goals, usually defined by
the set of attributes specified in the non-functional
requirements, such as reliability, usability,
maintainability, and portability. Our study focuses on
reliability engineering, but the framework and the
tool developed can also be extended to other quality
attributes. The basic idea of building in the required
software quality is that the attributes of the product
depend on the process followed to produce it, so by
improving the development process the software

376
0-7695-0368-3/99 $10.00 0 1999 IEEE

VI, [51, 161, P I , PI, PI,[lo], [Ill, [W, 1151, [I61
and many others, and by interviews with experts.
More information about this process can be found in
[ 141. The resulting taxonomy of reliability
engineering practices is presented in Figure 1.

A software reliability strategy is a combination of
practices defined for a specific project by selecting
appropriate reliability achievement and assessment
practices according to the characteristics of the
project. A set of reliability engineering activities and
methods has been identified by literature survey: [2],

Defect avoidance
activities

prediction

Fault tolerance
activities

Design for
fault tolerance

Rigorous
requirements
development

Practicing a
development
methodology
(design to
coding)

modular
systems

reuse

integration
testing

Conducting
inspections
and
reviews

-

Configuration
management

(*) not SRE specific

Figure 1. Software reliability engineering practices
manager; it rather assists the manager to tailor the
development process in reliability driven manner.

Some of the practices grouped under Manage
fault introduction and propagation category are not
necessarily reliability engineering specific; they are
software engineering best practices that lead to defect
reduction, therefore having a positive impact on
reliability.
In order to select the reliability strategy for each
specific project, one must first identify what are the
selection criteria and then compare different
strategies against these criteria. A set of decision
factors for reliability strategy selection is presented in
Tables 1,2, and 3, grouped into product, process, and
resource factors.
The next section describes our approach for SRE
strategy selection and assessment. The system we
have developed does not make decisions for the

3. The SRE decision support system
As defined in [ 6 ] , “a decision support system
(DSS) is a set of computer-based tools used by a
manager in connecting with his or her problemsolving and decision making-duties. The manager, of
course, makes the actual decisions and solves specific
problems”. The main tasks of a DSS consist of a)
problem definition, b)alternative solutions generation
and evaluation that finalize with a decision, and c)
decision implementation. Of course, after the
execution, the result of the selected decision must be
assessed and the feedback must be used for

.

377

improving future instances of the decision making
process.
The SRE strategy selection module of the DSS
that
assists
with
generating
alternatives
(combinations of practices) had been designed and
implemented as an expert system. The user can use
the simulator component of the system for analyzing
the predicted effect of each alternative on other
project parameters such as cost and schedule.
Selecting an alternative strategy and evaluating it can
be an iterative process, with a repetitive use of the
DSS, until the user believes he/she has found the
most appropriate solution for a specific project.
This paper focuses on the expert system
component of the DSS, the process simulator being
described in [ 141.
In order to associate SRE practices to project

I Factor

characteristics and to identify the decision factors to
select a SRE strategy for a particular Qroject, we
started with a literature survey and examined SRE
practices to identify in what situations and for what
projects they have been used andor recommended.
Then we interviewed SRE experts, asking them about
the applicability of practices for various projects
according to their experience and what are their
potential recommendations. To the experts that could
not be interviewed, we sent out questionnaires and
received useful replies. As a result of this process, a
set of decision factors and their corresponding
discriminatory values were identified, as well as the
rules that connect the factors with reliability
practices. The decision factors can be classified into
the following categories: product, process, and
resources factors, as shown in Tables 1,2, and 3.

Table 1. Product factors

I Value

I

Size

Functional requirements
Source code
Process interaction
Number of modules

Small (<ZOO FP)/Medium (200-1000 FP)/Complex (>lo00FP)
Small (< 20 IUOC)/Medium(2o-1000 IUOC)/Large(5 1000 IUOC)
ManylFew
Knowunknown

Mission
CSCI

Hazards can be ekily identified
Software is mission critical
Criticality known for each CSCI

Yes /No
Yes No
Yes No

Reliability

Objective

Ultra k10e-4 failurewhr)

Requirements

Profile usage data

Failures

High (loe-4-10e-2 failurewhr)
Normal (1Oe-2 - 0.5 failurewhr) .

Ambiguous
ExDected to chanee

Yes No
Yes No

System distribution
Exists
Can be determined
Modules execution order
Varying activity arrival rates
Varying operational patterns
Historical data for reliability prediction exists
Industry data for reliability prediction exists
Equally severe
Design faults must be tolerated
Data faults must be tolerated
Fault tolerance variant execution
Suspension of service delivery

Centralized/Distributed
Yes No
Yes I No
SequentiWoncurrent
Yes No
Yes No
Yes /No
Yes No
Yes /No
Yes /No
Yes I No
SequentiabParallel
Accepted/Not accepted

support
Service life

Transient (<3 months)
Nontransient (>3 months)

378

Table 3. Resource factors
I Factor

I Vnlr1tJ

1

I Budget allows increase of cost with 0.8"

I Yes/No

I

Personnel

Developers

Developers' experience with formal methods

V&Vpersonnel

Enough developers to form f+Z independent teams
Experience with similar applications
Enough
Skills

Users

Low (e lyear)/Medium (1-2
years)/High (> 2 years)
Yes /No
Yes No
Yes No
Low (e lyear)/Medium (1-2
years)/High I> 2 years)
Yes /No
Yes No

External organization available
Familiar with automated system

.............

The SRE strategy selection guidance system
presented in the next section maps the decision
factors' values in Tables 1, 2, and 3 to the SRE
practices in Figure 1, through heuristics used by the
experts in order to recommend certain practices to be
applied to a specific project.

d.r
decision factors

................

4

pamanenti

knowledge

4. SRE strategy selection assistant

j

II

and activities
andmethods
attributes

An expert system approach was chosen to
implement this mapping because it offers a flexible
architecture, making domain knowledge explicit and
separate from the rest of the system. This feature is
needed because the system's knowledge base is
expected to evolve in time, as more SRE expertise is
acquired. The system captures human expertise,
which is perishable, difficult to transfer, difficult to
document, and expensive, and transforms it into
"artificial" expertise, which is permanent, easy to
transfer, easy to document, and affordable [ 13.
For developing the expert system, we used the
FuzzyCLIPS off-the-shelf expert system shell [ 121.
This shell offers the inference mechanism needed for
reasoning, and it also has the capability of handling
uncertainty. This is a necessary feature of the DSS,
since software development is an activity where there
exists much knowledge that is vague, imprecise,
uncertain, ambiguous, inexact, or probabilistic in
nature. Another reason is that both crisp symbolic
values as well as fuzzy values can be used for the
decision factors. "Fuzziness occurs when the
boundary of a piece of information is not clearcut"[l2] and this is the case with most of the
parameters that define a software product and the
corresponding development process.
The architecture of the expert system is shown in
Figure 2. A graphical user interface (GUI),developed
in Java, handles the dialogue between system and
user. The user inputs the values for the factors that
characterize the project (the factors in Tables 1 , 2 and
3), which are stored in files for each specific project
identified by the project name (Temporary
Knowledge in Figure 2).

GUI

...........

t

Figure 2. Expert system architecture
Each attribute has associated a certainty factor, with
value between 0 and 1, which illustrates how
confident is the user about the fact that a factor has
the selected value (0 means value unknown; 1 means
that the user is absolutely sure about the value
selected). After all the decision factors' values and
corresponding certainty factors have been set the
FuzzyCLIPS is invoked. The inference engine
examines the If-then rules in the permanent
knowledge base (which is valid for all projects), and
activates those rules whose If part matches the
decision factors' values, in a feed forward manner.
An example of inference rules is shown in Figure 3.
The rules have a name, an Ifpart (condition) and a
then part (action performed when the condition is
true). Each rule has a certainty factor CF associated
with it. The certainty factor has a value 0.x between 0
and 1 and.it means "in x% of the cases, if condition
then action". For example, the rule named
Accelerated Testing is read as "If the reliability
intensity objective is ultra reliability, or high
reliability and the system testing period is much
shorter then the mean time to failure (MTTF), then it
is recommended to perform accelerated, in 95% of
the projects". The rule Design diversity method 2

379

recommends in 75% of the projects the use of the NVersion programming method for implementing
design diversity, if variant execution scheme is
parallel, suspension of service delivery during error

-

processing is not accepted, the number of sequential
faults to be tolerated is f, and resources exist for
developing N=f+2 independent software variants.

Rule: Desia diversitv method 1 CF-0.75
IF ((performdesign diversity)AND
(variantexecution scheme is parallel) AND
(suspension of service delivery during error processing
is not accepted) AND
(numberof sequential faults to be tolerated is 0 AND
(resources exist for developing N-f+z independent
variants))
THEN (use N-Version programming)
Figure 3. Sample If-then rules

Rule: Accelerated testin CF 0.95
IF (((ultrareliability) 02
(high reliability))AND
(system testing period compared to MT"F is much
shorter))
THEN (use reliability accelerated testing)

consists of providing a frame and a tool for enabling
the achievement of the required software reliability
level by offering guidance in improving the
development process. The set of addressed reliability
practices (activities and methods), decision factors,
and rules are general at this point. However, it is
expected that they will evolve, as more experience is
gained and acquired in the software reliability
engineering discipline. The knowledge can also be
extended (e.g. to include organizational practices)
and/or specialized to a specific application domain or
company profile. The system is flexible in that
regard, and has been designed such that it allows
adaptation and enhancement.

When the inference engine is running, the certainty
factors associated with the facts (i.e., factors' values)
are combined with the certainty factors of the rules
and give the certainty factors for the recommended
activities and methods. The DSS displays the
activities and methods that are in the system's
repository (part of the Permanent Knowledge in
Figure 2). For each activity and method it is indicated
whether or not the expert system recommends it, and
what is the corresponding confidence factor, as
shown in Figure 4 for activities. The screen for
methods and more sample screens can be found in
[ 141. The user is left the freedom to select or not any
of these activities or methods, regardless of the
system recommendation, because the expert system is
just an assistant to the manager, it guides and helps
and does not impose its results upon the user's
judgement. The user will indicate that he/she wants to
include an activity or to apply a method by clicking
on the corresponding Select button. For each activity
the system displays the life cycle phase in which it is
to be applied and the type of activity (defect
prevention, defect detection, or fault tolerance)
(Figure 4). For methods, the corresponding activity
and life cycle phase will also be displayed. By
clicking on any activity or method name the user can
read a set of attributes of that specific activity or
method, such as purpose, description, and advantages
and disadvantages of applying it (Figure 5). The user
can thus become (more) familiar with that activity or
method and decide whether or not to include it in the
project.

6. Acknowledgments
We want to thank to the software reliability
experts, who were of great help in identifying the
decision factors, software reliability engineering
practices, and heuristics for building and validating
the knowledge base. We appreciate the dialogue with,
and comments and suggestions from David Peercy,
John Musa, Herbert Hecht, Peter Lakey, Steven
Meyer, Raymond Sandfoss, Michael Lyu, Laura
Pullum, Ted Tenny, James Cusick, and all the other
software engineers who generously contributed with
their experience to the development of this decision
support system.

7. References
[I] Giarratano, Joseph and G. Riley, Expert Systems Principles and
Programming,
PWS Publishing Company, Boston, 1994.

5. Summary and future work

[2] Grant Ireson, W.,C.F.Coombs and R. Moss, Handbook of
Reliability Engineering and Munugement, McGraw-Hill, 1996.

We presented a decision support system that can
be used in the planning phase of a project to tailor the
software development process in reliability driven
manner. The DSS consists of a knowledge-based
system for strategy selection and a process simulator
for strategy assessment. The expert system transfers
the knowledge of technical decision making from
SRE experts to less experienced engineers and
managers. The main contribution of this work

[3] Hudak, John, B. Suh. D, Siewiorek, and Z. Segall, Evaluation
& Comparison of Fault-Tolerant Software Techniques, IEEE
Transactions on Reliability, vol. 42, no. 2, June 1993, pp.

190.

[4] Laprie, Jean-Claude, J. Arlat, C. Beounes and K. Kanoun,
Definition and Analysis of Hardware-and Software Fault-Tolerant
Architectures, Computer, July 1990, pp. 39.

380

-~

Name

Phase

Recommended

CF

Rigurous requirements speciflcation

Requirements

Fault preventlon

Yes

1

0Yes

Formal requirements speciflcation

Fault preventlon

Yes

0.5

0 Yes

Deflne s o m a r e failures

Requirements
Requirements

Fault prevention

YES

1

c3

Reviews

Requirements

Fault detection

Yes

1

@ Yes

Operational proflie

Requirements

Fault prevention

Yes

0.75

@

Operational proflie

Deslgn

Fault prevention

Yes

0.75

Reviews

Deslgn

Fault detection

Yes

1

@ Yes
Q Yes

D e s i g n diversity

Design
Design

Fault tolerance

Yes

0.9

0Yes

Fault detection

Yes

0.e5

0

D e s i g n diversity

Coding

Fault tolerance

Yes

0.85

0Yes

identi,

& eliminate d e s i g n potential failure

~~

Select

Yes
Yes

Yes

Data diversily

Coding

Fault tolerance

No

1

0

C o d i n g error prevention

Coding

Fault prevention

Yes

1

0 Yes

inspections

Coding

Fault detection

Yes

1

@ Yes

Unlt testlnp

Codlnp

Fault detection

Yes

independent VBV

Testing

Fault detection

No

0.9
1

@ Yes
0 Yes

Reliability growth testing

Testing

Fault detection

Yes

1

@3 Y e s

Operational profile

Tasting

Fault detection

Yes

0.e5

@

Yes

Yes

Attributes
Deflne sonware failures
Operational profile

Figure 4. Sample output screen for recommended activities
Name :

Design diversity

Purpose :

)when applied :

Design. coding, testing
Requirements speciflcations
A redundant version ofthe somare

Developers
Requirements (including reviews)
Reliability objective is Ultra high or high (for safely or mission critical systems. development faults must be tolerated
No restrictions, other than cost
Tolerates remaining fauns
Costiyto develop (resources. time, 5) and potentially run time overhead
lfYethods :

N-version programming I Recovew biockl Recovew block I N-Self Checking programminu

Figure 5. Sample output screen for activity attributes
[5] Lakey, Peter and A.M. Neufelder, System & So@are
Reliability Assurance Notebook, Produced for Rome Laboratory by
SoftRel, September 1996.

[ 1I ] Neufelder, Ann Marie, Ensuring Software Reliability, Marcel
Dekker, Inc., 1993.

[6] Leigh, William and M.D.Doherty, Decision Supporr and Experr
System, South-Western Publishing Company, 1986.

[12] Orchard, R.A., FuztyCLIPS Version 6.04, User> Guide,
Knowledge Systems Laboratory, Institute for Information
Technology, National Research Council Canada, 1995.

[7] Littlewood B. ed., So@are Reliability, Achievement and
Assessment, Blackwell Scientific Publications, 1987.

[ 131 Rook, Paul ed., Sofnvare Reliability Handbook, Elsevier
Applied Science, 1990.

[8] Lyu, Michael R.ed., Sofnvare Fault Tolerance, John Wiley &
Sons, 1995.

[I41 Rus, Ioana, Modeling The Impact on Cost and Schedule of
Software Quality Engineering Practices, Ph.D. Dissertation.
Arizona State University, Tempe, AZ, 1998.

[9] Lyu, Michael ed., Handbook of Sofhvare Reliability
Engineering,McGraw-Hill, IEEE Computer Society Press, 1996.

[15] SAIC and RTI, Software Reliability Measurement and Testing
Guidebook, Rome Laboratory RL-lR-92-52,1992.

1101 Musa, John D., A. Iannino, and K. Okumoto, Sofnvare
Reliability Measurement, Prediction, Application, McGraw-Hill
Book Company, 1987.

[16] SAIC and RTI, Software Reliability Measurement and Testing
Guidebook, Rome Laboratory RLTR-92-52, 1992.

381

Using System Dynamics Modeling to Manage Projects
Douglas Sycamore
Sofnyare Consultant
607 West Spur Avenue
Gilbert, Arizona 85233
dougsycamore @ worldnet.att.net
James S . Collofello
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287-5406
collofello@asu.edu
by Tarek Abdel-Hamid and Stuart Madnick [l].
System dynamics modeling is primarily based on
cause-effect relationships. These cause-effect
relationships constantly interact while the computer
model is being executed, simulating the dynamic
interactions of a system. The most powerful feature
of system dynamics modeling is realized when
multiple cause-effect relationships are connected
forming a circular relationship, known as a feedback
loop. The concept of a feedback loop reveals that any
actor in a system will eventually be affected by its
own action [4].
In the remainder of this paper, a prototyped tool
is described which incorporates a system dynamic
model to provide better insight into project planning
and control. The research differs from that of AbdelHamid in several ways. First, the work addresses
concurrent incremental software development in
contrast to that of Abdel-Hamid’s, which addressed
the waterfall development. The modeling is also at a
lower level of detail in terms of describing software
development activities. The purpose differs in the
sense that this model provides more control over
project parameters and the output enables a project
manager to understand the status of a project, make
the appropriate decisions, and simulate the results of
those decisions.

Abstract
This paper focuses on improving the planning
and tracking abilities of software projects through
the utilization of system dynamics modeling. A
prototyped software project management tool, which
incorporates a system dynamics model of the
concurrent incremental development process, was
developed and is described in this paper. The tool
enables project personnel to plan and track a project
in terms of schedule, budget, and quality. It also
provides a “what if” capability that enables project
personnel to assess the impact of proposed project
control actions in terms of schedule, budget, and
rework hours. These proposed actions are analyzed
utilizing the underlying system dynamics model
which evaluates the actions in terms of dependencies
and feedback loops among project variables. The
concepts discussed in this paper and demonstrated
through the prototype tool should be integrated into
existing project management tools to increase their
power and efectiveness.

System Dynamics Modeling Background
System dynamics is defined as “the application
of feedback control systems principles and
techniques to modeling, analyzing, and understanding
the dynamic behavior of complex systems” [ 11. This
technique uses mathematical, non-linear differential
equations which translates into a quantitative
approach. It was initially developed in the late 1950’s
at M.1.T by Dr. Jay W. Forrester [7].It was applied
for the first time to the software development process

Tool Introduction
Planning and tracking are critical activities a
project manager performs on a software development
project. Successful projects usually take action when
a project manager determines that a project is

213
0-7695-0368-3/99 $10.000 1999 IEEE

number and domain expertise of the engineers
assigned to the project.

beginning to veer from the development plan.
Numerous indicators exist that can help a project
manager assess the status of a project. A graphical
user interface presenting these indicators in the form
a “control panel” or “dashboard” display has been
developed by the Software Program Managers
Network [3]. Our prototype tool does not currently
possesses all of the capabilities nec&$ary to represent
the Software Program Managers Network display
but, instead, utilizes a subset of this information
abstracted in terms of four primary indicators:

Work Seauence: The Work Sequence parameter
denotes the execution order and degree of
overlap of the increments within the project.
Proiect Scope: The Project Scope parameter is
the measurement of the person hours required to
complete an activity.
Productivity: The Productivity parameter is the
rate at which the effort is occurring on an
activity.

Schedule: The Schedule indicator shows when
an event will begin and when it will finish.
Budget: The Budget indicator
project’s expenditures.

To assist in project management decisions,
primarily planning and tracking, numerous
commercial software project management tools have
been developed. Most tools will perform basic
features, such as scheduling, budget tracking, manhour reporting, resource allocation, and tracking of
capital expenditures. More advanced software
project management tools have the graphical
capability to display Gantt charts, PERT charts,
critical paths, pie graphs, or other types of charts,
graphs, and histograms. They allow for task
breakdowns, personnel and resource assignments,
and conflict identification among the assignments.
However, very few project management tools have
the capability to provide a project with “What-If’
scenarios, and they all lack the ability to model the
system dynamic parameters that influence a project’s
schedule, budget, percent complete, and quality. For
example, the further a project deviates negatively
from the planned schedule, the greater the schedule
pressure, which initially increases productivity. As
the schedule pressure increases to a limit tolerated by
the employees, productivity may begin decreasing
and a greater risk of turnover occurs [5]. None of the
tools available today incorporate this type of system
dynamic feedback loop.

tracks the

Percent Complete: The Percent Complete
indicator reflects the amount of effort already
expended versus how much effort is still
remaining.
Ouality: The Quality indicator provides visibility
into the quality of the product. Quality is
defined in terms as the person hours spent doing
rework to fix errors or defects.
In order to evaluate the status of a project,
especially a project of substantial size, a project
manager must monitor and analyze the status
indicators. A project meeting all of the scheduled
milestones, operating within budget, and showing the
Percent Complete indicator corresponds with the
schedule, still may not be on target. If the number of
defects being injected into the system is greater than
anticipated and is affecting the quality of the product,
this may result in disastrous problems later by
causing unplanned rework and resulting in missed
schedule deadlines.
As a pilot monitors many instruments to verify
the status of an aircraft, a project manager must
monitor many status indicators to verify the status of
a project.
These four indicators are good
representatives of the possible set used to monitor the
status of a project. By watching the Schedule,
Budget, Percent Complete, and Quality indicators,
better decisions can be made when a project requires
a corrective action plan [2].
If the status information collected on a project
suggests a problem, there are numerous parameters to
manipulate in an attempt to bring a project back inline with the plan. This prototype tool abstracts the
controllable elements into four basic parameters:
Resources: The Resource parameter defines the

Description of System Dynamics Model
The prototype tool has four basic feedback loops
comprised of non-linear system dynamic variables as
shown in Figure 1. All of these feedback loops begin
and end at the object labeled, Schedule and Effort,
which is the nucleus of the system. This object
represents a project schedule and the effort in staff
hours to complete the schedule.

214

inputs that affect the entire system. The first input
within the "Initialization" box is the productivity
level for the various levels of engineers or domain
experts.
This input allows customizing the
productivity rate for each level of engineer. The
productivity level is relative to a normalized staff
hour.
The next input is "schedule pressure". This input
is a binary value, either include the schedule pressure
feedback loop or do not include the schedule pressure
feedback loop in the simulation.
The third input is "communication overhead".
This input is a binary value like schedule pressure.
Communication overhead models the reduction in
productivity due to factors like meetings and informal
discussions among engineers.
The next set of inputs can be viewed as a group
of values relating to "quality". The first input to this
grouping is once again, a binary value. Including
quality causes the simulation to model the reality that
as engineers work they produce defects. In order to
fix the defects, rework hours are generated. The
rework hours are then fed back into the schedule as
additional hours needed for completing the project.
The second input to the quality grouping is the
percentage of hours allocated for rework in the
schedule. This input allows the flexibility to allocate
a portion of the schedule for rework to fix defects
generated during the development process. The third
input to the quality grouping is the defects generated
per hour. This input provides the capability to
customize the simulation to an individual company or
project The fourth input to the quality grouping is
whether to include peer reviews. Peer reviews are
assumed to take place during the design and code
phases of an increment. The effectiveness of peer
reviews can be adjusted in the model as well as the
cost of fixing defects missed by these reviews.

Figure 1. Basic Feedback Loops of the Tool
The first feedback loop represents the staffing
profile of a project. The staffing profile affects
productivity based on the number of engineers
working on a project, the domain expertise of the
engineers, and amount of time an engineer
participates on a project
The second feedback loop models the
communication overhead. A project requiring many
engineers experiences greater communication
overhead than a project requiring only a few
engineers
The third feedback loop takes into consideration
the amount of defects generated by the engineers
during the design and coding phases of an increment,
which translates into rework hours. The tool also
models the impact of domain expertise on defect
generation. An engineer with less domain expertise
generates more defects than an engineer with a higher
degree of domain expertise.
The fourth feedback loop models the schedule
pressure associated with the percentage of work
complete per the schedule. The farther behind
schedule the greater the schedule pressure. As
schedule pressure increases, engineers will work
more efficiently and additional hours, increasing
productivity toward completing the work. However,
if schedule pressure remains high and engineers are
working many hours of overtime, they begin to
generate more defects and eventually an exhaustion
limit is reached. Once the exhaustion limit is
reached, productivity will decrease until a time
period such that the engineers can recoup and begin
working more productively again.

Increment Znformation
The remaining inputs consist of values for
individual increments. The estimated staff hours to
complete each phase of each increment is entered as
well as increment dependencies. Inputs for the
number and type of staff for each phase of each
increment are also provided.

Outuut Descriution
Figure 2 illustrates an example of an output
layout when running a simulation. The four graphs
visually represent the schedule, budget, percent
complete, and quality of a project.

Inuut Descriution
The inputs to the model are categorized into
sections.
Initialization
The first section, "Initialization" , consist of

215

2 lncr 2 Scop

I

3 lncr 3 Scop

1

The first display, SCHEDULE (Figure 2:a),
displays the remaining estimated development staff
hours for each increment. The "x" axis, which is the
same in all the output graphs, represents time in wall
clock hours relative to the start of the simulation and
the total time to complete a project. The "y" axis for
the Schedule graph is the estimated total person hours
remaining to complete an increment.
The second output, BUDGET (Figure 2:b),
displays the current staff loading on line #1 and the
cost incurred from staff loading on line #2..
The third output, PERCENT COMPLETE
(Figure 2:c), displays the actual and perceived project
completion measured against a linear completion and
a warning line. Line #1 is the warning, line #2
represents the linear completion line that corresponds
directly with the schedule ("x" axis), line #3
represents the perceived project completion, and line
#4 represents the actual project completion.
The fourth output, QUALITY (Figure 2:d),
displays the total hours allocated for rework, the total
rework hours performed, the rework hours associated
with defects found in the design and code phase, and
the rework hours associated with defects discovered
during the integration phase.

I

M.d*W,

a. Schedule
2: Total cost

1: Total Staff Load
1:

E

t

Han

b. Budget
1: Danger Zone

2: S c M u b X Comp

Evaluation

3 P m s l d Un C m p

4 Actual IC m p

To assess the reasonableness and usefulness of
the prototyped tool, six experts in project planning
and control were selected to evaluate the tool. Each
was given a tool demo and the opportunity to run

simulations using their own project data or generated
example project data and evaluate the results.
Overall, the tool fared extremely well against the
evaluation process and the experts were quite pleased
with the results. There were two particularly exciting
highlights that occurred during the reviews with
expert #3 and expert #5 that are worth noting.
The first highlight was the results from
simulating expert #3's Man-Machine Interface
project, which was 80% complete. Before simulating
the project, some actual historical data along with
remaining estimated data to complete the ManMachine Interface project was entered into the tool.
After simulating the project with this actual and
estimated data, the tool tracked within a couple of
percentage points of expert #3's actual and predicted
results [9].
The second highlight was simulating three sets
of project data supplied by expert #5. He generated
these three scenarios with the aid of the COCOMO
tool for the three engineering activities: Detail
Design, Code and Unit Test, and Integration. After

c. Percent Complete
1: Totel AUocstd R ~ v o r f2

6 Debeta 3 Total R m w l c

4: Total DdeMa

-d. Quality
Figure 2. Simulation Output

2 16

environments. The process improvement advisor
would enable an SEI level 5 organization to assess
the potential impact of improvements on cycle time
and cost via the simulator.

simulating the three scenarios, expert #5 concurred
that the tool produced believable results for all three
scenarios [9].

Conclusion and Future Work

References

Our research has shown that project management
activities can be improved through system dynamics
modeling. The additional insights gathered from
modeling the system dynamic feedback loops that
exist in projects can provide guidance during project
planning, tracking, and controlling activities. For a
company to use this approach to planning and
tracking, they would need to instantiate our model
with their process refinements and metrics. This will
typically require the equivalent of an SEI level 3 or 4
organization.
Much more research is needed in using system
dynamics to model software project management.
Additional feedback loops must be identified,
analyzed and modeled. Parameters must also be
refined and validated with project metrics. The
interface will be continually updated to reflect the
indicators reflected in the Software Program
Managers Network display. Other applications of
this research, which are being pursued, include the
development of a software project management
training simulator and a software process
improvement advisor.
The software project management training
simulator would be analogous to a flight simulator
for pilots. The simulator would provide project
personnel performing planning and tracking activities
with an opportunity to work in simulated project

[l] Abdel-Hamid, Tarek and Stuart E. Madnick, Software
Project Dynamics An Integrated Approach, Prentice-Hall,
Englewood Cliffs, New Jersey, 1991.

[2]Beagley, S . M., “Staying the Course With the Project
Control Panel.” American Programmer, (March 1994), 2934.
[3] Brown, Norm, “Industry-Strength Management
Strategies”,CrossTalk, August 1996,pp. 7-13.
[4] Collofello, J., Smith-Daniels,D., Rus, I., Chauhan, A.,
Houston, D., and Sycamore, D., “A System Dynamics
Software Process Simulator for Staffing Policies Decision
Support”, Arizona State University Paper, Tempe, AZ,
June 1997.
I‘

[SI DeMarco, T. and Lister, T., Peopleware: Productive
Projects and Teams. New York, NY: Dorset House
Publishing Co.,1987.
[6]Douglas, R. H., Merrill, D. S., Rus, I., and Suchan, W.
K.,“Process Modeling and System Dynamics Modeling.”
Arizona State University Research Paper for the course,
CSE 560 Sofmare Project Management and Development
11, Tempe, AZ,March 24,1995.
[7]Forrester, J.W., ‘The beginning of system dynamics”,
The McKinsey Quarterly, 1995,Number 4.

217

Teaching Technical Reviews in A One-Semester Software
E n g i n e e r i n g Course

Dr~ James S. Collofello
Computer Science Department
Arizona State University
Tempe, Arizona 85287
Abstract

Although much has been learned about software
technical review processes, very little of this
knowledge has been adequately integrated into
software engineering course content material. For
example,
the
typical
one-semester
software
engineering course with a team project is the
logical
choice
for educating students about
technical reviews. This course, however, usually
provides very little information about formal
review processes and often suggests that the
students perform reviews as part of their project
without an adequate discussion of the process.
This is a tremendous disservice to the students
since the reviews that they participate in may be
disorganized, unpleasant and unproductive due to
their lack of understanding of the process. The
experience may then leave the students with a
negative attitude towards processes which will be
much harder to overcome in the future.

Software technical reviews are essential to
the development and maintenance of high quality
software.
These review processes are complex
group
activities
for
which there exist an
abundance of basic concepts evolved over years of
practical experience.
In a typical one-semester
software engineering course very little of this
information is adequately conveyed to students.
Texts supporting this course are also very weak in
this
area.
This paper provides a practical
approach for teaching about software technical
reviews in a one-semester software engineering
course. The contents for two to three lectures on
this topic are described as well as suggested
exercises
and
an
approach
for integrating
technical reviews vlth the usual team project. An
extensive annotated bibliography is also provided
to assist instructors and students.

Many

Background

of

the

existing

software

engineering

texts which support the one-semester software
engineering course are also deficient in their
coverage of technical reviews. This contributes
to the problem of inegrating review processes into
the content of this course. In the remainder of
this paper, an approach for educating students
about software technical reviews in the scope of
this one-semester software engineering course will
be described.
The approach consists of two to
three lectures about the review process, review
process exercises and integration of the review
process into the course team project.

Software
technical
reviews
were
first
described in the computer science literature over
ten years ago [Fagan76].
Since then they have
become
a
well defined and widely practiced
technique
in the development and maintenance
methodologies
of
most
organizations.
These
organizations often possess development standards
which mandate when and how these reviews will take
place or a subject to external standards, such as
the military standards, which govern their review
processes [IEEE84, MILS85].
Over
the
ten years of experience with
software
technical reviews a vast amount of
information
has
been
learned
about
these
processes. Various review methodologies have been
developed
in
which
the
roles
of
review
participants are very well defined. Much has also
been learned about the behavioral factors involved
in review processes and their criticality to a
successful
review.
The mechanics of review
processes including planning issues and reporting
issues have also been carefully analyzed and
documented.

Review Process Lectures
In
concepts

order
to minimally
of review processes

present
the basic
in the scope of the

one-semester software engineering course, two to
three lectures are needed. These lectures should
be timed to follow the overall discussion of
software life cycle models and the description of
the course project and its development plan. The
topics to address in these lectures along with
time estimates per topic are outlined below:
o Rationale for software Technical Reviews
(10-15 minutes)
o Types of
minutes)

Software

Technical

Reviews (I0

Permission to copy without fee all or part of this mdterial is granted provided
that the copies are not made or distributed for direct commercial advantage,
the ACM copyright notice and the title of the publication and its date appear,
and notice is given that copying is by permission of the Association for
Computing Machinery. To copy otherwise, or to republish, requires a fee and/
or specific permission.

o Behavioral Factors (15-20) minutes)

©1987 ACM 0-87791-217-9/87/0002/0222

o Roles
of
minutes)

o Review M e t h o d o l o g i e s (20-30 m i n u t e s )

75¢

222

Review

Participants

(20-30

o Planning
minutes)

for

the

Review

Process

types of reviews which should be performed on a
project
are
dependent
upon
the
specific
intermediate deliverables which are produced. The
intermediate deliverables for the team project in
the course should be noted here as well as what
review
processes
are
appropriate
for
the
deliverables.

(15-20

o Review Reports (10-15 minutes)
In the remainder of this section, suggested
content and references for each of these topics
will be presented.

Behavioral Factors
Rationale

This topic should help students realize that
any review process is a human activity and as such
considerable attention must be spent on human
interaction
if the review process is to be
successful. The dynamics of a review group should
be
summarized
with
recognition that review
processes may lead to stress and conflict. Basic
techniques for reducing stress and dealing with
conflicts should be highlighted [Forsyth83].

f o r S o f t w a r e T e c h n i c a l Reviews

The intent of this topic is for the students
to realize that review processes are absolutely
essential to the development and maintenance of
reliable software [Weinberg84].
The complexity
and
error-prone
nature
of
developing
and
maintaining software should be demonstrated with
statistics
depicting
error
frequencies
for
software products.
These statistics should also
convey the message that errors occur throughout
the development process and that the later these
errors are detected the higher the cost for their
repair [Boehm76]. Review processes should then be
presented as a form of testing that is applicable
throughout the life cycle and is effective in
reducing total test time and production failures
[McKissick84].

The discussion of this topic should also
include an examination of review logistics which
contribute
to
a
successful
review.
These
logistics include issues regarding timing of the
review, location of the review, duration of the
review, number of reviewers and their physical
organization [Fagan76, Forsyth83].

Review Methodologies

The
importance of technical reviews for
tracking
a project should also be stressed.
Through
identification
of
well-defined
deliverables
and
successful review of these
deliverables,
progress
on a project can be
followed [Fagan76, McConnel184].

This topic should introduce the students to
three major approaches for performing technical
reviews.
The first approach which should be
overviewed is that of walkthrough [Yourdon78].
Inspections should then be presented as a more
formal approach which require a high degree of
preparation
on
the
part
of
the
review
participants, but whose benefits include a more
systematic review of the product and a more
controlled and less stressed meeting [Ackerman83,
Buck83, Fagan76, Freedman82]. The last approach
to be introduced is that of audits. Audits should
be described as an external type of review process
which
serves to insure that the product is
properly validated [Walker79].
It is important
for
the
students to understand the primary
differences among walkthrough, inspections and
audits
as well as some of their respective
advantages and disadvantages [Fagan76, Freedman82,
Quirk85].

The student should also be made aware of the
fact that review processes can lead to more
maintainable software.
The very nature of a
review process requires the technical aspects of
the product undergoing review to be understandable
to the review participants. To be understandable,
the product must be well documented. One of the
acknowledged benefits of technical reviews is that
they
force developers to produce incremental
documentation necessary for the review which might
not have been produced until the end of the
project when schedule constraints often reduce the
quality of the documentation effort. The review
process
itself also improves the developers'
general understanding of the whole system which
further
facilltiates
error
diagnosis during
maintenance [Hart82]. Although understandability
is not the only attribute of a maintainable
product, the students should be made aware that
review processes at lease contribute in part to a
more maintainable product.

Roles of Review Participants
This topic should convey to the students that
within any review process there are specific well
defined
roles that must be executed by the
participants of a review. These roles will vary
depending upon the specific review methodology
which is being followed. These roles should be
viewed as being functional which implies that it
is possible in some reviews for a participant to
execute more than one role. The intent of this
section is for the students to understand that
when participating in a review in a particular
role there are responsibilities and activities
which must take place prior to a review, during
the
review
and
after
the review.
Several
references exist for defining the various roles of
review participants [Ackerman83, Fagan76, Uart82,

Types o f S o f t w a r e T e c h n i c a l Reviews

The discussion in this topic should convey to
the students that the selection of appropriate
software
technical reviews for a project is
dependent upon the developmental model followed,
the
type of product being produced and the
standards which must be adhered to. The students
should be made aware of the fact that standards
exist which govern review processes for some
organizations [IEEE84, MILS85].
The intent of
this topic is for the students to realize that the

223

Remus83, Peele82, Weinberg84, McKissick84]. At
this level the students should be exposed to the
basic
responsibilities of the review leader,
recorder, reviewer and producer.

Scheduling issues regarding the timing of a
review, should also be summarized to the students.
These include the fact that the scheduling of a
review should ideally take place soon after a
producer has completed the product but before
additional effort is expended. Another problem to
address
concerning
scheduling
involves
the
duration of the review and problems which may
occur if the review is too long. This requires
review processes to be very focused in terms of
their objectives.

The first role which should be described is
that of the review leader. The review leader is
the one individual responsible for the review.
This
role
requires
scheduling
the review,
conducting an orderly review meeting and preparing
the review report. The review leader may also be
responsible for ensuring that action items are
properly followed-up on after the review process.
Review leaders must possess both technical and
personal characteristics. The personal qualities
include leadership ability, mediator skills and
organization talents. The review leader must keep
the review group focused at all times and prevent
the
meeting from becoming a problem solving
session.

Another planning issue to address is the
importance of developing review agendas.
The
students should be shown some sample agendas and
understand that review agendas serve the same role
as meeting agendas.
Review Reports

An o t h er r o l e which must be d e s c r i b e d i s t h a t
of the r e c o r d e r .
The r e c o r d e r r o l e i n a r e v i e w
process guarantees that all information necessary
f o r an a c c u r a t e r e v i e w r e p o r t i s p r e s e r v e d . The
r e c o r d e r must d i g e s t c o m p l i c a t e d d i s c u s s i o n s and
capture
their
e s s e n c e in a c t i o n i t e m s .
The
students
s h o u l d be l e a d to u n d e r s t a n d t h a t the
role of recorder is clearly a technical function
and one t h a t cannot be performed by a s e c r e t a r y .

The i n t e n t o f t h i s t o p i c i s f o r the s t u d e n t s
to s e e and u n d e r s t a n d
the type o f i n f o r m a t i o n
which i s n e c e s s a r y to c a p t u r e from a r e v i e w in the
form o f a r e p o r t .
R e f e r e n c e s c o n t a i n i n g sample
reports
which a r e good c a n d i d a t e s
for student
review
include
[Ackerman83, Buck83, Fagan76,
Weinberg84].
In p a r t i c u l a r , t h e s t u d e n t s should
understand
what a c t i o n i t e m s a r e and how they a r e
recorded.
Additional information often collected
on r e v i e w r e p o r t s
for historical
a n a l y s i s of
projects
and e v a l u a t i o n o f s o f t w a r e development
a p p r o a c h e s s h o u l d a l s o be d e s c r i b e d [Bucke83].

Another r o l e which must be d e s c r i b e d i s t h a t
of
the
reviewer,
the r e v i e w e r r o l e i s
to
objectively
analyze
the
product and to be
accountable for the review.
In an inspection
methodology,
the reviewer must spend considerable
time preparing for the review. Basic guidelines
for reviewers, such as the importance for a
reviewer to keep in mind the fact that a product
is being reviewed and not the producer of the
product, should also be presented.

Review P r o c e s s E x e r c i s e s
In o r d e r f o r t h e s t u d e n t s to comprehend the
many
ideas
presented
in
the l e c t u r e s
and
appreciate
their
i m p o r t a n c e , the s t u d e n t s should
be
asked
to
participate
in some c a r e f u l l y
structured
review
processes.
These
review
p r o c e s s e s can be conducted as e x e r c i s e s e i t h e r
during class
time or more p r a c t i c a l l y
out o f
class.
Each s t u d e n t should have the o p p o r t u n i t y
o f p l a y i n g each o f t h e r e v i e w p a r t i c i p a n t r o l e s
f o r both an i n s p e c t i o n and a w a l k t h r o u g h .
During
each review, some students should also be assigned
the role of observers and report back problems
which they perceived hindered these reviews as
well as actions which enchanced the reviews. The
materials to utilize in these practice reviews
should
be simple and not require a lot of
preparation
or
review
time.
The materials
selected can include source code but should also
include some other intermediate software products
such as a specification.
A good example of a
specification for students to review is a typical
programming assignment utilized in a beginning
programming course.
It is important in these
practice
reviews
to
emphasize
good review
principles to the students and make sure the
students have had time to prepare before attending
the review.

The producer role is the review process
should
also
be outlined.
This role varies
depending upon the review methodology.
In a
walkthrough,
the reviewer may actually lead the
meeting in an organized discussion of the product.
A high degree of preparation and planning is
needed in a walkthrough to present material at the
proper
level
and
pace.
In an inspection
methodology,
the producer must be prepared to
address all points brought up by the review team.
Attitude
is
a l s o important on the part o f
producers and it is essential that defensive
approach is not taken.

Planning for the Review Process
This topic should outline the planning steps
necessary for a successful review. This planning
entails
selecting
participants
and
their
respective
roles,
scheduling the review and
developing a review agenda. The intent of this
section is for the students to understand what
these planning activities are and some simple
guidelines for completing them. The discussion of
selecting
review
participants
should stress
selecting reviewers based on technical knowledge.
Other considerations which contribute to stress,
such as management involvement, should also be
addressed.

A n o t h er i m p o r t a n t s e t of e x e r c i s e s f o r the
student
to p er f o r m i s d e v e l o p i n g c h e c k l i s t s and
agendas
that will
be f o l l o w e d d u r i n g the a c t u a l
r e v i e w p r o c e s s e s which they w i l l perform on t h e i r
project.
These c h e c k l i s t s can be d e v e l o p e d from
scratch
or c o n s i s t
of modified c h e c k l i s t s
or

224

agendas which appear in the literature. It is a
good idea to have the students perform these
exercises well in advance of the real reviews that
they will be conducting on their project.

conducted earlier in the course.

Conclusion
In
this paper a practical approach for
teaching software technical reviews in a onesemester
software
engineering
course
was
presented.
This
approach has been followed
successfully
several times with the students
acquiring
basic
review
skills,
but
more
i m p o r t a n t l y a p o s i t i v e a t t i t u d e toward the r e v i e w
process.
Many of the ideas presented in this
paper can also be included in a testing course or
software quality assurance course. To assist an
instructor in selecting the appropriate material
for incorporating into a course an annotated
bibliography has also been included.

Integrating Revlevs with the Team Project
The
team
project
for the one-semester
software engineering course normally consists of a
moderate size software development effort starting
with generation of the software specifications and
ending with tested product. There are, of course,
many variations for conducting this project which
will influence the types of review processes to
perform.
For the purpose of this paper, the
project will be assumed to follow the normal
development phases ending with testing. Based on
this
assumption,
a reasonable set of review
processes for the teams to follow would consist of
the following:
I. specifications review
2. preliminary design review

References

3. module detailed design, code and test plan
review

[Ackerman 83] Ackerman,
A.
Frank;
Flower,
Priscilla J.; Ebenau, Robert G. Software
Inspections and the Industrial Production of
Software.
In Hans-Ludwig Hausen, Editor,
Software
Validation,
Inspectlon-testlngverification-alternatives:
Proceedings of
the Symposium on Software Validation, Pages
13-40. Darmstadt, FRG, September, 1983.
The
inspection
processes
at
Bell
Laboratories
is
presented.
Sample
reports are included along with estimates
for reviewing lines of code.

4. software integration and test plan review
This list is certainly not exhaustive but
instead attempts to put review processes into
proper scope within the one-semester course. Each
of these reviews should be formally performed with
review participants roles defined, checklists and
agendas followed and review reports generated.
The specifications review should analyze the
specifications document produced by the team. The
entire team should participate in this review
along
with
the
individual representing the
customer to the team.
The preliminary design
review
should
also involve the entire team
inspecting
the
overall
architecture of the
software.
The module detailed design, code and
test
plan
review should involve the module
developer
and a small number of other team
members.
Although it is certainly possible to
review a module's detailed design, code and test
plan separately, in the one-semester course it is
more practical to examine all of these in the
context of a single review.
The last review
should involve the entire team examining the
strategy for integrating the modules together and
testing the software.

[Boehm 76]
Boehm. B.
IEEE Transactions
December 1976.
Provides data
the later
repaired in

W.

Software Engineering.
on Computers:1226-1241,

on increasing error costs
errors are detected and
the software life cycle.

[Buck 83] Buck, Robert D.; Dobbins, James H.
Applications
of
Software
Inspection
Methodology in Design and Code. In HansLudwig Hausen, Editor, Software Validation,
inspectlon-testing-verificatlon-alternatives:
Proceedings of the Symposium on Software
Validation,
Pages 41-63.
Darmstadt, FRG,
September 1983.
Another IBM variation of the inspection
process for design and code is detailed.
Samples
reports
are
included.
A
discussion of how to interpret data as
the result of review processes is also
Included.

It
i s i m p o r t a n t to n o t e t h a t the i n s t r u c t o r
should not observe these review processes.
The
students
should perform t h e s e r e v i e w s as peer
processes designed to identify defects before
submitting
the
work
to the instructor for
evaluation.
If the students perceive that an
instructor attending these review processes is
evaluating their work as presented in the review,
serious negative behavioral factors may occur.
Although it is tempting for an instructor to
attend these reviews to provide the teams with
feedback on their review processes, the potential
negative ramifications of attending these reviews
strongly suggest that such feedback is provided
only during the non-threatening practice reviews

[Fagan 76] Fagan,
M.
E.
Design
and
Code
Inspections
to Reduce Errors in Program
Development.
IBM System Journal, vol. 15(3),
1976.
A must read classic paper which introduces
the
whole
concept
of
software
inspections.
Sample forms, checklists
and experimental data from IBM are also
presented.

225

[McConneil 84] McConnell, Peter R. H.; Strigel,
Wolfgang
B.
Results of modern software
engineering principles applied to small and
large
projects.
In
AFIPS
Conference
Proceedings of the 1984 National Computer
Conference,
Pages 273-281.
Las Vegas, NV,
USA, July, 1984.
The Impact of review processes and their
cost to implement on two medium to large
real-time
software
projects
are
documented.
The utilization of review
processes to track a project is also
described.

[Fish 70] Fisher, A. Decision Emergency: Phases
in
Group
Decision
Making.
Speech
Monographs37, 1970.
This article describes a theory of group
development
in which a group passes
through
four
phases before becoming
effective.
This understanding of small
group interaction is valuable in better
dealing with the problems associated with
review groups.

[ F o r s y t h 83]
Forsyth,
Donelson
R.
An
Introduction
to Group DTnamics. B r o o k s / C o N
P u b l i s h i n g Company, 1983.
T h i s t e x t p r e s e n t s many of the i m p o r t a n t
behavioral factors
t h a t must be d e a l t
with
during
review
processes.
In
particular
group c o n f l i c t , the e f f e c t i v e
environment
on
review processes and
organizational
group
behavior
are
described.

[McKissick 84] McKissick, John Jr.: Somers, Mark
J.;
Marsh,
Wilhelmina.
Software Design
Inspection
for
Preliminary
Design.
In
Proceedln~s COMPSAC '84. The IEEE Computer
Society's
Eighth
International
Computer
Software and Applications Conference, Pages
518-519. November, 1984.
An inspection process at General Electric
company
for
preliminary
designs i s
outlined
including the roles of the
review participants.
The benefits of
this
process,
including
improved
education, are also cited.

[Freedman 82] Freedman,
Daniel
P.; Weinberg,
Gerald
M.
Handbook
of
Walkthroughs,
Inspections,
and
Technical
Reviews:
Evaluating Programs~ Projects and Products.
Little, Brown and Company, 1982.
This
text is written as a series of
questions and answers. It describes the
Fagan methodology and many aspects of
review
processes.
It
provides
a
discussion of how to review many typical
documents.
It is weak in its discussion
of sociological factors, review reports,
planning
issues
and
assessment
of
reviews.

[MILS 85] M i l i t a r y S t a n d a r d f o r T e c h n i c a l Reviews
and
A u d i t s f o r Systems, Equipments, and
Computer
Software
MIL-STD-1521B Edition,
1985.
This standard defines the required reviews
for military contracts. The appendices
c o n t a i n details about exactly what is to
be covered for each of the mandated
reviews as well as the role of the
contractor and the contracting agency.

[ H a r t 82] H a r t , J o l e n e J . Code I n s p e c t i o n s a t
First
Union C o r p o r a t i o n .
I n P r o c e e d i n g s of
COMPSAC ' 8 2 .
IEEE Computer S o c i e t y ' s S i x t h
International
Computer
Software
and
Applications
Conference,
Pages
515-522.
Chicago, IL, USA, November, 1982.
Many benefits of performing design and code
walkthroughs are cited.
Variation of
reviews, including "round robin reviews",
and their relative effectiveness are also
noted.
Actual
sociological
problems
encountered at Sperry are also briefly
mentioned.

[Peele 82] Peele,
Ronald. Code Inspections at
First Union Corporation.
In Proceedings of
COMPSAC '82. IEEE Computer Society's Sixth
International
Computer
Software
and
Applications
Conference,
Pages
445-446.
Chicago,IL, USA, November, 1982.
This paper presents a variation of the
Fagan inspection methodology defining the
process in depth along with the roles of
the review participants.
The benefits of
utilizing
their
process
are
also
documented.

[IEEE 84] IEEE Standard 730-1984 for Software
Assurance Plans 1984.
The IEEE standard for Software Quality
Assurance
puts review processes into
perspective
with the entire software
quality
assurance
process.
Specific
reviews are mandated by this standard.

[ Q u i r k 85] e d i t o r :
Quirk, ~. J . V e r i f i c a t i o n and
V a l i d a t i o n o f Real-Time S o f t w a r e . S p r i n g e r Verlag, 1985.
This
text
concentrates
on
testing
techniques
f o r r e a l - t i m e s o f t w a r e . The
utilization
of r e v i e w p r o c e s s e s i s a l s o
described.
The emphasis of these review
processes Is, however, not unique to
real-time
software
and
very little
insight into reviewing real-time systems
as opposed to other types of systems can
be obtained from this text.

[Janis 72[ Fisher,
I.
Victims of Groupthink.
Houghton Mifflin, 1972.
This
book describes the phenomenon of
groupthink, i t s impact on groups and ways
of dealing with the problem

226

[Remus 83[ Remus,
Eorst
Integrated
Software
Validation
in
the
View
of
I n s p e c t i o n s / R e v i e w s . I n Hans-Ludwig Eausen,
Editor,
Software
Validation, inspectiontesting-verification-alternatives:
P r o c e e d i n g s of the Symposium on Software
Validation,
Pages 57-63. Darmstadt, FRG,
September, 1983.
The r e l a t i o n s h i p of review p r o c e s s e s to
testing
in
an
IBM environment a r e
explored.
A v a r i a t i o n of the r o l e s of
the review p a r t i c i p a n t s i s p r e s e n t e d as
well.
The u t i l i z a t i o n of d e f e c t d a t a to
the d i s c o v e r y of wrong d e s i g n d i r e c t i o n s
and i m p l e m e n t a t i o n s i s a l s o d e s c r i b e d .
[Walker 79]
Walker,
Michael
C.
Auditing
Software Development Projects:
A Control
Mechanism for the Digital Systems Development
Methodology.
In Proceedings, COMPCON Spring,
Pages 310-314. IEEE, 1979.
Software audits and their functions in a
development
organization are defined.
Auditing techniques are presented as well
as experiences from the Computer Science
Corporation.
[Weinberg 84] Weinberg,
Gerald
M.; Freedman,
Daniel
P.
Reviews,
Walkthroughs,
and
Inspections.
IEEE Transactions on Software
Engineering,
vol. SE-10(1): 68-72, January,
1984.
An overview
paper
describing
the
distinction
between
walkthroughs and
inspections.
The
difference between
formal
and informal reviews is also
clarified.
The
paper also contains
sample
review reports and how these
reports can be utilized.
[Yourdon 78]
Yourdon,
E.
Structured
Walkthroughs. Yourdon, Inc., 1978.
A detailed discussion of the walkthrough
process. The benefits of walkthroughs as
well as the mechanics of the process are
presented.
Psychological
issues for
walkthroughs are also noted.

227

Coselection of Features and Instances for Unsupervised Rare
Category Analysis
Jingrui He1,∗ and Jaime Carbonell2
1

Machine Learning Group, IBM T.J. Watson Research Center, 1101 Kitchawan Road, Route 134, Yorktown Heights,
NY, 10598, USA
2 Language

Technologies Institute, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, 15213, USA
Received 3 May 2010; revised 13 July 2010; accepted 2 August 2010
DOI:10.1002/sam.10091
Published online 29 September 2010 in Wiley Online Library (wileyonlinelibrary.com).

Abstract: Rare category analysis is of key importance both in theory and in practice. Previous research work focuses on
supervised rare category analysis, such as rare category detection and rare category classification. In this paper, for the first
time, we address the challenge of unsupervised rare category analysis, including feature selection and rare category selection.
We propose to jointly deal with the two correlated tasks, so that they can benefit from each other. To this end, we design
an optimization framework which is able to coselect the relevant features and the examples from the rare category (a.k.a. the
minority class). It is well justified theoretically. Furthermore, we develop the Partial Augmented Lagrangian Method (PALM) to
solve the optimization problem. Experimental results on both synthetic and real data sets show the effectiveness of the proposed
method.  2010 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 3: 417–430, 2010
Keywords:

rare category; unsupervised; feature selection; optimization; augmented Lagrangian

1. INTRODUCTION
Rare category analysis refers to the problem of detecting
and characterizing the minority classes in an unlabeled
data set. It is of key importance both in theory and in
practice. Take financial fraud detection as an example. Most
financial transactions are legitimate, which constitute the
majority class; whereas the fraudulent transactions of the
same type are similar to each other, and they correspond
to one minority class. For example, small-amount-probing
type of fraudulent transactions is similar to each other in
terms of the small amount stolen in each transaction and
the recurring nature. Detecting and analyzing a new type of
fraudulent transactions help us prevent similar transactions
from happening in the future.
Existing research work on rare category analysis applies
in supervised settings, either having access to a labeling
oracle (rare category detection) or given labeled examples
from all the classes (rare category classification). In this
paper, we focus on unsupervised rare category analysis,
that is, no label information is available in the learning
process, and address the following two problems: (i) rare
category selection, that is, selecting a set of examples that
Correspondence to: Jingrui He (jingrui.he@gmail.com)
 2010 Wiley Periodicals, Inc.

are likely to come from the minority class; (ii) feature
selection, that is, selecting the features that are relevant to
identify the minority class. Take small-amount-probing type
of fraudulent transactions as an example. The percentage of
such transactions is small, and they are similar to each other
in terms of the small amount stolen in each transaction and
the recurring nature. They may not share the same location,
time, etc. Therefore, these features can be seen as being
irrelevant to this class.
The key observation is that the above two tasks are
correlated with each other. On one hand, the analysis of
the minority class examples helps us identify the relevant
features; on the other hand, the identification of the relevant features is crucial to the selection of the minority
class examples. Therefore, we propose to jointly deal with
the two tasks, so that they can benefit from each other.
To this end, we formulate the problem as a well-justified
optimization framework, which coselects the relevant features and the examples from the minority class. Furthermore, we design an effective search procedure based on
augmented Lagrangian method. The basic idea is to alternatively find the relevant features and the minority class
examples. Finally, we demonstrate the performance of the
proposed method by extensive experimental results.

418

Statistical Analysis and Data Mining, Vol. 3 (2010)

The main contributions of this paper can be summarized
as follows:
Problem Definition. We are the first to address the
two important tasks in unsupervised rare category
analysis; and we propose to jointly deal with them;
Problem Formulation. We design an optimization
framework for the coselection of features and
instances, which is well justified theoretically;
Search Procedure. We develop an effective algorithm
to solve the optimization problem which is based on
augmented Lagrangian.
The rest of the paper is organized as follows: in Section 2,
we review related work; then in Section 3, we present
the optimization framework with theoretical justification;
Section 4 introduces the algorithm for solving the optimization problem; experimental results are given in Section 5;
following some discussion in Section 6, finally, we conclude in Section 7.

2. RELATED WORK
In this section, we review related work on supervised
rare category analysis, outlier detection, and unsupervised
feature selection. Supervised rare category analysis can
be further divided into two major groups: rare category
detection and rare category classification.
Rare Category Detection. Here, the goal is to find at least
one example from each minority class with the help of a
labeling oracle, minimizing the number of label requests.
Assuming the relevance of all the features, researchers have
developed several methods for rare category detection. For
example, Pelleg and Moore [1] assumed a mixture model to
fit the data, and experimented with different hint selection
methods, of which interleaving performs the best; Fine
and Mansour [2] studied functions with multiple output
values, and used active sampling to identify an example for
each of the possible output values; He and Carbonell [3]
developed a new method for detecting an instance of each
minority class via an unsupervised local-density-differential
sampling strategy; and Dasgupta and Hsu [4] presented an
active learning scheme that exploits cluster structure in the
data, which was proven to be effective in rare category
detection. Different from these methods, in our paper, no
labeling oracle is available for querying, and the goal is to
select a set of examples that are likely to come from the
minority class. Furthermore, we assume only some of the
features are relevant to the minority classes, and hope to
identify those features.
Rare Category Classification (Imbalanced Classification).
Here, the goal is to construct an accurate classifier for
Statistical Analysis and Data Mining DOI:10.1002/sam

the minority classes given labeled examples from all the
classes. Existing methods can be roughly categorized into
three groups [5], that is, sampling based methods [6–8],
adapting learning algorithms by modifying objective functions or changing decision thresholds [9,10], and ensemble
based methods [11,12]. Furthermore, some researchers have
worked on feature selection for imbalanced data to improve
the performance of the classifier, such as in the work of
Zheng et al. [13]. The major difference between these methods and our method is that we work in an unsupervised
fashion, that is, no labeled data are available.
Outlier Detection. Outlier detection refers to the problem
of finding patterns in data that do not conform to expected
behavior [14]. According to Chandola et al. [14], the majority of outlier detection techniques can be categorized into
classification based [15], nearest neighbor based [16], clustering based [17], information theoretic [18], spectral [19],
and statistical techniques [20]. Compared with our method,
outlier detection finds individual and isolated instances that
differ from a given class and from each other. Typically
these are in low-density regions. This is a very different
process than discovering a new compact class, where we
are looking for a local density spike and the minority class
instances are strongly self-similar. Furthermore, in outlier
detection, the discovery of one outlier cannot help discover other outliers since the outliers are different from
each other; whereas in rare category analysis, the discovery of a minority class instance can help us understand the
properties of this class, and thus help us discover instances
from the same class. For example, in rare disease diagnosis,
by studying a patient with a particular kind of rare disease,
the doctors may gain more insights about this disease, and
design better techniques for diagnosing and treating such
disease.
Unsupervised Feature Selection. Generally speaking,
existing methods can be categorized as wrapper models and
filter models. The wrapper models evaluate feature subsets
based on the clustering results, such as the FSSEM algorithm [21], the mixture-based approach which extends to
the unsupervised context the mutual-information based criterion [22], and the ELSA algorithm [23]. The filter models
are independent of the clustering algorithm, such as the
feature selection algorithm based on maximum information compression index [24], the feature selection method
using distance-based entropy [25], and the feature selection
method based on Laplacian score [26]. Similar to unsupervised feature selection, in our paper, we also assume that
the class labels are unknown. However, in our settings, the
class proportions are extremely skewed, and we are only
interested in the features relevant to the minority classes.
In this case, both wrapper and filter methods select the features primarily relevant to the majority classes. Therefore,
we need new methods that are tailored for our problem.

He and Carbonell: Unsupervised Rare Category Analysis

3. OPTIMIZATION FRAMEWORK
In this paper, we focus on the binary case, that is, one
majority class and one minority class, and our goal is to (i)
select a set of examples that are likely to come from the
minority class, and (ii) identify the features relevant to this
minority class. In this section, we formulate this problem
as an optimization framework, and provide some theoretical
justification.
3.1. Notation
Let D = {x1 , . . . , xn }, xi ∈ Rd denote a set of n unlabeled examples, which come from two classes, that is, the
class labels yi ∈ {1, 2}, i = 1, . . . , n. yi = 1 corresponds
to the majority class with prior 1 − p, and yi = 2 corresponds to the minority class with prior p, p  1. Furthermore, of the d features, only dr features are relevant to
the minority class. In other words, the examples from the
minority class have very similar values on those features,
and their values on the other features may be quite diverse.
For the sake of simplicity, assume that the dr features are
independent to each other. Therefore, the examples from the
minority class are tightly clustered in the dr -dimensional
subspace spanned by the relevant features, which we call
the relevant subspace.
Let Sdr denote the set of all dr -dimensional subspaces of
Rd , and let Smin denote the relevant subspace, Smin ∈ Sdr .
Let f (x) denote the probability density function (pdf)
of the data in Rd , i.e. f (x) = (1 − p)fmaj (x) + pfmin (x),
where fmaj (x) and fmin (x) are the pdfs of the majority
and minority classes in Rd , respectively. Given feature
subspace S ∈ Sdr and x ∈ Rd , let x (S) denote the projec(S) (S)
(S) (S)
(x ) and fmin
(x )
tion of x on S, and f (S) (x (S) ), fmaj
denote the projection of f (x), fmaj (x) and fmin (x) on S
respectively.
To coselect the minority class examples and the relevant
features, we define two vectors: a ∈ Rn and b ∈ Rd . Let
ai and bj denote the ith and j th elements of a and b,
respectively. ai = 1 if xi is from the minority class, and
0 otherwise; bj = 1 if the j th feature is relevant to the
minority class, and 0 otherwise.
3.2. Objective Function
Given the prior p of the minority class and the number of
relevant features dr , we hope to find np data points whose
corresponding ai = 1, and dr features whose corresponding bj = 1. Intuitively, the np points should form a compact
cluster in the relevant subspace, and due to the characteristic
of the minority class, this cluster should be more compact
than any other np data points in any dr -dimensional subspace. To be more strict, we have the following optimization
problem.

419

PROBLEM 1:



n 
d
n


1
ai ak 
bj (xij − xkj )2 
min F (a, b) =
np
i=1 k=1

s.t.

n


ai = np,

ai = 0, 1

bj = dr ,

bj = 0, 1

j =1

i=1
d

j =1

d
2
In the objective function F (a, b),
j =1 bj (xij − xkj )
is the squared distance between xi and xk in the subspace
Sb spanned by the features with nonzero bj . This squared
distance contributes to F (a, b) if and only if both ai and
ak are equal to 1. Given a set of np points, define the set
distance of every data point to be the sum of the squared
distances between this point and all the points within this
set. Therefore, by solving this optimization problem, we
aim to find a set of np points and dr features such that
the average set distance of these points to this set in the
corresponding subspace Sb is the minimum.
Problem 1 can be easily applied to the case where either
a or b is known, and we want to solve for the other vector.
To be specific, if a is known, that is, we know the examples
that belong to the minority class, and we want to find the
dr -dimensional subspace where the minority class can be
best characterized, we can use the same objective function
F (a, b), and solve for b using the minority class examples.
Similarly, if b is known, that is, we know which features
are relevant to the minority class, and we want to find the
examples from the minority class, we can also use F (a, b),
and solve for a in the subspace Sb spanned by the relevant
features.
3.3. Justification
The optimization problem we introduced in the last
subsection is reasonable intuitively. Next, we look at it from
a theoretical perspective.
For any dr -dimensional subspace, ∀S ∈ Sdr , and for any
data point, x ∈ Rd , let function ψ S (x (S) ) denote the average
squared distance between x (S) and its np nearest neighbors.
1 
More precisely, ψ S (x (S) ) = minDnp ⊂D,|Dnp |=np np
y∈Dnp

np
1
(S) − z(i) 2 , where z(i) denotes
x (S) − y (S) 2 = np
x
i=1
x (S)
x (S)
the ith nearest neighbor of x (S) within x1(S), . . . , xn(S) . Furthermore, define function φ S to be the expected average
squared distance between x (S) and its np nearest neighbors. More precisely, φ S (x (S) ) = E(ψ S (x (S) )). Here, the
expectation is with respect to zx(i)(S) , i = 1, . . . , np.
Based on the above definitions, we have the following
theorem.

Statistical Analysis and Data Mining DOI:10.1002/sam

420

Statistical Analysis and Data Mining, Vol. 3 (2010)

THEOREM 1: If

4.

1. In Smin , the support region of the minority class is
within hyperball B of radius r;
2. The support region of f in any dr -dimensional
subspace is bounded, that is, maxS∈Sdr
maxx,y∈Rd ,f (S) (x (S) )>0,f (S) (y (S) )>0 x (S) − y (S)  = α <
+∞;
3. The density of the majority class in hyperball B
(S )
is nonzero, that is, miny∈Rd ,y (Smin ) ∈B (1 − p)fmajmin
 (S ) 
y min = f0 > 0;
4. For a certain data point, if its projection in the
dr -dimensional subspace S is not within hyperball B,
then the function value of φ S evaluated at this point
S (S)
is big enough, that is, minS∈Sdr ,x∈Rd ,x (S) ∈B
/ φ (x ) −
2
4r > β > 0;
5. The number of examples is sufficiently large, that
is, n ≥ max{ 2(V 1f

B 0

8

)2

log 2δ , 4pα2 β 4 log

2Cddr
δ

is the volume of hyperball B, and
choose dr ;

}, where VB

Cddr

equals d

then with probability at least 1 − δ, in the solution to
Problem 1, the subspace Sb spanned by the features with
bj = 1 is the relevant subspace Smin , and the data points
with ai = 1 are within hyperball B.
PROOF: Please refer to the appendix.



The conditions of Theorem 1 are straight-forward except
conditions (3) and (4). The purpose of condition (3) is
to limit our attention to the problems where the support
regions of the majority and the minority classes overlap.
According to condition (4), for any dr -dimensional subspace, ∀S ∈ Sdr , if the projection of x on S and the projection of y on the relevant subspace Smin are not within
/ B and y (Smin ) ∈ B, φ S (x (S) ) is
hyperball B, that is, x (S) ∈
S
(S
)
min
min
) by at least β when there are at
bigger than φ (y
least np points within hyperball B in the relevant subspace
Smin . Therefore, this condition can be roughly interpreted
as follows. The density around x (S) is smaller than the density around y (Smin ) such that the expected average squared
distance between x (S) and its np nearest neighbors is much
larger than that between y (Smin ) and its np neighbors. In this
way, assuming the other conditions in Theorem 1 are also
satisfied, with high probability, we can identify the relevant
subspace and pick the examples within B according to a.
It should be pointed out that if we want to select np points
from the minority class, picking them from hyperball B is
the best we can hope for. In this way, each selected example
has a certain probability of coming from the minority class.
On the other hand, if some selected points are outside B,
their probability of coming from the minority class is 0.
Statistical Analysis and Data Mining DOI:10.1002/sam

PARTIAL AUGMENTED LAGRANGIAN
METHOD

In this section, we introduce the Partial Augmented
Lagrangian Method (PALM) to effectively solve Problem
1. In our method, we alternate the optimization of a and b,
that is, given the current estimate of a, we solve for b that
leads to the minimum value of F (a, b); given the current
estimate of b, we solve for a that decreases the value of
F (a, b) as much as possible.
be 
specific,
F (a, b) can be rewritten as F (a, b) =
To
d
n n
1
2
b
j =1 j
i=1
k=1 np ai ak (xij − xkj ) . Therefore, given
a, we can solve for b asfollows.
For each feature j , caln n
1
2
culate its score sja = np
i=1
k=1 ai ak (xij − xkj ) . Then
find the dr features with the smallest scores, and set their
corresponding bj = 1. It is easy to show that this vector
b minimizes F (a, b) given a. On the other hand, given b,
solving for a is not straight-forward, since F (a, b) is not
a convex function of a. Therefore, this problem cannot be
solved by general binary integer programming (BIP) algorithms. Even though BIP algorithms can be combined with
heuristics, the performance largely depends on the heuristics employed. In this paper, we first relax the constraints
on a: instead of requiring that ai be binary, we require
that ai ∈ [0, 1], that is, we solve the following optimization
problem of a:
PROBLEM 2:


n
d
n

1 
ai ak 
bj (xij − xkj )2 
min Fb (a) =
np
i=1 k=1

s.t.

n


ai = np,

j =1

ai ∈ [0, 1]

i=1

Next we use augmented Lagrangian method [27] to
solve Problem 2 in an iterative way. The reason for using
augmented Lagrangian method is the following: it is a
combination of Lagrangian and quadratic penalty methods;
compared with the Lagrangian method, the addition of the
penalty terms to the Lagrangian function does not alter the
stationary point of the Lagrangian function, and can help
damp oscillations and improve convergence. Furthermore,
the penalty parameter does not have to go to infinity in
order to get the optimal solution [28]. Here, we define the
following augmented Lagrangian function


n
d
n

1 
ai ak 
bj (xij − xkj )2 
LA (a, λ, σ ) =
np
i=1 k=1

−

2n+1

i=1

λi di (a) +

j =1

2n+1
σ  2
di (a)
2
i=1

(1)

He and Carbonell: Unsupervised Rare Category Analysis

where λi , i = 1, . . . , 2n + 1 are the Lagrange multipliers,
σ is a positive parameter, and di (a), i = 1, . . . , 2n + 1 are
a set of functions defined as follows:
	
c (a) if i ≤ 1 or ci (a) ≤ λσi
di (a) = iλi
otherwise
σ
c1 (a) =

n


ai − np = 0

i=1

cj +1 (a) = aj ≥ 0, 1 ≤ j ≤ n
ck+n+1 (a) = 1 − ak ≥ 0, 1 ≤ k ≤ n
Here ci (a), i = 1, . . . , 2n + 1, denote the original constraints on a, both equality and inequality, and di (a) are
truncated versions of ci (a), that is, di (a) is equal to ci (a)
if and only if the corresponding constraint is active or nearactive; it is fixed at λσi otherwise.
We minimize LA (a, λ, σ ) based on Algorithm 4.20 in the
work of Madsen et al. [28]. Putting together the optimization of a and b, we have the Partial Augmented Lagrangian
Method, which is presented in Algorithm 1.
The algorithm works as follows. Given the initial values
λ0 and σ0 of λ and σ , and the maximum number of iteration
steps stepmax , it will output vectors a and b that correspond
to a local minimum of F (a, b). In Step 1, we initialize a and
b. Next, in Step 2, we assign λ and σ to their initial values,
and calculate Kprev , which is the maximum absolute value
of all the di (a) functions, i = 1, . . . , 2n + 1. Then Steps
4–16 are repeated stepmax times. In Step 4, we minimize
the augmented Lagrangian function with respect to a, given
the current estimates of λ, σ , and b. To be specific, we
use gradient descent to update a, and gradually decrease
the step size until convergence. Once we have obtained an
updated estimate of a, calculate K, which is the maximum
absolute value of the current di (a) functions. If the value of
K is less than a half of Kprev , then we update the Lagrange
multipliers using the formula in Step 7, which is called the
steepest ascent formula in the work of Madsen et al. [28].
Furthermore, we update Kprev using the smaller value of
K and Kprev . Otherwise, if the value K is bigger than a
half of Kprev , we double the value of σ . Next, we update
the value of b based on the current estimate of a. To be
specific, for each feature, we calculate its score based on
the formula in Step 14. Then in Step 16, we pick dr features
with the smallest scores, and set the corresponding bj to 1,
which minimizes F (a, b) given a. In our experiments, the
algorithm always converges around 20 iteration steps, so
we set stepmax = 30.
The computational complexity of PALM can be analyzed
as follows. The complexity of Steps 1 and 2 is O(n). Inside
the outer loop, the complexity of Step 4 is O(n2 dr ), the
complexity of the first inner loop is O(n), the complexity

421

Algorithm 1 Partial Augmented Lagrangian Method
(PALM)
Require: Initial values of λ and σ : λ0 and σ0 , stepmax
Ensure: a and b
1: Initialize a and b
2: λ = λ0 , σ = σ0 , Kprev = d(a)∞
3: for step = 1 to stepmax do
4:
a := arg mina LA (a, λ, σ ), K := d(a)∞
K
5:
if K ≤ prev
then
2
6:
for i = 1 to 2n + 1 do
7:
λi := λi − σ di (a)
8:
end for
9:
Kprev := min(K, Kprev )
10:
else
11:
σ := 2 × σ
12:
end if
13:
for j = 1 to d do
14:
Calculate the score for the j th feature sja =
1 n n
2
i=1
k=1 ai ak (xij − xkj )
np
15:
end for
16:
Pick dr features with the smallest scores, and set their
corresponding bj to 1
17: end for

of the second inner loop is O(n2 d), and the complexity of
Step 16 is O(dr d). Putting everything together, the overall
complexity of PALM is O(stepmax · max(n2 d, dr d)).
Notice that the vectors a and b generated by PALM
correspond to a local minimum of F (a, b). To improve
its performance, we can run PALM with different initializations of a and b in Step 1 of Algorithm 1, and pick
the best values of a and b that correspond to the smallest
F (a, b).
The vectors a and b can be interpreted as follows.
For b, its dr nonzero elements correspond to the relevant
features. For a, ideally the minority class examples should
correspond to ai = 1. However, this may not be the case in
practice. Therefore, we rank the elements of a from large to
small, and hope to find all the minority class examples from
the top of the ranked list. In other words, the elements of a
that correspond to the top np examples of the ranked list are
converted to 1; whereas the elements of a that correspond
to the remaining examples are converted to 0.

5.

EXPERIMENTAL RESULTS

In this section, we demonstrate the performance of PALM
from the following perspectives: (i) the quality of rare category selection; (ii) the quality of feature selection; (iii)
the benefit of coselecting features and instances simultaneously. In addition, we also want to (i) test the sensitivity of
the proposed PALM to small perturbations in p and dr ; and
(ii) compare the performance of PALM with binary integer
programming (BIP).
Statistical Analysis and Data Mining DOI:10.1002/sam

422

Statistical Analysis and Data Mining, Vol. 3 (2010)

In our experiments, we retrieve the minority class
examples from the ranked list generated by different
methods, and use the following performance measures:
(i) the precision-scope curve, that is, the percentage of the
minority class examples when a certain number of examples
are retrieved, such as 10% × np, . . . , 100% × np; (ii) the
recall-scope curve, that is, the percentage of the minority class examples when a certain number of MINORITY class examples are retrieved, such as 10% × np, . . . ,
100% × np.

variance method (selecting the features with the largest
variance), CRO [29], and random sampling. The x-axis is
the proportion of irrelevant features, and the y-axis is the
precision of the selected features. From these results, we can
see that PALM is much better than the other four methods
especially when the prior p is small. As for Laplacian score
method, although it is comparable with PALM for large p,
its performance quickly deteriorates as p decreases (e.g.
panels (a) and (b) of Fig. 2), which is the case we are
interested in for rare category analysis.

5.1. Synthetic Data Sets

5.2. Real Data Sets

An illustrative example. To demonstrate the performance
of PALM, we first use a simple synthetic data set shown
in Fig. 1. In this figure, there are 1000 examples from the
majority class, denoted as black dots, which are uniformly
distributed in the feature space, and only 10 examples from
the minority class, denoted as red circles, whose features on
Z are uniformly distributed. Of the three features, only two
features (X and Y ) are relevant to the minority class, that
is, the minority class examples have very similar values
on these features; and one feature (Z) is irrelevant to
the minority class, that is, the minority class examples
spread out on this feature. Using PALM, given the number
of minority class examples and the number of relevant
features, we are able to identify the relevant features, with
the corresponding bj = 1. Of the 10 examples with the
largest ai values, nine examples are from the minority
class, and the remaining minority class example has the
11th largest ai value.
Accuracy of feature selection. Next we test the precision
of the selected features of PALM using synthetic data
sets with different prior p. Figure 2 shows the comparison
results of PALM with Laplacian score method [26], feature

Methods for comparison and data sets. In this subsection, we test the performance of PALM on rare category
selection. To the best of our knowledge, there are no existing methods for this task. For example, existing clustering
algorithms (such as K-means) mainly work in the balanced
settings, that is, the proportions of different classes do not
vary a lot; and they assume that all the features are relevant
to each class. Therefore, we have designed the following
methods for the sake of comparison.

2. NNDB-based: calculate the score of each example
using NNDB [3]. Note that no feedback from the
labeling oracle is available, so the scores are not
updated.
3. Interleave-based: calculate the score of each example
using the Interleave principle [1]. Similar as the
NNDB-based method, the scores of the examples are
not updated in this method.
4. PALM-full: assume that all the features are relevant
to the minority class, i.e. bj = 1, j = 1, . . . , d, and
run PALM with dr = d.

0.5

Z

1. Random: randomly rank all the examples, and select
the first np points from the ranked list as the minority
class examples.

0

−0.5
0.5

0
0.5

Y

−0.5

0
−0.5

X

Fig. 1 An illustrative example (best viewed in color).
Statistical Analysis and Data Mining DOI:10.1002/sam

Note that NNDB-based method and Interleave-based
method are both derived from rare category detection methods. For PALM, we tune the number of relevant features
dr without any label information.
Here we use four real data sets, which are summarized
in Table 1. In this paper, we focus on binary problems,
that is, there is only one majority class and one minority
class in the data set. Therefore, for each data set, we
construct several subproblems as follows. We combine the
examples from two different classes into a smaller binary
data set, using the larger class as the majority class, the
smaller class as the minority class, and test the different
methods on these binary subproblems. For each data set, we
present the results on two binary subproblems. On the other

He and Carbonell: Unsupervised Rare Category Analysis

(a)

1

0.6
0.4
0.2
0

PALM
Laplacian
Variance
CRO
Random

0.8
Precision

0.8
Precision

(b)

PALM
Laplacian
Variance
CRO
Random

1

0.6
0.4
0.2

0

0.2
0.4
0.6
0.8
Proportion of Irrelevant Features

0

1

0

0.2
0.4
0.6
0.8
Proportion of Irrelevant Features

p = 0.01
(d)
1

1

0.8

0.8
Precision

Precision

1

p = 0.015

(c)

0.6
0.4
PALM
Laplacian
Variance
CRO
Random

0.2
0

0

0.2
0.4
0.6
0.8
Proportion of Irrelevant Features
p = 0.02

0.6
0.4
PALM
Laplacian
Variance
CRO
Random

0.2
0

1

(e)

0

0.2
0.4
0.6
0.8
Proportion of Irrelevant Features
p = 0.05

1

(f)
1

1

0.8

0.8
Precision

Precision

423

0.6
0.4
PALM
Laplacian
Variance
CRO
Random

0.2
0

0

0.2
0.4
0.6
0.8
Proportion of Irrelevant Features
p = 0.1

0.6
0.4
PALM
Laplacian
Variance
CRO
Random

0.2

1

0

0

0.2
0.4
0.6
0.8
Proportion of Irrelevant Features
p = 0.2

1

Fig. 2 Precision of selected features on synthetic data.

subproblems, similar results are observed and therefore
omitted for brevity.
Accuracy of rare category selection. Figs. 3–10 compare
the performance of different methods on the four real data
sets. In these figures, subfigure (a) shows precision versus
scope, and subfigure (b) shows recall versus scope. On all
the data sets, PALM performs the best: the precision and

recall sometimes reach 100%, such as in Figs. 8 and 9. As
for the other methods (Interleave-based, NNDB-based, and
PALM-full), their performance depends on different data
sets, and none of them is consistently better than Random.
Comparing with Random, Interleave-based, and NNDBbased, we can see that PALM does a better job at selecting
the minority class examples; comparing with PALM-full,
Statistical Analysis and Data Mining DOI:10.1002/sam

Statistical Analysis and Data Mining, Vol. 3 (2010)
Table 1.

Properties of the data sets [30] used.

Data set

n

d

Largest
class (%)

Ecoli
Glass
Abalone
Yeast

336
214
4177
1484

7
9
7
8

42.56
35.51
16.50
31.20

2.68
4.21
0.34
1.68

0.8

0.8
0.7
0.6

0.4
0.3
0.2

0.6

1

0.4

0.9

0.3

0.8

20

40

60

80

100

40

60
Scope

80

100

60
Scope

80

100

PALM
PALM-full
Random
NNDB-based
Interleave-based

0.7
0.6
0.5

1

0.8

20

(b) 1.1

0.5

0.9

PALM
PALM-full
Random
NNDB-based
Interleave-based

0.5

PALM
PALM-full
Random
NNDB-based
Interleave-based

Scope
PALM
PALM-full
Random
NNDB-based
Interleave-based

0.4
0.3
0.2

0.7
Recall

0.9

0.7

0.2

(b)

1

1
0.9

Precision

Smallest
class (%)

Recall

(a)

(a) 1.1

Precision

424

20

40

Fig. 4 Abalone data set: class 2 versus class 7, p = 0.381, four
features selected by PALM.

0.6
0.5
0.4
0.3
0.2

20

40

60

80

100

Scope

Fig. 3 Abalone data set: class 1 versus class 7, p = 0.362, four
features selected by PALM.

we can see that the features selected by PALM indeed help
improve the performance of rare category selection.
Notice that in some figures (subfigure (b) of Figs. 3–5,
7, and 8), at the end of the recall curves, the different
methods seem to overlap with each other. This is because
with no supervision, it is sometimes difficult to retrieve all
the examples from the minority class, and the last example
Statistical Analysis and Data Mining DOI:10.1002/sam

from the minority class tends to appear toward the end of
the ranked list. Therefore, the recall value at 100%np is
often close to the prior of the minority class in the data set.
Comparison with BIP. Next, in Figs. 11 and 12, we
compare the performance of PALM and Binary, where the
vector a is obtained by a BIP algorithm combined with
heuristics on Abalone data set. To be specific, in Binary,
we randomly initialize a binary vector a which satisfies all
the constraints in Problem 1. Then we pick each pair of
elements in a with different values, and swap their values
if this leads to a smaller value of the objective function.1
The vector b is obtained in the same way as PALM. From
these figures, we can see that the performance of Binary is
consistently worse than PALM in terms of both precision
1
We tested different heuristics, and only the best performance
is reported here.

He and Carbonell: Unsupervised Rare Category Analysis

(a)

(b)

PALM
PALM-full
Random
NNDB-based
Interleave-based

0.9
0.8
0.7

425

1
0.9
0.8
0.7

PALM
PALM-full
Random
NNDB-based
Interleave-based

0.6

0.5

Recall

Precision

0.6

0.4

0.5

0.3

0.4

0.2

0.3

0.1

0.2

0

0.1

−0.1

20

40

60

80

0

100

Scope

20

40

60

80

100

Scope

Fig. 5 Ecoli data set: class 1 versus class 4, p = 0.197, three features selected by PALM.
(a)

PALM
PALM-full
Random
NNDB-based
Interleave-based

1

1

PALM
PALM-full
Random
NNDB-based
Interleave-based

0.9
0.8
0.7
0.6

0.6

Recall

Precision

0.8

(b)

0.5
0.4

0.4

0.3
0.2

0.2

0.1
0

20

40

60
Scope

80

0

100

20

40

60
Scope

80

100

Fig. 6 Ecoli data set: class 2 versus class 4, p = 0.313, four features selected by PALM.
(a)
1

PALM
PALM-full
Random
NNDB-based
Interleave-based

1
0.8

Recall

0.8
Precision

(b)

PALM
PALM-full
Random
NNDB-based
Interleave-based

0.6

0.6
0.4

0.4
0.2
0.2
0
0

20

40

60

80

100

20

Scope

40

60

80

100

Scope

Fig. 7 Glass data set: class 1 versus class 3, p = 0.195, two features selected by PALM.
Statistical Analysis and Data Mining DOI:10.1002/sam

426

Statistical Analysis and Data Mining, Vol. 3 (2010)

(a)

PALM
PALM-full
Random
NNDB-based
Interleave-based

1

1
PALM
PALM-full
Random
NNDB-based
Interleave-based

0.9
0.8
0.7
0.6

0.6

Recall

Precision

0.8

(b)

0.4

0.5
0.4
0.3

0.2
0.2
0.1

0
20

40

60
Scope

80

0

100

20

40

60
Scope

80

100

60
Scope

80

100

Fig. 8 Glass data set: class 2 versus class 3, p = 0.183, three features selected by PALM.
1

(b)

0.9

0.8

0.8

0.7
Precision

1

0.9

PALM
PALM-full
Random
NNDB-based
Interleave-based

0.6
0.5
0.4

0.7
0.6
Recall

(a)

0.5
0.4

0.3

0.3

0.2

0.2

0.1

0.1

0

20

40

60
Scope

80

PALM
PALM-full
Random
NNDB-based
Interleave-based

0

100

20

40

Fig. 9 Yeast data set: class 2 versus class 6, p = 0.093, two features selected by PALM.
(a)

1

(b)
PALM
PALM-full
Random
NNDB-based
Interleave-based

0.9
0.8

0.8
0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0

PALM
PALM-full
Random
NNDB-based
Interleave-based

0.9

Recall

Precision

0.7

1

20

40

60
Scope

80

100

0

20

Fig. 10 Yeast data set: class 2 versus class 9, p = 0.055, three features selected by PALM.
Statistical Analysis and Data Mining DOI:10.1002/sam

40

60
Scope

80

100

He and Carbonell: Unsupervised Rare Category Analysis

PALM
Binary

1

(b)

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60
Scope

80

PALM
Binary

1

0.8
Recall

Precision

(a)

427

100

20

40

60
Scope

80

100

Fig. 11 Abalone data set: class 1 versus class 7, p = 0.362.
PALM
Binary

1

(b)

1

0.8

0.8

0.6

0.6

Recall

Precision

(a)

0.4

0.4

0.2

0.2

0

0
20

40

60
Scope

80

PALM
Binary

100

20

40

60
Scope

80

100

Fig. 12 Abalone data set: class 2 versus class 7, p = 0.381.
(a)

1

(b)

1
PALM+5%

PALM+5%

PALM

PALM
0.8

PALM 5%

0.6

PALM 5%

0.6
Recall

Precision

0.8

0.4

0.4

0.2

0.2

0
Abalone(2 vs.7) Ecoli(2 vs.4) Glass(1 vs.3) Yeast(2 vs. 6)

0
Abalone(2 vs. 7) Ecoli(2 vs. 4) Glass(1 vs. 3) Yeast(2 vs.6)

Fig. 13 Perturbations on the prior of the minority class (best viewed in color).

and recall, showing the effectiveness of PALM in obtaining
the vector a.
Sensitivity of PALM. Finally, we test the performance of
PALM when there are small perturbations in the prior of
the minority class and the number of relevant features.
To this end, we first run PALM with p increased by

5% (PALM + 5%) and decreased by 5% (PALM−5%)
respectively, and compare their performance with PALM
in Fig. 13. From this figure, we can see that PALM is quite
robust against small perturbations in p. Then we run PALM
with dr increased by 1 (PALM + 1) and decreased by 1
(PALM − 1) respectively, and compare their performance
Statistical Analysis and Data Mining DOI:10.1002/sam

428

(a)

Statistical Analysis and Data Mining, Vol. 3 (2010)

1

(b)

0.9
0.8

PALM
0.8

PALM+1
PALM- full

0.6

0.6

Recall

Precision

0.7

1

PALM–1

PALM1
PALM
PALM+1
PALM-full

0.5

0.4

0.4
0.3

0.2

0.2
0.1
0

0
Abalone(2 vs.7)Ecoli(2 vs.4) Glass(1 vs.3) Yeast(2 vs. 6)

Abalone(2 vs.7)Ecoli(2 vs. 4) Glass(1 vs. 3) Yeast(2 vs.6)

Fig. 14 Perturbations on the number of relevant features (best viewed in color).

with PALM and PALM-full in Fig. 14. From this figure, we
can see that PALM is also robust against small perturbations
in dr in most cases (Abalone, Ecoli, and Glass), and in
all the cases, the performance of PALM + 1 and PALM
− 1 is better than PALM-full (i.e., PALM without feature
selection).

6.

DISCUSSION

In this section, we discuss about the extension of our
work in the following two aspects: (i) multiple minority
classes; (ii) no prior information.
6.1. Multiple Minority Classes
The current version of PALM can only be applied to the
binary settings, that is, there is one majority class and one
minority class in the data set. PALM can be generalized
to multiple minority classes in the data set by targeting
the minority classes one at a time. To be specific, we may
repeatedly form Problem 1 for each minority class in order
to find the examples from that class as well as the relevant
features. Notice that examples identified as coming from a
certain minority class will be removed from the data set.
Therefore, the key problem here is how to design a good
ordering for the minority classes to be processed by PALM.
The simplest answer is to randomly permute the minority
classes. However, if minority class A has more examples
and more relevant features than minority class B, and we
happen to process class B first, we may accidentally select
some examples and relevant features from class A and
assign them to class B. Since these examples and relevant
features will be removed from the data set, when we process
class A, we will not be able to correctly identify the
Statistical Analysis and Data Mining DOI:10.1002/sam

examples and relevant features from this class. Therefore,
we propose to order the minority classes according to their
priors, and run PALM from the largest minority class to the
smallest one.
On the order hand, in many real applications, we may
have multiple majority classes. However, since we are not
interested in modeling the majority classes, as long as they
satisfy the conditions in Theorem 1, we can still use PALM
to coselect the relevant features and examples from the
minority classes.
6.2. No Prior Information
As shown in Section 5, PALM is robust to small perturbations in the prior of the minority class and the number of
relevant features. In some applications, we may not have
even a rough estimate of such information. One possible
solution is to evaluate different values of p and dr based
on the objective function. However, notice that the objective function in Problem 1 favors smaller values of p and
dr . In other words, smaller values of p and dr will produce
smaller values of the objective function. To address this
problem, we may plot the value of the objective function
versus p and dr , and use the elbow point to find a good
estimate for p and dr .

7. CONCLUSION
In this paper, we address the problem of unsupervised
rare category analysis. To be specific, our goal is to coselect
the relevant features and the examples from the minority
class. To this end, we proposed an optimization framework,
which is well justified theoretically. To solve this optimization problem, we designed the PALM algorithm, which

He and Carbonell: Unsupervised Rare Category Analysis

alternatively finds the relevant features and the minority
class examples. The effectiveness of PALM is demonstrated
by extensive experimental results. Future research work
includes: (i) extending the optimization framework to multiple classes, which may be addressed by running PALM
with respect to the prior of each minority class, from large
to small; (ii) generalizing PALM to the cases where the
prior information (i.e., the prior of the minority class p
and the number of relevant features dr ) is not available,
which may be addressed by introducing objective functions
to evaluate different values of p and dr .
APPENDIX
Proof of Theorem 3.1
The basic idea of the proof is to show that if the selected feature
subspace Sb is NOT Smin , or at least one point in the set of np points
is outside B in Smin , we can always use Smin , and find another set of np
points such that all the points are within B, and its objective function
is smaller than the original set. To do this, first, notice that according
to condition (3), the expected proportion of data points falling inside
n
B, E( nB ) ≥ p + VB f0 , where nB denotes the number of points within
B. Second, according to condition (2), ∀x ∈ D, Pr[0 ≤ x (S) − z(i)(s) 2 ≤
x
α 2 ] = 1, i = 1, . . . , np. Therefore,
Pr


n

B

n


<p

log 2δ ,

α8
4p2 β 4

log δd }, then with probability at least 1 − δ, there are at least np points
within hyperball B, and ∀x ∈ D, ∀S ∈ Sdr , ψ S (x (S) ) ≥ φ S (x (S) ) − β.
/ B,
Furthermore, according to condition (4), ∀x ∈ D, ∀S ∈ Sdr , if x (S) ∈
ψ S (x (S) ) > 4r 2 .

(S )
Notice that ∀a, ∀b, F (a, b) ≥ i:a =1 ψ Sb (xi b ). On the other hand,
i
if Sb = Smin , and the points with ai = 1 are within B in Smin , then
F (a, b) < 4npr 2 . This is because the squared distance between any two
points within B in Smin is no bigger than 4r 2 .
Given a and b, if Sb is not Smin , we can design a 
 and b
 in such
a way that Sb
 is Smin , and the points that correspond to ai
 = 1 are
within B in Smin . We can always find such a vector a 
 since we have
shown that there are at least np points within B. Therefore, F (a, b) ≥

Sb (Sb ) ) > 4npr 2 > F (a 
 , b
 ). On the other hand, if S is S
b
min ,
i:ai =1 ψ (xi
but at least one point with ai = 1 is outside B, we can design a 
 and
b
 in such a way that b
 = b, and a 
 replaces the points with ai = 1
that are outside B with some points within B that are different from
existing points in a. For the sake of simplicity, assume that only xt is out
(Smin )
(S
)
1 
side B. Therefore, F (a, b) = np
− xk min 2 +
i=t
k=t ai ak xi

(Smin )
(Smin ) 2
(Smin )
(Smin ) 2
2 n
1 
− xt
 ≥ np i=t k=t ai ak xi
− xk

i=1 ai xi
np

(S
)
(Smin )
(Smin ) 2
1 
2 ≥
+ 2ψ Smin (xt min ) > np
a
a

x
−
x

+
8r
i=t
k=t i k
i
k
F (a 
 , b
 ). The above derivation can be easily generalized to the case where
more than one point with ai = 1 are outside B. Therefore, in the solution
to Problem 1, Sb is the relevant subspace Smin , and the data points with

ai = 1 are within B.

REFERENCES

+ Pr[∃x ∈ D, ∃S ∈ Sdr , s.t. ψ S (x (S) ) < φ S (x (S) ) − β]

n 

n
B
B
< −VB f0
−E
≤ Pr
n
n
+ nCddr Pr[ψ S (x (S) ) < φ S (x (S) ) − β]


n
n 
B
B
< −VB f0
≤ Pr
−E
n
n
+ nCddr ·


 







(np+1)
(np+1)
Pr ψ S x (S) < φ S x (S) − β|z (S)
dPr z (S)
(np+1)
x

z
x (S)

x

≤ exp(−2n(VB f0 )2 )






2npβ 2
(np+1)
dPr
z
+ nCddr (np+1) exp −
(S)
x
α4
z

(a)

x (S)



2npβ 2
≤ exp(−2n(VB f0 )2 ) + nCddr exp −
4
α
where Cddr is an upper bound on the number of subspaces in Sdr , and
Inequality (a) is based on Hoeffding’s inequality and condition (2).2
2

δ
Let exp(−2n(VB f0 )2 ) ≤ 2δ , and nCddr exp(− 2npβ
4 ) ≤ 2 , we get n ≥

2

d
2C r

1
2(VB f0 )2

or ∃x ∈ D, ∃S ∈ Sdr ,

s.t. ψ S (x (S) ) < φ S (x (S) ) − β]

n

B
≤ Pr
<p
n

1
2(VB f0 )2

examples n is sufficiently large, that is, n ≥ max{

429

log 2δ , and n ≥

α8
4p2 β 4

(np+1)

log

d
2C r
d
δ

α

. In other words, if the number of

Note that given zx (S) , ψ S (x (S) ) can be seen as the average
of np independent items.

[1] D. Pelleg and A. W. Moore, Active learning for anomaly
and rare-category detection, NIPS, Cambridge, MA, USA,
MIT Press, 2004, 1073–1080.
[2] S. Fine and Y. Mansour, Active sampling for multiple output
identification, In COLT, 2006.
[3] J. He and J. Carbonell, Nearest-neighbor-based active
learning for rare category detection. NIPS, Cambridge, MA,
USA, MIT Press, 2007, 633–640.
[4] S. Dasgupta and D. Hsu, Hierarchical sampling for active
learning, In ICML, 2008, 208–215.
[5] N. Chawla, Mining when classes are imbalanced, rare events
matter more, and errors have costs attached, In SDM, 2009.
[6] C. X. Ling and C. Li, Data mining for direct marketing:
problems and solutions, In KDD, 1998.
[7] M. Kubat and S. Matwin, Addressing the curse of
imbalanced training sets: one-sided selection, In ICML,
1997, 179–186.
[8] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W.
P. Kegelmeyer, Smote: synthetic minority over-sampling
technique, Journal of Artificial Intelligence Research (JAIR)
16 (2002), 321–357.
[9] G. Wu and E. Y. Chang, Adaptive feature-space conformal
transformation for imbalanced-data learning, In ICML, 2003,
816–823.
[10] K. Huang, H. Yang, I. King, and M. R. Lyu, Learning
classifiers from imbalanced data based on biased minimax
probability machine, In CVPR (2), 2004, 558–563.
[11] Y. Sun, M. S. Kamel, and Y. Wang, Boosting for learning
multiple classes with imbalanced class distribution, In
ICDM, 2006, 592–602.
[12] N. V. Chawla, A. Lazarevic, L. O. Hall, and K. W. Bowyer,
Smoteboost: improving prediction of the minority class in
boosting, In PKDD, 2003, 107–119.
Statistical Analysis and Data Mining DOI:10.1002/sam

430

Statistical Analysis and Data Mining, Vol. 3 (2010)

[13] Z. Zheng, X. Wu, and R. K. Srihari, Feature selection for text
categorization on imbalanced data, SIGKDD Explorations
6(1) (2004), 80–89.
[14] V. Chandola, A. Banerjee, and V. Kumar, Anomaly
detection: a survey, ACM Computing Surveys, 2009.
[15] D. Barbará, N. Wu, and S. Jajodia, Detecting novel network
intrusions using bayes estimators, In Proceedings of the First
SIAM Conference on Data Mining, April 2001.
[16] S. Ramaswamy, R. Rastogi, and K. Shim, Efficient
algorithms for mining outliers from large data sets, In
SIGMOD, W. Chen, J. F. Naughton, and P. A. Bernstein,
eds. New York, NY, USA, ACM, 2000, 427–438.
[17] D. Yu, G. Sheikholeslami, and A. Zhang, Findout: finding
outliers in very large datasets. Knowledge and Information
Systems 4(4) (2002), 387–412.
[18] Z. He, X. Xu, and S. Deng, An optimization model for outlier
detection in categorical data, In CoRR, abs/cs/0503081,
2005.
[19] H. Dutta, C. Giannella, K. D. Borne, and H. Kargupta,
Distributed top-k outlier detection from astronomy catalogs
using the demac system, In SDM, 2007.
[20] C. C. Aggarwal and P. S. Yu, Outlier detection for high
dimensional data, In SIGMOD, 2001, 37–46.
[21] J. G. Dy and C. E. Brodley, Feature subset selection and
order identification for unsupervised learning, In ICML,
2000, 247–254.

Statistical Analysis and Data Mining DOI:10.1002/sam

[22] M. H. C. Law, A. K. Jain, and M. A. T. Figueiredo,
Feature selection in mixture-based clustering, In NIPS, 2002
625–632.
[23] Y. Kim, W. N. Street, and F. Menczer, Feature selection
in unsupervised learning via evolutionary search, In KDD,
2000. 365–369.
[24] P. Mitra, C. A. Murthy, and S. K. Pal, Unsupervised
feature selection using feature similarity, IEEE Transactions
on Pattern Analysis Machine Intelligence 24(3) (2002),
301–312.
[25] M. Dash, K. Choi, P. Scheuermann, and H. Liu, Feature
selection for clustering—a filter solution, In ICDM, 2002,
115–122.
[26] X. He, D. Cai, and P. Niyogi, Laplacian score for feature
selection, In NIPS, 2005.
[27] J. Nocedal and S. J. Wright, Numerical Optimization, New
York, NY, USA, Springer, 1999.
[28] K. Madsen, H. B. Nielsen, and O. Tingleff, Optimization
with Constraints, (2nd ed.), 2004.
[29] Y.-D. Kim and S. Choi, A method of initialization
for nonnegative matrix factorization, In ICASSP, 2007,
II–537–II–540.
[30] A. Asuncion and D. Newman, UCI machine learning
repository, 2007.

A

Testing Methodology Framework

James Collofello
Computer Science Department
Arizona State University
85287-5406
Tempe, Arizona

Terry Fisher
Mary Rees
McDonnell Douglas Helicopter
Company
5000 East McDowell Road
Mesa, Arizona 85205-9797

Abstract
The testing phase of a project spans the range of tasks
from a developer testing a new module to the overall
system test of the entire product. To perform these tasks a
vast number of techniques have been developed and
described in the literature. These techniques are often
classified based on their applicability during a particular
phase of testing or the type of information utilized in the
development of the tests. What is neededfor a particular
project, however, are practical guidelines for selecting
among this vast array of testing techniques and deciding
when to apply each of the selected techniques.

testing activities.
The c o s t o f t h e s e
testing a c t i v i t i e s is estimated t o
range from 5 0 t o 80 p e r c e n t of t h e
t o t a l p r o j e c t c o s t (from p r o j e c t
c o n c e p t i o n u n t i l i n i t i a l r e l e a s e ) . To
m e e t t h e s e high r e l i a b i l i t y challenges
a v a s t number o f t e s t i n g t e c h n i q u e s
have been developed.
Several studies
have a l s o been conducted t o i n v e s t i g a t e
t h e effectiveness of selected
t e c h n i q u e s [ 3 , 9, 1 7 1 .
One common
t h e m e e m e r g i n g t h r o u g h much o f t h i s
r e s e a r c h is t h e f a c t t h a t many
t e c h n i q u e s a r e complementary i n n a t u r e
and t h a t a sequence of t e s t i n g
t e c h n i q u e s a p p l i e d i n v a r i o u s p h a s e s is
most c o s t e f f e c t i v e .
This has l e a d t o
t h e development of t e s t methodologies.

This paper presents a testing methodology framework
which attempts to provide guidance in testing technique
selection and application. Theframework defines several
testing levels with complementary technique groups
defined for each level. Each testing technique group is a
collection of testing techniques with a common objective
and emphasis. In addition, several categories of testing
metrics are defined which can be utilized to increase
testing effectiveness.

A t e s t methodology p r o v i d e s guidance t o
a project i n defining testing levels
and s e l e c t i n g techniques t o apply a t
each l e v e l . S e v e r a l t e s t methodologies
h a v e b e e n d e v e l o p e d a n d described i n
the testing literature.
T y p i c a l of
t h e s e approaches a r e t h e methodologies
developed by P r e s s o n and Matchinski
[18, 131. A common a p p r o a c h of t h e s e
t e s t methodologies i s t o attempt t o
c h a r a c t e r i z e a p r o j e c t i n t e r m s of i t s
reliability considerations, error
p o t e n t i a l , and economic c o n s t r a i n t s
such a s c o s t and schedule.
This
project characterization is then
u t i l i z e d t o select t e s t i n g t e c h n i q u e s .

The testing methodology framework presented in this
paper was initially developed to satisfr the diverse project
needs of one organization. The methodologyframework,
however, is expandable and can be customized to meet the
needs of other organizations and updated to reflect
technology improvements in the testing area.
Background
The c r i t i c a l i t y a n d h i g h r e l i a b i l i t y
d e m a n d s i m p o s e d u p o n many s o f t w a r e
systems i s r e f l e c t e d i n t h e growing
allocation of production c o s t s t o

0730-3157/90/0000/0577$01.OO 0 1990 IEEE

ST?

an identification of possible
testing techniques along with
recommendations for their use
a determination of the type of
test documentation to be utilized
the number and nature of testing
reviews
designation of testing
responsibilities
definition of suitable metrics
that can b e captured during
testing that will impact future
development and testing
methodologies.

Although interesting from a theoretical
perspective and potentially promising,
these approaches are generally not
practical due to the difficulty of
characterizing a project as well as
evaluating the reliability contribution
of various techniques. This evaluation
is further compounded when combinations
of testing techniques applied at
different phases are considered. Much
research is needed in this area before
realistic information can be utilized.
In order t o avoid t h e practical
problems encountered by previous
approaches, this paper defines a
testing methodology framework which
does not attempt to reach the level of
detail where quantitative project
characterization or technique
evaluation is necessary.
Our
methodology framework, instead,
recognizes these tasks as being largely
subjective and best left to the
experience of the project's development
and testing organizations. Thus, the
basic philosophy of o u r testing
methodology framework is to provide a
testing organization with guidelines
for technique selection rather than
automatic technique selection based
upon somewhat nebulous quantitative
factors. In the remainder of this
paper our testing methodology framework
will be described.

Clearly the issues may vary from one
organization to another suggesting the
need for flexibility in testing
methodologies. In the remainder of
this section, a testing methodology
framework will be described which was
developed in one large company in order
to improve the testing effectiveness of
its many developing projects. The
framework does not address all of the
issues listed above, but can be easily
expanded or modified to incorporate
unique organizational concerns. The
framework can, thus, serve as a
template for the development of a
testing methodology.
The testing methodology framework
consists of the following parts:
1. definition of testing levels
including entry and exit criteria
2 . mapping of testing techniques to
t e s t i n g l e v e l s along with
recommended techniques to apply
at each level
3 . identification of metrics

Methodology Framework

There are many issues that must be
addressed by a testing methodology.
These include:

-

Definition of T e s t i n g Levels
All testing methodologies recognize the
need for defining multiple levels of
testing. A usual set of testing levels
consists of:

the definition of testing phases
the establishment of entry and
exit criteria for each phase

578

-

u n i t o r module t e s t i n g
integration testing
system t e s t i n g

Although t h e s e l e v e l s of t e s t i n g a r e
g e n e r a l l y u n d e r s t o o d , t h e y become
problematic t o use i n a testing
m e t h o d o l o g y w h i c h must a p p l y t o many
d i v e r s e t y p e s of p r o j e c t s .
For
i n s t a n c e , what m i g h t b e v i e w e d a s a
u n i t t e s t on one type o f p r o j e c t might
b e c o n s t r u e d a s a n i n t e g r a t i o n t e s t on
another.
S i n c e a t e s t i n g methodology
n o r m a l l y prescribes a c t i v i t i e s i n
r e l a t i o n s h i p t o t e s t i n g l e v e l s , any
confusion concerning t h e n a t u r e of a
l e v e l c a n have p r o f o u n d e f f e c t s i n t h e
a p p l i c a t i o n of t h e methodology.
To
c i r c u m v e n t some o f t h e s e problems, t h e
p r o p o s e d t e s t i n g methodology framework
defines four levels of testing:
s t r u c t u r a l ,
f u n c t i o n a l ,
m u l t i f u n c t i o n a l , and system.
These
f o u r l e v e l s correspond t o conceptually
d i f f e r e n t forms of t e s t i n g w i t h v a r y i n g
o b j e c t i v e s a n d t e c h n i q u e s . Any g i v e n
p r o j e c t may a d o p t t h e s e l e v e l s o r
d e f i n e a l t e r n a t i v e l e v e l s . Applicable
techniques f o r a l t e r n a t i v e l e v e l s can
b e i d e n t i f i e d v i a a mapping t o one of
t h e f o u r c o n c e p t u a l l e v e l s of t e s t i n g .
Each o f t h e l e v e l s i s d e f i n e d below
along with a d e s c r i p t i o n of t h e
o b j e c t i v e of t h e t e s t i n g l e v e l and i t s
e n t r y and e x i t c r i t e r i a .
S t r u c t u r a l Level. The s t r u c t u r a l l e v e l
is concerned with the smallest
c o n c e p t u a l i t e m t o undergo t e s t i n g .
Examples o f s u c h items, r e f e r r e d t o a s
s t r u c t u r a l e n t i t i e s , might i n c l u d e a
s o f t w a r e f u n c t i o n o r a s u b r o u t i n e . The
o b j e c t i v e of t h i s l e v e l i s t o i n s u r e
t h a t a d e q u a t e code c o v e r a g e h a s been
accomplished and t h a t t h e s t r u c t u r a l
e n t i t y h a s b e e n t h o r o u g h l y tested.
A
reasonable e n t r y c r i t e r i a t o t h i s l e v e l
i s a s u c c e s s f u l compilation and a

completed code i n s p e c t i o n .
The e x i t
c r i t e r i a is demonstrated s t r u c t u r a l
c o v e r a g e a n d f u n c t i o n a l c o r r e c t n e s s of
the entity.
F u n c t i o n a l Level. The f u n c t i o n a l l e v e l
i s concerned w i t h t e s t i n g combinations
of s t r u c t u r a l e n t i t i e s which p e r f o r m a
single function, referred t o a s a
functional entity.
Examples of
f u n c t i o n a l e n t i t i e s might include
s o f t w a r e programs o r modules.
The
o b j e c t i v e of t h i s l e v e l i s t o i n s u r e
that the functional entity satisfies
its s p e c i f i c a t i o n s and t h a t t h e
i n t e r f a c e s a r e c o r r e c t among t h e
A reasonable
structural entities.
e n t r y c r i t e r i a t o t h i s l e v e l is t h a t
e a c h o f t h e s t r u c t u r a l e n t i t i e s which
compose t h e f u n c t i o n a l e n t i t y have been
tested and m e e t t h e e x i t c r i t e r i a from
the structural level.
The e x i t
c r i t e r i a is demonstration t h a t t h e
f u n c t i o n a l e n t i t y m e e t s i t s
specifications.
M u l t i f u n c t i o n a l Level.
The
m u l t i f u n c t i o n a l l e v e l i s concerned w i t h
t e s t i n g c o m b i n a t i o n s of
functional
e n t i t i e s which f o r m m u l t i f u n c t i o n a l
e n t i t i e s such a s subsystems.
Each
multifunctional e n t i t y performs
m u l t i p l e f u n c t i o n s which a r e c l o s e l y
related.
The o b j e c t i v e o f t h i s l e v e l
is t o insure that the functional
A
entities interact correctly.
r e a s o n a b l e e n t r y c r i t e r i a to t h i s l e v e l
i s t h a t each of t h e f u n c t i o n a l e n t i t i e s
which compose t h e m u l t i f u n c t i o n a l
e n t i t y have been tested and m e e t t h e
e x i t c r i t e r i a from t h e f u n c t i o n a l
l e v e l .
The e x i t c r i t e r i a i s
demonstration t h a t t h e m u l t i f u n c t i o n a l
e n t i t y meets i t s s p e c i f i c a t i o n s .
System Level.
The s y s t e m l e v e l i s
concerned with t e s t i n g an e n t i r e
It is t h e highest
i n t e g r a t e d system.

level of testing with all system
components in place. A reasonable
entry criteria to this level is that
all of the multifunctional entities
have been tested and meet the exit
criteria from the multifunctional
level.
The exit criteria is
demonstration that the system meets its
specifications within its target
environment. Ideally, the target
environment should be the identical
environment in which the system or
software will be utilized upon release.
However, during initial system test,
the environment may be simulated using
devices or a host system which can
duplicate the inputs into the device,
and receive the outputs for recording
purposes. This preliminary step is
often due to limitations and safety
issues. For example, aircraft systems
which communicate with multiple
processors on the actual aircraft are
often tested in a simulation
environment rather than in the actual
aircraft. The other processors are
s i m u l a t e d by o n e o r m o r e h o s t
processors which duplicate the
communication for a specific test
scenario. Prior to release, however,
the aircraft is normally tested as a
unit with the new or updated aircraft
system.

Combining T e s t i n g T e c h n i q u e s into T e s t
T e c h n i q u e Groups
There are a large number of testing
techniques that have been developed
over the last twenty years. The goal
of an effective testing methodology is
t o provide guidance t o testing
personnel in selecting a set of
complementary testing techniques
suitable for their project as well as
assisting them in deciding the most
effective level to apply each
technique. Our approach to achieving
this goal was the development of an

expandable testing methodology
framework based upon the notion of a
testing technique group. A testing
technique group is a collection of
testing techniques with a common
objective and emphasis. Our testing
methodology framework defines for each
testing level a set of complementary
testing technique groups. The groups
are selected in such a way that at
least one technique from each technique
group must be applied if the objectives
of t h e t e s t i n g l e v e l a r e t o b e
satisfied.
Furthermore, the
methodology provides a recommendation
of a technique to apply from each
technique group.
The test technique groups that have
been defined for the current version of
the methodology framework are defined
in the following paragraphs. The
methodology framework is easily
expandable t o include additional
testing technique groups such as:
1.
2.
3.

security
compatibility
configuration

Boundary Value. Boundary value testing
is an extremely effective approach
which tests the values at the extremes
of each input range. It is used to
detect logic, computation, and
precision errors. Boundary value
testing applies to all levels of
testing, with the scope of testing
increasing at higher test levels. At
one extreme, boundary value testing at
t h e structural level focuses on
boundaries corresponding to inputs and
outputs of entities such as procedures
and functions. At the other extreme,
boundary value testing at the system
level typically corresponds to testing
the values at either end of the range

o f h i g h l e v e l i n p u t s e x p e c t e d by t h e
s o f t w a r e system a s a whole.
I n p u t Domain.
I n p u t domain t e c h n i q u e s
analyze requirements o r s p e c i f i c a t i o n
documents t o c r e a t e t e s t c a s e s .
They
a r e used t o detect r e q u i r e m e n t s , l o g i c ,
computational, i n t e r f a c e , and p r e c i s i o n
errors.
I n p u t domain t e s t i n g a p p l i e s
t o a l l l e v e l s of t e s t i n g w i t h t h e scope
of t e s t i n g v a r y i n g from l e v e l t o l e v e l .
The t e s t c a s e s c r e a t e d must i n s u r e t h a t
r e p r e s e n t a t i v e s a m p l e s of i n p u t t o t h e
e n t i t y under test a r e executed.
Implicit i n t h i s t e c h n i q u e group i s t h e
f a c t t h a t a l l functions i n the e n t i t y
under test a r e e x e c u t e d with t h e i n p u t
selected.
This group includes
techniques such a s
Equivalence
P a r t i t i o n i n g and Dual Programming.
C o n t r o l Flow. C o n t r o l f l o w t e c h n i q u e s
i n s u r e t h a t t h e c o n t r o l flow s t r u c t u r e
of t h e s o f t w a r e h a s b e e n e x e r c i s e d .
They a r e u s e d t o d e t e c t l o g i c e r r o r s
a n d a r e most e f f e c t i v e l y p e r f o r m e d a t
t h e s t r u c t u r a l t e s t l e v e l . Most of t h e
t e c h n i q u e s i n t h i s g r o u p a r e dynamic
a n d a t t e m p t t o a s s e s s t h e degree o f
c o n t r o l flow l o g i c coverage a t v a r i o u s
l e v e l s of d e t a i l . T h i s g r o u p i n c l u d e s
such t e c h n i q u e s a s s t a t e m e n t , branch,
m u l t i p l e c o n d i t i o n and p a t h coverage,
a s w e l l a s L i n e a r Code S e q u e n c e a n d
Jump Coverage and McCabe A n a l y s i s .

d u p l i c a t i o n , and o t h e r a n o m a l i e s .
The
code i s n o t e x e c u t e d ; o n l y t h e c o n t e n t s
a r e analyzed.
Dynamic t e c h n i q u e s
c o n s i s t of a c t u a l code e x e c u t i o n , w i t h
o b s e r v a t i o n a n d e v a l u a t i o n of r e s u l t s
following t h e execution.
The code may
b e e x e c u t e d i n e i t h e r a h o s t or t a r g e t
environment.
The s t a t i c t e c h n i q u e s
p r i m a r i l y detect a n o m a l i e s c o n c e r n i n g
t h e u t i l i z a t i o n of e i t h e r d a t a items or
data base entries.
The dynamic
t e c h n i q u e s a t t e m p t t o assess t e s t i n g
thoroughness via tracking data
p r o p a g a t i o n amongst s e v e r a l l e v e l s of
d a t a use.
I n t e q r a t i o n Coveraqe.
Integration
coverage t e c h n i q u e s a r e used t o v e r i f y
t h e o p e r a t i o n of s o f t w a r e a s i t i s
combined w i t h o t h e r e n t i t i e s ( e i t h e r
hardware o r a d d i t i o n a l s o f t w a r e ) . They
a r e u s e d t o detect e r r o r s which a r e a
r e s u l t of improper s o f t w a r e i n t e g r a t i o n
such a s i n t e r f a c e o r sequencing e r r o r s .
T h e s e t e c h n i q u e s r e q u i r e s some
i n f o r m a t i o n o f how t h e s y s t e m i s
d i v i d e d i n t o smaller u n i t s t o p r o p e r l y
u n d e r s t a n d t h e i n t e g r a t i o n needs.
I n t e g r a t i o n coverage i s used d u r i n g t h e
f u n c t i o n a l and m u l t i f u n c t i o n a l t e s t
levels.
During t h e f u n c t i o n a l test
l e v e l , i n t e g r a t i o n e r r o r s between
structural entities are targeted.
Techniques a r e selected f o r t h i s g r o u p
which i n s u r e t h a t i n t e r f a c e s among t h e
s t r u c t u r a l e n t i t i e s a r e adequately
tested.
During t h e m u l t i f u n c t i o n a l
l e v e l , i n t e g r a t i o n e r r o r s between
functional entities are targeted.
Techniques a r e selected for t h i s g r o u p
which i n s u r e t h a t i n t e r f a c e s among t h e
functional e n t i t i e s a r e adequately
tested.
These t e c h n i q u e s a r e not
normally a p p l i e d a t system l e v e l , s i n c e
t h i s l e v e l stresses t h e w h o l e n e s s of
t h e e n t i t i e s i n s t e a d of t h e i r
A t system
individual interaction.

Data Flow.
Data flow i s a broad
t e c h n i q u e group which a s s e s s e s t h e
c o r r e c t u t i l i z a t i o n of d a t a i n t h e
system.
It i s used t o detect
requirements, i n t e r f a c e , and data
corruption errors.
Data flow t e s t i n g
i s most e f f e c t i v e l y p e r f o r m e d a t t h e
s t r u c t u r a l t e s t l e v e l . Both s t a t i c and
d y n a m i c d a t a f l o w t e c h n i q u e s may be
a p p l i e d . S t a t i c t e c h n i q u e s review code
o r d e s i g n u s i n g a u t o m a t i c o r manual
means t o d e t e c t i n c o n s i s t e n c i e s ,

581

level, all entities are considered as
one unit, without separate integration
areas to inspect.
This group includes techniques such as
state transition anaylsis and testing
and transaction flow testing.
Concurrency. Concurrency techniques
verify t h e interaction of software
units which operate independently but
s h a r e resources. They a r e used t o
detect errors due t o issues such as
synchronization, sequencing, race
condition a n d deadlocks.
Considerations are primarily addressed
in the multifunctional test level.
Performance. Performance techniques
assess how well the entity undergoing
test conforms t o its performance
specifications.
They are used t o
detect timing errors and performance
errors concerned with time limits and
allotments.
This technique group
applies t o the functional,
multifunctional and system levels, with
the scope varying from level to level.
Recovery. Recovery techniques assess
the system's ability to recover from
various problems. They are used t o
detect requirements and interface
errors. This technique group applies
to the system test level. Recovery
aspects of the system relate to power
surge and sag conditions and extreme
error states that potentially affect
system failure.
Stress. Stress techniques attempt to
saturate one or more of the system's
resources. They are used t o detect
errors d u e t o corruption of shared
data, timing, and design flaws. This
technique group applies to the
functional, multifunctional and system
levels.
This technique group is

essential since a significant
percentage of problems in computer
systems surface under stress
conditions.
Reliabilitv Prediction. Reliability
prediction techniques attempt t o
estimate the completeness of testing.
They are used t o detect all error
types, and are applied during system
level testing.
This group of
techniques gives some indication about
t h e reliability of t h e system under
test. This information is necessary to
determine when t o exit t e s t i n g a n d
deliver the system. There are many
possible techniques which can serve
this purpose. Some approaches track
defects and attempt t o assess
reliability in terms of Mean Time to
Failure (MTTF) or a probability that a
failure will occur in some t i m e
interval. Other approaches are more
indirect a n d attempt t o assess t h e
reliability of the system based on the
quality of the testing effort. This
includes such things as error seeding,
program mutation, and statistical
testing.
Y a p p i n g of T e s t i n g T e c h n i q u e s t o
T e s t i n g Levels

The technique groups are mapped to the
testing levels via tables. For a given
project, a thread is selected which
identifies techniques from each
t e c h n i q u e g r o u p t o a p p l y at e a c h
t e s t i n g level. F i g u r e 1 shows how
different threads, or paths, may be
obtained by selecting different
t e c h n i q u e s f r o m e a c h of t h e t e s t
technique groups. Note that Path A and
Path B have some techniques in common
(C2 in the Structural Level and A2 in
the Functional Level).
This
demonstrates the idea that a thread may
choose any technique from a group, with
multiple threads all using the same

582

1
System

T
e
C

1

2

h
n 3
i
9 4
U

e
S

5

A B C D

A B C D

A B C D

A

B C D

GROUPS
Figure 1:

Selection of Paths through Technique Groups

technique if desired. Also note that
Path B uses multiple techniques for a
f e w t e s t s g r o u p s (D3 and D 5 in t h e
Structural Level and A2 and A5 in the
Functional Level). This shows how
several techniques may be used from one
test group. If the suggested technique
from a group is not chosen, several
techniques may be combined, if desired,
to more fully fulfill the goals of that
group.

are shown in bold. These techniques
were isolated for various reasons
including:

-

-

minimal need for automated tools
extent to which the technique met
the goals of the technique group
ease of instructing minimally
experienced engineers in the use
of the technique

In some cases, only one or t w o
techniques were available for a certain
technique group.
I n t h a t case, a
technique may have been recommended
only because it was the only technique
in the group.

Table 1 is an example of a table which
specifies the mapping of testing
techniques to testing technique groups,
a s w e l l a s t h e m a p p i n g of t e s t i n g
technique groups t o testing levels.
Selected references are also noted for
each technique. Recommended techniques

583

Table 1.

GROUP

Structural Testing

TEST TECHNIQUES

REFERENCES

Boundary Value

*

Control Flow

Branch Testing
LCSAJ* Coverage
McCabe Analysis
Multiple Condition Coverage
Path Testing
Statement Coverage
Symbolic Execution

Data Flow

Data Base Analysis
Data Flow Analysis
Data Flow Testing
All Definitions
All Uses
All C-Uses/Some P-Uses*
All P-Uses/Some C-Uses*
A l l DU-Paths*

Input Doma in

Enhanced Condition Table
Equivalence Partitioning

LCSAJ:
linear code sequence and jump
DU-Paths: definition-use Paths

Identification of Metrics
A theme throughout the discussion thus
f a r of o u r t e s t i n g m e t h o d o l o g y
framework has been its flexibility and
expandability. I n order for an
organization to increase its testing
effectiveness, data must be collected
which helps the organization and
individual projects customize both
their development and testing
methodologies. Much of this data can
and should be collected during the
various testing activities. Our
testing methodology organizes this
information into four categories. The
four categories are described below
along with some sample metrics.

C-Uses: computational uses
P-Uses: predicate uses

Development Method Evaluation
Indicators. These metrics address how
well a product is being developed.
They can help developers identify areas
where improvements in development
methodology are needed. Some sample
metrics in this category include:
ERRORS/KLOC
- Raw measurement of error density.
KLOC is thousand lines of code.
ERROR COMPOSITION
Determination of the percentage
of different error types.

-

584

1

I201
1171

ERROR CRITICALITY
- Profile of the seriousness of
errors detected; their impact on
the system as a whole and on
safety issues.
Product Evaluation Indicators. These
metrics address quality issues of the
software product. Some sample metrics
in this category include:
MEAN TIME TO FAILURE
Estimate of time lag until next
error detected.

-

EXPECTED REMAINING ERRORS
- Estimate of number of errors left
in the product.
EXPECTED TIME TO FIX ERRORS
Estimate of time t o repair
detected faults.

-

Trackinq Indicators. These metrics
help evaluate the progress being made
during testing and how close the
testing effort is to completion. Some
sample metrics i n this category
include :
CUMULATIVE DEFECT COUNT
Used to graph defect detection
rate against time.

-

CODE COVERAGE PERCENTAGE
Can include statements, branches,
paths, etc.

-

ERROR DETECTION EFFICIENCY
Determine the percentage of
errors that were detected by a
particular technique.

-

ERROR DETECTION COST PER ERROR
Determine the cost of detecting
errors; may be grouped by type
and/or criticality.

-

PERCENTAGE TEST COST TO
DEVELOPMENT COST
Helpful only if development cost
standard f o r methods under
comparison.

-

Future Research
This paper has presented a testing
methodology framework which provides
guidance in testing technique selection
and application. Much more research is
planned for our testing methodology
framework. Our major thrust will
involve analyzing the effectiveness of
the framework in several diverse types
of projects. Measurements will be
defined which address issues such as
defect leakage and defect detection
cost. These measures will aid the
identification process of recommended
techniques for each testing level. AS
more experimental and experiential
knowledge becomes available on testing
techniques and their combinations, they
will be incorporated into the testing
methodology framework.
References

PERCENT REQUIREMENTS TESTED
Similar to code coverage, but
from a functional viewpoint.

[l]

Test Method Evaluation Indicators.
These metrics address the effectiveness
of the testing effort. Some sample
metrics include:

Appelbe, W.F., R.A. Demillo, D.S. Guindi,
K.N. K i n g a n d W.M. M c C r a c k e n , " U s i n g
Mutation Analysis for Testing Ada
Programs", Proc. Ada Europe Int. Conf. Ada
in Industry, 1988, pp. 65-80.

[Z]

Basili, Victor R., Richard W. Selby, David
H. Hutchens, "Experimentation in Software
Engineering", IEEE Transactions on
Software Enqineerinq, Vol SE-12 RI, 1/86.

-

B a s i l i , V i c t o r R., a n d R i c h a r d W. S e l b y ,
"Comparing t h e E f f e c t i v e n e s s o f S o f t w a r e
T e s t i n g S t r a t e g i e s " , I E E E T r a n s a c t i o n s on
S o f t w a r e E n g i n e e r i n q , V o l SE-13 t12,
12/87, pp. 1278-1296.

Development:
Alternatives.
pp. 207-213.

Tools,Techniques,
and
IEEE, S i l v e r S p r i n g , MD,

Beizer, Boris, Software Testing
Techniques. Van Nostrand Reinhold Company
Ltd., 1983.

M a t c h i n s k i , Donald J. F e a s i b i l i t y s t u d y
of g e n e r a t i n q p l a n s and s t r a t e q i e s f o r
s o f t w a r e t e s t i n g by knowledqe b a s e d
system, f i n a l r e p o r t .
Sonex E n t e r p r i s e s ,
Inc., F a i r f a x , VA, 10/87.

B e i z e r , B o r i s , S o f t w a r e System T e s t i n g and
Q u a l i t y Assurance.
Van N o s t r a n d R e i n h o l d
Company, N e w York, 1984.

McCabe, T. J . , -A Complexity Measure,
T r a n s . S o f t w a r e E n g . , v o l . SE-2, no.
pp. 3-15, 12/76.

-

Dunn, W i l l i a m R., " S o f t w a r e r e l i a b i l i t y
Measures and e f f e c t s i n f l i g h t c r i t i c a l
d i g i t a l a v i o n i c s systems".
Proceedings
IEEE/AIAA 7 t h D i g i t a l Avionics Systems
Conference, IEEE ( N e w York, NY, 1986), pp.

4,

McCabe, T . J . a n d G.G.Schulmeyer,
"System
A
T e s t i n g Aided by S t r u c t u r e d A n a l y s i s :
P r a c t i c a l Experience,"
I E E E T r a n s . on
S o f t w a r e Enq., vol SE-11, no. 9, pp. 917-

921, 9/85.

664-669.
Dyer, M . , "A Formal Approach t o S o f t w a r e
E r r o r Removal," J o u r n a l System and
S o f t w a r e , v o l . 7, n o . 2 , p p . 109-114,

Musa, J o h n D . a n d A . F r a n k A c k e r m a n ,
" Q u a n t i f y i n g S o f t w a r e V a l i d a t i o n : When To
S t o p T e s t i n g ? " , IEEE T r a n s a c t i o n s o n
Software Engineerinq, 5/89, pp. 19-27.

6/87.
F r a n k l , P h y l l i s G . and E l a i n e J. Weyuker,
" D a t a Flow T e s t i n g i n t h e P r e s e n c e o f
Unexecutable P a t h s " , 1986 P r o c Workshop on
Software T e s t i n q , pp. 4-13.
G i r g i s , M.R.
a n d M . R . Woodward, "An
Experimental Comparison o f t h e E r r o r
Exposing A b i l i t y o f Program T e s t i n g
C r i t e r i a " , P r o c Workshop on S o f t w a r e
T e s t i n g 1986, pp. 64-73.
Hamlet, D.,
"Partition t e s t i n g does not
i n s p i r e confidence", Proceedings of t h e

H e n n e l l , M.A.,
D. Hedley, and I.J.
R i d d e l l , "The LDRA S o f t w a r e T e s t b e d s :
T h e i r R o l e s a n d C a p a b i l i t i e s " , 1983
Softfair
S o f t w a r e Development: T o o l s ,
IEEE,
Techniques, and A l t e r n a t i v e s .
S i l v e r S p r i n g , MD, pp. 69-77.

-

Marion, Robert A.,
" C r i t i c a l Event
M o d e l i n g : A S t e p Beyond S y s t e m L e v e l
Software
T e s t i n g " , 1983 S o f t f a i r

-

Myers, G e n f o r d J .
The A r t o f S o f t w a r e
T e s t i n q , John Wiley C Sons, I n c . , 1979.
P r e s s o n , Edward.
S o f t w a r e T e s t Handbook
S o f t w a r e T e s t Guidebook, Boeinq Aerospace
C o m p a n y , 3 / 8 4 , Rome A i r D e v e l o p m e n t
Center, G r i f f i s s A i r F o r c e Base, NY.
R a p p s , S. a n d E. J . Weyuker, " S e l e c t i n g
S o f t w a r e T e s t D a t a U s i n g Data Flow
I n f o r m a t i o n , " IEEE T r a n s . o n S o f t w a r e
Eng., v o l SE-11, no. 4, pp. 367-375, 4/85.
T a i , Ann, Myron Hecht, and H e r b e r t Hecht,
" A new m e t h o d f o r t h e v e r i f i c a t i o n o f
1987 I E E E
f a u l t t o l e r a n t software".
E a s c o n ( E a s c o n '87; P r o c e e d i n g s o f t h e
2 0 t h Annual E l e c t r o n i c s and Aerospace
S y s t e m s C o n f e r e n c e ) , W a s h i n g t o n , DC,
10/14-16/87, I E E E ( N e w York, 19871, p p .

53-59.
Taylor, R.N.,
"Analysis of Concurrent
Software by C o o p e r a t i v e A p p l i c a t i o n of
S t a t i c a n d Dynamic T e c h n i q u e s , " P r o c .
Symposium S o f t w a r e V a l i d a t i o n , N o r t h 137.
Holland, S e p t . 1983, pp. 127

-

The Software Engineering Institute
Norman E. Gibbs, Software Engineering Institute
Clyde Chittister, Software Engineering Institute
James S. Collofelio, Arizona State University
Gary A. Ford, Software Engineering Institute
A. Joseph Turner, Clemson University

Summary

generation of software engineers. The Technology Transition and
Training Department is responsible for the rapid insertion of new
technology into practice, and for the improvementof the technology
transition process itself.

The Software Engineering Institute at Carnegie-Mellon University is
described, as are two ot its major programs, the Academic Nfiliates
Program and the Industry Affiliates Program. The major project of
the Education Division, a graduate curriculum in software
engineering, is also discussed.

The SEI's method is Io use dedicated, full-time technical staff
rather than subcontracting technical work. In order to speed
technology transition through personal experience, affiliate
programs have been established to expand contact with industrial,
academic, and government organizations. Participants in the
affiliate programs will be able to nominate individuals for long-term
assignments at the SEI, where top engineers and scientists in the
software engineering field work together of a variety of projects that
lead to prototypes and demonstrationsystems.

Overview of the Software Engineering Institute
The Software Engineering Institute is a new Federally Funded
Research and Development Center (FFRDC), sponsored by the
Department of Defense, and operated by Carnegie-Mellon
University. The mission of the Software Engineering Institute is to
serve the public interest by establishing the standard of excellence
for the art and practice of software engineering and by accelerating
the transition of software technology.

The SEI began early in 1985, under a five-year, $103 million
contract from the Department of Defense. By the end of 1985,
approximately one third of the anticipated 250 technical staff
members had been hired. The temporary home of the SEI is a
remodeled factory building in the Shadyside section of Pittsburgh;
in 1987 the Institute will move into a new permanent facility
adjacent to the Carnegie-Melloncampus.

Some of the specific objectives of the SEI are the following:
• Identify, acquire, assess, and develop useful software
engineering knowledge and skills, and facilitate the
propogation and dissemination of that knowledge. In
support of this objective, the SEI will become a
showcase for all aspects of software engineering, and
a center of software engineering expertise.

A Graduate Curriculum in Software Engineering
One of the objectives of the SEI is to influence the education of
future software engineers. Toward this end, a graduate level
curriculum in software engineering is being developed. The
curriculum will be highly modular to allow packaging for a variety of
audiences, and to allow easy updating through revision or
replacement of modules.

• Catalyze the development of software engineering
from a labor-intensive activity to a technology-intensive
activity.
• Foster the development of software engineering as an
engineeringdiscipline and as a profession.

Currently, we expect the curriculum to consist of ten to fifteen core
modules and twenty to thirty specialty or elective modules. A
module will correspond to one university semester hour, so three
modules would make up a lypical semester course. Two modules
could be combined into a course for a school using the quarter
system.

The SEI is organized as four major departments. The Technical
and Administrative Services Department provides the underlying
support for the activities of the SEI, including clerical,
documentation, and business support. It also provides the
hardware and software support for technical activities. The
Technology Exploration Department is responsible for identifying,
assessing, and developing software engineering technology,
including methodologies and tools to support those methodologies.
The Research and Education Department carries out basic and
applied research in software engineering, and is responsible for
furthering the education of both current practitioners and the next

The core modules will emphasize the concepts and fundamentals
common to all aspects of software engineering. These will be
organized in three categories: technical, management, and quality
assurance. These modules are intended to make students aware
of the various aspects of the software engineering process,
including reasoning about the process itself. The specialty
modules will include current methodologies and tools to support
software engineering. Because these aspects are ever-changing, it
is expected that these modules will be the most dynamic part of the
curriculum. The whole curriculum is expected to be evaluated and
revised annually, so that it will always present the most modern
knowledge.
The full curriculum might be offered by a university, leading to a
degree of Master of Software Engineering (MSE). Such a degree
might be viewed as a professional degree (like an MBA) that
prepares a person for both the technical and management aspects

37g

of software projects. The curriculum might also be offered within
the scope of an existing Master of Science in Computer Science
degree program.

Academic Affiliates will encourage faculty members with relevant
experience to apply for visiting appointments at the SEI to
participate in these activities, and will provide, jointly with the SEI,
appropriate support for those selected.

Parts of the curriculum will also be organized into elective courses
for advanced undergraduates and graduate students of computer
science. The modules might be offered individually as industry
short courses; since a module is designed to require about fifteen
hours of student contact time, it could be a two-day intensive
course.

An academic institution may become an Academic Affiliate by
submitting a proposal to the SEI. Subsequently, individuals from
that institution may submit proposals for projects of mutual interest.
A document describing beth kinds of proposals is available upon
request from the SEI Education Division.

The curriculum will be designed and developed in consultation with
leading software engineers and educators from industry and
academia. It is intended that each module be designed by a leader
in that particular aspect of software engineering, but with support
from the SEI staff and other experts. To achieve this goal, the SEI
will invite persons with appropriate expertise to come to the SEI for
a month or mere to work with us on particular modules.

The Industry Affiliates

Program

The Industry Affiliates Program is one of the most important
activities of the SEI in support of its objectives of identifying and
disseminating software engineering knowledge and technology.
Through the program, industrial organizations can cooperate with
the SEI in a variety of activities. This cooperation is essential to the
mission of the SEI, since most of the advances in software
engineering have come and will continue to come from the
industrial software community.

In support of the curriculum, the SEI will produce or cause to be
produced all kinds of educational materials. These will include
course outlines and reading lists, texts, and educational software.
Many of these materials will be available through national networks
(ARPAnet, CSNet), so that educators may have immediate access
to the most recentcurriculum. In addition, the SEI expects to
conduct symposia and workshops to bring educators up to date on
the content of the curriculum.

There are three levels of affiliation in this program. The first level
requires an Information Exchange Agreement between a company
and the SEI.
Under the terms of this agreement, both
organizations share information about software engineering
concepts, methodologies, techniques, tools, etc. The second level
adds technology exchange to the agreement. Both organizations
may share technological advances, such as software tools, as
appropriate to the needs of both. The third level provides for
employees of the company to come to the SEI for an extended
period of residence, during which time they will work with SEI
technical staff members and resident affiliates from other
organizations on specific projects of mutual interest.

The Academic Affiliates Program
The Academic Affiliates Program provides a means whereby
educational institutions can join the Software Engineering Institute
in cooperative efforts of mutual interest. These efforts may include
cooperative work in education, research, development, or
technology transition.

At all three levels of affiliation, the agreements provide for the
protection of proprietary information, software, or hardware that is
brought to the SEI from the affiliated organization. It is through the
close interaction of many professionals working toward common
goals, rather than through the mere exchange of technological
artifacts, that the SEI and the affiliates derive the most benefit.
Regardless of the level of affiliation, the technology developed and
showcased at the SEI will be available to all affiliates.

Most Academic Affiliates will be involved in the project to develop a
graduate curriculum in software engineering, at least to the extent
that they will send a representative to the SEI conferences and
workshops for curriculum development and will plan to implement
at least a part of the curriculum.
A subset of the Affiliates participating in this project will be selected
to be Primary Test Sites for the curriculum. These Affiliates will be
those that are the primary contributors, in addition to the SEI, to the
development and implementation of the graduate curriculum. SEI
personnel will work closely with these Affiliates and will closely
monitor the implementation of the curriculum by the Affiliates.

Ideas gained by such interaction will guide the SEI in its technology
transition objectives. The most valuable ideas will be developed
and showcased at the SEI. Affiliates at all levels will benefit from
these ideas.

The Technology Exploration Department of the SEI will also
provide opportunities for joint activities with Academic Affiliates.
Current projects include evaluation of software development
support environments and tools, evaluation of Ada environments,
and the development of the showcase systems. Academic
Affiliates may nominate faculty members to come to the SEI to
work on these projects, or to propose new projects within the scope
of the Technology Exploration Department.

The more active affiliates will derive even greater benefits. In many
cases, an organization may have good ideas, but not enough
resources to develop those ideas. By joining forces with the SEI,
there is a multiplier effect, in that one person from an affiliated
organization may work with several other resident affiliates and SEI
staff members to develop new technology very rapidly. The
resident affiliates may then return to their own organizations with
that new technology and the knowledge necessary to put it into
practice immediately.

Academic Affiliates are selected on the basis of their ability and
their commitment to participate in and contribute to the activities of
the SEI. Affiliates may participate in the SEI's education efforts
through active participation in the development of the graduate
curriculum in software engineering, or by implementing part or all of
the curriculum. In addition, Affiliates may participate in research,
development, or technology transition projects, as well as
workshops, conferences, and other activities. Affiliates will receive
preferential consideration for access to the SEI projects,
information, and technology; inclusion at limited attendance
meetings at the SEI; and selection as test sites for educational and
technology transition efforts of the SEI.

The SEI realizes that good ideas can come from many sources,
and thus seeks affiliates from all kinds of industrial organizations,
both large and small. Further information is available from the SEI
Assistant Director for Affiliate Relations,

380

Evaluating the Effectiveness of Process Improvements on Software
Development Cycle Time via System Dynamics Modeling
John D. Tvedt and James S. Collofello
Arizona State University
Department of Computer Science and Engineering
Tempe, AZ 85287-5406 USA
E-mail: {tvedt, collofello} @ a.su.edu
Abstract

Being the first to market along with the high
cost of switching products can give a product a
great advantage in capturing and obtaining a
large market share.

Reducing software development cycle time without
sacrificing quality is crucial to the continued success of
most software development organizations. Software
companies are investing time and money in reengineering
processes incorporating improvements aimed ut reducing
their cycle time. Unfortunately, the impact of process
improvements on the cycle time of complex software
processes is not well understood. The objective of our
research has been to provide decision makers with U
model that will enable the prediction of the impact a set
of process improvements will have on their software
development cycle time. This paper describes our initial
results of developing such a model and applying it to
assess the impact of software inspections. The model
enables decision makers to gain insight and perjorm
controlled experiments to answer "What if?" type
questions, such as, "What kind of cycle time reduction can
I expect to see if I implement inspections?" or "How
much time should I spend on inspections?"

Higher profit margins. Being first to market
gives a company greater freedom in setting
profit margins.
The ability to start later and finish on
schedule. Starting later allows the use of
technological advances that may not have been
available to the competition.
"According to a study released in January 1990 by United
Research Co. of Morristown, N.J., six out of 10 CEOs
listed shorterproduct development cycles as vital to their
company..." [9].
Most of these cycle time reduction reengineering
efforts attempt to take advantage of process improvement
technology such as that advocated by the Software
Engineering Institute's Capability Maturity Model and
I S 0 9000. The implication is that a mature development
process increases the quality of the work done, while
reducing cost and development time [8]. Some software
companies that have begun to mature their process have,
indeed, reported cycle time reductions via process
improvements [51.

1: Background
1.1: Importance of reducing cycle time
The rush to reengineer processes taking place in
many organizations is the result of increasing competitive
pressures to shorten development cycle time and increase
quality. Reducing the time to produce software benefits
not only the customer, but also the software organization.
A number of benefits of reduced cycle time are listed by
Collier [4].
0

1.2: The need for selecting among alternative
process improvements
A common problem faced by organizations
attempting to shorten their cycle time is selecting among
the numerous process improvement technologies to
incorporate into their newly reengineered development
process. Even though there is promising evidence

Market life of the product is extended.

3 18
0730-3157/95 $04.00 0 1995 IEEE

which all factors except the independent variable (the
process improvement) are kept constant. Although much
can be learned through this approach about the general
effectiveness of the process improvement, it is normally
not practical to experiment over the entire development
life cycle of a significant size project. Thus, it is often
impossible to assess the overall impact of the proposed
process improvement on development cycle time.
Another approach typically followed is the utilization
of some sort of "pilot project" to assess the new
technology.
Although considerably weaker than
controlled experimentation, the pilot studies often reveal
the general merit of the: proposed technology. It remains
difficult to assess ithe unique impact of the proposed
improvement technology on cycle time due to the
interaction of other project variables and to generalize the
results to other projects.
A third approach to assessing the impact of process
improvements is the u1.ilization of traditional cost
estimation models such as COCOMO [3] and SLIM [lo].
These types of models contain dimensionless parameters
used to indicate the productivity and technology level of
the organization. The parameter values are determined by
calibration of the model to previous projects or by
answering a series of questions on a questionnaire. For
example, the Basic COGOMO model calculates the
number of man-months needed to complete a project as:

trickling into the pages of computer publications, most
companies are still skeptical of the investment that must
be made before possible improvements will be
experienced. They recognize that process improvements
do not exist in isolation. The impact an improvement
has may be negated by other factors at work in the
particular development organization. For example,
consider a tool that allows some task to be completed in a
shorter period of time. The newly available time may
just be absorbed as slack time by the worker, resulting in
no change to cycle time. Thus, software organizations
need to know, in advance of committing to the process
improvement technology, the kind of impact they can
expect to see. Without such information, they will have
a hard time convincing themselves to take action. In
particular, a software organization needs to be able to
answer the following types of questions:
What types of process improvements will have
the greatest impact on cycle time?;
How much effort should be allocated to the
improvement?;
What will the overall impact be on the
dynamics of software development?;
How much will the process improvement
cost?;

MM = C (KDSIjK, where
MM = number of man-months,
C = a constant,
KDSI == thousands of delivered source
instructions and
K = a constant.

How will the process improvement affect cycle
time?; and
How will this improvement be offset by
reduced schedule pressure, reduced hiring and
increased firing?

The user enters the size of the code along with the two
predefined constants to determine the number of manmonths. C and K are determined by calibrating the model
to previous projects.
An impoirtant question for a software organization to
answer is how C and K should be modified to reflect the
impact of process improvements when no historical data
exists. One potential answer is to use the Intermediate or
the Advanced COCOMO models. These models contain
cost drivers that allow more information about the
software project to be input than the Basic modlel. Cost
drivers are basically multipliers to the estimated effort
produced by the model. Modern Programming Practices
is one such cost driver. If a software project is using
modern programming practices, the cost driver would be
set such that its multiplier effect would reduce the amount
of effort estimated. Two questions arise when using this
cost driver:
what is the definition of modern

Today, the limited mental models often used to answer
these questions are insufficient. The mental models often
fail to capture the complex interaction inherent in the
system. "It is the rare [manager] whose intuition and
experience, when he is presented with the structure and
parameters of [only] a second order linear feedback system,
will permit him to estimate its dynamic characteristics
correctly," [2].

1.3: Current approaches for evaluating the
impact of process improvements
There does not exist a standard method for
determining the impact of specific process improvements
on cycle time. An ideal way for performing this
assessment is conducting a controlled experiment in

3 19

perception that the product is back on schedule
(effedclose feedback loop).

programming practices and how should the cost driver's
value be altered with the addition of a specific
improvement to the existing programming practices.
These questions have no simple answers because the cost
drivers are at a level of granularity too coarse to reflect the
impact of specific process improvements. Some
researchers also believe that models such as COCOMO
are flawed due to their static nature. Abdel-Hamid states,
"The problem, however, is that most such tools are static
models, and are therefore not suited for supporting the
dynamic and iterative planning and control of [resource
allocation]" [2].

The developers perceived the product to be behind
schedule, took action, and finally perceived the product to
be back on schedule, as a result of their actions.
Secondary effects due to the developers' actions, however,
such as the lower quality of the product, will also
eventually impact the perception the developers have as to
being on schedule and will necessitate further actions
being performed. These cause-effect relationships are
explicitly modeled using system dynamics techniques.
Because system dynamics models incorporate the
ways in which people, product and process react to
various situations, the models must be tuned to the
environment that they are modeling. The above scenario
of developers reacting to a product that is behind schedule
would not be handled the same way in all organizations.
In fact, different organizations will have different levels of
productivity due to the experience level of the people
working for them and the difficulty of the product being
developed. Therefore, it is unrealistic for one model to
accurately reflect all software development organizations,
or even all projects within a single development
organization.
Once a system dynamics model has been created and
tailored to the specific development environment, it can
be used to find ways to better manage the process to
eliminate bottlenecks and reduce cycle time. Currently,
the state-of-the-practice of software development is
immature. Even immature software development
environments, however, can benefit from this technique.
The development of the model forces organizations to
define their process and aids in identifying metrics to be
collected. Furthermore, the metrics and the models that
use them do not have to be exact in order to be useful in
decision-making [13]. The model and its use result in a
better understanding of the cause-effect relationships that
underlie the development of software. The power of
modeling software development using system dynamics
techniques is its ability to take into account a number of
factors that affect cycle time to determine the global
impact of their interactions, which would be quite difficult
to ascertain without a simulation. The model may be
converted to a management flight simulator to allow
decision-makers the ability to perform controlled
experiments of their development environment.

2: Proposed approach for evaluating the
effectiveness of process improvements
2.1: Overview of system dynamics modeling
Due to the weaknesses of the approaches presented in
the previous section, we chose to evaluate the impact of
process improvements on software development cycle
time through system dynamics modeling. System
dynamics modeling was developed in the late 1950's at
M.I.T. It has recently been used to model "high-level"
process improvements corresponding to SEI levels of
maturity [ 121. System dynamics models differ from
traditional cost estimation models, such as COCOMO, in
that they are not based upon statistical correlations, but
rather cause-effect relationships that are observable in a
real system. An example of a cause-effect relationship
would be a project behind schedule (cause) leading to
hiring more people (effect). These cause-effect
relationships are constantly interacting while the model is
being executed, thus the dynamic interactions of the
system are being modeled, hence its name. A system
dynamics model can contain relationships between people,
product and process in a software development
organization. The most powerful feature of system
dynamics modeling is realized when multiple cause-effect
relationships are connected forming a circular relationship,
known as a feedback loop. The concept of a feedback
loop reveals that any actor in a system will eventually be
affected by its own action. A simple example of the ideas
of cause-effect relationships and feedback loops affecting
people, product and process can be illustrated by the
following scenario:
Consider the situation in which developers,
perceiving their product is behind schedule
(cause), modify their development process by
performing fewer quality assurance activities
(effect/cause), leading to a product of lower
quality (effecthause), but giving the temporary

2.2: Modular system dynamics modeling
Due to the large number of process improvements
available for evaluation and the necessity of conducting
each evaluation in the context of a system dynamics

320

model customized to the organization for which the
evaluation is being performed, a modular system
dynamics modeling approach was adopted. A modular
system dynamics model consists of two parts, the base
software process model and the process improvement
model. Once a base model of the software development
process exists, any number of process improvement
models may be plugged in to determine the effect they
will have on software development cycle time. This
modularity gives the process improvement models the
advantage of being used in more than one base model of a
software development environment. For example, the
process improvement model could be integrated with a
base model of development environment "A" and also a
base model of development environment "B". The only
change to the process improvement model would be the
values of its parameters in order to calibrate it to the
specific development environment. The models of the
development environments may have to be modified
structurally, such that they include all necessary model
elements, e.g., system dynamics modeling rates and
levels, for integration.
A conceptual image of the modular model is shown
in Figure 1. On the left is a box representing an existing
system dynamics model of a software development
process. This model would be tailored to a particular
organization. On the right is a box representing the
model of the process improvement.

System dynamicselements
necessary to integrate the models

Figure 1.

modified to take advantage of the proposed model, such
that it contains the necessary elements for the interface.

2.3: Approach for constructing modular system
dynamics models
An organization wishing to utilize system dynamics
modeling for assessing the impact of new technologies on
cycle time must perform the following steps:

1. Construct d k s e model of the software
development Dracess. Construction of a base
model requires detailed modeling of the
organization's software development process as
well as identification of cause-effect relationships
and feedback loops. Sources of information for
model construction include: observation of the
actual software process, interviews of personnel,
analysis of metrics and literature reviews of
relevant publications. An organization wishing
to adapt an existing generic model for rough
assessments might consider the model developed
by Abdel-Hamid [11.
2. Construct the pacess improvement models.
For each process improvement being considered,
a modular process improvement model must be
developed utilizing the same approach as that of
the base model.

I

Graphic of existing process with
process improvement.

The two models are integrated through the interface
pictured. Information will flow in both directions
between the models. This interface is dependent on
certain model elements, such as levels and rates, existing
in both models. The ovals in Figure 1 represent a cutaway view showing the underlying structure of the system
dynamics model and the elements needed for the interface.
An existing model of an organization may need to be

321

3. Validate thebase and process improvement
models. Both the base and process improvement
models must be validated to their accuracy
requirements in order to build confidence in the
models ability to replicate the real system. The
validation process, must consider the suitability
of the model for its intended purpose, the
consistency of the: model with the real system,
and the utility and effectiveness of the modlel [ 6 ] ,
[71, [ i l l .
4. Execute the models. The base model and any
of the process improvement models can be
combined to assess the impact of various process
improvements on cycle time.
Analysis of
outputs may also lead to new ideas and
understanding of the system triggering additional
process reengineering as well as consideratiion of
other process improve:ments.

3: Demonstration of approach

Our software inspection model does not incorporate the
following:

3.1: Model development
Software developers achieve higher
productivity due to an increase in product
knowledge acquired through the software
inspection process;

In order to illustrate the feasibility and usefulness of
system dynamics modeling for process improvement
assessment, we applied our approach to the software
inspection process. Our model has the ability to provide
answers to the types of questions, concerning process
improvements, posed in Section 1.2. For the purpose of
our demonstration, we focus mainly on the question of
cycle time reduction. We initially developed a base model
corresponding to a typical organization’s waterfall
software development process. We then constructed a
model of the software inspection process which we
integrated with the base model. The software inspection
model enables manipulation of a number of variables
connected to the inspection process in order to understand
their impact on software development cycle time. Direct
manipulation of the following variables are allowed:

Software developers achieve improved design
estimates due to attention paid to size
estimates during inspection; and
Software inspections lead to increased
visibility of the amount of work completed.
The model also excludes the interaction that the
inspection team size and the time spent performing
inspection tasks have on the percent of errors found during
inspection. The inspection team size, the time spent on
inspection tasks and the percent of errors found during
inspection can be set at the beginning of a model
simulation according to historical metrics or altered during
the actual simulation.
In order to judge the impact that software inspections
have on software development cycle time, the software
inspection model must be integrated into a model of the
software development process. Once integrated, the
software inspection model will impact a number of
elements in the software development process model.
Figures 2 and 3 are an incomplete, but representative view
of the integrated model. Figure 2 represents the process
steps and effort involved in inspecting work products.
Figure 2, however, does not reveal how time and
manpower are allocated to perform each step in the
inspection process, in order to keep the diagram and ideas
presented simple. Each rate in Figure 2 requires that
manpower be consumed in order to move work products
from one step to the next. Figure 3 shows an
incomplete, but representative implementation of the
interface between the base model of the software
development process and the process improvement model,
that is shown abstractly in Figure 1. Figure 3 represents
the modeling of errors in the base process model of
software development and illustrates the impact
inspections have on error generation and error detection in
the base process model. The impacts that software
inspections have on software development are: software
inspections consume development man-hours, errors are
less expensive to find during software inspection than
system testing, software inspections promote defect
prevention and software inspections reduce the amount of
error regeneration. Before discussing Figure 2 and Figure

The time spent on each inspection task per
unit of work to be inspected (e.g., the
preparation rate and the inspection rate);
The size of the inspection team;
The percent of errors found during inspection;
The percent of tasks that undergo reinspection;

and
The defect prevention attributable to the use of
inspections.
Our software inspection model is based on the following
assumptions:
Time allocated to software inspections takes
time away from software development;
A larger inspection team will consume more
man-hours per inspection than a smaller team;
Software inspections find a high percentage of
errors early in the development life cycle; and
The use of inspections can lead to defect
prevention, because developers get early
feedback as to the types of mistakes they are
making.

322

used for other activities, such as writing software. The
amount of time that an inspection consumes is based on
the size of the inspectiom team and the time spent on
inspection tasks, e.g., the preparation rate and the
inspection rate. Figure 2 is a simplified view of the steps
that must be taken to perform inspections. It implicitly
models the effort expended to move work products
through all of the process steps associated with
inspection; effort that takes time away from development
activities.
The second impact that software inspections have on
software development is in the detection of errors. Errors
are found early in the life cycle. Errors are less expensive
to find and correct during development than during testing.
This impact is not explicitly shown in Figure 3, except
that a higher error detection rate will increase the number
of errors found early in the life cycle, rather than later
during testing.
The third impact that software inspections have on
software development is in the prevention of defects.
Successful inspections attack the generation of future
defects. Developers that are involved with inspections
learn about the types of errors that they have been making
and are likely to make during the project. This feedback
about the errors they are making leads to fewer errors
being made during the project. Figure 3 shows defect
prevention, due to inspections, impacting the rate of error
generation.
The fourth impact that software inspections have on
software development is a reduction in the regeneration of
errors. Errors that remain undetected often lead to new
errors being generated. For example, undetected design
errors will lead to coding errors, because they are coding
to a flawed design. Software inspections detect errors
early in the life cycle, thus reducing the amount of error
regeneration. Figure 3 indicates that the errors escaping
detection impact the error generation rate. The number of
errors escaping detection is dependent on the pireparation
and inspection rates, as shown in Figure 3.
The four impacts that software inspections have on
software development, mentioned above, are the
foundation for the theory upon which the software
inspection model is based. Many details of the
implementation of the theory, using system dynamics
techniques, are abseint from Figure 2 and Figure 3, but are
shown in detail elsewhere [ 141.

3, a brief discussion of flow diagrams, a notation used to
represent system dynamics models, is in order.

Figure 2. Flow diagram of simplified
inspection process steps.

Potentrally Detestable Errors

Errors Ercrpmg Detection

Figure 3.

Detected Errors

Reworked Errors

iospestrm vat@

Flow diagram of inspection's
impact on errors.

Flow diagrams are composed of levels, material
flows, rates, auxiliaries and information links. Levels,
depicted as rectangles, represent the accumulation of a
material in a system. For example, work products and
errors are materials that reside in levels in Figure 2 and
Figure 3. Material flows, depicted as hollow arrows,
indicated the ability for material to flow from one level to
another. Material flows connected to clouds indicate a
flow to, or from, a portion of the system not pictured.
Rates, depicted as circles attached to material flows,
represent the rate of material flow into and out of a level.
For example, error detection rate in Figure 3 controls the
rate at which potentially detectable errors are detected.
Auxiliaries, depicted as circles, aid in the formulation of
rate equations. In Figure 3, defect prevention is an
auxiliary that affects the error generation rate.
Information links, depicted as arrows, indicate the flow of
information in a system. Information about levels, rates
and auxiliaries are transferred using information links.
Information links, unlike material flows, do not affect the
contents of a level.
The first impact that software inspections have on
software development is i n the allocation of man-hours
for development. Software inspections require that time
be set aside for them to be done properly. The time
consumed by software inspections is time that cannot be

3.2: Example model output
The integrated model has been turned into a simple
management flight simulator, allowing for simple
experimentation. This section describes the user interface
of the simulator and the output generated by its use.

323

being developed is estimated to be 64,000 lines of code
requiring a total workforce of eight developers at the
height of development. Two scenarios of the project
development are simulated holding all variables fixed,
except for the size of the inspection team and the percent
of errors found during inspection.
Figure 5 is the output generated by executing the
model with an inspection team size of six developers
discovering 40 percent of the errors during inspection.
When interpreting the graphical output, the story of the
project is revealed. From Figure 5 , the following story
emerges. Curve 1, the currently perceived job size in
work products, reveals that the project size was initially
underestimated. As development progressed, the true size
of the project was revealed. Curve 5 , the scheduled
completion date, was not adjusted even as it became
apparent that the project had grown in size. Instead, curve
4, the total size of workforce, indicates that the workforce
was increased in size. In addition, though not shown on
this graph, the workforce worked longer hours to bring
the project back on schedule. Curve 2, cumulative work
products developed, reveals that the project appeared to be
back on schedule, because there were no visible delays in
development of work products. It was not until system
testing that problems in development were discovered.
Curve 3, cumulative work products tested, reveals that
system testing did not go as smoothly as expected. The
poor performance of the inspection team pushed the
discovery of errors back to system testing. During
system testing it was revealed that there was a good
amount of rework to be done and as a result, the scheduled
completion date, curve 5, was once again pushed back.

Figure 4 shows the simple interface to the simulator.
Input to the simulator is in the form of sliders. The
simple interface has just five slider inputs: an on/off
switch for inspections, the size of the inspection team,
the percent of errors found during inspection, the percent
of work products that fail inspection and the percent of
incorrect error fixes.
@
l
J

1 Job Size m WP

2 - W Dweloped

3 WP Tested

4 Total Workfar-

5 C a n p k t i o n Oat.

. .

150000,

II

...........................................................

.........................
5'

250.00

........

0 00
125 00

250 00

375

I
[Rc.tor.l

insprotion. on\off

00

IsPrtore]

I
I

WP Completed
ManDayr

L

mspecimn

2.0

8

Figure 4.

0.0

-

team size =

[TI
1

9 0w

=
p r s m t fall

-

1/28/95

1

7

E-

1.0

prcent Of errors fW"d =

1,056
3,192

4

w
4 Z9PM

Day 5

II

1 [TJ

o0#-=1o

0

D@percent bad fixes =

v

9 .ow
0-:-

I?]
T-J

User interface of the inspection
simulator.

Output from the simulator comes in two forms:
numeric displays and graphs. Numeric displays show the
current value of a simulation variable. Man-Days and
Work Products Completed are two examples of numeric
displays. Graphs, on the other hand, display the value of
simulation variables versus time. Each output curve is
labeled with a number for ease of reading. There may be
multiple units of measure on the vertical axis, each
matched to the number of the curve it is representing.
The unit of measure on the horizontal axis is days. The
five output curves represent: 1) currently perceived job
size in terms of work products, 2 ) cumulative work
products developed, 3) cumulative work products tested, 4)
total size of workforce and 5) scheduled completion date.
A demonstration of the use of the system dynamic
models for predicting the cycle time reduction due to a
process improvement is in order. Using the integrated
model of the baseline waterfall development life cycle and
the software inspection process improvement, it will be
shown how this modeling technique can be used for
evaluating the impact that a proposed process
improvement would have on development cycle time.
The following demonstration is a simulation of a
hypothetical software team employing the simple
inspection model presented in this paper. The project

p

1I

1 bt SYe

n WP

* WP DDvelOped

3. WP issled

I Total

W0MO.S.

5 cemp,s,,on Dale

...........................................................................................

75-1

lis5

Figure 5.

Software inspection scenario 1.

Figure 6 is the output generated by executing the
model with an inspection team size of three developers
discovering 90 percent of the errors during inspection.
The story is much the same as that shown in Figure 5.
The big difference between Figures 5 and 6 is shown by
curve 3, cumulative work products tested. Using more
effective software inspections, this project was able to
324

Approach, Prentice-Hall, Englewood Cliiffs, New
Jersey, 1991.

discover errors early in the life cycle and correct them for
much less cost than if they had been found in system test.
In addition, there were n o major surprises in system
testing as t o the quality o f the product developed.
Therefore, with n o major a m o u n t o f r e w o r k t o b e
performed in system test, the project was able t o finish
close to its revised schedule.

Tarek K. Abdel-Hamid, "THINKING IN CIRCLES,"
American Programmer, May 1993, pp. 3-9.
Barry W. Boehm, SOFTWARE ENGINEERING
ECONOMICS., Prentice-Hall, Englewood Cliffs, New
Jersey, 1981.
Ken W. Collier and James S. Collofello, "Issues in
Software Cycle Time Reduction," International
Phoenix C o n f e r e n c e on C o m p u t e r s a n d
Communicaticm, 1995.
Raymond D i m , "Process Improvement and the
Corporate Balance Sheet," IEEE Software, July 1993,
pp. 28-35.
Jay W. Forrester, Industrial Dvnamics, The M.I.T.
Press, Cambridge, MA, 1961.

Figure 6.

Software inspection scenario 2.

Jay W. Forrester and Peter M. Senge, "Tests for
Building Confidence in System Dynamics Models,"
Svstem Dvnarn&TIMS Studies in Management
Sciences, 14 (19&0),pp. 209-228.

4: Conclusions and future work
O u r research grew o u t of the questions posed in
S e c t i o n 1.2 c o n c e r n i n g t h e i m p a c t o f process
improvements on software development cycle time. O u r
approach in answering those questions has been t o use
system d y n a m i c s m o d e l i n g t o m o d e l t h e s o f t w a r e
development process, allowing experimentation with the
system. W e have tried to demonstrate how this technique
may be used to evaluate the effectiveness of process
improvements. A t this point in our work w e h a v e
developed a base model of the waterfall development life
cycle and a process improvement model of software
inspections. W e p l a n t o c o n t i n u e this effort b y
developing a base model of the incremental development
process and creating a library of process improvement
models. S o m e examples of process improvements that
w e plan to add t o our library are meetingless inspections,
software reuse and risk management. W e then plan to
validate o u r base and process improvement models i n
several software development organizations. Finally,
another area of future research is to generalize the interface
between the base process models of software development
and the models of process improvements. T h e interface is
analogous to tool interfaces. A well defined, generalized
interface would facilitate integration of base process
models with process improvement models.

Mark C. Paulk, Bill Curtis, Mary Beth Chrissis and
Charles V. Weber, "Capability Maturity Model,
Version 1.1," IEElE Software, July 1993, pp. 18-27.
T. S. Perry, "Teamwork plus Technology Cuts
Development 'Time," IEEE Spectrum, October 1990,
pp. 61-67.
Lawrence H. Putrrami, "General empirical solution to
the macro software sizing and estimating problem,"
IEEE Transactions o n S o f t w a r e , Vol. SE4, NO. 4, July 1978, pp. 345-361.
George P. Richardson and Alexander L. Pugh 111,
Introduction t o A S t e m Dvnamics Modeling with
DYNAMO, The ML1.T. Press, Cambridge, MA, 1981.
Howard A. Rubin, Margaret Johnson and Ed Yourdon,
"With the SEI as ]My Copilot Using Software Process
'Flight Simulation' to Predict the Impact of
Improvements in Process Maturity," A m e r i c a n
Programmer, Septemlber 1994, pp. 50-57.
George Stark, Robert C. Durst and C. W. Vowell,
"Using Metricir in Management Decision Making,"
IEEE Computeir, September 1994, pp. 42-48.
John D. Tvedt, "A System Dynamics Model of the
Software Inspection ]Process," Technical Report TR95-007, Computer Science and Engineering
Department, Arizona State University, Tempe,
Arizona, 1995.

References
[I]

Tarek Abdel-Hamid and Stuart E. Madnick,
SOFTWARE PROJECT DYNAMICS An Integrated

325

RIPPLE EFFECT ANALYSIS OF SOFTWARE MAINTENANCE*

S. S . Yau, J. S. C o l l o f e l l o and I:. MacGregor
Department of E l e c t r i c a l Engineering and Computer Science
Northwestern University
Evanston, I l l i n o i s 60201
Maintenance of l a r g e - s c a l e software systems i s
Large-scale s o f t ware systems o f t e n possess both a s e t of functiona l and performance requirements. Thus, i t i s i m p o r t a n t f o r maintenance personnel t o consider t h e
r a m i f i c a t i o n s of a proposed program modification
from both a f u n c t i o n a l and a performance perspect i v e . I n t h i s paper t h e r i p p l e e f f e c t which res u l t s as a consequence of program modification w i l l
b e analyzed,
A technique is developed t o analyze
t h i s r i p p l e e f f e c t from both f u n c t i o n a l and p e r formance p e r s p e c t i v e s . A figure-of-merit i s then
proposed t o estimate t h e complexity of program
modification. Th.is f i g u r e can be used a s a b a s i s
upon which v a r i o u s m o d i f i c a t i o n s can be evaluated.

shows a r e l a t i o n s h i p between t h e budget whLch
bounds t h e t o t a l a c t i v i t y of e r r o r e l i m i n a t i o n a n d
documentation p l u s o t h e r miscellaneous a c t i o n s and
program complexity. These models are admittedly
incomplete, b u t they do however r e p r e s e n t a s i g n i f i c a n t s t a r t i n g p o i n t i n t r y i n g t o understand
the n a t u r e of software maintenance.

a complex and expensive process.

One of t h e important conclusions made by
Belady and Lehman i s t h a t program compleldty w i l l
always remain a monotonically i n c r e a s i n g f u n c t i c n
of t i m e p r i m a r i l y because of maintenance a c t i v i t y .
Thus, t h e primary t h r u s t i n performing software
maintenance r e s e a r c h should b e t o develop b e t t e r
maintenance techniques s o t h a t unexpected s i d e
e f f e c t s t h a t u s u a l l y g r e a t l y i n c r e a s e t h e complcbxi t y of the modified program w i l l n o t b e introduced.

INTRODUCTION
I n t h i s paper, we s h a l l p r e s e n t a software
maintenance technique f o r h e l p i n g t h e maintenance
programmer t o d e a l w i t h t h e r i p p l e e f f e c t from t:he
l o c a t i o n of t h e m o d i f i c a t i o n t o t h e o t h e r p a r t s of
the system t h a t a r e a f f e c t e d by t h e m o d i f i c a t i o n .
Ripple e f f e c t i s t h e phenomena by which changes t o
one pro:gtam area have tendencies t o be f e l t i n
o t h e r program a r e a s 6 . One a s p e c t of t h i s r i p p l e
e f f e c t is l o g i c a l i n n a t u r e , It i n v o l v e s i d e n t i f y i n g program a r e a s which r e q u i r e a d d i t i o n a l maintenance a c t i v i t y t o i n s u r e their consistency w i t h the
i n i t i a l change. Another a s p e c t of t h i s r i p p l e
e f f e c t concerns t h e performance of t h e system. It
involves analyzing changes t o one program a r e a
which may a f f e c t t h e performance of o t h e r progrnm
a r e a s , The maintenance technique presented i n t h i s
paper analyzes t h i s r i p p l e e f f e c t from both t h e
l o g i c a l and performance p e r s p e c t i v e s . T h i s i s requfred s i n c e a l a r g e - s c a l e program u s u a l l y has both
f u n c t i o n a l and performance requirements which m u s t
be preserved by t h e maintenance a c t i v i t y .

It is w e l l known t h a t t h e maintenance c o s t of

l a r g e - s c a l e software systems h a s been continuously
i n c r e a s i n g and i t : has become t h e s i n g l e dominant
c o s t i t e m during a large-scale software system's
l i f e cycle. Although maintenance a c t i v i t i e s have
n o t been c l e a r l y defined - one may consider maintenance a c t i v i t i e s t o i n c l u d e t h e processes of corr e c t i n g software e r r o r s , improving performance and
accommodating new c a p a b i l i t i e s a f t e r t h e system i s
~ p e r a t i o n a l l --~ estimates of maintenance c o s t have
ranged from 40%4 t o 67%5 of t h e t o t a l c o s t during a
l a r g e - s c a l e software systems' l i f e c y c l e . It i s
obvious t h a t i n o r d e r t o reduce high c o s t of s o f t ware, t h e most e E f e c t i v e way i s t o understand t h e
n a t u r e of software maintenance and develop more
e f f i c i e n t maintenance techniques.
Although very l i t t l e r e s e a r c h has been done i n
t h i s area, a s i g n i f i c a n t r e s u l t developed by Belady
and Lehman2 i s t h e two models f o r understanding t h e
n a t u r e of software maintenance. The f i r s t model
a i d s i n understanding t h e i n t e r n a l d i s t r i b u t i o n and
propagation of e r r o r s i n t h e program. I n t h i s
model, e r r o r s are e l i m i n a t e d from t h e program
through e f f o r t s of the programming team. The only
purpose of t h e team i s t o e l i m i n a t e e r r o r s i n t h e
program. The second model expands t h e r o l e of t h e
team t o make d e c i s i o n s a s t o what a c t i o n s (e.g.
communications, a d m i n i s t r a t i o n , e r r o r c o r r e c t i o n ,
documentation, e t c . ) should be performed. Model 1
i s a measure of complexity due t o aging. Model 2

The maintenance technique based on r i p p l e
e f f e c t a n a l y s i s can b e a powerful t o o l f o r maintenance p r a c t i t i o n e r s . It can h e l p them unders t a n d t h e scope of e f f e c t of t h e i r changes on t h e
program, It a l s o a i d s them i n determining what
p a r t s clf t h e program must b e checked f o r c o n s i s t ency. The n e t r e s u l t s of applying our t e c h n i q t e

are:
'Smoother implementation of changes is made.
*Eewer f a u l t s are i n j e c t e d i n t o t h e program
d u r i n g t h e changes.
.Less degradation (due t o i n c r e a s e d unders t a n d i n g ) of program s t r u c t u r e w i l l o c c u r .
*?'lie growth rate of c o m p l e d t y decreases.

*This work was supported by Rome Air Development
Center, U.S. Air Force, under Contract No. F3060276-C-0397.

60
CH1339-3/78/0000-00h0~00.7~ a
1
9
7
8 IEEE

C

.&erall program's o p e r a t i n g l i f e i s extended.

C MODULE MAIN
C SOLUTION OF THE QUADRATIC EQUATION
C A*X*X
B*XK = 0
COMMON X R l , X R Z , X I
READ 100 (A,B,C)
100 FORMAT (3F10.4)

Another s i g n i f f c a n t product of our a n a l y s i s
of r i p p l e e f f e c t is the computation of t h e comp l e x i t y of a proposed program m o d i f i c a t i o n . The
complexity proposed i n t h i s paper provides a
measure which r e f l e c t s the amount of work i n volved i n performing maintenance and, t h u s , prov i d e s a s t a n d a r d on w h i c h comparisons of modificat i o n s can b e made.

+

X 1 = -B/(Z.*A)
XR = X l * X l
DISC = XR - C / A
CSID = X l * X l - C I A
CALL RROOTS (CSID,DISC,Xl)
WRITE 100 XR1,XR2,XI
END

It should be noted that t h e maintenance technique presented i n this paper i s independent of
t h e language used i n t h e program and a p p l i c a b l e t o
e x i s t i n g programs as w e l l as newly implemented
programs i n c o r p o r a t i n g s t a t e - o f - t h e - a r t d e s i g n
techniques. The technique does not provide maintenance personnel w i t h proposals f o r modifying the
program. I n s t e a d , the technique is a p p l i e d a f t e r
t h e maintenance personnel have generated a number
of p o s s i b l e maintenance p r o p o s a l s . The complexity
of m o d i f i c a t i o n can then b e computed f o r each of
t h e proposed program m o d i f i c a t i o n s and t h e maintenance personnel can then s e l e c t t h e b e s t modificat i o n from b o t h a l o g i c a l and performance perspect i v e . The changes f e l t i n o t h e r program a r e a s a s a
r e s u l t of t h e m o d i f i c a t i o n can then be made cons i s t e n t by a n a l y s i s of t h e r i p p l e e f f e c t .

C
C MODULE RROOTS
SUBROUTINE RROOTS (CSID,DISC,Xl)
COMMON X R l , X R Z , X I
I F (DISC.LT.0) GOTO 10
X 2 = SQRT(D1SC)
XR1 = x1 + x2
XR2 = x1 - x2
X I = 0.
10 CONTINUE
CALL IROOTS (CSID,DICS,Xl)
RETURN
C
C MODULE IROOTS
SUBROUTINE IROOTS (CSID,DISC,Xl)
COMMON X R l , X R Z , X I
I F (CSID.GE.0) GOTO 10
X2 = SQRT (-DISC)

LOGICAL RIPPLE EFFECT
Maintenance i n v o l v e s changes of a program t o
c o r r e c t e r r o r s , improve e f f i c i e n c y , and extend
t h e program's c a p a b i l i t y . When making a program
m o d i f i c a t i o q t h e maintenance p r a c t i t i o n e r tries t o
choose a s o l u t i o n which i s compatible w i t h t h e rest
of t h e program. A change of t h i s s o r t would t r y t o
use e x i s t i n g program v a r i a b l e s when r e a l i z i n g the
modification. Unfortunately, t h e p r a c t i t i o n e r canRot always choose a s o l u t i o n which i s compatible.
I n s t e a d of u s i n g program v a r i a b l e s t o r e a l i z e a
s o l u t i o n , t h e p r a c t i t i o n e r might be f o r c e d t o red e f i n e s e v e r a l program v a r i a b l e s i n o r d e r t o i n t r o duce an a c c e p t a b l e s o l u t i o n . S o l u t i o n s of t h i s
type tend t o spawn e r r o r s i n o t h e r program a r e a s ,
because t h e maintenance p r a c t i t i o n e r f a i l s t o
check a l l p o r t i o n s of t h e program a f f e c t e d by
t h e i n i t i a l change t o i n s u r e t h e i r c o n s i s t e n c y .
Program areas which are i n c o n s i s t e n t w i t h an
i n i t i a l change c o n t a i n p o t e n t i a l e r r o r s .

x R 1 = x1
xR2 = x1
X I = x2

10 CONTINUE
RETURN

F i g . 1. An Example f o r I l l u s t r a t i n g
Logical Ripple E f f e c t

IROOTS i s d e f i n e d as l o g i c a l r i p p l e e f f e c t . For
s m a l l programs, such as t h a t shown i n F i g . 1,
l o g i c a l r i p p l e e f f e c t can be completely accounted
f o r by manually scanning t h e program t o i d e n t i f y
a l l p o t e n t i a l e r r o r s which r e q u i r e a d d i t i o n a l
maintenance a c t i v i t y t o i n s u r e t h e i r c o m p a t i b i l i t y
w i t h t h e i n i t i a l m o d i f i c a t i o n . For l a r g e s c a l e
programs composed of many modules, manually scanning of the program is slow, t i m e consuming,
c o s t l y and may r e s u l t i n some p o t e n t i a l e r r o r s bei n g undetected. I d e n t i f i c a t i o n of program a r e a s
c o n t a i n i n g p o t e n t i a l e r r o r s can b e automated
u s l n g e r r o r flow a n a l y s i s , where a l l program
v a r i a b l e d e f i n i t i o n s involved i n the i n i t i a l modif i c a t i o n r e p r e s e n t primary e r r o r s o u r c e s from
which i n c o n s i s t e n c y propagates t o o t h e r program
areas. I n essence, those program v a r i a b l e d e f i n i t i o n s are t h e s o u r c e s from which p o t e n t i a l e r r o r s
flow. By knowing t h e primary e r r o r s o u r c e s i n volved i n a program m o d i f i c a t i o n and t h e program's
e r r o r c h a r a c t e r i s t i c s , program areas a f f e c t e d by
l o g i c a l r i p p l e e f f e c t can be i d e n t i f i e d . These
a r e a s r e q u i r e a d d i t i o n a l maintenance a c t i v i t y .

To i l l u s t r a t e t h i s concept, l e t us assume t h a t
a m o d i f i c a t i o n i s made t o t h e program shown i n
F i g . 1. L e t the r e s u l t of this m o d i f i c a t i o n redef i n e t h e v a r i a b l e d e f i n i t i o n CSID t o X l * X l .
To
complete the m o d i f i c a t i o n , the p r a c t i t i o n e r must
examine a l l o t h e r program a r e a s f o r i n c o n s i s t e n c y
caused by CSID. A s c a n of the program r e v e a l s that
c o n t r o l d e f i n i t i o n (CSID.GE.0) i n module IROOTS can
be i n c o n s i s t e n t w i t h t h e new d e f i n i t i o n of CSID.
The maintenance p r a c t i t i o n e r must examine
(CSID.GE.0) t o i n s u r e t h a t i t i s compatible w i t h
t h e i n i t i a l m o d i f i c a t i o n . F a i l u r e t o supply t h i s
a d d i t i o n a l maintenance e f f o r t r e s u l t s i n
(CSID.GE.0) being spawned as a p o t e n t i a l e r r o r .
The phenomena by which a change t o CSID i n the
module a f f e c t s c o n t r o l d e f i n i t i o n (CSID.GE.0) i n

61

XZ=SQRT(-DISC) , XR1=X1, XRZ=Xl, and XI=X:!.
Le rital a n a l y s i s produces t h e following c h a r a c t e r i aat i o n sets: C ={X2,XRl,XR2,XI], P ={DISC,Xl), m d
flow mapping ${X2,XIkf (DISC); {Xkl,XRZ]+f(XI) 1
The source capable s e t , p o t e n t i a l propagator set,
and flow mapping d e f i n e t h e e r r o r flow propert:.es
of the block. The e r r o r flow p r o p e r t i e s of vai'io w blocks i n t h e module i n conjunction with
c o n t r o l flow graph of the module form a charact e r i c a t i o n used by S t a g e 2 t o compute r i p p l e
effect

I d e n t i f i c a t i o n of a f f e c t e d program a r e a s i s
made by i n t e r n a l l y t r a c k i n g each primary e r r o r
source and i t s r e s p e c t i v e secondary e r r o r sources
w i t h i n t h e r e s p e c t i v e module t o a p o i n t of e x i t .
A secondary e r r o r source is a v a r i a b l e o r c o n t r o l
d e f i n i t i o n implicated through t h e usage o f a p r i mary e r r o r s o u r c e . Secondary e r r o r sources must
be examined t o i n s u r e that they are n o t i n c o n s i s t e n t w i t h the v a r i a b l e s involved i n t h e i n i t i a l
change. A p o i n t of e x i t i n a module e x i s t s a t a
p l a c e i n t h e module where another module i s i n voked o r a t i t s normal termination p o i n t . A t
each p o i n t of e x i t , a determination i s made as t o
which primary and secondary e r r o r sources propagate
a c r o s s module boundaries. Those e r r o r s o u r c e s
which propagate a c r o s s module boundaries become
primary e r r o r sources w i t h i n t h o s e modules whose
e f f e c t s must be t r a c e d . Tracing and propagation
c o n t i n u e u n t i l no new secondary e r r o r s o u r c e s are
created.

.

.

Stage 2 of t h e f u n c t i o n a l processing; a p p l i e s
an a l g o r i t h m t o compute r i p p l e e f f e c t . The al-.
gorithm o p e r a t e s upon each module c h a r a c t e r i z a t i o n
t o t r a c e e r r o r sources from t h e i r p o i n t s of d e f i n i t i o n t o t h e i r e x i t p o i n t s . The a l g o r i t h m i s
i n i t i a l i z e d w i t h a s e t of modules and t h e i r p r i mary e r r o r s o u r c e s involved i n t h e i n i t i a l change.
These ,are s u p p l i e d by t h e maintenance p r a c t i t i o n e r . For each module M. i n i t i a l l y involved
i n t h e modification, t h e a j g o r i t h m t r a c e s t h e
intramodule flow of p o t e n t i a l e r r o r s from t h e
primary e r r o r s o u r c e s through t h e v a r i o u s program
b l o c k s . When t h e flow of e r r o r s o u r c e s s t a b i l i z e s , the a l g o r i t h m a p p l i e s a b l o c k i d e n t i f i c a t i o n c r i t e r i o n t o determine which b l o c k s w i t h i n
t h e module must b e examined t o i n s u r e t h a t they
are not i n c o n s i s t e n t w i t h t h e i n i t i a l change.
A f t e r block i d e n t i f i c a t i o n i s complete, a propag a t i o n c r i t e r i o n is a p p l i e d t o module M j t o de-.
f i n e t h o s e e r r o r s o u r c e s which flow from M j t o
o t h e r modules which M invokes, and t o modules
j
which invoke Mj. E r r o r flow a c r o s s module
boundaries c o n s t i t u t e s intermodule e r r o r flow.
For each module a f f e c t e d by intermodule e r r o r
flow, the a l g o r i t h m t r a c e s intramodule e r r o r
flow i n t h e same manner a s f o r Mj t o determine
the net: e f f e c t t h a t t h e propagated e r r o r s o u r c e s
have on t h e i r r e s p e c t i v e modules. The algorit1.m
executes i n t h i s manner u n t i l intermodule e r r o r
flow s t a b i l i z e s , An i n t e r m e d i a t e r e s u l t which
exists a t t h i s p o i n t is t h e s e t of modules i n
t h e program which are a f f e c t e d by t h e i n t e r m o d r l e
flow of e r r o r sources c r e a t e d by t h e primary
e r r o r s o u r c e s involved i n t h e change. The a l gorithm completes i t s e x e c u t i o n by applying a
r i p p l e e f f e c t c r i t e r i o n t o each module a f f e c t e d
by intermodule e r r o r flow t o determine i f t h e
module r e q u i r e s a d d i t i o n a l maintenance a c t i v i t y
t o i n s u r e t h a t t h e module i s n o t i n c o n s i s t e n t
w i t h t h e i n i t i a l change. I n t h i s example, both
modules RROOTS and IROOTS a r e a f f e c t e d by the
intermodule e r r o r flow: however, only IROOTS i s
a f f e c t e d by r i p p l e e f f e c t .

To i l l u s t r a t e t h e s e concepts, l e t u s reexamine t h e program shown i n F i g . 1. V a r i a b l e
d e f i n i t i o n CSID i s a primary e r r o r source which can
be t r a c e d through module M A I N without c r e a t i n g any
new secondary e r r o r s o u r c e s . A t t h e e x i t p o i n t of
MAIN, which is t h e i n v o c a t i o n c a l l t o RROOTS, a
determination i s made by examining RROOTS invocat i o n c a l l t h a t primary e r r o r source C S I D propagates
t o RROOTS. CSID becomes a primary e r r o r source
a s s o c i a t e d w i t h RROOTS whose e f f e c t must be t r a c e d .
Tracing CSID through RROOTS does not cause any
secondary e r r o r s o u r c e s t o be c r e a t e d . A t an e x i t
p o i n t of RROOTS, a d e t e r m i n a t i o n is made t h a t C S I D
propagates t o IROOTS. C S I D becomes a primary
e r r o r s o u r c e a s s o c i a t e d w i t h IROOTS. Tracing t h e
flow of CSID r e v e a l s t h a t C S I D c r e a t e s secondary
e r r o r s o u r c e (CSID.GE.0).
Tracing and propagat i o n h a l t because no new secondary e r r o r sources
can be c r e a t e d . C S I D i s a primary e r r o r s o u r c e .
(CSID.GE.0) i s a secondary e r r o r source which must
b e examined t o 3.nsure t h a t i t i s not i n c o n s i s t e n t
w i t h the i n i t i a l . m o d i f i c a t i o n .
Computation of l o g i c a l r i p p l e e f f e c t f o r a
modular program i s a complex process which r e q u i r e s
t h e completion of two f u n c t i o n a l s t a g e s of proc e s s i n g b e f o r e l o g i c a l r i p p l e can be computed.
Stage one, l e x i c a l a n a l y s i s , produces t h e b a s i s
which is used by t h e second s t a g e f o r computing
l o g i c a l r i p p l e e f f e c t . For each module i n a precedence o r d e r defined from the program's i n v o c a t i o n
graph, t h e module's t e x t is s t a t i c a l l y scanned t o
produce a c o n t r o l flow graph based on program
blocks7. A program block is a maximal set of o r dered s t a t e m e n t s such t h a t i t i s always executed
from t h e f i r s t statement t o t h e l a s t s t a t e m e n t and
t h a t a l l t h e s t a t e m e n t s are executed i f one of
them is executed. L e x i c a l a n a l y s i s c h a r a c t e r i z e s
each program block vi i n terms of i t s source capab l e s e t C , i t s p o t e n t i a l propagator s e t P . , and a
flow mappfng C .
f ( P . 1 . C . i s t h e s e t of'definit i o n s i n block'v
whi$h c a d e p o t e n t i a l e r r o r t o
i
e x i s t w i t h i n and flow from vi.
P . i s t h e s e t of
a l l usages i n v . which can cause &lements i n t h e
source capable :et t o flow from v . . To i l l u s t r a t e
t h i s c h a r a c t e r i z a t i o n , consider the block of statements i n module IROOTS shown i n F i g . 1 composed of

PERFORMANCE RIPPLE EFFECT
Analysis of performance r i p p l e e f f e c t r e q u i r e s t h e i d e n t i f i c a t i o n of modules whose performance may change as a consequence of s o f t w a r e
m o d i f i c a t i o n s . This i s a complex t a s k . The i c e n t i f k a t i o n is complicated by t h e f a c t t h a t performanc:e dependencies o f t e n e x i s t among modules
which a r e otherwise f u n c t i o n a l l y and l o g i c a l l y i n dependent. A performance dependency r e l a t i o n s h , i e
i s d e f i n e d t o exist from module A t o module B i f
and only i f a change i n module A can have an

f

62

r e s o u r c e c o n t e n t i o n t h a t can lead t o performance
degradation.
8. A b s t r a c t i o n s . The use of a b s t r a c t i o n s i s
a popular d e s i g n t o o l and adds t o t h e maintainab i l i t y of t h e system by h i d i n g d e s i g n d e c i s i o n s .
From the performance p e r s p e c t i v e of m a i n t a i n a b i l i t y , however, a b s t r a c t i o n s are " t r o j a n h o r s e s . "
i s i s because a change i n the implementation of
the a b s t r a c t i o n w i l l very l i k e l y a f f e c t the performance of t h e a b s t r a c t i o n , and, t h u s , t h e p e r formance of a l l modules u t i l i z i n g t h e a b s t r a c t i o n .
This i s a c l a s s i c example of a c a s e where a modif i c a t i o n t o t h e software d u r i n g maintenance does
n o t produce any f u n c t i o n a l o r l o g i c a l changes, b u t
i t does r e s u l t i n performance changes.

e f f e c t on t h e performance of module B.
It is obvious t h a t when a l o g i c a l o r funct i o n a l e r r o r is discovered i n t h e s o f t w a r e , t
e r r o r can a f f e c t o t h e r modules. Analogously,
a performance change i s made, t h e scope of e f
of t h e change can be determined by examining t h
mechanisms by which t h i s change can a f f e c t o t h e r
modules. W e have i d e n t i f i e d ;he following e i g h t
mechanisms which may e x i s t i n large-scale programs
by which changes i n performance as a consequence
of a s o f t w a r e m o d i f i c a t i o n a r e propagated througho u t t h e program:
1. P a r a l l e l Execution. I n the maintenance
phase i t i s p o s s i b l e t o i n t r o d u c e software modific a t i o n s t o a module which can d e s t r o y i t s a b i l i t y
t o be executed w i t h o t h e r modules i n p a r a l l e l .
Major changes i n performance may r e s u l t due t o
execution d e l a y s and c o n t e n t i o n f o r r e s o u r c e s prev i o u s l y a l l e v i a t e d through t h e p a r a l l e l execution.
2. Shared Resources. When modules are
f o r c e d t o s h a r e r e s o u r c e s , t h e time when each
module r e q u e s t s and releases common r e s o u r c e s are
important performance parameters. I n a multiprogramming environment, performance degradation may
be experienced by modules whose execution i s being
a f f e c t e d by t h e d e n i a l of requested r e s o u r c e s
which a r e c u r r e n t l y dedicated t o o t h e r modules.
3. I n t e r p r o c e s s Communication. When one
module must send a message t o another module, t h e
performance of t h e module r e c e i v i n g t h e message i s
dependent upon when t h e message i s s e n t . Thus,
m o d i f i c a t i o n s t o t h e module sending t h e message
t h a t a l t e r t h e time when t h e message i s s e n t can
a f f e c t t h e performance of t h e module designated t o
r e c e i v e t h e message.
4 . Called Modules. Modifications t o modules
i n t h e maintenance phase can be divided i n t o two
types. A bounded m o d i f i c a t i o n t o a module i s a
m o d i f i c a t i o n which does n o t a l t e r t h e performance
of t h e module. An unbounded m o d i f i c a t i o n t o a
module i s a m o d i f i c a t i o n which a l t e r s t h e performance of t h e module. An unbounded m o d i f i c a t i o n t o
a c a l l e d module w i l l a f f e c t t h e performance of
a l l modules c a l l i n g i t .
5 . Shared Data S t r u c t u r e s . Changes i n t h e
c o n t e n t s of s h a r e d d a t a s t r u c t u r e s can a f f e c t t h e
performance of o t h e r modules u t i l i z i n g t h e shared
d a t a s t r u c t u r e . The b a s i c dynamic a t t r i b u t e s cont r i b u t i n g t o performance i n t h i s area a r e a modu l e ' s s t o r a g e and r e t r i e v a l times f o r e n t r i e s i n
the data structures.
6 . S e n s i t i v i t y t o t h e Rate of I n p u t .
Changes i n i n p u t rates t o a process can have major
r e p e r c u s s i o n s i n terms of i t s f u n c t i o n a l and performance requirements. For example, i t can l e a d t o
t o s a t u r a t i o n and p o s s i b l y overflow of d a t a s t r u c t u r e s involved w i t h t h e processing of t h e i n p u t .
The i n c r e a s e d frequency of i n p u t a r r i v a l s may a l s o
l e a d t o i n t e r r u p t i o n s i n processing which can l e a d
t o both f u n c t i o n a l and performance requirement
violations.
7. Execution P r i o r i t i e s . During t h e maintenance phase, i t i s important f o r t h e maintenance
p r a c t i t i o n e r t o recognize t h e e f f e c t of a proposed
modification w i t h r e s p e c t t o t h e e x i s t i n g p r i o r i t i e s i n t h e system. Modification of e x i s t i n g p r i o r i t i e s can c r e a t e c o n f l i c t s i n t h e system such as

Performance a t t r i b u t e s of a program are def i n e d as a t t r i b u t e s corresponding t o measurements
of key a s p e c t s o f t h e execution of t h e program.
There i s a d i s t i n c t r e l a t i o n s h i p between performance a t t r i b u t e s and t h e e i g h t mechanisms f o r t h e
propagation of performance changes. The e i g h t
mechanisms o p e r a t e a s l i n k s between performance
a t t r i b u t e s of modules. I n o t h e r words, a change
i n a performance a t t r i b u t e of one module can a f f e c t
a performance a t t r i b u t e i n another module v i a one
of t h e e i g h t mechanisms.

We have i d e n t i f i e d the following 13 performance a t t r i b u t e s l i n k e d with t h e 8 mechanisms
discussed b e f o r e :
1. The a b i l i t y of the module t o e x e c u t e i n
p a r a l l e l w i t h another module.
2. For each r e s o u r c e i n c o n t e n t i o n , t h e rela t i v e t i m e t h a t t h e module s e i z e s t h e r e s o u r c e .
3. For each r e s o u r c e i n c o n t e n t i o n , the r e l a t i v e t i m e t h a t t h e module releases t h e r e s o u r c e .
4 . The r e l a t i v e time t h a t the module b e g i n s
execution.
5 . The r e l a t i v e time t h a t t h e module t r a n s m i t s a message t o another module.
6 . The execution t i m e of the module.
7 . For each r e s o u r c e u t i l i z e d i n the module,
the r e s o u r c e u t i l i z a t i o n by the module.
8. For each dependent i t e r a t i v e s t r u c t u r e i n
the module, the number of i t e r a t i o n s .
9 . For each d a t a s t r u c t u r e , t h e s t o r a g e and
r e t r i e v a l times f o r e n t r i e s i n the d a t a s t r u c t u r e .
10. For each d a t a s t r u c t u r e , t h e number of
entries i n the data structure.
11. For each d a t a s t r u c t u r e , the s e r v i c e t i m e
of an e n t r y i n t h e d a t a s t r u c t u r e , i . e . t h e relat i v e t i m e t h a t an e n t r y remains i n t h e d a t a
s t r u c t u r e before being serviced.
1 2 . For each dependent i t e r a t i v e s t r u c t u r e i n
the module c o n t a i n i n g overhead i n c u r r i n g r e f e r ences, t h e number o f i t e r a t i o n s .
13. The r a t e of i n p u t t o t h e module.
There i s a l s o a r e l a t i o n s h i p between t h e performance a t t r i b u t e s and t h e performance r e q u i r e ments of the program. The performance r e q u i r e ments can be decomposed q u a l i t a t i v e l y i n t o performance a t t r i b u t e s which c o n t r i b u t e t o the prese r v a t i o n o r v i o l a t i o n of t h e performance r e q u i r e ments. Thus, we can a s s o c i a t e a s e t of performance a t t r i b u t e s w i t h each performance requirement
s u c h t h a t i f a performance a t t r i b u t e i n the set i s

63

examp:te, P.A.2 and P.A.3 of Module A would be
a f f e c t e d . Thus, performance requirements P.R.l
and P.R.2 would have t o be r e t e s t e d t o i n s u r e t h a t
they have n o t been v i o l a t e d . I n a d d i t i o n , P.P .2
of Module B would be a f f e c t e d v i a mechanism 1
Thus, P.R.4 would a l s o have t o be r e t e s t e d t o ins u r e t h a t i t too h a s n o t been v i o l a t e d .

a f f e c t e d by a modification, then t h e performance
requirement a s s o c i a t e d w i t h t h i s s e t is a l s o
affected.
Since t h e performance a t t r i b u t e s of a program correspond t o measurements of key a s p e c t s of
t h e execution of the program, they can b e a f f e c t ed d u r i n g t h e maintenance process by modificat i o n s t o t h e program. A c r i t i c a l s e c t i o n of a
program can be a s s o c i a t e d w i t h each performance
a t t r i b u t e such t h a t i f t h i s c r i t i c a l s e c t i o n i s
modified, t h e corresponding performance a t t r i b u t e
may be a f f e c t e d . For example, i f t h e performance
a t t r i b u t e under c o n s i d e r a t i o n i s t h e execution
time between when a module b e g i n s e x e c u t i o n and
when i t t r a n s m i t s a message, t h e corresponding
c r i t i c a l s e c t i o n i s t h a t s e c t i o n of code between
module i n v o c a t i o n and t r a n s m i s s i o n of t h e message.

MAINTENANCE TECHNIQUE

'In t h i s s e c t i o n , we w i l l o u t l i n e 0c.r maintenance technique which analyzes both l o g i c a l arid
performance r i p p l e . The maintenance technique
analyzes t h e program w i t h r e s p e c t t o the proposed
m o d i f i c a t i o n i n o r d e r t o i d e n t i f y t h e program
block!; which may be a f f e c t e d by the modi.ficat:.on
and must be analyzed f o r c o n s i s t e n c y as w e l l its
t h e performance requirements which may Ece a f f e c t e d
by t h e m o d i f i c a t i o n . The maintenance technique
c o n s i s t s of t h e following s t e p s :
1. Perform l e x i c a l a n a l y s i s on a l l . modu:les
i n t h e program.
:2. Decompose t h e performance requi.remen::s
i n t o performance a t t r i b u t e s .
:3. I d e n t i f y a l l of the mechanisms f o r the
propagation o f performance changes, c r i t : i c a l sect i o n s , and performance a t t r i b u t e s i n the! program.
For each module i n i t i a l l y invol-ved i n t h e
14.
maintenance change, determine i t s primaiy e r r o r
sources,
5. Apply t h e r i p p l e e f f e c t a1gorit:hm t o comp u t e t h e s e t of b l o c k s i n each module a f f e c t e d by
ripple.
6 . Identify the c r i t i c a l sections i n i t i a l l y
involved i n t h e maintenance change.
I?. For each of t h e c r i t i c a l s e c t i o n s , itlent i f y the corresponding performance a t t r i b u t e s
affected.
U . U t i l i z i n g t h e performance dependency rel a t i o n s h i p s i n e x i s t e n c e i n the program, c o n s t r u c t
a r i p p l e l i s t of modules and t h e i r performancc?
a t t r i b u t e s which a r e a f f e c t e d by t h e modificai:ion.
!?. I d e n t i f y t h e performance requirements
which a r e a f f e c t e d by m o d i f i c a t i o n of t b e performance a t t r i b u t e s i d e n t i f i e d i n t h e previous s t e p .

The r e l a t i o n s h i p of performance a t t r i b u t e s ,
performance requirements, c r i t i c a l s e c t i o n s , and
t h e mechanisms f o r t h e propagation of performance
changes i n a program forms t h e b a s i s f o r t h e conc e p t of a performance change r i p p l e e f f e c t as a
consequence of s o f t w a r e m o d i f i c a t i o n . When a
c r i t i c a l s e c t i o n is modified, i t may a f f e c t t h e
corresponding performance a t t r i b u t e s . A change i n
t h e s e performance a t t r i b u t e s may then r i p p l e , i.e.
a f f e c t o t h e r performance a t t r i b u t e s v i a any a p p l i c a b l e mechanisms. This performance change r i p p l e
e f f e c t is i l l u s t r a t e d i n F i g . 2 .

'

MECHANISM 2

I

I

I

I

P.A.3

P.A.2

The maintenance technique can be automated
f o r l a r g e - s c a l e s o f t w a r e . A t p r e s e n t , w e are ref i n i n g the algorithms involved i n t h i s technicue.
The r e s u l t s w i l l be presented i n subsequent p s p e r s .
COMPLEXITY OF PROGRAM MODIFICATION
1:n t h i s s e c t i o n , w e w i l l propose an esti-

mate o f the complexity of program m o d i f i c a t i o r ,
w h i c h should r e f l e c t the amount of p r o g r a m e r ' s
e f f o r t required t o incorporate a p a r t i c u l a r piogram m o d i f i c a t i o n and t a k e c a r e of a l l i t s r i r p l e
effect:. T h i s f i g u r e can b e used as a b a s i s upon
which v a r i o u s program m o d i f i c a t i o n s can be evzlua t e d f n terms of programmer's e f f o r t .
I d e t us c o n s i d e r t h a t a m o d i f i c a t i o n R i s t o
b e macle on a module M j that w i l l r e q u i r e tRe
change:: of the set B . of b l o c k s i n M j without any
be the set
r i p p l e e f f e c t considA?ation. L e t
of modules that have t o be examined and p o s s i b l y
changed due t o nP o r i t s r i p p l e e f f e c t , and i t i n -

elP

I n this example, assume we are a t t e m p t i n g t o ident i f y the performance r i p p l e e f f e c t as a conseI n this
quence of modifying C . S . l of Module A.

64

L e t Ei b e the set of blocks i n module
cludes M..
$hat have eo b e examined and p o s s i b l y
Mi€ )I
chang& due t o n and i t s r i p p l e e f f e c t from b o t h
l o g i c a l and perfgrmance p o i n t s of view. Then, the
programmer's e f f o r t r e q u i r e d t o perform the modif i c a t i o n n on M j and t a k e c a r e o f a l l i t s r i p p l e
e f f e c t canebe e s t i m a t e d by t h e following expression:
G(Q.,B.
J
J P ) +M i $jp
~D(Qi)+F(Qi,Eip)+G(Qi,Eip)),

estimates, CD(Q.) and EF(Qi,Eip) c o n s t i t u t e most of
t h e p r o g r a m e r ' ; t o t a l e f f o r t . I n o t h e r words, t h e
amount of programmer's e f f o r t CG(Qi,Eip) r e q u i r e d
t o make t h e changes of t h e code i s only a small
p o r t i o n of t h e t o t a l e f f o r t , and i n some c a s e s i t
may be n e g l e c t e d i n e v a l u a t i n g v a r i o u s proposed
program m o d i f i c a t i o n s . Future work i s r e q u i r e d t o
e s t a b l i s h q u a n t i t a t i v e measures of t h e s e terms.

where Qiis t h e complexity of module M . , D(Qi) i s
t h e amount of p r o g r a m e r ' s e f f o r t t o u h e r s t a n d Mi
t h a t is a f u n c t i o n of Q
G(Q.,Bjp) is t h e programmer's e f f o r t f o r making t i e m o d i f i c a t i o n
),f o r t
F(Q , E ) and G(Qi,E. ) a r e t h e p r o g r a m e r ' s e
foriex%ining Ei ani'making t h e n e c e s s a r y changes
due t o rip's rippye e f f e c t i n Mi.

f n t h i s paper, w e have presented the framework of a maintenance technique f o r a n a l y z i n g t h e
r i p p l e e f f e c t of program m o d i f i c a t i o n s from both
l o g i c a l and performance p e r s p e c t i v e . This techn i q u e can be automated and s e r v e s a s a v a l u a b l e
tool f o r b o t h maintenance programmers and managers.
T h e d e t a i l s of t h e algorithms i n t h i s technique
w i l l b e presented i n subsequent papers. We are
a l s o i n t h e process of demonstrating t h e automat i o n of t h i s technique.

CONCLUSIONS

I f the original modification
involves t
modules, M$, k=1,2,. . , t , then t h e programmer's
e f f o r t r e q u i r e d t o perform t h e m o d i f i c a t i o n q and
t a k e c a r e of a l l i t s r i p p l e e f f e c t w i l l depen8 upon
t h e way t h e p r o g r a m e r doing t h e changes, such a s
whether t h e programmer w i l l perform the changes i n
Mk's
i n a p a r t i c u l a r sequence and t h e r i p p l e e f f e c t
J
f o r one module b e f o r e s t a r t i n g t h e changes i n t h e
next module i n t h e p a r t i c u l a r sequence, o r t h e programmer w i l l perform t h e changes i n M k ' s and t h e i r
r i p p l e e f f e c t i n some f a s h i o n i n v o l v i d g more than
one module a t a t i m e . For l a r g e - s c a l e programs, i n
order t o have b e t t e r chance t o avoid i n t r o d u c i n g
e r r o r s , i t is a good p r a c t i c e f o r t h e programmer t o
perform t h e changes i n Mk's and t h e i r r i p p l e e f f e c t
i n t h e former way. I n tiis case, t h e programmer's
e f f o r t r e q u i r e d t o perform t h e m o d i f i c a t i o n TI on
Mk, k=1,2,
t and t a k e care of a l l i t s r i p p t e
e2fect i n a p a r t i c u l a r sequence of Mk's can be
estimated by the following expressioA.

.

Much work s t i l l needs t o be done i n t h i s a r e a .
I n a d d i t i o n t o developing q u a n t i t a t i v e measures of
the programmer's e f f o r t described i n t h e l a s t sect i o n , w e a l s o need t o develop techniques f o r genera t i n g various modifications t o s a t i s f y specificat i o n changes.
REFERENCES

1. C , V. Ramamoorthy and S. B . Ho, "Testing Large
Software w i t h Automated Software Evaluation
Systems," IEEE Trans. on Software Engineering,
Vol. SE-1, No. 1, March, 1975, pp. 46-58.

...,

2.

L. A. Belady and M. M. Lehman, "A Model of
Large Program Development, " I B M System
Journa1,Vol. 15, No. 3, 1976, pp. 225-252.

3.

H. D. Mills, "Software Development,"

IEEE

Trans. on Software Engineering, Vol. SE-2,
No. 4, December, 1976, pp. 265-273.
4.

B. W. Boehm, "Software and Its Impact: A
Quantative Assessment," Datamation, May, 1973,
pp. 48-59,

where $se i s t h e s e t of modules t h a t have t o be examined and p o s s i b l y changed due t o t h e changes of
n p i n M$ and i t s r i p p l e e f f e c t , G(Qk,Ek ) is t h e
programmer's e f f o r t f o r performing ghe'thanges of
11 i n !M i n t h e p a r t i c u l a r sequence a f t e r a l l the
cl?angesJin preceding modules and t h e i r r i p p l e
e f f e c t have been taken care o f , and F(Qi,Ei 1 and
G(Qi,Eip) are t h e programmer's e f f o r t f o r etamini n g Eip and making t h e n e c e s s a r y changes due t o the
r i p p l e e f f e c t i n Mi f o r t h e o r i g i n a l changes i n Mk
J
i n t h e p a r t i c u l a r sequence.
It i s noted t h a t i n t h e s e estimates, the comp l e x i t y Q i of t h e modules Mi involved plays an
important r o l e i n t h e required programmer's e f f o r t .
The complexity of t h e program s t r u c t u r e above t h e
module l e v e l is not e x p l i c i t l y included i n any part i c u l a r term of these estimates, b u t i t i s implici t l y considered when we i d e n t i f y the r i p p l e e f f e c t
of program modification.
It i s a l s o noted t h a t among the terms i n these

65

5.

M. V. Zelkowitz, "Perspective on Software

6.

F . M. Haney, "Module Connection Analysis - A
Tool f o r Scheduling Software Debugging
A c t i v i t i e s , " Proc. 1972 F a l l J o i n t Computer
Conf., December, 1972, pp. 173-179.

7.

A. V. Ah0 and J . D. Ullman, P r i n c i p l e s of
Compiler Design, Ch. 1 2 , Addison-Wesley, 1977.

Ecgineering, I' ACM Computing Surveys , Vol. 10,
No. 2 , June, 1978, pp. 197-216.

Work in Progress: A Holistic Approach to ReEngineering the Freshmen Engineering Course
James Collofello, Amy Hall
Ira A. Fulton Schools of Engineering
Arizona State University
Abstract— Engineering schools must strive to evolve a new
paradigm for undergraduate education that recognizes the
evolution of the skills and learning styles of its incoming students
and prepares them to tackle society’s grand challenges of the
future, while at the same time increases the probability of their
success in their chosen engineering program. Most researchers
and experts in the field agree on some basic tenants of retention
[1-3], which include developing community amongst freshmen,
creating connections for freshmen through meaningful
interactions with returning students and faculty, engaging
freshmen in active learning environments, helping freshmen
understand and internalize the vision and mission of the school,
and assisting freshmen to develop a personal identity as an
Engineer. This paper describes the creation of new freshmen
engineering courses, taught in a new learning environment with a
new instructional model that addresses all of these important
tenants of retention.
Keywords- freshmen engineering; first-year experience; multidisciplinary; project-based learning; engineering design

I.

BACKGROUND

The freshmen introduction to engineering course has been
the focus of research and experimentation for decades. There
are countless models of instruction that differ in ways such as
whether or not the course is general for all majors or taught to a
specific major, the extent of project based instruction, and the
optimal size of the class.
Common introduction to engineering classes are usually
embedded in curriculum in which the institution has made a
decision to admit students into a pre-engineering program in
which all engineering students proceed through a set of mostly
common courses. In this approach, students select a specific
engineering major after several semesters of common courses.
The primary rationale for this approach is to provide students
with more time to explore their options before committing to a
major. The challenges, of course, lie in structuring a
curriculum with enough major depth that can be completed
after the selection has been made. Potential advantages of
admitting students into a chosen major include helping the
student identify with their chosen profession earlier with more
targeted classroom experiences as well as interactions with
faculty in their chosen discipline. Introduction to engineering
approaches vary greatly across institutions with both common
and discipline specific first year programs, with some
institutions requiring discipline specific or common
introduction to engineering courses, while other institutions

978-1-4673-1352-0/12/$31.00 ©2012 IEEE

offer only elective engineering courses or no required
engineering courses in the first year [4].
It has been considered best practice for some time to
include “hands-on”, team based projects in introduction to
engineering courses [5-8, 9]. The type and duration of these
projects, however, varies considerably and is usually
constrained by class size which also varies greatly among
institutions [4, 5, 7, 10, 11]. Some common introduction to
engineering courses include a series of short projects or
laboratory exercises to introduce students to each engineering
discipline [10-13]. Others use project-based learning, in which
engineering concepts are taught using a just-in-time approach
in the context of a semester-long team design project, which
may be discipline-specific or multidisciplinary in nature [4, 6,
14, 15, 16]. Both approaches have shown positive results and
the approach taken depends on the structure of the curriculum
and class sizes. There is also great variability in the number of
credit hours (typically between 1 and 4) as well as the type and
number of contact hours (i.e. lecture, lab, recitation) available
in the introduction to engineering course, which impacts the
structure and content of the course.
Common learning outcomes for the introduction to
engineering classes most often include:
Learning to work in teams
Gaining an understanding
Profession and Disciplines

of

the

Engineering

Developing Effective Communication Skills
Improving skills that support academic success
Learning and practicing the engineering design process
Developing a sense of community with peers
Connecting with elder peers and faculty
The specific content of freshmen introduction to
engineering classes also varies considerably across institutions,
depending on the desired outcomes of the course. Typical
topics covered in freshmen engineering courses include the
engineering design process, engineering analysis (general or
discipline specific), engineering problem solving, engineering
careers, academic success topics (time management, study
skills, etc.), ethics, and communication skills as indicated by
the learning outcomes above. Most of these topics are taught
in a single semester or two semester
introduction to
engineering course at many universities, while at other

institutions topics are split between an engineering
design/analysis course and a general university success course
(not engineering-specific). Depending on the specific
objectives of the course, some classes focus mostly on
technical skills necessary for future courses while others focus
more on non-technical ‘soft’ skills including teamwork and
communication.
Despite the advances made across the freshmen year via
these courses, retention remains a concern. Many institutions
have found that active learning, working in teams, hands-on
projects, and community building with peers and faculty can
help improve retention and student success [1-3, 5, 7]. We hope
to address many of the important factors contributing to
retention and provide first year engineering students with a fun
and rewarding educational experience in our new freshmen
engineering courses.
II.

HOLISTIC APPROACH TO FRESHMEN ENGINEERING
EXPERIENCE

Our institution is a large state university which grants
degrees in almost all of the engineering disciplines. Our
freshmen engineering class exceeds 1,500 students. Our
freshmen are admitted directly into the engineering discipline
of their choice. The engineering school itself is not organized
around traditional departments but instead consists of 5
schools, each of which contains faculty and students working
together on theme based research and curricula. For example
the School of Computing Informatics and Decisions Systems
Engineering integrates faculty and students in the computer
science, computer systems engineering, industrial engineering,
engineering management, and informatics degree programs.
Prior to the introduction of these new courses, freshmen
students in each of the academic programs took a discipline
specific introduction to engineering course. The content and
instruction of each of these courses varied greatly both by
design as well as quality of instruction. As part of a renewed
commitment to improve our freshmen engineering experience
incorporating the latest best practices, all aspects of these
courses were analyzed leading to the new approach described
in this paper. In particular we analyzed retention rates among
the programs, differences in course content and project
experiences, interviews with students leaving engineering and
faculty focus groups to identify areas for improvement.
The discipline specific 3 credit hour Introduction to
Engineering course was changed to an experiential
multidisciplinary 2 hour lab and lecture course (FSE 100)
complemented with a 1 hour engineering success class (ASU
101) both of which are described in detail later in the paper.
Rather than create a common course that all freshmen would
take, the FSE 100 classes are theme based closely aligned to
the schools with projects that are closely tied and very relevant
to the students’ majors. The instruction of the new FSE 100
courses is primarily performed by a core of lecturers
specifically hired to work together in the instruction,
evaluation, and continuous improvement of the courses. The
classes are taught in a newly constructed classroom
environment (eSpaces) in class sizes of 40 to maximize
instructional effectiveness. The new innovative eSpaces
provide a collaborative learning environment in which students

sit at work tables in teams of four to complete a variety of
hands-on engineering activities and projects. In eSpace, student
teams have the opportunity to use electronic equipment,
sensors, computers with engineering software, and hand tools
to visualize designs, complete analysis tasks, and build
prototypes. Throughout eSpace there are whiteboards and
writeable glass wall panels to encourage student collaboration.
All furniture in the room is mobile, with wheeled tool cabinets
and demonstration tables, to make eSpace modular allowing for
a variety of activities and courses.
All of the classes also utilize Undergraduate Teaching
Assistants (UGTAs) who are current engineering students that
are selected to assist in problem based instruction and to serve
as role models. Each UGTA works alongside a Graduate
Teaching Assistant (TA) and the faculty instructor to instruct
and assist students in the laboratory portion of the course.
UGTAs interact with the students primarily in class, answering
questions, and offering assistance and suggestions as needed as
students complete various engineering lab activities and design
projects. As a near peer, the UGTA is able to relate well to
students and their difficulties and successes, and offer them
valuable insight that may help improve student motivation and
confidence.
Unlike the FSE 100 classes, the ASU 101 classes are
discipline specific and are typically taught by tenure track
faculty with a class size limited to 19 with the goal of achieving
a high degree of faculty-student interaction. The ASU 101
class is where students in the same major will build a
community and identify most closely with their chosen
profession. In the remainder of this section, the courses will be
described in detail.
A. FSE 100 Introduction to Engineering
FSE100 is a 2 credit multidisciplinary lab and lecture
course, where the focus is on hands-on active learning and
working in teams. Class size is limited to 40 students to
maximize interaction. The course introduces students to the
engineering design process and some basic engineering tools
and software, culminating in a 7-8 week team design project.
Students attend one 50 minute lecture and one 3 hour lab each
week for the entire 15 week semester. Faculty lead the lectures,
introduce the labs and project, and are present in lab to assist
students along with a graduate TA and UGTA. The lectures
involve group discussions and student participation whenever
possible, and are used to introduce and reinforce new concepts
that are explored in the lab activities and projects. With the
help of the tools and equipment available in eSpace, freshmen
students have the opportunity to actually ‘do’ engineering in
the lab throughout the semester. Lab activities range from
building circuits, programming, or testing gear ratios to
building functional prototypes for their design projects. The
hands-on lab is the focus of this course, and the lecture
supports the lab by providing students with the knowledge
required to obtain deeper understanding of the concepts
explored. Topics are presented in the context of the design
project whenever possible.
There are currently three different versions of the FSE100
course in the college, each theme based with a project related to

the disciplines of students taking the course. The students are
grouped by discipline into one of the three theme based
versions of the course as follows: (1) ME/AE/EE: Mechanical,
Aerospace, and Electrical Engineering; (2) CS/CSE/IE:
Computer Science, Computer Systems Engineering, and
Industrial Engineering; (3) CEE/ConstE: Civil and
Environmental Engineering and Construction Engineering.
The overall course structure is consistent throughout each
version of the course. The general course structure begins with
6-7 weeks of introducing design and engineering topics/tools,
and ending with a 7-8 week team design project. The specific
engineering tools and topics covered in each School’s course
(with the exception of the design process), however, is related
to the course ‘theme’ which is evident in the course design
projects.
1) Learning outcomes: The specific learning outcomes for
the FSE100 course are the same across all themes, and are
closely aligned with the outcomes of introduction to
engineering and introduction to engineering design courses at
other institutions as well as ABET [16]. As a result of
completing the FSE100 course, students will:
1.

Understand and practice using the engineering design
process;

2.

Become familiar with tools,
terminology used in engineering;

3.

Learn to use engineering models, physical principles,
measurements and data to solve problems;

4.

Gain the ability to work effectively in teams and
recognize the importance of teamwork;

5.

Gain basic skills in technical communication (oral
presentations, technical reports, presentation of data);

6.

Gain experience using basic project management
techniques (scheduling, budgeting) to complete
projects successfully; and

7.

Use creativity to solve an engineering problem
related to challenges facing our world.

software,

and

2) Major topics: The main topics covered in FSE100
include teamwork, technical communication skills, and the
engineering design process. Teamwork is learned through
practice, with students working in teams of four throughout
the semester. Both oral and written communication skills are
emphasized through presentations and technical reports.
Students gain experience working through the stages of the
engineering design process in an introductory design process
lesson or mini-project, and in their team design projects.
In addition to the major topics discussed above, students
also explore various engineering topics related to the project
theme in that particular course and are introduced to topics,
tools, and software that are most relevant and useful for their
major. For ME/AE/EE students, these topics typically include
mechanical concepts (e.g. gears, motors, drag, friction, etc.),
electrical concepts (e.g. basic circuits, power, etc.),
multidisciplinary
project
topics
(e.g.
solar
cell
characterization, renewable energy, etc.), and general

engineering concepts including engineering drawing,
measurements, and energy conservation. Other tools and
software introduced to ME/AE/EE students include
MATLAB, LabVIEW, and various measurement and
construction tools. The course topics for CS/CSE/IE students
typically include programming, robotics, manufacturing,
system modeling and analysis, and various programming
languages and software tools are introduced. In the third
version of the course, CE/ConstE students learn and practice
AutoCAD, an important tool in their field, which they use to
visualize their prototypes during the design project.
3) Project Information: In the team design projects,
students work as engineers to design and build a functional
prototype of a system related to their field of study. Students
are provided with a project description which describes the
customer needs and constraints. Student teams are then
required to work through all stages of the engineering design
process to design and build a small-scale functional model.
They develop engineering requirements, generate multiple
conceptual designs, select a design, build a prototype, and test
prototype(s), repeating steps when necessary as they
experience the iterative nature of design. Modeling and
analysis are included in the projects in different forms and to
differing degrees, but all projects require some quantitative
analysis.
All projects are multidisciplinary in nature and require
students to learn and apply skills relevant to their majors.
Whenever possible, students are placed in multidisciplinary
teams so that they can work with students in other disciplines
as well as their own. Projects that were done in Fall 2011 (and
the disciplines involved in each project) include the following:
Renewable Energy Power plant (including wind, water, and/or
solar power) (ME/AE/EE), Solar-powered car (ME/AE/EE),
Autonomous maze-solving robot (CS/CSE/IE), Sumo Robot
(CS/CSE/IE), hydroelectric power generation system
(CEE/ConstE). In all projects, students must design and build
functional prototypes that meet functional requirements and
other various project constraints. For the Renewable Energy,
Solar car, and Hydroelectric projects students acquire various
construction materials including wood, plastics, motors, gears,
and various other raw materials from an in-class ‘store’ and
must stay within a specified ‘budget’. For the two robotics
projects, each team is provided with a Lego Mindstorms NXT
robot kit and use Microsoft Visual Programming Language
(VPL) to program the robots.
Throughout the project, the student teams complete a series
of project deliverables to document their progress and receive
feedback throughout the design process. Deliverables include
preliminary design proposals, progress report memos and/or
meeting minutes, a final design report, a final presentation,
and prototype demonstration (or competition). All project
deliverables are completed as a team, and follow technical
communication guidelines discussed in class.
B. ASU101: Engineering Success
ASU101 is a 1 credit lecture course intended to help
students gain the skills they need to be successful students and
engineers. The class meets once a week in classes of 19

students in the same major and is taught by a faculty member in
that school. The small class size provides an excellent
environment for the students to interact with other students in
their discipline, and to make connections to a faculty member
in their school. Through discussions and other active learning,
students explore career opportunities in their discipline, start to
develop personal goals based on their interests, and learn about
the resources available at the university to help them reach their
goals. This course helps students to really understand the
missions and values of the school and what they can do as
engineers, and helps them start to see themselves as future
engineers.
1) Learning outcomes: The specific learning outcomes for
ASU101 are all related to students gaining the skills they need
to be successful students and professional engineers. These
outcomes are those typically covered in introduction to
engineering courses at other institutions that are not covered
explicitly in our introduction to engineering course (FSE100).
As a result of completing ASU101, students will:
1.

Set Personal, Academic, and Career Goals to guide
their success at ASU;

2.

Learn about the resources available through ASU and
the Ira A. Fulton Schools of Engineering that will help
them to be successful in their academic careers;

3.

Enhance skills to support academic success (time
management, academic/career planning, study skills);

4.

Understand the ethical issues related to a career in
engineering and the role of academic integrity in
becoming an ethical and responsible professional
engineer;

5.

Gain an understanding of the exciting career options
available in engineering and what they can do as
engineers; and

6.

Become a part of the engineering community and learn
about ways to become more involved in the School’s
and University‘s culture

2) Major topics: In the discipline specific ASU101
classes, students learn about their engineering major and
career options, practice skills that will help them to be
successful students and professionals, and learn about the
resources available at ASU to support them through their
studies. Towards the beginning of the course, students learn
about engineering careers and career planning through a three
week career exploration module. In the career exploration
module, they first explore the wide variety of career options
available to them as engineers, and the difference between
engineering disciplines (majors) and job functions. In the
second week, students have the unique opportunity to talk to
working engineers from all engineering disciplines at the
‘Career Exploration Night’ event held specifically for
Freshmen engineering students. The third week of the module
discusses career planning, tools, and resources they can use to
help them achieve their career goals.

Specific academic success related topics covered in the
course include time management, learning goals, and learning
from feedback. Additional topics covered in the course include
opportunities in the college and/or university (e.g. clubs,
research experience, etc.), Academic Integrity and
Engineering Ethics, and academic resources (e.g. advising,
library, etc.).
The course culminates with an oral presentation
assignment, in which students complete individual or group
presentations on a selected topic (topic areas vary between
instructors and disciplines). Topic areas for presentations have
included
Engineering
Failures,
Top
Engineering
Achievements, National Academy of Engineering (NAE)
Grand Challenges for Engineering, and other topics related to
their interests and engineering.
III.

EXPERIENCE WITH THE NEW COURSES

The new freshmen engineering courses, FSE100 and
ASU101, were successfully implemented on a large scale
throughout all disciplines mentioned above in the Fall 2011
semester. There were approximately 1,100 freshmen enrolled
in FSE100 across 28 sections of the course (16 ME/AE/EE, 8
CS/CSE/IE, 4 CEE/ConstE). The total number of ASU101
sections across all engineering disciplines was approximately
double that of FSE100, since the class size is limited to 19.
A. Lessons learned
Under the new instructional model, the student experience
in FSE100 was more consistent across all disciplines involved
with each course using an experiential lecture and lab format.
The transition from a lecture based course to lab based course
had a positive impact on the students’ experience, as expected
due to the increase in hands-on learning. Students also
appreciated the opportunity to interact with and receive
assistance from the faculty, TAs, and UGTAs when necessary
throughout the design project and other lab activities.
The design project was well received by most students, and
they enjoyed the opportunity of learning challenging concepts
while actually ‘doing’ engineering. The opportunity to design
and build a complete system from the ground up excited and
motivated the students; they embraced the opportunity to act as
engineers. The open-ended nature of the problem caused some
students to be apprehensive and/or uncomfortable at first, but
once the teams started working together on the designs they all
appeared more confident that they could be successful. The
multidisciplinary nature of the projects and the related lab
activities completed in preparation for the projects gave
students an opportunity to see how all engineering disciplines
work together to solve real world problems. All students were
able to complete the projects successfully and produced
functional prototypes of the system explored in their project.
The design project was cited as the best part of the course by
many students, and students clearly were motivated by this type
of activity.
One of the challenges with theme based courses is choosing
a ‘theme’ or project that the majority of students are interested
in. This fall, most students were genuinely interested in the
projects, but at times it was difficult to maintain student interest

in some of the other lab activities, particularly if it was outside
a student’s chosen discipline. During the early labs, some
students would complain or question why they needed to learn
certain topics, but once they started their design projects they
no longer questioned the concepts covered. Through this, we
learned the importance of providing a context for all topics and
activities, and the necessity of emphasizing how each lab or
lecture topic connected to the course design project. As the
course continues in the future, we hope to continuously
improve the projects and content of the course, based on
student and faculty feedback and interests.
Another challenge encountered is choosing an appropriate
level of workload for the students for a 2 credit course.
Although many students enjoyed the course, there were still
several complaints about the workload associated with this 2
credit course. The additional workload for students also
increases the grading workload for TAs and faculty as well, as
it is multiplied by the number of sections taught. We are
currently exploring this issue and are planning to try different
forms of work and assessment to remedy this problem.
In the ASU101 courses, the student experience varied, and
appeared to be dependent on the faculty and their level of
involvement in the course. Overall, students enjoyed the small
class environment and were happy to have met other students
in their discipline and make a connection to a faculty member
in their school. Students appreciated when faculty members
were willing to help and showed that they cared about their
success. Students enjoyed the opportunity to learn about
engineering careers in the career exploration module, and
especially interacting with professional engineers at the Career
Exploration Night event. Student responses to academic
success skills related topics in the course were mixed, with
some students appreciating the information and others feeling
that they already have the skills presented and thus found the
course material to be ‘common sense’.
B.

What the students had to say
The majority of student feedback for FSE100 was positive,
and the recurring theme of what students liked most was
usually the ‘hands-on’ projects and activities. As seen in the
student comments below from course evaluations, students
enjoyed the opportunities to learn by ‘doing’ engineering.
“What I liked most about this course was EVERYTHING!
Everything seemed applicable to my degree and I enjoyed the
final project. I learned the most in this class and I wish there
was another class like this to take next semester!”
“I like how it gave the students a glimpse of engineering. It
helps show us that team work and critical thinking are essential
in this line of business.”

Most of the negative student feedback for the FSE100
course was related to the course load and grading, or the speed
at which topics were covered. The number of different topics
covered in some of the courses due to the multidisciplinary
nature of the project was overwhelming at first for some
students.
As mentioned above, student feedback on the ASU101
course varied greatly across sections and instructors. The
majority of positive feedback commented on learning about
engineering and their disciplines, and resources available to
them as students. Student comments also indicated that they
enjoyed having an opportunity to meet other freshman students
in their discipline and to meet and connect with a faculty
member in their school. Negative feedback included students
feeling that some topics were ‘useless’, or that the course
included too much ‘busy work’.
Freshmen retention data for our Fall 2011 cohort of
engineering students within the university improved to 96%
from 94% the previous year. Freshmen retention within the
engineering school also improved to 88% up from 85% the
previous year. Drilling down into the data, retention in the
mechanical engineering program increased from 85% to almost
92%. and retention in aerospace engineering increased from
79% to over 83%.
IV.

CONCLUSION

Overall the introduction of our new freshmen engineering
courses was very successful. Our freshmen welcomed the
opportunity to “be engineers from day one”. We have seen
significant increased participation of our freshmen in
engineering student organizations, undergraduate research
programs, outreach activities and service learning projects.
The quality and consistency of our instruction has improved
through the coordination and communication of our freshmen
engineering instructional team. Active learning is fully
integrated into our courses. We are confident that we have
developed a sustainable instructional model that will
continuously improve over time.

[1]

[2]
[3]

“I got to test engineering concepts and I worked with a
group to design a mini-powered city to supply energy. The
design project was very interesting and challenging.”

[4]

“The thing that I liked most about this course was the
ability to think creatively and express those ideas and opinions
to my group during lab.”

[5]

REFERENCES
S. S. Courter and G. Johnson, "Building community and
retention among first-year students: Engineering first-year
interest groups (eFIGSs)," in Frontiers in Education
Conference-Global Engineering: Knowledge without
Borders, Opportunities without Passports, 2007. FIE'07.
37th Annual, 2007, pp. F4A-3-F4A-8.
C. P. Veenstra, E. L. Dey and G. D. Herrin, "A model for
freshman
engineering
retention,"
Advances
in
Engineering Education, vol. 1, pp. 1-31, 2009.
M. W. Ohland, S. D. Sheppard, G. Lichtenstein, O. Eris,
D. Chachra and R. A. Layton, "Persistence, engagement,
and migration in engineering programs," 2008.
D. Silverstein, M. Vigeant, D. Visco and D. Woods,
"How we teach: Freshman introduction to chemical
engineering," in Proceedings of the 2010 Annual Meeting
of the American Society for Engineering Education, 2010.
S. Sheppard and R. Jennison, "Freshman engineering
design experiences and organizational framework,"
International Journal of Engineering Education, vol. 13,
pp. 190-197, 1997.

[6]

[7]

[8]
[9]

[10]

[11]

M. B. R. Vallim, J. M. Farines and J. E. R. Cury,
"Practicing engineering in a freshman introductory
course," Education, IEEE Transactions on, vol. 49, pp.
74-79, 2006.
J. Richardson and J. Dantzler, "Effect of a freshman
engineering program on retention and academic
performance," in Frontiers in Education, 2002. FIE 2002.
32nd Annual, 2002, pp. S2C-16-S2C-22 vol. 3.
C. Dym, A. Agogino, O. Eris, D. Frey and L. Leifer,
"Engineering design thinking, teaching, and learning,"
2005.
S. D. Sheppard and R. Jenison, "Thoughts on freshman
engineering design experiences," in Frontiers in Education
Conference, 1996. FIE'96. 26th Annual Conference.,
Proceedings of, 1996, pp. 909-913 vol. 2.
D. Hanesian and A. J. Perna, "An evolving freshman
engineering design program-the NJIT experience," in
Frontiers in Education Conference, 1999. FIE'99. 29th
Annual, 1999, pp. 12A6/22-12A6/27 vol. 1.
J. J. Kellar, W. Hovey, M. Langerman, S. Howard, L.
Simonson, L. Kjerengtroen, L. Sttler, H. Heilhecker, L.
Ameson-Meyer and S. D. Kellogg, "A problem based

[12]
[13]

[14]

[15]

[16]

learning approach for freshman engineering," in Frontiers
in Education Conference, 2000. FIE 2000. 30th Annual,
2000, pp. F2G/7-F2G10 vol. 2.
L. R. Carley, P. Khosla and R. Unetich, "Teaching
“Introduction to electrical and computer engineering” in
context," Proc IEEE, vol. 88, pp. 8-22, 2000.
J. Tezcan, J. Nicklow, J. Mathias, L. Gupta and R.
Kowalchuk, "An innovative freshmen engineering course
to improve retention," in American Society for
Engineering Education Annual Conference, 2008 .
D. Mascaro and S. Bamberg, "AC 2010-1837:
INTEGRATION
AND
REINFORCEMENT
OF
MECHANICAL ENGINEERING SKILLS BEGINNING
IN THE FIRST-YEAR DESIGN EXPERIENCE," 2010.
M. Rush, D. Newman and D. Wallace, "Project-Based
Learning in First Year Engineering Curricula: Course
Development and Student Experiences in Two New
Classes at MIT," .
D. Cordes, D. Evans, K. Frair, J. Froyd, “The NSF
Foundation Coalition: The First Five Years, Journal of
Engineering Education, January 1999.

SOFTWARE PROCESS IMPROVEMENT AND PRACTICE
Softw. Process Improve. Pract. 2000; 5: 169–182 (2000)

System Dynamics
Modeling Applied to
Software Outsourcing
Decision Support
1,*,†

Research Section

1

Stephen T. Roehling , James S. Collofello , Brian G.
Hermann1 and Dwight E. Smith-Daniels2
1

Department of Computer Science and Engineering, College of
Engineering and Applied Sciences, Arizona State University, Tempe,
Arizona 85287-5406, USA
2
Department of Management, College of Business, Arizona State
University, Tempe, Arizona 85287-4006, USA

Requirements in the software market for reduced costs, reduced development cycle time, as
well as shortages of software developers, have motivated software organizations to outsource
product development processes or components. The primary objective of this research is to
determine whether software organizations can improve their software outsourcing strategies
and processes in response to these forces. This research utilizes simulation modeling to
explore the dynamics of outsourcing relationships, including both positive and negative
outcomes, as well as to provide potential decision support for strategic outsourcing decisions.
The model’s current implementation, applicability and usefulness are demonstrated with an
example use case and analysis of simulation results. We also present suggestions for future
research directions on software outsourcing strategies and processes. Copyright  2000 John
Wiley & Sons Ltd
KEY WORDS:

software process modeling; software outsourcing

1. INTRODUCTION

the context of this research effort, software outsourcing has been defined as:

A comprehensive research effort is underway at
Arizona State University to better understand the
software outsourcing problem, and build a suite
of software tools to provide insight and guidance
to software practitioners in decision making roles
(see ⬍http://www.eas.asu.edu/苲outsrc⬎). Within

쐌 contracting with an external organization to
develop one or more product components; or
쐌 contracting with an external organization to
assist with one or more process components,
such as testing or coding.

*Correspondence to: Stephen T. Roehling, Department of
Computer Science and Engineering, College of Engineering and
Applied Sciences, Arizona State University, Tempe, Arizona
85287-5406, USA
†E-mail: steve얀endrun.com

Figure 1 illustrates that total in-house development
and total acquisition are included in this definition
and represent the boundaries of software development outsourcing. Nearly every software acquisition involves some project oversight, tracking
and relationship management, just like typical
product or process component outsourcing. At

Copyright  2000 John Wiley & Sons, Ltd.

Research Section

S. T. Roehling et al.

Figure 1. Dimensions of software outsourcing

1.

the other end of the spectrum, in-house efforts
frequently reuse product components, much like
projects where part of the product development
is outsourced.
A system dynamics model is being built and
refined to complement and effectively interface
with this larger research effort and suite of tools.
This effort has been motivated by similar modeling
efforts to represent software development processes
(Abdel-Hamid and Madnick 1991).
1.1. Role of Modeling
At this point, we envision the model in a strategic
role, where observed trends and insights gained
indirectly influence real-world decision making. In
particular, as part of a larger suite of decision
support tools, the model is envisioned in the
following roles:
(A)

(B)

At the bottom of a top-down decision support process:
1.
A customer meets with an outsourcing
domain expert, and describes his objectives and concerns.
2.
The domain expert uses a qualitative,
informational framework, or an expert
system, to identify potentially successful outsourcing types, strategies and
tactics.
3.
The simulation model is customized to
represent the customer’s outsourcing
project characteristics.
4.
The domain expert walks the customer
through several simulation runs, which
corroborate results from step 2.
Training sessions, where the model is used
in a stand-alone manner:

Copyright  2000 John Wiley & Sons, Ltd.

170

(C)

A customer meets with an outsourcing
domain expert, and describes his outsourcing goals and constraints.
2.
A modeling domain expert chooses a
set of pre-modeled outsourcing types
to illustrate key properties associated
with the customer’s outsourcing goals
and constraints.
3.
The domain expert walks through several simulation runs and analyses,
which collectively elevate the customer’s sensitivity to benefits and
drawbacks of various candidate
decisions.
As a complement to other relevant tools and
practices, for example:
1.
An estimation model, such as
COCOMO, is used to determine the
software project’s expected cost, effort
and schedule.
2.
The customer asks the question ‘Does
it make sense for me to outsource a
piece of the development effort?’
3.
The customer leverages an outsourcing
specific model to more closely understand expected outsourcing related
consequences.

In the roles described above, the model is expected
to have the following types of strategic benefits:
쐌 elevate decision makers’ sensitivities to the
trends, dynamics and constraints of outsourcing relationships;
쐌 encourage software practitioners to focus
resources towards activities most conducive to
successful
outsourcing
(e.g.
continuous
training);
Softw. Process Improve. Pract., 2000; 5: 169–182

Research Section

쐌 give customers the ability to perform ‘what if’
analyses to choose from any modeled outsourcing scenario, rather than the one or two
described in vendors’ proposals;
쐌 since models require explicit, logical, and precise inputs, all parties in outsourcing relationships will benefit from estimations and metrics
gathered to support modeling efforts.
1.2. Scope and Applicability
In large part, our modeling efforts have been
constrained by the types of quantitative outsourcing-specific information which can be usefully
represented in a continuous dynamic software
simulation model. In this regard, outsourcingrelated metrics correspond to the types of quantitative information which may be collected or
estimated for modeling efforts, or serve as outsourcing performance gauges. With respect to the metric
classes listed in Table 1, our modeling efforts have
emphasized productivity, staffing, rework (quality),
cost and schedule.

Software Outsourcing Decision Support
쐌 there is minimal risk of losing intellectual
property, since maintenance development
occurs on the code base for a publicly
released product;
쐌 the vendor has a positive track record;
쐌 the vendor has sufficient domain expertise;
쐌 maintenance is considered a non-core activity.
In addition to the indicators given above, literature
also provides qualitative factors and planning steps
associated with successful outsourcing (Jones 1998).
After running the model, successful or unsuccessful maintenance outsourcing is indicated by the
following outputs and analyses:
쐌 Relative to other simulation runs, when will
the second release of the product be completed?
쐌 Relative to in-house development what is the
total cost of outsourced maintenance development?
쐌 Maintenance request backlog versus time (a
measure of responsiveness to customer maintenance requests).
쐌 In-house developers’ availability for new development versus time.

1.3. Current Modeling Capabilities
To eventually fit into each of the roles and realize
each of the benefits described above, the model
will need to support several types of outsourcing.
The model currently supports maintenance outsourcing, where maintenance outsourcing is
defined as using an external vendor to handle
customer originated requests to fix or enhance the
software. The model can be used at the bottom of
a top-down decision support process, as described
in (A) above. In this role, the model can provide
support to project decision-makers after a general
high-level indication of successful outsourcing has
been established, for example:
Table 1. Outsourcing metrics classes (Yourdon et al. 1996)
Finance & Budget
Customer Satisfaction
Work Product Delivery
Quality
Time and Schedule
Business Value
Operational Service Levels
Human Resources
Productivity
Copyright  2000 John Wiley & Sons, Ltd.

2. MODEL IMPLEMENTATION
The Extend simulation tool is being used for model
implementation (see ⬍http:///www.imaginethatinc.com⬎). Extend is not a system dynamics
modeling package per se, but does allow modelers
to construct continuous system dynamics models
with feedback, stocks and flows.
The current model supports maintenance outsourcing, but is designed with the underlying
support to represent other outsourcing types. To
this end, the model’s organization is illustrated in
Figure 2, and the following approach was used to
develop the model’s current maintenance outsourcing capabilities:
1. Develop a set of generic, reusable building
blocks to simulate software project fundamentals such as work, rework, costs, experience
levels and talent pools.
2. Develop a set of potentially reusable outsourcing specific capabilities, such as simultaneous
efforts on behalf of in-house and outsourced
teams.
3. Customize the model for maintenance outsourSoftw. Process Improve. Pract., 2000; 5: 169–182

171

Research Section

S. T. Roehling et al.

Figure 2. Overall current and future model organization

cing by implementing new behaviors or reusing
generic building blocks and/or outsourcing
specific capabilities.
2.1. Generic Building Blocks
Figure 3 shows many of the generic building blocks
implemented into the model, and the information
flow and feedback associated with each. These
building blocks will be reused when other types
of outsourcing are implemented.

2.1.1. Available Talent
Two talent pools are supported: one for an inhouse team, and one for an outsourced team.
The following attributes are associated with each
talent pool:
쐌 Experience profile – relative number of beginners, practitioners, and experts.
쐌 Learning rates – duration, not including overtime or idle time required to advance to the
next experience level.

Figure 3. Basic information flow and feedback loops
Copyright  2000 John Wiley & Sons, Ltd.

172

Softw. Process Improve. Pract., 2000; 5: 169–182

Research Section

쐌 Attrition rate – percentage of talent which turns
over each year.
쐌 Hiring delay, in project weeks, to hire new
talent, or replace talent which leaves due
to attrition.
쐌 Initial talent immediately available at the beginning of the project phase.
쐌 Needed talent at the beginning of the project
phase.
쐌 Overtime availability.
Talent is available for one or more tasks. For each
talent pool, and for each task, there are several
input variables:
쐌 Productivity – for each experience level, the
amount of time it takes to complete a work item.
쐌 Rework generation rate – number of new work
items (e.g. maintenance requests) generated for
each completed one.
2.1.2. Work Assignment
Several types of work assignment rules support
task prioritization, parallel tasks and tasks requiring
simultaneous efforts. These rules can be combined
to implement the customized work assignment
policy for a particular type of outsourcing. For
example, within the current model, work is
assigned such that work started on existing maintenance requests will be finished before work on
new maintenance requests begins.
2.1.3. Costs
The model includes the following cost input
parameters, which are designed to generically
represent several types of compensation methods
for either in-house or outsourced efforts:
쐌 Normal rate per hour per developer.
쐌 Overtime rate per hour per developer, expressed
as a fraction of the normal rate.
쐌 Idle time rate per hour per developer, expressed
as a fraction of the normal rate.
Table 2 provides example formulations of the
model’s cost inputs to represent different compensation types. Observe that a ‘per unit of work
completed’ compensation type is not directly supported (e.g. pay the vendor for each function point,
each maintenance request, or each line of code).
However, the amount of work completed over
time is an output of the model, so the calculation
Copyright  2000 John Wiley & Sons, Ltd.

Software Outsourcing Decision Support

Table 2. Formulating cost inputs
Cost inputs

Example meaning

쐌 Normal rate is average yearly
salary divided by number of
projected weeks per year
쐌 Overtime rate equals 0
쐌 Idle time rate equals 1
쐌 Overtime rate equals 1
쐌 Idle time rate equals 0

Salaried, in-house
employees

쐌 Overtime rate equals 1
쐌 Idle time rate equals 1

Outsourced, possibly offsite effort, where vendor
works on another project
during idle time
Flat hourly rate

to determine cost under such a compensation
system is trivial.
2.1.4. Complete Work
Based upon work assignment, a certain amount of
work capacity is available for each task. This work
capacity is used to draw from the pool of incomplete
work, then generate rework and completed work.
2.1.5. Rework
Rework is an important model feature, since rework
generated from outsourced efforts also requires
additional in-house support. Rework is a means
to gauge the sensitivity of overall schedule and cost
to the relative work quality of different vendors.
The model assumes the effort associated with
rework will be less than or equal to the original
effort. In terms of model inputs, the effort associated
with rework is expressed as a fraction of the
original effort.
Figure 4 is an example plot showing rework
generation rates for in-house and outsourced talent
over time. In general, less rework is generated as
more and more organizational learning takes place.
2.2. Learning
A learning rate is associated with each experience
level, for both in-house and outsourced talent.
However, if based upon work assignment, some
talent remains idle, the learning rate will decrease.
Similarly, if some talent works overtime, then the
learning rate will increase. The model takes into
account idle time and overtime in speeding up
or slowing down the learning of in-house and
outsourced talent.
Softw. Process Improve. Pract., 2000; 5: 169–182

173

Research Section

S. T. Roehling et al.

Figure 4. Rework versus time and experience

The model supports an ‘idle time learning
rate’ input parameter. This parameter controls the
learning rate during idle time. This parameter may
approach 1 for the in-house talent, since this talent is
immersed in an environment doing complementary
tasks, where applicable learning occurs even during
idle time. However, this parameter may approach
0 for off-site, outsourced talent, where applicable
learning does not occur, because the outsourced
talent is assumed to be working on dissimilar tasks
during idle time.
2.2.1. Incomplete Work
A user defined work input distribution fills a work
pool, which is drawn from when talent is assigned
to a task and the work is completed. For each
simulation time step, the amount of new work can
be specified. Observe that ‘requirements creep’, or
a mid-project change of scope, is supported by
inputting new work during later simulation time
steps.
Units for inputted work are generic, but for a
particular set of inputs, and depending upon the
type of outsourcing, units of work may represent
maintenance requests, function points, test case
executions etc.
Copyright  2000 John Wiley & Sons, Ltd.

174

2.3. Outsourcing Specifics
At its core, the model is a basic software project
simulator, where tasks are worked on with a
certain productivity level, rework is generated and
costs accrue. However, two design and implementation strategies have been adopted to give the
simulation outsourcing-specific features and capabilities:
1. Pay special attention to outsourcing-specific
overhead, interactions and dependencies
between in-house and outsourced efforts.
2. Instrument the model to support outsourcingspecific sensitivity analyses.
With respect to the strategies described above, the
model represents simultaneous efforts on behalf
of both in-house and outsourced talent.
The model also represents ‘front-end’ and ‘backend’ overhead, and scheduling dependencies
between in-house and outsourced efforts. For
example, before the outsourced maintenance efforts
can begin, sufficient front-end in-house support
must be provided.
Front-end or back-end overhead represent communication and support required above and beyond
that which would be required to do the work inSoftw. Process Improve. Pract., 2000; 5: 169–182

Research Section

house. For example, back-end overhead to outsource maintenance requests could include careful
acceptance testing to ensure completed work meets
quality standards; similarly, front-end overhead
could include formal specifications needed to
complete the work under a contract, language
translation, and/or manual labor needed to extract
the maintenance request from the in-house database, and communicate the request via e-mail to
the vendor.
The model assumes front-end and back-end
overhead can require different levels of effort for
in-house and outsourced talent. For example, as
part of back-end overhead, it could take more
effort for the vendor to prepare for a formal
acceptance test than it would take an in-house representative.
2.4. Maintenance Outsourcing Customization
For maintenance outsourcing, a certain customization effort was required to link generic building
blocks together, and customize user input forms.
With respect to the development lifecycle in
Figure 5 and partial or full maintenance outsourcing, maintenance efforts are assumed to have the
following characteristics:
쐌 An outside vendor may be used for some or
all maintenance efforts.
쐌 After the first release of the product, development and maintenance tasks occur in parallel.
쐌 Based upon the number of backlogged maintenance requests, in-house development talent is
diverted from new development to handle
maintenance requests.
쐌 Based upon historical data, the stream of new
maintenance requests over time can be approximated.
2.4.1. Maintenance Specific Inputs
The following input parameters are supported
to represent a particular vendor’s maintenance

Figure 5. Software development and maintenance lifecycle
Copyright  2000 John Wiley & Sons, Ltd.

Software Outsourcing Decision Support

development capabilities, or to perform sensitivity
analyses by varying one or more inputs:
쐌 front-end overhead associated with formally
communicating, translating, or routing a maintenance request to the vendor;
쐌 back-end overhead associated with accepting a
completed maintenance request (e.g. acceptance
tests to ensure the fix was done correctly);
쐌 relative staff sizes, learning curves, productivity,
overtime availability, attrition rates, hiring
delays and experience profiles for both inhouse and outsourced talent;
쐌 maintenance effort rework generation rates for
in-house and outsourced developers at different
experience levels;
쐌 maintenance request backlog threshold – if this
threshold is exceeded, some or all in-house
development talent will be diverted to work
on the maintenance task;
쐌 cost structure for in-house and outsourced
talent. Fixed and hourly billing methods are
supported, as well as overtime and idle time;
쐌 maintenance request arrival distribution.
2.4.2. Maintenance Work Assignment
The following work assignment policy represents
outsourced maintenance:
1. Assign some or all of both the in-house and
outsourced team to simultaneously work on
back-end communication overhead associated
with the outsourced team completing maintenance requests (e.g. the work associated with
acceptance testing).
2. Assign in-house and outsourced talent to work
in parallel on maintenance requests. A backlog
threshold can be set whereby in-house talent
will only be assigned to work on maintenance
requests if the threshold is exceeded.
3. Assign both in-house and outsourced talent to
work on front-end communication overhead
work associated with outsourcing maintenance
requests. For maintenance, this work could
represent extracting the maintenance request
from the problem tracking system, then formally communicating the request to the outsourced team.
4. In-house talent not already assigned is assigned
to work on new development.
5. Any remaining in-house talent not assigned to
Softw. Process Improve. Pract., 2000; 5: 169–182

175

Research Section

S. T. Roehling et al.

Figure 6. Schedule dependencies between different tasks

Figure 7. Rework flow and feedback for outsourced maintenance efforts
Copyright  2000 John Wiley & Sons, Ltd.

176

Softw. Process Improve. Pract., 2000; 5: 169–182

Research Section

Software Outsourcing Decision Support

Figure 8. Simulation control panel

new development is assigned to work on any
remaining maintenance requests.
2.4.3. Maintenance Work Flow
Figure 6 shows the schedule dependencies for
outsourced maintenance tasks. These schedule
dependencies influence staff utilization within the
model, and ultimately affect schedule durations in
the model’s outputs. New maintenance requests
are either routed to the in-house or outsourced
team, subject to the work assignment policy
Copyright  2000 John Wiley & Sons, Ltd.

described in Section 2.4.2. Front-end and back-end
overhead for outsourced maintenance requests
must be done concurrently.
2.4.4. Maintenance Rework
As shown in Figure 7, the model currently represents four possible rework types for outsourced
maintenance efforts. The assumption is that efforts
required to rework a maintenance request may or
may not require additional front-end overhead,
but all rework is assumed to require some
Softw. Process Improve. Pract., 2000; 5: 169–182

177

Research Section

additional back-end overhead. The model also
represents when rework is detected: either before
or after back-end overhead effort.

3. EXAMPLE USE CASE AND ANALYSIS
An example maintenance outsourcing use case, or
project scenario, serves as an example for conducting model runs, discussion and analysis.
The example scenario and simulation runs are
not based upon an actual project. However, walking
through an example spells out the type of metrics,
corresponding to model inputs, that an actual
organization needs to collect or estimate to effec-

S. T. Roehling et al.

tively use the model. The example also illustrates
the model’s scope and applicability with respect
to addressing software organizations’ outsourcing
issues. Finally, discussion and analysis of model
runs, based upon an example use case, shows
the sensitivity of cost and schedule outputs to
outsourcing-specific productivity, cost and quality inputs.
3.1. Project Inputs
The following were assumed to be valid project
parameters:
쐌 Developers are expected to write one to five

Figure 9. Baseline outsourced talent input profile
Copyright  2000 John Wiley & Sons, Ltd.

178

Softw. Process Improve. Pract., 2000; 5: 169–182

Research Section

쐌

쐌
쐌
쐌
쐌
쐌

lines of code per hour, based upon their
experience level.
To give enough time for final system testing,
maintenance efforts must be completed before
new development. Therefore, simulation runs
where maintenance efforts finish after new
development are invalid.
The vendor works off-site, and does not get
paid for idle time.
The vendor can provide between five and ten
people for the project.
The maintenance request input distribution can
be estimated from past projects.
There are 20 000 lines of code (KLOC) to be
developed for Release 2.
Seventy-five percent of maintenance requests
are received the first month, and the remaining
25% in the second month.

3.2. Example Runs
Example model runs started with a set of baseline
inputs for in-house and outsourced talent. Figure 8
shows a control panel which can be used to vary
inputs, and gauge the effects upon development
and maintenance completion. The settings shown
in Figure 8 correspond to example run number one.
Figure 9 represents baseline inputs for outsourced
talent. Table 3 discusses the correlation between
the input parameters shown in Figure 9 and model
outputs; understanding the relationships between
model inputs and outputs helps in the analysis of
simulation results shown in Table 4.
The simulation runs test the sensitivity of schedule and cost to changes in the following outsourced
talent input parameters (in-house parameters
remain fixed):
쐌
쐌
쐌
쐌
쐌

relative
relative
relative
relative
relative

amount of experience;
quality;
size of outsourced team;
learning rate;
labor costs.

For the baseline run, labor rates were assumed to
be $50/hour for in-house talent, and $75/hour for
outsourced talent. Attrition was assumed to be
zero for all runs, since the project is relatively
short. The input distributions for maintenance
requests and new development were the same for
each simulation run.
Copyright  2000 John Wiley & Sons, Ltd.

Software Outsourcing Decision Support

Table 3. Input and output correlation
Input parameter(s)

Output correlation

쐌 Existing Talent after First
Release
쐌 Existing Talent Relative
Proportions
쐌 Weeks to Reach Next
Experience Level

These inputs determine the
overall amount, and relative
proportion of beginning,
practitioner and expert talent
with respect to time. Higher
levels of overall existing talent
yield lower schedule duration
outputs. Higher proportions of
less experienced talent, and/or
longer durations to gain
experience result in higher
cost and schedule outputs

쐌 Hours per Maintenance
Request to Complete Backend Overhead
쐌 Hours per Maintenance
Request to Complete
Front-end Overhead
쐌 Weeks to Complete a
Maintenance Request
쐌 New Development LOC
per Hour

For beginner, practitioner and
expert experience levels, these
inputs specify development
and maintenance staff
productivity. Increased
productivity results in lower
cost and schedule outputs

쐌 Percentage Maintenance
Requests Requiring
Rework
쐌 Fraction of Original Effort
Required for Rework
쐌 Fraction of Rework
Requiring New Front-end
Overhead
쐌 Fraction of Rework
Detected Before Back-end
Overhead

These fields collectively
determine the overall amount,
feedback, and flow of rework,
as shown in Figure 7. Higher
rework percentages represent
lower quality, and increase
schedule and cost outputs

쐌 Attrition percentage per
year
쐌 New or Replenished
Talent Hiring Delay
(Weeks)

These fields specify turnover
and re-hiring delays. Since
newly hired talent starts with
less experience (and lower
productvity), higher attrition
results in increased schedule
and cost outputs

3.3. Analysis
In this example, the general motivation for outsourcing is to free up in-house developers to work
on new development, and therefore finish the
second release of the product earlier (i.e. reduce
product release cycle time). In this regard, output
results in Table 4 indicate when development and
maintenance finish, and the total cost associated
Softw. Process Improve. Pract., 2000; 5: 169–182

179

Research Section

S. T. Roehling et al.

Table 4. Example simulation runs
Run number

1
2
3

4

5
6
7
8
9
10
11

12

Setup

Total project
cost (US $x
1000)

No setup required – run with baseline inputs
1. Start with baseline inputs
2. Turn off all outsourcing
1. Start with baseline inputs
2. Double outsourced team rework generation rate (decrease
relative quality)
1. Start with baseline inputs
2. Half outsourced team rework generation rate (increase
relative quality)
1. Start with baseline inputs
2. Double amount of outsourced talent
1. Start with baseline inputs
2. Half amount of outsourced talent
1. Start with baseline inputs
2. Half outsourced talent learning rate (learn slower)
1. Start with baseline inputs
2. Double outsourced talent learning rate (learn faster)
1. Start with baseline inputs
2. Set outsourced labor rate to $50/hour (baseline is $75/hour)
1. Start with baseline inputs
2. Set outsourced labor rate to $25/hour (baseline is $75/hour)
1. Start with baseline inputs
2. Set outsourced labor rate to $25/hour (baseline is $75/hour)
3. Double the amount of effort required for front-end and
back-end overhead
1. Start with baseline inputs
2. Make initial outsourced experience level the same as inhouse experience

792
582

31
33

37
59

808

34

38

781

31

37

909

25

32

710

40

43

879

36

39

752

30

35

648

31

37

505

31

37

508

31

37

739

30

33

with the second phase of development and maintenance.
In the context of outsourcing decision making, a
manager would be expected to digest the results
from Table 4, then make a management decision
about whether to outsource, and with what input
parameters. In other words, the model will not
directly indicate which particular set of inputs will
produce the best outsourcing results. In the context
of digesting the results in Table 4, a manager may
leverage the following types of insights in his or
her decision making:
쐌 Labor costs are equal for in-house and outsourced talent in runs 2 and 9, but outsourcing
still costs more. The outsourced talent is assumed
to be less experienced at the start of the second
project phase than the in-house team, and there
is a certain support overhead associated with
outsourcing maintenance requests.
Copyright  2000 John Wiley & Sons, Ltd.

180

Maintenance
Development
phase duration phase duration
(weeks)
(weeks)

쐌 For the example project, rework does not play
a major role in cost or schedule.
쐌 Run 5 doubles the amount of outsourced talent
with respect to run 1, and the overall cost in
run 5 is more than run 1. In run 5, more
maintenance work is performed while outsourced talent is still on the learning curve and
less productive. Therefore, based upon hourly
billing, the outsourced talent in run 5 will work
longer and cost more than run 1.
쐌 In general, the model will produce relatively
more attractive cost and schedule results for
larger projects, where outsourced talent has
the chance to reach the top of their learning
curve and work at peak productivity levels
for longer periods. This is consistent with
literature, which also suggests larger outsourcing ventures tend to be more successful
(Yourdon et al. 1996).
Softw. Process Improve. Pract., 2000; 5: 169–182

Research Section

쐌 Run 10 represents off-shore outsourcing, where
labor rates may be lower. Run 11 also assumes
front-end and back-end support overhead will
be higher for off-shore outsourcing. However,
the costs and levels of overhead in runs 10 and
11 do not include travel time and other costs
which may be associated with off-shore outsourcing.
쐌 If ‘time to market’ is the manager’s only
driving concern, comparing runs 1 and 2 makes
outsourcing look like an attractive alternative.
The simulation runs in Table 4 and the observations
above collectively illustrate the model’s level of
detail, flexibility and completeness in representing
an outsourced maintenance effort.

Software Outsourcing Decision Support

Table 5. Future outsourcing types
Outsourcing
type

Project characteristics

Development
outsourcing

쐌 Motivation for outsourcing is to have a
‘late in the project pressure relief value’ to
keep in-house staffing levels stable, and
ship the product sooner
쐌 Both in-house and outsourced teams work
on development
쐌 Front-end overhead associated with
screening which features are to be
outsourced
쐌 Back-end overhead associated with
carefully ensuring outsourced development
completed to specifications

Development
with
outsourced
testing

쐌 Testing not a ‘core’ activity
쐌 Test case input distribution a function of
in-house development team work output
쐌 In-house team must be large enough to
ensure sufficient work output to keep
outsourced team busy
쐌 Development rework requires retesting
쐌 Front-end overhead associated with
developers carefully explaining features to
be tested
쐌 Back-end overhead associated with sign-off
on testing completeness, test case design,
test execution etc.

4. FUTURE WORK
The current model assumes all maintenance
requests are of equal complexity. Future revisions
of the model may allow a user-specified distribution
of maintenance request complexities. With this
feature, the work assignment policy for maintenance could factor complexity, and include rules
like ‘outsource the easy maintenance requests’.
Another area of future work is to customize and
run the model for different types of outsourcing.
This work will establish the degree to which
outsourcing can be generically modeled, and will
allow the model to more effectively interface with
other outsourcing-specific decision support tools.
This work will also motivate new features and
determine the customization required to realistically simulate other outsourcing types.
Table 5 shows some candidate outsourcing types
to be implemented in the future. The project
characteristics given for each outsourcing type
illustrate model features, such as front-end overhead, back-end overhead and rework, which will
be reused for other outsourcing types. Furthermore,
these characteristics hint at the customization which
will be required to support the other outsourcing
types. For example, a new customized work
assignment policy will be needed to support the
‘pressure relief valve’ in the development scenario.
Finally, a validation effort will be conducted to
assess the believability of simulation results, and
Copyright  2000 John Wiley & Sons, Ltd.

determine the completeness of the model. Expert
opinion, and a careful analysis with respect to an
ongoing outsourcing survey, will be components
of validation (see ⬍http://www.eas.asu.edu/
苲outsrc⬎ for a copy of the survey). This effort is
underway and current results show promise. For
example, for all respondents as of December
1999, the top five goals for outsourcing were
the following:
1.
2.
3.
4.
5.

Obtain particular expertise.
Shorten schedule duration.
Improve responsiveness to the customer.
Add people.
Improve product quality.

In terms of completeness, the current model in some
way addresses each of the five expectations above.
Softw. Process Improve. Pract., 2000; 5: 169–182

181

Research Section

5. CONCLUSION
The maintenance outsourcing model described
herein represents the first increment of our modeling efforts. The current model represents fundamental software project components, such as
staffing, schedule, learning and costs. These components are combined with outsourcing-specific
rework, overhead and work effort scheduling. In
its current form, the model allows users to gain
some interesting insights into software outsourcing relationships.

Copyright  2000 John Wiley & Sons, Ltd.

182

S. T. Roehling et al.

REFERENCES
Abdel-Hamid T, Madnick SE. 1991. Software Project
Dynamics: An Integrated Approach. Prentice-Hall: New
Jersey.
Jones C. 1998. Marry in haste, repent at leisure: successful
outsourcing requires careful consideration and preparation. Cutter IT Journal 11(7):
Yourdon E et al. 1996. Offshore outsourcing: the benefits
and risks. Cutter IT Journal

Softw. Process Improve. Pract., 2000; 5: 169–182

Cost Excessive Paths in Cloud Based Services
Kevin Buell and James Collofello
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
Tempe, Arizona, USA
kevin.buell@asu.edu

Abstract
The pay-as-you-go economic model of cloud computing
leads naturally to an earn-as-you-go proﬁt model for many
cloud based services. These applications can beneﬁt from
low level analyses for cost optimization and veriﬁcation.
Testing cloud applications to ensure they meet monetary
cost objectives has not been well explored in the current
literature. We present a static analysis approach for determining which control ﬂow paths in cloud applications can
exceed a cost threshold. We build on tools used in Worst
Case Execution Time analysis that provide a tight bound on
processing time, and we implement provisions for adding
bandwidth, storage, and service costs. Our approach determines the magnitude of cost excess for nodes in an application’s call graph so that cloud developers can better understand where to focus their efforts to lower costs (or deem
some excesses acceptable based on business case analysis).

1

leads to a proﬁt model we could call ‘earn-as-you-go’. Understanding exactly how much the software costs as it runs
is essential for these types of applications. However, different paths in software may produce signiﬁcantly different
costs [6].
See ﬁgure 1 for an illustration of what various transactions in a software service might cost. Note that the transactions exceeding the threshold are marked with an ‘x’. The
rate that we charge our customers is also noted.
For the purposes of this discussion, we deﬁne a transaction as a single usage of a cloud application, typically
one for which we derive a speciﬁc amount of revenue (see
[6]). For example, a transaction with stock trading middleware running as a service in the cloud might consist of the
fulﬁllment of a single purchase of stock. The middleware
provider might charge $0.10 for this transaction which must
cover processing and bandwidth costs plus the cost of any
outside services used to complete the stock purchase.
As we think about low level economic concerns of our
cloud applications, we become interested in the following
types of issues:

Introduction

Cloud computing obviates the need for application developers to concern themselves with the logistics and economics of large, upfront infrastructure investments [3]. Instead, cloud application developers pay only for the processing, bandwidth, and storage their applications require.
Likewise, application developers may use other software
services for which they pay a fee, according to their Service
Level Agreement (SLA). It is not unlikely that this fee will
take a pay-as-you-go approach similar to the cost structure
of cloud infrastructure, realizing the vision of computing as
a utility (see [7]).
Many cloud applications are offered in the form of Software as a Service (SaaS) and charge their users on a per use
basis. For these types of cloud applications, not only do we
have a pay-as-you-go cost model, but our revenue model
could similarly be termed ‘collect-as-you-go’, which then
IEEE IRI 2012, August 8-10, 2012, Las Vegas, Nevada, USA
978-1-4673-2284-3/12/$31.00 ©2012 IEEE

• How much a transaction costs
– How much do common transactions cost?
– How much do transaction costs vary?
– What causes cost variation in our transactions?
• How much to charge for a transaction
– Fixed rate or variable?
– Based on an average transaction cost, or closer to
the max transaction cost?
Based on business case analysis and intuition, we may
have an idea of some of the answers to these questions. Furthermore, we ﬁx a rate that we charge our customers for
the different transactions available with our cloud application. When ﬁxing this rate, we have in mind a cost threshold
such that most of our transactions will cost us less than the

324

Rate

Threshold
Cost

Transactions

Figure 1. Cost of some sample transactions
threshold. In this way, we expect to make a proﬁt roughly
equal to the rate we charge our customers minus the cost
threshold, multiplied by the number of transactions we expect to service.
During the veriﬁcation phase of a software development
process, we test certain nonfunctional attributes of our software. One of these attributes that is especially visible and
traceable in cloud computing is monetary cost. As part of a
veriﬁcation process for earn-as-you-go cloud applications,
it would be desirable to ensure at least one of the following
conditions holds:
• Transaction costs are always below the threshold

tion has been addressed extensively for cloud providers (e.g.
[19]). Cost optimization of cloud applications has also received some attention in speciﬁc areas such as workﬂows
[16] and data storage [1]. For more information on usagebased pricing (the pricing scheme assumed in this work),
see [4].
The general problem of determining resource consumption through various control ﬂow paths (as well as data
ﬂow paths) has also been addressed extensively in previous
work. For example, Worst Case Execution Time (WCET)
analysis calculates a bound on the processing component of
program execution. The real time systems where WCET is
often used generally require a tight upper bound calculation
to satisfy hard timing constraints. Therefore, much of the
WCET research to date has been in static analysis in an attempt to guarantee that timing constraints are satisﬁed [20].
In addition to WCET, there has also been signiﬁcant interest in verifying and optimizing an application’s energy
consumption. For example, an approach for determining
worst case energy consumption is presented in [13] which,
like the current work, builds on WCET analysis.
The scaling of costs down to a level of granularity ﬁne
enough to be associated with an application’s source code
has been discussed in [6]. For example, we can simply divide a cloud provider’s cost of processing per hour by the
number of nanoseconds in an hour to obtain a per nanosecond cost. Likewise with bandwidth (cost per byte) and storage (cost per byte per second). The cost of services we use
varies and should be speciﬁed along with an SLA.

• Transactions above the threshold will be rare
• We can reduce the cost or frequency of transactions
above the threshold
Note that monetary costs in cloud applications probably
do not have hard constraints. As in most business situations,
we can ensure we turn an overall proﬁt even if we service
some transactions at a loss, as long as most of our transactions cost us somewhat less than we charge for them.
In this work, we describe an approach for calculating
the maximum cost of a transaction in a cloud application
through static analysis. We also determine the control ﬂow
paths that can cause cost excessive transactions, and we
rank methods and decisions based on the amount they can
contribute to the cost excess. In this way, we provide a
means for verifying cost in cloud applications, identifying
control ﬂow paths that can lead to cost excess, and pointing out areas in which to concentrate our effort in order to
reduce costs.

2

Related Work

Important research has been conducted on topics related
to the one discussed here. The problem of cost optimiza-

3

WCET Adaptation

Our approach for determining cost excessive paths begins with an adaptation of WCET analysis (which has already been identiﬁed as applicable to cloud computing in
[6] and [12]). Our intermediate goal is to determine the
maximum monetary cost of a given method within our application. A given transaction with our application begins with some method and follows a control ﬂow path
through an interprocedural graph. Note that we use the term
‘method’ here but this might be used interchangeably with
‘function’ or ‘procedure’ depending on the programming
language in use.
We derive the processing component of a method’s maximum cost directly from the WCET. This cost component
is simply the WCET (converted to nanoseconds) multiplied
by the cost per nanosecond that our cloud provider charges
(the per hour cost scaled down to the nanosecond level).
For the other cost components (bandwidth, storage, and
services), we rely on user inserted annotations. WCET analysis already embraces annotations for information that cannot be (or cannot fully be) calculated through static analysis
[20]. For example, loop bounds are often annotated. In

325

our calculation of worst case cost, we insert annotations in
source code directly where bandwidth or storage are used,
or where services are invoked. Following is an example of
an annotated method:
@ServiceCost(0.005)
@ServiceTimeBound(500000000)
@ServiceBandwidthBound(4096)
public Byte[] retrieveOutsideData()
{
...
}

These annotations indicate that the method invokes an
outside service costing $0.005. The bound on the amount
of time we may wait for the results of the service invocation
is 500 milliseconds. The amount of bandwidth we require
for data transfer with the service is 4KB. We use the service
cost directly in our max cost calculations. We convert the
time and bandwidth bounds to monetary cost based on our
provider’s rates, and then we account for those contributions
to the max monetary cost.
It should be noted that our annotations do not place a
bound on a certain cost type for an entire transaction. We
require user inserted annotations only at those points of the
call graph where resources are used. Our WCET adaptation
must account for all the annotations within the context of
loops and decisions while traversing the call graph to arrive
at a ﬁnal bound on the cost of a method. So our WCET
adaptation is still important even for annotated costs. An
attempt to calculate a bound or derive such a bound through
analysis, simulation, or measurement is outside the scope of
the current work. For now, we rely on annotations.
Also note that the scaling and tracking of costs down to
the transaction level, though discussed to some extent in the
literature (see [6] and [10]), is not immediately compatible with how cloud providers charge for cloud resources.
Cloud providers generally charge at a higher scale (e.g. per
hour for bandwidth and per GB for bandwidth). It is implicit in our veriﬁcation techniques that the cloud provider’s
costs are scaled down to the transaction level, but that as
the number of transactions increases, the actual usage of
resources rises to the level tracked and charged by cloud
providers. Also note that some cloud providers are offering
usage statistics at a scale much lower than per hour, as in
Amazon’s CloudWatch [2].
Our WCET adaptation is simply the sum of processing
cost plus bandwidth, storage, and service costs accounted
for throughout an application’s call graph. Whereas WCET
deals only with processing time and uses either a graph
based or linear programming based approach to determine
worst case cost [20], our WCET adaptation determines
monetary cost and only uses a graph based approach (so
that additional node level information can be saved for later

calculations). In particular, a graph corresponding to the
control ﬂow nodes in a program is built and the (maximum)
cost of each node is determined by adding together and ﬁnding the maximum of all the resource costs associated with
the node and its children.

4

Assumptions and Limitations

In addition to the user inserted annotations previously
discussed, several other assumptions and limitations are
part of this work. Most of the limitations are inherited
directly from WCET, which has traditionally been implemented for low level languages commonly used in real time
systems. WCET research has more recently been extended
to higher level languages that are more likely to be used
in cloud applications. However, limitations/restrictions for
exception handling, polymorphism, and even recursion remain. As real time systems increasingly use higher level
languages, it is likely that these limitations will be addressed. Indeed, some research has already been undertaken
to address these limitations (see [18], [8], and [5]).
An important assumption in this work is that we can determine the processing time of our transactions. In WCET
analysis, this is accomplished by having intimate knowledge of the target processor. In cloud computing, low level
hardware concerns are generally abstracted away from the
developer. Therefore, we use some assumptions (or possibly some measurements taken on representative hardware
from the cloud provider) to derive speciﬁc costs for speciﬁc
low level operations in our software.

5

Decision/Call Graph Analysis

As mentioned previously, we begin with a WCET adaptation that calculates maximum monetary cost of a method
in a cloud application. More speciﬁcally, as the max cost
algorithm runs, we save off information about each method
as well as each branch of each decision, all within our interprocedural graph. The result is a Decision/Call Graph
(DCG) where we consider a node to be either a method or a
decision branch (not the decision itself).
See ﬁgure 2 for an example of a DCG. Methods and decision branches are represented by circles. Decisions are
represented by triangles. Nodes on the path of worst case
cost are shown in bold.
Note that the set of unique paths through an interprocedural call graph is based on decision branches. Whether we
take one branch or the other determines the path we are following. Loops complicate the matter, but we make a simpliﬁcation by assuming that the same decision branch is taken
for each iteration of a loop for a given path.
Let us deﬁne the DCG as similar to a standard directed
acyclic graph, but with a fully ordered set of nodes (i.e. a

326

loops appropriately). This is calculated as part of the
WCET adaptation and can be used to ﬁnd cost excessive paths, but it is not used in any of the node ranking
calculations described later.
• BaseCost(node) - The base cost of the node, or the
cost of only the statements within the node. The base
cost is not path speciﬁc. It is calculated by counting
only ‘local’ statements in the node (not invoked methods or contained decision branches).
• M axT imes(node) - The maximum number of times
each node can be invoked by its parent(s). This value
is path speciﬁc (i.e. it may vary and depends on the
particular path). For decision branches, it is calculated
by starting with the M axT imes value for the containing method and multiplying by any loop bounds
in which the branch is contained (multiplied together
if nested). For methods, it is also calculated based
on the M axT imes of the invoking method and loop
bounds at the site of each invocation. All invocations
anywhere in the parent method are added together to
determine the total number of invocations from that
parent. We currently do not handle recursion.

Figure 2. A sample Decision/Call Graph (DCG)

method m or branch br) encountered as a program is executed, along with the set of edges that connect the nodes.
By convention, we draw the nodes in order from left to right
when we represent them graphically. We also draw the two
branches of a condition together to show that they are related, but only one of the branches of a given decision will
be part of any path of execution (from each unique parent).
The DCG can be represented formally as a set of nodes
and edges, as follows:
node
DCGN
edge
DCGE

=

m ∨ br

=
=
=

{node1 , . . . , noden }
{nodei , nodej }, nodei , nodej ∈ DCGN
{edge1 , . . . , edgem }

Note that an execution path will follow all invoked methods leading from a source node (assuming no early returns).
However, only one execution branch will be followed when
a decision is encountered. Therefore, when representing a
path (P ), we can omit the methods without loss of precision.
We represent a path formally as follows:
P

=

{br1 , . . . , brr }, with bri ∈ DCGN

During the calculation of worst case cost, we calculate
and save the following pieces of information for each node
in the DCG:
• M axCost(node) - The full cost of the node. This includes the cost of all children nodes (accounting for

We also calculate for each path:
• M axCost(path) - The maximum cost of the path.
The M axCost of a top level node will be produced by
the max cost path. However, we are also interested in
the (max) cost for all paths, not just the path of highest
cost.
A path’s max cost is calculated using the worst case cost
as a starting point and subtracting lower cost branches as we
traverse down the DCG, as follows:
float MaxCost(P) :// topLevelNode is the entry point
//
of the transaction
Set pathCost = MaxCost(topLevelNode)
For each decision branch br in P
// brs is the sibling branch of br
Set brDiff = MaxCost(brs) - MaxCost(br)
If brDiff > 0
pathCost -= (brDiff * MaxTimes(br))
return pathCost

Of course, another approach is to simply add the base
cost of each DCG node along the path multiplied by the
max number of times that node can be invoked in the path,
like this:

327

float MaxCost(P) :// Assuming that we already have calculated
// MaxTimes and BaseCost for each DCG node
Set pathCost = 0
For each DCG node nd along P
pathCost += BaseCost(nd) * MaxTimes(nd)
return pathCost

A straightforward but very important deﬁnition we
should make is for cost excessive paths. Given a cost
threshold T HR, we say that some path is cost excessive
if M axCost(path) > T HR. Note once again that the
cost threshold, though not arbitrary, is not calculated as
part of this work but is determined by business case analysis likely involving expected revenue, usage, resource costs,
and other information.

6

Characterizing Cost Excess

Enumerating cost excessive paths is important, but it is
even more helpful to understand the degree to which nodes
in the DCG contribute to the max cost. We make the following calculations to determine the magnitude to which a
DCG node contributes to cost threshold. Since these magnitudes are normalized by the amount of cost excess for a
given path, they can be compared between paths. Therefore,
we can ultimately determine a single/maximum magnitude
(or weight) for a given DCG node. This determination is
made without regard to actual path usage statistics, but other
approaches could account for usage information.
We start by calculating the raw cost excess for a particular path. This is simply the maximum cost of the path minus
the cost threshold:
CostExcess(path)

=

M axCost(path) − T HR

We next determine the ‘weight’ of the cost excess for a
path. The weight is intended to measure the magnitude of
the path’s cost excess as it measures the percentage of excess in relation to the threshold. It is calculated as follows:
P athW eight(path)

=

CostExcess(path)
T HR

Likewise, the weight of a node measures the contribution
of an individual node to a path’s cost (the percentage of the
path cost attributed to the node). It is calculated as follows:
N odeW eight(path, node)
BaseCost(node) × M axT imes(node)
=
M axCost(path)

path’s cost excess. Since it is ‘normalized’ by the path
weight, it can be compared to the normalized weights of
nodes from other paths. It is calculated as follows:
N ormalizedN odeW eight(path, node)
= N odeW eight(path, node) × P athW eight(path)
The overall normalized weight of a node, independent of
path, is simply the maximum normalized weight of the node
on any path and is calculated as follows:
N ormalizedN odeW eight(node)
= M ax(N ormalizedN odeW eight(path, node))

7

An Example

In our stock trading middleware example, we charge
$0.10 per transaction and we have set a $0.02 cost threshold for the transaction. This margin and the vast number of transactions that we handle allow us to pay staff
and other costs plus derive some proﬁt. Suppose we have
run our WCET adaptation and saved the M axCost and
BaseCost for each node in the DCG. We have also determined M axT imes of each node for each path as well as
M axCost of each path.
Now, let’s say path P1 is the servicing of a basic stock
purchase with minor database access plus average latency
and bandwidth with outside service providers. We determine that M axCost(P1 ) = $0.024 so this yields
CostExcess(P1 ) = $0.004
Furthermore, the path weight shows that P1 exceeds the
cost threshold by 20%.
P athW eight(P1 ) =

$0.004
= 0.2
$0.02

Since this is a common case, this cost excess may be
of concern. We also calculate the node weight for each
node along P1 . For example, let’s say there is a method
m1 along P1 (i.e. between two decision branches that deﬁne P1 ). The method’s base cost is $0.00004 and the max
number of times it can be called is 30. We now determine
that this node accounts for 5% of the path’s max cost.
N odeW eight(P1 , m1 ) =

$0.00004 × 30
= 0.05
$0.024

In order to make useful comparisons between other
nodes in the DCG, we normalize the node weight based on
the path’s weight. This yields
N ormalizedN odeW eight(P1 , m1 )
= 0.05 × 0.2 = 0.01

We can now determine a normalized node weight which
measures the magnitude of the node’s cost in terms of its

328

Table 1. An example of normalized node
weights
Method
m4
br15
m31
m1
br9
m11
···
m3

Normalized Weight
0.15
0.11
0.06
0.01
0.009
0.007
···
0.0005

For this example, let’s say that the normalized node
weights for m1 along all other paths were smaller than the
weight for P1 . Therefore, we conclude that the normalized
node weight of m1 is 0.01.
Now, let’s say that we have made similar calculations
for all the other nodes and paths. We could then order the
normalized node weights as shown in Table 1.
From this analysis, we might conclude that m1 is of average importance in terms of its contribution to cost excess.
We would likely be more concerned with m4 and br15 . We
would look at which cost excessive paths those nodes fall on
and try to determine how likely those paths are to occur. If
they are likely, we would focus on reducing the cost excess
of those nodes in order to reduce the overall probability of
cost excess.
Indeed, we ﬁnd that our common path P1 does contain
m4 . Upon further inspection, we ﬁnd that m4 invokes an
outside data service whose per transaction cost is no longer
market competitive. We decide that we must either renegotiate a better rate for the data service or ﬁnd an alternate
provider.

8
8.1

Evaluation Methodology
Research Prototype

Evaluation of this work was accomplished by implementing and verifying a research prototype of the WCET
adaptation described earlier. We built our prototype on the
Volta suite of tools [11] for WCET calculations on Java programs. By default, Volta’s target processor is JOP, hardware
speciﬁcally designed for running Java for real time embedded applications [17].
Volta is easy to use and already supports the concept of
pluggable strategies for various parts of the WCET calculations. Using a new cost based strategy along with other

minor changes to the framework proved to be a very reasonable approach for implementing a prototype of this work.
Adapting another WCET system for Java, like the one described in [18], might also be useful in the future.
The WCET adaptation calculates the max monetary cost
of a program starting at a root method. It also ﬁnds cost excessive paths and determines path independent normalized
node weights for the nodes in the input program’s DCG.
The implementation of the prototype relies heavily on
the control ﬂow analysis and execution time features of the
WCET system. It builds a DCG during the WCET/cost
analysis, and it calculates and saves base cost and max times
values for the nodes in the DCG. It then uses the algorithms
described in this work to determine path costs and node
weights.

8.2

Veriﬁcation Utility

We also implemented a veriﬁcation utility to test the
prototype. Whereas the research prototype performs static
analysis on an input program, the veriﬁcation utility executes programs and measures costs dynamically.
The veriﬁcation utility ﬁrst generates random programs
by producing a random DCG. Though the DCG is random,
it is bounded by several constants in the veriﬁcation utility
including a maximum number of children nodes for a given
node, a maximum program call stack depth, and the approximate ratio of decisions to method calls. These are designed
to be within reason for actual programs while keeping the
size of the random programs amenable to producing and analyzing many programs.
The veriﬁcation utility produces Java source code from
the random DCG. In addition to the methods and conditions which make up the DCG, the Java source code includes loops (randomly selected DCG nodes are surrounded
by loops). Furthermore, methods are chosen at random to
be annotated with the various resource costs described in
this work (bandwidth, service, and storage usage).
It is not uncommon to use generated programs to verify
static analysis tools. In [9], refactoring engines are tested
using generated programs. An approach for testing an optimizing compiler using randomly generated programs is described in [15].
Costs/rates for each of the different cloud resources (processing, storage, bandwidth) are ﬁxed randomly for each
program within appropriate bounds. The cost threshold
for each random program is also generated randomly and
within a reasonable bound.
The Java source code produced by the veriﬁcation utility introduces some limitations. The DCGs on which they
are built contain no cycles and children nodes always have
exactly one parent. The bodies of decision branches always
consist of exactly one method, though method bodies con-

329

tain a random (but bounded) number of method calls. Also,
loop bodies always consist of exactly one DCG node (either
decision or method call), and the loops themselves always
execute to their annotated bounds. If this were not the case,
it would be difﬁcult to use the random programs to verify
the accuracy of the research prototype. On the other hand, it
may better simulate actual variations in runtime conditions,
depending on the particular application and domain.
The root method in the generated source accepts a random integer as input. This integer is used in the condition
of each decision in the program. Each decision randomly
selects a bit position to mask and test. When the bit is set,
the then branch is taken otherwise the else branch is taken.
This allows for a set of randomly generated inputs to produce an extensive variety of control ﬂows.
The overall idea is to generate enough random programs
and execute the random programs with enough random inputs so that a credible list of paths and calculated node
weights can be built. Both the veriﬁcation utility and the
particular structure of the random programs themselves allow for capturing the cost of a particular execution of the
program on a given input as well as the other essential information for each node (base cost and max times) required
for the additional calculations described here.
If the cost of a path (i.e. one execution of the program for
a given random input) exceeds the predetermined (but random) threshold, then the additional calculations described
in this work are performed to determine the path independent normalized node weight for each node that appears
somewhere in a cost excessive path.
The veriﬁcation utility also orchestrates the entire process of producing the random DCG, converting it to Java
source code, compiling the Java source code, then executing
it on random inputs. All of this was accomplished without
much trouble using Java features including dynamic compilation as well as extensive use of the Reﬂection API.
Our goal for the veriﬁcation utility is to produce two
pieces of data that we can compare with corresponding data
produced by the static analysis performed in the research
prototype. First, the cost excessive paths found by the veriﬁcation utility should always be a subset of those found by
the research prototype. Second, the path independent normalized node weight values of the veriﬁcation utility should
be very close to those of the research prototype.

weights determined by the veriﬁcation utility were captured
and saved. The program was then analyzed by the research
prototype and the output data were then compared to the
data saved from the random test runs of the program.
We veriﬁed that all cost excessive paths found by the veriﬁcation utility were also found by the research prototype.
This met our expectation that the cost excessive paths from
the veriﬁcation would be a subset of those found by the research prototype.
Of course, this does not fully guarantee that the research
prototype is correct in this aspect, since there was no analysis of the cost excessive paths identiﬁed by the prototype
but not by the veriﬁcation utility. It is assumed that those
paths (if any) were simply not found by any of the random
test runs. However, future work could perform further analysis on these paths to verify (independent of the research
prototype itself, since it is the component under test here)
that they are indeed cost excessive.
We also found a close correlation between the path independent normalized node weights of the veriﬁcation utility
and the research prototype. For those nodes with a nontrivial weight (i.e. at least 0.0001), the median weight difference between the veriﬁcation utility and research prototype
was just 4.2%. We feel this demonstrates that the prototype is meeting its objectives in this aspect and making its
calculations accurately.
Looking through the test data and the source code of randomly generated programs, we also made some anecdotal
observations. First, when all cost excessive paths are found
by test runs in the veriﬁcation utility, differences in node
weights from the research prototype are most likely due to
execution time measurements and are generally very small.
More signiﬁcant differences in node weights are probably
introduced when some cost excessive paths are not found
by the veriﬁcation utility.
Also, highly weighted nodes can often be predicted simply by scanning the source code. Those nodes that have
annotated resource costs, those that are called within loops,
and those with ancestor methods called within loops are almost always the nodes that are weighted more heavily. This
does not trivialize the importance of the research prototype
since real world programs can be very large and complex so
that simple visual analysis would likely be prohibitive.

10
9

Conclusion

Results

To ensure diversity in testing inputs for the research prototype, 1000 random programs were generated by the veriﬁcation utility. As described earlier, the programs varied in
size and complexity within certain bounds. For each random program, 10000 random inputs were generated and
provided as input to the program. The paths and node

Finding cost excessive paths in cloud applications can
be an important part of predeployment veriﬁcation. It is
a type of nonfunctional testing that is particularly important for cloud services with usage based pricing. Without
it, we increase our risk of scaling up resource usage to meet
high demand while possibly losing money on important and
costly transactions.

330

We have described an approach for determining paths
within a cloud application that can exceed a cost threshold.
These paths, along with the most important/costly nodes
within a program’s control ﬂow can be brought to the attention of developers before an application is deployed. This
can help enable business case analysis that ultimately ensures economic success for the cloud application.
By building on the research and tools of worst case execution time analysis, the approach presented here inherits
all the beneﬁts of a well established ﬁeld as well as its limitations (and the work in progress to address them).

11

Future Work

Future work could build more intelligence into our algorithms about how our cloud applications are likely to be
used. For example, we might associate control ﬂow paths
with operational proﬁles. Since operational proﬁle data includes a probability that the associated range of inputs will
be chosen [14], we would then be able to better inform our
path costs and node ranking. We might even derive an overall probability that our application (or a particular transaction) will exceed a cost threshold. This could be an important dashboard style metric useful in project management.
Addressing the limitations of this work would also be
important. Aside from the limitations of WCET, one of the
biggest limitations in our adaptation is its reliance on user
inserted annotations of ﬁxed bounds on resource usage. An
improvement could be to assign a range of values and perhaps a probability for each value. Ultimately, we’d like to
free the user from inserting any annotations. This could require much more extensive data ﬂow analysis.

References
[1] S. Agarwala, D. Jadav, and L. Bathen. Icostale: Adaptive
cost optimization for storage clouds. In Cloud Computing,
IEEE International Conference on (CLOUD), July 2011.
[2] Amazon. Amazon cloudwatch, Feb. 2012.
[3] M. Armbrust, A. Fox, R. Grifﬁth, A. D. Joseph, R. Katz,
A. Konwinski, G. Lee, D. Patterson, A. Rabkin, I. Stoica,
and M. Zaharia. A view of cloud computing. Commun.
ACM, 53:50–58, April 2010.
[4] R. Bala and S. Carr. Usage-based pricing of software services under competition. Journal of Revenue and Pricing
Management, 9(3):204–216, 2010.
[5] J. Blieberger. Real-time properties of indirect recursive procedures. Inf. Comput., 171:156–182, January 2002.
[6] K. Buell and J. Collofello. Transaction level economics of
cloud applications. Services, IEEE Congress on, 0:515–518,
2011.
[7] R. Buyya, C. S. Yeo, S. Venugopal, J. Broberg, and
I. Brandic. Cloud computing and emerging it platforms: Vision, hype, and reality for delivering computing as the 5th
utility. Future Gener. Comput. Syst., 25:599–616, June 2009.

[8] R. Chapman, A. Burns, and A. Wellings. Worst-case timing
analysis of exception handling in ada. In Proceedings of the
Ada UK Conference, pages 148–164. IOS Press, 1993.
[9] B. Daniel, D. Dig, K. Garcia, and D. Marinov. Automated
testing of refactoring engines. In Proceedings of the the 6th
joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations
of software engineering, ESEC-FSE ’07, pages 185–194,
New York, NY, USA, 2007. ACM.
[10] E. Deelman, G. Singh, M. Livny, B. Berriman, and J. Good.
The cost of doing science on the cloud: The montage example. In High Performance Computing, Networking, Storage
and Analysis, 2008. SC 2008. International Conference for,
pages 1 –12, 2008.
[11] T. W. Harmon. Interactive worst-case execution time analysis of hard real-time systems. PhD thesis, University of
California, Irvine, 2009.
[12] T. A. Henzinger, A. V. Singh, V. Singh, T. Wies, and D. Zufferey. A marketplace for cloud resources. In Proceedings of
the tenth ACM international conference on Embedded software, EMSOFT ’10, pages 1–8, New York, NY, USA, 2010.
ACM.
[13] R. Jayaseelan, T. Mitra, and X. Li. Estimating the worstcase energy consumption of embedded software. In Proceedings of the 12th IEEE Real-Time and Embedded Technology and Applications Symposium, pages 81–90, Washington, DC, USA, 2006. IEEE Computer Society.
[14] J. Musa. Operational proﬁles in software-reliability engineering. Software, IEEE, 10(2):14 –32, Mar. 1993.
[15] M. H. Palka, K. Claessen, A. Russo, and J. Hughes. Testing
an optimising compiler by generating random lambda terms.
In Proceeding of the 6th international workshop on Automation of software test, AST ’11, pages 91–97, New York, NY,
USA, 2011. ACM.
[16] S. Pandey, A. Barker, K. Gupta, and R. Buyya. Minimizing execution costs when using globally distributed cloud
services. In Advanced Information Networking and Applications (AINA), 2010 24th IEEE International Conference
on, pages 222 –229, Apr. 2010.
[17] M. Schoeberl. JOP: A Java Optimised Processor for Embedded Real-Time Systems. PhD thesis, Vienna University
of Technology, 2005.
[18] M. Schoeberl, W. Pufﬁtsch, R. U. Pedersen, and B. Huber. Worst-case execution time analysis for a java processor.
Softw. Pract. Exper., 40:507–542, May 2010.
[19] K. Tsakalozos, H. Kllapi, E. Sitaridi, M. Roussopoulos,
D. Paparas, and A. Delis. Flexible Use of Cloud Resources
through Proﬁt Maximization and Price Discrimination. In
Proc. of the 27th IEEE Int. Conf. on Data Engineering
(ICDE’11), Hannover, Germany, Apr. 2011.
[20] R. Wilhelm, J. Engblom, A. Ermedahl, N. Holsti,
S. Thesing, D. Whalley, G. Bernat, C. Ferdinand, R. Heckmann, T. Mitra, F. Mueller, I. Puaut, P. Puschner, J. Staschulat, and P. Stenström. The worst-case execution-time
problem–overview of methods and survey of tools. ACM
Trans. Embed. Comput. Syst., 7:36:1–36:53, May 2008.

331

IEEE TRANSACTIONS ON EDUCATION, VOL. 43, NO. 4, NOVEMBER 2000

389

University/Industry Collaboration in Developing A
Simulation-Based Software Project Management
Training Course
James S. Collofello, Member, IEEE

Abstract—A significant factor in the success of a software
project is the management skill of the project leader. The ability to
effectively plan and track a software project utilizing appropriate
techniques and tools requires training, mentoring, and experience.
This paper describes a collaborative effort between Arizona State
University and Motorola University to develop a software project
management training course. Although many such courses exist
in academia and industry, this course incorporates a system
dynamics simulator of the software development process. The
use of this simulator to train future software project managers is
analogous to the use of a flight simulator to train pilots. This paper
describes the software project simulator and how it is utilized in
the software project management training course. Feedback from
the training course participants is also shared and discussed.
Index Terms—Simulation, software project management.

I. INTRODUCTION

A

SUCCESSFUL software project is one that meets the customer’s requirements, possesses high quality and is delivered on time and on budget. Unfortunately, many software development efforts do not meet these success criteria and lead
to project failures. Many of these projects may not, however,
have been a failure if common project management pitfalls had
been avoided. Boehm notes in [1], “Poor management can increase software costs more rapidly than any other factor.” Moreover, both Jones and Statz note inadequate project management
training and inadequate standards with which to judge project
management performance as the root causes of poor project
management [2], [3].
Software project management involves, among other tasks:
planning, tracking, and control of a software development
project. These abilities require two things: knowledge, such as
knowing common project management pitfalls, and skills, such
as the ability to recognize those pitfalls and formulate a new
project plan. Some of the methods by which software project
management skills can be improved include:
• reading relevant texts and papers;
• working with a mentor;
• studying appropriate case studies;
• trial and error;
• classroom instruction;
Manuscript received August 16, 2000.
The author is with the Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287-5406 USA (e-mail:
collofello@asu.edu).
Publisher Item Identifier S 0018-9359(00)10234-1.

Fig. 1. A cause–effect relationship.

• practicing with a simulator.
The motivation for exploring computer simulation as a software project management training vehicle is best introduced by
analogy. Software project management is a highly complex, ongoing process similar to flying a plane in rough conditions. A
pilot must plan the course, continually track progress, monitor
the condition of the plane, and react to problems as they are
encountered. Pilots are often trained in flight simulators which
provide the pilot with experience in many different scenarios
without the high risk and expense of learning using a real plane.
A pilot could not be expected to learn piloting skills from a text
or instructor alone, because a pilot needs hands-on, active experience to become effective. Similarly, hands-on, active experience is needed to build effective project management skills. The
basis for this flight simulation analogy was given in [4] which
was built upon the system dynamics modeling of the software
development process documented in [5].
II. SOFTWARE PROCESS SIMULATION BACKGROUND
Simulation has been used in engineering disciplines for
many years to great advantage. In recent years, an increasing
amount of attention has been paid to using simulation to
advance software engineering. According to Christie [6],
there are a number of areas in which simulation can benefit
software engineering, including: assessing the costs of software
development, supporting metric collection, building consensus
and communication, requirements management, project management, training, process improvement, risk management,
and acquisition management. This paper will focus on the
training benefits of simulation. In particular, simulation based
on system dynamics models will be addressed.
System dynamics models are based on cause-effect relationships that are observable in a real system. Fig. 1 provides an
example of a cause-effect relationship that might be observed
during software development. The cause-effect relationship
indicates that the time developers spend on quality activities
impacts the number of undiscovered defects remaining in the
software product. The “I” on the arrow indicates that an inverse
relationship exists between the two entities. In this example,

0018–9359/00$10.00 © 2000 IEEE

390

IEEE TRANSACTIONS ON EDUCATION, VOL. 43, NO. 4, NOVEMBER 2000

training. The approach used here is consistent with the objectives of the others but differs in the level of project management
control as previously discussed.
III. INTEGRATION OF SIMULATOR INTO TRAINING COURSE

Fig. 2.

Feedback loops.

when “time spent on quality activities” increases, the number
of “undiscovered defects in the product” decreases. When
the “time spent on quality activities” decreases, the number
of “undiscovered defects in the product” increases. A second
type of cause-effect relationship exists, in which a direct
relationship exists between the two entities. These relationships
are indicated by a “D” label on the arrow.
Cause–effect relationships in a simulation model are constantly interacting during a simulation. The most powerful feature of this type of simulation model is realized when multiple
cause–effect relationships are connected forming a circular relationship, known as a feedback loop. The concept of a feedback
loop reveals that any entity in a system will eventually be affected by its own action. Fig. 2 provides a simple example of
the two types of feedback loops.
The first type of feedback loop is called a reinforcing loop.
Reinforcing loops are often characterized as modeling a downward spiral. In Fig. 2, the reinforcing loop is labeled with a
“ ” sign. In the example, as the developers perceive that their
“ability to meet the schedule” is lessened, they spend less “time
on quality activities,” which results in an increase in “undiscovered defects in the product,” which further lessens their “ability
to meet the schedule,” and so on. Without anything to stop it,
this loop will spiral out of control. A second type of feedback
loop, called a balancing loop, has the purpose of reigning in reinforcing loops. Balancing loops bring the system back into balance. In Fig. 2, the balancing loop is labeled with a “ ” sign.
In the example, as the developers perceive that their “ability to
meet the schedule” is lessened, they spend less “time on quality
activities,” which they perceive to increase their “ability to meet
the schedule.”
The literature contains numerous descriptions of successful
applications of simulation models of the software development
process demonstrating the growing maturity of this discipline
[4], [7]–[15]. Most of these models, however, limit the software
development process to the waterfall development approach.
They are also modeling at a high level often lumping together
related activities to simply the modeling. For example, software
inspection and testing activities might be lumped into a more abstract quality assurance function. The model described here provides a low-level concurrent incremental development process
modeling simulation capability.
The use of simulators for management training is not unique,
and has been common in the project management area for some
time. There are also software project management consultants
who utilize simulators in their software project management

Arizona State University (ASU) and Motorola began their
work together with the objective of developing a system dynamics model of the software development process at a level
suitable for performing software process improvement activities. Several such models were developed over a period of about
four years. One of the models was brought into a Motorola University software project management training course to pilot its
capabilities in support of software project management training.
The demonstration was successful and lead to the updating of
the course to fully integrate the management process simulator.
The current version of the software project management training
consists of three days of training providing instruction on software project planning and tracking activities. In the remainder
of this section, the simulator will be described along with the
class exercises which utilize it.
A. Description of Software Project Management Simulator
The software project management simulator is implemented
in iThink software from High Performance Systems, Inc. The
highest level input panel is shown in Fig. 3. Additional layers of
menus provide more detailed inputs. User inputs to the simulator
are clustered as follows.
1) Model Initialization: Contains underlying metrics
driving the simulator such as productivity rates, defect rates,
and cost to repair defects.
2) Project Information: Contains information specific to the
project to be simulated including:
— planned completion time;
— project complexity;
— time allocated to rework;
— time allocated to inspections;
— time allocated to regression testing;
— degree of inter-increment communication overhead applicable to the project.
3) Increment Scheduling Information: Enables estimates to
be entered for each development phase of each increment. Dependencies among increments can also be entered.
4) Staffing: Enables staff to be assigned to each phase of
each increment. Three levels of staffing expertise are provided.
Once all of the inputs are entered, the simulator can be started
and paused at any time. The simulator can also be set to stop at
predetermined points such as monthly or completion of milestones. Once the simulator has stopped, the various input parameters can be adjusted. While the simulator is running various output panels can be observed. An example of a highlevel output panel is shown in Fig. 4. As can be seen from
Fig. 4, this screen has monitoring devices, some in the form of
number boxes while others in the form of devices that resemble
a speedometer in appearance. Also placed on the control panel
are two push buttons that would take the user to two different
sets of graphs. Each of the variables showed in Fig. 4 are explained below.

COLLOFELLO: UNIVERSITY/INDUSTRY COLLABORATION

391

Fig. 3. High-level simulator input panel.

1) Current Staff Load: The number displayed here at any
instance is the sum total of all the engineers working on the
project at the time this value is monitored.
2) Elapsed Man Hours: This variable displays the total
number of man hours that have been consumed since the project
has started.
3) Elapsed Days: The number of days the project has taken
so far.
4) Remaining Hours Boxes: This section depicts the estimated number of hours left for each increment.
5) Schedule Pressure Gauge: If the project is lagging
behind schedule, schedule pressure will increase. The current
schedule pressure on the engineers is monitored here. A
value of zero indicates no schedule pressure and six indicates
maximum schedule pressure.
6) Exhaustion Rate Gauge: The exhaustion rate is an indication of how stressed the engineers are. The exhaustion rate is
depicted on a scale of zero to 160. Once the team reaches maximum exhaustion, the productivity of the project decreases until
the team is de-exhausted.
7) Percent Complete Button: The percent complete button
takes the user to a graph that plots the perceived percentage
completion of the project.
8) Remaining Hours Button: This group displays data
stating the number of hours left to be completed in each of the
phases of all the increments.
9) Outputs for Earned Value Project Management: On another panel, outputs are provided to support earned value project

management. The values for the budgeted cost work scheduled
(BCWS), budgeted cost work planned (BCWP) and the actual
cost work performed (ACWP) are plotted as graphs and also displayed in the monitoring devices.
B. Simulation Exercises
In this section, each of the exercises utilizing the simulator
will be described. The exercises follow lecture material and examples presenting the concepts addressed by the exercises.
1) Life Cycle Model Comparison: The objective of this exercise is to illustrate the differences of various software development life cycle models and factors that govern their selection.
The first part of the exercise has students simulating a project
using both the waterfall and sequential incremental development life cycles. Students observe the additional overhead associated with incremental development but also the additional
benefits derived by a smaller development effort and earlier
quality feedback. The students then re-execute both life cycle
models on the same project varying inspection effectiveness.
Students observe that incremental development will mitigate the
impact of weak inspections better than waterfall development.
The students then re-execute both life cycle models varying
project complexity. Students observe that incremental development is advantageous as complexity levels increase. Finally,
students are asked to create a concurrent incremental project
varying staffing levels. Students observe that concurrent incremental development can lead to a shorter development life cycle
if additional staffing is available.

392

IEEE TRANSACTIONS ON EDUCATION, VOL. 43, NO. 4, NOVEMBER 2000

Fig. 4. High-level simulator output panel.

2) Risk Management: The objective of this exercise is to illustrate the impact of various project risk factors and contingency plans on a project. Students initially create a concurrent
incremental development project and assess the risk of underestimating the complexity of the project. Various contingency
plans are then simulated to assess their impact at different points
in time. The students then simulate the impact of losing key personnel at various points of time along with the impact of specified contingency plans.
3) Software Inspections: The objective of this exercise is to
illustrate the impact of software inspections on software development and testing schedules. Students create a software project
and simulate the impact of varying levels of inspection commitment on testing time and overall project completion date.
4) Critical Path Scheduling: The objective of this exercise
is to illustrate the importance of managing the critical path. Students are given a set of tasks, task durations. and dependencies
and asked to identify the critical path. Students then simulate the
project varying the experience level of staff assigned to critical
path activities to assess the impact.
5) Overall Planning and Tracking: The final exercise serves
as the capstone project for the course. Students are given the responsibility of planning and tracking a simulated project from
inception to completion. Information available for planning includes:
— features to be implemented;
— estimated development time per feature;
— feature dependencies;
— available staff.

Students develop an overall schedule and plan for the project
including:
— planned start and end date for each phase of each increment;
— planned staffing for each phase of each increment;
— planned rework;
— percent of effort allocated to inspections;
— allocation for regression testing.
Once the plan is completed students simulate the project
based on their plan and track the project’s performance. Periodic reports are written for “upper management” and decisions
must be made concerning adding staff, reducing functionality,
or delaying the delivery date. This final exercise also provides
an opportunity for the students to react to “surprises” in a
project such as requirements changes or loss of personnel.
IV. EVALUATION AND FUTURE WORK
The updated course integrating the simulator was taught in
the fall of 1999 at Motorola University and again in the summer
of 2000. Students completed a written course evaluation at the
end of the course and participants from the first class were also
invited back for a more detailed critique a few weeks later. All
participants found the use of the simulator added significantly
to the value of the course and achieved each of the exercise
objectives. Additional unanticipated learning and insight was
gained by the participants while “playing” with the simulator
during class “free time.” Upon completing exercises participants often continued to vary model parameters and observe the

COLLOFELLO: UNIVERSITY/INDUSTRY COLLABORATION

results. Through informal discussion among the participants and
instructor, the results were scrutinized and assessed for reasonableness. When the model replicated expected results, students
gained new confidence in their skills beyond those practiced in
the exercises. Even when the model did not generate reasonable
results, typically due to the level of modeling abstraction, the
discussion among the students and the instructor about the parameters and relationships not addressed by the model provided
an in-depth learning opportunity. Based on these experiences,
instructors utilizing simulation models in their courses should
provide additional opportunities for participants to experiment
with the model in addition to the structured class exercises.
The participants also learned the value of software process
simulations in software process improvement and management
activities and inquired about how they might utilize software
process simulation models in the workplace.
The basic capabilities of the project management simulator
are continually being expanded and refined. Several additional
course offerings are planned at Motorola University and the
feedback from these offerings will be used to further refine the
management simulator and its exercises. A web-based graduate
level software project management course is also being developed at ASU utilizing the simulator.

393

[6] A. M. Christie, “Simulation: An enabling technology in software engineering,” CrossTalk, April 1999.
[7] B. J. Smith, N. Nguyen, and R. F. Vidale, “Death of a software manager:
How to avoid career suicide through dynamic software process modeling,” Amer. Programmer, pp. 10–17, May 1993.
[8] R. Rembert Aranda, T. Fiddaman, and R. Oliva, “Quality microworlds:
Modeling the impact of quality initiatives over the software product life
cycle,” Amer. Programmer, pp. 52–61, May 1993.
[9] K. J. Chichakly, “The bifocal vantage point: Managing software projects
from a systems thinking perspective,” Amer. Programmer, pp. 18–25,
May 1993.
[10] K. G. Cooper and T. W. Mullen, “Swords and plowshares: The rework
cycles of defense and commercial software development projects,”
Amer. Programmer, pp. 41–51, May 1993.
[11] C. Y. Lin, “Walking on battlefields: Tools for strategic software management,” Amer. Programmer, pp. 33–40, May 1993.
[12] R. J. Madachy, “System dynamics modeling of an inspection-based
process,” in Proc. 18th Int. Conf. Software Eng., Berlin, Germany, Mar.
1996.
[13] H. Rubin, M. Johnson, and E. Yourdon, “With the SEI as my copilot
using software process ‘flight simulation’ to predict the impact of improvements in process maturity,” Amer. Programmer, pp. 50–57, Sept.
1994.
[14] Proc. Software Process Simulation Modeling Workshop: Oregon Center
Advanced Technol. Educ., 1998.
[15] Proc. Software Process Simulation Modeling Workshop: Oregon Center
Advanced Technol. Educ., 1999.

REFERENCES
[1] B. Boehm, Software Engineering Economics. Englewood Cliffs, NJ:
Prentice-Hall, 1981.
[2] C. Jones, Applied Software Measurement: Assuring Productivity and
Quality. New York: McGraw-Hill, 1991.
[3] J. Statz, “Training effective project managers,” Amer. Programmer, pp.
43–48, June 1994.
[4] H. A. Rubin, M. J. Johnson, and E. Yourdon, “Software process flight
simulation: Dynamic modeling tools and metrics,” Inform. Syst. Management, summer 1995.
[5] T. Abdel-Hamid and S. E. Madnick, Software Project Dynamics: An
Integrated Approach. Englewood Cliffs, NJ: Prentice-Hall, 1991.

James S. Collofello (M’79) received the B.S. and M.S. degrees in mathematics
and computer science from Northern Illinois University, De Kalb, and the Ph.D.
degree in computer science from Northwestern University, Evanston, IL.
He is a Professor in the Department of Computer Science and Engineering
at Arizona State University, Tempe. His research and teaching interests include
software engineering with an emphasis on software project management, software process improvement, and software quality assurance. He is also very
active in software engineering education activities and has written curriculum
modules for the education group at the Software Engineering Institute.
Dr. Collofello is a member of the IEEE Computer Society and the Association
for Computering Machinery.

0

U A L I T Y

moommmommomom

A S S U R A N C E
moommolvmnmmmmmmm

Software Quality
Assurance for
Maintenance
James S. Collofello, Arizona State University
Jeffrey J. Buck, GTE Communication Systems

Maintenance plays a
vital role in protecting
quality as a system
evolves. The results of
this study pinpoint how
to make maintenance a
part of SQA.

46

O ver the past few years, abundant
knowledge has been acquired and
published on software quality
assurance. Much of it, however, is
oriented toward the development of new
software products. Although the development of high-quality products is certainly
important, it is equally important to preserve the quality of these products as they
are maintained.
We define software maintenance to
include activities undertaken during the
operational phase of the software: correcting errors, adding new capabilities, deleting obsolete capabilities, and optimizing
the software.'
Performing maintenance is difficult,
and the problems associated with it are
beginning to get more attention, as evidenced by recent software maintenance
workshops, conferences, and publications. Although there are many problems
to contend with, three issues are central:
(1) the difficulty of performing maintenance activities, (2) the high degree of
error associated with performing maintenance tasks, and (3) the low morale of
software maintainers.
0740-7459/87/0900/0046/$01.00 (©1987 IEEE

The primary difficulty in performing
maintenance tasks stems from the fact that
the complexity of large systems increases
dramatically over time, largely due to
deteriorating program structure.2 This
difficulty is compounded by inaccurate,
outdated documentation and flagrant violations of programming standards.
The high degree of error associated with
performing maintenance tasks is often a
direct result of the programs' increasing
complexity. One organization reported at
a major computer conference that 55 percent of all one-line maintenance changes
and 80 percent of all changes were incorrect on the first run. Another study by
Hughes Aircraft found that as much as 17
percent of error fixes are performed incorrectly.3 These alarming statistics are probably comparable for many organizations.
The morale problems are many and too
complex to cover in detail here. Part of the
problem, however, is related to the lack of
modern software-engineering and QA
approaches in software maintenance and
to the maintainers' perception that they
are working without support. There is also
widespread pressure during maintenance

IEEE Software

.01MMMIMENUM

activities to produce quantity, not quality.
In light of these serious problems, the
need to apply cost-effective SQA practices
to maintenance activities is evident. These
maintenance SQA practices must be established and followed if the quality of a
product is to be preserved, yielding the
long-term benefits for which the original
investment in quality was made.

Project study

To learn more about the areas where
SQA might be cost-effective, we investigated a large maintenance project. We
selected a version of a large, Centrex-type
telephone switching system for our study.
The version we analyzed differed from the
preceding version in that some callprocessing features had been added,
known problems had been corrected, some
existing features had been partially reimplemented, and attempts had been made
to correct defects identified during testing.
The two software subsystems where the
vast majority of maintenance changes
took place comprised approximately one
third of the total base-system source code.
Four percent of the lines of source code in
these two subsystems had been added or
changed to accommodate this maintenance effort.
We selected this particular project
because it provided an excellent example
of software maintenance, as opposed to
the design of a new software product. The
base version to which changes were made
was stable and was operating in the field
with few problems.
The project managers estimated designeffort percentages expended for the
software-maintenance categories of correction, optimization, and addition of new
capabilities were 10, 10, and 80 percent,
respectively. They also estimated the percentage of design effort expended within
each main subprogram. The percentage of
effort estimated for the central callprocessing, database administration,
device call-processing, and miscellaneous

September 1987

mmummmmmumwmm

other subprograms was 60, 30, 5, and 5
percent, respectively. These planned
changes to the system were implemented
in parallel and then placed together into a
testing environment. Limited reviews were
performed during this maintenance
process.
A complete testing effort was performed on the product. This effort concentrated not only on features or areas
which were expected to be affected by the
changes, but also on features not expected
to be affected. This effort differed from
other projects in the organization, where
the testing effort concentrated primarily
on the design enhancements being made.
We felt that this was the most practical
way to determine how the changes affected
the product. Nearly all testing performed
consisted of black-box functional testing,
similar to how the customer would use the
product. During testing, the source-code
modules of the system were periodically
recompiled, with changes being added to
correct the software defects being found.
After some limited testing for stability, the
testing effort continued, using this latest
source-level version of the modules.

possible values for origin.
* The physical-location field identified
in which subprogram the defect was found
and corrected. Figure 2 lists the possible
values for physical location.
* The defect-type field documented
what type of defect was detected. Our
defect categories were documentation,
logic, interface, standards, environment,
and miscellaneous. Figure 3 lists the possible values for defect type.
* The defect-cause field identified the
cause of the defect detected. The cause categories deal with the existence of the concept being verified, problems in how that
concept was understood and communicated, problems in how that concept was
translated into the product, and unknown.
Figure 4 lists the possible values for defect
cause.

Results

All defects detected were documented
using a formal reporting system. A direct
result of categorizing each defect was that
we were able to view the data several ways.
This was crucial to obtain insights into
some of the dynamics of the maintenance
environment.
In summary, we found that:
* 53 percent of the defects were found
We adopted a postmortem defectfeatures that were previously operain
categorization scheme to capture relevant
tional.
Our
defect.
each
information about
* 10 percent of the defects were introscheme documented when the defect was
introduced, into what subsystem it was duced as a result of corrections being
introduced, its type, and its cause.4 Based made. However, this percentage is vastly
on information in the defect report, each understated, because we counted only
those defects not detected by regression
identified product defect was analyzed.
The defect report had four data fields: testing performed at the time.
* For the most part, the defects found
- The origin field identified when the
in the group of changed modules.
occurred
We
defect was introduced into the system.
* 64 percent of the defects were caused
wanted to know if the defect was introduced into new features being added by by incorrect logic errors, as opposed to
product-enhancement code changes, into missing and extra logic errors.
* 79 percent of the defects were caused
previously functioning features as a result
problems related to production and
by
a
result
as
code,
product-enhancement
of
of corrections being made, or as a result of understanding.
Figure 5 shows where the defects were
residual product defects. Figure 1 lists the

Defect categories

47

-The defect was found by the execution of a test case (or operation of a feature) verifying a new feature or capability that was added to the product.
The individual test case or user sequence detecting the defect must not have.
been successfully executed.
2 - The defect was found by the execution of a test case (or operation of a feature) verifying a feature that existed in the previous version of the product.
The individual test case or user sequence detecting this defect functioned
properly in the previous version of the product and had not yet been
executed on the version of the product being tested.

3

-

The defect was found by the execution of a test case (or operation of a feature) verifying a feature determined to be previously functioning properly
on the version of the product being tested. The defect was introduced into
the product as a result of corrections to other defects.

4 - The defect was found by the execution of a test case (or operation of a feature) that exists in the base version of the product and the test case or user
sequence detecting this defect was never tested before. This type of defect is

called residual.

Figure 1. Defect origin.
R - The defect was resolved in the database-administration subprogram (called
a recent change).

C - The defect was resolved in the central call-processing subprogram.
D - The defect was resolved in the device call-processing subprogram.

S - The defect was resolved in the device status-processing subprogram.

0 - The defect was resolved in one of the other processing subprograms.

Figure 2. Physical location.
D

Documentation
F Clarification
M Missing
E Extraneous
I Incorrect
C Clerical

I

Interface
Man/machine
D
Database
H
Hardware
S Software
M

-

-

E
L

F
M
E
I

S

Logic

Clarification

Missing

Extraneous
Incorrect

Standards

Figure 3. Defect type.
48

S
E
T
R

NT
NA
TL
CO

Environment
Size
Efficiency
Timing
Recover
No trouble found
Not addressed, deferred
Tool-related
Copy or duplicate

introduced into the system. Several things
are significant. First, defects reported in
previously working features constituted
more than 50 percent of the corrections
that needed to be made. Obviously, we
have not achieved the goal of adapting and
perfecting a system with a nominal adverse
effect. Unexpected side effects to other
system features and capabilities as a result
of using current software maintenance
methodologies continues to plague software products.
Second, defects that were introduced
into the system as the result of corrections
being applied accounted for about 10 percent of the total defects reported. These
were defects that were not found during
the limited regression testing, which was
performed after the change had been
implemented. The defects that were found
as a result of the limited regression testing
were not formally documented. Instead,
they were rejected and sent back to the
programmer whose change caused the
problem. It was not uncommon to make
multiple attempts to resolve the defect.
One result of using a formal defectreporting system is that the number of
defects (as well as the effort involved) is
seriously understated. Estimates from the
individuals involved and a sample of the
formal defect reports showed that about
65 percent of the initial solutions to correct
defects were themselves faulty.
Detecting these potential defects with a
limited regression testing effort kept them
out of the product. If all of them had been
reported, they would have accounted for
more than 75 percent of the total number
of defects reported and corrected. Consistent with other maintenance research and
folklore, attempts to correct defects are
extremely prone to the ripple effect.5
Figure 6 shows the estimate of the percentage of effort spent in the functional
areas of the software versus the actual percentage of defects identified and resolved.
The first number represents the estimated
effort and the second the percentage of
defects actually reported. This figure
shows that the percentage of defects
experienced in the subprogram was about
the same as the percentage of effort
expended to modify the subprogram.
Although coming from a single study, this
observation has some potentially imporIEEE Software

P - Concepts
M - Missing concept
E - Extraneous concept
I - Incorrect concept
C - Conflicting concept

D - Producing the deliverable
T - Translation of the concept
tant implications in terms of how much
Figure 12 shows the distribution of
S - Caused by support environment
defect
causes. While our classification
and where SQA activities might be effiU - Use of the support environment
ciently applied in a maintenance envi- scheme provided for a more detailed
M - Methods specified for
ronment.
categorization of defect causes, the broad
producing the deliverable
Figure 7 shows the distribution of the categories are concept-related (requireP
Producer's
use of methods
type of corrections that were needed to ments),
communicationand
resolve reported defects. Although more comprehension-related, and productionC - Communication of concepts
finely partitioned in Figure 3, the broad related.
D - Documentation
Only
slightly
more
than
40
percent
of
categories of defect types can be defined
U - Understanding
as documentation, missing logic, extra the defects were associated with implemenB - Between team members
logic, incorrect logic, interface, and tation of the design into code. Nearly as
- Between review teams
T
many (38 percent) were caused by commuenvironment-related.
- To customer
C
nication
and
comprehension
problems
It is not surprising that the total of
F
From customer
experienced
by
the
individual
programmer
to
the
is
code
close
to
100
changes
percent
(because changes are what resolve defects), (or between programmers). Another 21
UN - Cause unknown
but what is interesting is the breakdown of percent were attributable to problems in
what kinds of changes were made. Fully 64 the requirements and design documen- Figure 4. Defect cause.
percent of the defects were resolved by cor- tation.
recting or moving existing logic in a module. Missing logic accounted for 23 percent
Residual
of the defects. And problems associated
product
~~~~~~~~Defects to
defects: 17%
with software interfaces contributed to 5
new features: 21%
percent of the defects.
Figures 8 through 11 present these
defect types in relation to when the defects
Defects induced
were introduced into the system.
by corrective
Defects caused by missing logic
changes: 10%
accounted for a higher percentage when
the changes were made to add new features
than when they were made to resolve
Defects to
correction-induced defects, as shown in
features
existing
Figures 8 and 10. This reflects on both the
resulting from
relative completeness of the feature
new features: 53%
requirements and the maintenance methodology used. If the requirements had
been more detailed and complete or the Figure 5. Distribution of defect origin.
methodology had placed greater emphasis
on identifying and resolving incomplete0.70 60%/59%
ness, the percentage of missing-logic
defects would have been smaller.
0.60 Figure 9 shows that the overwhelming
majority (60 percent) of defects to previ0.50 ously working features were corrected by
changes to incorrect logic. Again,
0.40 although this represents only a single
study, this information suggests that
0.30impact analysis of adaptive type changes
0.20must be incorporated into the maintenance
process. Likewise, incorrect-logic defects
0.10
accounted for 66 percent of the defects
induced by corrective changes. A similar
0.00 response to potential ripple effects for corDevice callRecent
Other
Central callrective type changes is necessary. The fact
processing
change
processing
that interface defects were relatively more
common when dealing with corrective type Figure 6. The estimate of effort (the first number) versus the actual functional
changes must also be addressed.
area defect distribution (the second number).
-

MERk"

September 1987

49

Interface
defects: 5%

Interface
defects: 4%

Documentation
defects: 4%

Documentation
defects: 4%

Missing-logic
defects: 23%

Extra-logic
defects: 3%
Incorrect-logic
defects: 64%

Missing-logic
defects: 31%

Incorrect-logic
defects: 57%

Extra-logic
defects: 3%

Figure 7. Distribution of defect type (total).
Interface
defects: 5%

Figure 8. New-feature defect types.
Environment Documentation
Interface defects: 2% defects: 2%

Documentation
defects: 2%

defects: 10%

Missing-logic
defects: 20%

Missing-logic
defects: 21%

Extra-logic

I defects: 4%

Incorrect-logic
defects: 69%

Incorrect-logic
defects: 66%

Figure 9. Defect types introduced by new features into
existing features.
Interface
defects: 4%

Figure 10. Defect types introduced by corrective change.
centage of defects caused by implementation problems is not substantial, only that
there are a number of fundamental issues
that also need to be addressed. The solutions to these problems will require fundamental changes in how software
maintenance is viewed and performed.

Documentation
defects: 11 %

issing-logic
deftects: 28%
Incorrect-logic
defects: 52%

Extra-logic
defects: 5%

Figure 11. Residual product defect types.
Therefore, on a percentage basis, more
defects resulted from vague, inconsistent,
and incomplete written documentation
(requirements and design) and programmer comprehension than from actual
50

problems translating the design into code.
While there is some p)ublished information
documenting similair observations,6'7 the

perception is much different.
Our point is not t;o imply that the per-

common

SQA implications

Our data has implications for the scope
and direction of SQA organizations functioning in a maintenance environment.
Some of the SQA approaches suggested by
our data might even be currently performed in support of new software development. In this case, they simply need to
be extended into the maintenance environment.
In most cases, however, these activities
must be tailored to address the dynamics
and special challenges faced during maintenance before being introduced to the
software-maintenance environment.
Standards. Historically, development
standards for the various representations
of design, coding, and test documentation

IEEE Software

have been a cornerstone of many SQA
organizations. Even in organizations
where the details in the standards were not
developed directly by them, the SQA
organization has recognized the benefits
obtained by adhering to standards.
The increased complexity of a system
that occurs over time warrants the extension and monitoring of software development standards to the maintenance
environment. While addressing many of
the same issues, these standards should be
tailored to minimize the increase in logical
complexity and design inconsistencies that
occur when maintenance activities are performed.

Reviews. The severity of unintended
side effects caused by changes made during maintenance, as evidenced by our data,
has direct implications for the review process as it exists in many organizations.
The scope of the regular review process
performed for new software development
must be expanded to address the reality of
change made during software maintenance. The goal of regular reviews is
usually restricted to evaluating the change
being made in relation to its intended
function.
Maintenance reviews are different
because their scope is expanded to address
unintended side effects and maintainability issues that are related, directly or
indirectly, to almost every problem
encountered during maintenance.
Metrics. SQA organizations should likewise broaden their perspective concerning
metrics used while in the maintenance

environment.
Making correlations and judgments
about the maintenance experience of the
organization requires specific information
reflecting the dynamics that are occurring.
Often the metrics monitored during new
software development simply do not capture necessary information reflecting the
forces involved when the software is being
changed. Specific examples of useful metrics for software maintenance include
changes in logical complexity, ripple
effects of individual and groups of
changes, and predictive-type defectestimation models tailored for the maintenance environment.

September 1987

Conceptrelated

Understandingrelated

Productionrelated

Figure 12. Distribution of defect cause.

i ith the

proper focus in a
software-maintenance enviVY ronment, SQA functions can
play a key part in the continued commercial success of existing software products.
This can be accomplished by recognizing
and providing understanding of key
aspects of maintenance activities.
By proposing or assisting technical
approaches based on the latest understanding of software maintenance, the
SQA function can assume a leadership role
addressing software maintenance
challenges in the organization. The fundamental approach should address development standards, review processes, and
metrics in the maintenance environment,
but should not be limited to those areas.-6W

References

1. E. Swanson, "The Dimensions of Maintenance, " Proc. Second Int 'l Software Eng.
Conf., CS Press, Los Alamitos, Calif.,
1976, pp. 492-497.
2. M.M. Lehman, "On Understanding Laws,
Evolution, and Conservation in the LargeProgram Life Cycle," J. Systems and Software, 1980, pp. 213-221.
3. J. Bowen, "Software Maintenance: An
Error-Prone Activity," Proc. Software
Maintenance Workshop, CS Press, Los

Alamitos, Calif., 1983, pp. 51-54.

4. J.J. Buck, A Proposed Set of Software
Maintenance Reviews, master's thesis,
Computer Science Dept., Arizona State
Univ., Tempe, Ariz., 1986.
5. S. Yau, J.S. Collofello, and T. MacGregor,
"Ripple-Effect Analysis of Software Maintenance," Proc. Computer Software and
Applications, CS Press, Los Alamitos,
Calif., 1978, pp. 60-65.

6. B.W. Boehm, "Software and Its Impact: A

Quantitative Assessment," Datamation,
1973, pp 48-59.
7. D.S. Alberts, "The Economics of Software
Quality Assurance," Proc. Nat'l Computer
Conf., AFIPS, Reston, Va., 1976, pp.
415-424.

James S. Collofello is an associate professor of
computer science at Arizona State University.
His research and teaching interests are software
engineering and quality assurance. He has been
a visiting computer scientist at the Software
Engineering Institute, where he was involved in
the graduate curriculum project.
Collofello received a PhD in computer science
from Northwestern University.

Jeffrey J. Buck is a supervisor of the system test
and first-office application group at GTE Communication Systems Corp. His research interests
are software maintenance and regression

testing.

Buck received an MS in computer science
from Arizona State University.

Questions about this article can be addressed
Buck at GTE Communications Systems
Corp., 2500 W. Utopia Rd., Phoenix, AZ

to

85027.

51

SOFTWARE PROCESS IMPROVEMENT AND PRACTICE
Softw. Process Improve. Pract. 2006; 11: 385–409
Published online 5 July 2006 in Wiley InterScience
(www.interscience.wiley.com) DOI: 10.1002/spip.281

A Software Product Line
Process Simulator‡
Yu Chen,1,† Gerald C. Gannod2 * and James S. Collofello3
1

Department of Computer Science and Engineering, Arizona State
University - Tempe Campus, Tempe, AZ 85287, USA
2
Division of Computing Studies, Arizona State University Polytechnic
Campus, Mesa, AZ 85212, USA
3
Department of Computer Science and Engineering, Arizona State
University - Tempe Campus, Tempe, AZ 85287, USA

Research Section

Organizations are moving towards the use of software product line approaches to build product
families. Cases have shown that software product line approaches can reduce time-to-market
(TTM), costs, and resource usage. However, those benefits are not guaranteed in all situations,
as they are affected by many factors including the number of available resources, market
demands, reuse rates, and product line adoption and evolution strategies. Before initiating a
software product line, an organization needs to evaluate available process options in order to
see which ones best fit its goals. The aim of this research is to help this decision-making process
by providing practical approaches and tools. In this article, a process evaluation approach
is proposed, a process meta-model is introduced, and a product line process simulator is
presented. The approach contains three steps: process definition, simulation, and evaluation.
The process meta-model is used for defining software product line processes. The simulator
can predict the development costs, schedule, and resource usage rates for a selected software
product line process at a high level. The simulator uses DEVSJAVA as the modeling and
simulation formalism and COPLOMO as the cost model. An example is also given and some
simulation results are discussed. Copyright  2006 John Wiley & Sons, Ltd.
KEY WORDS: process simulation; software product lines; product line economics

1. INTRODUCTION
A software product line is a set of software-intensive
systems; they share a common, managed set of
features satisfying the specific needs of a particular
market segment or mission, and they are developed
from a common set of core assets in a prescribed way
∗
Correspondence to: Gerald C. Gannod, Division of Computing
Studies, Arizona State University Polytechnic Campus, Mesa,
AZ 85212, USA
†
E-mail: gannod@asu.edu
‡
This material is based upon work supported by the National
Science Foundation under Career grant No. CCR-0133956.

Copyright  2006 John Wiley & Sons, Ltd.

(Clements and Northrop 2001). Cases have shown
that the software product line approach can reduce
time-to-market (TTM), costs, and resource usage
(Clements and Northrop 2001). There are various
ways to develop and evolve a product line, and
these approaches may have different results in terms
of cost, schedule, resource usage, etc. A product
line development often has some constraints, such
as available engineers and funds. Before initiating
a software product line, an organization needs to
evaluate the available process options in order to
see which ones best fit its goals. The aim of this
research is to help this decision-making process by
providing practical approaches and tools.

Research Section

In this article, a software product line process evaluation approach is proposed and a software product line process simulator is presented.
The process evaluation approach consists of three
steps: process definition, simulation, and evaluation. The simulator uses DEVSJAVA (Zeigler and
Sarjoughian 2003) as the modeling and simulation
formalism and COPLIMO (Boehm et al. 2004) as the
underlying cost model. The input to the simulator
is a product line life-cycle process plan. The plan
specifies the available resources, product demand
intervals, process hierarchical and temporal relationships, cost model parameter values, and so on.
The input is at a level that allows organizational
level product line planning. The outputs from the
simulator include the estimates of the first release
time (FRT), initial development effort (IDE), lifecycle efforts for each product, life-cycle efforts for
the whole product line, and resource usage rates.
By varying the inputs and comparing the outputs, a
manager can make decisions about whether a product line approach should be used, and given that
one is used, what strategies should be used, and
what the potential resource allocations should be.
The remainder of the article is organized as
follows. Section 2 presents background information
and related work. Section 3 presents a product line
example, which will be used throughout this article.
Section 4 describes the approach and the simulator.
Some simulation results are provided in Section 5.
Section 6 discusses the evaluation of the work.
Section 7 draws conclusions and suggests future
investigations.

2. BACKGROUND AND RELATED WORK
This section describes background and related work
on software product lines, product line cost models,
and software process simulation.
2.1. Software Product Lines
Software product line development involves three
essential activities: core asset development, product development, and management (Clements and
Northrop 2001). Core asset development (domain
engineering) involves the creation of common assets
and the evolution of the assets in response to product feedback, market demands, etc. Product development (application engineering) creates individual
Copyright  2006 John Wiley & Sons, Ltd.

386

Y. Chen, G. C. Gannod and J. S. Collofello

products by reusing the common assets, gives feedback to core asset development, and evolves the
individual products. Management includes technical and organizational management, where technical management is responsible for requirement
control and the coordination between core asset
and product development.
With the product line approach, core assets are
generally updated more frequently than individual
products, as they need to meet the requests
from both the products and markets. Different
approaches can be used to evolve existing products
upon the change of the core. One approach is to
require the impacted products to incorporate the
core update immediately, which we call required
product update (RPU). Another approach is to leave
the product updates as an option to product
teams, which we call optional product update (OPU).
RPU makes product line maintenance easier by
reducing core variants but may receive resistance
from customers who do not want to take the risks
of conducting unnecessary product updates. OPU
appeals to customers but increases the product line
maintenance costs by allowing core asset variants
grow. In this study, we only consider the RPU
product evolution approach.
Several software product line development
approaches have been discussed in (Bosch 2002),
(Krueger 2002) and (Schmid and Verlage 2002).
There are two main software product line development approaches: proactive and reactive. With the
proactive approach, core assets are developed to
support both current and future products. With
the reactive approach, core assets are incrementally
created only to support the current products.
Depending on the degree of advanced planning,
the proactive approach can be classified into big bang
and incremental approaches (Schmid and Verlage
2002). With the big bang approach, core assets
are developed for all foreseeable products prior
to the creation of any individual product. With the
incremental approach, core assets are incrementally
developed to support the next few upcoming
products. Figure 1 shows the process flows of
the proactive approaches with the RPU product
evolution strategy.
Some common reactive approaches are: infrastructure-based, branch-and-unite, and bulk-integration
(Schmid and Verlage 2002). The infrastructurebased approach does not allow deviation between
the core assets and the individual products, and
Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

A Software Product Line Process Simulator

Get next new
product request

Get next new
product request

Has core?

Increase core?

No

Yes

Create core

Yes

Create new
product

Increase core

Update existing
products

Big bang & RPU

No

Create new
product

Incremental & RPU

Figure 1. Process flows of the proactive approaches with
the RPU product evolution strategy

Get next new
product request

Get next new
product request

New common
features?

New common
features?

No

No

Create new
product

Yes

Create new
product

Update core assets

Yes

Create new product
Update core assets

update existing products

update existing products

Infrastructure-Based & RPU

Branch-and-Unite & RPU

Figure 2. Process flows of the reactive approaches with
the RPU product evolution strategy

requires that new common features be first implemented into the core assets and then built into individual products. Both the branch-and-unite and the
bulk-integration approaches allow temporal deviation between the core assets and the individual
products. The branch-and-unite strategy requires
that the new common features be reintegrated into
the core assets immediately after the release of the
new product, while the bulk-integration strategy
allows the new common features to be reintegrated
after the release of a group of products. Figure 2
shows the process flows of the infrastructure-based
Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

and branch-and-unite approach with the RPU product evolution strategy.
Those approaches are not mutually exclusive. For
instance, a proactive approach can be used during
a product line creation stage, and then a reactive
approach can be applied when the product line
becomes mature. Four of the above approaches,
big bang, incremental, infrastructure-based, and
branch-and-unite, will be discussed later in this
article. The simulator described in this article can
handle all these approaches and their mixtures,
which are represented as process definitions by
using the meta-model that will be discussed later.
2.2. Product Line Cost Models
Existing works on software product line cost
estimation include some cost analysis approaches
(Bockle et al. 2003, Cohen 2003, Withey 1996) and
cost models (Boehm et al. 2004, Clements et al. 2005,
Lamine et al. 2005, Poulin 1997).
Clements, McGregor, and Cohen proposed the
SIMPLE (Structured Intuitive Model for Product
Line Economics) (Clements et al. 2005). SIMPLE
considers organization costs and a general product
line situation, but provides only a high-level cost
estimation framework and cannot be used until
the low-level functions are defined. Using an
object-oriented analogy, SIMPLE can be viewed
as an abstract class with some abstract methods.
To use SIMPLE, an organization has to create
a subclass of SIMPLE and implement all the
abstract methods. Boehm et al. suggested COPLIMO
(Constructive Product Line Investment Model)
(Boehm et al. 2004), which is a COCOMO IIbased (Boehm et al. 2000) model for software
product line cost estimation. It has a basic life-cycle
model that consists of a product line development
cost model and an annualized postdevelopment
extension. Poulin presented a return on investment
(ROI) model for estimating the financial benefit of
software development within product lines (Poulin
1997). The COPLIMO and Poulin models do not
consider organization costs and deal only with a
less general product line situation, but provide
concrete cost estimation solutions. Lamine, Jilani,
and Ghezala introduced another product line cost
model, SoCoEMo-PLE2 (Lamine et al. 2005). It is
an integrated cost model and uses COCOMO
and Poulin models to estimate the flows of costs
and benefits among different engineering cycles
Softw. Process Improve. Pract., 2006; 11: 385–409

387

Research Section

within an organization. It also considers the use
of COTS (Commercial Off-The-Shelf) components.
These four models are at different levels. COPLIMO
and Poulin models are at a lower level and provide
cost estimation for the basic development activities.
The other two models try to aggregate the basic
development costs and present them in a way that
is more meaningful for organizational management.
Despite the differences, these four models are
consistent.
In this article, we use COPLIMO (Boehm et al.
2004) as the underlying cost model. Although the
Poulin model requires fewer parameters and is easier to use, it was not used in the simulator owing to
its lack of detail in handling maintenance activities
as well as schedule and resource estimations. Both
the SIMPLE and SoCoEMo-PLE2 models were not
used because they do not add new information on
how to estimate the low-level development costs,
which are needed in this simulator. A future extension to the simulator could be using the integrated
SIMPLE and COPLIMO model as the cost model by
subclassing it from a general cost model.
2.3. Software Process Simulation
A software process is a set of activities, methods, practices, and transformations that people use
to develop and maintain software and associated
products, such as project plans, design documentation, code, test cases, and user manuals (Kellner
et al. 1999). Adopting a new software process is
expensive and risky, so software process simulation is often used to predict the potential impact
of the new process. Software process simulation
can be used for various purposes and scopes, and
has been supported by many technologies (Kellner et al. 1999). The software product line process
simulator discussed in this article is for long-term
organizational strategic management and uses a
discrete-event simulation formalism.
DEVSJAVA was used as the modeling and simulation formalism, which is a Java implementation
of the Discrete Event System Specification (DEVS)
modeling formalism (Zeigler and Sarjoughian 2003).
The external view of a DEVSJAVA model is a
black box with input and output ports. A model
receives messages through its input ports and
sends out messages via its output ports. A message consists of a collection of port and value pairs,
where port denotes the destination port and value
Copyright  2006 John Wiley & Sons, Ltd.

388

Y. Chen, G. C. Gannod and J. S. Collofello

encapsulates the data object. Ports and messages
are the only means by which a model communicates with the external world. A DEVSJAVA model
is either atomic or coupled. An atomic model is
indivisible and generally used to build coupled
models. An atomic model has eight essential elements: (i) input set, (ii) output set, (iii) state set,
(iv) internal transition function, (v) external transition function, (vi) confluent function, (vii) time
advance function, and (viii) output function. The
internal transition function determines the state
change upon the occurrence of internal events,
while the external transition function determines
the state change when external events arrive. The
confluent function resolves the conflicts when internal and external events occur at the same time. The
time advance function determines how long the
system stays in the current state. The output function maps state set to output set. A coupled model
consists of input and output ports, a finite number
of (atomic or coupled) models, and couplings. The
couplings link the ports together and are essentially
message channels.
DEVSJAVA was used as the modeling and simulation formalism because of the following reasons:
First, from a high-level product line development
processes are mostly discrete-event based, and
DEVSJAVA provides discrete-event modeling and
simulation mechanism. Second, it uses a general
object-oriented language, JAVA, as the modeling
language, which provides enough flexibility in
model development. Third, it allows modeling at
multiple levels of granularity, which supports incremental model development. Furthermore, owing to
the availability of its source code, DEVSJAVA can
be extended to incorporate domain-specific (e.g.
Software Product Line) logic and semantics.
Previous works on software process simulation,
including that of (Host et al. 2001) are mainly
concerned with traditional product development
approaches. Recently, Schmid and Biffl discussed a
simulator to study product line planning strategies
(Schmid and Biffl 2005). The inputs to the simulator include features and feature sizes within a
product line, scope of features to be developed for
reuse, productivity with and without reuse, mapping of the features to individual products, the
number of developers available, deadlines of the
individual products, task scheduling and resource
allocation strategies, and scheduling priority functions. The outputs from the simulator include
Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

the accumulative number of weeks that products
within the product lines are finished late (deadline
violation) and finished early (slack). The simulator was used to analyze the impact of scheduling
strategies, resource allocation strategies, scheduling
priority functions, the number of developers, and
product deadlines on deadline violation and slack.
The implicative assumption under their approach
is that product development is simply the composition of reusable and product-specific features,
and the feature integration effort is very small and
can be ignored. As such, product line scheduling is mainly scheduling feature development to
meet product deadlines. Two scheduling strategies
were considered in their simulator, Start Everything Immediately (SEI) and Just In Time (JIT). SEI
starts all features as soon as possible, while JIT
starts all features as late as possible without creating a deadline violation. Two resource allocation
strategies were considered in the simulator, Direct
Competition (DC) and Keep Developers in Project
(KDIP). DC allows high-priority features to take
away developers from ongoing low-priority features, while KDIP allocates only developers who
are not currently used in a development task to
high-priority tasks. Also, three priority functions
were considered in the simulator.
The simulator presented in this article and the
simulator developed by Schmid and Biffl are both
deterministic. Similar product line parameters are
used by both simulators. For instance, product line
features, feature product map, and the number of
developers in Shcmid and Biffl’s simulator are similar to product line components, component product
map, and the number of engineers in the simulator
presented in this article. Also, similar assumptions
are made by both simulators, such as developers
are considered with the same skills and productivity. However, there are major differences between
these two simulators. First, the two simulators are
meant to be used in different situations. Schmid
and Biffl’s simulator is to assist managers in making day-to-day product line development decisions,
while our simulator is to assist decision making at
a higher level, such as comparing different process options during the product line transition
stage. Thus, Schmid and Biffl’s simulator supports
impact analysis of low-level scheduling strategies
on deadline violations and slacks, while our simulator provides costs, schedule, and resource usage
estimate of high-level product line adoption and
Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

A Software Product Line Process Simulator

evolution strategies. Using Schmid and Biffl’s terms,
currently our simulator supports SEI scheduling
and KDIP resource allocation strategies. The priority of each task in our simulator is obtained from
inputs instead of priority functions. Second, our
simulator considers product line evolution aspects,
white-box reuse, and component integration effort,
which are not included in Schmid and Biffl’s simulator. Thus, our simulator treats product development
as the basic development unit, while Schmid and
Biffl’s simulator considers feature development as
the basic development unit.

3. OVERVIEW OF THE EXAMPLE
This section provides an example software product
line, which will be used throughout this article.
The target of this case study is a hypothetical simulator product line. After the initial market analysis,
scoping, and high-level feature and architecture
study, the feature model, component product map,
and the scope of the core assets are defined. The
feature model (Czarnecki and Eisenecker 2000),
shown in Figure 3, indicates that a simulator must
have an IO (Input and Output) a cost model, and
a simulation model. The IO can include a basic
IO, and optionally a Web or GUI (Graphic User
Interface) IO. The cost model can be one of the N
variants, CM1 to CMN. The simulation model can
include an early stage model, a late stage model, or
both. Table 1 provides the name, description, and
size information for some high-level components
within the product line. For this case study, 10
products are planned within the product line. The
Simulator

IO

Cost Model

Basic Web GUI

CM1

CMN

Simulation Model

EarlyStage

LateStage

exclusive or

mandatory feature

inclusive or

optional feature

Figure 3. Feature model
Softw. Process Improve. Pract., 2006; 11: 385–409

389

Research Section

Y. Chen, G. C. Gannod and J. S. Collofello

Table 1. High-level components
Name

C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
C11
C12
C13

Table 3. Product summary

Description

Size
(KSLOC)

Project unique code
Common framework
Basic IO – early stage
Web IO – early stage
GUI IO – early stage
Basic IO – late stage
Web IO – late stage
GUI IO – late stage
Simulation model – early stage
Simulation model – late stage
Cost model 1
Cost model 2
Cost model 3

10
50
10
10
10
10
10
10
20
20
10
10
10

relationships between components and products
are shown in Table 2. A cell with a letter means
that the component is included in the product.
P means that the component is developed from
scratch, R indicates that the component is developed
by reusing existing components, and A denotes that
the component is developed by adapting some existing components. Table 3 summarizes the products
in terms of size, fraction of project unique, adapted,
and reused code.
On the basis of the above information, the organization wants to compare the available process
options to develop and evolve the product line.
Fourteen process options, as shown in Table 4, are
identified. These processes differ in the demand
intervals, number of employees, and development
approaches. Process 6, 10, 11, 12, and 14 all use
the traditional product development approach,

ProdName

Size
(KSLOC)

pFrac
(%)

aFrac
(%)

rFrac
(%)

prod01
prod02
prod03
prod04
prod05
prod06
prod07
prod08
prod09
prod10

100
110
120
130
150
170
150
170
150
170

10.00
9.09
8.33
7.69
6.67
5.88
6.67
5.88
6.67
5.88

10.00
18.18
25.00
15.38
26.67
35.29
26.67
35.29
26.67
35.29

80.00
72.73
66.67
76.92
66.67
58.82
66.67
58.82
66.67
58.82

FunName
Average
Maximum
Minimum

142
170
100

7.28
10.00
5.88

25.44
35.29
10.00

67.28
80.00
58.82

which can be viewed as a special case of product line approach where the core is empty. The
rest of processes all use some kind of product line
development approaches and they all require products to incorporate core change immediately upon
a core update.

4. APPROACH
This section presents the overview of our approach,
the software product line process meta-model, and
the simulation tool.
4.1. Overview
Before initiating a software product line, an
organization needs to conduct market analysis,

Table 2. Product component map
Components

C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
C11
C12
C13

Products
Prod1

Prod2

Prod3

Prod4

Prod5

Prod6

Prod7

Prod8

Prod9

Prod10

P
R
A

P
R
A
A

P
R
A
A
A

P
R
A

P
R
A
A

P
R
A
A

A
A

R
R
R

R
R
R

R
R

P
R
A
A
A
A
A
A
R
R

P
R
A
A

A

P
R
A
A
A
A
A
A
R
R
R

R
R

P
R
A
A
A
A
A
A
R
R

R

R
R

R

R

R

R

R

R

R

Copyright  2006 John Wiley & Sons, Ltd.

390

A
A

A
A

Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

A Software Product Line Process Simulator

analyze its current situation and available resources,
define product line feature models and architectures, and set product line development goals. Very
often, it may have several process options to create and evolve the product line. An example set of
process options are shown in Table 4. As those processes may end up with different results in terms of
costs and benefits, the organization needs to know
which ones best fit its goals. Our approach to solve
the problem, shown in Figure 4, consists of three
steps. First, each process is defined by taking architectures, feature models, available resources, market demands, development strategies, etc. as inputs.
The architecture information permits decomposing
products into high-level components. Second, each
process is simulated. Finally, the alternative processes are evaluated by comparing the simulation
outputs. The simulation results can also be used to
refine the process definition. The tools used for these
steps are Microsoft Project (Microsoft Corporation
2006b), the simulator discussed here, and Microsoft
Excel (Microsoft Corporation 2006a), respectively.

into three levels: product line, product, and module.
A product line process consists of several product
line or product processes. Each product process is
one of the following types: product demand, product
development, or product maintenance. Product maintenance has two subtypes, annual maintenance and
product enhancement. Product demand represents
new product requirements issued by the external world. Product development refers to initial
product creation. Product maintenance represents
product modification. Annual maintenance refers
to annually planned product maintenance. Product
enhancement models the maintenance activity that
improves the product functionality and involves
major product change. A product process has several module processes. Each module process is
either a module development or module maintenance
process. A module development process is either
module new or module adoption. Module adoption
has a subtype, module reuse. Module new represents development without reuse, module adoption
refers to white-box reuse, and module reuse models
black-box reuse. Module maintenance represents
module modification activities.
Some attributes associated with the models are
shown in Table 5. The attributes include standard
Microsoft Project parameters (e.g. UID (Unique
Identification) and Work breakdown structure (WBS)),

4.2. Software Product Line Process Meta-model
A software product line process meta-model, as
shown in Figure 5, was created to facilitate process definition. This model categorizes processes
Table 4. Processes

Processes
Parameters
Demand interval
Number of employees
Approach
Big bang
Incremental (5 products per inc)
Incremental (3 products per inc)
Incremental (3 products per inc) & Infrastructure-based
Incremental (3 products per inc) & Branch-and-unite
Traditional

Features
Architectures
Market analysis
Resources
Strategies
...

1
12
90
√

2
12
90
√

3
12
90

√

4
12
90

√

5
12
90

√

6
12
90

√

7
12
40

8
12
45

9
12
50

√

√

√

10
12
40

11
12
45

12
12
50

13
6
90

14
6
90

√
√

√

√

√

Refined Process Definition
Process Definition
(MS Project)

Process Simulation
(SPLPS)

Process Evaluation
(MS Excel)

Figure 4. Process evaluation approach
Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

Softw. Process Improve. Pract., 2006; 11: 385–409

391

Research Section

Y. Chen, G. C. Gannod and J. S. Collofello

Process
UID
WBS
Name
Priority
Start
Finish
PredecessorLinks
*
dest
processType
processStatus
tdev
pm
...

Resource
UID
Name
* Units
...

ProductLineProcess
ModuleProcess
eaf
sloc

Module
Development

ModuleNew

ModuleReuse

* ProductProcess
scaleFactor
sced
scedCompression

*
*

Module
Maintenance
mcf
su
unfm

Module
Adaptation
dm
cm
im
su
unfm
aa

Product
Maintenance

Product
Enhancement
Product
Demand
lifeSpan
numFeatures

Product
Development

Annual
Maintenance

Figure 5. Process meta-model

cost model (Boehm et al. 2004) parameters (e.g. dm,
cm, and im), and simulator specific parameters (e.g.
processType and numFeatures). For a detailed explanation of the Microsoft Project and cost model
parameters, please refer to (Microsoft Corporation
2006b) and (Boehm et al. 2000). The temporal relationships among processes are described by ‘PredecessorLinks’, which is a list of ‘PredecessorLink’.
If Process A has a ‘PredecessorLink’ that points to
Process B, Process A cannot start until Process B is
finished. Each process can have resources associated
with it. For this study, only the human resources to
conduct the processes are considered. If source line
Copyright  2006 John Wiley & Sons, Ltd.

392

of code (SLOC) is not available, function sizes can
be used to derive the equivalent SLOC. COCOMO
II (Boehm et al. 2000) provides such a conversion
function.
To define a software product line process, market
analysis results, high-level feature models, product
component maps, and product line development
strategies are needed as inputs. Market analysis
results tell what kind of products will be needed
and when they will be needed. The feature models
and product component maps show the relationship
between features, components, and products. The
product line development strategies, such as the
Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

A Software Product Line Process Simulator

Table 5. Attribute description
Attribute

Description

Microsoft Project Related Attributes
UID
Unique identification, automatically assigned
WBS
Work breakdown structure code, automatically assigned
Name
Process name
Priority
Process priority
Start
Process starting time
Finish
Process finishing time
PredecessorLinks
A list of predecessor links
Units
The number of resources
Cost Model Related Attributes
scaleFactor
The sum of scale drivers
sced
Project schedule
scedCompression
Project schedule compression
lifeSpan
Product LS in years
eaf
The product of effort multipliers
sloc
SLOC
mcf
Maintenance change factor
dm
Percentage of design modification
cm
Percentage of code modification
im
Percent of integration required for modified software
su
Percentage of reuse effort due to software understanding
unfm
Programmer unfamiliarity with software
aa
Percentage of reuse effort due to assessment and assimilation
Simulator Specific Attributes
dest
The target of the process, such as the name of a product line, product, or
module
processType
The type of the process, as described by the meta-model
processStatus
The status of a process
tdev
Process duration in months, only required for ‘‘Product Maintenance’’
process and is optional for ‘‘Product Enhancement’’ process
pm
Process effort in person-months, current for output use only
numFeatures
Number of features

proactive and reactive approaches illustrated by
Figures 1 and 2, provide guidance on how to
create and evolve core assets as well as individual
products. The process meta-model is mainly for
organizational level management, so currently it
does not allow processes embedded in model
processes.
4.3. Simulation Tool
The simulator uses DEVSJAVA (Zeigler and Sarjoughian 2003) as the modeling and simulation
formalism and COPLIMO (Boehm et al. 2004) as
the cost model.
4.3.1. Simulator Inputs
The input to the simulator is a process definition.
Currently, Microsoft Project (Microsoft Corporation
2006b) is used as the process definition tool.
Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

A valid process definition is created by using
Microsoft Project (Microsoft Corporation 2006b)
to specify the product line process according to
the previously discussed meta-model and then
exported in XML format. In the process definition,
only the resources for the topmost product line
level process need to be specified. The simulator
can use the cost model to assign resources to lowerlevel processes. Also, a property file is used to set
some properties of the simulator, such as the effort
and schedule coefficients of the cost model. The
14 processes shown in Table 4 can be defined in
this way.
4.3.2. Simulation Models
Twelve DEVSJAVA simulation models have been
developed, and their hierarchical relationships are
shown in Figure 6. These models are independent
of a specific product line. The topmost model is
Softw. Process Improve. Pract., 2006; 11: 385–409

393

Research Section

Y. Chen, G. C. Gannod and J. S. Collofello

PLPEF
1

1

ExperimentalFrame

ProductLineProcess

1

ProductEngineering

TechnicalManagement
1

ProductAnnualMaintenance

1

0..1

1..*

CoreEngineering

EmployeePool

1
DemandGnr

1
Transducer

1
1
1
Development

CoreAnnualMaintenance

Figure 6. Simulation models

Figure 7. Simulation tool in execution

PLPEF (Product Line Process Experimental Framework). A PLPEF model contains an Experimental
Frame and a Product Line Process model. An Experimental Frame model has a DemandGnr instance
Copyright  2006 John Wiley & Sons, Ltd.

394

that issues new product demands according to
the process definition and a Transducer instance
that observes the product line process and produces statistical outputs. A Product Line Process
Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

A Software Product Line Process Simulator

model contains of a Technical Management and an
Employee Pool instance, zero or one Core Engineering
instance, and several Product Engineering instances.
The Technical Management receives the product
demands and process reports, and issues process
requests according to their hierarchical and temporal relationships defined by the process definition.
The Employee Pool receives resource requests and
allocates resources. It has one task queue and one
server. The task with the highest priority and which
has waited longest in the queue is processed first.
The service time is either zero or equals the resource
waiting time when there are not enough resources.
The Core Engineering models domain engineering activities while the Product Engineering models
application engineering activities. Core Engineering
or Product Engineering has a Development instance
for initial development and unplanned maintenance
activities and an Annual Maintenance instance for
annually planned maintenance activities. Both the
Development and Annual Maintenance need to
request resources before starting a new task and
return the resources upon finishing the task. They
both have one task queue and one server and the
tasks are handled in FIFO order. The service time
for each task is the sum of the resource waiting time
and the development time.

4.3.3. Simulator User Interface
The simulator user interface is depicted in Figure 7.
The upper part of the interface identifies the current
running model and its package (‘PLPEF’ and ‘productLineProcess’, respectively). The large middle
area shows the model instances and their hierarchical relationships. The bottom of the window
contains execution control components.
4.3.4. Simulator Outputs
At the end of each simulation run, a result table
is generated similar to Table 6. The table has three
sections. The product section provides data related
to individual products including initial source line
of code (ISLOC), FRT, TTM, IDE, initial development time (IDT), accumulated development and
maintenance effort (AE), accumulated development and maintenance time (AT), and accumulated
waiting time (AWT). The product line section summarizes the product-line-related statistics, which
include AE, AT, product line life span (LS), average annual effort (AAE), average time-to-market
(ATTM), and AWT. In the table, the unit of effort
is person-months and the unit of time is months.
The resource section provides resource usage data
including percentage of the time the resource pool
is in the wait stage for the lack of resources

Table 6. Simulation results
Product section
ProdName

ISLOC

FRT

TTM

IDE

IDT

AE

AT

AWT

core
prod01
prod02
prod03
prod04
prod05
prod06
prod07
prod08
prod09
prod10

110 000
100 000
110 000
120 000
130 000
150 000
170 000
150 000
170 000
150 000
170 000

30.81
44.07
45
45.83
68.3
69.86
77.18
96.86
101.18
121.73
126.05

30.81
44.07
33
21.83
32.3
21.86
17.18
24.86
17.18
25.73
18.05

806.1
56.8
70.34
84.12
72.06
99.89
128.44
99.89
128.44
99.89
128.44

30.81
13.26
14.19
15.02
14.3
15.86
17.18
15.86
17.18
15.86
17.18

3302
125.8
155.1
184.8
145.7
201.6
258.8
186.3
239.3
171.1
219.7

282.1
142.3
143.2
144
140.3
141.9
143.2
138.9
140.2
135.9
137.2

0.86
0
0
0
0
0
0
0
3
0
0.86

Product line section
Name
prodLine

AE

AT

LS

AAE

ATTM

AWT

5190.3

1689

246.05

253.13

25.61

4.73

Resource Section
Name

PWT

AUR

MinUR

MaxUR

EmpPool

0.02

0.42

0.17

0.99

Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

Softw. Process Improve. Pract., 2006; 11: 385–409

395

Research Section

(PWT), average resource usage rate (AUR), minimum resource usage rate (MinUR), and maximum
resource usage rate (MaxUR). The Resource section
and the ISLOC and AWT fields are the new additions to this version of the simulator.
4.3.5. Model Calibration, Assumptions, and
Limitations
Project development productivity differs from organization to organization, and even within the same
organization it differs from project to project. When
history data are available, simulator calibration
should be performed. The calibration can be done by
assigning new values to effort and schedule coefficients and exponents in the simulator property
file. The new values can be obtained by following
COCOMO II (Boehm et al. 2000) calibration steps.
The following assumptions are made in the
simulation model:
1.
2.

3.

All the employees have the same capability and
can work on any project.
Every product engineering (or core engineering) process has two concurrent subprocesses:
one for planned annual maintenance and one for
other development and maintenance activities.
For each product, its fractions of productspecific, adapted, and reused code stay relatively constant across its life cycle.

Assumption 1 is a simplification of the reality, where
each employee has different skills and productivity.
The purpose of this simulator is to provide highlevel cost estimates and not day-to-day project management, so the average is used to model employee
productivity. If it is needed, more Employee Pool
instances can be used to model employees with different productivity levels. It would be nice to be
able to model employees with different skills, such
as design and testing. However, currently that is a
hard task because the existing cost models do not
provide such support. In the traditional software
product development context, moving employees
from one project to another often requires considerable effort because of the differences in product
architectures, development approaches and tools,
etc. The effort for switching employees among
projects is reduced considerably with the product
line approach because similar product architectures,
development tools, and components are used across
different projects (Schmid and Biffl 2005). Assumption 2 is based on experience with software development in real development organizations. Normally,
Copyright  2006 John Wiley & Sons, Ltd.

396

Y. Chen, G. C. Gannod and J. S. Collofello

a fixed number of engineers are assigned to maintain a product. When a major product update or
enhancement comes, extra engineers can be allocated to the product development if more resources
are needed. In the case where there are more concurrent processes for a product, the simulator can be
extended by adding more Development or Maintenance instances. Assumption 3 is made by the cost
model, COPLIMO (Boehm et al. 2004). Currently,
there are some exceptions to this. For instance, new
modules can be added to products or core assets.
Totally removing this assumption is possible at the
expense of increased model complexity. However,
we did not do so because currently we are looking
at the product line cost-benefits at a high level, and
the code-segment changes of a product may not be
available at the time of initiating a product line.
If it is needed, the simulator can be extended by
removing this assumption.
The current implementation of the simulator has
several limitations. The simulator does not consider
organizational costs, other scheduling strategies
(e.g. JIT), and a product line with hardware and
software developed together. Also, it only deals
with the situation where a new product line is
developed from scratch. In the future, we plan to
build a set of simulators based on the current one to
address different situations.

5. RESULTS
The process definitions from the 14 processes,
shown in Table 4, were run through the simulator,
and their results are discussed here.
5.1. Effect of Adoption Approaches
Processes 1, 2, and 3 differ in product line adoption
approaches. Process 1 uses the big bang approach.
Process 2 uses the incremental approach and
considers five products within one core increment.
Process 3 also uses the incremental approach but
considers only three products within one core
increment. Their results differ in TTM, resource
usage rates, and efforts. For all these approaches,
the development of new products depends on the
availability of the needed core assets. If the required
core assets do not exist, the product development
cannot start until the core assets are created by the
domain engineering process.
Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

A Software Product Line Process Simulator

Figure 8 shows that core development/update
has a great impact on TTM of the subsequent products. The larger the effort of the core development
or update, the longer the TTM. The impact may
have a ripple effect on multiple products. For Process 1, the TTM for the first three products is the
longest, because it requires the largest effort to

develop the initial core. The initial core development also impacts the TTM of Products 1–4. After
that, the TTM becomes the shortest, because no core
update overhead is involved. Process 2 results in the
shortest TTM for Product 4 because the initial core
development impacts only the first three products
and Product 4 is developed without any delay. It

55
50

Time-to-Market

45
40
35
30
25
20
15
10
1

2

3

4

5
6
Products
1

2

7

8

9

10

3

Figure 8. Effect of adoption approaches on time-to-market
0.6

0.5

Percentage

0.4

0.3

0.2

0.1

0
PWT

AUR

MinUR
1

2

MaxUR

3

Figure 9. Effect of adoption approaches on resource usage
Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

Softw. Process Improve. Pract., 2006; 11: 385–409

397

Research Section

results in the longest TTM for Product 6, because of
the core update. For Process 3, the TTM for the first
three products is the shortest, because it needs the
smallest effort to develop the initial core. It results
in the longest TTM for Products 4, 5, and 7, because
of the core update.
Figure 9 shows the resource usage rates of
processes 1–3. Processes 1–3 have zero percentage
of resource waiting time, which means the resources
are sufficient for all these cases. The AUR has
a nondecreasing trend from Process 1 to 3. The
product line LS is the same for these three cases;
in this case it is the release time of Product 10
plus the LS of Product 10. Figure 10 shows that the
accumulated development and maintenance effort
has an increasing trend from Process 1 to 3. With that
we would expect that the AUR has a nondecreasing
trend from Process 1 to 3, which is consistent
with the simulation results. The minimum resource
usage rates for these three processes are nearly
the same. In this case, it happens at the end of
the product life cycle when only Product 10 and
core assets are in service. The maximum resource
usage rate increases from process 1 to 3, and it
often happens during or after core development or
update.
Figure 10 shows that more core increments result
in more efforts, because frequent core updates often
require more effort to modify the existing core and
products.

Y. Chen, G. C. Gannod and J. S. Collofello

5.2. Effect of Evolution Approaches
Processes 3–5 differ in product line evolution
approaches: incremental, infrastructure-based, and
branch-and-unite, respectively. The evolution strategy starts from Product 7. Their results differ in
TTM, resource usage rates, and efforts. Figure 11
once again shows that the core updates impact the
TTM. Process 3 results in the longest TTM for Product 7, because it requires the largest effort for core
update. Then TTM for Process 3 becomes the shortest because no core updates are involved. Process 5
results in the shortest TTM for Product 7. Although
Product 7 requires core update, the product development is allowed to start before the core update.
It results in the longest TTM for Product 8 and 10,
because product development for these products
cannot be started until the cores updates caused by
the previous product demands are finished. In this
case, Process 5 has the longest ATTM because it
requires the highest effort, as shown in Figure 13.
Figure 12 shows that for Processes 3–5 the
resources are sufficient, the minimum resource
usage rates are the same, and the average and maximum resource usage rates have a nondecreasing
trend, which is consistent with the increasing trend
in accumulated efforts shown in Figure 13. Process
4 requires more efforts than Process 3, because Process 4 involves more core updates, which often
result in more efforts. Compared to Process 4, Process 5 results in more efforts owing to the extra

4950
4900
4850

Effort (PM)

4800
4750
4700
4650
4600
4550
4500
4450
AE
1 2

3

Figure 10. Effect of adoption approaches on accumulated effort
Copyright  2006 John Wiley & Sons, Ltd.

398

Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

A Software Product Line Process Simulator

50

Time-to-Market

45
40
35
30
25
20
15
1

2

3

4

5

6

7

8

9

10

Products
3

4

5

Figure 11. Effect of evolution approaches on time-to-market
0.8
0.7

Percentage

0.6
0.5
0.4
0.3
0.2
0.1
0
PWT

AUR

MinUR
3

4

MaxUR

5

Figure 12. Effect of evolution approaches on resource usage

efforts needed for developing and reworking new
products. Although Processes 4 and 5 have the same
AUR, Process 5 has higher total resource usage rate
because it results in longer product line LS.
5.3. Product Line and Traditional Approach
Comparison
Processes 1–6 differ in product development
approaches. Processes 1–5 all use some product line
Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

development approaches, while Process 6 uses the
traditional development approach, where products
are developed and evolved independently.
Figure 14 shows the comparison of these processes in TTM. Process 6 has the shortest TTM for
the first two products, because it does not incur
overheads from core asset creation. It results in the
longest TTM for Product 5 through 10. By stepping
through the simulator, we found that Product 1
Softw. Process Improve. Pract., 2006; 11: 385–409

399

Research Section

Y. Chen, G. C. Gannod and J. S. Collofello

5500
5400
5300

Effort (PM)

5200
5100
5000
4900
4800
4700
4600
AE
3

4

5

Figure 13. Effect of evolution approaches on accumulated effort
55
50

Time-to-Market

45
40
35
30
25
20
15
10
1

2

3

4

1

5
6
Products
2

3

7

4

5

8

9

10

6

Figure 14. Time-to-market comparison

through 6 are developed without any delay, while
the development for Product 7 to 10 was delayed
because of lack of resources, which allowed only
two concurrent new product development activities a time. Thus, the longer TTM for Product 5 and
6 is purely caused by larger development efforts
as the results of no reuse, and the longer TTM for
Product 7 through 10 is caused by the combination
of larger efforts and the lack of resources.
Copyright  2006 John Wiley & Sons, Ltd.

400

Figure 15 shows that the percentages of waiting
time for all the cases are zero except Process
6. Process 6 also has the highest average and
maximum resource usage rate. These indicate that
the product line approach can reduce resource
usage. Process 6 results in the lowest minimum
resource usage rate, which happens at the end of
the product line life cycle when only one product
is in service. By large-scale reuse, product line
Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

A Software Product Line Process Simulator

1
0.9
0.8

Percentage

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
PWT

AUR

MinUR
1

2

3

4

5

MaxUR

6

Figure 15. Resource usage comparison
14000
12000

Effort (PM)

10000
8000
6000
4000
2000
0
AE
1

2

3

4

5

6

Figure 16. Accumulated development and maintenance effort comparison

approaches result in smaller code size to develop
and maintain. Thus, the total effort of creating and
evolving the products in a product line is smaller,
as shown in Figure 16.
5.4. Effect of Resources
Processes 4, and 7 through 9 all use the same product
development approach, but vary in the number
Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

of employees (90, 40, 45, and 50, respectively).
Processes 6, and 10 through 12 all use the traditional
development approach, but differ in the number of
employees (90, 40, 45, and 50, respectively).
The results of TTM for Processes 4, and 7
through 9 are shown in Figure 17. Although the
resource difference between Processes 4 and 9 is
40, their TTM difference is very small. On the other
Softw. Process Improve. Pract., 2006; 11: 385–409

401

Research Section

Y. Chen, G. C. Gannod and J. S. Collofello

hand, the resource difference between Process 9
and 8 as well as Process 8 and 7 is only 5, but
their TTM difference is larger. In this case, the
minimum number of resources is 56 to keep the
product line approach running without waiting
for resources. When the number of resources is
above this threshold, adding resources would not
shorten the TTM. When the number of resources is
below this threshold, reducing the resources quickly
deteriorate the TTM performance.

The results of TTM for Processes 6, and 10 through
12 are shown in Figure 18. The same trend is seen
here: as the number of resources decreases, TTM
gets longer. However, for traditional approaches,
the TTM deteriorates very quickly as the number
of resources decreases. The traditional approaches
require more resources than the product line
approaches, so when the resources are not sufficient,
the same amount of resource reduction will have
a larger impact on traditional approaches than on

50
45

Time-to-Market

40
35
30
25
20
15
1

2

3

4

5
6
Products
7

8

7
9

8

9

10

8

9

10

4

Figure 17. Effect of resources on time-to-market for product line approaches
265

Time-to-Market

215

165

115

65

15
1

2

3

4

5
6
Products
10

11

7

12

6

Figure 18. Effect of resources on time-to-market for traditional approaches
Copyright  2006 John Wiley & Sons, Ltd.

402

Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

A Software Product Line Process Simulator

the product line approaches. From another point of
view, these results show that the substantial TTM
reduction for the product line approach occurs
when limited resources restrict the concurrent
product development activities for the traditional
approach.

is the results of longer waiting time for resources
to become available. The shorter market demand
interval increases the average resource demands.
When the resources are not sufficient, the increase
of resource demand worsens the lack of resources.
Figure 20 shows the resource usage rates for these
processes. As the demand interval decreases, the
AUR increases. The zero percentage of resource
waiting time for Process 4 and 13 indicates that
the available resources are sufficient for these two
processes. For Process 6 and 14, the percentage
of resource waiting time increases as the market
interval decreases.

5.5. Effect of Demands
Processes 4 and 13 both use the same product development approach, but vary in the demand interval
(12 and 6, respectively). Processes 6 and 14 both
use the traditional development approach, but differ in the demand interval (12 and 6, respectively).
Here demands refer to new product demands and
exclude product updates and enhancements.
The results of TTM for these four processes are
shown in Figure 19. The figure shows that as the
demand interval gets shorter, the TTM get longer.
However, a shorter demand interval has a larger
impact on the traditional approach (Process 14) than
the product line approach (Process 13). For Process
13, the longer TTM of the products is caused by
longer waiting time for their predecessors to finish.
If the predecessors of a new product development
cannot finish on time, then the new product
development will be delayed. The shorter demand
interval will make the new product development
wait longer to start, which results in longer TTM.
For Process 14, the longer TTM of the products

5.6. Summary
The simulation results show that the product line
approaches can reduce costs, TTM, and resource
usage, and the cost-benefits of the product line
approaches are affected by many factors including
adoption and evolution approaches, number of
resources, and market demands. In most cases, high
percentage of cost and resource usage reduction
can be easily achieved if the reuse rates are not
too low. However, in general, the substantial TTM
reduction can occur only when the reuse rates
are high enough or the limited resources restrict
the concurrent product development activities for
the traditional approach. The TTM of the product
line approaches is affected by reuse rates process

70

Time-to-Market

60
50
40
30
20
10
1

2

3

4

5

6

7

8

9

10

Products
4

13

6

14

Figure 19. Effect of demands on time-to-market
Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

Softw. Process Improve. Pract., 2006; 11: 385–409

403

Research Section

Y. Chen, G. C. Gannod and J. S. Collofello

1
0.9
0.8

Percentage

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
PWT

AUR

MinUR
4

13

6

MaxUR

14

Figure 20. Effect of demands on resource usage

dependencies and available resources. Increasing
reuse rates can shorten product development time
but increase core development or update time.
It generally can lead to reduced average TTM if
there are enough products within a product line.
When resources are sufficient and reuse rates are
the same, reducing core development and update
duration can shorten the TTM, but it is restricted by
productivity and will increase the resource usage.

6. EVALUATION

Table 7. Working group hypotheses (Knauber et al. 2002)
No.
1
2
3
4
5
6
7

We have compared the simulation results with published works on product line economics (Clements
and Northrop 2001, Cohen 2003, Knauber et al. 2002,
Schmid and Verlage 2002). Because of the page limit,
in the following we will only compare the simulation results with some product line hypotheses
(Knauber et al. 2002), which are obtained after the
simulator is implemented. The results are from the
same example presented in this article.
6.1. Working Group Hypotheses
Knauber et al. formulated seven hypotheses regarding software product line development (Knauber
et al. 2002). The seven hypotheses are summarized
in Table 7. In addition, the table shows that four of
the hypotheses (1, 2, 4, and 5) are used to evaluate
Copyright  2006 John Wiley & Sons, Ltd.

404

Description

Simulated

Effort reduction over number of products
Time-to-market reduction over number of
products
Time to close customer issues over number of
products
Number of features developed over money
invested
Number of features developed over
development time
Time to integrate COTS per product over
number of products
Time to certify products over number of
products

X
X

X
X

the simulator. The remaining hypotheses are outside the scope of the current simulator.
6.1.1. Hypothesis 1 (Knauber et al. 2002)
The first product line hypothesis, shown in
Figure 21, states that after some initial investment,
the effort needed to derive a product from a product
line decreases significantly when compared to the
effort needed to develop a product using a traditional development approach (Knauber et al. 2002).
Some simulation results are shown in Figure 22 for
processes 1 through 6. We can see that after the first
four products, the effort required to develop a new
product with the product line approaches is much
lower than that with the traditional approach. The
Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

accumulated effort

accumulated effort

A Software Product Line Process Simulator

products

products
With PL

With PL

Without PL

Without PL

Figure 23. Hypothesis 2 (Knauber et al. 2002)

Figure 21. Hypothesis 1 (Knauber et al. 2002)

simulation results show that the cross over point
happens between the second and third product. In
the literature different cross over points have been
reported, such as around 3 (Clements and Northrop
2001, Weiss and Lai 1999) and after 1 (Clements and
Northrop 2002). We found that the cross over point
is product line dependent, and is affected by factors
such as reuse rates and product line development
strategies.
6.1.2. Hypothesis 2 (Knauber et al. 2002)
The second product line hypothesis, shown in
Figure 23, states that after some initial investment,

the TTM for each product developed via the product
line approach is significantly lower than the TTM
of the corresponding product developed by the
traditional single product development approach
(Knauber et al. 2002). Our results are depicted in
Figure 14. There are two major differences between
our results and the hypothetical graph. First, our
results show that only after Product 6, the TTM of
a product with the product line approach is much
lower than that with the traditional approach. In our
example, with the traditional approach, products 1
through 6 are developed without delay and only the

Accumulated Development Effort

8000
7000
6000
5000
4000
3000
2000
1000
0
1

2

3

4
1

5
6
Products
2

3

7
4

5

8

9

10

6

Figure 22. Results on accumulated development effort over products
Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

Softw. Process Improve. Pract., 2006; 11: 385–409

405

Research Section

Y. Chen, G. C. Gannod and J. S. Collofello

number of features

development of product 7 through 10 are delayed
because of the lack of resources. We think the
unspecified assumption of this hypothesis is that the
product development with the traditional approach
starts after some delay. Second, the simulation
results yield a TTM curve that starts high and then
drops low, while the hypothetical graph indicates
a gradually increasing trend. We think our TTM
results are reasonable, because the initial products
with the product line approach usually take longer
to develop, owing to the initial core creation
overhead. Also, our TTM trends are consistent with
general findings from product line case studies
(Clements and Northrop 2001). Furthermore, we
believe that the diagram pictured in Figure 23
might not be intended by Knauber et al. (Knauber
et al. 2002), as the y-axis is labeled as ‘accumulated
effort’ instead of ‘time-to-market’. Note that in our
example, the same schedule compression values are
assigned to all processes. We could assign different
schedule compression values to different projects to
improve the TTM performance. However, that will
result in differences in product qualities (Boehm
et al. 2000).

cost(investment)
With PL
Without PL

Figure 24. Hypothesis 4 (Knauber et al. 2002)

in Figure 25, are consistent with this hypothesis. In
this scenario, we equate accumulated development
effort with cost. Our simulation captures the sharp
increase in number of features developed but does
not provide the flattening projections indicated by
the hypothesis, because we did not model the
deterioration of the product line architecture.

6.1.3. Hypothesis 4 (Knauber et al. 2002)
The fourth hypothesis, depicted in Figure 24, states
that beyond a certain minimum investment, the
product line approach can develop more features
with a given amount of money than the traditional
approach (Knauber et al. 2002). Our results, shown

6.1.4. Hypothesis 5 (Knauber et al. 2002)
The fifth hypothesis, depicted in Figure 26, states
that given the same amount of time, the product
line approach can develop more features than
the traditional approach (Knauber et al. 2002). Our

70

Number of Features

60
50
40
30
20
10
0
0

1000

2000

3000

4000

5000

6000

7000

8000

Accumulated Development Effort
1

2

3

4

5

6

Figure 25. Results on features over costs
Copyright  2006 John Wiley & Sons, Ltd.

406

Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

A Software Product Line Process Simulator

number of features

notice that the lines in our results are less regular
than those in the hypothetical graphs. The reason
is that in our example products vary in sizes and
reuse rates, while the assumption under the hypothetical graphs is that each product has the same
size and reuse rates. We have also run the simulator through other examples and compared the
simulation results with the product line hypotheses
(Knauber et al. 2002) and other works on product line economics (Clements and Northrop 2001,
Cohen 2003, Schmid and Verlage 2002). In general,
we feel that the simulation results are reasonable.
To validate our simulator, real-world product line
historical data is needed. To date, the accessibility
of such data for software product line development
is scarce. To address this issue, we have requested
some external experts to evaluate the simulator.
Currently, we only got some initial results and they
are mostly positive.

time
With PL
Without PL

Figure 26. Hypothesis 5 (Knauber et al. 2002)

results, shown in Figure 27 are consistent with this
hypothesis. The primary difference between the
simulation results and the hypothesis comes in the
initial releases. We believe that in general, a product
line approach requires more time for the initial
features than the traditional approach.

7. CONCLUSIONS
Software product line approach requires higher
up-front investment and results in increased process
complexity. In order to reduce investment risk
and ease process management, effective decisionmaking approaches and tools are necessary. In
this article, an approach for product line process

6.2. Discussion
By comparing the above graphs with the hypothetical graphs (Knauber et al. 2002), readers may

70

Number of Features

60
50
40
30
20
10
0
0

20

40

60

80

100

120

140

160

180

Time
1

2

3

4

5

6

Figure 27. Results on features over time
Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

Softw. Process Improve. Pract., 2006; 11: 385–409

407

Research Section

decision making is presented. The approach contains three steps: process definition, simulation,
and evaluation. A process meta-model was created to assist process definition and a process
simulator was developed to provide product line
development cost, schedule, and resource usage
estimation. An example is given. The simulation
results show that the product line approaches can
reduce costs, schedule, and resource usage. The
impact of product line development approaches,
number of resources, and market demands on
product line cost-benefits are discussed. The simulation results are compared with some product
line hypotheses (Knauber et al. 2002) and they seem
reasonable.
In the future, we plan to further verify and validate the simulator by soliciting expert feedback
and comparing simulation results with real product line data. Also, the simulator can be extended
in many ways. Although the currently used cost
model, COMPLIMO (Boehm et al. 2004), can provide more accurate estimations, it requires more
data and does not consider organizational costs.
The simulator can be extended by incorporating
other cost models, which will allow users choose
the appropriate cost models according to their own
situations. The current simulation models are all
deterministic. In reality, some input parameter values (e.g. complexity) and cost models outputs (e.g.
effort) are randomly distributed numbers within
certain ranges. Stochastic behaviors can be added
to the simulator, which will permit the results
to be analyzed within certain confidence intervals. Other extensions include removing or relaxing
some existing model assumptions and dealing with
a more general product line case. We also plan
on improving the process definition by providing
a smart process definition tool and using some
standard process definition language. Currently,
the process definition is a time-consuming activity. Although process templates are provided, a
better definition tool will relieve users from routines and let them concentrate more on creative
works. The current process definition used by
the simulator is in Microsoft Project (Microsoft
Corporation 2006b) file format. Using a standard
process definition language, such as XPDL (Workflow Management Coalition 2005), to define product
line processes would improve the interoperability between the simulator and existing workflow
tools.
Copyright  2006 John Wiley & Sons, Ltd.

408

Y. Chen, G. C. Gannod and J. S. Collofello

REFERENCES
Bockle G, Clements P, McGregor JD, Muthig D, Schmid K.
2003. Calculating ROI for software product lines. IEEE
Software 21(3): 23–31.
Boehm BW, Brown AW, Madachy R, Yang Y. 2004. A
software product line life cycle cost estimation model.
2004 International Symposium on Empirical Software
Engineering (ISESE’04). IEEE Computer Society: Redondo
Beach, CA, 156–164.
Boehm BW, Horowitz E, Madachy R, Reifer D, Clark BK,
Steece B, Brown AW, Chulani S, Abts C. 2000. Software
Cost Estimation with COCOMO II. Prentice Hall:
Englewood Cliffs, NJ.
Bosch J. 2002. Maturity and evolution in software product
lines: Approaches, artefacts and organization. In The
Second Software Product Line Conference, San Diego, CA,
257–271.
Clements P, Northrop LM. 2001. Software Product Lines:
Practices and Patterns. Addison-Wesley: Boston, MA.
Clements PC, Northrop LM. 2002. Salion, Inc.: A software
product line case study. Technical Report CMU/SEI-2002TR-038, Software Engineering Institute, Carnegie Mellon
University: Pittsburgh, PA.
Clements PC, McGregor JD, Cohen SG. 2005. The
structured intuitive model for product line economics
(SIMPLE). Technical Report CMU/SEI-2005-TR-003,
Software Engineering Institute, Carnegie Mellon
University: Pittsburgh, PA.
Cohen S. 2003. Predicting when product line investment
pays. Technical Report CMU/SEI-2003-TN-017, Software
Engineering Institute, Carnegie Mellon University:
Pittsburgh, PA.
Czarnecki K, Eisenecker UW. 2000. Generative Programming: Methods, Tools, and Applications. Addison-Wesley:
Boston & London.
Host M, Regnell B, och Dag JN, Nedstam J, Nyberg C.
2001. Exploring bottlenecks in market-driven requirements management processes with discrete event simulation. Systems and Software 59(3): 323–332.
Kellner MI, Madachy RJ, Raffo DM. 1999. Software
process simulation modeling: Why? What? How?. Systems
and Software 46(2–3): 91–105.
Knauber P, Bermejo J, Bockle G, do Prado Leite JCS, van
der Linden F, Northrop LM, Stark M, Weiss DM. 2002.
Quantifying product line benefits. PFE ‘01: Revised Papers
from the 4th International Workshop on Software ProductFamily Engineering. Springer-Verlag: London, England,
155–163.
Softw. Process Improve. Pract., 2006; 11: 385–409
DOI: 10.1002/spip

Research Section

Krueger CW. 2002. Easing the transition to software
mass customization. PFE ‘01: Revised Papers from the
4th International Workshop on Software Product-Family
Engineering. Springer-Verlag: London, England, 282–293.
Lamine SBAB, Jilani LL, Ghezala HHB. 2005. Cost
estimation for product line engineering using COTS
components. In 9th International Software Product Line
Conference, Rennes, France. 113–123.
Microsoft Corporation. 2006a. Microsoft office online:
Excel 2003 home page. http://office.microsoft.com/enus/FX010858001033.aspx. Last accessed Jan 2006.
Microsoft Corporation. 2006b. Microsoft office online:
Project 2003 home page. http://office.microsoft.com/enus/FX010857951033.aspx. Last accessed Jan 2006.
Poulin JS. 1997. The economics of product line
development. International Journal of Applied Software
Technology 3(1): 15–28.
Schmid K, Biffl S. 2005. Systematic management of
software product lines. Software Process Improvement and
Practice 10(1): 61–76.

Copyright  2006 John Wiley & Sons, Ltd.
DOI: 10.1002/spip

A Software Product Line Process Simulator

Schmid K, Verlage M. 2002. The economic impact of
product line adoption and evolution. IEEE Software 19(4):
50–57.
Weiss DM, Lai CTR. 1999. Software Product-Line
Engineering: A Family-Based Software Development Process.
Addison-Wesley: Reading, MA.
Withey J. 1996. Investment analysis of software assets
for product lines. Technical Report CMU/SEI-96-TR010, Software Engineering Institute, Carnegie Mellon
University: Pittsburgh, PA.
Workflow Management Coalition. 2005. Process definition interface – XML process definition language. Technical Report WFMC-TC-1025, http://www.wfmc.org/
standards/docs/TC-1025 10 Xpd/ 102502.pdf. Workflow Management Coalition.
Zeigler BP, Sarjoughian HS. 2003. Introduction to
DEVS modeling & simulation with JAVA: Developing component-based simulation models. http://www.
acims.arizona.edu/SOFTWARE/software.shtml.
Last
accessed, Jan 2006.

Softw. Process Improve. Pract., 2006; 11: 385–409

409

545

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-6, NO. 6, NOVEMBER 1980

Some

Stability

Measures for Software

Maintenance

STEPHEN S. YAU, FELLOW, IEEE, AND JAMES S. COLLOFELLO, MEMBER, IEEE

enhancements of capabilities, deletion of obsolete capabilities,
and optimization [7]. The cost of these software maintenance
activities has been very high, and it has been estimated ranging
from 40 percent [1 ] to 67 percent [2] of the total cost during
the life cycle of large-scale software systems. This very high
software maintenance cost suggests that the maintainability of
a program is a very critical software quality attribute. Measures
are needed to evaluate the maintainability of a program at
each phase.of its development. These measures must be easily
calculated and subject to validation. Techniques must also be
developed to restructure the software during each phase of its
development in order to improve its maintainability.
In this paper, we will first discuss the software maintenance
process and the software quality attributes that affect the
Index Terms-Algorithms, applications, logical stability, module maintenance effort. Because accommodating the ripple effect
stability, maintenance process, normalization, potential ripple effect, of modifications in a program is normally a large portion of
program stability, software maintenance, software quality attributes, the maintenance effort, especially for not well designed provalidation.
grams [7], we will present some measures for estimating the
stability of a program, which is the quality attribute indicating
the resistance to the potential ripple effect which a program
I. INTRODUCTION
would
have when it is modified. Algorithms for computing
IT IS well known that the cost of large-scale software sysmeasures and for normalizing them will be
these
stability
tems has become unacceptably high [1], [2]. Much of this
of these measures during the maintenance
Applications
given.
excessive software cost can be attributed to the lack of meanan
with
example are also presented. Future realong
phase
ingful measures of software. In fact, the definition of software
the application of these measures
search
efforts
involving
quality is very vague. Since some desired attributes of a program can only be acquired at the expense of other attributes, during the design phase, program restructuring based on these
program quality must be environment dependent. Thus, it is measures, and the development of an overall maintainability
impossible to establish a single figure for software quality. measure are also discussed.
Instead, meaningful attributes which contribute to software
II. THE MAINTENANCE PROCESS
quality must be identified. Research results in this area have
As
discussed, software maintenance is a very
previously
contributed to the definition of- several software quality atbroad
activity. Once a particular maintenance objective
tributes, such as correctness, flexibility, portability, effithe maintenance personnel must first underis
established,
ciency, reliability, integrity, testability, and maintainability
are to modify. They must then modify the
stand
what
they
[3] - [6]. These results are encouraging and provide a reasonthe maintenance objectives. After modito
satisfy
program
ably strong basis for the definition of the quality of software.
must
ensure that the modification does not
fication,
they
Since software quality is environment dependent, some atof the program. Finally, they must test
affect
other
portions
tributes may be more desirable than others. One attribute
These
activities can be accomplished in the
the
program.
which is almost always desirable except in very limited applicain
as
shown
four
Fig. 1.
phases
tions is the maintainability of the program. Software mainteof analyzing the program in order to
first
consists
The
phase
nance is a very broad activity that includes error corrections,
understand it. Several attributes such as the complexity of the
Manuscript received April 1, 1980; revised July 25, 1980. This program, the documentation, and the self-descriptiveness of
work was supported by the Rome Air Development Center, U.S. Air the program contribute to the ease of understanding the proForce System Command, under Contracts F30602-76-C0397 and gram. The complexity of the program is a measure of the efF30602-80-C-0139.
S. S. Yau is with the Department of Electrical Engineering and Com- fort required to understand the program and is usually based
puter Science, Northwestern University, Evanston, IL 60201.
on the control or data flow of the program. The self-descripJ. S. Collofello was with the Department of Electrical Engineering tiveness of the program is a measure of how clear the program
and Computer Science, Northwestern University, Evanston, IL 60201.
He is now with the Department of Computer Science, Arizona State is, i.e., how easy it is to read, understand, and use [5].
University, Tempe, AZ 85281.
The second phase consists of generating a particular mainteAbstract-Software maintenance is the dominant factor contributing
to the high cost of software. In this paper, the software maintenance
process and the important software quality attributes that affect the
maintenance effort are discussed. One of the most important quality
attributes of software maintainability is the stability of a program,
which indicates the resistance to the potential ripple effect that the
program would have when it is modified. Measures for estimating the
stability of a program and the modules of which the program is composed are presented, and an algorithm for computing these stability
measures is given. An algorithm for normalizing these measures is also
given. Applications of these measures during the maintenance phase
are discussed along with an example. An indirect validation of these
stability measures is also given. Future research efforts involving application of these measures during the design phase, program restructuring based on these measures, and the development of an overall
maintainability measure are also discussed.

0098-5589/80/1100-0545$00.75 © 1980 IEEE

546

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-6, NO. 6, NOVEMBER 1980
Correct
program errors

Fig.

1.

The software maintenance process.

nance proposal to accomplish the implementation of the maintenance objective. This requires a clear understanding of both
the maintenance objective and the program to be modified.
However, the ease of generating maintenance proposals for a

is primarily affected by the attribute extensibility. The
extensibility of the program is a measure ofthe extent to which
the program can support extensions of critical functions [5].
The third phase consists of accounting for all of the ripple
effect as a consequence of programn modifications. In software, the effect of a modification may not be local to the
modification, but may also affect other portions of the program. There is a ripple effect from the location of the modification to the other parts of the programs that are affected
by the modification [7]. One aspect of this ripple effect is
logical or functional in nature. Another aspect of this ripple
effect concems the performance of the program. Since a
large-scale program usually has both functional and performance requirements, it is necessary to understand the potential effect of a program modification from both a logical and
a performance point of view [7]. The primary attribute affecting the ripple effect as a consequence of a program modification is the stability of the program. Program stability is
defined as the resistance to the amplification of changes in
the program.
The fourth phase consists of testing the modified program
to ensure that the modified program has at least the same reliability level as before. It is important that cost-effective
program

testing techniques be applied during maintenance. The primary factor contributing to the development of these costeffective techniques is the testability of the program. Program testability is defined as a measure of the effort required
to adequately test the program according to some well defined
testing criterion.
Each of these four phases and their associated software
quality attributes are critical to the maintenance process. All
of these software quality attributes must be combined to form
a maintainability measure. One of the most important quality
attributes is the stability of the program. This fact can be illustrated by considering a program which is easy to understand, easy to generate modification proposals for, and easy
to test. If the stability of the program is poor, however, the
impact of any modification on the program is large. Hence,
the maintenance cost will be high and the reliability may also
suffer due to the introduction of possible new errors because
of the extensive changes that have to be made.
Although the potential benefits of a validated program stability measure are great, very little research has been conducted
in this area. Previous stability measures have been developed
by Soong [3], Haney [6], and Myers [4]. There exist several
weaknesses in these measures which have prevented their wide
acceptance. Their largest problem has been the inability to
validate the measures due to model inputs that are questionable or difficult to obtain. Other weaknesses of these measures include an assumption that all modifications to a module
have the same ripple effect, a symmetry assumption that if
there exists a nonzero probability of having to change a module i given that module i is changing then there exists a nonzero probability of having to change module i given that module i is changing, and a failure to incorporate a performance
component as part of the stability measure.
III. DEVELOPMENT OF LOGICAL STABILITY MEASURES
The stability of a program has been defined as the resistance
to the potential ripple effect that the program would have
when it is modified. Before considering the stability of a
program, it is necessary to develop a measure for the stability
of a module. The stability of a module can be defined as a
measure of the resistance to the potential ripple effect of a
modification of the module on other modules in the program.
There are two aspects of the stability of a module: the logical
aspect and the performance aspect. The logical stability of a
module is a measure of the resistance to the impact of such a
modification on other modules in the program in terms of
logical considerations. The performance stability of a module
is a measure of the resistance to the impact of such a modification on other modules in the program in terms of performance
considerations. In this paper, logical stability measures will be
developed for a program and the modules of which the program is composed. Performance stability measures are currently under development and the results will be reported in
a subsequent paper. Both the logical and the performance
stability measures are being developed to overcome the weaknesses of the previous stability measures. In addition, the stability measures are being developed with the following requirements to increase their applicability and acceptance:

YAU AND COLLOFELLO: STABILITY MEASURES FOR SOFTWARE MAINTENANCE

1) ability to validate the measures,
2) consistency with current design methodologies,
3) utilization in comparing alternate designs, and
4) diagnostic ability.
It should be noted that the stability measures being described
are not in themselves indicators of program maintainability.
As previously mentioned, program stability is a significant
factor contributing to program maintainability. Although the
measures being described estimate program stability, they
must be utilized in conjunction with the other attributes
affecting programn maintainability. For example, a single module program of 20 000 statements will possess an excellent
program stability since there cannot be any ripple effect among
modules; however, the maintainability of the program will
probably be quite poor.
Development of a Module Logical Stability Measure
The logical stability of a module is a measure of the resistance to the expected impact of a modification to the module
on other modules in the program in terms of logical considerations. Thus, a computation of the logical stability of a module
must be based upon some type of analysis of the maintenance
activity which will be performed on the module. However,
due to the diverse and almost random nature of software
maintenance activities, it is virtually meaningless to attempt
to predict when the next maintenance activity will occur and
what this activity will consist of. Thus, it is impossible to develop a stability measure based upon probabilities of what the
maintenance effort will consist of. Instead, the stability measure must be based upon some subset of maintenance activity
for which the impact of the modifications can readily be determined. For this purpose, a primitive subset of the maintenance activity is utilized. This consists of a change to a single
variable definition in a module. This primitive subset of maintenance activity is utilized because regardless of the complexity of the maintenance activity, it basically consists of modifications to variables in the modules. A logical stability measure
can then be computed based upon the impact of these primitive modifications on the program. This logical stability measure, will accurately predict the impact of these primitive modifications on the program and, thus, can be utilized to compute
the logical stability of the module with respect to the primitive modifications.
Due to the nature of the logical stability of a module, an
analysis of the potential logical ripple effect in the program
must be conducted. There are two aspects of the logical ripple
effect which must be examined. One aspect concerns intramodule change propagation. This involves the flow of program
changes within the module as a consequence of the modification. The other aspect concerns intermodule change propagation. This involves the flow of program changes across module
boundaries as a consequence of the modification.
Intramodule change propagation is utilized to identify the
set Zki of interface variables which are affected by log,l al
ripple effect as a consequence of a modification to vaiiable
definition i in module k. This requires an identification of
which variables constitute the module's interfaces and a
characterization of the potential intramodule change propa-

547

gation among the variables in the module. The variables that
constitute the module's interfaces consist of its global variables, its output parameters and its variables utilized as input
parameters to called modules. Each utilization of a variable
as an input parameter to a called module is regarded as a
unique interface variable. Thus, if variable x is utilized as an
input parameter in two module invocations, then each occurrence of x is regarded as a unique interface variable. Each
occurrence must be regarded as a separate interface variable
since the complexity of affecting each occurrence of the
variable as well as the probability of affecting each occurrence may differ.
Once an interface variable is affected, the flow of program
changes may cross module boundaries and affect other modules. Intermodule change propagation is then utilized to compute the set Xki consisting of the set of modules involved in
intermodule change propagation as a consequence of affecting
interface variable j of module k. In the worst case logical ripple effect analysis, Xk, is calculated by first identifying all the
modules for which j is an input parameter or global variable.
Then, for each of these modules in Xkj, the intramodule
change propagation eminating from j is traced to the interface
variables within the module. Intermodule change propagation
is then utilized to identify other modules affected and these
are added to Xkj. This continues until the ripple effect terminates or no new modules can be added to Xk,. An algorithm
for performing this worst case ripple effect has already been
developed [7], [8].
The worst case ripple effect tracing can significantly be refined if explicit assumptions exist for each module in the program for its input parameters or global variables. Intermodule
change propagation tracing would then examine if a module's
assumptions have been violated to determine whether it should
become a part of the change propagation. If a modiule's assumptions have not been violated, then the ripple effect will
not affect the module.
There are many possible approaches to refining the worst
case ripple effect which would not require a complete set of
assumptions made for each interface variable for every module. For example, a significant refinement to the worst case
change propagation can result by utilizing the simple approach of examining whether or not a module makes any
assumptions about the values of its interface variables. These
assumptions can be expressed as program assertions. If it
does not make any assumptions about the values of its interface variables, then the module cannot be affected by intermodule change propagation. However, if it does make an
assumption about the value of an interface variable, then
the worst case is automatically in effect and the module is
placed in the change propagation resulting from affecting
the interface variable if the interface variable is also in the
change propagation as a consequence of some modification.
Both intramodule and intermodule change propagation
must be utilized to compute the expected impact of a primitive modification to a module on other modules in the program. A measure is needed to evaluate the magnitude of this
logical ripple effect which occurs as a consequence of modifying a variable definition. This measure must be associated

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-6, NO. 6, NOVEMBER 1980

548

with each variable definition in order that the impact of of a primitive type of modification to a module k, denoted
modifying the variable definition during maintenance can by LREk, can be computed. The potential logical ripple
be determined. This logical complexity of modification effect of a module is a measure of the expected impact on
figure will be computed for each variable definition i in the program of a primitive modification to the module. Thus,
every module k and is denoted by LCMki. There are many the potential logical ripple effect can be computed as follows:
possible measures which may be used for LCMki. All of
LREk = Z [P(ki) . LCMkiI
these measures are dependent upon computation of the modi E Vk
ules involved in the intermodule change propagation as a consequence of modifying i. The modules involved in the inter- where Vk is the set of all variable definitions in module k.
module change propagation as a consequence of modifying
A measure for the logical stability of a module k, denoted
variable definition i of module k can be represented by the by LSk, can then be established as follows:
set Wki which is constructed as follows:
LSk, = 1/LREk.

Wki= U Xkj.
jGZki

The simplest measure for LCMki would be the number of
modules involved in the intermodule change propagation as
a consequence of modifying i. This measure provides a crude
measure of the amount of effort required to analyze the program to ensure that the modification does not introduce any
inconsistency into the program. Other measures which examine not only the number of modules involved in the intermodule change propagation, but also the individual complexity of the modules, provide more realistic measures of the
amount of effort required to analyze the program to ensure
that inconsistencies are not introduced. One such easily computed measure is McCabe's cyclomatic number [9]. The
cyclomatic number V(G) is defined in terms of the number of
basic paths in the module. A basic path is defined as a path in
the module that when taken in combination can generate all
possible paths. Computation of the cyclomatic number is,
thus, based on a directed-graph representation of the module.
For such a graph G1, the cyclomatic number can be calculated
as the number of branches in Gi minus the number of vertices
in GJ plus two. Utilizing the cyclomatic number or any other
complexity measure, the complexity of modification of variable definition i of module k can be computed as follows:

LCMki=

L
t e Wki

ct

where Ct is the complexity of module t.
Since the logical stability of a module is defined as the resistance to the potential logical ripple effect of a modification
to a variable definition i on other modules in the program, the
probability that a particular variable definition i of a module
k will be selected for modification, denoted by P(ki), must
be determined. Now, a basic assumption of utilizing primitive types of maintenance activity is that a modification can
occur with equal probability at any point in the module. This
implies that each occurrence of each variable defmition has
an equal probability of being affected by the maintenance
activity. Thus, for each module we can calculate the number
of variable definitions. If the same variable is defined twice
within a module, each definition is regarded separately. The
probability that a modification to a module will affect a particular variable definition in the module can then be computed
as l/(number of variable definitions in the module).
With the information of LCMki and P(ki) for each variable
definition i of a module k, the potential logical ripple effect

Development of a Program Logical Stability Measure
A measure for the potential logical ripple effect of a primitive modification to a program, denoted by LREP, can easily
be established by considering it as the expected value of
LREk over all of the modules in the program. Thus, we have
n

LREP= E [P(k)
k=l

LREkJ

where P(k) is the probability that a modification to module
k may occur, and n is the number of modules in the program.
A basic assumption of utilizing primitive modifications is that
a modification can occur with equal probability to any module and at any point in the module. Utilizing this assumption,
the probability that a modification will affect a particular
module can be computed as l/n, where n is the number of
modules in the program. This assumption can be relaxed
if additional information regarding the program is available.
For example, if the program has only recently been released
and it is believed that a significant part of the maintenance
activity will involve error correction, then the probabilities
that particular modules may be affected by a modification
may be altered to reflect the probabilities that errors in these
modules may be discovered. This can be accomplished by
utilizing some complexity or software science measures [10].
A measure for the logical stability of a program, denoted
by LSP, can then be established as follows:
LSP= 1/LREP.
IV. ALGORITHM FOR THE COMPUTATION OF THE
LOGICAL STABILITY MEASURES
In this section, an algorithm will be outlined for the computation of these logical stability measures. The following description of this algorithm assumes that there does not exist
any prior knowledge which might affect the probabilities of
program modification, and McCabe's complexity measure [91
is utilized. The algorithm can easily be modified to allow for
prior knowledge concerning the probabilities of program modification or to utilize a different complexity measure. The
algorithm consists of the following steps.
Step 1: For each module k, identify the set Vk of all variable definitions in module k. Each occurrence of a variable
in a variable definition is uniquely identified in Vk. Thus, if
the same variable is defined twice within a module, then Vk

549

YAU AND COLLOFELLO: STABILITY MEASURES FOR SOFTWARE MAINTENANCE

contains a unique entry for each definition. The set Vk is
created by scanning the source code of module k and adding
variables which satisfy any of the following criteria to Vk.
a) The variable is defined in an assignment statement.
b) The variable is assigned a value which is read as input.
c) The variable is an input parameter to module k.
d) The variable is an output parameter from a called module.
e) The variable is a global variable.
Step 2: For each module k, identify the set Tk of all interface variables in module k. The set Tk is created by scanning
the source code of module k and adding variables which satisfy
any of the following criteria to Tk.
a) The variable is a global variable.
b) The variable is an input parameter to a called module.
Each utilization of a variable as an input parameter to a called
module is regarded as a unique interface variable. Thus, if
variable x is utilized as an input parameter in two module
invocations, then each occurrence of x is regarded as a unique
interface variable.
c) The variable is an output parameter of module k.
Step 3: For each variable definition i in every module k,
compute the set Zki of interface variables in Tk which are
affected by a modification to variable definition i of module k by intramodule change propagation [7], [8].
Step 4: For each interface variable j in every module k,
compute the set Xkf consisting of the modules in intermodule change propagation as a consequence of affecting interface variable j of module k.
Step 5: For each variable definition i in every module k,
compute the set Wki consisting of the set of modules involved
in intermodule change propagation as a consequence of
modifying variable definition i of module k. Wki is formed
as follows:

Wki= U Xkj.

jEZki
Step 6: For each variable definition i, in every module k,
compute LCMki as follows:
LCMki= Z Ct
tE Wki

where Ct is the McCabe's complexity measure of module t.
Step 7: For each variable definition i in every module k,
compute the probability that a particular variable definition
i of module k will be selected for modification, denoted by
P(ki), as follows:
P(ki) = l/(the number of elements in Vk).
Step 8: For each module k, compute LREk and LSk as
follows:
LREk

=

i

E
Vk

[P(ki) * LCMk,i

LSk = l/LREk.

Step 9: Compute LREP and LSP as follows:
n

LREP = E [P(k) * LREkI
k=l

where P(k) = 1/n, and n is the number of modules in the program. Then
LSP = 1 /LREP.
V. APPLICATIONS

OF THE LOGICAL STABILITY
MEASURES
The logical stability measures presented in this paper can be
utilized for comparing the stability of alternate versions of a
module or a program. The logical stability measures can also
be normalized to provide an indication of the amount of
effort which will be needed during the maintenance phase
to accommodate for inconsistency created by logical ripple
effect as a consequence of a modification. Based upon these
figures, decisions can be made regarding the logical stability
of a program and the modules of which the program is composed. This information can also help maintenance personnel
select a particular maintenance proposal among altematives.
For example, if it is determined that a particular maintenance
proposal affects modules which have poor stability, then
alternative modifications which do not affect these modules
should be considered. Modules whose logical stability is too
low may also be selected for restructuring in order to improve their logical stability.
The logical stability measures can be normalized by first
modifying the computation of the module logical ripple
effect measure to include the complexity of the module
undergoing maintenance. Let LREZ denote this new logical
ripple effect measure for module k which is calculated as

follows:
E =
LREk

+ E

i G Vk

[P(ki) LCMki]

where Ck is the complexity of module k. This enables LREk
to become an expected value for the complexity of a primitive modification to module k. Let Cp be the total complexity
of the program which is equal to the sum of all the module
complexities in the program. Note that LREk < Cp since the
ripple effect is bounded by the number of modules in the program. The normalized logical ripple effect measure for module k, denoted as LREZ, can then be calculated as follows:
LRE* = LRE,+/Cp.

The normalized logical stability measure for module k, denoted as LS*, can then be calculated as follows:

LS* =1 - LREZ.
The normalized logical stability measure has a range of 0 to
1 with 1 the optimal logical stability. This normalized logical
stability can be utilized qualitatively or it can be correlated
with collected data to provide a quantitative measure of

stability.
The normalized logical stability measure for the program,
denoted as LSP*, can be computed by first calculating the
nornalized logical ripple effect measure for the program,
denoted as LREP*, as follows:
LREP*= L [P(k) * LRE*].
k=l

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-6, NO. 6, NOVEMBER 1980

550

The normalized logical stability measure for the program can
then be calculated as follows:
LSP* = 1 - LREP*.
LSP* has the same range and interpretation as LS*k.
VI. EXAMPLE

In this section the logical stability measures for the program in Fig. 2 will be calculated according to the previously
described algorithm as follows:

LREMAIN = 4, LRERROOTS = 2.9, LREIROOTS

=

2.7.

The logical stability of each of the modules is given by

LSMAIN = 0.25, LSRROOTS = 0.34, LSIROOTS

=

0.37.

The potential logical ripple effect of the program is
LREP= 3.2
and hence the logical stability of the program is given by
LSP = 0.31.
The normalized logical stability measures for each of the
modules and the program are given as follows:

LSMAIN

0

LS* ROOTS 0 .02
LSIROOTS
= 0.0267.
LSP*
= 0.06

These measures indicate that the stability of the program in
Fig. 2 is extremely poor. An examination of the program provides intuitive support of these measures since the program
utilizes common variables in every module as well as shared
information in the form of passed parameters. Thus, the
change propagation potential is very high in the program.
VII. VALIDATION OF STABILITY MEASURES

As previously mentioned, an important requirement of the
stability measures necessary to increase their applicability
and acceptance is the capability of validating them. The
previous stability measures [3], [4], [61 failed to satisfy this
requirement due to calculations involving subjective or difficult to obtain inputs about the program being measured.
The stability measures presented in this paper do not suffer
from these limitations since they are produced from algorithms which calculate intermodule and intramodule change
propagation properties of the program being measured. Thus,
these measures easily lend themselves to validation studies.
The stability measures presented in this paper can be validated either directly through experimentation or indirectly
through a discussion of how they are influenced by various
established attributes of a program which affect its stability
during maintenance. The direct approach to validation requires a large database of maintenance information for a significant number of various types of programs in different
languages which have undergone a significant number of
modifications of a wide variety. One experimental approach
would be to examine sets of programs developed to identical

C MODULE MAIN
C SOLUTION OF THE QUADRATIC EQUATION

C A*X*X+B*X+C = 0
COMMON HR1,HR2,HI
READ 100 (A,B,C)
100 FORMAT (3F10.4)

Hi

=

-B/(2.*A)

HR = Hl*H1
DISC = HR - C/A
CSID = Hl*Hl - C/A
CALL RROOTS (CSID,DISC,H1)
WRITE 100 HR1,HR2,HI
END

C MODULE RROOTS
SUBROUTINE RROOTS (CSID,DISC,H1)
COMMON HR1,HR2,HI
IF (DISC.LT.0) GOTO 10
H2 = SQRT (DISC)
HR1 = H1 + H2
HR2 = Hl - H2
HI = 0.
10 CONTINUE
CALL IROOTS (CSID,DISC,H1)
RETURN

C MODULE IROOTS
SUBROUTINE IROOTS (CSID,DISC,H1)

COMMON HR1,HR2,HI

IF (CSID.GE.0) 01TO 10

H2 = SQRT(-DISC)
HRI = Hl
HR2 = Hl
HI = H2
10 CONTINUE

RETURN

Fig. 2. An example program for computing the stability measures.

specifications but differing in design or coding. Logical stability measures for each version of the program could then
be calculated to determine which possesses the best stability.
A set of identical modifications to the specifications of each
program could then be performed. For each modification to
each program, a logical complexity of modification, LCM,
could then be calculated based upon the difficulty of implementing the particular modification for the program. One
particular method for calculating an LCM has previously been
described [7], [8] . After a significant number of identical specification modifications have been implemented on all versions
of the program, an average logical complexity of modification,
ALCM, could be computed for each version of the program.
This ALCM reflects the stability of the program and, thus,
the ALCM can be utilized as a variable in the experiment.
After a significant number of sets of programs have undergone their sets of modifications, experimental conclusions
based upon a statistical analysis of the ALCM figures and
the stability measures could be formulated.
This direct approach to validation of the stability measures
will be difficult due to the number of programs and modifications necessary to produce significant statistical results. Thus,
this direct approach to validation will be performed utilizing
the maintenance data base which will be created in conjunction with the validation of our program maintainability measure which is currently under investigation.
The stability measures presented here can also be indirectly
validated by showing how the measures are affected by some
attributes of the program which affect its stability during
maintenance. One program attribute which affects maintainability is the use of global variables. The channeling of communication via parameter-passing rather than global variables
is characteristic of more maintainable programs [11]. Thus,

YAU AND COLLOFELLO: STABILITY MEASURES FOR SOFTWARE MAINTENANCE

an indirect validation of the stability measures must show
that the stability of programs utilizing parameter passing is
generally better than that of programs utilizing global variables. This can be easily shown since the calculation of LS1
is based upon the LCM of each interface variable in module
i. Since -global variables are regarded as interface variables
and since the LCM of an interface variable is equal to the sum
of the complexity of the modules affected by modification
of the interface variable, LSi will be small for modules sharing
the global variable. Thus, the logical stability of the program
will also be small. On the other hand, if communication is
via parameter passing instead of global variables, the LCM of
the parameters will generally be small, and hence LSi and LSP
will generally be improved. Thus, the stability measures indicate that the stability of programs utilizing parameter passing is generally better than that of programs utilizing global
variables.
The stability of a program during maintenance is also affected by the utilization of data abstractions. Data abstractions hide information about data which may undergo modification from the program modules which manipulate it.
Thus, data abstraction utilization is characteristic of more
maintainable programs. An indirect validation of the stability
measures must, therefore, show that the stability of programs
utilizing data abstractions is generally better than that of programs whose modules directly manipulate data structures.
This can easily be shown by examining the stability measures
of a program that utilizes data abstractions and comparing
those measures to that of an equivalent program in which the
modules directly access the data structure, i.e., data abstractions are not utilized. The modules which utilize a data abstraction to access a data structure will have fewer assumptions
about their interface variables and hence have higher stability
than that of the modules directly accessing the data structure
and hence having many assumptions about it. For example,
consider a data structure consisting of records where each
record has an employee number and a department number.
Assume that module INIT initializes the data structure and
orders the records by the employee number. Also, assume
modules X, Y, and Z must access the data structure to obtain
the department for a given employee number. In this design,
if module INIT is modified so that the records in the data
structure are ordered by the department instead of the employee number, then modules X, Y, and Z must also be modified. This potential modification is reflected in the calculation of LSINIT and, consequently, LSP. If, however, modules
X, Y, and Z access the data structure through a data abstraction, then the same modification to module INIT will affect
the data abstraction algorithm, but not modules X, Y, and Z.
Consequently, LSINIT and, consequently, LSP will be larger
in the program which utilizes the data abstraction than the
measures for the program which does not. Thus, the stability
measures proposed in this paper indicate that the stability of
programs utilizing data abstractions is generally better than
that of programs which do not.
Another attribute affecting program stability during maintenance is a program control and data structure in which the
scope of effect of a module lies within the scope of control
of the module. This implies that the only part of a program

551

affected by a change to a module, i.e., its scope of effect, is
a subset of the modules which are directly or indirectly invoked by the modified module, i.e., its scope of control [12].
An indirect validation of the stability measures must, therefore, show that the stability of programs possessing this type
of control and data structure are better than that of programs
which do not possess this attribute. Now a program which
exhibits this scope of effect/scope of control property has a
logical stability which is calculated from the logical stability
of its modules, each of which is bounded above by the sum of
the complexity of the modules which lie within its scope of
control. If the scope of effect of a modification to a module
does not lie within the scope of control of the module, the
logical stability of the module is only bounded above by the
complexity of the entire program. Thus, the stability measures indicate that the stability of programs possessing the
scope of effect/scope of control attribute are generally better
than that of programs which do not possess this attribute.
Another attribute affecting program stability during maintenance is the complexity of the program. Program complexity directly affects the understandability of the program and,
consequently, its maintainability. Thus, an indirect validation
of the stability measures must, therefore, show that the stability of programs with less complexity is generally better
than that of programs with more complexity. This is readily
apparent from the calculation of the logical complexity of
modification of an interface variable. Thus, complexity is
clearly reflected in the calculation of the stability measures.
The stability measures presented here can, thus, be indirectly
validated since they incorporate and reflect some aspects of
program design generally recognized as contributing to the
development of program stability during maintenance.
VIII. CONCLUSION AND FUTURE RESEARCH
In this paper, measures for estimating the logical stability
of a program and the modules of which the program is composed have been presented. Algorithms for computing these
stability measures and for normalizing them have also been
given. Applications and interpretations of these stability measures as well as an indirect validation of the measures have
been presented.
Much research remains to be done in this area. One area of
future research involves the application of the logical stability
measures to the design phase of the software life cycle. An
analysis of the control flow and the data flow of the design
of the program should provide sufficient information for
calculation of a logical stability measure during the design

phase.
Another area of future research involves the development of
a performance stability measure. Since a program modification may result in both a logical and a performance ripple effect, a measure for the performance stability of a program and
the modules of which the program is composed is also necessary

[7], [81.

Much research also remains to be done in the identification
of the other software quality factors contributing to maintainability. Suitable measures for these software quality factors
must also be developed. These measures must then be integrated with the stability measures to produce a maintainability

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-6, NO. 6, NOVEMBER 1980

552
measure.

This maintainability

measure

must be calculatable

[7] S. S. Yau, J. S. Coliofello, and T. M. MacGregor, "Ripple effect

at each phase of the software life cycle and must be validated.

Another area of future research involves the development of
automated restructuring techniques to improve both the stability of a program and the modules of which the program is composed. These restructuring techniques should be applicable at
each phase of the software development. Restructuring techniques must also be developed to improve the other quality
factors contributing to maintainability. These restructuring
techniques must automatically improve the maintainability of
the program at each phase of its development. The net results
of this approach should be a significant reduction of the maintenance costs of software programs and, consequently, a substantial reduction in their life cycle costs. Program reliability
should also be improved because fewer errors may be injected
into the program during program changes due to its improved
maintainability.
REFERENCES
[1] B. W. Boehm, "Software and its impact: A quantitative assessment," Datamation, pp. 48-59, May 1973.
[2] M. V. Zelkowitz, "Perspectives on software engineering," ACM
Comput. Surveys, vol. 10, pp. 197-216, June 1978.
[3] N. L. Soong, "A program stability measure," in Proc. 1977Annu.
A CM Conf., pp. 163-173.
[4] G. J. Myers, Reliable Software through Composite Design. Petrocelli/Charter, 1979, pp. 137-149.
[5] J. A. McCall, P. K. Richards, and G. F. Walters, Factors in Software Quality, Volume 111: Preliminary Handbook on Software
Quality for an Acquisition Manager, NTIS AD-A049 055, Nov.
1977, pp. 2-1-3-7.
[6] F. M. Haney, "Module connection analysis," in Proc. AFIPS
1972 Fall Joint Comput. Conf., vol. 41, part I, pp. 173-179.

[81

[9]
[10]

[111

[121

analysis of software maintenance," in Proc. COMPSAC 78, pp.
60-65.
S. S. Yau, "Self-metric software-Summary of technical progress," Rep. NTIS AD-A086-290, Apr. 1980.
T. J. McCabe, "A complexity measure," IEEE Trans. Software
Eng., vol. SE-2, pp. 308-320, Dec. 1976.
M. H. Halstead, Elements of Software Science. New York:
Elsevier North-Holland, 1977, pp. 84-91.
L. A. Belady and M. M. Lehman, "The characteristics of large
systems," in Research Directions in Software Technology, P.
Wegner, Ed. Cambridge, MA: M.I.T. Press, 1979, pp. 106-139.
E. Yourdon and L. Constantine, Structured Design. Yourdon,
1976.

Stephen S. Yau (S'60-M'61-SM'68-F'73), for a photograph and biography, see p. 434 of the September 1980 issue of this TRANSACTIONS.

W

James S. Collofello (S'78-M'79) received the
B.S. and M.S. degrees in mathematics/computer science from Northern Illinois University, Dekalb, in 1976 and 1977, respectively,
and the Ph.D. degree in computer science

from Northwestern University, Evanston, IL,
in 1978.

After graduating, he was a visiting Assistant
Professor in the Department of Electrical Enand Computer Science, Northwestern
| __ gineering
wF bl
University. He joined the faculty of the Department of Computer Science, Arizona State University, Tempe, in
August 1979 and is currently an Assistant Professor there. He is interested in the reliability and maintainability of computing systems
and the development, validation, and application of software quality
metrics.
Dr. Collofello is a member of the Association for Computing Machinery and Sigma Xi.

JOURNAL OF SOFTWARE MAINTENANCE AND EVOLUTION: RESEARCH AND PRACTICE
incorporating SOFTWARE PROCESS: IMPROVEMENT AND PRACTICE
J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
Published online 18 October 2010 in Wiley Online Library (wileyonlinelibrary.com). DOI: 10.1002/smr.515

Reducing the risk of requirements volatility: findings from an
empirical survey
Susan Ferreira1, ∗, † , Dan Shunk2 , James Collofello2 , Gerald Mackulak2
and AmyLou Dueck3
1 Systems
2 School

Engineering Research Center, The University of Texas at Arlington, Arlington, TX, U.S.A.
of Computing, Informatics, and Decision Systems Engineering, Arizona State University,
Tempe, AZ, U.S.A.
3 Mayo Clinic, Scottsdale, AZ, U.S.A.

SUMMARY
Requirements volatility is a common development project risk that can have severe impacts. An empirical
survey of over 300 software project managers and other software development personnel was performed
to examine effects of various software development factors on requirements volatility. This paper reports
the survey data results showing relationships between a number of software development process factors
and requirements volatility. Key software project factors studied for their relationship with requirements
volatility include process maturity level and various process techniques used for requirements engineering
activities, such as requirements elicitation, prototyping, analysis and modeling, specification, and reviews.
Significant correlations between the process factors and requirements volatility resulted from the analysis of
some of the factors. The use of particular requirements engineering process techniques showed correlations
with lower levels of requirements volatility. Other findings indicated that projects which used some types
of prototypes to elicit requirements had higher levels of requirements volatility in later phases of the
development cycle than lower levels, as one might expect. The presented results can be used by software
development managers to proactively address and possibly mitigate the risk of requirements volatility,
and to understand the potential for increased requirements volatility when certain methods are utilized.
Copyright q 2010 John Wiley & Sons, Ltd.
Received 22 April 2009; Revised 9 January 2010; Accepted 25 June 2010
KEY WORDS:

requirements volatility; requirements engineering; software risk mitigation; requirements
process; requirements risk

1. INTRODUCTION
Requirements volatility, in its various forms, surfaces as a frequent and high impact risk in numerous
empirical studies performed to identify risk factors or to understand variables leading to a project’s
success or failure (examples include [1–11]). There are multiple aliases commonly associated with
or related to the phenomenon of requirements volatility including requirements change, requirements creep, scope creep, requirements scrap, requirements instability, and requirements churn.
A simple metric for requirements volatility defines it as the number of additions, deletions, and
modifications made to the requirements set per time unit of interest (per week, month, phase,
etc.). Costello [12] provides a relatively detailed set of metrics for requirements volatility. Requirements volatility can be a metric used for understanding the stability of the requirements generation
∗ Correspondence
†

to: Susan Ferreira, The University of Texas at Arlington, Box 19017, Arlington, TX 76019, U.S.A.
E-mail: ferreira@uta.edu

Copyright q

2010 John Wiley & Sons, Ltd.

376

S. FERREIRA ET AL.

process [13]. Kulk and Verhoef [14] identify requirements volatility metrics that can be used to
help predict projects which may fail due to unhealthy levels of requirements volatility.
The past philosophy dictated that requirements should not change after the completion of the
requirements phase. However, this view is now understood to be unrealistic [15]. Kotonya and
Sommerville [16] discuss that requirements change is unavoidable and that requirement changes
can occur at multiple points during the development process.
Requirements volatility experienced during the requirements phase is expected because this is
the timeframe when requirements are being identified and documented. However, once the design
process begins, the cost to change requirements is progressively greater due to the investment in
time and effort as the project continues to generate artifacts and complete tasks. Requirements
volatility can increase the job size dramatically, cause significant rework, and affect other project
variables such as morale, and productivity [17].
A relatively small number of studies consider requirements volatility and its associated effects,
especially in a manner integrated with other software project management factors. These existing
studies primarily fall into a few major research method categories: survey or software assessmentbased research [4, 18–22], interviews and case studies [23–27], regression analysis [28], reliability
growth model [29], analytic hierarchy process analysis [30], and simulation models [3, 31–37].
Relatively little empirical research has been performed on the topic of requirements volatility
that considers the project factors and quantitative effects on requirements volatility. Multiple techniques have been discussed in the literature for mitigating the effects of requirements volatility.
Jones [18] identifies a set of techniques for minimizing the effects of creeping requirements.
These techniques include joint application design (JAD), prototyping key product aspects, quality
function deployment, formal requirements inspections, contractual penalties for late requirements,
automated configuration control tools, software estimation tools with creep prediction capabilities, and the use of function point metrics. Pfahl and Lebsanft [36] discuss optimizing the level
of requirements engineering effort in order to address requirements volatility problems at one
company. Zowghi et al. [22] found that a formal notation used for specifying requirements and the
use of a defined method to model requirements resulted in lower levels of requirements volatility.
These researchers also found results to suggest that more capable developers result in lower
levels of requirements volatility. Zowghi and Nurmuliani [21] determined that use of a defined
requirement engineering methodology, performing requirements inspections, and frequent communications between developers and customers reduce requirements volatility. They also found that
an increase in the number of user representatives involved in the development team correlates with
an increase in requirements volatility.
This paper concentrates on various process factors and their contributing relationship to requirements volatility. The paper presents results, including statistically significant relationships that show
volatility differences based on the process characteristics or activities performed on the project.
Empirical survey findings show relationships between various factors, such as process maturity
and requirements engineering process techniques, and levels of requirements volatility experienced
on software development projects.
The paper is organized as follows. Information about the survey objectives, the survey instrument,
and a characterization of the respondents, organization, and projects is briefly discussed. Survey
findings related to factors and their associated relationship to requirements volatility are then
presented. Finally, the paper summarizes findings and presents conclusions from the survey analysis
and discusses the potential future work.

2. SURVEY INTRODUCTION
2.1. Survey objectives and process
A survey of over 300 software development personnel was developed to examine the effects
of requirements volatility and associated factors on software development projects. One of the
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

REDUCING THE RISK OF REQUIREMENTS VOLATILITY

377

objectives of this survey effort was to understand the current state of requirements engineering
processes performed on projects and to determine whether relationships exist between various
project factors, including levels of requirements volatility seen in the course of the project. To
achieve this goal, the survey collected important information about the respondents’ requirements
engineering processes and related project factors. This collected information sheds light on an area
where there is currently very little data, especially quantitative data, published on requirements
engineering methods that are used on actual software development projects.
The Project Management Institute Information Systems Specific Interest Group (PMI-ISSIG)
sponsored the survey by providing the host site for the web-based survey and sending notifications
about the survey to its members. A total of 312 software project managers and other software
development personnel submitted responses for the survey.
PMI-ISSIG was the primary target population for the survey. However, in addition to the
mailings sent to PMI-ISSIG members, one mailing was sent to individual Software Engineering
Institute (SEI) Software Process Improvement Network (SPIN) group contacts within the United
States and to individual professional contacts. These contacts were encouraged to forward the
survey notice to others who would fit the respondent profile and might be interested in responding.
Individuals were asked to respond to the survey with a recently completed project in order to
reduce problems in memory recall or with the level of response accuracy that might exist with an
older project. The survey targeted software development personnel, especially project managers,
leads, and consultants who would know a project’s overall parameters and outcomes. Additional
information about the survey instrument is found in Ferreira [17].
The survey solicited information about the project and organization characteristics. The survey
included questions to solicit project and product background data, and change request and requirements volatility data as well as important project factors. For example, respondents were asked
to report their Software Engineering Institute (SEI) Capability Maturity Model (CMM) process
maturity level [38]. The survey collected information about the project’s levels of requirements
volatility. Respondents were also asked to provide the process techniques used for requirements
engineering activities. Requirements engineering refers to the processes used to generate and
maintain the requirements through the product’s life cycle and includes eliciting, negotiating,
analyzing/modeling, specifying, validating, and managing the set of requirements. Respondents
reported the methods they used for the following requirements engineering activities:
•
•
•
•
•
•

Elicitation/identification
Prototyping
Analysis and modeling
Specification
Reviews/inspection
Requirements change management

The respondent was asked to pick from predefined techniques for each requirements engineering
activity. In the majority of cases, the respondent was also offered the option to include a free
form response if they used a technique that was not already listed in the survey question. Where
appropriate, the respondent could select a combination of multiple responses for the question if they
used more than one technique for each type of requirements engineering activity. The techniques
for each activity were individually studied and then analyzed for their relationship to requirements
volatility.
2.2. Data checks
All respondents were asked to provide their name and either an e-mail address or phone number
to validate their responses. Respondents indicated their level of confidence in the qualitative
and quantitative inputs that they supplied on the survey by selecting from a set of confidence
levels (extremely confident, very confident, confident, somewhat confident, and not confident at
all) for both qualitative and quantitative inputs. This information was used in the analysis to
filter out responses. Where respondents selected the ‘not confident at all’ response, their survey
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

378

S. FERREIRA ET AL.

input was removed from further analysis. Four respondents submitted this response for their
quantitative confidence. Qualitative responses were all at levels above ‘not confident at all’. The
results indicate that respondents were more confident about their qualitative answers than their
quantitative responses.
The quantity of respondents is listed in the figures related to the survey. Respondents may
not have answered all questions. They may have missed or elected to skip particular questions.
Responses were removed from analysis if there was a conflict in their response with another survey
question. This conflict might occur in the case of conditional questions where one question is
dependent on another question.
2.3. Threats to validity
Internal validity considers whether the design of the study allows the ability to draw valid conclusions [39]. Researchers took multiple measures to minimize threats to validity. The study used
a short timeframe (6 weeks) during which the survey would be answered in order to reduce the
risk that external events would influence the study. The study endeavored to reduce drop-out rates
by describing the intent of the survey to participants and offering incentives for their response.
The incentives were drawn after the completion of the survey in order to encourage respondent
participation. Multiple submissions from the same participant and the risk of fake respondents
was controlled by asking participants to provide their PMI identifier, e-mail address, name, and
phone number. The results of the study are believed to provide an acceptable level of internal
validity as the variables included in the research were based on an extensive literature review and
the questions were evaluated using reviews and field tests.
The assessment of internal validity considers the limitations to the study. One factor which
influences internal validity is non-response error. A significant number of respondents did not
respond to the questionnaire. There may have been multiple reasons for this including that potential
respondents did not fit the survey’s desired respondent profile, respondents did not have sufficient
knowledge about a project, and detailed information might not be readily available. The quantity
of questions asked and the amount of time required to respond may also have been a deterrent to
potential respondents. The length of the survey may have led to reduce the quantity of respondents
below what it might have been with a shorter and more familiar subject area for respondents. A
related limitation is mortality during the survey. This is where respondents started the survey but
did not complete all the questions. It is possible that this situation may have occurred due to the
length of the survey.
External validity assesses whether the survey results can be generalized to other populations
beyond the sampled group. Survey respondents elected to participate in the survey and were not
chosen randomly. It is unknown if the responding set is representative of the entire set of potential
respondents. The study cannot be generalized to describe the impacts and effects related to requirements volatility for the entire software development process community since respondents were not
chosen randomly. Respondents provided information about completed projects. A problem may be
inaccurate recall of information. Responses may have been provided based on individual perception and not on an accurate account of the information. Another consideration is that the research
effort considered only finished projects. The percentage of projects that experienced requirements
volatility and the resultant effects may be different for projects that were not completed. All of
these issues may threaten the external validity of the study results.
3. SURVEY FINDINGS ON RELATIONSHIPS BETWEEN PROCESS FACTORS AND
REQUIREMENTS VOLATILITY
3.1. Respondent and project overview
A total of 87% of the survey respondents were project managers or leaders for their projects.
While the majority of the respondents held one role on the project, approximately 39% of the
participants held more than one role on the project. The 312 respondents averaged 9 years as an
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

REDUCING THE RISK OF REQUIREMENTS VOLATILITY

379

Figure 1. Software organization size.

Figure 2. Product types.

individual contributor (e.g., developer, analyst), 7 years as a project manager or leader, and 2 years
of other experience. These roles may have overlapped based on the quantity of roles respondents
had on various projects. The respondent’s software work organization ranged in employee quantity
from small to very large. Figure 1 shows the range of organization employee quantity in different
size categories. A total of 34% of respondents are from a software organization larger than 999
employees. The software product types in the survey represent a wide range of possibilities
(Figure 2). The largest group, ‘other’ comprised many areas that could not be adequately represented
in the figure.
29% of the projects used a waterfall development cycle. While the waterfall represented the
largest category, two variants of the incremental life cycle combined together represented 40%
of the life cycles used in the projects. The two variants of the incremental life cycle include (1)
requirements developed in each increment (23% of projects) and (2) requirements generated prior
to all increment development (17% of projects).
3.2. Data analysis
The researchers analyzed the relationships between the key study factors. Here, the analysis focus
was primarily on process maturity and requirements engineering-related processes, where only
selected factors were analyzed. Factors were chosen specifically to answer key questions about
processes used during the project and their relationship to project outcomes. The analysis used a
measure of the level of requirements volatility as a project outcome.
3.2.1. Statistical methods. Analysis of variance is used to analyze survey factor relationships with
multiple groups. Where factor relationships are significant, the p-value (significance) is presented
along with the transformation used and key statistics. The t-test is used when comparing two
means. When the t-test is used, the p-value, t-statistic, degrees of freedom, and the dependent
variable transformation are presented. Factor relationship results are identified as significant using
an  = 0.05 level (alpha level) except where noted otherwise. Where the relationships are not
significant, only the p-value and transformation used on the dependent variable are presented.
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

380

S. FERREIRA ET AL.

In one case, a post hoc test, the Tukey test, was used to determine which factor means were
statistically different. Nonparametric tests are used in instances where normality of the data could
not be assumed or the data violate the assumptions of equality of variance. The paper notes the
specific test used when appropriate.
Where users entered a freeform quantity versus selecting from preset survey values, the researcher
removed extreme outliers [40] from the data. Outliers were identified by using quartile calculations,
normally used for box plot generation. Extreme outliers were examined to assess their distance
from neighboring points and removed when appropriate. Analyses were performed both with
and without outliers. In some cases, the regressions were run with and without outliers so that
the results could be compared and outliers could then be checked for their degree of influence
where necessary. In the cases discussed in this paper, the analysis without outliers is presented
primarily due to the better transformation fit of the data. In many of the net requirements volatility
analyses, with the outliers included, the analysis only marginally passed the normality assumption
tests. When extreme outliers were removed from the data set no issues occurred with normality
assumption violations.
3.2.2. Model diagnostics. The regression error term was checked in all linear regression cases to
determine whether there were violations of regression assumptions. Constancy of error variance
was assessed using a scatter plot of the predicted dependent variable against the standardized
residual. A normal probability plot of the standardized residuals, where each residual was plotted
against its expected value under normality, was used to determine the normality of the error term.
Levene’s test was used to check the equality of variances. Independence between terms was not
checked as each survey response was independent of the others given the nature of the survey.
In all linear regression analysis cases discussed in the rest of the paper, the error term was found
to violate regression assumptions. Transformations of the dependent variable were performed to
correct severe violations of error term normality or the lack of constancy of error variance across
factor levels. In almost all cases, running an appropriate transformation on the dependent variable corrected the regression assumption violations, making the regression model appropriate for
the transformed data. When a transformation could not be found to address regression assumption violations, the data results are not presented. A natural log transformation was used for all
transformations discussed in this paper.
3.3. Requirements volatility
78% of the survey respondents reported that they experienced requirements volatility on their
project. The survey asked respondents to identify how much requirements change, specifically,
additions, deletions, and modifications to the requirements, affected the job size as a percent of the
original job size estimated prior to design. This net effect was used as a measure of requirements
volatility in the analyses performed for the survey study. Net requirements volatility job size
change is identified as the dependent variable in the analyses presented in this paper. The survey
data showed that surveyed projects had an average of 32.4% requirements volatility-related job
size increases. More details on the survey findings showing primary and secondary effects of
requirements volatility are addressed in Ferreira [17].
3.4. Process relationships to requirements volatility
3.4.1. Process maturity level. The SEI’s five successive level CMM rating scale was used by
respondents to identify their organization’s software process maturity. The largest group, 45%,
of projects reported that they were at SEI CMM level 2, repeatable. Figure 3 illustrates the
representation of projects at the other SEI CMM levels.
The relationship between maturity level and net requirements volatility (RV) job size change
was evaluated using a univariate analysis of variance. Statistically significant ( p-value of 0.010,
F = 3.385) differences were found to exist between the volatility values for at least two of the
SEI CMM levels. The analysis used a natural log transformation on the dependent variable, net
requirements volatility job size change. As there were significant differences between levels (e.g.,
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

REDUCING THE RISK OF REQUIREMENTS VOLATILITY

381

Figure 3. Software process maturity level.

Table I. Process maturity descriptive statistics.
Maturity
1
2
3
4
5

N

Mean

36
102
62
11
8

32.278
27.325
15.848
15.273
9.000

Figure 4. Net requirements volatility job size change versus process maturity level.

the different maturity levels) within the factor, a post hoc test was performed to simultaneously test
the means between the SEI CMM levels. Levels 1 and 3 ( p-value of 0.045) and 2 and 3 ( p-value
of 0.033) appeared significantly different in the Tukey means comparison tests, under assumed
equal variances across levels. Levels 4 and 5 had fewer observations, which may not have allowed
for identifying statistical differences between the means for these two groups.
The untransformed means for requirements volatility job size change related to each maturity
level and observation quantities are provided in Table I. Figure 4 shows the plotted means for the
untransformed data. One can clearly see the reduced level of volatility in levels 1 through 5 by
looking at the mean values of the data in Table I and the trend in reduced requirements volatility
as the maturity level increases in both Table I and Figure 4. Based on the results, higher maturity
levels appear to show a relationship with decreases in requirements volatility job size change.
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

382

S. FERREIRA ET AL.

Figure 5. Requirements elicitation techniques.
Table II. Requirements elicitation/identification techniques.
Independent variable

p-value

Reuse of existing requirements
Ethnographic techniques
Introspective techniques
JAD
Storyboarding
Prototyping
Interviews
Questionnaires

0.018∗
0.017∗
0.535
0.988
0.642
0.013∗
0.609
0.381

Transformation
Natural
Natural
Natural
Natural
Natural
Natural
Natural
Natural

log
log
log
log
log
log
log
log

t
2.374
−2.400
—
—
—
−2.506
—
—

∗ Significant results.

This volatility difference between maturity levels is certainly feasible. Increased maturity levels
correspond with increased capability for rigor in processes and the use of quantitative data to
control processes. As projects move up the maturity scale, theoretically they have better control
over the processes used to develop the product, including requirements definition and management
techniques. Also, they are more apt to collect and use metrics to assess their levels of requirements
volatility and determine if problems exist.
3.4.2. Requirements engineering process relationships. The subsequent sections report the survey
data for the techniques used for various requirements engineering activities and consider the
relationships between these techniques and net requirements volatility job size change.
3.4.2.1. Elicitation/identification. Respondents were asked to identify the techniques used
to elicit/identify the software requirements in their projects. They were provided with a list
of 9 techniques, including ‘other’, where they could specify a freeform response. Figure 5
presents the usage of the various elicitation techniques. One should be aware that respondents
could select multiple types of techniques to elicit requirements. Interviews, with 66.3% of
respondents using this method, were the most popular requirements elicitation/identification
technique. The second most popular technique was prototyping with 61.6% of respondents
identifying the use of this technique. 54.4% of the survey population reused requirements in their
project.
The requirements elicitation/identification techniques were analyzed individually to determine
if they had any relationship with the net requirements volatility job size change. Three of the eight
techniques (analysis did not include ‘other’) have a statistically significant relationship with the
net requirements volatility job size change amount. These methods include (1) reuse of existing
requirements, (2) ethnographic techniques, and (3) prototyping. Table II provides the analysis
details with the results for each type of technique. Additional data are provided for the significant
results in subsequent sections.
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

383

REDUCING THE RISK OF REQUIREMENTS VOLATILITY

Table III. Reuse technique descriptive statistics for untransformed data.
Variable level
No reuse
Reuse
Total

Mean

N

28.672
19.701
23.647

95
121
216

Table IV. Reuse technique independent samples test for transformed data.
t
2.374

df

Sig.

Mean diff.

Std error diff.

214

0.018

0.1114

0.0469

Table V. Ethnographic techniques descriptive statistics for untransformed data.
Variable level

Mean

N

Did not use ethnographic techniques
Used ethnographic techniques
Total

20.368
27.244
23.647

113
103
216

Table VI. Ethnographic techniques independent samples test for transformed data.
t
−2.400

df

Sig.

Mean diff.

Std error diff.

214

0.017

−0.1118

0.0466

3.4.2.1.1. Reuse of existing requirements. Table III provides the untransformed means and
number of respondents in each of the two groups for the evaluation of requirements reuse. One group
did not reuse already documented requirements whereas the other group did reuse requirements.
Reusing requirements shows a correlation with reduced levels of net requirements volatility. Table
IV provides the independent samples test analysis statistics for the transformed data.
If a project is similar to another project, reusing requirements may reduce volatility for a number
of reasons. Many requirements related concerns and issues may have been already been addressed
or uncovered by the previous project in time for the requirements reuse on the successive project.
Owing to reuse, many of the requirements errors may have been removed in the course of the
previous project. A number of creative iterations may have been incorporated into the previous
set of requirements that would not have been considered until later if the project had not reused
requirements.
3.4.2.1.2. Ethnographic techniques. Table V shows the untransformed means and the number
of respondents in each of the two groups for the ethnographic technique analysis. One group did
not use ethnographic techniques whereas the other group did. Ethnographic techniques showed a
correlation with increased levels of volatility.
Why this might be the case is unknown. However, there is some speculation as to why volatility
may increase. Ethnographic techniques involve immersion in the customer domain or observance of
the customer’s environment. Perhaps, as the requirements engineer learns more about the domain,
assuming the domain is not already known, more insight is gained as to the real requirements
that may have been previously vague or perceived to be unimportant to the customer or business.
Another possible reason may be that the use of the technique correlates with projects that already
have a pre-disposition to high levels of volatility. Table VI provides the independent samples test
analysis statistics for the transformed data.
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

384

S. FERREIRA ET AL.

Table VII. Prototype techniques descriptive statistics for untransformed data.
Variable level
Did not use prototypes
Used prototypes
Total

Mean

N

18.433
26.714
23.647

80
136
216

Table VIII. Prototype techniques independent samples test for transformed data.
t
−2.506

df

Sig.

Mean diff.

Std error diff.

214

0.013

−0.1207

0.0482

Figure 6. Requirements elicitation prototype methods.

3.4.2.1.3. Prototyping techniques. Table VII provides the untransformed means and the number
of respondents in each of the two groups for prototype use analysis. One group did not use prototype
techniques whereas the other group did. The group that used prototypes to identify requirements
showed a relationship with increased levels of volatility.
What is interesting is that prototypes are one of the techniques recommended to reduce the
effects of volatility. Perhaps a project using prototypes already has the tendency to have increased
levels of volatility due to the project’s nature. While this rationale is purely speculative at this
point, Kulk and Verhoef [14] discuss a set of results where the development organization indicated
that the development method was ‘geared towards dealing with volatility’ and where the projects
experienced high requirements volatility. Table VIII provides the independent samples test analysis
statistics for the transformed data.
3.4.2.1.4. Requirements elicitation/identification prototypes. If respondents reported that they
used a prototype to elicit requirements, the respondents were asked to specify what type of prototype
they had used. Choices included evolutionary, throw away, and user interface prototypes. 66.5%
of the respondents that reported their project used a prototype cited that they used a user interface
prototype. 24.8% of respondents utilized more than one type of prototype. Figure 6 presents the
distribution of respondents for each type of prototype.
The different variants of prototypes were analyzed individually to determine whether they had
any relationship with the net requirements volatility. Table IX shows the analysis statistics for
the prototype methods. Both the evolutionary and user interface prototypes showed statistically
significant results.
Table X provides the untransformed means and the number of respondents in each of the two
groups for evolutionary prototype use analysis. One group did not use evolutionary prototypes
whereas the other group did. Evolutionary prototypes used to identify requirements showed a
relationship with increased levels of volatility. Table XI provides the independent samples test
analysis statistics for the transformed data.
Table XII provides the untransformed means and the number of respondents in each of the two
groups used for user interface prototype analysis. One group did not use the user interface prototype
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

385

REDUCING THE RISK OF REQUIREMENTS VOLATILITY

Table IX. Requirements elicitation/identification prototype techniques.
Independent variable

p-value

Transformation

t

Throw away prototype
Evolutionary prototype
User interface prototype

0.503
0.041*
0.028*

Natural log
Natural log
Natural log

—
−2.061
−2.209

∗ Significant results.

Table X. Evolutionary prototype descriptive statistics for untransformed data.
Variable level
Did not use evolutionary prototype
Used evolutionary prototype
Total

Mean

N

20.786
29.414
23.702

143
73
216

Table XI. Evolutionary prototype independent samples test for transformed data.
t
−2.061

df

Sig.

Mean diff.

Std error diff.

214

0.041

−0.1016

0.0493

Table XII. User interface prototype descriptive statistics for untransformed data.
Variable level
Did not use user interface prototype
Used user interface prototype
Total

Mean

N

20.652
27.813
23.702

124
92
216

Table XIII. User interface prototype independent samples test for transformed data.
t
−2.209

df

Sig.

Mean diff.

Std error diff.

214

0.028

−0.1041

0.0471

whereas the other group did. User interface prototypes used to identify requirements showed a
relationship with increased levels of volatility. Table XIII provides the independent samples test
analysis statistics for the transformed data.
As discussed earlier, the increased volatility seen with the use of the evolutionary and user
interface prototypes might be related to projects that are already predisposed to a higher level
of volatility. The results are very interesting in the case of prototype use because, in the literature, prototypes are identified as a method to mitigate the effects of volatility and assist in the
identification of requirements.
3.4.2.2. Analysis and modeling. Respondents were asked to identify the techniques used to
analyze and/or model the software requirements on their projects. They were provided with a
list of 4 selectable options, including ‘other’, where they could specify a freeform response and
‘no analysis or modeling was performed’. 73% of respondents stated that they used some type of
requirements analysis and/or modeling technique. Figure 7 presents a summary of the requirements
analysis and modeling techniques used by respondents. Structured analysis and design was the
most popular technique with 34% of respondents using this analysis method.
The three standard responses for analysis/modeling methods (object oriented, structured analysis
and design, and no analysis or modeling used) were combined in custom regression models
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

386

S. FERREIRA ET AL.

Figure 7. Requirements analysis and modeling techniques.

Table XIV. Requirements analysis/modeling technique.
Independent variable
Object oriented analysis
Structured analysis and design
No analysis or modeling used

p-value

Transformation

t

0.931
0.156
—

Natural log
Natural log
Natural log∗

—
—
—

∗ Levene test significant at p = 0.032 indicated cannot assume equality of error variances; therefore

additional data is not shown. The Levene tests for the use of object oriented analysis and
structured analysis were not significant.

Table XV. Requirements analysis/modeling use descriptive statistics for untransformed data.
Variable level

Mean

N

Used analysis/modeling method(s)
Did not use an analysis/modeling method
Total

21.951
31.779
24.373

159
52
211

that considered main effects, and a single two-way interactions. Obviously, the ‘no analysis or
modeling used’ response does not occur in conjunction with either of the other two responses.
The overall models had no significant independent variable relationships with net requirements
volatility. The factors were also tested independently, where they were not part of an overall model
that included all the analysis methods as defined above. Table XIV shows the analysis statistics for
the significant variable. The selection of ‘no analysis or modeling used’ was mildly significant ( pvalue = 0.062). However, the test failed the Levene test for equality of variances ( p-value = 0.032).
Thus, a test was run with the nonparametric Mann–Whitney test to determine whether there was
a difference between projects that did perform analysis or modeling versus those that did not.
This nonparametric test does not require a check for normally distributed data or homogeneity of
variances. Table XV lists the untransformed means and the number of respondents in each of the
two groups. One group used an analysis or modeling technique whereas the other group did not.
The Mann–Whitney test estimates (U = 3439.500, p-value = 0.068) for the untransformed data
show that projects that used a requirements analysis or modeling method (median = 15.0) showed
a mildly significant reduction in requirements volatility as compared to those that did not use a
requirements analysis or modeling method (median = 22.5). The significant results had a small
effect size of r = −0.125.
The results indicate that those that did not use an analysis or modeling method showed a
correlation with a higher level of requirements volatility measured by job size change than those
that used an analysis or modeling method. This result makes sense in that analysis or modeling
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

REDUCING THE RISK OF REQUIREMENTS VOLATILITY

387

Figure 8. Requirements specification methods.

provides a more thorough review of the requirements and a different perspective or view that can
assist in identifying additional requirements or uncovering requirements errors.
3.4.2.3 Requirements specification. Respondents were asked to identify the techniques used to
document the software requirements on their projects. They were provided with a list of 4 selectable
options, including ‘other’, where they could specify a freeform response. Figure 8 presents the
combinations of the specification methods used by respondents. Use of natural language (for
example, ‘shall’, ‘should’, or other statements) was by far the largest method cited for specifying
requirements with 43% of respondents using the method. Alternative combined methods refer to
all other combinations not broken out separately in Figure 8.
The three standard responses for requirements specification methods (natural language, semiformal, and formal) were combined in a custom model that considered main effects, and two-way
interactions. There were inadequate three-way interactions to include this combination. The overall
model had no significant independent variables. The factors were also tested independently, where
they were not part of an overall model that included all the specification methods as defined above.
The specification techniques did not show statistically significant differences in their use.
3.4.2.4 Requirements specification review methods. Respondents were asked to identify the
type of reviews or inspections held for software requirements specifications. They were provided
with a list of 5 selectable options (ad hoc walkthrough, structured review, formal inspection, no
reviews or inspections, and other where they could specify a freeform response). Figure 9 shows
that structured reviews are the most commonly used requirements review techniques, with 46% of
projects using them. Ad hoc walkthroughs are a close second, with 41% of projects citing their use.
8% of projects identified that they had no requirements reviews or inspections on their projects.
Those who responded with ‘other’ were requested to specify what method they used in freeform
text. The freeform text response indicated various combinations of the other methods.
The untransformed means and the number of observations per group are provided in Table XVI.
Analysis of variance was initially used to determine whether relationships existed between requirements specification review methods and net requirements volatility. However, the test failed the
Levene test for equality of error variances ( p-value = 0.014). Therefore, the nonparametric Kruskal–
Wallis test was used to assess whether statistically significant differences between methods exist.
The Kruskal–Wallis test showed a mildly significant result ( p-value = 0.086). Individual pairs of
methods were tested using the Mann–Whitney test. The ‘other’ option was not included in tests
because results indicate the use of various combinations of other methods. The Mann–Whitney
test showed statistically significant differences (U = 3697.500, p-value = 0.031) between ad hoc
walkthroughs (median = 20.0) and structured reviews (median=15.0). The significant results show
a small effect of r = −0.156.
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

388

S. FERREIRA ET AL.

Figure 9. Requirements specification review methods.
Table XVI. Requirements specification review method descriptive statistics.
Review method
Ad hoc walkthrough
Structured review
Formal inspection
No reviews or inspections
Total

N

Untransformed mean

86
105
7
13
211

31.186
18.821
10.000
20.077
23.646

A published analysis by Zowghi and Nurmuliani [21] found that performing requirements
inspections appear to reduce requirements volatility. The results of the Mann–Whitney test from the
survey results are interesting because more stringent reviews such as formal inspections may have
shown a lower level of volatility. However, the formal inspections and no reviews or inspections
groups had significantly smaller numbers of observations than the other two groups. This may have
caused the test to be unable to adequately distinguish between the means given the significantly
smaller number of observations. As can be seen by the results in Table XVI, those projects that
use structured reviews show a lower level of requirements volatility than those that use ad hoc
reviews. This result makes sense because of the increased rigor of structured reviews in contrast
to the open-ended processes that might exist with ad hoc reviews.
3.4.2.5. Requirements change management. 70% of respondents stated that they performed
some type of requirements change management. The data showed no statistically significant relationship between requirements change management and net requirements volatility ( p-value = 0.426,
natural log transformation on net requirements volatility). However, it is commonly believed that
using a requirements change management process is very important. The lack of significance between
requirements change management and net requirements volatility may be due to other reasons.

4. SUMMARY AND CONCLUSIONS
Requirements volatility has major effects on factors that directly influence a project’s cost and
schedule. Increases in the project job size and rework lead to increases in the amount of effort
required to complete the project. These increases in effort cause the cost of the project to rise
if additional resources need to be added to accommodate the extra workload. In addition to the
schedule hit, if the project duration lengthens due to the additional work, costs, in general, increase
due to the cost of other non-development personnel resources (computers, office space, overhead,
etc.) that must be used to complete the project tasks. If the schedule is not changed and/or no
more resources are added to address the additional workload, schedule pressure also rises. Another
consideration with the increased job size is the complexity of the task. As the project job size
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

REDUCING THE RISK OF REQUIREMENTS VOLATILITY

389

increases, the complexity of the task is also likely to increase [41]. This increase in complexity
also directly affects the amount of effort and duration required to complete the project.
Based on these problematic effects of volatility, a project manager should be interested in
proactively reducing requirements volatility in later stages of development, unless the customer’s
satisfaction is affected by not making changes or the customer is willing to accept increases in cost
and schedule. Past philosophy dictated that requirements had to be firm in the completion of the
requirements phase and that they should not change after this time. This view is now understood
to be unrealistic [15]. Requirements indeed change. However, this should not be used as an excuse
to allow a poor requirements engineering job to be performed at the inception of a project. Requirements volatility is not necessarily caused by exercising poor requirements engineering processes
or the lack of appropriate change management but may exist for other reasons outside the project
manager’s control [16]. In these cases, changes must be made and project managers need to effectively
balance out important project factors to achieve success in the project and for the organization.
Analysis of the survey data provided some surprising results. Ethnographic techniques and
use of evolutionary and user interface prototypes during requirements identification showed a
relationship with increased levels of requirements volatility, as measured by job size change, later
in development, once design started. There was some speculation as to why this would be the
case. Prototyping key features of the product is normally recommended as a method to mitigate
requirements creep. Possibly, the projects that used ethnographic techniques and evolutionary or
user interface prototypes may have already had a pre-disposition to requirements volatility. The use
of prototypes may have already addressed more serious levels of volatility. Perhaps the prototype
use did not provide sufficient rigor to identify requirements. More research needs to be done to
understand the possible reasons for why these particular relationships exist.
The survey results and other research show that one may be able to adopt a proactive approach to
addressing some types of requirements volatility and avoid some of the more serious consequences
by understanding the effects of volatility and the dynamics of the project factors involved. Proactive
mitigation approaches and findings identified by the survey recommend the following:
• Move up the process maturity level ladder. Higher maturity levels showed statistically significant levels of reduced requirements volatility effects on job size changes due to requirements
volatility. The higher process maturity levels incorporate additional processes and rigor that
may proactively reduce the potential for requirements volatility or assist in requirements
volatility control. Activities in key process areas (for example, requirements management,
configuration management, and peer reviews) may assist in addressing or controlling some
levels of volatility.
• Reuse a quality set of pre-existing requirements. Reusing requirements showed a relationship
with reduced levels of requirements volatility. It almost goes without saying that projects
that want to reuse requirements later need to have generated a solid initial set that has the
potential for future reuse. Many times project teams may not want to invest the time and
effort to generate a quality set of requirements knowing that it will not benefit them in the
short term. This is where the organization’s management needs to recognize the possible
longer term benefits of reuse, knowing that producing quality requirements in the current
project may save significant money and time on the current project and reap benefits on many
later projects. Long-term enterprise strategy should play a part in determining the potential
for reuse and when to generate the products that can be reused. Documenting requirements
patterns can allow for the reuse of requirements knowledge on subsequent projects [42].
Requirements patterns can be used to facilitate reuse of the existing requirements knowledge
and can provide a template that can be used as a starting point for new projects [43], thus
saving time and money.
• Use a requirements analysis or modeling technique. There were significant statistical differences for those that used an analysis or modeling techniques against those that did not. As
analysis or modeling involves generating different perspectives of requirements, in this way
generating a more complete picture of the requirements, it makes sense that fewer problems
or gaps might exist, reducing later requirements volatility.
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

390

S. FERREIRA ET AL.

• Utilize more structured reviews or formal inspections for requirements specifications. Although
the survey results did not point to a statistically significant difference between structured
reviews and formal walkthroughs (possibly because of the limited number of respondents using
the technique), the average requirements volatility for those projects using formal inspections
was the lowest of all methods surveyed. Structured reviews had the second lowest mean
of all techniques. The survey analysis showed a statistically significant difference between
structured reviews and ad hoc walkthroughs where structured reviews showed a lower level
of requirements volatility. Structured reviews or formal inspections would be recommended
to mitigate the risk of requirements volatility based on the survey results.
In the authors’ personal experience, the importance of expending appropriate effort and producing
quality products in the requirements engineering activities is not sufficiently recognized in the
software industry. More focus needs to be placed on the value of requirements engineering; on
developing a good set of requirements and understanding what this entails. A better balance needs to
be achieved between requirements engineering activities, not always a prestigious activity, and later
development activities. Requirements engineering skills, which are not the same as the set of skills
for developing code, should be a prerequisite for performing requirements engineering activities.
Unfortunately, software companies do not always acknowledge or understand that problems
with requirements will have serious downstream consequences if the wrong product is produced
or key undocumented product portions are not built. What happens if the group attempts to later
reuse or has to maintain requirements and finds serious gaps in the requirements or major errors? The
benefits of good proactive processes and allowing sufficient effort in the requirements process almost
speak for themselves in the potential to avoid requirements volatility and its more severe penalties.

5. FUTURE WORK
There is much work that can be done with the existing survey data to analyze relationships
between project factors including those factors that may show a relationship with affecting levels
of requirements volatility. Only a subset of the projects factors were analyzed for their influence on
requirements volatility. In addition to the survey factors, there are undoubtedly still other factors
to be discovered or analyzed that may play a role in requirements volatility.
Requirements engineering work involves significant communication and other skills beyond those
required for other software engineering activities. Can more work be done to understand these skills and
apply this knowledge in the industry so that better requirements can result? Are there other techniques
that might help in efficiently producing a quality set of requirements that may also help in reducing
volatility in later phases? Many questions remain to be answered in requirements engineering.
Surprising results from the survey indicated that the use of certain requirements engineering
techniques showed a relationship with higher levels of requirements volatility versus lower levels,
as was initially expected. Further research is required to understand why this is the case. Do projects
that use these techniques already have a predisposition to higher levels of requirements volatility?
It would be interesting to understand if other techniques also share the increased relationship to
requirements volatility. Additional volatility effects can also be reviewed for their relationship
with techniques. For example, levels of rework were not analyzed for their relationship with
requirements engineering techniques. Additional requirements volatility metrics may provide a
more balanced picture.
ACKNOWLEDGEMENTS

The authors thank the Program Management Institute for their support in hosting the survey and sending
notifications of the survey to their membership. In particular, we thank Kent Hamblen, Dave Karpinsky,
Tresia Schumacher, and Lance Schumacher for all their efforts. They also thank the respondents who
participated and responded to the survey. The work presented in this paper would not exist without their
help. The authors also thank the editor and the three anonymous reviewers who provided valuable feedback
and recommendations in key areas that allowed us to improve the quality of this paper.
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

REDUCING THE RISK OF REQUIREMENTS VOLATILITY

391

REFERENCES
1. Boehm BW. Software risk management: Principles and practices. IEEE Software 1991; 8(1):32–41.
2. Curtis B, Krasner H, Iscoe N. A field study of the software design process for large systems. Communications
of the ACM 1988; 31(11):1268–1287.
3. Houston DX. A software project simulation model for risk management. PhD Dissertation, Arizona State
University, 2000.
4. Jones C. Assessment and Control of Software Risks. PTR Prentice-Hall: Englewood Cliffs, NJ, 1994.
5. Känsälä K. Integrating risk assessment with cost estimation. IEEE Software 1997; 14(3):61–67.
6. Moynihan T. How experienced project managers assess risk. IEEE Software 1997; 14(3):35–41.
7. Ropponen J. Risk assessment and management practices in software development. Beyond the IT Productivity
Paradox. ch. 8. Wiley: New York, 1999; 247–266.
8. Ropponen J, Lyytinen K. Components of software development risk: How to address them? A project manager
survey. IEEE Transactions on Software Engineering 2000; 26(2):98–112.
9. Schmidt R, Lyytinen K, Keil M, Culle P. Identifying software project risks: An international Delphi study. Journal
of Management Information Systems 2001; 17(4):5–36.
10. The Standish Group. The Chaos Report 1995.
11. Tirwana A, Keil M. Functionality risk in information systems development: an empirical investigation. IEEE
Transactions on Engineering Management 2006; 53(3):412–425.
12. Costello RJ. Metrics for requirements engineering. Master of Science Thesis, California State University, Long
Beach. 1994.
13. Hyatt L, Rosenberg L. A software quality model and metrics for identifying project risks and assessing software
quality 1996. Available at: http://sato.gsfc.nasa.gov/support/STC APR96/qualtiy/stc qual.html [3 August 2008].
14. Kulk GP, Verhoef C. Quantifying requirements volatility effects. Science of Computer Programming 2008;
72(3):136–175.
15. Reifer DJ. Requirements management: The search for nirvana. IEEE Software 2000; 17(3):45–47.
16. Kotonya G, Sommerville I. Requirements Engineering: Processes and Techniques. Wiley: New York, 1998.
17. Ferreira S. Measuring the effects of requirements volatility on software development projects. PhD Dissertation,
Arizona State University, 2002.
18. Jones C. Estimating Software Costs. McGraw-Hill: New York, 1998.
19. Lane MS. Enhancing software development productivity in Australian firms. Proceedings of the 9th Australasian
Conference on Information Systems (ACIS ’98), vol. 1, 29 September–2 October 1998, Sydney, Australia, 1998;
337–349.
20. Nidumolu SR. Standardization, requirements uncertainty and software project performance. Information and
Management 1996 31(3):135–150.
21. Zowghi D, Nurmuliani. A study of the impact of requirements volatility on software project performance.
Proceedings of the Ninth Asia-Pacific Software Engineering Conference (ASPEC ’02), Gold Coast, Australia,
2002; 3–11.
22. Zowghi D, Offen R, Nurmuliani, The impact of requirements volatility on the software development lifecycle.
Proceedings of the International Conference on Software Theory and Practice (IFIP World Computer Congress),
Beijing, China, 2000; 19–27.
23. Javed T, Maqsood M, Durrani QS. A study to investigate the impact of requirements instability on software
defects. ACM SIGSOFT Software Engineering Notes 2004; 29(3):7.
24. Loconsole A, Börstler J. An industrial case study on requirements volatility measures. Proceedings of the 12th
Asia-Pacific Software Engineering Conference (APSEC ’05), Taipei, Taiwan, 2005; 8.
25. Loconsole A, Börstler J. Are size measures better than expert judgment? An industrial case study on requirements
volatility. Proceedings of the 14th Asia-Pacific Software Engineering Conference (APSEC ’07), Nagoya, Aichi,
Japan, 2007; 238–245.
26. Nurmuliani N, Zowghi D, Fowell S. Analysis of requirements volatility during software development life cycle.
Proceedings of the 2004 Australian Software Engineering Conference (ASWEC ’04), Melbourne, Australia, 2004;
28–37.
27. Zowghi D, Nurmuliani. Investigating requirements volatility during software development: research in progress.
Proceeding of the 3rd Australian Conference on Requirements Engineering (ACRE98) Geelong, Australia, 1998;
38–48.
28. Stark GE, Oman P, Skillicorn A, Ameele A. An examination of the effects of requirements changes on software
maintenance releases. Journal of Software Maintenance: Research and Practice 1999; 11(5):293–309.
29. Malaiya YK, Denton J. Requirements volatility and defect density. Proceedings of the 10th International
Symposium on Software Reliability Engineering, Boca Raton, FL, U.S.A., 1999; 285–294.
30. Finnie GR, Witting GE, Petkov DI. Prioritizing software development productivity factors using the analytic
hierarchy process. Journal of Systems and Software 1993; 22(2):129–139.
31. Ferreira S, Collofello J, Shunk D, Mackulak G. Understanding the effects of requirements volatility in software
engineering using analytical modeling and software process simulation. Journal of Systems and Software 2009;
82(10):1568–1577.
32. Lin CY, Abdel-Hamid T, Sherif JS. Software-engineering process simulation model (SEPS). Journal of Systems
and Software 1997; 38(3):263–277.
Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

392

S. FERREIRA ET AL.

33. Lin CY, Levary RR. Computer aided software development process design. IEEE Transactions on Software
Engineering 1989; 15(9):1025–1037.
34. Madachy R, Boehm B, Lane J. Assessing hybrid incremental processes for SISOS development. Software Process:
Improvement and Practice 2007; 12(5):461–473.
35. Madachy R, Tarbet D. Initial experiences in software process modeling. Software Quality Professional 2000;
2(3):1–13.
36. Pfahl D, Lebsanft K. Using simulation to analyze the impact of software requirements volatility on project
performance. Information and Software Technology 2000; 42(14):1001–1008.
37. Smith BJ, Nguyen N, Vidale RF. Death of a software manager: How to avoid career suicide through dynamic
software process modeling. American Programmer 1993; 6(5):10–17.
38. Humphrey W. Managing the Software Process. Addison-Wesley: Reading MA, 1989.
39. Ciolkowski M, Laitenberger O, Vegas S, Biffi S. Practical Experiences in the Design and Conduct of Surveys
in Empirical Software Engineering (Empirical Methods and Studies in Software Engineering: Experiences from
ESERNET). Springer: Berlin, 2003.
40. Ott L. An Introduction to Statistical Methods and Data Analysis. PWS-KENT Publishing Company: Boston, MA,
1988.
41. Sommerville I. Software Engineering (5th edn). Addison-Wesley: Reading MA, 1996.
42. Lam W, McDermid JA, Vickers AJ. Ten steps toward systematic requirements reuse. Requirements Engineering
1997; 2(2):102–113.
43. Withall S. Software Requirement Patterns. Microsoft Press: Redmond WA, 2007.

AUTHOR’S BIOGRAPHY

Susan Ferreira is an Assistant Professor and the founding Director of the Systems
Engineering Research Center (SERC) at The University of Texas, Arlington (UTA).
Before joining UTA, Dr Ferreira worked as a systems engineer in the Defense
industry on complex software intensive systems. Her industry background includes
work for Lockheed Martin, General Dynamics/Motorola, and Northrop Corporation. Her
research interests are in the area of systems engineering and include requirements engineering, systems engineering (SE) process modeling and simulation, and SE return on
investment.

Dr Dan L. Shunk is a Professor in Industrial Engineering at Arizona State University.
He is currently pursuing research into collaborative commerce, global new product
development, model-based enterprises and global supply network integration. He won a
Fulbright Award in 2002–2003, the 1996 SME International Award for Education, the
1991 and 1999 I&MSE Faculty of the Year award, the 1989 SME Region VII Educator
of the Year award, chaired AutoFact in 1985, and won the 1982 SME Outstanding
Young Engineer award. Dr Shunk studied at Purdue where he received his PhD in
Industrial Engineering in 1976.

Jim Collofello is currently Associate Dean for the Engineering School and Professor
of Computer Science and Engineering at Arizona State University. He received his PhD
in Computer Science from Northwestern University. His teaching and research interests
lie in the software engineering area with an emphasis on software project management,
software quality assurance, and software process modeling. In addition to his academic
activities, he has also been involved in applied research projects, training and consulting
with many large corporations over the last 25 years.

Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

REDUCING THE RISK OF REQUIREMENTS VOLATILITY

393

Gerald T. Mackulak is a Professor Emeritus at Arizona State University after recently
retiring from the Department of Industrial, Systems and Operations Engineering within
the Ira A Fulton College of Engineering at Arizona State. His primary area of research
was simulation methodology with a focus on model abstraction, execution speed, and
output analysis. He has authored over 75 articles, chaired over a dozen doctoral committees, served as an associate editor for two simulation journals, and guided over $2,000,000
of external research during his years at ASU. He has continued to stay current with the
developments in his field but has recently been focusing more on travelling with his
wife to locations they dreamed of visiting but were unable to visit as they both focused
on their engineering careers.

Amylou C. Dueck is an Assistant Professor of Biostatistics at the Mayo Clinic in
Scottsdale, AZ. Her primary area of research is the design and analysis of clinical trials
in cancer and studies with patient-reported outcomes. Dr Dueck previously provided
general statistical consulting on this project through Arizona State University.

Copyright q

2010 John Wiley & Sons, Ltd.

J. Softw. Maint. Evol.: Res. Pract. 2011; 23:375–393
DOI: 10.1002/smr

The Journal of Systems and Software 82 (2009) 1568–1577

Contents lists available at ScienceDirect

The Journal of Systems and Software
journal homepage: www.elsevier.com/locate/jss

Understanding the effects of requirements volatility in software engineering by
using analytical modeling and software process simulation
Susan Ferreira a,*, James Collofello b, Dan Shunk c, Gerald Mackulak c
a

Industrial and Manufacturing Systems Engineering Department, The University of Texas at Arlington, Arlington, TX 76019, USA
Computer Science and Engineering Department, Arizona State University, Tempe, AZ 76019, USA
c
Industrial Engineering Department, Arizona State University, Tempe, AZ 76019, USA
b

a r t i c l e

i n f o

Article history:
Available online 19 March 2009
Keywords:
Requirements volatility
Software process modeling
Requirements engineering risk

a b s t r a c t
This paper introduces an executable system dynamics simulation model developed to help project
managers comprehend the complex impacts related to requirements volatility on a software development project. The simulator extends previous research and adds research results from an empirical survey, including over 50 new parameters derived from the associated survey data, to a base model. The
paper discusses detailed results from two cases that show signiﬁcant cost, schedule, and quality impacts
as a result of requirements volatility. The simulator can be used as an effective tool to demonstrate the
complex set of factor relationships and effects related to requirements volatility.
Ó 2009 Elsevier Inc. All rights reserved.

1. Requirements volatility introduction
Requirements volatility refers to growth or changes in requirements during a project’s development lifecycle. There are multiple
aliases commonly associated with or related to the phenomenon of
requirements volatility. These terms include requirements change,
requirements creep, scope creep, requirements instability, and
requirements churn among others. Costello (1994) provides a relatively detailed set of metrics for requirements volatility. Other
simple metrics for requirements volatility deﬁne it as the number
of additions, deletions, and modiﬁcations made to the requirements set per time unit of interest (per week, month, phase,
etc.). Requirements volatility, in its various forms, surfaces as a frequent and high impact risk in numerous empirical studies performed to identify risk factors or to understand variables leading
to a project’s success or failure (examples include Boehm, 1991;
Curtis et al., 1988; Houston, 2000; Jones, 1994; Känsälä, 1997;
Moynihan, 1997; Ropponen, 1999; Ropponen and Lyytinen, 2000;
Schmidt et al., 2001; The Standish Group, 1995; Tirwana and Keil,
2006).
Changes to a set of requirements can occur at multiple points
during the development process (Kotonya and Sommerville,
1998). These changes can take place ‘‘while the requirements are
being elicited, analyzed and validated and after the system has
gone into service”. Past philosophy dictated that requirements
had to be ﬁrm by the completion of the requirements phase and
* Corresponding author. Tel.: +1 817 272 1332; fax: +1 817 272 3406.
E-mail addresses: ferreira@uta.edu (S. Ferreira), collofello@asu.edu (J. Collofello),
dan.shunk@asu.edu (D. Shunk), mackulak@asu.edu (G. Mackulak).
0164-1212/$ - see front matter Ó 2009 Elsevier Inc. All rights reserved.
doi:10.1016/j.jss.2009.03.014

that requirements should not change after this time. This view is
now understood to be unrealistic (Reifer, 2000). Kotonya and Sommerville (1998) discuss that requirements change is unavoidable.
They also indicate that requirements changes do not necessarily
imply that poor requirements engineering practice was utilized
as requirements changes could be the result of a combination of
factors. The term ‘‘requirements engineering” refers to the processes required to generate and maintain the software requirements throughout the duration of the project.
Concern for the effects of requirements volatility is not usually
associated with the front end of the process, for example, during
requirements deﬁnition. Volatility during the requirements deﬁnition phase is expected because this is when requirements are being
created. However, once the design process begins, the impact of
requirements change is progressively greater due to the additional
investment in time and effort as the project continues to generate
artifacts and complete required tasks. Additions or modiﬁcations
may need to be made to previously generated or in process project
artifacts and additional time investment or scrapped effort can
result. Due to the additional unplanned effort, severe consequences
can potentially occur, including signiﬁcant cost and schedule overruns, and at times, cancelled projects. The impact of changing
requirements during later phases of a project and approaches for
assessing the impacts of these changes has been well documented
(Yau et al., 1978, 1986, 1988; Yau and Kishimoto, 1987; Yau and
Liu, 1988). In an agile software development environment, changes
are welcomed throughout the development process (Beck et al.,
2001). The target of this article is not agile type projects but more
traditional development type projects.

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

2. The need for a requirements volatility assessment tool
The effects of requirements volatility have been discussed in the
literature for some time. However, little empirical research has
been carried out on the topic of requirements volatility that considered the factors involved and the integrated quantitative effects
of requirements volatility on factors related to key project management indicators (cost, schedule, and quality). A relatively small
number of studies consider requirements volatility and its associated effects, especially in a manner integrated with other software
project management factors. These existing studies primarily fall
into a few major research method categories: survey or software
assessment based research (Jones, 1998, 1994; Lane, 1998; Nidumolu, 1996; Zowghi et al., 2000; Zowghi and Nurmuliani, 2002),
interviews and case studies (Javed et al., 2004; Loconsole and Börstler, 2005, 2007; Nurmuliani et al., 2004; Zowghi and Nurmuliani,
1998), regression analysis (Stark et al., 1999), reliability growth
model (Malaiya and Denton, 1999), analytic hierarchy process
analysis (Finnie et al., 1993), and simulation models (Houston,
2000; Lin and Levary, 1989; Lin et al., 1997; Madachy et al.,
2007; Madachy and Tarbet, 2000; Pfahl and Lebsanft, 2000; Smith
et al., 1993).
The existing simulation models discussed in the literature were
developed and tailored for one organization, have a limited view of
requirements volatility or requirements engineering, or do not include requirements engineering processes considered in concert
with the rest of the lifecycle or other critical project factors. A paucity of the literature exists on process modeling and simulation
work performed in requirements engineering, an area now receiving more focused attention because of the impact that it has on the
rest of the systems and software engineering lifecycle.
The limited research and relative importance of requirements
volatility as a risk and the relatively sparse level of requirements
engineering process modeling led the researchers to more analysis
and examination of these areas. A system dynamics process model

1569

simulator, the Software Project Management Simulator (SPMS)
that includes data which is stochastically based on industry survey
data distributions, was then developed as part of a doctoral dissertation (Ferreira, 2002). SPMS illustrates a software business model
that considers the effects of requirements volatility on a software
project’s key management parameters: cost, schedule, and quality.
SPMS presents a more comprehensive and detailed view of the
researched areas than previous models. The development of the
SPMS simulator and associated results developed in this paper
are discussed in this journal article.
3. Requirements volatility tool development process
This section of the paper brieﬂy discusses the research method
used to develop the simulation model. Key research questions addressed in the initial research study include: (1) Which software
factors are affected by requirements volatility? (2) How can these
factors and the uncertainty associated to these factors be modeled?
and (3) What is the project management impact of requirements
volatility? Fig. 1 provides a summary view of the processes used
during the research effort. Starting with the ﬁgure’s top left and
top right sides and ﬂowing down, the ﬁgure illustrates that two efforts (one per ﬁgure side) were initiated concurrently and these efforts ﬂowed into the development of the software process
simulator discussed in this paper.
A rigorous review of the requirements engineering and requirements volatility related literature was performed. Various process
and information models were created to represent and assist in
analysis and synthesis of the knowledge gained during the literature review. Requirements engineering process models and workﬂows, an information model, and a causal model were developed
prior to the simulator development. Relevant factors and associated relationships were identiﬁed based on analysis of the literature review material and discussions with software engineering
experts. Further analysis of the captured information led to the

Fig. 1. Research method.

1570

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

development of a causal model (Fig. 2). The causal model is important because it illustrates the cause and effect relationships between software development factors related to requirements
volatility and includes hypothesized relationships between factors.
The causal model was iteratively developed based on fundamental
factor relationships (for example, job size and overall project effort), researcher industry and academic experience, and hypothesized relationships. The ﬁgure highlights (blue shading) factors
and associated relationships (blue lines) that were further explored
during the research effort. This causal model is expected to evolve
over time as more relationships are explored and understood.
One of the proposed solutions to address the identiﬁed research questions was to develop a software process simulator.
A simulator was selected because it provides a tool for software
project managers and researchers to perform ‘‘what if” analyses,
and enables users to examine the risk of various levels of
requirements volatility and determine project outcomes. A simulator can represent the complexity of relationships between large
quantities of interrelated factors and effectively illustrates the effects and impact of requirements volatility. A simulator with a
graphical icon format was chosen to represent project factors
and relationships. The previously developed causal model was
used to develop the simulator. A subset of the previously generated causal model relationships and factors were selected and
modeled. A subset of the causal model relationships and factors
was chosen because follow-on work needed to be limited in
length to allow key factor data to be collected or these areas
were already modeled in the pre-existing simulator that was extended to create the new simulator, SPMS. Concepts and constructs from all of the generated process and information
models contributed to the creation of the simulator’s workﬂows
and other model sectors.
Joint research was performed with Daniel Houston to characterize four deterministic software process simulators using statistical
design of experiments (Houston et al., 2001). Results of this experimentation work fed into Dan Houston’s development of the Software Process Actualized Risk Simulator (SPARS) (Houston, 2000).
The SPMS research simulation model evolved from Houston’s
SPARS model. The SPARS model also represents an adaptation, as
it reuses or modiﬁes large portions from Abdel-Hamid and Madnick’s model (1991) and Tvedt’s model (1996) and then extends
the consolidated model to incorporate effects of a number of risk
factors. The SPARS model was selected because of its comprehensive software project management scope and updates for more
modern development practices. Reusing components from SPARS
facilitated the development of the SPMS model in that common
constructs did not need to be recreated and the model used previously validated simulator components. As part of the research effort, the SPARS model was modiﬁed to eliminate some
unnecessary factors and extended to create the research model,
SPMS.
As the initial simulator sector designs were generated, walkthroughs were conducted with an initial set of individuals who
were familiar with the research. Once an initial version of the model containing the key constructs was completed, a secondary walkthrough of the model was held with four reviewers outside of the
research group. These reviewers included representatives from
industry and academia that were currently performing research
in requirements engineering and/or software process modeling
and simulation. Following the model walkthroughs, the simulator
was modiﬁed to incorporate reviewer comments and suggestions.
The model was then ready to include quantitative data.
Many of the model variables required data that was not available in the literature. In order to populate these model parameters,
a survey was developed and administered to collect the needed
data. The Project Management Institute’s Information Systems

Speciﬁc Interest Group (PMI-ISSIG) sponsored the survey by providing the host site for the web-based survey and sending notiﬁcations about the survey to its members. Although PMI-ISSIG was the
primary target population for the survey, one mailing was sent to
individual Software Engineering Institute (SEI) Software Process
Improvement Network (SPIN) group contacts within the United
States and to individual professional contacts. Three hundred
twelve software project managers and other software development
personnel submitted responses for the survey.
Survey results indicated that 78% of the respondents experienced some level of requirements volatility on their project. The
survey ﬁndings highlight that requirements volatility can increase
the job size dramatically, extend the project duration, cause major
rework, and affect other project variables such as morale, schedule
pressure, productivity, and requirements error generation. Fig. 3
shows an example of requirements volatility related data from
the survey. The histogram in the ﬁgure depicts how requirements
change affected the job size as a percent of the original job size. In
the vast majority of cases, requirements volatility increased the job
size. However, one can see that there were also cases where there
was no change or a net decrease in the job size. Survey respondents
had an average of 32.4% requirements volatility related job size increases. Other captured volatility effects included signiﬁcant increases in project rework and reduced team morale. The survey
also captured effects from schedule pressure. As the resource effort
increases due to requirements volatility (to address job size additions and rework), schedule pressure also increases. The survey
data showed increases in requirements error generation as the
schedule pressure increases. These effects cause consequences
leading to impacts on key project management indicators such as
cost, schedule, and quality. More details on the survey ﬁndings
showing primary and secondary effects of requirements volatility
are addressed in Ferreira (2002).
Statistical analysis of the survey responses allowed the generation of stochastic distributions for many of the simulation model’s
requirements volatility and requirements engineering factors and
relationships. The model’s stochastic inputs are primarily generated using either empirical discrete distributions derived from analyzing histograms of the data or are generated using the inverse
transform method (Abramowitz and Stegun, 1964). The selection
of the type of distribution depended on the survey analysis results.
Using these stochastic inputs, the simulation model’s random variates use a random number as an input to generate the desired distribution. Based on the sampling frequency, the distributions are
sampled once per run or are sampled continuously throughout a
run to be drawn as necessary.
Once the derived stochastic factor distributions and relationships were added to the simulator, veriﬁcation and validation
(V&V) exercises were performed. Model validation determines
whether a model is a ‘‘useful or reasonable representation of the
system” (Pritsker et al., 1997). Veriﬁcation checks that the simulation model runs as intended. Matko et al. (1992) indicate that modeling work is not an exact science since the real system is never
completely known. Given this situation, the validation and veriﬁcation effort primarily focused on building conﬁdence in the model
as a reasonable representation of the system and in its usefulness
in the provision of results. The overall approach for veriﬁcation and
validation included tests of the structure and behavior of the model. This strategy follows a framework of guidelines presented by
Richardson and Pugh (1981) that builds conﬁdence in the model
and its results. The tests focus on suitability and consistency
checks. The model veriﬁcation and validation activities were performed by the model developer, and various software process
and project experts.
Additional veriﬁcation work was performed in order to understand differences between the SPMS and SPARS model when the

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

1571

Fig. 2. Causal model.

risk factors are not actualized. Differences between the models in
this case were relatively small. More information about this veriﬁcation work is discussed in Ferreira et al. (2003).
4. Assessment tool capability and use
SPMS illustrates researched effects of requirements volatility
and includes requirements engineering extensions to the SPARS

software project management model. The SPARS model was modiﬁed to eliminate some unnecessary factors and extended to create
the research model. Major additions to the base model include the
researched results for the effects of requirements volatility and signiﬁcant extensions to add and support the requirements engineering portions of the software development lifecycle. SPMS
encompasses the requirements engineering through test phases
of the software development lifecycle. Requirements volatility

1572

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

Fig. 3. Distribution of requirements volatility related percent change in job size.

starts and is stochastically simulated during the project’s development and test phases. The model workﬂows also cover the entry of
change requests, change request analysis and review activities, and
their disposition (accepted, rejected/deferred).

Table 1
Model classiﬁcation.
Purpose
Scope
Model
approach

Planning, understanding, process improvement
Medium to large size project, short to long duration, one product/
project team
Continuous, mixed mode (stochastic and deterministic variables),
iThinkTM simulation tool

SPMS demonstrates causal model effects of requirements volatility. Survey ﬁndings showing the impact of requirements volatility on increasing software project job size (this occurs a majority of
the time), increased rework, and lowered staff morale are represented in the model using stochastic relationships derived during
the survey data analysis. Over 50 new parameters that used distributions derived from the survey data were added to the model. In
addition to these survey drawn distributions, a signiﬁcant number
of distributions were reused from other sources, or parameters
were modeled using single point inputs that could be changed by
a user. The effects of lowered morale on requirements engineering
productivity and requirements error generation are represented in
the model. Schedule pressure effects on requirements error generation were also studied as part of the survey data analysis and were
added to pre-existing schedule pressure effects in the model.
Among other survey data used in the simulator, the model includes
requirements defect detection effectiveness for various software
development activities or milestones and relative work rates of
requirements volatility related activities compared to their normal
work rate. Other model contributions include the addition of a
requirements engineering staff type and requirements engineering
support activities which were added to pre-existing simulator
development and test personnel and activities.
Table 1 presents a view of the model’s classiﬁcation, according
to the characterization framework from Kellner et al. (1999). The
typical model audience is expected to be software development
project managers or researchers seeking to gain an understanding
of requirements engineering and requirements volatility and its effects integrated with other software project risks. The model is relatively complex, given its purpose, and assumes a sophisticated

Table 2
Model sector descriptions.
Sector name

Description

Change request work ﬂow

Stochastic change request entry over 10 intervals. Incoming change request (CR) analysis, change request control board (CCB) review,
and disposition
Requirements generation and review process including requirements error and defect rework. Stochastic entry of requirements
volatility related project scope changes and rework over 10 intervals
Work product ﬂow through the development lifecycle, from design and code through testing and rework of design and code errors.
Work is pulled from development and test activities for requirements volatility related rework and reduction and for rework of
requirements defects
A support sector that calculates the amount of product to add to the product cycle for requirements volatility additions based on
survey data
A support sector that calculates the amount of product to pull from the development and test work for requirements volatility related
rework based on survey data
A support sector that calculates the amount of product to pull from development and testing work ﬂow activities for requirements
related reductions. Includes policy choice that deﬁnes where to remove product
Entry and summation of requirements engineering, developer, and tester planned stafﬁng proﬁle information
Allocation of requirements engineer effort to requirements engineering activities based on activity priority

Requirements work ﬂow
Development and test work ﬂow

Requirements change additions
Requirements change rework
Requirements change reductions
Planned stafﬁng
Requirements engineer effort
allocation
Developer and tester effort
allocation
Requirements quality management
Development and test quality
management
Actual stafﬁng
Attrition and replacement
Planning
Control
Adjustment of job effort
Productivity inputs
Productivity
Progress measure
Senior management commitment

Allocation of developer and tester effort to project activities based on activity priority
Generation and detection of requirements errors and defects
Generation and detection of design and code errors and defects
Entry and exit of staff based on planned stafﬁng proﬁles, assimilation of new staff, attrition, and replacements. Entry of contract
personnel and organizationally experienced personnel handled separately for the different staff groups
Attrition (including attrition due to low morale) and replacement calculations for requirements engineers and the grouped set of
developers and testers
Calculation of requirements engineer and the grouped developer and tester work force levels needed based on willingness to hire
Calculation of effort perceived still needed to complete project, effort shortages, schedule pressure, and assumed requirements
engineer, developer, tester productivity. Calculations to determine start of development and testing
Adjustment of job effort based on requirements volatility related additions, rework, and reductions as well as from underestimation
Productivity inputs and calculation of project activity productivity based on productivity multipliers
Work rate modiﬁcation due to effort remaining, effort required, and staff exhaustion. Generation of two staff type productivity
multipliers (including learning, communication overhead, staff experience, and morale)
Calculation of requirements engineering and development and testing progress. Modiﬁcation of currently perceived job size based on
requirements volatility and discovered work due to underestimation
Adjustment of stafﬁng and schedule multipliers based on senior management commitment. This is the only sector that was not
modiﬁed from the original SPARS model

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

user that is educated on the use of simulation models and software
development project management. The model is practically based,
relying on a signiﬁcant and proven foundation of software project
management and simulation research.
The model is segmented into 20 sectors. The sectors are organized into convenient and logical groupings of related factors. Table 2 provides an overview of the model sectors with a brief
description of each in order to give the reader an introduction to
the model’s scope. An example of one sector excerpted from the
model is shown in Fig. 4. The view illustrates the requirements
engineering work ﬂow sector. The connections on the right of the
sector ﬂow into or out of the development and test work ﬂow sector (sector not shown).
The requirements work ﬂow sector encompasses a normal
product work ﬂow. Requirements are generated and reviewed during the normal product work ﬂow. The normal requirements work

1573

ﬂow begins at the initiation of the requirements engineering phase,
at the To_Be_Worked stock. The To_Be_Worked stock is initially
populated with the estimated starting job size. The work then
ﬂows through the Generated_Reqts, Reqts_Awaiting_Review,
Reviewed_Reqts stocks, and then into the development process
as an inﬂow. The requirements generation activities are aggregated, encompassing requirements elicitation, analysis, negotiation, and initial requirements management. Once reviewed, the
requirements product is dispositioned and defective product containing requirements errors is removed from the normal work ﬂow
and becomes part of the requirements error rework work ﬂow, to
be reworked before ﬂowing into the development and test activities. Requirements defects caught post the requirements phase
come back into the requirements defect rework work ﬂow to be reworked. The reworked product then ﬂows back into the development and test activities. Additions to job size due to volatility

Fig. 4. SPMS simulator requirements engineering work ﬂow sector.

1574

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

and underestimation ﬂow into the normal work ﬂow through the
course of the project. The lower half of the sector includes the
requirements change rework work ﬂow. Rework due to requirements volatility is drawn from the development and test work
ﬂows and is worked though the requirements change rework work
ﬂow. This work ﬂow contains separate requirements error related
activities.
The model allows the user to enter detailed inputs related to
their project. Other data is automatically extracted from the stochastic distributions derived from the survey data. Table 3 provides
a listing of a subset of the new inputs added to the model to provide a ﬂavor for the types of data required from the user. These inputs can be input per run or a set of runs. Table 4 identiﬁes the new

Table 3
New factor user inputs (subset only).
Factor

Input quantity

Quantity of experienced requirements
engineers (REs), per interval
Quantity of new requirements
engineers, per interval
Quantity of experienced personnel allocated
to training new reqts engineers (%)
RE transfer in day (days)
Experienced organization RE transfer quantity
Time delay to transfer REs off project (days)
Max quantity of new RE hires per experienced
staff (staff/staff)
Time for organization experienced REs (but not
project experienced) to be productive (days)
Project day that person in org is scheduled to
come onto project (day)
Quantity of RE staff experienced in the organization who
are to be transferred on project (staff)
Requirements review adjustment policy – adjusts
review effort (boolean switch)
Reworked requirements review adjustment policy
(boolean switch)
Requirement error bad ﬁx fraction (%)
Requirements defect bad ﬁx fraction (%)
Nominal change request analysis productivity
(function points/person-day)
Nominal requirements generation productivity
(function points/person-day)
Nominal requirements review productivity
(function points/person-day)
Requirements volatility (boolean switch)

10 (1 for each of 10
intervals)
10 (1 for each of 10
intervals)
1
1
1
1
1
1
1
1
1 Selection
1 Selection
1
1
1
1
1
1 Selection

survey distributions added to the model. Data from the survey is
extracted from stochastic distributions with timing as deﬁned in
Table 4. ‘‘1 Selection per run” means that a value is pulled from
the stochastic distribution one time per run and used throughout
the run. While the data listed in Tables 3 and 4 do not include all
the new or modiﬁed factors in the model it does provide a perspective of the type of data that the user can enter and use.
5. Assessment tool results
The simulator was used to run two cases for the purpose of
comparing them. Each case was setup for 100 runs apiece. The
set of 100 runs for each case was selected for convenience as the
modeling tool allows additional runs, if desired. The two cases
are as follows: (1) A baseline case without the requirements volatility actualized [baseline] and (2) a case with the requirements
volatility related factors actualized [reqts volatility]. The data in
the square brackets corresponds to the case identiﬁer in later ﬁgures. Actualizing the requirements volatility risk for the second
case allows the model to represent the stochastic researched effects of requirement volatility. These include survey-based effects
related to requirements additions, changes, and modiﬁcations as
well as entering change requests. Also included, among the other
effects, are morale, and related schedule pressure effects on morale. The initial job size was estimated to be 400 function points.
The original schedule estimate (planned duration) was deﬁned to
be 408 days.
Figs. 5–8 depict the differences, at the completion of the projects, between the baseline runs (no requirements volatility considered) and runs with the other case where the requirements
volatility risk is actualized for various summary outputs. The model allows additional detailed outputs, if desired. Box plots were
used to represent the data because the simulation results were
positively skewed given the tendency for higher project size, cost,
duration, and released defects (among other outputs) with the
actualization of the requirements volatility risk. The box plots provide a graphical display of the center and the variation of the data,
allowing one to see the symmetry of the data set (Ott, 1988). With
the exception of the ﬁnal project size (Fig. 5), the baseline results
show a small level of variability because some of the model parameters (e.g. requirements engineering process factors) were modeled
stochastically. Therefore, even when the requirements volatility
risk is not actualized, some variability in results will appear.

Table 4
New survey data distributions.
Factor

Selection frequency

Quantity of requirements change requests, per interval (intervals 1–10)
Change request time span with requirements volatility (ratio of duration)
Determination of change request acceptance/deferral due to schedule pressure (ratio)
Relative requirements defect rework productivity (ratio)
Relative requirements generation rework productivity (ratio)
Relative requirements change error rework productivity (ratio)
Relative requirements change requirement review productivity (ratio)
Relative design and code requirements defect rework productivity (ratio)
Percentage of perceived job size increased due to requirements volatility, per interval (%)
Percentage of perceived job size reworked due to requirements volatility, per interval (%)
Percentage of perceived job size reduced due to requirements volatility, per interval (%)
Requirements review effectiveness (%)
Design and code requirement defect detection effectiveness (%)
Design and code review requirement defect detection effectiveness (%)
Test requirement defect detection effectiveness (%)
Reworked requirements review effectiveness (%)
Reworked requirements design and code defect detect effectiveness (%)
Requirements error multiplier for schedule pressure (ratio)
Requirements error multiplier for morale (ratio)

10 Selections per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
10 Selections per run
10 Selections per run
10 Selections per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run

(1 selection for each interval)

(1 selection for each interval)
(1 selection for each interval)
(1 selection for each interval)

1575

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

Fig. 5. Project size box plots.

Fig. 7. Project duration box plots.

Fig. 8. Project released defect density box plots.
Fig. 6. Project cost box plots.

Fig. 5 illustrates the difference in project size between the baseline case and the requirements volatility case. The baseline size is
400 function points. The average size for the requirements volatility case is 499.9 function points with a standard deviation of 82.0,
and a range from 382.0 to 788.3 function points over the 100 runs.
The runs that have values below the baseline 400 function points
(e.g. 382.0) can be explained by the fact that requirements changes
do not always add additional project scope, but can be made to remove scope or unnecessary requirements. However, 93 of the 100
runs had a ﬁnal project size greater than the base case of 400 functions points with relatively large scope added in the course of the
project. This scope addition has a signiﬁcant negative ripple effect
on the project cost (effort in person-days) and project duration. Table 5 contains the detailed statistics for each case including average, median, standard deviation calculations and the minimum
and maximum run values.
Fig. 6 presents the results for cost. Cost is considered to be human effort consumed during the project in person-days. This cost is
equivalent to effort and does not include other project costs. The
effort or cost presented in the ﬁgure encompasses the cumulative
effort for the requirements engineering through test activities.

Table 5
Case study simulation results.
Output/Case

Median

Std. Dev.

Minimum

Maximum

Size (function points)
Baseline
400
Reqts volatility
499.9

Average

400
485.7

0
82.0

400
382.0

400
788.3

Cost (effort in person days)
Baseline
4107.6
Reqts volatility
6208.2

4119.0
5968.9

35.9
1398.1

3969.5
3988.5

4155.5
11196.1

Duration (days)
Baseline
Reqts volatility

452.0
608.5

4.3
132.8

442.0
444.0

461.0
1101.0

Released defect density (defects per function point)
Baseline
0.753
0.753
0.013
Reqts volatility
0.894
0.886
0.075

0.726
0.747

0.792
1.170

452.4
634.0

The detailed statistics are presented in Table 5. As mentioned
earlier, the baseline results do show a small level of variability because some of the model parameters are represented stochastically. The average for the requirements volatility case is over
2000 person-days more than the average for the baseline case.

1576

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

Considering that the baseline cost is an average of 4107.6 person
days, this is a very large number for a program manager to justify
as it represents more than a 50% increase in resource costs. The
cost range for the requirements volatility case is very large as well.
The requirements volatility case has a maximum of 11,196.1 person-days. This represents a very signiﬁcant difference from the
baseline maximum.
Fig. 7 displays the results for project duration in days. The duration
encompasses the requirements engineering through test activities for
the project. The detailed statistics for the two cases are shown in Table
5. It takes an average of 452.4 days to complete the project for the
baseline case. The average for the requirements volatility case is signiﬁcantly higher at an average of 634.0 days. As in the case of the cost,
the range for the requirements volatility case has a wide span.
Fig. 8 presents the project released defect density box plots. The
data represents the defects released post the test process and is in
units of defects per function point. The quantity of released defects
is also signiﬁcantly higher for the requirements volatility case. The
baseline case has an average of 0.753 defects per function point.
The average for the requirements volatility case is 0.894. The range
span for this case is also relatively large.
For the case that include the risk of requirements volatility, the
box plots for each parameter have a wide span. The wide span in outcomes indicates a large potential for unpredictable results as well as
considerable impact on project results. A comparison of the model
results for projects with no requirements volatility and those with
requirements volatility show signiﬁcant differences in the box plots.
One can clearly see the potential impact of requirements volatility
on key project management parameters such as cost (effort), schedule, and quality. Differences between the baseline and requirements volatility case results can be attributed to the addition (or
reduction) in project scope caused by requirements volatility, cause
and effect relationships between factors, and the stochastic representation of a number of the factors.

6. Summary and conclusions
Key research questions were addressed as part of the research. These questions include: (1) Which software factors are
affected by requirements volatility? (2) How can these factors
and the uncertainty associated to these factors be modeled?
and (3) What is the project management impact of requirements
volatility? A causal model was developed and used to evaluate
which software factors are affected by requirements volatility.
A survey was administered to collect information for a subset
of the factors identiﬁed in the causal model. This allowed the
researchers to verify the relationships and quantify the level of
the relationships. A simulator was chosen as the means to model
how the factors relate to other project factors and to represent
the uncertainty associated to the factors using stochastic distributions derived from survey data. The SPMS model assists in
understanding and evaluating the program management impact
of requirements volatility.
This simulator can help an interested user to better understand
the requirements engineering process and the impact of requirements volatility via its process-oriented workﬂows and comprehensive scope. The factor relationships represented in the model
represent a signiﬁcant contribution. This work expands our understanding of the requirements engineering process and the effects of
requirements volatility. The rigorous research to both understand
and leverage the previous foundation of knowledge in requirements engineering and requirements volatility and the thorough
nature of the survey and analysis of this valuable data to assess factor relationships and populate the simulator with empirical data is
a signiﬁcant contribution.

In addition to simulating the effects of requirements volatility,
SPMS offers the ability to simulate various project scenario combinations starting with the requirements engineering portion of the
lifecycle. The model offers a robust set of parameters that capture
various facets of the project including requirements errors and defects, requirements defect density, defect containment effectiveness for various milestones, and other parameters integrated
with a very comprehensive development and test related set of factors and relationships. Each of the model distributions can be easily calibrated and tailored to a speciﬁc organization’s environment.
All the other factors are also setup to be readily modiﬁable to reﬂect an organization’s historical data.
Survey data used in the simulator captures information on factors and relationships not previously available from a wide population in the software industry and allows modeling variables
stochastically given the large quantity of survey responses. Many
of the model variables were modeled stochastically to allow the
user to assess project outcomes probabilistically. The model results
quantitatively illustrate the potential for signiﬁcant cost, schedule,
and quality impacts related to requirements volatility.
Software project managers can use this tool to better understand the effects of requirements volatility and this risk in concert
with other common risks. The simulator can be used to assess potential courses of action to address the risk of requirements
volatility.

7. Future research
Additional research in the area of requirements engineering, as
it continues to evolve, is expected to provide a continuous stream
of new ideas, perspectives, and information that can allow for the
development of richer models and that can represent different facets of understanding in this under-represented yet critical research
area. More work needs to be done to model the impact of requirements engineering related processes and policies so that project
managers and software development personnel are more aware
of the impact of the decisions they make during this phase and
how the rest of the lifecycle may be affected by their choices. This
work can lead to the further identiﬁcation of best practices and
generate insights valuable to managing software development projects in the future.
As this and other simulators that incorporate requirements
engineering processes and relationships continue to evolve, additional experimentation with the models may prove valuable in
identifying common factors and relationships. The research model
presented in this paper is relatively large and complex. Sensitivity
analysis can assist in the identiﬁcation of inﬂuential factors in this
and other models. The results of further experimentation may be
used to ‘‘prune” insigniﬁcant factors from the model so that a more
efﬁcient model can result (Houston et al., 2001). One of the beneﬁts
of a simpler model includes a shortened training and learning
ramp-up time. Core concepts that make the most difference in results can be emphasized. Since the model is less complex it becomes easier to understand and may perhaps be more popular
given the reduced time to understand and then populate the model
with organizational and project speciﬁc data. Maintenance time
may also be reduced because the model is smaller and it is easier
to ﬁnd and ﬁx problems.
Agile processes which welcome requirements changes present
another valuable area to study. As these processes continue to mature and more quantitative results are available, simulating different types of agile processes, (e.g. XP, Scrum, etc.) would be of
interest to determine the project management ramiﬁcations related to cost, schedule, and quality as compared to more traditional
approaches.

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

References
Abdel-Hamid, T., Madnick, S., 1991. Software Project Dynamics: An Integrated
Approach. Prentice-Hall, Englewood Cliffs, NJ.
Abramowitz, M., Stegun, I.A. (Eds.), 1964. Handbook of Mathematical Functions,
Applied Mathematics Series 55. National Bureau of Standards, Washington, DC.
Beck, K., Beedle, M., van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M.,
Grenning, J., Highsmith, J., Hunt, A., Jeffries, R., Kern, J., Marick, B., Martin, R.,
Mellor, S., Schwaber, K., Sutherland, J., Thomas, D., 2001. Principles Behind the
Agile Manifesto. Retrieved 11.6.2008 from: <http://www.agilemanifesto.org/
principles.html>.
Boehm, B.W., 1991. Software risk management: principles and practices. IEEE
Software 8 (1), 32–41.
Costello, R.J., 1994. Metrics for Requirements Engineering. Master of Science Thesis,
California State University, Long Beach.
Curtis, B., Krasner, H., Iscoe, N., 1988. A ﬁeld study of the software design process for
large systems. Communications of the ACM 31 (11), 1268–1287.
Ferreira, S., 2002. Measuring the Effects of Requirements Volatility on Software
Development Projects, Ph.D. Dissertation, Arizona State University.
Ferreira, S., Collofello, J., Shunk, D., Mackulak, G., Wolfe, P., 2003. Utilization of
Process Modeling and Simulation in Understanding the Effects of Requirements
Volatility in Software Development. In: Proceedings of the 2003 Process
Simulation Workshop (ProSim 2003).
Finnie, G.R., Witting, G.E., Petkov, D.I., 1993. Prioritizing software development
productivity factors using the analytic hierarchy process. Journal of Systems
and Software 22 (2), 129–139.
Houston, D.X., 2000. A Software Project Simulation Model for Risk Management,
Ph.D. Dissertation, Arizona State University.
Houston, D.X., Ferreira, S., Collofello, J.S., Montgomery, D.C., Mackulak, G.T., Shunk,
D.L., 2001. Behavioral characterization: ﬁnding and using the inﬂuential factors
in software process simulation models. Journal of Systems and Software 59 (3),
259–270.
Javed, T., Maqsood, M., Durrani, Q.S., 2004. A study to investigate the impact of
requirements instability on software defects. ACM SIGSOFT Software
Engineering Notes 29 (3), 7.
Jones, C., 1994. Assessment and Control of Software Risks. PTR Prentice-Hall, Inc.,
Englewood Cliffs, NJ.
Jones, C., 1998. Estimating Software Costs. McGraw-Hill, New York.
Känsälä, K., 1997. Integrating risk assessment with cost estimation. IEEE Software
14 (3), 61–67.
Kellner, M.I., Madachy, R., Raffo, D.M., 1999. Software process modeling: why?
what? how? Journal of Systems and Software 46 (2–3), 91–105.
Kotonya, G., Sommerville, I., 1998. Requirements Engineering: Processes and
Techniques. John Wiley and Sons, Ltd..
Lane, M.S., 1998. Enhancing software development productivity in Australian ﬁrms.
In: Proceedings of the Ninth Australasian Conference on Information Systems
(ACIS ’98), vol. 1, pp. 337–349.
Lin, C.Y., Abdel-Hamid, T., Sherif, J.S., 1997. Software-engineering process
simulation model (SEPS). Journal of Systems and Software 38 (3), 263–277.
Lin, C.Y., Levary, R.R., 1989. Computer aided software development process design.
IEEE Transactions on Software Engineering 15 (9), 1025–1037.
Loconsole, A., Börstler, J., 2005. An industrial case study on requirements volatility
measures. In: Proceedings of the 12th Asia-Paciﬁc Software Engineering
Conference (APSEC ’05), 8 p.
Loconsole, A., Börstler, J., 2007. Are size measures better than expert judgment? An
industrial case study on requirements volatility. In: Proceedings of the 14th
Asia-Paciﬁc Software Engineering Conference (APSEC ’07), pp. 238–245.
Madachy, R., Tarbet, D., 2000. Initial experiences in software process modeling.
Software Quality Professional 2 (3), 1–13.
Madachy, R., Boehm, B., Lane, J., 2007. Assessing hybrid incremental processes for
SISOS development. Software Process: Improvement and Practice 12 (5), 461–
473.
Malaiya, Y.K., Denton, J., 1999. Requirements volatility and defect density. In:
Proceedings of the 10th International Symposium on Software Reliability
Engineering, pp. 285–294.
Matko, D., Zupancic, B., Karba, R., 1992. Simulation and Modeling of Continuous
Systems: A Case Study Approach. Prentice-Hall International Ltd., Great Britain.
Moynihan, T., 1997. How experienced project managers assess risk. IEEE Software
14 (3), 35–41.
Nidumolu, S.R., 1996. Standardization, requirements uncertainty and software
project performance. Information and Management 31 (3), 135–150.
Nurmuliani, N., Zowghi, D., Fowell, S., 2004. Analysis of requirements volatility
during software development life cycle. In: Proceedings of the 2004 Australian
Software Engineering Conference (ASWEC ’04), pp. 28–37.
Ott, L., 1988. An Introduction to Statistical Methods and Data Analysis. PWS-KENT
Publishing Company.
Pfahl, D., Lebsanft, K., 2000. Using simulation to analyze the impact of software
requirements volatility on project performance. Information and Software
Technology 42 (14), 1001–1008.
Pritsker, A. Alan B., O’Reilly, Jean J., LaVal, David K., 1997. Simulation with Visual
SLAM and AweSim. System Publishing Company, West Lafayette, IN.
Reifer, D.J., 2000. Requirements management: the search for Nirvana. IEEE Software
17 (3), 45–47.
Richardson, George, P., Alexander, L., Pugh III, 1981. Introduction to System
Dynamics Modeling with DYNAMO. The MIT Press, Cambridge, MA.

1577

Ropponen, J., 1999. Risk assessment and management practices in software
development. Chapter 8 in Beyond the IT Productivity Paradox. John Wiley
and Sons. pp. 247–266.
Ropponen, J., Lyytinen, K., 2000. Components of software development risk: how to
address them? A project manager survey. IEEE Transactions on Software
Engineering 26 (2), 98–112.
Schmidt, R., Lyytinen, K., Keil, M., Culle, P., 2001. Identifying software project risks:
an international Delphi study. Journal of Management Information Systems 17
(4), 5–36.
Smith, B.J., Nguyen, N., Vidale, R.F., 1993. Death of a software manager: how to avoid
career suicide through dynamic software process modeling. American
Programmer 6 (5), 10–17.
The Standish Group, 1995. The Chaos Report. Obtained from <http://www.
standishgroup.com/chaos.html>.
Stark, G.E., Oman, P., Skillicorn, A., Ameele, A., 1999. An examination of the effects of
requirements changes on software maintenance releases. Journal of Software
Maintenance: Research and Practice 11 (5), 293–309.
Tirwana, A., Keil, M., 2006. Functionality risk in information systems development:
an empirical investigation. IEEE Transactions on Engineering Management 53
(3), 412–425.
Tvedt, J.D., 1996. An Extensible Model for Evaluating the Impact of Process
Improvements on Software Development Cycle Time. Ph.D. Dissertation,
Arizona State University.
Yau, S.S., Collofello, J.S., MacGregor, T., 1978. Ripple effect analysis of software
maintenance. In: Proceedings of the IEEE Computer Society’s Second International
Computer Software and Applications Conference (COMPSAC ’78), pp. 60–65.
Yau, S.S., Kishimoto, Z., 1987. A method for revalidating modiﬁed programs in the
maintenance phase. In: Proceedings of 11th IEEE International Computer
Software and Applications Conference (COMPSAC ‘87), pp. 272–277.
Yau, S.S., Liu, C.S., 1988. An approach to software requirement speciﬁcations. In:
Proceedings of 12th International Computer Software and Applications
Conference (COMPSAC ‘88), pp. 83–88.
Yau, S.S., Nicholl, R.A., Tsai, J.P., 1986. An evolution model for software maintenance.
In: Proceedings of 10th IEEE International Computer Software and Applications
Conference (COMPSAC ‘86), pp. 440–446.
Yau, S.S., Nicholl, R.A., Tsai, J.P., Liu, S.S., 1988. An integrated life-cycle model for software
maintenance. IEEE Transactions on Software Engineering 14 (8), 1128–1144.
Zowghi, D., Nurmuliani, 1998. Investigating requirements volatility during software
development: research in progress. In: Proceeding of the Third Australian
Conference on Requirements Engineering (ACRE98), pp. 38–48.
Zowghi, D., Nurmuliani, 2002. A study of the impact of requirements volatility on
software project performance. In: Proceedings of the Ninth Asia-Paciﬁc
Software Engineering Conference (ASPEC ’02), pp. 3–11.
Zowghi, D., Offen, R., Nurmuliani, 2000. The impact of requirements volatility on the
software development lifecycle. In: Proceedings of the International Conference
on Software Theory and Practice (IFIP World Computer Congress), pp. 19–27.

Susan Ferreira is an Assistant Professor and the founding Director of the Systems
Engineering Research Center (SERC) at The University of Texas, Arlington (UTA).
Before joining UTA, Dr. Ferreira worked as a systems engineer in the Defense
industry on complex software intensive systems. Her industry background includes
work for Lockheed Martin, General Dynamics/Motorola, and Northrop Corporation.
Her teaching and research interests are related to systems engineering (SE) and
include requirements engineering, SE process modeling and simulation, lean SE, SE
return on investment, SE cost estimation, and system of systems engineering.
James Collofello is currently Associate Dean for the Engineering School and Professor of Computer Science and Engineering at Arizona State University. He received
his Ph.D. in Computer Science from Northwestern University. His teaching and
research interests lie in the software engineering area with an emphasis on software project management, software quality assurance and software process modeling. In addition to his academic activities, he has also been involved in applied
research projects, training and consulting with many large corporations over the
last 25 years.
Dan Shunk is the Avnet Professor of Supply Network Integration in Industrial Engineering at Arizona State University. He is currently pursuing research into collaborative commerce, global new product development, model-based enterprises and
global supply network integration. He won a Fulbright Award in 2002-2003, the 1996
SME International Award for Education, the 1991 and 1999 I&MSE Faculty of the Year
award, the 1989 SME Region VII Educator of the Year award, chaired AutoFact in 1985,
and won the 1982 SME Outstanding Young Engineer award. Dr. Shunk studied at
Purdue where he received his Ph.D. in Industrial Engineering in 1976.
Gerald Mackulak is an Associate Professor of Engineering in the Department of
Industrial, Systems and Operations Engineering at Arizona State University. He is a
graduate of Purdue University receiving his B.Sc., M.Sc., and Ph.D. degrees in the
area of Industrial Engineering. His primary area of research is simulation methodology with a focus on model abstraction, execution speed and output analysis. He
has authored over 75 articles, served as an associate editor for two simulation
journals and continues to guide research in simulation efﬁciency.

SOME STABILITY MEASURES FOR SOFTWARE MAINTENANCE*

Stephen S . Yau'
Northwest e r n U n i v e r s i t y
Evanston, I l l i n o i s 60201

and

e s t a b l i s h a s i n g l e f i g u r e f o r software q u a l i t y .
I n s t e a d , meaningful a t t r i b u t e s which c o n t r i b u t e t o
s o f t w a r e q u a l i t y must b e i d e n t i f i e d .
Research
r e s u l t s i n t h i s a r e a have c o n t r i b u t e d t o t h e d e f i n i t i o n of s e v e r a l s o f t w a r e q u a l i t y a t t r i b u t e s ,
such a s c o r r e c t n e s s , f l e x i b i l i t y , p o r t a b i l i t y ,
e f f i c i e n c y , r e l i a b i l i t y , i n t e g r i t y , t e s t a b i l i t y and
These r e s u l t s a r e encouragm a i n t a i n a b i l i t y [3-51.
i n g and p r o v i d e a r e a s o n a b l y s t r o n g b a s i s f o r t h e
d e f i n i t i o n o f t h e q u a l i t y of s o f t w a r e .

SUMMARY
Software maintenance h a s been t h e dominant
f a c t o r c o n t r i b u t i n g t o t h e h i g h c o s t of s o f t w a r e .
I n t h i s p a p e r , t h e s o f t w a r e maintenance p r o c e s s
and t h e i m p o r t a n t s o f t w a r e q u a l i t y a t t r i b u t e s t h a t
a f f e c t t h e maintenance e f f o r t a r e d i s c u s s e d . Among
t h e s e q u a l i t y a t t r i b u t e s , t h e s t a b i l i t y of a p r o gram, which i n d i c a t e s t h e r e s i s t a n c e t o t h e potent i a l r i p p l e e f f e c t t h a t t h e program would have when
i t i s m o d i f i e d , i s a n i m p o r t a n t one. Measures f o r
e s t i m a t i n g t h e s t a b i l i t y o f a program and t h e
modules of which t h e program i s composed a r e p r e s e n t e d , and a n a l g o r i t h m f o r computing t h e s e s t a b i l i t y measures i s g i v e n . A p p l i c a t i o n of t h e s e
measures d u r i n g t h e maintenance phase i s d i s c u s s e d
a l o n g w i t h a n example. F u r t h e r r e s e a r c h e f f o r t s
i n v o l v i n g v a l i d a t i o n of t h e s t a b i l i t y measures,
a p p l i c a t i o n of t h e s e measures d u r i n g t h e d e s i g n
p h a s e , and r e s t r u c t u r i n g based on t h e s e measures
a r e a l s o discussed.

S i n c e s o f t w a r e q u a l i t y i s environment depend e n t , some a t t r i b u t e s may b e more d e s i r a b l e t i a n
o t h e r s . One a t t r i b u t e which i s a l m o s t always
d e s i r a b l e except i n very l i m i t e d a p p l i c a t i o n s i s
t h e m a i n t a i n a b i l i t y of t h e program. S o f t w a r e maint e n a n c e i s a v e r y broad a c t i v i t y t h a t include:;
e r r o r c o r r e c t i o n s , enhancements of c a p a b i l i t i e s ,
d e l e t i o n of o b s o l e t e c a p a b i l i t i e s and o p t i m i z a t i o n
[ 6 ] . The c o s t of t h e s e s o f t w a r e maintenance a c t i v i t i e s h a s been v e r y h i g h , and i t h a s b e e n e s t i mated r a n g i n g from 40% [ l ] t o 67% [ 2 ] of t h e t o t a l
cost during the l i f e cycle of large-scale s o f t ware systems. T h i s v e r y h i g h s o f t w a r e maintenance
c o s t s u g g e s t s t h a t t h e m a i n t a i n a b i l i t y of a program
i s a very c r i t i c a l software q u a l i t y a t t r i b u t e .
Measures a r e needed t o e v a l u a t e t h e m a i n t a i n a b i l i t y
of a program a t each phase o f its development.
These measures must b e e a s i l y c a l c u l a t e d and subj e c t t o v a l i d a t i o n . Techniques must a l s o b e d e v e l oped t o r e s t r u c t u r e t h e s o f t w a r e d u r i n g each p h a s e
of i t s development i n o r d e r t o improve i t s maint a i nabi 1i t y

I n d e x T e r m s - S o f t w a r e maintenance, maintenance p r o c e s s , s o f t w a r e q u a l i t y a t t r i b u t e s , maint a i n a b i l i t y , p o t e n t i a l r i p p l e e f f e c t , module s t a b i l i t y , program s t a b i l i t y , l o g i c a l s t a b i l i t y .

INTRODUCTION

I t i s w e l l known t h a t t h e c o s t of l a r g e s c a l e s o f t w a r e systems h a s become u n a c c e p t a b l y h i g h
[1,2].
Much of t h i s e x c e s s i v e s o f t w a r e c o s t c a n
b e a t t r i b u t e d t o d e f i c i e n c i e s i n t h e q u a l i t y of
t h e software.
The d e f i n i t i o n of s o f t w a r e q u a l i t y
i s v e r y vague. S i n c e some d e s i r e d a t t r i b u t e s of a
program c a n o n l y b e a c q u i r e d a t t h e expense of
o t h e r a t t r i b u t e s , program q u a l i t y must b e e n v i r o n ment dependent. Thus, i t i s i m p o s s i b l e t o

.

In t h i s p a p e r , we w i l l f i r s t d i s c u s s t h e s o f t ware maintenance p r o c e s s and t h e s o f t w a r e q u a l i t y
a t t r i b u t e s t h a t a f f e c t t h e maintenance e f f o r t .
Because accommodating r i p p l e e f f e c t of m o d i f i c s t i o n s i n a program i s normally a l a r g e p o r t i o n of
t h e maintenance e f f o r t , e s p e c i a l l y f o r n o t w e l l
d e s i g n e d programs [ 6 ] , w e w i l l p r e s e n t some meds u r e s f o r e s t i m a t i n g t h e s t a b i l i t y of a program,
which i s t h e q u a l i t y a t t r i b u t e i n d i c a t i n g t h e
r e s i s t a n c e t o t h e p o t e n t i a l r i p p l e e f f e c t which a
program would h a v e when i t i s modified. An a l g o r i t h m f o r computing t h e s e s t a b i l i t y measures w i l l
b e g i v e n , and a p p l i c a t i o n s of t h e s e s t a b i l i t y meas u r e s a r e a l s o d i s c u s s e d a l o n g w i t h a n example.
F u t u r e r e s e a r c h e f f o r t s i n v o l v i n g v a l i d a t i o n of t h e
s t a b i l i t y measures, a p p l i c a t i o n of t h e measures
d u r i n g t h e d e s i g n phase, and r e s t r u c t u r i n g based
upon t h e measures a r e a l s o d i s c u s s e d .

*The

work was s u p p o r t e d by U.S. A i r F o r c e
Rome A i r Development C e n t e r under C o n t r a c t No.
F30602-76-C-0397.
'Department of
Computer S c i e n c e .

2
James S. C o l l o f e l l o
Arizona S t a t e U n i v e r s i t y
Tempe, Arizona 85281

E l e c t r i c a l E n g i n e e r i n g and

2Department of Mathematics; and f o r m e r l y w i t h
Department o f E l e c t r i c a l E n g i n e e r i n g and Computer
S c i e n c e , Northwestern U n i v e r s i t y , Evanston,
I l l i n o i s 60201.

CH I5 I 5 - 6/79/OOOO0674$oO.75 0 1979 IEE E
674

THE MAINTENANCE PROCESS

The f i r s t p h a s e c o n s i s t s of a n a l y z i n g t h e program i n o r d e r t o u n d e r s t a n d i t . S e v e r a l a t t r i b u t e s
such a s t h e c o m p l e x i t y of t h e program, t h e
m e n t a t i o n , and t h e s e l f - d e s c r i p t i v e n e s s of t h e p r o gram c o n t r i b u t e t o t h e e a s e of u n d e r s t a n d i n g t h e
program [ 5 ] .

w-

A s p r e v i o u s l y d i s c u s s e d , s o f t w a r e maintenance
i s a v e r y broad a c t i v i t y . Once a p a r t i c u l a r maint e n a n c e o b j e c t i v e i s e s t a b l i s h e d , i t c a n b e accomp l i s h e d i n t h e f o u r p h a s e s a s shown i n F i g . 1.

I
Add new capabilities

Determine
Maintenance
Objective

The t h i r d phase c o n s i s t s of a c c o u n t i n g f o r a l l
of t h e r i p p l e e f f e c t a s a consequence o f t h e modif i c a t i o n . In s o f t w a r e , t h e e f f e c t of a m o d i f i c a t i o n may n o t b e l o c a l t o t h e m o d i f i c a t i o n , b u t may
a l s o a f f e c t o t h e r p o r t i o n s of t h e program. There
i s a r i p p l e e f f e c t from t h e l o c a t i o n of t h e m o d i f i c a t i o n t o t h e o t h e r p a r t s of t h e program t h a t a r e
a f f e c t e d by t h e m o d i f i c a t i o n [ 6 ] . One a s p e c t of
t h i s r i p p l e e f f e c t i s logical o r functional i n
n a t u r e . Another a s p e c t o f t h i s r i p p l e e f f e c t conc e r n s t h e performance o f t h e program. S i n c e a
l a r g e - s c a l e program u s u a l l y h a s b o t h f u n c t i o n a l and
performance r e q u i r e m e n t s , i t i s n e c e s s a r y t o unders t a n d t h e p o t e n t i a l e f f e c t of a program m o d i f i c a t i o n from b o t h a l o g i c a l and a performance p o i n t of
view [ 6 ] . The primary a t t r i b u t e a f f e c t i n g t h e
r i p p l e e f f e c t a s a consequence of a program modific a t i o n i s t h e s t a b i l i t y of t h e program. Program
s t a b i l i t y i s defined a s the resistance to the
a m p l i f i c a t i o n of changes i n t h e program.

Delete obsolete
features
Optimization

Understand
Program

_-

1

Documen t a t i on

\

S e l f descriptiveness

Phase 2

Particular
Maintenance
Proposal

__

Extensibility

The fourth phase c o n s i s t s of t e s t i n g t h e modif i e d program t o i n s u r e t h a t t h e modified program
h a s a t l e a s t t h e sane r e l i a b i l i t y l e v e l a s b e f o r e .
I t i s important t h a t c o s t - e f f e c t i v e t e s t i n g techn i q u e s be a p p l i e d d u r i n g maintenance. The primary
f a c t o r c o n t r i b u t i n g t o t h e development of t h e s e
c o s t - e f f e c t i v e techniques i s the t e s t a b i l i t y of
t h e program.

Phase 3
Account f o r
Ripple
Effect

Stability

Each of t h e s e f o u r p h a s e s and t h e i r a s s o c i ated software q u a l i t y a t t r i b u t e s a r e c r i t i c a l t o
t h e maintenance p r o c e s s . A l l of t h e s e s o f t w a r e
q u a l i t y a t t r i b u t e s must b e combined t o form a maint a i n a b i l i t y measure. One of t h e most i m p o r t a n t
q u a l i t y a t t r i b u t e s i s t h e s t a b i l i t y of t h e program.
T h i s f a c t c a n b e i l l u s t r a t e d by c o n s i d e r i n g a p r o gram which i s e a s y t o u n d e r s t a n d , e a s y t o g e n e r a t e
m o d i f i c a t i o n p r o p o s a l s f o r , and e a s y t o t e s t . I f
t h e s t a b i l i t y of t h e program i s p o o r , however, t h e
impact of any o r i g i n a l m o d i f i c a t i o n on t h e program
i s l a r g e . Hence, t h e maintenance c o s t w i l l b e
h i g h and t h e r e l i a b i l i t y may a l s o s u f f e r due t o
t h e i n t r o d u c t i o n of p o s s i b l e new e r r o r s because of
t h e e x t e n s i v e changes t h a t have t o b e made.

Phase 4
Tes t a b i l i t y

No
Testin

F i g u r e 1.

The second p h a s e c o n s i s t s o f g e n e r a t i n g a
p a r t i c u l a r maintenance p r o p o s a l t o accomplish t h e
implementation of t h e maintenance o b j e c t i v e .
This
r e q u i r e s a c l e a r u n d e r s t a n d i n g o f both t h e maintenance o b j e c t i v e and t h e program t o b e modified.
However, t h e e a s e of g e n e r a t i n g maintenance propos a l s f o r a program i s p r i m a r i l y a f f e c t e d by t h e
a t t r i b u t e e x t e n s i b i l i t y [5].

The S o f t w a r e Maintenance P r o c e s s

Although t h e p o t e n t i a l b e n e f i t s of a v a l i d a t e d program s t a b i l i t y measure a r e g r e a t , v e r y
l i t t l e r e s e a r c h h a s been conducted i n t h i s a r e a .
P r e v i o u s s t a b i l i t y measures have been developed by
Soong [ 3 ] and Myers [ 4 ] . There e x i s t s e v e r a l
weaknesses i n t h e s e measures which have p r e v e n t e d

675

t h e i r wide a c c e p t a n c e . The l a r g e s t problem h a s
been t h e i n a b i l i t y t o v a l i d a t e t h e measures due t o
model i n p u t s t h a t a r e q u e s t i o n a b l e o r d i f f i c u l t t o
o b t a i n . Other weaknesses of b o t h measures i n c l u d e
an assumption t h a t a l l m o d i f i c a t i o n s t o a module
have t h e same r i p p l e e f f e c t , a symmetry assumption
t h a t i f t h e r e e x i s t s a nonzero p r o b a b i l i t y of havi n g t o change a module i g i v e n t h a t module j i s
changing then t h e r e e x i s t s a nonzero p r o b a b i l i t y
of having t o change module j g i v e n t h a t module i
i s changing, and a f a i l u r e t o i n c o r p o r a t e a p e r formance component a s p a r t of t h e s t a b i l i t y
measure.

Development of a Module L o g i c a l S t a b i l i t y Measure

DEVELOPMENT OF LOGICAL STABILITY MEASURES

The s t a b i l i t y of a program h a s been d e f i n e d
as the resistance t o the potential ripple effect
t h a t t h e program would have when i t i s modified.
Before c o n s i d e r i n g t h e s t a b i l i t y of a program, i t
i s n e c e s s a r y t o develop a measure f o r t h e s t a b i l i t y
of a module. The s t a b i l i t y of a module c a n be
d e f i n e d a s a measure of t h e r e s i s t a n c e t o t h e p o t e n t i a l r i p p l e e f f e c t of a m o d i f i c a t i o n t o t h e module
on o t h e r modules i n t h e program. There a r e two
a s p e c t s of t h e s t a b i l i t y of a module:
the logical
a s p e c t and t h e performance a s p e c t . The l o g i c a l s t a
b i l i t y of a module i s a measure of t h e r e s i s t a n c e
t o t h e impact of such a m o d i f i c a t i o n on o t h e r
modules i n t h e program i n terms of l o g i c a l c o n s i d e r a t i o n s . The performance s t a b i l i t y of a module
i s a measure of t h e r e s i s t a n c e t o impact of such a
m o d i f i c a t i o n on o t h e r modules i n t h e program i n
terms of performance c o n s i d e r a t i o n s .
In this
p a p e r , l o g i c a l s t a b i l i t y measures w i l l b e developed
f o r a program and t h e modules of which t h e program
i s composed. Performance s t a b i l i t y measures i s
c u r r e n t l y under development and t h e r e s u l t s w i l l b e
r e p o r t e d i n a subsequent p a p e r . Both t h e l o g i c a l
and t h e performance s t a b i l i t y measures a r e b e i n g
developed toovercome t h e weaknesses of t h e p r e v i o u s
s t a b i l i t y measures. I n a d d i t i o n , t h e s t a b i l i t y
measures a r e b e i n g developed w i t h t h e f o l l o w i n g
r e q u i r e m e n t s t o i n c r e a s e t h e i r a p p l i c a b i l i t y and
acceptance:

1.

A b i l i t y t o v a l i d a t e t h e measures.

2.

Consistency w i t h c u r r e n t d e s i g n methodolo_gies.

3.

U t i l i z a t i o n i n comparing a l t e r n a t e
designs.

4.

Diagnostic a b i l i t y .

The l o g i c a l s t a b i l i t y of a module i s a measure
of t h e r e s i s t a n c e t o t h e expected impact of a modif i c a t i o n t o t h e module on o t h e r modules i n t h e p r o Thus, a
gram i n terms of l o g i c a l c o n s i d e r a t i o n s .
computation of t h e l o g i c a l s t a b i l i t y of a module
must b e based upon some type of a n a l y s i s of t h e
maintenance a c t i v i t y which w i l l b e performed on
t h e module. However, due t o t h e d i v e r s e and almost
random n a t u r e of s o f t w a r e maintenance a c t i v i t i e s ,
i t i s v i r t u a l l y meaningless t o a t t e m p t t o p r e d i c t
when t h e n e x t maintenance a c t i v i t y w i l l c o n s i s t o f .
Thus, i t i s i m p o s s i b l e t o develop a s t a b i l i t y measu r e based upon p r o b a b i l i t i e s of what t h e maintenance e f f o r t w i l l c o n s i s t of. I n s t e a d , t h e s t a b i l i t y measure must be based upon some s u b s e t of
maintenance a c t i v i t y f o r which t h e impact of t h e
m o d i f i c a t i o n s can r e a d i l y be determined.
For t h i s
purpose t h e s i m p l e s t t y p e of maintenance a c t i v i t y
i s u t i l i z e d . T h i s c o n s i s t s of a change t o a s i n g l e
v a r i a b l e d e f i n i t i o n i n a module. A l o g i c a l :;tab i l i t y measure f o r t h e module can t h e n be computed
based upon t h e impact of t h e s e m o d i f i c a t i o n s of t h e
s i m p l e s t t y p e on t h e module. T h i s l o g i c a l si:ab i l i t y measure w i l l a c c u r a t e l y p r e d i c t t h e i n p a c t
o f t h e s e simple m o d i f i c a t i o n s on t h e program and,
t h u s , can b e u t i l i z e d t o compute t h e l o g i c a l s t a b i l i t y of t h e module w i t h r e s p e c t t o t h e simple
modifications.
I t i s strongly believed t h a t t h i s
l o g i c a l s t a b i l i t y measure based upon t h e simple
modifications i s closely correlated t o the logical
s t a b i l i t y measure f o r t h e module based upon ~ l l
p o s s i b l e t y p e s of m o d i f i c a t i o n s .
This correlation
should e x i s t because r e g a r d l e s s of t h e complexity
of t h e maintenance a c t i v i t y , i t b a s i c a l l y c o n s i s t s
of m o d i f i c a t i o n s t o v a r i a b l e s i n t h e modules.
Thus, a r e a s o n a b l e e s t i m a t e of module l o g i c a l s t a b i l i t y can b e computed based upon t h e impact of
s o f t w a r e m o d i f i c a t i o n s of t h e s i m p l e s t t y p e of t h e
module.
Due t o t h e n a t u r e of t h e l o g i c a l s t a b i l i t y of
a module, a n a n a l y s i s of t h e p o t e n t i a l l o g i c a l
r i p p l e e f f e c t i n t h e program must b e conducted.
There a r e two a s p e c t s of t h e l o g i c a l r i p p l e e f f e c t
which must b e examined.
One a s p e c t concerns i n t r a module e r r o r flow. T h i s i n v o l v e s t h e flow of p r o gram changes w i t h i n t h e module a s a consequence o f
t h e m o d i f i c a t i o n . The o t h e r a s p e c t concerns i n t e r module e r r o r flow. T h i s i n v o l v e s t h e flow of p r o gram changes. a c r o s s module b o u n d a r i e s a s a consequence of t h e m o d i f i c a t i o n .
Intramodule e r r o r flow i s , u t i l i z e d t o i d e n t i f y t h e s e t Zi of i n t e r f a c e v a r i a b l e s which a r e
a f f e c t e d by l o g i c a l r i p p l e e f f e c t a s a consequence
of a m o d i f i c a t i o n t o v a r i a b l e d e f i n i t i o n i . This
r e q u i r e s a n i d e n t i f i c a t i o n of which v a r i a b l e s cons t i t u t e t h e module's i n t e r f a c e s and a c h a r a c t e r i z a t i o n of t h e p o t e n t i a l i n t r a m o d u l e e r r o r f l o h
among t h e v a r i a b l e s i n t h e module. The v a r i a b l e s
t h a t c o n s t i t u t e t h e module's i n t e r f a c e s c o n s i s t of
i t s g l o b a l v a r i a b l e s , i t s o u t p u t p a r a m e t e r s and
i t s variables u t i l i z e d a s input parameters t o
c a l l e d modules.
Each u t i l i z a t i o n of a v a r i a b l e a s
an i n p u t parameter t o a c a l l e d module i s r e g a r d e d
a s a unique i n t e r f a c e v a r i a b l e . Thus, i f v a r i a b l e

I t should b e noted t h a t t h e s t a b i l i t y measu r e s b e i n g d e s c r i b e d a r e not i n themselves i n d i c a t o r s of program m a i n t a i n a b i l i t y . A s p r e v i o u s l y
mentioned, program s t a b i l i t y i s a s i g n i f i c a n t f a c t o r c o n t r i b u t i n g t o program m a i n t a i n a b i l i t y .
Although t h e measures b e i n g d e s c r i b e d e s t i m a t e p r o gram s t a b i l i t y , t h e y must b e u t i l i z e d i n c o n j u n c t i o n
w i t h t h e o t h e r a t t r i b u t e s a f f e c t i n g program maint a i n a b i l i t y . For example, a s i n g l e module program
of 20,000 s t a t e m e n t s w i l l p o s s e s s an e x c e l l e n t p r o gram s t a b i l i t y measure s i n c e t h e r e cannot b e any
The m a i n t a i n a b i l i t y
r i p p l e e f f e c t among modules.
of t h e program, however, w i l l probably be q u i t e
poor.

676

x i s u t i l i z e d a s a n i n p u t p a r a m e t e r i n two module

The s i m p l e s t measure f o r L C q would b e t h e
number of modules i n v o l v e d i n t h e i n t e r m o d u l e e r r o r
flow a s a consequence o f modifying i . T h i s measure
p r o v i d e s a c r u d e measure of t h e amount of e f f o r t
r e q u i r e d t o a n a l y z e t h e program t o i n s u r e t h a t t h e
m o d i f i c a t i o n does n o t i n t r o d u c e any i n c o n s i s t e n c y
i n t o t h e program. O t h e r measures which examine n o t
o n l y t h e number of modules i n v o l v e d i n t h e i n t e r module e r r o r f l o w , b u t a l s o t h e i n d i v i d u a l complexi t y of t h e modules p r o v i d e more r e a l i s t i c measures
of t h e amount of e f f o r t r e q u i r e d t o a n a l y z e t h e
program t o i n s u r e t h a t i n c o n s i s t e n c i e s a r e n o t
i n t r o d u c e d . One such e a s i l y computed measure i s
McCabe's c y c l o m a t i c number [ 7 1 . U t i l i z i n g t h e
c y c l o m a t i c number o r any o t h e r complexity measure,
t h e complexity of m o d i f i c a t i o n of v a r i a b l e d e f i n i t i o n i can t h e n b e computed a s f o l l o w s :

i n v o c a t i o n s , t h e n each o c c u r r e n c e of x i s r e g a r d e d
a s a unique i n t e r f a c e v a r i a b l e .
Each o c c u r r e n c e
must b e regarded a s a s e p a r a t e i n t e r f a c e v a r i a b l e
s i n c e t h e complexity of a f f e c t i n g each o c c u r r e n c e
of t h e v a r i a b l e a s w e l l a s t h e p r o b a b i l i t y of
a f f e c t i n g each o c c u r r e n c e may d i f f e r .
Once a n i n t e r f a c e v a r i a b l e i s a f f e c t e d , t h e
flow of program changes c r o s s e s module b o u n d a r i e s .
Intermodule e r r o r flow i s t h e n u t i l i z e d t o compute
t h e s e t X . which c o n s i s t s of t h e modules i n v o l v e d
J
i n i n t e r m o d u l e e r r o r flow a s a consequence of
a f f e c t e d i n t e r f a c e v a r i a b l e j . A module i s d e f i n e d
t o b e i n i n t e r m o d u l e e r r o r flow a s a consequence of
a f f e c t e d i n t e r f a c e v a r i a b l e j according t o the
following c r i t e r i o n :

*

I f j i s a g l o b a l v a r i a b l e , t h e n a l l modules
u t i l i z i n g j a r e defined t o be i n t h e i n t e r module e r r o r flow.

LCMi =

c

tmi

Ct

where Ct i s t h e complexity of module t .
4

'k

I f j i s an i n p u t parameter t o a c a l l e d
module, t h e n t h e c a l l e d module i s d e f i n e d
t o b e i n t h e i n t e r m o d u l e e r r o r flow.

S i n c e t h e l o g i c a l s t a b i l i t y of a module i s
defined a s the resistance t o the potential logical
r i p p l e e f f e c t of a m o d i f i c a t i o n t o a v a r i a b l e d e f i n i t i o n i on o t h e r modules i n t h e program, t h e
p r o b a b i l i t y p ( i ) t h a t i w i l l b e s e l e c t e d f o r modif i c a t i o n must b e d e t e r m i n e d . Now a b a s i c assumpt i o n of t h e s i m p l e s t t y p e of maintenance a c t i v i t y
i s t h a t a m o d i f i c a t i o n c a n occur w i t h e q u a l proba b i l i t y a t any p o i n t i n t h e module. T h i s i m p l i e s
t h a t each o c c u r r e n c e of each v a r i a b l e d e f i n i t i o n
h a s a n e q u a l p r o b a b i l i t y of b e i n g a f f e c t e d by t h e
maintenance a c t i v i t y . Thus, f o r each module w e
can c a l c u l a t e t h e number of v a r i a b l e d e f i n i t i o n s .
I f t h e same v a r i a b l e i s d e f i n e d t w i c e w i t h i n a
module, each d e f i n i t i o n i s r e g a r d e d a s s e p a r a t e .
The p r o b a b i l i t y t h a t a m o d i f i c a t i o n t o a module
w i l l affect a particular variable definition i n
t h e module c a n t h e n b e computed a s l/(number of
v a r i a b l e d e f i n i t i o n s i n t h e module).

I f j i s a n o u t p u t p a r a m e t e r of a module k ,
t h e n a l l t h e modules which invoke k a r e
d e f i n e d t o be i n t h e i n t e r m o d u l e e r r o r
flow.

T h i s d e f i n i t i o n of i n t e r m o d u l e e r r o r flow i s ~IJ
corresponding t o a worst-case l o g i c a l r i p p l e e f f e c t
a s a consequence of a f f e c t e d i n t e r f a c e v a r i a b l e j.
I n s t e a d , t h e program i s viewed a s a c o l l e c t i o n of
modules, and each module i s viewed a s a b l a c k box.
I n a worst-case l o g i c a l r i p p l e e f f e c t , i f the i n p u t
t o a module i s a f f e c t e d , t h e n t h e c o r r e s p o n d i n g
o u t p u t s which a r e a f f e c t e d a r e u t i l i z e d t o c o n t i n u e
t r a c i n g r i p p l e e f f e c t . The d e f i n i t i o n of i n t e r module e r r o r flow p r e s e n t e d i n t h i s p a p e r t a k e s t h e
more r e a l i s t i c approach t h a t i f t h e i n p u t t o a modu l e i s a f f e c t e d , t h e n t h e module w i l l b e m a i n t a i n e d
i n such a manner a s t o p r e s e r v e i t s i n t e r f a c e s w i t h
t h e o t h e r modules i n t h e program.

Thus, a measure f o r t h e p o t e n t i a l l o g i c a l
r i p p l e e f f e c t of a m o d i f i c a t i o n of t h e s i m p l e s t
t y p e t o a module k , denoted by LREk, can b e
defined a s follows:

Both i n t r a m o d u l e and i n t e r m o d u l e e r r o r flows
must b e u t i l i z e d t o compute t h e expected impact o f
a m o d i f i c a t i o n of t h e s i m p l e s t t y p e on o t h e r modu l e s i n t h e program. A measure i s needed t o e v a l u a t e t h e magnitude of t h i s l o g i c a l r i p p l e e f f e c t
which o c c u r s as a consequence of modifying a v a r i a b l e d e f i n i t i o n . T h i s measure must b e a s s o c i a t e d
w i t h each v a r i a b l e d e f i n i t i o n i n o r d e r t h a t t h e
impact of modifying t h e v a r i a b l e d e f i n i t i o n d u r i n g
maintenance can b e determined.
T h i s l o g i c a l comp l e x i t y of m o d i f i c a t i o n f i g u r e w i l l b e computed f o r
each v a r i a b l e d e f i n i t i o n i , and i s denoted by LCM..
There a r e many p o s s i b l e measures which may b e useh
f o r LCMi.
A l l of t h e s e measures a r e dependent upon
computation of t h e modules i n v o l v e d i n i n t e r m o d u l e
e r r o r flow a s a consequence of modifying i . The
modules i n v o l v e d i n i n t e r m o d u l e e r r o r flow a s a consequence of modifying i can b e r e p r e s e n t e d by t h e
s e t Wi which i s c o n s t r u c t e d as f o l l o w s :

wi=

U

jcz i

LREk =

E

[P(i)*LCMi],

i EVk

where Vk i s t h e s e t of a l l v a r i a b l e d e f i n i t i o n s i n
module k .
A measure f o r t h e l o g i c a l s t a b i l i t y of a
module k , d e n o t e d by LSk, can b e e s t a b l i s h e d a s
follows:

LSk = l / L R E k .
Development of a Program L o g i c a l S t a b i l i t y Measure
A measure f o r t h e p o t e n t i a l l o g i c a l r i p p l e
e f f e c t of a m o d i f i c a t i o n of t h e s i m p l e s t t y p e t o a
program, denoted by LREP, can e a s i l y b e e s t a b l i s h e d
by c o n s i d e r i n g i t a s t h e expected v a l u e of LREk
o v e r a l l t h e modules i n t h e program. Thus, w e
have

x
j

677

n

i of module k.

Wki

i s formed a s f o l l o w s :

[ P ( k ) *LREk],

LREP =
k=l

where P(k) i s t h e p r o b a b i l i t y t h a t a m o d i f i c a t i o n
t o module k may o c c u r , and n i s t h e number of
modules i n t h e program.

S t e p 6. F o r each v a r i a b l e d e f i n i t i o n i , i n every
module k , compute LCMki a s f o l l o w s :

A measure f o r t h e l o g i c a l s t a b i l i t y of a program denoted by LSP can t h e n be e s t a b l i s h e d a s
f 01lows :

=

?'"kitty

where Ct i s t h e McCabe’s complexity measure of
modulet

LSP = l/LREP.

.

A b a s i c assumption of t h e s i m p l e s t t y p e of
maintenance a c t i v i t y i s t h a t a m o d i f i c a t i o n can
o c c u r with e q u a l p r o b a b i l i t y t o any module and a t
any p o i n t i n t h e module. U t i l i z i n g t h i s assumpt i o n , the probability t h a t a modification w i l l
a f f e c t a p a r t i c u l a r module c a n b e computed a s l / n ,
where n i s t h e number of modules i n t h e program.
T h i s assumption can b e r e l a x e d i f a d d i t i o n a l i n f o r mation r e g a r d i n g t h e program i s a v a i l a b l e . For
example, i f t h e program h a s o n l y r e c e n t l y been
r e l e a s e d and i t i s b e l i e v e d t h a t a s i g n i f i c a n t p a r t
of t h e maintenance a c t i v i t y w i l l i n v o l v e e r r o r
c o r r e c t i o n , then t h e p r o b a b i l i t i e s t h a t p a r t i c u l a r
modules may b e a f f e c t e d by a m o d i f i c a t i o n may b e
a l t e r e d t o r e f l e c t the p r o b a b i l i t i e s t h a t e r r o r s
i n t h e s e modules may b e d i s c o v e r e d .
T h i s can b e
accomplished by u t i l i z i n g some complexity o r s o f t ware s c i e n c e measures [8].

S t e p 7 . F o r each v a r i a b l e d e f i n i t i o n i i n e v e r y
module k , compute t h e p r o b a b i l i t y t h a t a p a r t i c u l a r
v a r i a b l e s d e f i n i t i o n i of module k w i l l be s e l e c t e d
f o r m o d i f i c a t i o n , denoted by a s f o l l o w s :

ALGORITHM FOR THE COMPUTATION OF THE
LOGICAL STABILITY MEASURES

where P(k) = l / n , and n i s t h e number of modules i n
t h e program.

P ( k i ) = l / ( t h e number of e l e m e n t s i n Vk).
S t e p 8.

F o r each module k , compute

K

and
LSk = l/LREk.
S t e p 9.

Compute
n

LREP =

c [P(k)*LREk],
k=l

I n t h i s s e c t i o n , an a l g o r i t h m w i l l be o u t l i n e d
f o r t h e computation of t h e s e l o g i c a l s t a b i l i t y
measures. The f o l l o w i n g d e s c r i p t i o n of t h i s a l g o r i t h m assumes t h a t t h e r e does n o t e x i s t any p r i o r
knowledge w h i c h m i g h t a f f e c t t h e p r o b a b i l i t i e s of
program m o d i f i c a t i o n , and McCabe’s complexity
measure [ 7 ] i s u t i l i z e d . The a l g o r i t h m can e a s i l y
be modified t o a l l o w f o r p r i o r knowledge concerni n g t h e p r o b a b i l i t i e s of program m o d i f i c a t i o n o r
t o u t i l i z e a d i f f e r e n t complexity measure.
The
a l g o r i t h m c o n s i s t s of t h e f o l l o w i n g s t e p s :
S t e p 1. F o r each module k i d e n t i f y t h e s e t Vk of
a l l v a r i a b l e d e f i n i t i o n s i n module k.
S t e p 2 . F o r each module k i d e n t i f y t h e s e t Tk of
a l l i n t e r f a c e v a r i a b l e s i n module k.
S t e p 3. F o r each v a r i a b l e d e f i n i t i o n i i n e v e r y
module k , compute t h e s e t z k i of i n t e r f a c e v a r i a b l e s i n Ti which a r e a f f e c t e d by a m o d i f i c a t i o n t o
v a r i a b l e d e f i n i t i o n i of module k by i n t r a m o d u l e
e r r o r flow [ 6 ] .
S t e p 4. For each i n t e r f a c e v a r i a b l e j i n e v e r y
module k , compute t h e s e t x k j c o n s i s t i n g of t h e
modules i n i n t e r m o d u l e e r r o r flow a s a consequence
of a f f e c t e d i n t e r f a c e v a r i a b l e j of module k.

S t e p 10.

The l o g i c a l s t a b i l i t y of t h e program i s
LSP = l/LREP.

APPLICATIONS OF THE LOGICAL STABILITY MEASURES
The l o g i c a l s t a b i l i t y measures p r e s e n t e d h e r e
c a n b e used f o r t h e s e l e c t i o n of a p a r t i c u l a r maintenance p r o p o s a l f o r implementation, and t h e s e l e c t i o n of t h e p o r t i o n s of a program when r e s t r u c u t r i n g i s performed f o r improving t h e m a i n t a i n a b i l i t y
of t h e program. For example, i f t h e l o g i c a l s t a b i l i t y of a p a r t i c u l a r module i s t o o low, t h e n
m o d i f i c a t i o n s t o t h a t module should be avoided
d u r i n g t h e maintenance phase, i f p o s s i b l e . MIDdules
w i t h low l o g i c a l s t a b i l i t y should a l s o b e s e l e c t e d
f o r r e s t r u c t u r i n g i n o r d e r t o improve t h e maint a i n a b i l i t y of t h e program.
EXAMPLE
I n t h i s s e c t i o n t h e l o g i c a l s t a b i l i t y of t h e
program shown i n F i g u r e 2 w i l l b e analyzed.
According t o t h e a l g o r i t h m , we have
LREMAIN = 4 ;

mRROOTS

= 2.9;

LREIROOTS = 2 . 7 .

The l o g i c a l s t a b i l i t y of each of t h e modules i n t h e
program i s g i v e n by

S t e p 5. For each v a r i a b l e d e f i n i t i o n i i n e v e r y
module k , compute t h e s e t wki c o n s i s t i n g of t h e
s e t of modules i n v o l v e d i n i n t e n n o d u l e e r r o r flow
a s a consequence o f modifying v a r i a b l e d e f i n i t i o n

678

The p o t e n t i a l l o g i c a l r i p p l e e f f e c t of t h e program
is

i d e n t i f i c a t i o n of o c h e r s o f t w a r e q u a l i t y a t t r i b u t e s
c o n t r i b u t i n g t o t h e m a i n t a i n a b i l i t y of programs i n
o r d e r t o produce a r e a s o n a b l e program m a i n t a i n a b i l i t y measure.
The s t a b i l i t y measures must a l s o
b e v a l i d a t e d . Due t o t h e complex i n t e r a c t i o n s of
t h e s o f t w a r e q u a l i t y a t t r i b u t e s and human f a c t o r s
a f f e c t i n g t h e maintenance p r o c e s s , t h e s t a b i l i t y
measures w i l l b e v a l i d a t e d d u r i n g t h e v a l i d a t i o n of
o u r m a i n t a i n a b i l i t y measure which i s c u r r e n t l y
u n d e r development. This v a l i d a t i o n w i l l u t i l i z e
maintenance d a t a t o produce c o m p l e x i t i e s of p r o gram m o d i f i c a t i o n s i n o r d e r t o v a l i d a t e t h e p r e d i c t e d m a i n t a i n a b i l i t y measure [ 6 ] . The n e t b e n e f i t s
of t h i s r e s e a r c h should be a s i g n i f i c a n t reduction
i n s o f t w a r e maintenance c o s t and a s u b s t a n t i a l
improvement i n t h e q u a l i t y of t h e program e v e n a f t e r
a n extended p e r i o d of maintenance a c t i v i t i e s s i n c e
fewer e r r o r s might b e i n j e c t e d i n t h e program d u r i n g maintenance due t o i t s improved m a i n t a i n a b i l i t y .

LREP = 3.2,
and hence t h e l o g i c a l s t a b i l i t y of t h e program i s
g i v e n by
LSP = 0.31.
These measures i n d i c a t e t h a t t h e s t a b i l i t y of
t h e program i n F i g u r e 2 i s e x t r e m e l y poor. An
e x a r i n a t i o n of t h e program p r o v i d e s i n t u i t i v e
s u p p o r t of t h e s e measures s i n c e t h e program u t i l i z e s common v a r i a b l e s i n e v e r y module a s w e l l a s
s h a r e d i n f o r m a t i o n i n t h e form of p a s s e d parameters.

C MODULE M A I N
C SOLUTION OF THE QUADRATIC EQUATION
C A*X*X+B*Xi€
=0
COMMON HRl,HR2,HI
READ 100 (A,B,C)
100 FORMAT (3F10.4)
H 1 = -B/(2.*A)
HR = Hl*Hl
DISC = HR
CfA
CSID = Hl*Hl
C/A
CALL RROOTS (CSID,DISC,Hl)
WRITE 100 HRl,HR2,HI
END

-

REFERENCES
" P e r s p e c t i v e s on Software
Zelkowitz, M.V.,
Engineering,'' ACM Computing Surveys, Vol. 1 0 ,
No. 2 , June 1978, pp. 197-216.
E l s h o f f , J. L . , "An A n a l y s i s of Some Commerical
PL/1 Programs," I E E E Trans. on S o f t . Eng.,
June 1976, pp. 113-120.

-

Soong, N. L., "A Program S t a b i l i t y Measure,"
Proceedings 1977 Annual ACM Conference,
pp. 163-173.

C MODULE RROOTS
SUBROUTINE RROOTS (CSID ,DISC ,H1)
COMMON HRl,HRP,HI
I F (DISC.LT.0) GOTO 1 0
H2 = SQRT (DISC)
HR1 = H1
H2
HR2 = H 1
H2
H I = 0.
10 CONTINUE
CALL IROOTS (CSID,DISC,Hl)
RElURN

Myers, G. J . , R e l i a b l e Software through C o m t s i t e Design, P e t r o c e l l i / C h a r t e r , 1979, pp. 137149.

+
-

McCall, J. A., R i c h a r d s , P. K O , and W a l t e r s ,
G. F., F a c t o r s i n S o f t w a r e Q u a l i t y , Volume 111
P r e l i m i n a r y Handbook on Software Q u a l i t y f o r a n
A c q u i s i t i o n Manager, NTIS AD-A049 055,
November 1977, pp. 2-1 t o 3-7.

C MODULE IROOTS

Yau, S. S . , C o l l o f e l l o , J. S . , and MacGregor,
T . , "Ripple E f f e c t A n a l y s i s of Software Maint e n a n c e , " e o c . COMPSAC 78, pp. 60-65.

SUBROUTINE IROOTS (CSID ,DISC,Hl)
COMMON HRl,HR2,HI
I F (CSID.GE.0) GOTO 1 0
H2 = SQRT(-DISC)
HR1 = H 1
HR2 = H 1
H I = H2
10 CONTINUE
RETURN
F i g u r e 2.

McCabe, T. J., "A Complexity Measure,'!
Trans. on S o f t . Eng., Vol. SE-2, No. 4 ,
December 1976, pp. 308-320.

IEEE

H a l s t e a d , M. H., Elements of Software S c i e n c e ,
E l s e v i e r North-Holland, I n c . , 1977, pp. 84-91.

An Example Program f o r Computing t h e
S t a b i l i t y Measures
CONCLUSION AND FUTURE RESEARCH

I n t h i s p a p e r , program s t a b i l i t y measures have
been p r e s e n t e d and t h e i r a p p l i c a t i o n s have been
discussed.
Much r e s e a r c h remains t o b e done i n t h e
a p p l i c a t i o n of t h e s e s t a b i l i t y measures t o t h e
d e s i g n phase, t h e development o f performance s t a b i l i t y measures, and t h e development o f automated
r e s t r u c t u r i n g t e c h n i q u e s based upon t h e s e measures.
T h i s r e s e a r c h must a l s o b e extended t o t h e

679

Software productivity measurement
by J. S. COLLOFELLO, S. N. WOODFIELD, and N.E. GIBBS
Computer Science Department
Arizona State University

ABSTRACT
Productivity is a crucial concern for most organizations. This is especially true for
software development organizations. Although the term productivity is widely used,
the difficulty of defining it leads to serious problems in productivity measurement.
This paper will attempt to survey some current productivity measures for software
development organizations and discuss their deficiencies. A theoretical productivity
model that overcomes these deficiencies will also be presented. A practical productivity measure that exceeds current measures by including a quality component will
also be described. Although this measure is only a small improvement over contemporary measures, it is a promising step in the direction of better productivity
measurement.

757

Software Productivity Measurement

759

INTRODUCTION

STATE-OF-THE-ART PRODUCTIVITY MEASURES

Productivity has become a major concern for most organizations attempting to survive in today's competitive market.
Productivity improvements are being sought to produce a
higher quality product at a lower cost. To assess the effect of
steps taken to improve productivity, it is first necessary to
have an objective measure. This measure normally assumes
the form of a ratio of acceptable products developed to unit of
time. Thus, measures such as manufactured components per
hour are typical.
Productivity is a major concern for software development
organizations. As in other organizations, productivity improvements are being sought to produce higher quality software at reasonable costs. High-level programming languages,
software engineering techniques and tools, and new project
management methods are all being proposed as ways to improve productivity. For an organization to assess the effects of
these new approaches, an objective software development
productivity measure is needed. With this type of measure a
software development organization can assess its current
productivity and determine if and where improvements are
needed. Appropriate new techniques can then be applied and
a determination made whether productivity was increased by
the new techniques.
Productivity measures can also be used by many organizations for comparative purposes. Comparisons may be made of
productivity differences between individuals, projects, or
competitive organizations. In software development wide
variations of productivity are common. Observations of large
productivity differences for individuals performing the same
task have been cited. When productivity is used to evaluate
different software organizations, this comparison must be performed carefully. The reason for this is that although all software developers are producing programs, their products may
be no more comparable than transportation manufacturers'
products, such as bicycles and aircraft.
Another major use of productivity measures occurs in the
product development time and cost estimation phase. An organization must have an estimate of production productivity
to project development time and expense adequately. Although software development is often regarded more as research and development rather than manufacturing, costs
must nevertheless be estimated. Although still in their infancy, most software cost estimation models require an estimate of the organization's software development productivity
as a major input.
Thus, productivity measures are needed for most organizations, including software development organizations. The remainder of this paper will focus on productivity measurement
for software development.

Several different approaches to software productivity measurement are currently used. Historically, the two major approaches are lines of code per programmer month, which
expresses productivity in terms of work units; and cost per line
of code, which expresses productivity in terms of cost units.
These productivity measures can be used to assess the productivity of the entire software development effort. Other
productivity measures for specific tasks in the software life
cycle can also be used, such as pages of documentation per
programmer month and number of test case runs per programmer month.
Another approach to productivity measurement is the software science approach.1 In this approach lines of code are
decomposed into operators and operands in an attempt to
produce a more invariant measure than lines of code. Based
upon some ratios and counts of unique operators and operands and total numbers of operators and operands, some
interesting work and complexity estimates have been
formulated.
A new approach to productivity measurement, which abandons the lines-of-code approach completely, is quantifying the
functions of programming.2 This approach concentrates on
the number of external user inputs, inquiries, outputs, and
master files to be dealt with by the program and uses a weighting approach to calculate a dimensionless number called a
function point. A variation of this approach is a cost-per-function method, in which a function is defined as a program
segment with a single input and output that performs a single
transformation or action.3
Of the major approaches to productivity measures just discussed, the measure applied most frequently is lines of code
per programmer month. This measure as well as the others
have several severe deficiencies, which will now be discussed.
DEFICIENCIES OF CURRENT PRODUCTIVITY
MEASURES
One of the biggest problems with software productivity is the
difficulty of measuring the work accomplished. In other industries, work can easily be measured in terms of products, such
as radios produced per unit of time. In software the products
are not as easy to quantify. This fact has left the lines-of-code
measure as the traditional unit of work. This measure, as well
as the software science and functionality types of measures, all
suffer from the same problem of not representing adequate
work units. Since the difficulty in developing software depends upon the problem being solved, implementation-based
metrics such as lines of code, software science size or complexity measures, or function counts will vary with the complexity

760

National Computer Conference, 1983

of the task. Thus, all current software productivity measures
fail to take into account the problem size of the task being
performed.
Another major deficiency with contemporary measures is
the difficulty of extending them into the maintenance phase.
One measure for estimating maintenance productivity, reported by Boehm,4 consists of the average number of instructions that can be modified per man-month of maintenance effort. Modified instructions consist of new instructions
or changed instructions. This measure suffers from the same
deficiencies of the lines-of-code measure discussed for software development, and it also presents difficulties during
maintenance. Since modification of a line of code requires
understanding the program, changing it, and accommodating
to a possible ripple effect, this measure may be a poor estimate of the amount of work performed.5
Another severe problem of all current productivity measures is their failure to incorporate a quality component. The
quality of a product must be reflected in productivity measurement. For example, if two organizations achieve the same
lines-of-code measure for identical problems, but one organization produces a flawless system and the other is plagued
with errors, the software productivity of the organizations
should not be regarded as equal. This implies that defect
removal costs must be factored into the productivity calculation. In general, other quality factors must also be incorporated into the productivity calculation, and none of them are
included in any current measures.
Current productivity measures thus have several deficiencies that have prevented their being reliable or widely used. In
the next section a proposed theoretical productivity model
will be discussed that overcomes these deficiencies.
PROPOSED PRODUCTIVITY MODEL
For a productivity measure to gain universal acceptance, the
problems discussed in the previous section must be resolved.
The magnitude of these problems is so great that many feel
that the development of such a measure is nearly impossible.
Despite the difficulty of solving these problems, it is still essential to identify the characteristics of the ideal solution so
that a direction of research can be planned and progress
charted toward an eventual solution. It is to attain this objective that a theoretical productivity model is proposed.
The proposed productivity model is calculated as a function
of problem size, resources consumed in production, and quality of the product produced. This productivity will be denoted
as:
P=f(PS,RC,

Q)

where PS is the problem size, RC is the resources consumed,
and Q is the quality of the product.
The problem size component of this productivity measure is
meant to correspond to the magnitude and complexity of the
task under development. It replaces the lines-of-code and
functions measures of work in current productivity measures.
This concept is essential to avoiding the current pitfall of

productivity measures: rewarding long, inelegant solutions to
problems. For example, the current lines-of-code productivity
measure would conclude that a programmer who writes a
2000-lines-of-code solution to a problem in 1 month is twice as
productive as a programmer who writes a 1000-lines-of-code
equivalent solution to the same problem in 1 month. If, instead of lines of code, the problem size is used as a measure
of work, the productivity of the two programmers would be
regarded as equal, assuming the product was of equal quality.
The problem size for a particular software development
effort must be calculated at the requirements level. This is
essential, because problem size is independent of implementation size. This notion of problem size rewards cost- and
timesaving measures such as reusing code and identifying simple and elegant solutions. The problem size notion is also
consistent with the way productivity is measured in other industries. For example, in the automotive industry, productivity is measured by the assembly time of a car; the car
corresponds to the problem size. It is not normally measured
by the number of welds per hour, since this is an implementation metric. The number of welds per hour is, however,
analogous to the lines-of-code measures; and they both significantly affect productivity once a development approach is
selected. The key here is that productivity measurement must
focus upon the problem size and not upon implementation
approaches. The implementation approach is a factor that
affects productivity but does not serve as a measure of work
in its calculation.
The resources-consumed component of this productivity
measure is equivalent to that used in contemporary measures.
Thus, this measure may correspond to either a time or a cost
calculation of the resources consumed to complete the development of a particular problem size. The time component
may be measured in programmer months and the cost component in dollars.
The quality component of this productivity measure recognizes the necessity of evaluating the quality of a product developed. This concept is essential to avoiding the current pitfall
of productivity measures: encouraging quantity of code produced without taking into account the reliability of the resulting sofware product. Unless the assumption is made that
all software produced is of the same quality, this component
must be a part of any productivity measure.
The measurement of software quality is a complex task. In
fact, it is impossible to establish a single figure for software
quality, since it has many attributes. Some typical software
quality attributes are correctness, flexibility, portability, reliability, efficiency, integrity, and maintainability.6 In a given
software development effort, some of these quality attributes
may be stressed more than others. Several of the quality attributes, such as efficiency and portability, may also be in conflict with each other. The point here is that a single measure
of software quality is unlikely; instead, software quality must
be examined in terms of attributes affecting it.
Ths quality component and the problem size component of
the proposed productivity measure must be combined to form
a measure of the work to be performed. Thus, for a given
problem size, the quality desired will determine the actual
amount of work to be done. For example, consider a scientific

Software Productivity Measurement

program. If this program were placed on a spacecraft, reliability requirements would cause a larger amount of work to be
expended than if the same program did not have these stringent requirements or was used in a different application.
Thus, quality and problem size must be combined to form a
measure of work in the productivity model.
The proposed productivity model combines these components to form a relationship between work accomplished
and resources consumed in the same manner as contemporary
productivity measures, but without their deficiencies. Quality
is a definite component of this model; therefore, if a product
does not meet its quality requirements, the additional rework
expenses are factored into the model by adding them to the
resources consumed in order to lower the productivity of the
developers. Thus, the productivity model does not reward
hasty work of poor quality.
It should be noted that this model provides an absolute
measure of productivity as a function of work accomplished
and resources consumed. This implies all software development organizations can be equally productive, regardless of
the product under development. This contrasts greatly with
the lines-of-code productivity measures, which expect wide
variances of productivity, depending on the application.7 This
absolute nature of the productivity model stems from the fact
that work is being measured as a function of problem size and
quality. There are many factors, however, that may affect this
absolute productivity, such as development tools and techniques, management approaches, and computer resources.8
These are factors that can be manipulated by an organization
in an effort to improve productivity. They are not, however,
factors to be included in the productivity calculation.
The proposed productivity model is, of course, currently at
a theoretical level. The major difficulty lies in the measurement of problem size and software quality. Once acceptable measures for these two factors are found or reasonably
estimated, another problem will be combining these factors
into an acceptable measure of work. These are formidable
tasks for researchers, with potential spinoffs to many other
facets of computer science. The goals of the research, however, are clear; and progress can be charted against these
objectives.
PRAGMATIC FIRST STEP TOWARD THE
THEORETICAL PRODUCTIVITY MODEL
The proposed productivity model presented in the last section
is not usable by current software developers. In this section a
pragmatic first step toward the theoretical productivity model
calculable today will be described. This productivity measure
attempts to go beyond current measures by incorporating a
quality component, which will be very simple and will concentrate on the reliability of the software. Other software quality
characteristics will be ignored. The productivity measure will
also not attempt to calculate a problem size, but instead can
use any contemporary measure of work, such as lines of code.
Thus, the only significant difference between this productivity
measure and current measures is the incorporation of the
reliability component. The reliability component will be calculated as the amount of rework time required to fix devel-

761

opment errors. This rework time includes both debugging
time and the cost of removing the error. Thus, productivity
will be calculated as follows:
Productivity

work accomplished
development resources consumed
+ rework resources consumed

The work accomplished can be measured in lines of code or
by some measure of functionality currently in use. The resources consumed can be programmer months or any other
suitable measure. The rework costs are the resources consumed by correction of errors in the development process.
This information is available from the error reports filed
against the development effort. This productivity measure
penalizes hasty efforts that produce many errors. This productivity measure also supports current software engineering development methodologies that call for extensive reviews
throughout the life cycle. These reviews take time, and with
current productivity measures the added time detracts from
productivity. Under the proposed productivity model, these
reviews can increase productivity as long as they are cost
effective.
Thus, the proposed productivity measure can be used in an
organization to assess its current productivity and measure the
effect of development improvement on productivity. This
measure can be calculated at the end of the coding phase or
at the end of the testing phase to assess the productivity of
different life cycle phases. If it is calculated at the end of the
coding phase, then error rework costs incurred during the
testing and maintenance phases are factored into the productivity equation. If it is calculated at the end of the testing
phase, then some interval may be selected, such as the next
release, where error rework costs will be calculated.
This productivity measure may also be used (with caution)
to compare productivity with that of other organizations. In
addition to comparing work measures across organizations
(which presents potential problems), error detection and correction mechanisms in the organizations must be examined.
They must be examined because the rework component due
to errors in the productivity calculation depends on error
detection and removal costs. Thus, the productivity of two
development organizations producing identical software with
identical resources consumed may vary if the error detection
and correction tools and techniques of the organizations vary.
This forces meaningful productivity comparisons only among
organizations with equivalent error detection and correction
mechanisms.
Although this proposed productivity measure suffers many
of the same problems as other contemporary measures and is
limited in scope, the measure does advance the state of productivity measurement to include one facet of software quality. It is hoped that future research efforts will continue to
progress toward the development of a complete productivity
measure similar to the model proposed in this paper.
FUTURE RESEARCH
We plan to continue our research efforts on improving our
pragmatic productivity measure. An attempt is being made to

762

National Computer Conference, 1983

include other quality factors besides reliability in its calculation. The problem size calculation problem is also being
addressed. This research has led us into an examination of
cost estimation models. Finally, the feasibility of productivity
calculation based upon predictive quality metrics, which
would enable productivity measurements to take place earlier, is being examined.
REFERENCES
1. Halstead, M. H. Elements of Software Science. New York: North-Holland,
1977.
2. Albrecht, A. J. "Measuring Application Development Productivity." In

Proceedings of the Joint Share/Guide/IBM Application Development Symposium, 1979, pp. 83-92.
3. Crossman, T. D. "Taking the Measure of Programmer Productivity." Datamation, 25 (1979), pp. 144-147.
4. Boehm, B. W. Software Engineering Economics. Englewood Cliffs, N.J.:
Prentice-Hall, 1981, pp. 533-553.
5. Collofello, J. S., and S. S. Yau. "Some Stability Measures for Software
Maintenance." IEEE Transactions on Software Engineering, Vol. SE-6
(1980), pp. 545-553.
6. Boehm, B. W. Characteristics of Software Quality. New York: NorthHolland, 1978.
7. Sommerville, I. Software Engineering. Reading, Mass.: Addison-Wesley,
1982.
8. Walston, C. E., and C. P. Felix. "A Method of Programming Measurement
and Estimation." IBM Systems Journal, 10 (1977), pp. 10-29.

Using Simulation to Facilitate the Study of Software Product Line Evolution1
Yu Chen†, Gerald C. Gannod‡2, James S. Collofello†, and Hessam S. Sarjoughian†
†
‡
Dept. of Computer Science and Engineering
Division of Computing Studies
Arizona State University – Tempe Campus
Arizona State University – Polytechnic Campus
Tempe AZ 85287, USA
Mesa AZ 85212, USA
{yu_chen, gannod, collofello, sarjoughian}@asu.edu
Abstract
A product line approach is a disciplined methodology
for strategic reuse of source code, requirement
specifications, software architectures, design models,
components, test cases, and the processes for using the
aforementioned artifacts. Software process simulation
modeling is a valuable tool for enabling decision making
for a wide variety of purposes, ranging from adoption
and strategic management to process improvement and
planning. In this paper, discrete event simulation is used
to provide a framework for the simulation of software
product line engineering.
We have created an
environment that facilitates strategic management and
long-term forecasting with respect to software product
line development and evolution.

1. Introduction
A software product line is defined as a set of softwareintensive systems sharing a common, managed set of
features that satisfy the specific needs of a particular
market segment or mission and are developed from a
common set of core assets in a prescribed way [8]. A
product line approach is a disciplined methodology for
strategic reuse of source code, requirement specifications,
software architectures, design models, components, test
cases, and the processes for using the aforementioned
artifacts. Software product line engineering promises
large-scale productivity gains, shorter time-to-market,
higher product quality, increased customer satisfaction,
decreased development and maintenance cost [8].
However, those benefits are not guaranteed under all
situations, and they are affected by many factors such as
the initiation situation, the adoption and evolution
approaches, the market demands, and the available
resources. The goal of this research is to develop a
1
2

simulator that facilitates software product line decision
making at an early stage by providing time and cost
estimates under various situations.
In this paper, discrete event simulation theory and
Constructive
Product
Line
Investment
Model
(COPLIMO) [2] are used to create an environment that
facilitates strategic management and long-term
forecasting with respect to software product line
development and evolution. Specifically, the simulator
facilitates the study of the effect of a number of process
decisions, including choice of evolution approach, upon
factors such as effort and time-to-market. The simulator
not only gives statistical results at the end of the
simulation, but also visually presents how major product
line engineering activities progress and interact over
time. The simulator is built upon DEVSJAVA [1], a
general-purpose Java-based discrete event simulation
framework. The tool is extensible and allows other
simulation frameworks and cost models to be used.
The remainder of this paper is organized as follows.
Section 2 presents background information. Section 3
describes the simulation model and the simulator.
Results are discussed in Section 4. Section 5 contains
related work and Section 6 draws conclusions and
suggests future investigations.

2. Background
This section describes background information on
Software Product Lines, software process simulation,
DEVSJAVA [1], and COPLIMO [2].

2.1. Software product lines
Software product line development involves three
essential activities: core asset development, product
development, and management [8].
Core asset

This material is based upon work supported by the National Science Foundation under grant No. CCR-0133956.
Contact author.

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

development (domain engineering) involves the creation
of common assets and the evolution of the assets in
response to product feedback, new market needs, etc.
Product development (application engineering) creates
individual products by reusing the common assets, gives
feedback to core asset development, and evolves the
products.
Management includes technical and
organizational
management,
where
technical
management is responsible for requirement control and
the coordination between core asset and product
development activities.
There are two main software product line adoption
approaches: big bang and incremental [10]. With the big
bang approach, core assets are developed for a whole
range of products prior to the creation of any individual
product. With the incremental approach, core assets are
incrementally developed to support the next few
upcoming products. In general, the big bang approach
has a higher return on investment but involves more
risks, while the incremental approach has lower entry
costs but higher total costs. The four common software
product line adoption situations are: independent,
project-integration, reengineering-driven, and leveraged
[10]. Under the independent situation, a product line is
created without any pre-existing products. Under the
project-integration situation, a product line is created to
support both existing and future products. Under a
reengineering-driven scenario, a product line is created
by reengineering existing legacy systems. And the
leveraged situation is where a new product line is created
based on some existing product lines.
Some common product line evolution strategies are:
infrastructure-based, branch-and-unite, and bulkintegration [10]. The infrastructure-based strategy does
not allow deviation between the core assets and the
individual products, and requires that new common
features be first implemented into the core assets and
then built into products. Both the branch-and-unite and
the bulk-integration strategies allow temporal deviation
between the core assets and the individual products. The
branch-and-unite strategy requires that the new common
features be reintegrated into the core assets immediately
after the release of the new product, while the bulkintegration strategy allows the new common features to
be reintegrated after the release of a group of products.

2.2. Simulation
A software process is a set of activities, methods,
practices, and transformations that people use to develop
and maintain software and associated products, such as
project plans, design documentations, code, test cases,
and user manuals [4]. Adopting a new software process is

expensive and risky, so software process simulation
modeling is often used to reduce the uncertainty and
predict the impact.
Software process simulation
modeling can be used for various purposes and scopes,
and have been supported by many technologies [3]. The
software product line process simulator described in this
paper is for long-term organization strategic
management, and is implemented in DEVSJAVA [1], a
Java implementation of the Discrete Event System
Specification (DEVS) modeling formalism [1].
The external view of a DEVSJAVA model is a black
box with input and output ports. A model receives
messages through its input ports and sends out messages
via its output ports. Ports and messages are the means
and the only means by which a model can communicate
with the external world. A DEVSJAVA model is either
“atomic” or “coupled”. An atomic model is undividable
and generally used to build coupled models. A coupled
model consists of input and output ports, a finite number
of (atomic or coupled) models, and couplings. The
couplings link model ports together and are essentially
message channels. They also provide a simple way to
construct hierarchical models.
To execute atomic and coupled models, DEVSJAVA
uses distinct atomic and coupled simulators that support
incremental simulation model development. These
simulators can execute in alternative settings (i.e.,
sequential, parallel, or distributed). An important feature
of the DEVS framework is the ability for models to
seamlessly execute either in logical or (near) real-time.
Furthermore, due to its availability of its source code and
object-oriented design, DEVSJAVA can be extended to
incorporate domain-specific (e.g., Software Product Line)
logic and semantics.

2.3. COPLIMO
In the simulator, COMPLIMO [2] is used as the cost
model to provide cost estimates. COPLIMO is a
COCOMO II [9] based model for software product line
cost estimation, and has a basic life cycle model and an
extended life cycle model. The basic life cycle model has
two sub-models: a development model for product line
creation and a post-development model for product line
evolution. Although the basic life cycle model has many
simplification assumptions, it is thought to be good
enough for early stage product line trade-off
considerations [2]. The basic model also can be easily
extended to the extended life cycle model, which allows
products have different parameter values instead of the
same values. In the implementation, the cost model is
designed as a plug-in model, thus other cost models can
be plugged in to meet other needs.

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

3. Approach
A simulation framework and a software cost model
were used to develop the simulator.
Although
DEVSJAVA [1] and COMPLIMO [2] are currently used,
they can be replaced by other suitable simulation
frameworks and cost models. This section presents the
abstract software product line engineering model, the
specifics of the simulation models, the assumptions, and
the simulation tool.

products are developed by reusing the existing core
assets, and existing products are updated after the change
of the core assets. Figure 3.1 depicts the process flow.
Costs associated with this approach include core asset
development costs, new product development costs, and
existing product maintenance costs. Compared to the big
bang approach, the incremental approach has higher
product development costs because of the incompleteness
of the core assets, and extra product maintenance costs as
the result of a short-term planning penalty.

3.1. Abstract product line model

Figure 3.2. SPL evolution approaches
Figure 3.1. SPL adoption approaches
Software product line engineering typically involves a
creation phase and an evolution phase [10]. Currently
the simulator provides two options (big bang and
incremental) for the creation stage and two options
(infrastructure-based and branch-and-unite) for the
evolution stage. In the following, we will discuss the
costs associated with those cases in detail.
With the big bang adoption approach, core assets are
first developed to meet the requirements for a whole
range of products. Products are then developed by
reusing the core assets [10]. Figure 3.1 illustrates the
process flow. Costs associated with this approach
include core asset development costs and new product
development costs.
With the incremental adoption approach, the core
assets are incrementally developed to meet the
requirements of the next few upcoming products, new

With the infrastructure-based product line evolution
strategy, the process for building a new product is the
following: core assets are updated by incorporating new
common requirements, and the new product is developed
and existing products are updated. Figure 3.2 shows the
process flow. The COPLIMO [2] basic life cycle model
assumes that a change to a product causes the same
percentage of change on reused code, adapted code, and
product unique code. So if the change rate caused by
new product requirements is α, then the costs for one
product development iteration include the costs of
maintaining the core assets with a change rate of α, the
costs of developing the new product, and the costs of
maintaining existing products with a change rate of α.
With the branch-and-unite product line evolution
strategy, the process for building a new product is the
following: the new product is developed, core assets are
updated to incorporate new common features, and
existing products are updated (including the newly

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

created product). Figure 3.2 illustrates the process flow.
If α is the change rate caused by new product
requirements, then the costs for one product development
iteration include the costs of developing a new product
with (1-α) percentage of the reuse rate, the costs of
maintaining the core assets with a change rate of α, and
the costs of maintaining existing products (including the
newly created one) with a change rate of α. Product
maintenance costs are higher in this case because it has
one more product to update, the newly created one. The
new product is first created with new features that are not
supported by the core assets, then after the core assets are
updated to incorporate the new features the new product
needs to be updated to keep consistent with the core
assets. The new product development costs are also
higher with this approach, because of the lower reuse
rate.

project cannot be started until the requested resources are
granted from the Employee Pool. If the number of
employees in the employee pool is not less than the
requested number of employees, the requested amount of
employees will be granted. Otherwise, if the number of
available employees meets the minimum employee level
(a model parameter, between 5/8 and 1), then the number
of available employees will be granted. In that case, a job
can be started with fewer resources but longer
development time. In other cases, the employee pool will
not grant any resources until enough resources are
returned.

3.2. Model development
Twelve DEVSJAVA [1] models were developed to
model software product line engineering activities.
Figure 3.3 shows a UML diagram depicting the
hierarchical structure of the model.
Some time
constraints are imposed in the simulator: for each atomic
model, jobs are processed one by one in FIFO order (or in
combination with some priority).
The PLPEF (Product Line Process Experimental
Frame) is the top level coupled model and contains a
Product Line Process instance and an Experimental
Frame instance.
The Product Line Process models software product
line engineering activities. It contains an instance of
Technical Management, Core Asset Development, and
Employee Pool, and a finite number of Product
Development instances.
The number of Product
Development models to be included depends on the
number of projected products in the product line. The
Product Line Process receives market demands and
dispatches them to Technical Management. It sends out
requirements (generated by Technical Management and
Maintenance Requirement Generator) and development
reports (generated by Core Asset Development and
Product Development), which can be used for process
monitoring.
The Employee Pool models human resource
management. It receives resource request and resource
return messages, and sends out reply messages to grant
resources.
Currently, Employee Pool manages the
resource requests in either a pure FIFO manner or a
FIFO manner where new development jobs are given
higher priority. Before starting any development activity,
a resource request must be sent to Employee Pool. A

Figure 3.3. Model hierarchical structure
The Product Development models the application
engineering activity. It has a Development instance for
product creation and inter-product synchronization
(development instance), a Development instance for
inner-product maintenance (maintenance instance), and a
Maintenance Requirement Generator instance. When the
Product Development gets the first requirement, the
development instance starts product creation, once that is
done, the Maintenance Requirement Generator sends out
a requirement to maintain the product for N years (the
number of years in the product life cycle), which starts
the maintenance instance.
After N years, the
Maintenance Requirement Generator sends out a stop
message, which stops the maintenance activity and the
acceptance of new development requirements.
The Development models a general software
engineering activity. When a new requirement is
received, Development sends a resource request to the
Employee Pool, waits for the reply from the Employee
Pool, starts developing activity when the resources are
granted, then returns resources to the Employee Pool and
sends a report to Technical Management upon
completion. The Development model will stop accepting
new requirements when it receives a stop message on its
stop port, which means the product reaches the end of its
life cycle and needs to be phased out.
The Maintenance Requirement Generator models
product maintenance requirement generation. Once a
new product is released, it sends out a requirement to
maintain the product for N years (the number of years in

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

the product life cycle), and sends a stop message when
the product reaches the end of the product life cycle.
The Core Asset Development models domainengineering processes. Currently, it is modeled in the
same way as the Development model. The domain
engineering is not modeled as the same as the application
engineering, because in practice technical management
often collects the core asset feedback from product
development and issues the core asset requirements in the
context of product development.
Table 3.1. Behavior of technical management
Stage
Creation

Approach
Big bang

Incremental

Evolution

Infrastructure
-Based

Branch-andUnite

Activities
1. Create core assets if they
do not exist
2. Create new product by
fully reusing core assets
1. Increase core assets if
necessary
2. Create new product by
fully reusing core assets
3. Update existing products
1. Update core assets
2. Create new product by
fully reusing core assets
and updating existing
products (excluding the
newly created product)
1. Create new product by
partially reusing core
assets
2. Update core assets
3. Update existing products
(including newly created
product)

The Technical Management models requirement
generation and control as well as the coordination
between core asset and product development. It receives
market demands (which are processed in FIFO order),
generates requirements for core asset or product
development, and accepts development reports. Which
requirements will be generated, when will they be
generated, and where they will be sent depend on the
selected product line adoption and evolution approaches.
Technical Management coordinates core asset
development and product development activities through
keeping the requirement generation in a certain order.
Table 3.1 summaries the behavior of Technical
Management according to the given strategies.
The Experimental Frame consists of a Market
Demand instance and a Transducer instance. It feeds
Product Line Process with inputs and receives Product
Line Process’ outputs.

The Market Demand models the demands for new
products from the market. It sends out a new product
request after a certain interval, which can be set through
the model parameter, “interval”.
The Transducer observes product line engineering
activities for a certain amount of time. During the
observation period, it receives development requirements
and reports, and tracks the generating and finishing time
of each requirement. At the end of a simulation run it
will output some statistical information to a file.

3.3. Assumptions
In the simulator, we made a number of assumptions as
follows.
1. All the employees have the same capability and can
work on any project.
2. If task B needs to make use of the results from task
A, task B cannot start until task A is finished.
3. Product maintenance starts right after the release of
the product and the maintenance activity holds the
staff until the product is phased out.
The assumptions made by the COPLIMO [2] basic
life cycle model are:
4. All products in the product line have the same size,
the same fractions of reused (black-box reuse) code,
adapted (white-box reuse) code, and product unique
code, and the same values for cost drivers and effort
modifiers.
5. For each product in the product line, the size, the
values of cost drivers, and the values of effort
modifiers remain constant over time. For each
product, the fractions of reused code, the fractions of
adapted code, the fractions of product unique code
remain constant during the time when core assets
stay the same.
6. A change to a product will cause the same
percentage of change to reused code, adapted code,
and product unique code.
Assumption 2 states that concurrency between
interdependent tasks is not supported in the current
version, which will be supported to some extent in the
future. Assumption 4 can be relaxed by using COPLIMO
[2] extended life cycle model, which allows products to
have different parameter values. Assumption 5 can be
relaxed by allowing users to specify the change trend.
Assumption 6 can also be relaxed by allowing users to
specify the change rate on different portions of the
products. Because COPLIMO is currently used as the
underlying cost model, its assumptions are adopted in the
simulator.
If another cost model is used, these
assumptions would be replaced by those made by the new
cost model.

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

Figure 3.4 Simulation tool in execution

3.4. Simulation tool
The simulation tool was developed in Java and runs in
the DEVSJAVA [1] environment. Figure 3.4 shows the
user interface. The upper part of the interface shows the
current running model and its package, which are
“PLPEF” and “productLineProcess”, respectively. The
middle part shows the models and their hierarchical
relationships.
The bottom part contains execution
control components. The “step” button allows running
the simulator step by step, the “run” button allows
executing the simulator to the end, and the “restart”
button allows starting a new simulation run without
quitting the system. The “clock” label displays the
current simulation time in the unit of months, and
selecting the “always show couplings” checkbox will
allow couplings between models to be displayed. The
simulation speed can be manipulated at run time to allow
execution in near real-time or logical time (slower/faster
than real-time).

Figure 3.4 shows that at time 27.783, Core Asset
Development is idle, Products 1 is under initial
development, Product 2 and 3 are waiting for resources,
and Products 4 and 5 are in planning. The messages tell
that Product 2 just received requested resources and
Product 3 just sent out a resource request. Because of the
lack of resources, the Employee Pool cannot grant the
requested resources and is waiting for more resources to
be returned, which in turn puts Product 3 in wait.
Technical Management is idle, Market Demand
generates new product requirement in every 12 months.
Finally, and Transducer is observing the simulation.
This case shows a situation where limited resources cause
development activity delay.
At the end of each simulation run, a result table is
generated similar to Table 3.2. The table has two
sections that are divided by a blank line. The top section
lists the created products, their first release time (FR),
time-to-market (TTM), initial development effort (DPM),
initial development time (DT), accumulated development
and
maintenance
effort
(APM),
accumulated

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

TPM 2810.1
FT
220.1
APM
153.2
TR
20
ATTM
44.9

1
2
3
4
5
6
7

4. Results
In this section some simulation cases are presented to
illustrate the use of the simulator and to demonstrate the
analytical capability of the simulator.

4.1. Overview
The inputs to the simulator include general
parameters and product (core asset) parameters. The
general parameters are used to describe software product
line process attributes and organization characteristics.
These parameters include the maximum number of
products that will be supported by the product line, the
number of products that will be created during the
creation stage, the product line adoption and evolution
approaches, the number of employees in an organization,
and the market demand intervals. The product (core
asset) parameters are primarily determined by the
employed cost model (COPLIMO, in this case). These
parameters include the size of the product (core assets) in
source lines of code, fraction of product unique code,
fraction of reused code, fraction of adapted code,
percentage of design modified, percentage of code
modified, and percent of integration required for
modified software. In addition, a number of parameters
related to reuse and maintenance are included, such as
software understanding of product unique code, software
understanding of adapted code, unfamiliarity with

12
12
12
12
12
12
12

50
40
30
50
50
50
50














Branch-and-unite

AT FR
50.5
3
159.0
4
159.0
4
159.0
4
149.6
3
140.3
2

Infrastructure-based

APM
651.6
443.0
443.0
443.0
424.2
405.4

Resources

DT
27.8
20.3
20.3
20.3
20.3
20.3

Scenario

core
p01
p02
p03
p04
p05

DPM
582.2
217.7
217.7
217.7
217.7
217.7

Incremental

Table 4.1. Scenarios

TTM
27.8
48.1
36.1
44.4
43.8
52.1

Big bang

Table 3.2. Simulation Result
FR
27.8
48.1
48.1
68.4
79.8
100.1

Single product only

product unique code, unfamiliarity with adapted code,
and average change rate caused by new market demands.
To study the effect of resources, adoption approaches,
and evolution approaches on software product line
engineering, we ran the simulator seven times using the
same basic parameter values. Accordingly, we varied the
number of resources, the type of adoption approach, and
type of evolution approach.

Market Demand Interval

development and maintenance time (AT), and the
number of finished requirements (FR). The bottom
section summarizes the total product line evolution effort
(TPM), the time when all the requirements are finished
(FT), the average annual development effort (APM), the
number of requirements generated (TR), and the average
time-to-market (ATM). The unit of effort is personmonths and the unit of time is months.






4.2. Effect of resources
The inputs to Scenarios 1, 2 and 3 only differ in the
values of number of employees (50, 40, and 30,
respectively). The results differ in time-to-market, as
shown in Figure 4.1. At the beginning, there is little
difference, as time progresses, the gap of time-to-market
between
resource-constrained
and
non-resourceconstrained scenarios increases. The reason is that as
more products are developed, more resources for product
maintenance are required, thus less resources are left for
new product development, which may increase the
resource waiting time and in turn result in a longer timeto-market. The effort associated with Scenario 3 is
smaller than the other two cases. That is because in
Scenario 3, when Product 10 is released at 202.69,
Product 1 and 2 are already phased out (at the time
168.1). Accordingly, no effort is needed to update those
two products due to the change of the core.

4.3. Effect of adoption approach
The inputs to Scenarios 1 and 4 only differ in product
line adoption approach (big bang and incremental,
respectively). The results differ in time-to-market, as
shown in Figure 4.2. As specified by the inputs, the core

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

Time-to-Market

100
80
60
40
20
0
1

2

3

4

5

6

7

8

9

10

Products
numEmp=30

numEmp=40

numEmp=50

Time-to-Market

Figure 4.1. Effect of resources
60
50
40
30
20
10
0
1

2

3

4

5

6

7

8

9

10

9

10

Products
Big Bang

Incremental

Figure 4.2. Effect of adoption approach

Time-to-Market

100
80
60
40
20
0
1

2

3

4

5

6

7

8

Products
Branch-and-unite

Infrastructure-based

Figure 4.3. Effect of evolution approach

Time-to-Market

120
100
80
60
40
20
0
1

2

3

4

5

6

7

8

9

10

Products
Big_Inf

Big_Bra

Inc_Inf

Inc_Bra

Tra

Figure 4.4. Effect of combined adoption and evolution approach

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

assets are developed in two steps with the incremental
approach. The first increment happens right before the
development of Product 1, and implements half of the
core assets. The second increment happens right before
the development of Product 4, and implements the rest of
the core assets. For the first three products, the
incremental approach appears to have shorter time-tomarket, mainly because fewer core assets means less time
is required for asset development. As request for Product
4 comes, with the incremental approach, the development
of the new product can not be started until the rest of the
core assets have been implemented. So, we see a big
jump in time-to-market from Product 3 to 4. The
incremental approach results in higher total effort
(6173.36) than the big band approach (5338.47). That is
the nature of its process flow.

4.4. Effect of evolution approach
The inputs to Scenarios 1 and 5 differ in product line
evolution approach (infrastructure-based and branch-andunite, respectively). Figure 4.3 shows the comparison of
the results in time-to-market. As specified by the inputs,
the evolution stage starts from Product 7. For Product 7,
the branch-and-unite approach has smaller time-tomarket because the product gets developed earlier and
does not have to wait for core asset updates. For the later
products, the branch-and-unite approach results in longer
time-to-market because it requires extra effort to rework
new products and imposes more task dependencies, thus
reducing concurrency. The total effort of the branchand-unite approach (5340.37) is only a slightly higher
than the infrastructure-based approach (5338.47). That
is because when Product 9 and 10 are released, some
early products have already been phased out, so the costs
for updating existing products are reduced.

4.5. Effect of adoption and evolution approaches
A situation an organization might face is the need to
determine which software development and evolution
approaches best fit its goals. Scenarios 1 and 4 – 7 show
the alternatives the organization might have. Scenario 7
is the case where a traditional software development
approach (single product only) is taken, where products
are created and evolved independently.
Figure 4.4 shows the comparison of the results in
time-to-market. As we can see, the big bang with
infrastructure-based approach has the shortest average
time-to-market, and the incremental with branch-andunite approach has the longest average time-to-market.
The traditional approach has the shortest time-to-market
for the first two products, the longest time-to-market on

the third product, then its time-to-market stays between
the incremental and the big bang approaches, afterwards
its time-to-market starts climbing dramatically but still
stays in between the branch-and-unite and infrastructurebased approaches. In our experiment, the reuse rates are
not very high (30% for both black-box and white-box
reuse) and the product is relatively small (100KSLOC),
so the traditional product development time is only
slightly longer (about 5 months) than product line
engineering approaches. In the case of branch-and-unite
evolution approach, the dependencies imposed by that
approach overweighs the benefits of reusing the core
assets. The total effort of Scenario 1 and 4-7 are
5338.47, 5340.37, 6173.36, 6194.03, and 8760.55,
respectively. As we have expected, the traditional
approach requires considerably more effort. By largescale reuse, product line approaches generally result in
smaller code size to development and maintain. Thus,
the total effort on creating and evolving the products in a
product line is smaller.

4.6. Validation of model and results
Several steps have been taken to verify and validate
the model. First, the results of the simulator have been
compared with the results of COCOMO II [9] to make
sure the mathematic calculations are correct, and the
results are the same (ignoring rounding errors). Second,
the results of the simulator have been compared with the
common knowledge about the product line, and we feel
the results confirm to the common knowledge. Third, a
initial small set of experts have reviewed the simulation
results, and they feel that the results are consistent with
what have been observed in the real world and the
abstract model reflects the real process flow at a high
level. In future investigations, we plan to continue
soliciting expert feedback and compare simulation results
with real product line data.

5. Related work
Cohen [7] presents an approach for making a software
product line investment determination. The approach
uses three factors to justify software product line
investment:
applications,
benefits,
and
costs.
Applications include the number of projected products in
the product line, the time they will be developed, and
their annual change traffic; benefits consists of the
tangible and intangible goals the organization wishes to
achieve through a product line approach; costs are the
life cycle costs associated with core assets and individual
products. Costs are affected by some factors, such as

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

costs of reuse, degree of reuse, and core assets change
rate. Our cost estimation method is consistent with the
Cohen approach but provides more capabilities.
Regnell et al. use a simulator to study a specific
market-driven requirement management process [5].
The goal of simulator is to help in exploring bottleneck
and overload situations in the requirement engineering
process, investigating which resources are needed to
handle a certain frequency of new requirements, and
analyzing process improvement proposals. The specific
process is modeled using queuing network and discrete
event simulation [6]. Our simulator also uses discrete
event simulation, but its purpose is to study life cycle
issues for a product family instead of a portion of a
software engineering process for a single product.
Riva and Delrosso recently discussed issues related to
software product family evolution [11]. They state that a
product family typically evolves from a copy-and-paste
approach to a mature software platform. They point out
some issues that harm the family evolution, such as
organization bureaucracy, dependencies among tasks,
slower process of change, and the new requirements that
can break the architectural integrity. Their notion of
product family used in that paper is different from the
definition of a product line [8]. Creating a product
family by copy-and-paste is not a product line approach,
because the product line approach emphasizes a
disciplined strategic reuse, not opportunistic reuse. A
product line is actually a product family that has already
evolved to a mature software platform. Our simulation
results also show that in some cases dependencies
imposed by product line approaches result in slower
market response than the traditional software
engineering approach.

6. Conclusions and future investigations
Software product line engineering promises of reduced
cost while still supporting differentiation makes adoption
and continued use of the associated approaches attractive.
However, in order to make appropriate planning,
decision tools are necessary. In this paper, we described
a simulator that is intended to support early stage
decision-making. The simulator provides both static and
dynamic information for the selected software product
line engineering process. The statistical result generated
at the end of the simulation can be used for trade-off
analysis. Stepping through the simulator helps analyzing
product line processes, uncovering problems, and
improving the understanding of software product line
evolution.
Currently the simulation tool supports the study of
independent product line initiation using big bang or

incremental product line adoption approaches and
infrastructure-based or branch-and-unite product line
evolution strategies. Our future investigations include
providing estimates for other software product line
initiation
situations and approaches, allowing
concurrency between inter-dependent tasks to some
extent, providing probabilistic demand intervals,
incorporating other cost models, and removing a number
of the simplification assumptions. Furthermore, we plan
to validate the model by comparing the results with real
product line data and getting more expert feedback.
Also, we want to combine the simulator with an
optimization model, so users can specify their end-goal
criteria and then allow the simulator to search for the
best results.

7. References
[1] B.P. Zeigler and H.S. Sarjoughian, “Introduction to DEVS
Modeling & Simulation with JAVA(TM): Developing
Component-based
Simulation
Models”,
2003,
http://www.acims.arizona.edu/SOFTWARE/software.shtml.
[2] B. Boehm, A.W. Brown, R. Madachy, and Y. Yang, “A
Software Product Line Life Cycle Cost Estimation Model”,
USC, June 2003
[3] M. I. Kellner, R. J. Madachy, and D. M. Raffo, “Software
Process Modeling and Simulation: Why, What, How”, The
Journal of Systems and Software, April 1999, pp. 91-105.
[4] M. Paulk, et al., “Key Practices of the Capability Maturity
Model”, Version 1.1, Tech. Rept. CMU/SEI-93-TR-25,
Software Engineering Institute, Feb 1993.
[5] M. Höst, B. Regnell, et al, “Exploring Bottlenecks in
Market-Driven Requirements Management Processes with
Discrete Event Simulation”, The Journal of Systems and
Software, Dec 2001, pp. 323-332.
[6] J. Banks, J.S. Carson, and B.L. Nelson, Discrete-Event
System Simulation, 2nd Ed., Prentice Hall, Aug 2000.
[7] S. Cohen, “Predicting When Product Line Investment Pays”,
Proceedings of the Second International Workshop on Software
Product Lines: Economics, Architectures, and Implications,
Toronto Canada, 2001, pp. 15--18.
[8] P. Clements and L.M. Northrop, Software Product Lines -Practices and Patterns, Addison-Wesley, Aug 2001
[9] B. Boehm, B. Clark, E. Horowitz, C. Westland, R.
Madachy, and R. Selby, “Cost Models for Future Software Life
Cycle Processes: COCOMO 2.0,” Annals of Software
Engineering Special Volume on Software Process and Product
Measurement, Science Publishers, Amsterdam, The
Netherlands, 1995, pp. 45 - 60.
[10] K. Schmidt and M. Verlage, “The Economic Impact of
Product Line Adoption and Evolution”, IEEE Software, Jul/Aug
2002, pp. 50-57.
[11] C. Riva and C.D. Rosso, “Experiences with Software
Product Family Evolution”, Proceedings of International
Workshop on Principles of Software Evolution, Helsinki
Finland, Sep 2003, pp. 161-169.

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

A Design-based Model for the Reduction of Software Cycle Time
Ken W. Collier, Ph.D.
Assistant Professor
Computer Science and Engineering
Northern Arizona University, Flagstaff, AZ
Ken.Collie@ nau.edu

Abstract
This paper presents a design-based sofhYare cycle time
reduction model that can be easily implemented without
replacement of existing development paradigms or design
methodologies, The research results suggest that there are
many cycle-time factors that are influenced by design
decisions. If manipulated carefully it appears that an
organization can reduce cycle-time and improve quality
simultaneously. The preliminary results look promising
and it is expected that further experimentation will support
the use of this model. This paper will analyze the basis for
the model proposed here, describe the model’s details, and
summarize the preliminary results of the model.

Introduction
Many organizations have relatively mature, effective
software-development processes in place and have
employed talented software engineers and managers to
implement these processes. In such organizations, it is
appropriate to question whether each softwaredevelopment effort is performed at peak efficiency and
effectiveness.
One of the primary goals of any organization is to
release high quality software in as little time as possible.
Therefore, an increase in software-development efficiency
may be manifest as a reduction in development time.
However, simply reducing development time is not good
enough. It is important that these reduction efforts
maintain high levels of both process and product quality.
This research involves the examination and control of
factors that have an impact on software development time.
More specifically, this paper presents a model for reducing
the development cycle time by restructuring software
design. Reduction of development time, together with
increasing quality and productivity, is the goal of many
software development organizations. The benefits are
numerous including: extended market life; increased
market share; higher profit margins; and the ability to take

James S. Collofello, Ph.D.
Professor
Computer Science
Arizona State University, Tempe, AZ
James.Collofello@ Asu.Edu
advantage of emerging technologies [l]. Although much
has been written about cost estimation, effort estimation
and productivity and process improvement, little research
has been directly aimed at reducing software cycle time.
The proposed model is based on a systematic and
rigorous analysis of the software process and identification
of product factors that affect cycle-time, as well as the
impact of design decisions on those factors. This
design-based model attempts to reduce software
development time by iteratively and strategically
restructuring the design of a software system. In order to be
practical, such a model must provide significant benefits at
a minimal cost. It must be able to be implemented simply
without requiring an excess of special training. It must be
applicableto as many different types of software systems as
possible, and it must provide measurable benefits. It should
also work in concert with, instead of as a replacement for,
existing models and methodologies.
This paper is divided into three major sections. The
first section provides motivation for the design-based
model presented in the paper. It does so by demonstrating
the impact of software design decisions on cycle-time,
software quality, and scheduling options. The second
section details the model and provides theoretical
justification for an algorithmic approach to schedule
refinement. The model also provides some guidance for
restructuring the design to further shorten cycle time.
Finally, the results of an evaluation of the model are
provided. Although these results are inconclusive, they
suggest that the model has promise and meets the
requirements previously established.

Software Cycle Time Factors
Design Decisions Affect Cycle Time and Software

Quality
Early in this research project, a systematic analysis of
cycle-time factors and issues was conducted. The details of
this analysis are outlined in [2]. However, an overview of
731

1060-3425/96$5.00 0 1996 IEEE

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996
scheduling decisions that increase the chance that a good
schedule will be found. The software design and
scheduling process can be viewed as a process of making
decisions to maximize the chance of finding the best
accessible schedule, given all scheduling constraints [2].
Currently, this attempt at finding a best accessible
schedule is a function of intelligent decision-making,
intuition, and the application of design and scheduling
heuristics. An approach that is highly subject to the
abilities and expertise of the software engineers and project
managers. An ideal scheduling methodology should be a
prescriptive process that encourages the result of a good
schedule regardless of expertise and ability.

the results of that study will serve to motivate the designbased model proposed in this paper.
First, the software development cycle was examined to
establish a comprehensive set of factors impacting cyclet i e . These factors were divided into process factors and
product factors. Process factors include: software reuse,
requirements change, risk management, productivity,
personnel availability, software tools, equipment resources,
method maturity, verification and validation techniques,
quality assurance techniques, integration strategies, and
integration schedule. Product factors include: subsystem
dependencies, module reuse, subsystem complexity,
subsystem independence, subsystem risk, subsystem size,
and subsystem generalization.
Notably, most of these factors are impacted by design
decisions. This recognition led to a set of cycle-time
improving design goals including: maximize reuse; design
independent subsystems: design simple subsystems; ensure
completeness; localize risk: design to minimize risk
occurrence; ensure correct design; design for low coupling;
design for high cohesion; design to maximize
independence: design unique subsystems; and design for
flexible scheduling.
A fortunate, and unexpected side-effect of this analysis
is that these are the same design goals that improve design
quality. This analysis showed that design decisions impact
both design quality and cycle-time, and that these goals are
not mutually exclusive. Moreover, the analysis revealed
that improving cycle-time through design decisions is
likely to improve quality and vice versa.

An Overview of‘Project Scheduling
In the late 1950s two computer-based scheduling
systems were developed to aid in the scheduling of large
engineering projects. Both are based on task dependency
networks. The critical-path method (CPM) was developed
by Du Pont and Remington Rand. The CPM method is a
deterministic scheduling strategy that is based on the best
estimate of task-completion time. The program evaluation
and review technique (PERT) is a similar method and was
developed for the US Navy. The PERT method used
probabilistic time estimates [3].
In the CPM approach, a task-dependency network is a
directed acyclic graph (DAG), which is developed from
two basic elements: activities (tasks) and events
(completion of tasks). An activity cannot be started until
its tail event has been reached due to the completion of
previous activities. An event has not been reached until all
activities leading to it have been completed.
Associated with each activity is an estimated time to
completion. The critical path is the path through the DAG
that represents the longest time to project completion. To
help calculate critical path and identify critical activities a
set of parameters is established for each activity including:
duration, earliest start time, latest start time, earliest frnish
time, latest finish time, and float. Float refers to the slack
time between earliest start and latest start. All activities on
the critical path have a total float of 0. This reflects the
idea that critical-path activities must be completed on time
in order to keep the project on schedule [3].
PERT scheduling uses many of the same ideas as CPM.
However, instead of task completion being a “most likely”
estimate, it is a probabilistic estimate. The estimator
predicts an optimistic estimate, 0, and a pessimistic
estimate, p , of task completion. The most likely time, m,
falls somewhere between those values. The time estimates
are assumed to follow a beta distribution. The expected
time is given as te= (0 + 4m + p)/6. The expected times are
calculated for each activity in the task-dependency
network, and the critical path is then determined as in
CPM [31.

Design Decisions Impact Development Schedule
Scheduling decisions play a critical role in reducing
cycle-time. Therefore, it is appropriate to analyze the
factors that constrain scheduling options. The architectural
design of a system dictates subsystem dependencies,
thereby dictating the possible scheduling strategies. Prior
to software design, implementation scheduling possibilities
are relatively unbounded by product decisions, i.e., all
schedules are accessible.
Selecting a design alternative can be viewed as a
reduction in the set of accessible schedules [2]. There are
many decisions that further constrain the set of accessible
schedules. Staffing, resource allocation, risk analysis, and
identifying high-priority functionality, all constrain the set
of accessible schedules. Therefore, making design
decisions that home in on the optimal schedule is a cycletime reduction goal.
It is unrealistic to suggest that one can always arrive at
the best possible design or schedule. Moreover, there is no
way of knowing when the best design or schedule has been
developed. A more practical goal is to make design and

732

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

Shortening the Critical Path

increases to simulate the dependency, thereby increasing
the task completion time. This in turn increases the
likelihood of defects being introduced into the code. As
defects increase, the debugging time increases.
When resource dependencies are violated, the cost
depends upon the type of resource. If two tasks require the
use of some limited-access hardware device, then
dependency violation might require the purchase of a
second such device. The cost is monetary. If two tasks both
require some very specialized expertise that few team
members possess, then dependency violation means
training other programmers. The cost in this case is in
terms of time spent learning and retraining.
Many factors contribute to the completion time of
programming tasks. Each programming task represents a
cycle of subactivities that includes detailed design, coding,
unit testing, integration testing, and system testing; all of
which are time-consuming. Additionally, there are
process-management and control activities involved in
each task.
One must decide whether the benefits of violating a
dependency outweigh the costs. Furthermore, it is
unreasonable to expect that all dependencies share the
same violation cost. The stronger or more complex the
dependency, the greater the cost of violation. Dependency
strength may be caused by the degree of coupling between
modules or simply by the amount of access of the module’s
components by other modules.
Consider two tasks, A and B, in which B is dependent
upon the completion of A. Dependency-violation cost can
be viewed as the addition of some percentage of the
completion time of A to B’s completion time. This
percentage reflects the amount of A that must be simulated
in order to complete task B before A is actually complete.
A violation cost approaching 100% reflects the idea that
task A is being simulated in its entirety, which defeats the
purpose of violating the dependency. Conversely, an
estimated violation cost approaching 0% reflects the notion
that B’s dependency on A is artificial and both could be
performed in parallel with little consequence.
A dependency classification scheme is proposed to help
determine dependencies that are good candidates for
violation. If a dependency violation cost is estimated to
range between 0% and 25%, then the dependency is
classified as a weak dependency. If the cost is in the range
26%-50%, then the dependency is moderate. If the cost is
in the range 51%-75%, then the dependency is strong;
while 76%-100% violation cost would be considered very
strong.
Accurately estimating the cost of violating
dependencies is a topic for future research. However, for
programming tasks that are functionally or data dependent
upon other tasks, the cost is primarily in the creation of
code scaffolding. In this case, the estimated cost can be

Software development time is measured by the critical
path through the project’s task dependency network.
Hence, all cycle-time reduction efforts can be viewed as
efforts to shorten the critical path. There are basically two
ways to shorten the critical path Shorten the completion
time of tasks on the critical path, or, remove activities from
the critical path
The first option can be achieved either by simplifying
the task or by increasing the productivity of the work team
assigned to the task. Task simplification can be
accomplished by either dividing the task into simpler
concurrent tasks or by eliminating unnecessary work from
the task. Productivity can be increased by improving:
management techniques, resources, or development
techniques; or by allocating additional resources [4,5,6].
Although it is not a primary topic of this paper, the
maximization of productivity is fundamental to cycle-time
reduction.

Violating Task Dependencies
The second approach to eliminating critical-path
activities implies the violation of task dependencies. This
goal requires an understanding of the types of relationships
between task dependencies. Tasks can be divided into two
general subcategories: programming tasks and
nonprogramming tasks (e.g., training, technical reviews,
etc.). Programming tasks correspond to design components
in a software system, and may be low-level modules, or the
integrations of multiple modules into subsystems.
Therefore, dependencies between programming tasks are
connected to dependencies between the components in a
design. There are three types of dependencies between
tasks:
1. Data Dependency - E module A imports information
that module B exports, then module A is data
dependent upon module B.
2. Functional Dependency - If module A requires
functionality provided by module B to complete its own
function, then A is functionallydependent upon B.
3. Resource Dependency - If the completion of module A
requires resources that are currently allocated to
module B, then A is resource dependent upon B.
There is some cost involved in dependency violation.
Otherwise, the ideal scheduling approach would be to
complete all tasks in parallel and then integrate all at one
time. Of course, as Fred Brooks observed, project
scheduling and management is not this simple [7].
For programming tasks, the dependency violation cost
occurs in the form of scaffolding (i.e., test drivers and code
stubs) to simulate the parts of one module upon which
another depends. As data and functional dependencies are
violated, the amount of required scaffold development

733

Proceedings of the 29th Annual Hawaii Intemational Conference on System Sciences - I996
simple.
Component complexity in design determines task
staflng - Highly complex components may require
additional personnel resources thereby limiting the
degree of concurrency in the schedule.
Development learning curves affect productivity and
productivity affects task completion time - Design
components that require programmers to learn special
skills will take longer to implement than those
components for which programmers are already
trained.
Sofhvare reuse afSects task-completion time Code
reuse is almost certainly faster than designing,
developing, and testing code from scratch.
In general, a design with complex components that are
highly dependent upon one another will result in a highly
serial schedule with long task-completion times and excess
staffing needs. Conversely, a design with simple,
independent components will result in a highly concurrent
schedule with short task completion times and lower
required staffing.
The schedule-improvement goals previously listed are
impacted by design decisions. Task dependency strength is
determined by intermodular dependencies (coupling). Task
completion time is affected by module complexity. Task
concurrency is affected by intramodular dependencies
(cohesion).
Clearly, the coupling and cohesion heuristics play a
role in determining the critical path length in a schedule.
Additionally, information hiding, data abstraction, data
localization, and fan-&/fan-out will affect the de
pendencies in a development schedule. This is further
evidence that existing quality metrics are coincident with
the goal of cycletime reduction. Furthermore, these
existing heuristics can provide the necessary mechanisms
for improving the development cycle time by a judicious
refinement of the design.

derived from the software cost models used to estimate the
initial task durations. Resource dependencies and
nonprogramming tasks are not likely to be so simple.
From these ideas on critical path shortening, a set of
scheduling goals for the reduction of cycle-time form the
basis for the model presented in this paper:
1. Violate low cost dependencies to increase concurrent
development whenever cost effective.
2. Transform high cost dependencies into low cost
dependencies.
3. Reduce task-completion time by simplifyingthe task.
4. Reduce task-completion time by dividing it into
concurrent subtasks.
5. Reduce task-completion time
by increasing
productivity.

-

The Relationship Between Schedule and Design
The previous section was devoted to project scheduling
and identifying the general strategies for shortening the
critical path in a schedule. This section examines the
impact of design decisions on scheduling outcomes.
Moreover, identifying connections between design and
scheduling provides a set of techniques for achieving the
critical-path-shorteninggoals previously identified.
In general a software design and its development
schedule are closely connected due to the fact that design
components dictate the work tasks in the development
schedule. It is useful to identify other, more subtle,
design-schedule connections.
Design dependencies determine schedule dependencies
- Any degree of coupling between two modules in the
design translates into a dependency between the
corresponding tasks in the schedule.
Design independence determines task concurrency Modules in a design that are uncoupled can be
developed in parallel, given no other constraints and
assuming that they are not resource dependent.
* Component complexity in design determines
task-completion time - Modules that are complex will
take longer to implement and test than modules that are

A Cycle Time Reduction Model
In [2] a link is made between the factors that affect

Architectural Design

Figure 1 - Current Design Cycle

734

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996
development cycle time and software design decisions.
This link combined with the link between design and
scheduling provide sufficient motivation for a design-based
cycle-time reduction model. The model proposed is based
on the convergence of the ideas presented in previous
sections. This model aims to shorten cycle time by
shortening the implementation phase through design and
schedule refinement.

sacrificing product quality. This model addresses some of
the problems with current iterative design models. Critical
path length is the driving metric for determining when it is
cost-effective to continue design refinement or when to
move on to development. Furthermore, because critical
path is used to reflect the improvement of each iterative
design refinement, the benefits of the iterative process are
measurable and can be compared directly to the cost of
refinement. For example, if it takes five programmer days
to conduct a single design iteration and the critical path is
shortened by only three programmer days, then it is clearly
not cost-effective to continue refining the design. In this
case, the method helps designers determine when to stop
designing. The approach taken by this model is to
selectively apply design-improving techniques to key
components in the design. The aim of this approach is to
minimize the effort and maximize the benefit.

Model Description
Current state of the art in software design dictates an
iterative process of design and refinement to converge on a
design of the highest quality. Figure 1reflects this notion.
Under this model the development schedule is deferred
until design is complete. There are some drawbacks to this
design model. First, widely used design-based metrics do
not provide a mechanism for determining when it is
cost-effective to continue design refinement and when to
stop refining and move to the next phase. Second, the
current design model is not prescriptive. It is difficult to
determine which parts of the design deserve refinement at
any iteration of the cycle. Finally, although this iterative
design process is the state of the art, it is not necessarily
the state of the practice. It remains the tendency of many
designers to adopt the first design that is generated. This
may be because the benefits of iteratively improving design
are difficult to quantify.
The model proposed in this paper is also iterative in
nature. However, development scheduling becomes an
integral part of the design cycle. In this model the
designers develop an initial design and then iteratively
work to develop the best schedule (i.e., shortest critical
path) that can be implemented using that design. Then the
design is refined according to standard practices.
Following refinement, a new “best schedule” is developed
for the improved design and so on until the schedule does
not continue to improve. This model is graphically
represented in figure 2.
The ultimate goal of this cycle-time reduction model is
a design that improves development time without

How The Design Model Works
This design cycle follows five basic phases:
Initial Design - During this phase the system is
designed using existing methods and techniques. At
this point in the process, the model does not differ from
current design models. In fact, this cycle-time reduction
design model may be thought of as a design
metamodel, since it does not replace popular design
techniques and paradigms but is symbiotic with
existing methods.
Initial Schedule - The initial scheduling phase in this
model employs current state-of-the-art scheduling and
effort estimation techniques.
Schedule Refinement - The schedule-refinement phase
of this design model is the point at which the schedule
is iteratively analyzed and refined in an effort to
shorten the critical path without altering the design.
Problem Identification - Now that the schedule has
been refined sufficiently, this phase serves the purpose
of identifying those system components that, if
improved, represent a significant cycle-time
improvement.

Architectural Design

Figure 2 - Cycle Time Design Model

735

~~

~-

~~

Proceedings of the 29th Annual Hawaii Intemational Conference on System Sciences - I996
5 . Design Refinement - During this phase efforts are made
to improve those tasks that were identified as
problematic by the previous phase.
This cycle is iterated using the critical path length as a
cycle-time improvement metric. For each iteration of the
design cycle, the critical path will reflect the benefit of the
refinement efforts. Ideally, the architectural design cycle
should halt as soon as the critical path becomes stable or
when the benefits at each iteration do not justify the effort.
We do not propose to replace existing state-of-the-art
design and scheduling methods in this model. Instead, the
remainder of this section will focus on the development of
methods for accomplishingsteps 3,4 and 5.

test v (i.e., scaffolding). This reflects the cost of
violating task dependencies.
Some observations can be made from this view of
schedule optimization. First, it is not likely to be beneficial
to reposition slack path vertices. Second, a path that is
critical at one point in the optimization process may cease
to be critical at some future time. Third, there may be
multiple critical paths in the graph at any given time.
These conditions and others that are not so obvious must
be addressed by any strategy that is to be used to solve this
problem.
An algorithmic strategy might implement a cycle of
vertex repositioning followed by recalculating critical-path
length. Such an algorithm must be able to determine which
vertex to reposition at each iteration and when to stop.
The selection of a vertex for repositioning must be
conducted in light of a cost-benefit analysis. The benefit is
measured as a reduction in W. The cost is represented by
the increase in wt(v) due to dependency violation. The
vertex that produces the greatest benefit and least cost is
the prime candidate.
Some additional observations can be made about the
structure of critical paths in this problem space. It is
possible that the critical path in a DAG of significant size
will not be a single linear sequence of vertices and edges.
Multiple parallel critical paths may require repositioning
multiple vertices before seeing a reduction in W.
Furthermore, if the critical path diverges at one task and
then reconverges later (i.e., becomes temporarily parallel),
vertices prior to this divergence or following the
reconvergence should be manipulated if possible in order
to realize immediate benefits.
The trouble with each of these scenarios is that they
obscure the benefits of repositioning certain candidate
vertices. The value of W may remain steady over several
iterations before it begins to decrease. Rather than halting
when no further decrease in critical path length is seen, it
may be more beneficial to halt when an increase is
detected.
These observations motivate a simple greedy algorithm
that iteratively repositions critical path vertices based on a
cost-benefit analysis and recalculates the value of W ,
halting when the value of W increases. This algorithm
always identifies the vertex that appears to promise the
greatest immediate reduction in W. Greedy algorithms are
often used in optimizing problems such as this. However,
most greedy algorithms accept a ‘‘good” solution rather
than guaranteeing an optimal solution. This algorithm is
no exception. The problem of finding an optimal solution
lies in the probability of reaching a local minimum It is
possible that the repositioning of a vertex will temporarily
increase W in order to realize a greater decrease on future
iterations. Unfortunately, guaranteeing an optimal solution
to graph-shortening for a graph of any complexity is a hard

Schedule Refinement
The third phase in this model deserves closer
inspection. Ideally, this phase will reveal the optimal
schedule for the current version of the design. It has been
observed that the task dependencies along the critical path
in a schedule can, potentially, be violated. A brief foray
into graph theory will motivate an algorithmic approach to
the problem of schedule optimization.
The scheduling problem can be temporarily simplified
by stating it as a graph-manipulationproblem resmcted by
a set of rules. Given a directed acyclic graph (DAG) in
which each vertex v has a weight that is represented by
wt(v), the weight of the entire graph W is:
W = X wt(vJ ;3cVv, {c is a critical path, v, is a vertex
1 v, is on c}
The burden of a vertex v, b(v) is the sum of the
weights of all immediate predecessors of v, which are on a
critical path. The prehistory of v, ph(v) is the sum of the
weights of all predecessors of v, which are on the longest
serial path from the initial vertices to v. These are likely to
be the predecessors of v that are on a critical path. The
object is to reposition vertices in the graph in order to
achieve the smallest weight (i.e., shortest critical path).
However, there are rules for moving vertices:
1. Never disconnect the goal vertex from its predecessors.
This is equivalent to eliminating subsystems from the
f d integration.
2. Vertices are repositioned by disconnecting them from
their critical-path predecessors. This reflects the notion
of violating task dependencies.
3. Whenever a vertex v is disconnected from its critical
path predecessors, these predecessors are reconnected
to all of v‘s immediate successors. This is to prevent the
graph from becoming disconnected.
4. Whenever a vertex v is disconnected from its
critical-path predecessors, its weight is adjusted by the
expression: wt(v) = wt(v) + (m x b(v)). In this
expression m represents the percentage of v’s dependent
tasks that must be simulated in order to develop and

736

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996
problem which cannot be solved in polynomial time and is
NP-complete [8,9, lo].

conditions and its use may result in a loss of accuracy. The
remainder of the discussion of this model will use .25 as a
value for m. However, until an empirically established
upper bound value is established one may prefer to use
actual violation cost estimates.

Calculating Dependency Violation Cost

In determining the net benefit of dependency violation,
m represents the percentage of simulation of the tasks on
which another task relies. This requires that up to 100%of
the first task must be simulated in order to complete the
second task. Therefore, the cost of violating a dependency
lies between 0% and 100% of the duration of the first task.
Hence, for a single dependency the value of m is in the
range [0.0,1.0]. The value of m increases as the number of
tasks on which the target task is dependent increases and
may exceed 1.0.
Furthermore, whenever a task dependency is eliminated
there are likely to be hidden costs due to added
communication requirements, inaccuracies in estimating,
etc. As m approaches 1.O the likelihood increases that these
hidden costs will cause the dependency violation cost to
exceed the benefit. Therefore, it is useful to establish a
maximum value for m above which a dependency should
not be violated.
Empirically defining an upper bound value for m is
extremely difficult if not impossible. However, during the
development of the model presented here, fifty different
schedules and scheduling scenarios were examined. The
purpose of these examinations was to identify the behaviors
and consequences of task dependency violation. These
efforts led to the observation that .25 appears to be a
conservatively reasonable upper boundary value for m [11.
It was consistently observed that whenever the
estimated cost of violating a dependency was below 50%,
the violation resulted in a decrease in critical path length.
In fact, many critical path decreases were observed for cost
estimates as high as 80%. However, providing for hidden
costs, it is deemed prudent to take a highly conservative
approach to selecting an upper bound value for m. It was
observed during these exercises that 100% of the cases in
which the value for m was below 30% resulted in a
shortening of the critical path. Compensating for hidden
costs, this model uses an upper bound of 25% for m. This
choice has the added benefit of allowing the cycle-time
reduction model to focus only on weak dependencies as
candidates for violation. Assuming inaccuracies in the
estimates of dependency violation cost, .25 represents a
worst-case value within that range. Fixing m at .25
simplifies the algorithm since categorizing dependency
strength is easier than accurately estimating the cost of
dependency violation.
This use of .25 as a fixed value for m in the cycle-time
reduction model can easily be replaced by a more empirical
value or by the actual violation cost estimate. The selection
of a fixed value serves primarily to simplify the use of the
model. This value was selected under ultraconservative

An Algorithm for Schedule Refinement
The graph theory concepts of the previous section form
the basis for a schedule refinement algorithm that attempts
to reposition critical path tasks. Both CPM and PERT
represent a project schedule as a DAG in which vertices
represent development activities and milestones, while
edges represent dependencies between tasks. The duration
of each task corresponds to the weight of a vertex, wtfv),
and the critical path duration corresponds to W. The
prehistory of a task t, ph(t), is the sum of the durations of
all tasks preceding t along the critical path. The burden of
a task t, b(t), is the sum of the durations of all tasks that
immediately precede t.
The process of refining the schedule is, in essence, a
process of selectively violating task dependencies in a
cost-effective manner. It is assumed that the schedule
follows a typical CPM or PERT format, in which the
information for each task includes: earliest start date @S),
latest start date (LS), earliest finish date (EF), latest f d s h
date (LF), and duration (D). Furthermore, it is assumed
that the dependencies between tasks in a schedule have
been categorized using a scheme similar to the one
previously described. The following strategy takes the
conservative approach of violating only weak dependencies
and assumes the worst case within that range (i.e., m =
.25). Given more information, the procedure can be
modified as is necessary.
The algorithm is as follows:
1. Build a set D of weak dependencies on the critical path.
These are the primary candidates for violation.
2. For each dependency i+j E D, where task i is
dependent on task j, calculate ph(i) and b(i). Remove
from D any dependencies whose cost outweighs i’s
prehistory, wt(i) + ph(i) 5 wt(i) + .25b(i).
3. For each dependency i+j E D, calculate the net
benefit, n(i) = wt(i) - .25b(i), of removing the
dependent task from the critical path and placing it in a
slack path. Remove any dependencies from D that have
no net benefit, n(i) < 0. D now contains the final set of
candidates for elimination.
4. Select the dependency i+j E D, for which n(i) is the
greatest, remove the dependent task from the critical
path, and replace it on a slack path using the following
rules:
0
If i has no successors and j becomes disconnected
from all successors, then do not violate this
dependency since j will never become integrated

737

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996
into the system without an additional integration
step. In this case, i is the final system integration.
This is not likely to be cost-effective.
Connect j to all immediate successors of i and
update i’s ES, LS, EF, and LF values. This reflects
the change in the integration strategy for i a n d j (i
can now be developed concurrently with/].
e
Update i’s duration by a factor of .25,D = 2
.50
All additional constraints that affect the
development schedule should be maintained (e.g.,
resource allocation, risk prioritization, etc.). It is
impossible to define rules for maintaining these
constraints. Therefore, it is the responsibility of
the scheduler to ensure that they are not violated.
5. Because a new critical path may emerge as a result of
task repositioning, steps 1-4 are repeated until the
schedule reaches a stable state (i.e., no changes can be
made to the schedule) or until the critical path begins to
increase.
At best, the result of this algorithm is an optimal
schedule for the given design under the given constraints.
At the least, the resulting schedule will not be any worse
than the original schedule.
There are additional scheduling-improvement
techniques that have not been addressed here, such as
resource-leveling, which may help to shorten the critical
path [3]. Used in conjunction with such techniques, the
model described here offers a simple yet powerful means of
reducing development time. It is based on observation,
intuition, and the mathematical manipulation of task
networks and CPM components. A major benefit to this
scheduling method is that it is highly automatable once
tasks have been identified and a preliminary schedule is
developed.

2. Idenhfj dependencies that, if violated, would produce

the greatest reduction in the critical-path length.
3. For the identified tasks, examine the potential for either

simplifyins or decomposing their corresponding design
components and estimate the effort required to do so.
Retain those that have the greatest benefit for the least

effort4. For the identified dependencies, evaluate the effort
required to weaken the dependency and evaluate the

potential for success. Retain those that have the greatest
benefit for the least effort.
5. From the retained tasks and dependencies, select the
one that has the greatest overall net benefit in terms of
critical-path shortening.
6. Ideally, one task or dependency will be refined on each
design-cycle iteration. However, it may be that by
performing simple refinements to several tasks andor
dependencies, a major shortening of the critical path
will be experienced on one iteration. These decisions
must be determined based on individual project
scenarios.
Problem identification relies upon the talents of
software engineers to make good decisions. These
guidelines may easily be supplemented with additional,
project-specific information to increase their benefit.
Future work in validating cost and benefit estimating
techniques will further strengthen this phase in the model.

Design Refinement
After identifying the most beneficial task or
dependency for refinement, it is necessary to quickly
identify the cause of the problem and resolve it. If the
target of refinement is a dependency, then the aim is to
weaken that dependency to make its dependent task a
candidate for repositioning. Assuming the dependency is
not resource dependent, it may be that efforts should be
made to decrease coupling between corresponding design
components using existing design heuristics and
principles.
If the target of the refinement effort is task duration,
reducing the complexity of corresponding components
might be accomplished through simplification or
subdivision. Subdivision implies that the module has poor
cohesion. Ideally, system modules should be functionally
cohesive. Therefore, the designer should work to reduce
cohesion and divide the system component into multiple,
independent subcomponents.

Problem Identification
Once the development schedule has been refined there
are two ways to further shorten the critical path: either
remove tasks from the critical path or shorten the
completion time of tasks on the critical path. The only
remaining critical tasks are those with high
dependency-violation costs or weak dependencies for
which there is no net benefit in removing them from the
critical path.
This phase in the model attempts to shorten the
duration of a target task; or to prescribe design refinements
that will result in weaker dependencies. The approach
taken is to identify the design component that, if refined
will result in the greatest cycle-time benefit.
This approach is heuristic rather than algorithmic.
General guidelines are as follows:
1. Identify tasks in the schedule that have the greatest
impact on the critical path.

Preliminary Results
This model cannot guarantee a reduction in actual
cycle-time since many things can happen between software
design and product release. However, preliminary results
suggest that the model has promise in achieving the
738

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

Actual Effort

Publam

DPS
Publam
Table 1 :Demonstration Strategies
following results:
approach.
1. Significantlyreduce the estimated development time of
By combining each of these factors with each of the
a software product at the design phase. For the
five software designs, forty-two different design-scheduling
purposes of this effort, any reduction of estimated
scenarios were used to observe the effects of the model (in
development time of 5% or more will be considered
some cases, certain combinations were infeasible). Table 2
significant.
shows the statistical results of these 42 demonstrations.
2. Maintain or improve the quality of the initial design
The data is fairly scattered. However, in all but one of the
(i.e., the model will not reduce design quality).
demonstration cases, a significant (i.e., 5 % or greater)
3. Help to focus design-refinement efforts on beneficial
reduction in estimated development time was observed.
design components.
It is notable that the design refinements in each of the
Although empirical validation of this model is virtually
demonstration scenarios contributed as much or more to
improving quality as to improving estimated cycle-time.
impossible, demonstration of the model on a variety of
The quality of each of the poor quality designs improved,
software designs under a variety of conditions yields
encouraging results. The model was applied to five
while the high quality designs did not experience any loss
software systems ranging in size from 736 source lines of
in quality. The details of this demonstration of
effectiveness can be found in [l]. It should be noted that
code (SLOC) to 6,309 SLOC, and ranging in quality.
the demonstration scenarios are all small programs, and it
Three factors were identified as affecting the initial
schedule of a particular software design: size estimation
remains to be seen how well this model scales to largetechnique; effort estimation technique; and scheduling
scale software systems.
technique. For size estimation in this analysis actual size
and expected size were used. For effort estimation, actual
Summary and Conclusions
efforts were used in addition to the COCOMO and Putnam
models [ l l , 121. In developing initial schedules, two
Current design cycles do not provide mechanisms for
approaches were used: dependency preserving scheduling
determining on which parts of the system to focus
(DPS) and educated scheduling. DPS refers to preserving
refmement efforts, or for determining when to stop
iterating. This model provides a solution to both problems.
all intermodular dependencies from the design on the
Through a schedule analysis, the model guides the
corresponding development tasks. Educated scheduling
designer to focus refinement efforts on the most beneficial
refers to more common scheduling approaches in which
system components. By using the critical path as a metric
task dependencies are determined by functionality, risk,
of cycle-time improvement, this model helps determine
resources, etc. Table 1 shows the variety of combinations of
when to halt the design cycle. Additionally, this model
size estimation, effort estimation, and scheduling
.........................................................
............................................................................................................................................................................

Initial Critical Path
12.34 days
[... 212 days
73.98 days
54.96 days
....................... Le?%!!...................... 1..............................................................................................................................................................................................
Final Critical Path Length i
10.25
122.25
33.94 days
.................................................................................................................................................................
51:62..!?YS .................................................................
88 - 42.5 days
Critical Path Reduction i 7 1.91 -69.66 days
Critical Path Reduction j
3.13%
5 1.7%
29.94%
12.31 %
14
5.35
2.76
Scheduling Iterations f
1
..........................................................................................................................................................................................................................................................
2
1.22
0.42
Design Iterations
0
...........................................................
.i ..............................................................................................................................................................................................

1

i

739

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996
provides a mechanism for the designer to determine which
techniques to apply in order to resolve targeted problems.
In this manner, the cycle-time reduction model helps the
designer selectively apply design-improving techniques to
key components in the system in order to cost-effectively
improve the development cycle time. Applying Laws of
Pareto to cycle time, 20% of the software design will
account for 80% of the cycle time. The goal of this model
is to help software engineers focus their energy on the key
20% for improvement by identifying system components
that most significantly impact the critical path length.
There are a number of other benefits to this cycle time
reduction model:
It is not a replacement methodology. This design
approach allows software engineers to continue using
state-of-the-artdesign and scheduling methods.
This method provides design-improvement feedback.
Using the critical path as a metric, the impact of design
refinements is clear.
This method combines established practices. Popular
design heuristics, scheduling methods, and
cost-estimationtechniques are supported in this model.
This method provides motivation for iterative design
refinement. Currently, it is unclear how much benefit is
gained by placing energy into design iteration. This
method provides this information in the form of units
of time saved.
The method provides a metric for measuring cycle-time
reduction. Critical path length offers a quantitative
measure of the effects of the model on estimated
cycle-time reduction.
The method allows for fixed resource allocations.
Resource constraints that limit scheduling flexibility
are accommodated by this model. Furthermore, if
resources are not fmed, this model provides useful
information for determining resource requirements.
Unrealistic schedules are identified early by this
model. As the designs are refined for shorter cycle
times, the feasibility of initial scheduling estimates
becomes apparent allowing for contingency planning
and deadline renegotiation early in the product life
cycle.
Due to the small size of the software designs that were
used to evaluate the model, the usefulness of the model on
medium or large software designs is unclear. Further
empirical evidence is required to make any conclusive
statements about the model’s effectiveness. However,
initial results are encouraging. This design-based cycletime reduction model can only improve as cost/effort
estimation models become more accurate. Furthermore,
this model is easily adaptable to organizational
idiosyncrasiesand to changing technologies,

Bibliography
[l] Collier, K.W. “A Design-based Model for the Reduction of
Software Cycle Time”, Ph.D. Dissertation, Arizona State
University, 1993.
[2] Collier, K.W. and J.S. Collofello, “Issues in Software Cycle
Time Reduction”, Proceedings: International Phoenix
Conference on Computers and Communications, March 2831, 1995, pp. 302-309.
[3] Dieter, G.E. Engineering Design: A Materials and Process
Amroach, McGraw-Hill New York 1983.
[4] Bisant, D.B. and J.R. Lyle, “A Two-Person Inspection
Method to Improve Programming Productivity”, IEEE
Transactions on Sofhyare Engineering vol. 15 no. 10, 1989,
pp. 1294-1304.

[SI Boehm,B., M.H. Penedo, D.E. Stuckle, R.D. Williams and
A.B Pyster, “A Software Development Environment for
Improving Productivity”, Computer, vol. 17 no. 6, 1984, pp.
3042.

[q Dart, S.A., R.J. Ellison and P.H. Feiler, “Software
Development Environments”, Computer, vol. 20 no. 11,
1987, pp. 18-28.
[7] Brooks, F.P.The Mythical Man-Month: Essavs on Software
Engineering, Addison-Wesley,Massachusetts, 1982.

[SI Boctor, F.F. “Some Efficient Multi-heuristic Procedures for
Resource Constrained Project Scheduling”, European
Journal of Operational Research,vol. 49, 1990, pp. 3-13.
[9] Khattab, M.M. and F. Choobineh, “A New Approach for
Project Scheduling With a Limited Resource”, International
Journal of Production Research, vol. 29 no. 1, 1991, pp.
185-198.
[lo] Khattab, M.M. and F. Choobineh, “A New Heuristic for
Project Scheduling With a Single Resource Constraint”,
Compuers and Industrial Engineering, vol. 20 no. 3, 1991,
pp. 381-387.
[ l l ] Boehm, B. “Software Engineering Economics”, IEEE
Transactions on Sofhvare Engineering vol. SE-10 no. 1,
1984, pp. 4-21.
[12] Putnam, L.H. and W. Myers, Measures for Excellence:
Reliable Software on Time, Within Budget, Yourdon Press,
New Jersey, 1992.
[13] Yourdon, E. Manaving the Structured Techniques, PrenticeHall, New Jersey, 1989.

740

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-11 NO. 9, SEPTEMBER 1985
[17] R. E. Schaffer, J. E. Angus, and J. F. Alter, "Validation of software reliability models," Tech. Rep. RADC-TR-79-147, June
1979.
[18] R. E. Schaffer, J. E. Angus, and A. Sukert, Software reliability
model validation," in 1980 Proc. Annu. Reliability and Maintainability Symp., pp. 191-199.
[19] R. E. Schellenberger, "Criteria for assessing model validity for
managerial purposes," Decision Sci, vol. 5, 1974.
[20] N. F. Schneidewind, "Analysis of error processes in computer
software," in Proc. Int. Conf Reliable Software, Los Angeles,
CA, 1976, pp. 47-53.
[21] A. N. Sukert, "An investigation of software reliability models,"
in Proc. 19 77Annu. Reliability and Maintainability Symp., Philadelphia, PA, Jan. 18-20, 1977.
[22] -, "A multi-project comparison of software reliability models,"
in Proc. AIAA Conf. Computers in Aerospace, Oct. 31-Nov. 2,
1977, pp. 413-421.
[231 -, "Empirical validation of three software error prediction
models," IEEE Trans. Rel., vol. R-28, pp. 199-204, Aug. 1979.
[24] W. L. Wagoner, "The final report on a software reliability measurement study," The Aerospace Corp., Rep. TOR-0074 (4112)-i,
Aug. 15, 1973.

849

Robert Troy (M'84) received the Doctor degree
in computer systems from the University of
Toulouse, Toulouse, France, in 1974.
He has been President of VERILOG, an independent software verification and validation
company, since 1984. From 1980 to 1984 he
worked as Director of the Software Engineering
Observatory for Agence de l'Informatique. As
a Research Engineer for the French Research
Institute in Computer Systems and Automation
(IRIA), he participated in the Direction team
of the French National Action for Systems Dependability. From 1972
to 1975 he was an Assistant Professor of Computer Science at the University Paul Sabatier. His current research interests are in software
qualification techniques: quality assurance, quality evaluation, and testing. He has been involved with dependability evaluation of computer
systems since 1972.
Dr. Troy is a member of the Association for Computing Machinery
and Chairman of the AFCIQ Software Reliability Group.
Ramadan Moawad, photograph and biography not available at the time
of publication.

Design Stability Measures for Software Maintenance
STEPHEN S. YAU, FELLOW,

IEEE, AND

Abstract-The high cost of software during its life cycle can be attributed largely to software maintenance activities, and a major portion of
these activities is to deal with the modifications of the software. In this
paper, design stability measures which indicate the potential ripple effect characteristics due to modifications of the program at the design
level are presented. These measures can be generated at any point in
the design phase of the software life cycle which enables early maintainability feedback to the software developers. The validation of these
measures and future research efforts involvlng the development of a
user-oriented maintainability measure, which incorporates the design
stability measures as weli as other design measures, are discussed.
Index Terms-Design stability measures, program modifications, software maintenance.

INTRODUCTION
THE major expenses in computer systems at present are in
T software. While the cost of hardware is decreasing rapidly,

JAMES S. COLLOFELLO, MEMBER, IEEE

ity of this software cost can be attributed to software maintenance. The cost of maintenance activities has been very high
ranging from 40 percent to as high as 80 percent of the total
cost during the life cycle of large-scale software systems

[1] -[3]-

The control of software maintenance costs can be approached
in several ways. One approach is to improve the productivity
of maintenance practitioners by providing them with tools and
techniques to help them perform their maintenance tasks. Advances in this area have included debugging tools, program
flowcharters, and ripple effect analysis tools. Although these
tools are definitely important in the maintenance phase, a
more effective approach to reducing maintenance costs is to
develop tools and techniques which are applicable during the
earlier life cycle phases which guide program developers into
producing more maintainable software. These tools and techniques include a broad range of design methodologies and pro-

software productivity improves only slowly. Thus, the cost of
software relative to hardware is rapidly increasing. The major- gramming techniques.
Another approach to controlling software maintenance costs
Manuscript received November 8, 1982. This work was supported by is the utilization of software metrics during the program develthe Rome Air Development Center, U.S. Air Force System Command opment phase [4], [5]. Many metrics have been developed
under Contract F30602-80-CO1 39, and was presented at the Sixth International Computer Software and Applications Conference Chicago, which assess different quality characteristics of software. These
metrics can be utilized as indicators of program quality and can
November 10-12, 1982.
S. S. Yau is with the Department of Electrical Engineering and Com- help identify potential problem areas in programs. More effort
puter Science, Northwestern University, Evanston, IL 60201.
J. S. Collofello is with the Department of Computer Science, Arizona is needed in the development of measures for evaluating the
State University, Temnpe, AZ 85287.
maintainability of a program at each phase of its development.

0098-5589/85/0900-0849$01.00 © 1985 IEEE

850

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-11, NO. 9, SEPTEMBER 1985

In this paper, we will discuss an approach to reducing maintenance costs through the utilization of metrics. We will first
discuss the software maintenance process and the software
quality attributes that affect the maintenance effort. Because
accommodating the ripple effect of modifications in a program
is a difficult, time consuming, and error-prone activity, especially for badly designed programs [61, we will present some
measures for estimating the stability of a program design. The
stability of a program design is the quality attribute indicating
the resistance to the potential ripple effect which a program
developed from the design would have when it is modified.
These design stability measures can be obtained at any point in
the design process and, thus, enable examination of programs
early in their life cycle for possible maintenance problems.
Algorithms for computing these design stabilty measures will
be presented in detail. Applications of these measures, an illustrative example, and some experimental results for an indirect
validation will also be presented. Future research efforts involving the development of a user-oriented maintainability
measure will also be discussed.
THE MAINTENANCE PROCESS
Software maintenance is a very broad activity that includes
error corrections, enhancements of capabilities, deletion of
obsolete capabilities, and optimization [7]. We have previously modeled this maintenance process as shown in Fig. 1
[6]. The first phase of the maintenance process consists of
analyzing a program in order to understand it. This phase is
affected by the complexity, documentation, and self-descriptiveness of the program. The second phase consists of generating a particular modification proposal to accomplish the
implementation of the maintenance objective. This phase is
affected by the extensibility of the program. The third phase
consists of accounting for the ripple effect as a consequence
of program modifications. The primary attribute affecting
ripple effect is the stability of the program, where stability is
defined as the resistance to the amplification of changes in the
program. The fourth phase consists of testing the modified
program to ensure the modified program has at least the same

reliability as before. This phase is affected by the testability
of the program.
Each of these four phases and their associated software quality attributes are critical to the maintenance process. Several
metrics have already been developed for some of these phases
applicable during program coding. These metrics include source
code complexity metrics [8], [9], extensibility and testability metrics [4], and stability metrics [6]. The utilization of
these types of metrics during the program coding phase can
help identify potential maintenance problem areas.
Although the source code metrics can be utilized to provide
important maintainability information, the program restructuring costs to improve maintainability can be quite high during the coding phase of the life cycle. A more cost-effective
approach would be the application of these types of metrics
during the program design phase. Any restructuring necessary
to improve maintainability could then be accomplished much
easier than restructuring code. Several metrics applicable during the design phase which contribute to program maintain-

Correct

program errors

Fig. 1. The software maintenance process and associated program quality attributes in each phase.

ability have been developed. For example, there exist metrics
for evaluating the complexity [51, [10] and testability [11]
of software designs. Module connectivity metrics such as coupling, strength, fan-in, fan-out, and scope-of-effect/scope-ofcontrol, which are also applicable for software designs, have
been developed [51, (121, [13].
Although these design metrics contribute towards the evaluation of program maintainability, they do not directly address
the quality attribute of design stability. Design stability measurement requires a more in-depth analysis of the interfaces of
modules than that provided by current design metrics. The
specific nature of design stability measurement and how it relates to potential ripple effect will now be described.
DESIGN STABILITY MEASURES
In this section, the design stability measures will be presented
in detail. These measures are based upon the well-established
notions of data abstraction and information hiding introduced
by Parnas [14] . These notions can have a profound impact on
the maintainability of a program. The lack of adequate data
abstraction and information hiding can result in modules possessing a lot of assumptions. During program maintenance, if
changes are made which affect these assumptions, a ripple ef-

YAU AND COLLOFELLO: DESIGN STABILITY MEASURES

fect may occur throughout the program requiring additional
costly changes [7].
The design stability measures will be derived from an examination of these assumptions existing in the program. The design stability of a module will be calculated as the reciprocal
of the potential ripple effect as a consequence of modifying
the module. The potential ripple effect will be defined as the
total number of assumptions made by other modules, which
either invoke the module whose stability is being measured,
or share global data or files with the module, or are invoked
by the module. This implies that modules with poor design
stability are likely to affect many assumptions made by other
modules, and consequently can produce a large ripple effect
if modified. Ripple effects involving chains of modules which
call upon each other will not be considered since it is assumed
that the ripple effects can be limited in most cases to pairs of
modules. The design stability of a program will be calculated
as the reciprocal of the total potential ripple effect of all of its
modules. This approach to design stability measurement enables the calculation of design stability measures at any point
in the design process.
It should be noted that the definition of these metrics asmodular program structure. If the program consists
single module, stability loses significance as do the
concepts of data abstraction and information hiding. Thus,
sumes, a

of only

a

these metrics are applicable to program designs embodying
modern program development concepts.
Since the examination of assumptions is fundamental to the
calculation of the design stability measures, the problems in
working with assumptions will now be addressed. One of the
major problems is the identification of assumptions. For example, given the design of a program, it is difficult to identify
all of the assumptions made by a particular module about the
other modules in the program as well as the environment in
which the program will execute. Even when these assumptions
can be identified, they are normally only specified as comments
in the do-cumentation. This problem has long been recognized
and partially eliminated by modern programming practices
which advocate the black box concept for modules. This black
box concept emphasizes that the assumptions primarily concerned with module interfaces must be more explicit [131.
The calculation of the design stability measures will be restricted to these assumptions concerning module interfaces.
This restriction is reasonable in light of modern programming
techniques and the extremely difficult task of identifying and
specifying all assumptions. The utilization of assumptions
concerning module interfaces is also more accurate than current design heuristics, such as that of module fan-in and fanout [13] which only examines module invocations, or module
coupling measures [5], [12] which examine module interfaces
without analyzing the assumptions associated with these interfaces. The design stability measures described in this paper
exceed the accuracy of these design measures by incorporating
the assumptions associated with these interfaces.
The identification of the assumptions associated with a module's interface

requires

initially the identification of a module's

interface. A module's interface is defined to consist of the
module's passed parameters, global variables, and shared files.

851

This incorporates the notions of module coupling [i2] and
actually exceeds them by including shared files. Shared files
are included as part of a module's interface because of their
potential utilization in module communication. In order to
describe this module interface more precisely, each parameter,
global variable, and shared file will be examined to see if it is
composed of other identifiable entities. For example, a record
can be decomposed into its respective fields and other structured data types into their respective basic types. The decomposition criterion simply tries to establish those minimal entities for which assumptions can be specified. Thus, if a record
data type is part of a module's interface and that record consists of a character, an integer and a rehl number, then the three
minimal entities are the character, the integer, and the real
number. Specific assumptions concerning each of these entities could then be established.
In order to standardize and simplify the recording of the
assumptions made by each module about its minimal interface
entities, two categories of assumptions will be utilized. The
first type of assumptions concerns the basic type of the entity such as integer, real, Boolean, character, etc. This assumption is always recorded and can be checked automatically by
a compiler or other intercompilation type-checking tool [15].
The second type of assumptions concerns the value of the
basic entity and is recorded if the module has any assumptions
about the values which the minimal entity may assume. Since
some modules do not make any assumptions about the values
of their interface variables, this second type of assumptions
may not always be evident.
For example, a module which is passed an integer may not
have any restrictions upon the permissable values for the integer, and thus, may not possess any assumptions of this second
category. In a strongly typed language such as Pascal, the type
declaration of a minimal entity may include restrictions on the
entity's values analogous to those of category two assumptions.
For example, a subrange data type includes assumptions about
both the type and value of the entity. In order to be consistent across programming languages, type declarations which
include restrictions on values will be recorded as both category
one and category two assumptions.
Since the design stability measures are calculated by counting assumptions about interfaces, each minimal entity in an interface can contribute a maximum of one assumption of each
category. The parameters, global variables, and shared files
in an interface which are composed of these minimal entities
should also contribute to this assumption count. For example,
an array of integers utilized as part of an interface implies an
assumption about its structure. This assumption should be
counted towards that of the module's interface. The minimal
entity for this structure is an integer which implies a maximum
of two more assumptions may be made concerning this interface. Thus, a total of three assumptions may be recorded
for the array of integers in the module's interface. In general,
each structured data type in an interface should be decomposed into its base type and one assumption for the structure
recorded. The base type of the interface should then be examined and additional assumptions recorded. As another example, consider an array of students where a student is a rec-

852

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-11, NO. 9, SEPTEMBER 1985

ord consisting of an ID number and a grade. Assumptions may Step 4: For each set R' and each parameter i E Ri find
be recorded for the array structure, the record structure, and the number of assumptions made by module y about i utilizing
the ID number and grade for a maximum of six assumptions. the pseudocode algorithm in step 3.
This method of recording assumptions is far more realistic Set TPxy equal to the total number of assumptions made by
than simply counting parameters as evidenced in the preceding y about the parameters in Rxy
Step 5: For each module x and every global data item i E
example.
GDX, find the number of assumptions made about i by other
AN ALGORITHM FOR COMPUTING DESIGN
modules in the program. This requires utilization of the set
STABILITY METRICS
Gi and application of the algorithm in step 3 for each global
In this section an algorithm will be presented for calculating data item i and every module y E (Gi - {x}). Set .TG equal
the design stability measures for a program and its modules to the total number of assumptions made by other modules
based upon the assumption counting ideas for module inter- about the global data items in GDx.
faces developed in the previous section. Basically, the design Step 6: For each module x, compute the design logical ripple
stability of a module will be computed as the reciprocal of effect (DLRE) as follows:
the total number of assumptions made by other modules concerning the module whose stability is being measured. The alDLREX =TGx + Y E TPx + y EZ Px
gorithm consists of the following steps.
Jx
Jx
Step 1: From the program design documentation, analyze
the module invocation hierarchy for the program and for each Step 7: For each module x, calculate the design stability
(DS) as follows:
module x, identify the following sets:
DSX = 1/(1 + DLREx).
Jx = {modules which invoke module x},
Step 8: Compute the program design stability (PDS) as
ix, = {modules invoked by module x},
follows:
Rxy {passed parameters returned from
module x to module y, where y E Jx }, and
PDS= 17(1
DLREX)
x
module x
RkXY {parameters passed from
to module y, where y E Jx}.
where x is a module in the program.
Step 2: From the program design documentation, analyze
AN EXAMPLE
the program's global data which is defined to consist of global
In this section, computation of the design stability measures
variables and shared files, and for each module x identify the
will
be illustrated for two designs for the same problem. This
following sets:
example is taken from [13J]. The problem consists of reading
GRX = {global data referenced in module x},
text from an on-line keyboard and text in. a card file, dissectthe text into words, and combining these words according
ing
and
to code from the keyboard and codes contained in the cards.
GDx {global data defined in module x}
Input begins with the keyboard and continues, character-bycharacter, until the ideograph "SRC" is received. At that point,
From these sets, for each global data item i, identify the set
the reading of input from cards is to commence and continue
G= {x Iie(GRx UGDx)}.
until the ideograph "11" is reached. Input from the keyboard
It should be noted that calculation of the set G is undecidable then resumes. An end-of-transmission from the keyboard trigfor languages having pointer variables. In this situation, set G gers reading the remaining cards. The continuous stream of
is calculated as the worst case, i.e., all global items which may text from these two sources is to be broken into separate English words, which are then passed individually to a preexisting
be accessed via the pointers.
module
named PROCWORD. The high level structure charts
Step 3: For each set Rxy and each parameter i E Rxyi find
for
each
module with corresponding inputs and outputs are
the number of assumptions made by module y about i utilizing
shown
in
2 for alternative 1 and Fig. 3 for alternative 2.
Fig.
the following pseudocode algorithm.
The
design
stability
algorithm will now be applied for both
If parameter i is a structured data element, then decompose
alternates.
i into its base types and increment the assumption count by 1,
else consider i to be a minimal entity.
ALTERNATIVE 1
While more base elements can be decomposed, select a base Step I
element which is not a minimal entity and decompose it into
its base elements and increment the assumption count by 1.
JSCANWORD =
For each minimal entity comprising i, if module y makes
JSCANWORD = {INKEY, READCARD, FINDWORD,
PROCWORDI
assumptions about the values which the minimal entity may
assume, then increment the assumption count by 2, else increJINKEY = JREADCARD = JFINDWORD = J = {SCANWORD},
ment the assumption count by 1. Set 7Px, equal to the total
RINKEY, SCANWORD = {character, end-of-transmission
flag},
number of assumptions made by y about the parameters in
RREADCARD, SCANWORD = {card image, last-card flag},
Rxy.

+Y

YAU AND COLLOFELLO: DESIGN STABILITY MEASURES

853

Step S TGX = 0 for all modules x.
Step 6

DLRESCANWORD 8,
DLREKEY

= 2,

DLREREADCARD 3,
DLREFINDWORD

6,

DLREpRoc-oRD = 0.
MODULE

Step 7

OUTPUTS

INPUTS

INKEY

character, end-oftransmission flag

READCARD

card image, lastcard flag

FINDWORD

character, endof-transmission
flag, card image,
last-card flag,

PROCWORD

word

DSSCANWORD = 1/9,

= 1/3,
DSNKEY
DSREADCARD 1/4,
DSFNDWORD 1/7,

DSPROCWORD- 1.
Step 8 PDSaltemlate I = 1/20.

word, end-of words
flag, getcharacter flag,
get-card flag,
word-done flag

source

ALTERNATIVE 2
Stepl

Fig. 2. Design for alternative 1.

JSCANTEXT =

iSCANTEXT = {GETWORD, PROCWORD},

|-SCANTEXL

MODULE

INPUTS

GETCHAR

JGETWORD = JPROCWORD = {SCANTEXT},
JGETWORD = {GETCHAR, GETCARD},
JGETCHAR = JGETCARD = {GETWORD},
JPROCWORD = JGETCHAR JGETCARD =
RGETWORD, SCANTEXT = {word, end-of-words flag},
RPROCWORD, SCANTEXT =
RGETCHAR, GETWORD = {character, end-of-transmission
flag},

OUTPUTS

character, end-oftransmission flag

RGETCARD, GETWORD={word, end-of-words flag,

GETCARD

card-image, last-card flag

GETWORD

word, end-of-words flag

PROCWORD

RSCANTEXT, GETWORD =

RSCANTEXT, PROCWORD = {word},

RGETWORD, GETCHAR RGETWORD, GETCARD =

word

Step 2 There are no global data items.
Step 3

Fig. 3. Design for alternative 2.
RFINDWORD,

SCANWORD

RPROCWORD, SCANWORD
RSCANWORD, INKEY

=

4,

RSCANWORD,

RSCANWORD, PROCWORD
RSCANwORD,

flag, ge 'tcharacter flag, get-card fla g,
word-done flag},

= {word, end-of-words

=

FINDWORD =

READCARD

°,

{word},
{character, end-of-transmissio

'n

flag, card image, last-card flalg
source}.

= 1+ 1 =2,
TPINKEY, SCANWORD
TPREADCARD, SCANWORD =2 + 1 =3,
TPFINDWORD, SCANWORD 2 + 1 + 1 +
TPPROCWORD, SCANWORD

+

TPSCANTEXT,

GETWORD

1

+

1

=6

+ 2+

+ 1 =6.

=,

I

TPSCANTEXT, PROCWORD = 2,
TPGETWORD, GETCHAR = TPGETWORD, GETCHAR =0..
DLRESCANTEXT

TPSCANWORD, INKEY TPSCANWORD, READCARD
TPSCANWORD, PROCWORD
= 1

Step 4

2,

DLREGETWORD -3,
DLREPROCWORD 0,

Step 4

FINDWORD

TPGETCARD, GETWORD =2+ 1 =3.

Step S TG 0 for all modules x.
Step 6

Step 2 There are no global data items.
Step 3

TPSCANWORD,

TPGETWORD,SCANTEXT =2+ 1 =3,

TPPROCWORD, SCANTEXT = 0.
TPGETCHAR, GETWORD = 1 + 1=2,

DLREGETCHAR
DLREGETCARD
Step 7

2,
3.

DSSCANTEXT= 1/3,
= 1/4,

DSGETWORD

854

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-1 1, NO. 9, SEPTEMBER 1985

DSPROCWORD 1,
1/3,
DSGETCHAR
1/4.
DSGETCARD
Step 8

PDSAteae 2 = 1/11
Analysis of the metrics obtained for both alternatives indicates that alternative 2 is more stable than alternative 1. This
finding is supported by the discussion in the source of the
example that alternative 2 is easier to program and maintain.
Further analysis of these metrics indicates that the primary
sources of instability in alternative 1 are nodules FINDWORD
and SCANWORD. This fmding is again supported by the discussion in the source of the example 1131 .
VALIDATION
An important requirement of any metric is the capability of
validating it. In this section both direct and indirect approaches
to validating the design stability measures will be discussed. A
direct approach to validation consisting of experimentation
with the mnetrics was performed by the authors utilizing a graduate software engineering class. The class consisted of 24 professional programmers with diverse company experiences. The
course assignment was to design and implement an automated
gradebook system in Pascal. The class was divided into-4 teams
each of which was to build' a program of an estimated 4K lines.
The class utilized the structured design methodology to produce a complete program design specification. This design
specification was then utilized to compute the design stability
measures. The module design stability measures obtained had
a broad range from 1/145 to 1. It was interesting to note that
the degree of module fan-in/fan-out did not always correlate
with the design stability. For example, many modules with
small fan-in/fan-out had poor stability and vice versa.
Upon completion of the program design specification, the
class was then asked to submit proposals for possible changes
to the program. Over 200 such change proposals were received.
These proposals were analyzed in terms of their potential ripple
effect if they were to be implemented. Several interesting results of this experiment will now be 'described.
The first result is that those modules which would have contributed large ripple effects if modified are among the modules
possessing poor design stability measures. The converse, however, is not necessarily true. Since the design stability measures
reflect a potential worst case ripple effect, it is possible for
modules with poor stability to be modified in certain ways
without producing a large ripple effect.
Another result of the experiment illustrated the diagnostic
capabilities of the design stability measures. Many of the
modules found to possess poor stability also were of weak
functional strength as defined in [12] and [13] and were common coupled to many other modules. It should be noted,
however, that some modules which possess poor stability are
not necessarily bad. For example, implementations of data abstractions usually possess poor stability. The important point
is that if the assumptions made upon a module with poor stability are violated, the potential ripple effect is large. Thus,
these assumptions must be examined carefully with an eye
towards future modifications.

Although the experimentation with the design stability measures produced several interesting results, it cannot be utilized
as a validation of the measures. In fact, the purpose of the experiment was not to provide a statistical evaluation of the metrics, but instead to apply the metrics in a realistic situation in
order to assess their practicality. A formal validation experiment was not performed because experiments with maintenance-type measures can be very misleading due to the
diverse and numerous types of maintenance tasks which may
be performed. For example, maintenance data collected regarding the maintenance activity that a particular program
experienced may not be representative of the maintenance activity in other programs. A' complete direct validation of the
design stability measures will, thus, require a large database of
maintenance information for a significant number of various
types of programs which have undergone a sufficient number
of modifications of a wide variety. The short-term possibility
of utilizing such a maintenance database for validating maintenance-type measures is not very promising. In light of this
reality and diverse nature of the maintenanrce tasks performed
by users of software systems, a more user-oriented approach to
maintenance metric computation is needed. These user-oriented maintainability metrics will combine the unique potential
future maintenance requirements of a user with the characteristics of the software associated with these potential modifications to produce a tailored measure of the exp,cted maintainability to be experienced by the user. These ideas will be
described in more detail later.
Since further experimentation utilizing the design stability
measures could be misleading without a large maintenance
database, a complete direct validation will be delayed until the
development of a user-oriented maintainability measure. The
design stability measures can be, however, indirectly validated
by arguing how the measures are affected by various already
established attributes of programs which affect maintainability.
It should be noted that most of these established attributes
suffer from the same validation problems as the design stability
measures, and their acceptance is largely a consequence of
intuitive arguments.
Because one program attribute which affects maintainability
is the utilization of data abstraction and information hiding
[14], an indirect validation of the design stability measures
must show that the design stability of programs utilizing data
abstraction and information hiding is generally better than that
of programs which do not. Since our measures are based upon
counts of assumptions made concerning interface variables and
since a lack of data abstraction and information hiding manifests itself in an increase in assumption counts, it is apparent
that the design stability of programs utilizing data abstraction
and information hiding is generally better than that of programs which do not.
The relationship of the qesign stability measures with both
the data abstraction and global variable notions can be further

illustrated by the following example.
Consider the case of 3 modules A, B, and C, which share a
global array of records, where each record consists of an integer
ID number and a real balance as indicated in Fig. 4. If we also
assume that no parameters are passed between the MAIN module and modules A, B, and C and that modules A, B, and C

855

YAU AND COLLOFELLO: DESIGN STABILITY MEASURES

Fig. 4. An illustration of modules A, B, and C sharing a global
of records.

array

Fig. 5. An illustration of models A, B, and C utilizing a data abstraclion module to access an array of records.

make assumptions about the values of the ID number and the
balance, the following values can be obtained:
DSMAIN = 1,
TGA = TGB =TGc = 6 + 6 = 12,
DLREA

=

DLREB

=

DLREC = 12,

DSA =DSB = DSC = 1/13,
PDS= 1/37.
In Fig. 5, the program is redesigned to utilize a data abstraction module X to eliminate the need for having a global array
of records. The data abstraction passes a single record to the
modules A, B, and C depending upon some index variable.
From the design, the following values may be obtained:
DSMAIN

1,

TGA =TGB =TGC=O,
PX = TPBx

=

TP&x

(assuming that X makes
the record)
TPXA

=

DLREA

TPXB
=

=

TPXC

DLREB

=

= 3 +2 = 5

no

assumptions about the values in

5,

DLREC

5,

DLREX = 15,
DSA

=

DSB = DSC= 1/6,

DSX = 1/16,
PDS = 1/31.
These two examples illustrate the detrimental effect of global data on stability as well as the positive effect of data abstraction modules. The data abstraction modules, although

quite unstable themselves, improve the stability of the modules
which utilize them.
The design stability measures presented here can, thus, be
indirectly validated since they incorporate and reflect some
aspects of program design generally recognized as contributing
to the development of program stability during maintenance.
APPLICATIONS OF THE DESIGN
STABILITY MEASURES
The design stability measures presented in this paper can be
utilized for comparing alternative designs of a module or program at any point in the design phase of the software life cycle.
The selection of alternatives which exhibit favorable design
stability measures can lead to more maintainable programs.
The design stability measures can also be utilized to identify
portions of the program which exhibit poor stability and, thus,
may contribute to ripple effect problems during the maintenance phase. These portions of the program can be easily identified by the measures and examined for deficiencies. Those
areas of the program with poor stability can then be redesigned
incorporating such favorable design approaches as abstraction,
information hiding, restriction of global variables and functionality in order to improve the design stability measures.
The design stability measures will also be a key component
of any overall maintainability measure. As previously discussed,
stability is an important attribute of program maintainability
which must be combined with other attributes in order to
formulate a maintainability measure. Thus, our future research
efforts in the development of a user-oriented maintainability
measure will incorporate these design stability measures.
CONCLUSIONS AND FUTURE RESEARCH
In this paper, measures for estimating design stability of a
program and of the modules within a program have been presented. Algorithms for computing these design stability measures, applications of these measures, an illustrative example,
some experimental results, and an indirect validation of the
measures have also been presented.
Much research remains to be done in this area. Our primary
emphasis will be on the development of a user-oriented maintainability measure computable during the design phase of the
software life cycle. This metric will incorporate our design
stability measure as well as design complexity and testability
measures. Much experimentation will be needed in combining
these quality attributes into a single measure. Extensive validation on large-scale programs will also be performed.
ACKNOWLEDGMENT

The authors would like to express their appreciation for the
helpful discussions with A. Bowles and S. C. Chang.
REFERENCES
[1] B. W. Boehm, "Software and its impact: A quantitative assess-

ment," Datamation, pp. 48-59, May 1973.
[2] B. P. Lientz, E. B. Swanson, and G. E. Tompkins, "Characteristics
of application software maintenance," Commun. A CM, vol. 12,
pp. 466-471, June 1978.
[3] M. V. Zelkowitz, "Perspectives on software engineering," ACM
Comput. Surveys, vol. 10, pp. 197-216, June 1978.
[4] T. Gilb, Software Metrics. New York: Winthrop, 1977.
[5] S. Henry and D. Kafura, "Software structure metrics based on
information flow," IEEE Trans. Software Eng., vol. SE-7, pp.
510-518, Sept. 1981.

856

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-ll, NO. 9, SEPTEMBER 1985

[6] S. S. Yau and J. S. Collofello, "Some stability measures for soft- Plate Award of the American Academy of Achievement in 1964, the
ware maintenance," IEEE Trans. Software Eng., vol. SE-6, pp. first Richard E. Merwin Award of IEEE Computer Society in 1981,
545-552, Nov. 1980.
and the IEEE Centennial Medal in 1984. He was the President of the
[7] S. S. Yau, J. S. Coflofello, and T. M. MacGregor, "Ripple effect IEEE Computer Society in 1974-1975, the Division V (Computer

[8]
[9]

[10]
[11]

[12]

[13]

[14]

analysis of software maintenance," in Proc. COMPSAC'78, Nov.
1978, pp. 60-65.
T. J. McCabe, "A complexity measure," IEEE Trans. Software
Eng, voL SE-2, pp. 308-320, Dec. 1976.
M. H. Halstead, Elements of Software Science. New York: Elsevier North-Holland, 1977, pp. 84-91.
M. H. Whitworth and P. A. Szulewski, "The measurement of control and data flow complexity in software designs," in Proc.
COMPSAC '80, Oct. 1980, pp. 735-743.
B. H. Yin, "Software design testability analysis," in Proc. COMPSAC '80, Oct. 1980, pp. 729-734.
G. J. Myers, Reliable Software Through Composite Design. New
York: Petrocelli/Charter, 1975.
E. Yourdon and L. L. Constantine, Structured Design. Englewood Cliffs, NJ: Prentice-Hall, 1979.
D. L. Parnas, "On the criteria to be used in decomposing systems
into modules," Commun. ACM, vol. 15, no. 12, pp. 1053-1058,

Society) Director of the IEEE in 1976-1977, Chairman of IEEE
Technical Activities Board Development Committees, 1979, and
Editor-in-Chief of Computer magazine in 1981-1984. He has been
a Director of the American Federation of Information Processing
Societies (AFIPS) since 1972 and is now the President of AFIPS.
He was Conference Chairman of the First Annual IEEE Computer
Conference, Chicago, 1967, General Chairman of the 1974 National
Computer Conference, Chicago; General Chairman of IEEE Computer
Society's First International Computer Software and Applications
Conference, Chicago, 1977 (COMPSAC '77), and Chairman of National
Computer Conference Board in 1982-1983. He is a Fellow of the
American Association for the Advancement of Science and a member
of the association for Computing Machinery, the Society for Industrial
and Applied Mathematics, the American Society for Engineering Education, Sigma Xi, Tau Beta Pi, and Eta Kappa Nu.

Dec. 1978.

[15] W. F. Tichy, "Software development control based on module
interconnection," in Proc. Fifth Int. Conf Software Eng., Mar.
1981,pp. 29-41.

James S. Collofello (S'78-M'79) received the
B.S. and M.S. degrees in mathematics/computer
science from Northern Illinois University,
Dekalb, in 1976 and 1977, respectively, and
the Ph.D. degree in computer/science from
Northwestern University, Evanston, IL, in

Stephen S. Yau (S'60-M'61-SM'68-F'73) received the B.S. degree from the National TaiAfter graduating, he was a visiting Assistant
wan University, Taipei, Taiwan, China, in 1958.
Professor in the Department of Electrical Engiand the M.S. and Ph.D. degrees from the Unineering and Computer Science, Northwestern
versity of Illinois, Urbana, in 1959 and 1961,
University. He joined the faculty of the Derespectively, all in electrical engineering.
partment of Computer Science, Arizona State University, Tempe, in
He joined the faculty of the Department of August 1979 and is currently an Associate Professor there. He has
Electrical Engineering, Northwestern University, worked on industry sponsored research contracts in the area of softEvanston, IL, in 1961, and is now Professor ware quality and productivity and has served as a consultant in the
and Chairman of the Department of Electrical software testing and maintenance areas. His current research interests
Engineering and Computer Science. He is cur- include the reliability and maintainability of computing systems; the
rently interested in distributed computing systems, software engineer- development, validation, and application of software quality metrics;
ing, and reliability and maintainability of computing systems. He has the development and evaluation of software engineering tools; and
published numerous technical papers in these and other areas.
the creation of a software maintenance environment.
Dr. Yau is a Life Fellow of the Franklin Institute from which he reDr. Coliofello is a member of the Association for Computing Maceived the Louis E. Levy Medal in 1963; he also received the Golden chinery, Sigma Xi, and Upsilon Pi Epsilon.

Dynamic Cost Verification for Cloud Applications
Kevin Buell

James Collofello

Arizona State University
Tempe, Arizona, USA

Arizona State University
Tempe, Arizona, USA

kevin.buell@asu.edu

james.collofello@asu.edu

ABSTRACT

Since cloud computing costs are incurred at a much finer
scale (e.g. server usage per hour) than traditional computing, cost verification can also be scaled down to a much lower
level [3]. Indeed, we advocate tracking cost as a quality attribute starting at the use case level and flowing down even
to the unit and procedure level.
Ultimately, we’d like to understand how much our applications cost to run for at least one of two reasons. First,
we may have budgeted a certain amount for a specific transaction within an application and we must ensure that we
do not go over budget. For example, scientific workflows
are not concerned with profit but are often conscious of
cost. Scientific workflows often involve many components
and procedures but they still operate under a fixed budget.
Second, we may be running our application as a direct
part of an activity for which we charge a fee and from which
we would like to make a profit. In this case, we must ensure that the software stays within expected costs in order
to ensure that our profit objectives are met. Pay-per-use
services running in the cloud are a classic example of such
applications.
As with most software quality attributes, the cost to run
an application is one that should be understood well before
the application is deployed. An obvious consequence of having a cost excessive use case within an application would be a
cost overrun. Precisely because of the cloud’s resource elasticity, the extent of cost overruns is possibly much higher
than in a resource limited traditional computing environment, particularly for customer facing applications where
customer behavior may be forecast but not entirely known
ahead of time.
Fixing a budgeted cost for a use case may be quite natural
for many cloud applications, but particularly for services operating on a pay-per-use model. If we charge our customers a
certain amount per transaction to use our service, we would
also want to account for our costs of servicing the transaction. For example, if we are running credit card transaction
processing middleware in the cloud, then it would be important to account for the cost of each transaction when
deciding how much to charge customers per transaction.
On the other hand, some cloud applications provide only
ancillary services to an ultimate customer transaction. Others have no direct revenue at all, but may still be under some
sort of budget constraints. We believe that many types of
cloud applications can benefit from monetary cost verification. A dynamic, measurement based approach is likely to
produce adequate and accurate cost verifications for a variety of applications.

The pay-as-you-go economic model of cloud computing increases the visibility, traceability, and verifiability of software costs. Application developers must understand how
their software uses resources when running in the cloud in
order to stay within budgeted costs and/or produce expected profits. Scientific workflows often involve data intensive transactions which may be costly. Business and consumer application developers are likely to be particularly
sensitive to costs in order to maximize profits. Verification
of economic attributes of cloud applications has only been
touched on lightly in the literature to date. Possibilities
for cost verification of cloud applications include both static
and dynamic analysis. We advocate for increased attention
to economic attributes of cloud applications at every level
of software development, and we discuss some measurement
based approaches to cost verification of applications running
in the cloud.

Categories and Subject Descriptors
D.2.4 [Software Engineering]: Software/Program Verification

General Terms
Measurement, Economics, Verification

Keywords
Cost Verification, Cloud Computing

1.

INTRODUCTION

Monetary cost has always been an important attribute of
software systems, but the pay-as-you-go economic model of
cloud computing changes how we plan for and incur costs.
Cloud computing provides elastic resources that scale on
demand and mimic the consumption and charging patterns
of a traditional utility [4].

Permission to make digital or hard copies of all or part of this work for
Permission
make digital
hard copies
of fee
all provided
or part ofthat
thiscopies
work are
for
personal or to
classroom
use isorgranted
without
personal
granted
without feeadvantage
provided and
thatthat
copies
are
not madeororclassroom
distributeduse
for is
profit
or commercial
copies
not made or distributed for profit or commercial advantage and that copies
bear this
this notice
notice and
and the
the full
full citation
citation on
bear
on the
the first
first page.
page. To
To copy
copy otherwise,
otherwise, to
to
republish, to
to post
post on
on servers
servers or
republish,
or to
to redistribute
redistribute to
to lists,
lists, requires
requires prior
prior specific
specific
permission and/or
and/or aa fee.
fee.
permission
WODA ’12,July
July15,
15,2012,
2012,Minneapolis,
Minneapolis,MN,
MN,USA
USA
WODA’12,
Copyright 2012 ACM 978-1-4503-1455-8/12/07 ...$15.00
...$15.00.

18

ing tools from cloud providers and research communities in
[14].
Cost has been identified as an important QoS attribute for
workflows in very early work [16]. Not unlike many software
applications, workflows involve several, often disparate systems or components which individually analyze input data,
make specific calculations, and produce output data which
is often then used by another component in the workflow
[8]. Shibata presents a study of five workflow applications
and their associated costs [12]. Deelman studied costs of scientific workflows running in the cloud [6], and Dun profiled
data intensive workflows in a similar way to the detection
approach for measurement covered in the current work [7].
Truong’s work is probably the most relevant [13]. It provides extensive measurement based calculations for a scientific workflow, but it focuses on runtime estimation, optimization, and decision making rather than predeployment
verification. Specifically, it does not cover in depth the types
and methods of measurements, their tradeoffs, and the associated implications on cost verification.

Component 1

Component 2

Component 3

Cloud
Resources
e.g.
-Bandwidth
-Storage
-CPU

Component 4

Figure 1: Sample cloud application
Furthermore, dynamic measurement is probably the verification approach most likely to be used in the near term
(and possibly currently in use) in production systems. Given
the absence of other approaches to verifying monetary costs
in cloud applications, a very simple, very high level measurement of the costs incurred by cloud applications are the
costs reported by the cloud provider during billing. These
are tracked in terms of the associated resources consumed,
as shown in Figure 1. Breaking this down further, a cloud
application developer might measure the types of applications that were run during a billing cycle to get an idea of
which applications (perhaps even which components and use
cases) contributed to the total cost in specific ways.
Just as we can associate development of certain parts of
components and units with specific use cases, we can break
the budgeted cost of a use case down into constituent pieces.
For example, we may decide that a particular component,
unit, or even procedure in our software will be allotted 75%
of the cost of a transaction while another procedure gets
20% and another 5%.
Associating a cost constraint with a procedure is probably the ultimate decomposition of the problem and would
promote cost tracking down to the unit test level. Though
budgeted costs at this level would likely be extremely small,
they would scale back up as the usage of the software scales
up to service thousands and millions of transactions, or even
more.
For example, let’s say that a particular procedure in a
credit card processing service generally uses a small amount
of bandwidth and CPU. This procedure may have a budgeted cost of $0.00001. In terms of direct cloud pricing, this
is negligible since server usage is generally charged per hour.
However, if we consider that we will service millions of transactions per hour, our bandwidth and CPU costs then scale
to the level of normal cloud costs. Hence, we can scale down
computing costs to the procedure level for verification, but
scale back up to hourly costs when we consider normal usage
patterns (see [3]).

2.

3.

STATIC VERIFICATION

One approach to verifying software costs is to perform
static analysis of the constituent components of an application. The goal of this kind of analysis would be to predict,
without actually running the software, what sorts of costs
would be incurred for the various types and magnitudes of
data for each component. This would be more of a white
box testing approach which would necessarily make use of
full access to the source code.
Such an analysis might be similar to worst case execution
time analysis (WCET) where static analysis approaches are
used to derive a tight bound on the amount of time a procedure or program can take to execute [15]. This type of
analysis is good for discovering the different control flows
that produce variations in a program’s execution time and
making precise calculations based on target hardware characteristics. It is particularly relevant (and possible) for real
time and embedded applications. In fact, there has been
some discussion of using it as a basis (with several required
extensions) for cloud computing cost analysis [3].
Unfortunately, static analysis is likely to have many limitations for some applications (e.g. data-intensive applications). Particularly with highly variable amounts of data,
static analysis is unlikely to produce meaningful results unless it took an approach where data sizes could be accounted
for parametrically. Though some work has been done in
parametric static analysis and dataflow analysis (e.g. [5]),
the research is still somewhat preliminary and not clearly
applicable to cost verification of cloud applications.
Furthermore, static analysis might require assumptions
about target hardware. Static analysis approaches may vary
widely based on the programming languages used due to
features such as exception handling, dynamic binding, and
asynchronous programming. A more immediately applicable
solution would likely be of value to practitioners looking for
an easier approach.

RELATED WORK

Cost analysis and verification for cloud applications has
been discussed in [3], but with a greater focus on static analysis and control flow aspects. Bala provides a nice overview
of usage-based software pricing [2], which is the model most
directly benefitted by this work. There is a good discussion
of cost modeling and a call for cost estimation and monitor-

4.

DYNAMIC VERIFICATION

Unlike static analysis approaches, a dynamic approach
would involve taking measurements (continuous or sampled)
on applications as they run and calculating costs from these

19

measurements. As with any black box testing approach,
this would not guarantee that cost objectives are met in
every case. However, using standard software testing techniques like boundary values and equivalence partitions, this
approach could provide a reasonable and practical estimate.
Here we discuss three basic approaches to dynamic measurement for cost verification. First, an instrumentation approach takes measurements by modifying the software under
test, essentially wrapping it with a measurement layer. Second, a detection approach requires no modification to the
software under test but instead detects (i.e. monitors or
observes) requests made by the software for resources that
incur cost.
These first two approaches are specifically designed so that
they can be used off the cloud. Indeed, the measurements
they produce are independent of the cloud provider. This is
important because they have the advantage that their measurements can be applied to cost profiles of different cloud
providers and the various resulting costs can be compared.
On the other hand, the third approach would be delivered
by a cloud provider. It would require no modification to the
software and relies on a fine grain measurement layer built
into the provider’s infrastructure.
Though these three approaches may seem similar, their
differences have non trivial implications for the realization
of a cost verification strategy.

4.1

Component 1

Component 2

Component 3

Cloud
Resources
e.g.
-Bandwidth
-Storage
-CPU

Component 4

Figure 2: Sample cloud application showing measurement via instrumentation
to interface with mock resource objects. This would allow
for extensive testing of various control flow paths in an application and would track the corresponding resources that
are requested by individual units. These could then be rolled
up into higher level component tests, and developers could
more easily track how resource costs are accrued in their applications at various levels of abstraction, design, and architecture. The ability to ensure extensive test coverage (likely
through unit and component testing) for the purposes of
and from the perspective of resource cost tracking would
generally be well handled by an instrumentation approach.
Let’s now consider a concrete example that could use an
instrumentation approach. Suppose we are responsible for a
corporate wide enterprise accounting application for a very
large, multinational corporation with several divisions and
ultimately hundreds of separate accounting units. This software is internal to the corporation and its users are employees in the various payroll departments of the different
accounting units.
The application’s components are the payroll applications
for each of the corporation’s accounting units, and each component is comprised of subcomponents that query relevant
databases for pay rates, timecards, personnel data, etc. All
of the components and subcomponents have been migrated
to the cloud, and it is our responsibility to ensure both that
the components run in a timely manner and that they stay
within a budgeted execution cost.
The overall payroll application is a fairly extensive end to
end solution for locating and executing the components from
the different accounting units, aggregating data, interfacing
with outside entities (i.e. banks) for direct deposits, etc.
Under the instrumentation approach, we may either modify the individual components to make use of instrumentation hooks, or we would require that the different IT departments for each accounting unit make the change so that we
can collect data for the overall application. Although this
could represent a significant retrofitting of existing applications, it allows different organizations to manage their own
components as long as they use a common instrumentation
approach.

Instrumentation Approach

The instrumentation approach is characterized by surrounding each component with a measurement layer, as in
Figure 2. This layer provides an interface to cloud resources
such as bandwidth and storage, but it is only a pass through
to the actual resource layer. The measurement layer simply
keeps track of the amount of resources requested before forwarding requests to the resource layer. A very basic measurement layer could be very easy to implement.
The potential downside here is that the user may need
to update the software under test to use this measurement
layer. If the software is developed using the measurement
layer from the start, the burden for such an approach could
be minimal. As an alternative, the user may only need to run
the source code through a preprocessor or special compiler to
automate the task of inserting the measurement layer where
appropriate. This would be more complicated to implement
but would ease the burden on the user considerably.
Note that the contribution of processing time toward the
overall cost of running a component would not be instrumented. It is a fairly simple calculation (end time minus
start time) and would be determined much as described below for other resources in the detection approach. However,
any processing time spent in the measurement layer would
be subtracted from the overall time as it will not be used
during a production run of the software.
An instrumentation approach could be used to analyze
and test software locally (off the cloud). The measurement
layer could forward requests to local resource providers (locally hosted DB, sockets, etc.). But the measurement layer
could also forward requests to the cloud provider and so
the instrumentation approach could be used for verification
directly in the cloud as well.
The instrumentation approach is very much compatible
with (and similar to) unit testing. A special implementation of the measurement layer could be made for unit tests

4.2

Detection Approach

The detection approach is less invasive than the measurement approach. It relies on an outside entity to monitor
(i.e. observe via listeners, logs, or any other possible means)

20

tection approach on the surface seems very promising since
the various IT departments would not have to update their
components. In practice, the detection approach may be
quite challenging since the different departments may use
dissimilar systems. We might require each department to
be responsible for providing the detection functionality for
their own unique systems, or we might have to ensure that
the solution we provide accounts for all of the different systems in use (if that is even possible).

Component 1

Component 2

Component 3

Cloud
Resources
e.g.
-Bandwidth
-Storage
-CPU

4.3

Provider Based Approach

Another possible approach is for the cloud provider to
make available an option to enable fine grain cost and/or
resource measurements, as shown in Figure 4. This would be
similar from a user’s perspective to the detection approach
since it requires no update to the cloud application. An
approach to fine grain billing can be found in [10], and could
form the basis for a provider based measurement option.
To clarify what this approach entails, let’s enumerate what
a cloud provider would need to add beyond what a basic
cloud billing system would already record and make available to users. Currently, cloud providers generally charge
on the scale of per hour for processing costs, per gigabyte
for bandwidth costs, and per gigabyte per month for storage
costs.
Ultimately, we’d like to be able to record resource usage
down to the unit and procedure level of cloud applications.
This would entail recording CPU usage down to the millisecond or even nanosecond, bandwidth down to the kilobyte or
even byte, and storage usage also to the kilobyte or even
byte level. To reiterate, the reason for these fine grain metrics would be for us to understand and project costs as we
dramatically scale up the number of transactions, combine
transactions, and/or iterate through large amounts of data
for a given transaction.
Though some cloud providers make available some amount
of data below the basic billing scale, Amazon’s CloudWatch
feature is currently the closest to enabling the types of metrics that would be required from the cloud provider for fine
grain cost verification of cloud applications [1]. Though not
yet at the level we’re proposing, its features at least demonstrate the demand for and usefulness of cloud resource usage
metrics far below the per hour, per gigabyte scale.
Note that an interesting aspect of the cloud provider approach is that the software would likely have to be tested in
the cloud. In many ways, this is ideal since it more closely
simulates the production environment. However, it could
also incur unwanted costs. For an in-depth discussion of
strategies for and levels of testing in the cloud, see [11].
Furthermore, this approach by definition is tied to a specific cloud provider. The results of the measurements will
only be applicable to that particular provider, and the user
may not be able to derive enough information from the results to determine what the cost would be using a different
provider. Although this kind of comparison shopping is not
the main focus of the current work, it is certainly a related
concern.
Allowing users to comparison shop other cloud providers
based on resource usage profiles is probably not an incentive
for cloud providers to make low level cost and resource usage data available to users. However, there could be other
incentives. Amazon’s CloudWatch is offered for a fee [1],
so deriving revenue from these metrics is one option. If the

Component 4

Figure 3: Sample cloud application showing measurement via detection

requests for storage and bandwidth resources as shown in
Figure 3. Such an entity would be running apart from the
actual application under test and would not interfere with
its execution. This approach would likely require no modification to the software under test, so it would be easy to
use.
The monitoring application is responsible for measuring
resource usage using whatever mechanisms are made available to it. For example, a socket library may allow for logging of bandwidth usage. Likewise, a database provider may
have mechanisms for triggering detection software (listeners)
when updates are made or queries are run.
The detection approach is assumed to run off the cloud.
(A similar approach that runs on the cloud is discussed in
the next section.) We may wish to verify the resource usage
of our cloud applications off the cloud for several reasons.
We may want to develop a cost profile (or resource usage
profile) independent of cloud provider that can help us decide which cloud provider’s pricing scheme would result in
the lowest cost for our application. Furthermore, certain
levels of testing (e.g. unit level) or harnesses (isolating certain components) may not be compatible with running on
the cloud. Our testing needs may also be immediate, isolated, and/or sporadic which may decrease the advantages
of running in the cloud.
A significant problem with the detection approach is that
its implementation could be quite difficult. Depending on
the different computing environments (DB, OS, etc.) supported by the implementation or test framework, the work
could be substantial. Since there is no standard for making logging information available for storage and bandwidth
resources, the monitoring applications would have to include some individual strategy for each database provider
and bandwidth provisioning mechanism. Furthermore, the
capabilities for performing detection and logging are likely
to vary from vendor to vendor, so achieving a common set
of functionality in this space may be difficult or impossible.
This is different than the instrumentation approach, under
which it is assumed that we have a well defined interface for
requesting resources but we are simply inserting a layer to
take measurements. Unlike that approach, the detection
approach relies on the resource providers’ functionality for
detecting and logging resource usage.
Returning to our accounting application example, the de-

21

tion. Given the advantages and limitations of the various
approaches, we are inclined to believe that an instrumentation based approach would make a good first step in measuring cloud costs.
The detection approach has significant barriers to implementation and/or realization. Because this approach assumes the usage of local resources (testing locally, not on the
cloud), it is complicated by the various resource providers’
capabilities and interfaces.
The cloud provider approach is interesting and may ultimately be the easiest and most accurate, but it relies on the
willingness of cloud providers to make metrics available at a
very fine scale.
An instrumentation based approach could start very simply and eventually be made more complex (with the intent
of making it more useful for the user). It can be used for
cost verification strategies that run on or off the cloud. The
instrumentation approach does not rely on metrics made
available by cloud providers, so it can both compare cost
profiles from different providers and it can stand independent of various levels of cloud providers’ measurement features. Though each of the three approaches deserves more
analysis, the instrumentation approach would likely be the
best for independent research on measurement based cost
verification.

Component 1

Component 2

Component 3

Cloud
Resources
e.g.
-Bandwidth
-Storage
-CPU

Component 4

Figure 4: Sample cloud application showing measurement within the cloud provider
demand for such metrics increases, it may become an assumption that a cloud provider will make them available as
part of a value added package expected by cloud users.
Returning once again to our enterprise accounting and
payroll example, this system would fit very well with a cloud
provider based system. Each of the separate divisions could
provide their own components to the application, none of
which would have to be updated to use the system. All the
cost verification data could be aggregated in one place and
used through one interface.
On the other hand, we would be completely reliant on
the cloud provider for the kinds of metrics that would enable our cost verification. Furthermore, our cost verification
approach may not be portable to other providers since the
metrics and gathering would likely be provider specific.

5.

7.

[1] Amazon. Amazon cloudwatch, Feb. 2012.
[2] R. Bala and S. Carr. Usage-based pricing of software
services under competition. Journal of Revenue and
Pricing Management, 9(3):204–216, 2010.
[3] K. Buell and J. Collofello. Transaction level economics
of cloud applications. Services, IEEE Congress on,
0:515–518, 2011.
[4] R. Buyya, C. S. Yeo, S. Venugopal, J. Broberg, and
I. Brandic. Cloud computing and emerging it
platforms: Vision, hype, and reality for delivering
computing as the 5th utility. Future Gener. Comput.
Syst., 25:599–616, June 2009.
[5] S. Bygde and B. Lisper. Towards an automatic
parametric wcet analysis. In R. Kirner, editor, 8th
Intl. Workshop on Worst-Case Execution Time
(WCET) Analysis, Dagstuhl, Germany, 2008.
[6] E. Deelman, G. Singh, M. Livny, B. Berriman, and
J. Good. The cost of doing science on the cloud: The
montage example. In High Performance Computing,
Networking, Storage and Analysis, 2008. SC 2008.
International Conference for, pages 1 –12, 2008.
[7] N. Dun, K. Taura, and A. Yonezawa. Paratrac: a
fine-grained profiler for data-intensive workflows. In
Proceedings of the 19th ACM International Symposium
on High Performance Distributed Computing, HPDC
’10, pages 37–48, New York, NY, USA, 2010. ACM.
[8] I. Gorton, J. Chase, A. Wynne, J. Almquist, and
A. Chappell. Services + components = data intensive
scientific workflow applications with medici. In
Proceedings of the 12th International Symposium on
Component-Based Software Engineering, CBSE ’09,
pages 227–241, Berlin, Heidelberg, 2009.
Springer-Verlag.
[9] D. Nurmi, R. Wolski, C. Grzegorczyk, G. Obertelli,
S. Soman, L. Youseff, and D. Zagorodnov. The

FUTURE WORK

The most important piece of future work here is to further
pursue implementations of the outlined approaches for measurement based cost verification. This kind of work could
help identify more advantages and drawbacks beyond those
discussed here. Once a workable approach is identified, a
complete implementation could yield important data and
results for further analysis. More importantly, this could
be used to compare measurements gathered before deployment to actual data gathered in the field and could point
to additional areas of research for fine tuning dynamic cost
verification.
For the instrumentation and detection approaches, an implementation would realize the descriptions discussed earlier. For the provider based approach, a research oriented
cloud implementation (e.g. [9]) could be modified to provide the metrics required to enable cost verification from
the cloud user’s perspective. This could form the basis for
(and could provide encouragement for) further commercial
development of low level cost and resource tracking in cloud
providers.

6.

REFERENCES

CONCLUDING REMARKS

We have discussed the importance of verifying the cost of
running applications in the cloud. Cost verification through
dynamic measurement is practical and may be more immediately available to practitioners than is cost verification
through static analysis. We have presented three measurement based approaches for cloud application cost verifica-

22

eucalyptus open-source cloud-computing system. In
Proceedings of the 2009 9th IEEE/ACM International
Symposium on Cluster Computing and the Grid,
CCGRID ’09, pages 124–131, Washington, DC, USA,
2009. IEEE Computer Society.
[10] K.-W. Park, S. K. Park, J. Han, and K. H. Park.
Themis: Towards mutually verifiable billing
transactions in the cloud computing environment. In
Cloud Computing (CLOUD), 2010 IEEE 3rd
International Conference on, pages 139 –147, 2010.
[11] P. Robinson and C. Ragusa. Taxonomy and
requirements rationalization for infrastructure in
cloud-based software testing. In Cloud Computing
Technology and Science (CloudCom), 2011 IEEE
Third International Conference on, pages 454 –461,
December 2011.
[12] T. Shibata, S. Choi, and K. Taura. File-access
patterns of data-intensive workflow applications and
their implications to distributed filesystems. In
Proceedings of the 19th ACM International Symposium
on High Performance Distributed Computing, HPDC
’10, pages 746–755, New York, NY, USA, 2010. ACM.

[13] H.-L. Truong and S. Dustdar. Composable cost
estimation and monitoring for computational
applications in cloud computing environments.
Procedia Computer Science, 1(1):2175 – 2184, 2010.
ICCS 2010.
[14] H.-L. Truong and S. Dustdar. Cloud computing for
small research groups in computational science and
engineering: current status and outlook. Computing,
91(1):75–91, Jan. 2011.
[15] R. Wilhelm, J. Engblom, A. Ermedahl, N. Holsti,
S. Thesing, D. Whalley, G. Bernat, C. Ferdinand,
R. Heckmann, T. Mitra, F. Mueller, I. Puaut,
P. Puschner, J. Staschulat, and P. Stenström. The
worst-case execution-time problem–overview of
methods and survey of tools. ACM Trans. Embed.
Comput. Syst., 7:36:1–36:53, May 2008.
[16] J. Yu and R. Buyya. A taxonomy of scientific
workflow systems for grid computing. SIGMOD Rec.,
34:44–49, September 2005.

23

Variable Strength Interaction Testing of Components
Myra B. Cohen
Peter B. Gibbons
Warwick B. Mugridge
Dept. of Computer Science
University of Auckland
Private Bag 92019
Auckland, New Zealand
myra,peter-g,rick@cs.auckland.ac.nz
Abstract
Complete interaction testing of components is too costly
in all but the smallest systems. Yet component interactions
are likely to cause unexpected faults. Recently, design of experiment techniques have been applied to software testing
to guarantee a minimum coverage of all -way interactions
across components. However, is always ﬁxed. This paper
examines the need to vary the size of in an individual test
suite and deﬁnes a new object, the variable strength covering array, that has this property. We present some computational methods to ﬁnd variable strength arrays and provide
initial bounds for a group of these objects.

1. Introduction
In order to shorten development times, reduce costs and
improve quality, many organizations are developing software utilizing existing components. These may be commercial off-the-shelf (COTS) or internally developed for
reuse among products. The utilization of existing components requires new development and veriﬁcation processes
[2]. In particular the interaction of these new components
with each other as well as with the newly developed components within the application must be tested.
Software is becoming increasingly complex in terms of
components and their interactions. Traditional methods of
testing are useful when searching for errors caused by unmatched requirements. However, component based development creates additional challenges for integration testing. The problem space grows rapidly when searching
for unexpected interactions. Suppose we have ﬁve components, each with four possible conﬁgurations. We have
   potential interactions. If we combine 100 com-

Charles J. Colbourn
James S. Collofello
Dept of Computer Science and Engineering
Arizona State University
P.O. Box 875406
Tempe, Arizona 85287
charles.colbourn,collofello@asu.edu

ponents there are  possible combinations. This makes
testing all combinations of interactions infeasible in all but
the smallest of systems.
Instead one can create test suites that guarantee pairwise
or -wise coverage. For instance we can cover all pairwise
interactions for ten components, each with four possible
conﬁgurations, using only 25 test cases. A covering array,
      is an    array such that every   subarray contains all ordered subsets from  symbols of size
at least once. The covering array number is the minimum
 required to satisfy the parameters    .
Covering arrays have been used for software interaction
testing by D. Cohen et al. in the Automatic Efﬁcient Test
Generator (AETG) [5]. Williams et al. use these to design
tests for the interactions of nodes in a network [12]. Dalal
et al. present empirical results suggesting that testing of
all pairwise interactions in a software system indeed ﬁnds a
large percentage of existing faults [7]. In further work, Burr
et al. provide more empirical results to show that this type
of test coverage is effective [3].
In a software test, the columns of the covering array represent the  components or ﬁelds. Each component has 
levels or conﬁgurations. The ﬁnal test suite is an    array where  is the number of test cases and each test contains one conﬁguration from each component. By mapping
a software test problem to a covering array of strength we
can guarantee that we have tested all -way interactions.
In many situations pairwise coverage is sufﬁcient for
testing. However, we must balance the need for stronger
interaction testing with the cost of running tests. For instance a       can be achieved for as little as 16
tests, while a    	   requires at least 64 tests. In
order to appropriately use our resources we want to focus
our testing where it has the most potential value.
The recognition that all software does not need to be

Proceedings of the 27th Annual International Computer Software and Applications Conference (COMPSAC’03)
0730-3157/03 $ 17.00 © 2003 IEEE

tested equally is captured in the concept of risk-based testing [1]. Risk-based testing prioritizes testing based on
the probability of a failure occurring and the consequences
should the failure occur. High risk areas of the software are
identiﬁed and targeted for more comprehensive testing.
The following scenarios point to the need for a more ﬂexible way of examining interaction coverage.

¯ We completely test a system, and ﬁnd a number of
components with pairwise interaction faults. We believe this may be caused by a bad interaction at a higher
strength, i.e. some triples or quadruples of a group
of components. We may want to revise our testing
to handle the “observed bad components” at a higher
strength.
¯ We thoroughly test another system but have now revised some parts of it. We want to test the whole system with a focus on the components involved in the
changes. We use higher strength testing on certain
components without ignoring the rest.
¯ We have computed software complexity metrics on
some code, and ﬁnd that certain components are more
complex. These warrant more comprehensive testing.
¯ We have certain components that come from automatic
code generators and have been more/less thoroughly
tested than the human generated code.
¯ One part of a project has been outsourced and needs
more complete testing.
¯ Some of our components are more expensive to test
or to change between conﬁgurations. We still want to
test for interactions, but cannot afford to test more than
pairwise interactions for this group of components.
While the goal of testing is to cover as many component interactions as possible, trade-offs must occur. This
paper examines one method for handling variable interaction strengths while still providing a base level of coverage. We deﬁne the variable strength covering array, provide
some initial bounds for these objects and outline a computational method for creating them.

2. Background
Suppose we are testing new integrated RAID controller
software. We have four components, (RAID level, operating system (OS), memory conﬁguration and disk interface).
Each one of these components has three possible conﬁgurations. We need 81 tests to test all interactions. Instead
we can test all pairwise interactions of these components
with only nine tests. Perhaps though, we know that there

RAID
Level
RAID 0
RAID 1
RAID 5

Component
Operating
Memory
System
Conﬁg
Windows XP
64 MB
Linux
128 MB
Novell Netware 6.x
256 MB

Disk
Interface
Ultra-320 SCSI
Ultra-160 SCSI
Ultra-160 SATA

Table 1. Raid integrated controller system: 4
components, each with 3 conﬁgurations

are more likely to be interaction problems between three
components: RAID level, OS and memory. We want to
test these interactions more thoroughly. But it may be too
expensive to run tests involving all three way interactions
among components. In this instance we can use three-way
interaction testing among the ﬁrst three components while
maintaining two-way coverage for the rest. We still have a
minimal coverage guarantee across the components and we
still do not need to run 81 tests. The test suite shown in Table 2 provides this level of variable strength coverage with
27 tests.

RAID
Level
RAID 0
RAID 0
RAID 5
RAID 1
RAID 0
RAID 0
RAID 1
RAID 0
RAID 5
RAID 0
RAID 5
RAID 5
RAID 1
RAID 5
RAID 0
RAID 1
RAID 0
RAID 1
RAID 1
RAID 5
RAID 1
RAID 1
RAID 1
RAID 5
RAID 5
RAID 5
RAID 0

Component
Operating Memory
System
Conﬁg
Linux
128 MB
Novell
128 MB
Linux
64 MB
XP
128 MB
Novell
256 MB
XP
128 MB
Novell
256 MB
Linux
64 MB
XP
256 MB
XP
64 MB
Novell
128 MB
XP
128 MB
Linux
256 MB
XP
64 MB
XP
256 MB
Linux
128 MB
Novell
64 MB
Novell
64 MB
Linux
64 MB
Novell
64 MB
XP
256 MB
Novell
128 MB
XP
64 MB
Linux
256 MB
Linux
128 MB
Novell
256 MB
Linux
256 MB

Disk
Interface
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 320
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 320
Ultra 320
Ultra 160-SATA
Ultra 320
Ultra 160-SATA
Ultra 160-SCSI
Ultra 160-SATA
Ultra 160-SATA
Ultra 320
Ultra 320
Ultra 160-SATA
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 160-SATA
Ultra 160-SCSI
Ultra 160-SATA
Ultra 320
Ultra 320
Ultra 160-SCSI
Ultra 160-SATA

Table 2. Variable strength array for Table 1

Proceedings of the 27th Annual International Computer Software and Applications Conference (COMPSAC’03)
0730-3157/03 $ 17.00 © 2003 IEEE

Commercial software test generators, like AETG, provide only a ﬁxed level of interaction strength [5]. We might
use this to build two separate test suites and run each independently, but this is a more expensive operation and
does not really satisfy the desired criteria. We could instead
just default to the higher strength coverage with more tests.
However, the ability to tune a test suite for speciﬁc levels
of coverage is highly desirable, especially as the number of
components and levels increases. Therefore it is useful to
deﬁne and create test suites with ﬂexible strengths of interaction coverage and to examine some methods for building
these.

3. Deﬁnitions
In a covering array,       ,  is called the
strength,  the degree and  the order. A covering array
is optimal if it contains the minimum possible number of
rows. We call this minimum number the covering array
number,     . For example,      
[4].
A mixed level covering array, denoted as a
            , is an    array on  symbols, where     , with the following properties:



	  contains only elements
1. Each column 	 
from a set 
 with 
    .
2. The rows of each    sub-array cover all
of values from the  columns at least once.

tuples

We can use a shorthand notation to describe our mixed covering array by combining  ’s that are the same. For example if we have three  ’s of size two we can write this  .
Consider an       ½  ¾   . This can also
be written as an              where
    and         .
The following holds:







1. The columns are partitioned into  groups 
  
  

where group 
 contains  columns. The ﬁrst 
columns belong to the group 
 , the next  columns
belong to group 
 , and so on.
2. If column 

 
 , then 
    .

We can now use this notation for a ﬁxed-level covering
array as well.       indicates that there are  parameters each containing a set of  symbols. This makes it
easier to see that the values from different components can
come from different symbol sets.
A variable strength covering array, denoted as a
            , is an    mixed level
covering array, of strength  containing , a multi-set of
disjoint mixed level covering arrays each of strength  .

We can abbreviate duplicate  ’s in the multi-set using a similar notation to that of the covering arrays. Suppose we have two identical covering arrays       in
. This can be written as       . Ordering of the
columns in the representation of a   is important since
the columns of the covering arrays in are listed consecutively from left to right.
An example of a             
can be seen in Table 3. The overall array is a mixed level
array of strength two with nine columns containing three
symbols and two columns containing two. There are three
sub-arrays each with strength three. All three way interactions therefore among columns 0-2, 3-5, 6-8 are included.
All two way interactions among all columns are also covered. This has been achieved with 27 rows which is the optimal size for a      . A covering array that would
cover all three way interactions for all 11 columns, on the
other hand, might need as many as 52 rows.

4. Construction Methods
Fixed strength covering arrays can be built using algebraic constructions if we have certain parameter combinations of   and  [4, 12]. Alternately we may use computational search algorithms [5, 6, 11]. Greedy algorithms
form the basis of the AETG and the IPO generators [5, 11].
It has also been shown that simulated annealing is effective
for building covering arrays of both mixed and ﬁxed levels
[6, 10]. Since there are no known constructions for variable
strength arrays at the current time we have chosen to use
a computational search technique to build these. We have
written a simulated annealing program that has been used
to produce all of the results presented in this paper.

4.1. Simulated Annealing
Simulated annealing is a variant of the state space search
technique for solving combinatorial optimization problems.
The hope is that the algorithm ﬁnds close to an optimal solution. Most of the time, however, we do not know when
we have reached an optimal solution for the covering array
problem. Instead such a problem can be speciﬁed as a set 	
of feasible solutions (or states) together with a cost  
  associated with each feasible solution 
 . An optimal solution
corresponds to a feasible solution with overall (i.e. global)
minimum cost. For each feasible solution 
  	, we deﬁne
a set  of transformations (or transitions), each of which
can be used to change 
 into another feasible solution 
 ¼ .
The set of solutions that can be reached from 
 by applying
a transformation from  is called the neighborhood  
 
of 
 .
To start, we randomly choose an initial feasible solution. The algorithm then generates a set of sequences (or

Proceedings of the 27th Annual International Computer Software and Applications Conference (COMPSAC’03)
0730-3157/03 $ 17.00 © 2003 IEEE

0
0
1
2
2
2
1
0
2
1
1
0
2
0
1
1
2
0
2
0
1
2
0
1
0
1
2

2
1
2
1
2
0
0
0
0
2
0
1
2
0
2
0
1
2
0
1
1
1
0
1
2
1
2

Table 3.

2
2
2
0
0
0
0
0
2
0
2
0
1
2
1
1
2
1
1
1
2
1
1
1
0
0
2

0
1
2
0
2
0
1
0
2
0
2
0
2
2
2
1
2
1
1
1
0
0
1
1
1
2
0



1
2
1
0
2
2
1
2
0
1
1
0
1
2
0
0
0
2
0
0
1
0
1
1
2
2
2

2
0
0
2
0
2
2
1
0
0
2
0
1
1
2
1
1
1
0
2
1
1
0
1
2
2
0

2
1
2
2
1
2
1
0
2
0
1
1
0
1
0
1
0
2
0
1
0
2
2
2
0
1
0

    

2
1
1
0
2
2
2
0
0
2
1
1
0
2
1
0
2
2
0
0
1
1
1
0
2
0
1



2
0
0
0
1
1
0
1
1
1
2
1
2
2
1
1
0
0
0
2
0
2
1
2
2
0
2

0
0
0
0
1
1
1
1
0
0
1
1
0
0
0
0
1
1
0
0
0
0
1
1
1
0
1

1
0
0
1
0
0
0
1
1
1
0
1
1
0
0
1
1
1
1
0
0
0
1
0
0
1
1

    

Markov chains) of trials. If   ¼      then the transition is accepted. If the transition results in a feasible solution  ¼ of higher cost, then  ¼ is accepted with probability
¼
       , where  is the controlling temperature of
the simulation. The temperature is lowered in small steps
with the system being allowed to approach “equilibrium” at
each temperature through a sequence of trials at this temperature. Usually this is done by setting    , where
 (the control decrement) is a real number slightly less than
.
The idea of allowing a move to a worse solution helps
avoid being stuck in a bad conﬁguration (a local optimum),
while continuing to make progress. Sometimes we know
the cost of an optimal solution and can stop the algorithm
when this is reached. Otherwise we stop the algorithm when
insufﬁcient progress is being made (as determined by an
appropriately deﬁned stopping condition). In this case the
algorithm is said to be frozen.
Simulated annealing has been used by Nurmela and
Östergård [9], for example, to construct covering designs
which have a structure very similar to covering arrays. It
has also been used by Stardom and Cohen, et al. to generate
minimal ﬁxed strength covering arrays for software testing
[6, 10].
In the simulated annealing algorithm the current feasible

solution is an approximation  to a covering array in which
certain 	-subsets are not covered. The cost function is based
on the number of 	-subsets that are not currently covered.
A covering array itself will have cost 	. In the variable
strength array, the cost is 	 when (i) all of the 	-subsets are
covered and (ii) for each covering array of strength 	¼ in  ,
all 	¼ -subsets are covered. A potential transition is made
by selecting one of the 
 -sets belonging to  and then replacing a random point in this 
 -set by a random point not
in the 
 -set. We calculate only the change in numbers of
	-sets that will occur with this move. Since the subsets of
 are disjoint, any change we make can affect the overall
coverage of the  and at most one of the subsets of  .
This means that the extra work required for ﬁnding variable
strength arrays over ﬁxed strength arrays does not grow in
proportion to the number of subsets of higher strength.
We keep the number of blocks (test cases) constant
throughout a simulated annealing run and use the method
described by Stardom to determine a ﬁnal array size [10].
We start with a large random array and then bisect our array
repeatedly until we ﬁnd a best solution.

4.2. Program Parameters
Good data structures are required to enable the relative
cost of the new feasible solution to be calculated efﬁciently,
and the transition (if accepted) to be made quickly. We
build an exponentiation table prior to the start of the program. This allows us to approximate the transition probability value using a table lookup. We use ranking algorithms from [8] to hold the values of our 	-sets. This allows
us to generalize our algorithms for different strengths without changing the base data structure. When calculating the
change in cost for each transition, we do not need to recalculate all of the 	-sets in a test case, but instead only calculate
the change (	-1 subsets).
A constant is set to determine when our program is
frozen. This is the number of consecutive trials allowed
where no change in the cost of the solution has occurred.
For most of our trials this constant has been set to 1,000.
The cooling schedule is very important in simulated annealing. If we cool too quickly, we freeze too early because the probability of allowing a worse solution drops too
quickly. If we cool too slowly or start at too high a temperature, we allow too many poor moves and fail to make
progress
Therefore, if we start at a low temperature and cool
slowly we can maintain a small probability of a bad move
for a long time allowing us to avoid a frozen state, at the
same time continuing to make progress. We have experimented using ﬁxed strength arrays compared with known
algebraic constructions (see [6]). We have found that a starting temperature of approximately 0.20 and a slow cooling

Proceedings of the 27th Annual International Computer Software and Applications Conference (COMPSAC’03)
0730-3157/03 $ 17.00 © 2003 IEEE

C

VCA

      












   

    
    
   
   

  ,
  ,
   

   

       

      

   
   
    


   
    
   
   ,
   
     
    
     


   
    

Min
N
16
27
27
27
27
33

Max
N
17
27
27
27
27
33

Avg
N
16.1
27
27
27
27
33

33*
34
33*
34
41
50
67
36
64
100
125

35

34.8

35
42
51
69
36
64
104
125

34.9
41.4
50.8
67.6
36
64
101
125

125
171
180
214
100
100
304

125
173
180
216
100
100
318

125
172.5
180
215
100
100
308.5

Table 4. Table of sizes for variable strength
arrays after 10 runs (We have omitted the parameter



in our notation for covering arrays in this table due to
space limitations.)

* The minimum values for these VCA’s were found during a separate set of experiments

factor, , of between 0.9998 and 0.99999 every 2,500 iterations works well. Using these parameters, the annealing
algorithm completes in a “reasonable” computational time
on a PIII 1.3GHz processor running Linux. For instance,
the ﬁrst few  ’s in Table 4, complete in seconds, while
the larger problems, such as the last   in Table 4, complete within a few hours. The delta in our cost function is
counted as the change in -sets from our current solution.
Since we can at any point make changes to both the base
array and one of the higher strength arrays, these changes
are added together.
As there is randomness inherent in this algorithm, we run
the algorithm multiple times for any given problem.

5. Results
Table 4 gives the minimum, maximum and average sizes
obtained after 10 runs of the simulated annealing algorithm
for each of the associated  ’s. Each of the 10 runs uses

a different random seed. A starting temperature of .20 and
a decrement parameter of .9998 is used in all cases. In two
cases a smaller sized array was found during the course of
our overall investigation, but was not found during one of
these runs. The numbers are included in the table as well
and are labeled with an asterisk, since these provide a previously unknown bound for their particular arrays. In each
case we show the number of tests required for the base array of strength two. We then provide some examples with
variations on the contents of  . Finally we show the arrays
with all of the columns involved in strength three coverage.
We have only shown examples using strength two and three,
but our methods should generalize for any strength .
What is interesting in Table 4 is that the higher strength
sub-arrays often drive the size of the ﬁnal test suite. Such
is the case in the ﬁrst and second   groups in this table.
We can use this information to make decisions about how
many components can be tested at higher strengths. Since
we must balance the strength of testing with the ﬁnal size
of the test suite we can use this information in the design
process.
Of course there are cases where the higher strength subsets do not determine the ﬁnal test suite size since the number of test cases required is a combination of the number
of levels and the strength. In the last   group in Table
4 the two components each with 10 levels require a minimum of 100 test cases to cover all pairs. In this case we can
cover all of the triples from the 20 preceding columns with
the same number of tests. In such cases, the quality of the
tests can be improved without increasing the number of test
cases. We can set the strength of the 20 preceding columns
to the highest level that is possible without increasing the
test count.
Both situations are similar in the fact that they allow us
to predict a minimum size test suite based on the ﬁxed level
sub-arrays. Since there are better known bounds for ﬁxed
strength arrays we can use this information to drive our decision making processes in creating test suites that are both
manageable in size while providing the highest possible interaction strengths.

6. Conclusions
We have presented a combinatorial object, the variable
strength covering array, which can be used to deﬁne software component interaction tests and have discussed one
computational method to produce them. We have presented
some initial results with sizes for a group of these objects.
These arrays allow one to guarantee a minimum strength
of overall coverage while varying the strength among disjoint subsets of components. Although we present these objects for their usefulness in testing component based software systems they may be of use in other disciplines that

Proceedings of the 27th Annual International Computer Software and Applications Conference (COMPSAC’03)
0730-3157/03 $ 17.00 © 2003 IEEE

currently employ ﬁxed strength covering arrays.
The constraining factor in the ﬁnal size of the test suite
may be the higher strength sub-array. We can often get a
second level of coverage for almost no extra cost. We see
the potential to use these when there is a need for higher
strength, but we cannot afford to create an entire array of
higher strength due to cost limitations.
Where the constraining factor is the large number of levels in a set of ﬁelds at lower strength, it may be possible to
increase the strength of sub-arrays without additional cost,
improving the overall quality of the tests.
Another method of constructing ﬁxed strength covering
arrays is to combine smaller arrays or related objects and to
ﬁll the uncovered -sets to complete the desired array [4].
We are currently experimenting with some of these techniques to build variable strength arrays. Since the size of a
   may be dependent on the higher strength arrays, we
believe that building these in isolation followed by annealing or other processes to ﬁll in the missing lower strength
-sets will provide fast and efﬁcient methods to create optimal variable strength arrays.

Acknowledgments
Research is supported by the Consortium for Embedded and Internetworking Technologies and by ARO grant
DAAD 19-1-01-0406. Thanks to the Consortium for Embedded and Internetworking Technologies for making a
visit to ASU possible.

References
[1] J. Bach. James Bach on risk-based testing In STQE
Magazine, Nov/Dec,1999.
[2] L. Brownsword, T. Oberndorf and C. Sledge. Developing new processes for COTS-based systems.
IEEE Software, 17(4):48–55, 2000.
[3] K. Burr and W. Young. Combinatorial test techniques: Table-based automation, test generation
and code coverage. In Proc. of the Intl. Conf. on
Software Testing Analysis & Review, 1998, San
Diego.

[4] M. Chateauneuf and D. Kreher. On the state of
strength-three covering arrays. Journal of Combinatorial Designs, 10(4):217–238, 2002
[5] D. M. Cohen, S. R. Dalal, M. L. Fredman, and
G. C. Patton. The AETG system: an approach
to testing based on combinatorial design. IEEE
Transactions on Software Engineering, 23(7):437–
44, 1997.
[6] M. B. Cohen, C. J. Colbourn, P. B. Gibbons and
W. B. Mugridge. Constructing test suites for interaction testing. In Proc. of the Intl. Conf. on
Sofware Engineering (ICSE 2003), 2003, pp. 3848 , Portland.
[7] S. R. Dalal, A. J. N. Karunanithi, J. M. L. Leaton,
G. C. P. Patton, and B. M. Horowitz. Model-based
testing in practice. In Proc. of the Intl. Conf. on
Software Engineering,(ICSE ’99), 1999, pp. 28594, New York.
[8] D. L. Kreher and D. R. Stinson. Combinatorial
Algorithms, Generation, Enumeration and Search.
CRC Press, Boca Raton, 1999.
[9] K. Nurmela and P. R. J. Östergård. Constructing
covering designs by simulated annealing. Technical report, Digital Systems Laboratory, Helsinki
Univ. of Technology, 1993.
[10] J. Stardom. Metaheuristics and the search for covering and packing arrays. Master’s thesis, Simon
Fraser University, 2001.
[11] K. C. Tai and L. Yu. A test generation strategy for
pairwise testing. IEEE Transactions on Software
Engineering, 28(1):109-111, 2002.
[12] A. W. Williams. Determination of test conﬁgurations for pair-wise interaction coverage In Proc.
Thirteenth Int. Conf. Testing Communication Systems, 2000, pp. 57–74.

Proceedings of the 27th Annual International Computer Software and Applications Conference (COMPSAC’03)
0730-3157/03 $ 17.00 © 2003 IEEE

A System Dynamics Software Process Simulator for
Staffing Policies Decision Support
Dr. James Collofello
Dept. of Computer Science and Engineering
Arizona State University
Tempe, Arizona 85287-5406
(602)965-3190
collofe/lo@ am. edu

Dan Houston
Honeywell, Inc. and Dept. of Industrial and
Management Systems Engineering
Arizona State University
Douglas M. Sycamore
Motorola Communication Systems Division
Scottsdale, Arizona

Ioana Rus, Anamika Chauhan
Dept. of Computer Science and Engineering
Arizona State University

Dr. Dwight Smith-Daniels
Department of Management
Arizona State University

Abstract

study software development include the opportunity to
engage in experimentation with various project
configurations, lower cost and time requirements, and
the ability to conduct experiments without the
intrusiveness of process observation.
This paper describes the use of System Dynamics
Modeling in a software process model designed for the
purpose of studying workforce staffing control decisions
in software projects. Our study estimates the effects of
three different staffing policies on cost, schedule and
rework for a software development project. The results
of this study indicate that in some cases, overstaffing a
project can be a desirable alternative for anticipating
staff attrition.

Staff attrition is a problem often faced by software
development organizations. How can a manager plan
f o r the risk of losses due to attrition? Can policies f o r
this purpose be formulated to address hidher specific
organization and project?
A software development process simulator tuned to
a specific organization is used f o r running "what-if"
scenarios f o r assessing the effects of managerial
staffing decisions on a project's budget, schedule and
quality. We developed a system dynamics simulator of
an incremental software development process and used
it f o r analyzing the effect of the following policies: to
replace engineers who leave the project, to overstaff in
the beginning of the project or to do nothing, hoping
that the project will still be completed in time and
within budget. This paper presents the simulator, the
experiments that we ran, the results that we obtained
and our analysis and conclusions.

2. System Dynamics Modeling
A software process model represents the process
components (activities, products, and roles), together
with their attributes and relationships, in order to satisfy
the modeling objectives. These objectives may include
the facilitation of better understanding and
communication about a complex system, evaluation of
alternative process improvement and process
management strategies, automated guidance of process
performance, and automated process execution [6]. In
this study, we demonstrate how process modeling
experimentation can be used to investigate alternatives
for organizational policy formulation.
A variety of modeling techniques have been used in

1. Introduction
Assessment of managerial decisions might utilize
any of several methods, including case studies or formal
experiments involving actual projects, pilot projects, or
analysis of expert opinions and surveys. However,
modeling and simulation are often preferred over these
other methods. The advantages of using simulation to
1060-3425/98 $10.000 1998 IEEE
Proc. 31st Annual Hawaii International
Conference on System Sciences

103

Simulation enables experimentation with the model
of the software process, without impacting on the real
process. The effects of one factor in isolation can be
examined, which is usually cost-prohibitive in a real
project. The outcome of the SD simulation has two
aspects: an intellectual one, consisting of a better
understanding of the process and a practical one,
consisting of prediction, tracking, and training.
SDM was applied to the software development
process for the first time by Abdel-Hamid and Madnick
[2]. Their model captures the managerial aspects of a
waterfall software life cycle. It was the starting point for
many subsequent models of the entire process [15], [14]
or parts of it [5] and [91 that have been successfully
used for resource management [8], [13], process
reengineering [4], project planning, and training [ 121.
Abdel-Hamid [ l ] studied the impact of turnover,
acquisition, and assimilation rates on software project
cost and schedule. He found that these two response
variables can be significantly affected by an
employment time of less than 1000 days, by a hiring
delay less than or greater than 40 days, or by an
assimilation delay greater than 20 days. We advance
this research area further by focusing on turnover issues
and demonstrate how SDM can be used in decision
support for strategies that address staff turnover.

recent years to model the software process. For instance,
COCOMO and SLIM have been used to model the
correlation between input and output metrics for the
software process. In contrast, we use a structural
approach, Systems Dynamics Modeling (SDM) in this
research to allow for the modeling of specific system
characteristics and their dynamic interactions.
Richmond considers system dynamics as a subset of
systems thinking. Systems thinking is “the art and
science of making reliable inferences about behavior by
developing an increasingly deep understanding of
underlying structure” [ l 11. Systems thinking is,
according to Richmond, a paradigm and a learning
method. System dynamics is defined as “the application
of feedback control systems principles and techniques to
modeling, analyzing, and understanding the dynamic
behavior of complex systems” [2].
SDM was developed in the late 1950’s at M.I.T. It is
based on cause-effect relationships that are observable
in a real system. These cause-effect relationships
constantly interact while the computer model is being
executed, thus the dynamic interactions of the system
are being modeled, hence its name. A system dynamics
model can contain relationships between people,
product, and process in a software development
organization. The most powerful feature of system
dynamics modeling is realized when multiple causeeffect relationships are connected forming a circular
relationship, known as a feedback loop. The concept of
a feedback loop reveals that any actor in a system will
eventually be affected by its own action. Because system
dynamics models incorporate the ways in which people,
product, and process react to various situations, the
models must be tuned to the organizational
environment that they are modeling.
The automated support for developing and executing
SDM simulators enables us to model the complexity of
a software development process which can not be dealt
with by a human mental model. Building and using the
model results in a better understanding of the causeeffect relationships that underlie the development of
software.
SDM utilizes continuous simulation through
evaluation of difference and differential equations.
These equations implement both the feedback loops that
model the project structures as flows, as well as the
rates that describe the dynamics of these flows. Thus,
application of an SDM model requires an organization
to define its software development processes and to
identify and collect metrics that characterize these
processes.

3. A SDM software process model
Our system dynamics modeling tool was developed
using the ithink simulation software [7]. The model
incorporates eight objects related through four feedback
loops, each loop comprised of non-linear system
dynamic equations. The four feedback loops were
chosen because they encompass the factors that are
generally regarded as most influential in the control of
software projects [16].
All of the feedback loops begin and end at the
Schedule and Effort object, which is the nucleus of the
system. This object represents a project schedule and
the effort in person hours to complete a schedule plan
(Figure 1).
The first feedback loop (connecting Schedule and
Effort, Staffing Profile, Experience Level, and
Productivity) represents the staffing profile of a project.
The staffing profile affects productivity based on the
number of engineers working on a project, the domain
expertise of the engineers, and amount of time an
engineer participates in a project.

104

Schedule
Pressure

Q
Generated

@A

I
Figure 1. Basic Feedback Loops of the Model

The fourth feedback loop (connecteing Schedule
and Effort, Schedule Pressure, and Productivity)
models the schedule pressure associated with the
percentage of work complete, as per the schedule.
Schedule pressure increases in relationship to the
negative deviation from schedule. As schedule pressure
increases, personnel will work more efficiently and
additional hours, increasing productivity. However, in
the case that schedule pressure remains high and
engineers are work many hours of overtime, they begin
to generate more defects and eventually an exhaustion
limit is reached. Once the exhaustion limit is reached,
productivity will decrease until personnel can
recuperate and begin working more productively again.

The second feedback loop (connecting Schedule and
Effort, Staffing Profile, Communication Overhead, and
Productivity) models communication
overhead.
Increasing the number of people assigned to a project
increases the communication overhead among the team
members, resulting in decreased productivity [14] [ 151.
The third feedback loop (connecting Schedule and
Effort, Staffing Profile, Experience Level, Defects
Generated, and Rework Hours) takes into consideration
defects generated during the design and coding phases
of an increment, which translates into rework hours.
The model also considers the impact of domain
expertise on defect generation. An engineer with less
domain expertise generates more defects than an
engineer with a higher degree of domain expertise.

Design-Productivity-1

lncr -1 -Design

Incr-1 -Productivity
Design-Rework-1
Figure 2. Flow Diagram for Increment 1 Design Production

105

Incr-1-Design(t) = Incr-1-Design(t-dt) + (Design-Rework-1
Design-Rework-1 = Incr-1 -Rework-Hours
Design-Productivity-1 = Incr-1-Productivity

-

Design-Productivity-I)

* dt

Figure 3. Equations for Increment 1 Design Production

Two methods, expert opinion and reproduction of
actual project results, were used to validate the model.
Six software professionals with a cumulative software
management experience of over 75 years examined the
simulation model and expressed their confidence in its
ability to model the general behavior of a software
project with respect to cost, schedule, and defect
generation and removal. In addition, one of the experts
used the model to accurately reproduce the results of a
project with which he was familiar.
An updated version of the simulator is being used in
a corporate training program for software project
management. It is being used to demonstrate the
advantages of an incremental development approach
over a waterfall development approach. With respect to
specific operating policies and their outcomes, it is used
to illustrate the effectiveness of peer reviews, the effects
of schedule compression, the mythical man-month
perception, and the 90% completion syndrome.

Figure 2 illustrates one of the modeled flows,
Increment 1 Design Production. The input of this flow
is controlled by the amount of design rework,
Design-Rework-1,
which uses an input of
Incr-1-Rework-Hours.
The amount of design work
produced in Increment 1 is represented by a storage
tank, Incr-1-Design. The outflow from this tank is
controlled by the design productivity rate,
Design-Productivity-1,
which uses an input of
Incr-1-Productivity.
Figure 3 lists the equations
represented in the Increment 1 Design Flow diagram of
Figure 2.
In summary, the model inputs are provided in Table
1.
The model outputs are provided in the form of
graphs that plot the work remaining in each increment,
current staff loading, total cost, percent complete, and
quality costs. Figure 4 is an example of the output
graph that plots Staff Loading and Total Cost.

Table 1. Model Inputs
Number of increments
Productivity
Engineer 1 (domain inexperienced)
Engineer 2 (domain experienced)
Engineer 3 (very domain experienced)
Defect generation rates
Engineer 1 (domain inexperienced)
Engineer 2 (domain experienced)
Engineer 3 (very domain experienced)
Defect detection
YOof defects found in peer reviews
YOof defects found in integration test
Percent of schedule allocated to rework
Defect removal costs
Found in peer reviews
Found in integration testing

4. Description of the experiment

3

.a

1.o
1.2
.0300/hr
.0250/hr
.0225/hr
80%
20%
10%

2 hr/defect
10 hr/defect

manager faces the question of whether to replace the
departing individual or to forego the expense of
replacement and make other adjustments, such as
reductions in product functionality, slippage in the
schedule, and rearrangement of assignments. The

Attrition is clearly detrimental to a software
development project. When an experienced developer
leaves a project prior to its completion, then a project

106

answer to the question of replacement may turn on
factors such as the experience of the developer, the
percent completion of the project, the number of
engineers on the project, the time required for hiring or
transfer, and the attrition rate of the organization.
Replacement is costly, but may be required to keep a
project on schedule. It leads one to wonder, can the
dilemma presented by attrition be resolved by yet

1:
2:

another alternative, that of overstaffing a project with
more than the necessary number of development
engineers? Are there project situations in which it is
economically feasible, or even desirable, to mitigate the
risk of attrition by overstaffing? The experiment
described here was motivated by these questions and the
results offer an indication of the desirability of staffing
policies that include the overstaffing option.

50.00'
2932.12

25.00.

0.00

500.00

1000.00

Budget: Page 1

1500.00

2000.00

Hours

Figure 4. Sample Output for Staff Loading and Total Cost

Three strategies were considered:
No replacement after attrition
Replace engineers as they leave
Overstaff at the beginning of the project at the
same level as the attrition
These strategies were considered for combinations of
three factors:
Number of engineers on the project
Time in the project at which attrition occurs
0
Attrition rate
The model's response variables recorded for this
experiment were:
0
Project duration relative to the estimated schedule
Project cost
0
Rework cost (hrs)
Each strategy was evaluated for two values of each of
the three factors (Table 2).

The experiment was conducted for an incremental
project, consisting of three equal and non-overlapping
increments, with a total effort estimate of 9000 personhours. This is roughly equivalent to ten engineers
completing an increment every eight weeks. Three
levels of application domain experience (inexperienced,
experienced, and very experienced) were used with
corresponding productivity factors (Table 1). The
departing engineers were assumed to be very
experienced, while the replacements were assumed to be
inexperienced.
The learning rate is such that
inexperienced and experienced engineers advance one
experience level with each increment completed, until
they become very experienced.

107

Number of engineers
Time of attrition
Attrition rate

10
At the end of increment 1
10%

20
At the end of increment 2
30%

5. Experimental results
The data in Table 3 reveals the trends found in the simulation results.

Table 3. Results from Simulating Staffing Strategies for Attrition

Run
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

Number of
Enaineers
10
10
10
10
10
10
10

10
10
20
20
20
20
20
20

Time of
Attrition
(End of
Increment)
1
1
1
1
1
1
2
2
2
1
1
1
2
2
2

Attrition
Rate
10
10

Action:
Replace
(Y/N) or
Overstaff

Duration
Relative to

N
Y
0
N
Y
0
N
Y
0
N
Y
0
N
Y
0

1.oo
.97
.96
1.09
.98
.95
1.04
98
.91
1.10
1.08
1.08
1.09
1.09
1.08

10
30
30
30
30
30
30
10
10
10
10
10
10

Estimated Project Cost
Schedule
1$1oool

675.10
683.68
701.10
558.30
646.32
692.17
641.10
670.57
738.58
682.70
705.32
741.60
71 3.90
71 6.1 1
772.40

Rework
Cost (hrl
771
789
822
724
816
902
778
789
918
833
859
924
851
851
940

attrition becomes more visible. Consider Runs 4, 5 ,
and 6 in Table 3. In Run 4, the estimated schedule is
overrun by 9%, but Runs 5 and 6 complete early.
Also, the effects of attrition are more visible when
attrition occurs at the end of increment 1 than when it
occurs at the end of increment 2. In Run 4 when
attrition occurs at the end of increment 1, the project
overruns the schedule by 9%, whereas in Run 7 when
it occurs at the end of increment 2, the schedule
overrun is only 4%. Also, in Run 6, the schedule
underrun is 5 % , whereas in Run 9, it is 9%. These
runs also indicate that attrition costs more when it
occurs later in the project.
The results in Table 3 may also be viewed as an

When the number of engineers working on a project
is small and attrition rate is low, the effect of attrition
is not very visible. For example, consider Runs 1, 2,
and 3 in Table 3. In this case, the number of engineers
is at the low level (10) and the attrition rate is also at
the low level (10%). The results of Run 1 indicate that
the project is completed on time without replacement
or overstaffing. The fact that the project completes
early or on time in all three of these runs suggests that
attrition does not have a significant effect if the
number of engineers is low and the attrition rate is
low.
As both the number of engineers working on a
project and the attrition rate increase, the effects of

108

of Figure 5. In Figure 6, N represents the strategy of
no replacement and no overstaffing, R represents the
replacement strategy, and 0 represents the overstaffing
strategy. In some situations either of two strategies is
desirable, depending on whether one wishes to
minimize cost or complete the project on schedule (or
in the (20,2,10)case, minimize the schedule overrun).

evaluation of three staffing strategies for attrition in
five sets of the three factors. Each set is comprised of
number of engineers, increment number ending at time
of attrition, and attrition rate.
Because we are
considering three factors in this experiment, the
orthogonal relationships of these five sets of factors are
illustrated in the cube of Figure 5.
When the staffing strategy for attrition is compared
within each set of factors, desirable strategies can be
identified based on cost and project duration against
the estimated schedule. These results are summarized
in Figure 6, which adds the desirable strategies to the
cube

Increment no.
completed
at time of
attrition

(lO,l,lO)

I

No. of engineers

Figure 5. Relationshipsof Factor Sets in Table 3.

For example, point (lO,l,lO) in Figure 6 represents a
project in which the ratio of estimated effort to the
number of engineers is high (say 900 hours) and one
person leaves at the end of Increment 1. In this scenario,
the project can finish on-time without replacing the
person. Replacing the person will allow the project to
finish about 3% early but will add about 1% to the cost of
the project. Overstaffing to anticipate this attrition will
allow the project to complete about 4% early and will
increase the project cost about 4%. The most desirable
strategy for this set of factors, then, is no action (N)

because it allows the project to finish on time at the
lowest cost.
As another example, in the case of 20 engineers on
the project and two people leave at the end of increment
2 (point (20,2,10) in Figure 6), other strategies are
indicated. The project will overrun the estimated
schedule regardless of the chosen course of action, but
overstaffing (0) will limit the overrun.
Neither
replacing nor overstaffing (N) remains the best option
for minimizing cost.

109

N for c o s t
R for schedule
Increment
no.
completed
at time of

N for c o s t
0 for schedule

R for schedule
_..’

No. of engineers
Figure 6. Desirable Staffing Strategies for Attrition under Each Set of Factors

software development process simulator, one which
models the major dynamic influences in a project. The
software practitioners who examined the results
produced by this experiment found them to be both
acceptable and credible. This model is capable of
supporting designed experimentation and providing
data for supporting staffing decisions.
This group continues research in software
development process simulation.
In addition to
questions related to project staffing, the group is
investigating questions related to project management
training, project risk assessment, and product quality.
In terms of process modeling issues, future research will
identify and validate the most influential factors for
modeling software development dynamics and, more
importantly, their degree of influence on project
outcomes.

6. Conclusions and future research
Our research arose from the need for having
strategies for dealing with attrition in software
development organizations. We also wished to examine
the ability of a more abstract simulator, composed of
four process feedback loops, to provide realistic data for
supporting the formulation of staffing policies.
This experiment suggests several implications for
software project staffing in response to attrition. In
general, no action for attrition is the least expensive
choice and overstaffing is the most expensive choice.
The choice of no action is indicated when schedule
pressure is high and cost containment is a priority. The
replacement strategy has the advantage of alleviating
the exhaustion rate and concomitant increases in
attrition.
Even though overstaffing is the more
expensive option, it can have the very desirable effect of
minimizing project duration. Thus, this strategy should
be considered for projects in which completion date has
been identified as the highest priority.
Such a

7. References
[I] Abdel-Hamid, Tarek, A Study of Staff Turnover,
“Acquisition, and Assimilation and Their Impact on Software
Development Cost and Schedule“, Journal of Manager
Information Systems, Summer 1989, vol. 6, no. 1, pp. 21-40.

circumstance might occur if delivery of a commercial

application is required during a market “window of
opportunity,” or if the contract for a custom application
has a penalty attached to late delivery. The cost of
overstaffing must be weighed with the other factors in
the project and process simulation can provide data for
this decision.
Regarding process modeling objectives and the use
of SDM, this experiment has demonstrated the use of a

[2] Abdel-Hamid, Tarek and Stuart E. Madnick, Software
Project Dynamics An Integrated Approach, Prentice-Hall,
Englewood Cliffs, New Jersey, 1991.
[3] Abdel-Hamid, Tarek, “Thinking in Circles”, American

Programmer, May 1993, pp. 3-9.

110

The M.I.T. Press, Cambridge, MA, 1981
[4] Bevilaqua, Richard J. and D.E. Thornhill, "Process
Modeling", American Programmer, May 1992, pp. 3-9.

[ 1 11 Richmond, Barry, "System Dynamics/Systems Thinking:
Let's Just Get On With It", International System Dynamics
Conference, Sterling, Scotland, 1994.

[SI Collofello, James S., J. Tvedt, Z. Yang, D. Merrill, and I.
Rus, "Modeling Software Testing Processes", Proceedings of
Computer Software
(CompSAC95), 1995.

and

Applications

Conference

[12] Rubin, Howard A., M. Johnson and Ed Yourdon, "With
the SEI as My Copilot Using Software Process "Flight
Simulation" to Predict the Impact of Improvements in Process
Maturity", American Programmer, September 1994, pp. 5057.

[6] Curtis, Bill, M. I. Kellner and J. Over, "Process
Modeling", Communications of the ACM, 35(9), Sept. 1992,
pp. 75-90.

[13] Smith, Bradley J, N. Nguyen and R. F. Vidale, "Death of
a Software Manager: How to avoid Career Suicide through
Dynamic
Software
Process
Modeling",
American
Programmer, May 1993, pp. 11-17.

[7]ithinkManual, High Performance Systems, Inc., Hanover,
NH, 1994.

[8] Lin, Chi Y., "Walking on Battlefields: Tools for Strategic
Software Management", American Programmer, May, 1993,
pp. 34-39.

[141 Sycamore, Douglas M., Improving Software Project
Management Through System Dynamics Modeling, Master of

Science Thesis, Arizona State University, 1996.
[9] Madachy Raymond, "System Dynamics Modeling of an
Inspection-based Process", Proceedings of the Eighteenth
International Conference on Software Engineering, Berlin,
Germany, March 1996.

[ 151 Tvedt, John D., An Extensible Model for Evaluating the
Impact of Process Improvements on Software Development
Cycle Time, Ph.D. Dissertation, ASU, 1996.

[lo] Richardson, George P. and Alexander L. Pugh HI,
Introduction to System Dynamics Modeling with DYNAMO,

[16] Beagley, S.M., "Staying the Course with the Project
Control Panel." American Programmer (March 1994) 29-34.

111

Early Course and Grade Predictors of Persistence
in Undergraduate Engineering Majors
James A. Middleton, Stephen Krause, Sean Maass,
Kendra Beeley, James Collofello
Ira A. Fulton Schools of Engineering
Arizona State University
Tempe, AZ, USA
jimbo@asu.edu
Abstract—A comprehensive examination of the mathematics
courses undergraduate engineering majors take in their first two
years, and their success in those courses was conducted to
determine the impact of course level and grade on student
persistence in engineering. Six hundred fifty five, full-time
freshmen enrolled as engineering majors at a major university in
the Southwest United States participated in the study. Data was
gathered from the 2007 freshman cohort. Students’ grades for
each of their first University mathematics courses were tracked,
and used to gauge probabilities of subsequently persisting as an
enrolled Engineering major. Participants were grouped into 9
categories representing 3 levels of course grade (A or B; C; and
D, F, or W), crossed with 3 levels of course (Below Calculus I,
Calculus I, and Above Calculus I). Student enrollment and
graduation rates for each of these groups were examined for the
next 5 years. A binary logistic regression with two main effects
(Course and Grade) and one interaction was performed. Results
show that if a student’s first course was above Calculus I, they
were 2.3 times more likely to be retained in Engineering than a
student who took Calculus I. If a student took Pre-calculus or
another course below Calculus I they were less than half as likely
to persist in Engineering than those who took Calculus I
(p<.001). Likewise, if a student received an A or B for their first
mathematics course regardless of which course they took, they
were 6.5 times more likely to persist than someone who received
a D, F, or W in their first mathematics course (p< 0.001). The
special role of Calculus as a gatekeeper, preventing engineeringintending students from obtaining an engineering degree, is
discussed.
Keywords—freshman retention; persistence; mathematics

I.

INTRODUCTION

Research on undergraduate success shows that there are a
number of critical factors that need to be developed
simultaneously to increase student motivation, achievement,
and retention in engineering.
A. Persistence
Tinto’s [1] [2], model of post-secondary integration shows
that persistence in college is determined by both the
consistency with which students see the goals and
opportunities of the institution corresponding to their own set
of academic values, and the the affinity students have for the
institution. Halpin [3] points out that the interplay between the
academic and social systems in college color these perceptions

978-1-4799-3922-0/14/$31.00 ©2014 IEEE

Robert Culbertson
Department of Physics
Arizona State University
Tempe, AZ, USA

to a heavy degree. In a study of first and second-year students
in community college, he found strong predictive coefficients
for students’ perceptions of faculty concern for their
development, supportive interactions with faculty, consistency
of institutional goals with students’ goals, and students’
academic and intellectual development, on thier persistance.
Boiled down, some of the chief predictors of persistence in
post-secondary students in their first two years concern
students’ sense of belonging: 1) making connections with
faculty in their chosen field; and 2) making connections with
other students [4]. These two factors are among the most
critical overall, because they focus on the emotional needs of
(typically) young people who are often away from home for
the first time, and who are just learning the norms,
expectations, and practices of University life. This integration
is most important during students’ freshman year in college-- a
time when students transition from their family home and
neighborhood school to a more independent role.
STEM coursework in particular has been shown to turn off
such young people to a great extent because members of the
faculty are seen to be unresponsive and uncaring for freshmen
as individuals. Large class sizes and course assignment
practices by departments are two contributors to this [5].
Separating the social system of a university, however, from
its academic system is overly reductionist. The students an
engineering freshman meets in class; who form their study
groups, project teams, and roommates are typically other
engineers. These relationships greatly impact students’ learning
and academic success. Research that has examined both
academic and social systems simultaneously illustrates the
complexity of the interplay between the two [6]. Cabrera,
Nora, and Castaneda developed a rigorous structural equations
model showing clearly how social integration and academic
integration contribute to each other, and collectively to the
students’ sense of institutional commitment. Institutional
commitment, as a function of these two factors, had a direct
impact on the single most predictive factor on student
persistence: Students’ intent to persist. But having a strong
intention to persist is only half of the equation. Student GPA
has nearly as strong an impact on persistence as intention.
Putting this body of research together, we can see that
students’ intent to persist is critical to nurture, and that
strategies that foster establishment of commitment to the major

and its field, coupled with a sense of identification with and
commitment to the institution, are important approaches to
intervention. But, intent only goes so far if the students’
academic performance does not allow them to complete
prerequisites in time, or to effectively gain competence in the
subject matter.
B. Engineering Persistence
Engineering as a field is somewhat of an anomaly
concerning persistence. Ohland, Sheppard, Lichtenstein, Eris,
Chachra, & Layton studied a nationwide sample of engineering
majors. They expected to find lower rates of persistence,
higher attrition, and lower satisfaction in engineering students
compared to students in other fields. They found that these
expectations were simply not borne out by the data. In general,
they insist that engineers tend to persist at higher rates than
other majors. Moreover, they show roughly equivalent rates of
satisfaction and engagement, but also felt that they gained
more practical competence from their programs than did other
majors. Still, at a research institution, roughly 45% of
engineering majors either switch to a non-STEM major or do
not complete a 4-year degree [7] [8].
Concentrating on those ‘switchers,’ Marra, Rodgers, Shen,
& Bogue surveyed 113 students who left engineering over 3
years [9]. These students cited poor teaching and advising, high
difficulty level of the engineering curriculum, and lack of
relevance of what they were being taught to their interests and
proclivities. In a similar study, students who switched from
their STEM major to a non-STEM major responded: 1) they
were much more satisfied with advising and career counseling
in their non-STEM major than in their STEM major and felt
that their new major gave them greater career options; 2) they
felt they experienced more conceptual difficulty, more
problems related to class size, and poor tutorial support in
STEM majors compared to their new majors; 3) Students
experienced lower morale due to the competitive culture and
felt that they lacked peer support in STEM majors compared
to the new majors; 4) Students felt that STEM faculty were less
approachable than faculty in the new major, and they were less
satisfied with the quality of instruction in STEM courses; and
5) students felt that they lacked research opportunities with
faculty while enrolled in STEM. These feelings were generally
more pronounced for Engineering students than for students in
other majors [9].
As a response to these and other findings, institutions
across the nation have instituted evidence-based reforms to the
“freshman experience” aimed at retaining students in
engineering, improving their academic and social support
experiences, improving instruction, and offering extended
opportunities to engage in research and other professional
activities early on in their program. Much of these efforts are
focused on courses that typically display high DFW rates, high
proportions of students who receive grades of D, F, or who
withdraw from the course, presumably to avoid receiving a
poor grade. Mathematics courses are chief among those
targeted for reform. In the United States, Calculus I is viewed
as a university-level course in addition to its role as a core
Engineering subject. Each fall, approximately 300,000 college
or university students, most of them in their first post-

secondary year, take this course [10]. In peer countries in
Europe, students pursuing a STEM degree in university
typically enroll in higher-level mathematics such as abstract
algebra or proof-based calculus [11]. There is, therefore,
considerable institutional pressure to stay competitive by
increasing students’ mathematical preparation, ensuring
readiness for calculus when they enter an engineering major.
The impact of prior STEM preparation, in particular
mathematics preparation, has been posited as a causal factor
for student success in engineering, and poor performance in
collegiate mathematics as a primary factor for students
switching from an engineering major or dropping out of school
entirely [12]. In general, students more often receive poor
grades in science, mathematics, and engineering courses in
their first two years than other fields, and they tend to drop out
of STEM fields at a higher rate than other majors in the
university. Nationally, the DFW rate for Calculus I courses
hovers around 30%, though it ranges widely from about 37%
for community colleges to 25% for research universities [13].
For Pre-Calculus, only about 30%-40% of students who pass
pre-calculus go on to take Calculus I [14]. Much of this has to
do with student intentions to major in STEM or not, but much
may also hinge on the negative feedback low grades give
students regarding their ability to succeed in a mathematically
intensive field such as engineering. Because of these reasons,
first-year university mathematics courses tend to filter out large
numbers of STEM-intending students from continuing to
pursue a career in engineering or other related fields [15] [11].
Some research, however [4], shows that grades in STEM
subject-matter are not as important as the ways in which
students are enculturated, and the extent to which their
particular learning needs are addressed. These factors may
manifest themselves in poor grades, but grades in such cases
are not the primary causal agent for lack of persistence. For
example, research universities seem to do a worse job at
building and maintaining Calculus students’ confidence in their
mathematical abilities, enjoyment of mathematics and interest
in continuing with the mathematics that is needed to pursue
their intended careers, in contrast to regional masters,
undergraduate, and community college institutions [13].
The purpose of this study was to examine, longitudinally,
the impact of a student’s first mathematics course taken at a
college level in addition to the impact of the grade received in
that course and the student’s subsequent persistence in an
engineering discipline. We acknowledge that grades are often
poor substitutes for actual mathematics performance, and that
mathematical performance is only one contributing factor to
engineering persistence. However, grades are a readily
available overt indicator of student success, they are used by
students as information regarding their readiness for
subsequent coursework, they also constitute hard-and-fast prerequisites for course milestones in the engineering curriculum,
and students use grades, particularly low grades, as indicators
of their acceptance into engineering [16] [17]. As such grades
can be used as indices pointing to the extent to which curricular
experiences influence students’ decisions to continue in
engineering or to switch to another major.
For most engineering students in the US, Calculus I is taken
in their first college semester. Coupled with the criticality of

calculus as a gate-keeper in the US engineering curriculum, the
academic and social engagement that students are exposed to
during their calculus course plays a crucial role in students’
decisions to persist in engineering.

because of their grade in Calculus 1 [24]. When pedagogy is
taken into account, class sections that exhibited Good,
Ambitious teaching practices reduced the switching rate from
16.2% to 7.0%.

Factors that determine academic socialization in first year
mathematics include the nature of the pedagogy in the
classroom, perceived difficulty of the curriculum, and
engagement and satisfaction with academic services. Astin, for
example, shows that students in engineering tend to have more
pronounced extremes in attitudes, compared to other fields
[18]. In particular, engineering majors tend to report having
more developed analytic skills and practical skills than other
majors, but lower overall satisfaction with their collegiate
experience, and dissatisfaction with curriculum and instruction.
This is pronounced even for those engineering majors who
persist through to graduation: Engineering persisters are less
likely to attribute their motivation to study engineering to an
academic mentor. They report lower levels of confidence in
their mathematics and science skills. They show slightly lower
involvement in team-based problem-based learning. They
show higher levels of academic disengagement in liberal arts
courses, in engineering-related courses, and overall college
coursework. However, they tend to report a higher degree of
satisfaction with academic facilities and services [19].

Looking closely at those who switch majors or drop out,
Eris, Chachra, Chen, Sheppard, Ludlow, Rosca, & Toye found
that those who drop out of engineering early on, tend to report
less intention to study engineering from the very beginning.
The authors suggest that many students doubt their choice of
engineering as a major from the start, and that this doubt
intensifies over the first year or so of university, reinforced by
lower grades and other academic and social feedback [25].

To address the curriculum and instruction shortcomings,
Hilpert & Husman provide evidence by three means for
engineering instructors to more effectively engage students:
activate prior knowledge, facilitate active learning, and
promote reflection [20]. Drilling down to mathematics, we see
that students’ disinclination to take higher mathematics courses
is intensified by the “transmissionist” pedagogy nearly
ubiquitous across first year mathematics classes in the US [21].
Similarly, in a seminal study, Seymour and Hewitt show that
US students who switch out of STEM majors (Switchers versus
Persisters) typically cite pedagogy that emphasizes rote
memorization and lack of applications as a primary reason for
leaving [22]. Ellis, Kelton, & Rasmussen list specific
pedagogical practices that tend to impact students’ decisions to
continue: Perceived frequency of the instructor showing
students how to work specific problems, preparing extra
material to help students understand calculus concepts or
procedures, holding whole-class discussions, and requiring
students to explain their thinking on exams [23].
Interestingly, these perceptions vary for persisters versus
switchers. Even in the same class section, Persisters tend to
experience higher levels of whole-class discussion than
Switchers; while Switchers tend to experience lower levels of
discussion. This indicates that switchers have a different
perception of what is happening in the class, and how it
impacts their own learning. BUT, Switchers ARE
disproportionately in classes with low frequency of lecture,
where they do not engage in whole-class discussions, being
shown how to solve problems, or being given extra material for
difficult topics.
What is interesting about this national sample is that fully
90% of students switching out of STEM majors fully believed
they would succeed in Calculus 1 prior to taking the course.
Following the course, only 11% switched out of STEM

II.

METHOD

A. Participants
Data consisted of the institutional records of 615 first-time,
full-time freshmen engineering students enrolled at a major
University in the Southwest United States in the academic
2007 calendar year.
While the data reported in this paper records student
persistence from Summer/Fall 2007 through Fall 2013, it is
important to note that enrollment has grown precipitously over
the past 7 or 8 years; from approximately 650 full-time
freshmen in 2007 to 1,438 in 2012. This has changed the
characteristics of the student body in important ways. Table 1
shows overall undergraduate engineering enrollment by
ethnicity. Asian, Hispanic, Pacific Islander, and Nonresident
Alien ethnic group populations have grown proportionally
compared from 2007 to 2012. The proportion of Hispanic
students is significantly higher, the proportion of African
Americans have remained the same, while Whites and
American Indians have dropped proportionally.
Gender ratio changed only slightly in that time period:
Women represented 17% of the overall engineering student
body in 2007 compared to 18% in 2012.
To test the hypothesis that the courses in which students
initially enroll predict subsequent persistence in engineering,
we tracked this single cohort and their persistence to
graduation as a function of their first year mathematics course
level and the grade they received in their first mathematics
course. First-time, full-time Engineering freshman from year
2007 were further grouped into three categories: 1) students
that took Calculus I for their first mathematics course in
college; 2) students that enrolled in a course above Calculus I
for their first mathematics course; and 3) students that took a
course below Calculus I (typically Pre-calculus).
We then tracked what grade each student received in their
first mathematics course and grouped the students into 3 grade
groups: Highly successful students (those that received a grade
of A or B), moderately successful students (those that received
a C), and less successful students (those that received a D, F, or
W).

TABLE I. Undergraduate Enrollment Ira A. Fulton Schools
of Engineering 2007
Am.
Indian

Asian

Afr.
Amer.

Hisp.

Pac.
Isl.

White

Nonres
Alien

Total

2007

112
(2%)

417
(7%)

148
(2%)

3,140
621
0 (0%)
(50%)
(10%)

1,480
(23%)

6,343

2012

96
(1%)

717
(8%)

186
(2%)

1,167
(13%)

2,186
(25%)

8,775

10
(<1%)

4,096
(47%)

These nine groups of the 2007 student cohort were then
tracked to see whether or not they persisted in Engineering by
looking at subsequent years following the student’s first math
course. If a student was enrolled or had graduated in
Engineering, that student was coded as persisting in
Engineering.
III.

TABLE III. Graduation within Same STEM Major, STEM
and Related Majors at ASU, Fall 2007 Cohort
1-year
Retention

2-year
Retention

4-year
Grad.

6-year
Grad.

Engin.

67.4%

48.9%

25.6%

40.6%

Business

71.1%

58.1%

40.4%

50.4%

Physics

51.9%

22.2%

11.1%

11.1%

Chem.

48.8%

31.7%

19.5%

26.2%

Biology

53.5%

24.6%

15.0%

19.8%

Math

55.8%

34.9%

7.0%

20.9%

RESULTS

Overall 1-year retention for ASU students regardless of
major was 79.5% for the 2007 cohort. The 4-year graduation
rate was 37%, and the 6-year graduation rate was 59%.

Fig. 1 plots the number of persisters, grouped by course
level and grade, as a function of the time (years)
enrolled/graduated after their first mathematics class.

Tables II and III contrasts study participants’ retention and
graduation rates with those of the larger campus community.
What is immediately apparent is that students who begin their
University career in engineering show higher retention and
graduation rates in their chosen field than other STEM majors,
and even those switching majors are retained in the University
and graduate at a higher rate than average.

It is clear at a glance that the first 2 years in engineering
show the largest drop in persistence rates for all nine groups.
After the first three years, the persistence rate stabilizes across
all groups (Engineering students who graduated in Engineering
were grouped with enrolled/retained in Engineering).

TABLE II. Retention and Graduation Rates within
University, STEM and Related Majors at ASU, Fall 2007
Cohort
1-year
Retention

2-year
Retention

4-year
Grad.

A majority of students received an A or a B in their first
mathematics class and of those that received an A or B there
was relatively equal numbers of students enrolled in the three
math level groups. Tables 1 and 2 below show the number of
first-time, full-time engineering students organized by Course
Group and Grade Group.

6-year
Grad.

Engin.

84.9%

76.5%

37.2%

63.1%

Business

85.4%

79.2%

50.2%

70.1%

Physics

66.7%

63.0%

40.7%

40.7%

Chem.

77.4%

65.2%

34.8%

52.4%

Biology

81.7%

72.9%

39.7%

59.8%

Math

83.7%

67.4%

34.9%

58.1%

ASU

79.5%

68.9%

37.0%

58.6%

Of the persisters, those who are retained and continue to
pursue an engineering degree, we found significant differences
in retention rates as a function of the level of the first
mathematics course in which they were placed, and as a
function of the grades they received in that first mathematics
course.

Figure 1.

Persistence as a Function of Time Broken Out by First
Mathematics Course and Grade Received

The biggest student loss in numbers clearly comes from
our group of students, regardless of course, who receive As and
Bs. Forty-two percent of the students who received As or Bs in
their first mathematics course at ASU decided to leave
engineering between 2007 and 2012. But, as discussed before,
receiving a lower grade DOES correspond with a higher
probability of leaving engineering. Three-fourths of freshmen
who received Cs on their first mathematics course left
engineering before graduation, and 84% with Ds, Fs, or W
grades, regardless of course, left.
Beyond grades earned, we also found that the mathematics
course level students take in their first semester of their
engineering program predicts their persistence, regardless of
grade. Only 50 out of the 180 (28%) students that took a course
beyond Calculus I for their first mathematics course switched
from engineering to another major or left the university
altogether. This contrasts with switching rates of about half of
the students who took Calculus 1 as their first course, and 71%
of students that took below Calculus for their first mathematics
course.
Examining Tables IV and V, we see both an obvious grade
effect and a course effect for persistence.
To test this effect, a binary logistic regression with two
main effects (Course and Grade) was conducted for this study
using IBM’s SPSS software Version 22 (see Table VI). The
dependent variable in this analysis was the log-odds ratios for
the main effects. By taking the natural logarithm of the Betaweights for levels of the main effects, one can estimate the risk
factors for switching out of engineering as a function of
enrollment in one course versus another, or receiving one grade
versus another. Table 6 shows that our binary logistic
regression has a chi-square value of 130.713 with a p-value <
0.001 which states that our model as a whole fits significantly
better than an empty model (i.e., a model with no predictors).
Interpreting these statistics, if a student from the 2007
cohort took a course that was above Calculus 1 for their first
mathematics course at the university, they were 2.3 times more
likely to be retained in Engineering than a student who took
Calculus 1 for their first math course. If a student took Precalculus or another course below Calculus, they were less than
half as likely to persist in Engineering than those who took
Calculus. Both coefficients were statistically significant (p <
0.001).
For the main effect of Grade, if a student received an A or
B for their first mathematics course regardless of the course
they took, they were 6.5 times more likely to persist than
someone who received a D, F, or W in their first mathematics
course (reference group). This coefficient was statistically
significant with a p < 0.001. If the student received a C in their
first mathematics course, this did not statistically differ from
persistence results from someone who received a DFW (p =
0.178).

TABLE IV. First Time Engineering Freshman Tracking by
Course Group

Course
Group

Fall
2007
Enrolled

Total
Loss
(as of
Fall,
2012)

147

95

C

41

35

DFW

29

25

145

58

C

50

38

DFW

23

18

155

35

C

15

6

DFW

10

9

615

319

Grade
Group
AB

Below
Calc

AB

Calc

AB

Above
Calc

Total

First
Enrolled
Total

Total
Loss

%
Loss

217

155

71%

218

114

52%

180

50

28%

615

319

52%

TABLE V. First Time Engineering Freshman Tracking by
Grade Group

Fall 2007
Enrolled

Total
Loss (as
of Fall,
2012)

147

95

145

58

Above
Calc

155

35

Below
Calc

41

35

50

38

Above
Calc

15

6

Below
Calc

29

25

23

18

Above
Calc

10

9

Total

615

319

Course
Group

Grade
Group

Below
Calc
Calc

Calc

Calc

AB

C

DFW

First
Enrolled
Total

Total
Loss

%
Loss

447

188

42%

106

79

75%

62

52

84%

615

319

52%

TABLE VI. Binary Logistic Regression Model Coefficients
Predicting Persistence in Engineering by First Mathematics
Course Taken and Grade Received in that Course

.226
.213

Wald
57.21*
13.94*
18.24*

df
2
1
1

Log
Odds
Ref.
2.33
.40

1.866
.573

.371
.425

45.88*
25.30*
1.82 (NS)

2
1
1

Ref.
6.46
1.77

-1.500

.372

16.29*

1

.22

Source
Calc I
Above Calc
Below Calc

B

S.E.

.844
-.910

DFW
AB
C
Constant

•

significant P<.0001

III.

DISCUSSION

Results of this analysis provide evidence that both the
level of mathematics a student begins her or his engineering
career with, and the degree of success a student achieves in
that freshman class determine, at least in part, their probability
of persisting in an engineering major or moving to another
major. At the very least this is true for the population that was
examined. This data corroborates earlier work by Cabrera,
Nora, and Castaneda [6] that demonstrated GPA is the most
important academic determinant of students’ intention to
persist in University. Our work refines this argument to show
that, for engineering majors, the grade a student receives in
their first mathematics course is a singularly important factor
predicting retention and graduation, with level of course being
somewhat less important relative to grade, but still a strong
predictor of overall retention.
Several contributing factors must also be taken into
consideration. First, at the University studied, first semester
Calculus is required for all subsequent courses in the
engineering major with a grade of C or better. If a student
received a D, E, or W in the course, he or she would
automatically be considered off-track for progress in their
chosen major, and would then be at least a semester behind
her or his peers. This is not necessarily a death sentence, so to
speak, for an aspiring engineer, but it does mean that she or he
and their families would have to add at least another semester
of time and cost to the student’s education.
More critically, as reviewed earlier, persistence is, at its
most basic, a function of the social and academic integration
of students to the institution [1, 26]. Our evidence suggests
that, during engineering students’ freshman year in particular,
and through their sophomore year secondarily, this social and
academic integration is severely jeopardized solely as a
function of the first mathematics course they take, their first
grade in mathematics, and a combination of the two factors.
Psychologically, receiving a poor or failing grade in their very
first semester at the University may be interpreted by students
as indicative of their ability in engineering relative to their
peers. Many students may take their non-success as a sign to
change majors to ones, which are less mathematically

intensive than engineering. Issues of academic self-concept
and self-efficacy have been shown to drive students’
interpretations of feedback from academic pursuits and their
subsequent persistence in engineering [9]. These feelings of
lowered efficacy may negatively influence the already
tentative decisions students have made regarding their chosen
career, leading them to switch out of engineering or leave the
university altogether [2].
It must be noted that engineering courses of study in
countries other than the US may have different mathematics
requirements, and may also differ in the degree to which
mathematics is integrated with mechanics and other physical
sciences in students’ program of study. Additionally, practices
of assigning marks or grades to student work may differ in
their impact on students’ intent to persist. Further comparative
research on the ways in which indicators of success are
utilized by students to support their persistence decisions is
clearly necessary.
We are following up on these analyses to include the
probable impact and interaction of mathematics, chemistry
and physics courses, student grades, and self-efficacy related
beliefs. Additionally, we are performing the same analyses
presented here for student cohorts from 2007 through 2014
graduates to examine the nature of cohort effects, and to test
the efficacy of introduced reforms to the freshman year
experience.
ACKNOWLEDGMENT
This material is based upon work supported by the National
Science Foundation under Grant No. 1226586.

REFERENCES
[1]
[2]

[3]

[4]

[5]

[6]

[7]

[8]

V. Tinto, “Leaving college: Rethinking the causes and cures of student
attrition.” University of Chicago Press., 1994.
V. Tinto, “Student Retention and Graduation: Facing the Truth, Living
with the Consequences. Occasional Paper 1,” Washington, DC: Pell
Institute for the Study of Opportunity in Higher Education, 2004.
R. L. Halpin, & L. Richard, "An Application of the Tinto Model to the
Analysis of Freshman Persistence in a Community College."
Community College Review vol. 17, pp. 22-32., 1990.
P. Schuetz, “A theory-driven model of community college student
engagement,” Community College Journal of Research and Practice,
vol. 32, pp. 305-324, 2008.
J. Gorham, J. and D. M. Millette, “A comparative analysis of teacher
and student perceptions of sources of motivation and demotivation in
college classes,” Communication Education, vol 46, pp. 245-261,1997.
A. F. Cabrera, N. Amaury, and M. B. Castaneda. "College persistence:
Structural equations modeling test of an integrated model of student
retention." Journal of Higher Education, pp. 123-139, 1993.
M. W. Ohland, S. D. Sheppard, G. Lichtenstein, O. Eris, D. Chachra,
and R. A. Layton, “Persistence, engagement, and migration in
engineering programs,” Journal of Engineering Education, vol. 97, pp.
259-278., 2008.
M. W. Ohland, C. E. Brawner, M. M. Camacho, R. A. Layton, R. A.
Long, S. M. Lord, and M. H. Wasburn, “Race, gender, and measures of
success in engineering education.” Journal of Engineering Education,
vol. 100, pp. 225-252, 2011.

[9]

[10]

[11]

[12]

[13]

[14]
[15]

[16]

[17]

R. M. Marra, K. A. Rodgers, D. Shen, and B. Bogue, “Leaving
Engineering: A Multi‐Year Single Institution Study.” Journal of
Engineering Education, vol. 101, pp. 6-27, 2012.
E. Kirkman, CBMS 2010 survey. Providence, RI: American
Mathematical
Society,
http://www.ams.org/profession/data/cbmssurvey/cbms-reports. 2013.
G. Wake, “Introduction to the Special Issue: deepening engagement in
mathematics in preuniversity education,” Research in Mathematics
Education, vol. 13, pp. 109-118, 2011.
G. C. Wolniak, M. J. Mayhew, and M. E. Engberg, “Learning's Weak
Link to Persistence,” The Journal of Higher Education, vol. 83, pp. 795823, 2012.
D. M. Bressoud, M. P. Carlson, V. Mesa, and C. Rasmussen, “The
calculus student: insights from the Mathematical Association of
America national study.” International Journal of Mathematical
Education in Science and Technology, vol. 44, pp. 685-698, 2013.
S. R. Herriott, and S. R. Dunbar, “Who Takes College Algebra?”
PRIMUS, vol. 19, pp. 74-87, 2009.
L. A. Steen, “Calculus for a new century: a pump, not a filter, MAA
notes number 8,” Washington, DC: Mathematical Association of
America, 1988.
A. V. Maltese, and R. H. Tai, “Pipeline persistence: Examining the
association of educational experiences with earned degrees in STEM
among US students,” Science Education, vol. 95, pp. 877-907, 2011.
N. C. Ware, and V. E. Lee, “Sex differences in choice of college science
majors,” American Educational Research Journal, vol. 25, pp. 593-614,
1988.

[18] A. Astin, What matters in college. San Francisco: Jossey-Bass, 1993.
[19] O. Eris, D. Chachra, H. Chen, C. Rosca, L. Ludlow, S. Sheppard, and K.
Donaldson, “A preliminary analysis of correlates of engineering
persistence: Results from a longitudinal study.” in Proceedings of the
American Society for Engineering Education Annual Conference, pp.
24-27, June 2007.
[20] J. C. Hilpert and J. Husman, Linking Instruction and Engagement in the
Engineering Classroom. Tempe, AZ: Arizona State University, 2014.
[21] M. Pampaka, J. Williams, G. D. Hutcheson, P. Davis, and G. Wake,
“Association between mathematics pedagogy and learners’ dispositions
for university study,” British Educational Research Journal, vol. 38, pp.
473-496., 2012.
[22] E. Seymour, and N. M. Hewitt, “Talking about leaving: Why
undergraduates leave the sciences” (Vol. 12), Boulder, CO: Westview
Press., 1997.
[23] J. Ellis, M. L. Kelton, and Rasmussen, C. “Student perceptions of
pedagogy and associated persistence in calculus.” ZDM, pp. 1-13,
March, 2014.
[24] C. Rasmussen, J. Ellis, and D. Bressaud, “Who Are the Students that
Switch out of Calculus and Why?” Unpublished, 2014.
[25] O. Eris, D. Chachra, D., H. L. Chen, S. Sheppard, L. Ludlow, C. Rosca,
and G. Toye, “Outcomes of a longitudinal administration of the
persistence in engineering survey.” Journal of Engineering Education,
vol. 99, pp. 371-395, 2010

