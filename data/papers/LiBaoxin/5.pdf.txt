Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)

Clustering-Based Joint Feature Selection
for Semantic Attribute Prediction⇤
Lin Chen and Baoxin Li
Arizona State University, Tempe Arizona
{lin.chen.cs, baoxin.li}@asu.edu
Abstract
Semantic attributes have been proposed to bridge
the semantic gap between low-level feature representation and high-level semantic understanding
of visual objects. Obtaining a good representation of semantic attributes usually requires learning
from high-dimensional low-level features, which
not only significantly increases the time and space
requirement but also degrades the performance
due to numerous irrelevant features. Since multiattribute prediction can be generalized as an multitask learning problem, sparse-based multi-task feature selection approaches have been introduced,
utilizing the relatedness among multiple attributes.
However, such approaches either do not investigate the pattern of the relatedness among attributes,
or require prior knowledge about the pattern. In
this paper, we propose a novel feature selection approach which embeds attribute correlation modeling in multi-attribute joint feature selection. Experiments on both synthetic dataset and multiple public benchmark datasets demonstrate that the proposed approach effectively captures the correlation
among multiple attributes and significantly outperforms the state-of-the-art approaches.

1

Figure 1: Illustration of Shoe images with three corresponding attributes “High Heel”, “Formal” and “Red”.
Good representations of semantic attributes are often built
on top of high-dimensional, low-level features. Attribute
learning directly based on such raw, high-dimensional features may suffer from the curse of dimensionality curse. Further, often it is reasonable to assume that not all the low-level
features would have equal contribution to all the attributes.
Feature selection, selecting a subset of most relevant features for a compact and accurate presentation, is proven to
be an effective and efficient way to handle high-dimensional
data [Tang et al., 2014].
Multi-task joint feature selection has been introduced by
[Chen et al., 2014] for attribute ranking by exploring the correlation among attributes. However, this work assumes that
all attributes are correlated by sharing the same subset of features, which is not always accurate. For example, as shown in
Figure 1, a “high-heel” shoe is usually considered as a “formal” shoe as well. It is reasonable to assume these attributes
share the same subset of features, e.g., shape-related descriptors. However, it is hard to identify whether “high heel” or
“formal” shoes are in red, which suggests the attribute “color”
may not share the same subset of features with the other attributes but is determined by, e.g., color-related descriptors.
In other words, attributes are usually related in clustering
structures. [Jayaraman et al., 2014] first explores such clustered relatedness on attribute prediction. However, their approach requires manually specified group structure as prior.
To our knowledge, there is still lack of a feature selection
approach being able to identify grouping/clusering structures
among attributes for improved attribute prediction.
In this paper, we propose a regularization-based multi-task
feature selection approach that aims at automatically partitioning the attributes into groups while simultaneously uti-

Introduction

Recent literature has witnessed fast development of representations using semantic attributes, whose goal is to bridge
the semantic gap between low-level feature representation
and high-level semantic understanding of visual objects. Attributes refer to visual properties that help describe visual
objects or scenes such as “natural” scenes, “fluffy” dogs,
or “formal” shoes. Visual attributes exist across object category boundaries and many methods have been employed
in applications including object recognition [Farhadi et al.,
2010], face verification [Song et al., 2012], image search [Kovashka et al., 2012; Scheirer et al., 2012] and sentiment analysis [Wang et al., 2015].
⇤
The work was supported in part by ONR grant N00014-15-12344 and ARO grant W911NF1410371. Any opinions expressed in
this material are those of the authors and do not necessarily reflect
the views of ONR or ARO.

3338

lizing such group information for attribute-dependent feature
selection. We employ a clustering regularizer for attribute
partition, where strong attribute relatedness is assumed to exist within each cluster. Besides, a group-sparsity regularizer is
imposed on the objective function to encourage intra-cluster
feature sharing and inter-cluster feature competition. Under
this formulation, we propose an alternating structure optimization algorithm, which efficiently solves the relaxed form
of the proposed formulation. We verify the effectiveness and
generalization capability of our approach on both synthetic
and real-world benchmark datasets. The results show that our
approach outperforms the state-of-the-art approaches on feature selection, attribute prediction and zero-shot learning.

2

where mi denotes the mean vector of the i-th cluster. Let
ei = [1, 1, . . . , 1]> 2 Rni ⇥1 , then Eq. (1) can be derived as
ni
k X
X
i=1 j=1

=

2.2

+ (Tr(W > W )

min

Based on the assumption that correlated attributes would
share the same features, we propose to model attribute correlation via learning the clustering structures through k-means.
Let E be a permutation partition matrix, then a partition of
the projection matrix W into k clusters can be formed as:

W,F ;F > F =Ik

L(W > X, Y ) + ↵

k
X
i=1

kWi k2,1 + Tr(W > W )

+ (Tr(W > W ) Tr(F > W > W F ))
(5)
where ↵ controls the sparsity of W . The key idea lying here
is that we use the clustering regularizer to partition the tasks
into groups where strong correlation exists among tasks in
the same group; and feature selection based on such group
structures would make sure appropriate feature subsets are
selected to represent the respective semantic attributes.

(i)

W E = [W1 , W2 , . . . , Wk ], Wi = [w1 , w2 , . . . , w(i)
ni ];
where Wi 2 Rd⇥ni (i = 1, 2, . . . , k) is the i-th partitioned
group includes ni projection vectors (or attribute labels). The
associated sum-of-squares cost function for the partition can
be formulated as
mi k , mi =

Tr(F > W > W F ))

s.t. F > F = Ik , s 2 {0, 1}n , s> 1n = K
(4)
where controls the contribution from modeling label correlation and controls the generalization performance.
The constraint on s makes Eq. (4) a mixed integer programming problem, which is difficult to solve. We observe
that diag(s) and W are in the form of W T diag(s). Since s
is a binary vector and d K rows of the diag(s) are all zeros, W T diag(s) is a matrix where the elements of many rows
are all zeros. This motivates us to absorb diag(s) into W as
W = W T diag(s), and add `2,1 -norm on each grouped Wi
to encourage sparse-based group-wise joint feature selection.
With this relaxation, Eq. (4) can be rewritten as:

Modeling Label Correlation

i=1 j=1

Feature Selection

min L(W > diag(s)X, Y ) + Tr(W > W )

where L(·) is the loss function and typical choices of loss
functions include least square and logistic regression.

(i)
wj /ni

(2)

W,F,s

s.t., s 2 {0, 1}n , sT 1n = K

ni
X

ei >
ei
( p )Wi> Wi ( p )
ni
ni

With the model component to capture attribute correlation in
Eq. (3), the proposed feature selection framework is to solve
the following optimization problem:

W,s

2

ei ei > 2
)kF
ni

min Tr(W > W ) Tr(F > W > W F )+ Tr(W > W ) (3)

min L(W > diag(s)X, Y )

(i)
kwj

i=1

kWi (Ini

F > F =Ik

K

ni
k X
X

mi k =

k
X

Tr(W > W ) Tr(F > W > W F )
To make the problem tractable, we ignore the special structure of F and let it be an arbitrary orthonormal matrix. By
adding a global penalty Tr(W > W ) measuring how large the
weight vectors are, capturing label correlation is to partition
W into k clusters, which can be achieved by solving the following optimization problem:

z }| { z }| {
⇡(0, . . . , 0, 1, . . . , 1), where ⇡(·) is the permutation function
and K is the number of features to select where si = 1 indicates that the i-th feature is selected. The original data can
be represented as diag(s)X with K selected features where
diag(s) is a diagonal matrix. We assume that a linear projection matrix W = [w1 , w2 , . . . , wm ] 2 Rd⇥m maps the
data X to its label matrix Y where wi 2 Rd is the projection
vector for the i-th class ci . If we do not consider attribute correlation, we can select K features via solving the following
optimization problem:

(i)

2

Let F = diag( pen1 1 , pen2 2 , . . . , penkk ) 2 Rm⇥k be an orthonormal matrix, then Eq. (2) can be rewritten as

Let F = {f1 , f2 , . . . , fd } be the set of d features and then
we can represent a set of n instances by the feature set F as
X = [x1 , x2 , . . . , xn ] 2 Rd⇥n . Let C = {c1 , c2 , . . . , cm }
be the set of m attribute labels and Y = [y 1 , y 2 , . . . , y n ] 2
{0, 1}m⇥n denotes the label matrix where y i 2 Rm (i =
1, 2, . . . , n) is the label vector of the i-th instance. We aim
to select K(Kd) most relevant features from F by leveraging X, Y and the attribute correlation in C. Let s =

2.1

Tr(Wi> Wi )

i=1

Methodology

d K

k
X

(i)
kwj

3

Algorithm

In this section, we first introduce an optimization algorithm
to seek an optimal solution (summarized in Algorithm 1) for
Eq. (5). Then we propose an approach to estimate the attribute assignment (summarized in Algorithm 2).

(1)

j=1

3339

3.1

Optimization

Algorithm 1 Feature Selection Optimization
Input:
1. Multiple attribute data {X, Y };
2. Parameters ↵, , k(optional) and the number of selected features K;
3. The initial projection matrix W0 ;
Procedure:
1: Set W = W0 ;
2: repeat
3:
Update M according to Eq. (8);
4:
Update r according to Alg. 2;
5:
Update according to Eq. (10);
6:
Update W according to Eq. (11);
7: until Converges
8: Sort each feature according to kwi k2 in descending order
of each group;
9: return The group-wise top-K ranked features;

The optimization problem in Eq. (5) is non-convex nonsmooth, which makes the formulation difficult to solve in its
original form. Thus we adopt several relaxations to make it
solvable.
The attribute correlation regularization in Eq. (3) can be
rewritten as:
Tr(W ((1 + ⌘)I F F > )W > )
where ⌘ = / > 0. Let M = F F > , according to [Zhou
et al., 2011] the previous regularizer can be relaxed into the
following convex form:
⌘(1 + ⌘)Tr(W (⌘I + M ) 1 W > )
s.t. tr(M ) = k, M I, M 2 Sm
(6)
+
m
where S+ is the set of m⇥m positive semidefinite matrices.
Following a similar idea in [Bach, 2008], we reformulate
Eq. (5) by squaring the `2,1 norm. Since the `2,1 norm is positive, the squaring represents a smooth monotonic mapping.
Without loss of the generality, we adopt the traditional least
square loss for demonstration in this paper. Then we get the
following jointly convex smooth objective function regarding
to W and M .
k
X
arg min kW > X Y k2F + ↵
(kWi k2,1 )2
W,M

Algorithm 2 Cluster Assignment Estimation
Input: M;
Procedure:
1: Approximate F by top-ranked eigenvector of Q;
2: Calculate R11 , R12 by applying QR decomposition with
column pivoting on F by Eq. (12);
3: Calculate R̂ by Eq. (13);
4: calculate r by Eq. (14) for each attribute;
5: return Cluster assignment vector r;

i=1

⌘(1 + ⌘)Tr(W (⌘I + M ) 1 W > )
s.t. tr(M ) = k, M I, M 2 Sm
(7)
+
Since it is difficult to optimize the linear projection matrix W and attribute correlation matrix M simultaneously,
we employ Alternating Structure Optimization (ASO), which
has been shown to be effective in many practical applications [Blitzer et al., 2006; Quattoni et al., 2007] and is guaranteed to converge to a global optimal solution.

Optimizing W When Fixing M
The squared group-wise `2,1 norm in Eq. (7) is still difficult
to derive directly. To alleviate that, we introduce
Psome
P positive dummy variables ij 2 R+ which satisfies i j ij =
1. [Argyriou et al., 2008] proves an upper bound of the
squared `2,1 norm in terms of the positive dummy variables

Optimizing M when fixing W
Given a fixed W , the optimization problem is decoupled into
the following optimization problem:
min Tr(W (⌘I + M )
M

1

k
X

W >)

i=1

s.t.

i=1

i

= k, 0 

i 1

k X
d
X
i=1 j=1

kwi,j k2 )2 

k X
d
X
(kwi,j k2 )2
ij

i=1 i=1

where wi,j 2 R1⇥m is the row vector of Wi . Thus
updated by holding the equality:

s.t. tr(M ) = k, M I, M 2 Sm
(8)
+
We solve the problem based on the following Lemma due
to [Zhou et al., 2011]:
Lemma 1 For the optimization problem in Eq. (8), let W =
U ⌃V be the singular value decomposition of W where ⌃ =
diag([ 1 , 2 , . . . , m ]), M = Q⇤Q> be the Eigen decomposition of M where ⇤ = diag([ 1 , 2 , . . . , q ]) and q be the
rank of ⌃. Then the optimal Q⇤ is given by Q⇤ = V and
the optimal ⇤⇤ is given by solving the following optimization
problem:
q
2
X
i
⇤⇤ = arg min
⇤
⌘+ i
i=1
q
X

(kWi k2,1 )2 = (

ij

= kwi,j k2 /

d
X
j=1

ij

can be

(10)

kwi,j k2 .

Given a fixed M , each projection vector w can then be
updated by optimize the following problem
arg min kW T X
W

Y k2F + ↵

k X
d
X
(kwi,j k2 )2
ij

i=1 i=1

⌘(1 + ⌘)Tr(W (⌘I + M )

1

>

W )

(11)

which can be solved by gradient-type approach.

3.2

(9)

Estimating Attribute Assignment

The group-wise feature selection is conducted by the clustering structure of the attribute. However, given the M optimized by the previous algorithm, it is not readily possible

Eq. (9) can be solved using the similar technology in [Jacob
et al., 2009].

3340

zero-shot learning capabilities on image benchmark datasets.
All the datasets are standardized to zero-mean and normalized by the standard deviation. For all approaches, the super
parameters are selected via cross-validation. We cannot get
the number of cluster k without any prior knowledge for realworld, thus we also select k by the prediction accuracy on a
small subset of datasets.

to observe the cluster assignment of the attributes because M
is spectrally relaxed. In this subsection, we propose an approach to acquire the cluster structure.
We first need to obtain a good approximation of the cluster
indicator matrix F . Given M , we first apply Eigen decomposition M = Q⇤Q> where each column of Q is the eigenvector and each diagonal element of ⇤ is the eigenvalue. Then
we rank the columns of Q in decreasing order according to
its corresponding eigenvalues, and the top-ranked k columns
give an approximation of the cluster assignment matrix F .
The number of the cluster k can be either manually specified
or automatically explored by setting a threshold (10e 8 in
our experiment) regarding to the absolute value of the eigenvalue.
After obtaining F , without loss of generality, we assume
the optimized W = [W1 , W2 , · · · , Wk ]T where the submatrix Wi includes all attributes belonging to the i-th cluster.
Let ti = [ti1 , ti2 , . . . , tini ]T denote the largest eigenvector of
Wi T Wi , [Zha et al., 2002] showed that F can be reformulated as

4.1

Since it is difficult to obtain the groundtruth cluster structure for real applications, we first verify the effectiveness
of the proposed approach in obtaining the cluster structures on simulated dataset. Following [Jacob et al., 2009;
Zhou et al., 2011], we construct the synthetic data containing
5 clusters with 10 learning tasks in each cluster, generating a
total number of 50 tasks. For the i-th task, a dataset Xi 2Rd⇥n
is randomly drawn from a normal distribution N (0, 1) for
learning, with the dimension d = 30 and the sample size
n = 60.
The projection model is constructed as follows. For the ith cluster, we generate a cluster weight vector wci 2Rd drawn
from the normal distribution N (0, 900). Then 15 dimensions
of wci are randomly but carefully selected and assigned to zeros, to ensure all wc are orthogonal to each other. Similarly,
for the j-th task belonging to cluster i, we generate a taskspecific weight vector wsj 2Rd drawn from the normal distribution N (0, 16) with the same dimensions of wci assigned
to zeros. Thus, the ultimate weight vector of the j-th task is
the linear combination of the cluster and task-specific weight
vector wj = wci + wsj .
The corresponding response y i of the i-th samples xi of
task j is then obtained by y i = wTj xi + "i where " is the
noise vector drawn from N (0, 0.1). We choose 0.5 as the
threshold to assign binary label to each sample.
We verify the effectiveness of our proposed approach by
comparing the learned cluster structure and the selected features with the groundtruth. Based on the prior knowledge
implied by the construction of the groundtruth, We set k = 5
and the number of selected features as K = 15. Figure 2
shows one example of the learned projection matrix 2(b) with
the comparison of the groundtruth 2(a) where the white part
represents zeros and the black part represents non-zeros. The
result shows that our approach is able to roughly capture the
correct group sparse structures.

F T = [t11 v 1 , · · · , t1s1 v 1 , · · · , tk1 v k , · · · , tks1 v k ]
|
{z
}
|
{z
}
cluster1

clusterk

where V = [v1 , v2 , · · · , vk ] 2 R
is an orthogonal matrix.
Since vi is orthogonal to each other, the cluster structure
can be acquired by picking up a column of F which has the
largest norm as the first cluster, and orthogonalizing the other
columns against this column. Then the same process is executed on the rest of columns until all clusters are identified.
This process is identical to a QR decomposition with column
pivoting on F
F T = Q[R11 , R12 ]P T
(12)
T

k⇥k

where Q 2 Rk⇥k is an orthogonal matrix, R11 2 Rk⇥k is
an upper triangular matrix and P 2 Rm⇥m is a permutation
matrix. Then we calculate the cluster assignment matrix R̂ 2
Rk⇥m by
R̂ = [Ik , R111 R12 ]P T
(13)
where Ik 2 Rk⇥k is an identity matrix. The cluster assignment information can then be inferred from R̂. The cluster
membership of each attribute (column) is determined by the
row index of the largest element (in absolute value) of the
corresponding column in R̂. Denote r 2 Rm as the cluster
identification vector where ri records which cluster the i-th
class belongs to, then r can be calculated by
ri = arg max r̂ij
j

Simulation Study

(14)

where r̂ij is the (i, j)-th entry of R̂.

4

Experiments

(a) Groundtruth model

In this section, we first verify the effectiveness of our proposed approach on one synthetic dataset. Since the proposed
approach can be generalized to general multi-label problem,
we evaluate the feature selection capability on various benchmark datasets. At last we evaluate the attribute prediction and

(b) Learned model

Figure 2: The learned projection matrix and the corresponding groundtruth in the simulation experiments. The white
parts are zeros and the black parts are non-zeros.

3341

4.2

Feature Selection

The experiments are conducted on three benchmark
datasets: aYahoo [Farhadi et al., 2009], Animals with Attributes (AwA) [Lampert et al., 2009] and SUN attribute [Patterson and Hays, 2012] and the statistics of the datasets are
summarized in Table 4. To obtain a good representation of
the high-level attributes, we require that the features can capture both the spatial and context information. Thus, we constructed the features by pooling a variety types of feature histograms including GIST, HoG, SSIM. For aPascal/aYahoo
and AwA datasets we use predefined seen/unseen split published with the datasets. For SUN dataset, 60% of categories
are randomly split out as “seen” categories in each round with
the rest as “unseen” categories. During training 50% of samples are randomly and carefully drawn from each seen categories to ensure the balance of the positive and negative attribute labels. The rest samples from “seen” classes and all
samples from “unseen” classes are used for testing.
Table 3 shows the average prediction accuracy of each
approach over all attributes by running the experiment 10
rounds. The result shows that for both “seen” and “unseen”
categories, DSVA outperforms MTAL in prediction accuracy and our proposed approach further outperforms DSVA
by 2%⇠4%. DSVA decorrelates low-correlated attributes
compared with MTAL thus achieves better prediction performance. However, the manually specified or off-line learned
group structures are not able to achieve the optimal result.
Our approach iteratively optimizes the clustering structure
and the projection model, which achieves the best performance.

We verify the feature selection capability on general multilabel datasets in this section. The experiment is conducted
on 6 public benchmark feature selection datasets including
one object image dataset COIL100 [COI, 1996], one handwritten digit image dataset USPS [Hull, 1994], one spoken
letter speech dataset Isolet [Fanty and Cole, 1991], three
face image dataset YaleB [Georghiades et al., 2001], ORL
[Samaria and Harter, 1994] and PIX10P1 . The statistics of
the datasets are summarized in Table 2. We compare the proposed approach with the following representative feature selection algorithms: Fisher Score [Duda et al., 2001], mRMR
[Peng, 2005], Relief-F [Liu and Motoda, 2008], Information Gain [Cover and Thomas, 1991], MTFS [Argyriou et al.,
2008].
Following the common way to evaluate supervised feature selection, we assess the quality of selected features in
terms of the classification performance [Han et al., 2013;
Cai et al., 2013]. The larger classification accuracy is, the better performance the corresponding feature selection approach
achieves. In our experiments, we employ linear Support Vector Machine (SVM) and k-nearest neighbors (kNN) classifier
with k = 3 for evaluation. How to determine the optimal
number of selected features is still an open question for feature selection; hence we vary the number of selected features
as {10,30, 50 . . . ,90} in this work. In each setup 50% samples are randomly selected for training and the remaining is
for testing. Specific constrains are imposed to make sure the
class labels of the training set are balanced. The whole experiment is conducted 10 rounds and average accuracies are
reported.
Figure 1 shows the comparison results for SVM and kNN
on the 6 benchmark datasets when 50 features are selected.
The result shows that MTFS and the proposed framework
outperform Fisher Score, mRMR and Information Gain. The
performance gain comes from that Fisher Score, mRMR and
Information Gain select features one by one while MTFS and
FSMC select features in a batch model. It is consistent with
what was suggested in [Tang and Liu, 2012] that it is better
to analyze features jointly for feature selection. Besides, in
most cases, the proposed framework outperforms MTFS. Better performance gain is usually achieved when fewer number
of features are selected. This performance gain suggests that
modeling label correlation can significantly improve feature
selection performance for multi-class data.

4.3

4.4

We also experiment on the zero-shot learning problem on all
three datasets. Zero-shot learning aims to learn a classifier
based on training samples from some seen categories, and
classify some new samples to a new unseen category. We
adopt the Direct Attribute Prediction (DAP) framework proposed in [Lampert et al., 2009] with attribute prediction probability from each approaches as input. Since only continuous
image level attribute labels are provided on the SUN dataset,
we construct the class level attribute labels by thresholding
the average attribute label values of all samples from the
class. Same “Seen”\“Unseen” categories splits are adopted
as previous experiments.
The Average classification accuracies of 10 rounds experiment are reported in Table 5. The result shows that on aYahoo and AwA, our approach achieves significant performance
gains than the baseline approaches. The large number of categories in SUN dataset make the classification problem very
hard which leads to all low performance of all approaches.
Our approach still works better than the baseline approaches.

Attribute Prediction

We then compare our approach with state-of-the-art attribute
learning work [Chen et al., 2014] (referred as MTAL) and
[Jayaraman et al., 2014] (referred as DSVA). Since MTAL is
initially proposed for attribute ranking, we replace the original loss function with the one adopted in this paper for fair
comparison. DSVA requires attribute groups as prior, thus we
run k-means offline to obtain the clusters for datasets do not
have such information.
1
PIX10P
is
publicly
available
https://featureselection.asu.edu/datasets.php

Zero-shot Learning

4.5

On Choosing the Parameters

The proposed framework has three important parameters - ↵
controlling the sparsity of W , controlling the contribution
of modeling label correlation and gamma controls the global
penalty. We study the effect of each parameter by fixing the
other to see how the performance of the proposed approach
varies with the number of selected features. Due to the page

from

3342

Table 1: Classification results (ACC%±std) of different feature slection algorithm on different datasets. (the higher the better).
Algorithm DataSet
Fisher
mRMR
Relief-F
Information Gain
MTFS
Proposed
COIL100 60.66±3.54 55.72±3.34 62.80±2.56
62.00±2.84
78.77±2.35 79.08±2.12
USPS
86.30±2.81 58.44±4.02 86.83±2.83
70.25±3.16
86.25±2.52 93.15±2.18
Isolet
75.64±3.01 70.92±3.72 82.30±2.81
76.51±2.56
84.05±2.24 87.06±1.98
SVM
YaleB
66.85±3.65 56.91±4.21 71.91±2.24
71.74±2.11
76.08±2.14 78.17±2.18
ORL
46.50±4.21 84.51±2.32 67.18± 3.01
53.24±2.96
85.62±1.94 90.51±1.78
PIX10P
93.56±2.01 90.45±3.32 96.00±1.77
92.01±1.97
96.81±1.54 99.54±1.68
COIL100 63.33±3.21 54.86±4.32 65.11±2.01
63.44±2.76
81.86±1.94 82.48±1.68
USPS
89.39±2.11 59.17±3.72 89.61±2.01
74.70±2.76
90.44±1.54 95.53±1.18
Isolet
75.38±2.45 57.56±3.42 79.87±2.21
73.71±2.42
77.01±2.14 83.21±2.18
kNN
YaleB
69.17±3.24 58.41±3.72 65.53±2.81
65.37±2.42
77.08±2.45 78.96±2.28
ORL
53.01±3.44 72.56±2.42 60.38±2.71
52.44±2.76
85.86±2.24 88.10±2.10
PIX10P
94.56±1.91 86.45±2.22 96.00±1.81
86.04±2.04
97.81±1.54 99.34±1.22
regularizer encourages intra-group feature-sharing and intergroup feature competition. With an efficient alternating optimization algorithm, the proposed approach is able to obtain a good group structure and select appropriate features
to represent semantic attributes. The proposed approach was
verified on both synthetic and real-world benchmark datasets
with comparison with state-of-the-art approaches. The result shows effective group structure identification capability
of our method, as well as its significant performance gains on
feature selection, attribute prediction and zero-shot learning.

Table 2: Statistics of the Feature Selection datasets
Dataset
# of Samples # of Features # of Classes
COIL100
7200
1024
100
YaleB
2414
1024
38
ORL
400
4096
40
PIX10P
100
10000
10
USPS
9298
256
10
Isolet
7797
617
150

References
[Argyriou et al., 2008] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning.
J. Mach. Learn. Res., 73(3):243–272, December 2008.
[Bach, 2008] Francis R. Bach. Consistency of the group
lasso and multiple kernel learning. J. Mach. Learn. Res.,
9:1179–1225, June 2008.
[Blitzer et al., 2006] John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspondence learning. EMNLP ’06, pages 120–128, 2006.
[Cai et al., 2013] Xiao Cai, Feiping Nie, and Heng Huang.
Exact top-k feature selection via l2,0-norm constraint. In
IJCAI ’13, pages 1240–1246, 2013.
[Chen et al., 2014] Lin Chen, Qiang Zhang, and Baoxin Li.
Predicting multiple attributes via relative multi-task learning. In Proc. of CVPR’14, pages 1027–1034, June 2014.
[COI, 1996] Columbia Object Image Library (COIL-100).
Technical report, Columbia University, 1996.
[Cover and Thomas, 1991] T. M. Cover and J. A. Thomas.
Elements of Information Theory. Wiley, 1991.
[Duda et al., 2001] R.O. Duda, P.E. Hart, and D.G. Stork.
Pattern Classification. John Wiley & Sons, New York, 2
edition, 2001.
[Fanty and Cole, 1991] Mark Fanty and Ronald Cole. Spoken letter recognition. In NIPS ’91, pages 220–226. 1991.
[Farhadi et al., 2009] A. Farhadi, I. Endres, D. Hoiem, and
D. Forsyth. Describing objects by their attributes. In Com-

Figure 3: Parameter Analysis on SVM.
limitation, we only report the result on the Isolet dataset with
SVM but we have similar observations in other datasets.
Figure 3 demonstrates the performance variance w.r.t. different parameters and the number of selected features. With
the increase of , the performance first increases, demonstrating the importance of modeling label correlation, and then
decreases. This property is practically useful because we can
use this pattern to set . When ↵ increases, the performance
also increases dramatically, which suggests the capability of
`2,1 -norm for feature selection. The performance also increases with and then decrease, but relatively stable. The
best performance is achieved around 0.1.

5

Conclusions

In this paper, we proposed a clustering-base multi-task joint
feature selection framework for semantic attribute prediction.
Our approach employs both clustering and group-sparsity
regularizers for feature selection. The clustering regularizer
partitions the attributes into different groups where strong
correlation lies among attributes in the same group while
weak correlation exists between groups. The group-sparsity

3343

Table 3:
DataSet
Methods
MTAL
DSVA
Porposed

Average prediction accuracies of all attributes on Seen and Unseen categories (the higher the better).
aPascal/aYahoo
AwA
SUN
Seen
Unseen
Seen
Unseen
Seen
Unseen
0.5967±0.020 0.5663±0.022 0.5976±0.011 0.5587±0.012 0.6326±0.021 0.6020±0.022
0.6105±0.018 0.5826±0.019 0.6053±0.015 0.5622±0.018 0.6469±0.025 0.6165±0.027
0.6363±0.014 0.6011±0.015 0.6254±0.007 0.5837±0.008 0.6682±0.011 0.6324±0.013
[Liu and Motoda, 2008] H. Liu and H. Motoda, editors.
Computational Methods of Feature Selection. Chapman
& Hall, 2008.
[Patterson and Hays, 2012] Genevieve Patterson and James
Hays. Sun attribute database: Discovering, annotating, and
recognizing scene attributes. In Proc. of CVPR’12, 2012.
[Peng, 2005] F. Ding C. Peng, H. Long. Feature selection
based on mutual information: Criteria of max-dependency,
max-relevance, and min-redundancy. IEEE Trans. Pattern
Anal. Mach. Intell., 27(8):1226–1238, 2005.
[Quattoni et al., 2007] A. Quattoni, M. Collins, and T. Darrell. Learning visual representations using images with
captions. In CVPR ’07, pages 1–8, June 2007.
[Samaria and Harter, 1994] F.S. Samaria and A.C. Harter.
Parameterisation of a stochastic model for human face
identification. In Applications of Computer Vision’ 94,
pages 138–142, Dec 1994.
[Scheirer et al., 2012] W.J. Scheirer, N. Kumar, P.N. Belhumeur, and T.E. Boult. Multi-attribute spaces: Calibration for attribute fusion and similarity search. In Proc. of
CVPR’12, pages 2933–2940, June 2012.
[Song et al., 2012] Fengyi Song, Xiaoyang Tan, and Songcan Chen. Exploiting relationship between attributes for
improved face verification. In Proc. of BMVC’12, pages
27.1–27.11, 2012.
[Tang and Liu, 2012] Jiliang Tang and Huan Liu. Feature selection with linked data in social media. In SDM ’12, pages
118–128. SIAM, 2012.
[Tang et al., 2014] Jiliang Tang, Salem Alelyani, and Huan
Liu. Feature selection for classification: A review.
Data Classification: Algorithms and Applications. Editor:
Charu Aggarwal, CRC Press In Chapman & Hall/CRC
Data Mining and Knowledge Discovery Series, 2014.
[Wang et al., 2015] Yilin Wang, Suhang Wang, Jiliang Tang,
Huan Liu, and Baoxin Li. Unsupervised sentiment analysis for social media images. In Proceedings of the TwentyFourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31,
2015, pages 2378–2379, 2015.
[Zha et al., 2002] Hongyuan Zha, Xiaofeng He, Chris Ding,
Ming Gu, and Horst D. Simon. Spectral relaxation for kmeans clustering. In Proc. of NIPS’02, pages 1057–1064.
2002.
[Zhou et al., 2011] Jiayu Zhou, Jianhui Chen, and Jieping
Ye. Clustered multi-task learning via alternating structure
optimization. In Proc. of NIPS’11, pages 702–710. 2011.

Table 4: Statistics of Attribute Prediction Image Datasets.
Dataset
aPascal/aYahoo AwA
SUN
# of images
15339
30475 14340
# of attributes
64
85
102
# of classes
32
50
611
# of features
2429
1200
1112
Table 5: Zero-shot learning accuracy on both real dataset.
aYahoo
AwA
SUN
MTAL
0.1834 0.2953 0.1842
DSVA
0.2052 0.3085 0.2010
Proposed
0.2262 0.3258 0.2133
puter Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on, pages 1778–1785, June 2009.
[Farhadi et al., 2010] A. Farhadi, I. Endres, and D. Hoiem.
Attribute-centric recognition for cross-category generalization. In Proc. of CVPR’10, pages 2352–2359, June
2010.
[Georghiades et al., 2001] A.S. Georghiades, P.N. Belhumeur, and D.J. Kriegman.
From few to many:
Illumination cone models for face recognition under
variable lighting and pose. IEEE Trans. Pattern Anal.
Mach. Intell., 23(6):643–660, 2001.
[Han et al., 2013] Yahong Han, Yi Yang, and Xiaofang
Zhou. Co-regularized ensemble for feature selection. In
IJCAI ’13, pages 1380–1386, 2013.
[Hull, 1994] J. J. Hull. A database for handwritten text
recognition research. IEEE Trans. Pattern Anal. Mach.
Intell., 16(5):550–554, May 1994.
[Jacob et al., 2009] Laurent Jacob, Jean philippe Vert, and
Francis R. Bach. Clustered multi-task learning: A convex
formulation. In Proc. of NIPS’09, pages 745–752. 2009.
[Jayaraman et al., 2014] D. Jayaraman, Fei Sha, and
K. Grauman. Decorrelating semantic visual attributes by
resisting the urge to share. In Proc. of CVPR’14, pages
1629–1636, June 2014.
[Kovashka et al., 2012] A. Kovashka, D. Parikh, and
K. Grauman. Whittlesearch: Image search with relative attribute feedback. In Proc. of CVPR’12, pages
2973–2980, June 2012.
[Lampert et al., 2009] C.H. Lampert, H. Nickisch, and
S. Harmeling. Learning to detect unseen object classes
by between-class attribute transfer. In Proc. of CVPR’09,
pages 951–958, June 2009.

3344

