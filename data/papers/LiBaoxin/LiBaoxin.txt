The performance of action recognition in video sequences depends significantly on the representation of actions and the similarity measurement between the representations. In this paper, we combine two kinds of features extracted from the spatio-temporal interest points with context-aware kernels for action recognition. For the action representation, local cuboid features extracted around interest points are very popular using a Bag of Visual Words (BOVW) model. Such representations, however, ignore potentially valuable information about the global spatio-temporal distribution of interest points. We propose a new global feature to capture the detailed geometrical distribution of interest points. It is calculated by using the 3D  R  transform which is defined as an extended 3D discrete Radon transform, followed by the application of a two-directional two-dimensional principal component analysis. For the similarity measurement, we model a video set as an optimized probabilistic hypergraph and propose a context-aware kernel to measure high order relationships among videos. The context-aware kernel is more robust to the noise and outliers in the data than the traditional context-free kernel which just considers the pairwise relationships between videos. The hyperedges of the hypergraph are constructed based on a learnt Mahalanobis distance metric. Any disturbing information from other classes is excluded from each hyperedge. Finally, a multiple kernel learning algorithm is designed by integrating the  l2l2  norm regularization into a linear SVM classifier to fuse the  îˆ¾R  feature and the BOVW representation for action recognition. Experimental results on several datasets demonstrate the effectiveness of the proposed approach for action recognition.
Although several methods have been developed for protein-protein interaction (PPI) prediction, each method has a specialised emphasis, and it is often necessary to use multiple methods to avoid a high false-negative rate. We here describe a method that is based on binding profiles and only requires protein sequence as an input. We also developed an online platform, the PPI Prediction Platform (PPIPP), to predict PPI networks (PPINs). PPIPP, which is freely accessible at http://ppipp.songbx.me, provides two main functions: PPI prediction, which uses the binding profile method, domain-motif interactions from structural topology, and PPIN-based detection of functionally similar proteins within species. PPIPP offers a web-based interface to facilitate PPIN predictions and a high-performance server to overcome the problems of user access and large-scale computation. The wheat proteome was used to evaluate the performance of this platform.
In many Web image retrieval applications, adapting the retrieval results according to some model of the user is a desired feature as the returned images can be made specifically relevant to a userâ€™s needs. Making retrieval user-adaptive faces several practical challenges, including the ambiguity of user query, the lack of user-adaptive training data, and lack of proper mechanisms for supporting adaptive learning. To address some of these challenges, we propose a hybrid learning strategy that fuses knowledge from both pointwise and pairwise training data into one framework for attribute-based, user-adaptive image retrieval. An online learning algorithm is developed for updating the ranking performance based on user feedback. The framework is also derived into a kernel form allowing easy application of kernel techniques. We use both synthetic and real-world datasets to evaluate the performance of the proposed algorithm. Comparison with other state-of-the-art approaches suggests that our method achieves obvious performance gains over ranking and zero-shot learning. Further, our online learning algorithm was found to be able to deliver much better performance than batch learning, given the same elapsed running time.
The objective of this study was to improve individual tree crown delineation by fully exploiting the crown information exhibited in multi-wavelength LiDAR data. The data used in this study were obtained by an Optech's Titan instrument with three wavelengths: 532 nm, 1064 nm, and 1550 nm. Methods were developed to employ both spectral and structural information of tree crowns to separate crowns from other cover types and from each other. The methods were tested using a data set obtained over a study area in Toronto, Ontario, Canada. Preliminary results show that with both the canopy height model and intensities corresponding to the three wavelength, trees were distinguished from other cover types with a high accuracy and trees were separated from each other with a reasonable accuracy (based on visual observation).
With the advent of progressive format display and broadcast technologies, video deinterlacing has become an important video-processing technique. Numerous approaches exist in the literature to accomplish deinterlacing. While most earlier methods were simple linear filtering-based approaches, the emergence of faster computing technologies and even dedicated video-processing hardware in display units has allowed higher quality but also more computationally intense deinterlacing algorithms to become practical. Most modern approaches analyze motion and content in video to select different deinterlacing methods for various spatiotemporal regions. We introduce a family of deinterlacers that employs spectral residue to choose between and weight control grid interpolation based spatial and temporal deinterlacing methods. The proposed approaches perform better than the prior state-of-the-art based on peak signal-to-noise ratio, other visual quality metrics, and simple perception-based subjective evaluations conducted by human viewers. We further study the advantages of using soft and hard decision thresholds on the visual performance.
Employing correlation among images for improved reconstruction in compressive sensing is a conceptually attractive idea, although developing efficient modeling strategies and reconstruction algorithms are often the key to achieve any potential benefit. This paper presents a novel modeling strategy and an efficient reconstruction algorithm for processing a set of correlated images, jointly taking into consideration inter-image correlation, intra-image correlation and inter-channel correlation. The approach starts with joint modeling of the entire image set in the gradient domain, which supports simultaneous representation of local smoothness, nonlocal self-similarity of every single image, and inter-image correlation. Then an efficient algorithm is proposed to solve the joint formulation, using a Split-Bregman-based technique. Furthermore, to support color image reconstruction, the proposed algorithm is extended by using the concept of group sparsity to explore inter-channel correlation. The effectiveness of the proposed approach is demonstrated with extensive experiments on both grayscale and color image sets. Results are also compared with recently proposed compressive sensing recovery algorithms.
Hierarchical Attention Network for Action Recognition in Videos

Yilin Wang Arizona State University ywang370@asu.edu

Suhang Wang Arizona State University suhang.wang@asu.edu Yi Chang Yahoo Research yichang@yahoo-inc.com

Jiliang Tang Yahoo Research jlt@yahoo-inc.com Baoxin Li Arizona State University baoxin.li@asu.edu

arXiv:1607.06416v1 [cs.CV] 21 Jul 2016

Neil O'Hare Yahoo Research nohare@yahoo-inc.com

Abstract
Understanding human actions in wild videos is an important task with a broad range of applications. In this paper we propose a novel approach named Hierarchical Attention Network (HAN), which enables to incorporate static spatial information, short-term motion information and long-term video temporal structures for complex human action understanding. Compared to recent convolutional neural network based approaches, HAN has following advantages ­ (1) HAN can efficiently capture video temporal structures in a longer range; (2) HAN is able to reveal temporal transitions between frame chunks with different time steps, i.e. it explicitly models the temporal transitions between frames as well as video segments and (3) with a multiple step spatial temporal attention mechanism, HAN automatically learns important regions in video frames and temporal segments in the video. The proposed model is trained and evaluated on the standard video action benchmarks, i.e., UCF-101 and HMDB-51, and it significantly outperforms the state-of-the arts.

1

Introduction

Understanding human actions in wild videos can advance many real-world applications such as social activity analysis, video surveillance and event detection. Earlier works typically rely on hand craft features to represent videos [14, 19]. They often consist of two steps: motion detection and feature extraction. First, motion detectors are applied to detect the informative motion regions in the videos and then, hand craft descriptors such HOG [3], SIFT, or improved Dense Trajectories (iDT) [19] extract the feature patterns from those motion regions to represent the video. In contrast to hand-craft shallow video representation, recent efforts try to learn video representation automatically from large scale labeled video data [10, 16, 4, 20, 8]. For example, In [10], authors stack the video frames as the input for convolution neural networks (CNN) and two stream CNNs [16] combine optical flow and RGB video frames to train CNN and achieve comparable results with the state-of-the art hand craft based methods. Very recently, dense trajectory pooled CNN that combines iDT and two stream CNNs via the pooling layer achieves the state-of-the-art performance. However, [16] and [20] merely use short term motions that cannot capture the order of motion segments and semantic meanings. The challenges of action recognition in wild videos are three-fold. First, there are large intra-class appearance and motion variances in the same action due to different viewpoints, motion speeds, backgrounds, etc. Second, wild videos are often collected from movies, TV shows and media platforms and usually have very low resolutions and noise background clutters, which exacerbate the difficulty for video understanding. Third, long range temporal dependencies are very difficult to capture. For example, the Optical Flow, iDT and 3D ConvNets [8] are computed within a short-time
2016 ( May), Arizona State University, AZ.

Figure 1: An Illustration of the Proposed Model. The block LSTM means a LSTM cell, whose structure is given in Figure 2. The block ATTN indicates the operation to calculate attention weights by using the encoded features from both LSTMs. The block WA represents the weighted average of the input features with the weights from ATTN. window. Long Short Term Memory (LSTM) has been recently applied to video analysis [4] that provides a memory cell for long temporal information. However, it has been shown that the favorable time range of LSTM is around 40 frames [15, 22]. In this work, we aim to develop a novel framework to tackle these obstacles for action recognition in videos. In this paper, we study the problem of video representation learning for action recognition. In particular, we investigate ­ (1) how to utilize the temporal structures in the video to handle intraappearance variances and background clutters by capturing the informative spatial regions; and (2) how to model the short-term as well as long-term motion dependencies for action recognition. Providing answers to these two research questions, we propose a novel Hierarchical Attention Network (HAN) that employs a hierarchical structure with recurrent neural network unit e.g., LSTM and a soft spatial temporal attention mechanism for video action recognition. Our contributions can be summarized as below: · We propose a novel deep learning framework HAN for video action recognition which can explicitly capture both short-term and long-term motion information in an end to end process. · A soft attention is adopted on the spatial-temporal input features with LSTM to learn the important regions in a frame and the crucial frames in the videos. · We conduct an extensive set of experiments to demonstrate that the proposed framework HAN is superior to both state-of-the art shallow video representation based approaches and deep video representation based approaches on benchmark datasets. The rest of this paper is organized as follows. Section 2 reviews related works. Section 3 describes the proposed hierarchical attention deep learning framework in detail. Experimental results and comparisons are discussed in Section 4, followed by conclusions in Section 5.

2

Related Work

Hand crafted features: Video action recognition is a longstanding topic in computer vision community. Many hand-crafted features are used in still images. For example, [14] extends 2D harri corner detector to spatial temporal 3D interest points and achieves good performance with SVM classifier. Later, HOG3D that is based on HOG feature [3] shows its effectiveness by using integral images. 2

Then improved Dense trajectories [19] has dominated the filed of action recognition. It densely samples interest points and tracks them within a short time period. For each point, local descriptors such as HOG, MBH and HOF are extracted for representation. Then, all the features are encoded by the fish vector as the final video representation. Deep learned features: Deep learning such as convolutional neural network has been shown its success in object detection and image classification in recent years. Based on image CNN, [8, 10] extends the CNN framework to videos by stacking video frames. However, the performance is lower than iDT based approaches. In order to better incorporate temporal information, [16] proposes two stream CNNs and achieves comparable results with the state-of-the art performance of hand craft based feature representations [19]. To consider the time dependency, [22] proposes a LSTM based recurrent neural network to model the video sequences. Different from [22] that uses a stack based LSTM, the proposed HAN proposes a hierarchical structure to model the video sequences in a multi-scale fashion. Recurrent visual attention model: Visual attention model aims to capture the property of human perception mechanism by identifying the interesting regions in the images. Saliency detection [7] typically predicts the human eye movement and fixations for a given image. In [21], a recurrent is proposed to understand where the model focuses on image caption generations. Moreover, recurrent attention model has been applied to other sequence modeling such as machine translation [12] and image generation[6].

3

The Proposed Method

The overall architecture of HAN is shown in 1. We describe the three major components of HAN in this section ­ the input appearance and motion CNN feature extraction, temporal sequence modeling and hierarchical attention model. 3.1 Appearance and Motion Feature Extraction

In general, HAN can adopt any deep convolution networks [8, 16, 20] for feature extraction. In this paper, we use two stream ConvNets [16] to extract both appearance and motion features. Specifically, we train a VGG net [17] to extract feature map fP and fQ for the t-th frame image P t and the corresponding optical flow image Qt : fP t = CN Nvgg (P t ) fQt = CN Nvgg (Qt ) (1)

Unlike two stream ConvNets [16] that employs the last fully connected layer as the input feature, we use the features for fP and fQ from the last convolutional layer after pooling, which contains the spatial information of the input appearance and motion images. The input appearance and motion images are first rescaled to 224 × 224 and the extracted feature maps from the last pooling layer have the dimension of D × K × K (512 × 14 × 14 used in VGG net). K × K is the number of regions in the input image and D is the number of the feature dimensions. Thus at each time step, we extract K 2 D dimension feature vectors for both appearance and motion images. We refer these feature vectors as feature cube shown in Figure 3. Then, the feature maps fP t and fQ can be denoted in matrix forms t t D ×K 2 t t D ×K 2 as Pt = [pt and Qt = [qt , respectively. 1 , p2 , . . . , pK 2 ]  R 1 , q2 , . . . , qK 2 ]  R 3.2 Recurrent Neural Network

Long-short term memory (LSTM), which has the ability to preserve sequence information over time and capture long-term dependencies, has become a very popular model for sequential modeling tasks such as speech recognition [5], machine translation [1] and program execution [24]. Recent advances in computer vision also suggest that LSTM has potentials to model videos for action recognition [15]. We follow the LSTM implementation in [25], which is given as follows 3

Figure 2: Illustration of One LSTM Cell

it ft ot gt ct ht

=  (Wix xt + Wih ht-1 + bi ) =  (Wf x xt + Wf h ht-1 + bf ) =  (Wox xt + Woh ht-1 + b + o) = tanh(Wgx xt + Wgh ht-1 + bg ) = ft ct-1 + it gt = ot tanh(ct )

(2)

where it is the input gate, ft is the forget gate, ot is the forget fate, ct is the memory cell state at t and xt is the input features at t.  (·) means the sigmoid function and denotes the Hadmard product. The main idea of the LSTM model is the memory cell ct , which records the history of the inputs observed up to t. ct is a summation of ­ (1) the previous memory cell ct-1 modulated by a sigmoid gate ft , and (2) gt , a function of previous hidden states and the current input modulated by another sigmoid gate it . The sigmoid gate ft is to selectively forget its previous memory while it is to selectively accept the current input. it is the gate controlling the output. The illustration of a cell of LSTM at the time step t is shown in Figure 2. Next we will introduce how to model both appearance and motion features using LSTM and how to integrate attention by using encoded features. 3.3 Hierarchical LSTMs to Capture Temporal Structures

One natural way of modeling videos is to feed features Pt and Qt into two LSTMs and then put a classifier at the output of LSTMs for classification. However, this straightforward method doesn't fully utilize the structure of actions. In real world, an action is usually composed of a set of subactions, which means that video temporal structures are intrinsically layered. For example, a video about long jump consists of three sub-actions ­ pushing off the board, flying over the pit and landing. As the three actions take place sequentially, there are strong temporal dependencies among them thus we need to appropriately model the temporal structure among the three actions. In the meantime, the temporal structure within each action is composed of multiple actions. For example, pushing off the board is composed of running and jump. In other words, the actions we want to recognize are layered and we need to model video temporal structure with multiple granularities. However, directly applying LSTM cannot capture this property. To fully capture the video temporal structure, we develop a hierarchical LSTM. An illustration of the hierarchical LSTM is shown in the purple rectangle in Figure 1. This hierarchical LSTM is composed of two layers ­ the first layer accepts the appearance feature of each frame as the input and the output of the first layer LSTM is used as the input of the second layer LSTM. To capture the dependencies between different sub-actions such as dependencies between pushing off the board, flying over the pit and landing, we skip every k 4

encoded features from LSTM and use that as the input to the second layer. In addition to capturing the video temporal structure, another advantage of layered LSTM is to increase the learning capability of LSTM. By adding another layer in LSTM, we allow LSTM to learn higher level and more complex features, which is a common practice proven to work well in other deep architectures such as CNN, DNN and DBM. Thus, as shown in Figure 1, we use two hierarchical LSTMs to model the appearance and motion features, respectively. 3.4 Attention Model to Capture Spatial Structures

Figure 3: attention The K 2 vectors in the appearance Pt (or motion features Qt ) correspond to K 2 regions in the t-th frame, which essentially encode the spatial structures. For action recognition, not every region of the frames are relevant for the task at hand. Obviously, we want to focus on the regions where the action is happening. For the action shown in Figure 3, we want to mainly focus on hands and legs that are useful for identifying the action; while the background is noisy as a person can perform the same action at different locations. Therefore we could confuse the classifier if we also target on backgrounds. Thus, it is natural for us to assign different attention weights to different regions of the frame. Since video frames are sequential, neighboring frames have strong dependencies, which suggests that we can use the encoded features at time t - 1 to predict the attention weights at time t and then use the attention weights to refine the input. Specifically, at each time step t - 1, we use a softmax function over K × K locations to predict the importance of the K 2 locations in the frame, which is written as: T exp(wi ht-1 ) t li = K2 (3) T j =1 exp(wj ht-1 )
t where li is the importance weight of the i-th region of the t-th frame, W = {w1 , w2 , . . . , wK 2 }  1 q,1 2D ×K 2 R are the weights of the softmax function and ht-1 is the concatenation of hp, t-1 and ht-1 , i.e., the encoded appearance and motion features of the (t - 1)-th frame from the first layer LSTM. Note that we use the encoded appearance feature and motion feature jointly to compute the attention weights instead of computing two attention weights by using the two features, separately. Its advantages are two fold. First, the flow and appearance features capture different aspects of the frame but the

5

attention location on the video should be the same, thus we do not need to calculate two sets of attentions for appearance and optical LSTM separately, which may introduce more computational cost. Second, appearance and motion features provide complimentary information that may help predict more accurate attention. With the attention weights given above, the inputs of the two LSTMs are the weighted average of different locations as:
K2 K2 t t li pi and xq t = i=1 i=1 t t li qi

xp t = 3.5 Action Recognition with HAN

(4)

We use hp,i T to denote the encoding of the the video by the i-the layer LSTM for the appearance features and hq,i T for motion features. As mentioned above, the hierarchical LSTM captures multigranularity of video temporal structures, thus, encoded features in different levels (different i) provide distinct descriptions of different granularity about actions, which are all useful for action recognition. In addition, the two LSTMs encode complementary information from appearance and motion features, thus encoded features from appearance and motion are also relevant for action recognition. Therefore, 1 p,L q,L q,L we concatenate these features as hf = [hp, T , . . . , hT , hT , . . . , hT ], where L is the number of layers in each LSTM. We then use the softmax function to predict the probability that the video vi is classified into the class c as p(c|vi , HAN) = softmax(HAN(vi )) and the loss function is
HAN,Ws N

(5)

max

log p(yi |vi , HAN)
i=1

(6)

where N is number of videos, yi is the label of vi and Ws are the weights of the softmax classifier.

4

Experiments

In this section, we first present the details of datasets and the evaluation protocol. Then, we describe the details of the implementation of our method. Finally, we present the experimental results with discussions. 4.1 Datasets and evaluation protocol

The evaluation is conducted on two public benchmark datasets, i.e., UCF-101 [18] and HMDB51[11]. These two datasets are among the largest available annotated video action recognition datasets that have been used in [10, 15, 16, 19, 20]. Specifically, UCF-101 contains 13, 000 videos annotated into 101 action classes with each class having at least 100 videos. HMDB51 is composed of 6, 700 videos from 51 action categories and each category has at least 100 video clips. For both datasets, the evaluation protocol is the same ­ we follow the train/test splits provided by the corresponding organizers. The performance is measured by the mean of accuracies across all the splits in each dataset. 4.2 Experiments Setting

Training two stream CNNs and HANs: Compared to image classification and detection, training a good deep convolutional neural network for videos understanding is more challenging. Similar to [20, 16], we use the training data in UCF 101 split to train two stream CNNs. In our implementation, we use the Caffe toolbox [9] and the layer configuration is the same as [17]. All hidden layers use the rectification activation functions and max pooling is performed over 2 × 2. Finally, each of the two networks contains 13 convolutional layers and 3 fully connected layers. The training procedure is similar to [20, 16], where we use mini-batch stochastic gradient descent with momentum (0.9). The learning rate is initially set to 10-2 , then changed to 10-3 after 10, 000 iterations and stopped after 30, 000 iterations and 10, 000 iterations for spatial and temporal nets, respectively. We use the Theano toolbox for HAN implementation and the model is trained by using Adadelta [26]. The 6

dimension of LSTM is 1, 024 and the batch size is fixed to 128 . Techniques of dropout [2] and BPTT are used. Optical flow: The optical flow is computed by the off-the-shelf OpenCV toolbox with GPU implementation of [23]. Since the computational cost of optical flow is the bottleneck for the two stream CNN training. We pre-computed all the optical flow images and stored the horizontal and vertical components. The optical flow is computed by the adjacent two frames. In the testing stage, we fix the number of frames with the equal temporal window between them. 4.3 Results and Analysis

We compare our models with a set of baselines proposed recently [15, 22, 16, 20, 10, 13] including shallow video representation methods and deep ConvNets methods. We first evaluate our proposed Table 1: Average accuracy over three splits on UCF-101 and HMDB51 Model Full HAN (spatial CNN cube+temporal CNN cube) HAN without attention1 (spatial CNN cube +temporal CNN cube) HAN without attention2 (spatial CNN 4096+ temporal CNN 4096) Spatial HAN (spatial CNN cube) Temporal HAN (temporal CNN cube) UCF-101 92.7% 90.6% 91.1% 75.1% 85.4% HMDB51 64.3% 62.0% 62.7% 47.7% 58.3%

HAN on UCF-101 and HMDB51 datasets by comparing HAN with different settings to show the importance of each key component in HAN in Table 1. Then, we further compare HAN with state-of-the art methods and experimental results are reported in Table 2. From the tables, we can make the following observations: · The proposed method with hierarchical LSTM outperforms methods without hierarchical structures [22, 10, 16]. These results support that (1) the usage of LSTM can capture video sequences by considering the order of the motion transitions; and (2) the proposed hierarchical structure can effectively model the complex and long time range actions in videos. · Compared with methods without the attention components, the proposed HAN encourages the model to focus on the important regions in frames during the learning process, which improves the discriminative ability for classification. For example, in Figure 4 (b) and Figure 4 (e), we can see that our model can learn the important regions for actions more accurately. · The temporal and spatial features are complementary. First, by combining them together, both of them have been improved significantly. Second, compared with [15] that only considers attention in spatial, HAN can predict more motion related regions in the videos. Third, compared to TDD, the proposed HAN achieves comparable results without considering the iDT information, which suggests that the learned attention regions can have the similar ability to dense trajectory points and reduce the negative impact of background noises. · Compared to state-of-the art methods on UCF and HMDB51, HAN outperforms them remarkably except [20]. The major reason for the exception is that the dataset HMDB is relatively small and the content is unconstrained, while the method in [20] incorporates iDT features that are computationally expensive.

5

Conclusion and Future Work

In this paper, we propose a hybrid deep framework by incorporating a hierarchical structure and joint attention model to the two stream convnet approach for human action recognition. The experimental results suggest that the proposed framework outperforms the two stream convnet approach. Despite using the only optical flow images as input, HAN achieves comparable performance with the state-ofthe art method TDD that is much more computationally expensive. These results further support that (1) the hierarchical structure in HAN is important because it can model the frame transitions as well 7

(a) The sampled frame

(b) Attention results from HAN (c) Attention results from [15]

(d) The sampled frame

(e) Attention results from HAN (f) Attention results from [15]

Figure 4: Visual attention comparison between HAN and soft attention model in [15], the green and red circles highlight the most important region learned by HAN and [15] respectively. Table 2: Comparison with state of the art methods on UCF101 and HMDB51. Model Histogram of Oriented Gradient Improved dense trajectories (iDT) [19] iDT + Stack Fish Vector [13] spatial-temporal CNN [10] two stream CNN [16] two stream CNN+LSTM [22] two stream CNN + iDT [20] Soft Attention +LSTM [15] Hierarchical Attention Networks UCF-101 72.4 % 85.9% N/A 65.4% 88.0% 88.6% 91.5% 84.96% 92.7% HMDB51 40.2% 57.2% 66.8% N/A 59.4% N/A 65.9% 41.3% 64.3%

as long video segments and (2) the joint visual attention can help HAN focus on the important video regions and reduce the effect of noisy background. HAN is powerful in sequence modeling thus we would like to explore more applications for HAN in the future such as video event detection since a video event usually contains many sub-events and these sub-events have high dependencies to each. References [1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR, 2015. [2] George E Dahl, Tara N Sainath, and Geoffrey E Hinton. Improving deep neural networks for lvcsr using rectified linear units and dropout. In ICASSP, pages 8609­8613. IEEE, 2013. [3] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In CVPR. IEEE, 2005. [4] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, pages 2625­2634, 2015. 8

[5] Alan Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech recognition with deep bidirectional lstm. In ASRU Workshop, pages 273­278. IEEE, 2013. [6] Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. Draw: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015. [7] Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid scene analysis. TPAMI, (11):1254­1259, 1998. [8] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action recognition. TPAMI, 35(1):221­231, 2013. [9] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Multimedia, pages 675­678. ACM, 2014. [10] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In CVPR, 2014. [11] Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In ICCV. IEEE, 2011. [12] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015. [13] Xiaojiang Peng, Changqing Zou, Yu Qiao, and Qiang Peng. Action recognition with stacked fisher vectors. In Computer Vision­ECCV 2014, pages 581­595. Springer, 2014. [14] Christian Schüldt, Ivan Laptev, and Barbara Caputo. Recognizing human actions: a local svm approach. In ICPR, volume 3, pages 32­36. IEEE, 2004. [15] Shikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Action recognition using visual attention. In Proceedings Workshop of ICLR, 2015. [16] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In NIPS, pages 568­576, 2014. [17] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [18] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [19] Heng Wang and Cordelia Schmid. Action recognition with improved trajectories. In CVPR, pages 3551­3558, 2013. [20] Limin Wang, Yu Qiao, and Xiaoou Tang. Action recognition with trajectory-pooled deepconvolutional descriptors. In CVPR, pages 4305­4314, 2015. [21] Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015. [22] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, and George Toderici. Beyond short snippets: Deep networks for video classification. In CVPR, pages 4694­4702, 2015. [23] Christopher Zach, Thomas Pock, and Horst Bischof. A duality based approach for realtime tv-l 1 optical flow. In Pattern Recognition, pages 214­223. Springer, 2007. [24] Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014. [25] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014. [26] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. 9

Unsupervised Sentiment Analysis for Social Media Images
Yilin Wang, Suhang Wang, Jiliang Tang, Huan Liu, and Baoxin Li Arizona State University Tempe, Arizona {yilin.wang.1, suhang.wang, jiliang.tang, huan.liu, baoxin.li}@asu.edu

Abstract
Recently text-based sentiment prediction has been extensively studied, while image-centric sentiment analysis receives much less attention. In this paper, we study the problem of understanding human sentiments from large-scale social media images, considering both visual content and contextual information, such as comments on the images, captions, etc. The challenge of this problem lies in the "semantic gap" between low-level visual features and higher-level image sentiments. Moreover, the lack of proper annotations/labels in the majority of social media images presents another challenge. To address these two challenges, we propose a novel Unsupervised SEntiment Analysis (USEA) framework for social media images. Our approach exploits relations among visual content and relevant contextual information to bridge the "semantic gap" in the prediction of image sentiments. With experiments on two large-scale datasets, we show that the proposed method is effective in addressing the two challenges.

1

Introduction

Recent years have witnessed the explosive popularity of image-sharing services such as Flickr1 and Instagram2 . For example, as of 2013, 87 millions of users have registered with Flickr3 . Also, it was estimated that about 20 billion Instagram photos are shared to 20144 . Since by sharing photos, users could also express opinions or sentiments, social media images provide a potentially rich source for understanding public opinions/sentiments. Such an understanding may in turn benefit or even enable many real-world applications such as advertisement, recommendation, marketing and health-care. The importance of sentiment analysis for social media images has thus attracted increasing attention recently [Yang et al., 2014; You et al., 2015].
www.flickr.com www.instagram.com 3 http://en.wikipedia.org/wiki/Flickr 4 http://blog.instagram.com/post/ 80721172292/200m
2 1

Current methods of sentiment analysis for social media images include low-level visual feature based approaches [Jia et al., 2012; Yang et al., 2014], mid-level visual feature based approaches [Borth et al., 2013; Yuan et al., 2013] and deep learning based approaches [You et al., 2015]. The vast majority of existing methods are supervised, relying on labeled images to train sentiment classifiers. Unfortunately, sentiment labels are in general unavailable for social media images, and it is too labor- and time-intensive to obtain labeled sets large enough for robust training. In order to utilize the vast amount of unlabeled social media images, an unsupervised approach would be much more desirable. This paper studies unsupervised sentiment analysis. Typically, visual features such as color histogram, brightness, the presence of objects and visual attributes lack the level of semantic meanings required by sentiment prediction. In supervised case, label information could be directly utilized to build the connection between the visual features and the sentiment labels. Thus, unsupervised sentiment analysis for social media images is inherently more challenging than its supervised counterpart. As images from social media sources are often accompanied by textual information, intuitively such information may be employed. However, textual information accompanying images is often incomplete (e.g., scarce tags) and noisy (e.g., irrelevant comments), and thus often inadequate to support independent sentiment analysis [Hu and Liu, 2004; Hu et al., 2013b]. On the other hand, such information can provide much-needed additional semantic information about the underlying images, which may be exploited to enable unsupervised sentiment analysis. How to achieve this is the objective of our approach. In this paper, we study unsupervised sentiment analysis for social media images with textual information by investigating two related challenges: (1) how to model the interaction between images and textual information systematically so as to support sentiment prediction using both sources of information, and (2) how to use textual information to enable unsupervised sentiment analysis for social media images. In addressing these two challenges, we propose a novel Unsupervised SEntiment Analysis (USEA) framework, which performs sentiment analysis for social media images in an unsupervised fashion. Figure 1 schematically illustrates the difference between the proposed unsupervised method and existing supervised methods. Supervised methods use label informa-

tively. Let C = {c1 , c2 , . . . , ck } be the set of sentiment labels. Note that in this work we only consider positive, neutral and negative sentiments with k = 3 but the generalization of the proposed framework to multi-class sentiment analysis is straightforward. With the aforementioned notations/definitions, the problem of unsupervised sentiment analysis for social media images with textual information is formally defined as: Given n images with visual information Xv and textual information Xt , to predict sentiment labels in C for the given n images.
(a) Supervised Sentiment Analysis.

3

Unsupervised Sentiment Analysis for Social Media Images

In this section, we first present our method for exploiting text information and then introduce the unsupervised sentiment analysis framework with an optimization method.

3.1

Exploiting Textual Information

(b) The Proposed Unsupervised Sentiment Analysis.

Figure 1: Sentiment Analysis for Social Media Images. tion to learn a sentiment classifier; while the proposed method does not assume the availability of label information but employ auxiliary textual information. Our main contribution can be summarized as below: · A principled approach to enable unsupervised sentiment analysis for social media images. · A novel unsupervised sentiment analysis framework USEA for social media images, which captures visual and textual information into a unifying model. To our best knowledge, USEA is the first unsupervised sentiment analysis framework for social media images; and · Comparative studies and evaluations using datasets from real-world social media image-sharing sites, documenting the performance of USEA and leading existing methods, serving as benchmark for further exploration.

Without label information, it is challenging for unsupervised sentiment analysis to connect visual features with sentiment labels. Textual information associated with social media images may be exploited to help, as it provides semantics about the underly images and in particular rich sentiment signals such as sentiment words and emotion symbols may be found in the textual fields. Hence, to exploit textual information, we investigate (1) how to incorporate textual information into visual information; and (2) how to model sentiment signals in textual information. Since visual and textual information are two views about the same set of images, it is reasonable to assume that they share the same sentiment label space. More specifically, the sentiment of Ii should be consistent with that of its associated textual information pi . Let U0  Rn×k be the sentiment label space where U0 (i, j ) = 1 if the i-th data instance belongs to cj , and U0 (i, j ) = 0 otherwise. We propose the following formulation to incorporate visual information with textual information based on nonnegative matrix factorization:
T min Xv - Uv Vv UV 2 F 2 T +  X t - Ut V t 2 2 F

2

Problem Statement

In this paper, scalars are denoted by lower-case letters (a, b, . . . ; ,  , . . .), vectors are written as lower-case bolded letters (a, b, . . .), and matrices correspond to boldfaced uppercase letters (A, B, . . .). Let I = {I1 , I2 , . . . , In } be the set of images where n is the number of images. We use P = {p1 , p2 , . . . , pn } to denote associated textual information about images where pi is the textual information about Ii . Let Fv be set of mv visual features and Ft be set of mt textual features. We use Xv  Rn×mv and Xt  Rn×mt to denote visual and textual information about images, respec-

+  ( U v - U 0 F + Ut - U0 F ) subject to Uv  0, Ut  0; ||U0 (i, :)||0 = 1, i  {1, 2, ..n} U0 (i, j )  {0, 1} j  {1, 2, ..k } (1) where  controls how textual information contributes to the model and ||·||0 is 0 , which counts the number of nonzero entries in the vector. Uv  Rn×k and Ut  Rn×k are the sentiment label spaces learned from visual information and tex2 tual information, respectively. The term of  ( Uv - U0 F + 2 Ut - U0 F ) ensures that these two types of information should share the sentiment label space U0 . Vv  Rmv ×k and Vt  Rmt ×k indicate the sentiment polarities of visual and textual features, respectively. Textual information contains rich sentiment signals. First, some words may contain sentiment polarities. For example,

some words are positive such as "happy" and "terrific"; while others are negative such as "gloomy" and "disappointed". The sentiment polarities of words can be obtained via some public sentiment lexicons. For example, the sentiment lexicon MPQA contains 7,504 human labeled words which are commonly used in the daily life with 2,721 positive words and 4,783 negative words. Second, some abbreviations and emoticons are strong sentiment indicators. For example, "lol"(means laughing out loud) is a positive indicator while ":(" is a negative indicator. Let Vt0  Rmv ×k be the matrix coding sentiment signals in textual information where Vt0 (i, j ) = 1 if i-th word belongs to cj and Vt0 (i, j ) = 0 otherwise. To model sentiment signals, we force the learned sentiment polarities of textual features to be consistent with those indicated by sentiment signals. Furthermore, not all textual features in Ft contain sentiment polarities and Vt should be sparse. We propose the following formulation to achieve these two goals as: min Vt - Vt0
2,1

3.3

An Optimization Method

There are 5 components, i.e. Uv , Vv , Ut , Vt and U0 , in Eq. (4). Thus it is difficult to optimize all the components simultaneously. In the following parts, we demonstrate an alternating algorithm to optimize the objective function by updating each component iteratively. Update Vt : If U0 , Uv , Vv and Ut are fixed, then the objective function is decoupled and the constrains are independent of Vt . Thus we can optimize Vt separately and ignore the term without Vt , leading to the following:
T minJ (Vt ) = Xt - Ut Vt Vv 2 F

+  V t - V t0

2 F

(5)

 where  =  . Taking the derivation of J (Vt ) and setting it to zero, we can obtain the following form: T (-XT (6) t Ut + Vt Ut Ut ) +  Dt (Vt - Vt0 ) = 0 where Dt is a diagonal matrix with j th element on the di1 agonal D(j, j ) = 2 Vt (j,:)- Vt 0(j,:) . In Eq. (6), solving

(2)

X 2,1 is the 2,1 of the matrix X, which ensures the row sparsity of X [Nie et al., 2010]. The significance of textual information in unsupervised sentiment analysis for social media images is two-fold. First, textual information bridges the semantic gap between visual features and sentiment labels. Second, we are allowed to do sentiment analysis for social media images in an unsupervised scenarios by modeling textual information via Eqs. (1) and (2).

Vt directly is intractable. Since Dt and UT t Ut are symmetric and positive definite, we employ eigen decomposition for them as: T UT t U t = U1  1 U1 (7) D t = U2  2 UT 2 where U1 , U2 are eigen vectors and 1 , 2 are diagonal matrices with eigen values on the diagonal. Substituting UT t Ut and Dt in Eq. (6), we have:
T T Vt U1 1 UT (8) 1 +  U2 2 U2 Vt = Xt Ut +  Dt Vt0 T Multiplying U2 and U1 from left to right on both sides: T T UT 2 Vt U1 1 +  2 U2 Vt U1 = U2 (Xt Ut +  Dt Vt0 )U1 (9) T T Let Vt = U2 Vt U1 and Q = U2 (Xt Ut +  Dt Vt0 )U1 , Eq. (9) becomes Vt 1 +  2 Vt = Q, then we can obtain the Vt and Vt as: Q(s, l) Vt (s, l) = s 2 + l (10) 1

2

3.2

The Framework: USEA

By combining the above discussion, we can have the following initial framework, which provides a potential solution to inferring sentiments by jointly considering visual information and corresponding contextual information:
T min Xv - Uv Vv UV 2 F 2 T +  X t - Ut V t 2 2 F

s.t.

+  ( Uv - U0 F + U t - U0 F ) (3) +  Vt - Vt0 2,1 Uv  0; Ut  0, ||U0 (i, :)||0 = 1, i  {1, 2, ..n} U0 (i, j )  {0, 1} j  {1, 2, ..k }

V t = U 2 V t UT 1
l where s 2 is the s-th eigen value of Dt and 1 is l-th eigen value of UT U . The following theorem shows that the updatt t ing rule in Eq(10) can monotonically decrease the objective function J (Vt ). Theorem 1. The update rule in Eq. (10) can monotonically decrease the value of J (Vt ) Proof. The proof is similar to that in [Nie et al., 2010], due to space limit, we omit the details of the proof. Update Vv . If U0 , Ut , Vt and Uv are fixed, by setting the derivation of the objective function to zero, Vv can be T -1 easily obtained as Vv = XT . Moreover, we v Uv (Uv Uv ) can easily verify updating Vv will monotonically decrease the objective function. Update Uv : If Vv , Ut , Vt and U0 are fixed, Uv can be obtained by the following optimization problem: T minJ (Uv ) = Xv - Uv Vv Uv 2 F

The parameter  controls the sparsity of regularization term. However, the constrains of U0 in Eq. (3), mixed vector zero norm with integer programming, make the problem difficult to solve. To tackle this problem, we consider the relaxation of U0 by adding the extra orthogonal constraint on the value of U0 . With the relaxation, the proposed framework (USEA) is to solve the following optimization problem:
T min Xv - Uv Vv UV 2 F 2 T +  X t - Ut V t 2 F) 2 F

s.t.

+  ( Uv - U0 F + Ut - U0 +  Vt - Vt0 2,1 Uv  0, Ut  0, UT 0 U0 = I ; U0  0

(4)

+  Uv - U0

2 F

(11)

s.t.

Uv  0

The Lagrangian function of Eq. (11) is :
T minL(Uv ) = Xv - Uv Vv Uv 2 F

+  Uv - U0

2 F

(12)

- T r(Uv ) where  is Lagrangian multiplier. Taking the deviation of J (Uv ) and using the KKT condition ((s, l)Uv (s, l) = 0), we can obtain:
T (-Xv Vv + Uv Vv Vv +  Uv -  U0 )sl (Uv )sl = 0 (13)

Update Ut : It is worth noting that the procedure of solving Ut is exactly the same as that of Uv . Thus, we omit the solution of Ut here. Update U0 : With Uv , Ut , Vt and Vv fixed, the sentiment label U0 can be obtained by solving the following optimization problem: minJ (U0 ) = Uv - U0
U 2 F

+ Ut - U0

2 F

s.t. UT 0 U0 = I ; U0  0 The Lagrangian function of Eq. (17) is: minJ (U0 ) = Uv - U0
U 2 F

(17)

which leads to the following update rule for Uv :
T V )- +  U ) ((Xv Vv )+ + Uv (Vv v 0 sl - T ((Xv Vv ) + Uv (Vv Vv )+ +  Uv )sl (14) where (X(s, l))+ = (|X(s, l)| + X(s, l))/2, (X(s, l))- = (|X(s, l)| - X(s, l))/2 and X = X+ - X- . Theorem 2. Let

(Uv )sl  (Uv )sl

+ Ut - U0

2 F

+ T r((UT 0 U0 - I )) - T r (U0 )

(18)

where  and  are Lagrangian multipliers. Taking the derivation of J (U0 ) and using KKT conditions we can obtain (U0 - Uv + U0 - Ut + U0 )sl (U0 )sl = 0 which leads the following update rule for U0 : (Uv + Ut + (U0 )- )sl ((U0 )+ + 2U0 )sl (19)

T T H (Uv ) = T r(-2Xv Vv UT v + U v V v V v Uv ) T + T r(-2UT v U0 + Uv U v )

h(Uv , Uv ) =
sl

((Xv Vv )- (s, l) Uv (s, l)U2 v (s, l) Uv (s, l)

2 U2 v (s, l) + Uv (s, l)

Uv (s, l)

(U0 )sl  (U0 )sl

(20)

+

T + ( Vv Vv )+ (s, l)

Uv (s, l)U2 v (s, l) Uv (s, l)

) Uv (s, l) Uv (s, l) )) )

-
sl

(2(Xv Vv )+ Uv (s, l)(1 + log

+ 2 U0 (s, l)Uv (s, l)(1 + log -
k,s,l

Uv (s, l) Uv (s, l)

T (Vv Vv )- (s, l)Uv (k, s)Uv (k, l)

(1 + log

Uv (k, s)Uv (k, l) Uv (k, s)Uv (k, l)

)

Note that updating U0 needs updating the Lagrangian multiplier  as well. To obtain , we sum over s and get T (s, s) = (UT 0 Uv - I + U0 Ut - I )s,s . The offdiagonal elements of  are approximately obtained from non-negative T value of U0 , leading to (s, t) = (UT 0 Uv - I + U0 Ut - I )st . Overall, we can obtain  by combining the diagonal values and off-diagonal values. With the update rules for all the components in the proposed model, we summarize the solution in Algorithm 1. The convergence of Algorithm 1 is demonstrated as below: Theorem 4.With Algorithm 1, the objective function Eq. (4) will converge. Proof From Theorem 1 and Theorem 2, the object function monotonically decreases:
0 1 0 1 1 2 1 J (Vv , U0 v )  J (Vv , Uv )  J (Vv , Uv )J (Vv , Uv )...  0 (21) Similarly, we can have the inequality chain for J (Vt , Ut ). Thus we complete the proof.

(15) The auxiliary function h(Uv Uv ) of H (Uv ) is convex and the global minimum of h(Uv , Uv )is:
v v v v 0 sl v (Uv )sl  (Uv )sl ((Xv T V )+ + U ) Vv )- +Uv (Vv v v sl Proof : The proof is similar to [Ding et al., 2006] and [Ding et al., 2010], due to space limit, we omit the details. Theorem 3. Updating Uv in Eq. (14) will monotonically decrease the value of objective function J (Uv ) Proof : H (Uv ) is the KKT condition of the Lagrangian function for Eq. (11). Based on the definition of auxiliary function and Theorem 2 we can obtain the following equations:

((X V )+ +U (VT V )- + U )

4

Experiments

In this section, we conduct experiments to answer the following questions - (1) can the proposed framework do sentiment analysis in an unsupervised scenario? and (2) how does the textual information affect the performance of the proposed framework? We begin by giving details about the experimental settings.

0 0 0 1 1 1 1 4.1 Experiment Settings H ( U0 v ) = h(Uv , Uv )  h(Uv , Uv )  h(Uv , Uv )  H (Uv )... (16) We collect datasets from Flickr and Instagram for this study This shows the update rule will monotonically decrease the and we give more details below, objective function H (Uv ), which complete the proof.

Algorithm 1 The proposed USEA Input: {Xv , Xt , Vt0 } , ,  Output: k sentiment label for each data instance. Initialization: Ut , Uv , Vv , Vt while Not Converge do Update Vt using Eq.(10) and compute Vv = T -1 XT U . v ( Uv Uv ) v T Computing (Xv Vv )+,- , (Xt Vt )+,- , (Vv Vv )+,- T +,- and (Vt Vt ) Update Uv using Eq. (14), similarly update Ut Computing  Update U0 End Using max-pooling for U0 to predict sentiment labels. Flickr: On Flickr, an image-hosting Website, users can provide tags and descriptions for each uploaded image. Thus the textual information could be comments, image caption, user profile and tags. The collection of Flickr dataset is based on the image id provided by [Yang et al., 2014], which contains 350,4192 images from 4807 users. Some images are unavailable when we crawled the data; hence we limit the number of images from one user as 50, which leads to a dataset with 140,221 images from 4341 users. Instagram: Instagram is a service supporting photosharing via mobile app, where users take pictures and share them on social networking platforms like Facebook and Twitter. Similar to Flickr, we crawl at most 50 images for each user and get totally 131,224 images from 4853 users. Although the textual information as same as that on Flickr, for some images the number of comments is much bigger than that in Flickr, e.g., the images from celebrities usually contain thousands of comments, and we only consider the latest 50 comments for each image in Instagram. Establishing Ground Truth: For evaluation purpose, we need to create sentiment labels of images. We follow the scheme in [Yang et al., 2014; Liu, 2012] and create labels for images via images' tags. Since we use tags to create labels of images, we do not consider tag information as textual information in the proposed framework. Labeling each post solely relying on tags may cause noise in the ground truth. Therefore we additionally select 20000 images from Flickr and ask three human subjects to manually create labels for them. Feature extraction: the proposed method has the ability to incorporate visual and textual information. For visual information, we follow the recent approaches [Yuan et al., 2013; Borth et al., 2013] by using mid-level visual features. The visual features are extracted by a large-scale visual attribute detectors [Borth et al., 2013] and the feature dimension is 1200. Text-based features are formed by the term frequency in user profiles, image captions and comments. It is worth noting that textual features, which contain user descriptions, friends' comments and image captions, are preprocessed by stop word removing and stemming. MPQA5 lexicon is employed as sentiment signals.
5

The proposed framework USEA is compared with the following sentiment analysis algorithms: · Senti API:6 . This API is natural language processing API that performs unsupervised sentiment prediction using word-based sentiment. The method only uses textual information. · Sentibank: As a mid-level visual feature based sentiment analysis approach, it uses large-scale visual attribute detectors and low-level visual features to form the Adjective and Nouns visual sentiment description pairs [Borth et al., 2013]. · EL: A topical graphical model based sentiment analysis approach, which models the sentiment by low-level visual features and friends information [Yang et al., 2014]. · USEA-T: A variant of the proposed method that only considers the textual information including user profiles, image captions and friends' comments. · Random: It predicts sentiment labels of images by randomly guessing. Noting that SentiBank [Borth et al., 2013] and EL[Yang et al., 2014] are originally proposed for supervised sentiment analysis. We extend them to unsupervised scenarios by replacing original classifiers such as SVM or logistic regression with K-means. However, the clusters identified by K-means have no sentiment labels and we determine their sentiment labels with the Euclidean distance to the ground truth. We use SentiBank-K, and EL-K to represent these modifications.

4.2

Performance Evaluation

Table 1 lists the comparison results and we make several key observations: · Most of the time, textual based approaches obtain slight better performance than Random. These results support - (1) textual information is often incomplete and noisy and thus often inadequate to support independent sentiment analysis; and (2) textual information contains important cues for sentiment analysis. · The proposed framework often obtains better performance than baseline methods. There are two major reasons. First textual information provides semantic meanings and sentiment signals for images. Second we combine visual and textual information for sentiment analysis. The impact of textual information on the proposed framework will be discussed in the following subsection. In summary, compared to the performance Random, the proposed framework can significantly improve the sentiment analysis performance in a unsupervised scenario.

4.3

Impact of Textual Information

We introduce two parameters  and  to control contributions from textual information. In this subsection, we investigate the impact of textual information on the proposed framework by examining how the performance of USEA varies with the changes of these parameters.
6

http://mpqa.cs.pitt.edu/

http://sentistrength.wlv.ac.uk/

Table 1: The comparison results of different methods for sentiment analysis. Method Senti API SentiBank-K EL-K USEA-T USEA Random Flickr (#20,000) 32.30% 41.32% 36.39% 37.90% 55.22% 32.81% Flickr (#140,221) 34.15% 41.12% 42.90% 40.22% 56.18% 33.12% Instagram (#131,224) 37.80% 46.31% 43.21% 36.41% 59.94% 33.05%

To study the impact of , we fix  = 0.7 and vary the value of  as {0.001, 0.1, 0.2, 0.3, 0.5, 0.7, 1.5, 2, 10}. The performance variance of USEA w.r.t.  is demonstrated in Figure 2. Note that we only show results in Flickr with manual labels since we have similar observations for other datasets. In general, with the increase of , the performance first increases greatly, reach its peak value and then decrease dramatically. When we increase  from 0.001 to 0.1, the performance increases from 43.21% to 48.07%, which suggests the importance of textual information. With larger values of  (> 1.5), textual information dominates the learning process and the learnt parameters may overfit.

Figure 3: Performance Variance w.r.t.  . Y axis is the accuracy performance and X axis is the value of  . Jia et al., 2012; Yuan et al., 2013], such as images from Twitter and Flickr. Social media are heterogeneous, containing visual and other types of information. Some of existing methods use mainly textual information. For example, [Hu et al., 2013b] proposes a method by counting the word frequency in the user description and predict the sentiment by measuring the word's sentiment. In [Yang et al., 2014], it was argued that friends' comments are more related to the user's sentiment. There are also methods that use solely visual information. For example, [Borth et al., 2013; Yuan et al., 2013; Chen et al., 2014] employ mid-level attributes to model visual content, [Jia et al., 2012] provides a method based on low-level visual features, and [Wang et al., 2015] uses a regulated matrix factorization approach. Inspired by the success of deep learning, [You et al., 2015; Xu et al., 2014] employ a convolution neural network architecture for visual sentiment analysis. However, as discussed previously, these approaches are largely supervised, which means their performance is linked to the assumed availability of a good training set with labels.

Figure 2: Performance variance w.r.t. . Y axis is the accuracy performance and X axis is the value of . Similarly, to study the impact of  , we fix  = 0.7 and vary the value of  as {0.1, 0.2, 0.3, ..., 0.9, 1}. The performance variance of USEA w.r.t.  is demonstrated in Figure 3. We also only show results in Flickr with manual labels since similar observations are made for other datasets. When  increases from 0.1 to 0.6, the performance increases a lot, which further supports the importance of sentiment signals from textual information. After 0.8, the increase of  will reduce the performance dramatically because the proposed framework may overfit to sentiment signals from textual information.

6

Conclusion

5

Related Work

Recently sentiment analysis have shown success in many aspects, e.g., social response to special events [Hu et al., 2013b; Fukuhara et al., 2007; Diakopoulos and Shamma, 2010], product reviews [Pang and Lee, 2008; Cui et al., 2006], and opinion mining [Liu, 2012; Hu et al., 2013a; Pang et al., 2002; Pak and Paroubek, 2010; Godbole et al., 2007]. Besides, there have been increasing interests in social media images [Borth et al., 2013; Yang et al., 2014;

In this paper, we propose a novel unsupervised sentiment analysis framework USEA by leveraging textual information and visual information in a unified model. Moreover, USEA provides a new viewpoint for us to better understand how textual information helps bridge the "semantic gap" between visual feature and image sentiment. Experiments on three large-scale datasets demonstrated 1) the advantages of the proposed methods in unsupervised sentiment analysis; and 2) the importance of textual information. In the future, we will exploit more social media sources, such as link information, user history, geo-location, etc., for sentiment analysis.

7

Acknowledgments

Yilin Wang and Baoxin Li are supported in part by National Science Foundation (NSF) under grant number #1135616. Suhang Wang and Huan Liu are supported by, or in part by, the National Science Foundation (NSF) under grant number #1217466 and the U.S. Army Research Office (ARO) under contract/grant number #025071. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies

References
[Borth et al., 2013] Damian Borth, Rongrong Ji, Tao Chen, Thomas Breuel, and Shih-Fu Chang. Large-scale visual sentiment ontology and detectors using adjective noun pairs. In Proceedings of the 21st ACM international conference on Multimedia, pages 223­232. ACM, 2013. [Chen et al., 2014] Tao Chen, Felix X Yu, Jiawei Chen, Yin Cui, Yan-Ying Chen, and Shih-Fu Chang. Object-based visual sentiment concept analysis and application. In Proceedings of the ACM International Conference on Multimedia, pages 367­376. ACM, 2014. [Cui et al., 2006] Hang Cui, Vibhu Mittal, and Mayur Datar. Comparative experiments on sentiment classification for online product reviews. In AAAI, volume 6, pages 1265­ 1270, 2006. [Diakopoulos and Shamma, 2010] Nicholas A Diakopoulos and David A Shamma. Characterizing debate performance via aggregated twitter sentiment. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 1195­1198. ACM, 2010. [Ding et al., 2006] Chris Ding, Tao Li, Wei Peng, and Haesun Park. Orthogonal nonnegative matrix t-factorizations for clustering. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 126­135. ACM, 2006. [Ding et al., 2010] Chris Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factorizations. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(1):45­55, 2010. [Fukuhara et al., 2007] Tomohiro Fukuhara, Hiroshi Nakagawa, and Toyoaki Nishida. Understanding sentiment of people from news articles: Temporal sentiment analysis of social events. In ICWSM, 2007. [Godbole et al., 2007] Namrata Godbole, Manja Srinivasaiah, and Steven Skiena. Large-scale sentiment analysis for news and blogs. ICWSM, 7:21, 2007. [Hu and Liu, 2004] Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168­177. ACM, 2004. [Hu et al., 2013a] Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu. Unsupervised sentiment analysis with emotional signals. In Proceedings of the 22nd international conference on World Wide Web, pages 607­618. International World Wide Web Conferences Steering Committee, 2013.

[Hu et al., 2013b] Yuheng Hu, Fei Wang, and Subbarao Kambhampati. Listening to the crowd: automated analysis of events via aggregated twitter sentiment. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pages 2640­2646. AAAI Press, 2013. [Jia et al., 2012] Jia Jia, Sen Wu, Xiaohui Wang, Peiyun Hu, Lianhong Cai, and Jie Tang. Can we understand van gogh's mood?: learning to infer affects from images in social networks. In Proceedings of the 20th ACM international conference on Multimedia, pages 857­860. ACM, 2012. [Liu, 2012] Bing Liu. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1­167, 2012. [Nie et al., 2010] Feiping Nie, Heng Huang, Xiao Cai, and Chris H Ding. Efficient and robust feature selection via joint 2, 1-norms minimization. In Advances in Neural Information Processing Systems, pages 1813­1821, 2010. [Pak and Paroubek, 2010] Alexander Pak and Patrick Paroubek. Twitter as a corpus for sentiment analysis and opinion mining. In LREC, volume 10, pages 1320­1326, 2010. [Pang and Lee, 2008] Bo Pang and Lillian Lee. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1­135, 2008. [Pang et al., 2002] Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79­86. Association for Computational Linguistics, 2002. [Wang et al., 2015] Yilin Wang, Yuheng Hu, Subbarao Kambhampati, and Baoxin. Li. Inferring sentiment from web images with joint inference on visual and social cues: A regulated matrix factorization approach. In ICWSM, page 21, 2015. [Xu et al., 2014] Can Xu, Suleyman Cetintas, Kuang-Chih Lee, and Li-Jia Li. Visual sentiment prediction with deep convolutional neural networks. arXiv preprint arXiv:1411.5731, 2014. [Yang et al., 2014] Yang Yang, Jia Jia, Shumei Zhang, Boya Wu, Juanzi Li, and Jie Tang. How do your friends on social media disclose your emotions? 2014. [You et al., 2015] Quanzeng You, Jiebo Luo, Hailin Jin, and Jianchao Yang. Robust image sentiment analysis using progressively trained and domain transferred deep networks. 2015. [Yuan et al., 2013] Jianbo Yuan, Sean Mcdonough, Quanzeng You, and Jiebo Luo. Sentribute: image sentiment analysis from a mid-level perspective. In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, page 10. ACM, 2013.

In this paper, we propose a real-time system using vehicle back-up camera to alert for potential back-up collisions. We developed a highly efficient algorithm, combining segmenting pedestrians and vehicles from moving background using local optical flow value, and a scale adaptive method using Deformable Part Model to detect objects at different distances. To test out algorithm, we created our own vehicle back-up dataset that contains rich scenes recorded from a back-up camera on moving/stationary vehicles with unique and challenging scenarios such as frequent occlusion with cluttered and moving background, and we made this dataset available to public for other researchers. Experiments on the dataset shows that our algorithm achieves high accuracy in near real-time, and it is about 10 times faster than the comparable state-of-the-art algorithm.
Personalized and content-adaptive image enhancement can find many applications in the age of social media and mobile computing. This paper presents a relative-learning-based approach, which, unlike previous methods, does not require matching original and enhanced images for training. This allows the use of massive online photo collections to train a ranking model for improved enhancement. We first propose a multi-level ranking model, which is learned from only relatively-labeled inputs that are automatically crawled. Then we design a novel parameter sampling scheme under this model to generate the desired enhancement parameters for a new image. For evaluation, we first verify the effectiveness and the generalization abilities of our approach, using images that have been enhanced/labeled by experts. Then we carry out subjective tests, which show that users prefer images enhanced by our approach over other existing methods.
Video-based coaching systems have seen increasing adoption in various applications including dance, sports, and surgery training. Most existing systems are either passive (for data capture only) or barely active (with limited automated feedback to a trainee). In this paper, we present a video-based skill coaching system for simulation-based surgical training by exploring a newly proposed problem of instructive video retrieval. By introducing attribute learning into video for high-level skill understanding, we aim at providing automated feedback and providing an instructive video, to which the trainees can refer for performance improvement. This is achieved by ensuring the feedback is weakness-specific, skill-superior and content-similar. A suite of techniques was integrated to build the coaching system with these features. In particular, algorithms were developed for action segmentation, video attribute learning, and attribute-based video retrieval. Experiments with realistic surgical videos demonstrate the feasibility of the proposed method and suggest areas for further improvement.
Color demosaicking is used to reconstruct full color images from incomplete color filter array samples captured by cameras with a single sensor array. In reconstructing natural-looking images, one key challenge is to model and respect the statistics of natural images. This paper presents a novel modeling strategy and an efficient color demosaicking algorithm. The approach starts with joint modeling of the color images, which supports simultaneous representation of inter-channel correlation and structural information in an image. The inter-channel correlation is explored by measuring the channel difference signals in the gradient domain, while the structural information is explored by nonlocal low-rank regularization. An efficient algorithm is then proposed to solve the joint formulation, by dividing the minimization problem into two sub-problems and solving them iteratively. The effectiveness of the proposed approach is demonstrated with extensive experiments on both noiseless and noisy datasets, with comparison with existing state-of-the-arts color demosaicking methods.
In this proposal, we study the problem of understanding human sentiments from large scale collection of Internet images based on both image features and contextual social network information (such as friend comments and user description). Despite the great strides in analyzing user sentiment based on text information, the analysis of sentiment behind the image content has largely been ignored. Thus, we extend the significant advances in text-based sentiment prediction tasks to the higher level challenge of predicting the underlying sentiments behind the images. We show that neither visual features nor the textual features are by themselves sufficient for accurate sentiment labeling. Thus, we provide a way of using both of them, and formulate sentiment prediction problem in two scenarios: supervised and unsupervised. We develop an optimization algorithm for finding a local-optima solution under the proposed framework. With experiments on two large-scale datasets, we show that the proposed method improves significantly over existing state-of-the-art methods. In the future, we are going to incorporating more information on the social network and explore sentiment on signed social network.
In this paper, we propose a real-time system using vehicle back-up camera to alert for potential back-up collisions. We developed a highly efficient algorithm, combining segmenting pedestrians and vehicles from moving background using local optical flow value, and a scale adaptive method using Deformable Part Model to detect objects at different distances. To test out algorithm, we created our own vehicle back-up dataset that contains rich scenes recorded from a back-up camera on moving/stationary vehicles with unique and challenging scenarios such as frequent occlusion with cluttered and moving background, and we made this dataset available to public for other researchers. Experiments on the dataset shows that our algorithm achieves high accuracy in near real-time, and it is about 10 times faster than the comparable state-of-the-art algorithm.
STRUCTURE-PRESERVING IMAGE QUALITY ASSESSMENT Yilin Wang1 Qiang Zhang2 Baoxin Li1
1

Department of Computer Science, Arizona State University, Tempe AZ 2 Advanced Image Research Lab, Samsung Electronic, Pasadena CA {ywang370,Baoxin.Li}@asu.edu q.zhang1@samsung.com
ABSTRACT approaches have employed Bag of Words [6, 7], and DCT transformation [8], etc. In general, existing approaches belong to one of the above three categories and only work for their respective scenario. One objective of this paper is to develop a unifying approach for both FR-IQA and NR-IQA, hence maximizing the applicability of the IQA model. In parallel with perceptual/subjective IQA, various objective measures have been employed in multimedia. The most widely-used one is Mean Square Error (MSE) or its variants, due to its simplicity and general effectiveness. MSE simply measures the average per-sample difference between two signals. Since MSE is convex and differentiable, it is easy to use optimization approaches for finding solutions to various models based on MSE. Unfortunately, it is well understood (e.g., [9]) that pixel-wise MSE (or its variants) is not a good measure for perceptual quality, primarily due to the fact that no structural information of the image is considered in computing MSE. Some attempts have been tried to remedy this. For example, in [10], a "perceptual-aware" MSE was proposed by adding Gaussian filter or gradient operator, which helped to improve the correlation between the human perceptual score and the objective metric. In this work, we aim at building a structure-preserving MSE (SPMSE) which not only retains the computational efficiency and nice mathematical properties of MSE but also leads to the development of effective IQA metrics. Our new formulation of MSE is developed by employing the `kernel trick': we use HOG feature [11], an effective and efficient descriptor for describing object structures, as an quality kernel between two images, and show that the resultant formulation leads to many of the desired properties. For FR-IQA, experimental results show that SPMSE performs statistically better than the well-known SSIM method and leads to very competitive performance compared with with other state-of-the-art methods on three benchmark datasets. Moreover, we show that the proposed SPMSE can be employed in recent Bag-of-Words based models for NR-IQA [7, 6]. In such existing models, the distortion image is represented by a feature vector which is the coefficients under a codebook. However, most existing approaches focus on designing hand-crafted features to be used by the codebook training, rather than optimal representation under the code-

Perceptual Image Quality Assessment (IQA) has many applications. Existing IQA approaches typically work only for one of three scenarios: full-reference, non-reference, or reduced-reference. Techniques that attempt to incorporate image structure information often rely on hand-crafted features, making them difficult to be extended to handle different scenarios. On the other hand, objective metrics like Mean Square Error (MSE), while being easy to compute, are often deemed ineffective for measuring perceptual quality. This paper presents a novel approach to perceptual quality assessment by developing an MSE-like metric, which enjoys the benefit of MSE in terms of inexpensive computation and universal applicability while allowing structural information of an image being taken into consideration. The latter was achieved through introducing structure-preserving kernelization into a MSE-like formulation. We show that the method can lead to competitive FR-IQA results. Further, by developing a feature coding scheme based on this formulation, we extend the model to improve the performance of NR-IQA methods. We report extensive experiments illustrating the results from both our FR-IQA and NR-IQA algorithms with comparison to existing state-of-the-art methods. Index Terms-- Mean Square Error, Image Quality Assessment, kernel method. 1. INTRODUCTION Perceptual image quality assessment (IQA) has many multimedia applications such as image denoising [1] and image transmission. Based on the degree of reliance on a reference image, IQA models can be divided into three categories: Full Reference IQA (FR-IQA), Reduced Reference IQA (RRIQA) and Non-Reference IQA (NR-IQA). FR-IQA needs a reference image for estimating the distortion of a target image. Numerous FR-IQA models have been proposed, including those that incorporate image structure information [2], mutual information [3, 4], and wavelet information [5], etc. For RR-IQA, only partial information of a reference image is needed, while NR-IQA models predict image quality without any information from reference images. Recent NR-IQA

book. In other words, the feature encoding step, which should contribute to the final quality metric significantly, has been largely ignored. In this work, we propose and empirically compare several coding schemes for NR-IQA based on the SPMSE framework, and show that, compared to vector quantization or sparse coding, the proposed method, structure preserving coding, is more effective for NR-IQA models. The rest of the paper is organized as follows: Section 2 reviews the related work. Section 3 describes SPMSE for FR-IQA in details and introduces our SPMSE encoding for NR-IQA. In Section 4, experimental results on widely-used datasets are reported; and finally in Section 5, conclusions are made and future improvements and issues are discussed. 2. RELATED WORK We review the related work on FR-IQA and recent advanced methods in NR-IQA. FR-IQA: One of the most widely-used and influential FR-IQA method is Structure SIMilarity Index (SSIM). It is based on the assumption that the underlying image quality score is highly related to the image structure. For a pair of a reference image s and a distortion image t, SSIM compares them with image luminance, contrast and structure as: (2µs µt +C1 )(2st +C 2) SSIM (s, t) = (µ 2 +µ2 +C 1)( 2 + 2 +C 2) , where, for image s s t t i, j , µi is the local mean intensity, i is the local variance and ij is the local covariance. Visual Information Fidelity (VIF) is another IQA approach that captures the signal statistics for image fidelity assessment. In [12], it was argued that the HSV space is appropriate for full-reference image quality assessment, owing to distinctive features of high-quality and low-quality images in this space. In [13], the author provides a gradient similarity method for image quality assessment. For a thorough survey of modern IQA development, please refer to [14]. In contrast to the methods discussed above, the proposed framework starts from the widely used MSE and applies kernel method to the objective function for preserving image structure. NR-IQA: When images are transferred to some specific domain, e.g., the DCT domain, local descriptors may be modeled by some parametric distribution, based on this, some previous works [8, 15, 16] on NR-IQA have focused primarily on Natural Scene Statistics (NSS). On the other hand, inspired by the success of Bag-of-Words approaches in computer vision, [7] uses visual codebook to assess image quality. Quality measure of a new image is obtained by computing the average of quality scores of the codewords, weighted by their distances to visual words in the image. However, the method requires a large number of codewords and precomputed Gabor-filters. Other than hand-crafted features, [6] proposes an unsupervised feature-learning method based on raw image patches. The proposed method is similar to [7] in term of its codebook-based encoding. However, our goal is to learn the features based on raw image patches for both

non-distortion images and distortion images. 3. THE PROPOSED SPMSE FRAMEWORK 3.1. Structure Persevering Mean Square Error Given two signals s, t  RN , the objective function of MSE is ||s - t||2 2 /N . In SPMSE, we introduce a non-linear structure 1 ||(s) - (t)||2 extractor term for each signal as N 2 where  is a mapping function, which maps the original data space to a new feature space. The objective function of SPMSE is: SP M SE (s, t) = 1 ||(s) - (t)||2 2 N 1 = ( (s)(s) - 2 (s)(t) + (t)(t) ) N (1)

In Eq. 1, the SPMSE is guaranteed to be non-negative, and thus it can be viewed as a distance measure. Introducing a kernel operation, we can re-write SPMSE as: SP M SE (s, t) = 1 (K (s, s) - 2 × K (s, t) + K (t, t)) (2) N

where K is a valid Mercer kernel [18], which can be viewed as a non-linear feature similarity measure for the signals. In the next section we will discuss how to choose K . 3.1.1. Kernel Selection As definition in [18]: "a kernel is a function that returns the inner product between the images of two inputs in some feature space". The intuition of the kernel method is to measure the similarity between two data vectors in a new feature space. The most widely used kernels for images (signals) are polynomial kernels and RBF kernels [18]. A polynomial kernel is given by K (x, y ) = ( x, y + R)d where R and d are kernel parameters. The RBF kernel is defined as
||x-y ||2

K (x, z ) = exp -22 . However, trivially bringing them to the proposed objective function is not a good choice for FRIQA, since the resultant MSE-based kernel function is still based on pixel-wise computation and hence losing the sight of the image structure distortion, which has been argued to be an essential factor for IQA [4, 2]. Thus, one of our goals is to find a proper kernel that helps to retain structural information of an image. Inspired by its success in object detection, e.g. [11], we employ Histogram of Oriented Gradient(HOG) as image quality descriptor. HOG is one of the most-used low-level vision features for object detection and recognition, and the essential thought behinds HOG is that the local appearance and structure in images can be described by its gradient distribution. Based on the following theorem, we show it can be incorporated into our proposed framework as a valid kernel function.

Theorem 1: HOG operator is a valid kernel function. Proof : Let i and Mi be the orientation and magnitude of gradient at pixel i. Then the HOG feature of each pixel is represented by a hard binning indicator. in =
i 1 if 2  =n-1 0 otherwise

(3)

For each image block P , the oriented gradient is represented as  (P ) = iP Mi · i . When measuring the similarity between patches from two different images, it is equivalent to match the patches in the feature space. Thus, we can represent the similarity between image patches in the feature space with a linear kernel: K (P, Q) =
iP

the nearby bases of the encoded data. Based on these observations, we compare different coding schemes and then propose a novel feature coding scheme for NR-IQA, which supports feature learning with the proposed SPMSE metric. Let X be a set of M-dimensional feature vectors extracted from images, i.e., X = [x1 , x2 , ....., xN ]  RM ×N . C = [c1 , c2 , ..., cN ] is a set of code coefficients for X based on codebook B = [b1 , b2 , ...bK ]  RM ×K . C can be generated by different coding schemes for image representation. Based on [19, 17, 20], the proposed coding scheme solves the following optimization problem:
N

argmin
c i=1

2 ||xi - Bci ||2 2 + ||Di ci ||2 + µ|ci |

(5)

Mi · i
i Q

Mi ·  i =
iP i Q

T Mi  i i

Mi

=
iP i Q

T M i M i i i

(4) where P, Q are two patches from two images. Since Mi Mi t is a non negative scalar and i i is the inner product of two vectors, then we can substitute two linear kernel KM (i, i ) = T Mi · Mi , K (i, i ) = i i in Eq. 4. Thus, K (P, Q) is a valid kernel [18] and it provides a kernel view of HOG. It is worth noting that, in contrast to [13], where the metric is simply based on the similarity of gradient value from two signals. In the proposed framework, inspired by the success of using HOG for object detection, we utilize the property of the HOG for image structure description. Specifically, in Theorem 1, KM (i, i ) measures the similarity of gradient magnitude of two pixels and K (i, i ) measures the similarity of gradient orientations of two pixels. Thus, instead of measuring the pixel similarity in MSE, the proposed SPMSE can be viewed as a structure similarity measure for image patches (e.g. 8 × 8 rectangles in HOG). 3.2. Structure-Persevering Coding In Section 3.1, we proposed a SPMSE framework for the FRIQA problem, which captures image local structures instead of measuring the pixel-wise error. Noting Eq. 1 is convex and differentiable, we can easily build an objective function to minimize. We now show how the idea may be extended to handle NR-IQA problems. In recent NR-IQA approaches [7, 6, 15, 17], different features have been designed. However, feature coding has been largely ignored. In other words, how to efficiently encode the features for NR-IQA is still not well addressed. In [7], hard vector coding was used, and in [6], the authors argue soft coding is better, while [17] argues sparse coding is more efficient. In [19], locality linear coding is proposed, the authors observed that the non-zero coefficients are often assigned to

where Di  RK ×K is a diagonal matrix, with each element in the diagonal representing the SPMSE score of input image patch i and code basis j , i.e.Di(j,j ) = SP M SE (xi , bj ). The second term in Eq. 5 gives the input patch freedom to decide proportion of similar structure bases in the codebook, while the third term is the sparse regularization term which makes the nonlinear representation of the features. Unfortunately, the above objective function is computational expensive. To alleviate this, we propose an approximation scheme by relaxing the sparse term in the objective function to 1T ci = 1, i, which still achieves sparsity if we set small values in the solution to zero. Since the Eq. 5 can be decomposed, the encoded feature ci can be obtained by solving the following optimization problem: argmin
c 2 J = ||xi - Bci ||2 2 + ||Di ci ||2

subject to 1T ci = 1; The Lagrangian function of Eq. 6 is:
T 2 L = ||xi - Bci ||2 2 + ||Di ci ||2 +  (1 ci - 1)

(6)

(7)

where  is Lagrangian multiplier. Taking the derivation of L and setting it to zero, we can obtain:
2  1 = 2B T xi - 2B T Bci - 2Di ci T

(8)

Trick 1: 1 ci = 1 and xT i Bci is a scalar, Eq. 8 can be written as:
T T T 2  1 + 2 xT i Bci 1 = 2B xi 1 ci - 2B Bci - 2Di ci

+ 21xT i Bci

(9)

1 T T T 2 T  - ( + xT i Bci )1 = (B B - B xi 1 + Di - 1xi B )ci 2 (10) T Trick 2: xT x 1 c is a scalar, thus it can be added on the i i i both sides: 1 T T T T T - ( + xT i Bci )1 + xi xi 1 ci 1 = (B B - B xi 1 2 2 + Di - 1xT i B )ci
T + 1xT i x i 1 ci

(11)

MSE SSIM VIF IFC MAD SPMSE

Table 1. PLCC comparison of different FR-IQA models LIVE(779 images) TID2008(1300 images) CSIQ(750 images) 0.8739 0.7649 0.8882 0.9451 0.8530 0.9188 0.9604 0.8938 0.9321 0.9268 0.8007 0.8912 0.9394 0.8306 0.8881 0.9364 0.8876 0.9213

0.8279 0.8962 0.9226 0.8599 0.8762 0.9096

1 T T T T T  - (  + xT i Bci - 2xi xi 1 ci )1 = (B B - B xi 1 2 2 + Di - 1xT i B
T + 1 xT i xi 1 )ci

(12) Finally, the closed form solution can be obtained after normalization as :
T T T 2 ci = ((B T - 1xT i )(B - 1xi ) + Di ) \ 1

ci = ci /1ci

(13)

for us to deal with. Thus, the number of images from CSIQ is 750. Evaluation: We evaluate the performance of different methods using Pearson Linear Correlation Coefficient (PLCC) and Spearman Rank Order Correlation Coefficient (SROCC). PLCC is considered as a measurement of the prediction accuracy and SROCC is viewed as an evaluation of how well the relationship between the predicted score and the subject opinion score can be described. A good IQA model should have high PLCC and SROCC. 4.2. NR-IQA Evaluation Protocol Database for NR-IQA evaluation and codebook construction: We use LIVE database for evaluation and adopt CSIQ databse for codebook construction based on the following reasons: First, there is no overlap between CSIQ dataset and LIVE dataset. Second, both CSIQ and LIVE contain four types of distortion: JP2K, JPEG, GB, AWN. Thus, it is reasonable to use codebook generated from CSIQ to represent the images in LIVE instead of TID2008 which has much more noise types than CSIQ. For each image in CSIQ, we randomly extract 10000 7 by 7 raw patches, then using K-means clustering to generate the codebook. In our experiment, the codebook is fixed by 10000 × 49. This protocol is also used in [7]. NR-IQA Regression and Evaluation: The predicted score is calculated from linear support vector regression (SVR) directly. Since codebook is constructed from unlabeled data, in LIVE database, we randomly pick 80% images associate with human subject score to train the SVR and remaining 20% for testing. Moreover, we repeat the train-test scheme 100 times for cross-validation. It is worth noting that both the training set and the testing set only contain the distorted images. Finally, we use max-pooling to represent image feature. 4.3. Comparison with FR-IQA and NR-IQA algorithms In this sub-section, we first compare the results of the proposed method with state-of-the-art FR-IQA models including SSIM [2], VIF[3], IFC[4], MAD [12]. Table 1 and Table 2 list the results of SROCC and PLCC of different FR-IQA models respectively. The results are reported from the original papers

Compared to hard vector quantization encoding [7] which represents images from a single basis, the approximation scheme of Eq. 5 will achieve much smaller error because of the use of multiple bases (soft coding). It is worth noting that the method in [19] is based on pixel-wise representation, lacking the structural information captured by our SPMSE-based scheme. Moreover, we empirically observed that the coding results from [17] tend to select codebook bases that were from images under different distortions, while code bases from our approach tend to belong to images of similar distortion. 4. EXPERIMENTS 4.1. FR-IQA Evaluation Protocol Database for FR-IQA evaluation: To evaluate the proposed framework, we tested it on three benchmark IQA datasets: LIVE[21], TID2008[22], and CSIQ[12]. The images in these datasets are generated with different type of distortion and associated with human/subjective opinion score. The LIVE database contains 29 reference images and 779 distorted images with 5 different distortions: JPEG2000 compression (JP2K), JPEG compression (JPEG), additive white noise(AWN), Gaussian blur (GB), and Fast fading (FF). The TID2008 database contains 25 reference images and 1700 distorted images with 17 different noise types. Since the last four distortions (totally 400 images) are not structure distortions, e.g. intensity shift, which is a highly subjective task for people to distinguish with, we reported the results on first 13 distortions. This protocol also has been used in [6, 17]. The CSIQ contains 30 reference images and 866 distorted images generated from JP2K, JPEG, AWN, GB, and pink Gaussian noise, the contrast change is also not the structure distortion

with default parameter settings. It is worth noting that PLCC results are reported after logistic regression (Eq. (14) and Eq. 15) between predicted score and subject opinion score, which follows the instruction reported in [23]. From Table 2 and Table 1, we can draw the following conclusions. First, the proposed method outperforms a large margin to MSE and is superior to SSIM. Second, the proposed method is comparable to other state-of-the-art method, e.g., VIF, MAD, in terms of average resuls among three benchmark datasets. Moreover, in Table 3, we compare the speed1 of the proposed method and other top 3 FR-IQA metrics. It can be seen that the proposed is efficient in terms of computation time. Table 3. Speed Comparison with Top 3 metrics in FR-IQA to MSE MSE SSIM SPMSE VIF MAD Time(s) 0.0021 0.031 0.043 0.974 2.07 ratio to MSE 1 15 20 458 986 In Table 4 and Table 5, we report the results of our encoding scheme with comparison of state-of-the-art NR-IQA methods and other encoding schemes. The compared methods including BIQI [7], CORINA[7], DIIVINE [16] and BLIINDS (SVM) [8] and we also compared our encoding methods with hard encoding (HC), sparse encoding (SC) [20] and locality linear encoding (LLC) [19]. From the result we can see that our proposed achieves best result among all the encoding schemes which have same codebook, meanwhile, our result is comparable to state-of-the-arts models, e.g., CORINA. Noting that the evaluation of the proposed method only employs general procedures of BOW, the results can be further improved by employing a more powerful regressor (e.g. random forest) or using precomputed features (e.g., NSS) instead of raw image patches.

Table 4. SROCC comparison of different NR-IQA models on LIVE Method JP2K JPEG AWN GB FF ALL PSNR 0.872 0.885 0.941 0.764 0.875 0.867 SSIM 0.939 0.946 0.965 0.909 0.941 0.913 BIQI 0.856 0.786 0.972 0.910 0.762 0.819 CO0.943 0.955 0.976 0.969 0.906 0.942 RINA DIVI0.913 0.910 0.984 0.921 0.863 0.916 INE BLI0.929 0.955 0.956 0.923 0.889 0.931 INDS SPMSE 0.936 0.948 0.952 0.958 0.872 0.930 LLC 0.921 0.941 0.942 0.932 0.862 0.909 HC 0.919 0.948 0.945 0.908 0.905 0.917 SC 0.926 0.958 0.952 0.941 0.852 0.921

Table 5. PLCC comparison LIVE Method JP2K JPEG PSNR 0.873 0.874 SSIM 0.920 0.955 BIQI 0.809 0.901 CO0.951 0.965 RINA DIVI0.922 0.921 INE BLI0.935 0.968 INDS SPMSE 0.947 0.951 LLC 0.931 0.941 HC 0.921 0.950 SC 0.929 0.965

of different NR-IQA models on AWN 0.928 0.982 0.954 0.987 0.988 0.980 0.971 0.943 0.965 0.959 GB 0.774 0.891 0.829 0.968 0.923 0.938 0.970 0.942 0.929 0.945 FF 0.869 0.939 0.733 0.917 0.888 0.896 0.899 0.872 0.883 0.892 ALL 0.855 0.906 0.821 0.935 0.917 0.930 0.934 0.919 0.917 0.925

Quality (x) = 1 logistic(2 , (x - 3 )) + 4 x + 5 (14) 1 1 - 2 1 + exp( x)

logistic(, x) =

(15)

5. DISCUSSION AND FUTURE WORK We proposed a simple yet effective approach for image quality assessment. First, we proposed a structure-preserving MSE-like error function for FR-IQA, and the experimental results show that our method is competitive with respect to the state-of-the-art methods and in particular, outperforms the
the codes are implemented by Matlab and obtained from original authors' webpage. The HOG computation part is written in C and compiled by Matlab.
1 All

well-known SSIM. Second, we showed that the proposed approach can be applied to the NR-IQA framework as well, through incorporating it in a coding scheme. Even with only a fixed and unoptimized codebook, the experimental results still showed performance comparable to current approaches. Future efforts include at least two possible exentions: a learningbased method for selecting a kernel function more efficiently, and codebook learning for improved NR-IQA.

6. ACKNOWLEDGMENT Yilin Wang and Baoxin Li were supported in part by a grant (#1135616) from the National Science Foundation (NSF). Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.

MSE SSIM VIF IFC MAD SPMSE

Table 2. SROCC comparison of different FR-IQA models LIVE(779 images) TID2008(1300 images) CSIQ(750 images) 0.8756 0.7118 0.9060 0.9479 0.8742 0.9247 0.9636 0.8731 0.9282 0.9259 0.7589 0.8827 0.9438 0.8694 0.9604 0.9564 0.8887 0.9353

Weighted 0.8362 0.8946 0.9130 0.8383 0.9142 0.9179

7. REFERENCES [1] Keigo Hirakawa and Thomas W Parks, "Image denoising using total least squares," Image Processing, IEEE Transactions on, vol. 15, no. 9, pp. 2730­2742, 2006. [2] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli, "Image quality assessment: from error visibility to structural similarity," Image Processing, IEEE Transactions on, vol. 13, no. 4, pp. 600­612, 2004. [3] Hamid R Sheikh and Alan C Bovik, "Image information and visual quality," Image Processing, IEEE Transactions on, vol. 15, no. 2, pp. 430­444, 2006. [4] Hamid R Sheikh, Alan C Bovik, and Gustavo De Veciana, "An information fidelity criterion for image quality assessment using natural scene statistics," Image Processing, IEEE Transactions on, vol. 14, no. 12, pp. 2117­2128, 2005. [5] Damon M Chandler and Sheila S Hemami, "Vsnr: A waveletbased visual signal-to-noise ratio for natural images," Image Processing, IEEE Transactions on, vol. 16, no. 9, pp. 2284­ 2298, 2007. [6] Peng Ye, Jayant Kumar, Le Kang, and David Doermann, "Unsupervised feature learning framework for no-reference image quality assessment," in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 1098­1105. [7] Peng Ye and David Doermann, "No-reference image quality assessment based on visual codebook," in Image Processing (ICIP), 2011 18th IEEE International Conference on. IEEE, 2011, pp. 3089­3092. [8] Michele A Saad, Alan C Bovik, and Christophe Charrier, "Blind image quality assessment: A natural scene statistics approach in the dct domain," Image Processing, IEEE Transactions on, vol. 21, no. 8, pp. 3339­3352, 2012. [9] Zhou Wang and Alan C Bovik, "Mean squared error: love it or leave it? a new look at signal fidelity measures," Signal Processing Magazine, IEEE, vol. 26, no. 1, pp. 98­117, 2009. [10] Wufeng Xue, Xuanqin Mou, Lei Zhang, and Xiangchu Feng, "Perceptual fidelity aware mean squared error," . [11] Navneet Dalal and Bill Triggs, "Histograms of oriented gradients for human detection," in Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. IEEE, 2005, vol. 1, pp. 886­893. [12] Eric C Larson and Damon M Chandler, "Most apparent distortion: full-reference image quality assessment and the role

of strategy," Journal of Electronic Imaging, vol. 19, no. 1, pp. 011006­011006, 2010. [13] Anmin Liu, Weisi Lin, and Manish Narwaria, "Image quality assessment based on gradient similarity," Image Processing, IEEE Transactions on, vol. 21, no. 4, pp. 1500­1512, 2012. [14] Weisi Lin and C-C Jay Kuo, "Perceptual visual quality metrics: A survey," Journal of Visual Communication and Image Representation, vol. 22, no. 4, pp. 297­312, 2011. [15] Anush Krishna Moorthy and Alan Conrad Bovik, "A twostep framework for constructing blind image quality indices," Signal Processing Letters, IEEE, vol. 17, no. 5, pp. 513­516, 2010. [16] Anush Krishna Moorthy and Alan Conrad Bovik, "Blind image quality assessment: From natural scene statistics to perceptual quality," Image Processing, IEEE Transactions on, vol. 20, no. 12, pp. 3350­3364, 2011. [17] Lihuo He, Dacheng Tao, Xuelong Li, and Xinbo Gao, "Sparse representation for blind image quality assessment," in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 1146­1153. [18] John Shawe-Taylor and Nello Cristianini, Kernel methods for pattern analysis, Cambridge university press, 2004. [19] Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv, Thomas Huang, and Yihong Gong, "Locality-constrained linear coding for image classification," in Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 3360­3367. [20] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y Ng, "Efficient sparse coding algorithms," . [21] Hamid R Sheikh, Muhammad F Sabir, and Alan C Bovik, "A statistical evaluation of recent full reference image quality assessment algorithms," Image Processing, IEEE Transactions on, vol. 15, no. 11, pp. 3440­3451, 2006. [22] Nikolay Ponomarenko, Vladimir Lukin, Alexander Zelensky, Karen Egiazarian, M Carli, and F Battisti, "Tid2008-a database for evaluation of full-reference visual quality assessment metrics," Advances of Modern Radioelectronics, vol. 10, no. 4, pp. 30­45, 2009. [23] Video Quality Experts Group et al., "Final report from the video quality experts group on the validation of objective models of video quality assessment," VQEG, Mar, 2000.

Inferring Sentiment from Web Images with Joint Inference on Visual and Social Cues: A Regulated Matrix Factorization Approach
Yilin Wang1 Yuheng Hu2 Subbarao Kambhampati1 Baoxin Li1
1

Department of Computer Science, Arizona State University, Tempe AZ 2 IBM Almaden Research Center, San Jose CA {ywang370,rao,baoxin.li}@asu.edu yuhenghu@us.ibm.com

Abstract
In this paper, we study the problem of understanding human sentiments from large scale collection of Internet images based on both image features and contextual social network information (such as friend comments and user description). Despite the great strides in analyzing user sentiment based on text information, the analysis of sentiment behind the image content has largely been ignored. Thus, we extend the significant advances in text-based sentiment prediction tasks to the higherlevel challenge of predicting the underlying sentiments behind the images. We show that neither visual features nor the textual features are by themselves sufficient for accurate sentiment labeling. Thus, we provide a way of using both of them. We leverage the low-level visual features and mid-level attributes of an image, and formulate sentiment prediction problem as a non-negative matrix tri-factorization framework, which has the flexibility to incorporate multiple modalities of information and the capability to learn from heterogeneous features jointly. We develop an optimization algorithm for finding a local-optima solution under the proposed framework. With experiments on two large-scale datasets, we show that the proposed method improves significantly over existing state-of-the-art methods.

1

Introduction

A picture is worth a thousand words. It is surely worth even more when it comes to convey human emotions and sentiments. Examples that support this are abundant: great captivating photos often contain rich emotional cues that help viewers easily connect with those photos. With the advent of social media, an increasing number of people start to use photos to express their joy, grudge, and boredom on social media platforms like Flickr and Instagram. Automatic inference of the emotion and sentiment information from such ever-growing, massive amounts of user-generated photos is of increasing importance to many applications in health-care, anthropology, communication studies, marketing, and many sub-areas within computer science such as computer vision. Think about this: Emotional wellness impacts several aspects of people's lives. For example, it introduces self-empathy, giving an individual greater awareness
Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

of their feelings. It also improves one's self-esteem and resilience, allowing them to bounce back with ease, from poor emotional health, and physical stress and difficulty. As people are increasingly using photos to record their daily lives1 , we can assess a person's emotional wellness based on the emotion and sentiment inferred from her photos on social media platforms (in addition to existing emotion/sentiment analysis effort, e.g., see (De Choudhury, Counts, and Gamon 2012) on text-based social media). In this paper, our goal is to automatically infer human sentiments (positive, neutral and negative) from photos shared on Flickr and Instagram. While sentiment analysis of photos is still in its infancy, a number of tools have been proposed during past two years(Yuan et al. 2013; Jia et al. 2012). A popular approach is to identify visual features from a photo that are related to human sentiments, such as objects (e.g., toys, birthday cakes, gun), human actions (e.g., crying or laughing), and many other features like color temperature. However, such an approach is often insufficient because the same objects/actions may convey different sentiments in different photo contexts. For example, consider Figure 1: one can easily detect the crying lady and girl (using computer vision algorithms such as face detection(Zhu and Ramanan 2012) and expression recognition(Song et al. 2010)). However, the same "crying" action conveys two clearly different sentiments: the "crying" in Figure 1a is obviously positive as the result of a successful marriage proposal. In contrast, the tearful girl in Figure 1b looks quite unhappy thus expresses negative sentiment. In other words, the so-called "visual affective gap" (Machajdik and Hanbury 2010) exists between rudimentary visual features and human sentiment embedded in a photo. On the other hand, one may also consider inferring the sentiment of a photo via its textual descriptions (e.g., titles) using existing off-the-shelf text-based sentiment analysis tools (Pang, Lee, and Vaithyanathan 2002). Although these descriptions can provide very helpful context information of the photos, solely relying on them while ignoring the visual features of the photos can lead to poor performance as well. Consider Figure 1 again: by analyzing only the text description, we can conclude that both Figure 1a and 1b convey negative sentiment as the keyword "crying" is often
1 http://www.pewinternet.org/2015/01/09/social-media-update2014/

(a) "Girlfriend crying a lot when I(b) Crying baby after her toy was proposed to her". taken

Figure 1: An example shows affective gap.

classified as negative sentiment in standard sentiment lexicon (Taboada et al. 2011). Last, both visual feature-based and text-based sentiment analysis approaches require massive amounts of training data in order to learn high quality models. However, manually annotating the sentiment of a vast amount of photos and/or their textual descriptions is time consuming and error-prone, presenting a bottleneck in learning good models. The weaknesses discussed in the foregoing motivate the need for a more accurate automated framework to infer the sentiment of photos, with 1) considering the photo context to bridge the "visual affective gap", 2) considering a photo's visual features to augment text-based sentiment, and 3) considering the availability of textual information, thus a photo may have little or no social context (e.g., friend comments, user description). While such a framework does not exist, we can leverage some partial solutions. For example, we can learn the photo context by analyzing the photo's social context (text features). Similarly, we can extract visual features from a photo and map them to different sentiment meanings. Last, while manual annotation of all photos and their descriptions is infeasible, it is often possible to get sentiment labeling for small sets of photos and descriptions. Technical Contribution: We propose an efficient and effective framework, named RSAI (Robust Sentiment Analysis for Images), for inferring human sentiment from photos that leverages these partial solutions. Figure 2 depicts the procedure of RSAI. Specifically, to fill the visual affective gap, we first extract visual features from a photo using low-level visual features (e.g., color histograms) and a large number of mid-level (e.g., objects) visual attribute/object detectors (Yuan et al. 2013; Tighe and Lazebnik 2013). Next, to add sentiment meaning to these extracted non-sentimental features, we construct Adjective Noun Pairs (ANPs)(Borth et al. 2013). Note that ANP is a visual representation that describes visual features by text pairs, such as "cloudy sky", "colorful flowers". It is formed by merging the low-level visual features to the detected mid-level objects and mapping them to a dictionary (more details on ANP are presented in Section 3). On the other hand, to learn the image's context, we analyze the image's textual description and capture its sentiment based on sentiment lexicons. Finally, with

the help from ANPs and image context, RSAI infers the image's sentiment by factorizing an input image-features matrix into three factors corresponding to image-term, termsentiment and sentiment-features. The ANPs here can be seen as providing the initial information ("prior knowledge") on sentiment-feature factors. Similarly, the learnt image context can be used to constrain image-term and termsentiment factors. Last, the availability of labeled sentiment of the images can be used to regulate the product of imageterm, term-sentiment factors. We pose this factorization as an optimization problem where, in addition to minimizing the reconstruction error, we also require that the factors respect the prior knowledge to the extent possible. We derive a set of multiplicative update rules that efficiently produce this factorization, and provide empirical comparisons with several competing methodologies on two real datasets of photos from Flickr and Instagram. We examine the results both quantitatively and qualitatively to demonstrate that our method improves significantly over baseline approaches. The rest of this paper is organized as follows: first, we review the related work on sentiment prediction as well as work which utilizes the nonnegative matrix factorization. We then present a basic model for the problem and further improve the model by incorporating prior knowledge. The experimental results and a comprehensive analysis are presented in the experiment part. Last, we conclude by identifying future work.

2

Related Work

In this section, we review the related work on sentiment analysis and the methods for matrix factorization. Sentiment analysis on text and images: Recently, sentiment analysis has shown its success in opinion mining on textual data, including product review(Liu 2012; Hu and Liu 2004), newspaper articles (Pang, Lee, and Vaithyanathan 2002), and movie rating (Pang and Lee 2004). Besides, there have been increasing interests in social media data (Borth et al. 2013; Yang et al. 2014; Jia et al. 2012; Yuan et al. 2013), such as Twitter and Weibo data. Unlike textbased sentiment prediction approaches, (Borth et al. 2013; Yuan et al. 2013) employed mid-level attributes of visual feature to model visual content for sentiment analysis. (Yang et al. 2014) provides a method based on low-level visual features and social information via a topic model. While (Jia et al. 2012) tries to solve the problem by a graphical model which is based on friend interactions. In contrast to our approach, all such methods restrict sentiment prediction to the specific data domain. For example, in Figure 1, we can see that approaches using pure visual information (Borth et al. 2013; Yuan et al. 2013) may be confused by the subtle sentiment embedded in the image. e.g., two crying people convey totally different sentiment. (Jia et al. 2012; Yang et al. 2014) assume that the images belong to the same sentiment share the same low-level visual features is often not true, because positive and negative images may have similar low-level visual features, e.g., two black-white images contain smiling and sad faces respectively. Recent, deep learning has shown its success in feature learning for many computer vision problem, (You et al. 2015) provides a

Figure 2: The framework of our proposed method. Comparing to conventional methods, which focus on single source/feature, the proposed method learns the heterogeneous features, including text features, low-level features and mid-level visual features, for sentiment analysis. transfer deep neutral network structure for sentiment analysis. However, for deep learning framework, millions of images with associated sentiment labels are needed for network training. In real world, such label information is not available and how to deal with overfitting for small training data remains a challenging problem. Non-negative matrix factorization(NMF): Our proposed framework is also inspired by recent progress in matrix factorization algorithms. NMF has been shown to be useful in computer vision and data mining applications including face recognition(Wang et al. 2005), object detection (Lee and Seung 1999) and feature selection (Das Gupta and Xiao 2011), etc. Specifically, the work in (Lee and Seung 2001) brings more attention to NMF in the research community, where the author proposed a simple multiplicative rule to solve the problem and showed the factor coherence of original image data. (Ding, He, and Simon 2005) shows that if adding orthogonal constrains, the NMF is equivalent to K -means clustering. Further, (Ding et al. 2006) presents a work that shows, when incorporating freedom control factors, the non-negative factors will achieve a better performance on classification. In this paper, motivated by previous NMF framework for learning the latent factors, we extend these efforts significantly and propose a comprehensive formulation which incorporates more physically-meaningful constraints for regularizing the learning process in order to find a proper solution. In this respect, our work is similar in spirit to (Hu, Wang, and Kambhampati 2013) which develops a factorization approach for sentiment analysis of social media responses to public events. Table 1: Notations Notation X T S V T0 S0 V0 R0 Dimension n×m n×t t×k m×k n×t t×k m×k n×k Description Input data matrix Data-term matrix Term-sentiment matrix Feature-sentiment matrix Prior knowledge on T Prior knowledge on S Prior knowledge on V Prior knowledge on the labels

3.1

Basic Model

Assuming that all the images can be partitioned into K sentiment (K = 3 in this paper as we focus on positive, neutral and negative. However, our framework can be easily extended to handle more fine-grained sentiment.) Our goal is to model the sentiment for each image based on visual features and available text features. Let n be the number of images and the size of contextual vocabulary is t. We can then easily cluster the images with similar word frequencies and predict the cluster's sentiment based on its word sentiment. Meanwhile, for each image, which has m-dimensional visual features (ANPs, see below), we can cluster the images and predict the sentiment based on the feature probability. Accordingly, our basic framework takes these n data points and decomposes them simultaneously into three factors: photo-text, text-sentiment and visual feature-sentiment. In other words, our basic model tries to solve the following optimization problem:
T SV

3

The Proposed RSAI Framework
min

X - T SV T

2 F

+ T - T0

2 F

In this section, we first propose the basic model of our framework. Then we show the details of how to generate the ANPs. After that, we describe how to obtain and leverage the prior knowledge to extend the basic model. We also analyze the algorithm in terms of its correctness and convergence. Table 1 lists the mathematical notation used in this paper.

(1)

subject to T  0, S  0, V  0; , where X  Rn×m represents input data matrix, and T  Rn×t indicates the text features. That is, the ith row of matrix T corresponds to the posterior probability of the ith image's contextual social network information referring to the t text terms (vocabulary). Similarly, S  Rt×k indicates the

posterior probability of a text belonging to k sentiments. Finally, V  Rm×k represents the sentiment for each ANP. The regularization term T0 is the term-frequency matrix for the whole word vocabulary (which is built based on textual descriptions of all photos). It is worth noting that the nonnegativity makes the latent components easy to interpret. As a result of this factorization, we can readily predict the image sentiment whether the contextual information (comments, user descriptions,etc.) is available or not. For example, if there is no social information associated with the image, then we can directly derive the image sentiment by applying non-negative matrix factorization for the input data X , when we characterize the sentiment of each image through a new matrix R = T × S . Specifically, our basic model is similar to the probabilistic latent semantic indexing (PLSI) (Hofmann 1999) and the orthogonal nonnegative tri-matrix factorization (Ding et al. 2006). In their work, the factorization means the joint distribution of documents and words.

ageNet(Deng et al. 2009). The scene detectors are trained on SUN dataset (Xiao et al. 2010). It is worth noting that selfie is one of most popular images on the web (Hu, Manikonda, and Kambhampati 2014) and face expression usually conveys strong sentiment, consequently, we also adopt one of state-of-the-art face detection methods proposed in (Zhu and Ramanan 2012). Adjective Detection: Modeling the adjectives is more difficult than nouns due to the fact that there are no well defined features to describe them. Following (Borth et al. 2013), we collect 20,000 images associate with specific adjective tags from Web. The a set of discriminative global features, including Gist, color histogram and SIFT, are applied for feature extraction. Finally the adjective detection is formulated as a traditional image classification problem based on Bag of words(BOW)model. The dictionary size of BOW is 1,000 with the feature dimension size 1,500 after dimension reduction based on PCA.

3.2

Extracting and Modeling Visual Features

3.3

Constructing Prior Knowledge

In (Tighe and Lazebnik 2013; Tu et al. 2005; Yuan et al. 2013), visual content can be described by a set of midlevel visual attributes, however, most of the attributes such as "car", "sky","grass", etc., are nouns which make it difficult to represent high level sentiments. Thus, we followed a more tractable approach (Borth et al. 2013), which models the correlation between visual attributes and visual sentiment with adjectives, such as "beautiful" , "awesome", etc. The reason for employing such ANPs is intuitive: the detectable nouns (visual attributes) make the visual sentiment detection tractable, while the adjectives add the sentiment strength to these nouns. In (Borth et al. 2013), a large scale ANPs detectors are trained based on the features extracted from the images and the labeled tags with SVM. However, we find that such pre-defined ANPs are very hard to interpret. For example the pairs like "warm pool" , "abandoned hospital", and it is very difficult to find appropriate features to measure them. Moreover, in their work, during the training stage, the SVM is trained on the features extracted from the image directly, the inability of localizing the objects and scales bounds the detection accuracy. To address these problems, we have a two stage approach to detect ANPs based on the Visual Sentiment Ontology (Borth et al. 2013) and train a one vs all classifier for each ANP. Noun Detection: The nouns in ANPs refer to the objects presented in the image. As one of fundamental tasks in computer vision, object detection has been studied for many years. One of most successful works is Deformable Part Model (DPM) (Felzenszwalb et al. 2010) with Histogram of Oriented Gradient (HOG) (Dalal and Triggs 2005) features. In (Felzenszwalb et al. 2010), the deformable part model has shown its capability to detect most common objects with rigid structure such as: car, bike and non-rigid objects such as pedestrian, dogs. (Pandey and Lazebnik 2011) further demonstrates that DPM can be used to detect and recognize scenes. Hence we adopt DPM to for nouns detection. The common objects(noun) are trained by the public dataset Im-

So far, our basic matrix factorization framework provides potential solution to infer the sentiment regarding the combination of social network information and visual features. However, it largely ignores the sentiment prior knowledge on the process of learning each component. In this part, we introduce three types of prior knowledge for model regularization: (1) sentiment-lexicon of textual words, (2) the normalized sentiment strength for each ANP, and (3) sentiment labels for each image. Sentiment Lexicon The first prior knowledge is from a public sentiment lexicon named MPQA corpus 2 . In this sentiment lexicon, there are 7,504 human labeled words which are commonly used in the daily life. The number of positive words (e.g."happy", "terrific") is 2,721 and the number of negative words (e.g. "gloomy", "disappointed") is 4,783. Since this corpus is constructed without respect to any specific domain, it provides a domain independent prior on word-sentiment association. It should be noted that the English usage in social network is very casual and irregular, we employ a stemmer technique proposed in (Han and Baldwin 2011). As a result, the ill-formed words can be detected and corrected based on morphophonemic similarity, for example "good" is a correct version of "goooooooooooood". Besides some abbreviation of popular words such as "lol"(means laughing out loud) is also added as prior knowledge. We encode the prior knowledge in a word sentiment matrix S0 where if the ith word belongs to jth sentiment, then S0 (i, j ) = 1, otherwise it equals to zero. Visual Sentiment In addition to the prior knowledge on lexicon, our second prior knowledge comes from the Visual Sentiment Ontology (VSO) (Borth et al. 2013), which is based on the well known previous researches on human emotions and sentiments (Darwin 1998; Plutchik 1980). It generates 3000 ANPs using Plutchnik emotion model and
2

http://mpqa.cs.pitt.edu/

Table 2: Sentiment strength score examples ANP innocent smile happy Halloween delicious food cloudy mountain misty forest ... Sentiment Strength 1.92 1.81 1.52 -0.4 -1.00 ...

multiplicative updating scheme shown in (Ding et al. 2006) to find the optimal solutions. First, we use fixed V and S to update T as follows: [XV S T + T0 + R0 S T ]ij [T SV T V S T + T + T SS T ]ij

Tij  Tij

(4)

Next, we use the similar update rule to update S and V : [T T XV + S0 + T T R0 ]ij [T T T SV T V + S + T T T S ]ij [X T T S + V0 ]ij [V S T T T T S + V ]ij

associates the sentiment strength (range in[-2:2] from negative to positive) by a wheel emotion interface3 . The sample ANP sentiment scores are shown in Table 2. Similar to the word sentiment matrix S0 , the prior knowledge on ANPs V0 is the sentiment indicator matrix. Sentiment labels of Photos Our last prior knowledge focuses on the prior knowledge on the sentiment label associated with the image itself. As our framework essentially is a semi-supervised learning approach, this leads to a domain adapted model that has the capability to handle some domain specific data. The partial label is given by the image sentiment matrix R0 where R0  Rn×k . For example if the ith image belongs to jth sentiment, the R0 (i, j ) = 1 otherwise R0 (i, j ) = 0. The improvement by incorporating these label data is empirically verified in the experiment section.

Sij  Sij

(5)

Vij  Vij

(6)

The learning process consists of an iterative procedure using Eq (3), Eq (4) and Eq (5) until convergence. The description of the process is shown in Algorithm 1. Algorithm 1 Multiplicative Updating Algorithm Input: X, T0 , S0 , V0 , R0 , , , ,  Output: T, S, V Initialization: T, S, V while Not Converge do Update T using Eq(4) with fixed S,V Update S using Eq(5) with fixed T,V Update V using Eq(6) with fixed T,S End

3.4

Incorporating Prior Knowledge

After defining the three types of prior knowledge, we incorporate them into the basic model as regularization terms in following optimization problem:
T SV

min X - T SV T +  T - T0

2 F 2 F

+  V - V0 +  S - S0

2 F 2 F

3.5

Algorithm Correctness and Convergence

(2) 2 +  T S - R0 F subject to T  0, S  0, V  0 where   0,   0,   0 and   0 are parameters controlling the extent to which we enforced the prior knowledge on the respective components. The model above is generic and allows flexibility . For example, if there is no social information available for one image, we can simply set the corresponding row of T0 to zeros. Moreover, the square loss function leads to an unsupervised problem for finding the solutions. Here, we re-write Eq (2) as : L =T r(X T X - 2X T T SV T + V S T T T T SV T ) + T r(V T V - 2V T V0 + V0T V )
T + T r(T T T - 2T T T0 + T0 T0 )

In this part, we prove the guaranteed convergence and correctness for Algorithm 1 by the following two theorems. Theorem 1. When Algorithm 1 converges, the stationary point satisfies the Karush-Kuhn-Tuck(KKT) condition, i.e., Algorithm 1 converges correctly to a local optima. Theorem 2. The objective function is nondecreasing under the multiplicative rules of Eq (4), Eq (5) and Eq (6), and it will converge to a stationary point. The detailed proof is presented in Appendix.

4

Empirical Evaluation

We now quantitatively and qualitatively compare the proposed model on image sentiment prediction with other candidate methods. We also evaluate the robustness of the proposed model with respect to various training samples and different combinations of prior knowledge. Finally, we perform a deeper analysis of our results.

(3)
T R0 R0 )

+ +

T T r(S S - 2S S0 + S0 S0 ) T T T T T r(S T T S - 2S T R0 +

T

T

4.1

Experiment Settings

From Eq (3) we can find that it is very difficult to solve T , S and V simultaneously. Thus we employ the alternating
3

http://visual-sentiment-ontology.appspot.com

We perform the evaluation on two large scale image datasets collected from Flickr and Instagram respectively. The collection of Flickr dataset is based on the image IDs provided by (Yang et al. 2014), which contains 3,504,192 images from 4,807 users. Because some images are unavailable now, and without loss of generality, we limit the number of

images from each user. Thus, we get 120,221 images from 3921 users. For the collection of the Instagram dataset, we randomly pick 10 users as seed nodes and collect images by traversing the social network based on breadth first search. The total number of images from Instagram is 130,230 from 3,451 users. Establishing Ground Truth: For training and evaluating the proposed method, we need to know the sentiment labels. Thus, 20,000 Flickr images are labeled by three human subjects, the majority voting is employed. However, manually acquiring the labels for these two large scale datasets is expensive and time consuming. Consequently, the rest of more than 230,000 images are labeled by the tags, which was suggested by the previous works (Yang et al. 2014; Go, Bhayani, and Huang )4 . Since labeling the images based on the tags may cause noise issue, and for better reliability we only label the images with primary sentiment labels, which include: positive, neutral and negative. It is worth noting that the human labeled images have both primary sentiment labels and fine grained sentiment labels. The fine grained labels, including: happiness, amusement, anger, fear, sad and disgust, are used to for fine grained sentiment prediction. The comparison methods include: Senti API5 , SentiBank (Borth et al. 2013), EL(Yang et al. 2014) and the baseline method. · Senti API is a text based sentiment prediction API, it measures the text sentiment by counting the sentiment strength for each text term. · SentiBank is a state-of-the-art visual based sentiment prediction method. The method extracts a large number of visual attributes and associates them with a sentiment score. Similar to Senti API, the sentiment prediction is based on the sentiment of each visual attributes. · EL is a graphical model based approach, it infers the sentiment based on the friend interactions and several low level visual features. · Baseline: The baseline method comes from our basic model. To compare it fairly, we also introduce R0 with the basic model which makes the baseline method have the ability to learn from training data.

T0  Rn×t , where n is the number of images, m = 1200 and t is the vocabulary size, we decompose it via Aglorithm 1 and get the label based on max pooling each row of X  V . It is worth noting that in the proposed model, tags are not included as input feature. The results of comparison are shown in Table 3. We employ 30% data for training and remaining for testing. To verify the reliability of tags labeled images, we also included 20000 labeled Flickr images with primary sentiment label. Especially, the classifier setting for SentiBank and EL followed the original papers. The classifier of Sentibank is logistic regression and for EL it is SVM. From the results we can see that, the proposed method performs best in both datasets. Noting that proposed method improved 10% and 6% over state-of-the-art methods (Borth et al. 2013). Results from proposed method are shown in Figure 4. Noting that the number we reported in Table 3 is the prediction accuracy for each method. From the table, we can see that, even though noise exists in the Flickr and Instagram dataset, the results are similar to the performance on human labeled dataset. Another interesting observation is that the performance of EL on Instagram is worse than on Flickr, one reason could be that the wide usage of "picture filters" lowers discriminative ability of the low level visual features, while the models based on the mid level attributes can easily avoid this filter ambiguity. Another interesting observation is that our basic model performs fairly well even if it does not incorporate the knowledge from sentiment strength of ANPs, which indicates that the object based ANPs by our method are more robust than the features used in (Borth et al. 2013). Fine Grained Sentiment Prediction: Although our motivation is to predict the sentiment (positive, negative) on the visual data, to show the robustness and extension capability of the proposed model, we further evaluate the proposed model on a more challenging task in social media; predicting human emotions. Based on the definition of human emotion (Ekman 1992), our fine grained sentiment study labels the user posts with following human emotion categories including: happiness, amusement, disgust, anger, fear and sadness. The results on 20000 manually labeled flickr post are shown in Figure 5. Compared to sentiment prediction, fine grained sentiment prediction would give us more precise user behavior analysis and new insights on the proposed model. As Figure 5 shows, compared to SentiBank and EL, the proposed method has the highest average classification accuracy and the variance of proposed method on these 6 categories is smaller than that of the baseline methods, which demonstrates the potential social media applications of the proposed method such as predicting social response. We noticed that the sad images have the highest prediction accuracy, and both disgust and anger are difficult to predict. Another observation is the average performance of positive categories, happiness and amusement, is similar to the negative categories. Explaining reason for this drives us to dig deeper into sentiment understanding in the following section.

4.2

Performance Evaluation

Large scale image sentiment prediction: As mentioned in Sec 3, the proposed model has the flexibility to incorporate the information and capability to jointly learn from the visual features and text features. For each image, the visual features are formed by the confidence score of each ANP detector, the feature dimension is 1200, which is as large as VSO (prior knowledge V0 ). For the text feature, it is formed based on the term frequency and the dimension relies on the input data. To predict the label, the model input is unknown data X  Rn×m and its corresponding text feature matrix
More details can be found in(Yang et al. 2014) and (Go, Bhayani, and Huang ) 5 http://sentistrength.wlv.ac.uk/,a text based sentiment prediction API
4

(a) Negative

(b) Neutral

(c) Positive

Figure 3: Sample tag labeled images from Flickr and Instagram.

(a) Negative

(b) Neutral

(c) Positive

Figure 4: Sample results from our proposed method. Photos with red bounding box are false positive predictions.

4.3

Analysis and Discussion

In this section, we present an analysis of parameters for the proposed method and the results of the proposed method. Specifically, in last section we have studied the performance of different methods. In this part, our objective is to have deeper understanding on the datasets and the correlation between different features and the sentiments embedded in the images. Without loss of generality, we collected additional 20k images from Flickr and Instagram respectively (totally 40K) and we address the following research questions: · RQ1:What is the relationship between visual features and visual sentiments? · RQ2:Since the proposed method is better than pure visual feature based method, How does the model gain? First, we start with RQ1 by extracting the visual features used in (Borth et al. 2013) and (Yang et al. 2014) for each image in the Flickr and Instagram datasets. Then we use k-means clustering to obtain 3 clusters of images for each dataset, where the image similarity is measured as Euclidean distance in the feature spaces. Based on each cluster center, we used the classfier trained in the previous experiment for cluster labeling. The results are shown in Figure 6. The xaxis is the different class label for each dataset and the y-axis is the number of images that belong to each cluster. From the

Figure 5: Fine grained sentiment prediction results (Y-axis represents the accuracy for each method).

Table 3: Sentiment Prediction Results. The number means prediction accuracy, the higher the better. 20000 Flickr Flickr Instagram Senti API 0.32 0.34 0.27 SentiBank 0.42 0.47 0.56 EL 0.47 0.45 0.37 Baseline 0.48 0.48 0.54 Proposed method 0.52 0.57 0.62

Figure 6: Sentiment distribution based on visual features. From left to rigth is number of positive, neutral, negative images in Instagram and Flickr, receptively. Y axis represents the number of images.

Figure 7: Performance gain by incorporating training data.

results, we notice that the "visual affective gap" does exist between human sentiment and visual features. For the stateof-the art method (Borth et al. 2013), the neural images are largely misclassified based on the visual features. While for (Yang et al. 2014), we observe t the low level features, e.g., color histogram, contrast and brightness, are not closely related to human sentiment as visual attributes. We further analyze the performance of the proposed method based on these 40,000 images. Parameter study: In the proposed model, we incorporate three types of prior knowledge: sentiment lexicon, sentiment labels of photos and visual sentiment for ANPs. It is important and interesting to explore the impact of each of them on the performance of the proposed model. Figure 7 presents the average results (y-axis) of two datasets on sentiment prediction with different amount of training data (xaxis)6 , where the judgment is on the same three sentiment labels with different combinations respectively. It should be noted that each combination is optimized by Algorithm 1, which has similar formulations. Moreover, we set the same parameter for , ,  and  (0.9, 0.7, 0.8 and 0.7). Results give us two insights. First, employing more prior knowledge will make the model more effective than using only one type of prior knowledge. For our matrix factorization framework, T and V have independent clustering freedom by introducing S , thus it is natural to add more constraints for desired decomposed component. Second, when no training data, the basic model with S0 performs much better than SentiAPI (refer Table 3), which means incorporating ANPs significantly improves image sentiment prediction. It is worth noting that there is no training stage for the proposed method. Thus when compared to fully supervised approaches, our
6

Figure 8: The value of  versus model performance. X axis is  value, y axis is value of model performance.

method is more applicable in practice when the label information is unavailable. Bridging the Visual Affective Gap (RQ2): Figure 1 and Figure 7 demonstrate that a visual affective gap exists between visual features and human sentiments (i.e., the same visual feature may correspond to different sentiments in different context). To bridge this gap, we show that one possible solution is to utilize heterogeneous data and features available in social media to augment the visual feature-based sentiment. In the previous parameter study, we have studied the importance of the prior knowledge. Furthermore, we study importance of  which contains the degree of contextual social information used in the proposed model. From Figure 8, we can observe that the performance of the proposed model increases along the value of  . However, when  is greater than 0.8, the performance drops. This is because textual information in social media data is usually incomplete. Larger  will cause negative effects on the prediction accuracy where there is none or little information available.

The experiments setting is as same as discussed above.

5

Conclusion and Future Work

Can we learn human sentiments from the images on the web? In this paper, we proposed a novel approach for visual sentiment analysis by leveraging several types of prior knowledge including: sentiment lexicon, sentiment labels and visual sentiment strength. To bridge the "affective gap" between low-level image features and high-level image sentiment, we proposed a two-stage approach to general ANPs by detecting mid-level attributes. For model inference, we developed a multiplicative update algorithm to find the optimal solutions and proved the convergence property. Experiments on two large-scale datasets show that the proposed model is superior to other state-of-the-art models in both inferring sentiment and fine grained sentiment prediction. In the future, we will employ crowdsourcing tools, such as AmazonTurk7 , to obtain high-quality, manually-labeled data to test the proposed method. Furthermore, inspired by the recent development of advanced deep learning algorithms and their success in image classification and detection tasks, we will follow this research direction to perform the sentiment analysis via deep learning. In order to have a robust trained architecture and network parameters, we will focus on the deep learning models that work for smaller dataset. Moreover, beyond sentiment analysis, we will study social event and social response (Hu et al. 2012; Hu, Manikonda, and Kambhampati 2014) via visual data in the social media.

6

Acknowledgment

Yilin Wang and Baoxin Li are supported in part by a grant (#1135616) from the National Science Foundation. Kambhampati's research is supported in part by the ARO grant W911NF-13-1- 0023, and the ONR grants N00014-131-0176, N00014-13-1-0519 and N00014-15-1-2027. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.

References
Borth, D.; Ji, R.; Chen, T.; Breuel, T.; and Chang, S.-F. 2013. Large-scale visual sentiment ontology and detectors using adjective noun pairs. In Proceedings of the 21st ACM international conference on Multimedia, 223­232. ACM. Dalal, N., and Triggs, B. 2005. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, 886­893. IEEE. Darwin, C. 1998. The expression of the emotions in man and animals. Oxford University Press. Das Gupta, M., and Xiao, J. 2011. Non-negative matrix factorization as a feature selection tool for maximum margin classifiers. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, 2841­2848. IEEE. De Choudhury, M.; Counts, S.; and Gamon, M. 2012. Not all moods are created equal! exploring human emotional states in social media. In Sixth International AAAI Conference on Weblogs and Social Media.
7

https://www.mturk.com/mturk/welcome

Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, 248­255. IEEE. Ding, C.; Li, T.; Peng, W.; and Park, H. 2006. Orthogonal nonnegative matrix t-factorizations for clustering. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, 126­135. ACM. Ding, C. H.; He, X.; and Simon, H. D. 2005. On the equivalence of nonnegative matrix factorization and spectral clustering. In SDM, volume 5, 606­610. SIAM. Ekman, P. 1992. An argument for basic emotions. Cognition & Emotion 6(3-4):169­200. Felzenszwalb, P. F.; Girshick, R. B.; McAllester, D.; and Ramanan, D. 2010. Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence, IEEE Transactions on 32(9):1627­1645. Go, A.; Bhayani, R.; and Huang, L. Twitter sentiment classification using distant supervision. Han, B., and Baldwin, T. 2011. Lexical normalisation of short text messages: Makn sens a# twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, 368­378. Association for Computational Linguistics. Hofmann, T. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, 50­57. ACM. Hu, M., and Liu, B. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, 168­177. ACM. Hu, Y.; John, A.; Wang, F.; and Kambhampati, S. 2012. Et-lda: Joint topic modeling for aligning events and their twitter feedback. In Proceedings of the 6th AAAI Conference. Hu, Y.; Manikonda, L.; and Kambhampati, S. 2014. What we instagram: A first analysis of instagram photo content and user types. Hu, Y.; Wang, F.; and Kambhampati, S. 2013. Listening to the crowd: automated analysis of events via aggregated twitter sentiment. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, 2640­2646. AAAI Press. Jia, J.; Wu, S.; Wang, X.; Hu, P.; Cai, L.; and Tang, J. 2012. Can we understand van gogh's mood?: learning to infer affects from images in social networks. In Proceedings of the 20th ACM international conference on Multimedia, 857­860. ACM. Lee, D. D., and Seung, H. S. 1999. Learning the parts of objects by non-negative matrix factorization. Nature 401(6755):788­791. Lee, D. D., and Seung, H. S. 2001. Algorithms for non-negative matrix factorization. In Advances in neural information processing systems, 556­562. Liu, B. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies 5(1):1­167. Machajdik, J., and Hanbury, A. 2010. Affective image classification using features inspired by psychology and art theory. In Proceedings of the international conference on Multimedia, 83­92. ACM. Pandey, M., and Lazebnik, S. 2011. Scene recognition and weakly supervised object localization with deformable part-based models. In Computer Vision (ICCV), 2011 IEEE International Conference on, 1307­1314. IEEE.

Pang, B., and Lee, L. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, 271. Association for Computational Linguistics. Pang, B.; Lee, L.; and Vaithyanathan, S. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, 79­86. Association for Computational Linguistics. Plutchik, R. 1980. Emotion: A psychoevolutionary synthesis. Harper & Row New York. Song, M.; Tao, D.; Liu, Z.; Li, X.; and Zhou, M. 2010. Image ratio features for facial expression recognition application. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on 40(3):779­788. Taboada, M.; Brooke, J.; Tofiloski, M.; Voll, K.; and Stede, M. 2011. Lexicon-based methods for sentiment analysis. Computational linguistics 37(2):267­307. Tighe, J., and Lazebnik, S. 2013. Finding things: Image parsing with regions and per-exemplar detectors. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, 3001­ 3008. IEEE. Tu, Z.; Chen, X.; Yuille, A. L.; and Zhu, S.-C. 2005. Image parsing: Unifying segmentation, detection, and recognition. International Journal of Computer Vision 63(2):113­140. Wang, Y.; Jia, Y.; Hu, C.; and Turk, M. 2005. Non-negative matrix factorization framework for face recognition. International Journal of Pattern Recognition and Artificial Intelligence 19(04):495­511. Xiao, J.; Hays, J.; Ehinger, K. A.; Oliva, A.; and Torralba, A. 2010. Sun database: Large-scale scene recognition from abbey to zoo. In Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, 3485­3492. IEEE. Yang, Y.; Jia, J.; Zhang, S.; Wu, B.; Li, J.; and Tang, J. 2014. How do your friends on social media disclose your emotions? You, Q.; Luo, J.; Jin, H.; and Yang, J. 2015. Robust image sentiment analysis using progressively trained and domain transferred deep networks. Yuan, J.; Mcdonough, S.; You, Q.; and Luo, J. 2013. Sentribute: image sentiment analysis from a mid-level perspective. In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, 10. ACM. Zhu, X., and Ramanan, D. 2012. Face detection, pose estimation, and landmark localization in the wild. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, 2879­2886. IEEE.

Where µ is Lagrangian multiplier µij enforces the nonnegativity constraint on Vij . From the complementary slackness condition, we can obtain (2(V S T T T T S + V ) - 2(X T T S + V0 ))ij Vij = 0 (8) This is the fixed point relation that local minima for V must hold. Given the Algorithm 1., we have the convergence point to the local minima when Vij = Vij [X T T S + V0 ]ij [V S T T T T S + V ]ij (9)

Then the Eq (9) is equivalent to
2 (2(V S T T T T S + V ) - 2(X T T S + V0 ))ij Vij = 0 (10)

This is same as the fixed point of Eq (8),i.e., either Vij = 0 or the left factor is 0. Thus if Eq (10) holds the Eq (8) must hold and vice versa.

Theorem 2. The objective function is nondecreasing under the multiplicative rules of Eq (4), Eq (5) and Eq (6), and it will converge to a stationary point. Proof of Theorem 2. First, let H (V ) be: H (V ) = T r((V S T T T T S +V )V T -(X T T S +V0 +µ)V T ) (11) and it is very easy to verify that H (V ) is the Lagrangian function of Eq (3) with KKT condition. Moreover, if we can verify that the update rule of Eq (4) will monotonically decrease the value of H (V ), then it means that the update rule of Eq (4) will monotonically decrease the value of L(V )(recall Eq (3)). Here we complete the proof by constructing the following an auxiliary function h(V, V ). h(V, V ) =
ik 2 (V (V S T T T T S + V ))ik Vik

Vik (X T T S + V0 + µ)ik Vik (1 + log Vik

A

Appendix

Theorem 1. When Algorithm 1 converges, the stationary point satisfies the Karush-Kuhn-Tuck(KKT) condition, i.e., Algorithm 1 converges correctly to a local optima. Proof of Theorem 1. We prove the theorem when updating V using Eq (6), similarly, all others can be proved in the same way. First we form the gradient of L regards V as Lagrangian form: L = 2(V S T T T T S + V ) - 2(X T T S + V0 ) - µ (7) V

) Vi k (12) Since z  (1 + log z ), z > 0 and similar in (Ding et al. 2006), the first term in h(V, V ) is always larger than that in H (V ), then the inequality holds h(V, V )  H (V ). And it is easy to see h(V, V ) = H (V ), thus h(V, V ) is an auxiliary function of H (V ). Then we have the following inequality chain:
ik

-

H (V 0 ) = h(V 0 , V 0 )  h(V 0 , V 1 ) = H (V 1 )....

(13)

Thus, with the alternate updating rule of V, S and T , we have the following inequality chain: L(V 0 , T 0 , S 0 )  L(V 1 , T 0 , S 0 )  L(V 1 , T 1 , S 0 ).... (14) Since L(V, S, T )  0. Thus L(V, S, T ) is bounded and the Algorithm 1 converges , which completes the proof.

1206

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

Relative Hidden Markov Models for Video-Based Evaluation of Motion Skills in Surgical Training
Qiang Zhang, Student Member, IEEE and Baoxin Li, Senior Member, IEEE
Abstract--A proper temporal model is essential to analysis tasks involving sequential data. In computer-assisted surgical training, which is the focus of this study, obtaining accurate temporal models is a key step towards automated skill-rating. Conventional learning approaches can have only limited success in this domain due to insufficient amount of data with accurate labels. We propose a novel formulation termed Relative Hidden Markov Model and develop algorithms for obtaining a solution under this formulation. The method requires only relative ranking between input pairs, which are readily available from training sessions in the target application, hence alleviating the requirement on data labeling. The proposed algorithm learns a model from the training data so that the attribute under consideration is linked to the likelihood of the input, hence supporting comparing new sequences. For evaluation, synthetic data are first used to assess the performance of the approach, and then we experiment with real videos from a widely-adopted surgical training platform. Experimental results suggest that the proposed approach provides a promising solution to video-based motion skill evaluation. To further illustrate the potential of generalizing the method to other applications of temporal analysis, we also report experiments on using our model on speech-based emotion recognition. Index Terms--Relative hidden markov model, relative learning, temporal model, emotion recognition, surgical skill

Ç
1 INTRODUCTION
and quantifiable performance metrics, overcoming the shortcomings in traditional training that relies on costly practice of direct supervision by senior surgeons. Recognizing the sequential nature of motion data, many analysis approaches utilize state-transition models, such as the Hidden Markov Model (HMM). For example, [5] provided an HMM-based method to evaluate surgical residents' learning curve. The method first constructs different HMMs for each different levels of expertise, and then calculates a probability distance between the expert and a novice resident. The magnitude of the probability distance is used to rate the level of the novice resident. HMM was also adopted in [6] to measure motion skills in surgical tasks, where a recorded video is first segmented into basic gestures based on velocity and angle of movement, with segments of the gestures corresponding to the states of an HMM. In [7], Hierarchical Dircichlet process hidden Markov model (HDPHMM [8]) was utilized, which relaxed the requirement of predefining the number of the states for the model. One practical difficulty in these approaches is that they require the skill labels for the training data since the HMMs are typically learned from sets of data streams with corresponding skill levels. Labeling the skill of a trainee is currently done by senior surgeons, which is not only a costly practice but also one that is subjective and less quantifiable. Thus it is difficult, if not impossible, to obtain a large amount of data with sufficiently reliable skill labels for HMM training. This problem has also been encountered in other fields such as image classification. For example, in [9], it was argued that using binary labels to describe images is not only too restrictive but also unnatural and thus relative visual attributes were used and classifiers were trained based on such features. Relative information has also been used in other applications, e.g., distance metric learning [10], face verification [11], and human-machine interaction [12].

UMAN capability in mastering body motion is the key in domains such as sports, rehabilitation, surgery and dance. Computer-based approaches have been developed over the years for facilitating acquiring (e.g., training in sports and surgery) or regaining (e.g., in rehabilitation) such motion-related skills by human subjects. One central task faced by systems using such approaches is the analysis of motion skills based on some temporal sensory data. With such analysis, skill metrics may be extracted and assigned to a given movement and feedback may accordingly be provided to the subjects for taking actions to improve the underlying skill. For example, [1] utilized control trajectories and motion capture data for human skill analysis, [2] reported motion skill analysis in sports using data from motion sensors, [3] studied computational skill rating in manipulating robots, and [4] considered hand movement analysis for skill evaluation in console operation. Among others, surgery-related applications have attracted increasing interests, where motion expertise is the primary concern. To improve their motion expertise, surgeons often have to go through lengthy training processes. In recent years, simulation-based surgical training platforms have been developed and widely applied in surgical education. One prominent example is the Fundamentals of Laparoscopic Surgery (FLS) Trainer Box (www.flsprogram.org). With such platforms, it is possible to develop computational approaches to provide objective

H



The authors are with the Computer Science and Engineering, Arizona State University, Tempe AZ 85287. E-mail: {qzhang53, baoxin.li}@asu.edu.

Manuscript received 24 Feb. 2014; revised 21 Sept. 2014; accepted 25 Sept. 2014. Date of publication 1 Oct. 2014; date of current version 8 May 2015. Recommended for acceptance by D. Xu. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TPAMI.2014.2361121

0162-8828 ß 2014 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1207

In this paper, we propose a novel formulation termed Relative Hidden Markov Model and develop an algorithm for obtaining a solution under this model. The proposed method utilizes only relative ranking (based on certain attribute of interest, or motion skill in the surgical training application) between pairs of inputs, which is easier to obtain and often more consistent. This is especially useful for applications like video-based surgical training, where the trainees go through a series of training sessions with their skills improving over time, and thus the time of the sessions would already provide natural relative ranking of the skills at the corresponding time. The proposed algorithm effectively learns a model from the training data so that the attribute under consideration (i.e., the motion skill in our application) is linked to the likelihood of the inputs under the learned model. The learned model can then be used to compare new data pairs. For evaluation, we first design synthetic experiments to systematically evaluate the model and the algorithm, and then experiment with real data captured on a commonly-used surgical training platform. The experimental results suggest that the proposed approach provides a promising solution to the real-world problem of motion skill evaluation from video. The key contribution of the work lies in the novel formulation of learning temporal models using only relative information and the proposed algorithm for obtaining solutions under the formulation. Discussion of its relationship to the latent support vector machine is also provided to assist the understanding of why the proposed formulation is suitable for the proposed scenarios. Additional contributions include the specific application of the proposed method to the problem of video-based motion skill evaluation in surgical training, which has seen increasing importance in recent years. An earlier exposition of the proposed method can be found in [13]. This current paper represents a full exploration of the method, including a new learning algorithm that is more efficient, new comparative analysis of the method, and new and updated experiments. In particular, to illustrate that the proposed model is general in nature but not confined to video-based skill analysis, we report its application to a different problem, emotion recognition using speech. To facilitate further exploration and validation by other researchers, source code accompanying this paper has been made publicly available.1 In the remainder of this paper, we first review some of the related work in Section 2 and describe basic notations of the HMM in Section 3. The proposed method is then presented in Section 4, including a new algorithm for obtaining solutions in Section 4.3 and discussion of its relationship to latent support vector machine in Section 4.4. The proposed method is evaluated on three types of data in Section 5, including synthetic data (Section 5.1) and videos from surgical simulation systems (Section 5.2), and speech data (Section 6). The paper is concluded in Section 7. In this paper, we use upper-case bold font (e.g., X) for matrices, lower-case bold font (e.g., x) for vectors. We use Xi to reprei sent ith sequence, Xi t for the tth frame of sequence X .

2

RELATED WORK

1. The code is available at www.public.asu.edu/~bli24/Code SoftwareDatasets.html.

In this section, we first review two categories of existing work, discriminative learning for hidden Markov models and learning based on relative information, which are most related to our approach. Distinction between our proposed method and the reviewed work will be briefly stated. We also briefly discuss a few more related efforts on skill evaluation in surgery. Discriminative learning for HMM. Maximum-likelihood methods for learning HMM (e.g., the forward-backward algorithm) in general do not guarantee the discrimination ability of the learned models. To this end, several discriminative learning methods for HMM have been proposed. In [14], a discriminative training method for HMM was proposed based on perceptron algorithms. The methods iterates between the Viterbi algorithm and the additive update of the models. Hidden Markov support vector machine (HM-SVM) was proposed in [15], which combines SVM with HMM to improve the discrimination power of the learned model. These methods are "supervised" in nature, and thus the labeling of the state sequence is required for the training data, which limits their practical use. In [16], another discriminative learning method for HMM was proposed, which only requires the labels of the training sequences. The method initializes the HMMs with maximumlikelihood method and then updates the models with SVM. One drawback is that, the updated models do not always lead to valid HMMs, which could be problematic for a physics-driven problem where the model states have real meanings (like the gesture elements in [6]). Our proposed method requires neither the labeling of the states nor the class label for the training sequences, which are difficult to obtain or even not accessible in many applications. Instead, only a relative ranking of the training data is used, and the resultant model is a valid HMM. Learning with relative information. Several methods for learning with relative information have been proposed recently. In [10], a distance metric is learned from relative comparisons. Considering the limited training examples for object recognition, [17] proposes an approach based on comparative objective similarities, where the learned model scores high for objects of similar categories and low for objects of dissimilar categories. In [11], comparative facial attributes were learned for face verification. The method of [9] learns relative attributes for image classification and the problem is formulated as a variation of SVM. Similar idea was also been used in [12] for the purpose of human-machine interaction. In [18], relative attribute feedback, e.g., "Shoe images like these, but sportier", is used to improve the performance of image search. Relative information between scene categories has also been used to enhance the performances of scene categorization in [19]. These approaches are mostly for image-based attributes, whereas our current task is on modeling sequential data, for which it is natural to assume that the most relevant attributes (e.g., motion skills) are embedded in a temporal structure. This is what our proposed method attempts to address. Efforts has been observed for estimating the true continuous label of the data from a set of pairwise ranking of training data [20], [21]. However,

1208

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

those methods do not directly learn a model for ranking/ labeling new data. Skill evaluation for surgical simulations. Objective evaluation of surgical skills has been a topic of research for many years. The authors of [22], [23] used the time of each data, total path traveled and the number of hand movements to rate the surgical skills. It is evident that some of the criteria recommended in these studies (e.g., time of completion) may be relatively easily measured with proper sensory data, while some others cannot be (e.g., respect for tissues). A technique proposed in [24] called task deconstruction was implemented in a recent system by [25]. They used Markov Models to model a sequence of force patterns or positions of the tools. They showed that their Markov Models were suitable for decomposing a task (such as suturing) into basic gestures, and then the proficiency of the complex gesture could be analyzed. While this study offered an intriguing approach to expertise analysis, it required an expert surgeon to provide specifications for building the topology of the model; hence it cannot be easily generalized to new procedures. A similar idea was also utilized in [26]. Jun et al. [27] proposed to segment the training data into modular sub-procedures or therbligs and performance is measured over each sub-procedure.

4

PROPOSED METHOD

Based on the previous discussion, we are concerned with a new problem of learning temporal models using only relative information. This is a problem arising naturally in many applications involving motion or video data. In the case of video-based surgical training, the focus is on learning to rate/compare the performance of the trainees from recorded videos capturing their motion. To this end, in recognition of some fruitful trials of HMMs in this application domain, we propose to formulate the task as one of learning a Relative Hidden Markov Model, which not only maximizes the likelihood of the training data, but also maintains the given relative rankings of the input pairs. In its most basic form, the proposed model can be formally expressed as (following the notations defined in Eqn. (1)) u s:t: : max
u

Y

pðXi juÞ
j

F ðX ; uÞ > F ðX ; uÞ; 8ði; jÞ 2 E;

Xi 2 X i

(2)

3

BASIC NOTATIONS OF HMM

In this section, we briefly describe HMM and introduce some basic notations that will be used later. An HMM can be defined by a set of parameters: the initial transition probabilities p 2 RK Â1 , the state transition probabilities A 2 RK ÂK and the observation model ffk gK k¼1 , where K is the number of states. There are two central problems in HMM: 1) learning a model from the given training data; and 2) evaluating the probability of a sequence under a given model, i.e., the decoding problem. In the learning problem, one learns the model (u) by maximizing the likelihood of the training data (X): uÃ : max
u

where F ðX; uÞ is a score function for data X given by model u, which is introduced to maintain the relative ranking of the pair Xi and Xj and E is the set of given pairs with prior ranking constraint. Different score functions may be defined, e.g., data likelihood and data likelihood ratio, as described in the following sections in Section 4.1 and Section 4.2. From this formulation, the difference between the proposed method and any of the existing HMM-based methods is obvious. In an existing HMM-based method, a set of models is trained using the training data of each category independently. That is, explicit class labels are required for each training sequence. The proposed model has the following unique features: The model does not require explicit class labels. What needed is only a relative ranking.  The model explicitly considers the ranking constraint between given data pairs, whereas independently-trained HMMs in existing methods cannot guarantee it.  Only one model is learned for the entire set of data. There are two benefits: more data for training and less computation during testing. Our method is also different from the existing work on learning with relative attributes in that it models sequential data and the relative ranking information is capsulated in a temporal dynamic model of HMM (albeit new algorithms are thus called for), which has demonstrated performance in modeling physical phenomena like human movements. In the following sections, we present two instantiations of the general model expressed in Eqn. (2), and develop the corresponding algorithms in each case. It will become clear that the first model (Section 4.1), while being intuitive, has some practical difficulties, which motivated us to develop the improved model of Section 4.2. Both models/algorithms are presented (and evaluated later in Section 5) for the progressive nature of the methods and for facilitating the understanding of the improved model and algorithm of Section 4.2, which is the recommended solution. 

Y
Xi 2 X

pðXi juÞ $ max
u

X
Xi 2X

log pðXi juÞ;

(1)

where X is the set of i.i.d. training sequences. One efficient solution to the above problem is the wellknown Baum-Welch algorithm [28]. Another scheme, namely the segmental K-means algorithm [29], may also be used to seek a solution, and it has been shown that the likelihoods under models estimated by either of the two algorithms are very close [29]. When the training data include sequences of multiple categories, multiple models would be learned and each model will be learned from data of each category independently. In the decoding problem, given a hidden Markov model, one needs to determine the probability of a given sequence X being generated by the model. Generally we are more interested in the probability associated with the optimal state sequence (zÃ ), i.e., pðX; zÃ juÞ ¼ maxz pðX; zjuÞ. The optimal state path can be found via the Viterbi algorithm. To use HMM in classification, we first compute the probability of the given sequence drawn from each model, then we choose the model yielding the maximal probability.

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1209

4.1 The Baseline Model While one may use different score functions for F in Eqn. (2) for comparing the input pairs, upon successful training the likelihoods of the sequences should reflect the original ranking. Hence we may set F ðXi ; uÞ ¼ pðXi juÞ. With this, the formulation in Eqn. (2) can be rewritten as Y u : max pðXi juÞ u i X 2X (3) s:t: pðXi juÞ > pðXj juÞ; 8ði; jÞ 2 E:
It has been proven in [30] that, the marginal likelihood is dominated by the likelihood with the optimal path and their difference decreases exponentially with the length (number of frames) of a sequence. This idea was used in segmental K-means algorithm and similarly we can approximate the marginal data likelihood pðXjuÞ by the likelihood with optimal path pðX; zÃ juÞ (when there is no ambiguity, we will use z for zÃ ), which can be written as: log pðX; zjuÞ ¼ log pðX1 jfz1 Þ þ log pðz1 Þ þ
T Â X t¼2

we evaluate the data likelihood via the Viterbi algorithm and use the logarithm of the data likelihood as the score of the data. By definition, the obtained scores can be used to compare the pair.

log pðXt jfzt Þ þ log Aðzt jztÀ1 Þ

(4)

For some observation models, e.g., multinomial (more details in Appendix A), we can write log pðXi ; zi juÞ ¼ uT hðXi ; zi Þ. Accordingly, Eqn. 3 can be finally written as X u : max uT hðXi ; zi Þ u2V i:Xi 2X (5) T i i T j j s:t: u hðX ; z Þ ! u hðX ; z Þ þ r; 8ði; jÞ 2 E; where r ! 0 defines the required margin between the logarithms of likelihood for a pair of data and V defines the set of valid parameters for the hidden Markov model, i.e.: X euðiÞ ¼ 1; uðiÞ 0 ; X
i:uðiÞ2logðAj Þ i:uðiÞ2logðpÞ

4.2 The Improved Model In the model described in Eqn. (7), we compare the logarithm of the data likelihood, which is, according to Eqn. (4), roughly proportional to the length of the data. Thus a shorter sequence is likely to have a larger score. This means that the learned model would be biased towards shorter sequences. If the observation describes a long, periodic event, e.g., repeating an action multiple times within a sequence, we may consider normalizing the logarithm of the data likelihood by the number of frames of the observation. However, this cannot be applied directly for non-periodic observations like sequences from surgical simulation, where the length of a sequence (corresponding to the time taken for completing a task) is one of the skill metrics. To overcome the above practical problem, we consider an improved version. Recall that in HMM, we classify a sequence based on the model with which the sequence gets the maximal likelihood, i.e., it is the ratio of data likelihood with different models that decides the label of the data. For ^ju1 Þ ðX ; z example, if log p ~ju2 Þ > 0, then we assign X to Model u1 . pðX;z Thus we propose to use the ratio of the data likelihoods of ðX ; ^ zju1 Þ two HMMs as the score function, i.e., F ðX; uÞ ¼ log p pðX;~ zju2 Þ, where we "partition" the original model into two models (or, effectively, we train a pair of HMMs simultaneously). This results in the following improved model:
u 1 ; u2 : max
u1 ;u2

X
i2X1

^i ju1 Þ þ log pðXi ; z ij

X
j2X2

~j j u 2 Þ log pðXj ; z

Àg s:t: log

X

ði;jÞ2E

e

uðiÞ

¼1;

X

euðiÞ ¼ 1;

(6)

^i ju1 Þ ^j ju1 Þ pðXi ; z pðXj ; z À log þ ij ! r j j j j ~ ju2 Þ ~ ju2 Þ pðX ; z pðX ; z ij ! 0; (8)

i:uðiÞ2logðfj Þ

where i : uðiÞ 2 logðAj Þ is the set of the indexes which corresponds to the jth row of matrix A. For the model in Eqn. (3), we assumed that every pairwise ranking constraint provided in the data is correct (or valid). However, in real data, there may be outliers in such training pairs. To handle this, we further introduce some slack variables  and h, and accordingly Eqn. (5) can be written as following: X X hðXi ; zi Þ À g ij u : max uT
u 2V T

where X1 is the set of data associated with Model u1 (X2 for ^i is the optimal path for sequence xi with Model Model u2 ), z i z for optimal path with Model u2 . u1 and ~
pðX ;z ^ ju 1 Þ i i i i ^ Þ À uT ~ Þ, we can ¼ uT With log p 1 hðX ; z 2 hðX ; z ~j ju2 Þ ðXj ;z rewrite the model in Eqn. (9) (similar to Eqn. (7)):
i i

"P u s:t: : max uT P
u 2V i2X1

^i Þ hðXi ; z ~j Þ hðX ; z
j

# Àg

X
ði;jÞ2E

ij (9)

"

u

T

(7) u ½hðX ; z Þ À hðX ; z Þ þ ij ! r; 8ði; jÞ 2 E ij ! 0; P where g is the weight for the penalty term ði;jÞ2E ij . For initialization, we can set ij ¼ 0. We will defer the optimization algorithm for Eqn. (7) to Section 4.3. After the model is learned, it can be used to a testing pair: For each sequence s:t:
i j j

Xi 2X i

ði;jÞ2E

# ^i Þ À hðXj ; z ^j Þ hðXi ; z þ ij ! r ~j Þ À hðXi ; ~ zi Þ hðXj ; z

j2X2

ij ! 0;
T T where u ¼ ½uT 1 ; u2  . The optimization algorithm for Eqn. (9) will be presented in Section 4.3. After we learn the model with the improved algorithm, we can apply it to a given pair by first computing their likelihoods with respect to the

1210

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

"sub-models" given by u1 and u2 (with the Viterbi algorithm), and then we use the logarithm of the ratio of the data likelihoods as the score to rank/compare the pair. The learned models u1 and u2 serve as a unified model to rank the data. We may view them as the centers of two clusters, where the distances of the data to those two centers can be related to the ranking score. It needs to be emphasized that the improved model is not equivalent to a supervised HMM with two classes. In a 2-class HMM setting, two models will be independently trained with their respective training sets. Here, the proposed model trains two "sub-models" jointly with only relative ranking constraints. Specifically, if there is no further information for X, we could assume that X1 ¼ fijði; jÞ 2 E; 8jg and X2 ¼ fjjði; jÞ 2 E; 8ig, and thus there could be overlaps between X1 and X2 (which will become clear in the experiment with synthetic data in Section 5). This situation is not even allowed by a supervised HMM setting. We do not require any extra properties for X1 and X2 .

According to Eqn. (11), f will be a valid hidden Markov model (or hidden Markov model pairs ½f1 ; f2  for the improved model). We then apply the Augmented Lagrange multiplier method to the equality constraint log f ¼ u of the problem in Eqn. (11): u; ; f : min f T u þ g 1T þ
u;;f

< ; u À log f > þ s:t: : Au þ  Cf ¼ 1 u r f

m ku À log fk2 2 2

(12)

0;  ! 0; 0

1;

where  is the Lagrange multiplier and m is some nonnegative constant. In Eqn. (12), the nonlinear equality constraint is removed. Eqn. (12) can be solved via block coordinate descent by iterating between the following two sub-problems: Sub-problem 1: fix f to solve u and , which is u;  : min f T u þ g 1T þ
u;;f

4.3 Algorithms for Updating the Model One important step of both the baseline algorithm and the improved algorithm is updating the models, as formulated in Eqn. (7) and Eqn. (9) accordingly. It is a nonlinear programming problem (due to the nonlinear equality constraint). In our previous paper, we solved it by the primaldual interior point method, which is of dimension K ð1 þ K þ DÞ þ jEj (or 2K ð1 þ K þ DÞ þ jEj) with 2jEj þ K ð1 þ K þ DÞ (or 2jEj þ 2K ð1 þ K þ DÞ) linear inequality constraints and 1 þ K þ D (or 2ð1 þ K þ D)) nonlinear equality constraints for the baseline model (or the improved model). Although the Hessian matrix is diagonal, the computational cost could be still very high when there are a large number of training pairs. In this section, we propose a new algorithm by utilizing the special structure of the problems in Eqn. (7) and Eqn. (9). Eqn. (7) (similarly for Eqn. (9)) can be written in the following form:
u;  s:t: : min f T u þ g 1T 
u;

< ; u À log f > þ s:t: : Au þ  u r 0;  ! 0:

m ku À log fk2 2; 2

(13)

It is a quadratic programming problem with linear inequality constraints. Sub-problem 2: fix u and  to solve f, which is m f : min < ; u À log f > þ ku À log fk2 2 f 2 (14) Cf ¼ 1 0 f 1: It is a nonlinear problem with linear constraints. Given the special structures of C, where each column has one and only one element being nonzero (recall Eqn. (6)), Sub-problem 2 can be separated into a set of smaller problems: m fk : min < k ; uk À log fk > þ kuk À log fk k2 2 2 fk (15) 1T fk ¼ 1 0 fk 1;

: Au þ  Ce ¼ 1
u

r

(10)

u

0 ;  ! 0:

P For example, for Eqn. (7), we have f ¼ À Xi 2X hðXi ; zi Þ, A and C are constructed according to Eqns. (7) and (6). Eqn. (10) is a nonlinear programming problem (due to the nonlinear equality constraint). To solve this problem, we first introduce a slack variables f, where log f ¼ u. Then Eqn. (10) can be rewritten into the following problem: u; ; f s:t: : min f T u þ g 1T 
u;;f

where k is the set of indexes of columns, whose values are nonzero at the kth row of C. Those smaller problems are again a nonlinear problem with linear constraint, whose dimensions are only K (number of states) or D (number of feature dimensions). To solve this problem we can use the primal-dual interior point method, whose gradient and hessian are computed as Àk þ mk log fk À mk uk ; fk  k   À m log fk þ muk þ m ; H¼L fk Á fk J¼ where LðÁ Á ÁÞ converts a vector to a diagonal matrix. In addition, we can compute the starting point of the problem in Eqn. (15) as: by taking the gradient of the objective function

: Au þ  Cf ¼ 1

r (11) f 1:

log f ¼ u u 0;  ! 0; 0

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1211

Fig. 1. The experiment result with different numbers of states: (a) the computational time (blue solid curve) and number of iterations needed for convergence (green dashed curve); (b) the accuracy of the improved method. The X-axis is the number of states.

Fig. 2. The accuracy of the improved method: (a) with different g (r is fixed to 10), which controls the weight of the penalty term with slack variables; (b) with different r (g is fixed to 1; 000), which controls the margin of the learned models.

with regard to log fk , we have Àk þ mðlog fk À uk Þ ¼ 0, i.e., fk ¼ eðu . The linear constraint can be solved simply by k P ðuk þk Þ 1 ðu k þ  m Þ , where N ¼ m . e e projection, i.e., fk ¼ N
k þk Þ m

Algorithm 1. Algorithm for the Baseline (Improved) Model
Input: X, E, r, g , s (, X1 and X2 ) Output: f Initialization: Initialize f (or f1 and f2 ) via ordinary HMM u 1:25 learning algorithm,  ¼ log juj and m ¼ juj ;
2 2

below (noting the similarity in form of the algorithms and thus putting them compactly together): According to [31], the proposed method will converge to the local minimum of the problem in Eqn. (10). And for kuÀlog fk convergence, we check kuk 2 . If it is smaller than some 2 À6 value, e.g., 10 , the algorithm will be terminated. In initialization, juj2 is the vector L2 norm of of u. Remarks on the Parameters. The parameter g controls the weight of the penalty term with the slack variables, which is similar to the functionality of C in support vector machines [32]. The parameter r controls the desired gap of the score of two data points, i.e., line model and
pðXi ;zi juÞ ! er 8ði; jÞ 2 E pðXj ;zj juÞ pðXi ;z ^i ju1 Þ pðXi ;z ~j ju 2 Þ ! er 8ði; jÞ ~i ju2 Þ pðXi ;z ^j juÞ pðXi ;z

while not converged do ^ and z ~) for each sequence Compute the optimal path z (or z with f (or f1 and f2 ); solve Sub-problem 1; solve Sub-problem 2; update  ¼  þ mðu À log fÞ and m ¼ m Â s ; check convergence; end while

in the base2 E in the

Finally, we briefly summarize the algorithms for the baseline model (Eqn. (7)) and the improved model (Eqn. (9))

improved model. In Section 5.1, we will evaluate different parameter settings (Fig. 2), which leads us to set g ¼ 1; 000 and r ¼ 10 in our final experiments. The parameter s controls the convergence speed of the algorithm, which should be a positive number larger than 1. s is typically within 1:1 À 1:5, and 1:25 is used in this paper. The proposed algorithm, compared with the one used in [13], has lower computational cost, due to the removal of

1212

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

TABLE 1 Comparing the Method in [13] and the Proposed Method for Updating the Baseline Model, with Regarding to the Problem Size, Number of Linear Constraints and Nonlinear Constraints Method in [13] Problem Size # Linear Const. # Nonlinear Const. K ð1 þ K þ DÞ þ jEj 2jEj þ K ð1 þ K þ DÞ 1þKþD Proposed Method Sub-problem 1 K ð1 þ K þ DÞ þ jEj 2jEj þ K ð1 þ K þ DÞ 0 Sub-problem 2 K(or D) 1+2K(or 1+2D) 0

For Sub-problem 2 of the proposed method, it can be divided into several smaller problems.

the nonlinear equality constraint with augmented Lagrange multiplier. For Sub-problem 1, the quadratic term is a diagonal matrix and many solvers (e.g., CPLEX) can solve it quite efficiently. Sub-problem 2 is a nonlinear minimization problem with linear equality constraints; however, it can be decomposed into several smaller problems. A comparison between the method in [13] and the proposed method for updating the baseline model is shown in Table 1. In Section 5.1, we will also compare the computational time of those two methods under varying E on synthetica data (Fig. 6).

4.4 Relationship to Existing Methods The proposed method is related to latent support vector machine [33]. Given a training set of input-output pairs fðxi ; yi Þgn i¼1 , where yi 2 fÀ1; 1g, Latent SVM tries to learn a predictor of the form:
fw ðxÞ ¼ max wT Cðx; zÞ;
z

(16)

valid hidden Markov model while defining a fixedmargin, i.e., r. Thus the proposed method can always guarantee the learned model is a valid hidden Markov model. 2) In Eqn. (18), the two state sequences z (i.e., the latent variables) are optimized jointly, where no known efficient solution is available. In the proposed method, the two state sequences are optimized separately with regarding to the likelihood, which can be solved efficiently via dynamic programming (i.e., the Viterbi algorithm); 3) Given the model learned by the latent SVM, we can only rank a pair of sequences. However, the model learned by the proposed method is capable of not only ranking a pair of sequences but also assigning a score for each sequence. Those differences make the proposed method (both the baseline model and the improved model) more suitable for modeling the sequential data, e.g., video, speech.

where w is the parameter of the predictor, Cðx; zÞ is the feature mapping function and z is the latent variable. The training stage of Latent SVM can be formulated as the following problem: X 1 maxð0; 1 À yi fw ðxi ÞÞ: (17) min kwk2 2þC w 2 i Latent SVM is a non-convex problem, as the latent variable is unknown, and the coordinate descent approach is used for solving this problem. L R Given a training set fðxi ; yi Þgn i¼1 , where xi ¼ ðxi ; xi Þ is a pair of sequences and yi 2 fÀ1; 1g is the ranking of the pair, by defining the feature mapping function as Cðxi ; zi Þ ¼ L R R L R ½hðxL i ; zi Þ À hðxi ; zi Þ, with the latent variable zi ¼ ðzi ; zi Þ R being a pair of state sequences for the pair xi ¼ ðxL i ; xi Þ, we have X 1 min kwk2 i 2þC w 2 i È Â À Á À R R ÁÃÉ L þ i ! 1 (18) s:t: yi max wT h xL i ; zi À h xi ; zi
zL ;zR i i

5

EXPERIMENTS

In this section, we evaluate the proposed methods, including the baseline method and the improved method, using both synthetic data (Section 5.1) and realistic data collected from the surgical training platform FLS box (Section 5.2). The performance of the proposed methods is compared with a supervised 2-class HMM. (Lacking a comparative approach in the literature that is both unsupervised and works with only relative rankings, this is believed to be a reasonable way of generating a reference point to assess the proposed methods.) Since we do not have the label information for the training data, we train the HMM as follows. For the HMM algorithm, we initialize the two sets as X1 ¼ fijði; jÞ 2 E; 8jg and X2 ¼ fjjði; jÞ 2 E; 8ig. Each of the sets is then used to train a HMM. Note, the data generated from data-generating Models u2 $ u5 could be included in both X1 and X2 . Thus existing discriminative learning methods for HMM could not be applied here.

i ! 0 : We can find that Eqn. (18) is similar to our baseline model (Eqn. (7)), except for the following differences. 1) In Eqn. (18), the L2 norm is applied to the parameter of the predictor w (which is related to the margin). In the proposed methods we require w to be a

5.1 Evaluation with Synthetic Data To evaluate the proposed method, we generate synthetic data as follows. We first generate six different HMMs (u1 to u6 , referred as data-generating models), from each of which we draw 200 sequences, with the length being uniformly distributed between 80 to 120. Each data-generating model has five states. For the sequences from each data-generating model, we randomly assign 50 of them to the training set

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1213

Fig. 3. The results of four methods on training set (dashed curve) and testing set (solid curve) with different numbers of training pairs.

and the remaining to the testing set. We assume there exists a score function such that F ðXi Þ > F ðXj Þ if and only if Xi $ uk , Xj $ ul and k < l. That is, the sequences from a data-generating model with a lower index are viewed to have a higher score (or ranking) than those from a datagenerating model with a higher index. A set of pairs fði; jÞjXi $ uk ; Xj $ ukþ1 ; k ¼ 1; . . . ; 5g are then formed accordingly, some of which are then randomly selected as the training pairs E. We use the proposed methods and also HMM to learn models from the training pairs. The learned models are then used to evaluate the testing set, i.e., how many testing pairs that they rank the same as the ground truth. The result of the methods with different numbers of training pairs is summarized in Fig. 3, where due to the computational time it takes, we do not have the results for the baseline method when there are more than 3;750 training pairs. From Fig. 3, we can find that the improved method achieves the best results on both the training set and the testing set; and the HMM method gives the worse result. In addition, the performance of both of the proposed methods stabilized after certain number of training pairs. However the performance of the HMM method drops dramatically when the number of training pairs reaches about 6;250. It can be explained by that the two HMMs share a lot of common data (for those generated by u2 $ u5 ) and the models are trained independently without considering their discrimination ability. Normalizing the logarithm of the data likelihood does not improve the performance of baseline method, which could be explained by that, all the sequences have roughly the same length, i.e., 80 $ 120. Fig. 4 shows the logarithm of the data likelihood ratio with the models learned by the improved method, when about 1; 250 training pairs are provided. This clearly demonstrates that, although we formed the training pairs only with data from data-generating models of adjacent indexes (i.e., i and i þ 1), the learned model is able to recover the strict ranking of the original data. We can also try to classify the data into six models, by thresholding the logarithm of data likelihood ratio, where, for the model learned with the improved method, the classification accuracy is 86:44 and 98:60 percent for testing and training respectively.

Fig. 4. The logarithm of the data likelihood ratio with the models learned by the improved method. Top: the result for the testing set. Bottom: the result for the training set. The data are grouped (as the section partitioned by the red lines) according to the data generation model from which they are synthesized.

Convergence and Speed. For empirically understanding the convergence behavior of the improved method, we plot in Fig. 5 the objective value in the model as a function of the number of iterations. We can find that the improved method converges fairly quickly (within about 14 iterations) and the value of the objective function monotonically increases. We also compare the computational time of the optimization method in [13] (shown as the red/upper curve) and the proposed optimization method (in Section 4.3 and shown as the green/lower curve) in solving the improved model under varying number of training pairs in Fig. 6. In [13], a primal-dual interior point method is utilized to update the model; while in this paper, we design an augmented Lagrange multiplier method which utilizes the special structure of the objective function of the problem. From the plot, we can find that the proposed optimization method has a much lower computational cost than the one proposed in [13]. Parameter Selection. To understand the effect of parameters to the performances of the improved method, including accuracy and computation cost, we evaluate it with

Fig. 5. The convergence behavior of the improved method, where around 1; 250 training pairs were used. The blue curve/axis shows the value of the objective function, and the green curve/axis shows the number of constraints satisfied.

1214

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

Fig. 6. The computation time for solving the improved model with the method proposed in [13] (red/upper curve) and the method proposed in Section 4.3 (green/lower) under varying number of training pairs. For illustration purpose, we use log-log plot, where X-axis is the number of training pairs (from around 125 to around 9; 000) and Y-axis is the computation time in unit second (from about 20 to around 6; 000). The time is measured in Matlab on a dual-core PC platform.

varying combination of parameters. First we learn the model with varying numbers of states (K ), from 6 to 30. The result is shown Fig. 1. From Fig. 1b, we can find that, though the accuracy for the training data increases with the number of states, the accuracy for testing doesn't following this trend, which indicates a potential risk of overfitting. The computational time and number of iteration until convergence get minimum when the number of states is 11-13. We also do experiment with different combinations of g (controlling the weight of the penalty term with slack variables) and r (controlling the margin of the model), where the experiment result is shown in Fig. 2. From this experiment we can find that, g 2 ½1; 1;000 and r 2 ½4; 32 are good choices. It is obvious from this experiment that the sequences are different from (or similar to) each other only because they are from different (or the same) data-generating models, whereas their relative ranking can be arbitrarily defined. In the end, the proposed methods will learn a temporal model to reflect the defined rankings. This suggests that, as long as we can assume there are some data-generating models for the given sequential data, we can use the proposed methods to learn a relative HMM. This is the basis for applying the approach to the surgical training data in the following section, where it is reasonable to assume that movement patterns of subjects with different skill levels may be modeled by different underlying HMMs while the ranking can be based on the time of training, which reflects the skill level of the subject at the time.

5.2 Skill Evaluation Using Surgical Training Video We now evaluate the proposed method using real videos captured from the FLS trainer box, which has been widely used in surgical training. The data set contains 546 videos captured from 18 subjects performing the "peg transfer" operation, which is one of the standard training tasks a resident surgeon needs to perform and pass. The number of frames in each video varies from 1; 000 to 6; 000 (depending on the trainees' speed in completing a training session). The

data set covers a training period of four weeks, with every trainee performing three sessions each week. In the training, the subject needs to lift six objects (one by one) with a grasper by the non-dominant hand, transfer the object midair to the dominant hand, and then place the object on a peg on the other side of the board. Once all six objects are transferred, the process is reversed, and the objects are to be transferred back to the original side of the board. The videos capture the entire process inside the trainer box, showing how the tools and objects are moved by the subject. The motion skill is related to how well the subjects perform in such operation. In the existing practice, senior surgeons rate the performance of the trainees based on such videos. Our goal is to perform the rating automatically with the proposed model. Based on the reasonable assumption that the trainees improve their skills over time (which is the whole point of having the resident surgeons going through the training before taking the exam), the time of recording is used to rank the recorded videos within each subjects' corpus (i.e., a later video is associated with a better skill). Other than this relative ranking, there are no other labels assumed for the video, e.g., there is no rank information between videos of different subjects (which would be hard to obtain anyway, since there is no clearly-defined skill levels for a group of trainees with diverse background). Based on this, we randomly pick 300 pairs for training, similar to the experiment using synthetic data. Feature Extraction. We use the "bag of words" approach for feature extraction from the videos as follows. The spatiotemporal interest point detector [34] is applied to obtain the histogram-of-gradient (HoG) features, which was found to be useful in target application in the literature [35]. K-means (k ¼ 100) is then used to build a code book for the descriptors of the interest points. Finally, the code book is used to obtain a histogram of interest points for each frame, and thus each video is represented as a sequence of histograms. This representation, compared with the existing way of using bag of words in action recognition, i.e., transforming each video into a single histogram, can better capture the temporal information of the data. For all three methods, we set the number of states to ten. After learning the models from the training data, we compute the score of the test data as the logarithm of data likelihood (for the baseline method) or the logarithm of the data likelihood ratio (for the improved method and the HMM). We compare these scores for each pair of the testing data (within each subject) and compute the percentage of correctly labeled pairs (recall that, we use their time of recording as ground truth). To demonstrate the advantage of the proposed method, we also compare with the "relative attribute" method [9] (referred as "SVM" in the following discussions), which relies on ranking SVM. For "relative attribute", we represent each video as a histogram by accumulating the sequence of histograms of the video along the temporal direction. The result is summarized in Table 2, where the improved method obtained a significantly better result than the other approaches, including "relative attribute". Surprisingly, the baseline method even performed slightly worse than the HMM method. This is largely due to the wide range of

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1215

TABLE 2 The Result for Experiment on Evaluating Surgical Skills Method # Pairs Accuracy SVM 6335 78:91% HMM 6363 79:39% Baseline 6215 77:54% Improved 6993 87:25%

There are 8;015 pairs in total (only 300 for training), excluding the comparisons among data of different subjects.

variations of the length of the input sequences. Fig. 7 shows the computed scores with the learned models, where for better illustration purpose we group them by their subject ID and within each subject's corpus we sort the videos by their recording time. From the figure, we can find that the improved method (bottom) reveals a more clear trend for the data than both the HMM method (top) and the baseline method (middle), i.e., the scores of the data increase over times (X-axis) for each subject (segments within the red lines). It is worth emphasizing that only one joint model is learned from ranked pairs of subjects with potentially varying skill levels. Still the learned model is able to recover the improving trend, independent of the underlying skill levels. As shown in Fig. 7, the model learned with the proposed method can be used for comparing not only the videos of the same subjects but also the videos from different subjects, where the logarithm of data likelihood ratio can be used as a measurement of the skills. However, it is not possible to quantitatively measure the accuracy in comparing videos from different subjects, due to the lack of ground truth information for videos from different subjects.

It is also interesting to look at what the jointly-learned models look like. Fig. 8 depicts the two models learned by the improved method in this real-data based experiment. From the figure, we can see that the two models have different transition patterns. For example, the transition from State 8 to States 2 and 5 are only observed in Model 1. This may be linked to different motion patterns for data of different surgical skills, with the hidden states corresponding to some underlying action elements (and thus the transition patterns vary with the skill).

6

ADDITIONAL VALIDATION USING SPEECH DATA

Although the proposed approach was evaluated above in the context of motion skill analysis in surgical training, using visual data as the input, the approach itself is general and applicable for other applications involving temporal data. To show that the proposed method can be used to solve temporal inference problems other than video-based motion skill assessment, we now consider an exemplar problem, speech-based emotion recognition, where the attribute of interest (the underlying emotion of a speaker) needs to be inferred from sequential data. Emotion recognition has received attention from researchers due to its broad applications. For example, in human-machine interaction, better responses can be made if the emotional state of the human can be recognized. Existing work on this in the literature mainly focuses on developing models for assigning the labels like "pleasing", "angry" and "neural" to the data, e.g., [36], [37], [38], [39]. Most of the those efforts are

Fig. 7. Top: the logarithm of the data likelihood ratio from two models learned by HMM. Middle: the logarithm of data likelihood with the model learned by the baseline method. Bottom: the logarithm of the data likelihood ratio with the models learned by the improved method. The red vertical lines separate the data of different subjects, where X-axis is the corresponding subject ID. Within each subjects' corpus, the videos are sorted according to their time of recording.

1216

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

Fig. 8. The two component models (Model 1 for X1 and Model 2 for X2 ) learned by the improved method, where we only draw the edges with a transition probability larger than 0.01 and ignore self transitions. The number attached to each edge indicates the transition probability.

supervised in natural, i.e., the ground truth labeling for the training data is required. For example, [40] used support vector machines, [36] used hidden Markov models, both utilizing fully-labelled data. The ground truth data typically require manual labeling by human, which is an error-prone process especially if absolute labels must be assigned to ambiguous data. With the proposed model, we can support learning with only relative labels like "Audio a is more pleasing than Audio b", which is easier to obtain and also less error-prone. In this experiment, we use Utsunomiya University Spoken Dialogue Database For Paralinguistic Information Studies (UUDB)[41](http://uudb.speech-lab.org), which contains 4840 assets labeled across six dimensions (pleasantness, arousal, dominance, credibility, interest and positivity) on a scale of 1 to 7. The ground truth is based on the average of scores of three annotators. For our experiment, we pick the assets which are longer than 1 second to ensure the effectiveness of emotional recognition, which results in 991 assets, where half of the data are used for training and the remaining for testing. For generating the ground truth pairs, we randomly picks 1000 pairs from the training assets. Note that, we say two assets are similar, if the difference of the labeled scores of two assets is within the range of ðÀ1; 1Þ. For feature extraction, we use Hidden Markov Model Toolkit (HTK)[42], where the MFCC coefficients are extracted with the following configurations: sampling rate is 100 HZ, windows size is 25 millisecond, number of filter bank channels is 26, cepstral liftering coefficient is 22 with 12 cepstral parameters and the feature vector is normalized. K-means is applied to the MFCC coefficients of all the training data to generate a code book of 64 elements. Finally, each data is converted to a sequences of histograms. We use the same set of parameters as the previous experiment. The experimental results are reported in Table 3, where we also provide a comparison to the relative attribute [9] as

referred by "SVM". From the table, we can find that the improved method consistently outperforms than both plain HMM and also the baseline method in all six dimensions. We also find that the baseline method gets low accuracy on this experiment, which can be explained by that the length of the audio (in number of temporal frames) varies dramatically and the baseline method obviously cannot handle this variation very well.

7

DISCUSSIONS AND CONCLUSIONS

In this paper, we presented a new formulation for the problem of learning temporal models using only relative information. Algorithms were developed under the formulation, and experiments using both synthetic and real data were performed to verify the performance of the proposed method. In essence, the proposed method attempts to learn an HMM with relative constraints. Such a setting is useful for many practical applications where relative attributes are easier to obtain while explicit labeling is difficult to get. The application of video-based surgical training was the focus of this study, and the evaluation results using realistic data suggests that the proposed method provides a promising solution to the problem of motion skill evaluation from
TABLE 3 The Result for Experiment on UUDB Datasets Dimension Pleasantness Arousal Dominance Credibility Interest Positivity Average SVM 75:25% 82:11% 74:13% 69:15% 76:91% 68:08% 74:27% Improved 77:30% 86:95% 87:95% 76:68% 81:90% 74:99% 81:28% Baseline 57:96% 55:74% 63:04% 55:11% 62:56% 67:84% 53:14% HMM 75:05% 69:55% 77:32% 71:74% 78:07% 70:36% 73:72%

We evaulate the accuracy of ranking pairs with the learned models compared with the ground truth ones.

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1217

videos. For future work, we plan to extend the proposed method to cover different observation models so that more types of applications may be handled. That also includes investigating alternative feature spaces which may be more effective for the target problem.

[7]

[8] [9] [10] [11] [12] [13] [14]

APPENDIX A
For multinomial observation model, i.e., pðXt jfzt Þ ¼ QD Xt ðlÞ , where D is the dimension of each frame, d¼1 fzt ðlÞ Xt ðlÞ is the lth dimension of Xt and fzt are the parameters of observation model with State zt , we can further define the following variables for each sequence Xi : n 2R
i K Â1

Oi 2 RK ÂD Mi 2 RK ÂK

À Á : n ðkÞ ¼ d zi 1 ¼k ; X : Oi ðk; dÞ ¼ Xi t ðdÞ;
i t:zt ¼k

: Mi ðk; lÞ ¼

T X À Á À i Á d zi tÀ1 ¼ k d zt ¼ l ; t¼2

[15] [16] [17] [18] [19]

where dðÁÞ is Dirac Delta function. Then the log likelihood with the optimal path can be written as: logpðXi ; zi juÞ X X ¼ ni ðlÞlog pðlÞ þ Mi ðk; lÞlog Aðk; lÞ
l

þ

X
k;d

k;l

Oi ðk; dÞlog fk ðdÞ (19)

¼ uT hðXi ; zi Þ;

[20] [21] [22]

where u ¼ ½log p; vecðlog AÞ; vecðlog fÞ, hðXi ; zi Þ ¼ ½ni ; vecðMi Þ; vecðOi Þ and vec converts matrix to vector.

ACKNOWLEDGMENTS
The work was supported in part by a grant (#0904778) from the National Science Foundation (NSF). Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.

[23] [24]

REFERENCES
[1] F. Duan, Y. Zhang, N. Pongthanya, K. Watanabe, H. Yokoi, and T. Arai, "Analyzing human skill through control trajectories and motion capture data," in Proc. IEEE Int. Conf. Autom. Sci. Eng., Aug. 2008, pp. 454­459. K. Watanabe and M. Hokari, "Kinematical analysis and measurement of sports form," IEEE Trans. Syst., Man, Cybern. A, Syst., Humans, vol. 36, no. 3, pp. 549­557, May 2006. S. Suzuki, N. Tomomatsu, F. Harashima, and K. Furuta, "Skill evaluation based on state-transition model for human adaptive mechatronics (HAM)," in Proc. 30th Annu. Conf. IEEE Ind. Electron. Soc., 2004, vol. 1, pp. 641­646. S. Satoshi and H. Fumio, "Skill evaluation from observation of discrete hand movements during console operation," J. Robot., vol. 2010, 2010. J. Rosen, M. Solazzo, B. Hannaford, and M. Sinanan, "Task decomposition of laparoscopic surgery for objective evaluation of surgical residents' learning curve using hidden Markov model," Comput. Aided Surgery, vol. 7, pp. 49­61, 2002. K. Kahol, N. C. Krishnan, V. N. Balasubramanian, S. Panchanathan, M. Smith, and J. Ferrara, "Measuring movement expertise in surgical tasks," in Proc. 14th Annu. ACM Int. Conf. Multimedia, 2006, pp. 719­722. [25]

[2] [3]

[26]

[27]

[4] [5]

[28]

[29]

[6]

Q. Zhang and B. Li, "Video-based motion expertise analysis in simulation-based surgical training using hierarchical Dirichlet process hidden markov model," in Proc. Int. ACM Workshop Med. Multimedia Anal. Retrieval, 2011, pp. 19­24. E. Fox, "Bayesian nonparametric learning of complex dynamical phenomena," Ph.D. thesis, MIT, Cambridge, MA, USA, 2009. D. Parikh, and K. Grauman, "Relative attributes," in Proc. Int. Conf. Comput. Vis., Nov. 2011, pp. 503­510. M. Schultz, and T. Joachims, "Learning a distance metric from relative comparisons," in Proc. Advances Neural Inf. Process. Syst., 2004, p. 41. N. Kumar, A. Berg, P. Belhumeur, and S. Nayar, "Attribute and simile classifiers for face verification," in Proc. IEEE Int. Conf. Comput. Vis., Sep. 29, 2009­Oct. 2, 2009, pp. 365­372. D. Parikh, A. Kovashka, A. Parkash, and K. Grauman, "Relative attributes for enhanced human-machine communication," in Proc. AAAI Conf. Artif. Intell., 2012. Q. Zhang and B. Li, "Relative hidden Markov models for evaluating motion skill," in Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2013, pp. 548­555. M. Collins, "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms," in Proc. ACL-02 Conf. Empirical Methods Natural Language Process., 2002, pp. 1­8. Y. Altun, I. Tsochantaridis, and T. Hofmann, "Hidden Markov support vector machines," in Proc. 20th Int. Conf. Mach. Learning, 2003, vol. 20, no. 1, p. 3. A. Sloin and D. Burshtein, "Support vector machine training for improved hidden Markov modeling," IEEE Trans. Signal Process., vol. 56, no. 1, pp. 172­188, Jan. 2008. G. Wang, D. Forsyth, and D. Hoiem, "Comparative object similarity for improved recognition with few or no examples," in Proc. 23rd IEEE Conf. Comput. Vis. Pattern Recog., 2010, pp. 3525­3532. A. Kovashka, D. Parikh, and K. Grauman, "Whittlesearch: Image search with relative attribute feedback," in Proc. IEEE Conf. Comput. Vis. Pattern Recog., Jun. 2012, pp. 2973­2980. I. Kadar and O. Ben-Shahar, "Small sample scene categorization from perceptual relations," in Proc. IEEE Conf. Comput. Vis. Pattern Recog., Jun. 2012, pp. 2711­2718. S. Guo, S. Sanner, T. Graepel, and W. Buntine, "Score-based Bayesian skill learning," in Proc. Eur. Conf. Mach. Learn. Knowl. Discovery Databases, 2012, pp. 106­121. P. Dangauthier, R. Herbrich, T. Minka, and T. Graepel, "Trueskill through time: Revisiting the history of chess," in Proc. Adv. Neural Inf. Process. Syst., 2007, pp. 337­344. N. R. Howells, M. D. Brinsden, R. S. Gill, A. J. Carr, and J. L. Rees, "Motion analysis: A validated method for showing skill levels in arthroscopy," Arthroscopy: J. Arthroscopic Related Surgery, vol. 24, no. 3, pp. 335­342, 2008. H. Lin, I. Shafran, D. Yuh, and G. Hager, "Towards automatic skill evaluation: Detection and segmentation of robot-assisted surgical motions," Comput. Aided Surgery, vol. 11, no. 5, pp. 220­230, 2006. A. G. Gallagher, E. M. Ritter, H. Champion, G. Higgins, M. P. Fried, G. Moses, C. D. Smith, and R. M. Satava, "Virtual reality simulation for the operating room: proficiency-based training as a paradigm shift in surgical skills training," Ann. Surgery, vol. 241, no. 2, p. 364, 2005. J. Rosen, J. Brown, L. Chang, M. Sinanan, and B. Hannaford, "Generalized approach for modeling minimally invasive surgery as a stochastic process using a discrete Markov model," IEEE Trans. Biomed. Eng., vol. 53, no. 3, pp. 399­413, Mar. 2006. K. Kahol, N. C. Krishnan, V. N. Balasubramanian, S. Panchanathan, M. Smith, and J. Ferrara, "Measuring movement expertise in surgical tasks," in Proc. 14th Annu. ACM Int. Conf. Multimedia, 2006, pp. 719­722. S.-k. Jun, P. Singhal, M. Sathianarayanan, S. Garimella, A. Eddib, and V. Krovi, "Evaluation of robotic minimally invasive surgical skills using motion studies," in Proc. Workshop Performance Metrics Intell. Syst., 2012, pp. 198­205. L. E. Baum, T. Petrie, G. Soules, and N. Weiss. (1970). A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. Ann. Math. Statist. [Online]. 41(1), pp. 164­171. Available: http://www.jstor.org/stable/2239727 B. Juang and L. Rabiner, "The segmental k-means algorithm for estimating parameters of hidden Markov models," IEEE Trans. Acoustics, Speech Signal Process., vol. 38, no. 9, pp. 1639­1641, Sep. 1990.

1218

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

[30] N. Merhav and Y. Ephraim, "Maximum likelihood hidden Markov modeling using a dominant sequence of states," IEEE Trans. Signal Process., vol. 39, no. 9, pp. 2111­2115, Sep. 1991. [31] D. P. Bertsekas, "Constrained optimization and Lagrange multiplier methods," in Computer Science and Applied Mathematics, Boston, MA, USA: Academic, vol. 1, 1982. [32] C.-C. Chang and C.-J. Lin. (2011). LIBSVM: A library for support vector machines. ACM Trans. Intell. Syst. Technol. [Online]. 2, pp. 27:1­27:27, Software available at http://www.csie.ntu.edu. tw/ cjlin/libsvm. [33] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan, "Object detection with discriminatively trained part-based models," IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, pp. 1627­ 1645, Sep. 2010. [34] I. Laptev, "On space-time interest points," Int. J. Comput. Vis., vol. 64, no. 2, pp. 107­123, 2005. [35] Q. Zhang, L. Chen, Q. Tian, and B. Li, "Video-based analysis of motion skills in simulation-based surgical training," in Proc. Int. IS&T/SPIE Electron. Imaging, 2013, p. 86670A. [36] T. L. Nwe, S. W. Foo, and L. C. De Silva, "Speech emotion recognition using hidden Markov models," Speech Commun., vol. 41, no. 4, pp. 603­623, 2003. [37] B. Schuller, G. Rigoll, and M. Lang, "Hidden Markov model-based speech emotion recognition," in Proc. IEEE Int. Conf. Acoustics, Speech, Signal Process., 2003, vol. 2, pp. II­1. [38] M. El Ayadi, M. S. Kamel, and F. Karray, "Survey on speech emotion recognition: Features, classification schemes, and databases," Pattern Recognit., vol. 44, no. 3, pp. 572­587, 2011. [39] A. Tarasov and S. J. Delany, "Benchmarking classification models for emotion recognition in natural speech: A multi-corporal study," in Proc. IEEE Int. Conf. Autom. Face Gesture Recognit. Workshops, 2011, pp. 841­846. [40] K. H. Kim, S. Bang, and S. Kim, "Emotion recognition system using short-term monitoring of physiological signals," Med. Biol. Eng. Comput., vol. 42, no. 3, pp. 419­427, 2004. [41] H. Mori, T. Satake, M. Nakamura, and H. Kasuya. (2008). UU database: A spoken dialogue corpus for studies on paralinguistic information in expressive conversation. Proc. Int. Conf. Text, Speech Dialogue, pp. 427­434 [Online]. Available: http://uudb.speechlab.org/ [42] P. C. Woodland, J. J. Odell, V. Valtchev, and S. J. Young. (1994). Large vocabulary continuous speech recognition using htk. Proc. IEEE Int. Conf. Acoustics, Speech, Signal Process., vol. 2, pp. II/125­ II/128 [Online]. Available: http://htk.eng.cam.ac.uk/

Qiang Zhang received the BS degree in electronic information and technology from Beijing Normal University, Beijing, China, in 2009 and the PhD degree in computer science from Arizona State University, Tempe, Arizona in 2014. Since 2014, he has been with Samsung, Pasadena, CA, as a staff research scientist in computer vision. His research interests include image/video processing, computer vision and machine vision, specialized in sparse learning, face recognition, and motion analysis. He is a student member of the IEEE.

Baoxin Li (S'97-M'00-SM'04) received the PhD degree in electrical engineering from the University of Maryland, College Park, in 2000. He is currently an associate professor of computer science and engineering with Arizona State University, Tempe. From 2000 to 2004, he was a senior researcher with SHARP Laboratories of America, Camas, WA, where he was the technical Lead in developing SHARP's HiIMPACT Sports technologies. From 2003 to 2004, he was also an adjunct professor with the Portland State University, Portland, OR. He holds nine issued US patents. His current research interests include computer vision and pattern recognition, image/video processing, multimedia, medical image processing, and statistical methods in visual computing. He won the SHARP Laboratories' President Award twice, in 2001 and 2004. He also received the SHARP Laboratories' Inventor of the Year Award in 2002. He received the National Science Foundation's CAREER Award from 2008 to 2009. He is a senior member of the IEEE.

" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

663

Multivehicle Cooperative Driving Using Cooperative Perception: Design and Experimental Validation
Seong-Woo Kim, Member, IEEE, Baoxing Qin, Zhuang Jie Chong, Xiaotong Shen, Wei Liu, Marcelo H. Ang, Jr., Emilio Frazzoli, Senior Member, IEEE, and Daniela Rus, Fellow, IEEE

Abstract--In this paper, we present a multivehicle cooperative driving system architecture using cooperative perception along with experimental validation. For this goal, we first propose a multimodal cooperative perception system that provides see-through, lifted-seat, satellite and all-around views to drivers. Using the extended range information from the system, we then realize cooperative driving by a see-through forward collision warning, overtaking/lane-changing assistance, and automated hidden obstacle avoidance. We demonstrate the capabilities and features of our system through real-world experiments using four vehicles on the road. Index Terms--Cooperative driving, cooperative perception, driving assistance, see-through system, vehicle communication.

I. I NTRODUCTION OOPERATIVE perception, via the exchange of sensor information between vehicles via wireless communications, is an emerging technology that is receiving attention from industry and academia [1]­[3]. The main advantage of cooperative perception can be summarized as an increase in situational awareness, without substantial additional costs. For example, cooperative perception can enable a driver to have a longer perception range--even beyond line-of-sight and fieldof-view--because the sensing region can be extended to the union of the sensing regions of all connected vehicles. Since the prices of sensors and radio devices for cooperative perception are affordable compared with conventional long-range sensors, this results in a clear benefit for users. Fig. 1 shows examples of this perception range extension. Fig. 1(a) and (b) are snapshots of maps of the ego vehicle running the proposed cooperative perception system, where the ego vehicle is represented as the box located at the bottom. The ego vehicle can see the preceding vehicles and an oncoming vehicle beyond line-of-sight in Fig. 1(a) and beyond field-ofManuscript received September 15, 2013; revised January 5, 2014, April 19, 2014, June 18, 2014, and June 24, 2014; accepted June 29, 2014. Date of publication July 28, 2014; date of current version March 27, 2015. This work was supported by the Future Urban Mobility project of the Singapore-MIT Alliance for Research and Technology (SMART) Center, with funding from Singapore's National Research Foundation. The Associate Editor for this paper was C. Olaverri-Monreal. S.-W. Kim is with Singapore-MIT Alliance for Research and Technology, Singapore 138602 (e-mail: sungwoo@smart.mit.edu). B. Qin, Z. J. Chong, X. Shen, W. Liu, and M. H. Ang Jr. are with the Department of Mechanical Engineering, National University of Singapore, Singapore 117575 (e-mail: baoxing.qin@nus.edu.sg; chongzj@nus.edu.sg; shen_xiaotong@nus.edu.sg; liu_wei@nus.edu.sg; mpeangh@nus.edu.sg). E. Frazzoli and D. Rus are with Massachusetts Institute of Technology, Cambridge, MA 02139 USA (e-mail: frazzoli@mit.edu; rus@csail.mit.edu). Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/TITS.2014.2337316

C

Fig. 1. Two examples of sensing range extension using cooperative perception: detecting an oncoming car (a) beyond line-of-sight, and (b) beyond fieldof-view at sharp curb. The blue and red boxes correspond to the ego and preceding vehicle, respectively. The red, green, and blue dots are laser scan points of the ego, first and second preceding vehicle, respectively. The white dotted boxes indicate coming vehicles not detected by the ego vehicle.

view at the sharp corner in Fig. 1(b), where the oncoming vehicle is represented by the white dotted box. This aspect enables better driving decisions, such as overtaking, avoiding hidden or sudden obstacles, or early lane change against lane drop, and eventually could improve traffic safety and efficiency [4], [5]. Despite these advantages, however, there have been comparatively less research works on cooperative perception for Intelligent Transportation Systems (ITS)-related applications. This paper considers cooperative perception to enable cooperative driving. In this paper, we explore a multivehicle cooperative driving system using cooperative perception along with experimental validation. The goals of this paper are to provide a comprehensive argument for such systems and to demonstrate the engineering feasibility. Building up to this, we first propose a cooperative perception system that can provide a far-sight seethrough, lifted-seat, satellite or all-around view to a driver. The key purpose of this system is to provide as much visibility as if driver's seat would be lifted as much as express buses or trucks. Based on the augmented on-road sensing capability, we then present a cooperative driving system that can improve traffic safety and efficiency, specifically, by a see-through forward collision warning, overtaking/lane-changing assistance, and automated hidden obstacle avoidance. All the presented concepts are evaluated in experiments using four vehicles. The contribution of this paper can be summarized as the following. · We present our design of a multimodal cooperative perception system that can provide a far-sight see-through, lifted-seat, satellite or all-around view to a driver.

1524-9050 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

664

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

· We present a cooperative driving system using cooperative perception for traffic safety and efficiency along with experimental validation using four vehicles. · We provide a comprehensive argument for the cooperative perception-based cooperative driving system from a viewpoint of engineering feasibility. The remainder of this paper is organized as follows. Section II presents the related works. Section III introduces the system architecture of our proposal consisting of a multimodal cooperative system and perspective visualization for driver assistance, whose details are presented in Sections IV and V. Section VI presents how our system can support a see-though forward collision warning, overtaking assistance, and lane-changing assistance. Section VII provides scalability analysis. Section VIII provides experimental results performed with vehicles on urban roads. Section IX concludes this paper. II. R ELATED W ORKS A. Cooperative Perception on the Road To enable cooperative perception, the distributed information from multiple vehicles should be properly fused. One of the key principles is to know the relative pose between spatial information from various sensors of different vehicles, which have been dealt with as a map merging problem [6] or relative localization [7]. The solving approaches include triangulation [8] and dead reckoning [9]. However, since rich sensing information has been recently available, sensing-information-based methods have been preferred due to their high accuracy. The principle is to find the best pose that maximizes the similarity of overlapping area between different maps using landmarks [10], topological maps [11], occupancy grid maps [12], or scan matching [13]. With the recent advancement of range sensor technology, scan matching has been one of the most essential technologies to enable map building, map merging, pose estimation [14], and visual odometry [15]. The principle of scan matching is to find a transformation between scan data from range sensors. The proposed solutions include iterative closest point or iterative correspondence point (ICP)-variant [16]­[18], adaptive randomwalk [12], Hough transform [19], correlative scan matching (CSM) [20], and histogram approaches [21], [22]. Although these map merging and scan matching algorithms have been researched over the last two decades with hundreds of variants [18], [23], they still have several difficulties for on-line multivehicle map merging on the road. First, there is no guarantee that sensor configuration of each vehicle is identical. Second, each map has different scan points even in the overlapping area. For these reasons, scan points and features may not be obvious to match. Furthermore, bushes, trees, or moving pedestrians make the situation worse. Finally, the overlapping area may not be sufficient due to longer safety gaps for collision avoidance. In this paper, we devise, apply, and compare the scan matching method to map merging for driving on the road scenario using both a closed-form solution (ICP) and a probabilistic one (CSM).

Meanwhile, one of the primary goals of this paper is to establish a spatial map sufficient for driving control and assistance. For this purpose, there have been several research efforts using GPS. Tan and Huang proposed a Differential GPS (DGPS)-based cooperative collision warning system partially motivated by the inaccuracy of off-the-shelf GPS [24]. Wender and Dietmayer proposed an algorithm to deliver onboard sensing information via wireless communications, whose main motivation was that a position from DGPS is still less accurate than an onboard sensor, specifically a laser scanner [25]. Chong et al. experimentally demonstrated that onboard sensor-based localization can provide centimeter-level position accuracy sufficient for fully autonomous vehicle control without GPS or DGPS in [26]. In this context, we focus on sensing information, rather than GPS or DGPS, to obtain the accurate relative position of vehicles. One of the strongest aspects of this approach is no common coordinate system is assumed among vehicles, which makes the proposed perception system robust from signal measurement variation according to the change of weather, or instability of the central system. B. Multimodal Perception and See-Through Systems The sensors used for driving assistance or automation purposes can be largely classified into active sensors, (e.g., radar, laser, and ultrasonic sensors), or passive sensors, (e.g., monocular and stereo-vision sensors). Since both are complementary to each other, a multimodal approach fusing information from active and passive sensors has been actively researched [27], [28] for various purposes, such as vehicle detection and tracking [29], pedestrian detection and tracking [30], intersection safety [31], and see-through systems [32]. Recently, see-through systems have begun to be considered as one of the emerging driver assistance technologies to mitigate traffic accident caused by perception limitations. OlaverriMonreal et al. [33] introduced a see-through system based on vehicular ad hoc networks for assisting overtaking maneuvers. The same group proposed a method that displays a see-through view to a human driver using augmented reality in [34]. Note that the see-through systems can be achieved through video streaming between vehicles [35], [36]. In contrast to simulation verification of these works, Li and Nashashibi provided experimental results using two vehicles on the road with the consideration of sensor multimodality in [32]. In the context of delivering additional sensor information for driver assistance, the see-through systems share a similar goal with this paper. In this paper, however, we will extend one step further toward cooperative driving. C. Cooperative Driving Using Cooperative Perception One of the most promising applications of cooperative perception on the road is cooperative driving for traffic safety and efficiency [24], [37], [38], because the results of map merging extend perception range beyond line-of-sight and sensing angles. Accordingly, they enable traffic flow prediction, early obstacle detection, and long-term perspective path planning. The work of Tsugawa et al. [39] included the architecture

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

665

Fig. 3. Concept of leader string where i is an ego vehicle. i + 1 and i + 2 are the first and second leader of i, respectively. i - 1 is a following vehicle behind ego vehicle i.

Fig. 2.

Architecture of the proposed system.

for cooperative driving using cooperative perception. The agenda and experimental results of cooperative autonomous driving have been presented in [4], [40], and[41]. In particular, Kim et al. provided the motivation and experimental results of cooperative autonomous driving using cooperative perception in [3]. Liu et al. proposed the concept of motion planning using cooperative perception on the road [42]. Rebsamen et al. proposed infrastructure-support sensing for driving [43]. In addition, vehicle communication and vehicle identification problems are addressed in [44]­[48], respectively. In this paper, we focus on cooperative driving using cooperative perception in an engineering feasibility standpoint. III. S YSTEM A RCHITECTURE Fig. 2 shows the overall system architecture of the proposed system. The system consists of three subsystems: cooperative perception, perspective visualization, and cooperative driving. For cooperative perception, each vehicle is equipped with range sensors, such as laser or radar scanners, vision sensors, and radios, as shown in the leftmost box of Fig. 2. An odometry system is utilized for ego-motion estimation such as moving direction and speed. The range sensors are used for vehicle detection and tracking. The vision sensors are used for classification and identification of vehicles and pedestrians and provide driver-friendly visual traffic information. The vehicles exchange this local sensing information with other vehicles or infrastructure via wireless communications. In this paper, we define the exchanging of information as a message. Since there exists a tradeoff between communication performance and information quantity, the message profile, e.g., message size and transmission period, should be carefully chosen according to application requirements and driver preferences. In this paper, we investigate several message profiles, such as laser scan data only, raw image only, compressed image only, point clouds only, and both laser scan and point clouds, which are investigated in detail with the real measurement data on the road in Section VIII-B. All local and remote sensing information is properly fused at the box of cooperative perception in Fig. 2. Compared with data fusion of on-board sensing information, data fusion of remote sensing information on the road includes a number of practical challenges [3], among which this paper focuses on the map merging problem and sensor multimodality.

After the information fusion procedure at cooperative perception, the fused information can be used for driving assistance for a human driver and motion planner for an autonomous driver. In addition, it can be delivered to the vehicles following behind or closest infrastructure. For the purpose of driving assistance, our proposed system represents the fused information in a perspective visualization manner, to provide intuitive guidance information, which is dealt with in Section V. Finally, one of the primary goals of a cooperative driving subsystem is to let a driver know the moment when the driver should be careful, such as hidden obstacle detection or sudden braking of preceding vehicles beyond line-of-sight. The notification is performed by visual and sound alarms, which enable a driver to focus on driving until any dangerous situation is detected by our system. Moreover, the subsystem notifies a driver when there are any vehicles coming from behind or at blind spots, which can contribute to safe lane changing or overtaking. For self-driving vehicles, the notification triggers path re-planning for automated lane changing in Section VIII-D. IV. C OOPERATIVE P ERCEPTION ON THE ROAD On the road, the preceding vehicles highly affect the driving decision of the ego driver. In this sense, a leader is a preceding vehicle: 1) connected via wireless communications; and 2) observable by the ego vehicle through its local sensors or remote sensing information. Let Vi = {i + 1, . . .} be a leader string of vehicle i, where, j, j + 1  Vi , j and j + 1 are connected via wireless communications and j + 1 is observable by local sensors of j . The concept of leader string is represented in Fig. 3. In a broad sense, following vehicles of ego vehicle i can be included in the leader string Vi , even though a vehicle i - 1 is not a leader literally. The information of a vehicle i - 1 can be useful for lane-changing assistance or blind spot detection, because the following vehicle i - 1 can watch the ego vehicle in the third-person view from behind the ego vehicle. In vehicle driving scenarios, sensing information is typically dealt with as a map. Let M = {. . . , m, . . .} be a map for navigation of vehicles, which consists of the set of points obtained and filtered from sensors, where m  Rn , n = 2, or 3. Given a position p  M in a map, M[p]  R can be defined in several ways, such as the belief that the position p is obstaclefree, or the height of obstacle in case of p  R2 . Mi denotes a map of a vehicle i. Now, we can formulate cooperative perception as follows: Mi = Mi
j Vi

Mj

(1)

where the operation is called map merging. The map merging operation is merely a set union operation, if Mi and Mj Vi

666

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

are mapped into a global coordinate frame, such as GPS coordinates. However, the observation from sensors is typically mapped into a local coordinate frame. In addition, there is no guarantee that the initial poses of vehicles are known. In this case, the relative pose between vehicles is necessary to merge different spatial information. The relative pose can be represented as q = (, ), where  and  correspond to translation and rotation, respectively. We define a transformation operator as p  q = R()p +  . Finally, (1) can be rewritten in a more general form as follows: Mi = Mi
j Vi

1) ICP Algorithm: The ICP solution can be formally stated as follows:
 = arg min qi,j qi,j mj Mj

CMi (mj ) - mj  qi,j

(7)

where · is an l2 -norm, and CMi (mj ) is a point in Mi corresponding to mj . In the standard ICP, the closest point in Mi is selected as the correspondence, which is defined as CMi (mj ) = arg min
mi Mi

mi - m j .

(8)

Mj  qi,j

(2)

where qi,j is a relative pose between a vehicle i and j . One of the key challenges to solve (2) is to obtain an accurate qi,j . A. Problem Formulation 1) Peer-Vehicle Map Merging: The primary problem of map merging can be formulated as follows:
 qi,j = arg max S (Mi , Mj , qi,j ) qi,j

 ICP iterates (7) using Mj  Mj  qi,j until the solution  has converged. qi,j can be obtained by using a closed-form solution that Arun et al. proposed in [49]. Note that there is no guarantee that the closest point (8) is the right corresponding point. In many cases, two maps are partially overlapped, or scan points may not be exactly matched due to the physical limitation of a sensor, e.g., different sensing range or resolution. From an implementation perspective, the closest point should be limited by some threshold distance. Now, (7) can be rewritten as follows:  = arg min qi,j qi,j mj Mj

(3)

dMi (mj ) CMi (mj ) - mj  qi,j (9)

where the similarity measure S is formulated as L (Mi [p], (Mj  qi,j )[p])
p

(4)

where d M i ( mj ) = 1 0 if minmi Mi mi - mj < dth otherwise, (10)

where L(ai , aj ) is a point-to-point similarity measure, which is positive if ai = aj and 0 otherwise. Equations (3) and (4) attempt to find the relative pose that maximizes the overlapping area between two maps. Note that (3) and (4) are well-suited for a range-scanner-based approach. We present a method to merge vision-based data in Section IV-D. 2) Multivehicle Map Merging: Equations (3) and (4) can be extended to more than two vehicles. Let Qi = {qi,i+1 , . . . , qi,i+N } be the set of relative poses with respect to (w.r.t) the ego vehicle, where N is the number of leader vehicles. The problem can be rewritten as follows: Q i = arg max S (Mi , . . . , Mi+N , Qi )
Qi

where dth is the threshold distance. dth is one of the key design parameters to decide the performance, which has a tradeoff between accuracy and convergence speed. Using the concept of closest points, the similarity metric S can be reformulated as follows: S (Mi , Mj , qi,j ) =
p

dMi (mj ) Mi [p] - CMj qi,j (p) . (11)

(5)

where the similarity measure S is formulated as L (Mi [p], . . . , (Mj  qi,j )[p], . . .)
p

(6)

where L(ai , . . . , ai+N ) is positive if ai = · · · = ai+N and 0 otherwise. B. Scan Matching Algorithm
 To solve (3)­(6), we use a scan matching function qi,j = ScanMatching(Mi , Mj ). Several approaches can be used for realizing the function, as mentioned in Section II. In this paper, we consider two kinds of approaches: One is ICP, a closed-form solution, and the other is CSM, a probabilistic solution.

2) Probabilistic Scan Matching: Probabilistic scan matching methods approach the problem differently from the closedform solutions such as ICP, in particular, where several uncertainties are considered, such as sensor measurement errors. The key principle is to find a pose at which the current observation is maximally likely or most probable based on the previous observation, such as a map, which can be formulated as x = arg maxx P(z |x, M), where P(z |x, M) is an observation model, x is the pose of an observer, z is the sensor reading, and M is a map [50]. The formulation can be used for cooperative perception as follows:
 = arg max P(Mj |qi,j , Mi ) qi,j qi,j

(12)

where Mj corresponds to observation from a sensor located away from the observer. To efficiently compute the probability of (12), a lookup table is usually built in advance [51]. There are many solutions to obtain the maximum-likelihood  of (12). In this paper, the CSM method is considered, pose qi,j

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

667

Fig. 4. Coordinates and overlapping areas of the ego vehicle and its leader. Hi and Wi are the height and width of the local sensing area of a vehicle i. oi,i+1 is the height of overlapping sensing area between a vehicle i and i + 1 from the perspective of a vehicle i. Likewise, oi+1,i is the height of overlapping area between a vehicle i and i + 1 from the perspective of a vehicle i + 1.

because of several advantages for our purposes. One key aspect of CSM is the ability to find the global maximum instead of trusting a local search algorithm. By evaluating the entire search space, the scan matching algorithm is able to provide a robust solution even with a large initialization error. Because of this, CSM approaches the problem using multilevel resolution. Specifically, two lookup tables are used for avoiding local maxima and achieving lower computational complexity: a low- and a high-resolution table. First, the likelihood is quickly evaluated over the entire 3-D search window by using the low-resolution table. When the maximum-likelihood voxel is found in the process, it starts to find more accurate values by evaluating inside the voxel using the high-resolution table. One can find a specific description in detail in [20]. C. On-Road Map Merging Method In this paper, we consider a 2-D map for map merging, i.e., M[m]  R, m  R2 . Fig. 4 shows the spatial coordinates of the ego vehicle, and its predecessor vehicle, where a leader is used for the predecessor vehicle. We assume that range sensors are installed in a y -axis direction. (lx , ly ) represents the position of the leader. Algorithm 1 shows the overall procedure of our map merging method. Two maps from different vehicles are taken as input parameters, and a merged map is generated from the algorithm.

data of a leader has relatively clear features such as the side or backside of vehicles. The initial pose of a leader can be guessed from the features. More specifically, the proposed system keeps checking whether there are consecutive points lying in the desired path ahead. For this task, let Di be the set of positions that a vehicle i intends to move to, typically the set of the center positions of the current lane, which can be generated by combination of vehicle localization, lane detection and path planning methods. We use curb-intersection features based Monte Carlo localization [52] for lane-keeping, and a path planner for lane-changing as described in [42]. In Fig. 24(a), the yellow horizontal line passing through the vehicles depicts a simple Di to keep within a lane. In Fig. 24(c), the red curve depicts Di as a result of path replanning for lane changing. To find the leader position, the proposed system keeps watching straight or slightly curved lines in the following point set: {p|rth > p - pl , pl  Di } (13)

where rth is the maximum boundary that a vehicle can deviate from the desired path, typically half of the lane width. This method considers the current lane position to extrapolate the desired path of the leader vehicle, which can significantly reduce the search space and occurrence of false positives in leader detection compared with relying on scan matching only. If 1) the detected points compose a straight or slightly curved line, and 2) the line is almost orthogonal to the desired path, the center position of the line and the orthogonal angle to the line are used as t0 and 0 , respectively, which is used as the initial transformation q0 = (t0 , 0 ). Note that reliable leader vehicle detection is important to avoid circular reasoning [7]. Our method is to merge information that comes from the leader to information gathered by the ego vehicle. To identify the leader, vehicle identification is necessary, which is dealt with in [3] and[48]. In addition, with the support of vehicle identification and rear sensors, the following vehicle can be identified by the ego vehicle. Accordingly, the map merging process can be started from the following vehicle with information delivered from the following vehicle. This map merging from the following vehicle will be evaluated in Section VIII-B.2. 2) Overlapping Area Extraction: The size of overlapping area between two maps is relatively small during driving. Computational complexity can be reduced by restricting matching operation to the overlapping area instead of whole maps. Fig. 4 shows the overlapping area between two vehicles. In case of the ego vehicle, scan points in overlapping area are collected into i = {m|m > ly - (lx + Wi /2) tan 0 , m  Mi } (14)

where the leader position (lx , ly ) is obtained from t0 . Likewise, scan points of the leader in overlapping area, j , are {m|m < (ly - (lx + Wi /2) tan 0 ) cos 0 , m  Mj  q0 } . (15) 3) Virtual Leader Scan Points Recovery: We assume that a vehicle cannot scan itself. The map of the ego vehicle has

1) Leader Vehicle Detection: One of the distinct characteristics of scan data on the road is that features are not obvious to match due to trees, bushes, or pedestrians. However, scan

668

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Fig. 5. (a) Concept of IPM; (b) an example of projection on-road image captured by a forward-looking camera; and (c) its corresponding IPM result.

scan points detecting its leader vehicle, but the map delivered from the leader includes no scan point of the leader itself. Since a sensor is typically installed at a fixed position, the vehicle knows how it would be shown on its own map. Therefore, the vehicle can recover virtual scan points reflecting its shape as if the vehicle detects itself. The virtual laser scan points are one of the most obvious features to match, so that they can play a important key role to provide good matching points, especially when a few scan points are collected or scan points are not obvious to match. 4) Scan Matching: Now, we are ready to perform scan matching operation with Mi , Mj , and an initial transformation q0 . In this paper, we use ICP and CSM for our implementa is obtained. In tion. As a result, the optimal relative pose qi,j Section VIII-B, we will evaluate the performance in terms of computation time and accuracy with the comparison of two scan matching approaches. 5) Final Merged Map: The final map is the union of Mi  and transformed Mj with the relative pose qi,j , which can be represented as Mi
 Mj  qi,j .

Fig. 6.

Concept of perspective projection and inverse perspective projection.

(16)

D. Multimodal Cooperative Perception Our proposed system supports multiple sensory modalities for cooperative perception. In case of range sensors, such a laser scanner, it is relatively easy to perform map merging operations, because their readings are a set of physical quantities recorded with their spatial coordinates. However, in the case of vision sensors whose reading is an image, the map merging is not a straightforward task, because the vision image is the result of perspective projection. Therefore, the images should be mapped into the spatial coordinate of the ego vehicle. We use the inverse perspective mapping (IPM) [53] method to deal with this problem. IPM is an image transformation method to get a satellite view of the road surface. When a forward-looking camera is capturing an image, the shape of the road surface is usually distorted due to perspective projection. An IPM process can be applied to remove this distortion and recover original road shapes for the map merging purpose. Fig. 5(a) illustrates the basic idea of IPM. For a fixedmount on-board camera, its pose in the vehicle coordinate frame is usually known, and hence the pose relative to the road surface can be also obtained. Together with its intrinsic parameters from a calibration process, the perspective transformation matrix from the road surface to the camera image can be

calculated. An inverse operation of this perspective matrix will restore the original road surface, and store it in an IPM image. Fig. 5(b) and (c) shows an example of projection on-road image captured by a forward-looking camera, and its corresponding IPM result, respectively. One can find more details on IPM in ITS-related applications in [54]. From a mathematical point of view, IPM is an inverse operation to camera projection transformation. In the projection process, a 3-D point [X, Y, Z ] is projected onto an image with its projection pixel as [x, y ], where [x, y, 1]T = P [X, Y, Z, 1]T , and P is the 3 × 4 camera projection matrix. In the IPM process, we assume that the height of points on the road surface is 0, where their 3-D coordinates [X, Y, 0] can be easily recovered with the P matrix, and their image pixels [x, y ]. Fig. 6 describes these operations. Due to the assumption of IPM where points lie on the road surface, it is only suitable to recover the object shapes with Z = 0 height. In our experiment, we use it to facilitate our map merging for the road surface. Other objects such as pedestrians and vehicles, which violate the IPM assumption, will be taken care with other sensory modalities. One can find a general framework for road marking detection and analysis in [55]. Although visual odometry has been proposed and used [15], we primarily use laser scan points to obtain the relative pose  in the implementation of this paper, because our system qi,j is targeting on-road scenarios, and it is much faster to deal with scan data rather than images with the consideration of  , the this purpose. Using (16) and the obtained relative pose qi,j recovered road surfaces can be merged into one. Finally, three types of information can be represented in the merged map: 1) vehicle poses, including poses of the ego vehicle and other vehicles; 2) laser scan points; and 3) color points of the road surface from vision. Fig. 14(a) shows an example of merging multimodal sensing data from a vision sensor and laser scanner. By supporting multiple sensory modalities, the cooperative perception system helps the ego vehicle to perceive not only occluded vehicles and obstacles but also road surfaces that may be out of its sensing ability. V. P ERSPECTIVE V ISUALIZATION The cooperative perception process generates a merged map with various types of information. To provide the information as intuitive driving guidance, our system represents the information in a perspective projection.

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

669

Fig. 7. Example of a perspective see-through visualization on the road. The two numbers next to the cuboid are the distance from the ego vehicle and its speed, respectively. The gray dotted lines represent road lanes. The cuboid represents a simplified vehicle model. (a), (b), and (c) are examples of visual warning signs for driving assistance, where (a) is a forward collision warning, (b) is a sign for overtaking assistance and (c) is a sign for lane-changing assistance. These visual warnings can be activated along with audio or tactile warning.

Fig. 8. Schematic of a third-person visualization. A virtual camera is mounted above the roof of ego-vehicle.  is its tilted-down angle, and D is its mounting height. The tilted-down angle and mounting height of this virtual camera can be adjusted to find an optimal view to visualize the data from cooperative perception.

A. First-Person See-Through Visualization Through the map merging steps presented in Section IV, a 3-D spatial map is obtained from different sensor information, where x, y along with z as an occupancy probability or height of (x, y ). Contrary to the way of the IPM method, the perspective projection method can map 3-D points to a 2-D plane, as shown in 4 of Fig. 6. Based on the perspective projection method, we can provide a first-person see-through visualization consisting of the perspective visualization of the road surface and vehicle skeletons. The concept of the first-person see-through system using cooperative perception has been proposed in [32]. Here, we present our implementation in detail. Fig. 7 shows an example of the first-person see-through visualization on the road. The leftmost and rightmost boxes are used for providing visual warning to a driver, which is explained in detail in Section VI. 1) Road Surface Projection: As a first step, the road surface is restored from vision images of the ego vehicle or leader vehicles by the IPM method and represented as a cloud of color points, as addressed in Section IV-D. To visualize the road surface information for a human driver, we project the surface points onto the camera image and label the projection pixels to the color of their corresponding points. 2) Vehicle Skeleton Visualization: Vehicle detection and tracking is realized with laser scan data readings, with vehicle poses stored in the merged map. To visualize the detected vehicles more clearly, we use a cuboid as the simplified vehicle model, and then plot the skeleton of this cuboid. In a bad situation, e.g., at night, under shadow, in heavy rain, such as an explicit cuboid definitely helps to identify vehicles. The cuboid has 8 vertices and 12 edges. Its vertices are projected onto the camera image, and its edges are drawn by connecting the projection pixels of neighboring vertices. B. Adjustable Third-Person Visualization While the first-person see-through visualization provides an intuitive representation of the merged data, various kinds of information is squeezed and overlapped at a small region of its

image, due to the low mounting position of the vehicle camera, which makes them difficult to recognize ahead traffic situations, as we can see in Fig. 7. For this reason, we propose to use an adjustable third-person view to visualize the data. Fig. 8 illustrates the basic idea of third-person visualization. In Fig. 8, hv , gi,j , and hc are the camera installation height, the distance from a vehicle i to a preceding vehicle j , and the uplifted vision height, respectively. While the mounting height hv + hc and titled-down angle  of this virtual camera are both adjustable, the best view point can always be selected to visualize the merged data in different scenarios. Note that the camera is typically mounted with some tilted-down angle, e.g., 12 in our experiments. For determining a proper , we use the following boundary for deciding a tilt-down angle w.r.t. to camera height hc and distance gi,j   arctan hv + hc gi,i+1 - a 2 (17)

which can be derived from Fig. 8, where a is a horizontal angle of view. Likewise, if we want to restrict the maximum number of tracking leaders as s, the upper boundary of  is  < arctan hc s j =i+1 + a 2 (18)

gi,i+1 +

(gj,j +1 + lj )

where gi,i+1 is a distance between a vehicle i and i + 1, and li is the width of a vehicle i. For example, the minimum tilteddown angle  is 0.8  45.8 to obtain a bird's eye view at 20 m(= hc + hv ) if a vision camera is installed at hv = 1 m, a target vehicle is preceding at L = 10 m ahead, and the vertical field-of-view of the camera is a = 35 . One may want to watch the display only when necessary, instead of watching it continuously. For this purpose, the next section addresses a see-through forward collision warning. VI. C OOPERATIVE D RIVING A SSISTANCE Here, we present how cooperative perception results presented in Section IV and V can be applied to specific driving assistances, such as forward collision warning, overtaking/lanechanging assistance with an all-around view.

670

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Furthermore, our system can detect a vehicle approaching toward the ego vehicle using a spatio­temporal moving obstacle detection and tracking method. From the following, we investigate the benefit of an all-around view in terms of overtaking assistance and lane-changing assistance.
Fig. 9. Collision warning with and without cooperative perception.

C. Overtaking/Lane-Changing Assistance A. See-Through Forward Collision Warning A driver can focus on driving itself without having to keep watching a driving assistance display, if the system can let a driver know the moment when the driver has to drive carefully through proper visual, audio, or tactile feedback. In this context, one of the most desired functions is forward collision warning. Numerous forward collision warning algorithms have been proposed along with risk assessment methods, which include time-based and distance-based approaches. First, the time-based methods usually use time-to-collision (TTC), which can be formulated as T T Ci,j = gi,j , vi - vj j  Vi (19) When a driver should drive slowly or stop due to a slowmoving truck or obstacle ahead, the driver should decide whether to wait longer in the current lane or change lanes. In particular, this is an important issue for overtaking on a singlelane road. Various overtaking assistance methods are possible according to the sensor configurations, requirements, and sensing capability. The overtaking decision should be determined with the consideration of: 1) the number of lanes; 2) the speed of a preceding vehicle; 3) cut-in space availability; 4) distance from an on-coming vehicle; and 5) the existence of another overtaking vehicle from behind as in Fig. 16(h). Our system can inform 3), 4), and 5) that are difficult to be provided without cooperative perception due to the limitation of line-of-sight. Equations (19) and (20) can be applied to read-end collision warning at lane changing, which are corresponding to T T Cj,i and rj,i , respectively, where the vehicle j is approaching toward the ego vehicle behind in an adjacent lane, and j < i. D. Feedback to a Driver Once a forward collision is expected, or overtaking is not possible, or lane changing is not possible, it should be notified to a driver through visual, audio, or tactile feedback. Fig. 7(a)­(c) are examples of visual warnings for a forward collision warning, overtaking possibility, and lane-changing possibility, respectively. In Section VIII, we evaluate how our system can contribute to the see-through forward collision warning, overtaking assistance, and lane-changing assistance are, through real experiments on the road using vehicles equipped with the proposed system. VII. S CALABILITY A NALYSIS Information delivered via wireless communication inherently has uncertainty. In this paper, we quantify communication uncertainty by using communication delay, because the communication delay highly affects the overall system performance of moving vehicles. Let dc i,j be communication delay for message delivery from
p c a vehicle i to j . For simplicity, we define dc i = di+1,i . Let di be p the processing delay of a vehicle i. In general, dc i and di are not fully controllable, so they should be measured and estimated. The leader i + 1 is moving, whereas the information is being processed and delivered from i + 1 to i. From the perspective of a vehicle i, the estimated position p ~i+1 is p p ~i+1 = pi+1 + vi+1 (dc i + di ) 

where T T Ci,j is corresponding to TTC, and vi is the speed of a vehicle i. A forward collision warning is activated if T T Ci,j is less than a certain threshold time, typically 2­3 s according to safety requirements [56]. In distance-based methods, a forward collision warning is activated if, given two vehicles i and j , the recommended safety gap ri,j is larger than the distance between two vehicles gi,j . Formally, a forward collision warning is activated if gi,j < ri,j , j  Vi (20)

where ri,j is a minimum distance to avoid collision to a preceding vehicle j , which can be formulated as follows [57]: ri,j = (vi - vj )/2 2 + (vi - vj )  Trs , j  Vi (21)

where  is the deceleration of the ego vehicle, e.g., -0.2 g ( -2 m/s). Trs is the response time of a driver, e.g., 0.5­1.5 s. Note that one of the great advantages of our system is that j is not limited to only i + 1. In our system, due to the capability of a see-through view, j can be beyond the first leader according to the connectivity via wireless communications, as shown in Fig. 9. This see-through characteristic enables the driver to avoid hidden obstacles earlier than without cooperative perception systems. We will investigate this early hidden obstacle detection in Section VIII-C. B. All-Around View Using Cooperative Perception Another benefit of our system is that it enables an all-around view without having to install sensors covering all sides of a vehicle. In particular, a following vehicle can see the ego vehicle in a third-person view, including its blind spots. Fig. 16(e) shows one snapshot of the all-around view of the ego vehicle. This all-around view aspect of our system is investigated using real experiments in Section VIII-B.2.

(22)

 i+1 can be obtained by vehicle where pi+1 and vi+1 = p i, as addressed in Section IV. By using the definition of

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

671

and the driver's reaction time is 1.5 s. For example, total delay should be less than 2 s to detect and avoid the obstacle at 100 m ahead, if the vehicle runs at 100 km/h, and driver's reaction time is 1.5 s. The delay boundary, in this case 2 s, is a key parameter to determine message profile and radio interface. We evaluate this using measurement data from experiments on the road in Section VIII-B.3. VIII. E XPERIMENTAL R ESULTS Here, we provide experimental results to evaluate our proposed system with four automotive vehicles. A. Experimental Setup
Fig. 10. Delay upper boundary including processing time w.r.t. distance from an obstacle ahead, where a driver's reaction time is set to 1.5 s.

communication and processing delay and (22), we derive the worst case deadline for information delivery as follows. Assuming that no vehicle moves backward on a given path, information delivered from a vehicle k  Vi has an impact to path planning and vehicle control of an ego vehicle i whose maximum speed is v i , if the following inequality is satisfied: ds =
j =i  k -1 p dc j + dj <

pk - p i - Trs min (v i , vi + ds ui )

(23)

where ds is a total delay for delivering k 's information to i, and ui is the maximum acceleration of a vehicle i. A sketch of proof is as follows. Suppose that k is an obstacle staying at a position pk as a worst case, because we assume that no vehicle move backward. For intuitive understanding, suppose that i + 2 is a stopped vehicle or a static obstacle in Fig. 3. In this case, k is i + 2. Note that the position pk is recognized by a vehicle k - 1. In (23), min(v i , vi + ds ui ) represents the maximum speed that the vehicle i can perform during information delivery of the vehicle k . Therefore, the second rightmost term of (23) represents the minimum time that the vehicle i can reach the position pk . The rightmost term of (23), i.e., Trs , is a reaction time to steer or slow down for collision avoidance. As a result, the rightmost two terms represent the minimum time that the driver can react to a dangerous situation ahead. The ahead traffic information should be delivered within the time. Depending on the applications, an average delay boundary can be more useful. Suppose that a group of vehicles achieves velocity consensus at vi on highway. In this case, the delay boundary can be relaxed as follows: ds < pk - p i - Trs . vi (24)

In the experiments, we used several mass-produced automotive vehicles including one Mitsubishi iMiEV and three Yamaha golf carts. The vehicles are equipped with 2-D LIDARs (LIght Detection And Ranging), a vision camera, and wireless interface IEEE 802.11g, IEEE 802.11n, 3G HSDPA (HighSpeed Downlink Packet Access), and 4G LTE (Long-Term Evolution). The devices of 3G HSDPA and 4G LTE are Huawei E153 and Huawei E3276, respectively, which are both USBtype. The vision cameras were mounted with a tilted-down angle of 12 from horizontal. The software architecture of this system was established on robot operating system (ROS) suite [58] using only open source libraries. Detail specifications are available in [26]. The setup of three vehicles is represented in Fig. 9. i, i + 1, and i + 2 are corresponding to the ego vehicle, the first leader and the second leader, respectively. In Fig. 9, the second leader transmits its sensing information to the first leader via wireless communications, while moving forward. With the support of our system, the first leader merges the remote information with its local sensing information, and then transmits the combined information to the ego vehicle via wireless communications, while moving forward as well. In this experiment, we assumed vehicle identification is supported before map merging. B. General Evaluation The experiments was performed on a campus road at National University of Singapore (NUS), Singapore, where the rule of the road is to drive on the left and the speed limit is 40 km/h. Fig. 11 shows the test road. The fleet of test vehicles start to move at Start toward End via P-turn. The road from Start to P-turn is an urban road with buildings. The road from P-turn to End is a road often lined with trees and bushes on either side, and includes a hill road. Experiments were conducted on sunny or cloudy days between 12 P. M . and 5 P. M . 1) Map Merging Evaluation: Fig. 12 shows the comparison of the different scan matching methods used to perform map merging during driving on the test road. The initial pose is obtained from LIDAR-based vehicle detection using the method presented in Section IV-C.1. The obtained pose is used as the initial condition for both the ICP and CSM scan matching algorithm. In this paper, the ICP algorithm from [59] is used. In the ICP method, dth = 2.5 m is used. For CSM, two levels of

As long as k satisfies (24), k can be increased. In this way, the maximum size of leader string can be obtained by using (23) or (24). Note that k is highly affected by message profile, radio characteristics, and vehicle speed. Fig. 10 shows the upper boundary of total communication and processing delay under the assumption that the speed of the ego vehicle is constant

672

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Fig. 11. Test road at NUS. The fleet of test vehicles start to move at Start toward End via P-turn.

Fig. 13. Leader vehicle detection (a) in multilane roads and (b) at a curb. The horizontal blue dotted line is the desired path of the ego vehicle. Only vehicles detected near the desired path can be classified as a leader vehicle.

Fig. 12. (a) Translation errors and (b) rotation errors of map merging, according to the method. Average computation time of LIDAR, ICP, and CSM is 6, 6, and 108 ms, respectively.

0.5 and 0.1 m search grids are used. It is also set to search within a window of 10 m × 10 m × 60 . In order to obtain the ground truth, a particle filtered-based localization is used. We built a map using a 2-D laser scanner for localization on the test road before all experiments. Each of the three vehicles is localized by using the map and the relative position is extracted. We evaluated the whole trajectory of the vehicles on the test road. Details on the mapping method and its evaluation are available in [60]. In Fig. 12, the average error increases from the first leader to second leader across different map merging methods. The CSM, although slower, is able to ensure a high-quality scan matching and therefore able to achieve slightly over 1-m errors. This is also true when the rotation error is considered. There are several factors that contribute to the rather large standard deviation. Among others, the noisy observation obtained from LIDAR in an urban environment, LIDAR-based vehicle detection algorithm uncertainty and communication delays are the main contributing factors. Through Fig. 12, it was found that there are significant improvements in the merged map accuracy from the second leader by ICP and CSM. While CSM performed slower than ICP, it is able to robustly estimate the relative pose since CSM is robust to initialization error. We conclude that the results using the both scan matching approaches perform sufficient for our requirements. Fig. 13 shows the leader vehicle detection addressed in Section IV-C.1. The method works well in a situation where there are a number of vehicles in the adjacent lane. 2) Visualization Evaluation: Fig. 14 shows a variety of visualizations made possible through our system including a satel-

lite view, see-though views and third-person views. Fig. 14(a) is one snapshot of a merged map of the ego vehicle resulted from the proposed system. The red, green, and blue dots represent laser scan points of the ego vehicle, the first and second leader, respectively. Note that the ego vehicle cannot see the second leader due to the limitation of line-of-sight. Fig. 14(b) is the raw vision image from the second leader via wireless communications, where four small green circle indicate the region of interest for IPM. Fig. 14(c) and (d) are the raw vision image from the first leader via wireless communications and of the ego vehicle via a local vision sensor, respectively. Fig. 14(e) is the see-though view from the first leader where the cuboid is a simplified vehicle model. Comparing to Fig. 14(c), the otherwise obstructed road marking "AHEAD", as shown in Fig. 14(e) in a see-through manner. Fig. 14(f) is the see-though view from the ego vehicle, where the blue and green cuboid represent the second and first leader, respectively. Fig. 14(g) and (h) are the third-person views of the first leader and the ego vehicle, respectively, where viewpoint is lifted at 4 m from the ground in Fig. 14(g) and 30 m from the ground in Fig. 14(h). In brief, we can see that our system provides ahead traffic information even beyond line-of-sight, which can be applied to a see-though forward collision warning or an overtaking assistance. At a sharp curve, such as T-junction or intersection, oncoming cars at blind spots can be potentially dangerous. The proposed system can help see an hidden on-coming vehicle at blind spots. Fig. 1(b) is a good example of this. As like the sharp curve, a downhill also includes a hidden obstacle problem. In Fig. 15, the vehicle i cannot see the vehicle i + 2 due to the limitation of line-of-sight. However, the vehicle i + 1 can let the vehicle i know the situation of the vehicle i + 2, as long as the vehicles are connected via wireless connectivity. Instead of the vehicle i + 1, an infrastructure equipped with cooperative perception system can let the vehicle i know the traffic situation at a downhill ahead. Fig. 16(a)­(d) shows how our system aides in driving over a hill. Fig. 16(a) is the second leader's camera view, where the preceding vehicle is descending from the top of hill. Fig. 16(b) and (c) are the camera views of the first leader and the ego vehicle, respectively. The ego vehicle is ascending a hill. Fig. 16(d) is a third-person view lifted at 30 m from the ground.

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

673

Fig. 14. (a) Merged map of the ego vehicle, where the red, green, and blue dots represent laser scan points of the ego vehicle, the first and the second leader, respectively. (b) Raw vision data from the second leader, where four small green circles represent the region of interest for IPM. (c) Raw vision image from the first leader. (d) Raw vision image of the ego vehicle. (e) See-though view of the first leader where the cuboid is a simplified vehicle model. Comparing to (c), a traffic sign on the road surface can be seen in (e) in a see-through manner. (f) See-though view of the ego vehicle, where the blue and green cuboids represent the second and first leaders, respectively. (g) Third-person view of the first leader, where driver's seat is lifted at 4 m from the ground. (h) Third-person view of the ego vehicle, where driver's seat is lifted at 30 m from the ground.

Fig. 15. Hidden obstacle problem at downhill.

Fig. 16(e)­(h) shows how all-around view is supported by a following vehicle connected via wireless communications. For this reason, the vehicle order is different between Fig. 16(a)­(d) and (e)­(h). Fig. 16(e) is the satellite view of the ego vehicle, where the gray arrow indicates the ego vehicle. The ego vehicle is moving between the first leader and the following vehicle. In Fig. 16(e), the ego vehicle can detect a vehicle approaching behind, because the following vehicle connected via wireless communications can tell the ego vehicle about what is going on at blind spot of the ego vehicle, which can greatly improve situational awareness. Fig. 16(f) and (g) are a normal camera view and see-though view of the ego vehicle, respectively. Fig. 16(h) is a view of ego vehicle from behind given by the following vehicle. All-around view characteristics of Fig. 16(e) and (h) can be applied to the lane-changing and overtaking assistance. In addition, a normal vision camera can sense the adjacent left and right lane, as we can see Fig. 16(e). A fisheye lens can be considered for multiple lane detection.

Fig. 17 shows the perspective projection results according to mounting height and tilted-down angle, as presented in Section V-B. In the figure, the left-bottom (hc + hv , ) indicates a camera mounting height (unit: meter), and a tilted-down angle (unit: radian), respectively. Fig. 17(a)­(i) shows a thirdperson view according to the different hc + hv and different . In case of hc = 0, our system performs similarly to see-through systems. According to the growth of hC , the projection result generated by our system becomes close to a satellite view via a bird's eye view. Consequently, our proposed system supports drivers to be able to choose optimal parameters according to their preferences. 3) Communication Evaluation: Wireless communications are one of the key factors that enable cooperative perception, and at the same time, significantly affect overall system performances. In this paper, we suggest or propose no new communication method. Instead, we use IEEE 802.11g, IEEE 802.11n, 3G HSDPA and 4G LTE, which are off-the-shelf commercially ready solutions. Therefore, this evaluation focuses on how communication profile and uncertainty affect system performance in a given radio interface. Table I provides a message profile we used in the experiment. The message is transmitted to other vehicles at 20 Hz. More specifically, we use standard message types in ROS, to transmit our sensing data. First, laser scan data is transmitted in the form of LaserScan, where 180 laser beams are contained in each scan. The total amount of information

674

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Fig. 16. [Driving over a hill] (a) Second leader's camera view, where the preceding vehicle is descending from the top of a hill. (b) Camera views of the first leader. (c) Camera views of the ego vehicle that is ascending the hill. (d) Third-person view lifted at 30 m from the ground. [All-around view] (e) Satellite view of the ego vehicle. Note that the ego vehicle is moving between the first leader and the following vehicle. In (e), the ego vehicle can detect a vehicle approaching from behind. (f) and (g) are a normal vision image and see-though image of the ego vehicle, respectively. (h) is view of the ego vehicle from behind.

Fig. 17. Left-bottom indicates (camera height hc + hv , tilted-down angle  ). (a)­(i) show third-person view according to the different hc + hv and different  . In case of D = 0, our system performs similarly to the conventional see-through systems. As D increases, the projection result becomes closer to a satellite view. Drivers can choose optimal parameters according to their preferences.

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

675

TABLE I M ESSAGE P ROFILE AND DATA S IZE

Fig. 18. Average delay for one single message delivery between two nodes away from 10 m each other via IEEE 802.11g, 4G LTE, and 3G HSDPA.

Since the target of the proposed system is a moving vehicle on the road, the communication should be investigated from the perspective of impact on vehicle control, specifically position errors in this evaluation. Fig. 19(a) shows the impact of IEEE 802.11g communication delay on position error according to vehicle speed. From the perspective of average delay, the position error of one hop is 24 cm at 100 km/h in case of laser scan data only. The worst case position error becomes 5 m at 100 km/h. This may be good enough to be used as controlpurpose information, depending on the goal or task. However, communication delay becomes significantly uncertain, as the size of data increases. Depending on the application and purpose, the message profile must be carefully determined. For example, in case of fusion with laser scan and processed vision of this measurement data, the worst case position error is 8 m. Fig. 19(b) and (c) provides average/worst delay in case 4G LTE and 3G HSDPA, respectively. We can say that information from other vehicles are less reliable than locally sensed information in terms of delay or loss. From the perspective of vehicle control, time-critical control, such as emergency braking, should be determined based on local sensing information. Remote sensing information can be useful for long-term perspective path planning, such as decision problem between early lane changing and lane keeping for a human driver, as well as an autonomous driver. C. Usability Test We conducted usability tests with real drivers using real vehicles equipped with the proposed cooperative driving system. This paper proposes no new H/W to display our visualization results to drivers. Instead, one can use any display H/Ws that have been already used or considered for driver assistance systems such as dash board, center console, head-up display, or smartphone. In this test, we used a 20-in LCD monitor for visualization and installed it at a front passenger seat. Fig. 20 shows usability test scenarios. In this test, we aimed at evaluating a see-through forward collision warning and overtaking assistance, including lane-changing assistance. Initially, three vehicles move toward the leftmost flag on a single-lane road, where a test driver drives the vehicle i. Then, the vehicle i + 2 suddenly stops at a certain position. To avoid collision, the vehicle i + 1 and i stop accordingly. After a while, the vehicle i + 2 speeds up, but i + 1 moves slowly. The test driver should make a decision whether to overtake the slow-moving vehicle i + 1 or follow the vehicle in the same lane for safety. In the opposite lane, oncoming vehicles are approaching toward the vehicle i randomly, which makes the test driver hesitate the decision. In this experiment, the vehicle i + 2 was moving forward at 2­5 m/s before sudden braking. Accordingly, TTC was set to 15 s. These are somewhat conservative parameters mainly for safety concerns. Note that the brake lights of the vehicle i + 2 was turned off, which makes the vehicle i + 1 hard to notice whether the vehicle i + 2 decelerates or not. The warning sound includes two loud beeps that last for 2 s per one warning activation. We used IEEE 802.11n as a radio interface of vehicle-to-vehicle communications. LIDAR and compressed

per frame is 752 bytes/frame = 25 bytes message header) + 7 bytes message description) + 180 beams × float32 (4 bytes). Second, the raw data is 691,200 (640 × 360 × 3 (24 - bit color)) bytes. The size of compressed images is roughly range from 50 to 110 kB in these experiments. The processed vision contains metadata, such as lane information, which is represented by a point cloud and transmitted as PointCloud in ROS. The message size varies depending on the extracted information, which is usually less than 6 kB, much smaller than raw images. Finally, we evaluate sensor multimodality with laser and processed vision data. To monitor communication delay according to the message profile and radio interface, we set up two nodes, 10 m away from each other, and measured round-trip time (RTT). More specifically, the sender attached its CPU clock to the message. Once the message arrived at the receiver, the receiver resends the message to the sender without any message modification, such as an echo server. The sender can compare current local CPU clock with the CPU clock included in the message retransmitted from the receiver. In this evaluation, we use a half of RTT as a communication delay. The measurements were conducted for around one hour according to the message profile and radio interface. We measured communication delay in open public area, which means the delay can be affected by uncontrollable environmental interferences. Fig. 18 shows the delay measurement via IEEE 802.11g, IEEE 802.11n, 3G HSDPA and 4G LTE. In terms of average delay, IEEE 802.11g and IEEE 802.11n outperforms 3G HSDPA and 4G LTE. However, the communication performance decrease as the distance gi,j increases. In this experiment, it was observed that 3G HSDPA and 4G LTE were not affected by the distance between communication correspondences.

676

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Fig. 19. Impact of communication delay on position error in case of (a) IEEE 802.11, (b) 4G LTE, and (c) 3G HSDPA. TABLE II D ECELERATION S TARTING T IME AND WARNING ACTIVATING T IME

Fig. 20. Usability evaluation scenarios.

Fig. 21. Usability test with our proposed system on the road. (a) The bottom circle represents the vehicle i on the satellite view, where the green means no possible collision. (b) The green circle turns to red, because the vehicle i + 2 decelerates. Note that the vehicle i + 1 does not decelerate yet. (c) The vehicle i + 1 decelerates due to collision avoidance with the vehicle i + 2.

vision were used as a message profile. As shown inFig. 18, the average communication delay between the vehicle i + 1 and i is 16 ms. IEEE 802.11n performed well at more than 30-m distance on the road. Fig. 21 shows two views from the front passenger seat as seen at different time instances. The left-bottom screen of Fig. 21(a) shows the satellite view, where the bottom arrow indicates the vehicle i. The green circle indicates no collision possibility is detected at the moment. The right bottom screen of Fig. 21(a) is the perspective view. In principle, the test driver cannot see the vehicle i + 2 due to the limitation of line-of-sight. However, the test driver can see the vehicle i + 2 through our proposed system. Fig. 21(b) shows the snapshot of the activation of a seethrough forward collision warning. In Fig. 21(b), the vehicle i + 2 suddenly stopped. Accordingly, the forward collision

warning sign turned to red along with a loud sound alarm. Note that the vehicle i + 1 does not decelerate yet. In Fig. 21(c), (c) the vehicle i + 1 decelerates due to collision avoidance with the vehicle i + 2. We conducted real tests with three human drivers on the road eight times. Table II summarizes the average time of deceleration starting time and forwarding collision warning activating time. The vehicle i + 1 decelerated 3.3 s later than the deceleration of the vehicle i + 2. 3.3 s can be seen as somewhat a slow reaction time. The time is obtained when the braking lights of the vehicle i + 1 is turned on. The driver of the vehicle i + 1 released the acceleration pedal before pushing the brake pedal. The brake lights of the vehicle i + 2 were turned off, and the vehicles were moving not very fast. All these factors affected the response time of the vehicle i + 1. Average forward collision warning activating time is 2.1 s according to the preset TTC and the distance from the vehicle i + 2. The test drivers pushed the brake pedal averagely 0.82 s later after a forward collision warning sound is activating. In this experiment, the test drivers detected the sudden braking of the vehicle i + 2 earlier than the vehicle i + 1 and accordingly stopped earlier as much as 0.33 s. Fig. 22 shows the timing diagram of one of usability tests. The speed of the vehicle i, i + 1 and i + 2 were obtained from odometry, local LIDAR of the vehicle i, and cooperative perception, respectively. In this test, we can see that after the deceleration of the vehicle i + 2, a collision warning is activated around 2 s later. Then, both the vehicle i + 1 and i decelerate at the almost same time. Fig. 23 shows overtaking/lane-changing assistance test on the road. In Fig. 23(a), the preceding vehicle moves very slowly. The test driver could know there is enough space in front of the vehicle i + 1 and no oncoming vehicle in the opposite lane through the added see-through view. In Fig. 23(b), the test driver is overtaking the vehicle i + 1 with the support of our system.

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

677

Fig. 22. Timing diagram of speed of three vehicles and collision warning activation. After the deceleration of the vehicle i + 2, a collision warning is activated. Then, the vehicle i + 1 and i decelerate at the almost same time.

Fig. 23. Overtaking/lane-changing assistance test. (a) The test driver checks enough space in front of the preceding vehicle and no oncoming vehicle in the opposite lane. (b) The test driver overtakes the preceding vehicle.

Fig. 24. (a) A sudden obstacle appears in front of leader vehicle, and only the leader vehicle detects the obstacle due to the limitation of line-of-sight of the ego vehicle. (b) The leader and the ego vehicle stop, then the ego vehicle performs path replanning. (c) With the support of cooperative perception, early lane changing is triggered.

In summary, it was observed that all test drivers had more confidence of driving by our proposed system in terms of hidden obstacle detection, overtaking, and lane changing, because they could see ahead the traffic situation at their driving seat whenever they wanted, and our system notified them along with visual and sound feedback when they should watch the display. Not surprisingly, our system, particularly a see-through forward collision warning, works well even at night. D. Automated Lane Changing for Early Obstacle Avoidance Suppose that in an emergency situation, an autonomous driver could take over vehicle control, so that the vehicle could avoid obstacles earlier by early lane changing with the support of our system. Automated early lane changing could be activated if a forward collision was imminent and lane-changing possibility was positive. Fig. 24 shows simulations of the scenario. These simulations utilized a RRT path planner [61]. In Fig. 24(a), a sudden obstacle appears in front of the leader vehicle, and only the leader vehicle detects the obstacle due to the limitation of line-of-sight of the ego vehicle. In Fig. 24(b), the both of the leader and the ego vehicle suddenly stop, then the ego vehicle performs path replanning to overtake the stopped leader. However, it is difficult to find the feasible path to overtake the leader in the case of Fig. 24(b), because the ego vehicle and the leader are too close due to the sudden stop. In Fig. 24(c), with the support of cooperative perception, early lane changing is triggered according to (19) or (20). Since

Fig. 25. Our self-driving vehicle performs an automated lane-change by a seethrough collision warning trigger.

sufficient space is occupied to overtake the leader, the path planner can find the overtaking path quickly. Fig. 25 shows the moment when our self-driving vehicle performs an automated lane-change by a see-through collision warning trigger during an on-road experiment. One of the possible benefits of this automated lane changer is to reduce the reaction time Trs mentioned in (21), (23), and (24), as long as path planner and vehicle controller are faster than a human driver. The proposed method allows only one vehicle to carry out the overtaking at a time and it is not intended for a distributed overtaking. Future work includes further research on multivehicle overtaking in the context of cooperative driving. E. Impact of Weather Condition Weather conditions should be considered for potential impact on the performance of wireless communications and sensor measurement.

678

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

According to the ITU-R P.676-8 [62], signal attenuation is not significantly worsened by dry weather or water vapor at less than 10 GHz frequency, thus communications over IEEE 802.11, 3G, and 4G networks should be unaffected by rain. In contrary, sensor measurement can be highly affected by weather conditions. First of all, vision can provide abundant information under sunny or cloudy conditions during the day, but much less information during the night. Active sensors, such as LIDAR and radar, are significantly less affected by light conditions. However, LIDAR can be blocked by fog, smoke, or rain. LIDAR may be able to partially overcome this by regarding range readings less than a certain threshold as false positive during a fog/rain situation. Radar is robust against such adverse weather conditions, but provides less resolution in clean air [63]. Conclusively, the sensors should be carefully selected and located such that various sensor types can play complementary roles and help protect against any shortcomings due to adverse weather. IX. C ONCLUSION In this paper, we have provided our design and experimental validation of cooperative driving using cooperative perception. The extended perception range by the support of cooperative perception enables the driver to know the traffic situation even beyond line-of-sight or beyond field-of-view. The proposed system provides a see-through forward collision warning, overtaking/lane-changing assistance and automated lane-change capability using cooperative perception. We implemented the system and performed real experiments using vehicles on the road. The experimental results quantitatively show that the proposed system can contribute to the improvement of driving safety and non-myopic driving decision-making. Compared with cooperative driving without sharing of perception data, cooperative perception can better assist to make a driving decision in complex traffic situations by abundant information that cooperative perception provides. Furthermore, the information enables self-driving vehicles to build an online see-through map for navigation, which can be used for nonmyopic and safer decisions for cooperative self-driving. ACKNOWLEDGMENT The authors would like to thank Z. Cheng for mechanical design of the first vehicles used at our preliminary experiments, Dr. B. Luders and S. Pendleton for reading the manuscript and useful discussions, and anonymous reviewers for insightful and constructive comments, which help to improve this paper. R EFERENCES
[1] L. Merino, F. Caballero, J. M. de Dios, J. Ferruz, and A. Ollero, "A cooperative perception system for multiple UAVs: Application to automatic detection of forest fires," J. Field Robot., vol. 23, no. 3/4, pp. 165­184, Apr. 2006. [2] A. Rauch, F. Klanner, R. Rasshofer, and K. Dietmayer, "Car2X-based perception in a high-level fusion architecture for cooperative perception systems," in Proc. IEEE Intell. Veh. Symp., Jun. 2012, pp. 270­275. [3] S.-W. Kim et al., "Cooperative perception for autonomous vehicle control on the road: Motivation and experimental results," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., Nov. 2013.

[4] S.-W. Kim et al., "Multiple vehicle driving control for traffic flow efficiency," in Proc. IEEE Intell. Veh. Symp., Jun. 2012, pp. 462­468. [5] B. van Arem, C. J. G. van Driel, and R. Visser, "The impact of cooperative adaptive cruise control on traffic-flow characteristics," IEEE Trans. Intell. Transp. Syst., vol. 7, no. 4, pp. 429­436, Dec. 2006. [6] K. Konolige, D. Fox, B. Limketkai, J. Ko, and B. Stewart, "Map merging for distributed robot navigation," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2003, vol. 1, pp. 212­217. [7] A. Howard, M. J. Mataric, and G. S. Sukhatme, "Putting the'I'in'Team': An ego-centric approach to cooperative localization," in Proc. IEEE Int. Conf. Robot. Autom., 2003, vol. 1, pp. 868­874. [8] I. Rekleitis, G. Dudek, and E. Milios, "Experiments in free-space triangulation using cooperative localization," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2003, vol. 2, pp. 1777­1782. [9] N. Trawny, X. S. Zhou, K. X. Zhou, and S. I. Roumeliotis, "3D relative pose estimation from distance-only measurements," in Proc. IEEE/RSJ Int. Conf. IROS, 2007, pp. 1071­1078. [10] G. Dedeoglu and G. Sukhatme, "Landmark-based matching algorithm for cooperative mapping by autonomous robots," in Distributed Autonomous Robotic Systems 4. Tokyo, Japan: Springer-Verlag, 2000, pp. 251­260. [11] W. Huang and K. Beevers, "Topological map merging," Int. J. Robot. Res., vol. 24, no. 8, pp. 601­613, Aug. 2005. [12] A. Birk and S. Carpin, "Merging occupancy grid maps from multiple robots," Proc. IEEE, vol. 94, no. 7, pp. 1384­1397, Jul. 2006. [13] F. Amigoni, S. Gasparini, and M. Gini, "Merging partial maps without using odometry," in Multi-Robot Systems. From Swarms to Intelligent Automata Volume III . Amsterdam, The Netherlands: Springer-Verlag, 2005, pp. 133­144. [14] F. Lu and E. Milios, "Robot pose estimation in unknown environments by matching 2D range scans," J. Intell. Robot. Syst., vol. 18, no. 3, pp. 249­ 275, Mar. 1997. [15] D. Nistér, O. Naroditsky, and J. Bergen, "Visual odometry," in Proc. IEEE CVPR, 2004, vol. 1, pp. 652­659. [16] P. J. Besl and N. D. McKay, "Method for registration of 3-D shapes," in Proc. Int. Soc. Opt. Photon., Robotics-DL Tentative, 1992, pp. 586­606. [17] C. Yang and G. Medioni, "Object modelling by registration of multiple range images," Image Vis. Comput., vol. 10, no. 3, pp. 145­155, Apr. 1992. [18] S. Rusinkiewicz and M. Levoy, "Efficient variants of the ICP algorithm," in Proc. 3rd Int. Conf. 3-D Digit. Imag. Model., 2001, pp. 145­152. [19] A. Censi, L. Iocchi, and G. Grisetti, "Scan matching in the hough domain," in Proc. IEEE ICRA, 2005, pp. 2739­2744. [20] E. B. Olson, "Real-time correlative scan matching," in Proc. IEEE ICRA, 2009, pp. 4387­4393. [21] T. Rofer, "Using histogram correlation to create consistent laser scan maps," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2002, vol. 1, pp. 625­630. [22] M. Bosse and J. Roberts, "Histogram matching and global initialization for laser-only SLAM in large unstructured environments," in Proc. IEEE Int. Conf. Robot. Autom., 2007, pp. 4820­4826. [23] F. Pomerleau, F. Colas, R. Siegwart, and S. Magnenat, "Comparing ICP variants on real-world data sets," Autonom. Robots, vol. 34, no. 3, pp. 133­ 148, Apr. 2013. [24] H.-S. Tan and J. Huang, "DGPS-based vehicle-to-vehicle cooperative collision warning: Engineering feasibility viewpoints," IEEE Trans. Intell. Transp. Syst., vol. 7, no. 4, pp. 415­428, Dec. 2006. [25] S. Wender and K. Dietmayer, "Extending onboard sensor information by wireless communication," in Proc. IEEE Intell. Veh. Symp., Jun. 2007, pp. 535­540. [26] Z. J. Chong et al., "Autonomy for mobility on demand," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., Oct. 2012, pp. 4235­4236. [27] B. Steux, C. Laurgeau, L. Salesse, and D. Wautier, "Fade: A vehicle detection and tracking system featuring monocular color vision and radar data fusion," in Proc. IEEE Intell. Veh. Symp., 2002, vol. 2, pp. 632­639. [28] R. Labayrade, C. Royere, D. Gruyer, and D. Aubert, "Cooperative fusion for multi-obstacles detection with use of stereovision and laser scanner," Autonom. Robots, vol. 19, no. 2, pp. 117­140, Sep. 2005. [29] D. Gruyer, A. Cord, and R. Belaroussi, "Vehicle detection and tracking by collaborative fusion between laser scanner and camera," in Proc. IEEE/RSJ Int. Conf. IROS, 2013, pp. 5207­5214. [30] O. Ludwig, C. Premebida, U. Nunes, and R. Araújo, "Evaluation of boosting-SVM and SRM-SVM cascade classifiers in laser and visionbased pedestrian detection," in Proc. 14th Int. IEEE ITSC, 2011, pp. 1574­1579. [31] Q. Baig, O. Aycard, T. D. Vu, and T. Fraichard, "Fusion between laser and stereo vision data for moving objects tracking in intersection like scenario," in Proc. IEEE IV , 2011, pp. 362­367.

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

679

[32] H. Li and F. Nashashibi, "Multi-vehicle cooperative perception and augmented reality for driver assistance: A possibility to see through front vehicle," in Proc. 14th Int. IEEE ITSC, 2011, pp. 242­247. [33] C. Olaverri-Monreal, P. Gomes, R. Fernandes, F. Vieira, and M. Ferreira, "The see-through system: A VANET-enabled assistant for overtaking maneuvers," in Proc. IEEE Intell. Veh. Symp., 2010, pp. 123­128. [34] P. Gomes, C. Olaverri-Monreal, and M. Ferreira, "Making vehicles transparent through V2V video streaming," IEEE Trans. Intell. Transp. Syst., vol. 13, no. 2, pp. 930­938, Jun. 2012. [35] A. Vinel, E. Belyaev, K. Egiazarian, and Y. Koucheryavy, "An overtaking assistance system based on joint beaconing and real-time video transmission," IEEE Trans. Veh. Technol., vol. 61, no. 5, pp. 2319­2329, Jun. 2012. [36] E. Belyaev, A. Vinel, K. Egiazarian, and Y. Koucheryavy, "Power control in see-through overtaking assistance system," IEEE Commun. Lett., vol. 17, no. 3, pp. 612­615, Mar. 2013. [37] A. Girard, J. de Sousa, J. Misener, and J. Hedrick, "A control architecture for integrated cooperative cruise control and collision warning systems," in Proc. IEEE Conf. Decis. Control, Dec. 2001, vol. 2, pp. 1491­1496. [38] S. Kato, S. Tsugawa, K. Tokuda, T. Matsui, and H. Fujii, "Vehicle control algorithms for cooperative driving with automated vehicles and intervehicle communications," IEEE Trans. Intell. Transp. Syst., vol. 3, no. 3, pp. 155­161, Sep. 2002. [39] S. Tsugawa, S. Kato, T. Matsui, H. Naganawa, and H. Fujii, "An architecture for cooperative driving of automated vehicles," in Proc. IEEE Intell. Transp. Syst., 2000, pp. 422­427. [40] J. Kolodko and L. Vlacic, "Cooperative autonomous driving at the intelligent control systems laboratory," IEEE Intell. Syst., vol. 18, no. 4, pp. 8­ 11, Jul./Aug. 2003. [41] J. Baber, J. Kolodko, T. Noel, M. Parent, and L. Vlacic, "Cooperative autonomous driving: Intelligent vehicles sharing city roads," IEEE Robot. Autom. Mag., vol. 12, no. 1, pp. 44­49, Mar. 2005. [42] W. Liu, S.-W. Kim, Z. J. Chong, X. Shen, and M. H. Ang, Jr., "Motion planning using cooperative perception on urban road," in Proc. IEEE Int. Conf. Cybern. Intell. Syst., Robot., Autom. Mechantron., Nov. 2013, pp. 130­137. [43] B. Rebsamen et al., "Utilizing the infrastructure to assist autonomous vehicles in a mobility on demand context," in Proc. IEEE TENCON , Nov. 2012, pp. 1­5. [44] D. Jiang and L. Delgrossi, "IEEE 802.11p: Towards an international standard for wireless access in vehicular environments," in Proc. IEEE VTC Spring, 2008, pp. 2036­2040. [45] S. Biswas, R. Tatchikou, and F. Dion, "Vehicle-to-vehicle wireless communication protocols for enhancing highway traffic safety," IEEE Commun. Mag., vol. 44, no. 1, pp. 74­82, Jan. 2006. [46] A. Rauch, F. Klanner, and K. Dietmayer, "Analysis of V2X communication parameters for the development of a fusion architecture for cooperative perception systems," in Proc. IEEE Intell. Veh. Symp., 2011, pp. 685­690. [47] A. Von Arnim, M. Perrollaz, A. Bertrand, and J. Ehrlich, "Vehicle identification using near infrared vision and applications to cooperative perception," in Proc. IEEE Intell. Veh. Symp., 2007, pp. 290­295. [48] M. X. Punithan and S.-W. Seo, "King's graph-based neighbor-vehicle mapping framework," IEEE Trans. Intell. Transp. Syst., vol. 14, no. 3, pp. 1313­1330, Sep. 2013. [49] K. S. Arun, T. S. Huang, and S. D. Blostein, "Least-squares fitting of two 3-D point sets," IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-9, no. 5, pp. 698­700, Sep. 1987. [50] K. Konolige and K. Chou, "Markov localization using correlation," in Proc. Int. Joint Conf. Artif. Intell., 1999, pp. 1154­1159. [51] S. Thrun, W. Burgard, and D. Fox, "A real-time algorithm for mobile robot mapping with applications to multi-robot and 3D mapping," in Proc. IEEE ICRA, 2000, vol. 1, pp. 321­328. [52] B. Qin et al., "Curb-intersection feature based Monte Carlo localization on urban roads," in Proc. IEEE Int. Conf. Robot. Autom., 2012, pp. 2640­2646. [53] H. A. Mallot, H. H. Blthoff, J. Little, and S. Bohrer, "Inverse perspective mapping simplifies optical flow computation and obstacle detection," Biol. Cybern., vol. 64, no. 3, pp. 177­185, Jan. 1991. [54] M. Bertozzi and A. Broggi, "Gold: A parallel real-time stereo vision system for generic obstacle and lane detection," IEEE Trans. Image Process., vol. 7, no. 1, pp. 62­81, Jan. 1998. [55] B. Qin et al., "A general framework for road marking detection and analysis," in Proc. 16th Int. IEEE ITSC, 2013, pp. 619­625. [56] E. Dagan, O. Mano, G. P. Stein, and A. Shashua, "Forward collision warning with a single camera," in Proc. IEEE Intell. Veh. Symp., 2004, pp. 37­42.

[57] R. Parasuraman, P. Hancock, and O. Olofinboba, "Alarm effectiveness in driver-centered collision-warning systems," Ergonomics, vol. 40, no. 3, pp. 390­399, 1997. [58] M. Quigley et al., "ROS: An open-source robot operating system," in Proc. ICRA Workshop Open Source Softw., 2009, vol. 3, no. 2, pp. 1­6. [59] J. L. B. Claraco, Development of Scientific Applications with the Mobile Robot Programming Toolkit. The MRPT Reference book. Málaga, Spain: Mach. Perception Intell. Robot. Lab., Univ. Málaga, 2008. [60] Z. Chong et al., "Synthetic 2D LIDAR for precise vehicle localization in 3D urban environment," in Proc. IEEE ICRA, 2013, pp. 1554­1559. [61] S. Karaman, M. R. Walter, A. Perez, E. Frazzoli, and S. Teller, "Anytime motion planning using the RRT," in Proc. IEEE Int. Conf. Robot. Autom., May 2011, pp. 1478­1483. [62] "Attenuation by Atmospheric Gases," Geneva, Switzerland, P.676-8, Oct. 2009. [63] B. Yamauchi, "All-weather perception for man-portable robots using ultra-wideband radar," in Proc. IEEE ICRA, 2010, pp. 3610­3615.

Seong-Woo Kim (M'11) received the B.S. and M.S. degrees in electronics engineering from Korea University, Seoul, Korea, in 2005 and 2007, respectively, and the Ph.D. degree in electrical engineering and computer science from Seoul National University in 2011. He is currently a Postdoctoral Associate with the Singapore-MIT Alliance for Research and Technology. His research interests include online and offline optimization targeted for perception and control of intelligent and autonomous vehicles. Dr. Kim was a recipient of the Best Student Paper Award at the 11th IEEE International Symposium on Consumer Electronics and the Outstanding Student Paper Award at the First IEEE International Conference on Wireless Communication, Vehicular Technology, Information Theory, and Aerospace and Electronic Systems Technology.

Baoxing Qin received the B.S. degree in mechanical engineering from Shanghai Jiao Tong University, Shanghai, China, in 2010. He is currently working toward the Ph.D. degree in the Department of Mechanical Engineering, National University of Singapore, Singapore. His research interests include autonomous vehicle localization, object recognition, mapping and environment understanding.

Zhuang Jie Chong received the B.S degree in mechanical engineering (with first-class honors) from Nanyang Technological University, Singapore, in June 2010. He is currently working toward the Ph.D. degree in the Department of Mechanical Engineering, National University of Singapore. His research interests include autonomous vehicle, localization, and mapping.

Xiaotong Shen received the B.S. degree in mechanical engineering from Harbin Institute of Technology, Harbin, China, in 2012. He is currently working toward the Ph.D. degree in the Department of Mechanical Engineering, National University of Singapore, Singapore. His research interests include cooperative perception.

680

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Wei Liu received the B.S. degree in mechanical engineering from Nanjing University of Aeronautics and Astronautics, Nanjing, China, in 2011, and the M.S. degree in mechatronics from National University of Singapore, Singapore, in 2012, respectively. He is currently working toward the Ph.D. degree in the Department of Mechanical Engineering, National University of Singapore, Singapore. His research interests include motion planning under uncertainties.

Marcelo H. Ang, Jr. received the B.S. degrees (cum laude) in mechanical engineering and industrial management engineering from the De La Salle University, Manila, Philippines, in 1981, the M.Sc. degree in mechanical engineering from the University of Hawaii at Manoa, Honolulu, Hawaii, in 1985, and the M.Sc. and Ph.D. degrees in electrical engineering from the University of Rochester, Rochester, NY, USA, in 1986 and 1988, respectively. His work experience includes heading the Technical Training Division of Intel's Assembly and Test Facility in the Philippines, research positions at the East West Center in Hawaii and at the Massachusetts Institute of Technology, and a faculty position as an Assistant Professor of electrical engineering with the University of Rochester, NY. In 1989, he joined the Department of Mechanical Engineering, National University of Singapore, where he is currently an Associate Professor. He also holds a joint appointment with the Division of Engineering and Technology Management as Deputy Head. In addition to academic and research activities, he is actively involved in the Singapore Robotic Games as its founding Chairman. He also chairs the Steering Committee for the World Robot Olympiad (2008­2009) and the World Skills Singapore Competition (2005, 2007, and 2010). His research interests span the areas of robotics, mechatronics, and applications of intelligent systems methodologies. He teaches both at the graduate and undergraduate levels in the following areas: robotics, creativity and innovation, applied electronics and instrumentation, advanced computing, product design and realization, and special topics in mechatronics. He is also active in consulting work in these areas.

Emilio Frazzoli (SM'07) He received the Laurea degree in aerospace engineering from the University of Rome, "Sapienza", Italy, in 1994, and the Ph.D. degree from the Department of Aeronautics and Astronautics of the Massachusetts Institute of Technology (MIT), Cambridge, MA, USA, in 2001. He is a Professor of aeronautics and astronautics with the Laboratory for Information and Decision Systems, and the Operations Research Center with the MIT. Before returning to MIT in 2006, he held faculty positions with the University of Illinois, Urbana-Champaign, and with the University of California, Los Angeles. He is currently the Director of the Transportation@MIT initiative, and the Lead Principal Investigator of the Future Urban Mobility IRG of the Singapore-MIT Alliance for Research and Technology (SMART). He was the recipient of a NSF CAREER award in 2002. He is an Associate Fellow of the American Institute of Aeronautics and Astronautics. His current research interests focus primarily on autonomous vehicles, mobile robotics, and transportation systems, and in general lie in the area of planning and control for mobile cyberphysical systems.

Daniela Rus (F'10) received the Ph.D. degree in computer science from Cornell University, Ithaca, NY, USA, in 1992. She is currently the Andrew (1956) and Erna Viterbi Professor with the Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, where she serves as a Director of the Computer Science, and Artificial Intelligence Laboratory. Her research interests include distributed robotics and mobile computing, and her application focus includes transportation, security, environmental modeling and monitoring, underwater exploration, and agriculture. Dr. Rus was the recipient of the National Science Foundation Career Award. She is a Class of 2002 MacArthur Fellow and a fellow of the Association for the Advancement of Artificial Intelligence (AAAI).

Simpler non-parametric methods provide as good or better results to multiple-instance learning.
Ragav Venkatesan, Parag Shridhar Chandakkar and Baoxin Li Arizona State University, Tempe, AZ, USA
ragav.venkatesan@asu.edu, pchandak@asu.edu, baoxin.li@asu.edu

Abstract
Multiple-instance learning (MIL) is a unique learning problem in which training data labels are available only for collections of objects (called bags) instead of individual objects (called instances). A plethora of approaches have been developed to solve this problem in the past years. Popular methods include the diverse density, MILIS and DD-SVM. While having been widely used, these methods, particularly those in computer vision have attempted fairly sophisticated solutions to solve certain unique and particular configurations of the MIL space. In this paper, we analyze the MIL feature space using modified versions of traditional non-parametric techniques like the Parzen window and k-nearest-neighbour, and develop a learning approach employing distances to k-nearest neighbours of a point in the feature space. We show that these methods work as well, if not better than most recently published methods on benchmark datasets. We compare and contrast our analysis with the well-established diversedensity approach and its variants in recent literature, using benchmark datasets including the Musk, Andrews' and Corel datasets, along with a diabetic retinopathy pathology diagnosis dataset. Experimental results demonstrate that, while enjoying an intuitive interpretation and supporting fast learning, these method have the potential of delivering improved performance even for complex data arising from real-world applications.

Figure 1. DR image classification as a MIL problem.

1. Introduction
Multiple-instance learning (MIL) is a setting where labels are provided only for a collection of instances called bags. There are two types of instances: negative instances, which are found in either negative bags or positive bags, and positive instances, which are found only in positive bags. While a positive bag must contain at least one inherently positive instance, a negative bag must not contain any positive instances. In MIL, labels are not available at the in-

stance level. It is interesting to note however that the labelspace is the same for both at the bag level and at the instance level. One may attempt to learn instance-level labels during the training stage, thus reducing the problem to an instancelevel supervised classification. Alternatively, one may also localize and prototype the positive instances in the feature space and rely on the proximity to these prototypes for subsequent classification. MIL is an ideal set-up for many computer vision tasks and examples of its application include object tracking [4], image categorization [9] [26] [28] [12], scene categorization [20] and content-based image retrieval [36]. In particular, MIL can be an especially suitable model for medical image-based pathology classification and lesion detectionlocalization, where an image is labeled pathological just because of one or a few lesions localized to small portions of the image. Medical images collected in a clinical setting may readily have an image-level label (either normal or various levels of pathology) while lacking the exact location of the lesion(s). Figure 1 illustrates such an example: colour fundus images of eyes affected with different pathologies of diabetic retinopathy (DR). It is easy to notice that, although majority of the image looks normal, a small retinal landmark is enough to alter the label of the image from normal to pathological. In a MIL formulation for this problem, each image can be considered a bag and patches of images can be considered instances. Over the years, many methods have been proposed to solve the MIL problem [10] [29] [8] [2]. The most fun-

2605

P2 P1

P3 P4
Figure 2. An illustrative feature space for multiple-instance setting. The 'x' in red represents all instances from positive bags and the 'o' in blue represents all instances from negative bags.

damental one is the diverse density approach [19], which has been built upon by many variants [35] [24] [9]. Diverse density is in its basic sense, a function so defined over the feature space that it is large at any point in the feature space that is close to instances from positive bags while being far away from instances from negative bags and vice-versa. The various local maximas in this function are positive instance prototypes and any instance that is closer to these prototypes are labeled inherently positive instances. Other types of methods also exist in this setting [5] [3] [27] [31]. MIL has many different variants and perspective to its definition and indeed most MIL solutions are application centric [1]. This can be easily seen from table 1. Earlier methods perform as good or better in the MUSK dataset than the ones published recently although the recent methods perform better on more complex tasks but for certain exceptions. In this course of research while many particular and complicated solutions are sought after, MIL has never been sufficiently analyzed using traditional non-parametric learning methods. Despite the recent advances, MIL remains a challenging task as the feature space may be arbitrarily complex, the ratio of positive to negative instances can be arbitrarily low in a positive bag, and (by definition) no labeling information is directly available for positive instances. To illustrate these factors, we simulate a typical MIL feature space as depicted in figure 2. Each instance belonging to a particular cluster is independently drawn from a normal distribution that defines the said cluster. While positive bags can draw a subset of random cardinality of instances from negative distributions, negative bags cannot draw any data from positive distributions. Every positive bag must have at least one instance sampled from a positive distribution

(marked in green ellipses P 1 through P 4). The centroids of these clusters would be the ideal positive instance prototypes that a MIL algorithm should identify. With the help of this illustration, it is not difficult to imagine that, one or few noisy negative instances coming close to a true positive instance prototype could lower the diverse density drastically and thus lead to a dramatic decrease in performance. Herein lies a core argument to the MIL definition - the strictness of positive neighbourhood. We show that DD-based algorithms are not tolerant even to a single negative instance in an arbitrary positive instance neighbourhood. Such strict assumptions are not suitable for real-world (medical imaging) data wherein the feature space can be noisy. In this paper, we propose modifications to traditional non-parametric methods adapting them to MIL. We demonstrate their effectiveness against DD taking into consideration the complex arrangements of a typical MIL feature space. In particular, the formulation aims at easing the dramatic impact of noisy negative instances on instanceprototyping in DD-based approaches. The formulation draws intuition from k -nearest-neighbour classification and thus leads readily to an efficient learning algorithm. It employs an aggregated and weighted distance measure computed from any point to its neighbouring instances labeled according to their respective parent bags, conforming to MIL requirement. Analysis with simulated data and experiments with real data in comparison to existing state-ofthe-art approaches suggest that the proposed method, while enjoying simplicity in formulation and learning, has the potential of delivering superior performance for challenging benchmark datasets. The remainder of the paper is organized as follows. Section 2 cites related works, while Section 3 describes the proposed method. Section 4 presents the experimental setup and discusses results on the various evaluation datasets. Section 5 provides concluding remarks.

2. Related Works
MIL was first introduced for the problem of drug activity prediction [10], where axis-parallel hyper-rectangles (APR) were used to design three variants of enclosure algorithms. The APR algorithms tried to surround at least one positive instance from each positive bag while eliminating any negative instances inside it. Any test bag was classified positive as long as it had at least one instance within the APR. Conversely a bag was classified as negative when it had no instance represented within the APR. The first density-based formulation of MIL was diverse density (DD) [19]. DD is not a conventional density but is rather defined as the intersection of the positive bags against the intersection of the negative bags. It is a measure that is high at any point on the feature space x if x is closer to positive instances and is farther away from negative instances.

2606

The local maxima of DD would yield a potential concept for the positive instances. Several local maxima can yield several prototypes of positive instances that can be far apart in the feature space. Some of these prototypes can be separated by other negative instances. The concept point of a diverse density in a MIL feature space was defined as, arg max
x i + P r ( x = t| B i ) i - P r ( x = t| B i ).

(1)

These local maxima were termed as instance prototypes. A noisy-or model was used to intuitively maximize the DD in Equation 1. This was further developed to assume more complicated and disjoint concepts in EM-DD and further developed by other methods including DD-SVM and Accio [35] [9] [24]. The major drawback of the diverse density arises in a situation where the distribution of negative instances is noisy. In other terms, if one instance prototype has a negative distribution closer to the prototype than the others, then its diverse density is largely lower than that of the others, as DD unfairly favours the distribution of positive instances that is farther away from negative instances than those that are relatively closer. This makes it hard to define that particular prototype in such situations. Even the presence of one noisy negative instance near the potential instance prototype can lower the DD drastically as we show in the later sections. In figure 2 the prototype P 4 was the twenty second largest local maxima in the DD of the feature space. If there were a bag that contained only one positive instance near P 4 but was still close enough to the negative instances, chances are that this bag will be misclassified as negative. DD defined in such a formulation provides a density-like function that is fickle and is easily affected by introducing even just one negative sample closer to the positive prototype. The maximization procedure for DD is started from initial guesses. An idea was put forward by Chen and Wang that the maximization should start from every instance in every positive bag (or at least a large sample of positive bags) so that unique local maxima in DD can be identified [9]. A plethora of methods still use this DD formulation [9] [8] [24] [35] [18]. The decision boundary of a DD system is a hyper-ellipsoid in the feature space. A kernel based maximum-margin approach would construct hyperplanar decision boundaries characterizing complex decision surfaces. The first formulation of a support vector machine (SVM) for MIL was proposed in 2002 [2]. They devised an instance-level classifier mi-SVM and a bag-level classifier MI -SVM. In a way, MI -SVM maximized the margin between the most positive instances and the least negative instances in positive and negative bags respectively. The MI SVM framework is now modified and re-christened as latent-SVM which plays a central role in the deformable-part models based object recognition algorithms [12]. MILIS

provided a similar SVM-based approach with a feedback loop to select instances that provided a higher training stage confidence [13]. This was an idea adapted from a previously existing related idea, MILES [8]. The first distance-based non-parametric, lazy learning approach to MIL was taken by citation-k-NN [29]. Interbag distances were found using a minimal Hausdorff distance. A k-nearest neighbour approach was used along with this distance to classify a new bag or to retrieve closer bags. This did not always work in a MIL setting as k-NN uses a majority voting scheme. If a positive bag contains fewer number of inherently positive instances than inherently negative instances, majority of its neighbours are going to be negative and the algorithm was confused by the false-positives it reported. Therefore the concept of citers was introduced. If k-NN refereed its neighbours, then its neighbours are cited by citers. Citers are the backward propagated references, in the sense that they refer back the considered instance. Though it was a generalized approach, citation k-NN did not work as well when positive instances were clustered and such clusters were separated by negative instances, in which case the citers and references did not always compliment each other. This problem does not apply to all nearest neighbour based approaches. Nearest neighbour approaches should be used properly and their smart usage was discussed in [6]. A novel concept of bag to class (B2C) distance learning was adopted for the use of k-NN. A complimentary idea was utilized in a MIL set-up by learning class to bag (C2B) distances by combining all training bags of a particular class to form a super-bag [26] [31]. A similar instance specific distance learning approach was used in [27]. On further study, this was reformulated as a l2,1 minimax problem and was solved with some effort [28]. A similar idea was implemented to group faces in an image by considering inter bag or bag to bag (B2B) distances in [15]. A related bag to bag approach is used to quantify super-bags in [3]. Most of the MIL algorithms presented above assume that the bags are independent. Though it is a reasonable assumption in a computer vision context, it might not be a general idea. Zhang et al., explored the MIL idea for structured data [34]. A data-dependent mixture model approach was developed in [30]. Another approach designed specifically for special data space is the fast bundle algorithm for MIL [5]. One important assumption in the early understanding of MIL is that every positive bag must contain at least one positive instance. Chen et al. felt this was too restrictive and developed a feature mapping using instance selection that projects a MIL problem into a much simpler supervised learning problem using an instance similarity measure [8]. This counter-assumption was also used in a histopathology cancer image learning system using a multiple clustered instance learning approach [33]. Although in a MIL for-

2607

mulation bag level classification is sufficient and instance level classification though clever, is not required, many algorithms attempt to identify positive instances. A SVM was used to minimize the hinge loss (modeled as slack variables) to identify positive instances in [32]. The above methods cater to certain particular configurations of the MIL space and are suitable for particular domains.

3. The proposed approach
Consider figure 2. Though not universal, this figure illustrates a typical MIL feature space. The instances arising from regions P 1 to P 4 are potentially inherently positive instances as they are farther away from negative instances while being closer to other positive instances. The instances from positive bags in other regions, along with negative instances are in reality, negative instances as they are close in proximity to negative instances from negative  bags.  X (1) Y (1)  X (2) Y (2)   (3)   Y (3)  Suppose we have labeled data D =  X   .  . .  . . . 
( i) th

Figure 3. Parsing the MIL feature space with a Parzen window technique. It can be seen that this follows the properties of a MIL density-like.

X ( n) Y ( n) where X is the i bag in the dataset and Y (i)  {0, 1} is its label. Internally, each bag X (i) contains mi (often is a constant m by design, particularly in image classification ( i) ( i) ( i) contexts) instances such that X (i) = {x1 , x2 , . . . xmi }. Consider a small region R of volume V in this feature space. The estimate for the density of instances from positive bags + is given by (|k V|)/n , where k + is the set of instances from positive bags in the region R and |k + | its cardinality, and n is the number of instances in all of the feature space. Similarly the estimate for the density of negative instances is - given by (|k V|)/n , where k - is the set of instances from negative bags in the region R, |k - | is the number of negative instances in the region R. + - Putting them together, (|k V|)/n - (|k V|)/n is a measure that, will be high if the number of positives exceed the number of negatives in that region, will be low if the number of negatives exceed the number of positives in that region, and will be 0 if the number of positives equal the number of negatives within that region. Alternatively, if one considers a (rectangular) Parzen window,  ( u) = 1, 0, |uj |  h where, j = 1, 2, ...d, otherwise (2)

+ - where, x is any location on the feature space and ki and ki are instances from positive and negative bags within that region respectively. Such a parsing of the MIL feature space of figure 2 is shown in figure 3. The properties of the function fP arzen (x) hold similar to that of DD and can be easily observed in figure 3. The choice of the size of the region (analogous to the selection of the variance for the Gaussian in the DD formulation) and the Parzen window functions are in line with that of a traditional Parzen window: if the size becomes too large, the measure will not have sufficient resolution. Picking a proper region-size would be a practical difficulty.

the aforementioned measure can also be formulated as, 1 fparzen (x) = n
+ |k n |

i=1

+ 1 1 x - ki ( )- V h n

- |k n |

i=1

- 1 x - ki ( ) V h (3)

Instead of considering a region R of fixed size, let's limit to a fixed number of neighbours k . In this set-up, we start with a region of zero volume from x and grow two regions, one for positive instances and one for negative instances, until we just enclose for each of the regions, k points respectively. This enables us to have different sized regions for positive and negative estimates respectively. While it appears to be a simple k-NN approach to density estimation, we emphasize that we are not using the nearest-neighbour voting rules. In fact, a direct application of nearest-neighbour voting technique will not work on a MIL space as was pointed out by Wang et. al, but the idea of nearest neighbour can still be modified and used to suit the MIL needs [29]. The vote contributions of positive and negative neighbours enclosed by the two regions are their respective kernelized distances to the point x, instead of a uniform majority vote. This aggregated vote can be formu-

2608

This is an indicator function that classifies the bag 1 if it has at least a instances classified as positive and 0 other-wise. Typically in most MIL settings a = 1, although this need not be the case generally. The aim of this non-parametric empirical risk minimization formulation is to minimize the training error, ^ ( b) =
Figure 4. A region of a typical 2D MIL feature space and its parse using the k-NN measure. Red represents positive and blue represents negative.

1 n

n

1{b(X (i) ) = Y (i) }, (X, Y )  D.
i=1

(7)

^ ^ that best minimizes ( by estimating T b) as, ^ ^ = arg min ( b) T
T

(8)

lated as,
|k - | |k + | - (||x - ki ||) - i=1 i=1 + (||x - ki ||)

fkN N (x) =

such that, |k + | = |k - | = k.

(4)

Once the threshold is learnt, classification is performed directly by using the bag-level classifier in equation 6 with the learnt threshold. Note that in MIL, it is not required, although possible in this case, to label each instance in the bag. The labeling of instances can be as follows: y (x) = h(x)|T =T ^ (9)

where, (.) is a monotonically increasing sub-modular function, k is the number of neighbours considered, and k + and k - are now the set of k instances from positive and negative bags that are the nearest to x respectively. (.) is used as a way to scale distances when the feature space is arbitrarily large. It can be considered as normalization. For all our experiments, we typically use (x) = x. The advantage of fixing the number of neighbours is that in a region where there are no points or very few number of points, we will get a block of uniform measure and in a region where there is a high density of points, we will get a smoothly varying measure. Such a measure is shown in figure 4. The impact of the number of neighbours k is similar to that of the size of the region R in the Parzen window idea. If k is too small, the measure is going to give information about a very small local region and is thereby unreliable. If k is too large , the impact of proximity is going to be averaged out.

This process is equivalent to maximizing the equation 4 (or 3) for all points of feature space and considering the local maximas as instance prototypes, as was described by Chen et. al, for the DD formulation [9]. This now enables comparison to prototyping-based methods. Such a formulation can now be re-written as,
|k - | |k + | - (||x - ki ||) - i=1 i=1 + (||x - ki ||) ,

x ^ = arg max
x

such that,|k + | = |k - | = k.

(10)

Learning
Learning under this formulation is a straight forward threshold learning and this is done by maximizing the validation accuracy. An instance-level classifier using this measure can be constructed as, h(x) = 1{fkN N (x)  T } (5)

This is an indicator function that outputs 1 if the measure is above a threshold T and 0 if the measure is below the threshold T . We can use this instance-level classifier to construct a bag-level classifier.
m

b ( X ) = 1{
i=1

h(xi )  a}x1 , x2 , . . . , xm  X.

(6)

where x ^ is a prototype positive instance. One advantage of using equation 10 is that once the prototypes are found, we neither need the entire dataset anymore nor do we need to calculate distances to all the points in the dataset. The prototypes easily divide the feature space into probabilistic Voronoi tessellations such as in figure 4. We could also estimate a radius around every prototype to isolate hyperspherical regions that are positive. We solve this optimization problem by using an idea similar to the one used in [9]. We start a local gradient ascent from every instance from every positive bag in the training dataset and find a local maxima. Since such maximas can only ever end in a high density region of true positive instances from positive bags and since we start each gradient ascent from every instance in every positive bag, each ascent is computationally tractable in small number of iterations. Indeed, often few well-chosen instances from positive bags make this convergence faster and such techniques can be found for maximizing the DD in various papers previously surveyed in section 2. Similar techniques can be

2609

applied here as well. All the local maximas are sorted (after non-maximal suppression) and top N are considered as instance prototypes. It is to be noted that for the dataset shown in figure 2, while the top 5 maximas were enough to find all four prototypes for our approach, it takes top 24 maximas for DD to find the four prototypes. The k for k -NN is picked here by a typical elbow method. Once local maximas (instance prototypes) are found we can again maximize a validation accuracy jointly for all instance prototypes to find a threshold of classification for each prototype in terms of the distance to the prototype, hence creating a hyper-spherical decision regions around each prototype. Thus the decision boundaries of this method creates a tessellation of the feature space. The tessellation is a superposition of hyper-spherical regions around a positive prototype with varying radii.

Methods DD [19] EM-DD [35] citation (k)-NN [29] mi-SVM [2] MI-SVM [2] DD-SVM [9] MILES [8] MIforest [31] MILIS [13] ISD [27] ALP-SVM [3] MIC-Bundle [5] Ensemble [18] Proposed

MUSK 1 88.9% 84.8% 92.4% 87.4% 77.9% 85.8% 86.3% 85% 88.6% 85.3% 87.9% 84% 89.22% 92.4%

MUSK 2 82.5% 84.9% 86.3% 83.6% 84.3% 91.3% 87.7% 82% 91.1% 79.0% 86.6% 85.2% 85.04% 86.4%

4. Experiments and Results
In this section we provide details of our experiments and the results from those experiments. We evaluated our method using three standard MIL datasets: the musk dataset, Andrew's datasets, the Corel datasets (both 1k and 2k), and our own dataset: the DR dataset. For all the results shown on all the datasets, we used the most common implementation methodologies, including data splits, cross validations and average over runs that were found in literature. This enabled us to compare against results that were published in the same. When results were not available or when the protocol doesn't match, we evaluated the results using the codes from CMU MIL toolbox1 . In case of MILES, the results were obtained by using the author's original code2 .

Table 1. Performance of various MIL algorithms on the musk dataset.

MUSK datasets are uni-concept datasets. For instance, in MUSK 1, among a total of 476 unique instances each with feature values ranging from -348 degrees to 336 degrees, there are only 633 unique feature values. In such a heavily quantized feature space that is 166 dimensional, detecting one potential instance prototype is easier for density based algorithms.

4.2. Andrew's datasets
Andrews et. al, in their mi-SVM paper proposed the use of three classification datasets, elephant, fox and tiger, for the use of evaluating multiple-instance learning [2]. These are now popular benchmark datasets in the MIL literature. We also test our algorithm on these datasets using the same specifications mentioned on the said article. Each dataset has 200 images with 100 positive and 100 negative images. The number of instances in each category are 1391, 1320 and 1220 respectively with varying number of instances per bag. Each instance is a 230 dimensional feature vector. We train on a 2/3 random split of the data and test on the remaining 1/3 of the unseen data. The results are maximized over 15 runs of validation and are shown in table 2. Our result while being the best in the Elephant and Fox classes is almost as good as the best in the Tiger class. It is to be noted that we are significantly higher in the Fox class which is widely considered to be a notoriously noisy dataset for MIL. This is a strong indicator of our method's adaptability.

4.1. Musk dataset
An accepted benchmarking dataset in the MIL literature is the musk dataset. The musk dataset is well-described in [10]. Musk dataset is a benchmark feature space used to predict drug activity. It contains two sub-datasets: MUSK1 and MUSK2. MUSK 1 contains 92 molecules with 47 musk and 45 non-musk molecules. MUSK 2 contains 102 molecules with 39 musk and 63 non-musk molecules. Each bag contains variable number of instances with 166 dimensional features and binary labels. We use the standard implementation specifications that is used in the original APR paper and other published literature: ten-fold crossvalidation over the entire dataset, since its easier to compare against a plethora of methods [10]. Table 1 compares the performance of various algorithms against the proposed method. It can be seen that the proposed method is best in MUSK 1 and among the high performing methods in MUSK 2.
MIL toolbox: http://www.cs.cmu.edu/~juny/MILL homepage: http://www.cs.olemiss.edu/~ychen/ MILES.html
2 MILES 1 CMU

4.3. Corel dataset
Corel is another well known, image categorization dataset for MIL benchmarking. The Corel-2k dataset consists of 2000 images. There are 20 classes and each class consists of 100 images. The Corel-1k dataset is a subset

2610

Methods citation k-NN [29] mi-SVM [2] MILES [8] MIforest [31] ISD [27] ALP-SVM [3] MIC-Bundle [5] Ensemble [18] Proposed

Elephant 79.2% 79.7% 70% 84% 77.9% 84% 80.5% 84.25% 86%

Fox 62.5% 62.9% 56% 64% 63% 69% 58.3% 63.05% 73.94%

Tiger 82.6% 79% 62% 82% 85.3% 86% 79.11% 79.30% 85.7%

Methods DD [19] EM-DD [35] citation k-NN [29] mi-SVM [2] MILES [8] Proposed

Accuracy 61.29% 73.5% 78.7% 70.32% 71% 81.3%

Table 4. Performance of various MIL algorithms on DR dataset.

Table 2. Performance of various MIL algorithms on Andrew's dataset.

Methods mi-SVM [2] MI-SVM [2] MILES [8] DD-SVM [9] MILIS [13] Proposed

Corel-1k 76.4% 75.1% 82.3% 81.5% 83.8% 87.3%

Corel-2k 53.7% 55.1% 68.7% 67.5% 70.1% 71.9%

high-quality colour fundus image database of 425 images comprising 160 normal images, and 265 affected images to test our algorithm on. This dataset was constructed from publicly available databases including DiabRetDB0 [11], DiabRetDB1 [17], STARE [21] and Messidor3 and has been used in some existing studies [7] [25]. The balance of the database is more towards the positive bags and this makes it more challenging for a MIL algorithm. The results were all evaluated using a 2/3 - 1/3 train-test split. Prototyping DR instances

Table 3. Performance of various MIL algorithms on Corel dataset.

of this dataset with the first 10 difficult categories. Table 3 shows the performance of the proposed approach in the corel dataset. It is to be noted that we are producing the best results in the Corel dataset. Training-testing data is again a 2/3 - 1/3 split.

4.4. A DR dataset
As was briefly discussed in section 1, DR image classification is an application especially suitable for MIL. In practice, the difficulty in this problem arises from the fact that the physical and observable difference between a normal eye and a pathological eye can be very small, localizing to regions with slightly different characteristics. This can be seen in figure 1. A variety of classification and retrieval schemes have been tried on DR images. Structural Analysis of the Retina (STARE) is one of the earliest attempts to solve the DR conundrum [21] [14]. STARE performs automated diagnosis and comparison of images to search for images similar in content. Recently other learning approaches were developed to identify relevant patterns using local relevance scores [23]. Application of MIL approaches to DR is gaining interest in recent years [22]. In this study, we consider the auto colour correlogram (AuoCC) as a colour feature, which is well-studied in the medical imaging literature [16]. A modified and quantized 64-bin AutoCC feature is extracted for each instance in an image [25]. We neglect the black regions and sample 48 non-overlapping instances from every image. We use a

In the prototyping sense, each prototype of positive instances should roughly correspond to one type of lesion. As we use colour features this is easily possible. We estimated a total of about 35 different types of lesion prototypes using our algorithm and verified it with EM-DD's prototypes. EM-DD had its maximum accuracy at about 40 prototypes. It is reasonable to assume from this information that there are in the range of, 35-40 different positive prototypes, each of which in the feature space might correspond to a unique lesion type or character. In this feature space, the negative instances are of three types: normal skin, nerves and the optical disk. This is a reasonably noisy datasets and often has only one or two instances among 48 instances that are positive in a positive bag. Though the distribution of the optic disc might be noisy, and the number of true positive instances are very low, the proposed algorithm has the potential to adjust to it. Table 4 shows the results of the proposed approach on the DR dataset, where the proposed method stands best.

4.5. Sensitivity to labeling error
Although not an implicit feature of the proposal, we perform the experiments to demonstrate the proposed method's sensitivity to labeling error, exactly similar to the one described in [8]. We deliberately flip the labels for a range of percentages of labels randomly on our training split and test the trained model on the original labels in the testing split. The split was 2/3 - 1/3. The accuracies of the proposed method on various datasets are shown in figure 5. After
3 Kindly provided to us by the messidor program partners. Visit http: //messidor.crihan.fr

2611

Figure 5. Accuracy vs Percentage of labels flipped for the proposed method. Flatter curve is good.

simple, yet novel usage of non-parametric learning philosophy to the MIL problem. In particular, we analyzed the MIL feature space using a k- NN philosophy and proposed a new formulation based on distances to k-nearest neighbours. The new formulation was compared and contrasted with the widely used DD formulation. The proposed approach was tested on the musk datasets, Andrews dataset and the corel datasets, and was found to be effective. The algorithm was used to solve the DR image classification problem and was found to be the best among other algorithms. We therefore conclude that a non-parametric learning philosophy to MIL not only makes intuitive sense but can also be a powerful tool for most general cases.

Acknowledgement
The work was supported in part by a grant (#W911NF1410371) from the Army Research Office (ARO). Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the ARO.

References
[1] J. Amores. Multiple instance classification: Review, taxonomy and comparative study. Artificial Intelligence, 201:81­ 105, 2013. [2] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support vector machines for multiple-instance learning. Advances in neural information processing systems, 15:561­568, 2002. [3] B. Anti´ c and B. Ommer. Robust multiple-instance learning with superbags. In Computer Vision­ACCV 2012, pages 242­255. Springer, 2013. [4] B. Babenko, M. Yang, and S. Belongie. Robust object tracking with online multiple instance learning. IEEE PAMI, 33(8):1619­1632, 2011. [5] C. Bergeron, G. Moore, J. Zaretzki, C. M. Breneman, and K. P. Bennett. Fast bundle algorithm for multiple-instance learning. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(6):1068­1079, 2012. [6] O. Boiman, E. Shechtman, and M. Irani. In defense of nearest-neighbor based image classification. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1­8. IEEE, 2008. [7] P. S. Chandakkar, R. Venkatesan, and B. Li. Retrieving clinically relevant diabetic retinopathy images using a multiclass multiple-instance framework. In SPIE Medical Imaging, pages 86700Q­86700Q. International Society for Optics and Photonics, 2013. [8] Y. Chen, J. Bi, and J. Z. Wang. Miles: Multipleinstance learning via embedded instance selection. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 28(12):1931­1947, 2006. [9] Y. Chen and J. Wang. Image categorization by learning and reasoning with regions. The Journal of Machine Learning Research, 5:913­939, 2004.

Figure 6. Drop in accuracy at various noise levels for proposed and MILES on the DR dataset. The lower the value the better.

about 20% of labels are corrupted, the proposed method still loses only about 5% accuracy and only when about one-third of the labels are corrupted, the proposed method loses about 10% accuracy. The average drop in accuracy for both the proposed method and MILES are compared in figure 6. It is clear that MILES and the proposed algorithm follow the exact same trend. This trend is clearly indicative that the proposed method is as good as MILES and is often times better, when it comes to sensitivity to labeling noise. It is noteworthy that MILES is considered the stateof-the-art benchmark for sensitivity to labeling error out of all MIL methods published and that was one of its core contributions.

5. Conclusion
In this paper, we postulate whether lazy learning ideas can be carried over from traditional non-parametric methods for supervised learning to a MIL setup. We proposed a

2612

[10] T. Dietterich, R. Lathrop, and T. Lozano-P´ erez. Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence, 89(1):31­71, 1997. [11] T. et al. Diabretdb0: Evaluation database and methodology for diabetic retinopathy algorithms. In Technical Report. [12] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained partbased models. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9):1627­1645, 2010. [13] Z. Fu, A. Robles-Kelly, and J. Zhou. Milis: Multiple instance learning with instance selection. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(5):958­977, 2011. [14] M. Goldbaum, N. Katz, S. Chaudhuri, and M. Nelson. Image understanding for automated retinal diagnosis. In Proceedings of the Annual Symposium on Computer Application in Medical Care, page 756. American Medical Informatics Association, 1989. [15] M. Guillaumin, J. Verbeek, and C. Schmid. Multiple instance metric learning from automatically labeled bags of faces. Computer Vision­ECCV, pages 634­647, 2010. [16] J. Huang, S. Kumar, M. Mitra, W. Zhu, and R. Zabih. Image indexing using color correlograms. In Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, pages 762­768. IEEE, 1997. [17] T. Kauppi, V. Kalesnykiene, J. Kamarainen, L. Lensu, I. Sorri, A. Raninen, R. Voutilainen, H. Uusitalo, H. K¨ alvi¨ ainen, and J. Pietil¨ a. Diaretdb1 diabetic retinopathy database and evaluation protocol. Proc. Medical Image Understanding and Analysis (MIUA), pages 61­65, 2007. [18] Y. Li, D. M. Tax, R. P. Duin, and M. Loog. Multiple-instance learning as a classifier combining problem. Pattern Recognition, 46(3):865­874, 2013. [19] O. Maron and T. Lozano-P´ erez. A framework for multipleinstance learning. NIPS, pages 570­576, 1998. [20] O. Maron and A. Ratan. Multiple-instance learning for natural scene classification. In IEEE ICML, volume 15, pages 341­349, 1998. [21] B. McCormick and M. Goldbaum. Stare= structured analysis of the retina: Image processing of tv fundus image. In del USA-Japan Workshop on Image Processing, Jet Propulsion Laboratory, Pasadena, CA, 1975. [22] G. Quellec, M. Lamard, M. Abr` amoff, E. Decenci` ere, B. Lay, A. Erginay, B. Cochener, and G. Cazuguel. A multiple-instance learning framework for diabetic retinopathy screening. Medical Image Analysis, 2012. [23] G. Quellec, M. Lamard, B. Cochener, C. Roux, G. Cazuguel, E. Decenciere, B. Lay, and P. Massin. A general framework for detecting diabetic retinopathy lesions in eye fundus images. In Computer-Based Medical Systems (CBMS), 2012 25th International Symposium on, pages 1­6. IEEE, 2012. [24] R. Rahmani, S. Goldman, H. Zhang, S. Cholleti, and J. Fritts. Localized content-based image retrieval. IEEE transactions on pattern analysis and machine intelligence, 30(11):1902, 2008. [25] R. Venkatesan, P. Chandakkar, B. Li, and H. K. Li. Classification of diabetic retinopathy images using multi-class

[26] [27] [28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

multiple-instance learning based on color correlogram features. In Engineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE, pages 1462­1465. IEEE, 2012. H. Wang, H. Huang, F. Kamangar, F. Nie, and C. Ding. Maximum margin multi-instance learning. NIPS, 2011. H. Wang, F. Nie, and H. Huang. Learning instance specific distance for multi-instance classification. In AAAI, 2011. H. Wang, F. Nie, and H. Huang. Robust and discriminative distance for multi-instance learning. In IEEE CVPR, pages 2919­2924. IEEE, 2012. J. Wang and J. Zucker. Solving the multiple-instance problem: A lazy learning approach. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 1119­1126. Morgan Kaufmann Publishers Inc., 2000. Q. Wang, L. Si, and D. Zhang. A discriminative datadependent mixture-model approach for multiple instance learning in image classification,. In In Proceedings of the 12th European Conference on Computer Vision (ECCV-12),, 2012. Z. Wang, S. Gao, and L.-T. Chia. Learning class-to-image distance via large margin and l1-norm regularization. In Computer Vision ECCV 2012, pages 230­244. 2012. D. Wu, J. Bi, and K. Boyer. A min-max framework of cascaded classifier with multiple instance learning for computer aided diagnosis. In IEEE CVPR, pages 1359­1366. IEEE, 2009. Y. Xu, J. Zhu, E. Chang, and Z. Tu. Multiple clustered instance learning for histopathology cancer image segmentation, classification and clustering. CVPR, IEEE, 2012. D. Zhang, Y. Liu, L. Si, J. Zhang, and R. Lawrence. Multiple instance learning on structred data. In Twenty-Fifth Annual Conference on Neural Information Processing Systems (NIPS), 2011. Q. Zhang and S. Goldman. Em-dd: An improved multipleinstance learning technique. Advances in neural information processing systems, 14:1073­1080, 2001. Q. Zhang, S. Goldman, W. Yu, and J. Fritts. Content-based image retrieval using multiple-instance learning. In Machine Learning-International Worskshop-Then Conference-, pages 682­689, 2002.

2613

IEEE SIGNAL PROCESSING LETTERS, VOL. 23, NO. 4, APRIL 2016

449

Compressive Sensing Reconstruction of Correlated Images Using Joint Regularization
Kan Chang, Pak Lun Kevin Ding, and Baoxin Li, Senior Member, IEEE
Abstract--This letter proposes a novel compressive sensing reconstruction method for correlated images by using joint regularization, where a compensation-based adaptive total variation (CATV) regularization and a multi-image nonlocal low-rank (MNLR) regularization are included. In CATV, local weights are assigned to the residual values in the gradient domain so as to constrain the regularization strength at each pixel. In MNLR, the search of similar patches goes across different images so that both self-similarity and inter-image similarity are explored. Afterward, an efficient algorithm is proposed to solve the joint formulation, using a Split-Bregman-based technique. The effectiveness of the proposed approach is demonstrated with experiments on both multiview images and video sequences. Index Terms--Compressive sensing, motion estimation/disparity estimation (ME/DE), nonlocal low-rank regularization (NLR), total variation.

prior knowledge for regularizing the solution to the following minimization problem: ^ i = argmin (ui ) u
ui

s.t. yi = i ui

(2)

I. I NTRODUCTION

T

HIS LETTER focuses on the compressive sensing (CS) reconstruction of a set of correlated images, each of which is independently acquired by the CS technique [1], [2]. The correlated images could be multiview images which represent a scene from different view points, or a series of video frames which are taken at different time points. More specifically, the CS measurement of the original ith image is acquired by y i = i u i (1)

where ui  RN stands for the original ith image, yi  RM is the measurement of the ith image, and i  RM ×N is the measurement matrix. Usually, M << N , and we call M/N the subrate of CS. To reconstruct the underlying images from such an underdetermined system, one common way is to employ image
Manuscript received December 13, 2015; revised February 03, 2016; accepted February 04, 2016. Date of publication February 11, 2016; date of current version March 03, 2016. This work was supported by the Natural Science Foundation of China under Grants 61401108 and 61261023, and the Natural Science Foundation of Guangxi under Grant 2013GXNSFBA019272. The work of B. Li was supported by the Natural Science Foundation under Grants 1135616 and 0845469. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Deanna Needell. K. Chang is with the School of Computer and Electronic Information, the Guangxi Key Laboratory of Multimedia Communications and Network Technology (Cultivating Base), and the Key Laboratory of Multimedia Communications and Information Processing, Guangxi University, Nanning 530004, China (e-mail: changkan0@gmail.com). P. L. Kevin Ding and B. Li are with the Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287 USA (e-mail: kevinding@asu.edu; baoxin.li@asu.edu). Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/LSP.2016.2527680

where  is the regularization term denoting image prior. Different types of intra-image-based regularization have been investigated, including total variation (TV) minimization [3], nonlocal low-rank regularization (NLR) [4], nonlocal meansbased regularization [5], autoregressive model [6], dictionary learning-based sparse representation [7], etc. Besides intra-image prior information, inter-imagestructured sparsity also needs to be explored for correlated images. The most direct way to do this is to require a joint sparsity of the whole image set, such as [8]­[11]. However, such methods are sensitive to motion or disparity. Improvement may be obtained by using the neighboring images to help reconstruct the current one, such as [12]­[20]. In [21], correlation between a pair of images was directly estimated in the compressed domain so as to reduce the computational complexity. The main contributions of this letter are listed as follows. First, we propose a compensation-based adaptive TV regularization approach, where the reliability of each compensated pixel is considered. Second, we extend the existing NLR from single-image pattern to multiple-image pattern. Note that different from the similar model in [22], we additionally use optical flow (OF) fields to guide the central points of search windows. Finally, we jointly incorporate these two types of regularization into a minimization problem and design an optimization algorithm for CS reconstruction of correlated images. Experiments show that our proposed algorithm is capable of achieving significantly better reconstruction than several state-of-the-art methods. To facilitate evaluation and further exploration of the proposed algorithm, we will publish the source code on the third author's webpage.1 II. P ROPOSED J OINT R EGULARIZATION A. Compensation-Based Adaptive TV Regularization To explore inter-image correlation, we can utilize disparity estimation/disparity compensation (DE/DC) for multiview images or motion estimation/motion compensation (ME/MC) for video sequences. As have been proved in [17]­[20], requiring small prediction error in the gradient domain is able to get satisfactory results. In [17], the compensation-based TV (CTV) regularization term was written as CTV (ui ) = D(ui - si )
1 [Online].

1

(3)

Available: http://www.public.asu.edu/~bli24/

1070-9908 © 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

450

IEEE SIGNAL PROCESSING LETTERS, VOL. 23, NO. 4, APRIL 2016

T T where D = [DT h , Dv ] with Dh , Dv denoting the horizontal and vertical finite difference operators, respectively, and

si =

1 (Fi-1 ui-1 + Bi+1 ui+1 ) 2

(4)

with Fi-1 and Bi+1 standing for the forward and backward compensation operators by using the optical flow fields of the (i - 1)th and (i + 1)th images, respectively. We only used the closest images to build the compensated results because: 1) usually a further distance between two images leads to a less accurate compensated image and 2) the computational complexity of estimating the OF fields is high. Unfortunately, such an si is not always reliable on images with fine details, multiview images with large disparity, or video sequences with complex motion. Therefore, simply minimizing the regularization term (3) may sometimes deteriorate the quality of reconstructed images. Considering the reliability at each pixel in si , we propose to add local weights to the residual values in the gradient domain, leading to a compensation-based adaptive TV (CATV) regularization CATV (ui ) = Wi (D(ui - si ))
1

Fig. 1. Example of intra-image and inter-image search. OF fields are used to guide the central points of search windows in adjacent images. The sizes of search windows in different images are the same.

Xj . It is reasonable to expect the formed matrix Xj has a lowrank property (in practice, Xj is only approximately low rank due to noises and artifacts [4]). Hence for the ith image, NLR is calculated as NLR (ui ) =
j

1 ^ Rj ui - L j 2

2 F

+ Rank(Lj )

(10)

(5)

is the where Wi denotes the vector of local weights, and Hadamard product. If an unreliable predicted value occurs at a pixel position in si , a small weight should be assigned to it. To appropriately build Wi , a good spatial information indicator is needed. Here, the second derivative-based indicator called difference curvature [23] is utilized. It is a pixel-based indicator which can effectively discriminate edges from flat regions or noises. For the mth pixel, it is defined as Cm = ||e | - |e || e = e = e2 x exx + 2ex ey exy + 2 e2 x + ey e2 y eyy (6) (7) (8)

^ j stands for extracting similar patches for the j th exemwhere R ^ j ui ;  is a tradeoff parameter; The plar patch, i.e., Xj = R newly introduced low-rank matrix Lj is close to Xj . For correlated images, just exploring nonlocal low-rank property inside an image is not enough. Therefore, we propose to extend (10) to a new multi-image nonlocal low-rank (MNLR) regularization, which is written as
MNLR (U) =
i j

1 ~ Ri,j U - Li,j 2

2 F

+ Rank(Li,j )

(11)

2 e2 y exx + 2ex ey exy + ex eyy 2 e2 x + ey

where ex and ey represent the first-order gradients of the pixel along x (horizontal) and y (vertical) directions, respectively; exx , eyy , and exy are the second-order gradients of the pixel. With Cm , the mth local weight in Wi is computed as wm = 1 1 + Cm (9)

where  is the contrast factor. It should be noted that our Wi is designed for residual in the gradient domain, which is different from other works such as [24] and [25] (i.e., Cm is computed on (ui - si ) instead of ui ). In practice, since the original images are not available, the previously reconstructed images are needed to calculate Wi . B. Multi-Image Nonlocal Low-Rank Regularization NLR [4] is an efficient tool for describing single image characteristics. To apply it, a target image is first divided into overlapped patches with size Sp × Sp . As a similarity metric, the l2 differences between the j th exemplar patch and the candidate patches within a search window are computed. After that, Np similar patches are found and grouped into a matrix

T T T where U = [uT 1 , u2 , . . . , un ] , and n is the number of corre^ j in (10), for the j th exemplar lated images. Different from R ~ i,j extracts its similar patches patch in the ith image, operator R from the (i - 1)th, ith, and (i + 1)th images. To find similar patches in three neighboring images, both intra-image and inter-image searches are needed. Fig. 1 illustrates how the search works. Recall that we have obtained OF fields when calculating si in (4). Thus, here they can be reused to guide the central points of search windows in the (i - 1)th and (i + 1)th images. More specifically, given a central point of a window in the ith image, we locate its motion/disparity vectors in the OF fields and use these vectors to find where this point is in the adjacent images. This procedure helps to find the most similar patches, especially in cases where large disparity or motion occurs among neighboring images. Note that higher quality of reconstruction can be achieved if the search goes across more images. However, doing so would cause heavier computational burden. Through experiments, we found that searching in three continuous images has achieved significant improvement over single-image NLR [4]. In addition, we only have the OF fields for each pair of adjacent images, which means the guidance for the central points of search windows is not available in further images.

III. O PTIMIZATION A LGORITHM FOR CS R ECONSTRUCTION U SING J OINT R EGULARIZATION Since CATV and MNLR impose different prior knowledge within correlated images, jointly considering them can get satisfactory results. By doing so, the minimization problem for reconstructing a set of correlated images becomes

CHANG et al.: CS RECONSTRUCTION OF CORRELATED IMAGES USING JOINT REGULARIZATION

451

TABLE I M EAN PSNR S ( D B) C OMPARISON FOR M ULTIVIEW I MAGES R ECONSTRUCTION

Since rank-minimization problem is NP-hard, here, we have replaced Rank(Li,j ) in (11) with L(Li,j , ), which is a log det(·) surrogate function [26] and L(Li,j , ) = log det Li,j LT i,j
1/ 2

+ I

(13)

TABLE II M EAN PSNR S ( D B) C OMPARISON FOR V IDEO S EQUENCES R ECONSTRUCTION

where  is a small constant and I denotes the identity matrix. We use log det(·) here because it can better approximate rank than the widely used nuclear norm [4]. To efficiently solve problem (12), we introduce a new ~ (U - CU). Then, our joint variable d and let d = D regularization-driven Split-Bregman iteration [27] can be written as   Lk+1 = argmin{Li,j } i j (L(Li,j , )   i,j    k 2 ~  +1  2 Ri,j U - Li,j F )     Uk+1 = argmin Y - HU 2  2 U     +1 2  ~ + i j ( Ri,j U - Lk F) i,j (14) k k  ~ (U - CU) - b 2  + d - D  2     k+1 ~  dk+1 = argmind  - bk 2  2 2 d - D ( I - C) U     ~  + W d 1      k+1 k k+1 k+1 k+1
b ~ (U =b +D - CU )-d

where  is a tradeoff parameter. When considering the "L" subproblem, we treat every Li,j separately, and the solution can be written as [4]
+1 k T Lk i,j = Q( -  diag(g ))+ V

(15)

where QVT is the singular value decomposition (SVD) of ~ i,j Uk , and thin SVD is applied in our implementation. g k = R l k k 1/(l + ) ,  l denotes the lth singular value of Lk i,j , and (x)+ = max(x, 0). When solving the "U" subproblem, we use CUk to approximate CUk+1 and get the closed-form solution as follows:  -1
Uk+1 = HT H +  ~T ~ ~T ~ R i,j Ri,j +  D D
i j




i j k+1  ~T . R i,j Li,j

^ , {L ^ i,j }} = argmin 1 Y - HU {U U,{Li,j } 2 ~ + W +
i j

2 2

k ~ T (dk + DCU ~  D - bk ) + HT Y + 

(16)
2 F

~ (U - CU) D

1

1 ~ Ri,j U - Li,j 2

+ L(Li,j , )

(12)

To compute (16), conjugate gradient (CG) method is used. When dealing with the "d" subproblem, closed-form solution by the shrinkage formula [28] is given as ~ (I - C)Uk+1 + bk , W ~ / ). dk+1 = shrink(D With a vector x and a threshold Ts , we have shrink(x, Ts ) = max(|x| - Ts , 0) sgn(x). (18) (17)

T T where ,  , and  are tradeoff parameters, Y = [y1 , y2 , T T T T ~ = [W , W , . . . , . . . , yn ] , H = diag(1 , 2 , . . . , n ), W 1 2 T T ~ ] D = diag(D, D, . . . , D), and Wn   0 B2 0 ··· 0 0 B3 /2 ··· 0  F1 /2  .  . . . .   .. .. .. . C= . . . .  0 0 Bn /2 · · · Fn-2 /2 0 0 ··· 0 Fn-1

Note that the max operator here is implemented for each spatial index independently. Our algorithm, named joint regularization-based compressive sensing reconstruction (JR-CSR), is summarized as

452

IEEE SIGNAL PROCESSING LETTERS, VOL. 23, NO. 4, APRIL 2016

Fig. 2. Visual quality comparison for the ninth frame in City (subrate = 0.20). From left to right: DC-TV, DC-JTV, NLR-CS, JM-RCI, JR-CSR. TABLE III AVERAGE CPU T IME ( S ) FOR R ECONSTRUCTING O NE F RAME IN Foreman

Algorithm 1. JR-CSR ~ 0 , H, Y, ,  , ,  Input: U Outer loop for t = 0, 1, . . . , T ~ i,j }. ~ t to update C and {R Use U ~ = [1, 1, . . . 1]T Else Update W ~ by (9) If t  T0 W 0 ~ ( I - C) U ~ t , b0 = 0, U0 = U ~t Set d = D Inner loop for k = 0, 1, . . . , K +1 Update each Lk i,j by (15). Update Uk+1 by using CG method to solve (16). Update dk+1 via (17). ~ (Uk+1 - CUk+1 ) - dk+1 bk+1 = bk + D t+1 k+1 ~ Set U =U If Uk+1 - Uk 2 / Uk+1 2 < 10-4 Break End for End for ^ =U ~ T +1 return U ~ t and Uk denote the results in the outer loop Algorithm 1. U and the results in the inner loop, respectively. To get an accu~ , and {R ~ i,j } are updated several times in the rate result, C, W ~ is fixed. This outer loop. Note that in the first T0 iterations, W was found empirically to be able to improve the convergence while leading to a better result. The inner loop will stop if the relative change of U is smaller than a predefined threshold, or the maximum number of iterations is reached. IV. E XPERIMENTAL R ESULTS This section evaluates the performance of JR-CSR. All experiments were performed in MATLAB 2013b on a Lenovo computer with Intel(R) Core(TM) i7-4790 processor, 8.00G memory. Structurally random matrices (SRM) [29] were used as i , and we had the measurement yi = i ui . To generate ~ 0 , TVAL3 [30] software2 was utilized for each image. To U construct C, OF implementation3 of [31] was used. In this OF implementation, successive over-relaxation (SOR) was applied to solve the linear system resulting from the energy functional of [32], where the assumptions of brightness constancy, gradient constancy, and piecewise smooth flow field were combined. T , K , and T0 were set to 15, 25, and 5, respectively.  in (9) was set to 0.8, patch size Sp × Sp for MNLR was 6 × 6, and the number of similar patches Np was 45. ,  , , and  were tuned according to each subrate. The test datasets included four multiview image sets [half size Monopoly (MP), Tsukuba (TK), Venus and Art] from
2 [Online]. 3 [Online].

the middlebury multiview database4 and four video sequences [352 × 288 Foreman (FM), Football (FB), City, and Bus]. We tested the first five views of each multiview image set and the first 20 frames of each video sequence. Only grayscale images were considered. Four state-of-the-art algorithms were compared, including NLR-CS [4], JM-RCI [17], DC-JTV [20], and DC-TV [19]. For fair comparison, TVAL3 [30] was applied to obtain the initial results for all competing algorithms. The average peak signal-to-noise ratio (PSNR) results of each multiview image set and each video sequence are given in Tables I and II, respectively. One can see that JR-CSR beats all the benchmark methods in all cases. For example, in Table I at a subrate of 0.10, JR-CSR gets 3.05 dB PSNR improvement over the second best method, i.e., NLR-CS. For visual quality comparison, please see Fig. 2. The average running times for reconstructing one frame in Foreman are listed in Table III. We can find that JR-CSR is the slowest algorithm. The main computational burdens are introduced by iteratively updating {Li,j }, U, and C. For each Li,j , 2 Np r), where r is the rank the complexity of thin SVD is O(Sp ~ of Ri,j U. To update U and C, CG and SOR are used to solve the related linear systems, respectively. Given a linear system Ax = z, assume that Nn is the number of nonzero entries in matrix A, and Nc is the condition number of A. To get the  vector x, the complexity of one iteration of CG is O(Nn Nc ), while O(Nn ) is required for one iteration of SOR. To speed up JR-CSR, parallelization techniques may be the best choice. Other solutions, such as updating C in JR-CSR only once, removing either CATV or MNLR from (12), etc., would lead to different levels of quality loss. V. C ONCLUSION In this letter, we proposed two types of regularization, including CATV and MNLR, for CS reconstruction of correlated image sets. After incorporating the two regularization terms into the minimization problem, we designed an optimization algorithm called JR-CSR. Through experiments, we found that JR-CSR is able to deliver the best performance among all the tested methods, which demonstrates the effectiveness of the proposed joint regularization.
4 [Online].

Available: http://www.caam.rice.edu/~optimization/L1/TVAL3/ Available: http://people.csail.mit.edu/celiu/OpticalFlow/

Available: http://vision.middlebury.edu/stereo/data

CHANG et al.: CS RECONSTRUCTION OF CORRELATED IMAGES USING JOINT REGULARIZATION

453

R EFERENCES
[1] D. L. Donoho, "Compressed sensing," IEEE Trans. Inf. Theory, vol. 52, no. 4, pp. 1289­1306, Apr. 2006. [2] E. J. Candès, J. Romberg, and T. Tao, "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information," IEEE Trans. Inf. Theory, vol. 52, no. 2, pp. 489­509, Feb. 2006. [3] L. Rudin, S. Osher, and E. Fatemi, "Nonlinear total variation based noise removal algorithms," Phys. D: Nonlinear Phenom., vol. 60, no. 1, pp. 259­268, 1992. [4] W. Dong, G. Shi, X. Li, Y. Ma, and F. Huang, "Comressive sensing via nonlocal low-rank regularization," IEEE Trans. Image Process., vol. 23, no. 8, pp. 3618­3612, Aug. 2014. [5] J. Zhang, S. Liu, D. Zhao, R. Xiong, and S. Ma, "Improved total variation based image compressive sensing recovery by nonlocal regularization," in Proc. IEEE Int. Symp. Circuits Syst. (ISCAS), 2013, pp. 2836­2839. [6] X. Wu, W. Dong, X. Zhang, and G. Shi, "Model-assisted adaptive recovery of compressed sensing with imaging applications," IEEE Trans. Image Process., vol. 21, no. 2, pp. 451­458, Feb. 2012. [7] W. Dong, G. Shi, X. Li, L. Zhang, and X. Wu, "Image reconstruction with locally adaptive sparsity and nonlocal robust regularization," Signal Process.: Image Commun., vol. 27, pp. 1109­1122, 2012. [8] J. Ma, G. Plonka, and M. Y. Hussaini, "Compressive video sampling with approximate message passing decoding," IEEE Trans. Circuits Syst. Video Technol., vol. 22, no. 9, pp. 1354­1364, Sep. 2012. [9] C. Li, H. Jiang, W. Paul, and Y. Zhang, "A new compressive video sensing framework for mobile broadcast," IEEE Trans. Broadcast., vol. 59, no. 1, pp. 197­205, Mar. 2013. [10] M. Hosseini and K. N. Plataniotis, "High-accuracy total variation with application to compressed video sensing," IEEE Trans. Image Process., vol. 23, no. 9, pp. 3869­3884, Sep. 2014. [11] P. Nagesh and B. Li, "A compressive sensing approach for expressioninvariant face recognition," in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR), 2009, pp. 1518­1525. [12] T. T. Do, Y. Chen, D. T. Nguyen, N. Nguyen, L. Gan, and T. D. Tran, "Distributed compressed video sensing," in Proc. Int. Conf. Image Process. (ICIP), 2009, pp. 1393­1396. [13] J. Nebot, Y. Ma, and T. Huang, "Distributed video coding using compressive sampling," in Proc. Picture Coding Symp. (PCS), 2009, pp. 1­4. [14] L. W. Kang and C. S. Lu, "Distributed compressive video sensing," in Proc. Int. Conf. Acoust. Speech Signal Process. (ICASSP), 2009, pp. 1169­1172. [15] V. Thirumalai and P. Frossard, "Distributed representation of geometrically correlated images with compressed linear measurements," IEEE Trans. Image Process., vol. 21, no. 7, pp. 3206­3218, Jul. 2012. [16] Y. Liu, M. Li, and A. Dimitris, "Motion-aware decoding of compressedsensed video," IEEE Trans. Circuits Syst. Video Technol., vol. 23, no. 3, pp. 438­444, Mar. 2013.

[17] K. Chang and B. Li, "Joint modeling and reconstruction of a compressively-sensed set of correlated images," J. Visual Commun. Image Represent., vol. 33, pp. 286­300, 2015. [18] K. Chang, T. Qin, W. Xu, and Z. Tang, "Reconstruction of multi-view compressed imaging using weighted total variation," Multimedia Syst., vol. 20, no. 4, pp. 363­378, 2014. [19] M. Trocan, E. W. Tramel, J. E. Fowler, and B. Pesquet, "Compressedsensing recovery of multiview image and video sequences using signal prediction," Multimedia Tools Appl., vol. 72, no. 1, pp. 95­121, 2014. [20] Y. Liu, C. Zhang, and J. Kim, "Disparity-compensated total-variation minimization for compressed-sensed multiview image reconstruction," in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP), 2015, pp. 1458­1462. [21] V. Thirumalai and P. Frossard, "Correlation estimation from compressed images," J. Visual Commun. Image Represent., vol. 24, no. 6, pp. 649­ 660, 2013. [22] H. Yoon, K. S. Kim, D. Kim, Y. Bresler, and J. C. Ye, "Motion adaptive patch-based low-rank approach for compressed sensing cardiac cine MRI," IEEE Trans. Med. Imag., vol. 33, no. 11, pp. 2069­2085, Nov. 2014. [23] Q. Chen, P. Montesinos, Q. Sun, P. Heng, and D. Xia, "Adaptive total variation denoising based on difference curvature," Image Vis. Comput., vol. 28, no. 3, pp. 298­306, 2010. [24] W. Dong, X. Yang, and G. Shi, "Compressive sensing via reweighted TV and nonlocal sparsity regularisation," Electron. Lett., vol. 49, no. 3, pp. 184­186, 2013. [25] E. J. Candès, M. B. Wakin, and S. P. Boyd, "Enhancing sparsity by reweighted l1 minimization," J. Fourier Anal. Appl., vol. 14, no. 5, pp. 877­905, 2008. [26] M. Fazel, H. Hindi, and S. P. Boyd, "Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance matrices," in Proc. Amer. Control Conf., 2003, pp. 2156­2162. [27] T. Goldstein and S. Osher, "The split bregman method for l1 regularized problems," SIAM J. Imag. Sci., vol. 2, no. 2, pp. 323­343, 2009. [28] E. T. Hale, W. Yin, and Y. Zhang, "Fixed-point continuation for l1minimization: Methodology and convergence," SIAM J. Optim., vol. 19, no. 3, pp. 1107­1130, 2008. [29] T. T. Do, L. Gan, N. H. Nguyen, and T. D. Tran, "Fast and efficient compressive sensing using structurally random matrices," IEEE Trans. Signal Process., vol. 60, no. 1, pp. 139­154, Jan. 2012. [30] C. Li, W. Yin, H. Jiang, and Y. Zhang, "An efficient augmented lagrangian method with applications to total variation minimization," Comput. Optim. Appl., vol. 56, no. 3, pp. 507­530, 2013. [31] C. Liu, "Beyond pixels: Exploring new representations and applications for motion analysis," Ph.D. dissertation, Department of Electrical Engineering and Computer Science, Massachusetts Inst. Technol., Cambridge, MA, USA, 2009. [32] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert, "High accuracy optical flow estimation based on a theory for warping," in Proc. Eur. Conf. Comput. Vis. (ECCV), 2004, pp. 25­36.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/306576719

WeaklyHierarchicalLassobasedLearningto RankinBestAnswerPrediction
ConferencePaper·August2016
DOI:10.1109/ASONAM.2016.7752250

CITATIONS

READS

0
2authors,including: QiongjieTian ArizonaStateUniversity
12PUBLICATIONS172CITATIONS
SEEPROFILE

32

AllcontentfollowingthispagewasuploadedbyQiongjieTianon25August2016.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

Weakly Hierarchical Lasso based Learning to Rank in Best Answer Prediction
Qiongjie Tian
Computer Science and Engineering Arizona State University Email: qiongjie.tian@asu.edu

Baoxin Li
Computer Science and Engineering Arizona State University Email: baoxin.li@asu.edu

Abstract--In community question and answering sites, pairs of questions and their high-quality answers (like best answers selected by askers) can be valuable knowledge available to others. However lots of questions receive multiple answers but askers do not label either one as the accepted or best one even when some replies answer their questions. To solve this problem, highquality answer prediction or best answer prediction has been one of important topics in social media. These user-generated answers often consist of multiple "views", each capturing different (albeit related) information (e.g., expertise of the asker, length of the answer, etc.). Such views interact with each other in complex manners that should carry a lot of information for distinguishing a potential best answer from others. Little existing work has exploited such interactions for better prediction. To explicitly model these information, we propose a new learningto-rank method, ranking support vector machine (RankSVM) with weakly hierarchical lasso in this paper. The evaluation of the approach was done using data from Stack Overflow. Experimental results demonstrate that the proposed approach has superior performance compared with approaches in state-ofthe-art.

limitations inherent to these existing techniques. First, a binary classifier is not natural to this research problem, which often involves multiple answers for one given question. It is possible for a trained classifier to declare many or even all answers are the best ones (if they happen to lead to feature vectors lying on the positive side of the decision boundary). Also it is counter-intuitive as a human user would normally compare all received answers and decide on a single best one. The binary classification does not model directly on the difference of multiple answers, compared with learning-to-rank techniques. Second, the interaction between features from different views may carry a lot of information for distinguishing a potential best answer from others, however current existing methods do not readily support incorporation of such interactions, which by itself is a challenging task. In anther setting, best answer prediction is modeled as one ranking problem, which is conceptually more intuitive. This kind of modeling results from the fact that the best answer to one question is defined/discovered relatively by comparing it with all the other given answers. A ranking-based setting may benefit even more from considering the latent interactions between features designed from different views of the CQA data. Unfortunately, similar to the binary-classification cases, the existing learning-to-rank techniques have not attempted to explicitly to model such interactions among different views of the data [4][5][6]. In this paper, we focus on how to incorporate the interaction structure of features into one existing algorithm framework to improve the performance of best answer prediction. Similar to [5][7], we adopt the learning-to-rank formulation for its natural match to the prediction problem. Considering the interaction structure (or the hierarchical structure of feature dimensions in our study) and the ranking framework, we propose a new learning-to-rank formulation based on weakly hierarchical lasso. The contributions of our work are summarized as follows: Firstly, we propose a new RankSVM model by constructing the weakly hierarchical structure between features from different views. Secondly, to solve the new formulation, we propose an efficient algorithm and evaluate via experiments its efficiency and effectiveness with comparisons with other existing methods.

I. I NTRODUCTION In the era of Internet and social media, community question and answering (CQA) sites, like Baidu Zhidao1 , Yahoo! Answers2 and StackOverflow3 , are seeing phenomenal growth. As one form of user-generate content, data from CQA sites are typically very noisy, which does not lead to ready usage either by humans or by computers. Consequently, how to extract useful information from the noisy CQA data to form valuable knowledge base has become an important research task [1]. One popular task on this regard is best answer prediction, on which our paper focuses. Given a question with multiple answers, one way to solve best answer prediction is to reformulate it into a binary classification problem which is whether, in a question-answer pair, the answer is the best one or not. There have been some research efforts in this setting like [2], [3]. In these efforts, features were extracted from different views of the data to generate a good representation for the question-answer pairs, and the final feature vector was formed by concatenating them together. As a result, each feature dimension carries some information of the CQA data. But there are a couple of
1 http://zhidao.baidu.com/ 2 https://answers.yahoo.com/ 3 http://stackoverflow.com/

IEEE/ACM ASONAM 2016, August 18-21, 2016, San Fran- 1 cisco, CA, USA 978-1-5090-2846-7/16/$31.00 c 2016 IEEE

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

II. R ELATED W ORK In this section, we review briefly related research on community question and answering, and discuss the difference between the reviewed work and our proposed method. A. Content Quality Analysis Compared with traditional on-line search, as one supplementary approach to solving our daily problems, CQA sites contain a lot of valuable knowledge. Thus, since the first CQA site was launched, finding high quality content from these sites has become important. For example some early work was done in [8] where Jiwoon Jeon et al. crawled data from Naver Q&A site and manually labeled each pair of questions and their corresponding answers as bad, medium, good. They proposed to use non-textual features to represent each question-answer pair and used kernel density estimation and the maximum entropy approach to model the problem of answer quality. To have a better representation of questions and answers on CQA sites, more sources of information were used to extract new features like interactions between questions and answers and users, as studied in [2], where Eugene Agichtein et al. proposed to use non-content information to model question and answer pairs on CQA sites including the interaction features. Then different classifiers like support vector machine, log-linear classifier and stochastic gradient boosted trees were applied to learn the prediction model, whose efficiency and effectiveness were evaluated using data from Yahoo! Answers. The importance of social information for predicting answer quality was studied in [3], where Chirag Shah et al. found the importance of user information by studying the quality labeled manually. Besides research on the answer quality, question quality is also studied. In [9], Baichuan Li et al. worked on the question quality prediction problem. They first studied what factors may affect question quality and then proposed a model termed Mutual Reinforcement-based Label Propagation to predict question quality. In [10], it was found that the voting scores of questions have a strong positive correlation with that of the corresponding answers and they proposed a set of coprediction algorithms to predict the voting scores of questions and answers. The above work focused on content quality prediction (question quality and answer quality), which is modeled as one classification problem. These existing efforts mainly focused on finding a better representation of the data by introducing various features to facilitate the prediction problem. B. Best Answer Prediction and Answer Ranking Pairs of questions and their best answers can be easily used to answer similar questions, as the research in [11] shows. With the fast growth of CQA sites, there are a lot of questions which have high quality answers but no best ones eventually marked. To this end, a lot of research efforts have been devoted to best answer prediction and answer ranking. In [12], Lada Adamic et al. analyzed Yahoo! Answers for best answer prediction. They used simple four-dimensional features and reported that the length of answers is the most

important factor of answer quality. The problem they are worked on is to predict whether a given answer is the best one of the given question. They did not consider interaction information like relationship between questions and answers and users. It is not natural to model best answer prediction as a classification problem since the best answer is relatively defined. Thus there have been a lot of efforts on modeling best answer prediction as a ranking problem. In [13], Mihar Surdeanu et al. proposed a ranking model for non-factoid questions and studied whether ranking algorithms can be used to rank answers for given questions. They also showed the importance of different features in the answer ranking problem. This work was further extended in [14]. Instead of simply applying learning to rank algorithms, some researchers worked on improving the performance by using piggybacking and ranking aggregation techniques. In [7], Felix Hieber et al. applied RankSVM algorithms to best answer prediction with piggybacking being used to improve the performance. In their work, interaction features were used, like the similarity between questions and answers. Piggybacking is used to for obtaining a better representation of the questions so that similarity between the questions and answers can help improve the ranking performance of RankSVM. One example work to use ranking aggregation is [15], where Arvind Agarwal et al. made a comparison between different learning to rank algorithms and proposed to use ranking aggregation techniques to improve them. But that work focused on the factoid question and answers instead of CQA. In contrast, our work employs hierarchical interactions in the feature space. There are also some efforts on studying the influence of different combinations of features on the prediction accuracy and also comparison across different CQA sites [16]. Pointwise ranking techniques were also used to rank answers to each question. In [4], Daniel Dalip et al. assumed that the voting scores to be the quality scores of answers. Then random forest was used to model the relationship between the scores and features. The final predicted rating scores were used to rank each questions. To evaluate the performance, normalized discounted cumulative gain at top k (NDCG@K) is used. However, there is noise in the rating scores as shown in [17], and thus in our work we do not use this assumption. The information between answers to each question may help capture the relative information for better prediction, as shown in [18], where Tian et al. proposed to extract features from the context information between answers to each question. There are many other efforts on finding/defining new features for best answer prediction. For example, temporal features are proposed in [5]. One common observation in the most of the existing work is that, when new features are derived, all of them are concatenated to one vector to be the final feature vector. For example, in [12], these features are used: reply length, thread length, the total number of best answers of one user, the total number of replies one user has. They can be denoted as x1 , x2 , x3 , x4 . Then the final feature vectors are the simple concatenation of these features which are (x1 , x2 , x3 , x4 ). In

2

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

our work, we focus on proposing a new model which can capture the feature interactions based on hierarchical lasso. III. P ROBLEM D ESCRIPTION AND F ORMULATION The research problem in this paper is formally defined as follows: given a question with all of its received answers, to predict which one is the best one. To select the best answer, one has to compare it with the others, so that the best answer is relatively defined. Thus instead of using the classification framework, we employ the learning-to-rank strategy. The basis of our proposed approach is RankSVM [6]. While existing work focuses on designing new features, we study this prediction problem from the following angle: modeling the interaction of features from different views of data beyond simple concatenation of them. To achieve this goal, we employ weakly hierarchical lasso [19] in constructing a new ranking model. Notations of this paper are described in the following. Denote a dataset with N questions as {qi , i  {1, · · · , N }}. For each question qi , it receives a group of answers which are {Ai,j , j  {1, · · · , Mi }} where Mi is the total number of answers to qi . The feature vector xi,j  R1×d is used to represent the j th answer to the ith question. Moreover, the k th dimension of one feature vector xi,j is defined as xi,j,k where k  {1, · · · , d}. xi,j is the simple concatenation of features extracted from different views of our problem, as done in the existing work. It is named as the main effect. Then for each xi,j , we compute the second-order interaction 2 which is denoted as zi,j  R1×d , which is called the second-order interaction term. The final feature vector by considering the main effect and the interaction term is denoted 2 as x ^(i, j ) = [xi,j , zi,j ]  R1×(d+d ) . The interaction term is defined as follows (see Eqn.1): zi,j = [zi,j , zi,j , · · · , zi,j ]
(m) zi,j (1) (2) (d)

The RankSVM formulation is given below (Eqn. 3):
wRd×1

min

s.t.

1 w 2 i,j1 ,j2 2+C 2 S1 (i, j1 )  S1 (i, j2 ) + 1 - i,j1 ,j2 , i,j1 ,j2  0, (i, j1 , j2 )

(3) (i, j1 , j2 )

where (i, j1 , j2 ) is one ranked QA pair in P and S (i, j ) is the quality score function of the j th answer to qi and defined in Eqn.4. S1 (i, j ) = xi,j w + w0 (4)

where w0  R. To improve the performance of RankSVM, our model involves the second-order interactions via constructing one weakly hierarchical structure in the feature space. The formulation of the new ranking model is shown in Eqn.5. Compared with the existing work, we model the latent interaction structure between features from different views of the data, instead of simple concatenation. The hierarchical structure of the feature space is constructed through the first group of constraints (a.k.a Q.,j 1  |wj |, j  {1, · · · , d}) in Eqn.5. min
wR , QRd×d
d×1

w

1

+

1 Q 2

1

+C
(i,j1 ,j2 )P

i,j1 ,j2

(5)

s.t.

Q.,j

1

 |wj |,

j  {1, · · · , d} (i, j1 , j2 )  P

i,j1 ,j2  0,

(i, j1 , j2 )  P

S (i, j1 ) > S (i, j2 ) + 1 - i,j1 ,j2 ,

where Q.,j is the j th column of Q, Q 1 = i j |Qi,j | and S (·, ·) is the ranking score for each answer to one question defined in Eqn.6. For example S (i, j ) is the ranking score for answer Ai,j to qi . 1 (6) S (i, j ) = xi,j w + zi,j vec(Q) + w0 2 where vec(Q) is the vectorized version of Q and zi,j is shown in Eqn.1 and w0  R. To help illustrating the proposed model, we depict the hierarchical structure based on one example shown in Figure 1, in which we only show three features: the length of the answer (Alen ), the number of URLs in the answer (Nurl ), the number of pictures used in the answer (Npic ). In this illustration, we can see that the upper layer contains all main effects (a.k.a xi,j ) while the second layer shows the interaction terms (a.k.a zi,j in Eqn.1) excluding the square values of themselves. When one term contributes to the objective function, no matter it belongs to main effects or interaction terms, its corresponding coefficient is set to be non-zero. For each interaction term, if it contributes to the objective function, then at least one of its corresponding main effects contributes to the objective function. Satisfying these hierarchical constraints, it is easy for us to conclude that the interaction terms contribute less than their corresponding main effects. Specifically, in this figure, if the coefficient of Alen · Nurl is non-zero, then the coefficient of Alen is non-zero but that of Nurl can be zero.

(1)

= [xi,j,m · xi,j,1 , xi,j,m · xi,j,2 , · · · , xi,j,m · xi,j,d ]

where i  {1, · · · , N }, j  {1, · · · , Mi } and m  {1, · · · , d}. In our work, instead of classification methods, learning-torank techniques are used to model the relativeness of the best answers. Each relatively ranked pair is represented as (qi , Ai,j1 , Ai,j2 ) where the quality of Ai,j1 is higher than that of Ai,j2 . For simplicity, we may use (i, j1 , j2 ) as the short version of (qi , Ai,j1 , Ai,j2 ) in the following equations. The set Pi contains all these pairs of answers to the question qi . Furthermore, the entire set of these relatively ranked pairs is denoted as P in Eqn.2. P =
i{1,··· ,N }

Pi

(2)

RankSVM, as one state-of-the-art pair-wise learning-to-rank algorithm used in best answer prediction [5][7], is used as the basic building block of our new ranking model.

3

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

From Eqn. 5, the weakly hierarchical lasso is involved via the first group of constraints (a.k.a Q.,j 1  |wj |, j  {1, · · · , d}).

where L(w, Q) is given in the following: L(w, Q) = Set  =
1 C,

1 max(0, 1 - (~ xm w + z ~m vec(Q)))2 (13) 2 m=1

|P |

the final model is obtain as given in Eqn.14  Q 1 2 j  {1, · · · , d}
1

min L(w, Q) +  · w
w,Q

+

s.t. Fig. 1: One illustration to show hierarchical structure in the feature space, where "·" represents the scalar multiplication. The first layer contains the main effect, while the second layer consists of the 2nd order of interaction. IV. S OLVING THE P ROPOSED M ODEL To develop a solution to our proposed model in Eqn. 5, we first reformulate the problem as follows. Consider this group of constraints (Eqn.7) in the proposed model in Eqn. 5. Si,j1 > Si,j2 + 1 - i,j1 ,j2 Together with Eqn.6, we have the following computation: Si,j1 > Si,j2 + 1 - i,j1 ,j2 (8) 1 Si,j1 = xi,j1 w + zi,j1 vec(Q) + w0 2 1 Si,j2 = xi,j2 w + zi,j2 vec(Q) + w0 2 If we assume the relatively ranked pair (qi , Ai,j1 , Ai,j2 ) is the mth element in the set P of Eqn.2, then Eqn.8 can be simplified and the following is obtained: 1 ~m ~m · vec(Q) > 1 -  (9) x ~m w + z 2 where x ~m , z ~m should satisfy the following constraints in Eqn.10. x ~m = xi,j1 - xi,j2 z ~m = zi,j1 - zi,j2 As a result, Eqn.5 is converted to the following: min
w,Q

Q.,j

1

 |wj |,

(14)

(7)

To this point, our objective function has been reformulated into the standard form as in the weakly hierarchical lasso problem defined in [19] and [20]. To solve Eqn. 14, the scheme in [20] can be applied since it can directly solve the weakly hierarchical lasso without adding more penalty compared with approach in [19]. Since the optimization process in [20] is based on a general iterative shrinkage and thresholding algorithm (GIST) in [21], before we use the method in [20], we need to prove that L(w, Q) in Eqn. 14 is continuously differentiable with Lipschitz continuous gradient. Before proceeding with the proof, we introduce following notations: x ^ = (~ x, z ~) w ^= w
1 2 vec(Q)

(15)

As a consequence, x ^  R1×(d+d·d) and w ^  R(d+d·d)×1 . L(w, Q) is converted from Eqn.13 as Eqn.16. ^ (w L ^) =
m{1,··· ,|P |}

max(0, 1 - x ^m · w ^ )2

(16)

(10)

^ (w To show L ^ ) is differentiable with Lipschitz continuous gradient, this requirement needs to be satisfied: there exists a positive constant  such that ^ ^ dL dL (w1 ) - (w2 ) dw ^ dw ^
2

  w1 - w2

2

(17)

w

1

+

1 Q 2

1

+C
m{1,··· ,|P |}

~m 

(11)

^ (w Let us first consider one additive component of L ^ ). The point-wise maximum function can be written as Eqn.18. l(w ^ ) = max(0, 1 - x ^m · w ^ )2 = 0 (1 - x ^m · w ^ )2 if 1 - x ^m · w ^<0 if 1 - x ^m · w ^0 (18)

1 ~m , m  {1, · · · , |P |} s.t. x ~m w + z ~m · vec(Q) > 1 -  2 Q.,j 1  |wj |, j  {1, · · · , d} ~ m  0, m  {1, · · · , |P |} where |P | is the size of the set P . Now we can reformulate Eqn.11 into Eqn.12: min
w,Q

w

1

+
1

s.t.

Q.,j

1 Q 1 + C · L(w, Q) 2  |wj |, j  {1, · · · , d}

(12)

It is easy to see that when w1 , w2  {w|1 - x ^ m · w < 0} and w1 , w2  {w|1 - x ^m · w  0}, Eqn.17 is satisfied. Now considering w1  {w|1 - x ^m · w < 0}, w2  {w|1 - x ^m · w > 0}, it is easy to see that the left part of Eqn.17 becomes (1 - x ^ · w2 )^ xm . Moreover, define w ^  as 1 - x ^m · w = 0 and

4

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM) this inequality is satisfied: w1 - w2  w - w2 . Now to obtain the constant  , the following induction is performed: (1 - x ^m · w2 )^ xm   w1 - w2 (1 - x ^m · w2 )^ xm   w1 - w2 (1 - x ^m · w2 ) x ^m    w - w2 (1 - x ^m · w2 ) x ^m 2   w - w2 x ^m (1 - x ^m · w2 ) x ^m 2   1-x ^m · w2   x ^m 2

Now it is feasible to apply the algorithm in [20] to solve Eqn.14 which is equivalent to solving this proximal operator problem of Eqn.22. (w(k+1) , Q(k+1) ) = arg min 1 1 w - v (k) 2 Q - U (k) 2 2+ 2 w,Q 2 2 1  + ( k ) ( w 1 + Q 1) 2 t Q.,j 1  |wj | j  {1, · · · , d} (22)

s.t.

where v (k) , U (k) are defined as follows: v (k) = w(k) - (19) U (k) = U (k) - 1 t(k) ·
w L(w (k)

, Q(k) ) , Q(k) )

(23) (24)

1 · t(k)

Q L(w

(k)

Similarly, it is easy to obtain that   x ^m 2 also satisfies the case where w2  {w|1 - x ^m · w < 0}, w1  {w|1 - x ^m · w > 0}. Thus, there exists a proper positive constant  so that l(w ^ ) meets the requirement Eqn. 17. In conclusion, l(w ^ ) is continuously differentiable with Lipschitz continuous gradient. With this result, we will further introduce and prove the following lemma, together with which we will able to show the desired property for L(w, Q) is satisfied. Lemma IV.1. For each function f (w)i , i  {1, · · · , N } which is continuously differentiable with Lipschitz continuous N gradient, their summation f (w) = i=1 fi (w) is continuously differentiable with Lipschitz continuous gradient. Proof. d d f (w1 ) - f (w2 ) dw dw N N d d fi (w1 ) - fi (w2 ) dw dw i=1 i=1
N

where t(k) > 0 which is the step size. Considering w, Q are products of their signs and also absolute values, Eqn.22 can be re-written into Eqn.25. (w(k+1) , Q(k+1) ) = arg min
w,Q

s.t.

1 1 w - v (k) 2 Q - U (k) 2 2+ 2 2 2  1 + ( k ) ( w 1 + Q 1) 2 t Q~ ~j j (25) .,j  w

where Q.,j = sign(Q.,j ) Q~ ~j . The .,j and wj = sign(wj ) w above equation can be solved in a closed form as proved in [20]. The pseudocode of our entire algorithm is shown in the following. which is summarized in Algorithm 1. Algorithm 1 The pseudo-code to solve our model
1: 2: 3: 4: 5: 6: 7: 8: 9:

=

=
i=1 N

(

d d fi (w1 ) - fi (w2 )) dw dw


i=1

d d fi (w1 ) - fi (w2 ) dw dw (20)

  w1 - w2

Denote that there exists positive constant i such that fi (w) satisfies Eqn.17 where i  {1, · · · , N }. Thus Eqn.20 is valid when  meets this requirement:  = max i
i

10: 11: 12: 13: 14: 15: 16:

(21)

INPUT: data matrix X and ranking information of all data OUTPUT: model parameters w and Q BEGIN: compute the set P based on Eqn.2. compute the data difference {x ~m , m  {1, · · · , |P |}} and {z ~m , m  {1, · · · , |P |}} as Eqn.10. provide initial values for w and Q. choose one t via BB Rule [22]. while w, Q satisfy the stop criteria do while tk does not satisfy the stop criteria do update v k according to Eqn.23. update U k according to Eqn.24. obtain new w(k+1) and Q(k+1) based on Eqn.25, which can be in the closed form as [20]. update the step size t(k) =   t(k) where  is the constant update ratio. end while k = k + 1; end while

^ (w Since max(0, 1 - x ^m · w ^ )2 satisfies Eqn. 17 and L ^) = 2 max(0 , 1 - x ^ · w ^ ) , according to Lemma IV.1, m m{1,··· ,|P |} ^ L(w ^ ) satisfies Eqn. 17, same as L(w, Q) defined in Eqn. 13. Thus, L(w, Q) is continuously differentiable with Lipschitz continuous gradient.

V. E XPERIMENTS In this section, we present experimental results on Stack Overflow to show the performance of our proposed model and the comparison with existing methods.

5

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)
number of users number of votes number of comments number of questions number of answers 4,232,639 62,357,544 44,557,809 9,365,722 15,632,696

A. Data Description Founded in 2008, StackOverflow is active and well maintained. On this site, users can post questions and everyone can provide answers even including the askers. For each question and each answer, users can comment on it. For one question or answer, users can vote up or down based on its quality except the user who posts it. For one comment, users can only vote up if they think the comment is useful, but cannot vote down. Same as one question or one answer, the one cannot vote up his or her own comments. For one question or one answer, it can receive up-votes and also down-votes. Then the number of up-votes minus the number of down-votes is the vote score. It is easy to see that the vote score are integers and can be negative. Each question can receive multiple answers and only the asker can decide which one can be marked as the accepted answer which we call the best answer. This choice is not permanent, which means the asker can change his or her mind at any time and mark another answer as the best answer. There is one fact we need to point out. One question may receive multiple correct answers but only one of them can be marked as the best answer. So the best answer has the relatively best quality instead of absolutely best one. This is the reason why we use the learning to rank techniques instead of the classification methods. For users, they can earn reputations if their posts (e.g. questions ,answers, and comments) obtain upvotes or answers are accepted or suggestions on editing others' posts are accepted. Otherwise, they lose reputations if their posts receive downvotes or are reported as spam or offensive. Figure 2 shows one sample of one question with its answers from StackOverflow. Till May 8, 2015, the statistics

TABLE I: The information of Stack Overflow till May 8, 2015.

Fig. 2: Illustration of one sample question from Stack Overflow. of this site are as in Table. I. B. Experiment Settings In our experiment, part of StackOverflow dataset is used. We downloaded all questions posted from October 1, 2012 to December 31, 2012 and all related information like answers

was tracked until January 2014. This time period was chosen because of these reasons: First, questions and answers in this time period are not very out-dated; Second, few user activities on posts in this period are active. Thus, we assume that the best answer to one question is the final one. The dataset we use was dumped on January 2014 4 . Before feature extraction, posts without users' IDs are removed. Then, only questions which have best answers and at least two more answers are considered. The final processed dataset has 52,104 questions and 190,165 answers. On average, there are 3.65 answers per question. During the experiments, our data set is randomly split into two parts evenly: training and testing. To be specific, details as follows show how to generate relatively ranked pairs. For each question, only its best answer is considered as the high quality answer while others are treated as low-quality answers. Then each pair is generated in this way: one best answer and one of other answers to the same question. After all pairs are generated, feature extraction is performed based on information from three main aspects of each pair of questions and answers: content, interactions, users. These are briefly described below. The First group of features are extracted based on the content of the answer in each pair of questions and answers. Part of these features are based on comments to the answers like average score of comments, variance of the comments' scores, number of comments. Comment-based features at least show that the corresponding answer is interesting and incur a good discussion towards problem solving. Besides these, whether one answer has pictures, URL or codes are also factors to show that the current answer has a high quality, since these components are able to show more information than text. Moreover, the length of answers [12][2] and its readability [18] also play an important role on answer quality. Apart from the content information, features based on interaction are also considered, for example, the interaction between questions and answers, and that between different answers to one question. The first one is easy to understand since one answer has to be similar to its corresponding question, and thus the similarity between questions and answers is used as one feature. The second one is designed based on the assumption that users prefer the answers which is easy to understand. Computation of these features are shown in [18]. This is different from the feature interaction in our model. This one is on the feature-design level which focuses on exploring new information sources to design new features, while our case focuses on the model-design level.
4 http://blog.stackoverflow.com/category/cc-wiki-dump/

6

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

User information also has an impact on the quality of answers. One answer is likely to have a high quality if the answerer is one expert. To represent the expertise of one user, these features are extracted based on users' previous activities, for example the number of answers one provides, how many questions one asks, the number of best answers he or she posts. Our experiment is conducted by considering different groups of features and then results are presented respectively. In this way, it is easy to see the performance of different algorithms when we only consider informations from different aspects of our research problem (i.e. different groups of features). Finally, the experiment is conducted on the entire feature set we have. The three groups of features we consider in this experiment are: content, interactions and user information. C. Experiment Results & Discussion To show the performance of our proposed algorithm, we compare our model with approaches used in state-of-the-art. As mentioned in Section Introduction, there are two main trends in best answer prediction: one is to use classification techniques and then decision values are used as quality scores while the other one is to use ranking approaches directly. For the former case, linear support Vector Machine (SVM) is common used because data in social media is in large scale so that nonlinear algorithms are not computational efficient. In our experiment, linear SVM is the first baseline we choose. For the latter case, RankSVM [6] is used which is one main ranking algorithm used in the area of best answer prediction [5]. The code for RankSVM is from Microsoft Research 5 . On CQA sites, there are no direct information we can use as the metric to measure answer quality without manually labeling. For example, scores of each answer might be one proper metric. But this metric is not accurate. It is easy to see that it is easy for the answer which is posted early to have the high score. In fact, on Stack Overflow, there are a lot of answers having the higher scores than the corresponding best answers6 . Thus in our experiments, we only treat the best answers as the high-quality ones and others as low-quality. As a result, in our experiment, it is the pairwise ranking problem so we do not compare with listwise ranking algorithms. To make comparison between different models, two evaluation metrics are used: one is defined in Eqn. 26 and the other one is defined in Eqn. 27. e1 =
(qi ,Ai,j1 ,Ai,j2 )P

g (i) = arg max{si,j , j  {1, · · · , Mi }}
j

I (ji,0 == g (i)) (27) N where ji,0 is the index of the best answer of the ith question, si,j is the predicted score of the j th answer of the ith question and the function g (·) returns the index of the best answer of one given question and the function I (·) is given by Eqn.28. e2 =
i

I (x) =

1 0

if x is true otherwise

(28)

From the definitions, it is easy to see this fact: e1 shows how good one algorithm is when it considers the pairwise ranking regardless of whether one algorithm can find the best answer to one question or not, while e2 shows the performance of each algorithm when applied to best answer prediction. In other words, e1 measures what percentage of relatively ranked pairs are predicted correctly, which focuses on the answerlevel comparison. However e2 measures what percentage of questions have the correctly predicted best answers. To show the performance of different models on the pairwise ranking in best answer prediction, experiments were conducted to collect the metric e1 . The experimental results are shown in Table. II. Table. II presents the performance of
SVM RankSVM Ours fc 0.671 0.411 0.689 fi 0.541 0.534 0.552 fu 0.480 0.543 0.570 all 0.544 0.476 0.693

TABLE II: This table shows the results of different algorithms on Stack Overflow when considering the measurement metric e1 . Three groups of features: fc content, fi interactions, fu user information. algorithms used as learning to rank. From the results, we can see that our model performs best not only when only individual feature groups are considered but also when all features are considered. This shows that our model can be one good pairwise ranking algorithm in the area of community question and answering. From the results of SVM, we can see that when only fc is considered, the performance is best. However, when simple concatenation of all features from different views is applied, the final one gives worse performance instead of better one. Similarly, for RankSVM, its performance is best when only fu is considered. However after considering all features, the performance drops. For our approach, because we consider the interaction structure of features from different views, the final performance is best. This shows that there exists on latent interaction structure in the feature space. Incorporating weakly hierarchical lasso, we can capture this interaction structure. This shows the effectiveness of our proposed model. To show comparison of performance on best answer prediction, experiments were run to collect metric e2 . Table. III presents the performance of different models. From the results, it is easy to see that our model performs best in the problem

I (si,j1 > si,j2 )

|P |

(26)

where si,j1 , si,j2 are predicted scores of Ai,j1 , Ai,j2 respectively. The relatively ranking set P is defined in Eqn. 2 and the function I (·) is shown in Eqn. 28.
5 http://research.microsoft.com/en-us/um/beijing/projects/letor/baselines/ ranksvm-primal.html 6 https://data.stackexchange.com/stackoverflow/query/380215/whereaccepted-answer-does-not-have-the-highest-score

7

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)
fc 0.479 0.223 0.494 fi 0.331 0.321 0.334 fu 0.294 0.361 0.377 all 0.349 0.286 0.498

SVM RankSVM Ours

TABLE III: Experiment results (e2 ) of different algorithms' performance. Three groups of features: fc content, fi interactions, fu user information.

of best answer prediction not only when considering different groups of features independently but also when considering all features jointly. Similar to Table.II, the performance of SVM and RankSVM drop a lot when all features are considered by simple concatenation. For our model, it does not have this problem because of the fact that we incorporate the information from the latent interaction of features from different views. Consequently, we conclude that the proposed models perform better than those in the state-of-the-art. Performance of experiments using both metrics shows the effectiveness of hierarchical interactions between different views in the problem of best answer prediction. VI. C ONCLUSION & F UTURE W ORK We present a new learning-to-rank approach to best answer prediction on CQA sites. Incorporating the weakly hierarchical lasso, our proposed model is able to effectively exploit the interactions of features from different views of the data. To find a solution under this new model, we reformulate it into one existing optimization framework. Experiments on Stack overflow are used to evaluate the proposed approach, with comparison to other methods in state-of-the-art. The experimental results demonstrate the effectiveness and superior performance of our approach. Although our algorithm is designed originally for best answer prediction, it can be treated as one ranking algorithm and used in most ranking situations. Thus the application of our algorithm in different areas can be one piece of future work. Moreover, in our algorithm, one limitation is that we study the interaction structure of different feature dimensions, instead of different groups of feature dimensions. Another interesting future work is to extending our algorithm by considering the hierarchical structure of different groups of feature dimensions. ACKNOWLEDGMENT This work was supported in part by a grant (#1135616) from National Science Foundation. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the NSF. R EFERENCES
[1] A. Anderson, D. Huttenlocher, J. Kleinberg, and J. Leskovec, "Discovering value from community activity on focused question answering sites: a case study of stack overflow," in Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2012, pp. 850­858.

[2] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne, "Finding high-quality content in social media," in Proceedings of the 2008 International Conference on Web Search and Data Mining. ACM, 2008, pp. 183­194. [3] C. Shah and J. Pomerantz, "Evaluating and predicting answer quality in community qa," in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. ACM, 2010, pp. 411­418. [4] D. H. Dalip, M. A. Gonc ¸ alves, M. Cristo, and P. Calado, "Exploiting user feedback to learn to rank answers in q&a forums: a case study with stack overflow," in Proceedings of the 36th international ACM SIGIR conference on research and development in information retrieval. ACM, 2013, pp. 543­552. [5] Y. Cai and S. Chakravarthy, "Answer quality prediction in Q/A social networks by leveraging temporal features," Proceedings of International Journal of Next-Generation Computing, vol. 4, no. 1, 2013. [6] O. Chapelle and S. S. Keerthi, "Efficient algorithms for ranking with svms," Information Retrieval, vol. 13, no. 3, pp. 201­215, 2010. [7] F. Hieber and S. Riezler, "Improved answer ranking in social questionanswering portals," in Proceedings of the 3rd international workshop on Search and mining user-generated contents. ACM, 2011, pp. 19­26. [8] J. Jeon, W. B. Croft, J. H. Lee, and S. Park, "A framework to predict the quality of answers with non-textual features," in Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2006, pp. 228­235. [9] B. Li, T. Jin, M. R. Lyu, I. King, and B. Mak, "Analyzing and predicting question quality in community question answering services," in Proceedings of the 21st international conference companion on World Wide Web. ACM, 2012, pp. 775­782. [10] Y. Yao, H. Tong, T. Xie, L. Akoglu, F. Xu, and J. Lu, "Detecting high-quality posts in community question answering sites," Information Sciences, 2015. [11] A. Shtok, G. Dror, Y. Maarek, and I. Szpektor, "Learning from the past: answering new questions with past answers," in Proceedings of the 21st international conference on World Wide Web. ACM, 2012, pp. 759­768. [12] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman, "Knowledge sharing and yahoo answers: everyone knows something," in Proceedings of the 17th international conference on World Wide Web. ACM, 2008, pp. 665­674. [13] M. Surdeanu, M. Ciaramita, and H. Zaragoza, "Learning to rank answers on large online qa collections." in ACL, 2008, pp. 719­727. [14] ----, "Learning to rank answers to non-factoid questions from web collections," Computational Linguistics, vol. 37, no. 2, pp. 351­383, 2011. [15] A. Agarwal, H. Raghavan, K. Subbian, P. Melville, R. D. Lawrence, D. C. Gondek, and J. Fan, "Learning to rank for robust question answering," in Proceedings of the 21st ACM international conference on Information and knowledge management. ACM, 2012, pp. 833­ 842. [16] G. Burel, Y. He, and H. Alani, "Automatic identification of best answers in online enquiry communities," in The Semantic Web: Research and Applications. Springer, 2012, pp. 514­529. [17] S. Ravi, B. Pang, V. Rastogi, and R. Kumar, "Great question! question quality in community q&a," in Eighth International AAAI Conference on Weblogs and Social Media, 2014. [18] Q. Tian, P. Zhang, and B. Li, "Towards predicting the best answers in community-based question-answering services," in Seventh International AAAI Conference on Weblogs and Social Media, 2013. [19] J. Bien, J. Taylor, R. Tibshirani et al., "A lasso for hierarchical interactions," The Annals of Statistics, vol. 41, no. 3, pp. 1111­1141, 2013. [20] Y. Liu, J. Wang, and J. Ye, "An efficient algorithm for weak hierarchical lasso," in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 283­292. [21] P. Gong, C. Zhang, Z. Lu, J. Z. Huang, and J. Ye, "A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems," in Machine learning: proceedings of the International Conference. International Conference on Machine Learning, vol. 28, no. 2. NIH Public Access, 2013, p. 37. [22] J. Barzilai and J. M. Borwein, "Two-point step size gradient methods," IMA Journal of Numerical Analysis, vol. 8, no. 1, pp. 141­148, 1988.

8

View publication stats

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/306576742

FindingNeedlesofInterestedTweetsinthe HaystackofTwitterNetwork
ConferencePaper·August2016
DOI:10.1109/ASONAM.2016.7752273

CITATIONS

READS

0
3authors,including: QiongjieTian ArizonaStateUniversity
12PUBLICATIONS172CITATIONS
SEEPROFILE

47

AllcontentfollowingthispagewasuploadedbyQiongjieTianon25August2016.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

Finding Needles of Interested Tweets in the Haystack of Twitter Network
Qiongjie Tian
Computer Science and Engineering Arizona State University Email: qiongjie.tian@asu.edu

Jashmi Lagisetty
Computer Science and Engineering Arizona State University Email: jlagiset@asu.edu

Baoxin Li
Computer Science and Engineering Arizona State University Email: baoxin.li@asu.edu

Abstract--Drug use and abuse is a serious societal problem. The fast development and adoption of social media and smart mobile devices in recent years bring about new opportunities for advancing computer-based strategies for understanding and intervention of drug-related behaviors. However, the existing literature still lacks principled ways of building computational models for supporting effective analysis of large-scale, often unstructured social media data. Part of the challenge stems from the difficulty of obtaining so-called ground-truth data that are typically required for training computational models. This paper presents a progressive semi-supervised learning approach to identifying Twitter tweets that are related to personal and recreational use of marijuana. Based on a small, labeled dataset, the proposed approach first learns optimal mapping of raw features from the tweets for classification, using a method of weakly hierarchical lasso. The learned feature model is then used to support unsupervised clustering of Web-scale data. Experiments with realistic data crawled from Twitter are used to validate the proposed approach, demonstrating its effectiveness.

I. I NTRODUCTION Drug use/abuse is among the serious societal problems in the modern age. According to a 2011 report [1], in the United States alone, illicit drug use costs the society more than $193 billion annually and the number is increasing. The impact is also widespread: In 2013, about 24.6 million Americans 12 years old or older were illicit drug users [2]. Accordingly, a lot of research efforts have been devoted to understanding druguse-related behaviors and the analysis of potential benefits and limitations of various intervention strategies. A key step in such drug-use-related research is the collection of user behavior data. Most conventional approaches to user data collection are based on recruitment of participants who would provide inputs to a drug-use-related study, e.g., by answering questionnaires carefully designed to gather various types of behavioral and/or demographical data [3][4]. But there are some well-known limitations in such efforts. For example, the sample size is typically small, as it is in general very costly to involve a large population in such studies. More importantly, such questionnaires in general rely on a participant's explicit recall of his/her drug-use behavior, which could be a limiting factor on its own (e.g., issues like incorrect memory or intentional omission of some facts). The phenomenal growth of social media and smart mobile devices has led to more and more drug-use-related data

appearing online. For example, there are many drug-related discussion groups on Facebook, many drug-use-related questions asked and answered on Yahoo!Answers, as well as many drug-related tweets on Twitter. Such user-generated social media may be collected at a much larger scale (than an explicit user survey) and thus have the potential of offering realistic insights into understanding of substance-use behaviors, their situational factors, and social contexts. A few recent efforts illustrate this nicely. In [5], Christine Lee et al. found that the substance-use related behaviors have similar patterns in data from traditional surveybased approaches and those from social media. In [6], Jennifer Whitehill et al. studied the relationship between mobile usage of social networking sites (e.g. Facebook and Twitter) and the alcohol use in a large street festival. In [7], Joris Hoof et al. conducted one study on analyzing Facebook profiles to show that some Facebook profile elements can be the indicators of real-life behaviors. In [8], Sarah Stoddard et al. examined the influence of young people's social networking behaviors on their alcohol and other drug use. While having demonstrated to some extent the potential of using social media for substance-use research, these existing efforts also revealed the challenges of building computational models for analyzing largely-unstructured social-media. For example, some user attributes that may be readily available from an explicit survey now need complex inference strategies to figure them out. Further, any approach that relies on training from some labeled dataset cannot be easily extended to large-scale analysis. In this paper, we address some of these challenges in the context of illicit marijuana use and its manifestation on Twitter. Specifically, we propose one semisupervised approach to studying the user behaviors of the illicit marijuana use using noisy, unstructured and large-scale Twitter data. To our knowledge, this is the first work to study marijuana use behaviors using large-scale Twitter data. II. R ELATED W ORKS In this section, we briefly review some related work on study of use of marijuana and other substance, including both traditional methods of recruiting participants and more recent approaches using social media data.

IEEE/ACM ASONAM 2016, August 18-21, 2016, San Fran- 1 cisco, CA, USA 978-1-5090-2846-7/16/$31.00 c 2016 IEEE

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

A. Participant-recruitment Based Research Johnston et al. conducted follow-up surveys on young adults regarding their behaviors related to drug use in [9]. Similar recruitment based approaches were also used to study the effect of marijuana use in adolescent on their depressive symptoms and IQ development [10] [11]. As noted earlier, these population-survey-based efforts are usually very time-consuming merely for the stage of data collection. Another point to note is that the above-mentioned efforts focused more on finding features or trends from the data rather than developing computational approaches for modeling user behaviors. B. Research Using Social Media The non-medical use of Adderall (one psychostimulant drug) among college students using Twitter were studied in [12], where the frequencies, percentages and means were analyzed, and the experiments showed that their findings were similar to traditional survey-based methods. To study the smoking behavior on Twitter, Myslin et al. collected tweets from Twitter and performed content and sentiment analysis [13]. Cavazos-Rehg et al. also performed content analysis of tweets but with a pro-marijuana Twitter handle (@stillblazingtho) plus the demographics of the handle's followers [14]. Volkow et al. reported risks of the recreational use of marijuana like the risk of addiction, effect on brain development, relation to mental illness and so on in [15]. Krauss et al. studied the hookah smoking behavior on Twitter in [16]. Leah et al. reported their research on how posts on Twitter changed after legalizing recreational use of marijuana in two states [17]. Katsuki et al. studied the youth non-medical use of prescription medications (NUPM) on Twitter in order to model the frequency of NUPM-related tweets and identified the illegal access to drug abuse via online pharmacies in [18]. While demonstrating the great potential of using social media for substance-use-related analysis, these existing efforts have yet to be extended to Web-scale data. In particular, we have not seen specific computational models for analyzing Web-scale Twitter data for understanding marijuana-userelated behaviors. III. P ROBLEM D EFINITION To study the behavior of marijuana users on Twitter, a fundamental problem is to identify tweets that are related to some underlying users who use marijuana. This problem is more subtle than it appears. For example, one cannot simply rely on using the keyword "marijuana" to search the tweets for solving the problem. There are several complicating factors. First, many "street names" are used to describe marijuana. Second, there may be many tweets that involve medical or research-oriented references to marijuana but they are not at all useful for a study on illicit marijuana use. Considering these factors, we propose to classify a tweet into one of following three categories:

·

·

·

Class One: Tweets in this class are related to personal recreational use of marijuana. They are posted by individual users instead of some official accounts (for example, those for newspaper, companies, or medical institutes). Class Two: In this class, all tweets are related to marijuana but not in the sense of recreational use. For instance, they may discuss the medical or prescription use of marijuana, or report some news involving marijuana. Class Three: This is for those tweets having no identifiable relationship with marijuana use.

Figure 1 illustrates several real examples for each of the three classes defined above.

Fig. 1: Demos to show three classes: (a) is for Class One, (b) is for Class Two and (c) is for Class Three. Various text-based features may be extracted for the task of classifying the tweets. Also, as evident from the related work, it is important to consider social interactions among the underlying users. Furthermore, all these features are not mutually independent, and their intricate correlation may provide additional evidence for improved classification. Considering these, and with the goal of classifying large-scale tweets in mind, we now discuss our overall approach, which is illustrated in Figure 2. In the approach, we first extract a set of basic features from each tweet. Then, utilizing a small labeled training set, we learn a good feature mapping that takes into consideration both some basic features and their interactions, based on weakly-hierarchical lasso. The learned feature mapping model is used to process the large-scale data.

2

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM) where the ith data point is xi  R1×d , i  {1, · · · , N } which is normalized, and its label is yi  {1, 2, 3} and the coefficient to learn is w  Rd×1 . In this paper, the discriminant function is chosen to be one-vs-one linear SVM. The implementation details are provided as follows. We first train one linear regression model by optimizing Eqn.2. 1 w 2 (2) min Xw - y 2 2 2+ w 2 where X  RN ×d and y  RN ×1 . Then we apply one-vs-one linear SVM to s = Xw  RN ×1 to find the label for each tweet.
N

# $%& ( ( + ' ) *

( ! " +

( )

Fig. 2: It shows the entire framework of our methodology. In the following, we first present the basic set of features designed for our task. These features are extracted from either the content of the underlying tweets or the social interactions among the corresponding users, as elaborated below. A. Content-based Features
·

min
v

v

2 2

+C
i=1

i

(3) (4)

s.t.

yi (si  v + b)  1 - i i

·

·

·

The length of the tweet: For each tweet, its length can be one useful feature. For example, the tweets from ordinary users may be generally shorter than those from official accounts. Favorite Count & Retweeted Count: It shows how many people think the tweet is favorite and the number people who retweet this post. This is in general useful for measuring how influential the tweet is. The number of Hash-Tags: This calculates how many trends one tweet mentions. Our original dataset were obtained by crawling using selected street names of marijuana. The tweets with more trends are likely to be classified as Class Three or Two, instead of Class One. TF-IDF on Unigram: Unigram is one common feature used to capture characteristics of one tweet. We build TF-IDF for unigrams of each tweet and use it as one feature. Number of followings and followers: Each user on Twitter can follow others or be followed. However for some official accounts or famous people, they are likely to have a smaller number of followings but a large number of followers. These users are unlikely to post tweets related to personal and recreational use of marijuana. Number of Tweets: This records how many tweets one user has already posted, capturing the level of Twitter activity of the user.

where  is non-negative. However, in practice, the linear model is inadequate for capturing the high degree of non-linearity that typically exists in our problem, which has been shown in our experiments. To allow some level of nonlinearity while maintaining computational efficiency, we introduce to the problem 2nd -order interaction terms with a weakly hierarchical structure, as described in [19][20]. The resultant model is given in Eqn.5. y = f (z ) z = xw + 1 2
d d

(5) xi xj Qi,j
i j

where z is called the z -term of x (for simplicity) and the discriminant function f (·) is given in Eqn.3 (one-vs-one linear SVM in this paper) and xi is the ith dimension of the data point x and Qi,j  R is the coefficient for the interaction between ith and j th dimensions of the feature space. To solve the classification problem under this new model, we formulate the following optimization problem in Eqn.6.
w,v,Q

min

1 2

(f (zi , v ) - yi )2 + 1 w
i 1

1

+

3 Q 2

1

(6)

B. User-based Features
·

s.t.

Q.,j

 |wj | for

j = {1, · · · , d}

where zi is the z -term of xi as defined in Eqn.5, Q 1 = i,j |Qi,j | and v is the model parameter of the discriminant function (the one-vs-one linear SVM). A. Solving the Optimization Problem Solving Eqn.6) directly is difficult. Hence we simplify this optimization problem by a two-step process: We first learn parameters w and Q and then learn the model parameter v of the discriminant function. For parameters w and Q, we model them as one regression model as Eqn.7 when we do not consider the discriminant function. 3 1 (zi - yi )2 + 1 w 1 + Q 1 (7) min w,Q 2 i 2 s.t. Q.,j
1

·

IV. L EARNING F EATURE M APPING F ROM A S MALL DATASET Considering the computational efficiency needed for processing Web-scale data, we may employ a linear classifier as the baseline for doing the classification, as given by Eqn.1. yi = f (xi w) (1)

 |wj | for

j = {1, · · · , d}

3

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM) interaction, the dataset representation is converted as {x ~i , i  {1, · · · , N }} where x ~i is given by Eqn.11. x ~i = (xi , vec(Ri )) (11)

where zi is the z -term of xi as defined in Eqn.5. Then after w and Q are obtained, we learn v of the discriminant function by optimizing Eqn.3. Converting Eqn.6 into Eqn.7 and Eqn.3 allows us to solve the original optimization problem. By solving Eqn.7 and Eqn.3, we can obtain the model parameters v , w and Q which satisfy the original problem (Eqn.6) as well. However, since we add more constraints on these parameters in the process of simplification, the obtained v , w and Q are only the local optima of Eqn.6. While the details for solving Eqn.7 can be found in [19], a brief description is given below. From Eqn.7, we can see that this optimization problem is non-convex because of the existence of constraints, and as a result, we cannot solve it using convex optimization approaches. Thus in [19], one convex relaxation by setting w = w+ - w- is given, where w+ and w- are nonnegative. The convex relaxation version is given as Eqn.8. min 1 2 (^ zi - yi )2 + 1 (w+ + w- ) +
i - + wj

where the element at (j, k ) in the matrix Ri is the product of the j th and k th dimension which is xi,j xi,k . It is easy to see 2 x ~i  R1×(d+d ) . For the new representation, the interaction of the feature dimension is captured by parameters w and Q which are learned from the small labeled dataset (see Section IV). By treating the learned parameters as a kernel, we can have the new clustering as Eqn.12.
k j ,j {1,··· ,k}

min

(~ xv - cj )M (~ xv - cj )T
j =1 v j

(12)

where the learned metric matrix M = diag ((w; vec(Q)))  2 2 R(d+d )×(d+d ) . VI. E XPERIMENTS In this section, we evaluate the performance of our approach with comparison with several typical existing methods. A. Dataset Construction For constructing a small labelled dataset, instead of crawling random tweets online, we first use a list of keywords as one filter to remove unrelated tweets. These keywords are defined based on several Web sources and some government documents1 . The final keyword list was determined to be: marijuana, weed, blunt, cannabis, pot, reefer, buds, 420, mary jane, blaze. With the final list, the Twitter API 2 is utilized to crawl data. The time period we crawled is from January 09 to January 15 in 2016 and all tweets are in English. We crawled a total of 1,166,441 tweets. Among these we randomly labeled 10,000 with comparable proportion for each class (see Table 1 for exact composition in terms of class labels). This small labelled dataset was annotated by two people reading the tweets to decide their labels. B. Learning the Feature Mapping In this part, to compare with commonly used classifiers like linear classifier (Eqn.1) and linear SVM, we split the 10,000 tweets randomly into two parts: training set of 8,000 tweets and testing set with 2,000 tweets. Since in the our approach, we need to compute the feature interaction terms which is defined as the z -term in Eqn.5, we have to reduce the dimension of the original feature vectors. In this experiment, we use LDA [22] to do dimension reduction of TF-IDF of Unigram in the feature sets for our approach. For random guess, we randomly assign one label to every data point and then compute the accuracy based on Eqn.13. e=
Nt i=1

w+ ,w- ,Q

3 Q 2

1

(8)

s.t.

+ 1  wj + - wj , wj 0

Q.,j

for

j = {1, · · · , d} (9)

for

j = {1, · · · , d}
d d

where z ^i = xi · (w+ - w- ) + 1 j k xi,j xi,k Qi,j . A lot of 2 convex optimization approaches can be used to solve Eqn.8, such as FISTA [21]. After we obtain the parameters w and Q, the original problem will become equivalent to the support vector machine which can be solved using sequential minimal optimization. V. C LUSTERING WITH T HE L EARNED F EATURE M APPING A supervised approach cannot be directly applied to Webscale datasets as manually-labeled data are in general in a much smaller scale. A semi-supervised approach would rely on unsupervised clustering to first identify the structures of the data and then employ a small amount of labeled data to annotate the structures. For example, using K-means clustering, we can group a dataset into different clusters. For data points in each cluster, if we assume that they have the same labels, we can randomly select a small number of data points for labeling and then use the labels to annotate the clusters. Assuming k groups in a dataset, a basic K-means algorithm is equivalent to solving the following problem (Eqn.10):
k j ,j {1,··· ,k}

min

xv - cj
j =1 v j

2 2

(10)

where cj is the j th centroid and j is the j th cluster. As we have presumably found a feature mapping scheme in the previous section by maximizing classification accuracy for the labelled data, it is natural to use the learned feature mapping for the clustering stage. Denote the dataset as {xi , i  {1, · · · , N }}. Consider the influence of the 2-order feature

I (yi == y ^i ) Nt

(13)

1 In this paper, we use this forum (www.rehabs.com) and this official document(https://vva.org/wp-content/uploads/2014/12/street-terms.pdf). 2 https://dev.twitter.com/rest/public

4

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

where yi is the ground-truth label of the tweet xi and y ^i is the predicted label and I (x) = 1 0 if x is true otherwise (14)

The experiment results are shown in Table II and The confusion matrix of our approach is shown in Table I.
RG 0.326 LC 0.462 SVM 0.677 Ours 0.976

TABLE II: The table shows the performance of each baseline and our method. RG: random guess; LC: linear classifier; SVM: linear SVM. From Table. II, we can easily see that our algorithm stands out. Compared with the modified linear classifier (Eqn. 1 and Eqn. 3) with our algorithm, the difference is that we consider the interaction terms (the z -term) defined in Eqn.5. Thus these results also show that it is necessary to consider feature selection scheme using weakly hierarchical lasso. Furthermore, our approach performs better than linear SVM. This is also easy to understand because of the nonlinearity introduced in our formulation (Eqn. 5). Nonlinearity comes from the z -term. To further show the performance of each algorithm, the confusion matrices are shown in Table. I. It shows that our approach performs best in all of the three classes. From Table I(a), we can see that LC cannot distinguish Class 1 and Class 2. For example, for Class 2, almost the same number of tweets are classified into Class 1 and Class 2. The baseline with SVM performs better than LC, but the error is still significant. Our approach effectively solves the problem of how to fuse features and provides the optimal feature selection/combination scheme. It is possible to analyze which features (or their interactions) are most influential. Table III shows the top three main factors which affect the classification performance and their corresponding coefficients. From this
retweet num 4.41e-05 TF-IDF1 4.53e-01 TF-IDF2 3.62e-01

C. Clustering Structure on the Web-Scale Data In this part, we apply the learned feature mapping scheme to the large dataset, which contains not only the labeled data points but also unlabeled ones. To show the clustering structure of the partially labeled dataset, we perform two experiments: one using one baseline which is KMeans and the other one is our method based on Eqn. 12. For a good clustering outcome, we assume in each cluster, a majority of data points belong to the same class. To evaluate the performance of the results, we present two metrics (Eqn. 15) to show whether any class is dominant in a given cluster. In each cluster, there may be three classes with sizes n0 , n1 and n2 (in non-increasing order) respectively. If one class does not exist, it means its size is zero. n1 n2 m2 = (15) m1 = n0 n0 In our experiment, the large dataset is partially labeled and thus when we compute m1 and m2 , we only consider the labeled data in each cluster. Then the average is computed for the entire dataset. These two metrics are presented to measure what is the difference between the dominant class and the others. If the values of these metrics are small, then they shows that compared with the size of the dominant class, the others are small. In our experiment, we choose the number of clusters to be k  {10, 100, 200, 300, 400, 500, 1000}. In this way, we can learn the effect of the number of clusters on the clustering performance. The experiment results are shown in Table V.
k 10 100 200 300 400 500 1000 m ¯1 0.411 0.320 0.333 0.318 0.291 0.282 0.239 m ¯2 0.647 0.594 0.594 0.602 0.542 0.542 0.495 k 10 100 200 300 400 500 1000 m ¯1 0.381 0.280 0.240 0.263 0.243 0.228 0.116 m ¯2 0.555 0.487 0.436 0.485 0.423 0.427 0.320

(a) The baseline

(b) our approach

TABLE III: Illustration of top-3 main factors. The second one and the third one are from TF-IDF of Unigram. table, it can be seen that the number of retweets and also the TF-IDF of Unigram play important roles in distinguish these three classes. We can also see that the content of the tweets is most important for classification. Based on the results of Table III, the top interactions are from the two TF-IDF feature dimensions. This is also demonstrated by the experiment results (see Table IV).
TF-IDF1 * TF-IDF1 -2.258e-1 TF-ID2 * TF-ID2 1.844e-1 TF-ID1 * TF-IDF2 7.884e-2

TABLE V: Experiment results on studying the clustering structure of partially labeled dataset. (a) for the baseline and (b) for ours. They show the size of the other class compared with the dominant one. From Table V, we can see that our clustering approach by employing the learned feature mapping scheme performs better than the baseline. As the number of clusters goes up, m ¯1 and m ¯ 2 of KMeans and our approach become small, which means that the percentage of the dominant class becomes large. Compared with the baseline, the percentage of the dominant class is much larger since the corresponding metrics' values are smaller. The average percentage of the dominant class is shown in Fig.3. VII. C ONCLUSION AND F UTURE W ORK We presented one semi-supervised approach to analysis of Twitter data related to marijuana use, using web-scale data,

TABLE IV: This table shows top-3 interaction factors and their corresponding coefficients.

5

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

C1 C2 C3

C1 0.4616 0.3820 0.1751

C2 0.3108 0.3920 0.3146

C3 0.2276 0.2260 0.5103

C1 C2 C3

C1 0.6060 0.1820 0.1259

C2 0.1508 0.6120 0.0780

C3 0.2432 0.2060 0.7962

C1 C2 C3

C1 0.9831 0.0020 0.0014

C2 0.0130 0.9920 0.0410

C3 0.0039 0.0060 0.9576

(a) LC: modified linear classifier

(b) SVM : linear SVM

(c) Ours: our approach

TABLE I: Three confusion matrices for three algorithms: LC(a), SVM (b), Ours(c).

1 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 0 100 200 300 400 500 600 700 800 900 1000 1100 baseline Ours

Fig. 3: It shows the average percentages of the dominant class plotted based on the experiment result at each k  {10, 100, 200, 300, 400, 500, 1000}

which includes: learning the optimal feature mapping scheme and grouping the entire data using an improved clustering algorithm. The experimental results demonstrated the effectiveness and efficiency of our approach. There are still some limitations we need to work on. For example, when we learn the feature mapping scheme, we relax the problem to be one easier one, and thus the learned parameters are only locally optimal. Another problem is how to incorporate features reflecting temporal patterns of user behaviors. ACKNOWLEDGMENT This work was supported in part by a grant (#1135616) from National Science Foundation. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the NSF. R EFERENCES
[1] U. D. o. J. N. D. I. Center, "The economic impact of illicit drug use on american society," Product No. 2011-Q0317-002, 2011. [2] S. Abuse and M. H. S. Administration, "Results from the 2013 national survey on drug use and health: Summary of national findings," NSDUH Series H-48, vol. 14, no. 4863, 2014. [3] K. J. Quintelier, K. Ishii, J. Weeden, R. Kurzban, and J. Braeckman, "Individual differences in reproductive strategy are related to views about recreational drug use in belgium, the netherlands, and japan," Human Nature, vol. 24, no. 2, pp. 196­217, 2013. [4] J. C. A. Lacson, J. D. Carroll, E. Tuazon, E. J. Castelao, L. Bernstein, and V. K. Cortessis, "Population-based case-control study of recreational drug use and testis cancer risk confirms an association between marijuana use and nonseminoma risk," Cancer, vol. 118, no. 21, pp. 5374­ 5383, 2012.

[5] C. Lee, "Recruitment through social networking sites: Are substance use patterns comparable to traditional recruitment methods?" in Medicine 2.0 Conference. JMIR Publications Inc., Toronto, Canada, 2014. [6] J. M. Whitehill, M. A. Pumper, and M. A. Moreno, "Emerging adults use of alcohol and social networking sites during a large street festival: A real-time interview study," Substance abuse treatment, prevention, and policy, vol. 10, no. 1, p. 1, 2015. [7] J. J. van Hoof, J. Bekkers, and M. van Vuuren, "Son, youre smoking on facebook! college students disclosures on social networking sites as indicators of real-life risk behaviors," Computers in human behavior, vol. 34, pp. 249­257, 2014. [8] S. A. Stoddard, J. A. Bauermeister, D. Gordon-Messer, M. Johns, and M. A. Zimmerman, "Permissive norms and young adults alcohol and marijuana use: The role of online communities," Journal of Studies on Alcohol and Drugs, vol. 73, no. 6, pp. 968­975, 2012. [9] L. D. Johnston, Monitoring the Future: National Survey Results on Drug Use, 1975-2008: Volume II: College Students and Adults Ages 19-50. DIANe Publishing, 2010. [10] R. M. Schuster, R. Mermelstein, and L. Wakschlag, "Gender-specific relationships between depressive symptoms, marijuana use, parental communication and risky sexual behavior in adolescence," Journal of youth and adolescence, vol. 42, no. 8, pp. 1194­1209, 2013. [11] N. J. Jackson, J. D. Isen, R. Khoddam, D. Irons, C. Tuvblad, W. G. Iacono, M. McGue, A. Raine, and L. A. Baker, "Impact of adolescent marijuana use on intelligence: Results from two longitudinal twin studies," Proceedings of the National Academy of Sciences, p. 201516648, 2016. [12] C. L. Hanson, S. H. Burton, C. Giraud-Carrier, J. H. West, M. D. Barnes, and B. Hansen, "Tweaking and tweeting: exploring twitter for nonmedical use of a psychostimulant drug (adderall) among college students," Journal of medical Internet research, vol. 15, no. 4, 2013. [13] M. Mysl´ in, S.-H. Zhu, W. Chapman, and M. Conway, "Using twitter to examine smoking behavior and perceptions of emerging tobacco products," Journal of medical Internet research, vol. 15, no. 8, 2013. [14] P. Cavazos-Rehg, M. Krauss, R. Grucza, and L. Bierut, "Characterizing the followers and tweets of a marijuana-focused twitter handle," Journal of medical Internet research, vol. 16, no. 6, 2014. [15] N. D. Volkow, R. D. Baler, W. M. Compton, and S. R. Weiss, "Adverse health effects of marijuana use," New England Journal of Medicine, vol. 370, no. 23, pp. 2219­2227, 2014. [16] M. J. Krauss, S. J. Sowles, M. Moreno, K. Zewdie, R. A. Grucza, L. J. Bierut, and P. A. Cavazos-Rehg, "Peer reviewed: Hookah-related twitter chatter: A content analysis," Preventing chronic disease, vol. 12, 2015. [17] L. Thompson, F. P. Rivara, and J. M. Whitehill, "Prevalence of marijuana-related traffic on twitter, 2012­2013: a content analysis," Cyberpsychology, Behavior, and Social Networking, vol. 18, no. 6, pp. 311­319, 2015. [18] T. Katsuki, T. K. Mackey, and R. Cuomo, "Establishing a link between prescription drug abuse and illicit online pharmacies: Analysis of twitter data," Journal of medical Internet research, vol. 17, no. 12, 2015. [19] J. Bien, J. Taylor, and R. Tibshirani, "A lasso for hierarchical interactions," Annals of statistics, vol. 41, no. 3, 2013. [20] Y. Liu, J. Wang, and J. Ye, "An efficient algorithm for weak hierarchical lasso," in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 283­292. [21] A. Beck and M. Teboulle, "A fast iterative shrinkage-thresholding algorithm for linear inverse problems," SIAM journal on imaging sciences, vol. 2, no. 1, pp. 183­202, 2009. [22] Q. Gu, Z. Li, and J. Han, "Linear discriminant dimensionality reduction," in Machine Learning and Knowledge Discovery in Databases. Springer, 2011, pp. 549­564.

6

View publication stats

Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)

Clustering-Based Joint Feature Selection for Semantic Attribute Prediction
Lin Chen and Baoxin Li Arizona State University, Tempe Arizona {lin.chen.cs, baoxin.li}@asu.edu Abstract
Semantic attributes have been proposed to bridge the semantic gap between low-level feature representation and high-level semantic understanding of visual objects. Obtaining a good representation of semantic attributes usually requires learning from high-dimensional low-level features, which not only significantly increases the time and space requirement but also degrades the performance due to numerous irrelevant features. Since multiattribute prediction can be generalized as an multitask learning problem, sparse-based multi-task feature selection approaches have been introduced, utilizing the relatedness among multiple attributes. However, such approaches either do not investigate the pattern of the relatedness among attributes, or require prior knowledge about the pattern. In this paper, we propose a novel feature selection approach which embeds attribute correlation modeling in multi-attribute joint feature selection. Experiments on both synthetic dataset and multiple public benchmark datasets demonstrate that the proposed approach effectively captures the correlation among multiple attributes and significantly outperforms the state-of-the-art approaches.

Figure 1: Illustration of Shoe images with three corresponding attributes "High Heel", "Formal" and "Red". Good representations of semantic attributes are often built on top of high-dimensional, low-level features. Attribute learning directly based on such raw, high-dimensional features may suffer from the curse of dimensionality curse. Further, often it is reasonable to assume that not all the low-level features would have equal contribution to all the attributes. Feature selection, selecting a subset of most relevant features for a compact and accurate presentation, is proven to be an effective and efficient way to handle high-dimensional data [Tang et al., 2014]. Multi-task joint feature selection has been introduced by [Chen et al., 2014] for attribute ranking by exploring the correlation among attributes. However, this work assumes that all attributes are correlated by sharing the same subset of features, which is not always accurate. For example, as shown in Figure 1, a "high-heel" shoe is usually considered as a "formal" shoe as well. It is reasonable to assume these attributes share the same subset of features, e.g., shape-related descriptors. However, it is hard to identify whether "high heel" or "formal" shoes are in red, which suggests the attribute "color" may not share the same subset of features with the other attributes but is determined by, e.g., color-related descriptors. In other words, attributes are usually related in clustering structures. [Jayaraman et al., 2014] first explores such clustered relatedness on attribute prediction. However, their approach requires manually specified group structure as prior. To our knowledge, there is still lack of a feature selection approach being able to identify grouping/clusering structures among attributes for improved attribute prediction. In this paper, we propose a regularization-based multi-task feature selection approach that aims at automatically partitioning the attributes into groups while simultaneously uti-

1

Introduction

Recent literature has witnessed fast development of representations using semantic attributes, whose goal is to bridge the semantic gap between low-level feature representation and high-level semantic understanding of visual objects. Attributes refer to visual properties that help describe visual objects or scenes such as "natural" scenes, "fluffy" dogs, or "formal" shoes. Visual attributes exist across object category boundaries and many methods have been employed in applications including object recognition [Farhadi et al., 2010], face verification [Song et al., 2012], image search [Kovashka et al., 2012; Scheirer et al., 2012] and sentiment analysis [Wang et al., 2015].
 The work was supported in part by ONR grant N00014-15-12344 and ARO grant W911NF1410371. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of ONR or ARO.

3338

lizing such group information for attribute-dependent feature selection. We employ a clustering regularizer for attribute partition, where strong attribute relatedness is assumed to exist within each cluster. Besides, a group-sparsity regularizer is imposed on the objective function to encourage intra-cluster feature sharing and inter-cluster feature competition. Under this formulation, we propose an alternating structure optimization algorithm, which efficiently solves the relaxed form of the proposed formulation. We verify the effectiveness and generalization capability of our approach on both synthetic and real-world benchmark datasets. The results show that our approach outperforms the state-of-the-art approaches on feature selection, attribute prediction and zero-shot learning.

where mi denotes the mean vector of the i-th cluster. Let ei = [1, 1, . . . , 1]> 2 Rni 1 , then Eq. (1) can be derived as
ni k X X i=1 j =1 k X i=1 ( i) kwj

mi k =

2

k X i=1

kWi (Ini

ei ei > 2 )kF ni

=

Tr(Wi> Wi )

ei > ei ( p )Wi> Wi ( p ) ni ni

(2)

ek e1 e2 m k p p Let F = diag( p be an orn1 , n2 , . . . , nk ) 2 R thonormal matrix, then Eq. (2) can be rewritten as

2

Methodology

Let F = {f1 , f2 , . . . , fd } be the set of d features and then we can represent a set of n instances by the feature set F as X = [x1 , x2 , . . . , xn ] 2 Rdn . Let C = {c1 , c2 , . . . , cm } be the set of m attribute labels and Y = [y 1 , y 2 , . . . , y n ] 2 {0, 1}mn denotes the label matrix where y i 2 Rm (i = 1, 2, . . . , n) is the label vector of the i-th instance. We aim to select K (K d) most relevant features from F by leveraging X , Y and the attribute correlation in C . Let s =
d K K

Tr(W > W ) Tr(F > W > W F ) To make the problem tractable, we ignore the special structure of F and let it be an arbitrary orthonormal matrix. By adding a global penalty Tr(W > W ) measuring how large the weight vectors are, capturing label correlation is to partition W into k clusters, which can be achieved by solving the following optimization problem:
F > F =I k

min Tr(W > W ) Tr(F > W > W F )+ Tr(W > W ) (3)

2.2

Feature Selection

z }| { z }| {  (0, . . . , 0, 1, . . . , 1), where  (·) is the permutation function and K is the number of features to select where si = 1 indicates that the i-th feature is selected. The original data can be represented as diag(s)X with K selected features where diag(s) is a diagonal matrix. We assume that a linear projection matrix W = [w1 , w2 , . . . , wm ] 2 Rdm maps the data X to its label matrix Y where wi 2 Rd is the projection vector for the i-th class ci . If we do not consider attribute correlation, we can select K features via solving the following optimization problem: min L(W > diag(s)X, Y )
W,s

With the model component to capture attribute correlation in Eq. (3), the proposed feature selection framework is to solve the following optimization problem:
W,F,s

min L(W > diag(s)X, Y ) + Tr(W > W ) + (Tr(W > W ) Tr(F > W > W F ))

where L(·) is the loss function and typical choices of loss functions include least square and logistic regression.

s.t., s 2 {0, 1}n , sT 1n = K

s.t. F > F = Ik , s 2 {0, 1}n , s> 1n = K (4) where controls the contribution from modeling label correlation and controls the generalization performance. The constraint on s makes Eq. (4) a mixed integer programming problem, which is difficult to solve. We observe that diag(s) and W are in the form of W T diag(s). Since s is a binary vector and d K rows of the diag(s) are all zeros, W T diag(s) is a matrix where the elements of many rows are all zeros. This motivates us to absorb diag(s) into W as W = W T diag(s), and add `2,1 -norm on each grouped Wi to encourage sparse-based group-wise joint feature selection. With this relaxation, Eq. (4) can be rewritten as:
W,F ;F > F =Ik

2.1

Modeling Label Correlation

Based on the assumption that correlated attributes would share the same features, we propose to model attribute correlation via learning the clustering structures through k-means. Let E be a permutation partition matrix, then a partition of the projection matrix W into k clusters can be formed as:
i) W E = [W1 , W2 , . . . , Wk ], Wi = [w1 , w2 , . . . , w( ni ]; ( i) ( i)

min

L(W > X, Y ) + 

where Wi 2 Rdni (i = 1, 2, . . . , k ) is the i-th partitioned group includes ni projection vectors (or attribute labels). The associated sum-of-squares cost function for the partition can be formulated as
ni k X X i=1 j =1 ( i) kw j

+ (Tr(W > W ) Tr(F > W > W F )) (5) where  controls the sparsity of W . The key idea lying here is that we use the clustering regularizer to partition the tasks into groups where strong correlation exists among tasks in the same group; and feature selection based on such group structures would make sure appropriate feature subsets are selected to represent the respective semantic attributes.

k X i=1

kWi k2,1 + Tr(W > W )

3

Algorithm

mi k , mi =

2

ni X j =1

( i) wj /ni

(1)

In this section, we first introduce an optimization algorithm to seek an optimal solution (summarized in Algorithm 1) for Eq. (5). Then we propose an approach to estimate the attribute assignment (summarized in Algorithm 2).

3339

3.1

Optimization

The optimization problem in Eq. (5) is non-convex nonsmooth, which makes the formulation difficult to solve in its original form. Thus we adopt several relaxations to make it solvable. The attribute correlation regularization in Eq. (3) can be rewritten as: Tr(W ((1 +  )I F F > )W > ) where  = / > 0. Let M = F F > , according to [Zhou et al., 2011] the previous regularizer can be relaxed into the following convex form:  (1 +  )Tr(W ( I + M ) 1 W > ) s.t. tr(M ) = k, M I, M 2 Sm (6) + m where S+ is the set of mm positive semidefinite matrices. Following a similar idea in [Bach, 2008], we reformulate Eq. (5) by squaring the `2,1 norm. Since the `2,1 norm is positive, the squaring represents a smooth monotonic mapping. Without loss of the generality, we adopt the traditional least square loss for demonstration in this paper. Then we get the following jointly convex smooth objective function regarding to W and M . k X arg min kW > X Y k2 ( kW i k 2 , 1 ) 2 F +
W,M i=1

Algorithm 1 Feature Selection Optimization Input: 1. Multiple attribute data {X, Y }; 2. Parameters , , k (optional) and the number of selected features K ; 3. The initial projection matrix W0 ; Procedure: 1: Set W = W0 ; 2: repeat 3: Update M according to Eq. (8); 4: Update r according to Alg. 2; 5: Update according to Eq. (10); 6: Update W according to Eq. (11); 7: until Converges 8: Sort each feature according to kwi k2 in descending order of each group; 9: return The group-wise top-K ranked features; Algorithm 2 Cluster Assignment Estimation Input: M; Procedure: 1: Approximate F by top-ranked eigenvector of Q; 2: Calculate R11 , R12 by applying QR decomposition with column pivoting on F by Eq. (12); ^ by Eq. (13); 3: Calculate R 4: calculate r by Eq. (14) for each attribute; 5: return Cluster assignment vector r ; Optimizing W When Fixing M The squared group-wise `2,1 norm in Eq. (7) is still difficult to derive directly. To alleviate that, we introduce Psome P positive dummy variables ij 2 R+ which satisfies i j ij = 1. [Argyriou et al., 2008] proves an upper bound of the squared `2,1 norm in terms of the positive dummy variables
k X i=1

 (1 +  )Tr(W ( I + M ) 1 W > ) s.t. tr(M ) = k, M I, M 2 Sm (7) + Since it is difficult to optimize the linear projection matrix W and attribute correlation matrix M simultaneously, we employ Alternating Structure Optimization (ASO), which has been shown to be effective in many practical applications [Blitzer et al., 2006; Quattoni et al., 2007] and is guaranteed to converge to a global optimal solution. Optimizing M when fixing W Given a fixed W , the optimization problem is decoupled into the following optimization problem: min Tr(W ( I + M )
M 1

W >)

(kWi k2,1 )2 = (

s.t. tr(M ) = k, M I, M 2 Sm (8) + We solve the problem based on the following Lemma due to [Zhou et al., 2011]: Lemma 1 For the optimization problem in Eq. (8), let W = U V be the singular value decomposition of W where  = diag([ 1 , 2 , . . . , m ]), M = QQ> be the Eigen decomposition of M where  = diag([ 1 , 2 , . . . , q ]) and q be the rank of . Then the optimal Q is given by Q = V and the optimal  is given by solving the following optimization problem: q 2 X i  = arg min  + i i=1 s.t.
q X i=1 i

where wi,j 2 R1m is the row vector of Wi . Thus updated by holding the equality:
ij

k X d X i=1 j =1

kwi,j k2 )2 

k X d X (kwi,j k2 )2 i=1 i=1 ij ij

can be

= kwi,j k2 /

Given a fixed M , each projection vector w can then be updated by optimize the following problem arg min kW T X
W

d X j =1

kwi,j k2 .

(10)

Y k2 F +

 (1 +  )Tr(W ( I + M )

k X d X (kwi,j k2 )2 i=1 i=1 ij 1

W )

>

(11)

which can be solved by gradient-type approach.

= k, 0 

i 1

(9)

3.2

Estimating Attribute Assignment

Eq. (9) can be solved using the similar technology in [Jacob et al., 2009].

The group-wise feature selection is conducted by the clustering structure of the attribute. However, given the M optimized by the previous algorithm, it is not readily possible

3340

to observe the cluster assignment of the attributes because M is spectrally relaxed. In this subsection, we propose an approach to acquire the cluster structure. We first need to obtain a good approximation of the cluster indicator matrix F . Given M , we first apply Eigen decomposition M = QQ> where each column of Q is the eigenvector and each diagonal element of  is the eigenvalue. Then we rank the columns of Q in decreasing order according to its corresponding eigenvalues, and the top-ranked k columns give an approximation of the cluster assignment matrix F . The number of the cluster k can be either manually specified or automatically explored by setting a threshold (10e 8 in our experiment) regarding to the absolute value of the eigenvalue. After obtaining F , without loss of generality, we assume the optimized W = [W1 , W2 , · · · , Wk ]T where the submatrix Wi includes all attributes belonging to the i-th cluster. Let ti = [ti1 , ti2 , . . . , tini ]T denote the largest eigenvector of Wi T Wi , [Zha et al., 2002] showed that F can be reformulated as F T = [t11 v 1 , · · · , t1s1 v 1 , · · · , tk1 v k , · · · , tks1 v k ] | {z } | {z }
cluster 1 clusterk T k k

zero-shot learning capabilities on image benchmark datasets. All the datasets are standardized to zero-mean and normalized by the standard deviation. For all approaches, the super parameters are selected via cross-validation. We cannot get the number of cluster k without any prior knowledge for realworld, thus we also select k by the prediction accuracy on a small subset of datasets.

4.1

Simulation Study

where V = [v1 , v2 , · · · , vk ] 2 R is an orthogonal matrix. Since vi is orthogonal to each other, the cluster structure can be acquired by picking up a column of F which has the largest norm as the first cluster, and orthogonalizing the other columns against this column. Then the same process is executed on the rest of columns until all clusters are identified. This process is identical to a QR decomposition with column pivoting on F F T = Q[R11 , R12 ]P T (12) where Q 2 Rkk is an orthogonal matrix, R11 2 Rkk is an upper triangular matrix and P 2 Rmm is a permutation ^2 matrix. Then we calculate the cluster assignment matrix R Rkm by ^ = [Ik , R 1 R12 ]P T R (13) 11 where Ik 2 Rkk is an identity matrix. The cluster assign^ . The cluster ment information can then be inferred from R membership of each attribute (column) is determined by the row index of the largest element (in absolute value) of the ^ . Denote r 2 Rm as the cluster corresponding column in R identification vector where ri records which cluster the i-th class belongs to, then r can be calculated by ri = arg max r ^ij
j

Since it is difficult to obtain the groundtruth cluster structure for real applications, we first verify the effectiveness of the proposed approach in obtaining the cluster structures on simulated dataset. Following [Jacob et al., 2009; Zhou et al., 2011], we construct the synthetic data containing 5 clusters with 10 learning tasks in each cluster, generating a total number of 50 tasks. For the i-th task, a dataset Xi 2Rdn is randomly drawn from a normal distribution N (0, 1) for learning, with the dimension d = 30 and the sample size n = 60. The projection model is constructed as follows. For the id th cluster, we generate a cluster weight vector wc i 2R drawn from the normal distribution N (0, 900). Then 15 dimensions of wc i are randomly but carefully selected and assigned to zeros, to ensure all wc are orthogonal to each other. Similarly, for the j -th task belonging to cluster i, we generate a taskd specific weight vector ws j 2R drawn from the normal distribution N (0, 16) with the same dimensions of wc i assigned to zeros. Thus, the ultimate weight vector of the j -th task is the linear combination of the cluster and task-specific weight s vector wj = wc i + wj . The corresponding response y i of the i-th samples xi of task j is then obtained by y i = wT j xi + "i where " is the noise vector drawn from N (0, 0.1). We choose 0.5 as the threshold to assign binary label to each sample. We verify the effectiveness of our proposed approach by comparing the learned cluster structure and the selected features with the groundtruth. Based on the prior knowledge implied by the construction of the groundtruth, We set k = 5 and the number of selected features as K = 15. Figure 2 shows one example of the learned projection matrix 2(b) with the comparison of the groundtruth 2(a) where the white part represents zeros and the black part represents non-zeros. The result shows that our approach is able to roughly capture the correct group sparse structures.

(14)

^. where r ^ij is the (i, j )-th entry of R

4

Experiments

In this section, we first verify the effectiveness of our proposed approach on one synthetic dataset. Since the proposed approach can be generalized to general multi-label problem, we evaluate the feature selection capability on various benchmark datasets. At last we evaluate the attribute prediction and

(a) Groundtruth model

(b) Learned model

Figure 2: The learned projection matrix and the corresponding groundtruth in the simulation experiments. The white parts are zeros and the black parts are non-zeros.

3341

4.2

Feature Selection

We verify the feature selection capability on general multilabel datasets in this section. The experiment is conducted on 6 public benchmark feature selection datasets including one object image dataset COIL100 [COI, 1996], one handwritten digit image dataset USPS [Hull, 1994], one spoken letter speech dataset Isolet [Fanty and Cole, 1991], three face image dataset YaleB [Georghiades et al., 2001], ORL [Samaria and Harter, 1994] and PIX10P1 . The statistics of the datasets are summarized in Table 2. We compare the proposed approach with the following representative feature selection algorithms: Fisher Score [Duda et al., 2001], mRMR [Peng, 2005], Relief-F [Liu and Motoda, 2008], Information Gain [Cover and Thomas, 1991], MTFS [Argyriou et al., 2008]. Following the common way to evaluate supervised feature selection, we assess the quality of selected features in terms of the classification performance [Han et al., 2013; Cai et al., 2013]. The larger classification accuracy is, the better performance the corresponding feature selection approach achieves. In our experiments, we employ linear Support Vector Machine (SVM) and k -nearest neighbors (k NN) classifier with k = 3 for evaluation. How to determine the optimal number of selected features is still an open question for feature selection; hence we vary the number of selected features as {10,30, 50 . . . ,90} in this work. In each setup 50% samples are randomly selected for training and the remaining is for testing. Specific constrains are imposed to make sure the class labels of the training set are balanced. The whole experiment is conducted 10 rounds and average accuracies are reported. Figure 1 shows the comparison results for SVM and k NN on the 6 benchmark datasets when 50 features are selected. The result shows that MTFS and the proposed framework outperform Fisher Score, mRMR and Information Gain. The performance gain comes from that Fisher Score, mRMR and Information Gain select features one by one while MTFS and FSMC select features in a batch model. It is consistent with what was suggested in [Tang and Liu, 2012] that it is better to analyze features jointly for feature selection. Besides, in most cases, the proposed framework outperforms MTFS. Better performance gain is usually achieved when fewer number of features are selected. This performance gain suggests that modeling label correlation can significantly improve feature selection performance for multi-class data.

The experiments are conducted on three benchmark datasets: aYahoo [Farhadi et al., 2009], Animals with Attributes (AwA) [Lampert et al., 2009] and SUN attribute [Patterson and Hays, 2012] and the statistics of the datasets are summarized in Table 4. To obtain a good representation of the high-level attributes, we require that the features can capture both the spatial and context information. Thus, we constructed the features by pooling a variety types of feature histograms including GIST, HoG, SSIM. For aPascal/aYahoo and AwA datasets we use predefined seen/unseen split published with the datasets. For SUN dataset, 60% of categories are randomly split out as "seen" categories in each round with the rest as "unseen" categories. During training 50% of samples are randomly and carefully drawn from each seen categories to ensure the balance of the positive and negative attribute labels. The rest samples from "seen" classes and all samples from "unseen" classes are used for testing. Table 3 shows the average prediction accuracy of each approach over all attributes by running the experiment 10 rounds. The result shows that for both "seen" and "unseen" categories, DSVA outperforms MTAL in prediction accuracy and our proposed approach further outperforms DSVA by 2%4%. DSVA decorrelates low-correlated attributes compared with MTAL thus achieves better prediction performance. However, the manually specified or off-line learned group structures are not able to achieve the optimal result. Our approach iteratively optimizes the clustering structure and the projection model, which achieves the best performance.

4.4

Zero-shot Learning

4.3

Attribute Prediction

We then compare our approach with state-of-the-art attribute learning work [Chen et al., 2014] (referred as MTAL) and [Jayaraman et al., 2014] (referred as DSVA). Since MTAL is initially proposed for attribute ranking, we replace the original loss function with the one adopted in this paper for fair comparison. DSVA requires attribute groups as prior, thus we run k-means offline to obtain the clusters for datasets do not have such information.
1 PIX10P is publicly available https://featureselection.asu.edu/datasets.php

We also experiment on the zero-shot learning problem on all three datasets. Zero-shot learning aims to learn a classifier based on training samples from some seen categories, and classify some new samples to a new unseen category. We adopt the Direct Attribute Prediction (DAP) framework proposed in [Lampert et al., 2009] with attribute prediction probability from each approaches as input. Since only continuous image level attribute labels are provided on the SUN dataset, we construct the class level attribute labels by thresholding the average attribute label values of all samples from the class. Same "Seen"\"Unseen" categories splits are adopted as previous experiments. The Average classification accuracies of 10 rounds experiment are reported in Table 5. The result shows that on aYahoo and AwA, our approach achieves significant performance gains than the baseline approaches. The large number of categories in SUN dataset make the classification problem very hard which leads to all low performance of all approaches. Our approach still works better than the baseline approaches.

4.5

On Choosing the Parameters

from

The proposed framework has three important parameters -  controlling the sparsity of W , controlling the contribution of modeling label correlation and gamma controls the global penalty. We study the effect of each parameter by fixing the other to see how the performance of the proposed approach varies with the number of selected features. Due to the page

3342

Table 1: Classification results (ACC%±std) of different feature slection algorithm on different datasets. (the higher the better). Algorithm DataSet Fisher mRMR Relief-F Information Gain MTFS Proposed COIL100 60.66±3.54 55.72±3.34 62.80±2.56 62.00±2.84 78.77±2.35 79.08±2.12 USPS 86.30±2.81 58.44±4.02 86.83±2.83 70.25±3.16 86.25±2.52 93.15±2.18 Isolet 75.64±3.01 70.92±3.72 82.30±2.81 76.51±2.56 84.05±2.24 87.06±1.98 SVM YaleB 66.85±3.65 56.91±4.21 71.91±2.24 71.74±2.11 76.08±2.14 78.17±2.18 ORL 46.50±4.21 84.51±2.32 67.18± 3.01 53.24±2.96 85.62±1.94 90.51±1.78 PIX10P 93.56±2.01 90.45±3.32 96.00±1.77 92.01±1.97 96.81±1.54 99.54±1.68 COIL100 63.33±3.21 54.86±4.32 65.11±2.01 63.44±2.76 81.86±1.94 82.48±1.68 USPS 89.39±2.11 59.17±3.72 89.61±2.01 74.70±2.76 90.44±1.54 95.53±1.18 Isolet 75.38±2.45 57.56±3.42 79.87±2.21 73.71±2.42 77.01±2.14 83.21±2.18 k NN YaleB 69.17±3.24 58.41±3.72 65.53±2.81 65.37±2.42 77.08±2.45 78.96±2.28 ORL 53.01±3.44 72.56±2.42 60.38±2.71 52.44±2.76 85.86±2.24 88.10±2.10 PIX10P 94.56±1.91 86.45±2.22 96.00±1.81 86.04±2.04 97.81±1.54 99.34±1.22 Table 2: Statistics of the Feature Selection datasets Dataset # of Samples # of Features # of Classes COIL100 7200 1024 100 YaleB 2414 1024 38 ORL 400 4096 40 PIX10P 100 10000 10 USPS 9298 256 10 Isolet 7797 617 150 regularizer encourages intra-group feature-sharing and intergroup feature competition. With an efficient alternating optimization algorithm, the proposed approach is able to obtain a good group structure and select appropriate features to represent semantic attributes. The proposed approach was verified on both synthetic and real-world benchmark datasets with comparison with state-of-the-art approaches. The result shows effective group structure identification capability of our method, as well as its significant performance gains on feature selection, attribute prediction and zero-shot learning.

References
[Argyriou et al., 2008] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning. J. Mach. Learn. Res., 73(3):243­272, December 2008. [Bach, 2008] Francis R. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn. Res., 9:1179­1225, June 2008. [Blitzer et al., 2006] John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspondence learning. EMNLP '06, pages 120­128, 2006. [Cai et al., 2013] Xiao Cai, Feiping Nie, and Heng Huang. Exact top-k feature selection via l2,0-norm constraint. In IJCAI '13, pages 1240­1246, 2013. [Chen et al., 2014] Lin Chen, Qiang Zhang, and Baoxin Li. Predicting multiple attributes via relative multi-task learning. In Proc. of CVPR'14, pages 1027­1034, June 2014. [COI, 1996] Columbia Object Image Library (COIL-100). Technical report, Columbia University, 1996. [Cover and Thomas, 1991] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, 1991. [Duda et al., 2001] R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classification. John Wiley & Sons, New York, 2 edition, 2001. [Fanty and Cole, 1991] Mark Fanty and Ronald Cole. Spoken letter recognition. In NIPS '91, pages 220­226. 1991. [Farhadi et al., 2009] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. In Com-

Figure 3: Parameter Analysis on SVM. limitation, we only report the result on the Isolet dataset with SVM but we have similar observations in other datasets. Figure 3 demonstrates the performance variance w.r.t. different parameters and the number of selected features. With the increase of , the performance first increases, demonstrating the importance of modeling label correlation, and then decreases. This property is practically useful because we can use this pattern to set . When  increases, the performance also increases dramatically, which suggests the capability of `2,1 -norm for feature selection. The performance also increases with and then decrease, but relatively stable. The best performance is achieved around 0.1.

5

Conclusions

In this paper, we proposed a clustering-base multi-task joint feature selection framework for semantic attribute prediction. Our approach employs both clustering and group-sparsity regularizers for feature selection. The clustering regularizer partitions the attributes into different groups where strong correlation lies among attributes in the same group while weak correlation exists between groups. The group-sparsity

3343

Table 3: DataSet Methods MTAL DSVA Porposed

Average prediction accuracies of all attributes on Seen and Unseen categories (the higher the better). aPascal/aYahoo AwA SUN Seen Unseen Seen Unseen Seen Unseen 0.5967±0.020 0.5663±0.022 0.5976±0.011 0.5587±0.012 0.6326±0.021 0.6020±0.022 0.6105±0.018 0.5826±0.019 0.6053±0.015 0.5622±0.018 0.6469±0.025 0.6165±0.027 0.6363±0.014 0.6011±0.015 0.6254±0.007 0.5837±0.008 0.6682±0.011 0.6324±0.013 [Liu and Motoda, 2008] H. Liu and H. Motoda, editors. Computational Methods of Feature Selection. Chapman & Hall, 2008. [Patterson and Hays, 2012] Genevieve Patterson and James Hays. Sun attribute database: Discovering, annotating, and recognizing scene attributes. In Proc. of CVPR'12, 2012. [Peng, 2005] F. Ding C. Peng, H. Long. Feature selection based on mutual information: Criteria of max-dependency, max-relevance, and min-redundancy. IEEE Trans. Pattern Anal. Mach. Intell., 27(8):1226­1238, 2005. [Quattoni et al., 2007] A. Quattoni, M. Collins, and T. Darrell. Learning visual representations using images with captions. In CVPR '07, pages 1­8, June 2007. [Samaria and Harter, 1994] F.S. Samaria and A.C. Harter. Parameterisation of a stochastic model for human face identification. In Applications of Computer Vision' 94, pages 138­142, Dec 1994. [Scheirer et al., 2012] W.J. Scheirer, N. Kumar, P.N. Belhumeur, and T.E. Boult. Multi-attribute spaces: Calibration for attribute fusion and similarity search. In Proc. of CVPR'12, pages 2933­2940, June 2012. [Song et al., 2012] Fengyi Song, Xiaoyang Tan, and Songcan Chen. Exploiting relationship between attributes for improved face verification. In Proc. of BMVC'12, pages 27.1­27.11, 2012. [Tang and Liu, 2012] Jiliang Tang and Huan Liu. Feature selection with linked data in social media. In SDM '12, pages 118­128. SIAM, 2012. [Tang et al., 2014] Jiliang Tang, Salem Alelyani, and Huan Liu. Feature selection for classification: A review. Data Classification: Algorithms and Applications. Editor: Charu Aggarwal, CRC Press In Chapman & Hall/CRC Data Mining and Knowledge Discovery Series, 2014. [Wang et al., 2015] Yilin Wang, Suhang Wang, Jiliang Tang, Huan Liu, and Baoxin Li. Unsupervised sentiment analysis for social media images. In Proceedings of the TwentyFourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 2378­2379, 2015. [Zha et al., 2002] Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, and Horst D. Simon. Spectral relaxation for kmeans clustering. In Proc. of NIPS'02, pages 1057­1064. 2002. [Zhou et al., 2011] Jiayu Zhou, Jianhui Chen, and Jieping Ye. Clustered multi-task learning via alternating structure optimization. In Proc. of NIPS'11, pages 702­710. 2011.

Table 4: Statistics of Attribute Prediction Image Datasets. Dataset aPascal/aYahoo AwA SUN # of images 15339 30475 14340 # of attributes 64 85 102 # of classes 32 50 611 # of features 2429 1200 1112 Table 5: Zero-shot learning accuracy on both real dataset. aYahoo AwA SUN MTAL 0.1834 0.2953 0.1842 DSVA 0.2052 0.3085 0.2010 Proposed 0.2262 0.3258 0.2133 puter Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1778­1785, June 2009. [Farhadi et al., 2010] A. Farhadi, I. Endres, and D. Hoiem. Attribute-centric recognition for cross-category generalization. In Proc. of CVPR'10, pages 2352­2359, June 2010. [Georghiades et al., 2001] A.S. Georghiades, P.N. Belhumeur, and D.J. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE Trans. Pattern Anal. Mach. Intell., 23(6):643­660, 2001. [Han et al., 2013] Yahong Han, Yi Yang, and Xiaofang Zhou. Co-regularized ensemble for feature selection. In IJCAI '13, pages 1380­1386, 2013. [Hull, 1994] J. J. Hull. A database for handwritten text recognition research. IEEE Trans. Pattern Anal. Mach. Intell., 16(5):550­554, May 1994. [Jacob et al., 2009] Laurent Jacob, Jean philippe Vert, and Francis R. Bach. Clustered multi-task learning: A convex formulation. In Proc. of NIPS'09, pages 745­752. 2009. [Jayaraman et al., 2014] D. Jayaraman, Fei Sha, and K. Grauman. Decorrelating semantic visual attributes by resisting the urge to share. In Proc. of CVPR'14, pages 1629­1636, June 2014. [Kovashka et al., 2012] A. Kovashka, D. Parikh, and K. Grauman. Whittlesearch: Image search with relative attribute feedback. In Proc. of CVPR'12, pages 2973­2980, June 2012. [Lampert et al., 2009] C.H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In Proc. of CVPR'09, pages 951­958, June 2009.

3344

A Structured Approach to Predicting Image Enhancement Parameters
Parag Shridhar Chandakkar Baoxin Li School of Computing, Informatics and Decision Systems Engineering, Arizona State University
{pchandak,baoxin.li}@asu.edu

Abstract
Social networking on mobile devices has become a commonplace of everyday life. In addition, photo capturing process has become trivial due to the advances in mobile imaging. Hence people capture a lot of photos everyday and they want them to be visually-attractive. This has given rise to automated, one-touch enhancement tools. However, the inability of those tools to provide personalized and contentadaptive enhancement has paved way for machine-learned methods to do the same. The existing typical machinelearned methods heuristically (e.g. k NN-search) predict the enhancement parameters for a new image by relating the image to a set of similar training images. These heuristic methods need constant interaction with the training images which makes the parameter prediction sub-optimal and computationally expensive at test time which is undesired. This paper presents a novel approach to predicting the enhancement parameters given a new image using only its features, without using any training images. We propose to model the interaction between the image features and its corresponding enhancement parameters using the matrix factorization (MF) principles. We also propose a way to integrate the image features in the MF formulation. We show that our approach outperforms heuristic approaches as well as recent approaches in MF and structured prediction on synthetic as well as real-world data of image enhancement.

touch enhancement tools. However, most of these tools are pre-defined image filters which lack the ability of doing content-adaptive or personalized enhancement. This has fueled the development of machine-learning based image enhancement algorithms. Many of the existing machine-learned image enhancement approaches first learn a model to predict a score quantifying the aesthetics of an image. Then given a new lowquality image2 , a widely-followed strategy to generate its enhanced version is as follows: · Generate a large number of candidate enhancement parameters3 by densely sampling the entire range of image parameters. Computational complexity may be reduced by applying heuristic criteria such as, densely sampling only near the parameter space of most similar training images. · Apply these candidate parameters to the original lowquality image to create a set of candidate images. · Perform feature extraction on every candidate image and then compute its aesthetic score by using the learned model. · Present the highest-scoring image to the user. There are two obvious drawbacks for the above strategy. First, generating and applying a large number of candidate parameters to create candidate images may be computationally prohibitive even for low-dimensional parameters. For example, a space of three parameters where each parameter  {0, ..., 9} produces 103 combinations. Second, even if creating candidate images is efficient, extracting features from them is always computationally intensive and is the bottleneck. Also, such heuristic methods need constant interaction with the training database (which might be
2 We call the images before enhancement as low-quality and those after enhancement as high-quality in the rest of this article. The process of enhancing a new image is called "the testing stage". 3 The brightness, saturation and contrast are referred to as "parameters" of an image in this article.

1. Introduction
The growth of social networking websites such as Facebook, Google+, Instagram etc. along with the ubiquitous mobile devices has enabled people to generate multimedia content at an exponentially increasing rate. Due to the easy-to-use photo-capturing process of mobile devices, people are sharing close to two billion photos per day on the social networking sites 1 . People want their photos to be visually-attractive which has given rise to automated, one1 http://www.kpcb.com/internet-trends

stored on a server) that makes the parameter prediction suboptimal. All these factors contribute to making the testing stage inefficient. Our approach assumes that a model quantifying image aesthetics has already been learned and instead focuses on finding a structured approach to enhancement parameter prediction. During training, we learn the inter-relationship between the low-quality images, its features, its parameters and the high-quality enhancement parameters. During the testing stage, we only have access to a new low-quality image, its features, parameters and the learned model and we have to predict the enhancement parameters. Using these enhancement parameters, we can generate the candidate images and select the best one using the learned model. The stringent requirement of not accessing the training images arises from real-world requirements. For example, to enhance a single image, it would be inefficient to establish a connection with the training database, generate hundreds of candidate images, perform feature extraction on them and then find the best image. The search space spanned by the parameters is huge. However, the enhancement parameters are not randomly scattered. Instead they depend on the parameters and features of the original low-quality image. Thus we hypothesize that the enhancement parameters should have a lowdimensional structure in another latent space. We employ an MF-based approach because it allows us express the enhancement parameters in terms of three latent variables, which model the interaction across: 1. the lowquality images 2. their corresponding enhancement parameters 3. the low-quality parameters. The latent factors are learned during inference by Gibbs sampling. Additionally, we need to incorporate the low-quality image features since the enhancement parameters also depend on the color composition of the image, which can be characterized by the features. The feature incorporation in this framework is achieved by representing the latent variable which models the interaction across these images as a linear combination of their features, by solving a convex 2,1 -norm problem. We review the related work on MF as well as image enhancement in the following section.

2. Related Work
Development of machine-learned image enhancement systems has recently been an active research area of immense practical significance. Various approaches have been put forward for this task. We review those works which improve the visual appearance of an image using automated techniques. To encourage research in this field, a database named MIT-Adobe FiveK containing corresponding low and high-quality images was proposed in [4]. The authors also proposed an algorithm to solve the problem of global tonal adjustment. The tone adjustment problem only

manipulates the luminance channel, where we manipulate saturation, brightness and contrast of an image. Content-based enhancement approaches have been developed in the past which try to improve a particular image region [2, 9]. These approaches require segmented regions which are to be enhanced. This itself may prove to be difficult. Approaches which work on pixels have also been developed using local scene descriptors. Firstly, similar images from the training set are retrieved. Then for each pixel in the input, similar pixels were retrieved from the training set, which were then used to improve the input pixel. Finally, Gaussian random fields maintain the spatial smoothness in the enhanced image. This approach does not consider the global information provided by the image and hence the enhancements may not be visually-appealing when viewed globally. In [8], a small number of image enhancements were collected from the users which were then used along with the additional training data. Two recent works involving training a ranking model from low and high-quality images are presented in [5, 25]. The authors of [25] create a data-set of 1300 corresponding low and high-quality image pairs along with a record of the intermediate enhancement steps. A ranking model trained on this type of data can quantify the aesthetics of an image. In [5], non-corresponding low and high-quality image pairs extracted from the Web are used to train a ranking model. Both of these approaches use k NN-search during the testing stage to create candidate images. After extracting features and ranking them, the best image is presented to the user. The task of enhancement parameter prediction could be related to the attribute prediction [17, 18, 11, 7]. However, the goal of the work on attribute prediction has been to predict relative strength of an attribute in the data sample (or image). We are not aware of any work which predicts parameters of an enhanced version of a low-quality image given only the parameters and features of that image. Since our approach is based on MF principles, we review the related recent work on MF. MF [19, 15, 20, 10, 24] is extensively used in recommender systems [12, 1, 13, 23, 14, 22, 21]. These systems predict the rating of an item for a user given his/her existing ratings for other items. For example, in Netflix problem, the task is to predict favorite movies based on user's existing ratings. MF-based solutions exploit following two key properties of such user-item rating matrix data. First, the preferred items by a user have some similarity to the other items preferred by that user (or by other similar users, if we have sufficient knowledge to build a similarity list of users). Second, though this matrix is very high-dimensional, the patterns in that that matrix are structured and hence they must lie on a low-dimensional manifold. For example, there are 17, 770 movies in Netflix data and ratings range from 1 - 5. Thus, there are 517770 rating combinations possible

per user and there are 480, 189 users. Therefore, the number of actual variations in the rating matrix should be a lot smaller than the number of all possible rating combinations. These variations could be modeled by latent variables lying near a low-dimensional manifold. This principle is formalized in [15] with probabilistic matrix factorization (PMF). It hypothesizes that the rating matrix can be decomposed into two latent matrices corresponding to user and movies. Their dot product should give the user-ratings. This works fairly well on a large-scale data-set such as Netflix. However, a lot of parameters have to be tuned. This requirement is alleviated in [20] by developing a Bayesian approach to MF (BPMF). BPMF has been extended for temporal data (BPTF) in [24]. MF is used in other domains such as computer vision to predict feature vectors of another viewpoint of a person given a feature for one viewpoint [6]. We adopt and modify BPTF since it allows us to model joint interaction across low-quality images, corresponding enhancement parameters and the low-quality parameters. In the next section, we detail our problem formulation and proposed approach.

in three-dimensional matrix R  RN ×(M +1)×K . We need ^ k = Rk + Rk or in turn just Rk . Rk deto predict R ij i ij ij i notes the k th parameter value (k  {1, . . . , K }) of the ith ^ k is the k th parameter value of j th low-quality image and R ij version of the ith image. Given a new nth low-quality imk age, we only need to predict Rnj  j = {1, . . . , M },  k . k During training, we can compute Rij from available k k ^ Rij and Rij . Following MF principles, we express R as an inner product of three latent factors, U  RD×N , V  RD×M and T  RD×K [20, 24]. D is the latent factor dimension. These latent factors should presumably model the underlying low-dimensional subspace corresponding to the low-quality images, its enhanced versions and its parameters. This can be formulated as:
D k Rij =< Ui , Vj , Tk > d=1

Udi Vdj Tdk ,

(1)

3. Problem Formulation and Proposed Approach
We have a training set consisting of N images {S1 , . . . , SN }4 . Parameters of all images are represented as A = {A1 , . . . , AN } where Ai  RK ×1  i  {1, . . . , N }. Each image has M enhanced versions and each version has the same size as that of its corresponding low-quality image. All versions corresponding to the ith image are repreM sented as {W1 i , . . . , Wi }. All versions are of higher quality as compared to its corresponding image. Parameters of all M versions of the ith image (also called as candidate pa1 M rameters) are represented as A = {Ai , . . . , Ai }, where j K ×1 Ai R  i, j . Features of all low-quality images are represented as F = {F1 , . . . , FN } where Fi  RL×1  i. In practice, we observe that M N, K < M . Our goal is to be able to predict the candidate parameters for all the versions of the ith image by only using the information provided by Ai and Fi . To the best of our knowledge, this is a novel problem of real significance that has not been addressed in the literature. We now explain our proposed approach. As mentioned before, our task is to predict the candidate parameters for all the enhanced versions of a low-quality image with the help of its parameters and features. The values for all the K parameters corresponding to N images and their N · M versions (total N + N · M ) can be stored
4 We use bold letters to denote matrices. Non-bold letters denote scalars/vectors which will either be clear from the context or will be mentioned. X i , Xi , XT , Xij and ||X||p denote row, column, transpose, entry at row i and column j of a matrix X and pth norm of matrix X respectively.

where Udi denotes the dth feature of the ith column of U. Presumably, as we increase D, the approximation erk ror Rij - < Ui , Vj , Tk > should decrease (or stay constant) if the prior parameters for latent factors U, V and T are chosen correctly. Following [20], we choose normal distribution (with precision ) for: 1. the conditional distribution R|(U, V, T) and 2. for prior distributions - p(U|U ), p(V|V ) and p(T|T ), where U = 1 -1 -1 (µU , - U ), V = (µV , V ), T = (µT , T ). U , V and T are hyper-parameters, and µ and  are the multivariate precision matrix and the mean respectively. Since the Wishart distribution is a conjugate prior for multivariate normal distribution (with precision matrix), we put Gaussian-Wishart priors on all hyper-parameters5 . We could find the latent factors U, V and T by doing inference through Gibbs sampling. It will sample each latent variable from its distribution, conditional on the values of other varik ables. The predictive distribution for Rij can be found by using Monte-Carlo approximation (explained later). However, it is important to note the following major differences in our problem when compared with the previous work on MF [20, 24]. In product or movie rating prediction problems, an average (non-personalized) recommendation may be provided to a user who has not provided any preferences (not necessarily constant for all users). In our case, each image may require a different kind of parameter adjustment to create its enhanced version and thus no "average" adjustment exists. As explained before, the adjustment should depend on the image's features, which characterize that image (e.g. bright vs. dull, muted vs. vibrant). In our problem, it is particularly difficult to get a good generalizing performance on the testing set as we shall see later. The loss in performance of existing approaches on the testing set can
5 For

details, see supplementary material on author's website.

be attributed to the different requirements for parameter adjustments for each image. Thus it becomes necessary to include the information obtained from image features into the formulation. We show that simply concatenating the parameters and features and applying MF techniques presented in [20, 24] does not provide good performance, possibly because they lie in different regions of the feature space. To overcome this problem, we observe that the conditional distribution of each Ui factorizes with respect to the individual samples. We propose to express U as a linear function of F by using a convex optimization scheme. We then integrate it into the inference algorithm to find out the latent factors. The linear transformation can be expressed as, Ui = FiT P + Q,  i  {1, . . . , N },
L×1 D ×1 D ×D

(2)

and Q could be (trivially) obtained by just setting each entry of P to a very small value and letting a column of Q  Ui (which makes Fi redundant). Secondly, while testing for a new image, we would have to devise a strategy to determine the suitable value for Q. For example, we could take the column of Q that corresponds to the nearest training image. This adds unnecessary complexity and reduces generalization. By making Q a row vector, we consider that it may be possible to arrive to the space of enhancement parameters by linearly transforming the low-quality image features with a constant offset. In other words, we want P to transform the features into a region in the latent space where all the other high-quality images lie and Q provides an offset to avoid over-fitting. This is a joint 2,1 -norm problem which can be solved efficiently by reformulating it as convex. We thus reformulate Equation 3 as follows, inspired by [16]:

where Fi  R , Ui  R ,P  R and Q  R1×D . Note that to carry out this decomposition, we have to set D = L. This is not a severe limitation since L is usually large ( 1000) and as we have mentioned before, increasing D should decrease the approximation error at the cost of increased computation. Henceforth we assume that our feature extraction process generates Fi  RD×1 . Also, note that large L does not mean that the latent space is no longer low-dimensional, because L is still smaller as compared to all the possible combinations of parameters (e.g. 517770 ). We propose an iterative convex optimization process to determine coefficients P and Q of Equation 2. We propose the following objective function to determine them: q
N

min
P,Q

1 

N

||FiT P + Q - UiT ||2 + ||P||2,1 +
i=1

 ||Q||2 (4) 

The
2,1 (X)

2,1 -Norm M

of a matrix X  RM ×N is defined as,

=
i=1

||Xi ||2 . Also, for a row vector Q, we have

||Q||2 = ||Q||2,1 . Thus Equation 4 can be further written as:

min
P,Q

1 T ||F P + 1N Q - UT ||2,1 + ||P||2,1 +  ||Q||2,1 , (5) 

min
P,Q i=1

||FiT P + Q - UiT ||2 +  ||P||2,1 +  ||Q||2

(3)
 where  =  and 1N is a column vector of ones  RN . Now, put FT P + 1N Q -  E = UT . Thus Equation 5 becomes:

The objective function tries to reconstruct U using P, Q and F while controlling the complexity of coefficients. Let's concentrate on the structure of P (by neglecting the effect of Q momentarily). The columns of P act as coefficients for Fi . Ideally, we would want the elements of Ui to be determined by a sparse set of features, which implies sparsity in the columns of P. To this end, we impose 2,1 norm on P, which gives us a block-row structure for P. Let us consider the structure of Q along with P. Equation 2 shows that different columns of Ui depend on different image features Fi . Also, we expect that a different set of columns of P will get activated (take on large values) for different Fi . We add an offset Q  R1×D for regularization. Thus the offset introduced by Q remains constant across all the images but changes for each Fi,j . Making Q to be a row vector also forces P to play a major role in Equation 3. This in turn increases the dependence of Ui on Fi . If we were to define Q as the same size of U (which would mean different offsets for each image as well as its features), it would pose two potential disadvantages. Firstly, optimal P

E,P,Q

min ||E||2,1 + ||P||2,1 +  ||Q||2,1 ,

E,P,Q

min

s.t. FT P + 1N Q -  E = UT ,    (6) E E T - 1 N P s.t. - IN F  1  P  = UT Q 2,1 Q 

Equation 6 is now in the form of: min ||X||2,1 s.t. ZX =
X

B and thus convex. It can be iteratively solved by an efficient algorithm mentioned in [16]. We set  = 0.1 and  = 3. Once we have expressed U as a function of F, we use Gibbs Sampling to determine the latent factors P, Q, V and T [20]. As mentioned before, the predictive distribution ^ k is given by a multidimenfor a new parameter value R ij sional integral as:

Algorithm 1 Gibbs Sampling for Latent Factor Estimation Initialize model parameters {P(1) , Q(1) , V(1) , T(1) }. T Obtain U(1) = FT P(1) + Q(1) For y = 1, 2, . . . , Y · Sample the hyper-parameters according to the derivations 6 : (y)  p((y) |U(y) , V(y) , T(y) , R), (y ) (y ) (y ) (y ) U  p(U |U(y) ), V  p(V |V(y) ), (y ) (y ) (y ) T  p(T |T ) · For i = 1, ..., N , sample the latent features of an image (in parallel): (y +1) (y ) Ui  p(Ui |V(y) , T(y) , U , (y) , R) Determine P(y+1) and Q(y+1) using the iterative T optimization by substituting B = U(y+1) . ^ (y+1) Reconstruct U(y+1) : U
T

sample U, V, T and obtain P and Q. Note that it is required in the algorithm to reconstruct U(y+1) at every iteration since there will always be a small reconstruction ^ (y+1) - U(y+1) ||. The error occurs because we error ||U force Q to be a row vector, which makes the exact recovery of U(y+1) difficult. The reconstructed error causes adjustment of V and T. Once we obtain the four latent factors, our task is to predict the parameter values for M enhanced versions having K parameters each. Suppose Ft is the feature vector of a new image, then the param^ k can be simply obtained by computing, eter values R tj ^ k =< F T P + Q, Vj , Tk >  j  {1, . . . , M } and R t tj k  {1, . . . , K }. If the parameter value predictions lie beyond a certain range then a thresholding scheme can be used based on the prior knowledge. For example, to constrain the predictions between [0, 1], a logistic function may be used.

= FT P(y+1) + Q(y+1)

4. Experiments
We conduct two experiments to show the effectiveness of our approach. We did the first one on a synthetic data and compared it with: 1. BPMF 2. our own discrete version of BPTF, called D-BPTF. 3. multivariate linear regression (MLR) 4. twin Gaussian processes (TGP) [3] 5. Weighted k NN regression (WKNN). For D-BPTF, we make minor modifications in the original BPTF approach [24] by removing the temporal constraints on their temporal variable, since there are no temporal constraints in our case. The inference for their temporal variable is then done in the exactly same manner as the other non-temporal variables. This gave us a marginal boost in the performance. For MLR, We use a standard multivariate regression by maximum likelihood estimation method. Specifically, we use MATLAB's mvregress command. TGP is a generic structured prediction method. It accounts correlation between both input and output resulting in improved performance as compared to MLR or WKNN. The WKNN approach predicts the test sample as a weighted combination of the k -nearest inputs. The first two algorithms do not allow features inclusion. For MLR, TGP and WKNN, we j concatenate Ai and Fi , and use it to predict Ai . Even for our approach, we concatenate Ai and sample feature to form Fi . The intuition behind this concatenation is that the enhancement parameters should be a function of input parameters as well along with the features. We did observe performance boost after concatenating the features and parameters. The second experiment demonstrates the usefulness of this approach in a real-world setting where we have to predict paramters of the enhanced versions of an image (then generate those versions by applying predicted parameters to the input low-quality image) without using any information about the versions. We compare our approach with the

· For j = 1, ..., M , sample the latent features of the enhanced versions (in parallel): Vj
(y +1)

^ (y+1) , T(y) , (y) , (y) , R)  p(Vj |U V

· For k = 1, ..., K , sample the latent features of parameter (in parallel): Tk
(y +1)

^ (y+1) , V(y+1) ,  , (y) , R)  p(Tk |U T

(y )

k ^ ij p(R |R) =

k ^ ij p(R |Ui , Vj , Tk , )·

p(U, V, T, , U , V , T |R)· d(U, V, T, , U , V , T ).

(7)

We resort to numerical approximation techniques to solve the above integral. To sample from the posterior, we use Markov Chain Monte Carlo (MCMC) sampling. We use the Gibbs sampling as our MCMC algorithm. We can approximate the integral by,
Y k ^ ij p(R | R )  y =1 k ^ ij p R |Ui , Vj , Tk , (y) (y ) (y ) (y )

(8) Here we draw Y samples and the value of Y is set by observing the validation error. The sampling from U, V and T is simple since we use conjugate priors for the hyperparameters. Also, a random variable can be sampled in parallel while fixing others which reduces the computational complexity. Algorithm 1 shows how to iteratively
6 See supplementary material on author's website for detailed derivations.

Training RMSE for simulation

Testing RMSE for simulation

0.6 0.5 0.4 0.3 0.2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Proposed_Simulation PMF_Simulation D-BPTF_Simulation

0.6 0.55 0.5 0.45 0.4 0.35 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Proposed_Simulation PMF_Simulation D-BPTF_Simulation

Training RMSE for image enhancement

Testing RMSE for image enhancement

0.18 0.15 0.12 0.09 0.06 0.03 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Proposed_Enhancement PMF_Enhancement D-BPTF_Enhancement

0.18 0.16 0.14 0.12 0.1 0.08 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Proposed_Enhancement PMF_Enhancement D-BPTF_Enhancement

260 195 130 65 0
Image 1 Image 2 Image 3 Image 4 Image 5 Best image

KNN

Proposed Approach

Figure 1. Top plots: train and test RMSEs for both the experiments. Bottom plot: First 5 sets of bars show votes for version 1 to 5 of kNN vs. the best image of our approach. The last set of bars shows votes for the best image of both approaches. Please zoom in for better viewing. See in color.

competing 5 algorithms in addition to k NN-search as it is also used in [26, 5]. We also analyzed the effect of Q in our solution by: removing Q i.e. U = FT P.

linearly related to Ak,i  k,  m  {1, . . . , 4} and Fi . For example, Ak,i,m =  r1 1,i +
A 1 1+e-r2 A2,i
3 + Ar 3,i + (1 -

4.1. Data set description and experiment protocol
The synthetic data is carefully constructed by keeping the following task in mind. We are given a training set consisting of: 1. F  RD×N ; 2. A  RK ×N ; and 3. only parameters of M versions for each input sample A  RK ×N ×M . Our aim is to predict parameters for a set of M versions given a new Fi and Ai . In real-world problems, A and F are interdependent. The parameters of M versions are dependent on both A, F. Hence we construct the synthetic data as follows. Firstly, we generate a set of 3-D input parameters - A drawn from a uniform distribution [0, 1]. Then we generate a 50-D feature set F, where each element of Fi is related to all Ak,i  i = {1, . . . , 103 }, k = {1, 2, 3} by a nonlinA ear function. For example, Fj,i = r1 1,i + 1+e-1 r2 A2,i + 3 Ar ,  j  { 1 , . . . , 50 } and r , r , r are random numbers. 1 2 3 3,i The parameters of enhanced versions, Ak,i,m , are also non-

 ) · ||Fi ||2 . The contribution of Fi is decided by  . We perform 3-fold cross-validation. We predict the values of A in the test set (disjoint from training) using corresponding A and F. RMSE is computed between the predicted and actual A . The MIT-Adobe FiveK data-set contains 5000 highquality photographs taken with SLR cameras. Each photo is then enhanced by five experts to produce 5 enhanced versions. We extract average saturation, brightness and contrast for every image, which are parameters  A. We also extract 1274-D color histogram with 26 bins for hue, 7 bins each for saturation and value. We also calculate localized features of 144-D each for contrast, brightness and saturation. Finally, we append average saturation, brightness and contrast of the input low-quality image, which are its parameters. Thus we get a 1709-D (= 1274 + 3 × 144 + 3) representation for every image  F. We train using 4000 images and use 500 images each for validation and testing. We predict parameters for 5 versions in a 3 × 5 matrix for

Figure 2. Left: Original image, Middle: enhanced image by kNN and Right: proposed approach 7 . See in color.

each image in the testing set. An entry Ai,j denotes the value for ith parameter of j th enhanced version. To enable comparison with the expert-enhanced images of the data-set, we also compute parameters for 5 enhanced versions for each image, which we treat as ground-truth. We evaluate this experiment in two ways. Firstly, we calculate RMSE between the parameters of 5 expert-enhanced photos and the parameters of the predicted versions using five aforementioned algorithms. Secondly, we conduct a subjective test under standard test settings (constant lighting, position, distance from the screen). In this case, we compare our approach with the popular k NN-search-based approach. It first finds the nearest original image in the training set to the testing image - im - and then applies the same parameter transformation to im to generate 5 version. In our approach, we predict the parameters for enhanced versions using the proposed formulation. We threshold the parameter values as: Ak,i,m = min(Ak,i,m , Ak,i + k Ak,i ), Ak,i,m = max(Ak,i,m , Ak,i - k Ak,i ),

(9)

trast are:  = {0.4, 0.4, 0.05},  = {0.3, 0.3, 0.01}. As mentioned before, the clipping scheme in our formulation should be set using prior knowledge. Here, we know that the enhanced images usually have a larger increase (as compared to decrease) associated with their parameters. Also, changing contrast by a very small amount affects the image greatly. The predicted parameters are applied to the input image to obtain its enhanced versions. The procedure is the same for both the approaches and is as follows. First we change contrast till the difference between the updated and the predicted contrast is marginal. We update contrast first since changing it updates both brightness and saturation. We then update brightness and saturation till they come significantly closer to their corresponding predicted values. This gives us 5 versions for both approaches. To allow comparisons within a reasonable amount of time, we use a pre-trained ranking weight vector w (from [5]) to select the best image of our approach (im-proposed) and k NN-approach (imk NN ). For the subjective test, people are told to compare improposed with the 5 enhanced versions of k NN-approach as well as with im-k NN. Thus for every input image, people
7 See supplementary on author's website for additional full-resolution results.

where  and  are multipliers for the k th parameter. In our case, the multipliers for saturation, brightness and con-

perform 6 comparisons. The image order was randomized. We conducted the test with 11 people and 35 input images. Thus every person compared 210 pairs of images. They were told to choose a visually-appealing image. The third option of simultaneously preferring both images was also provided. This option has no effect on cumulative votes.

Table 1. Effect of varying  and 

4.2. Results
The parameters for the synthetic data were more accurately predicted by our approach than BPMF, D-BPTF, MLR, TGP and WKNN. It is worth noting that though the training error continues to decrease for our approach, BPMF and D-BPTF, the testing error starts increasing after only 5 and 8 iterations for BPMF and D-BPTF, respectively. However, testing error in our approach decreases rapidly for 4 iterations and then it decreases very slowly for the next 12, as shown in Fig. 1. The RMSE on test set for BPMF, D-BPTF, MLR, TGP, WKNN and the proposed approach is 0.4933, 0.4865, 0.6293, 0.4947, 0.8014 and 0.3644. The numbers show that our approach is able to effectively use the additional information provided by features and the interaction between A, F and all versions to provide better prediction. On the other hand, BPMF and D-BPTF start over-fitting quickly due to lack of sample feature information while MLR and WKNN fail to model the complex interaction between variables. TGP performs better because of its ability to capture correlations between input and output. However, TGP still treats each version independently and thus its performance still falls short of our approach. In the second experiment, the RMSE for BPMF, D-BPTF, MLR, TGP, WKNN and our approach is 0.1251, 0.1328, 1.2420, 0.1268, 0.1518 and 0.0820 respectively. The testing error starts increasing after only 3 and 5 iterations for BPMF and D-BPTF, respectively. It is important to note that we do not use the clipping scheme mentioned in Equation 9 in order to do a fair comparison of RMSEs between all the five approaches and the proposed appraoch. For the subjective evaluation, Fig. 1 shows cumulative votes obtained for ours and the k NN-based approach for comparison between 5 images chosen by k NN and the best image chosen by our approach. Fig. 1 also shows votes obtained for the best images chosen by both approaches. Fig. 2 shows two input images enhanced by both the approaches. The top row of Fig. 2 shows that k NN reduces the saturation while increasing the brightness. Our approach balances both of them to obtain a more appealing image. In the bottom row, however, both approaches fail to produce aesthetic images as images become too bright. It is probably due to the portion of the sky in the input image. For both the images, most people prefer images enhanced by our approach. Computationally, our approach is superior than k NN. Complexity of our approach is independent of data-set size at testing time whereas k NN searches the

Parameter setting  = 0.001,  = 6  = 0.01,  = 6  = 0.02,  = 0.1  = 0.2,  = 0.05  = 0.8,  = 0.05  = 0.1,  = 0.3  = 0.1,  = 0.8  = 0.1,  = 2

RMSE (lower the better) 0.3162 0.0962 0.0907 0.0930 0.0872 0.0820 0.0821 0.0820

entire data-set for the closet image and then applies its parameters. We reconstructed U = FT P and observed performance drop as it overfits. We get RMSE of 0.9305 and 0.3762 on enhancement and simulation data, respectively. We believe the real-world enhancement data has correlations naturally embedded in it unlike in synthetic data. Thus the performance drop is drastic in case of enhancement since the problem of recovering P only from U and F is ill-posed. We also analyzed the effect of varying  and  . Since our approach uses Bayesian probabilistic inference, small variations in  and  do not significantly affect the performance. Table 1 lists the various parameter settings and its effect on the performance of the second experiment (i.e. image enhancement):

5. Conclusion
In this paper, we introduced a novel problem of predicting parameters of enhanced versions for a low-quality image by using its parameters and features. We developed an MF-inspired approach to solve this problem. We showed that by modeling the interactions across low-quality images, its parameters and its versions, we can outperform five state-of-art models in structured prediction and MF. We proposed inclusion of feature information into our formulation through a convex 2,1 -norm minimization, which works in an iterative fashion and is efficient. Thus our approach utilizes information which helps characterize input image. This leads to better generalization and prediction performance. Since other approaches do not model interdependence between image features and parameters of their corresponding enhanced versions, they start over-fitting quickly and produce an inferior prediction performance on the test set. Experiments on synthetic and real data demonstrated superiority of our approach over other state-of-art methods. Acknowledgement: The work was supported in part by an ARO grant (#W911NF1410371) and an ONR grant (#N00014-15-1-2344). Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of ARO or ONR.

References
[1] L. Baltrunas, B. Ludwig, and F. Ricci. Matrix factorization techniques for context aware recommendation. In Proceedings of the fifth ACM conference on Recommender systems, pages 301­304. ACM, 2011. [2] F. Berthouzoz, W. Li, M. Dontcheva, and M. Agrawala. A framework for content-adaptive photo manipulation macros: Application to face, landscape, and global manipulations. ACM Trans. Graph., 30(5):120, 2011. [3] L. Bo and C. Sminchisescu. Twin gaussian processes for structured prediction. International Journal of Computer Vision, 87(1-2):28­52, 2010. [4] V. Bychkovsky, S. Paris, E. Chan, and F. Durand. Learning photographic global tonal adjustment with a database of input/output image pairs. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 97­ 104. IEEE, 2011. [5] P. S. Chandakkar, Q. Tian, and B. Li. Relative learning from web images for content-adaptive enhancement. In Multimedia and Expo (ICME), 2015 IEEE International Conference on, pages 1­6. IEEE, 2015. [6] C.-Y. Chen and K. Grauman. Inferring unseen views of people. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2011­2018. IEEE, 2014. [7] L. Chen, Q. Zhang, and B. Li. Predicting multiple attributes via relative multi-task learning. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 1027­1034. IEEE, 2014. [8] S. B. Kang, A. Kapoor, and D. Lischinski. Personalization of image enhancement. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 1799­1806. IEEE, 2010. [9] L. Kaufman, D. Lischinski, and M. Werman. Content-aware automatic photo enhancement. In Computer Graphics Forum, volume 31, pages 2528­2540. Wiley Online Library, 2012. [10] N. D. Lawrence and R. Urtasun. Non-linear matrix factorization with gaussian processes. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 601­608. ACM, 2009. [11] S. Li, S. Shan, and X. Chen. Relative forest for attribute prediction. In Computer Vision­ACCV 2012, pages 316­327. Springer, 2013. [12] H. Ma, H. Yang, M. R. Lyu, and I. King. Sorec: social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 931­940. ACM, 2008. [13] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King. Recommender systems with social regularization. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 287­296. ACM, 2011. [14] B. Marlin, R. S. Zemel, S. Roweis, and M. Slaney. Collaborative filtering and the missing at random assumption. arXiv preprint arXiv:1206.5267, 2012. [15] A. Mnih and R. Salakhutdinov. Probabilistic matrix factorization. In Advances in neural information processing systems, pages 1257­1264, 2007.

[16] F. Nie, H. Huang, X. Cai, and C. H. Ding. Efficient and robust feature selection via joint 2,1 -norms minimization. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1813­1821. Curran Associates, Inc., 2010. [17] D. Parikh and K. Grauman. Relative attributes. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 503­510. IEEE, 2011. [18] D. Parikh, A. Kovashka, A. Parkash, and K. Grauman. Relative attributes for enhanced human-machine communication. In AAAI, 2012. [19] J. D. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pages 713­719. ACM, 2005. [20] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo. In Proceedings of the 25th international conference on Machine learning, pages 880­887. ACM, 2008. [21] Y. Shi, M. Larson, and A. Hanjalic. Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges. ACM Computing Surveys (CSUR), 47(1):3, 2014. [22] Q. Song, J. Cheng, and H. Lu. Incremental matrix factorization via feature space re-learning for recommender system. In Proceedings of the 9th ACM Conference on Recommender Systems, pages 277­280. ACM, 2015. [23] S. Wang, J. Tang, Y. Wang, and H. Liu. Exploring implicit hierarchical structures for recommender systems. In International Joint Conference on Artificial Intelligence (IJCAI). IJCAI, 2015. [24] L. Xiong, X. Chen, T.-K. Huang, J. G. Schneider, and J. G. Carbonell. Temporal collaborative filtering with bayesian probabilistic tensor factorization. In SDM, volume 10, pages 211­222. SIAM, 2010. [25] J. Yan, S. Lin, S. B. Kang, and X. Tang. A learning-to-rank approach for image color enhancement. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2987­2994. IEEE, 2014. [26] J. Yan, S. Lin, S. B. Kang, and X. Tang. A learning-to-rank approach for image color enhancement. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2987­2994. IEEE, 2014.

Efficient Unsupervised Abnormal Crowd Activity Detection Based on a Spatiotemporal Saliency Detector
Yilin Wang Arizona State University Tempe, Arizona
ywang370@asu.edu

Qiang Zhang Samsung Pasadena, California
zhangtemplar@gmail.com

Baoxin Li Arizona State University Tempe, Arizona
baoxin.li@asu.edu

Abstract
Approaches to abnormality detection in crowded scene largely rely on supervised methods using discriminative models. In this paper, we presents a novel and efficient unsupervised learning method for video analysis. We start from visual saliency, which has been used in several vision tasks, e.g., image classification, object detection, and foreground segmentation. To detect saliency regions in video sequences, we propose a new approach for detecting spatiotemporal visual saliency based on the phase spectrum of the videos, which is easy to implement and computationally efficient. With the proposed algorithm, we also study how the spatiotemporal saliency can be used in two important vision tasks, saliency prediction and abnormality detection. The proposed algorithm is evaluated on several benchmark datasets with comparison to the state-of-the-art methods from the literature. The experiments demonstrate the effectiveness of the proposed approach to spatiotemporal visual saliency detection and its application to the above vision tasks.

1. Introduction
Automatic abnormality detection for online multimedia content has been an active area in recent years due to it potential applications for crowded surveillance[30], social media behavior monitoring[35, 37, 34]and event retrieval[7]. Early approaches [30, 8, 24] focus on either generating discriminative model for semantic indexing the video or decompose it into semantic parts. These approaches, which rely on frame-based video labels, have been shown effective on certain datasets. Unfortunately, frame-based labels are in general hard to obtain. Especially, for massive YouTube videos, it is too labor-and time-intensive to obtain labeled sets large enough for robust training. Thus, the unsupervised approach would be more desirable. This paper studies unsupervised video abnormality detection. 1

Typically, video features such as optical flow, motion trajectory, and spatialtemporal interest points, lack of semantic meanings required by the abnormality detection. In the supervised case, label information could be directly utilized to build the connection between video features and video labels. Thus, unsupervised video abnormality detection is inherently more challenging than its supervised counterpart. In this paper, we start from visual saliency, which has attracted a lot of interests in the vision community in recent years. One early work that is widely known is the approach by Itti et al. [19]. Since then, a lot of different models have been proposed for computing visual saliency. Moreover, visual saliency often depends on not only a static scene but also the changes in the scene. To this end, spatiotemporal saliency has been proposed, which tries to capture regions attracting visual attention in the spatiotemporal domain. Spatiotemporal saliency has been applied to vision tasks such as video summarization, human-computer interaction [18], and video compression. However, these approaches only focus on the video objects or foreground, but ignore irregular motion pattern changes, which is an essential part in abnormality event detection. On the other hand, the saliency information can be regarded as an abstract of the video frame (image) [32], which may be exploited to enable unsupervised abnormality detection. How to achieve this is the objective of our approach. In this paper, we study unsupervised video abnormality detection based on a spatiotemporal saliency detector by investigating two related challenges: (1) how to model the interaction between video content and spatiotemporal saliency systematically so as to augment video analysis using the information from saliency detection, and (2) how to use spatiotemporal saliency information to enable unsupervised video analysis. In addressing these two challenges, we propose a novel spatiotemporal visual saliency detector for video content analysis, based on the phase information of the video. With the saliency map computed using the proposed method, we analyze how it can be used for two fundamental vision tasks, namely saliency detec-

tion and abnormal event detection. We evaluate the performance of the proposed algorithm using several widely used datasets, with the comparison to the state-of-the-art in the literature. Our main contribution can be summarized as following: (1) A parameter free approach to enabling unsupervised video event detection. Neither normal examples nor abnormal examples are required for abnormality detection; (2) A novel and efficient framework for spatiotemporal saliency detection, which captures the global motion information and can be used to model complex activities. We demonstrate the complexity of the proposed algorithm is only O(N log N ), where N is the size of the input; and (3) Comprehensive comparisons and evaluations using several benchmark datasets on saliency detection and abnormality detection are used to demonstrate that the effectiveness of the proposed approach, suggesting its potential application for future video analysis tasks.

Figure 1. The spectrum analysis for normal videos. The first column is sample frame, the second column shows sampled video points in frequency domain. Please see the figures in color print.

2. The Proposed Method
2.1. Spectrum Analysis for Saliency Detection
There has been several explanations for why spectral domain based approach is able to detect saliency region from the image. For example, In [3], it has been shown that human visual system will select a subset of objects to focus. In other words, an attention competition exists among objects in the image. Only a small portion of objects, which are more distinctive, will be popped out, and rest of objects, which are usually in a uniform or common patterns, are suppressed. The spectral magnitude measures the total response of cells tuned to the specific frequency and orientation. According to lateral surround inhibition, similarly tuned cells will be suppressed depending on their total response, which can be modeled by dividing its spectral by the spectral magnitude [36]. [13] provided another explanation from sparse representation, which states that, if the foreground is sparse in spatial domain and background is sparse in DCT domain (e.g., periodic textures), the spectral domain based approach will highlight the foreground region in the saliency map. In a word, given an image (or 2D signal), f (x, y ), the saliency map can be calculated as: S (x, y ) = F -1 [h(m, n)  A(m, n) · e-iP (m,n) ] (1)

where h is a high pass filter and A, P represent amplitude and phase of Fourier transform F .

get object can be easily identified by human subjects. In this paper, we model the video abnormality detection as a spatiotemporal saliency detection problem, where normal video frame is regarded as non-salient and abnormality is perceptually salient. In [23, 13], it has been shown that, for natural image, the amplitude spectrum of background (non-salient region) has higher value at lower frequency. Essentially, our observation demonstrates that, in the temporal domains, the normal video frames, where object can be modeled in a uniform motion pattern , will have higher amplitude in lower frequencies than higher frequencies. Thus, the phase information of temporal domain can be used for abnormality modeling. We show that one can exploit such informative observation through spectrum analysis. In order to demonstrate the property of spectrum of normal videos, we generate two synthetic videos. The first video contains a uniform background (black) and second video with a moving object (33 by 33) which has its value uniformly distributed (white). Moreover, the motion trajectory of the object is followed as the red circle with same speed (we call it regular motion) in Figure 1. Specifically, two points, which are sampled from background and motion trajectory respectively, are further plotted in the frequency domain through the time period. From Figure 1, we can interpret the result as following: 1) if no global motion in the video, the background (even with dynamic scenes, we show later) has higher response in the lower frequency domain. 2)Since the result of the spectrum obeys the symmetry [2], the amplitude from the points within regular motion object also trend to be higher in lower frequencies.

2.2. Spectrum Analysis for Normal Videos
Spectrum analysis in spatiotemporal data, e.g., videos, is still in its infancy. In [16], the author studied how motion patterns contribute to saliency. It demonstrate that by setting the target object having different flicker rate, moving direction or motion velocity from other objects, the tar-

2.3. Modeling Video Abnormality via Saliency Detection
Based on the spectrum analysis of normal video, we have observed some potential properties. Then two research questions arises: 1) How to model the video abnormality only using the information from amplitude spectrum? 2)

How to automatically find the abnormality in a video? Answering these questions leads us to further analyze the amplitude spectrum with phase information. Given a signal f (x, y, t) it is first transformed to the frequency domain F f (x, y, t) -  F (m, n, k ), with the amplitude A(i, j, k ) = |F (m, n, k )| and phase P (m, n, k ) = angle(F (m, n, k ). Based on the Fourier Transform and inverse Fourier Transform, we have: f (x, y, t) = F -1 [F (m, n, k )] = F -1 A(m, n, k ) · ei·P (m,n,k)

2.4. Analysis
In this section, we provide evidence that, for a foreground object with irregular motion pattern, the proposed spatiotemporal saliency detector can approximately obtain its location in a video based on Parseval's theorem [2]. Parseval's theorem: The energy in u(t) equals the en+ ergy in U (f ), where u(t) = f =- U (f ) · ei2f t df Now, given a 3D signal and reconstruct it with only phase information: F (m, n, k ) ] |F (m, n, k )|

(2) S (x, y, t) = g  F -1 [ =gF
-1

Based on Parseval's theorem, we know the summation across all the dimensions of f (x, y, t) is equal to the summation across all the frequency component in the frequency domain. From the Eq 6, we can see that when only using S (x, y, t) = g  F -1 [h(k )  A(m, n, k ) · ei·P (m,n,k) ] (3) the phase information it is equal to replace the amplitude spectrum A(t) to a cube. In other words, all of the elements where h(k ) is the high-pass filter along the temporal diwhich have non-zero value in magnitude spectrum are set to rection in the frequency domain, and g is a low pass filter one. The region with repeat (regular) motion pattern creates in spatiotemporal domain, e.g., 3D Gaussian filter, which a high peak in the magnitude spectrum (Figure 1) is supsmooths the result. However, Eq 3 only considers the tempressed; while the region with salient (irregular motion patporal information for video abnormality detection, which tern instead corresponds to the spread-out magnitude specmay involve the noise from background if the video contrum will pop-out. Additionally, based on the proposition in tains global motion. To alleviate this issue, we further in[13], we can easily extend the sparse condition for saliency corporate the spatial saliency information to refine the dedetection in the spatial domain to the spatiotemporal dotection results. The improved model is described as below: main, which means the proposed method is also bounded with the ratio of salient region and non salient region. Due -1 i·P (m,n,k) to the space limits, we omit the proof. S (x, y, t) =g  F [h(k )  l(m, n)  (A(m, n, k ) · e )] To verify the correctness of the proposed model, we gen(4) erate one synthetic video to test the abnormality detection. where l(m, n) is the high pass filter along the spatial direcThe video contains a dynamic background with two movtion in the frequency domain. In frequency domain, setting ing squares with same texture. One of squares follows the the spectrum magnitude to uniform can achieve similar efred circle and moves steadily (we call normal object), anfect of high pass filter. In order to reduce the computation other moving square moving randomly (we call abnormal cost, we further relax Eq 4 (further analysis shown in Sec object). The motion trajectory of these square is defined as 2.4 ): following: S (x, y, t) = g  F -1 [ei·P (m,n,t) ] (5) F (m, n, k ) x(t) 128 + 64cos( t = g  F -1 ( ) 32 ) 1 (t) = = t |F (m, n, k )| y (t) 128 + 64sin( 32 ) t Eq 5 actually adopts the phase information of a video for x(t) 64 + 32cos( 32 ) +  2 (t) = = t saliency detection, it can be easily paralleled. The Fourier y (t) )+ 64 + 32sin( 32 transform for multiple dimensional data can be computed as a sequence of 1D Fourier transform on each coordinate of the data. Thus the computation cost of the proposed where  is a random variable uniformly draw from [0, 128]. spatiotemporal saliency detector is O(M N T log (M N T )) We view the object with trajectory 1 moving regularly. For when the input data size is X  RM ×N ××T . If the the Gauss filter used to smooth  the saliency map, we set the data has P feature channel, then the computational cost is standard deviation as 0.006 N 2 + M 2 and te filter size as 1 + 6 , where N × M is the size of each frame. O(P M N T log (M N T )).

In order to extract the video abnormality, and inspired by the saliency detection, we perform a high pass filtering on the frequency domain in temporal dimension, which will suppress the signals from normal videos. Then we model the abnormality in a saliency fashion:

(6) ]

[1(m, n, k ) · e

i·P (m,n,k)

In Fig. 2, we show some sample frames of the video (top), the results from the proposed method (middle) with the comparison to the results of the method proposed in [11] (bottom), where the frame differences of two adjacent frames are used as temporal information. From the figure, we can find the proposed method highlight the moving objects over the dynamic background; in addition, the object moving "irregularly" (i.e., with trajectory 2 ) gets higher values in the saliency map than the other object (the one with trajectory 1 ) does. In contrast, the method proposed in [11] not only has problem in segmenting the moving objects from changing background, but also can't discriminate the one moving "irregularly" from the one moving regularly. One explanation could be that simply the frame differences of two adjacent frames can't distinguish the object moving irregularly from the object moving reguarly. This reveals the potential of the proposed method to detect irregular events from the video (or abnormal events), as detailed in the next section.

abnormality detection. The performance of the proposed methods are compared with the existing methods, some of which are state-of-the-art methods.

3.1. Simulation Experiment
In this section, we evaluate the proposed method on synthetic data. In [16], how three properties of motion, namely flicker, direction and velocity, contribute to the saliency was studied. In this section, we generate the synthetic data according to the their protocol. The input data is a short clip where the resolution is 174 × 174 with 400 frames at the frame rate of 60 frames per second. We put 36 objects of size 5 × 13 in a 6 × 6 grid and a target object is randomly selected out of those 36 objects. All the objects are allowed to move within a 29 × 29 region centered at their initial position (and warped back, if they move out of this region). The video is black-and-white. We design the following three experiments: 1. Flicker: we set the objects on-off at a specified rate and the target object at a different rate from the other 35 objects; 2. Direction: we set the objects moving in a specified direction and the target object in a different direction. The velocity of all the objects are the same; 3. Velocity: we set the objects moving in a specified velocity and the target object moves in a different velocity. The moving direction of the all the objects are the same. All the other parameters are the same as used in [16]. According to [16], the target object could be easily identified by human subjects, when its motion property (e.g., flicker rate, moving direction, velocity) is different from the other objects. We also include some "blind" trials, where the target object has the same motion property as the other 35 objects. In this case, the target object can't be identified by the human subjects, i.e., there is no salient region. We apply the proposed method to the input data. For comparison, we also evaluate the method proposed in [4] and [13]. We use the area under receiver operating characteristic curve as the performance metric. The ground truth mask is generated according to the location of the target object. The experiment result is shown in Figure 3. From the experiment results, we can find that the proposed method detects the salient region much more accurately than [4] and [13] in all except the "blind" trials, which should be as lower as possible in terms of abnormality. However, [4] and [13] don't survive in those "blind" trials. Surprisingly, [4] and [13] achieves quite similar performances, though [4] was supposed to achieve better result as it include the differences of two adjacent frames as motion (temporal) information.

Figure 2. Top: some sample frames from the input video, where the red circle indicates the trajectory 1 ; middle: the corresponding saliency maps; and bottom: the saliency map computed with method described in [11]. For the saliency map, the warm color indicates high value and cold color for low value. The video can be found in the supplementary material. Please see the figures in color print.

3. Experiment
Since the proposed method is based on saliency detection, to verify the correctness in saliency detection, we first evaluate the proposed method on both syntheic data (Sec. 3.1) and two real-world video datasets (Sec. 3.2), CRCNSORIG and DIEM, for saliency detection. Then we evaluate the proposed method on several benchmark datasets on

Flicker
0.8 0.75 0.7 0.65 0.6 0.55 0.5 0.45 0.4 100 150 200 250 300 350 400 450 Absolute differences of the flicker rate 0.2 500 -10 0 0.6 0.7

Direction
0.7 0.65 0.6 0.55 0.5 0.4

Velocity

Proposed Bian[15] Hou[8]

0.5

0.3

Proposed Bian[15] Hou[8]
10 20 30 40 Absolute differences of the direction 50

0.45 0.4 0.35 60 -0.1 0

Proposed Bian[15] Hou[8]
0.1 0.2 0.3 0.4 Absolute differences of the velocity 0.5 0.6

Figure 3. The AUC on the synthetic data for the proposed method and two existing methods. For "Direction" and "Velocity", we also include some "blind" trials (X-axis has value 0), where the target object has exactly the same motion property as the other 35 objects. In those trials, the target object can't be identified by human subjects, i.e., there is no salient object [16].

Blind

Flicker

Direction

Velocity

Figure 4. Some visual sample of the synthetic data for different experiments.

3.2. Spatiotemporal Saliency Detection
In previous section, we test the proposed spatiotemporal saliency detector on synthetic videos, with the comparison to two other saliency detectors, where the proposed detector shows better performances in capturing the temporal information. In this section, we evaluate the proposed spatiotemporal saliency detector on two challenging video datasets for saliency evaluation, CRCNS-ORIG [20] and DIEM [29]. For this experiment, we first convert each frame into the LAB color space, then compute the spatiotemporal saliency in each channel independently and the final spatiotemporal saliency is the summation of the saliency maps of all three channels. CRCNS-ORIG includes 50 video clips from different genres, including TV programs, outdoor scenes and video games. Each clip is 6-second to 90-second long at 30 frames per second. The eye fixation data is captured from eight subjects with normal or correct-normal vision. In our experiment, we downsample the video from 640 × 480 to 160 × 120 and keep the frame rate untouched, then apply the our spatiotemporal saliency detector. To measure the performance, we compute the area under curve (AUC) and Fmeasure (harmonic mean of true positive rate and false positive rate). The experiment result is shown in Fig. 5, where the area under curve (AUC) is 0.6639 and F-measure is

0.1926. Tab. 1 compares the result of the proposed method with some state-of-art methods on CRCNS-ORIG, which indicates that our method outperforms them by at least 0.06 regarding AUC.
1 0.9

True Postive Rate

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.2 0.4

CRCNS AUC=0.6729 DIEM AUC=0.6715
0.6 0.8 1

False Postive Rate
Figure 5. The receiver operating characteristic curve of the propose method in CRCNS-ORIG dataset and DIEM dataset. The area under the curve is 0.6639 and 0.6896 accordingly.

DIEM dataset collects data of where people look during dynamic scene viewing such as film trailers, music videos, or advertisements. It currently consists of data from over

Method AWS [10] HouNIPS [15] Bian [4] IO SR [14] Torralba [33] Judd [21] Marat [27] Rarity-G [26] CIOFM [17] Proposed

AUC 0.6000 0.5967 0.5950 0.5950 0.5867 0.5833 0.5833 0.5833 0.5767 0.5767 0.6639

Method AWS [10] Bian [4] Marat [27] Judd [21] AIM [6] HouNIPS [15] Torralba [33] GBVS [12] SR [14] CIO [17] Proposed

AUC 0.5770 0.5730 0.5730 0.5700 0.5680 0.5630 0.5840 0.5620 0.5610 0.5560 0.6896

Method Optical flow [28] Social force [28] NN [9] Sparse reconstruction [9] Proposed

AUC 0.84 0.96 0.93 0.978 0.9378

Table 2. The result on UMN dataset. Note, we have cropped out the region which contains the text "abnormal", and results in frame resolution 214 × 320. Please note that, most of those methods, except the proposed one, need a training stage.

3.3. Abnormality Detection
In this section, we show how can we utilize the proposed spatiotemporal saliency detector to detect abnormality from the video.

Table 1. The result the proposed method compared with the results of the top ten existing methods on CRCNS dataset (left) and DIEM dataset (right) according to [5]. From this table, we can find that the propose method gets obvious better performances than the state-of-arts on both two datasets.

250 participants watching 85 different videos. Each video in DIEM dataset includes 1000 to 6000 frames at 30 frames per second. Similarly as CRCNS, we downsample the video to 1/4 (e.g., from 1280 × 720 to 320 × 180) while maintaining the aspect ratio and frame rate. We observe that each video in DIEM dataset is consisted of a sequence of short clips, where each clip has 30 to 100 frames. To properly detect the saliency from those videos, we apply the window function to our spatiotemporal saliency detector, where the size of the window (along temporal direction) is 60-frame. The experiment result is shown in Fig. 5 and Tab. 1, where the AUC is 0.6896 and F-measure is 0.35. From the table, we can find that the proposed method outperforms the stateof-arts by over 10%. To compare the performances of combining four visual cues via QFT and performances via summation of saliency maps of each visual cues, we design the following experiment. We run 1000 simulations and in each simulation we generate a r × c × 4 array, where r and c is a random number between [1, 1000] and 4 is the number of feature channels. We compute the saliency map with different methods then measures their similarities via cross-correlation, where 0.91 is reported for QFT and FFT. After smoothing the saliency map with a Gaussian kernel, the correlation is over 0.998. For natural image, we could expect an even higher correlation. This suggests that, we can compute the saliency map for each visual cue independently and then add them together, which will yield quite similar result by using quaternion Fourier transform. In addition, the proposed method other than QFT provides more flexibility, e.g., we can assign different weights to the visual cues as [21]. We also include the AUC of the proposed method for each video from the CRCNS-ORIG (Figure 6) and DIEM dataset (Figure 7).

Method Social force [28] MPPCA [22] MDT [25] Adam [1] Reddy [31] Sparse [9] Proposed

Ped1 31% 40% 25% 38% 22.5% 19% 27%

Ped2 42% 30% 25% 42% 20% N.A. 19%

Overall 37% 35% 25% 40% 21.25% N.A. 23%

Table 3. The frame level EER (the lower the better) for UCSD dataset. Please note that, most of those methods, except the proposed one, need a training stage. From the result, we can found that the proposed method, even without traing stage or training data, can still outperform social force, MPPCA.

For abnormality detection, we start with computing the saliency map for the input video as described above. The regions containing abnormalities can be detected by founding the region where the saliency value is above a threshold. Then the saliency score of a frame is computed as the average of saliency value of the pixels in that frame, i.e., s(t) = 1 NM X(i, j, t)
i j

(7)

where s(t) is the saliency score of tth frame, N × M is the size of one frame, i, j , t are row, column and frame index of the 3D saliency map accordingly. The frame with high saliency score would contain abnormality. To show the proposed method is not sensitive to the value of threshold, we choose the average value of the saliency in the video as threshold. We evaluate the proposed method for abnormality detection in videos from two datasets: UMN abnormal dataset1 and UCSD dataset [25]. Abnormal detection has attracted
1 http://mha.cs.umn.edu/Movies/Crowd-Activity-All.avi

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0

a lot efforts from the researchers. However, most of the existing works require training stage, e.g., social force [28], sparse reconstruction [9], MPPCA [22], i.e., they need training data to initialize the model. The proposed method, instead, does NOT need any training stage or training data. The result on UMN abnormal dataset is shown in Tab. 2, where we compute the frame-level true positive rate and false positive rate then compute the area under the ROC (Fig. 9). Fig. 10 shows the result for videos of three scenes, where we plot saliency value of each frame and show some sample frames. The result on UCSD dataset is shown in Tab. 3, where we report frame-level equal-error rate (EER) [25]. Fig. 11 shows the ROC for UCSD dataset with the proposed method; Fig. 8 shows eight samples frames, where red color highlights abnormal regions. We can find that, without training data, the proposed method

_p An eop le BBtarct _br C_ ica ook B w _l ly ad BC_ ildlif and n_1 ve w e_ sca r il e a t_b dlif ag pe am dve bc4 e_sp le_9 i rt_ _b e am _ib4 bra ees cial i_ 01 vi _ arc is10 0_c a_pa102 tic 00 los int ch _bea a_cl eup__ do illi_ rs ose 7 _ c do um plas 106 up_ c e t 6 do ume ntar ers_ x71 cu n y_ 12 ga men tary_ cora 80 m ta d l_ ga e_tr ry_p olph re ha me_ ailer lan ins ho iry_ tra _gh et_e me bi iler os k hu _m ers _wr tbu mm ovi _c ath e a _ mo ingb _Ch bba l v mo ie_ irds arliege _ _ v mu ie_t traile narr b ne sic rai r_a ato w _ ler li ne s_bered_ _qu ce_ a n ws e_ hot nt ne ews _she para _chi u ws _u rry sit li_ _w s_ _d es nig imb elec rink _7 pin htli ledo tion ing fe _ pingpon _in_n_m deb g g m pin po _a o acen gp ng_ ngl zam e o pla ng_ long _sh bi ne no _sh ot_ sc t_e _bo ot sp ottis arth die _9 or h_ _ s_ s t_b st jun 9 sp port arc arter gles o _ e sp rt_p foo lonas_12 ort ok tba _e sp _sla er_ ll_b xtr 1 sp ort_ m_d 280 est_ o w sp rt_w imb unk_ x640 or im le 1 ste ts_k ble don 280 wa en do _ba r do n_ l tv_ t_lee _12 ma t tv_ ket _12 80x gi un ch2 80 71 i_c _6 x7 ha 72 12 lle x5 ng 44 e_ fin

50

be beverl beve y01 beverrly03 beverlly05 ve y0 ga bev r 6 game erlly07 gamecub y08 gamecube02 gamecube04 gamecube05 gamecube06 gamecube13 cu e1 gamec be 6 me ub 17 mocube18 monice23 nica03 mo a0 nic samon a04 c i c c sta a a05 ete 6 standd standard0st a stand rd01 ndard 2 sta 03 standard0 a n r s a da d04 nd rd05 tvt- a 6 ac tv-tiord07 0 tv-adn s1 t tv- v-ads01 an tv-ads02 ouads03 tvn -m n 0 tv- usce04 tv-ne ic01 ws 1 tv-new 0 tv-ne s01 tv-news02 ws 3 tv-new 04 ne s0 tv- tv- news 5 tv-sp ws06 sports 09 tv- ort 01 tv-spo s0 2 tv-sp rts0 sports 3 o 0 tv- rts 4 ta 05 tv- tv-tallk01 k0 tv-talk 3 tal 04 k0 5
Figure 6. The AUC of the proposed method for each video from CRCNS-ORIG dataset.
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0

Figure 7. The AUC of the proposed method for each video from DIEM dataset.

still outperforms several state-of-arts in the literature, e.g., social force, MPPCA.

4. Conclusion and Discussion
In this paper, we proposed a novel approach for detecting spatiotemporal saliency, which was simple to implement and computationally efficient. The proposed approach was inspired by recent development of spectrum analysis based visual saliency approaches, where phase information was used for constructing the saliency map of the image. Recognizing that the computed saliency map captured the region of human's attention for dynamic scenes, we proposed two algorithms utilizing this saliency map for two important vision tasks. These approaches were evaluated on several well-known datasets with comparisons to the state-of-arts in the literature, where good results were demonstrated.

Peds1: Wheelchair

Peds1: Skater Scene 1

Peds1: Bike

Peds1: Cart

Scene 2 Peds2: Skater Peds2: Bike

Figure 8. Some sample results for the UCSD datasets, where the red color highlights the detected abnormal region, i.e., the saliency value of the pixel is higher than four times of the mean saliency value of the video. Please see the figures in color print.

Area under cuver: 0.937785
1

True Positive Rate

0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.2 0.4 0.6 0.8 1
0.9
X: 0.1978 Y: 0.8177

Scene 3
Figure 10. Some sample results for the UMN datasets, where we pick one video for each scene. The top is the saliency value (Yaxis) for each frame (X-axis) and bottom are sample frames picked from different frames (as shown by the arrow). Area under curve
1

False Positive Rate
Figure 9. The ROC for the UMN dataset computed with the propose method.

True Positive Rate

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
X: 0.2649 Y: 0.7456

X: 0.2792 Y: 0.7219

5. Acknowledgment
The work was supported in part by an ARO grant (#W911NF1410371) and an ONR grant (# N00014-15-12722). Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of ARO or ONR.

all: 0.8062 ped1: 0.7805 ped2: 0.8772
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

False Positive Rate
Figure 11. The ROC for the UCSD dataset computed with the propose method. [2] A. Antoniou. Digital signal processing. Toronto, Canada:, 2006. 2, 3 McGraw-Hill

References
[1] A. Adam, E. Rivlin, I. Shimshoni, and D. Reinitz. Robust real-time unusual event detection using multiple fixedlocation monitors. PAMI, 30(3):555 ­560, march 2008. 6

[3] D. M. Beck and S. Kastner. Stimulus context modulates com-

[4]

[5]

[6] [7]

[8]

[9]

[10]

[11]

[12] [13]

[14] [15] [16]

[17] [18]

[19] [20] [21]

[22]

petition in human extrastriate cortex. Nature neuroscience, 2005. 2 P. Bian and L. Zhang. Biological plausibility of spectral domain approach for spatiotemporal visual saliency. NIPS, pages 251­258, 2009. 4, 6 A. Borji, D. N. Sihite, and L. Itti. Quantitative analysis of human-model agreement in visual saliency modeling: a comparative study. 2012. 6 N. Bruce and J. Tsotsos. Saliency based on information maximization. In NIPS, pages 155­162, 2005. 6 L. Chen, Q. Zhang, P. Zhang, and B. Li. Instructive video retrieval for surgical skill coaching using attribute learning. In Multimedia and Expo (ICME), 2015 IEEE International Conference on, pages 1­6, June 2015. 1 X. Chen and K. S. Candan. GI-NMF: group incremental nonnegative matrix factorization on data streams. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014, Shanghai, China, November 3-7, 2014, pages 1119­1128, 2014. 1 Y. Cong, J. Yuan, and J. Liu. Sparse reconstruction cost for abnormal event detection. In CVPR 2011, pages 3449 ­3456, june 2011. 6, 7 A. Garcia-Diaz, X. R. Fdez-Vidal, X. M. Pardo, and R. Dosil. Decorrelation and distinctiveness provide with human-like saliency. In NIPS, pages 343­354. Springer, 2009. 6 C. Guo, Q. Ma, and L. Zhang. Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform. In CVPR, 2008. 4 J. Harel, C. Koch, and P. Perona. Graph-based visual saliency. In NIPS, pages 545­552, 2006. 6 X. Hou, J. Harel, and C. Koch. Image signature: Highlighting sparse salient regions. PAMI, pages 194­201, 2012. 2, 3, 4 X. Hou and L. Zhang. Saliency detection: A spectral residual approach. In CVPR, pages 1 ­8, 2007. 6 X. Hou and L. Zhang. Dynamic visual attention: Searching for coding length increments. NIPS, 21:681­688, 2008. 6 D. E. Huber and C. G. Healey. Visualizing data with motion. In Visualization, 2005. VIS 05. IEEE, pages 527­534. IEEE, 2005. 2, 4, 5 L. Itti and P. Baldi. Bayesian surprise attracts human attention. NIPS, 18:547, 2006. 6 L. Itti, N. Dhavale, and F. Pighin. Realistic avatar eye and head animation using a neurobiological model of visual attention. In Optical Science and Technology, pages 64­78. International Society for Optics and Photonics, 2004. 1 L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. PAMI, nov 1998. 1 R. Itti, Laurent; Carmi. Eye-tracking data from human volunteers watching complex video stimuli. Online, 2009. 5 T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to predict where humans look. In ICCV 2009, pages 2106 ­2113, 29 2009-oct. 2 2009. 6 J. Kim and K. Grauman. Observe locally, infer globally: A space-time mrf for detecting abnormal activities with incremental updates. In CVPR 2009, pages 2921 ­2928, june 2009. 6, 7

[23] J. Li, M. D. Levine, X. An, X. Xu, and H. He. Visual saliency based on scale-space analysis in the frequency domain. PAMI, pages 996­1010, 2013. 2 [24] X. Li, S. Huang, K. S. Candan, and M. L. Sapino. Focusing decomposition accuracy by personalizing tensor decomposition (PTD). In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014, Shanghai, China, November 3-7, 2014, pages 689­698, 2014. 1 [25] V. Mahadevan, W. Li, V. Bhalodia, and N. Vasconcelos. Anomaly detection in crowded scenes. In CVPR, pages 1975 ­1981, june 2010. 6, 7 [26] M. Mancas. Computational attention: Modelisation and application to audio and image processing. PhD thesis, PhD. Thesis, University of Mons, 2007. 6 [27] S. Marat, T. Ho Phuoc, L. Granjon, N. Guyader, D. Pellerin, and A. Gu´ erin-Dugu´ e. Modelling spatio-temporal saliency to predict gaze direction for short videos. IJCV, 82(3):231­ 243, 2009. 6 [28] R. Mehran, A. Oyama, and M. Shah. Abnormal crowd behavior detection using social force model. In CVPR 2009, pages 935 ­942, june 2009. 6, 7 [29] P. K. Mital, T. J. Smith, R. L. Hill, and J. M. Henderson. Clustering of gaze during dynamic scene viewing is predicted by motion. Cognitive Computation, 3(1):5­24, 2011. 5 [30] R. Raghavendra, A. Del Bue, M. Cristani, and V. Murino. Optimizing interaction force for global anomaly detection in crowded scenes. In Computer Vision Workshops (ICCV Workshops), pages 136 ­143, nov. 2011. 1 [31] V. Reddy, C. Sanderson, and B. Lovell. Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed, size and texture. In CVPRW, pages 55 ­61, june 2011. 6 [32] Q. Tian and B. Li. Simultaneous semantic segmentation of a set of partially labeled images. In IEEE Winter Conference on Applications of Computer Vision, 2016. 1 [33] A. Torralba. Modeling global scene factors in attention. JOSA A, 20(7):1407­1418, 2003. 6 [34] Y. Wang, Y. Hu, S. Kambhampati, and B. Li. Inferring sentiment from web images with joint inference on visual and social cues: A regulated matrix factorization approach. In Proceedings of the Ninth International Conference on Web and Social Media, ICWSM 2015, University of Oxford, Oxford, UK, May 26-29, 2015, pages 473­482, 2015. 1 [35] Y. Wang, S. Wang, J. Tang, H. Liu, and B. Li. Unsupervised sentiment analysis for social media images. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 2378­2379, 2015. 1 [36] L. Zhaoping and P. Dayan. Pre-attentive visual selection. Neural Networks, 19(9):1437­1439, 2006. 2 [37] D. Zhou, J. He, K. S. Candan, and H. Davulcu. MUVIR: multi-view rare category detection. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 2531, 2015, pages 4098­4104, 2015. 1

Diving deeper into mentee networks
Ragav Venkatesan Arizona State University
ragav.venkatesan@asu.edu

Baoxin Li Arizona State University
baoxin.li@asu.edu

arXiv:1604.08220v1 [cs.LG] 27 Apr 2016

Abstract
Modern computer vision is all about the possession of powerful image representations. Deeper and deeper convolutional neural networks have been built using larger and larger datasets and are made publicly available. A large swath of computer vision scientists use these pre-trained networks with varying degrees of successes in various tasks. Even though there is tremendous success in copying these networks, the representational space is not learnt from the target dataset in a traditional manner. One of the reasons for opting to use a pre-trained network over a network learnt from scratch is that small datasets provide less supervision and require meticulous regularization, smaller and careful tweaking of learning rates to even achieve stable learning without weight explosion. It is often the case that large deep networks are not portable, which necessitates the ability to learn mid-sized networks from scratch. In this article, we dive deeper into training these midsized networks on small datasets from scratch by drawing additional supervision from a large pre-trained network. Such learning also provides better generalization accuracies than networks trained with common regularization techniques such as l2 , l1 and dropouts. We show that features learnt thus, are more general than those learnt independently. We studied various characteristics of such networks and found some interesting behaviors.
Mentor

Mentee

Figure 1. Mentor mentoring mentee on the second hidden layer.

1. Introduction
With the proliferation of off-the-shelf, downloadable networks such as VGG-19, overfeat, R-CNN and several others in the caffe model zoo, it has become common practice in the computer vision community to simply fine-tune one of these networks for any task [21, 13, 8]. These networks are usually trained on a large dataset such as Imagenet and Pascal [20, 7]. The proponents of these networks argue that these networks have learnt image representations that are pertinent for most datasets that deal with natural images. Under the assumption that all these datasets are natural images and are derived from a similar distribution

this might as well be true. Even with such networks, features that are unique to each datasets do matter. While finetuning of an already trained network works to a certain extent, these features are not learnt in a traditional manner on the target dataset but are simply copied. There is also no guarantee that these features are the best representations for the target dataset, although there is some validity in expecting that such a representation might work well, since after all it was learnt from a large enough dataset. Most computer vision scientists do not attempt to train a new architecture from scratch (random initializations). Training even a mid-sized deep network with a small dataset is a notoriously difficult task. Training a deep network, even those with mid-level depth require a lot of supervision in order to avoid weight explosion. On most imaging datasets, with image sizes being 224X 224, the memory insufficiency of a typical GPU restricts the mini-batches to less than 100. Using small mini-batches and small datasets lead to very noisy and untrustworthy gradients. This leads to weight explosions unless the learning rates are made sufficiently smaller. With smaller learning rates, learning is slowed down. With smaller mini-batches learning is unstable. One way to avoid such problems is by using reg-

ularization. By regularizing we can penalize the gradients for trying to make the weights go higher and higher. Batch Normalization is another technique that is quite commonly used to keep weight explosion under check [12]. Even with these regularization techniques, the difficulty of training a deep network from scratch leads most computer vision scientists to use pre-trained networks. There are several reasons why one might favour a smaller or a mid-sized network even though there might be a better solution available using these large pre-trained networks. Large pre-trained networks are computationally intensive and often have a depth in excess of 20 layers. The computational requirement of these networks do not make them easily portable. Most of these networks require state-ofthe-art GPUs to work even in simple feed forward modes. The impracticality of using pre-trained networks on smaller computational form factors necessitates the need to learn smaller network architectures. The quandary now is that smaller networks architectures cannot produce powerful enough representations. Many methods have been recently proposed to draw additional supervision from large well-trained networks to regularize a new network while learning from scratch [23, 19, 2, 5]. All of these works were inspired from the Dark Knowledge (DK) approach [11]. All these techniques use at most one layer of supervision on top of the softmax supervision and try to use this technique to learn more deeper networks better. Figure 1 shows a conceptualization of this idea. In this paper, we try and make a shallower mentee network learn the same representation as a larger, well-trained mentor network at various depths of its network hierarchy. Mentorship happens by tagging on to the loss of the mentee network, a dissimilarity loss for each layer that we want mentored. To the best of our knowledge, there hasn't been any work that has regularized more than one layer this way. There also hasn't been any work that has trained a mid-sized network from a larger and deeper network from scratch. We study some idiosyncratic properties for some novel configurations of mentee networks. We argue that such mentoring avoids weight explosion. Even while using smaller minibatches, mentee networks get ample supervision and are capable of stable learning even at high learning rates. We show that mentee networks produce a better generalization performance than an independently learnt baseline network. We also show that mentee networks are better transferable than the independently learnt baselines and are also a good initializer. We also show that mentee networks can learn good representations from very little data and sometimes even without supervision from a dataset in an unsupervised fashion. The rest of the paper is organized as follows: section 2 discusses related works, section 3 details the mentored

learning, section 4 discusses designs for experiments, section 5 produces results and section 6 provides concluding remarks.

2. Related Works
Hinton et al., tried to make networks portable by learning the softmax outputs of a larger well-trained network along with the label costs [11]. This was previously explored using logits by Caruana et al., [1, 4]. By directly learning the softmax layers, they were forcing the softmax layer of a smaller network to mimic the same mapping as that of a larger network onto the label space. In a way they tried to learn a better second and third guesses. They called this dark knowledge, as the knowledge so learnt is only available to the larger network. By attempting to learn the softmax layer, they were able to transfer or distil knowledge between the two networks. The drawback of this work is that it only works as long as the larger network is already well-trained and stable. They relied upon the network's predictive softmax layer being learnt perfectly on the target dataset and propagate that knowledge. This also assumes that there are relationships between classes to be exploited. While this may work in cases where this is true, such as in character recognition or in voice recognition, it doesn't work in most object detection datasets where the relationship between classes is not a given in terms of its appearance features1 . They also distil only the softmax labels and not the representational space itself. This also requires that the smaller network is capable of training in a stable manner. Dark knowledge is extended upon by several previous works [23, 19, 2, 5]. One extension of this work that we generalize in this article is using layer-wise knowledge transfer for one layer in the middle of the network. This was used to show that thinner and deeper network can be trained with better regularization [19]. Another method uses a similar one-layer regularizer as knowledge transfer between a RNN and a CNN [5]. Mentored training has also been shown to be extremely useful when training LSTMs and RNNs with an independent mentor supervision [23]. All these methods discussed above are essentially the same technique as the dark-knowledge method extended beyond just the softmax layer. All of these methods have fixed one-layer regularizations and although trivial, we generalize this for many layers. Their mentee networks are typically much deeper and complex than their mentors and they use these as a means to build more complex models (albeit thinner as in the case of FitNets [19]). There has been no study to the best of our knowledge that builds less complex (both thinner and shallower) models with the same capability as larger models. Also, neither has there been a study that stud1 We tried this approach on Caltech101 and couldn't get reliable results.

ies various properties of these networks nor those that show the transferability and generality of these networks.

3. Generalized mentored learning
Let us first generalize all of the methods that use this knowledge transfer as follows: Consider a large mentor network with n layers Mn . Suppose we represent the k th neuron activations of the ith layer in the network as Mn (i, k ). Consider a smaller mentee network with m2 layers Sm . Suppose that M is already well-trained and stable on a general enough dataset D. Now consider that we are using S to learn classification3 on a newer dataset d1 which is less general and much smaller than D as determined a priori. Although this is not a constraint, having a smaller and less general dataset emphasizes the core need where such mentored learning is most useful.  l  n and j  m, we can define a probe as an error measure between Mn (l) and Sm (j ). This error can be modelled as an RMSE error as follows, 1 a
a

(l, j ) =

(Mn (l, i) - Sn (j, i))2 ,
i=0

(1)

where a is the minimum number of neurons between Mn (l, .) and S (j, .). If the neurons were convolutional, we consider element-wise errors between filters. By adding this cost to the label cost of the network S during back propagation, we learn not just a discriminative enough representation for the samples and labels of d1 , but also for layers j in a pre-determined set of layers, a representation closer to the one produced by M. Some implementations of such loses in literature tend to learn a regressor instead of simply adding the loss, but we concluded from our experiments that the computational requirements of such regressors do not justify their contributions. Adding a regressor would involve embedding the activations of the mentor and the mentee onto a common space and minimizing the distances between those embeddings. We quite simply circumvent that and consider the minimum number of matching neurons. This enables us to have a slimmer, fatter or same th sized mentee. Suppose db 1 is the b mini-batch of data from the dataset d1 and suppose we have a pre-determined set of probes B , which is a set of tuples of layers from (M, S ). The overall network cost is, e = t Ls (db 1 ) + t
(l,j )B
2 Although we adhere strictly to m < n, without losing any generality, we could have any m or n. In fact m > n with only one probe would be the special case of FitNets [19]. 3 Although we only consider the task of classification, the methods proposed are applicable to many forms of learning.

where L(.)s is the network loss of that mini-batch, t and t weighs the two losses together to enable balanced training and t is the weight of the probe between the two (temperature) softmax layers. t = g (t), t = g (t) and t = g (t) are annealing functions parametrized by the iteration t under progress. Although most methods in the literature use constants for t , t and t , we found it preferable to retain g (t) = 1, t throughout and anneal  and  linearly. We discuss the value and the need for these parameters in detail further. Since M is pre-trained and stable, the second and third terms of equation 2 are penalties for the activations of those layers in S not resembling the activations of the probed layer from M respectively. These losses as defined by equation 1 are functions of the weights of those layers from S only. They restrict the weights within a proximity or region, that produces activations that are known for the mentor to be better activations. This restricting behaviour acts as a guided regularization process, allowing the weights to explore in a direction that the mentor thinks is a good direction, while still not letting the gradients to explode or vanish. For a particular weight w  S at any layer, a typical update rule without the probe is, wt+1 = wt -   Ls , w (3)

where t is some iteration number,  is the learning rate and assuming t = 1, t. The update rule with mentored probes is, wt+1 = wt -  t t  Ls + w   (l, j ) + t (n, m) . (4) w w

(l,j )B

(l, j ) + t (n, m),

(2)

The last two terms add a guided version of a noise that decreases with each iteration. While at earlier stages of training, this allows the weights to explore the space, it also restricts the weights from exploding because the direction that the weights are allowed to explore is controlled by the mentor. The freedom to explore tightens up as the as learning proceeds, provided g (t) is a monotonically annealing function with respect to t. Note that even though to calculate these error gradients we need one forward propagation through M, we do not back propagate through M. This is a penalty on the weights, even though we are using the activations to penalize the weights indirectly. Although mentee networks can be further regularized with l2 , l1 , dropouts and batch normalizations, it is recommended that the mentee networks imposes additional regularizations mirroring the mentor networks for better learning.

Obedient Mentee 0.04 0.03 0.02 0.01 0 0
* * * 
Mentoring Phase Self-study Phase

Adamant Mentee 0.04 0.03 0.02 0.01 0 0
* * * 

50 Epochs

100

150

50 Epochs

100

150

Figure 2. Annealing ,  and  while learning for an obedient and an adamant network.

Different configurations of mentee networks Different combinations of ,  and  produces different characteristics of mentee networks. Equation 4 can be seen as learning with three different learning rates,    ,    and    . We can simulate using these three parameters, two idiosyncratic personalities of mentee networks: an obedient network and an adamant network. An obedient network is a network that focuses on learning the representation more than the label costs at the beginning stages and once a good representation is learnt, it focuses on learning the label space. It tends towards being over-regularized and its regularization relaxes with epochs. An adamant network is a network that focuses almost immediately on the labels as much as learning the representation, but its focus is positively towards learning the label only. The learning rates of these personalities are shown in figure 2. An independent network can be considered as a special case of the adamant network where probe weights are ignored ( = 0,  = 0, t). The other extreme case of an obedient network is perhaps a gullible network that learns just the embedding space of the mentor. Gullible networks are also a good way to initialize a network in an unsupervised mentoring fashion. Consider a dataset d2 , that does not have any labels. Neither the mentor nor the mentee could potentially learn any discriminative features. Using just the probes we could build an error function that could make the smaller mentee network still learn a good representation for the dataset. We use the information from the parent network to learn a good representation for d2 by simply back propagating the second term of equation 2 alone. These gullible mentees come in really handy when the dataset has considerably less samples to be supervised with. Unsupervised mentoring is also an aggressive way to initialize a network and is often helpful in learning large networks in a stable manner with a stable initialization. Typically the deeper one goes, the more difficult it becomes to learn the activations and the costs saturate quickly. The softmax layer is the most difficult to learn. To our surprise we find that probe costs converge much sooner than

the label costs, leading us to believe that the representations being mentored are indeed relevant as long as the datasets share common characteristics. There is a plethora of such configurations that could be tried and many unique characteristics discovered. In this article we limit ourselves to only those that enable us stability during learning and focus on those that help us with better generalizations. For learning large networks we prefer the use of obedient networks as obedient networks are heavily regularized at the beginning leading to careful initialization and stabilization of the network before learning of labels takes over. We call the stabilization phase as the mentoring phase and the rest, self-study phase. During the mentoring phase learning is slow but steady. In most cases,    is an increasing function due to the aggressive climb of . The annealing of these rates for a typical obedient mentee and an adamant mentee are shown in figure 2. We also find that typically the later layers are more stubborn in being mentored than earlier layers. Although this is typically to be expected, more obedience may be enforced by choosing higher  values for layers that are deeper in the network.

4. Design of experiments
We evaluate the effectiveness of mentorship through the following experiment designs:

4.1. Effectiveness
To demonstrate the effectiveness of learning, we first train a larger network on a dataset. Using this network as a mentor, we train the mentee network on the same dataset. Unlike those in literature, we choose mentee networks that are generally much smaller than the mentor. We show that this generalizes at least as well as an independent network of the exact same architecture regularized not by mentor, but by batch normalization, l2 and l1 norms and dropouts. Training mid-sized networks on small datasets are often difficult. To our best knowledge we have provided our best effort in meticulously learning all the networks. For learning an independent network often we spent additional effort in adjusting the learning rates at the opportune moments. We show that mentee networks outperforms the independent networks and even at the worst case performs as well as the independent networks.

4.2. Generality of the learnt representations
To demonstrate that the network learns a more general representation, we gather a pair of datasets of seemingly similar characteristics with one more general or larger than the other. We train the mentor with the more general dataset first and then fine tune it on the less general dataset. We then train both the independent and the mentee nets on the less general dataset and demonstrate again that at worst the mentee net performs the same as the independent net.

0.02 0.015 0.01 0.005 0 0

Conifguration For Learning VGG-19 using Caltech 101

Fine Tuning Begins Here

* * * 

10

20

30

40

50

60

70

80

90

100

Epochs

Figure 3. Annealing ,  and  while learning VGG-19 space for Caltech-101. We used an obedient network.

other works have been built. We try to learn the same 4096 dimensional representation of the VGG-19 network using ambitiously less number of layers. For the (Caltech-101-Caltech256) dataset pairs in all our experiments, there is no explicit mentor network that we learnt. We simply set g (t) = 0, t and learnt with probes without retraining the VGG-19 network. In a way we are attempting to learn VGG-19's view of the Caltech101 dataset and are probing into the representational frame of the VGG-19 network. We used a relatively obedient student as shown in figure 3 for this case.

4.4. Implementation details
The independent networks were all regularized with a l1 and l2 penalties with a weight of 1e-4 , which seems to give the best results. On all networks we also applied parametrized batch norm for both fully connected and convolutional layers and dropouts with rate of p = 0.5 for the fully connected layers [12, 22]. We find that dropout and bath norm together help in avoiding over-fitting. All our activation functions were rectified linear units [16]. For learning the mentee network we start with learning rates as high as 0.5, for the larger independent networks we are forced a learning rate of 0.001, while for the smaller experiments we were able to go as high as 0.01, since the batch sizes were larger. During training, if ever we ran into exploding gradients or NaNs, we reduce the learning rate by ten times, reset the parameters back to one epoch ago and continue training. We train until 75 epochs after which we reduce the learning rate by a hundred times and continue fine-tuning until early stopping. Unless early stopped, we train for 150 epochs. All our initializations were from a 0-mean Gaussian distribution, except the biases which were initialized with zeros. The experiment set-up was designed using Theano v0.8 and the programs were written by ourselves4 [3]. The experiments with MNIST datasets were conducted on a Nvidia GT 750M GPU, the others on an Nvidia Tesla K40c GPU, with cuDNN 3007 and Nvidia CUDA v7.5. The minibatch sizes for all the MNIST and cifar experiments were 500 (unless forced by small dataset size in which case we performed batch descent instead of the usual stochastic descent). The mini-batch sizes for all Caltech experiments were 36, with images resized to 224X 224 so as to the fit the VGG-19 requirement. Apart from normalization and meansubtraction, no other pre-processing were applied to any of the images. For the Caltech experiments we used Adagrad with Polyak's momentum [18, 9]. For the experiments that were smaller networks we used RMSprop with Nesterov's accelerated gradient [6, 17]. It is to be noted that we chose to use vanilla networks that are as simple as possible so as to enable us to compare against a baseline which is also vanilla. Since our aim is
4 Code

We then proceed to fine tune the classifier layer of both the mentee net and the independent net using the more general dataset but since the other layers are not allowed to change, the mentee net does not have any additional supervision. This tests the quality of the features learnt by these networks on a more general and more difficult dataset. For the sake of our experiments we consider the pairing of (Cifar-10 - Cifar-100) and (Caltech-101 - Caltech256) [14, 10]. We assume that Cifar-100 is more general than Cifar-10 and Caltech-256 is more general than Caltech101. Additionally, we conduct another experiment where we try to learn from a mentor network trained with the full MNIST dataset, a mentee network that only has supervision from a part of the dataset [15]. The independent network also in this case, learns with the same redacted dataset. We redact the dataset by only having p samples for each class in the dataset where p  {500, 250, 100, 50, 10, 1}. p = 1 is essentially an ambitious goal of 1-shot learning from scratch using a deep network. We also try this with a mentee network that is initialized by unsupervised mentoring from the same mentor network. We acknowledge that the comparison with unsupervised mentoring is unfair because the mentee net is initialized by the mentor with information filtered from data that is unavailable for the independent network. The latter results are to demonstrate that unsupervised mentoring could learn an effective feature space even without labels and with very less samples.

4.3. Learning the VGG-19 representation
In particular, while learning classification on the Caltech101 dataset, we try to learn the same representation as the popularly used VGG-19 network at various levels of the network hierarchy [21]. VGG-19 network's 4096 dimensional representation is one of the most coveted and iconic image features in computer vision at the time of the writing of this article. The VGG-19 network has 16 convolutional layers and 2 fully-connected layers the last of which produces the 4096 dimensions of features upon which many

is available at our GitHub page.

a) VGG-19 M entor

b) Gullible M entee

c) Obedient M entee

d) Adamant M entee

Figure 6. VGG-19 first layer filters and filters probed using Caltech101 for a Gullible, Obedient and an Adamant mentee after only one epoch of training. We recommend viewing this image on a computer monitor.

not to achieve state-of-the-art accuracies on any datasets, we didn't implement several techniques that are commonly applied to boost the network performances in modern day computer vision. The purpose of these experiments is to unequivocally demonstrate that among networks that learn from scratch, one that is mentored can perform better and learn more general features than one that is not.

5. Results
The results are split across two tables based on the network architectures. The smaller experiments on a 5 layer network are shown in figure 4 and the larger ones in figure 5. The  symbol shows which layers are probed and from where. In figure 4, the results clearly demonstrate the strong performance of the mentee networks over the independent networks. In the cifar experiments we under-weighted  purposely as we didn't want to propagate the 20% of error from the mentor network on to the mentee network. The results on Cifar 10 from scratch seem to indicate that both networks have reached the best possible performance for that architecture. We believe with the amount of supervision already provided from the 40,000 training images, mentoring is not as effective. When there is already ample supervision, men-

toring is ineffective, or rather unwanted, albeit it doesn't hurt. While fine-tuning on cifar 100, we find that there are great gains to be made. We find a similar trend with the MNIST experiments also. The less data there is, the higher the gain of the mentee networks. Note that even though mentee networks are regularized, care was taken to ensure that they both go through the exact same number of iterations at the exact same learning rate. We also found that unsupervised mentoring always keeps the learning at a very high standard although as was discussed in section 4.2 there was additional supervision on the entire dataset from the unsupervised mentoring, which is unfair. In the experiments with the Caltech101 datasets, we find that the mentee networks perform better than the vanilla network. The mentee network was also able to perform significantly better than the independent network when only the classifier/mlp sections were allowed to learn the Caltech256 dataset with representation learnt from Caltech101. This proves the generality of the feature space learnt. With an even obedient student, we were able to learn the feature space of the VGG-19 network to a remarkable degree. While with the first convolutional layer we were able to learn to a minimum rmse or 0.0023 from 6.54 at random. With the last two layers we were able to learn upto a rmse of 2.04 from 12.76 at random. Figure 6 shows the filters learnt after one epoch for a gullible network, an obedient network and an adamant network. All these networks were initialized with same random values at their inception. We can easily notice that the gullible network already sway towards the VGG-19 filters. In obedient mentee, we notice that most corner detector features are already swaying towards the mentee network but more complex features are not swaying as much as the gullible network. To our surprise we notice that even in an adamant network corner detectors are swaying towards VGG-19. This shows that even with low weights, the first layer features are learning the VGG-19's representation. It is to be noted that we are not learning the weights directly, but are learning the activations produced by the VGG-19 network for the Caltech101 dataset that leads us to learn the same filters as the VGG-19. This implies that corner features are more general among the Imagenet dataset, which VGG-19 was trained on, and the Caltech101 dataset, which explains why they are learnt earlier than others.

6. Conclusions
While the use of large pre-trained networks will continue to remain popular, because of the ease in just copying a network and fine-tuning the last layers, we believe that there is still a need for learning small and mid-sized networks from scratch. We also recognize the difficulty involved in reliably training deep networks with very few data sam-

Network
Convolutional Layers Activation: ReLU Stride: 1 Max Pooling: 2 Architecture Fully Connected Layers Activation: ReLU Dropout Input Rate: 0.5 Output Layer Trained from scratch Cifar 10 Fine-tuned last layer only Cifar 100 MNIST - 500

Mentor
Kernel Size: 5 Neurons: 20 Kernel Size: 3 Neurons: 50 Neurons: 800 Neurons: 800 Neurons: 10/100 79.36 % 41.21%

Mentee
Kernel Size: 5 Neurons: 20 Neurons: 800 Neurons: 800 Neurons: 10/100 68.5 % 33.2 % 97.73%
unsupervised mentoring

Independent
Kernel Size: 5 Neurons: 20 Neurons: 800 Neurons: 800 Neurons: 10/100 68.58 % 26.67% 97.71%

98.2%

MNIST-250 Accuracies

97.47 %
unsupervised mentoring

97.88%

96.89 %

MNIST-100 MNIST 99.59% MNIST-50

97.42%
unsupervised mentoring

96.01%

95.12%

92.95%
unsupervised mentoring

96.80%

90.96%

MNIST-10

78.5 %
unsupervised mentoring

96.7%

75.3%

MNIST - 1

48.5%
unsupervised mentoring

96.7%

41.5%

Figure 4. Architecture and results for the experiments with CIFAR and MNIST datasets.

ples. One way to meet the best of both worlds is by using a mentored learning approach. In our study, we find that a shallower mentee network was able to learn a new representation from scratch while being regularized by the mentor network's activations for the same input samples. We found that such mentoring provided much stabler training even at higher learning rates. We noted some special cases of these networks and recognize some idiosyncratic personalities. We extended one of these to be able to perform as an unsupervised initialization technique. We showed through compelling experiments, the strong performance and generality of mentor networks.

[3] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012. 5 [4] C. Bucilu, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535­541. ACM, 2006. 2 [5] W. Chan, N. R. Ke, and I. Lane. Transferring knowledge from a rnn to a dnn. arXiv preprint arXiv:1504.01483, 2015. 2 [6] Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for non-convex optimization. arXiv preprint arXiv:1502.04390, 2015. 5 [7] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303­ 338, 2010. 1 [8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition

References
[1] J. Ba and R. Caruana. Do deep nets really need to be deep? In Advances in neural information processing systems, pages 2654­2662, 2014. 2 [2] A. K. Balan, V. Rathod, K. P. Murphy, and M. Welling. Bayesian dark knowledge. In Advances in Neural Information Processing Systems, pages 3420­3428, 2015. 2

Network

Mentor (VGG-19)
Neurons: 64 Max Pooling: 1 Neurons: 64 Max Pooling: 2 Neurons: 128 Max Pooling: 1 Neurons: 128 Max Pooling: 2 Neurons: 256 Max Pooling: 1 Neurons: 256 Max Pooling: 1 Neurons: 256 Max Pooling: 1 Convolutional Layers Stride: 1 Kernel Size: 3 Activation ReLU Neurons: 256 Max Pooling: 2 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 2 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 2 Fully Connected Layers Activation: ReLU Dropout Input Rate: 0.5 Softmax Layer Trained from scratch on Caltech 101 Fine-tuned last layer only for Caltech 256 N/A N/A Neurons: 4096 Neurons: 4096

Mentee
Neurons: 64 Max Pooling: 2

Independent
Neurons: 64 Max Pooling: 2

Neurons: 128 Max Pooling: 2

Neurons: 128 Max Pooling: 2

Neurons: 256 Max Pooling: 2

Neurons: 256 Max Pooling: 2

Architecture

Neurons: 512 Max Pooling: 2

Neurons: 512 Max Pooling: 2

Neurons: 512 Max Pooling: 2

Neurons: 512 Max Pooling: 2

Neurons: 512 Max Pooling: 2 Neurons: 4096 Neurons: 4096 Neurons: 102/256 56.16% 66.12 %

Neurons: 512 Max Pooling: 2 Neurons: 4096 Neurons: 4096 Neurons: 102/256 45.46% 55.45%

Accuracies

Figure 5. Architecture and results for the experiments with Caltech datasets.

(CVPR), 2014 IEEE Conference on, pages 580­587. IEEE, 2014. 1 [9] S. Green, S. I. Wang, D. M. Cer, and C. D. Manning. Fast and

adaptive online training of feature-rich translation models. In ACL (1), pages 311­321, 2013. 5 [10] G. Griffin, A. Holub, and P. Perona. Caltech-256 object cat-

egory dataset. 2007. 5 [11] G. Hinton, O. Vinyals, and J. Dean. Dark knowledge. Presented as the keynote in BayLearn, 2014. 2 [12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 2, 5 [13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675­678. ACM, 2014. 1 [14] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images, 2009. 5 [15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998. 5 [16] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807­814, 2010. 5 [17] Y. Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372­376, 1983. 5 [18] B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1­17, 1964. 5 [19] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. 2, 3 [20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), pages 1­42, April 2015. 1 [21] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 1, 5 [22] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014. 5 [23] D. Wang, C. Liu, Z. Tang, Z. Zhang, and M. Zhao. Recurrent neural network training with dark knowledge transfer. arXiv preprint arXiv:1505.04630, 2015. 2

Neural Dataset Generality
Ragav Venkatesan Arizona State University
ragav.venkatesan@asu.edu

Vijetha Gattupalli Arizona State University
jgattupa@asu.edu

Baoxin Li Arizona State University
baoxin.li@asu.edu

arXiv:1605.04369v1 [cs.CV] 14 May 2016

Abstract
Often the filters learned by Convolutional Neural Networks (CNNs) from different datasets appear similar. This is prominent in the first few layers. This similarity of filters is being exploited for the purposes of transfer learning and some studies have been made to analyse such transferability of features. This is also being used as an initialization technique for different tasks in the same dataset or for the same task in similar datasets. Off-the-shelf CNN features have capitalized on this idea to promote their networks as best transferable and most general and are used in a cavalier manner in day-to-day computer vision tasks. It is curious that while the filters learned by these CNNs are related to the atomic structures of the images from which they are learnt, all datasets learn similar looking low-level filters. With the understanding that a dataset that contains many such atomic structures learn general filters and are therefore useful to initialize other networks with, we propose a way to analyse and quantify generality among datasets from their accuracies on transferred filters. We applied this metric on several popular character recognition, natural image and a medical image dataset, and arrived at some interesting conclusions. On further experimentation we also discovered that particular classes in a dataset themselves are more general than others.


2
3

1

4

5

Figure 1. Thought experiment to describe the dataset generality. S is the space of all possible atomic structures, D1 - D5 are the atomic structures present in respective datasets.

1. Introduction
Neural networks, particularly CNNs have broken all records recently in the computer vision research area. The growth of CNNs focused initially on the recognition of characters. Fukushima and LeCun were the initial pioneers. Independently they developed CNN based systems, some of which are still being used widely [7, 16]. Large networks are often trained with large number of data samples to achieve good accuracies [25, 14]. Still, scepticism over CNNs among the modern day computer vision scientists stems from the fact that one does not have a clear understanding of its inner working. Some studies show that a few (< 1%) nodes are all that are actively contributing to

classification [4]. They also suggest that large networks often overfit, but since the data is too large over-fitting often works as an advantage [20]. While it is reasonable to expect edge detectors and Gabor-like features in the lower-level filters and more sophisticated concepts at the higher levels, it is not clear as to why these filters adapt themselves in this manner. What is fairly clear though is that different datasets result in different sets of filters that are similar if the datasets are similar. It is only natural to ask, what role does the data itself play in such filters being learnt and how they compare with filters learnt from another dataset. In this paper we take the view that the filters learnt by networks when trained using a particular dataset represent the detectors for some atomic structure in the data itself. In which case each layer is a mapping form the previous layer to the next layer that is constructed using combinations of these atomic structures in the first layer in order to minimize a cost. Let us first define atomic structures to be the forms that CNN filters take by virtue of the entropy of the dataset it is learning on, analogous to dictionary atoms. Complex

datasets have more and varied atomic structures. Consider the following thought experiment: Let's assume that all possible atomic structures reside in an universe S . Suppose we have a set of three datasets D = {D1 , D2 , D3 } and D  S . Consider the system in figure 1. The figure describes the configuration of the elements of D. One would now recognize that D1 is a more general dataset with respect to D2 and D3 . It is so because, while D1 contains most of the atomic structures of D2 and D3 , the latter do not contain as many atomic structures of D1 . While this analysis is simplified for one layer, in typical CNNs, co-adaptation plays a major role in the learning of these atomic structures. Therefore, generality as defined by the overlap of areas in a layer-wise Venn diagram is impractical to obtain. In this paper we postulate that, the generalization performances of CNNs on one dataset re-trained on a network initialized by training using another, could be used to derive generality. We call this process of pre-training as prejudicing. By prejudicing on the first dataset, we froze and unfroze layers and retrained the networks on the second dataset. By freezing layers we are making a network more obstinate and we call this process obstination1 . The more the layers are frozen, the more obstinate the feature extractor is, therefore the harder the classifier has to work. If the prejudice was general enough, the classifier shall still generalize fairly well enough. What this means is that if the prejudicing dataset is more general than the re-train dataset, the classifier can generalize better than vice versa. We developed a generality metric by comparing the gain in performances of networks of various obstination. Using a generality such as the one proposed, it becomes clearer as to what kind of datasets are to be used to prejudice CNNs with during transfer learning. We even discovered that samples with particular labels within a dataset alone are general enough. So, if we begin by prejudicing the network on only those and then moved on to the rest of the labels, we were able to learn the rest of the dataset with considerably less training samples while achieving comparable generalization performances. Off-the-shelf networks such as VGG, overfeat and various published Caffe model weights are trained on large scale image datasets such as Imagenet or PASCAL [23, 12, 8, 22, 5]. For instance, while these may work on applications such as human pose recognition or vehicle detection, they do not necessarily work on tasks involving medical images. This is because the datasets on which they are trained are not general enough to adapt to the representational requirements of medical images, which is on a manifold unique and disjoint form the manifolds of natural images. This is visualized in D4 and D5 from figure 1. Even a large collection of natural images is not general enough
1 Obstinate layer or freezing implies that the weights were not changed during backprop. The layer remains prejudiced.

to have networks trained that are suitable to medical images. In these cases, the prejudiced network often fails. For instance, on the Colonoscopy dataset discussed later a 22 layer deep overfeat features, trained with a logistic regression performs poorer than a 3 layer deep CNN trained from random initialization, which is in turn outperformed when initialized by a network trained on an endoscopy dataset. In this article we considered popular offline character recognition datasets and arrived at some interesting analysis and generalities. We also show that within the MNIST dataset, classes [4, 5, 8] are general enough that we could learn the other classes with very few (even just one) samples, when prejudiced with networks trained on [4, 5, 8]. We also considered more sophisticated datasets such as Cifar 10 and Caltech 101 against some medical image datasets for colonoscopy video quality [13]. This study led us to two major research insights: 1. If one has very few data to learn from, which other dataset is better to prejudice the network with? The answer is particularly helpful when dealing with medical image datasets where data is very scarce and one can't simply use a network trained on VOC datasets as feature extractors as discussed above. 2. Among the various classes during the training procedure, if we prejudice with a certain general set of classes first and then move on to others later, generalization to all classes, even for those with few samples is better. This is particularly significant if the dataset has a lot of samples in certain classes and not as much of others. The rest of the paper is organized as follows: section 2 discusses related works, section 3 presents the design of our experiments, section 4 shows some results on the coreexperiment and section 5 provides concluding remarks.

2. Related work
One related work that this article shares with is the work by Yosinski et al [26]. In that article, the authors considered two tasks A and B that were essentially 500 classes each from the Imagenet dataset [22]. They trained an 8 layer network on one of the tasks (say A). They then initialized a new network carrying over the first n layers from the previous job while randomly initializing the others. This new network was used to retrain task B . Such a network was AnB + . They experimented by obstination of the carried over layers. Such a network was AnB . They also studied the specificity of each layer and their contributions to the overall performance. They also showed that networks working on similar tasks had a high memorability and that co-adaptation of layers increased the generalization performance.

While this analysis is interesting, it was performed on only one dataset: Imagenet. By design, the networks were forced to learn very general filters, so as to be best transferable. Since the images were all natural images, one would expect the layers to be more Gabor-like at earlier layers and have more label specific features at later layers, which was what was observed. Also, the paper analysed the transferability of the feature extractors from the perspective of the networks in terms of their fall in generalization performance. This analysis was not catered to the dataset's perspective, which is that the filters learned are a property of the dataset being trained on. This was not a problem for the authors as their datasets for tasks A and B occupied similar manifolds. This analysis also didn't explore re-training using the same network but rather went with re-initializing so that they could learn new co-adaptations. This is not interesting to the study of generality as we want to observe the effect of filters transferred from one dataset on another. The more general a dataset, the more variety of atomic structures it offers to the network to learn. We used this idea to define a generality metric between two datasets. To do so, we cannot follow the techniques used by Yosinski et al. Another closely related work is the work on dark knowledge by Hinton et al, [10]. Here the authors suggest that among the various classes in a dataset, there exists some amount of generalization knowledge that could be transferred. The authors construct a large network that learns all its classes. They then go on to train a smaller network with the same dataset (or with a dataset that is missing some of the classes altogether). While training this smaller network though, instead of using the the hard labels, they also use the softmax output from the large network also for backprop. This creates an effect of the larger network guiding the smaller network to not just generalize to the dataset, but also to generalize to unseen classes. This is because, as the argument goes, "the network learns the relationship between the classes" and "all the knowledge is among the relative probabilities or softmaxes that the network is almost certain is wrong" [10]. Although the author retrains an entire network that is randomly initialized using the softmax outputs from a trained network and uses this as prejudice, no information is actually being transferred in terms of actual filters. Ergo, this work, while interesting, also doesn't help in understanding generality of the data itself in a more direct manner. Some of the claims made by this article though were indirectly and independently verified by us through our generality results. The basic claim of their work is that among only a handful of classes, there is enough knowledge to generalize to other classes. Unless there exists some generality between classes, training on particular classes will not have been representational enough for the other classes to learn on. We directly verify this by showing that some classes

Figure 3. Samples of some of the datasets that we used in this analysis. From top to bottom: MNIST [17], MNISTrotated [15], MNIST-random-background [15], MNIST-rotatedbackground [15], Google street view house numbers [19], Char 74k English [3], Char 74k Kannada [3]. Last two rows, first five from left are CIFAR 10 and the rest are Caltech101 [13, 6]. The bottom row is the colonoscopy dataset.

alone have a high generalization to the rest of the dataset and make a similar conclusion from an entirely independent direction of research.

3. Design of experiments
Consider figure 3. Among the various datasets shown, it is natural to expect any network trained on MNIST to contain simpler filters than MNIST-rotated. This is because, while MNIST-rotated contains many structures from MNIST, due to the rotations, MNIST-rotated will contain additional structures that require the learning of more complicated filters. A network trained on MNIST-rotated on its first layers will be expected to additionally have filters for detecting sophisticated oriented edges than for MNIST. This would mean that prejudicing a network with MNIST to then re-train MNIST-rotated is much less helpful than vice versa. A network prejudiced with a general enough dataset is better to be retrained for it generalizes easily. A prejudice must come from a more general dataset if a prejudice transfers positive knowledge as shown in their generaliza-

0 1

0 1

0 1

0 1

9

9

9

9

Figure 2. Protocol of obstination: From left to right, all layers frozen, one, two and three layers unfrozen. Green represent unfrozen and red represent frozen. Note that the layers are always unfrozen from the end and that the softmax layer is always unfrozen and randomly initialized. This should be generalized similarly for more than three layers also.

tion performances. We use this simple intuition to argue that MNIST-rotated is a more general dataset with respect to MNIST. Our basic experiment is conducted between pairs of datasets Di and Dj . Firstly, we train (prejudice) a randomly initialized network with dataset Di . We call this network n(Di |r) or the base network (r implies random initialization). We then proceed to retrain n(Di |r) as per any of the setup shown in figure 2. nk (Dj |Di ) would imply that there are k degrees of freedom, or to be precise, k layers of filters that are allowed to learn by dataset Dj that is prejudiced by the filters of n(Di |r). nk (Dj |Di ) has N - k obstinate layers that carries the prejudice of dataset Di , where N is the total number of layers. Note that more degrees of freedom implies that the network is less obstinate to learn. Also note that these layers can be both convolutional or fully connected neural layers. Any idea expressed here can be extended to any type of parametrized layers. In fact while we perform operations such as batch normalizations, we even freeze and unfreeze the  and  of batch norm [11]. Obstination also includes the bias parameters. Layers learn in two facets. They learn some components that are purely their own and some that are co-adapted from previous layers that are allowed to learn as well. By freezing some layers we are making those layers a fixed functional transformation. Note that the performance gain from nk (Dj |Di ) and nk+1 (Dj |Di ) is not because of just the new layer k + 1 being allowed to learn, but of the combination of all k + 1 layers allowed to learn. Figure 2 shows the setup of our experiments and explains degrees of freedom. These are our obstination protocols. Notice that in all the various setup, the softmax layer remains non-obstinate. In fact the softmax layer is always randomly re-initialized because not all dataset pairs have the same number of labels. Also notice that the unfreezing of layers happen from the rear. We cannot unfreeze a layer that feeds into a frozen layer. This is because, while the unfrozen layer learns a new filter and therefore represents the image on new distributed domains, the latter layer is not adapting to such a transformation. When there are two

layers unfrozen, the two layers should be able to co-adapt together and must finally feed into an unfrozen classifier layer.

3.1. Dataset generality
Suppose the generalization performance of n(Dj |r) is (Dj |r) and the generalization performance of nk (Dj |Di ) is k (Dj |Di ). First order dataset generality or simply dataset generality of Di with respect to Dj at the layer k is given by, gk (Di , Dj ) = k (Dj |Di ) (Dj |r) (1)

This indicates the level of performance that is achieved by Dj using N - k layers worth of prejudice from Di and k layers worth of features from Di combined with k layers of novel knowledge from Dj together. Note that the generality is calculated for the base dataset as a measure of how the re-train performs with the prejudice of the base dataset. gk (Di , Dj ) > gk (Di , Dl ) indicates that at k layers, Di provides more general features to Dj than to Dl . Conversely, when initialized by n(Di |r), Dj has an advantage in learning than Dl . Note that, gk (Di , Di )  1 k . gk (Di , Dj ) for i = j might or might not be greater than 1. If gk (Di , Dj )  1 for i = j , it indicates that Dj is at least very similar to Di (such as the case considered by Yosinski et al.) and at most a perfect generalizer of Di [26].

3.2. Class generality
Di and Dj need not be entire datasets but can also be just disjoint class instances of the same dataset that is split in two. These generalities will tell us if particular classes are themselves more general than others. For instance, we divided the MNIST dataset into two parts. The first part contained the classes [4, 5, 8], the rest were contained by the second part2 . We performed the generality experiments
2 We chose this combination of classes strategically after trail and error as these are the most general among the classes and exaggerate the effect.

with MNIST[4, 5, 8] as base, which was trained over a random initialization. We re-trained this prejudiced network using the second part with the same experiment design as above. We defined class generality as the generality, of a class or a set of classes, retrained on the prejudice of the other mutually exclusive classes. We repeated this experiment several times with decreasing number of training samples per-class in the retrain dataset of MNIST [0, 1, 2, 3, 6, 7, 9]. All the while, the testing set remained the same size. This implies that the prejudiced network retrains on a much smaller dataset and tests on a much larger dataset. The re-train dataset had 7 classes. We created seven such datasets with 7p, p  [1, 3, 5, 10, 20, 30, 50] samples each. We now define subclass generality as the generality of these sub-sampled datasets (in each class we only consider a small random sample), retrained on the base of other mutually exclusive classes (MNIST[4, 5, 8]). . Initializing a network that was trained on only a small sub-set of well-chosen classes can significantly improve generalization performance on all classes, even if trained with arbitrarily few samples, even at the extreme case of one-shot learning.

batch sizes used in stochastic gradient descent of all the datasets used. No pre-processing were done on the images themselves except for cropping, resizing, normalizing. The images were all normalized to lie in [0, 1]. The character recognition datasets were all of a constant 28X 28 grayscale, the Caltech 101 vs. Cifar 10 experiments were performed ar 32X 32, RGB and the Caltech 101 vs. Colonsoscopy were at 128X 128, RGB. It is to be noted that the aim of the authors was not to set up the networks to achieve state-of-the-art. The authors did although try to achieve satisfactory performances on all base datasets involved before proceeding with the experimentation. Character Datasets. Our networks had three convolutional layers with 20, 20 and 50 kernels respectively. All the filters were 5 X 5 and were all stride 1 convolutions. The first layer didn't have any pooling. The second and the third layer maxpool by 2 subsampled. All the layers used rectified linear units (ReLU ) activations [18]. The classifier layer was a softmax layer and we didn't use any fully connected layers. We used a dropout of 0.5 only from the last convolutional layer to the softmax layer [24]. We optimized a categorical cross-entropy loss using an rmsprop gradient descent algorithm [2]. For acceleration we used Polyak Momentum that linearly increases in range [0.5, 1] from start to 100 epochs [21]. Unless early terminated, we ran 200 epochs. We also used a constant L1 and L2 regularizer co-efficients of 0.0001. Our learning rate was a 0.01 with a multiplicative decay of 0.0998. CIFAR10 Vs. Colonoscopy. Caltech101 and Caltech 101 vs

3.3. Datasets Used
We designed these experiments across three board categories of datasets: 1. Character datasets that included MNIST [17], MNIST-rotated [15], MNIST-randombackground [15], MNIST-rotated-background [15], Google street view house numbers [19], Char 74k English [3] and Char 74k Kannada [3] 2. Natural image datasets that includes Cifar 10 and Caltech 101 [13, 6] and 3. Natural images against medical images that included in addition to Caltech 101 a Colonoscopy video qualitty dataset. We leave it to the reader to find for themselves details about the datasets from the original articles, but the setup we have used can be found in table 1. Although we chose only a handful of datasets, the intention of this article was only to show that such generality measures could be made. The scope of this article was not to benchmark various publicly available popular datasets. Neither was it to make suggestions specific to types of datasets.

3.4. Network architecture and learning
We used one standard network architecture for all character datasets and experiments, one for Cifar 10 vs. Caltech 101 and another standard for Caltech 101 vs. Colonoscopy. The setup we have used can be found in table 1. The network architectures, learning rates and other details are provided below. The experiments were conducted on a Macbook Pro Laptop using an Nvidia GT 750M GPU, for character datasets and on an Nvidia Tesla K40 GPU for the others, with cuDNN v3 and Nvidia CUDA v7. Table 1 shows the train-test-validation splits and the

For this task, the networks had five convolutional layers with 20, 20, 50, 50 and 50 kernels respectively. We also had a last fully connected layer of 1800 nodes, which also had a dropout of 0.5. All the filters were 5 X 5 and were all stride 1 convolutions. Only the last layer maxpool by 2 subsampled. All the layers used rectified linear units (ReLU ) activations [18]. All CNN and MLP layers were also batch normalized [11].The classifier layer was a softmax layer and we didn't use any fully connected layers. We used a dropout of 0.5 only from the last convolutional layer to the softmax layer [24]. We optimized a categorical cross-entropy loss using an rmsprop gradient descent algorithm [2]. For acceleration we used Polyak Momentum that linearly increases in range [0.5, 0.85] from start to 100 epochs [21]. We use a learning rate of 0.001 for the first 150 epochs and then fine tune with a learning rate of 0.0001 for an additional 50 epochs unless early-terminated. Our learning decay of was subtractive 0.0005. Figure 4 shows more generality curves.

Dataset MNIST [17] MNIST-random-background [15] MNIST-rotated-background [15] NIST Special Dataset-19 [9] Google Street View House Numbers [19] Char 74k English [3] Char 74k Kannada [3] MNIST [4, 5, 8] MNIST [0, 1, 2, 3, 6, 7, 9] - p per-class CIFAR 10 [13] Caltech 101 [6] Colonoscopy3

Training 50,000 40,000 40,000 271,220 63,042 9,300 5,694 14,000 7p 40,000 5,080 2,700

Testing 10,000 12,000 12,000 271,220 63,042 3,355 1,314 2,500 7,000 10,000 3,048 900

Validation 10,000 10,000 10,000 271,220 63,042 305 1,752 2,500 7,000 10,000 1,016 100

Classes 10 10 10 62 10 62 100 3 7 10 102 2

Training Batch Size 500 500 500 191 399 305 438 500 500 500 254 100

Table 1. Datasets used and their properties.

4. Results and observations
4.1. Character Datasets
Figure 4 shows the generalities of MNIST-rotated-bg and Kannada prejudiced by all other the character datasets. For reference each plot also shows the generalization performance of a randomly initialized base convolutional network. The following are some observations of interest: While no dataset is qualitatively the most general, it is quite clear that MNIST dataset is the most specific. Rather, MNIST dataset is one that is generalized by all datasets very highly at all layers. Surprisingly, MNIST dataset actually gives better accuracy when prejudiced with other datasets, rather than when initialized with random, if all layers were allowed to learn. This is a strong indicator that all datasets contain all atomic structures of MNIST. NIST, Char74-English and Char74-Kannada follow similar generalization trends with almost all the datasets. With no degrees of freedom they all generalize rather poorly, but their generalities shoot up once one or many layers of the base networks are unfrozen. This indicates two properties: Firstly, these three datasets have similar manifolds. Secondly this also indicates that the last layers of the base datasets are extracting some particular quality of atomic structures that are present in the these datasets alone. Similarly, SVHN does not generalize in the first layer to most datasets, it generalizes much better in the latter layers. This is particularly noticeable in MNIST and Kannada. This further exemplifies the results. While initially one would have assumed that Kannada would be a general dataset, we observed the contrary. SVHN, Char74-English and Nist generalizes better to Kannada than even Kannada itself does. English characters seem to be a more general set than Kananda. While counter-intuitive, this result is immediately obvious when one pays close attention to the filers that are learnt and

300

Error Curve - Base Network:mnist-rotated-bg ; Retrained onmnist
1200

Error Curve - Base Network:mnist-rotated-bg ; Retrained onmnist-bg-rand

1000

800

250

600

400
Base Network -mnist-bg-rand 1-unfrozen 2-unfrozen 3-unfrozen all-frozen

200

200

0 0 20 40 60 80 100 120 140 160 180 200

150
Base Network -mnist 1-unfrozen 2-unfrozen 3-unfrozen all-frozen

100

50

0

50

100

150

200

Figure 5. Validation errors vs Epoch number for base-MNISTrotated-bg retrained on MNIST

the dataset itself. Kannada is dominated by predominantly curved edges only, whereas even MNIST has a multitude of unique atomic structures. Figure 5 demonstrates some interesting phenomenon that we discovered often. The gain in performance achieved, constantly decreases with increase in degrees of freedom. Through the epochs, unfreezing only the classifier layer, quickly converges. But while unfreezing, all layers converge at about the same number of epochs. We also observe, that MNIST retrained over MNIST-rotatedbackground, with the last degree of freedom does not learn antything at all. The error rate is within the statistical margin of error. This is a testament to the generality of MNISTrotated-background among the MNIST datasets. One might expect this because MNIST-rotated-background contains smooth background images (similar to natural image set) and MNIST characters that are rotated. These conditions provide for a good generality. For the intra-class experiment described in subsection 3.2 above, table 2 shows the accuracies. From the table one can observe that even with one-sample per class,

Retrainedmnist for different bases
1.02 1 0.98 1.1 1 0.9

Retrainedmnist-bg-rand for different bases

Generality

Generality

0.8 0.7 0.6 0.5 0.4
mnist-bg-rand MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

0.96 0.94 0.92 0.9
mnist MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

0

1

2

3

0

1

2

3

1.1 1 0.9

Number of Layers Unfrozen Retrainedmnist-rotated-bg for different bases

Number of Layers Unfrozen
1 0.9 0.8

Retrainedchar74-english for different bases

Generality

Generality

0.8 0.7 0.6 0.5 0.4 0.3 0 1
mnist-rotated-bg MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

0.7 0.6 0.5 0.4 0.3 0.2 0 1
char74-english MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

2

3

2

3

Number of Layers Unfrozen Retrainedchar74-kannada for different bases
1.2 1 0.8

Number of Layers Unfrozen
1 0.9 0.8

Retrainedchar74-english for different bases

Generality

Generality

0.7 0.6 0.5 0.4 0.3
char74-english MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

0.6 0.4 0.2 0

char74-kannada MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

0

1

2

3

0.2

0

1

2

3

Number of Layers Unfrozen Retrained CALTECH101 for different bases
1.1 1 0.9
1.6 1.4 1.2

Number of Layers Unfrozen
Caltech 101 vs. Colonoscopy

Generalities

0.8 0.7 0.6 0.5 0.4 0.3 0 1 2 3 4 5 6
CALTECH101-base CALTECH101 CIFAR10

1 0.8 0.6 0.4 0.2
caltech 101 on caltech 101 caltech 101 on colonoscopy colonsoopy on colonoscopy colonoscopy on caltech101

0

1

2

3

4

5

6

Figure 4. Generalities of datasets not shown in the actual paper. The dark line represents the accuracy of n(D|r). Please zoom on a computer monitor for closer inspection.

a 7-way classifier could achieve 22% more accuracy than a randomly initialized network. It is note worthy that the

last row of table 2 still has 100 times less data than the full dataset and it already achieves close to state-of-the-art ac-

p 1 3 5 10 20 30 50

base Random MNIST[458] Random MNIST[458] Random MNIST[458] Random MNIST[458] Random MNIST[458] Random MNIST[458] Random MNIST[458]

k=0 73.07 83.61 90.98 91.55 95.52 96.5 96.38

k=1 73.91 87.2 92.98 93.71 95.52 97.34 97.40

k=2 76.37 85.7 92.6 93.82 97.07 97.35 97.71

k=3 55.61 77.52 73.34 87.6 83.32 92.07 81.31 95.08 87.77 96.78 88.62 97.45 90.78 97.38

4.2. CIFAR 10 vs. Caltech 101
From figure 4 we observe that Caltech 101 doesn't generalize to Cifar 10, which is surprising because Caltech 101 has a lot more classes. One would expect it to be more general. Its quite the opposite because Caltech 101 although has a lot of classes, the variability of each class is not as much as the variability in the Cifar 10 dataset. But it is altogether a serendipitous result that Cifar 10 is more general than Caltech 101 on the lower layers. However after three layers of obstination, we find that when the generalities crosses 1, the effect nullifies and reverses slightly. Even though the low-level features are more general in Cifar 10, Caltech 101 generalizes more on higher layers.

4.3. Caltech 101 vs. Colonoscopy
The colonoscopy dataset's labels identify if a image is deemed to be of a quality that is good enough so as to make a diagnosis on the pathology of that particular image. Figure 7 show the filters learnt by Caltech 101 base network and Colonoscopy base network for the exact same architecture from random initialization. Two things are immediately apparent from the learnt filters that while Caltech 101 learns more structured and organized shape features, Colonoscopy dataset learns at first sight what appears to be unstructured blob detectors and detectors for dark colors. These features still produce state-of-the-art accuracy on the dataset. On observation of the activations produced after the first layer, and from observations of images and their labels, one can immediately recognize that what the network is learning is indeed changes in brightness patterns. Most often the video quality in colonoscopy is affected because of saturation when too much light is thrown at a scene. The quality is also affected due to light reflection from bodily fluids that is also noticeable in the activations. As also can be noticed that most of the filter colors are yellowish or blueish. On an colonoscopy video most often the video is also labelled poor quality when these colors are present, as these colors are often present mostly because of scattering and reflections. Having made these observations one would arrive at the obvious conclusion that neither dataset generalizes the other. This was indeed the result observed from figure 4. Although, Caltech 101 seem to generalize a bit better for even though it predominantly learns shapes, it learns some color features also.

Table 2. Sub-sample experiment and its generalization accuracies for different layers of freezing. The re-train network was MNIST[0, 1, 2, 3, 6, 7, 9]. For obvious reasons random initializations are trained only with all layers unfrozen, hence the missing values.

Figure 6. Sub-class generalities for MNIST [4, 5, 8]

curacy even when no layer is allowed to change. This is a remarkably strong indicator that the classes [4, 5, 8] generalizes the entire dataset. Figure 6 mimics the same. We also observed that once initialized with a general enough subset of classes from within the same dataset, the generalities didn't vary among the layers like it did when we initialized with data from outside the mother dataset. We also observed that the more the data we used, more stable the generalities remained. Point of take away from this experiment is that if the classes are general enough, one may now initialize the network with only those classes and then learn the rest of the dataset even with very small number of samples.

4.4. Summary of results
From all these results and observations, we could summarize that one should prefer to initialize with a general dataset that might have a lot of variability or rather generality in data, when attempting to train with very few number of samples. Whenever possible one must initialize the network trained by a general dataset as this always boosts generalization performance. When there are biased datasets

Figure 7. From left to right, separated by a line are filters learnt by a base Caltech101 base colonoscopy, sample images from the colonoscopy dataset and their first activation for a filter that detects smooth areas of brightness.

with large number of samples in some classes and fewer in others, one should train the most general classes first. Once the network is well-prejudiced one should start introducing the classes with fewer number of and less general samples, provided the general class is general enough.

5. Conclusions
In this paper, we used the performance of CNNs on a dataset when initialized with the filters from other datasets as a tool to measure generality. We proposed a generality metric using these generalization performances. We used the proposed metric to compare popular character recognition datasets and found some interesting patterns and generality assumptions that add to the knowledge-base of these datasets. In particular, we noticed that MNIST data is one of the most specific dataset. We also found that Char74k Kannada is less general than English datasets. We also calculated generality on class-level within a dataset and conclude that a few well-chosen classes used as pre-training could build a network that is well-initialized that even with 100 times less samples, we could learn the other classes. We also provided some practical guidelines for a CNN engineer to adopt. After performing similar experiments on popular imaging datasets and medical datasets, we made similar serendipitous observations.

References
[1] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012. [2] Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for non-convex optimization. arXiv preprint arXiv:1502.04390, 2015. 5 [3] T. E. de Campos, B. R. Babu, and M. Varma. Character recognition in natural images. In Proceedings of the International Conference on Computer Vision Theory and Applications, Lisbon, Portugal, February 2009. 3, 5, 6

[4] V. Escorcia, J. C. Niebles, and B. Ghanem. On the relationship between visual attributes and convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1256­1264, 2015. 1 [5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303­ 338, 2010. 2 [6] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding, 106(1):59­70, 2007. 3, 5, 6 [7] K. Fukushima and N. Wake. Handwritten alphanumeric character recognition by the neocognitron. Neural Networks, IEEE Transactions on, 2(3):355­365, 1991. 1 [8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580­587. IEEE, 2014. 2 [9] P. J. Grother. NIST Special Database 19 Handprinted Forms and Characters Database. 1995. 6 [10] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 3 [11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 4, 5 [12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675­678. ACM, 2014. 2 [13] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images, 2009. 2, 3, 5, 6 [14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097­1105, 2012. 1 [15] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473­480. ACM, 2007. 3, 5, 6 Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541­551, 1989. 1 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998. 3, 5, 6 V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807­814, 2010. 5 Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5. Granada, Spain, 2011. 3, 5, 6 A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 427­436, 2015. 1 B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1­17, 1964. 5 O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), pages 1­42, April 2015. 2 K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 2 N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014. 5 C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014. 1 J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems, pages 3320­3328, 2014. 2, 4

The performance of action recognition in video sequences depends significantly on the representation of actions and the similarity measurement between the representations. In this paper, we combine two kinds of features extracted from the spatio-temporal interest points with context-aware kernels for action recognition. For the action representation, local cuboid features extracted around interest points are very popular using a Bag of Visual Words (BOVW) model. Such representations, however, ignore potentially valuable information about the global spatio-temporal distribution of interest points. We propose a new global feature to capture the detailed geometrical distribution of interest points. It is calculated by using the 3D  R  transform which is defined as an extended 3D discrete Radon transform, followed by the application of a two-directional two-dimensional principal component analysis. For the similarity measurement, we model a video set as an optimized probabilistic hypergraph and propose a context-aware kernel to measure high order relationships among videos. The context-aware kernel is more robust to the noise and outliers in the data than the traditional context-free kernel which just considers the pairwise relationships between videos. The hyperedges of the hypergraph are constructed based on a learnt Mahalanobis distance metric. Any disturbing information from other classes is excluded from each hyperedge. Finally, a multiple kernel learning algorithm is designed by integrating the  l2l2  norm regularization into a linear SVM classifier to fuse the  îˆ¾R  feature and the BOVW representation for action recognition. Experimental results on several datasets demonstrate the effectiveness of the proposed approach for action recognition.
Although several methods have been developed for protein-protein interaction (PPI) prediction, each method has a specialised emphasis, and it is often necessary to use multiple methods to avoid a high false-negative rate. We here describe a method that is based on binding profiles and only requires protein sequence as an input. We also developed an online platform, the PPI Prediction Platform (PPIPP), to predict PPI networks (PPINs). PPIPP, which is freely accessible at http://ppipp.songbx.me, provides two main functions: PPI prediction, which uses the binding profile method, domain-motif interactions from structural topology, and PPIN-based detection of functionally similar proteins within species. PPIPP offers a web-based interface to facilitate PPIN predictions and a high-performance server to overcome the problems of user access and large-scale computation. The wheat proteome was used to evaluate the performance of this platform.
In many Web image retrieval applications, adapting the retrieval results according to some model of the user is a desired feature as the returned images can be made specifically relevant to a userâ€™s needs. Making retrieval user-adaptive faces several practical challenges, including the ambiguity of user query, the lack of user-adaptive training data, and lack of proper mechanisms for supporting adaptive learning. To address some of these challenges, we propose a hybrid learning strategy that fuses knowledge from both pointwise and pairwise training data into one framework for attribute-based, user-adaptive image retrieval. An online learning algorithm is developed for updating the ranking performance based on user feedback. The framework is also derived into a kernel form allowing easy application of kernel techniques. We use both synthetic and real-world datasets to evaluate the performance of the proposed algorithm. Comparison with other state-of-the-art approaches suggests that our method achieves obvious performance gains over ranking and zero-shot learning. Further, our online learning algorithm was found to be able to deliver much better performance than batch learning, given the same elapsed running time.
The objective of this study was to improve individual tree crown delineation by fully exploiting the crown information exhibited in multi-wavelength LiDAR data. The data used in this study were obtained by an Optech's Titan instrument with three wavelengths: 532 nm, 1064 nm, and 1550 nm. Methods were developed to employ both spectral and structural information of tree crowns to separate crowns from other cover types and from each other. The methods were tested using a data set obtained over a study area in Toronto, Ontario, Canada. Preliminary results show that with both the canopy height model and intensities corresponding to the three wavelength, trees were distinguished from other cover types with a high accuracy and trees were separated from each other with a reasonable accuracy (based on visual observation).
With the advent of progressive format display and broadcast technologies, video deinterlacing has become an important video-processing technique. Numerous approaches exist in the literature to accomplish deinterlacing. While most earlier methods were simple linear filtering-based approaches, the emergence of faster computing technologies and even dedicated video-processing hardware in display units has allowed higher quality but also more computationally intense deinterlacing algorithms to become practical. Most modern approaches analyze motion and content in video to select different deinterlacing methods for various spatiotemporal regions. We introduce a family of deinterlacers that employs spectral residue to choose between and weight control grid interpolation based spatial and temporal deinterlacing methods. The proposed approaches perform better than the prior state-of-the-art based on peak signal-to-noise ratio, other visual quality metrics, and simple perception-based subjective evaluations conducted by human viewers. We further study the advantages of using soft and hard decision thresholds on the visual performance.
Employing correlation among images for improved reconstruction in compressive sensing is a conceptually attractive idea, although developing efficient modeling strategies and reconstruction algorithms are often the key to achieve any potential benefit. This paper presents a novel modeling strategy and an efficient reconstruction algorithm for processing a set of correlated images, jointly taking into consideration inter-image correlation, intra-image correlation and inter-channel correlation. The approach starts with joint modeling of the entire image set in the gradient domain, which supports simultaneous representation of local smoothness, nonlocal self-similarity of every single image, and inter-image correlation. Then an efficient algorithm is proposed to solve the joint formulation, using a Split-Bregman-based technique. Furthermore, to support color image reconstruction, the proposed algorithm is extended by using the concept of group sparsity to explore inter-channel correlation. The effectiveness of the proposed approach is demonstrated with extensive experiments on both grayscale and color image sets. Results are also compared with recently proposed compressive sensing recovery algorithms.
Hierarchical Attention Network for Action Recognition in Videos

Yilin Wang Arizona State University ywang370@asu.edu

Suhang Wang Arizona State University suhang.wang@asu.edu Yi Chang Yahoo Research yichang@yahoo-inc.com

Jiliang Tang Yahoo Research jlt@yahoo-inc.com Baoxin Li Arizona State University baoxin.li@asu.edu

arXiv:1607.06416v1 [cs.CV] 21 Jul 2016

Neil O'Hare Yahoo Research nohare@yahoo-inc.com

Abstract
Understanding human actions in wild videos is an important task with a broad range of applications. In this paper we propose a novel approach named Hierarchical Attention Network (HAN), which enables to incorporate static spatial information, short-term motion information and long-term video temporal structures for complex human action understanding. Compared to recent convolutional neural network based approaches, HAN has following advantages ­ (1) HAN can efficiently capture video temporal structures in a longer range; (2) HAN is able to reveal temporal transitions between frame chunks with different time steps, i.e. it explicitly models the temporal transitions between frames as well as video segments and (3) with a multiple step spatial temporal attention mechanism, HAN automatically learns important regions in video frames and temporal segments in the video. The proposed model is trained and evaluated on the standard video action benchmarks, i.e., UCF-101 and HMDB-51, and it significantly outperforms the state-of-the arts.

1

Introduction

Understanding human actions in wild videos can advance many real-world applications such as social activity analysis, video surveillance and event detection. Earlier works typically rely on hand craft features to represent videos [14, 19]. They often consist of two steps: motion detection and feature extraction. First, motion detectors are applied to detect the informative motion regions in the videos and then, hand craft descriptors such HOG [3], SIFT, or improved Dense Trajectories (iDT) [19] extract the feature patterns from those motion regions to represent the video. In contrast to hand-craft shallow video representation, recent efforts try to learn video representation automatically from large scale labeled video data [10, 16, 4, 20, 8]. For example, In [10], authors stack the video frames as the input for convolution neural networks (CNN) and two stream CNNs [16] combine optical flow and RGB video frames to train CNN and achieve comparable results with the state-of-the art hand craft based methods. Very recently, dense trajectory pooled CNN that combines iDT and two stream CNNs via the pooling layer achieves the state-of-the-art performance. However, [16] and [20] merely use short term motions that cannot capture the order of motion segments and semantic meanings. The challenges of action recognition in wild videos are three-fold. First, there are large intra-class appearance and motion variances in the same action due to different viewpoints, motion speeds, backgrounds, etc. Second, wild videos are often collected from movies, TV shows and media platforms and usually have very low resolutions and noise background clutters, which exacerbate the difficulty for video understanding. Third, long range temporal dependencies are very difficult to capture. For example, the Optical Flow, iDT and 3D ConvNets [8] are computed within a short-time
2016 ( May), Arizona State University, AZ.

Figure 1: An Illustration of the Proposed Model. The block LSTM means a LSTM cell, whose structure is given in Figure 2. The block ATTN indicates the operation to calculate attention weights by using the encoded features from both LSTMs. The block WA represents the weighted average of the input features with the weights from ATTN. window. Long Short Term Memory (LSTM) has been recently applied to video analysis [4] that provides a memory cell for long temporal information. However, it has been shown that the favorable time range of LSTM is around 40 frames [15, 22]. In this work, we aim to develop a novel framework to tackle these obstacles for action recognition in videos. In this paper, we study the problem of video representation learning for action recognition. In particular, we investigate ­ (1) how to utilize the temporal structures in the video to handle intraappearance variances and background clutters by capturing the informative spatial regions; and (2) how to model the short-term as well as long-term motion dependencies for action recognition. Providing answers to these two research questions, we propose a novel Hierarchical Attention Network (HAN) that employs a hierarchical structure with recurrent neural network unit e.g., LSTM and a soft spatial temporal attention mechanism for video action recognition. Our contributions can be summarized as below: · We propose a novel deep learning framework HAN for video action recognition which can explicitly capture both short-term and long-term motion information in an end to end process. · A soft attention is adopted on the spatial-temporal input features with LSTM to learn the important regions in a frame and the crucial frames in the videos. · We conduct an extensive set of experiments to demonstrate that the proposed framework HAN is superior to both state-of-the art shallow video representation based approaches and deep video representation based approaches on benchmark datasets. The rest of this paper is organized as follows. Section 2 reviews related works. Section 3 describes the proposed hierarchical attention deep learning framework in detail. Experimental results and comparisons are discussed in Section 4, followed by conclusions in Section 5.

2

Related Work

Hand crafted features: Video action recognition is a longstanding topic in computer vision community. Many hand-crafted features are used in still images. For example, [14] extends 2D harri corner detector to spatial temporal 3D interest points and achieves good performance with SVM classifier. Later, HOG3D that is based on HOG feature [3] shows its effectiveness by using integral images. 2

Then improved Dense trajectories [19] has dominated the filed of action recognition. It densely samples interest points and tracks them within a short time period. For each point, local descriptors such as HOG, MBH and HOF are extracted for representation. Then, all the features are encoded by the fish vector as the final video representation. Deep learned features: Deep learning such as convolutional neural network has been shown its success in object detection and image classification in recent years. Based on image CNN, [8, 10] extends the CNN framework to videos by stacking video frames. However, the performance is lower than iDT based approaches. In order to better incorporate temporal information, [16] proposes two stream CNNs and achieves comparable results with the state-of-the art performance of hand craft based feature representations [19]. To consider the time dependency, [22] proposes a LSTM based recurrent neural network to model the video sequences. Different from [22] that uses a stack based LSTM, the proposed HAN proposes a hierarchical structure to model the video sequences in a multi-scale fashion. Recurrent visual attention model: Visual attention model aims to capture the property of human perception mechanism by identifying the interesting regions in the images. Saliency detection [7] typically predicts the human eye movement and fixations for a given image. In [21], a recurrent is proposed to understand where the model focuses on image caption generations. Moreover, recurrent attention model has been applied to other sequence modeling such as machine translation [12] and image generation[6].

3

The Proposed Method

The overall architecture of HAN is shown in 1. We describe the three major components of HAN in this section ­ the input appearance and motion CNN feature extraction, temporal sequence modeling and hierarchical attention model. 3.1 Appearance and Motion Feature Extraction

In general, HAN can adopt any deep convolution networks [8, 16, 20] for feature extraction. In this paper, we use two stream ConvNets [16] to extract both appearance and motion features. Specifically, we train a VGG net [17] to extract feature map fP and fQ for the t-th frame image P t and the corresponding optical flow image Qt : fP t = CN Nvgg (P t ) fQt = CN Nvgg (Qt ) (1)

Unlike two stream ConvNets [16] that employs the last fully connected layer as the input feature, we use the features for fP and fQ from the last convolutional layer after pooling, which contains the spatial information of the input appearance and motion images. The input appearance and motion images are first rescaled to 224 × 224 and the extracted feature maps from the last pooling layer have the dimension of D × K × K (512 × 14 × 14 used in VGG net). K × K is the number of regions in the input image and D is the number of the feature dimensions. Thus at each time step, we extract K 2 D dimension feature vectors for both appearance and motion images. We refer these feature vectors as feature cube shown in Figure 3. Then, the feature maps fP t and fQ can be denoted in matrix forms t t D ×K 2 t t D ×K 2 as Pt = [pt and Qt = [qt , respectively. 1 , p2 , . . . , pK 2 ]  R 1 , q2 , . . . , qK 2 ]  R 3.2 Recurrent Neural Network

Long-short term memory (LSTM), which has the ability to preserve sequence information over time and capture long-term dependencies, has become a very popular model for sequential modeling tasks such as speech recognition [5], machine translation [1] and program execution [24]. Recent advances in computer vision also suggest that LSTM has potentials to model videos for action recognition [15]. We follow the LSTM implementation in [25], which is given as follows 3

Figure 2: Illustration of One LSTM Cell

it ft ot gt ct ht

=  (Wix xt + Wih ht-1 + bi ) =  (Wf x xt + Wf h ht-1 + bf ) =  (Wox xt + Woh ht-1 + b + o) = tanh(Wgx xt + Wgh ht-1 + bg ) = ft ct-1 + it gt = ot tanh(ct )

(2)

where it is the input gate, ft is the forget gate, ot is the forget fate, ct is the memory cell state at t and xt is the input features at t.  (·) means the sigmoid function and denotes the Hadmard product. The main idea of the LSTM model is the memory cell ct , which records the history of the inputs observed up to t. ct is a summation of ­ (1) the previous memory cell ct-1 modulated by a sigmoid gate ft , and (2) gt , a function of previous hidden states and the current input modulated by another sigmoid gate it . The sigmoid gate ft is to selectively forget its previous memory while it is to selectively accept the current input. it is the gate controlling the output. The illustration of a cell of LSTM at the time step t is shown in Figure 2. Next we will introduce how to model both appearance and motion features using LSTM and how to integrate attention by using encoded features. 3.3 Hierarchical LSTMs to Capture Temporal Structures

One natural way of modeling videos is to feed features Pt and Qt into two LSTMs and then put a classifier at the output of LSTMs for classification. However, this straightforward method doesn't fully utilize the structure of actions. In real world, an action is usually composed of a set of subactions, which means that video temporal structures are intrinsically layered. For example, a video about long jump consists of three sub-actions ­ pushing off the board, flying over the pit and landing. As the three actions take place sequentially, there are strong temporal dependencies among them thus we need to appropriately model the temporal structure among the three actions. In the meantime, the temporal structure within each action is composed of multiple actions. For example, pushing off the board is composed of running and jump. In other words, the actions we want to recognize are layered and we need to model video temporal structure with multiple granularities. However, directly applying LSTM cannot capture this property. To fully capture the video temporal structure, we develop a hierarchical LSTM. An illustration of the hierarchical LSTM is shown in the purple rectangle in Figure 1. This hierarchical LSTM is composed of two layers ­ the first layer accepts the appearance feature of each frame as the input and the output of the first layer LSTM is used as the input of the second layer LSTM. To capture the dependencies between different sub-actions such as dependencies between pushing off the board, flying over the pit and landing, we skip every k 4

encoded features from LSTM and use that as the input to the second layer. In addition to capturing the video temporal structure, another advantage of layered LSTM is to increase the learning capability of LSTM. By adding another layer in LSTM, we allow LSTM to learn higher level and more complex features, which is a common practice proven to work well in other deep architectures such as CNN, DNN and DBM. Thus, as shown in Figure 1, we use two hierarchical LSTMs to model the appearance and motion features, respectively. 3.4 Attention Model to Capture Spatial Structures

Figure 3: attention The K 2 vectors in the appearance Pt (or motion features Qt ) correspond to K 2 regions in the t-th frame, which essentially encode the spatial structures. For action recognition, not every region of the frames are relevant for the task at hand. Obviously, we want to focus on the regions where the action is happening. For the action shown in Figure 3, we want to mainly focus on hands and legs that are useful for identifying the action; while the background is noisy as a person can perform the same action at different locations. Therefore we could confuse the classifier if we also target on backgrounds. Thus, it is natural for us to assign different attention weights to different regions of the frame. Since video frames are sequential, neighboring frames have strong dependencies, which suggests that we can use the encoded features at time t - 1 to predict the attention weights at time t and then use the attention weights to refine the input. Specifically, at each time step t - 1, we use a softmax function over K × K locations to predict the importance of the K 2 locations in the frame, which is written as: T exp(wi ht-1 ) t li = K2 (3) T j =1 exp(wj ht-1 )
t where li is the importance weight of the i-th region of the t-th frame, W = {w1 , w2 , . . . , wK 2 }  1 q,1 2D ×K 2 R are the weights of the softmax function and ht-1 is the concatenation of hp, t-1 and ht-1 , i.e., the encoded appearance and motion features of the (t - 1)-th frame from the first layer LSTM. Note that we use the encoded appearance feature and motion feature jointly to compute the attention weights instead of computing two attention weights by using the two features, separately. Its advantages are two fold. First, the flow and appearance features capture different aspects of the frame but the

5

attention location on the video should be the same, thus we do not need to calculate two sets of attentions for appearance and optical LSTM separately, which may introduce more computational cost. Second, appearance and motion features provide complimentary information that may help predict more accurate attention. With the attention weights given above, the inputs of the two LSTMs are the weighted average of different locations as:
K2 K2 t t li pi and xq t = i=1 i=1 t t li qi

xp t = 3.5 Action Recognition with HAN

(4)

We use hp,i T to denote the encoding of the the video by the i-the layer LSTM for the appearance features and hq,i T for motion features. As mentioned above, the hierarchical LSTM captures multigranularity of video temporal structures, thus, encoded features in different levels (different i) provide distinct descriptions of different granularity about actions, which are all useful for action recognition. In addition, the two LSTMs encode complementary information from appearance and motion features, thus encoded features from appearance and motion are also relevant for action recognition. Therefore, 1 p,L q,L q,L we concatenate these features as hf = [hp, T , . . . , hT , hT , . . . , hT ], where L is the number of layers in each LSTM. We then use the softmax function to predict the probability that the video vi is classified into the class c as p(c|vi , HAN) = softmax(HAN(vi )) and the loss function is
HAN,Ws N

(5)

max

log p(yi |vi , HAN)
i=1

(6)

where N is number of videos, yi is the label of vi and Ws are the weights of the softmax classifier.

4

Experiments

In this section, we first present the details of datasets and the evaluation protocol. Then, we describe the details of the implementation of our method. Finally, we present the experimental results with discussions. 4.1 Datasets and evaluation protocol

The evaluation is conducted on two public benchmark datasets, i.e., UCF-101 [18] and HMDB51[11]. These two datasets are among the largest available annotated video action recognition datasets that have been used in [10, 15, 16, 19, 20]. Specifically, UCF-101 contains 13, 000 videos annotated into 101 action classes with each class having at least 100 videos. HMDB51 is composed of 6, 700 videos from 51 action categories and each category has at least 100 video clips. For both datasets, the evaluation protocol is the same ­ we follow the train/test splits provided by the corresponding organizers. The performance is measured by the mean of accuracies across all the splits in each dataset. 4.2 Experiments Setting

Training two stream CNNs and HANs: Compared to image classification and detection, training a good deep convolutional neural network for videos understanding is more challenging. Similar to [20, 16], we use the training data in UCF 101 split to train two stream CNNs. In our implementation, we use the Caffe toolbox [9] and the layer configuration is the same as [17]. All hidden layers use the rectification activation functions and max pooling is performed over 2 × 2. Finally, each of the two networks contains 13 convolutional layers and 3 fully connected layers. The training procedure is similar to [20, 16], where we use mini-batch stochastic gradient descent with momentum (0.9). The learning rate is initially set to 10-2 , then changed to 10-3 after 10, 000 iterations and stopped after 30, 000 iterations and 10, 000 iterations for spatial and temporal nets, respectively. We use the Theano toolbox for HAN implementation and the model is trained by using Adadelta [26]. The 6

dimension of LSTM is 1, 024 and the batch size is fixed to 128 . Techniques of dropout [2] and BPTT are used. Optical flow: The optical flow is computed by the off-the-shelf OpenCV toolbox with GPU implementation of [23]. Since the computational cost of optical flow is the bottleneck for the two stream CNN training. We pre-computed all the optical flow images and stored the horizontal and vertical components. The optical flow is computed by the adjacent two frames. In the testing stage, we fix the number of frames with the equal temporal window between them. 4.3 Results and Analysis

We compare our models with a set of baselines proposed recently [15, 22, 16, 20, 10, 13] including shallow video representation methods and deep ConvNets methods. We first evaluate our proposed Table 1: Average accuracy over three splits on UCF-101 and HMDB51 Model Full HAN (spatial CNN cube+temporal CNN cube) HAN without attention1 (spatial CNN cube +temporal CNN cube) HAN without attention2 (spatial CNN 4096+ temporal CNN 4096) Spatial HAN (spatial CNN cube) Temporal HAN (temporal CNN cube) UCF-101 92.7% 90.6% 91.1% 75.1% 85.4% HMDB51 64.3% 62.0% 62.7% 47.7% 58.3%

HAN on UCF-101 and HMDB51 datasets by comparing HAN with different settings to show the importance of each key component in HAN in Table 1. Then, we further compare HAN with state-of-the art methods and experimental results are reported in Table 2. From the tables, we can make the following observations: · The proposed method with hierarchical LSTM outperforms methods without hierarchical structures [22, 10, 16]. These results support that (1) the usage of LSTM can capture video sequences by considering the order of the motion transitions; and (2) the proposed hierarchical structure can effectively model the complex and long time range actions in videos. · Compared with methods without the attention components, the proposed HAN encourages the model to focus on the important regions in frames during the learning process, which improves the discriminative ability for classification. For example, in Figure 4 (b) and Figure 4 (e), we can see that our model can learn the important regions for actions more accurately. · The temporal and spatial features are complementary. First, by combining them together, both of them have been improved significantly. Second, compared with [15] that only considers attention in spatial, HAN can predict more motion related regions in the videos. Third, compared to TDD, the proposed HAN achieves comparable results without considering the iDT information, which suggests that the learned attention regions can have the similar ability to dense trajectory points and reduce the negative impact of background noises. · Compared to state-of-the art methods on UCF and HMDB51, HAN outperforms them remarkably except [20]. The major reason for the exception is that the dataset HMDB is relatively small and the content is unconstrained, while the method in [20] incorporates iDT features that are computationally expensive.

5

Conclusion and Future Work

In this paper, we propose a hybrid deep framework by incorporating a hierarchical structure and joint attention model to the two stream convnet approach for human action recognition. The experimental results suggest that the proposed framework outperforms the two stream convnet approach. Despite using the only optical flow images as input, HAN achieves comparable performance with the state-ofthe art method TDD that is much more computationally expensive. These results further support that (1) the hierarchical structure in HAN is important because it can model the frame transitions as well 7

(a) The sampled frame

(b) Attention results from HAN (c) Attention results from [15]

(d) The sampled frame

(e) Attention results from HAN (f) Attention results from [15]

Figure 4: Visual attention comparison between HAN and soft attention model in [15], the green and red circles highlight the most important region learned by HAN and [15] respectively. Table 2: Comparison with state of the art methods on UCF101 and HMDB51. Model Histogram of Oriented Gradient Improved dense trajectories (iDT) [19] iDT + Stack Fish Vector [13] spatial-temporal CNN [10] two stream CNN [16] two stream CNN+LSTM [22] two stream CNN + iDT [20] Soft Attention +LSTM [15] Hierarchical Attention Networks UCF-101 72.4 % 85.9% N/A 65.4% 88.0% 88.6% 91.5% 84.96% 92.7% HMDB51 40.2% 57.2% 66.8% N/A 59.4% N/A 65.9% 41.3% 64.3%

as long video segments and (2) the joint visual attention can help HAN focus on the important video regions and reduce the effect of noisy background. HAN is powerful in sequence modeling thus we would like to explore more applications for HAN in the future such as video event detection since a video event usually contains many sub-events and these sub-events have high dependencies to each. References [1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR, 2015. [2] George E Dahl, Tara N Sainath, and Geoffrey E Hinton. Improving deep neural networks for lvcsr using rectified linear units and dropout. In ICASSP, pages 8609­8613. IEEE, 2013. [3] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In CVPR. IEEE, 2005. [4] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, pages 2625­2634, 2015. 8

[5] Alan Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech recognition with deep bidirectional lstm. In ASRU Workshop, pages 273­278. IEEE, 2013. [6] Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. Draw: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015. [7] Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid scene analysis. TPAMI, (11):1254­1259, 1998. [8] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action recognition. TPAMI, 35(1):221­231, 2013. [9] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Multimedia, pages 675­678. ACM, 2014. [10] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In CVPR, 2014. [11] Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In ICCV. IEEE, 2011. [12] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015. [13] Xiaojiang Peng, Changqing Zou, Yu Qiao, and Qiang Peng. Action recognition with stacked fisher vectors. In Computer Vision­ECCV 2014, pages 581­595. Springer, 2014. [14] Christian Schüldt, Ivan Laptev, and Barbara Caputo. Recognizing human actions: a local svm approach. In ICPR, volume 3, pages 32­36. IEEE, 2004. [15] Shikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Action recognition using visual attention. In Proceedings Workshop of ICLR, 2015. [16] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In NIPS, pages 568­576, 2014. [17] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [18] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [19] Heng Wang and Cordelia Schmid. Action recognition with improved trajectories. In CVPR, pages 3551­3558, 2013. [20] Limin Wang, Yu Qiao, and Xiaoou Tang. Action recognition with trajectory-pooled deepconvolutional descriptors. In CVPR, pages 4305­4314, 2015. [21] Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015. [22] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, and George Toderici. Beyond short snippets: Deep networks for video classification. In CVPR, pages 4694­4702, 2015. [23] Christopher Zach, Thomas Pock, and Horst Bischof. A duality based approach for realtime tv-l 1 optical flow. In Pattern Recognition, pages 214­223. Springer, 2007. [24] Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014. [25] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014. [26] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. 9

Unsupervised Sentiment Analysis for Social Media Images
Yilin Wang, Suhang Wang, Jiliang Tang, Huan Liu, and Baoxin Li Arizona State University Tempe, Arizona {yilin.wang.1, suhang.wang, jiliang.tang, huan.liu, baoxin.li}@asu.edu

Abstract
Recently text-based sentiment prediction has been extensively studied, while image-centric sentiment analysis receives much less attention. In this paper, we study the problem of understanding human sentiments from large-scale social media images, considering both visual content and contextual information, such as comments on the images, captions, etc. The challenge of this problem lies in the "semantic gap" between low-level visual features and higher-level image sentiments. Moreover, the lack of proper annotations/labels in the majority of social media images presents another challenge. To address these two challenges, we propose a novel Unsupervised SEntiment Analysis (USEA) framework for social media images. Our approach exploits relations among visual content and relevant contextual information to bridge the "semantic gap" in the prediction of image sentiments. With experiments on two large-scale datasets, we show that the proposed method is effective in addressing the two challenges.

1

Introduction

Recent years have witnessed the explosive popularity of image-sharing services such as Flickr1 and Instagram2 . For example, as of 2013, 87 millions of users have registered with Flickr3 . Also, it was estimated that about 20 billion Instagram photos are shared to 20144 . Since by sharing photos, users could also express opinions or sentiments, social media images provide a potentially rich source for understanding public opinions/sentiments. Such an understanding may in turn benefit or even enable many real-world applications such as advertisement, recommendation, marketing and health-care. The importance of sentiment analysis for social media images has thus attracted increasing attention recently [Yang et al., 2014; You et al., 2015].
www.flickr.com www.instagram.com 3 http://en.wikipedia.org/wiki/Flickr 4 http://blog.instagram.com/post/ 80721172292/200m
2 1

Current methods of sentiment analysis for social media images include low-level visual feature based approaches [Jia et al., 2012; Yang et al., 2014], mid-level visual feature based approaches [Borth et al., 2013; Yuan et al., 2013] and deep learning based approaches [You et al., 2015]. The vast majority of existing methods are supervised, relying on labeled images to train sentiment classifiers. Unfortunately, sentiment labels are in general unavailable for social media images, and it is too labor- and time-intensive to obtain labeled sets large enough for robust training. In order to utilize the vast amount of unlabeled social media images, an unsupervised approach would be much more desirable. This paper studies unsupervised sentiment analysis. Typically, visual features such as color histogram, brightness, the presence of objects and visual attributes lack the level of semantic meanings required by sentiment prediction. In supervised case, label information could be directly utilized to build the connection between the visual features and the sentiment labels. Thus, unsupervised sentiment analysis for social media images is inherently more challenging than its supervised counterpart. As images from social media sources are often accompanied by textual information, intuitively such information may be employed. However, textual information accompanying images is often incomplete (e.g., scarce tags) and noisy (e.g., irrelevant comments), and thus often inadequate to support independent sentiment analysis [Hu and Liu, 2004; Hu et al., 2013b]. On the other hand, such information can provide much-needed additional semantic information about the underlying images, which may be exploited to enable unsupervised sentiment analysis. How to achieve this is the objective of our approach. In this paper, we study unsupervised sentiment analysis for social media images with textual information by investigating two related challenges: (1) how to model the interaction between images and textual information systematically so as to support sentiment prediction using both sources of information, and (2) how to use textual information to enable unsupervised sentiment analysis for social media images. In addressing these two challenges, we propose a novel Unsupervised SEntiment Analysis (USEA) framework, which performs sentiment analysis for social media images in an unsupervised fashion. Figure 1 schematically illustrates the difference between the proposed unsupervised method and existing supervised methods. Supervised methods use label informa-

tively. Let C = {c1 , c2 , . . . , ck } be the set of sentiment labels. Note that in this work we only consider positive, neutral and negative sentiments with k = 3 but the generalization of the proposed framework to multi-class sentiment analysis is straightforward. With the aforementioned notations/definitions, the problem of unsupervised sentiment analysis for social media images with textual information is formally defined as: Given n images with visual information Xv and textual information Xt , to predict sentiment labels in C for the given n images.
(a) Supervised Sentiment Analysis.

3

Unsupervised Sentiment Analysis for Social Media Images

In this section, we first present our method for exploiting text information and then introduce the unsupervised sentiment analysis framework with an optimization method.

3.1

Exploiting Textual Information

(b) The Proposed Unsupervised Sentiment Analysis.

Figure 1: Sentiment Analysis for Social Media Images. tion to learn a sentiment classifier; while the proposed method does not assume the availability of label information but employ auxiliary textual information. Our main contribution can be summarized as below: · A principled approach to enable unsupervised sentiment analysis for social media images. · A novel unsupervised sentiment analysis framework USEA for social media images, which captures visual and textual information into a unifying model. To our best knowledge, USEA is the first unsupervised sentiment analysis framework for social media images; and · Comparative studies and evaluations using datasets from real-world social media image-sharing sites, documenting the performance of USEA and leading existing methods, serving as benchmark for further exploration.

Without label information, it is challenging for unsupervised sentiment analysis to connect visual features with sentiment labels. Textual information associated with social media images may be exploited to help, as it provides semantics about the underly images and in particular rich sentiment signals such as sentiment words and emotion symbols may be found in the textual fields. Hence, to exploit textual information, we investigate (1) how to incorporate textual information into visual information; and (2) how to model sentiment signals in textual information. Since visual and textual information are two views about the same set of images, it is reasonable to assume that they share the same sentiment label space. More specifically, the sentiment of Ii should be consistent with that of its associated textual information pi . Let U0  Rn×k be the sentiment label space where U0 (i, j ) = 1 if the i-th data instance belongs to cj , and U0 (i, j ) = 0 otherwise. We propose the following formulation to incorporate visual information with textual information based on nonnegative matrix factorization:
T min Xv - Uv Vv UV 2 F 2 T +  X t - Ut V t 2 2 F

2

Problem Statement

In this paper, scalars are denoted by lower-case letters (a, b, . . . ; ,  , . . .), vectors are written as lower-case bolded letters (a, b, . . .), and matrices correspond to boldfaced uppercase letters (A, B, . . .). Let I = {I1 , I2 , . . . , In } be the set of images where n is the number of images. We use P = {p1 , p2 , . . . , pn } to denote associated textual information about images where pi is the textual information about Ii . Let Fv be set of mv visual features and Ft be set of mt textual features. We use Xv  Rn×mv and Xt  Rn×mt to denote visual and textual information about images, respec-

+  ( U v - U 0 F + Ut - U0 F ) subject to Uv  0, Ut  0; ||U0 (i, :)||0 = 1, i  {1, 2, ..n} U0 (i, j )  {0, 1} j  {1, 2, ..k } (1) where  controls how textual information contributes to the model and ||·||0 is 0 , which counts the number of nonzero entries in the vector. Uv  Rn×k and Ut  Rn×k are the sentiment label spaces learned from visual information and tex2 tual information, respectively. The term of  ( Uv - U0 F + 2 Ut - U0 F ) ensures that these two types of information should share the sentiment label space U0 . Vv  Rmv ×k and Vt  Rmt ×k indicate the sentiment polarities of visual and textual features, respectively. Textual information contains rich sentiment signals. First, some words may contain sentiment polarities. For example,

some words are positive such as "happy" and "terrific"; while others are negative such as "gloomy" and "disappointed". The sentiment polarities of words can be obtained via some public sentiment lexicons. For example, the sentiment lexicon MPQA contains 7,504 human labeled words which are commonly used in the daily life with 2,721 positive words and 4,783 negative words. Second, some abbreviations and emoticons are strong sentiment indicators. For example, "lol"(means laughing out loud) is a positive indicator while ":(" is a negative indicator. Let Vt0  Rmv ×k be the matrix coding sentiment signals in textual information where Vt0 (i, j ) = 1 if i-th word belongs to cj and Vt0 (i, j ) = 0 otherwise. To model sentiment signals, we force the learned sentiment polarities of textual features to be consistent with those indicated by sentiment signals. Furthermore, not all textual features in Ft contain sentiment polarities and Vt should be sparse. We propose the following formulation to achieve these two goals as: min Vt - Vt0
2,1

3.3

An Optimization Method

There are 5 components, i.e. Uv , Vv , Ut , Vt and U0 , in Eq. (4). Thus it is difficult to optimize all the components simultaneously. In the following parts, we demonstrate an alternating algorithm to optimize the objective function by updating each component iteratively. Update Vt : If U0 , Uv , Vv and Ut are fixed, then the objective function is decoupled and the constrains are independent of Vt . Thus we can optimize Vt separately and ignore the term without Vt , leading to the following:
T minJ (Vt ) = Xt - Ut Vt Vv 2 F

+  V t - V t0

2 F

(5)

 where  =  . Taking the derivation of J (Vt ) and setting it to zero, we can obtain the following form: T (-XT (6) t Ut + Vt Ut Ut ) +  Dt (Vt - Vt0 ) = 0 where Dt is a diagonal matrix with j th element on the di1 agonal D(j, j ) = 2 Vt (j,:)- Vt 0(j,:) . In Eq. (6), solving

(2)

X 2,1 is the 2,1 of the matrix X, which ensures the row sparsity of X [Nie et al., 2010]. The significance of textual information in unsupervised sentiment analysis for social media images is two-fold. First, textual information bridges the semantic gap between visual features and sentiment labels. Second, we are allowed to do sentiment analysis for social media images in an unsupervised scenarios by modeling textual information via Eqs. (1) and (2).

Vt directly is intractable. Since Dt and UT t Ut are symmetric and positive definite, we employ eigen decomposition for them as: T UT t U t = U1  1 U1 (7) D t = U2  2 UT 2 where U1 , U2 are eigen vectors and 1 , 2 are diagonal matrices with eigen values on the diagonal. Substituting UT t Ut and Dt in Eq. (6), we have:
T T Vt U1 1 UT (8) 1 +  U2 2 U2 Vt = Xt Ut +  Dt Vt0 T Multiplying U2 and U1 from left to right on both sides: T T UT 2 Vt U1 1 +  2 U2 Vt U1 = U2 (Xt Ut +  Dt Vt0 )U1 (9) T T Let Vt = U2 Vt U1 and Q = U2 (Xt Ut +  Dt Vt0 )U1 , Eq. (9) becomes Vt 1 +  2 Vt = Q, then we can obtain the Vt and Vt as: Q(s, l) Vt (s, l) = s 2 + l (10) 1

2

3.2

The Framework: USEA

By combining the above discussion, we can have the following initial framework, which provides a potential solution to inferring sentiments by jointly considering visual information and corresponding contextual information:
T min Xv - Uv Vv UV 2 F 2 T +  X t - Ut V t 2 2 F

s.t.

+  ( Uv - U0 F + U t - U0 F ) (3) +  Vt - Vt0 2,1 Uv  0; Ut  0, ||U0 (i, :)||0 = 1, i  {1, 2, ..n} U0 (i, j )  {0, 1} j  {1, 2, ..k }

V t = U 2 V t UT 1
l where s 2 is the s-th eigen value of Dt and 1 is l-th eigen value of UT U . The following theorem shows that the updatt t ing rule in Eq(10) can monotonically decrease the objective function J (Vt ). Theorem 1. The update rule in Eq. (10) can monotonically decrease the value of J (Vt ) Proof. The proof is similar to that in [Nie et al., 2010], due to space limit, we omit the details of the proof. Update Vv . If U0 , Ut , Vt and Uv are fixed, by setting the derivation of the objective function to zero, Vv can be T -1 easily obtained as Vv = XT . Moreover, we v Uv (Uv Uv ) can easily verify updating Vv will monotonically decrease the objective function. Update Uv : If Vv , Ut , Vt and U0 are fixed, Uv can be obtained by the following optimization problem: T minJ (Uv ) = Xv - Uv Vv Uv 2 F

The parameter  controls the sparsity of regularization term. However, the constrains of U0 in Eq. (3), mixed vector zero norm with integer programming, make the problem difficult to solve. To tackle this problem, we consider the relaxation of U0 by adding the extra orthogonal constraint on the value of U0 . With the relaxation, the proposed framework (USEA) is to solve the following optimization problem:
T min Xv - Uv Vv UV 2 F 2 T +  X t - Ut V t 2 F) 2 F

s.t.

+  ( Uv - U0 F + Ut - U0 +  Vt - Vt0 2,1 Uv  0, Ut  0, UT 0 U0 = I ; U0  0

(4)

+  Uv - U0

2 F

(11)

s.t.

Uv  0

The Lagrangian function of Eq. (11) is :
T minL(Uv ) = Xv - Uv Vv Uv 2 F

+  Uv - U0

2 F

(12)

- T r(Uv ) where  is Lagrangian multiplier. Taking the deviation of J (Uv ) and using the KKT condition ((s, l)Uv (s, l) = 0), we can obtain:
T (-Xv Vv + Uv Vv Vv +  Uv -  U0 )sl (Uv )sl = 0 (13)

Update Ut : It is worth noting that the procedure of solving Ut is exactly the same as that of Uv . Thus, we omit the solution of Ut here. Update U0 : With Uv , Ut , Vt and Vv fixed, the sentiment label U0 can be obtained by solving the following optimization problem: minJ (U0 ) = Uv - U0
U 2 F

+ Ut - U0

2 F

s.t. UT 0 U0 = I ; U0  0 The Lagrangian function of Eq. (17) is: minJ (U0 ) = Uv - U0
U 2 F

(17)

which leads to the following update rule for Uv :
T V )- +  U ) ((Xv Vv )+ + Uv (Vv v 0 sl - T ((Xv Vv ) + Uv (Vv Vv )+ +  Uv )sl (14) where (X(s, l))+ = (|X(s, l)| + X(s, l))/2, (X(s, l))- = (|X(s, l)| - X(s, l))/2 and X = X+ - X- . Theorem 2. Let

(Uv )sl  (Uv )sl

+ Ut - U0

2 F

+ T r((UT 0 U0 - I )) - T r (U0 )

(18)

where  and  are Lagrangian multipliers. Taking the derivation of J (U0 ) and using KKT conditions we can obtain (U0 - Uv + U0 - Ut + U0 )sl (U0 )sl = 0 which leads the following update rule for U0 : (Uv + Ut + (U0 )- )sl ((U0 )+ + 2U0 )sl (19)

T T H (Uv ) = T r(-2Xv Vv UT v + U v V v V v Uv ) T + T r(-2UT v U0 + Uv U v )

h(Uv , Uv ) =
sl

((Xv Vv )- (s, l) Uv (s, l)U2 v (s, l) Uv (s, l)

2 U2 v (s, l) + Uv (s, l)

Uv (s, l)

(U0 )sl  (U0 )sl

(20)

+

T + ( Vv Vv )+ (s, l)

Uv (s, l)U2 v (s, l) Uv (s, l)

) Uv (s, l) Uv (s, l) )) )

-
sl

(2(Xv Vv )+ Uv (s, l)(1 + log

+ 2 U0 (s, l)Uv (s, l)(1 + log -
k,s,l

Uv (s, l) Uv (s, l)

T (Vv Vv )- (s, l)Uv (k, s)Uv (k, l)

(1 + log

Uv (k, s)Uv (k, l) Uv (k, s)Uv (k, l)

)

Note that updating U0 needs updating the Lagrangian multiplier  as well. To obtain , we sum over s and get T (s, s) = (UT 0 Uv - I + U0 Ut - I )s,s . The offdiagonal elements of  are approximately obtained from non-negative T value of U0 , leading to (s, t) = (UT 0 Uv - I + U0 Ut - I )st . Overall, we can obtain  by combining the diagonal values and off-diagonal values. With the update rules for all the components in the proposed model, we summarize the solution in Algorithm 1. The convergence of Algorithm 1 is demonstrated as below: Theorem 4.With Algorithm 1, the objective function Eq. (4) will converge. Proof From Theorem 1 and Theorem 2, the object function monotonically decreases:
0 1 0 1 1 2 1 J (Vv , U0 v )  J (Vv , Uv )  J (Vv , Uv )J (Vv , Uv )...  0 (21) Similarly, we can have the inequality chain for J (Vt , Ut ). Thus we complete the proof.

(15) The auxiliary function h(Uv Uv ) of H (Uv ) is convex and the global minimum of h(Uv , Uv )is:
v v v v 0 sl v (Uv )sl  (Uv )sl ((Xv T V )+ + U ) Vv )- +Uv (Vv v v sl Proof : The proof is similar to [Ding et al., 2006] and [Ding et al., 2010], due to space limit, we omit the details. Theorem 3. Updating Uv in Eq. (14) will monotonically decrease the value of objective function J (Uv ) Proof : H (Uv ) is the KKT condition of the Lagrangian function for Eq. (11). Based on the definition of auxiliary function and Theorem 2 we can obtain the following equations:

((X V )+ +U (VT V )- + U )

4

Experiments

In this section, we conduct experiments to answer the following questions - (1) can the proposed framework do sentiment analysis in an unsupervised scenario? and (2) how does the textual information affect the performance of the proposed framework? We begin by giving details about the experimental settings.

0 0 0 1 1 1 1 4.1 Experiment Settings H ( U0 v ) = h(Uv , Uv )  h(Uv , Uv )  h(Uv , Uv )  H (Uv )... (16) We collect datasets from Flickr and Instagram for this study This shows the update rule will monotonically decrease the and we give more details below, objective function H (Uv ), which complete the proof.

Algorithm 1 The proposed USEA Input: {Xv , Xt , Vt0 } , ,  Output: k sentiment label for each data instance. Initialization: Ut , Uv , Vv , Vt while Not Converge do Update Vt using Eq.(10) and compute Vv = T -1 XT U . v ( Uv Uv ) v T Computing (Xv Vv )+,- , (Xt Vt )+,- , (Vv Vv )+,- T +,- and (Vt Vt ) Update Uv using Eq. (14), similarly update Ut Computing  Update U0 End Using max-pooling for U0 to predict sentiment labels. Flickr: On Flickr, an image-hosting Website, users can provide tags and descriptions for each uploaded image. Thus the textual information could be comments, image caption, user profile and tags. The collection of Flickr dataset is based on the image id provided by [Yang et al., 2014], which contains 350,4192 images from 4807 users. Some images are unavailable when we crawled the data; hence we limit the number of images from one user as 50, which leads to a dataset with 140,221 images from 4341 users. Instagram: Instagram is a service supporting photosharing via mobile app, where users take pictures and share them on social networking platforms like Facebook and Twitter. Similar to Flickr, we crawl at most 50 images for each user and get totally 131,224 images from 4853 users. Although the textual information as same as that on Flickr, for some images the number of comments is much bigger than that in Flickr, e.g., the images from celebrities usually contain thousands of comments, and we only consider the latest 50 comments for each image in Instagram. Establishing Ground Truth: For evaluation purpose, we need to create sentiment labels of images. We follow the scheme in [Yang et al., 2014; Liu, 2012] and create labels for images via images' tags. Since we use tags to create labels of images, we do not consider tag information as textual information in the proposed framework. Labeling each post solely relying on tags may cause noise in the ground truth. Therefore we additionally select 20000 images from Flickr and ask three human subjects to manually create labels for them. Feature extraction: the proposed method has the ability to incorporate visual and textual information. For visual information, we follow the recent approaches [Yuan et al., 2013; Borth et al., 2013] by using mid-level visual features. The visual features are extracted by a large-scale visual attribute detectors [Borth et al., 2013] and the feature dimension is 1200. Text-based features are formed by the term frequency in user profiles, image captions and comments. It is worth noting that textual features, which contain user descriptions, friends' comments and image captions, are preprocessed by stop word removing and stemming. MPQA5 lexicon is employed as sentiment signals.
5

The proposed framework USEA is compared with the following sentiment analysis algorithms: · Senti API:6 . This API is natural language processing API that performs unsupervised sentiment prediction using word-based sentiment. The method only uses textual information. · Sentibank: As a mid-level visual feature based sentiment analysis approach, it uses large-scale visual attribute detectors and low-level visual features to form the Adjective and Nouns visual sentiment description pairs [Borth et al., 2013]. · EL: A topical graphical model based sentiment analysis approach, which models the sentiment by low-level visual features and friends information [Yang et al., 2014]. · USEA-T: A variant of the proposed method that only considers the textual information including user profiles, image captions and friends' comments. · Random: It predicts sentiment labels of images by randomly guessing. Noting that SentiBank [Borth et al., 2013] and EL[Yang et al., 2014] are originally proposed for supervised sentiment analysis. We extend them to unsupervised scenarios by replacing original classifiers such as SVM or logistic regression with K-means. However, the clusters identified by K-means have no sentiment labels and we determine their sentiment labels with the Euclidean distance to the ground truth. We use SentiBank-K, and EL-K to represent these modifications.

4.2

Performance Evaluation

Table 1 lists the comparison results and we make several key observations: · Most of the time, textual based approaches obtain slight better performance than Random. These results support - (1) textual information is often incomplete and noisy and thus often inadequate to support independent sentiment analysis; and (2) textual information contains important cues for sentiment analysis. · The proposed framework often obtains better performance than baseline methods. There are two major reasons. First textual information provides semantic meanings and sentiment signals for images. Second we combine visual and textual information for sentiment analysis. The impact of textual information on the proposed framework will be discussed in the following subsection. In summary, compared to the performance Random, the proposed framework can significantly improve the sentiment analysis performance in a unsupervised scenario.

4.3

Impact of Textual Information

We introduce two parameters  and  to control contributions from textual information. In this subsection, we investigate the impact of textual information on the proposed framework by examining how the performance of USEA varies with the changes of these parameters.
6

http://mpqa.cs.pitt.edu/

http://sentistrength.wlv.ac.uk/

Table 1: The comparison results of different methods for sentiment analysis. Method Senti API SentiBank-K EL-K USEA-T USEA Random Flickr (#20,000) 32.30% 41.32% 36.39% 37.90% 55.22% 32.81% Flickr (#140,221) 34.15% 41.12% 42.90% 40.22% 56.18% 33.12% Instagram (#131,224) 37.80% 46.31% 43.21% 36.41% 59.94% 33.05%

To study the impact of , we fix  = 0.7 and vary the value of  as {0.001, 0.1, 0.2, 0.3, 0.5, 0.7, 1.5, 2, 10}. The performance variance of USEA w.r.t.  is demonstrated in Figure 2. Note that we only show results in Flickr with manual labels since we have similar observations for other datasets. In general, with the increase of , the performance first increases greatly, reach its peak value and then decrease dramatically. When we increase  from 0.001 to 0.1, the performance increases from 43.21% to 48.07%, which suggests the importance of textual information. With larger values of  (> 1.5), textual information dominates the learning process and the learnt parameters may overfit.

Figure 3: Performance Variance w.r.t.  . Y axis is the accuracy performance and X axis is the value of  . Jia et al., 2012; Yuan et al., 2013], such as images from Twitter and Flickr. Social media are heterogeneous, containing visual and other types of information. Some of existing methods use mainly textual information. For example, [Hu et al., 2013b] proposes a method by counting the word frequency in the user description and predict the sentiment by measuring the word's sentiment. In [Yang et al., 2014], it was argued that friends' comments are more related to the user's sentiment. There are also methods that use solely visual information. For example, [Borth et al., 2013; Yuan et al., 2013; Chen et al., 2014] employ mid-level attributes to model visual content, [Jia et al., 2012] provides a method based on low-level visual features, and [Wang et al., 2015] uses a regulated matrix factorization approach. Inspired by the success of deep learning, [You et al., 2015; Xu et al., 2014] employ a convolution neural network architecture for visual sentiment analysis. However, as discussed previously, these approaches are largely supervised, which means their performance is linked to the assumed availability of a good training set with labels.

Figure 2: Performance variance w.r.t. . Y axis is the accuracy performance and X axis is the value of . Similarly, to study the impact of  , we fix  = 0.7 and vary the value of  as {0.1, 0.2, 0.3, ..., 0.9, 1}. The performance variance of USEA w.r.t.  is demonstrated in Figure 3. We also only show results in Flickr with manual labels since similar observations are made for other datasets. When  increases from 0.1 to 0.6, the performance increases a lot, which further supports the importance of sentiment signals from textual information. After 0.8, the increase of  will reduce the performance dramatically because the proposed framework may overfit to sentiment signals from textual information.

6

Conclusion

5

Related Work

Recently sentiment analysis have shown success in many aspects, e.g., social response to special events [Hu et al., 2013b; Fukuhara et al., 2007; Diakopoulos and Shamma, 2010], product reviews [Pang and Lee, 2008; Cui et al., 2006], and opinion mining [Liu, 2012; Hu et al., 2013a; Pang et al., 2002; Pak and Paroubek, 2010; Godbole et al., 2007]. Besides, there have been increasing interests in social media images [Borth et al., 2013; Yang et al., 2014;

In this paper, we propose a novel unsupervised sentiment analysis framework USEA by leveraging textual information and visual information in a unified model. Moreover, USEA provides a new viewpoint for us to better understand how textual information helps bridge the "semantic gap" between visual feature and image sentiment. Experiments on three large-scale datasets demonstrated 1) the advantages of the proposed methods in unsupervised sentiment analysis; and 2) the importance of textual information. In the future, we will exploit more social media sources, such as link information, user history, geo-location, etc., for sentiment analysis.

7

Acknowledgments

Yilin Wang and Baoxin Li are supported in part by National Science Foundation (NSF) under grant number #1135616. Suhang Wang and Huan Liu are supported by, or in part by, the National Science Foundation (NSF) under grant number #1217466 and the U.S. Army Research Office (ARO) under contract/grant number #025071. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies

References
[Borth et al., 2013] Damian Borth, Rongrong Ji, Tao Chen, Thomas Breuel, and Shih-Fu Chang. Large-scale visual sentiment ontology and detectors using adjective noun pairs. In Proceedings of the 21st ACM international conference on Multimedia, pages 223­232. ACM, 2013. [Chen et al., 2014] Tao Chen, Felix X Yu, Jiawei Chen, Yin Cui, Yan-Ying Chen, and Shih-Fu Chang. Object-based visual sentiment concept analysis and application. In Proceedings of the ACM International Conference on Multimedia, pages 367­376. ACM, 2014. [Cui et al., 2006] Hang Cui, Vibhu Mittal, and Mayur Datar. Comparative experiments on sentiment classification for online product reviews. In AAAI, volume 6, pages 1265­ 1270, 2006. [Diakopoulos and Shamma, 2010] Nicholas A Diakopoulos and David A Shamma. Characterizing debate performance via aggregated twitter sentiment. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 1195­1198. ACM, 2010. [Ding et al., 2006] Chris Ding, Tao Li, Wei Peng, and Haesun Park. Orthogonal nonnegative matrix t-factorizations for clustering. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 126­135. ACM, 2006. [Ding et al., 2010] Chris Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factorizations. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(1):45­55, 2010. [Fukuhara et al., 2007] Tomohiro Fukuhara, Hiroshi Nakagawa, and Toyoaki Nishida. Understanding sentiment of people from news articles: Temporal sentiment analysis of social events. In ICWSM, 2007. [Godbole et al., 2007] Namrata Godbole, Manja Srinivasaiah, and Steven Skiena. Large-scale sentiment analysis for news and blogs. ICWSM, 7:21, 2007. [Hu and Liu, 2004] Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168­177. ACM, 2004. [Hu et al., 2013a] Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu. Unsupervised sentiment analysis with emotional signals. In Proceedings of the 22nd international conference on World Wide Web, pages 607­618. International World Wide Web Conferences Steering Committee, 2013.

[Hu et al., 2013b] Yuheng Hu, Fei Wang, and Subbarao Kambhampati. Listening to the crowd: automated analysis of events via aggregated twitter sentiment. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pages 2640­2646. AAAI Press, 2013. [Jia et al., 2012] Jia Jia, Sen Wu, Xiaohui Wang, Peiyun Hu, Lianhong Cai, and Jie Tang. Can we understand van gogh's mood?: learning to infer affects from images in social networks. In Proceedings of the 20th ACM international conference on Multimedia, pages 857­860. ACM, 2012. [Liu, 2012] Bing Liu. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1­167, 2012. [Nie et al., 2010] Feiping Nie, Heng Huang, Xiao Cai, and Chris H Ding. Efficient and robust feature selection via joint 2, 1-norms minimization. In Advances in Neural Information Processing Systems, pages 1813­1821, 2010. [Pak and Paroubek, 2010] Alexander Pak and Patrick Paroubek. Twitter as a corpus for sentiment analysis and opinion mining. In LREC, volume 10, pages 1320­1326, 2010. [Pang and Lee, 2008] Bo Pang and Lillian Lee. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1­135, 2008. [Pang et al., 2002] Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79­86. Association for Computational Linguistics, 2002. [Wang et al., 2015] Yilin Wang, Yuheng Hu, Subbarao Kambhampati, and Baoxin. Li. Inferring sentiment from web images with joint inference on visual and social cues: A regulated matrix factorization approach. In ICWSM, page 21, 2015. [Xu et al., 2014] Can Xu, Suleyman Cetintas, Kuang-Chih Lee, and Li-Jia Li. Visual sentiment prediction with deep convolutional neural networks. arXiv preprint arXiv:1411.5731, 2014. [Yang et al., 2014] Yang Yang, Jia Jia, Shumei Zhang, Boya Wu, Juanzi Li, and Jie Tang. How do your friends on social media disclose your emotions? 2014. [You et al., 2015] Quanzeng You, Jiebo Luo, Hailin Jin, and Jianchao Yang. Robust image sentiment analysis using progressively trained and domain transferred deep networks. 2015. [Yuan et al., 2013] Jianbo Yuan, Sean Mcdonough, Quanzeng You, and Jiebo Luo. Sentribute: image sentiment analysis from a mid-level perspective. In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, page 10. ACM, 2013.

In this paper, we propose a real-time system using vehicle back-up camera to alert for potential back-up collisions. We developed a highly efficient algorithm, combining segmenting pedestrians and vehicles from moving background using local optical flow value, and a scale adaptive method using Deformable Part Model to detect objects at different distances. To test out algorithm, we created our own vehicle back-up dataset that contains rich scenes recorded from a back-up camera on moving/stationary vehicles with unique and challenging scenarios such as frequent occlusion with cluttered and moving background, and we made this dataset available to public for other researchers. Experiments on the dataset shows that our algorithm achieves high accuracy in near real-time, and it is about 10 times faster than the comparable state-of-the-art algorithm.
Personalized and content-adaptive image enhancement can find many applications in the age of social media and mobile computing. This paper presents a relative-learning-based approach, which, unlike previous methods, does not require matching original and enhanced images for training. This allows the use of massive online photo collections to train a ranking model for improved enhancement. We first propose a multi-level ranking model, which is learned from only relatively-labeled inputs that are automatically crawled. Then we design a novel parameter sampling scheme under this model to generate the desired enhancement parameters for a new image. For evaluation, we first verify the effectiveness and the generalization abilities of our approach, using images that have been enhanced/labeled by experts. Then we carry out subjective tests, which show that users prefer images enhanced by our approach over other existing methods.
Video-based coaching systems have seen increasing adoption in various applications including dance, sports, and surgery training. Most existing systems are either passive (for data capture only) or barely active (with limited automated feedback to a trainee). In this paper, we present a video-based skill coaching system for simulation-based surgical training by exploring a newly proposed problem of instructive video retrieval. By introducing attribute learning into video for high-level skill understanding, we aim at providing automated feedback and providing an instructive video, to which the trainees can refer for performance improvement. This is achieved by ensuring the feedback is weakness-specific, skill-superior and content-similar. A suite of techniques was integrated to build the coaching system with these features. In particular, algorithms were developed for action segmentation, video attribute learning, and attribute-based video retrieval. Experiments with realistic surgical videos demonstrate the feasibility of the proposed method and suggest areas for further improvement.
Color demosaicking is used to reconstruct full color images from incomplete color filter array samples captured by cameras with a single sensor array. In reconstructing natural-looking images, one key challenge is to model and respect the statistics of natural images. This paper presents a novel modeling strategy and an efficient color demosaicking algorithm. The approach starts with joint modeling of the color images, which supports simultaneous representation of inter-channel correlation and structural information in an image. The inter-channel correlation is explored by measuring the channel difference signals in the gradient domain, while the structural information is explored by nonlocal low-rank regularization. An efficient algorithm is then proposed to solve the joint formulation, by dividing the minimization problem into two sub-problems and solving them iteratively. The effectiveness of the proposed approach is demonstrated with extensive experiments on both noiseless and noisy datasets, with comparison with existing state-of-the-arts color demosaicking methods.
In this proposal, we study the problem of understanding human sentiments from large scale collection of Internet images based on both image features and contextual social network information (such as friend comments and user description). Despite the great strides in analyzing user sentiment based on text information, the analysis of sentiment behind the image content has largely been ignored. Thus, we extend the significant advances in text-based sentiment prediction tasks to the higher level challenge of predicting the underlying sentiments behind the images. We show that neither visual features nor the textual features are by themselves sufficient for accurate sentiment labeling. Thus, we provide a way of using both of them, and formulate sentiment prediction problem in two scenarios: supervised and unsupervised. We develop an optimization algorithm for finding a local-optima solution under the proposed framework. With experiments on two large-scale datasets, we show that the proposed method improves significantly over existing state-of-the-art methods. In the future, we are going to incorporating more information on the social network and explore sentiment on signed social network.
In this paper, we propose a real-time system using vehicle back-up camera to alert for potential back-up collisions. We developed a highly efficient algorithm, combining segmenting pedestrians and vehicles from moving background using local optical flow value, and a scale adaptive method using Deformable Part Model to detect objects at different distances. To test out algorithm, we created our own vehicle back-up dataset that contains rich scenes recorded from a back-up camera on moving/stationary vehicles with unique and challenging scenarios such as frequent occlusion with cluttered and moving background, and we made this dataset available to public for other researchers. Experiments on the dataset shows that our algorithm achieves high accuracy in near real-time, and it is about 10 times faster than the comparable state-of-the-art algorithm.
STRUCTURE-PRESERVING IMAGE QUALITY ASSESSMENT Yilin Wang1 Qiang Zhang2 Baoxin Li1
1

Department of Computer Science, Arizona State University, Tempe AZ 2 Advanced Image Research Lab, Samsung Electronic, Pasadena CA {ywang370,Baoxin.Li}@asu.edu q.zhang1@samsung.com
ABSTRACT approaches have employed Bag of Words [6, 7], and DCT transformation [8], etc. In general, existing approaches belong to one of the above three categories and only work for their respective scenario. One objective of this paper is to develop a unifying approach for both FR-IQA and NR-IQA, hence maximizing the applicability of the IQA model. In parallel with perceptual/subjective IQA, various objective measures have been employed in multimedia. The most widely-used one is Mean Square Error (MSE) or its variants, due to its simplicity and general effectiveness. MSE simply measures the average per-sample difference between two signals. Since MSE is convex and differentiable, it is easy to use optimization approaches for finding solutions to various models based on MSE. Unfortunately, it is well understood (e.g., [9]) that pixel-wise MSE (or its variants) is not a good measure for perceptual quality, primarily due to the fact that no structural information of the image is considered in computing MSE. Some attempts have been tried to remedy this. For example, in [10], a "perceptual-aware" MSE was proposed by adding Gaussian filter or gradient operator, which helped to improve the correlation between the human perceptual score and the objective metric. In this work, we aim at building a structure-preserving MSE (SPMSE) which not only retains the computational efficiency and nice mathematical properties of MSE but also leads to the development of effective IQA metrics. Our new formulation of MSE is developed by employing the `kernel trick': we use HOG feature [11], an effective and efficient descriptor for describing object structures, as an quality kernel between two images, and show that the resultant formulation leads to many of the desired properties. For FR-IQA, experimental results show that SPMSE performs statistically better than the well-known SSIM method and leads to very competitive performance compared with with other state-of-the-art methods on three benchmark datasets. Moreover, we show that the proposed SPMSE can be employed in recent Bag-of-Words based models for NR-IQA [7, 6]. In such existing models, the distortion image is represented by a feature vector which is the coefficients under a codebook. However, most existing approaches focus on designing hand-crafted features to be used by the codebook training, rather than optimal representation under the code-

Perceptual Image Quality Assessment (IQA) has many applications. Existing IQA approaches typically work only for one of three scenarios: full-reference, non-reference, or reduced-reference. Techniques that attempt to incorporate image structure information often rely on hand-crafted features, making them difficult to be extended to handle different scenarios. On the other hand, objective metrics like Mean Square Error (MSE), while being easy to compute, are often deemed ineffective for measuring perceptual quality. This paper presents a novel approach to perceptual quality assessment by developing an MSE-like metric, which enjoys the benefit of MSE in terms of inexpensive computation and universal applicability while allowing structural information of an image being taken into consideration. The latter was achieved through introducing structure-preserving kernelization into a MSE-like formulation. We show that the method can lead to competitive FR-IQA results. Further, by developing a feature coding scheme based on this formulation, we extend the model to improve the performance of NR-IQA methods. We report extensive experiments illustrating the results from both our FR-IQA and NR-IQA algorithms with comparison to existing state-of-the-art methods. Index Terms-- Mean Square Error, Image Quality Assessment, kernel method. 1. INTRODUCTION Perceptual image quality assessment (IQA) has many multimedia applications such as image denoising [1] and image transmission. Based on the degree of reliance on a reference image, IQA models can be divided into three categories: Full Reference IQA (FR-IQA), Reduced Reference IQA (RRIQA) and Non-Reference IQA (NR-IQA). FR-IQA needs a reference image for estimating the distortion of a target image. Numerous FR-IQA models have been proposed, including those that incorporate image structure information [2], mutual information [3, 4], and wavelet information [5], etc. For RR-IQA, only partial information of a reference image is needed, while NR-IQA models predict image quality without any information from reference images. Recent NR-IQA

book. In other words, the feature encoding step, which should contribute to the final quality metric significantly, has been largely ignored. In this work, we propose and empirically compare several coding schemes for NR-IQA based on the SPMSE framework, and show that, compared to vector quantization or sparse coding, the proposed method, structure preserving coding, is more effective for NR-IQA models. The rest of the paper is organized as follows: Section 2 reviews the related work. Section 3 describes SPMSE for FR-IQA in details and introduces our SPMSE encoding for NR-IQA. In Section 4, experimental results on widely-used datasets are reported; and finally in Section 5, conclusions are made and future improvements and issues are discussed. 2. RELATED WORK We review the related work on FR-IQA and recent advanced methods in NR-IQA. FR-IQA: One of the most widely-used and influential FR-IQA method is Structure SIMilarity Index (SSIM). It is based on the assumption that the underlying image quality score is highly related to the image structure. For a pair of a reference image s and a distortion image t, SSIM compares them with image luminance, contrast and structure as: (2µs µt +C1 )(2st +C 2) SSIM (s, t) = (µ 2 +µ2 +C 1)( 2 + 2 +C 2) , where, for image s s t t i, j , µi is the local mean intensity, i is the local variance and ij is the local covariance. Visual Information Fidelity (VIF) is another IQA approach that captures the signal statistics for image fidelity assessment. In [12], it was argued that the HSV space is appropriate for full-reference image quality assessment, owing to distinctive features of high-quality and low-quality images in this space. In [13], the author provides a gradient similarity method for image quality assessment. For a thorough survey of modern IQA development, please refer to [14]. In contrast to the methods discussed above, the proposed framework starts from the widely used MSE and applies kernel method to the objective function for preserving image structure. NR-IQA: When images are transferred to some specific domain, e.g., the DCT domain, local descriptors may be modeled by some parametric distribution, based on this, some previous works [8, 15, 16] on NR-IQA have focused primarily on Natural Scene Statistics (NSS). On the other hand, inspired by the success of Bag-of-Words approaches in computer vision, [7] uses visual codebook to assess image quality. Quality measure of a new image is obtained by computing the average of quality scores of the codewords, weighted by their distances to visual words in the image. However, the method requires a large number of codewords and precomputed Gabor-filters. Other than hand-crafted features, [6] proposes an unsupervised feature-learning method based on raw image patches. The proposed method is similar to [7] in term of its codebook-based encoding. However, our goal is to learn the features based on raw image patches for both

non-distortion images and distortion images. 3. THE PROPOSED SPMSE FRAMEWORK 3.1. Structure Persevering Mean Square Error Given two signals s, t  RN , the objective function of MSE is ||s - t||2 2 /N . In SPMSE, we introduce a non-linear structure 1 ||(s) - (t)||2 extractor term for each signal as N 2 where  is a mapping function, which maps the original data space to a new feature space. The objective function of SPMSE is: SP M SE (s, t) = 1 ||(s) - (t)||2 2 N 1 = ( (s)(s) - 2 (s)(t) + (t)(t) ) N (1)

In Eq. 1, the SPMSE is guaranteed to be non-negative, and thus it can be viewed as a distance measure. Introducing a kernel operation, we can re-write SPMSE as: SP M SE (s, t) = 1 (K (s, s) - 2 × K (s, t) + K (t, t)) (2) N

where K is a valid Mercer kernel [18], which can be viewed as a non-linear feature similarity measure for the signals. In the next section we will discuss how to choose K . 3.1.1. Kernel Selection As definition in [18]: "a kernel is a function that returns the inner product between the images of two inputs in some feature space". The intuition of the kernel method is to measure the similarity between two data vectors in a new feature space. The most widely used kernels for images (signals) are polynomial kernels and RBF kernels [18]. A polynomial kernel is given by K (x, y ) = ( x, y + R)d where R and d are kernel parameters. The RBF kernel is defined as
||x-y ||2

K (x, z ) = exp -22 . However, trivially bringing them to the proposed objective function is not a good choice for FRIQA, since the resultant MSE-based kernel function is still based on pixel-wise computation and hence losing the sight of the image structure distortion, which has been argued to be an essential factor for IQA [4, 2]. Thus, one of our goals is to find a proper kernel that helps to retain structural information of an image. Inspired by its success in object detection, e.g. [11], we employ Histogram of Oriented Gradient(HOG) as image quality descriptor. HOG is one of the most-used low-level vision features for object detection and recognition, and the essential thought behinds HOG is that the local appearance and structure in images can be described by its gradient distribution. Based on the following theorem, we show it can be incorporated into our proposed framework as a valid kernel function.

Theorem 1: HOG operator is a valid kernel function. Proof : Let i and Mi be the orientation and magnitude of gradient at pixel i. Then the HOG feature of each pixel is represented by a hard binning indicator. in =
i 1 if 2  =n-1 0 otherwise

(3)

For each image block P , the oriented gradient is represented as  (P ) = iP Mi · i . When measuring the similarity between patches from two different images, it is equivalent to match the patches in the feature space. Thus, we can represent the similarity between image patches in the feature space with a linear kernel: K (P, Q) =
iP

the nearby bases of the encoded data. Based on these observations, we compare different coding schemes and then propose a novel feature coding scheme for NR-IQA, which supports feature learning with the proposed SPMSE metric. Let X be a set of M-dimensional feature vectors extracted from images, i.e., X = [x1 , x2 , ....., xN ]  RM ×N . C = [c1 , c2 , ..., cN ] is a set of code coefficients for X based on codebook B = [b1 , b2 , ...bK ]  RM ×K . C can be generated by different coding schemes for image representation. Based on [19, 17, 20], the proposed coding scheme solves the following optimization problem:
N

argmin
c i=1

2 ||xi - Bci ||2 2 + ||Di ci ||2 + µ|ci |

(5)

Mi · i
i Q

Mi ·  i =
iP i Q

T Mi  i i

Mi

=
iP i Q

T M i M i i i

(4) where P, Q are two patches from two images. Since Mi Mi t is a non negative scalar and i i is the inner product of two vectors, then we can substitute two linear kernel KM (i, i ) = T Mi · Mi , K (i, i ) = i i in Eq. 4. Thus, K (P, Q) is a valid kernel [18] and it provides a kernel view of HOG. It is worth noting that, in contrast to [13], where the metric is simply based on the similarity of gradient value from two signals. In the proposed framework, inspired by the success of using HOG for object detection, we utilize the property of the HOG for image structure description. Specifically, in Theorem 1, KM (i, i ) measures the similarity of gradient magnitude of two pixels and K (i, i ) measures the similarity of gradient orientations of two pixels. Thus, instead of measuring the pixel similarity in MSE, the proposed SPMSE can be viewed as a structure similarity measure for image patches (e.g. 8 × 8 rectangles in HOG). 3.2. Structure-Persevering Coding In Section 3.1, we proposed a SPMSE framework for the FRIQA problem, which captures image local structures instead of measuring the pixel-wise error. Noting Eq. 1 is convex and differentiable, we can easily build an objective function to minimize. We now show how the idea may be extended to handle NR-IQA problems. In recent NR-IQA approaches [7, 6, 15, 17], different features have been designed. However, feature coding has been largely ignored. In other words, how to efficiently encode the features for NR-IQA is still not well addressed. In [7], hard vector coding was used, and in [6], the authors argue soft coding is better, while [17] argues sparse coding is more efficient. In [19], locality linear coding is proposed, the authors observed that the non-zero coefficients are often assigned to

where Di  RK ×K is a diagonal matrix, with each element in the diagonal representing the SPMSE score of input image patch i and code basis j , i.e.Di(j,j ) = SP M SE (xi , bj ). The second term in Eq. 5 gives the input patch freedom to decide proportion of similar structure bases in the codebook, while the third term is the sparse regularization term which makes the nonlinear representation of the features. Unfortunately, the above objective function is computational expensive. To alleviate this, we propose an approximation scheme by relaxing the sparse term in the objective function to 1T ci = 1, i, which still achieves sparsity if we set small values in the solution to zero. Since the Eq. 5 can be decomposed, the encoded feature ci can be obtained by solving the following optimization problem: argmin
c 2 J = ||xi - Bci ||2 2 + ||Di ci ||2

subject to 1T ci = 1; The Lagrangian function of Eq. 6 is:
T 2 L = ||xi - Bci ||2 2 + ||Di ci ||2 +  (1 ci - 1)

(6)

(7)

where  is Lagrangian multiplier. Taking the derivation of L and setting it to zero, we can obtain:
2  1 = 2B T xi - 2B T Bci - 2Di ci T

(8)

Trick 1: 1 ci = 1 and xT i Bci is a scalar, Eq. 8 can be written as:
T T T 2  1 + 2 xT i Bci 1 = 2B xi 1 ci - 2B Bci - 2Di ci

+ 21xT i Bci

(9)

1 T T T 2 T  - ( + xT i Bci )1 = (B B - B xi 1 + Di - 1xi B )ci 2 (10) T Trick 2: xT x 1 c is a scalar, thus it can be added on the i i i both sides: 1 T T T T T - ( + xT i Bci )1 + xi xi 1 ci 1 = (B B - B xi 1 2 2 + Di - 1xT i B )ci
T + 1xT i x i 1 ci

(11)

MSE SSIM VIF IFC MAD SPMSE

Table 1. PLCC comparison of different FR-IQA models LIVE(779 images) TID2008(1300 images) CSIQ(750 images) 0.8739 0.7649 0.8882 0.9451 0.8530 0.9188 0.9604 0.8938 0.9321 0.9268 0.8007 0.8912 0.9394 0.8306 0.8881 0.9364 0.8876 0.9213

0.8279 0.8962 0.9226 0.8599 0.8762 0.9096

1 T T T T T  - (  + xT i Bci - 2xi xi 1 ci )1 = (B B - B xi 1 2 2 + Di - 1xT i B
T + 1 xT i xi 1 )ci

(12) Finally, the closed form solution can be obtained after normalization as :
T T T 2 ci = ((B T - 1xT i )(B - 1xi ) + Di ) \ 1

ci = ci /1ci

(13)

for us to deal with. Thus, the number of images from CSIQ is 750. Evaluation: We evaluate the performance of different methods using Pearson Linear Correlation Coefficient (PLCC) and Spearman Rank Order Correlation Coefficient (SROCC). PLCC is considered as a measurement of the prediction accuracy and SROCC is viewed as an evaluation of how well the relationship between the predicted score and the subject opinion score can be described. A good IQA model should have high PLCC and SROCC. 4.2. NR-IQA Evaluation Protocol Database for NR-IQA evaluation and codebook construction: We use LIVE database for evaluation and adopt CSIQ databse for codebook construction based on the following reasons: First, there is no overlap between CSIQ dataset and LIVE dataset. Second, both CSIQ and LIVE contain four types of distortion: JP2K, JPEG, GB, AWN. Thus, it is reasonable to use codebook generated from CSIQ to represent the images in LIVE instead of TID2008 which has much more noise types than CSIQ. For each image in CSIQ, we randomly extract 10000 7 by 7 raw patches, then using K-means clustering to generate the codebook. In our experiment, the codebook is fixed by 10000 × 49. This protocol is also used in [7]. NR-IQA Regression and Evaluation: The predicted score is calculated from linear support vector regression (SVR) directly. Since codebook is constructed from unlabeled data, in LIVE database, we randomly pick 80% images associate with human subject score to train the SVR and remaining 20% for testing. Moreover, we repeat the train-test scheme 100 times for cross-validation. It is worth noting that both the training set and the testing set only contain the distorted images. Finally, we use max-pooling to represent image feature. 4.3. Comparison with FR-IQA and NR-IQA algorithms In this sub-section, we first compare the results of the proposed method with state-of-the-art FR-IQA models including SSIM [2], VIF[3], IFC[4], MAD [12]. Table 1 and Table 2 list the results of SROCC and PLCC of different FR-IQA models respectively. The results are reported from the original papers

Compared to hard vector quantization encoding [7] which represents images from a single basis, the approximation scheme of Eq. 5 will achieve much smaller error because of the use of multiple bases (soft coding). It is worth noting that the method in [19] is based on pixel-wise representation, lacking the structural information captured by our SPMSE-based scheme. Moreover, we empirically observed that the coding results from [17] tend to select codebook bases that were from images under different distortions, while code bases from our approach tend to belong to images of similar distortion. 4. EXPERIMENTS 4.1. FR-IQA Evaluation Protocol Database for FR-IQA evaluation: To evaluate the proposed framework, we tested it on three benchmark IQA datasets: LIVE[21], TID2008[22], and CSIQ[12]. The images in these datasets are generated with different type of distortion and associated with human/subjective opinion score. The LIVE database contains 29 reference images and 779 distorted images with 5 different distortions: JPEG2000 compression (JP2K), JPEG compression (JPEG), additive white noise(AWN), Gaussian blur (GB), and Fast fading (FF). The TID2008 database contains 25 reference images and 1700 distorted images with 17 different noise types. Since the last four distortions (totally 400 images) are not structure distortions, e.g. intensity shift, which is a highly subjective task for people to distinguish with, we reported the results on first 13 distortions. This protocol also has been used in [6, 17]. The CSIQ contains 30 reference images and 866 distorted images generated from JP2K, JPEG, AWN, GB, and pink Gaussian noise, the contrast change is also not the structure distortion

with default parameter settings. It is worth noting that PLCC results are reported after logistic regression (Eq. (14) and Eq. 15) between predicted score and subject opinion score, which follows the instruction reported in [23]. From Table 2 and Table 1, we can draw the following conclusions. First, the proposed method outperforms a large margin to MSE and is superior to SSIM. Second, the proposed method is comparable to other state-of-the-art method, e.g., VIF, MAD, in terms of average resuls among three benchmark datasets. Moreover, in Table 3, we compare the speed1 of the proposed method and other top 3 FR-IQA metrics. It can be seen that the proposed is efficient in terms of computation time. Table 3. Speed Comparison with Top 3 metrics in FR-IQA to MSE MSE SSIM SPMSE VIF MAD Time(s) 0.0021 0.031 0.043 0.974 2.07 ratio to MSE 1 15 20 458 986 In Table 4 and Table 5, we report the results of our encoding scheme with comparison of state-of-the-art NR-IQA methods and other encoding schemes. The compared methods including BIQI [7], CORINA[7], DIIVINE [16] and BLIINDS (SVM) [8] and we also compared our encoding methods with hard encoding (HC), sparse encoding (SC) [20] and locality linear encoding (LLC) [19]. From the result we can see that our proposed achieves best result among all the encoding schemes which have same codebook, meanwhile, our result is comparable to state-of-the-arts models, e.g., CORINA. Noting that the evaluation of the proposed method only employs general procedures of BOW, the results can be further improved by employing a more powerful regressor (e.g. random forest) or using precomputed features (e.g., NSS) instead of raw image patches.

Table 4. SROCC comparison of different NR-IQA models on LIVE Method JP2K JPEG AWN GB FF ALL PSNR 0.872 0.885 0.941 0.764 0.875 0.867 SSIM 0.939 0.946 0.965 0.909 0.941 0.913 BIQI 0.856 0.786 0.972 0.910 0.762 0.819 CO0.943 0.955 0.976 0.969 0.906 0.942 RINA DIVI0.913 0.910 0.984 0.921 0.863 0.916 INE BLI0.929 0.955 0.956 0.923 0.889 0.931 INDS SPMSE 0.936 0.948 0.952 0.958 0.872 0.930 LLC 0.921 0.941 0.942 0.932 0.862 0.909 HC 0.919 0.948 0.945 0.908 0.905 0.917 SC 0.926 0.958 0.952 0.941 0.852 0.921

Table 5. PLCC comparison LIVE Method JP2K JPEG PSNR 0.873 0.874 SSIM 0.920 0.955 BIQI 0.809 0.901 CO0.951 0.965 RINA DIVI0.922 0.921 INE BLI0.935 0.968 INDS SPMSE 0.947 0.951 LLC 0.931 0.941 HC 0.921 0.950 SC 0.929 0.965

of different NR-IQA models on AWN 0.928 0.982 0.954 0.987 0.988 0.980 0.971 0.943 0.965 0.959 GB 0.774 0.891 0.829 0.968 0.923 0.938 0.970 0.942 0.929 0.945 FF 0.869 0.939 0.733 0.917 0.888 0.896 0.899 0.872 0.883 0.892 ALL 0.855 0.906 0.821 0.935 0.917 0.930 0.934 0.919 0.917 0.925

Quality (x) = 1 logistic(2 , (x - 3 )) + 4 x + 5 (14) 1 1 - 2 1 + exp( x)

logistic(, x) =

(15)

5. DISCUSSION AND FUTURE WORK We proposed a simple yet effective approach for image quality assessment. First, we proposed a structure-preserving MSE-like error function for FR-IQA, and the experimental results show that our method is competitive with respect to the state-of-the-art methods and in particular, outperforms the
the codes are implemented by Matlab and obtained from original authors' webpage. The HOG computation part is written in C and compiled by Matlab.
1 All

well-known SSIM. Second, we showed that the proposed approach can be applied to the NR-IQA framework as well, through incorporating it in a coding scheme. Even with only a fixed and unoptimized codebook, the experimental results still showed performance comparable to current approaches. Future efforts include at least two possible exentions: a learningbased method for selecting a kernel function more efficiently, and codebook learning for improved NR-IQA.

6. ACKNOWLEDGMENT Yilin Wang and Baoxin Li were supported in part by a grant (#1135616) from the National Science Foundation (NSF). Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.

MSE SSIM VIF IFC MAD SPMSE

Table 2. SROCC comparison of different FR-IQA models LIVE(779 images) TID2008(1300 images) CSIQ(750 images) 0.8756 0.7118 0.9060 0.9479 0.8742 0.9247 0.9636 0.8731 0.9282 0.9259 0.7589 0.8827 0.9438 0.8694 0.9604 0.9564 0.8887 0.9353

Weighted 0.8362 0.8946 0.9130 0.8383 0.9142 0.9179

7. REFERENCES [1] Keigo Hirakawa and Thomas W Parks, "Image denoising using total least squares," Image Processing, IEEE Transactions on, vol. 15, no. 9, pp. 2730­2742, 2006. [2] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli, "Image quality assessment: from error visibility to structural similarity," Image Processing, IEEE Transactions on, vol. 13, no. 4, pp. 600­612, 2004. [3] Hamid R Sheikh and Alan C Bovik, "Image information and visual quality," Image Processing, IEEE Transactions on, vol. 15, no. 2, pp. 430­444, 2006. [4] Hamid R Sheikh, Alan C Bovik, and Gustavo De Veciana, "An information fidelity criterion for image quality assessment using natural scene statistics," Image Processing, IEEE Transactions on, vol. 14, no. 12, pp. 2117­2128, 2005. [5] Damon M Chandler and Sheila S Hemami, "Vsnr: A waveletbased visual signal-to-noise ratio for natural images," Image Processing, IEEE Transactions on, vol. 16, no. 9, pp. 2284­ 2298, 2007. [6] Peng Ye, Jayant Kumar, Le Kang, and David Doermann, "Unsupervised feature learning framework for no-reference image quality assessment," in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 1098­1105. [7] Peng Ye and David Doermann, "No-reference image quality assessment based on visual codebook," in Image Processing (ICIP), 2011 18th IEEE International Conference on. IEEE, 2011, pp. 3089­3092. [8] Michele A Saad, Alan C Bovik, and Christophe Charrier, "Blind image quality assessment: A natural scene statistics approach in the dct domain," Image Processing, IEEE Transactions on, vol. 21, no. 8, pp. 3339­3352, 2012. [9] Zhou Wang and Alan C Bovik, "Mean squared error: love it or leave it? a new look at signal fidelity measures," Signal Processing Magazine, IEEE, vol. 26, no. 1, pp. 98­117, 2009. [10] Wufeng Xue, Xuanqin Mou, Lei Zhang, and Xiangchu Feng, "Perceptual fidelity aware mean squared error," . [11] Navneet Dalal and Bill Triggs, "Histograms of oriented gradients for human detection," in Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. IEEE, 2005, vol. 1, pp. 886­893. [12] Eric C Larson and Damon M Chandler, "Most apparent distortion: full-reference image quality assessment and the role

of strategy," Journal of Electronic Imaging, vol. 19, no. 1, pp. 011006­011006, 2010. [13] Anmin Liu, Weisi Lin, and Manish Narwaria, "Image quality assessment based on gradient similarity," Image Processing, IEEE Transactions on, vol. 21, no. 4, pp. 1500­1512, 2012. [14] Weisi Lin and C-C Jay Kuo, "Perceptual visual quality metrics: A survey," Journal of Visual Communication and Image Representation, vol. 22, no. 4, pp. 297­312, 2011. [15] Anush Krishna Moorthy and Alan Conrad Bovik, "A twostep framework for constructing blind image quality indices," Signal Processing Letters, IEEE, vol. 17, no. 5, pp. 513­516, 2010. [16] Anush Krishna Moorthy and Alan Conrad Bovik, "Blind image quality assessment: From natural scene statistics to perceptual quality," Image Processing, IEEE Transactions on, vol. 20, no. 12, pp. 3350­3364, 2011. [17] Lihuo He, Dacheng Tao, Xuelong Li, and Xinbo Gao, "Sparse representation for blind image quality assessment," in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 1146­1153. [18] John Shawe-Taylor and Nello Cristianini, Kernel methods for pattern analysis, Cambridge university press, 2004. [19] Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv, Thomas Huang, and Yihong Gong, "Locality-constrained linear coding for image classification," in Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 3360­3367. [20] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y Ng, "Efficient sparse coding algorithms," . [21] Hamid R Sheikh, Muhammad F Sabir, and Alan C Bovik, "A statistical evaluation of recent full reference image quality assessment algorithms," Image Processing, IEEE Transactions on, vol. 15, no. 11, pp. 3440­3451, 2006. [22] Nikolay Ponomarenko, Vladimir Lukin, Alexander Zelensky, Karen Egiazarian, M Carli, and F Battisti, "Tid2008-a database for evaluation of full-reference visual quality assessment metrics," Advances of Modern Radioelectronics, vol. 10, no. 4, pp. 30­45, 2009. [23] Video Quality Experts Group et al., "Final report from the video quality experts group on the validation of objective models of video quality assessment," VQEG, Mar, 2000.

Inferring Sentiment from Web Images with Joint Inference on Visual and Social Cues: A Regulated Matrix Factorization Approach
Yilin Wang1 Yuheng Hu2 Subbarao Kambhampati1 Baoxin Li1
1

Department of Computer Science, Arizona State University, Tempe AZ 2 IBM Almaden Research Center, San Jose CA {ywang370,rao,baoxin.li}@asu.edu yuhenghu@us.ibm.com

Abstract
In this paper, we study the problem of understanding human sentiments from large scale collection of Internet images based on both image features and contextual social network information (such as friend comments and user description). Despite the great strides in analyzing user sentiment based on text information, the analysis of sentiment behind the image content has largely been ignored. Thus, we extend the significant advances in text-based sentiment prediction tasks to the higherlevel challenge of predicting the underlying sentiments behind the images. We show that neither visual features nor the textual features are by themselves sufficient for accurate sentiment labeling. Thus, we provide a way of using both of them. We leverage the low-level visual features and mid-level attributes of an image, and formulate sentiment prediction problem as a non-negative matrix tri-factorization framework, which has the flexibility to incorporate multiple modalities of information and the capability to learn from heterogeneous features jointly. We develop an optimization algorithm for finding a local-optima solution under the proposed framework. With experiments on two large-scale datasets, we show that the proposed method improves significantly over existing state-of-the-art methods.

1

Introduction

A picture is worth a thousand words. It is surely worth even more when it comes to convey human emotions and sentiments. Examples that support this are abundant: great captivating photos often contain rich emotional cues that help viewers easily connect with those photos. With the advent of social media, an increasing number of people start to use photos to express their joy, grudge, and boredom on social media platforms like Flickr and Instagram. Automatic inference of the emotion and sentiment information from such ever-growing, massive amounts of user-generated photos is of increasing importance to many applications in health-care, anthropology, communication studies, marketing, and many sub-areas within computer science such as computer vision. Think about this: Emotional wellness impacts several aspects of people's lives. For example, it introduces self-empathy, giving an individual greater awareness
Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

of their feelings. It also improves one's self-esteem and resilience, allowing them to bounce back with ease, from poor emotional health, and physical stress and difficulty. As people are increasingly using photos to record their daily lives1 , we can assess a person's emotional wellness based on the emotion and sentiment inferred from her photos on social media platforms (in addition to existing emotion/sentiment analysis effort, e.g., see (De Choudhury, Counts, and Gamon 2012) on text-based social media). In this paper, our goal is to automatically infer human sentiments (positive, neutral and negative) from photos shared on Flickr and Instagram. While sentiment analysis of photos is still in its infancy, a number of tools have been proposed during past two years(Yuan et al. 2013; Jia et al. 2012). A popular approach is to identify visual features from a photo that are related to human sentiments, such as objects (e.g., toys, birthday cakes, gun), human actions (e.g., crying or laughing), and many other features like color temperature. However, such an approach is often insufficient because the same objects/actions may convey different sentiments in different photo contexts. For example, consider Figure 1: one can easily detect the crying lady and girl (using computer vision algorithms such as face detection(Zhu and Ramanan 2012) and expression recognition(Song et al. 2010)). However, the same "crying" action conveys two clearly different sentiments: the "crying" in Figure 1a is obviously positive as the result of a successful marriage proposal. In contrast, the tearful girl in Figure 1b looks quite unhappy thus expresses negative sentiment. In other words, the so-called "visual affective gap" (Machajdik and Hanbury 2010) exists between rudimentary visual features and human sentiment embedded in a photo. On the other hand, one may also consider inferring the sentiment of a photo via its textual descriptions (e.g., titles) using existing off-the-shelf text-based sentiment analysis tools (Pang, Lee, and Vaithyanathan 2002). Although these descriptions can provide very helpful context information of the photos, solely relying on them while ignoring the visual features of the photos can lead to poor performance as well. Consider Figure 1 again: by analyzing only the text description, we can conclude that both Figure 1a and 1b convey negative sentiment as the keyword "crying" is often
1 http://www.pewinternet.org/2015/01/09/social-media-update2014/

(a) "Girlfriend crying a lot when I(b) Crying baby after her toy was proposed to her". taken

Figure 1: An example shows affective gap.

classified as negative sentiment in standard sentiment lexicon (Taboada et al. 2011). Last, both visual feature-based and text-based sentiment analysis approaches require massive amounts of training data in order to learn high quality models. However, manually annotating the sentiment of a vast amount of photos and/or their textual descriptions is time consuming and error-prone, presenting a bottleneck in learning good models. The weaknesses discussed in the foregoing motivate the need for a more accurate automated framework to infer the sentiment of photos, with 1) considering the photo context to bridge the "visual affective gap", 2) considering a photo's visual features to augment text-based sentiment, and 3) considering the availability of textual information, thus a photo may have little or no social context (e.g., friend comments, user description). While such a framework does not exist, we can leverage some partial solutions. For example, we can learn the photo context by analyzing the photo's social context (text features). Similarly, we can extract visual features from a photo and map them to different sentiment meanings. Last, while manual annotation of all photos and their descriptions is infeasible, it is often possible to get sentiment labeling for small sets of photos and descriptions. Technical Contribution: We propose an efficient and effective framework, named RSAI (Robust Sentiment Analysis for Images), for inferring human sentiment from photos that leverages these partial solutions. Figure 2 depicts the procedure of RSAI. Specifically, to fill the visual affective gap, we first extract visual features from a photo using low-level visual features (e.g., color histograms) and a large number of mid-level (e.g., objects) visual attribute/object detectors (Yuan et al. 2013; Tighe and Lazebnik 2013). Next, to add sentiment meaning to these extracted non-sentimental features, we construct Adjective Noun Pairs (ANPs)(Borth et al. 2013). Note that ANP is a visual representation that describes visual features by text pairs, such as "cloudy sky", "colorful flowers". It is formed by merging the low-level visual features to the detected mid-level objects and mapping them to a dictionary (more details on ANP are presented in Section 3). On the other hand, to learn the image's context, we analyze the image's textual description and capture its sentiment based on sentiment lexicons. Finally, with

the help from ANPs and image context, RSAI infers the image's sentiment by factorizing an input image-features matrix into three factors corresponding to image-term, termsentiment and sentiment-features. The ANPs here can be seen as providing the initial information ("prior knowledge") on sentiment-feature factors. Similarly, the learnt image context can be used to constrain image-term and termsentiment factors. Last, the availability of labeled sentiment of the images can be used to regulate the product of imageterm, term-sentiment factors. We pose this factorization as an optimization problem where, in addition to minimizing the reconstruction error, we also require that the factors respect the prior knowledge to the extent possible. We derive a set of multiplicative update rules that efficiently produce this factorization, and provide empirical comparisons with several competing methodologies on two real datasets of photos from Flickr and Instagram. We examine the results both quantitatively and qualitatively to demonstrate that our method improves significantly over baseline approaches. The rest of this paper is organized as follows: first, we review the related work on sentiment prediction as well as work which utilizes the nonnegative matrix factorization. We then present a basic model for the problem and further improve the model by incorporating prior knowledge. The experimental results and a comprehensive analysis are presented in the experiment part. Last, we conclude by identifying future work.

2

Related Work

In this section, we review the related work on sentiment analysis and the methods for matrix factorization. Sentiment analysis on text and images: Recently, sentiment analysis has shown its success in opinion mining on textual data, including product review(Liu 2012; Hu and Liu 2004), newspaper articles (Pang, Lee, and Vaithyanathan 2002), and movie rating (Pang and Lee 2004). Besides, there have been increasing interests in social media data (Borth et al. 2013; Yang et al. 2014; Jia et al. 2012; Yuan et al. 2013), such as Twitter and Weibo data. Unlike textbased sentiment prediction approaches, (Borth et al. 2013; Yuan et al. 2013) employed mid-level attributes of visual feature to model visual content for sentiment analysis. (Yang et al. 2014) provides a method based on low-level visual features and social information via a topic model. While (Jia et al. 2012) tries to solve the problem by a graphical model which is based on friend interactions. In contrast to our approach, all such methods restrict sentiment prediction to the specific data domain. For example, in Figure 1, we can see that approaches using pure visual information (Borth et al. 2013; Yuan et al. 2013) may be confused by the subtle sentiment embedded in the image. e.g., two crying people convey totally different sentiment. (Jia et al. 2012; Yang et al. 2014) assume that the images belong to the same sentiment share the same low-level visual features is often not true, because positive and negative images may have similar low-level visual features, e.g., two black-white images contain smiling and sad faces respectively. Recent, deep learning has shown its success in feature learning for many computer vision problem, (You et al. 2015) provides a

Figure 2: The framework of our proposed method. Comparing to conventional methods, which focus on single source/feature, the proposed method learns the heterogeneous features, including text features, low-level features and mid-level visual features, for sentiment analysis. transfer deep neutral network structure for sentiment analysis. However, for deep learning framework, millions of images with associated sentiment labels are needed for network training. In real world, such label information is not available and how to deal with overfitting for small training data remains a challenging problem. Non-negative matrix factorization(NMF): Our proposed framework is also inspired by recent progress in matrix factorization algorithms. NMF has been shown to be useful in computer vision and data mining applications including face recognition(Wang et al. 2005), object detection (Lee and Seung 1999) and feature selection (Das Gupta and Xiao 2011), etc. Specifically, the work in (Lee and Seung 2001) brings more attention to NMF in the research community, where the author proposed a simple multiplicative rule to solve the problem and showed the factor coherence of original image data. (Ding, He, and Simon 2005) shows that if adding orthogonal constrains, the NMF is equivalent to K -means clustering. Further, (Ding et al. 2006) presents a work that shows, when incorporating freedom control factors, the non-negative factors will achieve a better performance on classification. In this paper, motivated by previous NMF framework for learning the latent factors, we extend these efforts significantly and propose a comprehensive formulation which incorporates more physically-meaningful constraints for regularizing the learning process in order to find a proper solution. In this respect, our work is similar in spirit to (Hu, Wang, and Kambhampati 2013) which develops a factorization approach for sentiment analysis of social media responses to public events. Table 1: Notations Notation X T S V T0 S0 V0 R0 Dimension n×m n×t t×k m×k n×t t×k m×k n×k Description Input data matrix Data-term matrix Term-sentiment matrix Feature-sentiment matrix Prior knowledge on T Prior knowledge on S Prior knowledge on V Prior knowledge on the labels

3.1

Basic Model

Assuming that all the images can be partitioned into K sentiment (K = 3 in this paper as we focus on positive, neutral and negative. However, our framework can be easily extended to handle more fine-grained sentiment.) Our goal is to model the sentiment for each image based on visual features and available text features. Let n be the number of images and the size of contextual vocabulary is t. We can then easily cluster the images with similar word frequencies and predict the cluster's sentiment based on its word sentiment. Meanwhile, for each image, which has m-dimensional visual features (ANPs, see below), we can cluster the images and predict the sentiment based on the feature probability. Accordingly, our basic framework takes these n data points and decomposes them simultaneously into three factors: photo-text, text-sentiment and visual feature-sentiment. In other words, our basic model tries to solve the following optimization problem:
T SV

3

The Proposed RSAI Framework
min

X - T SV T

2 F

+ T - T0

2 F

In this section, we first propose the basic model of our framework. Then we show the details of how to generate the ANPs. After that, we describe how to obtain and leverage the prior knowledge to extend the basic model. We also analyze the algorithm in terms of its correctness and convergence. Table 1 lists the mathematical notation used in this paper.

(1)

subject to T  0, S  0, V  0; , where X  Rn×m represents input data matrix, and T  Rn×t indicates the text features. That is, the ith row of matrix T corresponds to the posterior probability of the ith image's contextual social network information referring to the t text terms (vocabulary). Similarly, S  Rt×k indicates the

posterior probability of a text belonging to k sentiments. Finally, V  Rm×k represents the sentiment for each ANP. The regularization term T0 is the term-frequency matrix for the whole word vocabulary (which is built based on textual descriptions of all photos). It is worth noting that the nonnegativity makes the latent components easy to interpret. As a result of this factorization, we can readily predict the image sentiment whether the contextual information (comments, user descriptions,etc.) is available or not. For example, if there is no social information associated with the image, then we can directly derive the image sentiment by applying non-negative matrix factorization for the input data X , when we characterize the sentiment of each image through a new matrix R = T × S . Specifically, our basic model is similar to the probabilistic latent semantic indexing (PLSI) (Hofmann 1999) and the orthogonal nonnegative tri-matrix factorization (Ding et al. 2006). In their work, the factorization means the joint distribution of documents and words.

ageNet(Deng et al. 2009). The scene detectors are trained on SUN dataset (Xiao et al. 2010). It is worth noting that selfie is one of most popular images on the web (Hu, Manikonda, and Kambhampati 2014) and face expression usually conveys strong sentiment, consequently, we also adopt one of state-of-the-art face detection methods proposed in (Zhu and Ramanan 2012). Adjective Detection: Modeling the adjectives is more difficult than nouns due to the fact that there are no well defined features to describe them. Following (Borth et al. 2013), we collect 20,000 images associate with specific adjective tags from Web. The a set of discriminative global features, including Gist, color histogram and SIFT, are applied for feature extraction. Finally the adjective detection is formulated as a traditional image classification problem based on Bag of words(BOW)model. The dictionary size of BOW is 1,000 with the feature dimension size 1,500 after dimension reduction based on PCA.

3.2

Extracting and Modeling Visual Features

3.3

Constructing Prior Knowledge

In (Tighe and Lazebnik 2013; Tu et al. 2005; Yuan et al. 2013), visual content can be described by a set of midlevel visual attributes, however, most of the attributes such as "car", "sky","grass", etc., are nouns which make it difficult to represent high level sentiments. Thus, we followed a more tractable approach (Borth et al. 2013), which models the correlation between visual attributes and visual sentiment with adjectives, such as "beautiful" , "awesome", etc. The reason for employing such ANPs is intuitive: the detectable nouns (visual attributes) make the visual sentiment detection tractable, while the adjectives add the sentiment strength to these nouns. In (Borth et al. 2013), a large scale ANPs detectors are trained based on the features extracted from the images and the labeled tags with SVM. However, we find that such pre-defined ANPs are very hard to interpret. For example the pairs like "warm pool" , "abandoned hospital", and it is very difficult to find appropriate features to measure them. Moreover, in their work, during the training stage, the SVM is trained on the features extracted from the image directly, the inability of localizing the objects and scales bounds the detection accuracy. To address these problems, we have a two stage approach to detect ANPs based on the Visual Sentiment Ontology (Borth et al. 2013) and train a one vs all classifier for each ANP. Noun Detection: The nouns in ANPs refer to the objects presented in the image. As one of fundamental tasks in computer vision, object detection has been studied for many years. One of most successful works is Deformable Part Model (DPM) (Felzenszwalb et al. 2010) with Histogram of Oriented Gradient (HOG) (Dalal and Triggs 2005) features. In (Felzenszwalb et al. 2010), the deformable part model has shown its capability to detect most common objects with rigid structure such as: car, bike and non-rigid objects such as pedestrian, dogs. (Pandey and Lazebnik 2011) further demonstrates that DPM can be used to detect and recognize scenes. Hence we adopt DPM to for nouns detection. The common objects(noun) are trained by the public dataset Im-

So far, our basic matrix factorization framework provides potential solution to infer the sentiment regarding the combination of social network information and visual features. However, it largely ignores the sentiment prior knowledge on the process of learning each component. In this part, we introduce three types of prior knowledge for model regularization: (1) sentiment-lexicon of textual words, (2) the normalized sentiment strength for each ANP, and (3) sentiment labels for each image. Sentiment Lexicon The first prior knowledge is from a public sentiment lexicon named MPQA corpus 2 . In this sentiment lexicon, there are 7,504 human labeled words which are commonly used in the daily life. The number of positive words (e.g."happy", "terrific") is 2,721 and the number of negative words (e.g. "gloomy", "disappointed") is 4,783. Since this corpus is constructed without respect to any specific domain, it provides a domain independent prior on word-sentiment association. It should be noted that the English usage in social network is very casual and irregular, we employ a stemmer technique proposed in (Han and Baldwin 2011). As a result, the ill-formed words can be detected and corrected based on morphophonemic similarity, for example "good" is a correct version of "goooooooooooood". Besides some abbreviation of popular words such as "lol"(means laughing out loud) is also added as prior knowledge. We encode the prior knowledge in a word sentiment matrix S0 where if the ith word belongs to jth sentiment, then S0 (i, j ) = 1, otherwise it equals to zero. Visual Sentiment In addition to the prior knowledge on lexicon, our second prior knowledge comes from the Visual Sentiment Ontology (VSO) (Borth et al. 2013), which is based on the well known previous researches on human emotions and sentiments (Darwin 1998; Plutchik 1980). It generates 3000 ANPs using Plutchnik emotion model and
2

http://mpqa.cs.pitt.edu/

Table 2: Sentiment strength score examples ANP innocent smile happy Halloween delicious food cloudy mountain misty forest ... Sentiment Strength 1.92 1.81 1.52 -0.4 -1.00 ...

multiplicative updating scheme shown in (Ding et al. 2006) to find the optimal solutions. First, we use fixed V and S to update T as follows: [XV S T + T0 + R0 S T ]ij [T SV T V S T + T + T SS T ]ij

Tij  Tij

(4)

Next, we use the similar update rule to update S and V : [T T XV + S0 + T T R0 ]ij [T T T SV T V + S + T T T S ]ij [X T T S + V0 ]ij [V S T T T T S + V ]ij

associates the sentiment strength (range in[-2:2] from negative to positive) by a wheel emotion interface3 . The sample ANP sentiment scores are shown in Table 2. Similar to the word sentiment matrix S0 , the prior knowledge on ANPs V0 is the sentiment indicator matrix. Sentiment labels of Photos Our last prior knowledge focuses on the prior knowledge on the sentiment label associated with the image itself. As our framework essentially is a semi-supervised learning approach, this leads to a domain adapted model that has the capability to handle some domain specific data. The partial label is given by the image sentiment matrix R0 where R0  Rn×k . For example if the ith image belongs to jth sentiment, the R0 (i, j ) = 1 otherwise R0 (i, j ) = 0. The improvement by incorporating these label data is empirically verified in the experiment section.

Sij  Sij

(5)

Vij  Vij

(6)

The learning process consists of an iterative procedure using Eq (3), Eq (4) and Eq (5) until convergence. The description of the process is shown in Algorithm 1. Algorithm 1 Multiplicative Updating Algorithm Input: X, T0 , S0 , V0 , R0 , , , ,  Output: T, S, V Initialization: T, S, V while Not Converge do Update T using Eq(4) with fixed S,V Update S using Eq(5) with fixed T,V Update V using Eq(6) with fixed T,S End

3.4

Incorporating Prior Knowledge

After defining the three types of prior knowledge, we incorporate them into the basic model as regularization terms in following optimization problem:
T SV

min X - T SV T +  T - T0

2 F 2 F

+  V - V0 +  S - S0

2 F 2 F

3.5

Algorithm Correctness and Convergence

(2) 2 +  T S - R0 F subject to T  0, S  0, V  0 where   0,   0,   0 and   0 are parameters controlling the extent to which we enforced the prior knowledge on the respective components. The model above is generic and allows flexibility . For example, if there is no social information available for one image, we can simply set the corresponding row of T0 to zeros. Moreover, the square loss function leads to an unsupervised problem for finding the solutions. Here, we re-write Eq (2) as : L =T r(X T X - 2X T T SV T + V S T T T T SV T ) + T r(V T V - 2V T V0 + V0T V )
T + T r(T T T - 2T T T0 + T0 T0 )

In this part, we prove the guaranteed convergence and correctness for Algorithm 1 by the following two theorems. Theorem 1. When Algorithm 1 converges, the stationary point satisfies the Karush-Kuhn-Tuck(KKT) condition, i.e., Algorithm 1 converges correctly to a local optima. Theorem 2. The objective function is nondecreasing under the multiplicative rules of Eq (4), Eq (5) and Eq (6), and it will converge to a stationary point. The detailed proof is presented in Appendix.

4

Empirical Evaluation

We now quantitatively and qualitatively compare the proposed model on image sentiment prediction with other candidate methods. We also evaluate the robustness of the proposed model with respect to various training samples and different combinations of prior knowledge. Finally, we perform a deeper analysis of our results.

(3)
T R0 R0 )

+ +

T T r(S S - 2S S0 + S0 S0 ) T T T T T r(S T T S - 2S T R0 +

T

T

4.1

Experiment Settings

From Eq (3) we can find that it is very difficult to solve T , S and V simultaneously. Thus we employ the alternating
3

http://visual-sentiment-ontology.appspot.com

We perform the evaluation on two large scale image datasets collected from Flickr and Instagram respectively. The collection of Flickr dataset is based on the image IDs provided by (Yang et al. 2014), which contains 3,504,192 images from 4,807 users. Because some images are unavailable now, and without loss of generality, we limit the number of

images from each user. Thus, we get 120,221 images from 3921 users. For the collection of the Instagram dataset, we randomly pick 10 users as seed nodes and collect images by traversing the social network based on breadth first search. The total number of images from Instagram is 130,230 from 3,451 users. Establishing Ground Truth: For training and evaluating the proposed method, we need to know the sentiment labels. Thus, 20,000 Flickr images are labeled by three human subjects, the majority voting is employed. However, manually acquiring the labels for these two large scale datasets is expensive and time consuming. Consequently, the rest of more than 230,000 images are labeled by the tags, which was suggested by the previous works (Yang et al. 2014; Go, Bhayani, and Huang )4 . Since labeling the images based on the tags may cause noise issue, and for better reliability we only label the images with primary sentiment labels, which include: positive, neutral and negative. It is worth noting that the human labeled images have both primary sentiment labels and fine grained sentiment labels. The fine grained labels, including: happiness, amusement, anger, fear, sad and disgust, are used to for fine grained sentiment prediction. The comparison methods include: Senti API5 , SentiBank (Borth et al. 2013), EL(Yang et al. 2014) and the baseline method. · Senti API is a text based sentiment prediction API, it measures the text sentiment by counting the sentiment strength for each text term. · SentiBank is a state-of-the-art visual based sentiment prediction method. The method extracts a large number of visual attributes and associates them with a sentiment score. Similar to Senti API, the sentiment prediction is based on the sentiment of each visual attributes. · EL is a graphical model based approach, it infers the sentiment based on the friend interactions and several low level visual features. · Baseline: The baseline method comes from our basic model. To compare it fairly, we also introduce R0 with the basic model which makes the baseline method have the ability to learn from training data.

T0  Rn×t , where n is the number of images, m = 1200 and t is the vocabulary size, we decompose it via Aglorithm 1 and get the label based on max pooling each row of X  V . It is worth noting that in the proposed model, tags are not included as input feature. The results of comparison are shown in Table 3. We employ 30% data for training and remaining for testing. To verify the reliability of tags labeled images, we also included 20000 labeled Flickr images with primary sentiment label. Especially, the classifier setting for SentiBank and EL followed the original papers. The classifier of Sentibank is logistic regression and for EL it is SVM. From the results we can see that, the proposed method performs best in both datasets. Noting that proposed method improved 10% and 6% over state-of-the-art methods (Borth et al. 2013). Results from proposed method are shown in Figure 4. Noting that the number we reported in Table 3 is the prediction accuracy for each method. From the table, we can see that, even though noise exists in the Flickr and Instagram dataset, the results are similar to the performance on human labeled dataset. Another interesting observation is that the performance of EL on Instagram is worse than on Flickr, one reason could be that the wide usage of "picture filters" lowers discriminative ability of the low level visual features, while the models based on the mid level attributes can easily avoid this filter ambiguity. Another interesting observation is that our basic model performs fairly well even if it does not incorporate the knowledge from sentiment strength of ANPs, which indicates that the object based ANPs by our method are more robust than the features used in (Borth et al. 2013). Fine Grained Sentiment Prediction: Although our motivation is to predict the sentiment (positive, negative) on the visual data, to show the robustness and extension capability of the proposed model, we further evaluate the proposed model on a more challenging task in social media; predicting human emotions. Based on the definition of human emotion (Ekman 1992), our fine grained sentiment study labels the user posts with following human emotion categories including: happiness, amusement, disgust, anger, fear and sadness. The results on 20000 manually labeled flickr post are shown in Figure 5. Compared to sentiment prediction, fine grained sentiment prediction would give us more precise user behavior analysis and new insights on the proposed model. As Figure 5 shows, compared to SentiBank and EL, the proposed method has the highest average classification accuracy and the variance of proposed method on these 6 categories is smaller than that of the baseline methods, which demonstrates the potential social media applications of the proposed method such as predicting social response. We noticed that the sad images have the highest prediction accuracy, and both disgust and anger are difficult to predict. Another observation is the average performance of positive categories, happiness and amusement, is similar to the negative categories. Explaining reason for this drives us to dig deeper into sentiment understanding in the following section.

4.2

Performance Evaluation

Large scale image sentiment prediction: As mentioned in Sec 3, the proposed model has the flexibility to incorporate the information and capability to jointly learn from the visual features and text features. For each image, the visual features are formed by the confidence score of each ANP detector, the feature dimension is 1200, which is as large as VSO (prior knowledge V0 ). For the text feature, it is formed based on the term frequency and the dimension relies on the input data. To predict the label, the model input is unknown data X  Rn×m and its corresponding text feature matrix
More details can be found in(Yang et al. 2014) and (Go, Bhayani, and Huang ) 5 http://sentistrength.wlv.ac.uk/,a text based sentiment prediction API
4

(a) Negative

(b) Neutral

(c) Positive

Figure 3: Sample tag labeled images from Flickr and Instagram.

(a) Negative

(b) Neutral

(c) Positive

Figure 4: Sample results from our proposed method. Photos with red bounding box are false positive predictions.

4.3

Analysis and Discussion

In this section, we present an analysis of parameters for the proposed method and the results of the proposed method. Specifically, in last section we have studied the performance of different methods. In this part, our objective is to have deeper understanding on the datasets and the correlation between different features and the sentiments embedded in the images. Without loss of generality, we collected additional 20k images from Flickr and Instagram respectively (totally 40K) and we address the following research questions: · RQ1:What is the relationship between visual features and visual sentiments? · RQ2:Since the proposed method is better than pure visual feature based method, How does the model gain? First, we start with RQ1 by extracting the visual features used in (Borth et al. 2013) and (Yang et al. 2014) for each image in the Flickr and Instagram datasets. Then we use k-means clustering to obtain 3 clusters of images for each dataset, where the image similarity is measured as Euclidean distance in the feature spaces. Based on each cluster center, we used the classfier trained in the previous experiment for cluster labeling. The results are shown in Figure 6. The xaxis is the different class label for each dataset and the y-axis is the number of images that belong to each cluster. From the

Figure 5: Fine grained sentiment prediction results (Y-axis represents the accuracy for each method).

Table 3: Sentiment Prediction Results. The number means prediction accuracy, the higher the better. 20000 Flickr Flickr Instagram Senti API 0.32 0.34 0.27 SentiBank 0.42 0.47 0.56 EL 0.47 0.45 0.37 Baseline 0.48 0.48 0.54 Proposed method 0.52 0.57 0.62

Figure 6: Sentiment distribution based on visual features. From left to rigth is number of positive, neutral, negative images in Instagram and Flickr, receptively. Y axis represents the number of images.

Figure 7: Performance gain by incorporating training data.

results, we notice that the "visual affective gap" does exist between human sentiment and visual features. For the stateof-the art method (Borth et al. 2013), the neural images are largely misclassified based on the visual features. While for (Yang et al. 2014), we observe t the low level features, e.g., color histogram, contrast and brightness, are not closely related to human sentiment as visual attributes. We further analyze the performance of the proposed method based on these 40,000 images. Parameter study: In the proposed model, we incorporate three types of prior knowledge: sentiment lexicon, sentiment labels of photos and visual sentiment for ANPs. It is important and interesting to explore the impact of each of them on the performance of the proposed model. Figure 7 presents the average results (y-axis) of two datasets on sentiment prediction with different amount of training data (xaxis)6 , where the judgment is on the same three sentiment labels with different combinations respectively. It should be noted that each combination is optimized by Algorithm 1, which has similar formulations. Moreover, we set the same parameter for , ,  and  (0.9, 0.7, 0.8 and 0.7). Results give us two insights. First, employing more prior knowledge will make the model more effective than using only one type of prior knowledge. For our matrix factorization framework, T and V have independent clustering freedom by introducing S , thus it is natural to add more constraints for desired decomposed component. Second, when no training data, the basic model with S0 performs much better than SentiAPI (refer Table 3), which means incorporating ANPs significantly improves image sentiment prediction. It is worth noting that there is no training stage for the proposed method. Thus when compared to fully supervised approaches, our
6

Figure 8: The value of  versus model performance. X axis is  value, y axis is value of model performance.

method is more applicable in practice when the label information is unavailable. Bridging the Visual Affective Gap (RQ2): Figure 1 and Figure 7 demonstrate that a visual affective gap exists between visual features and human sentiments (i.e., the same visual feature may correspond to different sentiments in different context). To bridge this gap, we show that one possible solution is to utilize heterogeneous data and features available in social media to augment the visual feature-based sentiment. In the previous parameter study, we have studied the importance of the prior knowledge. Furthermore, we study importance of  which contains the degree of contextual social information used in the proposed model. From Figure 8, we can observe that the performance of the proposed model increases along the value of  . However, when  is greater than 0.8, the performance drops. This is because textual information in social media data is usually incomplete. Larger  will cause negative effects on the prediction accuracy where there is none or little information available.

The experiments setting is as same as discussed above.

5

Conclusion and Future Work

Can we learn human sentiments from the images on the web? In this paper, we proposed a novel approach for visual sentiment analysis by leveraging several types of prior knowledge including: sentiment lexicon, sentiment labels and visual sentiment strength. To bridge the "affective gap" between low-level image features and high-level image sentiment, we proposed a two-stage approach to general ANPs by detecting mid-level attributes. For model inference, we developed a multiplicative update algorithm to find the optimal solutions and proved the convergence property. Experiments on two large-scale datasets show that the proposed model is superior to other state-of-the-art models in both inferring sentiment and fine grained sentiment prediction. In the future, we will employ crowdsourcing tools, such as AmazonTurk7 , to obtain high-quality, manually-labeled data to test the proposed method. Furthermore, inspired by the recent development of advanced deep learning algorithms and their success in image classification and detection tasks, we will follow this research direction to perform the sentiment analysis via deep learning. In order to have a robust trained architecture and network parameters, we will focus on the deep learning models that work for smaller dataset. Moreover, beyond sentiment analysis, we will study social event and social response (Hu et al. 2012; Hu, Manikonda, and Kambhampati 2014) via visual data in the social media.

6

Acknowledgment

Yilin Wang and Baoxin Li are supported in part by a grant (#1135616) from the National Science Foundation. Kambhampati's research is supported in part by the ARO grant W911NF-13-1- 0023, and the ONR grants N00014-131-0176, N00014-13-1-0519 and N00014-15-1-2027. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.

References
Borth, D.; Ji, R.; Chen, T.; Breuel, T.; and Chang, S.-F. 2013. Large-scale visual sentiment ontology and detectors using adjective noun pairs. In Proceedings of the 21st ACM international conference on Multimedia, 223­232. ACM. Dalal, N., and Triggs, B. 2005. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, 886­893. IEEE. Darwin, C. 1998. The expression of the emotions in man and animals. Oxford University Press. Das Gupta, M., and Xiao, J. 2011. Non-negative matrix factorization as a feature selection tool for maximum margin classifiers. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, 2841­2848. IEEE. De Choudhury, M.; Counts, S.; and Gamon, M. 2012. Not all moods are created equal! exploring human emotional states in social media. In Sixth International AAAI Conference on Weblogs and Social Media.
7

https://www.mturk.com/mturk/welcome

Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, 248­255. IEEE. Ding, C.; Li, T.; Peng, W.; and Park, H. 2006. Orthogonal nonnegative matrix t-factorizations for clustering. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, 126­135. ACM. Ding, C. H.; He, X.; and Simon, H. D. 2005. On the equivalence of nonnegative matrix factorization and spectral clustering. In SDM, volume 5, 606­610. SIAM. Ekman, P. 1992. An argument for basic emotions. Cognition & Emotion 6(3-4):169­200. Felzenszwalb, P. F.; Girshick, R. B.; McAllester, D.; and Ramanan, D. 2010. Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence, IEEE Transactions on 32(9):1627­1645. Go, A.; Bhayani, R.; and Huang, L. Twitter sentiment classification using distant supervision. Han, B., and Baldwin, T. 2011. Lexical normalisation of short text messages: Makn sens a# twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, 368­378. Association for Computational Linguistics. Hofmann, T. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, 50­57. ACM. Hu, M., and Liu, B. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, 168­177. ACM. Hu, Y.; John, A.; Wang, F.; and Kambhampati, S. 2012. Et-lda: Joint topic modeling for aligning events and their twitter feedback. In Proceedings of the 6th AAAI Conference. Hu, Y.; Manikonda, L.; and Kambhampati, S. 2014. What we instagram: A first analysis of instagram photo content and user types. Hu, Y.; Wang, F.; and Kambhampati, S. 2013. Listening to the crowd: automated analysis of events via aggregated twitter sentiment. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, 2640­2646. AAAI Press. Jia, J.; Wu, S.; Wang, X.; Hu, P.; Cai, L.; and Tang, J. 2012. Can we understand van gogh's mood?: learning to infer affects from images in social networks. In Proceedings of the 20th ACM international conference on Multimedia, 857­860. ACM. Lee, D. D., and Seung, H. S. 1999. Learning the parts of objects by non-negative matrix factorization. Nature 401(6755):788­791. Lee, D. D., and Seung, H. S. 2001. Algorithms for non-negative matrix factorization. In Advances in neural information processing systems, 556­562. Liu, B. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies 5(1):1­167. Machajdik, J., and Hanbury, A. 2010. Affective image classification using features inspired by psychology and art theory. In Proceedings of the international conference on Multimedia, 83­92. ACM. Pandey, M., and Lazebnik, S. 2011. Scene recognition and weakly supervised object localization with deformable part-based models. In Computer Vision (ICCV), 2011 IEEE International Conference on, 1307­1314. IEEE.

Pang, B., and Lee, L. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, 271. Association for Computational Linguistics. Pang, B.; Lee, L.; and Vaithyanathan, S. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, 79­86. Association for Computational Linguistics. Plutchik, R. 1980. Emotion: A psychoevolutionary synthesis. Harper & Row New York. Song, M.; Tao, D.; Liu, Z.; Li, X.; and Zhou, M. 2010. Image ratio features for facial expression recognition application. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on 40(3):779­788. Taboada, M.; Brooke, J.; Tofiloski, M.; Voll, K.; and Stede, M. 2011. Lexicon-based methods for sentiment analysis. Computational linguistics 37(2):267­307. Tighe, J., and Lazebnik, S. 2013. Finding things: Image parsing with regions and per-exemplar detectors. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, 3001­ 3008. IEEE. Tu, Z.; Chen, X.; Yuille, A. L.; and Zhu, S.-C. 2005. Image parsing: Unifying segmentation, detection, and recognition. International Journal of Computer Vision 63(2):113­140. Wang, Y.; Jia, Y.; Hu, C.; and Turk, M. 2005. Non-negative matrix factorization framework for face recognition. International Journal of Pattern Recognition and Artificial Intelligence 19(04):495­511. Xiao, J.; Hays, J.; Ehinger, K. A.; Oliva, A.; and Torralba, A. 2010. Sun database: Large-scale scene recognition from abbey to zoo. In Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, 3485­3492. IEEE. Yang, Y.; Jia, J.; Zhang, S.; Wu, B.; Li, J.; and Tang, J. 2014. How do your friends on social media disclose your emotions? You, Q.; Luo, J.; Jin, H.; and Yang, J. 2015. Robust image sentiment analysis using progressively trained and domain transferred deep networks. Yuan, J.; Mcdonough, S.; You, Q.; and Luo, J. 2013. Sentribute: image sentiment analysis from a mid-level perspective. In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, 10. ACM. Zhu, X., and Ramanan, D. 2012. Face detection, pose estimation, and landmark localization in the wild. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, 2879­2886. IEEE.

Where µ is Lagrangian multiplier µij enforces the nonnegativity constraint on Vij . From the complementary slackness condition, we can obtain (2(V S T T T T S + V ) - 2(X T T S + V0 ))ij Vij = 0 (8) This is the fixed point relation that local minima for V must hold. Given the Algorithm 1., we have the convergence point to the local minima when Vij = Vij [X T T S + V0 ]ij [V S T T T T S + V ]ij (9)

Then the Eq (9) is equivalent to
2 (2(V S T T T T S + V ) - 2(X T T S + V0 ))ij Vij = 0 (10)

This is same as the fixed point of Eq (8),i.e., either Vij = 0 or the left factor is 0. Thus if Eq (10) holds the Eq (8) must hold and vice versa.

Theorem 2. The objective function is nondecreasing under the multiplicative rules of Eq (4), Eq (5) and Eq (6), and it will converge to a stationary point. Proof of Theorem 2. First, let H (V ) be: H (V ) = T r((V S T T T T S +V )V T -(X T T S +V0 +µ)V T ) (11) and it is very easy to verify that H (V ) is the Lagrangian function of Eq (3) with KKT condition. Moreover, if we can verify that the update rule of Eq (4) will monotonically decrease the value of H (V ), then it means that the update rule of Eq (4) will monotonically decrease the value of L(V )(recall Eq (3)). Here we complete the proof by constructing the following an auxiliary function h(V, V ). h(V, V ) =
ik 2 (V (V S T T T T S + V ))ik Vik

Vik (X T T S + V0 + µ)ik Vik (1 + log Vik

A

Appendix

Theorem 1. When Algorithm 1 converges, the stationary point satisfies the Karush-Kuhn-Tuck(KKT) condition, i.e., Algorithm 1 converges correctly to a local optima. Proof of Theorem 1. We prove the theorem when updating V using Eq (6), similarly, all others can be proved in the same way. First we form the gradient of L regards V as Lagrangian form: L = 2(V S T T T T S + V ) - 2(X T T S + V0 ) - µ (7) V

) Vi k (12) Since z  (1 + log z ), z > 0 and similar in (Ding et al. 2006), the first term in h(V, V ) is always larger than that in H (V ), then the inequality holds h(V, V )  H (V ). And it is easy to see h(V, V ) = H (V ), thus h(V, V ) is an auxiliary function of H (V ). Then we have the following inequality chain:
ik

-

H (V 0 ) = h(V 0 , V 0 )  h(V 0 , V 1 ) = H (V 1 )....

(13)

Thus, with the alternate updating rule of V, S and T , we have the following inequality chain: L(V 0 , T 0 , S 0 )  L(V 1 , T 0 , S 0 )  L(V 1 , T 1 , S 0 ).... (14) Since L(V, S, T )  0. Thus L(V, S, T ) is bounded and the Algorithm 1 converges , which completes the proof.

1206

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

Relative Hidden Markov Models for Video-Based Evaluation of Motion Skills in Surgical Training
Qiang Zhang, Student Member, IEEE and Baoxin Li, Senior Member, IEEE
Abstract--A proper temporal model is essential to analysis tasks involving sequential data. In computer-assisted surgical training, which is the focus of this study, obtaining accurate temporal models is a key step towards automated skill-rating. Conventional learning approaches can have only limited success in this domain due to insufficient amount of data with accurate labels. We propose a novel formulation termed Relative Hidden Markov Model and develop algorithms for obtaining a solution under this formulation. The method requires only relative ranking between input pairs, which are readily available from training sessions in the target application, hence alleviating the requirement on data labeling. The proposed algorithm learns a model from the training data so that the attribute under consideration is linked to the likelihood of the input, hence supporting comparing new sequences. For evaluation, synthetic data are first used to assess the performance of the approach, and then we experiment with real videos from a widely-adopted surgical training platform. Experimental results suggest that the proposed approach provides a promising solution to video-based motion skill evaluation. To further illustrate the potential of generalizing the method to other applications of temporal analysis, we also report experiments on using our model on speech-based emotion recognition. Index Terms--Relative hidden markov model, relative learning, temporal model, emotion recognition, surgical skill

Ç
1 INTRODUCTION
and quantifiable performance metrics, overcoming the shortcomings in traditional training that relies on costly practice of direct supervision by senior surgeons. Recognizing the sequential nature of motion data, many analysis approaches utilize state-transition models, such as the Hidden Markov Model (HMM). For example, [5] provided an HMM-based method to evaluate surgical residents' learning curve. The method first constructs different HMMs for each different levels of expertise, and then calculates a probability distance between the expert and a novice resident. The magnitude of the probability distance is used to rate the level of the novice resident. HMM was also adopted in [6] to measure motion skills in surgical tasks, where a recorded video is first segmented into basic gestures based on velocity and angle of movement, with segments of the gestures corresponding to the states of an HMM. In [7], Hierarchical Dircichlet process hidden Markov model (HDPHMM [8]) was utilized, which relaxed the requirement of predefining the number of the states for the model. One practical difficulty in these approaches is that they require the skill labels for the training data since the HMMs are typically learned from sets of data streams with corresponding skill levels. Labeling the skill of a trainee is currently done by senior surgeons, which is not only a costly practice but also one that is subjective and less quantifiable. Thus it is difficult, if not impossible, to obtain a large amount of data with sufficiently reliable skill labels for HMM training. This problem has also been encountered in other fields such as image classification. For example, in [9], it was argued that using binary labels to describe images is not only too restrictive but also unnatural and thus relative visual attributes were used and classifiers were trained based on such features. Relative information has also been used in other applications, e.g., distance metric learning [10], face verification [11], and human-machine interaction [12].

UMAN capability in mastering body motion is the key in domains such as sports, rehabilitation, surgery and dance. Computer-based approaches have been developed over the years for facilitating acquiring (e.g., training in sports and surgery) or regaining (e.g., in rehabilitation) such motion-related skills by human subjects. One central task faced by systems using such approaches is the analysis of motion skills based on some temporal sensory data. With such analysis, skill metrics may be extracted and assigned to a given movement and feedback may accordingly be provided to the subjects for taking actions to improve the underlying skill. For example, [1] utilized control trajectories and motion capture data for human skill analysis, [2] reported motion skill analysis in sports using data from motion sensors, [3] studied computational skill rating in manipulating robots, and [4] considered hand movement analysis for skill evaluation in console operation. Among others, surgery-related applications have attracted increasing interests, where motion expertise is the primary concern. To improve their motion expertise, surgeons often have to go through lengthy training processes. In recent years, simulation-based surgical training platforms have been developed and widely applied in surgical education. One prominent example is the Fundamentals of Laparoscopic Surgery (FLS) Trainer Box (www.flsprogram.org). With such platforms, it is possible to develop computational approaches to provide objective

H



The authors are with the Computer Science and Engineering, Arizona State University, Tempe AZ 85287. E-mail: {qzhang53, baoxin.li}@asu.edu.

Manuscript received 24 Feb. 2014; revised 21 Sept. 2014; accepted 25 Sept. 2014. Date of publication 1 Oct. 2014; date of current version 8 May 2015. Recommended for acceptance by D. Xu. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TPAMI.2014.2361121

0162-8828 ß 2014 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1207

In this paper, we propose a novel formulation termed Relative Hidden Markov Model and develop an algorithm for obtaining a solution under this model. The proposed method utilizes only relative ranking (based on certain attribute of interest, or motion skill in the surgical training application) between pairs of inputs, which is easier to obtain and often more consistent. This is especially useful for applications like video-based surgical training, where the trainees go through a series of training sessions with their skills improving over time, and thus the time of the sessions would already provide natural relative ranking of the skills at the corresponding time. The proposed algorithm effectively learns a model from the training data so that the attribute under consideration (i.e., the motion skill in our application) is linked to the likelihood of the inputs under the learned model. The learned model can then be used to compare new data pairs. For evaluation, we first design synthetic experiments to systematically evaluate the model and the algorithm, and then experiment with real data captured on a commonly-used surgical training platform. The experimental results suggest that the proposed approach provides a promising solution to the real-world problem of motion skill evaluation from video. The key contribution of the work lies in the novel formulation of learning temporal models using only relative information and the proposed algorithm for obtaining solutions under the formulation. Discussion of its relationship to the latent support vector machine is also provided to assist the understanding of why the proposed formulation is suitable for the proposed scenarios. Additional contributions include the specific application of the proposed method to the problem of video-based motion skill evaluation in surgical training, which has seen increasing importance in recent years. An earlier exposition of the proposed method can be found in [13]. This current paper represents a full exploration of the method, including a new learning algorithm that is more efficient, new comparative analysis of the method, and new and updated experiments. In particular, to illustrate that the proposed model is general in nature but not confined to video-based skill analysis, we report its application to a different problem, emotion recognition using speech. To facilitate further exploration and validation by other researchers, source code accompanying this paper has been made publicly available.1 In the remainder of this paper, we first review some of the related work in Section 2 and describe basic notations of the HMM in Section 3. The proposed method is then presented in Section 4, including a new algorithm for obtaining solutions in Section 4.3 and discussion of its relationship to latent support vector machine in Section 4.4. The proposed method is evaluated on three types of data in Section 5, including synthetic data (Section 5.1) and videos from surgical simulation systems (Section 5.2), and speech data (Section 6). The paper is concluded in Section 7. In this paper, we use upper-case bold font (e.g., X) for matrices, lower-case bold font (e.g., x) for vectors. We use Xi to reprei sent ith sequence, Xi t for the tth frame of sequence X .

2

RELATED WORK

1. The code is available at www.public.asu.edu/~bli24/Code SoftwareDatasets.html.

In this section, we first review two categories of existing work, discriminative learning for hidden Markov models and learning based on relative information, which are most related to our approach. Distinction between our proposed method and the reviewed work will be briefly stated. We also briefly discuss a few more related efforts on skill evaluation in surgery. Discriminative learning for HMM. Maximum-likelihood methods for learning HMM (e.g., the forward-backward algorithm) in general do not guarantee the discrimination ability of the learned models. To this end, several discriminative learning methods for HMM have been proposed. In [14], a discriminative training method for HMM was proposed based on perceptron algorithms. The methods iterates between the Viterbi algorithm and the additive update of the models. Hidden Markov support vector machine (HM-SVM) was proposed in [15], which combines SVM with HMM to improve the discrimination power of the learned model. These methods are "supervised" in nature, and thus the labeling of the state sequence is required for the training data, which limits their practical use. In [16], another discriminative learning method for HMM was proposed, which only requires the labels of the training sequences. The method initializes the HMMs with maximumlikelihood method and then updates the models with SVM. One drawback is that, the updated models do not always lead to valid HMMs, which could be problematic for a physics-driven problem where the model states have real meanings (like the gesture elements in [6]). Our proposed method requires neither the labeling of the states nor the class label for the training sequences, which are difficult to obtain or even not accessible in many applications. Instead, only a relative ranking of the training data is used, and the resultant model is a valid HMM. Learning with relative information. Several methods for learning with relative information have been proposed recently. In [10], a distance metric is learned from relative comparisons. Considering the limited training examples for object recognition, [17] proposes an approach based on comparative objective similarities, where the learned model scores high for objects of similar categories and low for objects of dissimilar categories. In [11], comparative facial attributes were learned for face verification. The method of [9] learns relative attributes for image classification and the problem is formulated as a variation of SVM. Similar idea was also been used in [12] for the purpose of human-machine interaction. In [18], relative attribute feedback, e.g., "Shoe images like these, but sportier", is used to improve the performance of image search. Relative information between scene categories has also been used to enhance the performances of scene categorization in [19]. These approaches are mostly for image-based attributes, whereas our current task is on modeling sequential data, for which it is natural to assume that the most relevant attributes (e.g., motion skills) are embedded in a temporal structure. This is what our proposed method attempts to address. Efforts has been observed for estimating the true continuous label of the data from a set of pairwise ranking of training data [20], [21]. However,

1208

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

those methods do not directly learn a model for ranking/ labeling new data. Skill evaluation for surgical simulations. Objective evaluation of surgical skills has been a topic of research for many years. The authors of [22], [23] used the time of each data, total path traveled and the number of hand movements to rate the surgical skills. It is evident that some of the criteria recommended in these studies (e.g., time of completion) may be relatively easily measured with proper sensory data, while some others cannot be (e.g., respect for tissues). A technique proposed in [24] called task deconstruction was implemented in a recent system by [25]. They used Markov Models to model a sequence of force patterns or positions of the tools. They showed that their Markov Models were suitable for decomposing a task (such as suturing) into basic gestures, and then the proficiency of the complex gesture could be analyzed. While this study offered an intriguing approach to expertise analysis, it required an expert surgeon to provide specifications for building the topology of the model; hence it cannot be easily generalized to new procedures. A similar idea was also utilized in [26]. Jun et al. [27] proposed to segment the training data into modular sub-procedures or therbligs and performance is measured over each sub-procedure.

4

PROPOSED METHOD

Based on the previous discussion, we are concerned with a new problem of learning temporal models using only relative information. This is a problem arising naturally in many applications involving motion or video data. In the case of video-based surgical training, the focus is on learning to rate/compare the performance of the trainees from recorded videos capturing their motion. To this end, in recognition of some fruitful trials of HMMs in this application domain, we propose to formulate the task as one of learning a Relative Hidden Markov Model, which not only maximizes the likelihood of the training data, but also maintains the given relative rankings of the input pairs. In its most basic form, the proposed model can be formally expressed as (following the notations defined in Eqn. (1)) u s:t: : max
u

Y

pðXi juÞ
j

F ðX ; uÞ > F ðX ; uÞ; 8ði; jÞ 2 E;

Xi 2 X i

(2)

3

BASIC NOTATIONS OF HMM

In this section, we briefly describe HMM and introduce some basic notations that will be used later. An HMM can be defined by a set of parameters: the initial transition probabilities p 2 RK Â1 , the state transition probabilities A 2 RK ÂK and the observation model ffk gK k¼1 , where K is the number of states. There are two central problems in HMM: 1) learning a model from the given training data; and 2) evaluating the probability of a sequence under a given model, i.e., the decoding problem. In the learning problem, one learns the model (u) by maximizing the likelihood of the training data (X): uÃ : max
u

where F ðX; uÞ is a score function for data X given by model u, which is introduced to maintain the relative ranking of the pair Xi and Xj and E is the set of given pairs with prior ranking constraint. Different score functions may be defined, e.g., data likelihood and data likelihood ratio, as described in the following sections in Section 4.1 and Section 4.2. From this formulation, the difference between the proposed method and any of the existing HMM-based methods is obvious. In an existing HMM-based method, a set of models is trained using the training data of each category independently. That is, explicit class labels are required for each training sequence. The proposed model has the following unique features: The model does not require explicit class labels. What needed is only a relative ranking.  The model explicitly considers the ranking constraint between given data pairs, whereas independently-trained HMMs in existing methods cannot guarantee it.  Only one model is learned for the entire set of data. There are two benefits: more data for training and less computation during testing. Our method is also different from the existing work on learning with relative attributes in that it models sequential data and the relative ranking information is capsulated in a temporal dynamic model of HMM (albeit new algorithms are thus called for), which has demonstrated performance in modeling physical phenomena like human movements. In the following sections, we present two instantiations of the general model expressed in Eqn. (2), and develop the corresponding algorithms in each case. It will become clear that the first model (Section 4.1), while being intuitive, has some practical difficulties, which motivated us to develop the improved model of Section 4.2. Both models/algorithms are presented (and evaluated later in Section 5) for the progressive nature of the methods and for facilitating the understanding of the improved model and algorithm of Section 4.2, which is the recommended solution. 

Y
Xi 2 X

pðXi juÞ $ max
u

X
Xi 2X

log pðXi juÞ;

(1)

where X is the set of i.i.d. training sequences. One efficient solution to the above problem is the wellknown Baum-Welch algorithm [28]. Another scheme, namely the segmental K-means algorithm [29], may also be used to seek a solution, and it has been shown that the likelihoods under models estimated by either of the two algorithms are very close [29]. When the training data include sequences of multiple categories, multiple models would be learned and each model will be learned from data of each category independently. In the decoding problem, given a hidden Markov model, one needs to determine the probability of a given sequence X being generated by the model. Generally we are more interested in the probability associated with the optimal state sequence (zÃ ), i.e., pðX; zÃ juÞ ¼ maxz pðX; zjuÞ. The optimal state path can be found via the Viterbi algorithm. To use HMM in classification, we first compute the probability of the given sequence drawn from each model, then we choose the model yielding the maximal probability.

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1209

4.1 The Baseline Model While one may use different score functions for F in Eqn. (2) for comparing the input pairs, upon successful training the likelihoods of the sequences should reflect the original ranking. Hence we may set F ðXi ; uÞ ¼ pðXi juÞ. With this, the formulation in Eqn. (2) can be rewritten as Y u : max pðXi juÞ u i X 2X (3) s:t: pðXi juÞ > pðXj juÞ; 8ði; jÞ 2 E:
It has been proven in [30] that, the marginal likelihood is dominated by the likelihood with the optimal path and their difference decreases exponentially with the length (number of frames) of a sequence. This idea was used in segmental K-means algorithm and similarly we can approximate the marginal data likelihood pðXjuÞ by the likelihood with optimal path pðX; zÃ juÞ (when there is no ambiguity, we will use z for zÃ ), which can be written as: log pðX; zjuÞ ¼ log pðX1 jfz1 Þ þ log pðz1 Þ þ
T Â X t¼2

we evaluate the data likelihood via the Viterbi algorithm and use the logarithm of the data likelihood as the score of the data. By definition, the obtained scores can be used to compare the pair.

log pðXt jfzt Þ þ log Aðzt jztÀ1 Þ

(4)

For some observation models, e.g., multinomial (more details in Appendix A), we can write log pðXi ; zi juÞ ¼ uT hðXi ; zi Þ. Accordingly, Eqn. 3 can be finally written as X u : max uT hðXi ; zi Þ u2V i:Xi 2X (5) T i i T j j s:t: u hðX ; z Þ ! u hðX ; z Þ þ r; 8ði; jÞ 2 E; where r ! 0 defines the required margin between the logarithms of likelihood for a pair of data and V defines the set of valid parameters for the hidden Markov model, i.e.: X euðiÞ ¼ 1; uðiÞ 0 ; X
i:uðiÞ2logðAj Þ i:uðiÞ2logðpÞ

4.2 The Improved Model In the model described in Eqn. (7), we compare the logarithm of the data likelihood, which is, according to Eqn. (4), roughly proportional to the length of the data. Thus a shorter sequence is likely to have a larger score. This means that the learned model would be biased towards shorter sequences. If the observation describes a long, periodic event, e.g., repeating an action multiple times within a sequence, we may consider normalizing the logarithm of the data likelihood by the number of frames of the observation. However, this cannot be applied directly for non-periodic observations like sequences from surgical simulation, where the length of a sequence (corresponding to the time taken for completing a task) is one of the skill metrics. To overcome the above practical problem, we consider an improved version. Recall that in HMM, we classify a sequence based on the model with which the sequence gets the maximal likelihood, i.e., it is the ratio of data likelihood with different models that decides the label of the data. For ^ju1 Þ ðX ; z example, if log p ~ju2 Þ > 0, then we assign X to Model u1 . pðX;z Thus we propose to use the ratio of the data likelihoods of ðX ; ^ zju1 Þ two HMMs as the score function, i.e., F ðX; uÞ ¼ log p pðX;~ zju2 Þ, where we "partition" the original model into two models (or, effectively, we train a pair of HMMs simultaneously). This results in the following improved model:
u 1 ; u2 : max
u1 ;u2

X
i2X1

^i ju1 Þ þ log pðXi ; z ij

X
j2X2

~j j u 2 Þ log pðXj ; z

Àg s:t: log

X

ði;jÞ2E

e

uðiÞ

¼1;

X

euðiÞ ¼ 1;

(6)

^i ju1 Þ ^j ju1 Þ pðXi ; z pðXj ; z À log þ ij ! r j j j j ~ ju2 Þ ~ ju2 Þ pðX ; z pðX ; z ij ! 0; (8)

i:uðiÞ2logðfj Þ

where i : uðiÞ 2 logðAj Þ is the set of the indexes which corresponds to the jth row of matrix A. For the model in Eqn. (3), we assumed that every pairwise ranking constraint provided in the data is correct (or valid). However, in real data, there may be outliers in such training pairs. To handle this, we further introduce some slack variables  and h, and accordingly Eqn. (5) can be written as following: X X hðXi ; zi Þ À g ij u : max uT
u 2V T

where X1 is the set of data associated with Model u1 (X2 for ^i is the optimal path for sequence xi with Model Model u2 ), z i z for optimal path with Model u2 . u1 and ~
pðX ;z ^ ju 1 Þ i i i i ^ Þ À uT ~ Þ, we can ¼ uT With log p 1 hðX ; z 2 hðX ; z ~j ju2 Þ ðXj ;z rewrite the model in Eqn. (9) (similar to Eqn. (7)):
i i

"P u s:t: : max uT P
u 2V i2X1

^i Þ hðXi ; z ~j Þ hðX ; z
j

# Àg

X
ði;jÞ2E

ij (9)

"

u

T

(7) u ½hðX ; z Þ À hðX ; z Þ þ ij ! r; 8ði; jÞ 2 E ij ! 0; P where g is the weight for the penalty term ði;jÞ2E ij . For initialization, we can set ij ¼ 0. We will defer the optimization algorithm for Eqn. (7) to Section 4.3. After the model is learned, it can be used to a testing pair: For each sequence s:t:
i j j

Xi 2X i

ði;jÞ2E

# ^i Þ À hðXj ; z ^j Þ hðXi ; z þ ij ! r ~j Þ À hðXi ; ~ zi Þ hðXj ; z

j2X2

ij ! 0;
T T where u ¼ ½uT 1 ; u2  . The optimization algorithm for Eqn. (9) will be presented in Section 4.3. After we learn the model with the improved algorithm, we can apply it to a given pair by first computing their likelihoods with respect to the

1210

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

"sub-models" given by u1 and u2 (with the Viterbi algorithm), and then we use the logarithm of the ratio of the data likelihoods as the score to rank/compare the pair. The learned models u1 and u2 serve as a unified model to rank the data. We may view them as the centers of two clusters, where the distances of the data to those two centers can be related to the ranking score. It needs to be emphasized that the improved model is not equivalent to a supervised HMM with two classes. In a 2-class HMM setting, two models will be independently trained with their respective training sets. Here, the proposed model trains two "sub-models" jointly with only relative ranking constraints. Specifically, if there is no further information for X, we could assume that X1 ¼ fijði; jÞ 2 E; 8jg and X2 ¼ fjjði; jÞ 2 E; 8ig, and thus there could be overlaps between X1 and X2 (which will become clear in the experiment with synthetic data in Section 5). This situation is not even allowed by a supervised HMM setting. We do not require any extra properties for X1 and X2 .

According to Eqn. (11), f will be a valid hidden Markov model (or hidden Markov model pairs ½f1 ; f2  for the improved model). We then apply the Augmented Lagrange multiplier method to the equality constraint log f ¼ u of the problem in Eqn. (11): u; ; f : min f T u þ g 1T þ
u;;f

< ; u À log f > þ s:t: : Au þ  Cf ¼ 1 u r f

m ku À log fk2 2 2

(12)

0;  ! 0; 0

1;

where  is the Lagrange multiplier and m is some nonnegative constant. In Eqn. (12), the nonlinear equality constraint is removed. Eqn. (12) can be solved via block coordinate descent by iterating between the following two sub-problems: Sub-problem 1: fix f to solve u and , which is u;  : min f T u þ g 1T þ
u;;f

4.3 Algorithms for Updating the Model One important step of both the baseline algorithm and the improved algorithm is updating the models, as formulated in Eqn. (7) and Eqn. (9) accordingly. It is a nonlinear programming problem (due to the nonlinear equality constraint). In our previous paper, we solved it by the primaldual interior point method, which is of dimension K ð1 þ K þ DÞ þ jEj (or 2K ð1 þ K þ DÞ þ jEj) with 2jEj þ K ð1 þ K þ DÞ (or 2jEj þ 2K ð1 þ K þ DÞ) linear inequality constraints and 1 þ K þ D (or 2ð1 þ K þ D)) nonlinear equality constraints for the baseline model (or the improved model). Although the Hessian matrix is diagonal, the computational cost could be still very high when there are a large number of training pairs. In this section, we propose a new algorithm by utilizing the special structure of the problems in Eqn. (7) and Eqn. (9). Eqn. (7) (similarly for Eqn. (9)) can be written in the following form:
u;  s:t: : min f T u þ g 1T 
u;

< ; u À log f > þ s:t: : Au þ  u r 0;  ! 0:

m ku À log fk2 2; 2

(13)

It is a quadratic programming problem with linear inequality constraints. Sub-problem 2: fix u and  to solve f, which is m f : min < ; u À log f > þ ku À log fk2 2 f 2 (14) Cf ¼ 1 0 f 1: It is a nonlinear problem with linear constraints. Given the special structures of C, where each column has one and only one element being nonzero (recall Eqn. (6)), Sub-problem 2 can be separated into a set of smaller problems: m fk : min < k ; uk À log fk > þ kuk À log fk k2 2 2 fk (15) 1T fk ¼ 1 0 fk 1;

: Au þ  Ce ¼ 1
u

r

(10)

u

0 ;  ! 0:

P For example, for Eqn. (7), we have f ¼ À Xi 2X hðXi ; zi Þ, A and C are constructed according to Eqns. (7) and (6). Eqn. (10) is a nonlinear programming problem (due to the nonlinear equality constraint). To solve this problem, we first introduce a slack variables f, where log f ¼ u. Then Eqn. (10) can be rewritten into the following problem: u; ; f s:t: : min f T u þ g 1T 
u;;f

where k is the set of indexes of columns, whose values are nonzero at the kth row of C. Those smaller problems are again a nonlinear problem with linear constraint, whose dimensions are only K (number of states) or D (number of feature dimensions). To solve this problem we can use the primal-dual interior point method, whose gradient and hessian are computed as Àk þ mk log fk À mk uk ; fk  k   À m log fk þ muk þ m ; H¼L fk Á fk J¼ where LðÁ Á ÁÞ converts a vector to a diagonal matrix. In addition, we can compute the starting point of the problem in Eqn. (15) as: by taking the gradient of the objective function

: Au þ  Cf ¼ 1

r (11) f 1:

log f ¼ u u 0;  ! 0; 0

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1211

Fig. 1. The experiment result with different numbers of states: (a) the computational time (blue solid curve) and number of iterations needed for convergence (green dashed curve); (b) the accuracy of the improved method. The X-axis is the number of states.

Fig. 2. The accuracy of the improved method: (a) with different g (r is fixed to 10), which controls the weight of the penalty term with slack variables; (b) with different r (g is fixed to 1; 000), which controls the margin of the learned models.

with regard to log fk , we have Àk þ mðlog fk À uk Þ ¼ 0, i.e., fk ¼ eðu . The linear constraint can be solved simply by k P ðuk þk Þ 1 ðu k þ  m Þ , where N ¼ m . e e projection, i.e., fk ¼ N
k þk Þ m

Algorithm 1. Algorithm for the Baseline (Improved) Model
Input: X, E, r, g , s (, X1 and X2 ) Output: f Initialization: Initialize f (or f1 and f2 ) via ordinary HMM u 1:25 learning algorithm,  ¼ log juj and m ¼ juj ;
2 2

below (noting the similarity in form of the algorithms and thus putting them compactly together): According to [31], the proposed method will converge to the local minimum of the problem in Eqn. (10). And for kuÀlog fk convergence, we check kuk 2 . If it is smaller than some 2 À6 value, e.g., 10 , the algorithm will be terminated. In initialization, juj2 is the vector L2 norm of of u. Remarks on the Parameters. The parameter g controls the weight of the penalty term with the slack variables, which is similar to the functionality of C in support vector machines [32]. The parameter r controls the desired gap of the score of two data points, i.e., line model and
pðXi ;zi juÞ ! er 8ði; jÞ 2 E pðXj ;zj juÞ pðXi ;z ^i ju1 Þ pðXi ;z ~j ju 2 Þ ! er 8ði; jÞ ~i ju2 Þ pðXi ;z ^j juÞ pðXi ;z

while not converged do ^ and z ~) for each sequence Compute the optimal path z (or z with f (or f1 and f2 ); solve Sub-problem 1; solve Sub-problem 2; update  ¼  þ mðu À log fÞ and m ¼ m Â s ; check convergence; end while

in the base2 E in the

Finally, we briefly summarize the algorithms for the baseline model (Eqn. (7)) and the improved model (Eqn. (9))

improved model. In Section 5.1, we will evaluate different parameter settings (Fig. 2), which leads us to set g ¼ 1; 000 and r ¼ 10 in our final experiments. The parameter s controls the convergence speed of the algorithm, which should be a positive number larger than 1. s is typically within 1:1 À 1:5, and 1:25 is used in this paper. The proposed algorithm, compared with the one used in [13], has lower computational cost, due to the removal of

1212

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

TABLE 1 Comparing the Method in [13] and the Proposed Method for Updating the Baseline Model, with Regarding to the Problem Size, Number of Linear Constraints and Nonlinear Constraints Method in [13] Problem Size # Linear Const. # Nonlinear Const. K ð1 þ K þ DÞ þ jEj 2jEj þ K ð1 þ K þ DÞ 1þKþD Proposed Method Sub-problem 1 K ð1 þ K þ DÞ þ jEj 2jEj þ K ð1 þ K þ DÞ 0 Sub-problem 2 K(or D) 1+2K(or 1+2D) 0

For Sub-problem 2 of the proposed method, it can be divided into several smaller problems.

the nonlinear equality constraint with augmented Lagrange multiplier. For Sub-problem 1, the quadratic term is a diagonal matrix and many solvers (e.g., CPLEX) can solve it quite efficiently. Sub-problem 2 is a nonlinear minimization problem with linear equality constraints; however, it can be decomposed into several smaller problems. A comparison between the method in [13] and the proposed method for updating the baseline model is shown in Table 1. In Section 5.1, we will also compare the computational time of those two methods under varying E on synthetica data (Fig. 6).

4.4 Relationship to Existing Methods The proposed method is related to latent support vector machine [33]. Given a training set of input-output pairs fðxi ; yi Þgn i¼1 , where yi 2 fÀ1; 1g, Latent SVM tries to learn a predictor of the form:
fw ðxÞ ¼ max wT Cðx; zÞ;
z

(16)

valid hidden Markov model while defining a fixedmargin, i.e., r. Thus the proposed method can always guarantee the learned model is a valid hidden Markov model. 2) In Eqn. (18), the two state sequences z (i.e., the latent variables) are optimized jointly, where no known efficient solution is available. In the proposed method, the two state sequences are optimized separately with regarding to the likelihood, which can be solved efficiently via dynamic programming (i.e., the Viterbi algorithm); 3) Given the model learned by the latent SVM, we can only rank a pair of sequences. However, the model learned by the proposed method is capable of not only ranking a pair of sequences but also assigning a score for each sequence. Those differences make the proposed method (both the baseline model and the improved model) more suitable for modeling the sequential data, e.g., video, speech.

where w is the parameter of the predictor, Cðx; zÞ is the feature mapping function and z is the latent variable. The training stage of Latent SVM can be formulated as the following problem: X 1 maxð0; 1 À yi fw ðxi ÞÞ: (17) min kwk2 2þC w 2 i Latent SVM is a non-convex problem, as the latent variable is unknown, and the coordinate descent approach is used for solving this problem. L R Given a training set fðxi ; yi Þgn i¼1 , where xi ¼ ðxi ; xi Þ is a pair of sequences and yi 2 fÀ1; 1g is the ranking of the pair, by defining the feature mapping function as Cðxi ; zi Þ ¼ L R R L R ½hðxL i ; zi Þ À hðxi ; zi Þ, with the latent variable zi ¼ ðzi ; zi Þ R being a pair of state sequences for the pair xi ¼ ðxL i ; xi Þ, we have X 1 min kwk2 i 2þC w 2 i È Â À Á À R R ÁÃÉ L þ i ! 1 (18) s:t: yi max wT h xL i ; zi À h xi ; zi
zL ;zR i i

5

EXPERIMENTS

In this section, we evaluate the proposed methods, including the baseline method and the improved method, using both synthetic data (Section 5.1) and realistic data collected from the surgical training platform FLS box (Section 5.2). The performance of the proposed methods is compared with a supervised 2-class HMM. (Lacking a comparative approach in the literature that is both unsupervised and works with only relative rankings, this is believed to be a reasonable way of generating a reference point to assess the proposed methods.) Since we do not have the label information for the training data, we train the HMM as follows. For the HMM algorithm, we initialize the two sets as X1 ¼ fijði; jÞ 2 E; 8jg and X2 ¼ fjjði; jÞ 2 E; 8ig. Each of the sets is then used to train a HMM. Note, the data generated from data-generating Models u2 $ u5 could be included in both X1 and X2 . Thus existing discriminative learning methods for HMM could not be applied here.

i ! 0 : We can find that Eqn. (18) is similar to our baseline model (Eqn. (7)), except for the following differences. 1) In Eqn. (18), the L2 norm is applied to the parameter of the predictor w (which is related to the margin). In the proposed methods we require w to be a

5.1 Evaluation with Synthetic Data To evaluate the proposed method, we generate synthetic data as follows. We first generate six different HMMs (u1 to u6 , referred as data-generating models), from each of which we draw 200 sequences, with the length being uniformly distributed between 80 to 120. Each data-generating model has five states. For the sequences from each data-generating model, we randomly assign 50 of them to the training set

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1213

Fig. 3. The results of four methods on training set (dashed curve) and testing set (solid curve) with different numbers of training pairs.

and the remaining to the testing set. We assume there exists a score function such that F ðXi Þ > F ðXj Þ if and only if Xi $ uk , Xj $ ul and k < l. That is, the sequences from a data-generating model with a lower index are viewed to have a higher score (or ranking) than those from a datagenerating model with a higher index. A set of pairs fði; jÞjXi $ uk ; Xj $ ukþ1 ; k ¼ 1; . . . ; 5g are then formed accordingly, some of which are then randomly selected as the training pairs E. We use the proposed methods and also HMM to learn models from the training pairs. The learned models are then used to evaluate the testing set, i.e., how many testing pairs that they rank the same as the ground truth. The result of the methods with different numbers of training pairs is summarized in Fig. 3, where due to the computational time it takes, we do not have the results for the baseline method when there are more than 3;750 training pairs. From Fig. 3, we can find that the improved method achieves the best results on both the training set and the testing set; and the HMM method gives the worse result. In addition, the performance of both of the proposed methods stabilized after certain number of training pairs. However the performance of the HMM method drops dramatically when the number of training pairs reaches about 6;250. It can be explained by that the two HMMs share a lot of common data (for those generated by u2 $ u5 ) and the models are trained independently without considering their discrimination ability. Normalizing the logarithm of the data likelihood does not improve the performance of baseline method, which could be explained by that, all the sequences have roughly the same length, i.e., 80 $ 120. Fig. 4 shows the logarithm of the data likelihood ratio with the models learned by the improved method, when about 1; 250 training pairs are provided. This clearly demonstrates that, although we formed the training pairs only with data from data-generating models of adjacent indexes (i.e., i and i þ 1), the learned model is able to recover the strict ranking of the original data. We can also try to classify the data into six models, by thresholding the logarithm of data likelihood ratio, where, for the model learned with the improved method, the classification accuracy is 86:44 and 98:60 percent for testing and training respectively.

Fig. 4. The logarithm of the data likelihood ratio with the models learned by the improved method. Top: the result for the testing set. Bottom: the result for the training set. The data are grouped (as the section partitioned by the red lines) according to the data generation model from which they are synthesized.

Convergence and Speed. For empirically understanding the convergence behavior of the improved method, we plot in Fig. 5 the objective value in the model as a function of the number of iterations. We can find that the improved method converges fairly quickly (within about 14 iterations) and the value of the objective function monotonically increases. We also compare the computational time of the optimization method in [13] (shown as the red/upper curve) and the proposed optimization method (in Section 4.3 and shown as the green/lower curve) in solving the improved model under varying number of training pairs in Fig. 6. In [13], a primal-dual interior point method is utilized to update the model; while in this paper, we design an augmented Lagrange multiplier method which utilizes the special structure of the objective function of the problem. From the plot, we can find that the proposed optimization method has a much lower computational cost than the one proposed in [13]. Parameter Selection. To understand the effect of parameters to the performances of the improved method, including accuracy and computation cost, we evaluate it with

Fig. 5. The convergence behavior of the improved method, where around 1; 250 training pairs were used. The blue curve/axis shows the value of the objective function, and the green curve/axis shows the number of constraints satisfied.

1214

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

Fig. 6. The computation time for solving the improved model with the method proposed in [13] (red/upper curve) and the method proposed in Section 4.3 (green/lower) under varying number of training pairs. For illustration purpose, we use log-log plot, where X-axis is the number of training pairs (from around 125 to around 9; 000) and Y-axis is the computation time in unit second (from about 20 to around 6; 000). The time is measured in Matlab on a dual-core PC platform.

varying combination of parameters. First we learn the model with varying numbers of states (K ), from 6 to 30. The result is shown Fig. 1. From Fig. 1b, we can find that, though the accuracy for the training data increases with the number of states, the accuracy for testing doesn't following this trend, which indicates a potential risk of overfitting. The computational time and number of iteration until convergence get minimum when the number of states is 11-13. We also do experiment with different combinations of g (controlling the weight of the penalty term with slack variables) and r (controlling the margin of the model), where the experiment result is shown in Fig. 2. From this experiment we can find that, g 2 ½1; 1;000 and r 2 ½4; 32 are good choices. It is obvious from this experiment that the sequences are different from (or similar to) each other only because they are from different (or the same) data-generating models, whereas their relative ranking can be arbitrarily defined. In the end, the proposed methods will learn a temporal model to reflect the defined rankings. This suggests that, as long as we can assume there are some data-generating models for the given sequential data, we can use the proposed methods to learn a relative HMM. This is the basis for applying the approach to the surgical training data in the following section, where it is reasonable to assume that movement patterns of subjects with different skill levels may be modeled by different underlying HMMs while the ranking can be based on the time of training, which reflects the skill level of the subject at the time.

5.2 Skill Evaluation Using Surgical Training Video We now evaluate the proposed method using real videos captured from the FLS trainer box, which has been widely used in surgical training. The data set contains 546 videos captured from 18 subjects performing the "peg transfer" operation, which is one of the standard training tasks a resident surgeon needs to perform and pass. The number of frames in each video varies from 1; 000 to 6; 000 (depending on the trainees' speed in completing a training session). The

data set covers a training period of four weeks, with every trainee performing three sessions each week. In the training, the subject needs to lift six objects (one by one) with a grasper by the non-dominant hand, transfer the object midair to the dominant hand, and then place the object on a peg on the other side of the board. Once all six objects are transferred, the process is reversed, and the objects are to be transferred back to the original side of the board. The videos capture the entire process inside the trainer box, showing how the tools and objects are moved by the subject. The motion skill is related to how well the subjects perform in such operation. In the existing practice, senior surgeons rate the performance of the trainees based on such videos. Our goal is to perform the rating automatically with the proposed model. Based on the reasonable assumption that the trainees improve their skills over time (which is the whole point of having the resident surgeons going through the training before taking the exam), the time of recording is used to rank the recorded videos within each subjects' corpus (i.e., a later video is associated with a better skill). Other than this relative ranking, there are no other labels assumed for the video, e.g., there is no rank information between videos of different subjects (which would be hard to obtain anyway, since there is no clearly-defined skill levels for a group of trainees with diverse background). Based on this, we randomly pick 300 pairs for training, similar to the experiment using synthetic data. Feature Extraction. We use the "bag of words" approach for feature extraction from the videos as follows. The spatiotemporal interest point detector [34] is applied to obtain the histogram-of-gradient (HoG) features, which was found to be useful in target application in the literature [35]. K-means (k ¼ 100) is then used to build a code book for the descriptors of the interest points. Finally, the code book is used to obtain a histogram of interest points for each frame, and thus each video is represented as a sequence of histograms. This representation, compared with the existing way of using bag of words in action recognition, i.e., transforming each video into a single histogram, can better capture the temporal information of the data. For all three methods, we set the number of states to ten. After learning the models from the training data, we compute the score of the test data as the logarithm of data likelihood (for the baseline method) or the logarithm of the data likelihood ratio (for the improved method and the HMM). We compare these scores for each pair of the testing data (within each subject) and compute the percentage of correctly labeled pairs (recall that, we use their time of recording as ground truth). To demonstrate the advantage of the proposed method, we also compare with the "relative attribute" method [9] (referred as "SVM" in the following discussions), which relies on ranking SVM. For "relative attribute", we represent each video as a histogram by accumulating the sequence of histograms of the video along the temporal direction. The result is summarized in Table 2, where the improved method obtained a significantly better result than the other approaches, including "relative attribute". Surprisingly, the baseline method even performed slightly worse than the HMM method. This is largely due to the wide range of

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1215

TABLE 2 The Result for Experiment on Evaluating Surgical Skills Method # Pairs Accuracy SVM 6335 78:91% HMM 6363 79:39% Baseline 6215 77:54% Improved 6993 87:25%

There are 8;015 pairs in total (only 300 for training), excluding the comparisons among data of different subjects.

variations of the length of the input sequences. Fig. 7 shows the computed scores with the learned models, where for better illustration purpose we group them by their subject ID and within each subject's corpus we sort the videos by their recording time. From the figure, we can find that the improved method (bottom) reveals a more clear trend for the data than both the HMM method (top) and the baseline method (middle), i.e., the scores of the data increase over times (X-axis) for each subject (segments within the red lines). It is worth emphasizing that only one joint model is learned from ranked pairs of subjects with potentially varying skill levels. Still the learned model is able to recover the improving trend, independent of the underlying skill levels. As shown in Fig. 7, the model learned with the proposed method can be used for comparing not only the videos of the same subjects but also the videos from different subjects, where the logarithm of data likelihood ratio can be used as a measurement of the skills. However, it is not possible to quantitatively measure the accuracy in comparing videos from different subjects, due to the lack of ground truth information for videos from different subjects.

It is also interesting to look at what the jointly-learned models look like. Fig. 8 depicts the two models learned by the improved method in this real-data based experiment. From the figure, we can see that the two models have different transition patterns. For example, the transition from State 8 to States 2 and 5 are only observed in Model 1. This may be linked to different motion patterns for data of different surgical skills, with the hidden states corresponding to some underlying action elements (and thus the transition patterns vary with the skill).

6

ADDITIONAL VALIDATION USING SPEECH DATA

Although the proposed approach was evaluated above in the context of motion skill analysis in surgical training, using visual data as the input, the approach itself is general and applicable for other applications involving temporal data. To show that the proposed method can be used to solve temporal inference problems other than video-based motion skill assessment, we now consider an exemplar problem, speech-based emotion recognition, where the attribute of interest (the underlying emotion of a speaker) needs to be inferred from sequential data. Emotion recognition has received attention from researchers due to its broad applications. For example, in human-machine interaction, better responses can be made if the emotional state of the human can be recognized. Existing work on this in the literature mainly focuses on developing models for assigning the labels like "pleasing", "angry" and "neural" to the data, e.g., [36], [37], [38], [39]. Most of the those efforts are

Fig. 7. Top: the logarithm of the data likelihood ratio from two models learned by HMM. Middle: the logarithm of data likelihood with the model learned by the baseline method. Bottom: the logarithm of the data likelihood ratio with the models learned by the improved method. The red vertical lines separate the data of different subjects, where X-axis is the corresponding subject ID. Within each subjects' corpus, the videos are sorted according to their time of recording.

1216

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

Fig. 8. The two component models (Model 1 for X1 and Model 2 for X2 ) learned by the improved method, where we only draw the edges with a transition probability larger than 0.01 and ignore self transitions. The number attached to each edge indicates the transition probability.

supervised in natural, i.e., the ground truth labeling for the training data is required. For example, [40] used support vector machines, [36] used hidden Markov models, both utilizing fully-labelled data. The ground truth data typically require manual labeling by human, which is an error-prone process especially if absolute labels must be assigned to ambiguous data. With the proposed model, we can support learning with only relative labels like "Audio a is more pleasing than Audio b", which is easier to obtain and also less error-prone. In this experiment, we use Utsunomiya University Spoken Dialogue Database For Paralinguistic Information Studies (UUDB)[41](http://uudb.speech-lab.org), which contains 4840 assets labeled across six dimensions (pleasantness, arousal, dominance, credibility, interest and positivity) on a scale of 1 to 7. The ground truth is based on the average of scores of three annotators. For our experiment, we pick the assets which are longer than 1 second to ensure the effectiveness of emotional recognition, which results in 991 assets, where half of the data are used for training and the remaining for testing. For generating the ground truth pairs, we randomly picks 1000 pairs from the training assets. Note that, we say two assets are similar, if the difference of the labeled scores of two assets is within the range of ðÀ1; 1Þ. For feature extraction, we use Hidden Markov Model Toolkit (HTK)[42], where the MFCC coefficients are extracted with the following configurations: sampling rate is 100 HZ, windows size is 25 millisecond, number of filter bank channels is 26, cepstral liftering coefficient is 22 with 12 cepstral parameters and the feature vector is normalized. K-means is applied to the MFCC coefficients of all the training data to generate a code book of 64 elements. Finally, each data is converted to a sequences of histograms. We use the same set of parameters as the previous experiment. The experimental results are reported in Table 3, where we also provide a comparison to the relative attribute [9] as

referred by "SVM". From the table, we can find that the improved method consistently outperforms than both plain HMM and also the baseline method in all six dimensions. We also find that the baseline method gets low accuracy on this experiment, which can be explained by that the length of the audio (in number of temporal frames) varies dramatically and the baseline method obviously cannot handle this variation very well.

7

DISCUSSIONS AND CONCLUSIONS

In this paper, we presented a new formulation for the problem of learning temporal models using only relative information. Algorithms were developed under the formulation, and experiments using both synthetic and real data were performed to verify the performance of the proposed method. In essence, the proposed method attempts to learn an HMM with relative constraints. Such a setting is useful for many practical applications where relative attributes are easier to obtain while explicit labeling is difficult to get. The application of video-based surgical training was the focus of this study, and the evaluation results using realistic data suggests that the proposed method provides a promising solution to the problem of motion skill evaluation from
TABLE 3 The Result for Experiment on UUDB Datasets Dimension Pleasantness Arousal Dominance Credibility Interest Positivity Average SVM 75:25% 82:11% 74:13% 69:15% 76:91% 68:08% 74:27% Improved 77:30% 86:95% 87:95% 76:68% 81:90% 74:99% 81:28% Baseline 57:96% 55:74% 63:04% 55:11% 62:56% 67:84% 53:14% HMM 75:05% 69:55% 77:32% 71:74% 78:07% 70:36% 73:72%

We evaulate the accuracy of ranking pairs with the learned models compared with the ground truth ones.

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1217

videos. For future work, we plan to extend the proposed method to cover different observation models so that more types of applications may be handled. That also includes investigating alternative feature spaces which may be more effective for the target problem.

[7]

[8] [9] [10] [11] [12] [13] [14]

APPENDIX A
For multinomial observation model, i.e., pðXt jfzt Þ ¼ QD Xt ðlÞ , where D is the dimension of each frame, d¼1 fzt ðlÞ Xt ðlÞ is the lth dimension of Xt and fzt are the parameters of observation model with State zt , we can further define the following variables for each sequence Xi : n 2R
i K Â1

Oi 2 RK ÂD Mi 2 RK ÂK

À Á : n ðkÞ ¼ d zi 1 ¼k ; X : Oi ðk; dÞ ¼ Xi t ðdÞ;
i t:zt ¼k

: Mi ðk; lÞ ¼

T X À Á À i Á d zi tÀ1 ¼ k d zt ¼ l ; t¼2

[15] [16] [17] [18] [19]

where dðÁÞ is Dirac Delta function. Then the log likelihood with the optimal path can be written as: logpðXi ; zi juÞ X X ¼ ni ðlÞlog pðlÞ þ Mi ðk; lÞlog Aðk; lÞ
l

þ

X
k;d

k;l

Oi ðk; dÞlog fk ðdÞ (19)

¼ uT hðXi ; zi Þ;

[20] [21] [22]

where u ¼ ½log p; vecðlog AÞ; vecðlog fÞ, hðXi ; zi Þ ¼ ½ni ; vecðMi Þ; vecðOi Þ and vec converts matrix to vector.

ACKNOWLEDGMENTS
The work was supported in part by a grant (#0904778) from the National Science Foundation (NSF). Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.

[23] [24]

REFERENCES
[1] F. Duan, Y. Zhang, N. Pongthanya, K. Watanabe, H. Yokoi, and T. Arai, "Analyzing human skill through control trajectories and motion capture data," in Proc. IEEE Int. Conf. Autom. Sci. Eng., Aug. 2008, pp. 454­459. K. Watanabe and M. Hokari, "Kinematical analysis and measurement of sports form," IEEE Trans. Syst., Man, Cybern. A, Syst., Humans, vol. 36, no. 3, pp. 549­557, May 2006. S. Suzuki, N. Tomomatsu, F. Harashima, and K. Furuta, "Skill evaluation based on state-transition model for human adaptive mechatronics (HAM)," in Proc. 30th Annu. Conf. IEEE Ind. Electron. Soc., 2004, vol. 1, pp. 641­646. S. Satoshi and H. Fumio, "Skill evaluation from observation of discrete hand movements during console operation," J. Robot., vol. 2010, 2010. J. Rosen, M. Solazzo, B. Hannaford, and M. Sinanan, "Task decomposition of laparoscopic surgery for objective evaluation of surgical residents' learning curve using hidden Markov model," Comput. Aided Surgery, vol. 7, pp. 49­61, 2002. K. Kahol, N. C. Krishnan, V. N. Balasubramanian, S. Panchanathan, M. Smith, and J. Ferrara, "Measuring movement expertise in surgical tasks," in Proc. 14th Annu. ACM Int. Conf. Multimedia, 2006, pp. 719­722. [25]

[2] [3]

[26]

[27]

[4] [5]

[28]

[29]

[6]

Q. Zhang and B. Li, "Video-based motion expertise analysis in simulation-based surgical training using hierarchical Dirichlet process hidden markov model," in Proc. Int. ACM Workshop Med. Multimedia Anal. Retrieval, 2011, pp. 19­24. E. Fox, "Bayesian nonparametric learning of complex dynamical phenomena," Ph.D. thesis, MIT, Cambridge, MA, USA, 2009. D. Parikh, and K. Grauman, "Relative attributes," in Proc. Int. Conf. Comput. Vis., Nov. 2011, pp. 503­510. M. Schultz, and T. Joachims, "Learning a distance metric from relative comparisons," in Proc. Advances Neural Inf. Process. Syst., 2004, p. 41. N. Kumar, A. Berg, P. Belhumeur, and S. Nayar, "Attribute and simile classifiers for face verification," in Proc. IEEE Int. Conf. Comput. Vis., Sep. 29, 2009­Oct. 2, 2009, pp. 365­372. D. Parikh, A. Kovashka, A. Parkash, and K. Grauman, "Relative attributes for enhanced human-machine communication," in Proc. AAAI Conf. Artif. Intell., 2012. Q. Zhang and B. Li, "Relative hidden Markov models for evaluating motion skill," in Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2013, pp. 548­555. M. Collins, "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms," in Proc. ACL-02 Conf. Empirical Methods Natural Language Process., 2002, pp. 1­8. Y. Altun, I. Tsochantaridis, and T. Hofmann, "Hidden Markov support vector machines," in Proc. 20th Int. Conf. Mach. Learning, 2003, vol. 20, no. 1, p. 3. A. Sloin and D. Burshtein, "Support vector machine training for improved hidden Markov modeling," IEEE Trans. Signal Process., vol. 56, no. 1, pp. 172­188, Jan. 2008. G. Wang, D. Forsyth, and D. Hoiem, "Comparative object similarity for improved recognition with few or no examples," in Proc. 23rd IEEE Conf. Comput. Vis. Pattern Recog., 2010, pp. 3525­3532. A. Kovashka, D. Parikh, and K. Grauman, "Whittlesearch: Image search with relative attribute feedback," in Proc. IEEE Conf. Comput. Vis. Pattern Recog., Jun. 2012, pp. 2973­2980. I. Kadar and O. Ben-Shahar, "Small sample scene categorization from perceptual relations," in Proc. IEEE Conf. Comput. Vis. Pattern Recog., Jun. 2012, pp. 2711­2718. S. Guo, S. Sanner, T. Graepel, and W. Buntine, "Score-based Bayesian skill learning," in Proc. Eur. Conf. Mach. Learn. Knowl. Discovery Databases, 2012, pp. 106­121. P. Dangauthier, R. Herbrich, T. Minka, and T. Graepel, "Trueskill through time: Revisiting the history of chess," in Proc. Adv. Neural Inf. Process. Syst., 2007, pp. 337­344. N. R. Howells, M. D. Brinsden, R. S. Gill, A. J. Carr, and J. L. Rees, "Motion analysis: A validated method for showing skill levels in arthroscopy," Arthroscopy: J. Arthroscopic Related Surgery, vol. 24, no. 3, pp. 335­342, 2008. H. Lin, I. Shafran, D. Yuh, and G. Hager, "Towards automatic skill evaluation: Detection and segmentation of robot-assisted surgical motions," Comput. Aided Surgery, vol. 11, no. 5, pp. 220­230, 2006. A. G. Gallagher, E. M. Ritter, H. Champion, G. Higgins, M. P. Fried, G. Moses, C. D. Smith, and R. M. Satava, "Virtual reality simulation for the operating room: proficiency-based training as a paradigm shift in surgical skills training," Ann. Surgery, vol. 241, no. 2, p. 364, 2005. J. Rosen, J. Brown, L. Chang, M. Sinanan, and B. Hannaford, "Generalized approach for modeling minimally invasive surgery as a stochastic process using a discrete Markov model," IEEE Trans. Biomed. Eng., vol. 53, no. 3, pp. 399­413, Mar. 2006. K. Kahol, N. C. Krishnan, V. N. Balasubramanian, S. Panchanathan, M. Smith, and J. Ferrara, "Measuring movement expertise in surgical tasks," in Proc. 14th Annu. ACM Int. Conf. Multimedia, 2006, pp. 719­722. S.-k. Jun, P. Singhal, M. Sathianarayanan, S. Garimella, A. Eddib, and V. Krovi, "Evaluation of robotic minimally invasive surgical skills using motion studies," in Proc. Workshop Performance Metrics Intell. Syst., 2012, pp. 198­205. L. E. Baum, T. Petrie, G. Soules, and N. Weiss. (1970). A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. Ann. Math. Statist. [Online]. 41(1), pp. 164­171. Available: http://www.jstor.org/stable/2239727 B. Juang and L. Rabiner, "The segmental k-means algorithm for estimating parameters of hidden Markov models," IEEE Trans. Acoustics, Speech Signal Process., vol. 38, no. 9, pp. 1639­1641, Sep. 1990.

1218

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

[30] N. Merhav and Y. Ephraim, "Maximum likelihood hidden Markov modeling using a dominant sequence of states," IEEE Trans. Signal Process., vol. 39, no. 9, pp. 2111­2115, Sep. 1991. [31] D. P. Bertsekas, "Constrained optimization and Lagrange multiplier methods," in Computer Science and Applied Mathematics, Boston, MA, USA: Academic, vol. 1, 1982. [32] C.-C. Chang and C.-J. Lin. (2011). LIBSVM: A library for support vector machines. ACM Trans. Intell. Syst. Technol. [Online]. 2, pp. 27:1­27:27, Software available at http://www.csie.ntu.edu. tw/ cjlin/libsvm. [33] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan, "Object detection with discriminatively trained part-based models," IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, pp. 1627­ 1645, Sep. 2010. [34] I. Laptev, "On space-time interest points," Int. J. Comput. Vis., vol. 64, no. 2, pp. 107­123, 2005. [35] Q. Zhang, L. Chen, Q. Tian, and B. Li, "Video-based analysis of motion skills in simulation-based surgical training," in Proc. Int. IS&T/SPIE Electron. Imaging, 2013, p. 86670A. [36] T. L. Nwe, S. W. Foo, and L. C. De Silva, "Speech emotion recognition using hidden Markov models," Speech Commun., vol. 41, no. 4, pp. 603­623, 2003. [37] B. Schuller, G. Rigoll, and M. Lang, "Hidden Markov model-based speech emotion recognition," in Proc. IEEE Int. Conf. Acoustics, Speech, Signal Process., 2003, vol. 2, pp. II­1. [38] M. El Ayadi, M. S. Kamel, and F. Karray, "Survey on speech emotion recognition: Features, classification schemes, and databases," Pattern Recognit., vol. 44, no. 3, pp. 572­587, 2011. [39] A. Tarasov and S. J. Delany, "Benchmarking classification models for emotion recognition in natural speech: A multi-corporal study," in Proc. IEEE Int. Conf. Autom. Face Gesture Recognit. Workshops, 2011, pp. 841­846. [40] K. H. Kim, S. Bang, and S. Kim, "Emotion recognition system using short-term monitoring of physiological signals," Med. Biol. Eng. Comput., vol. 42, no. 3, pp. 419­427, 2004. [41] H. Mori, T. Satake, M. Nakamura, and H. Kasuya. (2008). UU database: A spoken dialogue corpus for studies on paralinguistic information in expressive conversation. Proc. Int. Conf. Text, Speech Dialogue, pp. 427­434 [Online]. Available: http://uudb.speechlab.org/ [42] P. C. Woodland, J. J. Odell, V. Valtchev, and S. J. Young. (1994). Large vocabulary continuous speech recognition using htk. Proc. IEEE Int. Conf. Acoustics, Speech, Signal Process., vol. 2, pp. II/125­ II/128 [Online]. Available: http://htk.eng.cam.ac.uk/

Qiang Zhang received the BS degree in electronic information and technology from Beijing Normal University, Beijing, China, in 2009 and the PhD degree in computer science from Arizona State University, Tempe, Arizona in 2014. Since 2014, he has been with Samsung, Pasadena, CA, as a staff research scientist in computer vision. His research interests include image/video processing, computer vision and machine vision, specialized in sparse learning, face recognition, and motion analysis. He is a student member of the IEEE.

Baoxin Li (S'97-M'00-SM'04) received the PhD degree in electrical engineering from the University of Maryland, College Park, in 2000. He is currently an associate professor of computer science and engineering with Arizona State University, Tempe. From 2000 to 2004, he was a senior researcher with SHARP Laboratories of America, Camas, WA, where he was the technical Lead in developing SHARP's HiIMPACT Sports technologies. From 2003 to 2004, he was also an adjunct professor with the Portland State University, Portland, OR. He holds nine issued US patents. His current research interests include computer vision and pattern recognition, image/video processing, multimedia, medical image processing, and statistical methods in visual computing. He won the SHARP Laboratories' President Award twice, in 2001 and 2004. He also received the SHARP Laboratories' Inventor of the Year Award in 2002. He received the National Science Foundation's CAREER Award from 2008 to 2009. He is a senior member of the IEEE.

" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

663

Multivehicle Cooperative Driving Using Cooperative Perception: Design and Experimental Validation
Seong-Woo Kim, Member, IEEE, Baoxing Qin, Zhuang Jie Chong, Xiaotong Shen, Wei Liu, Marcelo H. Ang, Jr., Emilio Frazzoli, Senior Member, IEEE, and Daniela Rus, Fellow, IEEE

Abstract--In this paper, we present a multivehicle cooperative driving system architecture using cooperative perception along with experimental validation. For this goal, we first propose a multimodal cooperative perception system that provides see-through, lifted-seat, satellite and all-around views to drivers. Using the extended range information from the system, we then realize cooperative driving by a see-through forward collision warning, overtaking/lane-changing assistance, and automated hidden obstacle avoidance. We demonstrate the capabilities and features of our system through real-world experiments using four vehicles on the road. Index Terms--Cooperative driving, cooperative perception, driving assistance, see-through system, vehicle communication.

I. I NTRODUCTION OOPERATIVE perception, via the exchange of sensor information between vehicles via wireless communications, is an emerging technology that is receiving attention from industry and academia [1]­[3]. The main advantage of cooperative perception can be summarized as an increase in situational awareness, without substantial additional costs. For example, cooperative perception can enable a driver to have a longer perception range--even beyond line-of-sight and fieldof-view--because the sensing region can be extended to the union of the sensing regions of all connected vehicles. Since the prices of sensors and radio devices for cooperative perception are affordable compared with conventional long-range sensors, this results in a clear benefit for users. Fig. 1 shows examples of this perception range extension. Fig. 1(a) and (b) are snapshots of maps of the ego vehicle running the proposed cooperative perception system, where the ego vehicle is represented as the box located at the bottom. The ego vehicle can see the preceding vehicles and an oncoming vehicle beyond line-of-sight in Fig. 1(a) and beyond field-ofManuscript received September 15, 2013; revised January 5, 2014, April 19, 2014, June 18, 2014, and June 24, 2014; accepted June 29, 2014. Date of publication July 28, 2014; date of current version March 27, 2015. This work was supported by the Future Urban Mobility project of the Singapore-MIT Alliance for Research and Technology (SMART) Center, with funding from Singapore's National Research Foundation. The Associate Editor for this paper was C. Olaverri-Monreal. S.-W. Kim is with Singapore-MIT Alliance for Research and Technology, Singapore 138602 (e-mail: sungwoo@smart.mit.edu). B. Qin, Z. J. Chong, X. Shen, W. Liu, and M. H. Ang Jr. are with the Department of Mechanical Engineering, National University of Singapore, Singapore 117575 (e-mail: baoxing.qin@nus.edu.sg; chongzj@nus.edu.sg; shen_xiaotong@nus.edu.sg; liu_wei@nus.edu.sg; mpeangh@nus.edu.sg). E. Frazzoli and D. Rus are with Massachusetts Institute of Technology, Cambridge, MA 02139 USA (e-mail: frazzoli@mit.edu; rus@csail.mit.edu). Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/TITS.2014.2337316

C

Fig. 1. Two examples of sensing range extension using cooperative perception: detecting an oncoming car (a) beyond line-of-sight, and (b) beyond fieldof-view at sharp curb. The blue and red boxes correspond to the ego and preceding vehicle, respectively. The red, green, and blue dots are laser scan points of the ego, first and second preceding vehicle, respectively. The white dotted boxes indicate coming vehicles not detected by the ego vehicle.

view at the sharp corner in Fig. 1(b), where the oncoming vehicle is represented by the white dotted box. This aspect enables better driving decisions, such as overtaking, avoiding hidden or sudden obstacles, or early lane change against lane drop, and eventually could improve traffic safety and efficiency [4], [5]. Despite these advantages, however, there have been comparatively less research works on cooperative perception for Intelligent Transportation Systems (ITS)-related applications. This paper considers cooperative perception to enable cooperative driving. In this paper, we explore a multivehicle cooperative driving system using cooperative perception along with experimental validation. The goals of this paper are to provide a comprehensive argument for such systems and to demonstrate the engineering feasibility. Building up to this, we first propose a cooperative perception system that can provide a far-sight seethrough, lifted-seat, satellite or all-around view to a driver. The key purpose of this system is to provide as much visibility as if driver's seat would be lifted as much as express buses or trucks. Based on the augmented on-road sensing capability, we then present a cooperative driving system that can improve traffic safety and efficiency, specifically, by a see-through forward collision warning, overtaking/lane-changing assistance, and automated hidden obstacle avoidance. All the presented concepts are evaluated in experiments using four vehicles. The contribution of this paper can be summarized as the following. · We present our design of a multimodal cooperative perception system that can provide a far-sight see-through, lifted-seat, satellite or all-around view to a driver.

1524-9050 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

664

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

· We present a cooperative driving system using cooperative perception for traffic safety and efficiency along with experimental validation using four vehicles. · We provide a comprehensive argument for the cooperative perception-based cooperative driving system from a viewpoint of engineering feasibility. The remainder of this paper is organized as follows. Section II presents the related works. Section III introduces the system architecture of our proposal consisting of a multimodal cooperative system and perspective visualization for driver assistance, whose details are presented in Sections IV and V. Section VI presents how our system can support a see-though forward collision warning, overtaking assistance, and lane-changing assistance. Section VII provides scalability analysis. Section VIII provides experimental results performed with vehicles on urban roads. Section IX concludes this paper. II. R ELATED W ORKS A. Cooperative Perception on the Road To enable cooperative perception, the distributed information from multiple vehicles should be properly fused. One of the key principles is to know the relative pose between spatial information from various sensors of different vehicles, which have been dealt with as a map merging problem [6] or relative localization [7]. The solving approaches include triangulation [8] and dead reckoning [9]. However, since rich sensing information has been recently available, sensing-information-based methods have been preferred due to their high accuracy. The principle is to find the best pose that maximizes the similarity of overlapping area between different maps using landmarks [10], topological maps [11], occupancy grid maps [12], or scan matching [13]. With the recent advancement of range sensor technology, scan matching has been one of the most essential technologies to enable map building, map merging, pose estimation [14], and visual odometry [15]. The principle of scan matching is to find a transformation between scan data from range sensors. The proposed solutions include iterative closest point or iterative correspondence point (ICP)-variant [16]­[18], adaptive randomwalk [12], Hough transform [19], correlative scan matching (CSM) [20], and histogram approaches [21], [22]. Although these map merging and scan matching algorithms have been researched over the last two decades with hundreds of variants [18], [23], they still have several difficulties for on-line multivehicle map merging on the road. First, there is no guarantee that sensor configuration of each vehicle is identical. Second, each map has different scan points even in the overlapping area. For these reasons, scan points and features may not be obvious to match. Furthermore, bushes, trees, or moving pedestrians make the situation worse. Finally, the overlapping area may not be sufficient due to longer safety gaps for collision avoidance. In this paper, we devise, apply, and compare the scan matching method to map merging for driving on the road scenario using both a closed-form solution (ICP) and a probabilistic one (CSM).

Meanwhile, one of the primary goals of this paper is to establish a spatial map sufficient for driving control and assistance. For this purpose, there have been several research efforts using GPS. Tan and Huang proposed a Differential GPS (DGPS)-based cooperative collision warning system partially motivated by the inaccuracy of off-the-shelf GPS [24]. Wender and Dietmayer proposed an algorithm to deliver onboard sensing information via wireless communications, whose main motivation was that a position from DGPS is still less accurate than an onboard sensor, specifically a laser scanner [25]. Chong et al. experimentally demonstrated that onboard sensor-based localization can provide centimeter-level position accuracy sufficient for fully autonomous vehicle control without GPS or DGPS in [26]. In this context, we focus on sensing information, rather than GPS or DGPS, to obtain the accurate relative position of vehicles. One of the strongest aspects of this approach is no common coordinate system is assumed among vehicles, which makes the proposed perception system robust from signal measurement variation according to the change of weather, or instability of the central system. B. Multimodal Perception and See-Through Systems The sensors used for driving assistance or automation purposes can be largely classified into active sensors, (e.g., radar, laser, and ultrasonic sensors), or passive sensors, (e.g., monocular and stereo-vision sensors). Since both are complementary to each other, a multimodal approach fusing information from active and passive sensors has been actively researched [27], [28] for various purposes, such as vehicle detection and tracking [29], pedestrian detection and tracking [30], intersection safety [31], and see-through systems [32]. Recently, see-through systems have begun to be considered as one of the emerging driver assistance technologies to mitigate traffic accident caused by perception limitations. OlaverriMonreal et al. [33] introduced a see-through system based on vehicular ad hoc networks for assisting overtaking maneuvers. The same group proposed a method that displays a see-through view to a human driver using augmented reality in [34]. Note that the see-through systems can be achieved through video streaming between vehicles [35], [36]. In contrast to simulation verification of these works, Li and Nashashibi provided experimental results using two vehicles on the road with the consideration of sensor multimodality in [32]. In the context of delivering additional sensor information for driver assistance, the see-through systems share a similar goal with this paper. In this paper, however, we will extend one step further toward cooperative driving. C. Cooperative Driving Using Cooperative Perception One of the most promising applications of cooperative perception on the road is cooperative driving for traffic safety and efficiency [24], [37], [38], because the results of map merging extend perception range beyond line-of-sight and sensing angles. Accordingly, they enable traffic flow prediction, early obstacle detection, and long-term perspective path planning. The work of Tsugawa et al. [39] included the architecture

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

665

Fig. 3. Concept of leader string where i is an ego vehicle. i + 1 and i + 2 are the first and second leader of i, respectively. i - 1 is a following vehicle behind ego vehicle i.

Fig. 2.

Architecture of the proposed system.

for cooperative driving using cooperative perception. The agenda and experimental results of cooperative autonomous driving have been presented in [4], [40], and[41]. In particular, Kim et al. provided the motivation and experimental results of cooperative autonomous driving using cooperative perception in [3]. Liu et al. proposed the concept of motion planning using cooperative perception on the road [42]. Rebsamen et al. proposed infrastructure-support sensing for driving [43]. In addition, vehicle communication and vehicle identification problems are addressed in [44]­[48], respectively. In this paper, we focus on cooperative driving using cooperative perception in an engineering feasibility standpoint. III. S YSTEM A RCHITECTURE Fig. 2 shows the overall system architecture of the proposed system. The system consists of three subsystems: cooperative perception, perspective visualization, and cooperative driving. For cooperative perception, each vehicle is equipped with range sensors, such as laser or radar scanners, vision sensors, and radios, as shown in the leftmost box of Fig. 2. An odometry system is utilized for ego-motion estimation such as moving direction and speed. The range sensors are used for vehicle detection and tracking. The vision sensors are used for classification and identification of vehicles and pedestrians and provide driver-friendly visual traffic information. The vehicles exchange this local sensing information with other vehicles or infrastructure via wireless communications. In this paper, we define the exchanging of information as a message. Since there exists a tradeoff between communication performance and information quantity, the message profile, e.g., message size and transmission period, should be carefully chosen according to application requirements and driver preferences. In this paper, we investigate several message profiles, such as laser scan data only, raw image only, compressed image only, point clouds only, and both laser scan and point clouds, which are investigated in detail with the real measurement data on the road in Section VIII-B. All local and remote sensing information is properly fused at the box of cooperative perception in Fig. 2. Compared with data fusion of on-board sensing information, data fusion of remote sensing information on the road includes a number of practical challenges [3], among which this paper focuses on the map merging problem and sensor multimodality.

After the information fusion procedure at cooperative perception, the fused information can be used for driving assistance for a human driver and motion planner for an autonomous driver. In addition, it can be delivered to the vehicles following behind or closest infrastructure. For the purpose of driving assistance, our proposed system represents the fused information in a perspective visualization manner, to provide intuitive guidance information, which is dealt with in Section V. Finally, one of the primary goals of a cooperative driving subsystem is to let a driver know the moment when the driver should be careful, such as hidden obstacle detection or sudden braking of preceding vehicles beyond line-of-sight. The notification is performed by visual and sound alarms, which enable a driver to focus on driving until any dangerous situation is detected by our system. Moreover, the subsystem notifies a driver when there are any vehicles coming from behind or at blind spots, which can contribute to safe lane changing or overtaking. For self-driving vehicles, the notification triggers path re-planning for automated lane changing in Section VIII-D. IV. C OOPERATIVE P ERCEPTION ON THE ROAD On the road, the preceding vehicles highly affect the driving decision of the ego driver. In this sense, a leader is a preceding vehicle: 1) connected via wireless communications; and 2) observable by the ego vehicle through its local sensors or remote sensing information. Let Vi = {i + 1, . . .} be a leader string of vehicle i, where, j, j + 1  Vi , j and j + 1 are connected via wireless communications and j + 1 is observable by local sensors of j . The concept of leader string is represented in Fig. 3. In a broad sense, following vehicles of ego vehicle i can be included in the leader string Vi , even though a vehicle i - 1 is not a leader literally. The information of a vehicle i - 1 can be useful for lane-changing assistance or blind spot detection, because the following vehicle i - 1 can watch the ego vehicle in the third-person view from behind the ego vehicle. In vehicle driving scenarios, sensing information is typically dealt with as a map. Let M = {. . . , m, . . .} be a map for navigation of vehicles, which consists of the set of points obtained and filtered from sensors, where m  Rn , n = 2, or 3. Given a position p  M in a map, M[p]  R can be defined in several ways, such as the belief that the position p is obstaclefree, or the height of obstacle in case of p  R2 . Mi denotes a map of a vehicle i. Now, we can formulate cooperative perception as follows: Mi = Mi
j Vi

Mj

(1)

where the operation is called map merging. The map merging operation is merely a set union operation, if Mi and Mj Vi

666

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

are mapped into a global coordinate frame, such as GPS coordinates. However, the observation from sensors is typically mapped into a local coordinate frame. In addition, there is no guarantee that the initial poses of vehicles are known. In this case, the relative pose between vehicles is necessary to merge different spatial information. The relative pose can be represented as q = (, ), where  and  correspond to translation and rotation, respectively. We define a transformation operator as p  q = R()p +  . Finally, (1) can be rewritten in a more general form as follows: Mi = Mi
j Vi

1) ICP Algorithm: The ICP solution can be formally stated as follows:
 = arg min qi,j qi,j mj Mj

CMi (mj ) - mj  qi,j

(7)

where · is an l2 -norm, and CMi (mj ) is a point in Mi corresponding to mj . In the standard ICP, the closest point in Mi is selected as the correspondence, which is defined as CMi (mj ) = arg min
mi Mi

mi - m j .

(8)

Mj  qi,j

(2)

where qi,j is a relative pose between a vehicle i and j . One of the key challenges to solve (2) is to obtain an accurate qi,j . A. Problem Formulation 1) Peer-Vehicle Map Merging: The primary problem of map merging can be formulated as follows:
 qi,j = arg max S (Mi , Mj , qi,j ) qi,j

 ICP iterates (7) using Mj  Mj  qi,j until the solution  has converged. qi,j can be obtained by using a closed-form solution that Arun et al. proposed in [49]. Note that there is no guarantee that the closest point (8) is the right corresponding point. In many cases, two maps are partially overlapped, or scan points may not be exactly matched due to the physical limitation of a sensor, e.g., different sensing range or resolution. From an implementation perspective, the closest point should be limited by some threshold distance. Now, (7) can be rewritten as follows:  = arg min qi,j qi,j mj Mj

(3)

dMi (mj ) CMi (mj ) - mj  qi,j (9)

where the similarity measure S is formulated as L (Mi [p], (Mj  qi,j )[p])
p

(4)

where d M i ( mj ) = 1 0 if minmi Mi mi - mj < dth otherwise, (10)

where L(ai , aj ) is a point-to-point similarity measure, which is positive if ai = aj and 0 otherwise. Equations (3) and (4) attempt to find the relative pose that maximizes the overlapping area between two maps. Note that (3) and (4) are well-suited for a range-scanner-based approach. We present a method to merge vision-based data in Section IV-D. 2) Multivehicle Map Merging: Equations (3) and (4) can be extended to more than two vehicles. Let Qi = {qi,i+1 , . . . , qi,i+N } be the set of relative poses with respect to (w.r.t) the ego vehicle, where N is the number of leader vehicles. The problem can be rewritten as follows: Q i = arg max S (Mi , . . . , Mi+N , Qi )
Qi

where dth is the threshold distance. dth is one of the key design parameters to decide the performance, which has a tradeoff between accuracy and convergence speed. Using the concept of closest points, the similarity metric S can be reformulated as follows: S (Mi , Mj , qi,j ) =
p

dMi (mj ) Mi [p] - CMj qi,j (p) . (11)

(5)

where the similarity measure S is formulated as L (Mi [p], . . . , (Mj  qi,j )[p], . . .)
p

(6)

where L(ai , . . . , ai+N ) is positive if ai = · · · = ai+N and 0 otherwise. B. Scan Matching Algorithm
 To solve (3)­(6), we use a scan matching function qi,j = ScanMatching(Mi , Mj ). Several approaches can be used for realizing the function, as mentioned in Section II. In this paper, we consider two kinds of approaches: One is ICP, a closed-form solution, and the other is CSM, a probabilistic solution.

2) Probabilistic Scan Matching: Probabilistic scan matching methods approach the problem differently from the closedform solutions such as ICP, in particular, where several uncertainties are considered, such as sensor measurement errors. The key principle is to find a pose at which the current observation is maximally likely or most probable based on the previous observation, such as a map, which can be formulated as x = arg maxx P(z |x, M), where P(z |x, M) is an observation model, x is the pose of an observer, z is the sensor reading, and M is a map [50]. The formulation can be used for cooperative perception as follows:
 = arg max P(Mj |qi,j , Mi ) qi,j qi,j

(12)

where Mj corresponds to observation from a sensor located away from the observer. To efficiently compute the probability of (12), a lookup table is usually built in advance [51]. There are many solutions to obtain the maximum-likelihood  of (12). In this paper, the CSM method is considered, pose qi,j

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

667

Fig. 4. Coordinates and overlapping areas of the ego vehicle and its leader. Hi and Wi are the height and width of the local sensing area of a vehicle i. oi,i+1 is the height of overlapping sensing area between a vehicle i and i + 1 from the perspective of a vehicle i. Likewise, oi+1,i is the height of overlapping area between a vehicle i and i + 1 from the perspective of a vehicle i + 1.

because of several advantages for our purposes. One key aspect of CSM is the ability to find the global maximum instead of trusting a local search algorithm. By evaluating the entire search space, the scan matching algorithm is able to provide a robust solution even with a large initialization error. Because of this, CSM approaches the problem using multilevel resolution. Specifically, two lookup tables are used for avoiding local maxima and achieving lower computational complexity: a low- and a high-resolution table. First, the likelihood is quickly evaluated over the entire 3-D search window by using the low-resolution table. When the maximum-likelihood voxel is found in the process, it starts to find more accurate values by evaluating inside the voxel using the high-resolution table. One can find a specific description in detail in [20]. C. On-Road Map Merging Method In this paper, we consider a 2-D map for map merging, i.e., M[m]  R, m  R2 . Fig. 4 shows the spatial coordinates of the ego vehicle, and its predecessor vehicle, where a leader is used for the predecessor vehicle. We assume that range sensors are installed in a y -axis direction. (lx , ly ) represents the position of the leader. Algorithm 1 shows the overall procedure of our map merging method. Two maps from different vehicles are taken as input parameters, and a merged map is generated from the algorithm.

data of a leader has relatively clear features such as the side or backside of vehicles. The initial pose of a leader can be guessed from the features. More specifically, the proposed system keeps checking whether there are consecutive points lying in the desired path ahead. For this task, let Di be the set of positions that a vehicle i intends to move to, typically the set of the center positions of the current lane, which can be generated by combination of vehicle localization, lane detection and path planning methods. We use curb-intersection features based Monte Carlo localization [52] for lane-keeping, and a path planner for lane-changing as described in [42]. In Fig. 24(a), the yellow horizontal line passing through the vehicles depicts a simple Di to keep within a lane. In Fig. 24(c), the red curve depicts Di as a result of path replanning for lane changing. To find the leader position, the proposed system keeps watching straight or slightly curved lines in the following point set: {p|rth > p - pl , pl  Di } (13)

where rth is the maximum boundary that a vehicle can deviate from the desired path, typically half of the lane width. This method considers the current lane position to extrapolate the desired path of the leader vehicle, which can significantly reduce the search space and occurrence of false positives in leader detection compared with relying on scan matching only. If 1) the detected points compose a straight or slightly curved line, and 2) the line is almost orthogonal to the desired path, the center position of the line and the orthogonal angle to the line are used as t0 and 0 , respectively, which is used as the initial transformation q0 = (t0 , 0 ). Note that reliable leader vehicle detection is important to avoid circular reasoning [7]. Our method is to merge information that comes from the leader to information gathered by the ego vehicle. To identify the leader, vehicle identification is necessary, which is dealt with in [3] and[48]. In addition, with the support of vehicle identification and rear sensors, the following vehicle can be identified by the ego vehicle. Accordingly, the map merging process can be started from the following vehicle with information delivered from the following vehicle. This map merging from the following vehicle will be evaluated in Section VIII-B.2. 2) Overlapping Area Extraction: The size of overlapping area between two maps is relatively small during driving. Computational complexity can be reduced by restricting matching operation to the overlapping area instead of whole maps. Fig. 4 shows the overlapping area between two vehicles. In case of the ego vehicle, scan points in overlapping area are collected into i = {m|m > ly - (lx + Wi /2) tan 0 , m  Mi } (14)

where the leader position (lx , ly ) is obtained from t0 . Likewise, scan points of the leader in overlapping area, j , are {m|m < (ly - (lx + Wi /2) tan 0 ) cos 0 , m  Mj  q0 } . (15) 3) Virtual Leader Scan Points Recovery: We assume that a vehicle cannot scan itself. The map of the ego vehicle has

1) Leader Vehicle Detection: One of the distinct characteristics of scan data on the road is that features are not obvious to match due to trees, bushes, or pedestrians. However, scan

668

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Fig. 5. (a) Concept of IPM; (b) an example of projection on-road image captured by a forward-looking camera; and (c) its corresponding IPM result.

scan points detecting its leader vehicle, but the map delivered from the leader includes no scan point of the leader itself. Since a sensor is typically installed at a fixed position, the vehicle knows how it would be shown on its own map. Therefore, the vehicle can recover virtual scan points reflecting its shape as if the vehicle detects itself. The virtual laser scan points are one of the most obvious features to match, so that they can play a important key role to provide good matching points, especially when a few scan points are collected or scan points are not obvious to match. 4) Scan Matching: Now, we are ready to perform scan matching operation with Mi , Mj , and an initial transformation q0 . In this paper, we use ICP and CSM for our implementa is obtained. In tion. As a result, the optimal relative pose qi,j Section VIII-B, we will evaluate the performance in terms of computation time and accuracy with the comparison of two scan matching approaches. 5) Final Merged Map: The final map is the union of Mi  and transformed Mj with the relative pose qi,j , which can be represented as Mi
 Mj  qi,j .

Fig. 6.

Concept of perspective projection and inverse perspective projection.

(16)

D. Multimodal Cooperative Perception Our proposed system supports multiple sensory modalities for cooperative perception. In case of range sensors, such a laser scanner, it is relatively easy to perform map merging operations, because their readings are a set of physical quantities recorded with their spatial coordinates. However, in the case of vision sensors whose reading is an image, the map merging is not a straightforward task, because the vision image is the result of perspective projection. Therefore, the images should be mapped into the spatial coordinate of the ego vehicle. We use the inverse perspective mapping (IPM) [53] method to deal with this problem. IPM is an image transformation method to get a satellite view of the road surface. When a forward-looking camera is capturing an image, the shape of the road surface is usually distorted due to perspective projection. An IPM process can be applied to remove this distortion and recover original road shapes for the map merging purpose. Fig. 5(a) illustrates the basic idea of IPM. For a fixedmount on-board camera, its pose in the vehicle coordinate frame is usually known, and hence the pose relative to the road surface can be also obtained. Together with its intrinsic parameters from a calibration process, the perspective transformation matrix from the road surface to the camera image can be

calculated. An inverse operation of this perspective matrix will restore the original road surface, and store it in an IPM image. Fig. 5(b) and (c) shows an example of projection on-road image captured by a forward-looking camera, and its corresponding IPM result, respectively. One can find more details on IPM in ITS-related applications in [54]. From a mathematical point of view, IPM is an inverse operation to camera projection transformation. In the projection process, a 3-D point [X, Y, Z ] is projected onto an image with its projection pixel as [x, y ], where [x, y, 1]T = P [X, Y, Z, 1]T , and P is the 3 × 4 camera projection matrix. In the IPM process, we assume that the height of points on the road surface is 0, where their 3-D coordinates [X, Y, 0] can be easily recovered with the P matrix, and their image pixels [x, y ]. Fig. 6 describes these operations. Due to the assumption of IPM where points lie on the road surface, it is only suitable to recover the object shapes with Z = 0 height. In our experiment, we use it to facilitate our map merging for the road surface. Other objects such as pedestrians and vehicles, which violate the IPM assumption, will be taken care with other sensory modalities. One can find a general framework for road marking detection and analysis in [55]. Although visual odometry has been proposed and used [15], we primarily use laser scan points to obtain the relative pose  in the implementation of this paper, because our system qi,j is targeting on-road scenarios, and it is much faster to deal with scan data rather than images with the consideration of  , the this purpose. Using (16) and the obtained relative pose qi,j recovered road surfaces can be merged into one. Finally, three types of information can be represented in the merged map: 1) vehicle poses, including poses of the ego vehicle and other vehicles; 2) laser scan points; and 3) color points of the road surface from vision. Fig. 14(a) shows an example of merging multimodal sensing data from a vision sensor and laser scanner. By supporting multiple sensory modalities, the cooperative perception system helps the ego vehicle to perceive not only occluded vehicles and obstacles but also road surfaces that may be out of its sensing ability. V. P ERSPECTIVE V ISUALIZATION The cooperative perception process generates a merged map with various types of information. To provide the information as intuitive driving guidance, our system represents the information in a perspective projection.

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

669

Fig. 7. Example of a perspective see-through visualization on the road. The two numbers next to the cuboid are the distance from the ego vehicle and its speed, respectively. The gray dotted lines represent road lanes. The cuboid represents a simplified vehicle model. (a), (b), and (c) are examples of visual warning signs for driving assistance, where (a) is a forward collision warning, (b) is a sign for overtaking assistance and (c) is a sign for lane-changing assistance. These visual warnings can be activated along with audio or tactile warning.

Fig. 8. Schematic of a third-person visualization. A virtual camera is mounted above the roof of ego-vehicle.  is its tilted-down angle, and D is its mounting height. The tilted-down angle and mounting height of this virtual camera can be adjusted to find an optimal view to visualize the data from cooperative perception.

A. First-Person See-Through Visualization Through the map merging steps presented in Section IV, a 3-D spatial map is obtained from different sensor information, where x, y along with z as an occupancy probability or height of (x, y ). Contrary to the way of the IPM method, the perspective projection method can map 3-D points to a 2-D plane, as shown in 4 of Fig. 6. Based on the perspective projection method, we can provide a first-person see-through visualization consisting of the perspective visualization of the road surface and vehicle skeletons. The concept of the first-person see-through system using cooperative perception has been proposed in [32]. Here, we present our implementation in detail. Fig. 7 shows an example of the first-person see-through visualization on the road. The leftmost and rightmost boxes are used for providing visual warning to a driver, which is explained in detail in Section VI. 1) Road Surface Projection: As a first step, the road surface is restored from vision images of the ego vehicle or leader vehicles by the IPM method and represented as a cloud of color points, as addressed in Section IV-D. To visualize the road surface information for a human driver, we project the surface points onto the camera image and label the projection pixels to the color of their corresponding points. 2) Vehicle Skeleton Visualization: Vehicle detection and tracking is realized with laser scan data readings, with vehicle poses stored in the merged map. To visualize the detected vehicles more clearly, we use a cuboid as the simplified vehicle model, and then plot the skeleton of this cuboid. In a bad situation, e.g., at night, under shadow, in heavy rain, such as an explicit cuboid definitely helps to identify vehicles. The cuboid has 8 vertices and 12 edges. Its vertices are projected onto the camera image, and its edges are drawn by connecting the projection pixels of neighboring vertices. B. Adjustable Third-Person Visualization While the first-person see-through visualization provides an intuitive representation of the merged data, various kinds of information is squeezed and overlapped at a small region of its

image, due to the low mounting position of the vehicle camera, which makes them difficult to recognize ahead traffic situations, as we can see in Fig. 7. For this reason, we propose to use an adjustable third-person view to visualize the data. Fig. 8 illustrates the basic idea of third-person visualization. In Fig. 8, hv , gi,j , and hc are the camera installation height, the distance from a vehicle i to a preceding vehicle j , and the uplifted vision height, respectively. While the mounting height hv + hc and titled-down angle  of this virtual camera are both adjustable, the best view point can always be selected to visualize the merged data in different scenarios. Note that the camera is typically mounted with some tilted-down angle, e.g., 12 in our experiments. For determining a proper , we use the following boundary for deciding a tilt-down angle w.r.t. to camera height hc and distance gi,j   arctan hv + hc gi,i+1 - a 2 (17)

which can be derived from Fig. 8, where a is a horizontal angle of view. Likewise, if we want to restrict the maximum number of tracking leaders as s, the upper boundary of  is  < arctan hc s j =i+1 + a 2 (18)

gi,i+1 +

(gj,j +1 + lj )

where gi,i+1 is a distance between a vehicle i and i + 1, and li is the width of a vehicle i. For example, the minimum tilteddown angle  is 0.8  45.8 to obtain a bird's eye view at 20 m(= hc + hv ) if a vision camera is installed at hv = 1 m, a target vehicle is preceding at L = 10 m ahead, and the vertical field-of-view of the camera is a = 35 . One may want to watch the display only when necessary, instead of watching it continuously. For this purpose, the next section addresses a see-through forward collision warning. VI. C OOPERATIVE D RIVING A SSISTANCE Here, we present how cooperative perception results presented in Section IV and V can be applied to specific driving assistances, such as forward collision warning, overtaking/lanechanging assistance with an all-around view.

670

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Furthermore, our system can detect a vehicle approaching toward the ego vehicle using a spatio­temporal moving obstacle detection and tracking method. From the following, we investigate the benefit of an all-around view in terms of overtaking assistance and lane-changing assistance.
Fig. 9. Collision warning with and without cooperative perception.

C. Overtaking/Lane-Changing Assistance A. See-Through Forward Collision Warning A driver can focus on driving itself without having to keep watching a driving assistance display, if the system can let a driver know the moment when the driver has to drive carefully through proper visual, audio, or tactile feedback. In this context, one of the most desired functions is forward collision warning. Numerous forward collision warning algorithms have been proposed along with risk assessment methods, which include time-based and distance-based approaches. First, the time-based methods usually use time-to-collision (TTC), which can be formulated as T T Ci,j = gi,j , vi - vj j  Vi (19) When a driver should drive slowly or stop due to a slowmoving truck or obstacle ahead, the driver should decide whether to wait longer in the current lane or change lanes. In particular, this is an important issue for overtaking on a singlelane road. Various overtaking assistance methods are possible according to the sensor configurations, requirements, and sensing capability. The overtaking decision should be determined with the consideration of: 1) the number of lanes; 2) the speed of a preceding vehicle; 3) cut-in space availability; 4) distance from an on-coming vehicle; and 5) the existence of another overtaking vehicle from behind as in Fig. 16(h). Our system can inform 3), 4), and 5) that are difficult to be provided without cooperative perception due to the limitation of line-of-sight. Equations (19) and (20) can be applied to read-end collision warning at lane changing, which are corresponding to T T Cj,i and rj,i , respectively, where the vehicle j is approaching toward the ego vehicle behind in an adjacent lane, and j < i. D. Feedback to a Driver Once a forward collision is expected, or overtaking is not possible, or lane changing is not possible, it should be notified to a driver through visual, audio, or tactile feedback. Fig. 7(a)­(c) are examples of visual warnings for a forward collision warning, overtaking possibility, and lane-changing possibility, respectively. In Section VIII, we evaluate how our system can contribute to the see-through forward collision warning, overtaking assistance, and lane-changing assistance are, through real experiments on the road using vehicles equipped with the proposed system. VII. S CALABILITY A NALYSIS Information delivered via wireless communication inherently has uncertainty. In this paper, we quantify communication uncertainty by using communication delay, because the communication delay highly affects the overall system performance of moving vehicles. Let dc i,j be communication delay for message delivery from
p c a vehicle i to j . For simplicity, we define dc i = di+1,i . Let di be p the processing delay of a vehicle i. In general, dc i and di are not fully controllable, so they should be measured and estimated. The leader i + 1 is moving, whereas the information is being processed and delivered from i + 1 to i. From the perspective of a vehicle i, the estimated position p ~i+1 is p p ~i+1 = pi+1 + vi+1 (dc i + di ) 

where T T Ci,j is corresponding to TTC, and vi is the speed of a vehicle i. A forward collision warning is activated if T T Ci,j is less than a certain threshold time, typically 2­3 s according to safety requirements [56]. In distance-based methods, a forward collision warning is activated if, given two vehicles i and j , the recommended safety gap ri,j is larger than the distance between two vehicles gi,j . Formally, a forward collision warning is activated if gi,j < ri,j , j  Vi (20)

where ri,j is a minimum distance to avoid collision to a preceding vehicle j , which can be formulated as follows [57]: ri,j = (vi - vj )/2 2 + (vi - vj )  Trs , j  Vi (21)

where  is the deceleration of the ego vehicle, e.g., -0.2 g ( -2 m/s). Trs is the response time of a driver, e.g., 0.5­1.5 s. Note that one of the great advantages of our system is that j is not limited to only i + 1. In our system, due to the capability of a see-through view, j can be beyond the first leader according to the connectivity via wireless communications, as shown in Fig. 9. This see-through characteristic enables the driver to avoid hidden obstacles earlier than without cooperative perception systems. We will investigate this early hidden obstacle detection in Section VIII-C. B. All-Around View Using Cooperative Perception Another benefit of our system is that it enables an all-around view without having to install sensors covering all sides of a vehicle. In particular, a following vehicle can see the ego vehicle in a third-person view, including its blind spots. Fig. 16(e) shows one snapshot of the all-around view of the ego vehicle. This all-around view aspect of our system is investigated using real experiments in Section VIII-B.2.

(22)

 i+1 can be obtained by vehicle where pi+1 and vi+1 = p i, as addressed in Section IV. By using the definition of

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

671

and the driver's reaction time is 1.5 s. For example, total delay should be less than 2 s to detect and avoid the obstacle at 100 m ahead, if the vehicle runs at 100 km/h, and driver's reaction time is 1.5 s. The delay boundary, in this case 2 s, is a key parameter to determine message profile and radio interface. We evaluate this using measurement data from experiments on the road in Section VIII-B.3. VIII. E XPERIMENTAL R ESULTS Here, we provide experimental results to evaluate our proposed system with four automotive vehicles. A. Experimental Setup
Fig. 10. Delay upper boundary including processing time w.r.t. distance from an obstacle ahead, where a driver's reaction time is set to 1.5 s.

communication and processing delay and (22), we derive the worst case deadline for information delivery as follows. Assuming that no vehicle moves backward on a given path, information delivered from a vehicle k  Vi has an impact to path planning and vehicle control of an ego vehicle i whose maximum speed is v i , if the following inequality is satisfied: ds =
j =i  k -1 p dc j + dj <

pk - p i - Trs min (v i , vi + ds ui )

(23)

where ds is a total delay for delivering k 's information to i, and ui is the maximum acceleration of a vehicle i. A sketch of proof is as follows. Suppose that k is an obstacle staying at a position pk as a worst case, because we assume that no vehicle move backward. For intuitive understanding, suppose that i + 2 is a stopped vehicle or a static obstacle in Fig. 3. In this case, k is i + 2. Note that the position pk is recognized by a vehicle k - 1. In (23), min(v i , vi + ds ui ) represents the maximum speed that the vehicle i can perform during information delivery of the vehicle k . Therefore, the second rightmost term of (23) represents the minimum time that the vehicle i can reach the position pk . The rightmost term of (23), i.e., Trs , is a reaction time to steer or slow down for collision avoidance. As a result, the rightmost two terms represent the minimum time that the driver can react to a dangerous situation ahead. The ahead traffic information should be delivered within the time. Depending on the applications, an average delay boundary can be more useful. Suppose that a group of vehicles achieves velocity consensus at vi on highway. In this case, the delay boundary can be relaxed as follows: ds < pk - p i - Trs . vi (24)

In the experiments, we used several mass-produced automotive vehicles including one Mitsubishi iMiEV and three Yamaha golf carts. The vehicles are equipped with 2-D LIDARs (LIght Detection And Ranging), a vision camera, and wireless interface IEEE 802.11g, IEEE 802.11n, 3G HSDPA (HighSpeed Downlink Packet Access), and 4G LTE (Long-Term Evolution). The devices of 3G HSDPA and 4G LTE are Huawei E153 and Huawei E3276, respectively, which are both USBtype. The vision cameras were mounted with a tilted-down angle of 12 from horizontal. The software architecture of this system was established on robot operating system (ROS) suite [58] using only open source libraries. Detail specifications are available in [26]. The setup of three vehicles is represented in Fig. 9. i, i + 1, and i + 2 are corresponding to the ego vehicle, the first leader and the second leader, respectively. In Fig. 9, the second leader transmits its sensing information to the first leader via wireless communications, while moving forward. With the support of our system, the first leader merges the remote information with its local sensing information, and then transmits the combined information to the ego vehicle via wireless communications, while moving forward as well. In this experiment, we assumed vehicle identification is supported before map merging. B. General Evaluation The experiments was performed on a campus road at National University of Singapore (NUS), Singapore, where the rule of the road is to drive on the left and the speed limit is 40 km/h. Fig. 11 shows the test road. The fleet of test vehicles start to move at Start toward End via P-turn. The road from Start to P-turn is an urban road with buildings. The road from P-turn to End is a road often lined with trees and bushes on either side, and includes a hill road. Experiments were conducted on sunny or cloudy days between 12 P. M . and 5 P. M . 1) Map Merging Evaluation: Fig. 12 shows the comparison of the different scan matching methods used to perform map merging during driving on the test road. The initial pose is obtained from LIDAR-based vehicle detection using the method presented in Section IV-C.1. The obtained pose is used as the initial condition for both the ICP and CSM scan matching algorithm. In this paper, the ICP algorithm from [59] is used. In the ICP method, dth = 2.5 m is used. For CSM, two levels of

As long as k satisfies (24), k can be increased. In this way, the maximum size of leader string can be obtained by using (23) or (24). Note that k is highly affected by message profile, radio characteristics, and vehicle speed. Fig. 10 shows the upper boundary of total communication and processing delay under the assumption that the speed of the ego vehicle is constant

672

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Fig. 11. Test road at NUS. The fleet of test vehicles start to move at Start toward End via P-turn.

Fig. 13. Leader vehicle detection (a) in multilane roads and (b) at a curb. The horizontal blue dotted line is the desired path of the ego vehicle. Only vehicles detected near the desired path can be classified as a leader vehicle.

Fig. 12. (a) Translation errors and (b) rotation errors of map merging, according to the method. Average computation time of LIDAR, ICP, and CSM is 6, 6, and 108 ms, respectively.

0.5 and 0.1 m search grids are used. It is also set to search within a window of 10 m × 10 m × 60 . In order to obtain the ground truth, a particle filtered-based localization is used. We built a map using a 2-D laser scanner for localization on the test road before all experiments. Each of the three vehicles is localized by using the map and the relative position is extracted. We evaluated the whole trajectory of the vehicles on the test road. Details on the mapping method and its evaluation are available in [60]. In Fig. 12, the average error increases from the first leader to second leader across different map merging methods. The CSM, although slower, is able to ensure a high-quality scan matching and therefore able to achieve slightly over 1-m errors. This is also true when the rotation error is considered. There are several factors that contribute to the rather large standard deviation. Among others, the noisy observation obtained from LIDAR in an urban environment, LIDAR-based vehicle detection algorithm uncertainty and communication delays are the main contributing factors. Through Fig. 12, it was found that there are significant improvements in the merged map accuracy from the second leader by ICP and CSM. While CSM performed slower than ICP, it is able to robustly estimate the relative pose since CSM is robust to initialization error. We conclude that the results using the both scan matching approaches perform sufficient for our requirements. Fig. 13 shows the leader vehicle detection addressed in Section IV-C.1. The method works well in a situation where there are a number of vehicles in the adjacent lane. 2) Visualization Evaluation: Fig. 14 shows a variety of visualizations made possible through our system including a satel-

lite view, see-though views and third-person views. Fig. 14(a) is one snapshot of a merged map of the ego vehicle resulted from the proposed system. The red, green, and blue dots represent laser scan points of the ego vehicle, the first and second leader, respectively. Note that the ego vehicle cannot see the second leader due to the limitation of line-of-sight. Fig. 14(b) is the raw vision image from the second leader via wireless communications, where four small green circle indicate the region of interest for IPM. Fig. 14(c) and (d) are the raw vision image from the first leader via wireless communications and of the ego vehicle via a local vision sensor, respectively. Fig. 14(e) is the see-though view from the first leader where the cuboid is a simplified vehicle model. Comparing to Fig. 14(c), the otherwise obstructed road marking "AHEAD", as shown in Fig. 14(e) in a see-through manner. Fig. 14(f) is the see-though view from the ego vehicle, where the blue and green cuboid represent the second and first leader, respectively. Fig. 14(g) and (h) are the third-person views of the first leader and the ego vehicle, respectively, where viewpoint is lifted at 4 m from the ground in Fig. 14(g) and 30 m from the ground in Fig. 14(h). In brief, we can see that our system provides ahead traffic information even beyond line-of-sight, which can be applied to a see-though forward collision warning or an overtaking assistance. At a sharp curve, such as T-junction or intersection, oncoming cars at blind spots can be potentially dangerous. The proposed system can help see an hidden on-coming vehicle at blind spots. Fig. 1(b) is a good example of this. As like the sharp curve, a downhill also includes a hidden obstacle problem. In Fig. 15, the vehicle i cannot see the vehicle i + 2 due to the limitation of line-of-sight. However, the vehicle i + 1 can let the vehicle i know the situation of the vehicle i + 2, as long as the vehicles are connected via wireless connectivity. Instead of the vehicle i + 1, an infrastructure equipped with cooperative perception system can let the vehicle i know the traffic situation at a downhill ahead. Fig. 16(a)­(d) shows how our system aides in driving over a hill. Fig. 16(a) is the second leader's camera view, where the preceding vehicle is descending from the top of hill. Fig. 16(b) and (c) are the camera views of the first leader and the ego vehicle, respectively. The ego vehicle is ascending a hill. Fig. 16(d) is a third-person view lifted at 30 m from the ground.

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

673

Fig. 14. (a) Merged map of the ego vehicle, where the red, green, and blue dots represent laser scan points of the ego vehicle, the first and the second leader, respectively. (b) Raw vision data from the second leader, where four small green circles represent the region of interest for IPM. (c) Raw vision image from the first leader. (d) Raw vision image of the ego vehicle. (e) See-though view of the first leader where the cuboid is a simplified vehicle model. Comparing to (c), a traffic sign on the road surface can be seen in (e) in a see-through manner. (f) See-though view of the ego vehicle, where the blue and green cuboids represent the second and first leaders, respectively. (g) Third-person view of the first leader, where driver's seat is lifted at 4 m from the ground. (h) Third-person view of the ego vehicle, where driver's seat is lifted at 30 m from the ground.

Fig. 15. Hidden obstacle problem at downhill.

Fig. 16(e)­(h) shows how all-around view is supported by a following vehicle connected via wireless communications. For this reason, the vehicle order is different between Fig. 16(a)­(d) and (e)­(h). Fig. 16(e) is the satellite view of the ego vehicle, where the gray arrow indicates the ego vehicle. The ego vehicle is moving between the first leader and the following vehicle. In Fig. 16(e), the ego vehicle can detect a vehicle approaching behind, because the following vehicle connected via wireless communications can tell the ego vehicle about what is going on at blind spot of the ego vehicle, which can greatly improve situational awareness. Fig. 16(f) and (g) are a normal camera view and see-though view of the ego vehicle, respectively. Fig. 16(h) is a view of ego vehicle from behind given by the following vehicle. All-around view characteristics of Fig. 16(e) and (h) can be applied to the lane-changing and overtaking assistance. In addition, a normal vision camera can sense the adjacent left and right lane, as we can see Fig. 16(e). A fisheye lens can be considered for multiple lane detection.

Fig. 17 shows the perspective projection results according to mounting height and tilted-down angle, as presented in Section V-B. In the figure, the left-bottom (hc + hv , ) indicates a camera mounting height (unit: meter), and a tilted-down angle (unit: radian), respectively. Fig. 17(a)­(i) shows a thirdperson view according to the different hc + hv and different . In case of hc = 0, our system performs similarly to see-through systems. According to the growth of hC , the projection result generated by our system becomes close to a satellite view via a bird's eye view. Consequently, our proposed system supports drivers to be able to choose optimal parameters according to their preferences. 3) Communication Evaluation: Wireless communications are one of the key factors that enable cooperative perception, and at the same time, significantly affect overall system performances. In this paper, we suggest or propose no new communication method. Instead, we use IEEE 802.11g, IEEE 802.11n, 3G HSDPA and 4G LTE, which are off-the-shelf commercially ready solutions. Therefore, this evaluation focuses on how communication profile and uncertainty affect system performance in a given radio interface. Table I provides a message profile we used in the experiment. The message is transmitted to other vehicles at 20 Hz. More specifically, we use standard message types in ROS, to transmit our sensing data. First, laser scan data is transmitted in the form of LaserScan, where 180 laser beams are contained in each scan. The total amount of information

674

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Fig. 16. [Driving over a hill] (a) Second leader's camera view, where the preceding vehicle is descending from the top of a hill. (b) Camera views of the first leader. (c) Camera views of the ego vehicle that is ascending the hill. (d) Third-person view lifted at 30 m from the ground. [All-around view] (e) Satellite view of the ego vehicle. Note that the ego vehicle is moving between the first leader and the following vehicle. In (e), the ego vehicle can detect a vehicle approaching from behind. (f) and (g) are a normal vision image and see-though image of the ego vehicle, respectively. (h) is view of the ego vehicle from behind.

Fig. 17. Left-bottom indicates (camera height hc + hv , tilted-down angle  ). (a)­(i) show third-person view according to the different hc + hv and different  . In case of D = 0, our system performs similarly to the conventional see-through systems. As D increases, the projection result becomes closer to a satellite view. Drivers can choose optimal parameters according to their preferences.

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

675

TABLE I M ESSAGE P ROFILE AND DATA S IZE

Fig. 18. Average delay for one single message delivery between two nodes away from 10 m each other via IEEE 802.11g, 4G LTE, and 3G HSDPA.

Since the target of the proposed system is a moving vehicle on the road, the communication should be investigated from the perspective of impact on vehicle control, specifically position errors in this evaluation. Fig. 19(a) shows the impact of IEEE 802.11g communication delay on position error according to vehicle speed. From the perspective of average delay, the position error of one hop is 24 cm at 100 km/h in case of laser scan data only. The worst case position error becomes 5 m at 100 km/h. This may be good enough to be used as controlpurpose information, depending on the goal or task. However, communication delay becomes significantly uncertain, as the size of data increases. Depending on the application and purpose, the message profile must be carefully determined. For example, in case of fusion with laser scan and processed vision of this measurement data, the worst case position error is 8 m. Fig. 19(b) and (c) provides average/worst delay in case 4G LTE and 3G HSDPA, respectively. We can say that information from other vehicles are less reliable than locally sensed information in terms of delay or loss. From the perspective of vehicle control, time-critical control, such as emergency braking, should be determined based on local sensing information. Remote sensing information can be useful for long-term perspective path planning, such as decision problem between early lane changing and lane keeping for a human driver, as well as an autonomous driver. C. Usability Test We conducted usability tests with real drivers using real vehicles equipped with the proposed cooperative driving system. This paper proposes no new H/W to display our visualization results to drivers. Instead, one can use any display H/Ws that have been already used or considered for driver assistance systems such as dash board, center console, head-up display, or smartphone. In this test, we used a 20-in LCD monitor for visualization and installed it at a front passenger seat. Fig. 20 shows usability test scenarios. In this test, we aimed at evaluating a see-through forward collision warning and overtaking assistance, including lane-changing assistance. Initially, three vehicles move toward the leftmost flag on a single-lane road, where a test driver drives the vehicle i. Then, the vehicle i + 2 suddenly stops at a certain position. To avoid collision, the vehicle i + 1 and i stop accordingly. After a while, the vehicle i + 2 speeds up, but i + 1 moves slowly. The test driver should make a decision whether to overtake the slow-moving vehicle i + 1 or follow the vehicle in the same lane for safety. In the opposite lane, oncoming vehicles are approaching toward the vehicle i randomly, which makes the test driver hesitate the decision. In this experiment, the vehicle i + 2 was moving forward at 2­5 m/s before sudden braking. Accordingly, TTC was set to 15 s. These are somewhat conservative parameters mainly for safety concerns. Note that the brake lights of the vehicle i + 2 was turned off, which makes the vehicle i + 1 hard to notice whether the vehicle i + 2 decelerates or not. The warning sound includes two loud beeps that last for 2 s per one warning activation. We used IEEE 802.11n as a radio interface of vehicle-to-vehicle communications. LIDAR and compressed

per frame is 752 bytes/frame = 25 bytes message header) + 7 bytes message description) + 180 beams × float32 (4 bytes). Second, the raw data is 691,200 (640 × 360 × 3 (24 - bit color)) bytes. The size of compressed images is roughly range from 50 to 110 kB in these experiments. The processed vision contains metadata, such as lane information, which is represented by a point cloud and transmitted as PointCloud in ROS. The message size varies depending on the extracted information, which is usually less than 6 kB, much smaller than raw images. Finally, we evaluate sensor multimodality with laser and processed vision data. To monitor communication delay according to the message profile and radio interface, we set up two nodes, 10 m away from each other, and measured round-trip time (RTT). More specifically, the sender attached its CPU clock to the message. Once the message arrived at the receiver, the receiver resends the message to the sender without any message modification, such as an echo server. The sender can compare current local CPU clock with the CPU clock included in the message retransmitted from the receiver. In this evaluation, we use a half of RTT as a communication delay. The measurements were conducted for around one hour according to the message profile and radio interface. We measured communication delay in open public area, which means the delay can be affected by uncontrollable environmental interferences. Fig. 18 shows the delay measurement via IEEE 802.11g, IEEE 802.11n, 3G HSDPA and 4G LTE. In terms of average delay, IEEE 802.11g and IEEE 802.11n outperforms 3G HSDPA and 4G LTE. However, the communication performance decrease as the distance gi,j increases. In this experiment, it was observed that 3G HSDPA and 4G LTE were not affected by the distance between communication correspondences.

676

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Fig. 19. Impact of communication delay on position error in case of (a) IEEE 802.11, (b) 4G LTE, and (c) 3G HSDPA. TABLE II D ECELERATION S TARTING T IME AND WARNING ACTIVATING T IME

Fig. 20. Usability evaluation scenarios.

Fig. 21. Usability test with our proposed system on the road. (a) The bottom circle represents the vehicle i on the satellite view, where the green means no possible collision. (b) The green circle turns to red, because the vehicle i + 2 decelerates. Note that the vehicle i + 1 does not decelerate yet. (c) The vehicle i + 1 decelerates due to collision avoidance with the vehicle i + 2.

vision were used as a message profile. As shown inFig. 18, the average communication delay between the vehicle i + 1 and i is 16 ms. IEEE 802.11n performed well at more than 30-m distance on the road. Fig. 21 shows two views from the front passenger seat as seen at different time instances. The left-bottom screen of Fig. 21(a) shows the satellite view, where the bottom arrow indicates the vehicle i. The green circle indicates no collision possibility is detected at the moment. The right bottom screen of Fig. 21(a) is the perspective view. In principle, the test driver cannot see the vehicle i + 2 due to the limitation of line-of-sight. However, the test driver can see the vehicle i + 2 through our proposed system. Fig. 21(b) shows the snapshot of the activation of a seethrough forward collision warning. In Fig. 21(b), the vehicle i + 2 suddenly stopped. Accordingly, the forward collision

warning sign turned to red along with a loud sound alarm. Note that the vehicle i + 1 does not decelerate yet. In Fig. 21(c), (c) the vehicle i + 1 decelerates due to collision avoidance with the vehicle i + 2. We conducted real tests with three human drivers on the road eight times. Table II summarizes the average time of deceleration starting time and forwarding collision warning activating time. The vehicle i + 1 decelerated 3.3 s later than the deceleration of the vehicle i + 2. 3.3 s can be seen as somewhat a slow reaction time. The time is obtained when the braking lights of the vehicle i + 1 is turned on. The driver of the vehicle i + 1 released the acceleration pedal before pushing the brake pedal. The brake lights of the vehicle i + 2 were turned off, and the vehicles were moving not very fast. All these factors affected the response time of the vehicle i + 1. Average forward collision warning activating time is 2.1 s according to the preset TTC and the distance from the vehicle i + 2. The test drivers pushed the brake pedal averagely 0.82 s later after a forward collision warning sound is activating. In this experiment, the test drivers detected the sudden braking of the vehicle i + 2 earlier than the vehicle i + 1 and accordingly stopped earlier as much as 0.33 s. Fig. 22 shows the timing diagram of one of usability tests. The speed of the vehicle i, i + 1 and i + 2 were obtained from odometry, local LIDAR of the vehicle i, and cooperative perception, respectively. In this test, we can see that after the deceleration of the vehicle i + 2, a collision warning is activated around 2 s later. Then, both the vehicle i + 1 and i decelerate at the almost same time. Fig. 23 shows overtaking/lane-changing assistance test on the road. In Fig. 23(a), the preceding vehicle moves very slowly. The test driver could know there is enough space in front of the vehicle i + 1 and no oncoming vehicle in the opposite lane through the added see-through view. In Fig. 23(b), the test driver is overtaking the vehicle i + 1 with the support of our system.

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

677

Fig. 22. Timing diagram of speed of three vehicles and collision warning activation. After the deceleration of the vehicle i + 2, a collision warning is activated. Then, the vehicle i + 1 and i decelerate at the almost same time.

Fig. 23. Overtaking/lane-changing assistance test. (a) The test driver checks enough space in front of the preceding vehicle and no oncoming vehicle in the opposite lane. (b) The test driver overtakes the preceding vehicle.

Fig. 24. (a) A sudden obstacle appears in front of leader vehicle, and only the leader vehicle detects the obstacle due to the limitation of line-of-sight of the ego vehicle. (b) The leader and the ego vehicle stop, then the ego vehicle performs path replanning. (c) With the support of cooperative perception, early lane changing is triggered.

In summary, it was observed that all test drivers had more confidence of driving by our proposed system in terms of hidden obstacle detection, overtaking, and lane changing, because they could see ahead the traffic situation at their driving seat whenever they wanted, and our system notified them along with visual and sound feedback when they should watch the display. Not surprisingly, our system, particularly a see-through forward collision warning, works well even at night. D. Automated Lane Changing for Early Obstacle Avoidance Suppose that in an emergency situation, an autonomous driver could take over vehicle control, so that the vehicle could avoid obstacles earlier by early lane changing with the support of our system. Automated early lane changing could be activated if a forward collision was imminent and lane-changing possibility was positive. Fig. 24 shows simulations of the scenario. These simulations utilized a RRT path planner [61]. In Fig. 24(a), a sudden obstacle appears in front of the leader vehicle, and only the leader vehicle detects the obstacle due to the limitation of line-of-sight of the ego vehicle. In Fig. 24(b), the both of the leader and the ego vehicle suddenly stop, then the ego vehicle performs path replanning to overtake the stopped leader. However, it is difficult to find the feasible path to overtake the leader in the case of Fig. 24(b), because the ego vehicle and the leader are too close due to the sudden stop. In Fig. 24(c), with the support of cooperative perception, early lane changing is triggered according to (19) or (20). Since

Fig. 25. Our self-driving vehicle performs an automated lane-change by a seethrough collision warning trigger.

sufficient space is occupied to overtake the leader, the path planner can find the overtaking path quickly. Fig. 25 shows the moment when our self-driving vehicle performs an automated lane-change by a see-through collision warning trigger during an on-road experiment. One of the possible benefits of this automated lane changer is to reduce the reaction time Trs mentioned in (21), (23), and (24), as long as path planner and vehicle controller are faster than a human driver. The proposed method allows only one vehicle to carry out the overtaking at a time and it is not intended for a distributed overtaking. Future work includes further research on multivehicle overtaking in the context of cooperative driving. E. Impact of Weather Condition Weather conditions should be considered for potential impact on the performance of wireless communications and sensor measurement.

678

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

According to the ITU-R P.676-8 [62], signal attenuation is not significantly worsened by dry weather or water vapor at less than 10 GHz frequency, thus communications over IEEE 802.11, 3G, and 4G networks should be unaffected by rain. In contrary, sensor measurement can be highly affected by weather conditions. First of all, vision can provide abundant information under sunny or cloudy conditions during the day, but much less information during the night. Active sensors, such as LIDAR and radar, are significantly less affected by light conditions. However, LIDAR can be blocked by fog, smoke, or rain. LIDAR may be able to partially overcome this by regarding range readings less than a certain threshold as false positive during a fog/rain situation. Radar is robust against such adverse weather conditions, but provides less resolution in clean air [63]. Conclusively, the sensors should be carefully selected and located such that various sensor types can play complementary roles and help protect against any shortcomings due to adverse weather. IX. C ONCLUSION In this paper, we have provided our design and experimental validation of cooperative driving using cooperative perception. The extended perception range by the support of cooperative perception enables the driver to know the traffic situation even beyond line-of-sight or beyond field-of-view. The proposed system provides a see-through forward collision warning, overtaking/lane-changing assistance and automated lane-change capability using cooperative perception. We implemented the system and performed real experiments using vehicles on the road. The experimental results quantitatively show that the proposed system can contribute to the improvement of driving safety and non-myopic driving decision-making. Compared with cooperative driving without sharing of perception data, cooperative perception can better assist to make a driving decision in complex traffic situations by abundant information that cooperative perception provides. Furthermore, the information enables self-driving vehicles to build an online see-through map for navigation, which can be used for nonmyopic and safer decisions for cooperative self-driving. ACKNOWLEDGMENT The authors would like to thank Z. Cheng for mechanical design of the first vehicles used at our preliminary experiments, Dr. B. Luders and S. Pendleton for reading the manuscript and useful discussions, and anonymous reviewers for insightful and constructive comments, which help to improve this paper. R EFERENCES
[1] L. Merino, F. Caballero, J. M. de Dios, J. Ferruz, and A. Ollero, "A cooperative perception system for multiple UAVs: Application to automatic detection of forest fires," J. Field Robot., vol. 23, no. 3/4, pp. 165­184, Apr. 2006. [2] A. Rauch, F. Klanner, R. Rasshofer, and K. Dietmayer, "Car2X-based perception in a high-level fusion architecture for cooperative perception systems," in Proc. IEEE Intell. Veh. Symp., Jun. 2012, pp. 270­275. [3] S.-W. Kim et al., "Cooperative perception for autonomous vehicle control on the road: Motivation and experimental results," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., Nov. 2013.

[4] S.-W. Kim et al., "Multiple vehicle driving control for traffic flow efficiency," in Proc. IEEE Intell. Veh. Symp., Jun. 2012, pp. 462­468. [5] B. van Arem, C. J. G. van Driel, and R. Visser, "The impact of cooperative adaptive cruise control on traffic-flow characteristics," IEEE Trans. Intell. Transp. Syst., vol. 7, no. 4, pp. 429­436, Dec. 2006. [6] K. Konolige, D. Fox, B. Limketkai, J. Ko, and B. Stewart, "Map merging for distributed robot navigation," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2003, vol. 1, pp. 212­217. [7] A. Howard, M. J. Mataric, and G. S. Sukhatme, "Putting the'I'in'Team': An ego-centric approach to cooperative localization," in Proc. IEEE Int. Conf. Robot. Autom., 2003, vol. 1, pp. 868­874. [8] I. Rekleitis, G. Dudek, and E. Milios, "Experiments in free-space triangulation using cooperative localization," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2003, vol. 2, pp. 1777­1782. [9] N. Trawny, X. S. Zhou, K. X. Zhou, and S. I. Roumeliotis, "3D relative pose estimation from distance-only measurements," in Proc. IEEE/RSJ Int. Conf. IROS, 2007, pp. 1071­1078. [10] G. Dedeoglu and G. Sukhatme, "Landmark-based matching algorithm for cooperative mapping by autonomous robots," in Distributed Autonomous Robotic Systems 4. Tokyo, Japan: Springer-Verlag, 2000, pp. 251­260. [11] W. Huang and K. Beevers, "Topological map merging," Int. J. Robot. Res., vol. 24, no. 8, pp. 601­613, Aug. 2005. [12] A. Birk and S. Carpin, "Merging occupancy grid maps from multiple robots," Proc. IEEE, vol. 94, no. 7, pp. 1384­1397, Jul. 2006. [13] F. Amigoni, S. Gasparini, and M. Gini, "Merging partial maps without using odometry," in Multi-Robot Systems. From Swarms to Intelligent Automata Volume III . Amsterdam, The Netherlands: Springer-Verlag, 2005, pp. 133­144. [14] F. Lu and E. Milios, "Robot pose estimation in unknown environments by matching 2D range scans," J. Intell. Robot. Syst., vol. 18, no. 3, pp. 249­ 275, Mar. 1997. [15] D. Nistér, O. Naroditsky, and J. Bergen, "Visual odometry," in Proc. IEEE CVPR, 2004, vol. 1, pp. 652­659. [16] P. J. Besl and N. D. McKay, "Method for registration of 3-D shapes," in Proc. Int. Soc. Opt. Photon., Robotics-DL Tentative, 1992, pp. 586­606. [17] C. Yang and G. Medioni, "Object modelling by registration of multiple range images," Image Vis. Comput., vol. 10, no. 3, pp. 145­155, Apr. 1992. [18] S. Rusinkiewicz and M. Levoy, "Efficient variants of the ICP algorithm," in Proc. 3rd Int. Conf. 3-D Digit. Imag. Model., 2001, pp. 145­152. [19] A. Censi, L. Iocchi, and G. Grisetti, "Scan matching in the hough domain," in Proc. IEEE ICRA, 2005, pp. 2739­2744. [20] E. B. Olson, "Real-time correlative scan matching," in Proc. IEEE ICRA, 2009, pp. 4387­4393. [21] T. Rofer, "Using histogram correlation to create consistent laser scan maps," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2002, vol. 1, pp. 625­630. [22] M. Bosse and J. Roberts, "Histogram matching and global initialization for laser-only SLAM in large unstructured environments," in Proc. IEEE Int. Conf. Robot. Autom., 2007, pp. 4820­4826. [23] F. Pomerleau, F. Colas, R. Siegwart, and S. Magnenat, "Comparing ICP variants on real-world data sets," Autonom. Robots, vol. 34, no. 3, pp. 133­ 148, Apr. 2013. [24] H.-S. Tan and J. Huang, "DGPS-based vehicle-to-vehicle cooperative collision warning: Engineering feasibility viewpoints," IEEE Trans. Intell. Transp. Syst., vol. 7, no. 4, pp. 415­428, Dec. 2006. [25] S. Wender and K. Dietmayer, "Extending onboard sensor information by wireless communication," in Proc. IEEE Intell. Veh. Symp., Jun. 2007, pp. 535­540. [26] Z. J. Chong et al., "Autonomy for mobility on demand," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., Oct. 2012, pp. 4235­4236. [27] B. Steux, C. Laurgeau, L. Salesse, and D. Wautier, "Fade: A vehicle detection and tracking system featuring monocular color vision and radar data fusion," in Proc. IEEE Intell. Veh. Symp., 2002, vol. 2, pp. 632­639. [28] R. Labayrade, C. Royere, D. Gruyer, and D. Aubert, "Cooperative fusion for multi-obstacles detection with use of stereovision and laser scanner," Autonom. Robots, vol. 19, no. 2, pp. 117­140, Sep. 2005. [29] D. Gruyer, A. Cord, and R. Belaroussi, "Vehicle detection and tracking by collaborative fusion between laser scanner and camera," in Proc. IEEE/RSJ Int. Conf. IROS, 2013, pp. 5207­5214. [30] O. Ludwig, C. Premebida, U. Nunes, and R. Araújo, "Evaluation of boosting-SVM and SRM-SVM cascade classifiers in laser and visionbased pedestrian detection," in Proc. 14th Int. IEEE ITSC, 2011, pp. 1574­1579. [31] Q. Baig, O. Aycard, T. D. Vu, and T. Fraichard, "Fusion between laser and stereo vision data for moving objects tracking in intersection like scenario," in Proc. IEEE IV , 2011, pp. 362­367.

KIM et al.: MULTIVEHICLE COOPERATIVE DRIVING USING COOPERATIVE PERCEPTION

679

[32] H. Li and F. Nashashibi, "Multi-vehicle cooperative perception and augmented reality for driver assistance: A possibility to see through front vehicle," in Proc. 14th Int. IEEE ITSC, 2011, pp. 242­247. [33] C. Olaverri-Monreal, P. Gomes, R. Fernandes, F. Vieira, and M. Ferreira, "The see-through system: A VANET-enabled assistant for overtaking maneuvers," in Proc. IEEE Intell. Veh. Symp., 2010, pp. 123­128. [34] P. Gomes, C. Olaverri-Monreal, and M. Ferreira, "Making vehicles transparent through V2V video streaming," IEEE Trans. Intell. Transp. Syst., vol. 13, no. 2, pp. 930­938, Jun. 2012. [35] A. Vinel, E. Belyaev, K. Egiazarian, and Y. Koucheryavy, "An overtaking assistance system based on joint beaconing and real-time video transmission," IEEE Trans. Veh. Technol., vol. 61, no. 5, pp. 2319­2329, Jun. 2012. [36] E. Belyaev, A. Vinel, K. Egiazarian, and Y. Koucheryavy, "Power control in see-through overtaking assistance system," IEEE Commun. Lett., vol. 17, no. 3, pp. 612­615, Mar. 2013. [37] A. Girard, J. de Sousa, J. Misener, and J. Hedrick, "A control architecture for integrated cooperative cruise control and collision warning systems," in Proc. IEEE Conf. Decis. Control, Dec. 2001, vol. 2, pp. 1491­1496. [38] S. Kato, S. Tsugawa, K. Tokuda, T. Matsui, and H. Fujii, "Vehicle control algorithms for cooperative driving with automated vehicles and intervehicle communications," IEEE Trans. Intell. Transp. Syst., vol. 3, no. 3, pp. 155­161, Sep. 2002. [39] S. Tsugawa, S. Kato, T. Matsui, H. Naganawa, and H. Fujii, "An architecture for cooperative driving of automated vehicles," in Proc. IEEE Intell. Transp. Syst., 2000, pp. 422­427. [40] J. Kolodko and L. Vlacic, "Cooperative autonomous driving at the intelligent control systems laboratory," IEEE Intell. Syst., vol. 18, no. 4, pp. 8­ 11, Jul./Aug. 2003. [41] J. Baber, J. Kolodko, T. Noel, M. Parent, and L. Vlacic, "Cooperative autonomous driving: Intelligent vehicles sharing city roads," IEEE Robot. Autom. Mag., vol. 12, no. 1, pp. 44­49, Mar. 2005. [42] W. Liu, S.-W. Kim, Z. J. Chong, X. Shen, and M. H. Ang, Jr., "Motion planning using cooperative perception on urban road," in Proc. IEEE Int. Conf. Cybern. Intell. Syst., Robot., Autom. Mechantron., Nov. 2013, pp. 130­137. [43] B. Rebsamen et al., "Utilizing the infrastructure to assist autonomous vehicles in a mobility on demand context," in Proc. IEEE TENCON , Nov. 2012, pp. 1­5. [44] D. Jiang and L. Delgrossi, "IEEE 802.11p: Towards an international standard for wireless access in vehicular environments," in Proc. IEEE VTC Spring, 2008, pp. 2036­2040. [45] S. Biswas, R. Tatchikou, and F. Dion, "Vehicle-to-vehicle wireless communication protocols for enhancing highway traffic safety," IEEE Commun. Mag., vol. 44, no. 1, pp. 74­82, Jan. 2006. [46] A. Rauch, F. Klanner, and K. Dietmayer, "Analysis of V2X communication parameters for the development of a fusion architecture for cooperative perception systems," in Proc. IEEE Intell. Veh. Symp., 2011, pp. 685­690. [47] A. Von Arnim, M. Perrollaz, A. Bertrand, and J. Ehrlich, "Vehicle identification using near infrared vision and applications to cooperative perception," in Proc. IEEE Intell. Veh. Symp., 2007, pp. 290­295. [48] M. X. Punithan and S.-W. Seo, "King's graph-based neighbor-vehicle mapping framework," IEEE Trans. Intell. Transp. Syst., vol. 14, no. 3, pp. 1313­1330, Sep. 2013. [49] K. S. Arun, T. S. Huang, and S. D. Blostein, "Least-squares fitting of two 3-D point sets," IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-9, no. 5, pp. 698­700, Sep. 1987. [50] K. Konolige and K. Chou, "Markov localization using correlation," in Proc. Int. Joint Conf. Artif. Intell., 1999, pp. 1154­1159. [51] S. Thrun, W. Burgard, and D. Fox, "A real-time algorithm for mobile robot mapping with applications to multi-robot and 3D mapping," in Proc. IEEE ICRA, 2000, vol. 1, pp. 321­328. [52] B. Qin et al., "Curb-intersection feature based Monte Carlo localization on urban roads," in Proc. IEEE Int. Conf. Robot. Autom., 2012, pp. 2640­2646. [53] H. A. Mallot, H. H. Blthoff, J. Little, and S. Bohrer, "Inverse perspective mapping simplifies optical flow computation and obstacle detection," Biol. Cybern., vol. 64, no. 3, pp. 177­185, Jan. 1991. [54] M. Bertozzi and A. Broggi, "Gold: A parallel real-time stereo vision system for generic obstacle and lane detection," IEEE Trans. Image Process., vol. 7, no. 1, pp. 62­81, Jan. 1998. [55] B. Qin et al., "A general framework for road marking detection and analysis," in Proc. 16th Int. IEEE ITSC, 2013, pp. 619­625. [56] E. Dagan, O. Mano, G. P. Stein, and A. Shashua, "Forward collision warning with a single camera," in Proc. IEEE Intell. Veh. Symp., 2004, pp. 37­42.

[57] R. Parasuraman, P. Hancock, and O. Olofinboba, "Alarm effectiveness in driver-centered collision-warning systems," Ergonomics, vol. 40, no. 3, pp. 390­399, 1997. [58] M. Quigley et al., "ROS: An open-source robot operating system," in Proc. ICRA Workshop Open Source Softw., 2009, vol. 3, no. 2, pp. 1­6. [59] J. L. B. Claraco, Development of Scientific Applications with the Mobile Robot Programming Toolkit. The MRPT Reference book. Málaga, Spain: Mach. Perception Intell. Robot. Lab., Univ. Málaga, 2008. [60] Z. Chong et al., "Synthetic 2D LIDAR for precise vehicle localization in 3D urban environment," in Proc. IEEE ICRA, 2013, pp. 1554­1559. [61] S. Karaman, M. R. Walter, A. Perez, E. Frazzoli, and S. Teller, "Anytime motion planning using the RRT," in Proc. IEEE Int. Conf. Robot. Autom., May 2011, pp. 1478­1483. [62] "Attenuation by Atmospheric Gases," Geneva, Switzerland, P.676-8, Oct. 2009. [63] B. Yamauchi, "All-weather perception for man-portable robots using ultra-wideband radar," in Proc. IEEE ICRA, 2010, pp. 3610­3615.

Seong-Woo Kim (M'11) received the B.S. and M.S. degrees in electronics engineering from Korea University, Seoul, Korea, in 2005 and 2007, respectively, and the Ph.D. degree in electrical engineering and computer science from Seoul National University in 2011. He is currently a Postdoctoral Associate with the Singapore-MIT Alliance for Research and Technology. His research interests include online and offline optimization targeted for perception and control of intelligent and autonomous vehicles. Dr. Kim was a recipient of the Best Student Paper Award at the 11th IEEE International Symposium on Consumer Electronics and the Outstanding Student Paper Award at the First IEEE International Conference on Wireless Communication, Vehicular Technology, Information Theory, and Aerospace and Electronic Systems Technology.

Baoxing Qin received the B.S. degree in mechanical engineering from Shanghai Jiao Tong University, Shanghai, China, in 2010. He is currently working toward the Ph.D. degree in the Department of Mechanical Engineering, National University of Singapore, Singapore. His research interests include autonomous vehicle localization, object recognition, mapping and environment understanding.

Zhuang Jie Chong received the B.S degree in mechanical engineering (with first-class honors) from Nanyang Technological University, Singapore, in June 2010. He is currently working toward the Ph.D. degree in the Department of Mechanical Engineering, National University of Singapore. His research interests include autonomous vehicle, localization, and mapping.

Xiaotong Shen received the B.S. degree in mechanical engineering from Harbin Institute of Technology, Harbin, China, in 2012. He is currently working toward the Ph.D. degree in the Department of Mechanical Engineering, National University of Singapore, Singapore. His research interests include cooperative perception.

680

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 2, APRIL 2015

Wei Liu received the B.S. degree in mechanical engineering from Nanjing University of Aeronautics and Astronautics, Nanjing, China, in 2011, and the M.S. degree in mechatronics from National University of Singapore, Singapore, in 2012, respectively. He is currently working toward the Ph.D. degree in the Department of Mechanical Engineering, National University of Singapore, Singapore. His research interests include motion planning under uncertainties.

Marcelo H. Ang, Jr. received the B.S. degrees (cum laude) in mechanical engineering and industrial management engineering from the De La Salle University, Manila, Philippines, in 1981, the M.Sc. degree in mechanical engineering from the University of Hawaii at Manoa, Honolulu, Hawaii, in 1985, and the M.Sc. and Ph.D. degrees in electrical engineering from the University of Rochester, Rochester, NY, USA, in 1986 and 1988, respectively. His work experience includes heading the Technical Training Division of Intel's Assembly and Test Facility in the Philippines, research positions at the East West Center in Hawaii and at the Massachusetts Institute of Technology, and a faculty position as an Assistant Professor of electrical engineering with the University of Rochester, NY. In 1989, he joined the Department of Mechanical Engineering, National University of Singapore, where he is currently an Associate Professor. He also holds a joint appointment with the Division of Engineering and Technology Management as Deputy Head. In addition to academic and research activities, he is actively involved in the Singapore Robotic Games as its founding Chairman. He also chairs the Steering Committee for the World Robot Olympiad (2008­2009) and the World Skills Singapore Competition (2005, 2007, and 2010). His research interests span the areas of robotics, mechatronics, and applications of intelligent systems methodologies. He teaches both at the graduate and undergraduate levels in the following areas: robotics, creativity and innovation, applied electronics and instrumentation, advanced computing, product design and realization, and special topics in mechatronics. He is also active in consulting work in these areas.

Emilio Frazzoli (SM'07) He received the Laurea degree in aerospace engineering from the University of Rome, "Sapienza", Italy, in 1994, and the Ph.D. degree from the Department of Aeronautics and Astronautics of the Massachusetts Institute of Technology (MIT), Cambridge, MA, USA, in 2001. He is a Professor of aeronautics and astronautics with the Laboratory for Information and Decision Systems, and the Operations Research Center with the MIT. Before returning to MIT in 2006, he held faculty positions with the University of Illinois, Urbana-Champaign, and with the University of California, Los Angeles. He is currently the Director of the Transportation@MIT initiative, and the Lead Principal Investigator of the Future Urban Mobility IRG of the Singapore-MIT Alliance for Research and Technology (SMART). He was the recipient of a NSF CAREER award in 2002. He is an Associate Fellow of the American Institute of Aeronautics and Astronautics. His current research interests focus primarily on autonomous vehicles, mobile robotics, and transportation systems, and in general lie in the area of planning and control for mobile cyberphysical systems.

Daniela Rus (F'10) received the Ph.D. degree in computer science from Cornell University, Ithaca, NY, USA, in 1992. She is currently the Andrew (1956) and Erna Viterbi Professor with the Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, where she serves as a Director of the Computer Science, and Artificial Intelligence Laboratory. Her research interests include distributed robotics and mobile computing, and her application focus includes transportation, security, environmental modeling and monitoring, underwater exploration, and agriculture. Dr. Rus was the recipient of the National Science Foundation Career Award. She is a Class of 2002 MacArthur Fellow and a fellow of the Association for the Advancement of Artificial Intelligence (AAAI).

Simpler non-parametric methods provide as good or better results to multiple-instance learning.
Ragav Venkatesan, Parag Shridhar Chandakkar and Baoxin Li Arizona State University, Tempe, AZ, USA
ragav.venkatesan@asu.edu, pchandak@asu.edu, baoxin.li@asu.edu

Abstract
Multiple-instance learning (MIL) is a unique learning problem in which training data labels are available only for collections of objects (called bags) instead of individual objects (called instances). A plethora of approaches have been developed to solve this problem in the past years. Popular methods include the diverse density, MILIS and DD-SVM. While having been widely used, these methods, particularly those in computer vision have attempted fairly sophisticated solutions to solve certain unique and particular configurations of the MIL space. In this paper, we analyze the MIL feature space using modified versions of traditional non-parametric techniques like the Parzen window and k-nearest-neighbour, and develop a learning approach employing distances to k-nearest neighbours of a point in the feature space. We show that these methods work as well, if not better than most recently published methods on benchmark datasets. We compare and contrast our analysis with the well-established diversedensity approach and its variants in recent literature, using benchmark datasets including the Musk, Andrews' and Corel datasets, along with a diabetic retinopathy pathology diagnosis dataset. Experimental results demonstrate that, while enjoying an intuitive interpretation and supporting fast learning, these method have the potential of delivering improved performance even for complex data arising from real-world applications.

Figure 1. DR image classification as a MIL problem.

1. Introduction
Multiple-instance learning (MIL) is a setting where labels are provided only for a collection of instances called bags. There are two types of instances: negative instances, which are found in either negative bags or positive bags, and positive instances, which are found only in positive bags. While a positive bag must contain at least one inherently positive instance, a negative bag must not contain any positive instances. In MIL, labels are not available at the in-

stance level. It is interesting to note however that the labelspace is the same for both at the bag level and at the instance level. One may attempt to learn instance-level labels during the training stage, thus reducing the problem to an instancelevel supervised classification. Alternatively, one may also localize and prototype the positive instances in the feature space and rely on the proximity to these prototypes for subsequent classification. MIL is an ideal set-up for many computer vision tasks and examples of its application include object tracking [4], image categorization [9] [26] [28] [12], scene categorization [20] and content-based image retrieval [36]. In particular, MIL can be an especially suitable model for medical image-based pathology classification and lesion detectionlocalization, where an image is labeled pathological just because of one or a few lesions localized to small portions of the image. Medical images collected in a clinical setting may readily have an image-level label (either normal or various levels of pathology) while lacking the exact location of the lesion(s). Figure 1 illustrates such an example: colour fundus images of eyes affected with different pathologies of diabetic retinopathy (DR). It is easy to notice that, although majority of the image looks normal, a small retinal landmark is enough to alter the label of the image from normal to pathological. In a MIL formulation for this problem, each image can be considered a bag and patches of images can be considered instances. Over the years, many methods have been proposed to solve the MIL problem [10] [29] [8] [2]. The most fun-

2605

P2 P1

P3 P4
Figure 2. An illustrative feature space for multiple-instance setting. The 'x' in red represents all instances from positive bags and the 'o' in blue represents all instances from negative bags.

damental one is the diverse density approach [19], which has been built upon by many variants [35] [24] [9]. Diverse density is in its basic sense, a function so defined over the feature space that it is large at any point in the feature space that is close to instances from positive bags while being far away from instances from negative bags and vice-versa. The various local maximas in this function are positive instance prototypes and any instance that is closer to these prototypes are labeled inherently positive instances. Other types of methods also exist in this setting [5] [3] [27] [31]. MIL has many different variants and perspective to its definition and indeed most MIL solutions are application centric [1]. This can be easily seen from table 1. Earlier methods perform as good or better in the MUSK dataset than the ones published recently although the recent methods perform better on more complex tasks but for certain exceptions. In this course of research while many particular and complicated solutions are sought after, MIL has never been sufficiently analyzed using traditional non-parametric learning methods. Despite the recent advances, MIL remains a challenging task as the feature space may be arbitrarily complex, the ratio of positive to negative instances can be arbitrarily low in a positive bag, and (by definition) no labeling information is directly available for positive instances. To illustrate these factors, we simulate a typical MIL feature space as depicted in figure 2. Each instance belonging to a particular cluster is independently drawn from a normal distribution that defines the said cluster. While positive bags can draw a subset of random cardinality of instances from negative distributions, negative bags cannot draw any data from positive distributions. Every positive bag must have at least one instance sampled from a positive distribution

(marked in green ellipses P 1 through P 4). The centroids of these clusters would be the ideal positive instance prototypes that a MIL algorithm should identify. With the help of this illustration, it is not difficult to imagine that, one or few noisy negative instances coming close to a true positive instance prototype could lower the diverse density drastically and thus lead to a dramatic decrease in performance. Herein lies a core argument to the MIL definition - the strictness of positive neighbourhood. We show that DD-based algorithms are not tolerant even to a single negative instance in an arbitrary positive instance neighbourhood. Such strict assumptions are not suitable for real-world (medical imaging) data wherein the feature space can be noisy. In this paper, we propose modifications to traditional non-parametric methods adapting them to MIL. We demonstrate their effectiveness against DD taking into consideration the complex arrangements of a typical MIL feature space. In particular, the formulation aims at easing the dramatic impact of noisy negative instances on instanceprototyping in DD-based approaches. The formulation draws intuition from k -nearest-neighbour classification and thus leads readily to an efficient learning algorithm. It employs an aggregated and weighted distance measure computed from any point to its neighbouring instances labeled according to their respective parent bags, conforming to MIL requirement. Analysis with simulated data and experiments with real data in comparison to existing state-ofthe-art approaches suggest that the proposed method, while enjoying simplicity in formulation and learning, has the potential of delivering superior performance for challenging benchmark datasets. The remainder of the paper is organized as follows. Section 2 cites related works, while Section 3 describes the proposed method. Section 4 presents the experimental setup and discusses results on the various evaluation datasets. Section 5 provides concluding remarks.

2. Related Works
MIL was first introduced for the problem of drug activity prediction [10], where axis-parallel hyper-rectangles (APR) were used to design three variants of enclosure algorithms. The APR algorithms tried to surround at least one positive instance from each positive bag while eliminating any negative instances inside it. Any test bag was classified positive as long as it had at least one instance within the APR. Conversely a bag was classified as negative when it had no instance represented within the APR. The first density-based formulation of MIL was diverse density (DD) [19]. DD is not a conventional density but is rather defined as the intersection of the positive bags against the intersection of the negative bags. It is a measure that is high at any point on the feature space x if x is closer to positive instances and is farther away from negative instances.

2606

The local maxima of DD would yield a potential concept for the positive instances. Several local maxima can yield several prototypes of positive instances that can be far apart in the feature space. Some of these prototypes can be separated by other negative instances. The concept point of a diverse density in a MIL feature space was defined as, arg max
x i + P r ( x = t| B i ) i - P r ( x = t| B i ).

(1)

These local maxima were termed as instance prototypes. A noisy-or model was used to intuitively maximize the DD in Equation 1. This was further developed to assume more complicated and disjoint concepts in EM-DD and further developed by other methods including DD-SVM and Accio [35] [9] [24]. The major drawback of the diverse density arises in a situation where the distribution of negative instances is noisy. In other terms, if one instance prototype has a negative distribution closer to the prototype than the others, then its diverse density is largely lower than that of the others, as DD unfairly favours the distribution of positive instances that is farther away from negative instances than those that are relatively closer. This makes it hard to define that particular prototype in such situations. Even the presence of one noisy negative instance near the potential instance prototype can lower the DD drastically as we show in the later sections. In figure 2 the prototype P 4 was the twenty second largest local maxima in the DD of the feature space. If there were a bag that contained only one positive instance near P 4 but was still close enough to the negative instances, chances are that this bag will be misclassified as negative. DD defined in such a formulation provides a density-like function that is fickle and is easily affected by introducing even just one negative sample closer to the positive prototype. The maximization procedure for DD is started from initial guesses. An idea was put forward by Chen and Wang that the maximization should start from every instance in every positive bag (or at least a large sample of positive bags) so that unique local maxima in DD can be identified [9]. A plethora of methods still use this DD formulation [9] [8] [24] [35] [18]. The decision boundary of a DD system is a hyper-ellipsoid in the feature space. A kernel based maximum-margin approach would construct hyperplanar decision boundaries characterizing complex decision surfaces. The first formulation of a support vector machine (SVM) for MIL was proposed in 2002 [2]. They devised an instance-level classifier mi-SVM and a bag-level classifier MI -SVM. In a way, MI -SVM maximized the margin between the most positive instances and the least negative instances in positive and negative bags respectively. The MI SVM framework is now modified and re-christened as latent-SVM which plays a central role in the deformable-part models based object recognition algorithms [12]. MILIS

provided a similar SVM-based approach with a feedback loop to select instances that provided a higher training stage confidence [13]. This was an idea adapted from a previously existing related idea, MILES [8]. The first distance-based non-parametric, lazy learning approach to MIL was taken by citation-k-NN [29]. Interbag distances were found using a minimal Hausdorff distance. A k-nearest neighbour approach was used along with this distance to classify a new bag or to retrieve closer bags. This did not always work in a MIL setting as k-NN uses a majority voting scheme. If a positive bag contains fewer number of inherently positive instances than inherently negative instances, majority of its neighbours are going to be negative and the algorithm was confused by the false-positives it reported. Therefore the concept of citers was introduced. If k-NN refereed its neighbours, then its neighbours are cited by citers. Citers are the backward propagated references, in the sense that they refer back the considered instance. Though it was a generalized approach, citation k-NN did not work as well when positive instances were clustered and such clusters were separated by negative instances, in which case the citers and references did not always compliment each other. This problem does not apply to all nearest neighbour based approaches. Nearest neighbour approaches should be used properly and their smart usage was discussed in [6]. A novel concept of bag to class (B2C) distance learning was adopted for the use of k-NN. A complimentary idea was utilized in a MIL set-up by learning class to bag (C2B) distances by combining all training bags of a particular class to form a super-bag [26] [31]. A similar instance specific distance learning approach was used in [27]. On further study, this was reformulated as a l2,1 minimax problem and was solved with some effort [28]. A similar idea was implemented to group faces in an image by considering inter bag or bag to bag (B2B) distances in [15]. A related bag to bag approach is used to quantify super-bags in [3]. Most of the MIL algorithms presented above assume that the bags are independent. Though it is a reasonable assumption in a computer vision context, it might not be a general idea. Zhang et al., explored the MIL idea for structured data [34]. A data-dependent mixture model approach was developed in [30]. Another approach designed specifically for special data space is the fast bundle algorithm for MIL [5]. One important assumption in the early understanding of MIL is that every positive bag must contain at least one positive instance. Chen et al. felt this was too restrictive and developed a feature mapping using instance selection that projects a MIL problem into a much simpler supervised learning problem using an instance similarity measure [8]. This counter-assumption was also used in a histopathology cancer image learning system using a multiple clustered instance learning approach [33]. Although in a MIL for-

2607

mulation bag level classification is sufficient and instance level classification though clever, is not required, many algorithms attempt to identify positive instances. A SVM was used to minimize the hinge loss (modeled as slack variables) to identify positive instances in [32]. The above methods cater to certain particular configurations of the MIL space and are suitable for particular domains.

3. The proposed approach
Consider figure 2. Though not universal, this figure illustrates a typical MIL feature space. The instances arising from regions P 1 to P 4 are potentially inherently positive instances as they are farther away from negative instances while being closer to other positive instances. The instances from positive bags in other regions, along with negative instances are in reality, negative instances as they are close in proximity to negative instances from negative  bags.  X (1) Y (1)  X (2) Y (2)   (3)   Y (3)  Suppose we have labeled data D =  X   .  . .  . . . 
( i) th

Figure 3. Parsing the MIL feature space with a Parzen window technique. It can be seen that this follows the properties of a MIL density-like.

X ( n) Y ( n) where X is the i bag in the dataset and Y (i)  {0, 1} is its label. Internally, each bag X (i) contains mi (often is a constant m by design, particularly in image classification ( i) ( i) ( i) contexts) instances such that X (i) = {x1 , x2 , . . . xmi }. Consider a small region R of volume V in this feature space. The estimate for the density of instances from positive bags + is given by (|k V|)/n , where k + is the set of instances from positive bags in the region R and |k + | its cardinality, and n is the number of instances in all of the feature space. Similarly the estimate for the density of negative instances is - given by (|k V|)/n , where k - is the set of instances from negative bags in the region R, |k - | is the number of negative instances in the region R. + - Putting them together, (|k V|)/n - (|k V|)/n is a measure that, will be high if the number of positives exceed the number of negatives in that region, will be low if the number of negatives exceed the number of positives in that region, and will be 0 if the number of positives equal the number of negatives within that region. Alternatively, if one considers a (rectangular) Parzen window,  ( u) = 1, 0, |uj |  h where, j = 1, 2, ...d, otherwise (2)

+ - where, x is any location on the feature space and ki and ki are instances from positive and negative bags within that region respectively. Such a parsing of the MIL feature space of figure 2 is shown in figure 3. The properties of the function fP arzen (x) hold similar to that of DD and can be easily observed in figure 3. The choice of the size of the region (analogous to the selection of the variance for the Gaussian in the DD formulation) and the Parzen window functions are in line with that of a traditional Parzen window: if the size becomes too large, the measure will not have sufficient resolution. Picking a proper region-size would be a practical difficulty.

the aforementioned measure can also be formulated as, 1 fparzen (x) = n
+ |k n |

i=1

+ 1 1 x - ki ( )- V h n

- |k n |

i=1

- 1 x - ki ( ) V h (3)

Instead of considering a region R of fixed size, let's limit to a fixed number of neighbours k . In this set-up, we start with a region of zero volume from x and grow two regions, one for positive instances and one for negative instances, until we just enclose for each of the regions, k points respectively. This enables us to have different sized regions for positive and negative estimates respectively. While it appears to be a simple k-NN approach to density estimation, we emphasize that we are not using the nearest-neighbour voting rules. In fact, a direct application of nearest-neighbour voting technique will not work on a MIL space as was pointed out by Wang et. al, but the idea of nearest neighbour can still be modified and used to suit the MIL needs [29]. The vote contributions of positive and negative neighbours enclosed by the two regions are their respective kernelized distances to the point x, instead of a uniform majority vote. This aggregated vote can be formu-

2608

This is an indicator function that classifies the bag 1 if it has at least a instances classified as positive and 0 other-wise. Typically in most MIL settings a = 1, although this need not be the case generally. The aim of this non-parametric empirical risk minimization formulation is to minimize the training error, ^ ( b) =
Figure 4. A region of a typical 2D MIL feature space and its parse using the k-NN measure. Red represents positive and blue represents negative.

1 n

n

1{b(X (i) ) = Y (i) }, (X, Y )  D.
i=1

(7)

^ ^ that best minimizes ( by estimating T b) as, ^ ^ = arg min ( b) T
T

(8)

lated as,
|k - | |k + | - (||x - ki ||) - i=1 i=1 + (||x - ki ||)

fkN N (x) =

such that, |k + | = |k - | = k.

(4)

Once the threshold is learnt, classification is performed directly by using the bag-level classifier in equation 6 with the learnt threshold. Note that in MIL, it is not required, although possible in this case, to label each instance in the bag. The labeling of instances can be as follows: y (x) = h(x)|T =T ^ (9)

where, (.) is a monotonically increasing sub-modular function, k is the number of neighbours considered, and k + and k - are now the set of k instances from positive and negative bags that are the nearest to x respectively. (.) is used as a way to scale distances when the feature space is arbitrarily large. It can be considered as normalization. For all our experiments, we typically use (x) = x. The advantage of fixing the number of neighbours is that in a region where there are no points or very few number of points, we will get a block of uniform measure and in a region where there is a high density of points, we will get a smoothly varying measure. Such a measure is shown in figure 4. The impact of the number of neighbours k is similar to that of the size of the region R in the Parzen window idea. If k is too small, the measure is going to give information about a very small local region and is thereby unreliable. If k is too large , the impact of proximity is going to be averaged out.

This process is equivalent to maximizing the equation 4 (or 3) for all points of feature space and considering the local maximas as instance prototypes, as was described by Chen et. al, for the DD formulation [9]. This now enables comparison to prototyping-based methods. Such a formulation can now be re-written as,
|k - | |k + | - (||x - ki ||) - i=1 i=1 + (||x - ki ||) ,

x ^ = arg max
x

such that,|k + | = |k - | = k.

(10)

Learning
Learning under this formulation is a straight forward threshold learning and this is done by maximizing the validation accuracy. An instance-level classifier using this measure can be constructed as, h(x) = 1{fkN N (x)  T } (5)

This is an indicator function that outputs 1 if the measure is above a threshold T and 0 if the measure is below the threshold T . We can use this instance-level classifier to construct a bag-level classifier.
m

b ( X ) = 1{
i=1

h(xi )  a}x1 , x2 , . . . , xm  X.

(6)

where x ^ is a prototype positive instance. One advantage of using equation 10 is that once the prototypes are found, we neither need the entire dataset anymore nor do we need to calculate distances to all the points in the dataset. The prototypes easily divide the feature space into probabilistic Voronoi tessellations such as in figure 4. We could also estimate a radius around every prototype to isolate hyperspherical regions that are positive. We solve this optimization problem by using an idea similar to the one used in [9]. We start a local gradient ascent from every instance from every positive bag in the training dataset and find a local maxima. Since such maximas can only ever end in a high density region of true positive instances from positive bags and since we start each gradient ascent from every instance in every positive bag, each ascent is computationally tractable in small number of iterations. Indeed, often few well-chosen instances from positive bags make this convergence faster and such techniques can be found for maximizing the DD in various papers previously surveyed in section 2. Similar techniques can be

2609

applied here as well. All the local maximas are sorted (after non-maximal suppression) and top N are considered as instance prototypes. It is to be noted that for the dataset shown in figure 2, while the top 5 maximas were enough to find all four prototypes for our approach, it takes top 24 maximas for DD to find the four prototypes. The k for k -NN is picked here by a typical elbow method. Once local maximas (instance prototypes) are found we can again maximize a validation accuracy jointly for all instance prototypes to find a threshold of classification for each prototype in terms of the distance to the prototype, hence creating a hyper-spherical decision regions around each prototype. Thus the decision boundaries of this method creates a tessellation of the feature space. The tessellation is a superposition of hyper-spherical regions around a positive prototype with varying radii.

Methods DD [19] EM-DD [35] citation (k)-NN [29] mi-SVM [2] MI-SVM [2] DD-SVM [9] MILES [8] MIforest [31] MILIS [13] ISD [27] ALP-SVM [3] MIC-Bundle [5] Ensemble [18] Proposed

MUSK 1 88.9% 84.8% 92.4% 87.4% 77.9% 85.8% 86.3% 85% 88.6% 85.3% 87.9% 84% 89.22% 92.4%

MUSK 2 82.5% 84.9% 86.3% 83.6% 84.3% 91.3% 87.7% 82% 91.1% 79.0% 86.6% 85.2% 85.04% 86.4%

4. Experiments and Results
In this section we provide details of our experiments and the results from those experiments. We evaluated our method using three standard MIL datasets: the musk dataset, Andrew's datasets, the Corel datasets (both 1k and 2k), and our own dataset: the DR dataset. For all the results shown on all the datasets, we used the most common implementation methodologies, including data splits, cross validations and average over runs that were found in literature. This enabled us to compare against results that were published in the same. When results were not available or when the protocol doesn't match, we evaluated the results using the codes from CMU MIL toolbox1 . In case of MILES, the results were obtained by using the author's original code2 .

Table 1. Performance of various MIL algorithms on the musk dataset.

MUSK datasets are uni-concept datasets. For instance, in MUSK 1, among a total of 476 unique instances each with feature values ranging from -348 degrees to 336 degrees, there are only 633 unique feature values. In such a heavily quantized feature space that is 166 dimensional, detecting one potential instance prototype is easier for density based algorithms.

4.2. Andrew's datasets
Andrews et. al, in their mi-SVM paper proposed the use of three classification datasets, elephant, fox and tiger, for the use of evaluating multiple-instance learning [2]. These are now popular benchmark datasets in the MIL literature. We also test our algorithm on these datasets using the same specifications mentioned on the said article. Each dataset has 200 images with 100 positive and 100 negative images. The number of instances in each category are 1391, 1320 and 1220 respectively with varying number of instances per bag. Each instance is a 230 dimensional feature vector. We train on a 2/3 random split of the data and test on the remaining 1/3 of the unseen data. The results are maximized over 15 runs of validation and are shown in table 2. Our result while being the best in the Elephant and Fox classes is almost as good as the best in the Tiger class. It is to be noted that we are significantly higher in the Fox class which is widely considered to be a notoriously noisy dataset for MIL. This is a strong indicator of our method's adaptability.

4.1. Musk dataset
An accepted benchmarking dataset in the MIL literature is the musk dataset. The musk dataset is well-described in [10]. Musk dataset is a benchmark feature space used to predict drug activity. It contains two sub-datasets: MUSK1 and MUSK2. MUSK 1 contains 92 molecules with 47 musk and 45 non-musk molecules. MUSK 2 contains 102 molecules with 39 musk and 63 non-musk molecules. Each bag contains variable number of instances with 166 dimensional features and binary labels. We use the standard implementation specifications that is used in the original APR paper and other published literature: ten-fold crossvalidation over the entire dataset, since its easier to compare against a plethora of methods [10]. Table 1 compares the performance of various algorithms against the proposed method. It can be seen that the proposed method is best in MUSK 1 and among the high performing methods in MUSK 2.
MIL toolbox: http://www.cs.cmu.edu/~juny/MILL homepage: http://www.cs.olemiss.edu/~ychen/ MILES.html
2 MILES 1 CMU

4.3. Corel dataset
Corel is another well known, image categorization dataset for MIL benchmarking. The Corel-2k dataset consists of 2000 images. There are 20 classes and each class consists of 100 images. The Corel-1k dataset is a subset

2610

Methods citation k-NN [29] mi-SVM [2] MILES [8] MIforest [31] ISD [27] ALP-SVM [3] MIC-Bundle [5] Ensemble [18] Proposed

Elephant 79.2% 79.7% 70% 84% 77.9% 84% 80.5% 84.25% 86%

Fox 62.5% 62.9% 56% 64% 63% 69% 58.3% 63.05% 73.94%

Tiger 82.6% 79% 62% 82% 85.3% 86% 79.11% 79.30% 85.7%

Methods DD [19] EM-DD [35] citation k-NN [29] mi-SVM [2] MILES [8] Proposed

Accuracy 61.29% 73.5% 78.7% 70.32% 71% 81.3%

Table 4. Performance of various MIL algorithms on DR dataset.

Table 2. Performance of various MIL algorithms on Andrew's dataset.

Methods mi-SVM [2] MI-SVM [2] MILES [8] DD-SVM [9] MILIS [13] Proposed

Corel-1k 76.4% 75.1% 82.3% 81.5% 83.8% 87.3%

Corel-2k 53.7% 55.1% 68.7% 67.5% 70.1% 71.9%

high-quality colour fundus image database of 425 images comprising 160 normal images, and 265 affected images to test our algorithm on. This dataset was constructed from publicly available databases including DiabRetDB0 [11], DiabRetDB1 [17], STARE [21] and Messidor3 and has been used in some existing studies [7] [25]. The balance of the database is more towards the positive bags and this makes it more challenging for a MIL algorithm. The results were all evaluated using a 2/3 - 1/3 train-test split. Prototyping DR instances

Table 3. Performance of various MIL algorithms on Corel dataset.

of this dataset with the first 10 difficult categories. Table 3 shows the performance of the proposed approach in the corel dataset. It is to be noted that we are producing the best results in the Corel dataset. Training-testing data is again a 2/3 - 1/3 split.

4.4. A DR dataset
As was briefly discussed in section 1, DR image classification is an application especially suitable for MIL. In practice, the difficulty in this problem arises from the fact that the physical and observable difference between a normal eye and a pathological eye can be very small, localizing to regions with slightly different characteristics. This can be seen in figure 1. A variety of classification and retrieval schemes have been tried on DR images. Structural Analysis of the Retina (STARE) is one of the earliest attempts to solve the DR conundrum [21] [14]. STARE performs automated diagnosis and comparison of images to search for images similar in content. Recently other learning approaches were developed to identify relevant patterns using local relevance scores [23]. Application of MIL approaches to DR is gaining interest in recent years [22]. In this study, we consider the auto colour correlogram (AuoCC) as a colour feature, which is well-studied in the medical imaging literature [16]. A modified and quantized 64-bin AutoCC feature is extracted for each instance in an image [25]. We neglect the black regions and sample 48 non-overlapping instances from every image. We use a

In the prototyping sense, each prototype of positive instances should roughly correspond to one type of lesion. As we use colour features this is easily possible. We estimated a total of about 35 different types of lesion prototypes using our algorithm and verified it with EM-DD's prototypes. EM-DD had its maximum accuracy at about 40 prototypes. It is reasonable to assume from this information that there are in the range of, 35-40 different positive prototypes, each of which in the feature space might correspond to a unique lesion type or character. In this feature space, the negative instances are of three types: normal skin, nerves and the optical disk. This is a reasonably noisy datasets and often has only one or two instances among 48 instances that are positive in a positive bag. Though the distribution of the optic disc might be noisy, and the number of true positive instances are very low, the proposed algorithm has the potential to adjust to it. Table 4 shows the results of the proposed approach on the DR dataset, where the proposed method stands best.

4.5. Sensitivity to labeling error
Although not an implicit feature of the proposal, we perform the experiments to demonstrate the proposed method's sensitivity to labeling error, exactly similar to the one described in [8]. We deliberately flip the labels for a range of percentages of labels randomly on our training split and test the trained model on the original labels in the testing split. The split was 2/3 - 1/3. The accuracies of the proposed method on various datasets are shown in figure 5. After
3 Kindly provided to us by the messidor program partners. Visit http: //messidor.crihan.fr

2611

Figure 5. Accuracy vs Percentage of labels flipped for the proposed method. Flatter curve is good.

simple, yet novel usage of non-parametric learning philosophy to the MIL problem. In particular, we analyzed the MIL feature space using a k- NN philosophy and proposed a new formulation based on distances to k-nearest neighbours. The new formulation was compared and contrasted with the widely used DD formulation. The proposed approach was tested on the musk datasets, Andrews dataset and the corel datasets, and was found to be effective. The algorithm was used to solve the DR image classification problem and was found to be the best among other algorithms. We therefore conclude that a non-parametric learning philosophy to MIL not only makes intuitive sense but can also be a powerful tool for most general cases.

Acknowledgement
The work was supported in part by a grant (#W911NF1410371) from the Army Research Office (ARO). Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the ARO.

References
[1] J. Amores. Multiple instance classification: Review, taxonomy and comparative study. Artificial Intelligence, 201:81­ 105, 2013. [2] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support vector machines for multiple-instance learning. Advances in neural information processing systems, 15:561­568, 2002. [3] B. Anti´ c and B. Ommer. Robust multiple-instance learning with superbags. In Computer Vision­ACCV 2012, pages 242­255. Springer, 2013. [4] B. Babenko, M. Yang, and S. Belongie. Robust object tracking with online multiple instance learning. IEEE PAMI, 33(8):1619­1632, 2011. [5] C. Bergeron, G. Moore, J. Zaretzki, C. M. Breneman, and K. P. Bennett. Fast bundle algorithm for multiple-instance learning. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(6):1068­1079, 2012. [6] O. Boiman, E. Shechtman, and M. Irani. In defense of nearest-neighbor based image classification. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1­8. IEEE, 2008. [7] P. S. Chandakkar, R. Venkatesan, and B. Li. Retrieving clinically relevant diabetic retinopathy images using a multiclass multiple-instance framework. In SPIE Medical Imaging, pages 86700Q­86700Q. International Society for Optics and Photonics, 2013. [8] Y. Chen, J. Bi, and J. Z. Wang. Miles: Multipleinstance learning via embedded instance selection. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 28(12):1931­1947, 2006. [9] Y. Chen and J. Wang. Image categorization by learning and reasoning with regions. The Journal of Machine Learning Research, 5:913­939, 2004.

Figure 6. Drop in accuracy at various noise levels for proposed and MILES on the DR dataset. The lower the value the better.

about 20% of labels are corrupted, the proposed method still loses only about 5% accuracy and only when about one-third of the labels are corrupted, the proposed method loses about 10% accuracy. The average drop in accuracy for both the proposed method and MILES are compared in figure 6. It is clear that MILES and the proposed algorithm follow the exact same trend. This trend is clearly indicative that the proposed method is as good as MILES and is often times better, when it comes to sensitivity to labeling noise. It is noteworthy that MILES is considered the stateof-the-art benchmark for sensitivity to labeling error out of all MIL methods published and that was one of its core contributions.

5. Conclusion
In this paper, we postulate whether lazy learning ideas can be carried over from traditional non-parametric methods for supervised learning to a MIL setup. We proposed a

2612

[10] T. Dietterich, R. Lathrop, and T. Lozano-P´ erez. Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence, 89(1):31­71, 1997. [11] T. et al. Diabretdb0: Evaluation database and methodology for diabetic retinopathy algorithms. In Technical Report. [12] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained partbased models. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9):1627­1645, 2010. [13] Z. Fu, A. Robles-Kelly, and J. Zhou. Milis: Multiple instance learning with instance selection. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(5):958­977, 2011. [14] M. Goldbaum, N. Katz, S. Chaudhuri, and M. Nelson. Image understanding for automated retinal diagnosis. In Proceedings of the Annual Symposium on Computer Application in Medical Care, page 756. American Medical Informatics Association, 1989. [15] M. Guillaumin, J. Verbeek, and C. Schmid. Multiple instance metric learning from automatically labeled bags of faces. Computer Vision­ECCV, pages 634­647, 2010. [16] J. Huang, S. Kumar, M. Mitra, W. Zhu, and R. Zabih. Image indexing using color correlograms. In Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, pages 762­768. IEEE, 1997. [17] T. Kauppi, V. Kalesnykiene, J. Kamarainen, L. Lensu, I. Sorri, A. Raninen, R. Voutilainen, H. Uusitalo, H. K¨ alvi¨ ainen, and J. Pietil¨ a. Diaretdb1 diabetic retinopathy database and evaluation protocol. Proc. Medical Image Understanding and Analysis (MIUA), pages 61­65, 2007. [18] Y. Li, D. M. Tax, R. P. Duin, and M. Loog. Multiple-instance learning as a classifier combining problem. Pattern Recognition, 46(3):865­874, 2013. [19] O. Maron and T. Lozano-P´ erez. A framework for multipleinstance learning. NIPS, pages 570­576, 1998. [20] O. Maron and A. Ratan. Multiple-instance learning for natural scene classification. In IEEE ICML, volume 15, pages 341­349, 1998. [21] B. McCormick and M. Goldbaum. Stare= structured analysis of the retina: Image processing of tv fundus image. In del USA-Japan Workshop on Image Processing, Jet Propulsion Laboratory, Pasadena, CA, 1975. [22] G. Quellec, M. Lamard, M. Abr` amoff, E. Decenci` ere, B. Lay, A. Erginay, B. Cochener, and G. Cazuguel. A multiple-instance learning framework for diabetic retinopathy screening. Medical Image Analysis, 2012. [23] G. Quellec, M. Lamard, B. Cochener, C. Roux, G. Cazuguel, E. Decenciere, B. Lay, and P. Massin. A general framework for detecting diabetic retinopathy lesions in eye fundus images. In Computer-Based Medical Systems (CBMS), 2012 25th International Symposium on, pages 1­6. IEEE, 2012. [24] R. Rahmani, S. Goldman, H. Zhang, S. Cholleti, and J. Fritts. Localized content-based image retrieval. IEEE transactions on pattern analysis and machine intelligence, 30(11):1902, 2008. [25] R. Venkatesan, P. Chandakkar, B. Li, and H. K. Li. Classification of diabetic retinopathy images using multi-class

[26] [27] [28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

multiple-instance learning based on color correlogram features. In Engineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE, pages 1462­1465. IEEE, 2012. H. Wang, H. Huang, F. Kamangar, F. Nie, and C. Ding. Maximum margin multi-instance learning. NIPS, 2011. H. Wang, F. Nie, and H. Huang. Learning instance specific distance for multi-instance classification. In AAAI, 2011. H. Wang, F. Nie, and H. Huang. Robust and discriminative distance for multi-instance learning. In IEEE CVPR, pages 2919­2924. IEEE, 2012. J. Wang and J. Zucker. Solving the multiple-instance problem: A lazy learning approach. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 1119­1126. Morgan Kaufmann Publishers Inc., 2000. Q. Wang, L. Si, and D. Zhang. A discriminative datadependent mixture-model approach for multiple instance learning in image classification,. In In Proceedings of the 12th European Conference on Computer Vision (ECCV-12),, 2012. Z. Wang, S. Gao, and L.-T. Chia. Learning class-to-image distance via large margin and l1-norm regularization. In Computer Vision ECCV 2012, pages 230­244. 2012. D. Wu, J. Bi, and K. Boyer. A min-max framework of cascaded classifier with multiple instance learning for computer aided diagnosis. In IEEE CVPR, pages 1359­1366. IEEE, 2009. Y. Xu, J. Zhu, E. Chang, and Z. Tu. Multiple clustered instance learning for histopathology cancer image segmentation, classification and clustering. CVPR, IEEE, 2012. D. Zhang, Y. Liu, L. Si, J. Zhang, and R. Lawrence. Multiple instance learning on structred data. In Twenty-Fifth Annual Conference on Neural Information Processing Systems (NIPS), 2011. Q. Zhang and S. Goldman. Em-dd: An improved multipleinstance learning technique. Advances in neural information processing systems, 14:1073­1080, 2001. Q. Zhang, S. Goldman, W. Yu, and J. Fritts. Content-based image retrieval using multiple-instance learning. In Machine Learning-International Worskshop-Then Conference-, pages 682­689, 2002.

2613

IEEE SIGNAL PROCESSING LETTERS, VOL. 23, NO. 4, APRIL 2016

449

Compressive Sensing Reconstruction of Correlated Images Using Joint Regularization
Kan Chang, Pak Lun Kevin Ding, and Baoxin Li, Senior Member, IEEE
Abstract--This letter proposes a novel compressive sensing reconstruction method for correlated images by using joint regularization, where a compensation-based adaptive total variation (CATV) regularization and a multi-image nonlocal low-rank (MNLR) regularization are included. In CATV, local weights are assigned to the residual values in the gradient domain so as to constrain the regularization strength at each pixel. In MNLR, the search of similar patches goes across different images so that both self-similarity and inter-image similarity are explored. Afterward, an efficient algorithm is proposed to solve the joint formulation, using a Split-Bregman-based technique. The effectiveness of the proposed approach is demonstrated with experiments on both multiview images and video sequences. Index Terms--Compressive sensing, motion estimation/disparity estimation (ME/DE), nonlocal low-rank regularization (NLR), total variation.

prior knowledge for regularizing the solution to the following minimization problem: ^ i = argmin (ui ) u
ui

s.t. yi = i ui

(2)

I. I NTRODUCTION

T

HIS LETTER focuses on the compressive sensing (CS) reconstruction of a set of correlated images, each of which is independently acquired by the CS technique [1], [2]. The correlated images could be multiview images which represent a scene from different view points, or a series of video frames which are taken at different time points. More specifically, the CS measurement of the original ith image is acquired by y i = i u i (1)

where ui  RN stands for the original ith image, yi  RM is the measurement of the ith image, and i  RM ×N is the measurement matrix. Usually, M << N , and we call M/N the subrate of CS. To reconstruct the underlying images from such an underdetermined system, one common way is to employ image
Manuscript received December 13, 2015; revised February 03, 2016; accepted February 04, 2016. Date of publication February 11, 2016; date of current version March 03, 2016. This work was supported by the Natural Science Foundation of China under Grants 61401108 and 61261023, and the Natural Science Foundation of Guangxi under Grant 2013GXNSFBA019272. The work of B. Li was supported by the Natural Science Foundation under Grants 1135616 and 0845469. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Deanna Needell. K. Chang is with the School of Computer and Electronic Information, the Guangxi Key Laboratory of Multimedia Communications and Network Technology (Cultivating Base), and the Key Laboratory of Multimedia Communications and Information Processing, Guangxi University, Nanning 530004, China (e-mail: changkan0@gmail.com). P. L. Kevin Ding and B. Li are with the Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287 USA (e-mail: kevinding@asu.edu; baoxin.li@asu.edu). Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/LSP.2016.2527680

where  is the regularization term denoting image prior. Different types of intra-image-based regularization have been investigated, including total variation (TV) minimization [3], nonlocal low-rank regularization (NLR) [4], nonlocal meansbased regularization [5], autoregressive model [6], dictionary learning-based sparse representation [7], etc. Besides intra-image prior information, inter-imagestructured sparsity also needs to be explored for correlated images. The most direct way to do this is to require a joint sparsity of the whole image set, such as [8]­[11]. However, such methods are sensitive to motion or disparity. Improvement may be obtained by using the neighboring images to help reconstruct the current one, such as [12]­[20]. In [21], correlation between a pair of images was directly estimated in the compressed domain so as to reduce the computational complexity. The main contributions of this letter are listed as follows. First, we propose a compensation-based adaptive TV regularization approach, where the reliability of each compensated pixel is considered. Second, we extend the existing NLR from single-image pattern to multiple-image pattern. Note that different from the similar model in [22], we additionally use optical flow (OF) fields to guide the central points of search windows. Finally, we jointly incorporate these two types of regularization into a minimization problem and design an optimization algorithm for CS reconstruction of correlated images. Experiments show that our proposed algorithm is capable of achieving significantly better reconstruction than several state-of-the-art methods. To facilitate evaluation and further exploration of the proposed algorithm, we will publish the source code on the third author's webpage.1 II. P ROPOSED J OINT R EGULARIZATION A. Compensation-Based Adaptive TV Regularization To explore inter-image correlation, we can utilize disparity estimation/disparity compensation (DE/DC) for multiview images or motion estimation/motion compensation (ME/MC) for video sequences. As have been proved in [17]­[20], requiring small prediction error in the gradient domain is able to get satisfactory results. In [17], the compensation-based TV (CTV) regularization term was written as CTV (ui ) = D(ui - si )
1 [Online].

1

(3)

Available: http://www.public.asu.edu/~bli24/

1070-9908 © 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

450

IEEE SIGNAL PROCESSING LETTERS, VOL. 23, NO. 4, APRIL 2016

T T where D = [DT h , Dv ] with Dh , Dv denoting the horizontal and vertical finite difference operators, respectively, and

si =

1 (Fi-1 ui-1 + Bi+1 ui+1 ) 2

(4)

with Fi-1 and Bi+1 standing for the forward and backward compensation operators by using the optical flow fields of the (i - 1)th and (i + 1)th images, respectively. We only used the closest images to build the compensated results because: 1) usually a further distance between two images leads to a less accurate compensated image and 2) the computational complexity of estimating the OF fields is high. Unfortunately, such an si is not always reliable on images with fine details, multiview images with large disparity, or video sequences with complex motion. Therefore, simply minimizing the regularization term (3) may sometimes deteriorate the quality of reconstructed images. Considering the reliability at each pixel in si , we propose to add local weights to the residual values in the gradient domain, leading to a compensation-based adaptive TV (CATV) regularization CATV (ui ) = Wi (D(ui - si ))
1

Fig. 1. Example of intra-image and inter-image search. OF fields are used to guide the central points of search windows in adjacent images. The sizes of search windows in different images are the same.

Xj . It is reasonable to expect the formed matrix Xj has a lowrank property (in practice, Xj is only approximately low rank due to noises and artifacts [4]). Hence for the ith image, NLR is calculated as NLR (ui ) =
j

1 ^ Rj ui - L j 2

2 F

+ Rank(Lj )

(10)

(5)

is the where Wi denotes the vector of local weights, and Hadamard product. If an unreliable predicted value occurs at a pixel position in si , a small weight should be assigned to it. To appropriately build Wi , a good spatial information indicator is needed. Here, the second derivative-based indicator called difference curvature [23] is utilized. It is a pixel-based indicator which can effectively discriminate edges from flat regions or noises. For the mth pixel, it is defined as Cm = ||e | - |e || e = e = e2 x exx + 2ex ey exy + 2 e2 x + ey e2 y eyy (6) (7) (8)

^ j stands for extracting similar patches for the j th exemwhere R ^ j ui ;  is a tradeoff parameter; The plar patch, i.e., Xj = R newly introduced low-rank matrix Lj is close to Xj . For correlated images, just exploring nonlocal low-rank property inside an image is not enough. Therefore, we propose to extend (10) to a new multi-image nonlocal low-rank (MNLR) regularization, which is written as
MNLR (U) =
i j

1 ~ Ri,j U - Li,j 2

2 F

+ Rank(Li,j )

(11)

2 e2 y exx + 2ex ey exy + ex eyy 2 e2 x + ey

where ex and ey represent the first-order gradients of the pixel along x (horizontal) and y (vertical) directions, respectively; exx , eyy , and exy are the second-order gradients of the pixel. With Cm , the mth local weight in Wi is computed as wm = 1 1 + Cm (9)

where  is the contrast factor. It should be noted that our Wi is designed for residual in the gradient domain, which is different from other works such as [24] and [25] (i.e., Cm is computed on (ui - si ) instead of ui ). In practice, since the original images are not available, the previously reconstructed images are needed to calculate Wi . B. Multi-Image Nonlocal Low-Rank Regularization NLR [4] is an efficient tool for describing single image characteristics. To apply it, a target image is first divided into overlapped patches with size Sp × Sp . As a similarity metric, the l2 differences between the j th exemplar patch and the candidate patches within a search window are computed. After that, Np similar patches are found and grouped into a matrix

T T T where U = [uT 1 , u2 , . . . , un ] , and n is the number of corre^ j in (10), for the j th exemplar lated images. Different from R ~ i,j extracts its similar patches patch in the ith image, operator R from the (i - 1)th, ith, and (i + 1)th images. To find similar patches in three neighboring images, both intra-image and inter-image searches are needed. Fig. 1 illustrates how the search works. Recall that we have obtained OF fields when calculating si in (4). Thus, here they can be reused to guide the central points of search windows in the (i - 1)th and (i + 1)th images. More specifically, given a central point of a window in the ith image, we locate its motion/disparity vectors in the OF fields and use these vectors to find where this point is in the adjacent images. This procedure helps to find the most similar patches, especially in cases where large disparity or motion occurs among neighboring images. Note that higher quality of reconstruction can be achieved if the search goes across more images. However, doing so would cause heavier computational burden. Through experiments, we found that searching in three continuous images has achieved significant improvement over single-image NLR [4]. In addition, we only have the OF fields for each pair of adjacent images, which means the guidance for the central points of search windows is not available in further images.

III. O PTIMIZATION A LGORITHM FOR CS R ECONSTRUCTION U SING J OINT R EGULARIZATION Since CATV and MNLR impose different prior knowledge within correlated images, jointly considering them can get satisfactory results. By doing so, the minimization problem for reconstructing a set of correlated images becomes

CHANG et al.: CS RECONSTRUCTION OF CORRELATED IMAGES USING JOINT REGULARIZATION

451

TABLE I M EAN PSNR S ( D B) C OMPARISON FOR M ULTIVIEW I MAGES R ECONSTRUCTION

Since rank-minimization problem is NP-hard, here, we have replaced Rank(Li,j ) in (11) with L(Li,j , ), which is a log det(·) surrogate function [26] and L(Li,j , ) = log det Li,j LT i,j
1/ 2

+ I

(13)

TABLE II M EAN PSNR S ( D B) C OMPARISON FOR V IDEO S EQUENCES R ECONSTRUCTION

where  is a small constant and I denotes the identity matrix. We use log det(·) here because it can better approximate rank than the widely used nuclear norm [4]. To efficiently solve problem (12), we introduce a new ~ (U - CU). Then, our joint variable d and let d = D regularization-driven Split-Bregman iteration [27] can be written as   Lk+1 = argmin{Li,j } i j (L(Li,j , )   i,j    k 2 ~  +1  2 Ri,j U - Li,j F )     Uk+1 = argmin Y - HU 2  2 U     +1 2  ~ + i j ( Ri,j U - Lk F) i,j (14) k k  ~ (U - CU) - b 2  + d - D  2     k+1 ~  dk+1 = argmind  - bk 2  2 2 d - D ( I - C) U     ~  + W d 1      k+1 k k+1 k+1 k+1
b ~ (U =b +D - CU )-d

where  is a tradeoff parameter. When considering the "L" subproblem, we treat every Li,j separately, and the solution can be written as [4]
+1 k T Lk i,j = Q( -  diag(g ))+ V

(15)

where QVT is the singular value decomposition (SVD) of ~ i,j Uk , and thin SVD is applied in our implementation. g k = R l k k 1/(l + ) ,  l denotes the lth singular value of Lk i,j , and (x)+ = max(x, 0). When solving the "U" subproblem, we use CUk to approximate CUk+1 and get the closed-form solution as follows:  -1
Uk+1 = HT H +  ~T ~ ~T ~ R i,j Ri,j +  D D
i j




i j k+1  ~T . R i,j Li,j

^ , {L ^ i,j }} = argmin 1 Y - HU {U U,{Li,j } 2 ~ + W +
i j

2 2

k ~ T (dk + DCU ~  D - bk ) + HT Y + 

(16)
2 F

~ (U - CU) D

1

1 ~ Ri,j U - Li,j 2

+ L(Li,j , )

(12)

To compute (16), conjugate gradient (CG) method is used. When dealing with the "d" subproblem, closed-form solution by the shrinkage formula [28] is given as ~ (I - C)Uk+1 + bk , W ~ / ). dk+1 = shrink(D With a vector x and a threshold Ts , we have shrink(x, Ts ) = max(|x| - Ts , 0) sgn(x). (18) (17)

T T where ,  , and  are tradeoff parameters, Y = [y1 , y2 , T T T T ~ = [W , W , . . . , . . . , yn ] , H = diag(1 , 2 , . . . , n ), W 1 2 T T ~ ] D = diag(D, D, . . . , D), and Wn   0 B2 0 ··· 0 0 B3 /2 ··· 0  F1 /2  .  . . . .   .. .. .. . C= . . . .  0 0 Bn /2 · · · Fn-2 /2 0 0 ··· 0 Fn-1

Note that the max operator here is implemented for each spatial index independently. Our algorithm, named joint regularization-based compressive sensing reconstruction (JR-CSR), is summarized as

452

IEEE SIGNAL PROCESSING LETTERS, VOL. 23, NO. 4, APRIL 2016

Fig. 2. Visual quality comparison for the ninth frame in City (subrate = 0.20). From left to right: DC-TV, DC-JTV, NLR-CS, JM-RCI, JR-CSR. TABLE III AVERAGE CPU T IME ( S ) FOR R ECONSTRUCTING O NE F RAME IN Foreman

Algorithm 1. JR-CSR ~ 0 , H, Y, ,  , ,  Input: U Outer loop for t = 0, 1, . . . , T ~ i,j }. ~ t to update C and {R Use U ~ = [1, 1, . . . 1]T Else Update W ~ by (9) If t  T0 W 0 ~ ( I - C) U ~ t , b0 = 0, U0 = U ~t Set d = D Inner loop for k = 0, 1, . . . , K +1 Update each Lk i,j by (15). Update Uk+1 by using CG method to solve (16). Update dk+1 via (17). ~ (Uk+1 - CUk+1 ) - dk+1 bk+1 = bk + D t+1 k+1 ~ Set U =U If Uk+1 - Uk 2 / Uk+1 2 < 10-4 Break End for End for ^ =U ~ T +1 return U ~ t and Uk denote the results in the outer loop Algorithm 1. U and the results in the inner loop, respectively. To get an accu~ , and {R ~ i,j } are updated several times in the rate result, C, W ~ is fixed. This outer loop. Note that in the first T0 iterations, W was found empirically to be able to improve the convergence while leading to a better result. The inner loop will stop if the relative change of U is smaller than a predefined threshold, or the maximum number of iterations is reached. IV. E XPERIMENTAL R ESULTS This section evaluates the performance of JR-CSR. All experiments were performed in MATLAB 2013b on a Lenovo computer with Intel(R) Core(TM) i7-4790 processor, 8.00G memory. Structurally random matrices (SRM) [29] were used as i , and we had the measurement yi = i ui . To generate ~ 0 , TVAL3 [30] software2 was utilized for each image. To U construct C, OF implementation3 of [31] was used. In this OF implementation, successive over-relaxation (SOR) was applied to solve the linear system resulting from the energy functional of [32], where the assumptions of brightness constancy, gradient constancy, and piecewise smooth flow field were combined. T , K , and T0 were set to 15, 25, and 5, respectively.  in (9) was set to 0.8, patch size Sp × Sp for MNLR was 6 × 6, and the number of similar patches Np was 45. ,  , , and  were tuned according to each subrate. The test datasets included four multiview image sets [half size Monopoly (MP), Tsukuba (TK), Venus and Art] from
2 [Online]. 3 [Online].

the middlebury multiview database4 and four video sequences [352 × 288 Foreman (FM), Football (FB), City, and Bus]. We tested the first five views of each multiview image set and the first 20 frames of each video sequence. Only grayscale images were considered. Four state-of-the-art algorithms were compared, including NLR-CS [4], JM-RCI [17], DC-JTV [20], and DC-TV [19]. For fair comparison, TVAL3 [30] was applied to obtain the initial results for all competing algorithms. The average peak signal-to-noise ratio (PSNR) results of each multiview image set and each video sequence are given in Tables I and II, respectively. One can see that JR-CSR beats all the benchmark methods in all cases. For example, in Table I at a subrate of 0.10, JR-CSR gets 3.05 dB PSNR improvement over the second best method, i.e., NLR-CS. For visual quality comparison, please see Fig. 2. The average running times for reconstructing one frame in Foreman are listed in Table III. We can find that JR-CSR is the slowest algorithm. The main computational burdens are introduced by iteratively updating {Li,j }, U, and C. For each Li,j , 2 Np r), where r is the rank the complexity of thin SVD is O(Sp ~ of Ri,j U. To update U and C, CG and SOR are used to solve the related linear systems, respectively. Given a linear system Ax = z, assume that Nn is the number of nonzero entries in matrix A, and Nc is the condition number of A. To get the  vector x, the complexity of one iteration of CG is O(Nn Nc ), while O(Nn ) is required for one iteration of SOR. To speed up JR-CSR, parallelization techniques may be the best choice. Other solutions, such as updating C in JR-CSR only once, removing either CATV or MNLR from (12), etc., would lead to different levels of quality loss. V. C ONCLUSION In this letter, we proposed two types of regularization, including CATV and MNLR, for CS reconstruction of correlated image sets. After incorporating the two regularization terms into the minimization problem, we designed an optimization algorithm called JR-CSR. Through experiments, we found that JR-CSR is able to deliver the best performance among all the tested methods, which demonstrates the effectiveness of the proposed joint regularization.
4 [Online].

Available: http://www.caam.rice.edu/~optimization/L1/TVAL3/ Available: http://people.csail.mit.edu/celiu/OpticalFlow/

Available: http://vision.middlebury.edu/stereo/data

CHANG et al.: CS RECONSTRUCTION OF CORRELATED IMAGES USING JOINT REGULARIZATION

453

R EFERENCES
[1] D. L. Donoho, "Compressed sensing," IEEE Trans. Inf. Theory, vol. 52, no. 4, pp. 1289­1306, Apr. 2006. [2] E. J. Candès, J. Romberg, and T. Tao, "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information," IEEE Trans. Inf. Theory, vol. 52, no. 2, pp. 489­509, Feb. 2006. [3] L. Rudin, S. Osher, and E. Fatemi, "Nonlinear total variation based noise removal algorithms," Phys. D: Nonlinear Phenom., vol. 60, no. 1, pp. 259­268, 1992. [4] W. Dong, G. Shi, X. Li, Y. Ma, and F. Huang, "Comressive sensing via nonlocal low-rank regularization," IEEE Trans. Image Process., vol. 23, no. 8, pp. 3618­3612, Aug. 2014. [5] J. Zhang, S. Liu, D. Zhao, R. Xiong, and S. Ma, "Improved total variation based image compressive sensing recovery by nonlocal regularization," in Proc. IEEE Int. Symp. Circuits Syst. (ISCAS), 2013, pp. 2836­2839. [6] X. Wu, W. Dong, X. Zhang, and G. Shi, "Model-assisted adaptive recovery of compressed sensing with imaging applications," IEEE Trans. Image Process., vol. 21, no. 2, pp. 451­458, Feb. 2012. [7] W. Dong, G. Shi, X. Li, L. Zhang, and X. Wu, "Image reconstruction with locally adaptive sparsity and nonlocal robust regularization," Signal Process.: Image Commun., vol. 27, pp. 1109­1122, 2012. [8] J. Ma, G. Plonka, and M. Y. Hussaini, "Compressive video sampling with approximate message passing decoding," IEEE Trans. Circuits Syst. Video Technol., vol. 22, no. 9, pp. 1354­1364, Sep. 2012. [9] C. Li, H. Jiang, W. Paul, and Y. Zhang, "A new compressive video sensing framework for mobile broadcast," IEEE Trans. Broadcast., vol. 59, no. 1, pp. 197­205, Mar. 2013. [10] M. Hosseini and K. N. Plataniotis, "High-accuracy total variation with application to compressed video sensing," IEEE Trans. Image Process., vol. 23, no. 9, pp. 3869­3884, Sep. 2014. [11] P. Nagesh and B. Li, "A compressive sensing approach for expressioninvariant face recognition," in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR), 2009, pp. 1518­1525. [12] T. T. Do, Y. Chen, D. T. Nguyen, N. Nguyen, L. Gan, and T. D. Tran, "Distributed compressed video sensing," in Proc. Int. Conf. Image Process. (ICIP), 2009, pp. 1393­1396. [13] J. Nebot, Y. Ma, and T. Huang, "Distributed video coding using compressive sampling," in Proc. Picture Coding Symp. (PCS), 2009, pp. 1­4. [14] L. W. Kang and C. S. Lu, "Distributed compressive video sensing," in Proc. Int. Conf. Acoust. Speech Signal Process. (ICASSP), 2009, pp. 1169­1172. [15] V. Thirumalai and P. Frossard, "Distributed representation of geometrically correlated images with compressed linear measurements," IEEE Trans. Image Process., vol. 21, no. 7, pp. 3206­3218, Jul. 2012. [16] Y. Liu, M. Li, and A. Dimitris, "Motion-aware decoding of compressedsensed video," IEEE Trans. Circuits Syst. Video Technol., vol. 23, no. 3, pp. 438­444, Mar. 2013.

[17] K. Chang and B. Li, "Joint modeling and reconstruction of a compressively-sensed set of correlated images," J. Visual Commun. Image Represent., vol. 33, pp. 286­300, 2015. [18] K. Chang, T. Qin, W. Xu, and Z. Tang, "Reconstruction of multi-view compressed imaging using weighted total variation," Multimedia Syst., vol. 20, no. 4, pp. 363­378, 2014. [19] M. Trocan, E. W. Tramel, J. E. Fowler, and B. Pesquet, "Compressedsensing recovery of multiview image and video sequences using signal prediction," Multimedia Tools Appl., vol. 72, no. 1, pp. 95­121, 2014. [20] Y. Liu, C. Zhang, and J. Kim, "Disparity-compensated total-variation minimization for compressed-sensed multiview image reconstruction," in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP), 2015, pp. 1458­1462. [21] V. Thirumalai and P. Frossard, "Correlation estimation from compressed images," J. Visual Commun. Image Represent., vol. 24, no. 6, pp. 649­ 660, 2013. [22] H. Yoon, K. S. Kim, D. Kim, Y. Bresler, and J. C. Ye, "Motion adaptive patch-based low-rank approach for compressed sensing cardiac cine MRI," IEEE Trans. Med. Imag., vol. 33, no. 11, pp. 2069­2085, Nov. 2014. [23] Q. Chen, P. Montesinos, Q. Sun, P. Heng, and D. Xia, "Adaptive total variation denoising based on difference curvature," Image Vis. Comput., vol. 28, no. 3, pp. 298­306, 2010. [24] W. Dong, X. Yang, and G. Shi, "Compressive sensing via reweighted TV and nonlocal sparsity regularisation," Electron. Lett., vol. 49, no. 3, pp. 184­186, 2013. [25] E. J. Candès, M. B. Wakin, and S. P. Boyd, "Enhancing sparsity by reweighted l1 minimization," J. Fourier Anal. Appl., vol. 14, no. 5, pp. 877­905, 2008. [26] M. Fazel, H. Hindi, and S. P. Boyd, "Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance matrices," in Proc. Amer. Control Conf., 2003, pp. 2156­2162. [27] T. Goldstein and S. Osher, "The split bregman method for l1 regularized problems," SIAM J. Imag. Sci., vol. 2, no. 2, pp. 323­343, 2009. [28] E. T. Hale, W. Yin, and Y. Zhang, "Fixed-point continuation for l1minimization: Methodology and convergence," SIAM J. Optim., vol. 19, no. 3, pp. 1107­1130, 2008. [29] T. T. Do, L. Gan, N. H. Nguyen, and T. D. Tran, "Fast and efficient compressive sensing using structurally random matrices," IEEE Trans. Signal Process., vol. 60, no. 1, pp. 139­154, Jan. 2012. [30] C. Li, W. Yin, H. Jiang, and Y. Zhang, "An efficient augmented lagrangian method with applications to total variation minimization," Comput. Optim. Appl., vol. 56, no. 3, pp. 507­530, 2013. [31] C. Liu, "Beyond pixels: Exploring new representations and applications for motion analysis," Ph.D. dissertation, Department of Electrical Engineering and Computer Science, Massachusetts Inst. Technol., Cambridge, MA, USA, 2009. [32] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert, "High accuracy optical flow estimation based on a theory for warping," in Proc. Eur. Conf. Comput. Vis. (ECCV), 2004, pp. 25­36.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/306576719

WeaklyHierarchicalLassobasedLearningto RankinBestAnswerPrediction
ConferencePaper·August2016
DOI:10.1109/ASONAM.2016.7752250

CITATIONS

READS

0
2authors,including: QiongjieTian ArizonaStateUniversity
12PUBLICATIONS172CITATIONS
SEEPROFILE

32

AllcontentfollowingthispagewasuploadedbyQiongjieTianon25August2016.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

Weakly Hierarchical Lasso based Learning to Rank in Best Answer Prediction
Qiongjie Tian
Computer Science and Engineering Arizona State University Email: qiongjie.tian@asu.edu

Baoxin Li
Computer Science and Engineering Arizona State University Email: baoxin.li@asu.edu

Abstract--In community question and answering sites, pairs of questions and their high-quality answers (like best answers selected by askers) can be valuable knowledge available to others. However lots of questions receive multiple answers but askers do not label either one as the accepted or best one even when some replies answer their questions. To solve this problem, highquality answer prediction or best answer prediction has been one of important topics in social media. These user-generated answers often consist of multiple "views", each capturing different (albeit related) information (e.g., expertise of the asker, length of the answer, etc.). Such views interact with each other in complex manners that should carry a lot of information for distinguishing a potential best answer from others. Little existing work has exploited such interactions for better prediction. To explicitly model these information, we propose a new learningto-rank method, ranking support vector machine (RankSVM) with weakly hierarchical lasso in this paper. The evaluation of the approach was done using data from Stack Overflow. Experimental results demonstrate that the proposed approach has superior performance compared with approaches in state-ofthe-art.

limitations inherent to these existing techniques. First, a binary classifier is not natural to this research problem, which often involves multiple answers for one given question. It is possible for a trained classifier to declare many or even all answers are the best ones (if they happen to lead to feature vectors lying on the positive side of the decision boundary). Also it is counter-intuitive as a human user would normally compare all received answers and decide on a single best one. The binary classification does not model directly on the difference of multiple answers, compared with learning-to-rank techniques. Second, the interaction between features from different views may carry a lot of information for distinguishing a potential best answer from others, however current existing methods do not readily support incorporation of such interactions, which by itself is a challenging task. In anther setting, best answer prediction is modeled as one ranking problem, which is conceptually more intuitive. This kind of modeling results from the fact that the best answer to one question is defined/discovered relatively by comparing it with all the other given answers. A ranking-based setting may benefit even more from considering the latent interactions between features designed from different views of the CQA data. Unfortunately, similar to the binary-classification cases, the existing learning-to-rank techniques have not attempted to explicitly to model such interactions among different views of the data [4][5][6]. In this paper, we focus on how to incorporate the interaction structure of features into one existing algorithm framework to improve the performance of best answer prediction. Similar to [5][7], we adopt the learning-to-rank formulation for its natural match to the prediction problem. Considering the interaction structure (or the hierarchical structure of feature dimensions in our study) and the ranking framework, we propose a new learning-to-rank formulation based on weakly hierarchical lasso. The contributions of our work are summarized as follows: Firstly, we propose a new RankSVM model by constructing the weakly hierarchical structure between features from different views. Secondly, to solve the new formulation, we propose an efficient algorithm and evaluate via experiments its efficiency and effectiveness with comparisons with other existing methods.

I. I NTRODUCTION In the era of Internet and social media, community question and answering (CQA) sites, like Baidu Zhidao1 , Yahoo! Answers2 and StackOverflow3 , are seeing phenomenal growth. As one form of user-generate content, data from CQA sites are typically very noisy, which does not lead to ready usage either by humans or by computers. Consequently, how to extract useful information from the noisy CQA data to form valuable knowledge base has become an important research task [1]. One popular task on this regard is best answer prediction, on which our paper focuses. Given a question with multiple answers, one way to solve best answer prediction is to reformulate it into a binary classification problem which is whether, in a question-answer pair, the answer is the best one or not. There have been some research efforts in this setting like [2], [3]. In these efforts, features were extracted from different views of the data to generate a good representation for the question-answer pairs, and the final feature vector was formed by concatenating them together. As a result, each feature dimension carries some information of the CQA data. But there are a couple of
1 http://zhidao.baidu.com/ 2 https://answers.yahoo.com/ 3 http://stackoverflow.com/

IEEE/ACM ASONAM 2016, August 18-21, 2016, San Fran- 1 cisco, CA, USA 978-1-5090-2846-7/16/$31.00 c 2016 IEEE

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

II. R ELATED W ORK In this section, we review briefly related research on community question and answering, and discuss the difference between the reviewed work and our proposed method. A. Content Quality Analysis Compared with traditional on-line search, as one supplementary approach to solving our daily problems, CQA sites contain a lot of valuable knowledge. Thus, since the first CQA site was launched, finding high quality content from these sites has become important. For example some early work was done in [8] where Jiwoon Jeon et al. crawled data from Naver Q&A site and manually labeled each pair of questions and their corresponding answers as bad, medium, good. They proposed to use non-textual features to represent each question-answer pair and used kernel density estimation and the maximum entropy approach to model the problem of answer quality. To have a better representation of questions and answers on CQA sites, more sources of information were used to extract new features like interactions between questions and answers and users, as studied in [2], where Eugene Agichtein et al. proposed to use non-content information to model question and answer pairs on CQA sites including the interaction features. Then different classifiers like support vector machine, log-linear classifier and stochastic gradient boosted trees were applied to learn the prediction model, whose efficiency and effectiveness were evaluated using data from Yahoo! Answers. The importance of social information for predicting answer quality was studied in [3], where Chirag Shah et al. found the importance of user information by studying the quality labeled manually. Besides research on the answer quality, question quality is also studied. In [9], Baichuan Li et al. worked on the question quality prediction problem. They first studied what factors may affect question quality and then proposed a model termed Mutual Reinforcement-based Label Propagation to predict question quality. In [10], it was found that the voting scores of questions have a strong positive correlation with that of the corresponding answers and they proposed a set of coprediction algorithms to predict the voting scores of questions and answers. The above work focused on content quality prediction (question quality and answer quality), which is modeled as one classification problem. These existing efforts mainly focused on finding a better representation of the data by introducing various features to facilitate the prediction problem. B. Best Answer Prediction and Answer Ranking Pairs of questions and their best answers can be easily used to answer similar questions, as the research in [11] shows. With the fast growth of CQA sites, there are a lot of questions which have high quality answers but no best ones eventually marked. To this end, a lot of research efforts have been devoted to best answer prediction and answer ranking. In [12], Lada Adamic et al. analyzed Yahoo! Answers for best answer prediction. They used simple four-dimensional features and reported that the length of answers is the most

important factor of answer quality. The problem they are worked on is to predict whether a given answer is the best one of the given question. They did not consider interaction information like relationship between questions and answers and users. It is not natural to model best answer prediction as a classification problem since the best answer is relatively defined. Thus there have been a lot of efforts on modeling best answer prediction as a ranking problem. In [13], Mihar Surdeanu et al. proposed a ranking model for non-factoid questions and studied whether ranking algorithms can be used to rank answers for given questions. They also showed the importance of different features in the answer ranking problem. This work was further extended in [14]. Instead of simply applying learning to rank algorithms, some researchers worked on improving the performance by using piggybacking and ranking aggregation techniques. In [7], Felix Hieber et al. applied RankSVM algorithms to best answer prediction with piggybacking being used to improve the performance. In their work, interaction features were used, like the similarity between questions and answers. Piggybacking is used to for obtaining a better representation of the questions so that similarity between the questions and answers can help improve the ranking performance of RankSVM. One example work to use ranking aggregation is [15], where Arvind Agarwal et al. made a comparison between different learning to rank algorithms and proposed to use ranking aggregation techniques to improve them. But that work focused on the factoid question and answers instead of CQA. In contrast, our work employs hierarchical interactions in the feature space. There are also some efforts on studying the influence of different combinations of features on the prediction accuracy and also comparison across different CQA sites [16]. Pointwise ranking techniques were also used to rank answers to each question. In [4], Daniel Dalip et al. assumed that the voting scores to be the quality scores of answers. Then random forest was used to model the relationship between the scores and features. The final predicted rating scores were used to rank each questions. To evaluate the performance, normalized discounted cumulative gain at top k (NDCG@K) is used. However, there is noise in the rating scores as shown in [17], and thus in our work we do not use this assumption. The information between answers to each question may help capture the relative information for better prediction, as shown in [18], where Tian et al. proposed to extract features from the context information between answers to each question. There are many other efforts on finding/defining new features for best answer prediction. For example, temporal features are proposed in [5]. One common observation in the most of the existing work is that, when new features are derived, all of them are concatenated to one vector to be the final feature vector. For example, in [12], these features are used: reply length, thread length, the total number of best answers of one user, the total number of replies one user has. They can be denoted as x1 , x2 , x3 , x4 . Then the final feature vectors are the simple concatenation of these features which are (x1 , x2 , x3 , x4 ). In

2

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

our work, we focus on proposing a new model which can capture the feature interactions based on hierarchical lasso. III. P ROBLEM D ESCRIPTION AND F ORMULATION The research problem in this paper is formally defined as follows: given a question with all of its received answers, to predict which one is the best one. To select the best answer, one has to compare it with the others, so that the best answer is relatively defined. Thus instead of using the classification framework, we employ the learning-to-rank strategy. The basis of our proposed approach is RankSVM [6]. While existing work focuses on designing new features, we study this prediction problem from the following angle: modeling the interaction of features from different views of data beyond simple concatenation of them. To achieve this goal, we employ weakly hierarchical lasso [19] in constructing a new ranking model. Notations of this paper are described in the following. Denote a dataset with N questions as {qi , i  {1, · · · , N }}. For each question qi , it receives a group of answers which are {Ai,j , j  {1, · · · , Mi }} where Mi is the total number of answers to qi . The feature vector xi,j  R1×d is used to represent the j th answer to the ith question. Moreover, the k th dimension of one feature vector xi,j is defined as xi,j,k where k  {1, · · · , d}. xi,j is the simple concatenation of features extracted from different views of our problem, as done in the existing work. It is named as the main effect. Then for each xi,j , we compute the second-order interaction 2 which is denoted as zi,j  R1×d , which is called the second-order interaction term. The final feature vector by considering the main effect and the interaction term is denoted 2 as x ^(i, j ) = [xi,j , zi,j ]  R1×(d+d ) . The interaction term is defined as follows (see Eqn.1): zi,j = [zi,j , zi,j , · · · , zi,j ]
(m) zi,j (1) (2) (d)

The RankSVM formulation is given below (Eqn. 3):
wRd×1

min

s.t.

1 w 2 i,j1 ,j2 2+C 2 S1 (i, j1 )  S1 (i, j2 ) + 1 - i,j1 ,j2 , i,j1 ,j2  0, (i, j1 , j2 )

(3) (i, j1 , j2 )

where (i, j1 , j2 ) is one ranked QA pair in P and S (i, j ) is the quality score function of the j th answer to qi and defined in Eqn.4. S1 (i, j ) = xi,j w + w0 (4)

where w0  R. To improve the performance of RankSVM, our model involves the second-order interactions via constructing one weakly hierarchical structure in the feature space. The formulation of the new ranking model is shown in Eqn.5. Compared with the existing work, we model the latent interaction structure between features from different views of the data, instead of simple concatenation. The hierarchical structure of the feature space is constructed through the first group of constraints (a.k.a Q.,j 1  |wj |, j  {1, · · · , d}) in Eqn.5. min
wR , QRd×d
d×1

w

1

+

1 Q 2

1

+C
(i,j1 ,j2 )P

i,j1 ,j2

(5)

s.t.

Q.,j

1

 |wj |,

j  {1, · · · , d} (i, j1 , j2 )  P

i,j1 ,j2  0,

(i, j1 , j2 )  P

S (i, j1 ) > S (i, j2 ) + 1 - i,j1 ,j2 ,

where Q.,j is the j th column of Q, Q 1 = i j |Qi,j | and S (·, ·) is the ranking score for each answer to one question defined in Eqn.6. For example S (i, j ) is the ranking score for answer Ai,j to qi . 1 (6) S (i, j ) = xi,j w + zi,j vec(Q) + w0 2 where vec(Q) is the vectorized version of Q and zi,j is shown in Eqn.1 and w0  R. To help illustrating the proposed model, we depict the hierarchical structure based on one example shown in Figure 1, in which we only show three features: the length of the answer (Alen ), the number of URLs in the answer (Nurl ), the number of pictures used in the answer (Npic ). In this illustration, we can see that the upper layer contains all main effects (a.k.a xi,j ) while the second layer shows the interaction terms (a.k.a zi,j in Eqn.1) excluding the square values of themselves. When one term contributes to the objective function, no matter it belongs to main effects or interaction terms, its corresponding coefficient is set to be non-zero. For each interaction term, if it contributes to the objective function, then at least one of its corresponding main effects contributes to the objective function. Satisfying these hierarchical constraints, it is easy for us to conclude that the interaction terms contribute less than their corresponding main effects. Specifically, in this figure, if the coefficient of Alen · Nurl is non-zero, then the coefficient of Alen is non-zero but that of Nurl can be zero.

(1)

= [xi,j,m · xi,j,1 , xi,j,m · xi,j,2 , · · · , xi,j,m · xi,j,d ]

where i  {1, · · · , N }, j  {1, · · · , Mi } and m  {1, · · · , d}. In our work, instead of classification methods, learning-torank techniques are used to model the relativeness of the best answers. Each relatively ranked pair is represented as (qi , Ai,j1 , Ai,j2 ) where the quality of Ai,j1 is higher than that of Ai,j2 . For simplicity, we may use (i, j1 , j2 ) as the short version of (qi , Ai,j1 , Ai,j2 ) in the following equations. The set Pi contains all these pairs of answers to the question qi . Furthermore, the entire set of these relatively ranked pairs is denoted as P in Eqn.2. P =
i{1,··· ,N }

Pi

(2)

RankSVM, as one state-of-the-art pair-wise learning-to-rank algorithm used in best answer prediction [5][7], is used as the basic building block of our new ranking model.

3

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

From Eqn. 5, the weakly hierarchical lasso is involved via the first group of constraints (a.k.a Q.,j 1  |wj |, j  {1, · · · , d}).

where L(w, Q) is given in the following: L(w, Q) = Set  =
1 C,

1 max(0, 1 - (~ xm w + z ~m vec(Q)))2 (13) 2 m=1

|P |

the final model is obtain as given in Eqn.14  Q 1 2 j  {1, · · · , d}
1

min L(w, Q) +  · w
w,Q

+

s.t. Fig. 1: One illustration to show hierarchical structure in the feature space, where "·" represents the scalar multiplication. The first layer contains the main effect, while the second layer consists of the 2nd order of interaction. IV. S OLVING THE P ROPOSED M ODEL To develop a solution to our proposed model in Eqn. 5, we first reformulate the problem as follows. Consider this group of constraints (Eqn.7) in the proposed model in Eqn. 5. Si,j1 > Si,j2 + 1 - i,j1 ,j2 Together with Eqn.6, we have the following computation: Si,j1 > Si,j2 + 1 - i,j1 ,j2 (8) 1 Si,j1 = xi,j1 w + zi,j1 vec(Q) + w0 2 1 Si,j2 = xi,j2 w + zi,j2 vec(Q) + w0 2 If we assume the relatively ranked pair (qi , Ai,j1 , Ai,j2 ) is the mth element in the set P of Eqn.2, then Eqn.8 can be simplified and the following is obtained: 1 ~m ~m · vec(Q) > 1 -  (9) x ~m w + z 2 where x ~m , z ~m should satisfy the following constraints in Eqn.10. x ~m = xi,j1 - xi,j2 z ~m = zi,j1 - zi,j2 As a result, Eqn.5 is converted to the following: min
w,Q

Q.,j

1

 |wj |,

(14)

(7)

To this point, our objective function has been reformulated into the standard form as in the weakly hierarchical lasso problem defined in [19] and [20]. To solve Eqn. 14, the scheme in [20] can be applied since it can directly solve the weakly hierarchical lasso without adding more penalty compared with approach in [19]. Since the optimization process in [20] is based on a general iterative shrinkage and thresholding algorithm (GIST) in [21], before we use the method in [20], we need to prove that L(w, Q) in Eqn. 14 is continuously differentiable with Lipschitz continuous gradient. Before proceeding with the proof, we introduce following notations: x ^ = (~ x, z ~) w ^= w
1 2 vec(Q)

(15)

As a consequence, x ^  R1×(d+d·d) and w ^  R(d+d·d)×1 . L(w, Q) is converted from Eqn.13 as Eqn.16. ^ (w L ^) =
m{1,··· ,|P |}

max(0, 1 - x ^m · w ^ )2

(16)

(10)

^ (w To show L ^ ) is differentiable with Lipschitz continuous gradient, this requirement needs to be satisfied: there exists a positive constant  such that ^ ^ dL dL (w1 ) - (w2 ) dw ^ dw ^
2

  w1 - w2

2

(17)

w

1

+

1 Q 2

1

+C
m{1,··· ,|P |}

~m 

(11)

^ (w Let us first consider one additive component of L ^ ). The point-wise maximum function can be written as Eqn.18. l(w ^ ) = max(0, 1 - x ^m · w ^ )2 = 0 (1 - x ^m · w ^ )2 if 1 - x ^m · w ^<0 if 1 - x ^m · w ^0 (18)

1 ~m , m  {1, · · · , |P |} s.t. x ~m w + z ~m · vec(Q) > 1 -  2 Q.,j 1  |wj |, j  {1, · · · , d} ~ m  0, m  {1, · · · , |P |} where |P | is the size of the set P . Now we can reformulate Eqn.11 into Eqn.12: min
w,Q

w

1

+
1

s.t.

Q.,j

1 Q 1 + C · L(w, Q) 2  |wj |, j  {1, · · · , d}

(12)

It is easy to see that when w1 , w2  {w|1 - x ^ m · w < 0} and w1 , w2  {w|1 - x ^m · w  0}, Eqn.17 is satisfied. Now considering w1  {w|1 - x ^m · w < 0}, w2  {w|1 - x ^m · w > 0}, it is easy to see that the left part of Eqn.17 becomes (1 - x ^ · w2 )^ xm . Moreover, define w ^  as 1 - x ^m · w = 0 and

4

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM) this inequality is satisfied: w1 - w2  w - w2 . Now to obtain the constant  , the following induction is performed: (1 - x ^m · w2 )^ xm   w1 - w2 (1 - x ^m · w2 )^ xm   w1 - w2 (1 - x ^m · w2 ) x ^m    w - w2 (1 - x ^m · w2 ) x ^m 2   w - w2 x ^m (1 - x ^m · w2 ) x ^m 2   1-x ^m · w2   x ^m 2

Now it is feasible to apply the algorithm in [20] to solve Eqn.14 which is equivalent to solving this proximal operator problem of Eqn.22. (w(k+1) , Q(k+1) ) = arg min 1 1 w - v (k) 2 Q - U (k) 2 2+ 2 w,Q 2 2 1  + ( k ) ( w 1 + Q 1) 2 t Q.,j 1  |wj | j  {1, · · · , d} (22)

s.t.

where v (k) , U (k) are defined as follows: v (k) = w(k) - (19) U (k) = U (k) - 1 t(k) ·
w L(w (k)

, Q(k) ) , Q(k) )

(23) (24)

1 · t(k)

Q L(w

(k)

Similarly, it is easy to obtain that   x ^m 2 also satisfies the case where w2  {w|1 - x ^m · w < 0}, w1  {w|1 - x ^m · w > 0}. Thus, there exists a proper positive constant  so that l(w ^ ) meets the requirement Eqn. 17. In conclusion, l(w ^ ) is continuously differentiable with Lipschitz continuous gradient. With this result, we will further introduce and prove the following lemma, together with which we will able to show the desired property for L(w, Q) is satisfied. Lemma IV.1. For each function f (w)i , i  {1, · · · , N } which is continuously differentiable with Lipschitz continuous N gradient, their summation f (w) = i=1 fi (w) is continuously differentiable with Lipschitz continuous gradient. Proof. d d f (w1 ) - f (w2 ) dw dw N N d d fi (w1 ) - fi (w2 ) dw dw i=1 i=1
N

where t(k) > 0 which is the step size. Considering w, Q are products of their signs and also absolute values, Eqn.22 can be re-written into Eqn.25. (w(k+1) , Q(k+1) ) = arg min
w,Q

s.t.

1 1 w - v (k) 2 Q - U (k) 2 2+ 2 2 2  1 + ( k ) ( w 1 + Q 1) 2 t Q~ ~j j (25) .,j  w

where Q.,j = sign(Q.,j ) Q~ ~j . The .,j and wj = sign(wj ) w above equation can be solved in a closed form as proved in [20]. The pseudocode of our entire algorithm is shown in the following. which is summarized in Algorithm 1. Algorithm 1 The pseudo-code to solve our model
1: 2: 3: 4: 5: 6: 7: 8: 9:

=

=
i=1 N

(

d d fi (w1 ) - fi (w2 )) dw dw


i=1

d d fi (w1 ) - fi (w2 ) dw dw (20)

  w1 - w2

Denote that there exists positive constant i such that fi (w) satisfies Eqn.17 where i  {1, · · · , N }. Thus Eqn.20 is valid when  meets this requirement:  = max i
i

10: 11: 12: 13: 14: 15: 16:

(21)

INPUT: data matrix X and ranking information of all data OUTPUT: model parameters w and Q BEGIN: compute the set P based on Eqn.2. compute the data difference {x ~m , m  {1, · · · , |P |}} and {z ~m , m  {1, · · · , |P |}} as Eqn.10. provide initial values for w and Q. choose one t via BB Rule [22]. while w, Q satisfy the stop criteria do while tk does not satisfy the stop criteria do update v k according to Eqn.23. update U k according to Eqn.24. obtain new w(k+1) and Q(k+1) based on Eqn.25, which can be in the closed form as [20]. update the step size t(k) =   t(k) where  is the constant update ratio. end while k = k + 1; end while

^ (w Since max(0, 1 - x ^m · w ^ )2 satisfies Eqn. 17 and L ^) = 2 max(0 , 1 - x ^ · w ^ ) , according to Lemma IV.1, m m{1,··· ,|P |} ^ L(w ^ ) satisfies Eqn. 17, same as L(w, Q) defined in Eqn. 13. Thus, L(w, Q) is continuously differentiable with Lipschitz continuous gradient.

V. E XPERIMENTS In this section, we present experimental results on Stack Overflow to show the performance of our proposed model and the comparison with existing methods.

5

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)
number of users number of votes number of comments number of questions number of answers 4,232,639 62,357,544 44,557,809 9,365,722 15,632,696

A. Data Description Founded in 2008, StackOverflow is active and well maintained. On this site, users can post questions and everyone can provide answers even including the askers. For each question and each answer, users can comment on it. For one question or answer, users can vote up or down based on its quality except the user who posts it. For one comment, users can only vote up if they think the comment is useful, but cannot vote down. Same as one question or one answer, the one cannot vote up his or her own comments. For one question or one answer, it can receive up-votes and also down-votes. Then the number of up-votes minus the number of down-votes is the vote score. It is easy to see that the vote score are integers and can be negative. Each question can receive multiple answers and only the asker can decide which one can be marked as the accepted answer which we call the best answer. This choice is not permanent, which means the asker can change his or her mind at any time and mark another answer as the best answer. There is one fact we need to point out. One question may receive multiple correct answers but only one of them can be marked as the best answer. So the best answer has the relatively best quality instead of absolutely best one. This is the reason why we use the learning to rank techniques instead of the classification methods. For users, they can earn reputations if their posts (e.g. questions ,answers, and comments) obtain upvotes or answers are accepted or suggestions on editing others' posts are accepted. Otherwise, they lose reputations if their posts receive downvotes or are reported as spam or offensive. Figure 2 shows one sample of one question with its answers from StackOverflow. Till May 8, 2015, the statistics

TABLE I: The information of Stack Overflow till May 8, 2015.

Fig. 2: Illustration of one sample question from Stack Overflow. of this site are as in Table. I. B. Experiment Settings In our experiment, part of StackOverflow dataset is used. We downloaded all questions posted from October 1, 2012 to December 31, 2012 and all related information like answers

was tracked until January 2014. This time period was chosen because of these reasons: First, questions and answers in this time period are not very out-dated; Second, few user activities on posts in this period are active. Thus, we assume that the best answer to one question is the final one. The dataset we use was dumped on January 2014 4 . Before feature extraction, posts without users' IDs are removed. Then, only questions which have best answers and at least two more answers are considered. The final processed dataset has 52,104 questions and 190,165 answers. On average, there are 3.65 answers per question. During the experiments, our data set is randomly split into two parts evenly: training and testing. To be specific, details as follows show how to generate relatively ranked pairs. For each question, only its best answer is considered as the high quality answer while others are treated as low-quality answers. Then each pair is generated in this way: one best answer and one of other answers to the same question. After all pairs are generated, feature extraction is performed based on information from three main aspects of each pair of questions and answers: content, interactions, users. These are briefly described below. The First group of features are extracted based on the content of the answer in each pair of questions and answers. Part of these features are based on comments to the answers like average score of comments, variance of the comments' scores, number of comments. Comment-based features at least show that the corresponding answer is interesting and incur a good discussion towards problem solving. Besides these, whether one answer has pictures, URL or codes are also factors to show that the current answer has a high quality, since these components are able to show more information than text. Moreover, the length of answers [12][2] and its readability [18] also play an important role on answer quality. Apart from the content information, features based on interaction are also considered, for example, the interaction between questions and answers, and that between different answers to one question. The first one is easy to understand since one answer has to be similar to its corresponding question, and thus the similarity between questions and answers is used as one feature. The second one is designed based on the assumption that users prefer the answers which is easy to understand. Computation of these features are shown in [18]. This is different from the feature interaction in our model. This one is on the feature-design level which focuses on exploring new information sources to design new features, while our case focuses on the model-design level.
4 http://blog.stackoverflow.com/category/cc-wiki-dump/

6

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

User information also has an impact on the quality of answers. One answer is likely to have a high quality if the answerer is one expert. To represent the expertise of one user, these features are extracted based on users' previous activities, for example the number of answers one provides, how many questions one asks, the number of best answers he or she posts. Our experiment is conducted by considering different groups of features and then results are presented respectively. In this way, it is easy to see the performance of different algorithms when we only consider informations from different aspects of our research problem (i.e. different groups of features). Finally, the experiment is conducted on the entire feature set we have. The three groups of features we consider in this experiment are: content, interactions and user information. C. Experiment Results & Discussion To show the performance of our proposed algorithm, we compare our model with approaches used in state-of-the-art. As mentioned in Section Introduction, there are two main trends in best answer prediction: one is to use classification techniques and then decision values are used as quality scores while the other one is to use ranking approaches directly. For the former case, linear support Vector Machine (SVM) is common used because data in social media is in large scale so that nonlinear algorithms are not computational efficient. In our experiment, linear SVM is the first baseline we choose. For the latter case, RankSVM [6] is used which is one main ranking algorithm used in the area of best answer prediction [5]. The code for RankSVM is from Microsoft Research 5 . On CQA sites, there are no direct information we can use as the metric to measure answer quality without manually labeling. For example, scores of each answer might be one proper metric. But this metric is not accurate. It is easy to see that it is easy for the answer which is posted early to have the high score. In fact, on Stack Overflow, there are a lot of answers having the higher scores than the corresponding best answers6 . Thus in our experiments, we only treat the best answers as the high-quality ones and others as low-quality. As a result, in our experiment, it is the pairwise ranking problem so we do not compare with listwise ranking algorithms. To make comparison between different models, two evaluation metrics are used: one is defined in Eqn. 26 and the other one is defined in Eqn. 27. e1 =
(qi ,Ai,j1 ,Ai,j2 )P

g (i) = arg max{si,j , j  {1, · · · , Mi }}
j

I (ji,0 == g (i)) (27) N where ji,0 is the index of the best answer of the ith question, si,j is the predicted score of the j th answer of the ith question and the function g (·) returns the index of the best answer of one given question and the function I (·) is given by Eqn.28. e2 =
i

I (x) =

1 0

if x is true otherwise

(28)

From the definitions, it is easy to see this fact: e1 shows how good one algorithm is when it considers the pairwise ranking regardless of whether one algorithm can find the best answer to one question or not, while e2 shows the performance of each algorithm when applied to best answer prediction. In other words, e1 measures what percentage of relatively ranked pairs are predicted correctly, which focuses on the answerlevel comparison. However e2 measures what percentage of questions have the correctly predicted best answers. To show the performance of different models on the pairwise ranking in best answer prediction, experiments were conducted to collect the metric e1 . The experimental results are shown in Table. II. Table. II presents the performance of
SVM RankSVM Ours fc 0.671 0.411 0.689 fi 0.541 0.534 0.552 fu 0.480 0.543 0.570 all 0.544 0.476 0.693

TABLE II: This table shows the results of different algorithms on Stack Overflow when considering the measurement metric e1 . Three groups of features: fc content, fi interactions, fu user information. algorithms used as learning to rank. From the results, we can see that our model performs best not only when only individual feature groups are considered but also when all features are considered. This shows that our model can be one good pairwise ranking algorithm in the area of community question and answering. From the results of SVM, we can see that when only fc is considered, the performance is best. However, when simple concatenation of all features from different views is applied, the final one gives worse performance instead of better one. Similarly, for RankSVM, its performance is best when only fu is considered. However after considering all features, the performance drops. For our approach, because we consider the interaction structure of features from different views, the final performance is best. This shows that there exists on latent interaction structure in the feature space. Incorporating weakly hierarchical lasso, we can capture this interaction structure. This shows the effectiveness of our proposed model. To show comparison of performance on best answer prediction, experiments were run to collect metric e2 . Table. III presents the performance of different models. From the results, it is easy to see that our model performs best in the problem

I (si,j1 > si,j2 )

|P |

(26)

where si,j1 , si,j2 are predicted scores of Ai,j1 , Ai,j2 respectively. The relatively ranking set P is defined in Eqn. 2 and the function I (·) is shown in Eqn. 28.
5 http://research.microsoft.com/en-us/um/beijing/projects/letor/baselines/ ranksvm-primal.html 6 https://data.stackexchange.com/stackoverflow/query/380215/whereaccepted-answer-does-not-have-the-highest-score

7

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)
fc 0.479 0.223 0.494 fi 0.331 0.321 0.334 fu 0.294 0.361 0.377 all 0.349 0.286 0.498

SVM RankSVM Ours

TABLE III: Experiment results (e2 ) of different algorithms' performance. Three groups of features: fc content, fi interactions, fu user information.

of best answer prediction not only when considering different groups of features independently but also when considering all features jointly. Similar to Table.II, the performance of SVM and RankSVM drop a lot when all features are considered by simple concatenation. For our model, it does not have this problem because of the fact that we incorporate the information from the latent interaction of features from different views. Consequently, we conclude that the proposed models perform better than those in the state-of-the-art. Performance of experiments using both metrics shows the effectiveness of hierarchical interactions between different views in the problem of best answer prediction. VI. C ONCLUSION & F UTURE W ORK We present a new learning-to-rank approach to best answer prediction on CQA sites. Incorporating the weakly hierarchical lasso, our proposed model is able to effectively exploit the interactions of features from different views of the data. To find a solution under this new model, we reformulate it into one existing optimization framework. Experiments on Stack overflow are used to evaluate the proposed approach, with comparison to other methods in state-of-the-art. The experimental results demonstrate the effectiveness and superior performance of our approach. Although our algorithm is designed originally for best answer prediction, it can be treated as one ranking algorithm and used in most ranking situations. Thus the application of our algorithm in different areas can be one piece of future work. Moreover, in our algorithm, one limitation is that we study the interaction structure of different feature dimensions, instead of different groups of feature dimensions. Another interesting future work is to extending our algorithm by considering the hierarchical structure of different groups of feature dimensions. ACKNOWLEDGMENT This work was supported in part by a grant (#1135616) from National Science Foundation. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the NSF. R EFERENCES
[1] A. Anderson, D. Huttenlocher, J. Kleinberg, and J. Leskovec, "Discovering value from community activity on focused question answering sites: a case study of stack overflow," in Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2012, pp. 850­858.

[2] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne, "Finding high-quality content in social media," in Proceedings of the 2008 International Conference on Web Search and Data Mining. ACM, 2008, pp. 183­194. [3] C. Shah and J. Pomerantz, "Evaluating and predicting answer quality in community qa," in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. ACM, 2010, pp. 411­418. [4] D. H. Dalip, M. A. Gonc ¸ alves, M. Cristo, and P. Calado, "Exploiting user feedback to learn to rank answers in q&a forums: a case study with stack overflow," in Proceedings of the 36th international ACM SIGIR conference on research and development in information retrieval. ACM, 2013, pp. 543­552. [5] Y. Cai and S. Chakravarthy, "Answer quality prediction in Q/A social networks by leveraging temporal features," Proceedings of International Journal of Next-Generation Computing, vol. 4, no. 1, 2013. [6] O. Chapelle and S. S. Keerthi, "Efficient algorithms for ranking with svms," Information Retrieval, vol. 13, no. 3, pp. 201­215, 2010. [7] F. Hieber and S. Riezler, "Improved answer ranking in social questionanswering portals," in Proceedings of the 3rd international workshop on Search and mining user-generated contents. ACM, 2011, pp. 19­26. [8] J. Jeon, W. B. Croft, J. H. Lee, and S. Park, "A framework to predict the quality of answers with non-textual features," in Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2006, pp. 228­235. [9] B. Li, T. Jin, M. R. Lyu, I. King, and B. Mak, "Analyzing and predicting question quality in community question answering services," in Proceedings of the 21st international conference companion on World Wide Web. ACM, 2012, pp. 775­782. [10] Y. Yao, H. Tong, T. Xie, L. Akoglu, F. Xu, and J. Lu, "Detecting high-quality posts in community question answering sites," Information Sciences, 2015. [11] A. Shtok, G. Dror, Y. Maarek, and I. Szpektor, "Learning from the past: answering new questions with past answers," in Proceedings of the 21st international conference on World Wide Web. ACM, 2012, pp. 759­768. [12] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman, "Knowledge sharing and yahoo answers: everyone knows something," in Proceedings of the 17th international conference on World Wide Web. ACM, 2008, pp. 665­674. [13] M. Surdeanu, M. Ciaramita, and H. Zaragoza, "Learning to rank answers on large online qa collections." in ACL, 2008, pp. 719­727. [14] ----, "Learning to rank answers to non-factoid questions from web collections," Computational Linguistics, vol. 37, no. 2, pp. 351­383, 2011. [15] A. Agarwal, H. Raghavan, K. Subbian, P. Melville, R. D. Lawrence, D. C. Gondek, and J. Fan, "Learning to rank for robust question answering," in Proceedings of the 21st ACM international conference on Information and knowledge management. ACM, 2012, pp. 833­ 842. [16] G. Burel, Y. He, and H. Alani, "Automatic identification of best answers in online enquiry communities," in The Semantic Web: Research and Applications. Springer, 2012, pp. 514­529. [17] S. Ravi, B. Pang, V. Rastogi, and R. Kumar, "Great question! question quality in community q&a," in Eighth International AAAI Conference on Weblogs and Social Media, 2014. [18] Q. Tian, P. Zhang, and B. Li, "Towards predicting the best answers in community-based question-answering services," in Seventh International AAAI Conference on Weblogs and Social Media, 2013. [19] J. Bien, J. Taylor, R. Tibshirani et al., "A lasso for hierarchical interactions," The Annals of Statistics, vol. 41, no. 3, pp. 1111­1141, 2013. [20] Y. Liu, J. Wang, and J. Ye, "An efficient algorithm for weak hierarchical lasso," in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 283­292. [21] P. Gong, C. Zhang, Z. Lu, J. Z. Huang, and J. Ye, "A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems," in Machine learning: proceedings of the International Conference. International Conference on Machine Learning, vol. 28, no. 2. NIH Public Access, 2013, p. 37. [22] J. Barzilai and J. M. Borwein, "Two-point step size gradient methods," IMA Journal of Numerical Analysis, vol. 8, no. 1, pp. 141­148, 1988.

8

View publication stats

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/306576742

FindingNeedlesofInterestedTweetsinthe HaystackofTwitterNetwork
ConferencePaper·August2016
DOI:10.1109/ASONAM.2016.7752273

CITATIONS

READS

0
3authors,including: QiongjieTian ArizonaStateUniversity
12PUBLICATIONS172CITATIONS
SEEPROFILE

47

AllcontentfollowingthispagewasuploadedbyQiongjieTianon25August2016.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

Finding Needles of Interested Tweets in the Haystack of Twitter Network
Qiongjie Tian
Computer Science and Engineering Arizona State University Email: qiongjie.tian@asu.edu

Jashmi Lagisetty
Computer Science and Engineering Arizona State University Email: jlagiset@asu.edu

Baoxin Li
Computer Science and Engineering Arizona State University Email: baoxin.li@asu.edu

Abstract--Drug use and abuse is a serious societal problem. The fast development and adoption of social media and smart mobile devices in recent years bring about new opportunities for advancing computer-based strategies for understanding and intervention of drug-related behaviors. However, the existing literature still lacks principled ways of building computational models for supporting effective analysis of large-scale, often unstructured social media data. Part of the challenge stems from the difficulty of obtaining so-called ground-truth data that are typically required for training computational models. This paper presents a progressive semi-supervised learning approach to identifying Twitter tweets that are related to personal and recreational use of marijuana. Based on a small, labeled dataset, the proposed approach first learns optimal mapping of raw features from the tweets for classification, using a method of weakly hierarchical lasso. The learned feature model is then used to support unsupervised clustering of Web-scale data. Experiments with realistic data crawled from Twitter are used to validate the proposed approach, demonstrating its effectiveness.

I. I NTRODUCTION Drug use/abuse is among the serious societal problems in the modern age. According to a 2011 report [1], in the United States alone, illicit drug use costs the society more than $193 billion annually and the number is increasing. The impact is also widespread: In 2013, about 24.6 million Americans 12 years old or older were illicit drug users [2]. Accordingly, a lot of research efforts have been devoted to understanding druguse-related behaviors and the analysis of potential benefits and limitations of various intervention strategies. A key step in such drug-use-related research is the collection of user behavior data. Most conventional approaches to user data collection are based on recruitment of participants who would provide inputs to a drug-use-related study, e.g., by answering questionnaires carefully designed to gather various types of behavioral and/or demographical data [3][4]. But there are some well-known limitations in such efforts. For example, the sample size is typically small, as it is in general very costly to involve a large population in such studies. More importantly, such questionnaires in general rely on a participant's explicit recall of his/her drug-use behavior, which could be a limiting factor on its own (e.g., issues like incorrect memory or intentional omission of some facts). The phenomenal growth of social media and smart mobile devices has led to more and more drug-use-related data

appearing online. For example, there are many drug-related discussion groups on Facebook, many drug-use-related questions asked and answered on Yahoo!Answers, as well as many drug-related tweets on Twitter. Such user-generated social media may be collected at a much larger scale (than an explicit user survey) and thus have the potential of offering realistic insights into understanding of substance-use behaviors, their situational factors, and social contexts. A few recent efforts illustrate this nicely. In [5], Christine Lee et al. found that the substance-use related behaviors have similar patterns in data from traditional surveybased approaches and those from social media. In [6], Jennifer Whitehill et al. studied the relationship between mobile usage of social networking sites (e.g. Facebook and Twitter) and the alcohol use in a large street festival. In [7], Joris Hoof et al. conducted one study on analyzing Facebook profiles to show that some Facebook profile elements can be the indicators of real-life behaviors. In [8], Sarah Stoddard et al. examined the influence of young people's social networking behaviors on their alcohol and other drug use. While having demonstrated to some extent the potential of using social media for substance-use research, these existing efforts also revealed the challenges of building computational models for analyzing largely-unstructured social-media. For example, some user attributes that may be readily available from an explicit survey now need complex inference strategies to figure them out. Further, any approach that relies on training from some labeled dataset cannot be easily extended to large-scale analysis. In this paper, we address some of these challenges in the context of illicit marijuana use and its manifestation on Twitter. Specifically, we propose one semisupervised approach to studying the user behaviors of the illicit marijuana use using noisy, unstructured and large-scale Twitter data. To our knowledge, this is the first work to study marijuana use behaviors using large-scale Twitter data. II. R ELATED W ORKS In this section, we briefly review some related work on study of use of marijuana and other substance, including both traditional methods of recruiting participants and more recent approaches using social media data.

IEEE/ACM ASONAM 2016, August 18-21, 2016, San Fran- 1 cisco, CA, USA 978-1-5090-2846-7/16/$31.00 c 2016 IEEE

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

A. Participant-recruitment Based Research Johnston et al. conducted follow-up surveys on young adults regarding their behaviors related to drug use in [9]. Similar recruitment based approaches were also used to study the effect of marijuana use in adolescent on their depressive symptoms and IQ development [10] [11]. As noted earlier, these population-survey-based efforts are usually very time-consuming merely for the stage of data collection. Another point to note is that the above-mentioned efforts focused more on finding features or trends from the data rather than developing computational approaches for modeling user behaviors. B. Research Using Social Media The non-medical use of Adderall (one psychostimulant drug) among college students using Twitter were studied in [12], where the frequencies, percentages and means were analyzed, and the experiments showed that their findings were similar to traditional survey-based methods. To study the smoking behavior on Twitter, Myslin et al. collected tweets from Twitter and performed content and sentiment analysis [13]. Cavazos-Rehg et al. also performed content analysis of tweets but with a pro-marijuana Twitter handle (@stillblazingtho) plus the demographics of the handle's followers [14]. Volkow et al. reported risks of the recreational use of marijuana like the risk of addiction, effect on brain development, relation to mental illness and so on in [15]. Krauss et al. studied the hookah smoking behavior on Twitter in [16]. Leah et al. reported their research on how posts on Twitter changed after legalizing recreational use of marijuana in two states [17]. Katsuki et al. studied the youth non-medical use of prescription medications (NUPM) on Twitter in order to model the frequency of NUPM-related tweets and identified the illegal access to drug abuse via online pharmacies in [18]. While demonstrating the great potential of using social media for substance-use-related analysis, these existing efforts have yet to be extended to Web-scale data. In particular, we have not seen specific computational models for analyzing Web-scale Twitter data for understanding marijuana-userelated behaviors. III. P ROBLEM D EFINITION To study the behavior of marijuana users on Twitter, a fundamental problem is to identify tweets that are related to some underlying users who use marijuana. This problem is more subtle than it appears. For example, one cannot simply rely on using the keyword "marijuana" to search the tweets for solving the problem. There are several complicating factors. First, many "street names" are used to describe marijuana. Second, there may be many tweets that involve medical or research-oriented references to marijuana but they are not at all useful for a study on illicit marijuana use. Considering these factors, we propose to classify a tweet into one of following three categories:

·

·

·

Class One: Tweets in this class are related to personal recreational use of marijuana. They are posted by individual users instead of some official accounts (for example, those for newspaper, companies, or medical institutes). Class Two: In this class, all tweets are related to marijuana but not in the sense of recreational use. For instance, they may discuss the medical or prescription use of marijuana, or report some news involving marijuana. Class Three: This is for those tweets having no identifiable relationship with marijuana use.

Figure 1 illustrates several real examples for each of the three classes defined above.

Fig. 1: Demos to show three classes: (a) is for Class One, (b) is for Class Two and (c) is for Class Three. Various text-based features may be extracted for the task of classifying the tweets. Also, as evident from the related work, it is important to consider social interactions among the underlying users. Furthermore, all these features are not mutually independent, and their intricate correlation may provide additional evidence for improved classification. Considering these, and with the goal of classifying large-scale tweets in mind, we now discuss our overall approach, which is illustrated in Figure 2. In the approach, we first extract a set of basic features from each tweet. Then, utilizing a small labeled training set, we learn a good feature mapping that takes into consideration both some basic features and their interactions, based on weakly-hierarchical lasso. The learned feature mapping model is used to process the large-scale data.

2

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM) where the ith data point is xi  R1×d , i  {1, · · · , N } which is normalized, and its label is yi  {1, 2, 3} and the coefficient to learn is w  Rd×1 . In this paper, the discriminant function is chosen to be one-vs-one linear SVM. The implementation details are provided as follows. We first train one linear regression model by optimizing Eqn.2. 1 w 2 (2) min Xw - y 2 2 2+ w 2 where X  RN ×d and y  RN ×1 . Then we apply one-vs-one linear SVM to s = Xw  RN ×1 to find the label for each tweet.
N

# $%& ( ( + ' ) *

( ! " +

( )

Fig. 2: It shows the entire framework of our methodology. In the following, we first present the basic set of features designed for our task. These features are extracted from either the content of the underlying tweets or the social interactions among the corresponding users, as elaborated below. A. Content-based Features
·

min
v

v

2 2

+C
i=1

i

(3) (4)

s.t.

yi (si  v + b)  1 - i i

·

·

·

The length of the tweet: For each tweet, its length can be one useful feature. For example, the tweets from ordinary users may be generally shorter than those from official accounts. Favorite Count & Retweeted Count: It shows how many people think the tweet is favorite and the number people who retweet this post. This is in general useful for measuring how influential the tweet is. The number of Hash-Tags: This calculates how many trends one tweet mentions. Our original dataset were obtained by crawling using selected street names of marijuana. The tweets with more trends are likely to be classified as Class Three or Two, instead of Class One. TF-IDF on Unigram: Unigram is one common feature used to capture characteristics of one tweet. We build TF-IDF for unigrams of each tweet and use it as one feature. Number of followings and followers: Each user on Twitter can follow others or be followed. However for some official accounts or famous people, they are likely to have a smaller number of followings but a large number of followers. These users are unlikely to post tweets related to personal and recreational use of marijuana. Number of Tweets: This records how many tweets one user has already posted, capturing the level of Twitter activity of the user.

where  is non-negative. However, in practice, the linear model is inadequate for capturing the high degree of non-linearity that typically exists in our problem, which has been shown in our experiments. To allow some level of nonlinearity while maintaining computational efficiency, we introduce to the problem 2nd -order interaction terms with a weakly hierarchical structure, as described in [19][20]. The resultant model is given in Eqn.5. y = f (z ) z = xw + 1 2
d d

(5) xi xj Qi,j
i j

where z is called the z -term of x (for simplicity) and the discriminant function f (·) is given in Eqn.3 (one-vs-one linear SVM in this paper) and xi is the ith dimension of the data point x and Qi,j  R is the coefficient for the interaction between ith and j th dimensions of the feature space. To solve the classification problem under this new model, we formulate the following optimization problem in Eqn.6.
w,v,Q

min

1 2

(f (zi , v ) - yi )2 + 1 w
i 1

1

+

3 Q 2

1

(6)

B. User-based Features
·

s.t.

Q.,j

 |wj | for

j = {1, · · · , d}

where zi is the z -term of xi as defined in Eqn.5, Q 1 = i,j |Qi,j | and v is the model parameter of the discriminant function (the one-vs-one linear SVM). A. Solving the Optimization Problem Solving Eqn.6) directly is difficult. Hence we simplify this optimization problem by a two-step process: We first learn parameters w and Q and then learn the model parameter v of the discriminant function. For parameters w and Q, we model them as one regression model as Eqn.7 when we do not consider the discriminant function. 3 1 (zi - yi )2 + 1 w 1 + Q 1 (7) min w,Q 2 i 2 s.t. Q.,j
1

·

IV. L EARNING F EATURE M APPING F ROM A S MALL DATASET Considering the computational efficiency needed for processing Web-scale data, we may employ a linear classifier as the baseline for doing the classification, as given by Eqn.1. yi = f (xi w) (1)

 |wj | for

j = {1, · · · , d}

3

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM) interaction, the dataset representation is converted as {x ~i , i  {1, · · · , N }} where x ~i is given by Eqn.11. x ~i = (xi , vec(Ri )) (11)

where zi is the z -term of xi as defined in Eqn.5. Then after w and Q are obtained, we learn v of the discriminant function by optimizing Eqn.3. Converting Eqn.6 into Eqn.7 and Eqn.3 allows us to solve the original optimization problem. By solving Eqn.7 and Eqn.3, we can obtain the model parameters v , w and Q which satisfy the original problem (Eqn.6) as well. However, since we add more constraints on these parameters in the process of simplification, the obtained v , w and Q are only the local optima of Eqn.6. While the details for solving Eqn.7 can be found in [19], a brief description is given below. From Eqn.7, we can see that this optimization problem is non-convex because of the existence of constraints, and as a result, we cannot solve it using convex optimization approaches. Thus in [19], one convex relaxation by setting w = w+ - w- is given, where w+ and w- are nonnegative. The convex relaxation version is given as Eqn.8. min 1 2 (^ zi - yi )2 + 1 (w+ + w- ) +
i - + wj

where the element at (j, k ) in the matrix Ri is the product of the j th and k th dimension which is xi,j xi,k . It is easy to see 2 x ~i  R1×(d+d ) . For the new representation, the interaction of the feature dimension is captured by parameters w and Q which are learned from the small labeled dataset (see Section IV). By treating the learned parameters as a kernel, we can have the new clustering as Eqn.12.
k j ,j {1,··· ,k}

min

(~ xv - cj )M (~ xv - cj )T
j =1 v j

(12)

where the learned metric matrix M = diag ((w; vec(Q)))  2 2 R(d+d )×(d+d ) . VI. E XPERIMENTS In this section, we evaluate the performance of our approach with comparison with several typical existing methods. A. Dataset Construction For constructing a small labelled dataset, instead of crawling random tweets online, we first use a list of keywords as one filter to remove unrelated tweets. These keywords are defined based on several Web sources and some government documents1 . The final keyword list was determined to be: marijuana, weed, blunt, cannabis, pot, reefer, buds, 420, mary jane, blaze. With the final list, the Twitter API 2 is utilized to crawl data. The time period we crawled is from January 09 to January 15 in 2016 and all tweets are in English. We crawled a total of 1,166,441 tweets. Among these we randomly labeled 10,000 with comparable proportion for each class (see Table 1 for exact composition in terms of class labels). This small labelled dataset was annotated by two people reading the tweets to decide their labels. B. Learning the Feature Mapping In this part, to compare with commonly used classifiers like linear classifier (Eqn.1) and linear SVM, we split the 10,000 tweets randomly into two parts: training set of 8,000 tweets and testing set with 2,000 tweets. Since in the our approach, we need to compute the feature interaction terms which is defined as the z -term in Eqn.5, we have to reduce the dimension of the original feature vectors. In this experiment, we use LDA [22] to do dimension reduction of TF-IDF of Unigram in the feature sets for our approach. For random guess, we randomly assign one label to every data point and then compute the accuracy based on Eqn.13. e=
Nt i=1

w+ ,w- ,Q

3 Q 2

1

(8)

s.t.

+ 1  wj + - wj , wj 0

Q.,j

for

j = {1, · · · , d} (9)

for

j = {1, · · · , d}
d d

where z ^i = xi · (w+ - w- ) + 1 j k xi,j xi,k Qi,j . A lot of 2 convex optimization approaches can be used to solve Eqn.8, such as FISTA [21]. After we obtain the parameters w and Q, the original problem will become equivalent to the support vector machine which can be solved using sequential minimal optimization. V. C LUSTERING WITH T HE L EARNED F EATURE M APPING A supervised approach cannot be directly applied to Webscale datasets as manually-labeled data are in general in a much smaller scale. A semi-supervised approach would rely on unsupervised clustering to first identify the structures of the data and then employ a small amount of labeled data to annotate the structures. For example, using K-means clustering, we can group a dataset into different clusters. For data points in each cluster, if we assume that they have the same labels, we can randomly select a small number of data points for labeling and then use the labels to annotate the clusters. Assuming k groups in a dataset, a basic K-means algorithm is equivalent to solving the following problem (Eqn.10):
k j ,j {1,··· ,k}

min

xv - cj
j =1 v j

2 2

(10)

where cj is the j th centroid and j is the j th cluster. As we have presumably found a feature mapping scheme in the previous section by maximizing classification accuracy for the labelled data, it is natural to use the learned feature mapping for the clustering stage. Denote the dataset as {xi , i  {1, · · · , N }}. Consider the influence of the 2-order feature

I (yi == y ^i ) Nt

(13)

1 In this paper, we use this forum (www.rehabs.com) and this official document(https://vva.org/wp-content/uploads/2014/12/street-terms.pdf). 2 https://dev.twitter.com/rest/public

4

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

where yi is the ground-truth label of the tweet xi and y ^i is the predicted label and I (x) = 1 0 if x is true otherwise (14)

The experiment results are shown in Table II and The confusion matrix of our approach is shown in Table I.
RG 0.326 LC 0.462 SVM 0.677 Ours 0.976

TABLE II: The table shows the performance of each baseline and our method. RG: random guess; LC: linear classifier; SVM: linear SVM. From Table. II, we can easily see that our algorithm stands out. Compared with the modified linear classifier (Eqn. 1 and Eqn. 3) with our algorithm, the difference is that we consider the interaction terms (the z -term) defined in Eqn.5. Thus these results also show that it is necessary to consider feature selection scheme using weakly hierarchical lasso. Furthermore, our approach performs better than linear SVM. This is also easy to understand because of the nonlinearity introduced in our formulation (Eqn. 5). Nonlinearity comes from the z -term. To further show the performance of each algorithm, the confusion matrices are shown in Table. I. It shows that our approach performs best in all of the three classes. From Table I(a), we can see that LC cannot distinguish Class 1 and Class 2. For example, for Class 2, almost the same number of tweets are classified into Class 1 and Class 2. The baseline with SVM performs better than LC, but the error is still significant. Our approach effectively solves the problem of how to fuse features and provides the optimal feature selection/combination scheme. It is possible to analyze which features (or their interactions) are most influential. Table III shows the top three main factors which affect the classification performance and their corresponding coefficients. From this
retweet num 4.41e-05 TF-IDF1 4.53e-01 TF-IDF2 3.62e-01

C. Clustering Structure on the Web-Scale Data In this part, we apply the learned feature mapping scheme to the large dataset, which contains not only the labeled data points but also unlabeled ones. To show the clustering structure of the partially labeled dataset, we perform two experiments: one using one baseline which is KMeans and the other one is our method based on Eqn. 12. For a good clustering outcome, we assume in each cluster, a majority of data points belong to the same class. To evaluate the performance of the results, we present two metrics (Eqn. 15) to show whether any class is dominant in a given cluster. In each cluster, there may be three classes with sizes n0 , n1 and n2 (in non-increasing order) respectively. If one class does not exist, it means its size is zero. n1 n2 m2 = (15) m1 = n0 n0 In our experiment, the large dataset is partially labeled and thus when we compute m1 and m2 , we only consider the labeled data in each cluster. Then the average is computed for the entire dataset. These two metrics are presented to measure what is the difference between the dominant class and the others. If the values of these metrics are small, then they shows that compared with the size of the dominant class, the others are small. In our experiment, we choose the number of clusters to be k  {10, 100, 200, 300, 400, 500, 1000}. In this way, we can learn the effect of the number of clusters on the clustering performance. The experiment results are shown in Table V.
k 10 100 200 300 400 500 1000 m ¯1 0.411 0.320 0.333 0.318 0.291 0.282 0.239 m ¯2 0.647 0.594 0.594 0.602 0.542 0.542 0.495 k 10 100 200 300 400 500 1000 m ¯1 0.381 0.280 0.240 0.263 0.243 0.228 0.116 m ¯2 0.555 0.487 0.436 0.485 0.423 0.427 0.320

(a) The baseline

(b) our approach

TABLE III: Illustration of top-3 main factors. The second one and the third one are from TF-IDF of Unigram. table, it can be seen that the number of retweets and also the TF-IDF of Unigram play important roles in distinguish these three classes. We can also see that the content of the tweets is most important for classification. Based on the results of Table III, the top interactions are from the two TF-IDF feature dimensions. This is also demonstrated by the experiment results (see Table IV).
TF-IDF1 * TF-IDF1 -2.258e-1 TF-ID2 * TF-ID2 1.844e-1 TF-ID1 * TF-IDF2 7.884e-2

TABLE V: Experiment results on studying the clustering structure of partially labeled dataset. (a) for the baseline and (b) for ours. They show the size of the other class compared with the dominant one. From Table V, we can see that our clustering approach by employing the learned feature mapping scheme performs better than the baseline. As the number of clusters goes up, m ¯1 and m ¯ 2 of KMeans and our approach become small, which means that the percentage of the dominant class becomes large. Compared with the baseline, the percentage of the dominant class is much larger since the corresponding metrics' values are smaller. The average percentage of the dominant class is shown in Fig.3. VII. C ONCLUSION AND F UTURE W ORK We presented one semi-supervised approach to analysis of Twitter data related to marijuana use, using web-scale data,

TABLE IV: This table shows top-3 interaction factors and their corresponding coefficients.

5

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

C1 C2 C3

C1 0.4616 0.3820 0.1751

C2 0.3108 0.3920 0.3146

C3 0.2276 0.2260 0.5103

C1 C2 C3

C1 0.6060 0.1820 0.1259

C2 0.1508 0.6120 0.0780

C3 0.2432 0.2060 0.7962

C1 C2 C3

C1 0.9831 0.0020 0.0014

C2 0.0130 0.9920 0.0410

C3 0.0039 0.0060 0.9576

(a) LC: modified linear classifier

(b) SVM : linear SVM

(c) Ours: our approach

TABLE I: Three confusion matrices for three algorithms: LC(a), SVM (b), Ours(c).

1 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 0 100 200 300 400 500 600 700 800 900 1000 1100 baseline Ours

Fig. 3: It shows the average percentages of the dominant class plotted based on the experiment result at each k  {10, 100, 200, 300, 400, 500, 1000}

which includes: learning the optimal feature mapping scheme and grouping the entire data using an improved clustering algorithm. The experimental results demonstrated the effectiveness and efficiency of our approach. There are still some limitations we need to work on. For example, when we learn the feature mapping scheme, we relax the problem to be one easier one, and thus the learned parameters are only locally optimal. Another problem is how to incorporate features reflecting temporal patterns of user behaviors. ACKNOWLEDGMENT This work was supported in part by a grant (#1135616) from National Science Foundation. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the NSF. R EFERENCES
[1] U. D. o. J. N. D. I. Center, "The economic impact of illicit drug use on american society," Product No. 2011-Q0317-002, 2011. [2] S. Abuse and M. H. S. Administration, "Results from the 2013 national survey on drug use and health: Summary of national findings," NSDUH Series H-48, vol. 14, no. 4863, 2014. [3] K. J. Quintelier, K. Ishii, J. Weeden, R. Kurzban, and J. Braeckman, "Individual differences in reproductive strategy are related to views about recreational drug use in belgium, the netherlands, and japan," Human Nature, vol. 24, no. 2, pp. 196­217, 2013. [4] J. C. A. Lacson, J. D. Carroll, E. Tuazon, E. J. Castelao, L. Bernstein, and V. K. Cortessis, "Population-based case-control study of recreational drug use and testis cancer risk confirms an association between marijuana use and nonseminoma risk," Cancer, vol. 118, no. 21, pp. 5374­ 5383, 2012.

[5] C. Lee, "Recruitment through social networking sites: Are substance use patterns comparable to traditional recruitment methods?" in Medicine 2.0 Conference. JMIR Publications Inc., Toronto, Canada, 2014. [6] J. M. Whitehill, M. A. Pumper, and M. A. Moreno, "Emerging adults use of alcohol and social networking sites during a large street festival: A real-time interview study," Substance abuse treatment, prevention, and policy, vol. 10, no. 1, p. 1, 2015. [7] J. J. van Hoof, J. Bekkers, and M. van Vuuren, "Son, youre smoking on facebook! college students disclosures on social networking sites as indicators of real-life risk behaviors," Computers in human behavior, vol. 34, pp. 249­257, 2014. [8] S. A. Stoddard, J. A. Bauermeister, D. Gordon-Messer, M. Johns, and M. A. Zimmerman, "Permissive norms and young adults alcohol and marijuana use: The role of online communities," Journal of Studies on Alcohol and Drugs, vol. 73, no. 6, pp. 968­975, 2012. [9] L. D. Johnston, Monitoring the Future: National Survey Results on Drug Use, 1975-2008: Volume II: College Students and Adults Ages 19-50. DIANe Publishing, 2010. [10] R. M. Schuster, R. Mermelstein, and L. Wakschlag, "Gender-specific relationships between depressive symptoms, marijuana use, parental communication and risky sexual behavior in adolescence," Journal of youth and adolescence, vol. 42, no. 8, pp. 1194­1209, 2013. [11] N. J. Jackson, J. D. Isen, R. Khoddam, D. Irons, C. Tuvblad, W. G. Iacono, M. McGue, A. Raine, and L. A. Baker, "Impact of adolescent marijuana use on intelligence: Results from two longitudinal twin studies," Proceedings of the National Academy of Sciences, p. 201516648, 2016. [12] C. L. Hanson, S. H. Burton, C. Giraud-Carrier, J. H. West, M. D. Barnes, and B. Hansen, "Tweaking and tweeting: exploring twitter for nonmedical use of a psychostimulant drug (adderall) among college students," Journal of medical Internet research, vol. 15, no. 4, 2013. [13] M. Mysl´ in, S.-H. Zhu, W. Chapman, and M. Conway, "Using twitter to examine smoking behavior and perceptions of emerging tobacco products," Journal of medical Internet research, vol. 15, no. 8, 2013. [14] P. Cavazos-Rehg, M. Krauss, R. Grucza, and L. Bierut, "Characterizing the followers and tweets of a marijuana-focused twitter handle," Journal of medical Internet research, vol. 16, no. 6, 2014. [15] N. D. Volkow, R. D. Baler, W. M. Compton, and S. R. Weiss, "Adverse health effects of marijuana use," New England Journal of Medicine, vol. 370, no. 23, pp. 2219­2227, 2014. [16] M. J. Krauss, S. J. Sowles, M. Moreno, K. Zewdie, R. A. Grucza, L. J. Bierut, and P. A. Cavazos-Rehg, "Peer reviewed: Hookah-related twitter chatter: A content analysis," Preventing chronic disease, vol. 12, 2015. [17] L. Thompson, F. P. Rivara, and J. M. Whitehill, "Prevalence of marijuana-related traffic on twitter, 2012­2013: a content analysis," Cyberpsychology, Behavior, and Social Networking, vol. 18, no. 6, pp. 311­319, 2015. [18] T. Katsuki, T. K. Mackey, and R. Cuomo, "Establishing a link between prescription drug abuse and illicit online pharmacies: Analysis of twitter data," Journal of medical Internet research, vol. 17, no. 12, 2015. [19] J. Bien, J. Taylor, and R. Tibshirani, "A lasso for hierarchical interactions," Annals of statistics, vol. 41, no. 3, 2013. [20] Y. Liu, J. Wang, and J. Ye, "An efficient algorithm for weak hierarchical lasso," in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 283­292. [21] A. Beck and M. Teboulle, "A fast iterative shrinkage-thresholding algorithm for linear inverse problems," SIAM journal on imaging sciences, vol. 2, no. 1, pp. 183­202, 2009. [22] Q. Gu, Z. Li, and J. Han, "Linear discriminant dimensionality reduction," in Machine Learning and Knowledge Discovery in Databases. Springer, 2011, pp. 549­564.

6

View publication stats

Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)

Clustering-Based Joint Feature Selection for Semantic Attribute Prediction
Lin Chen and Baoxin Li Arizona State University, Tempe Arizona {lin.chen.cs, baoxin.li}@asu.edu Abstract
Semantic attributes have been proposed to bridge the semantic gap between low-level feature representation and high-level semantic understanding of visual objects. Obtaining a good representation of semantic attributes usually requires learning from high-dimensional low-level features, which not only significantly increases the time and space requirement but also degrades the performance due to numerous irrelevant features. Since multiattribute prediction can be generalized as an multitask learning problem, sparse-based multi-task feature selection approaches have been introduced, utilizing the relatedness among multiple attributes. However, such approaches either do not investigate the pattern of the relatedness among attributes, or require prior knowledge about the pattern. In this paper, we propose a novel feature selection approach which embeds attribute correlation modeling in multi-attribute joint feature selection. Experiments on both synthetic dataset and multiple public benchmark datasets demonstrate that the proposed approach effectively captures the correlation among multiple attributes and significantly outperforms the state-of-the-art approaches.

Figure 1: Illustration of Shoe images with three corresponding attributes "High Heel", "Formal" and "Red". Good representations of semantic attributes are often built on top of high-dimensional, low-level features. Attribute learning directly based on such raw, high-dimensional features may suffer from the curse of dimensionality curse. Further, often it is reasonable to assume that not all the low-level features would have equal contribution to all the attributes. Feature selection, selecting a subset of most relevant features for a compact and accurate presentation, is proven to be an effective and efficient way to handle high-dimensional data [Tang et al., 2014]. Multi-task joint feature selection has been introduced by [Chen et al., 2014] for attribute ranking by exploring the correlation among attributes. However, this work assumes that all attributes are correlated by sharing the same subset of features, which is not always accurate. For example, as shown in Figure 1, a "high-heel" shoe is usually considered as a "formal" shoe as well. It is reasonable to assume these attributes share the same subset of features, e.g., shape-related descriptors. However, it is hard to identify whether "high heel" or "formal" shoes are in red, which suggests the attribute "color" may not share the same subset of features with the other attributes but is determined by, e.g., color-related descriptors. In other words, attributes are usually related in clustering structures. [Jayaraman et al., 2014] first explores such clustered relatedness on attribute prediction. However, their approach requires manually specified group structure as prior. To our knowledge, there is still lack of a feature selection approach being able to identify grouping/clusering structures among attributes for improved attribute prediction. In this paper, we propose a regularization-based multi-task feature selection approach that aims at automatically partitioning the attributes into groups while simultaneously uti-

1

Introduction

Recent literature has witnessed fast development of representations using semantic attributes, whose goal is to bridge the semantic gap between low-level feature representation and high-level semantic understanding of visual objects. Attributes refer to visual properties that help describe visual objects or scenes such as "natural" scenes, "fluffy" dogs, or "formal" shoes. Visual attributes exist across object category boundaries and many methods have been employed in applications including object recognition [Farhadi et al., 2010], face verification [Song et al., 2012], image search [Kovashka et al., 2012; Scheirer et al., 2012] and sentiment analysis [Wang et al., 2015].
 The work was supported in part by ONR grant N00014-15-12344 and ARO grant W911NF1410371. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of ONR or ARO.

3338

lizing such group information for attribute-dependent feature selection. We employ a clustering regularizer for attribute partition, where strong attribute relatedness is assumed to exist within each cluster. Besides, a group-sparsity regularizer is imposed on the objective function to encourage intra-cluster feature sharing and inter-cluster feature competition. Under this formulation, we propose an alternating structure optimization algorithm, which efficiently solves the relaxed form of the proposed formulation. We verify the effectiveness and generalization capability of our approach on both synthetic and real-world benchmark datasets. The results show that our approach outperforms the state-of-the-art approaches on feature selection, attribute prediction and zero-shot learning.

where mi denotes the mean vector of the i-th cluster. Let ei = [1, 1, . . . , 1]> 2 Rni 1 , then Eq. (1) can be derived as
ni k X X i=1 j =1 k X i=1 ( i) kwj

mi k =

2

k X i=1

kWi (Ini

ei ei > 2 )kF ni

=

Tr(Wi> Wi )

ei > ei ( p )Wi> Wi ( p ) ni ni

(2)

ek e1 e2 m k p p Let F = diag( p be an orn1 , n2 , . . . , nk ) 2 R thonormal matrix, then Eq. (2) can be rewritten as

2

Methodology

Let F = {f1 , f2 , . . . , fd } be the set of d features and then we can represent a set of n instances by the feature set F as X = [x1 , x2 , . . . , xn ] 2 Rdn . Let C = {c1 , c2 , . . . , cm } be the set of m attribute labels and Y = [y 1 , y 2 , . . . , y n ] 2 {0, 1}mn denotes the label matrix where y i 2 Rm (i = 1, 2, . . . , n) is the label vector of the i-th instance. We aim to select K (K d) most relevant features from F by leveraging X , Y and the attribute correlation in C . Let s =
d K K

Tr(W > W ) Tr(F > W > W F ) To make the problem tractable, we ignore the special structure of F and let it be an arbitrary orthonormal matrix. By adding a global penalty Tr(W > W ) measuring how large the weight vectors are, capturing label correlation is to partition W into k clusters, which can be achieved by solving the following optimization problem:
F > F =I k

min Tr(W > W ) Tr(F > W > W F )+ Tr(W > W ) (3)

2.2

Feature Selection

z }| { z }| {  (0, . . . , 0, 1, . . . , 1), where  (·) is the permutation function and K is the number of features to select where si = 1 indicates that the i-th feature is selected. The original data can be represented as diag(s)X with K selected features where diag(s) is a diagonal matrix. We assume that a linear projection matrix W = [w1 , w2 , . . . , wm ] 2 Rdm maps the data X to its label matrix Y where wi 2 Rd is the projection vector for the i-th class ci . If we do not consider attribute correlation, we can select K features via solving the following optimization problem: min L(W > diag(s)X, Y )
W,s

With the model component to capture attribute correlation in Eq. (3), the proposed feature selection framework is to solve the following optimization problem:
W,F,s

min L(W > diag(s)X, Y ) + Tr(W > W ) + (Tr(W > W ) Tr(F > W > W F ))

where L(·) is the loss function and typical choices of loss functions include least square and logistic regression.

s.t., s 2 {0, 1}n , sT 1n = K

s.t. F > F = Ik , s 2 {0, 1}n , s> 1n = K (4) where controls the contribution from modeling label correlation and controls the generalization performance. The constraint on s makes Eq. (4) a mixed integer programming problem, which is difficult to solve. We observe that diag(s) and W are in the form of W T diag(s). Since s is a binary vector and d K rows of the diag(s) are all zeros, W T diag(s) is a matrix where the elements of many rows are all zeros. This motivates us to absorb diag(s) into W as W = W T diag(s), and add `2,1 -norm on each grouped Wi to encourage sparse-based group-wise joint feature selection. With this relaxation, Eq. (4) can be rewritten as:
W,F ;F > F =Ik

2.1

Modeling Label Correlation

Based on the assumption that correlated attributes would share the same features, we propose to model attribute correlation via learning the clustering structures through k-means. Let E be a permutation partition matrix, then a partition of the projection matrix W into k clusters can be formed as:
i) W E = [W1 , W2 , . . . , Wk ], Wi = [w1 , w2 , . . . , w( ni ]; ( i) ( i)

min

L(W > X, Y ) + 

where Wi 2 Rdni (i = 1, 2, . . . , k ) is the i-th partitioned group includes ni projection vectors (or attribute labels). The associated sum-of-squares cost function for the partition can be formulated as
ni k X X i=1 j =1 ( i) kw j

+ (Tr(W > W ) Tr(F > W > W F )) (5) where  controls the sparsity of W . The key idea lying here is that we use the clustering regularizer to partition the tasks into groups where strong correlation exists among tasks in the same group; and feature selection based on such group structures would make sure appropriate feature subsets are selected to represent the respective semantic attributes.

k X i=1

kWi k2,1 + Tr(W > W )

3

Algorithm

mi k , mi =

2

ni X j =1

( i) wj /ni

(1)

In this section, we first introduce an optimization algorithm to seek an optimal solution (summarized in Algorithm 1) for Eq. (5). Then we propose an approach to estimate the attribute assignment (summarized in Algorithm 2).

3339

3.1

Optimization

The optimization problem in Eq. (5) is non-convex nonsmooth, which makes the formulation difficult to solve in its original form. Thus we adopt several relaxations to make it solvable. The attribute correlation regularization in Eq. (3) can be rewritten as: Tr(W ((1 +  )I F F > )W > ) where  = / > 0. Let M = F F > , according to [Zhou et al., 2011] the previous regularizer can be relaxed into the following convex form:  (1 +  )Tr(W ( I + M ) 1 W > ) s.t. tr(M ) = k, M I, M 2 Sm (6) + m where S+ is the set of mm positive semidefinite matrices. Following a similar idea in [Bach, 2008], we reformulate Eq. (5) by squaring the `2,1 norm. Since the `2,1 norm is positive, the squaring represents a smooth monotonic mapping. Without loss of the generality, we adopt the traditional least square loss for demonstration in this paper. Then we get the following jointly convex smooth objective function regarding to W and M . k X arg min kW > X Y k2 ( kW i k 2 , 1 ) 2 F +
W,M i=1

Algorithm 1 Feature Selection Optimization Input: 1. Multiple attribute data {X, Y }; 2. Parameters , , k (optional) and the number of selected features K ; 3. The initial projection matrix W0 ; Procedure: 1: Set W = W0 ; 2: repeat 3: Update M according to Eq. (8); 4: Update r according to Alg. 2; 5: Update according to Eq. (10); 6: Update W according to Eq. (11); 7: until Converges 8: Sort each feature according to kwi k2 in descending order of each group; 9: return The group-wise top-K ranked features; Algorithm 2 Cluster Assignment Estimation Input: M; Procedure: 1: Approximate F by top-ranked eigenvector of Q; 2: Calculate R11 , R12 by applying QR decomposition with column pivoting on F by Eq. (12); ^ by Eq. (13); 3: Calculate R 4: calculate r by Eq. (14) for each attribute; 5: return Cluster assignment vector r ; Optimizing W When Fixing M The squared group-wise `2,1 norm in Eq. (7) is still difficult to derive directly. To alleviate that, we introduce Psome P positive dummy variables ij 2 R+ which satisfies i j ij = 1. [Argyriou et al., 2008] proves an upper bound of the squared `2,1 norm in terms of the positive dummy variables
k X i=1

 (1 +  )Tr(W ( I + M ) 1 W > ) s.t. tr(M ) = k, M I, M 2 Sm (7) + Since it is difficult to optimize the linear projection matrix W and attribute correlation matrix M simultaneously, we employ Alternating Structure Optimization (ASO), which has been shown to be effective in many practical applications [Blitzer et al., 2006; Quattoni et al., 2007] and is guaranteed to converge to a global optimal solution. Optimizing M when fixing W Given a fixed W , the optimization problem is decoupled into the following optimization problem: min Tr(W ( I + M )
M 1

W >)

(kWi k2,1 )2 = (

s.t. tr(M ) = k, M I, M 2 Sm (8) + We solve the problem based on the following Lemma due to [Zhou et al., 2011]: Lemma 1 For the optimization problem in Eq. (8), let W = U V be the singular value decomposition of W where  = diag([ 1 , 2 , . . . , m ]), M = QQ> be the Eigen decomposition of M where  = diag([ 1 , 2 , . . . , q ]) and q be the rank of . Then the optimal Q is given by Q = V and the optimal  is given by solving the following optimization problem: q 2 X i  = arg min  + i i=1 s.t.
q X i=1 i

where wi,j 2 R1m is the row vector of Wi . Thus updated by holding the equality:
ij

k X d X i=1 j =1

kwi,j k2 )2 

k X d X (kwi,j k2 )2 i=1 i=1 ij ij

can be

= kwi,j k2 /

Given a fixed M , each projection vector w can then be updated by optimize the following problem arg min kW T X
W

d X j =1

kwi,j k2 .

(10)

Y k2 F +

 (1 +  )Tr(W ( I + M )

k X d X (kwi,j k2 )2 i=1 i=1 ij 1

W )

>

(11)

which can be solved by gradient-type approach.

= k, 0 

i 1

(9)

3.2

Estimating Attribute Assignment

Eq. (9) can be solved using the similar technology in [Jacob et al., 2009].

The group-wise feature selection is conducted by the clustering structure of the attribute. However, given the M optimized by the previous algorithm, it is not readily possible

3340

to observe the cluster assignment of the attributes because M is spectrally relaxed. In this subsection, we propose an approach to acquire the cluster structure. We first need to obtain a good approximation of the cluster indicator matrix F . Given M , we first apply Eigen decomposition M = QQ> where each column of Q is the eigenvector and each diagonal element of  is the eigenvalue. Then we rank the columns of Q in decreasing order according to its corresponding eigenvalues, and the top-ranked k columns give an approximation of the cluster assignment matrix F . The number of the cluster k can be either manually specified or automatically explored by setting a threshold (10e 8 in our experiment) regarding to the absolute value of the eigenvalue. After obtaining F , without loss of generality, we assume the optimized W = [W1 , W2 , · · · , Wk ]T where the submatrix Wi includes all attributes belonging to the i-th cluster. Let ti = [ti1 , ti2 , . . . , tini ]T denote the largest eigenvector of Wi T Wi , [Zha et al., 2002] showed that F can be reformulated as F T = [t11 v 1 , · · · , t1s1 v 1 , · · · , tk1 v k , · · · , tks1 v k ] | {z } | {z }
cluster 1 clusterk T k k

zero-shot learning capabilities on image benchmark datasets. All the datasets are standardized to zero-mean and normalized by the standard deviation. For all approaches, the super parameters are selected via cross-validation. We cannot get the number of cluster k without any prior knowledge for realworld, thus we also select k by the prediction accuracy on a small subset of datasets.

4.1

Simulation Study

where V = [v1 , v2 , · · · , vk ] 2 R is an orthogonal matrix. Since vi is orthogonal to each other, the cluster structure can be acquired by picking up a column of F which has the largest norm as the first cluster, and orthogonalizing the other columns against this column. Then the same process is executed on the rest of columns until all clusters are identified. This process is identical to a QR decomposition with column pivoting on F F T = Q[R11 , R12 ]P T (12) where Q 2 Rkk is an orthogonal matrix, R11 2 Rkk is an upper triangular matrix and P 2 Rmm is a permutation ^2 matrix. Then we calculate the cluster assignment matrix R Rkm by ^ = [Ik , R 1 R12 ]P T R (13) 11 where Ik 2 Rkk is an identity matrix. The cluster assign^ . The cluster ment information can then be inferred from R membership of each attribute (column) is determined by the row index of the largest element (in absolute value) of the ^ . Denote r 2 Rm as the cluster corresponding column in R identification vector where ri records which cluster the i-th class belongs to, then r can be calculated by ri = arg max r ^ij
j

Since it is difficult to obtain the groundtruth cluster structure for real applications, we first verify the effectiveness of the proposed approach in obtaining the cluster structures on simulated dataset. Following [Jacob et al., 2009; Zhou et al., 2011], we construct the synthetic data containing 5 clusters with 10 learning tasks in each cluster, generating a total number of 50 tasks. For the i-th task, a dataset Xi 2Rdn is randomly drawn from a normal distribution N (0, 1) for learning, with the dimension d = 30 and the sample size n = 60. The projection model is constructed as follows. For the id th cluster, we generate a cluster weight vector wc i 2R drawn from the normal distribution N (0, 900). Then 15 dimensions of wc i are randomly but carefully selected and assigned to zeros, to ensure all wc are orthogonal to each other. Similarly, for the j -th task belonging to cluster i, we generate a taskd specific weight vector ws j 2R drawn from the normal distribution N (0, 16) with the same dimensions of wc i assigned to zeros. Thus, the ultimate weight vector of the j -th task is the linear combination of the cluster and task-specific weight s vector wj = wc i + wj . The corresponding response y i of the i-th samples xi of task j is then obtained by y i = wT j xi + "i where " is the noise vector drawn from N (0, 0.1). We choose 0.5 as the threshold to assign binary label to each sample. We verify the effectiveness of our proposed approach by comparing the learned cluster structure and the selected features with the groundtruth. Based on the prior knowledge implied by the construction of the groundtruth, We set k = 5 and the number of selected features as K = 15. Figure 2 shows one example of the learned projection matrix 2(b) with the comparison of the groundtruth 2(a) where the white part represents zeros and the black part represents non-zeros. The result shows that our approach is able to roughly capture the correct group sparse structures.

(14)

^. where r ^ij is the (i, j )-th entry of R

4

Experiments

In this section, we first verify the effectiveness of our proposed approach on one synthetic dataset. Since the proposed approach can be generalized to general multi-label problem, we evaluate the feature selection capability on various benchmark datasets. At last we evaluate the attribute prediction and

(a) Groundtruth model

(b) Learned model

Figure 2: The learned projection matrix and the corresponding groundtruth in the simulation experiments. The white parts are zeros and the black parts are non-zeros.

3341

4.2

Feature Selection

We verify the feature selection capability on general multilabel datasets in this section. The experiment is conducted on 6 public benchmark feature selection datasets including one object image dataset COIL100 [COI, 1996], one handwritten digit image dataset USPS [Hull, 1994], one spoken letter speech dataset Isolet [Fanty and Cole, 1991], three face image dataset YaleB [Georghiades et al., 2001], ORL [Samaria and Harter, 1994] and PIX10P1 . The statistics of the datasets are summarized in Table 2. We compare the proposed approach with the following representative feature selection algorithms: Fisher Score [Duda et al., 2001], mRMR [Peng, 2005], Relief-F [Liu and Motoda, 2008], Information Gain [Cover and Thomas, 1991], MTFS [Argyriou et al., 2008]. Following the common way to evaluate supervised feature selection, we assess the quality of selected features in terms of the classification performance [Han et al., 2013; Cai et al., 2013]. The larger classification accuracy is, the better performance the corresponding feature selection approach achieves. In our experiments, we employ linear Support Vector Machine (SVM) and k -nearest neighbors (k NN) classifier with k = 3 for evaluation. How to determine the optimal number of selected features is still an open question for feature selection; hence we vary the number of selected features as {10,30, 50 . . . ,90} in this work. In each setup 50% samples are randomly selected for training and the remaining is for testing. Specific constrains are imposed to make sure the class labels of the training set are balanced. The whole experiment is conducted 10 rounds and average accuracies are reported. Figure 1 shows the comparison results for SVM and k NN on the 6 benchmark datasets when 50 features are selected. The result shows that MTFS and the proposed framework outperform Fisher Score, mRMR and Information Gain. The performance gain comes from that Fisher Score, mRMR and Information Gain select features one by one while MTFS and FSMC select features in a batch model. It is consistent with what was suggested in [Tang and Liu, 2012] that it is better to analyze features jointly for feature selection. Besides, in most cases, the proposed framework outperforms MTFS. Better performance gain is usually achieved when fewer number of features are selected. This performance gain suggests that modeling label correlation can significantly improve feature selection performance for multi-class data.

The experiments are conducted on three benchmark datasets: aYahoo [Farhadi et al., 2009], Animals with Attributes (AwA) [Lampert et al., 2009] and SUN attribute [Patterson and Hays, 2012] and the statistics of the datasets are summarized in Table 4. To obtain a good representation of the high-level attributes, we require that the features can capture both the spatial and context information. Thus, we constructed the features by pooling a variety types of feature histograms including GIST, HoG, SSIM. For aPascal/aYahoo and AwA datasets we use predefined seen/unseen split published with the datasets. For SUN dataset, 60% of categories are randomly split out as "seen" categories in each round with the rest as "unseen" categories. During training 50% of samples are randomly and carefully drawn from each seen categories to ensure the balance of the positive and negative attribute labels. The rest samples from "seen" classes and all samples from "unseen" classes are used for testing. Table 3 shows the average prediction accuracy of each approach over all attributes by running the experiment 10 rounds. The result shows that for both "seen" and "unseen" categories, DSVA outperforms MTAL in prediction accuracy and our proposed approach further outperforms DSVA by 2%4%. DSVA decorrelates low-correlated attributes compared with MTAL thus achieves better prediction performance. However, the manually specified or off-line learned group structures are not able to achieve the optimal result. Our approach iteratively optimizes the clustering structure and the projection model, which achieves the best performance.

4.4

Zero-shot Learning

4.3

Attribute Prediction

We then compare our approach with state-of-the-art attribute learning work [Chen et al., 2014] (referred as MTAL) and [Jayaraman et al., 2014] (referred as DSVA). Since MTAL is initially proposed for attribute ranking, we replace the original loss function with the one adopted in this paper for fair comparison. DSVA requires attribute groups as prior, thus we run k-means offline to obtain the clusters for datasets do not have such information.
1 PIX10P is publicly available https://featureselection.asu.edu/datasets.php

We also experiment on the zero-shot learning problem on all three datasets. Zero-shot learning aims to learn a classifier based on training samples from some seen categories, and classify some new samples to a new unseen category. We adopt the Direct Attribute Prediction (DAP) framework proposed in [Lampert et al., 2009] with attribute prediction probability from each approaches as input. Since only continuous image level attribute labels are provided on the SUN dataset, we construct the class level attribute labels by thresholding the average attribute label values of all samples from the class. Same "Seen"\"Unseen" categories splits are adopted as previous experiments. The Average classification accuracies of 10 rounds experiment are reported in Table 5. The result shows that on aYahoo and AwA, our approach achieves significant performance gains than the baseline approaches. The large number of categories in SUN dataset make the classification problem very hard which leads to all low performance of all approaches. Our approach still works better than the baseline approaches.

4.5

On Choosing the Parameters

from

The proposed framework has three important parameters -  controlling the sparsity of W , controlling the contribution of modeling label correlation and gamma controls the global penalty. We study the effect of each parameter by fixing the other to see how the performance of the proposed approach varies with the number of selected features. Due to the page

3342

Table 1: Classification results (ACC%±std) of different feature slection algorithm on different datasets. (the higher the better). Algorithm DataSet Fisher mRMR Relief-F Information Gain MTFS Proposed COIL100 60.66±3.54 55.72±3.34 62.80±2.56 62.00±2.84 78.77±2.35 79.08±2.12 USPS 86.30±2.81 58.44±4.02 86.83±2.83 70.25±3.16 86.25±2.52 93.15±2.18 Isolet 75.64±3.01 70.92±3.72 82.30±2.81 76.51±2.56 84.05±2.24 87.06±1.98 SVM YaleB 66.85±3.65 56.91±4.21 71.91±2.24 71.74±2.11 76.08±2.14 78.17±2.18 ORL 46.50±4.21 84.51±2.32 67.18± 3.01 53.24±2.96 85.62±1.94 90.51±1.78 PIX10P 93.56±2.01 90.45±3.32 96.00±1.77 92.01±1.97 96.81±1.54 99.54±1.68 COIL100 63.33±3.21 54.86±4.32 65.11±2.01 63.44±2.76 81.86±1.94 82.48±1.68 USPS 89.39±2.11 59.17±3.72 89.61±2.01 74.70±2.76 90.44±1.54 95.53±1.18 Isolet 75.38±2.45 57.56±3.42 79.87±2.21 73.71±2.42 77.01±2.14 83.21±2.18 k NN YaleB 69.17±3.24 58.41±3.72 65.53±2.81 65.37±2.42 77.08±2.45 78.96±2.28 ORL 53.01±3.44 72.56±2.42 60.38±2.71 52.44±2.76 85.86±2.24 88.10±2.10 PIX10P 94.56±1.91 86.45±2.22 96.00±1.81 86.04±2.04 97.81±1.54 99.34±1.22 Table 2: Statistics of the Feature Selection datasets Dataset # of Samples # of Features # of Classes COIL100 7200 1024 100 YaleB 2414 1024 38 ORL 400 4096 40 PIX10P 100 10000 10 USPS 9298 256 10 Isolet 7797 617 150 regularizer encourages intra-group feature-sharing and intergroup feature competition. With an efficient alternating optimization algorithm, the proposed approach is able to obtain a good group structure and select appropriate features to represent semantic attributes. The proposed approach was verified on both synthetic and real-world benchmark datasets with comparison with state-of-the-art approaches. The result shows effective group structure identification capability of our method, as well as its significant performance gains on feature selection, attribute prediction and zero-shot learning.

References
[Argyriou et al., 2008] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning. J. Mach. Learn. Res., 73(3):243­272, December 2008. [Bach, 2008] Francis R. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn. Res., 9:1179­1225, June 2008. [Blitzer et al., 2006] John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspondence learning. EMNLP '06, pages 120­128, 2006. [Cai et al., 2013] Xiao Cai, Feiping Nie, and Heng Huang. Exact top-k feature selection via l2,0-norm constraint. In IJCAI '13, pages 1240­1246, 2013. [Chen et al., 2014] Lin Chen, Qiang Zhang, and Baoxin Li. Predicting multiple attributes via relative multi-task learning. In Proc. of CVPR'14, pages 1027­1034, June 2014. [COI, 1996] Columbia Object Image Library (COIL-100). Technical report, Columbia University, 1996. [Cover and Thomas, 1991] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, 1991. [Duda et al., 2001] R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classification. John Wiley & Sons, New York, 2 edition, 2001. [Fanty and Cole, 1991] Mark Fanty and Ronald Cole. Spoken letter recognition. In NIPS '91, pages 220­226. 1991. [Farhadi et al., 2009] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. In Com-

Figure 3: Parameter Analysis on SVM. limitation, we only report the result on the Isolet dataset with SVM but we have similar observations in other datasets. Figure 3 demonstrates the performance variance w.r.t. different parameters and the number of selected features. With the increase of , the performance first increases, demonstrating the importance of modeling label correlation, and then decreases. This property is practically useful because we can use this pattern to set . When  increases, the performance also increases dramatically, which suggests the capability of `2,1 -norm for feature selection. The performance also increases with and then decrease, but relatively stable. The best performance is achieved around 0.1.

5

Conclusions

In this paper, we proposed a clustering-base multi-task joint feature selection framework for semantic attribute prediction. Our approach employs both clustering and group-sparsity regularizers for feature selection. The clustering regularizer partitions the attributes into different groups where strong correlation lies among attributes in the same group while weak correlation exists between groups. The group-sparsity

3343

Table 3: DataSet Methods MTAL DSVA Porposed

Average prediction accuracies of all attributes on Seen and Unseen categories (the higher the better). aPascal/aYahoo AwA SUN Seen Unseen Seen Unseen Seen Unseen 0.5967±0.020 0.5663±0.022 0.5976±0.011 0.5587±0.012 0.6326±0.021 0.6020±0.022 0.6105±0.018 0.5826±0.019 0.6053±0.015 0.5622±0.018 0.6469±0.025 0.6165±0.027 0.6363±0.014 0.6011±0.015 0.6254±0.007 0.5837±0.008 0.6682±0.011 0.6324±0.013 [Liu and Motoda, 2008] H. Liu and H. Motoda, editors. Computational Methods of Feature Selection. Chapman & Hall, 2008. [Patterson and Hays, 2012] Genevieve Patterson and James Hays. Sun attribute database: Discovering, annotating, and recognizing scene attributes. In Proc. of CVPR'12, 2012. [Peng, 2005] F. Ding C. Peng, H. Long. Feature selection based on mutual information: Criteria of max-dependency, max-relevance, and min-redundancy. IEEE Trans. Pattern Anal. Mach. Intell., 27(8):1226­1238, 2005. [Quattoni et al., 2007] A. Quattoni, M. Collins, and T. Darrell. Learning visual representations using images with captions. In CVPR '07, pages 1­8, June 2007. [Samaria and Harter, 1994] F.S. Samaria and A.C. Harter. Parameterisation of a stochastic model for human face identification. In Applications of Computer Vision' 94, pages 138­142, Dec 1994. [Scheirer et al., 2012] W.J. Scheirer, N. Kumar, P.N. Belhumeur, and T.E. Boult. Multi-attribute spaces: Calibration for attribute fusion and similarity search. In Proc. of CVPR'12, pages 2933­2940, June 2012. [Song et al., 2012] Fengyi Song, Xiaoyang Tan, and Songcan Chen. Exploiting relationship between attributes for improved face verification. In Proc. of BMVC'12, pages 27.1­27.11, 2012. [Tang and Liu, 2012] Jiliang Tang and Huan Liu. Feature selection with linked data in social media. In SDM '12, pages 118­128. SIAM, 2012. [Tang et al., 2014] Jiliang Tang, Salem Alelyani, and Huan Liu. Feature selection for classification: A review. Data Classification: Algorithms and Applications. Editor: Charu Aggarwal, CRC Press In Chapman & Hall/CRC Data Mining and Knowledge Discovery Series, 2014. [Wang et al., 2015] Yilin Wang, Suhang Wang, Jiliang Tang, Huan Liu, and Baoxin Li. Unsupervised sentiment analysis for social media images. In Proceedings of the TwentyFourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 2378­2379, 2015. [Zha et al., 2002] Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, and Horst D. Simon. Spectral relaxation for kmeans clustering. In Proc. of NIPS'02, pages 1057­1064. 2002. [Zhou et al., 2011] Jiayu Zhou, Jianhui Chen, and Jieping Ye. Clustered multi-task learning via alternating structure optimization. In Proc. of NIPS'11, pages 702­710. 2011.

Table 4: Statistics of Attribute Prediction Image Datasets. Dataset aPascal/aYahoo AwA SUN # of images 15339 30475 14340 # of attributes 64 85 102 # of classes 32 50 611 # of features 2429 1200 1112 Table 5: Zero-shot learning accuracy on both real dataset. aYahoo AwA SUN MTAL 0.1834 0.2953 0.1842 DSVA 0.2052 0.3085 0.2010 Proposed 0.2262 0.3258 0.2133 puter Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1778­1785, June 2009. [Farhadi et al., 2010] A. Farhadi, I. Endres, and D. Hoiem. Attribute-centric recognition for cross-category generalization. In Proc. of CVPR'10, pages 2352­2359, June 2010. [Georghiades et al., 2001] A.S. Georghiades, P.N. Belhumeur, and D.J. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE Trans. Pattern Anal. Mach. Intell., 23(6):643­660, 2001. [Han et al., 2013] Yahong Han, Yi Yang, and Xiaofang Zhou. Co-regularized ensemble for feature selection. In IJCAI '13, pages 1380­1386, 2013. [Hull, 1994] J. J. Hull. A database for handwritten text recognition research. IEEE Trans. Pattern Anal. Mach. Intell., 16(5):550­554, May 1994. [Jacob et al., 2009] Laurent Jacob, Jean philippe Vert, and Francis R. Bach. Clustered multi-task learning: A convex formulation. In Proc. of NIPS'09, pages 745­752. 2009. [Jayaraman et al., 2014] D. Jayaraman, Fei Sha, and K. Grauman. Decorrelating semantic visual attributes by resisting the urge to share. In Proc. of CVPR'14, pages 1629­1636, June 2014. [Kovashka et al., 2012] A. Kovashka, D. Parikh, and K. Grauman. Whittlesearch: Image search with relative attribute feedback. In Proc. of CVPR'12, pages 2973­2980, June 2012. [Lampert et al., 2009] C.H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In Proc. of CVPR'09, pages 951­958, June 2009.

3344

A Structured Approach to Predicting Image Enhancement Parameters
Parag Shridhar Chandakkar Baoxin Li School of Computing, Informatics and Decision Systems Engineering, Arizona State University
{pchandak,baoxin.li}@asu.edu

Abstract
Social networking on mobile devices has become a commonplace of everyday life. In addition, photo capturing process has become trivial due to the advances in mobile imaging. Hence people capture a lot of photos everyday and they want them to be visually-attractive. This has given rise to automated, one-touch enhancement tools. However, the inability of those tools to provide personalized and contentadaptive enhancement has paved way for machine-learned methods to do the same. The existing typical machinelearned methods heuristically (e.g. k NN-search) predict the enhancement parameters for a new image by relating the image to a set of similar training images. These heuristic methods need constant interaction with the training images which makes the parameter prediction sub-optimal and computationally expensive at test time which is undesired. This paper presents a novel approach to predicting the enhancement parameters given a new image using only its features, without using any training images. We propose to model the interaction between the image features and its corresponding enhancement parameters using the matrix factorization (MF) principles. We also propose a way to integrate the image features in the MF formulation. We show that our approach outperforms heuristic approaches as well as recent approaches in MF and structured prediction on synthetic as well as real-world data of image enhancement.

touch enhancement tools. However, most of these tools are pre-defined image filters which lack the ability of doing content-adaptive or personalized enhancement. This has fueled the development of machine-learning based image enhancement algorithms. Many of the existing machine-learned image enhancement approaches first learn a model to predict a score quantifying the aesthetics of an image. Then given a new lowquality image2 , a widely-followed strategy to generate its enhanced version is as follows: · Generate a large number of candidate enhancement parameters3 by densely sampling the entire range of image parameters. Computational complexity may be reduced by applying heuristic criteria such as, densely sampling only near the parameter space of most similar training images. · Apply these candidate parameters to the original lowquality image to create a set of candidate images. · Perform feature extraction on every candidate image and then compute its aesthetic score by using the learned model. · Present the highest-scoring image to the user. There are two obvious drawbacks for the above strategy. First, generating and applying a large number of candidate parameters to create candidate images may be computationally prohibitive even for low-dimensional parameters. For example, a space of three parameters where each parameter  {0, ..., 9} produces 103 combinations. Second, even if creating candidate images is efficient, extracting features from them is always computationally intensive and is the bottleneck. Also, such heuristic methods need constant interaction with the training database (which might be
2 We call the images before enhancement as low-quality and those after enhancement as high-quality in the rest of this article. The process of enhancing a new image is called "the testing stage". 3 The brightness, saturation and contrast are referred to as "parameters" of an image in this article.

1. Introduction
The growth of social networking websites such as Facebook, Google+, Instagram etc. along with the ubiquitous mobile devices has enabled people to generate multimedia content at an exponentially increasing rate. Due to the easy-to-use photo-capturing process of mobile devices, people are sharing close to two billion photos per day on the social networking sites 1 . People want their photos to be visually-attractive which has given rise to automated, one1 http://www.kpcb.com/internet-trends

stored on a server) that makes the parameter prediction suboptimal. All these factors contribute to making the testing stage inefficient. Our approach assumes that a model quantifying image aesthetics has already been learned and instead focuses on finding a structured approach to enhancement parameter prediction. During training, we learn the inter-relationship between the low-quality images, its features, its parameters and the high-quality enhancement parameters. During the testing stage, we only have access to a new low-quality image, its features, parameters and the learned model and we have to predict the enhancement parameters. Using these enhancement parameters, we can generate the candidate images and select the best one using the learned model. The stringent requirement of not accessing the training images arises from real-world requirements. For example, to enhance a single image, it would be inefficient to establish a connection with the training database, generate hundreds of candidate images, perform feature extraction on them and then find the best image. The search space spanned by the parameters is huge. However, the enhancement parameters are not randomly scattered. Instead they depend on the parameters and features of the original low-quality image. Thus we hypothesize that the enhancement parameters should have a lowdimensional structure in another latent space. We employ an MF-based approach because it allows us express the enhancement parameters in terms of three latent variables, which model the interaction across: 1. the lowquality images 2. their corresponding enhancement parameters 3. the low-quality parameters. The latent factors are learned during inference by Gibbs sampling. Additionally, we need to incorporate the low-quality image features since the enhancement parameters also depend on the color composition of the image, which can be characterized by the features. The feature incorporation in this framework is achieved by representing the latent variable which models the interaction across these images as a linear combination of their features, by solving a convex 2,1 -norm problem. We review the related work on MF as well as image enhancement in the following section.

2. Related Work
Development of machine-learned image enhancement systems has recently been an active research area of immense practical significance. Various approaches have been put forward for this task. We review those works which improve the visual appearance of an image using automated techniques. To encourage research in this field, a database named MIT-Adobe FiveK containing corresponding low and high-quality images was proposed in [4]. The authors also proposed an algorithm to solve the problem of global tonal adjustment. The tone adjustment problem only

manipulates the luminance channel, where we manipulate saturation, brightness and contrast of an image. Content-based enhancement approaches have been developed in the past which try to improve a particular image region [2, 9]. These approaches require segmented regions which are to be enhanced. This itself may prove to be difficult. Approaches which work on pixels have also been developed using local scene descriptors. Firstly, similar images from the training set are retrieved. Then for each pixel in the input, similar pixels were retrieved from the training set, which were then used to improve the input pixel. Finally, Gaussian random fields maintain the spatial smoothness in the enhanced image. This approach does not consider the global information provided by the image and hence the enhancements may not be visually-appealing when viewed globally. In [8], a small number of image enhancements were collected from the users which were then used along with the additional training data. Two recent works involving training a ranking model from low and high-quality images are presented in [5, 25]. The authors of [25] create a data-set of 1300 corresponding low and high-quality image pairs along with a record of the intermediate enhancement steps. A ranking model trained on this type of data can quantify the aesthetics of an image. In [5], non-corresponding low and high-quality image pairs extracted from the Web are used to train a ranking model. Both of these approaches use k NN-search during the testing stage to create candidate images. After extracting features and ranking them, the best image is presented to the user. The task of enhancement parameter prediction could be related to the attribute prediction [17, 18, 11, 7]. However, the goal of the work on attribute prediction has been to predict relative strength of an attribute in the data sample (or image). We are not aware of any work which predicts parameters of an enhanced version of a low-quality image given only the parameters and features of that image. Since our approach is based on MF principles, we review the related recent work on MF. MF [19, 15, 20, 10, 24] is extensively used in recommender systems [12, 1, 13, 23, 14, 22, 21]. These systems predict the rating of an item for a user given his/her existing ratings for other items. For example, in Netflix problem, the task is to predict favorite movies based on user's existing ratings. MF-based solutions exploit following two key properties of such user-item rating matrix data. First, the preferred items by a user have some similarity to the other items preferred by that user (or by other similar users, if we have sufficient knowledge to build a similarity list of users). Second, though this matrix is very high-dimensional, the patterns in that that matrix are structured and hence they must lie on a low-dimensional manifold. For example, there are 17, 770 movies in Netflix data and ratings range from 1 - 5. Thus, there are 517770 rating combinations possible

per user and there are 480, 189 users. Therefore, the number of actual variations in the rating matrix should be a lot smaller than the number of all possible rating combinations. These variations could be modeled by latent variables lying near a low-dimensional manifold. This principle is formalized in [15] with probabilistic matrix factorization (PMF). It hypothesizes that the rating matrix can be decomposed into two latent matrices corresponding to user and movies. Their dot product should give the user-ratings. This works fairly well on a large-scale data-set such as Netflix. However, a lot of parameters have to be tuned. This requirement is alleviated in [20] by developing a Bayesian approach to MF (BPMF). BPMF has been extended for temporal data (BPTF) in [24]. MF is used in other domains such as computer vision to predict feature vectors of another viewpoint of a person given a feature for one viewpoint [6]. We adopt and modify BPTF since it allows us to model joint interaction across low-quality images, corresponding enhancement parameters and the low-quality parameters. In the next section, we detail our problem formulation and proposed approach.

in three-dimensional matrix R  RN ×(M +1)×K . We need ^ k = Rk + Rk or in turn just Rk . Rk deto predict R ij i ij ij i notes the k th parameter value (k  {1, . . . , K }) of the ith ^ k is the k th parameter value of j th low-quality image and R ij version of the ith image. Given a new nth low-quality imk age, we only need to predict Rnj  j = {1, . . . , M },  k . k During training, we can compute Rij from available k k ^ Rij and Rij . Following MF principles, we express R as an inner product of three latent factors, U  RD×N , V  RD×M and T  RD×K [20, 24]. D is the latent factor dimension. These latent factors should presumably model the underlying low-dimensional subspace corresponding to the low-quality images, its enhanced versions and its parameters. This can be formulated as:
D k Rij =< Ui , Vj , Tk > d=1

Udi Vdj Tdk ,

(1)

3. Problem Formulation and Proposed Approach
We have a training set consisting of N images {S1 , . . . , SN }4 . Parameters of all images are represented as A = {A1 , . . . , AN } where Ai  RK ×1  i  {1, . . . , N }. Each image has M enhanced versions and each version has the same size as that of its corresponding low-quality image. All versions corresponding to the ith image are repreM sented as {W1 i , . . . , Wi }. All versions are of higher quality as compared to its corresponding image. Parameters of all M versions of the ith image (also called as candidate pa1 M rameters) are represented as A = {Ai , . . . , Ai }, where j K ×1 Ai R  i, j . Features of all low-quality images are represented as F = {F1 , . . . , FN } where Fi  RL×1  i. In practice, we observe that M N, K < M . Our goal is to be able to predict the candidate parameters for all the versions of the ith image by only using the information provided by Ai and Fi . To the best of our knowledge, this is a novel problem of real significance that has not been addressed in the literature. We now explain our proposed approach. As mentioned before, our task is to predict the candidate parameters for all the enhanced versions of a low-quality image with the help of its parameters and features. The values for all the K parameters corresponding to N images and their N · M versions (total N + N · M ) can be stored
4 We use bold letters to denote matrices. Non-bold letters denote scalars/vectors which will either be clear from the context or will be mentioned. X i , Xi , XT , Xij and ||X||p denote row, column, transpose, entry at row i and column j of a matrix X and pth norm of matrix X respectively.

where Udi denotes the dth feature of the ith column of U. Presumably, as we increase D, the approximation erk ror Rij - < Ui , Vj , Tk > should decrease (or stay constant) if the prior parameters for latent factors U, V and T are chosen correctly. Following [20], we choose normal distribution (with precision ) for: 1. the conditional distribution R|(U, V, T) and 2. for prior distributions - p(U|U ), p(V|V ) and p(T|T ), where U = 1 -1 -1 (µU , - U ), V = (µV , V ), T = (µT , T ). U , V and T are hyper-parameters, and µ and  are the multivariate precision matrix and the mean respectively. Since the Wishart distribution is a conjugate prior for multivariate normal distribution (with precision matrix), we put Gaussian-Wishart priors on all hyper-parameters5 . We could find the latent factors U, V and T by doing inference through Gibbs sampling. It will sample each latent variable from its distribution, conditional on the values of other varik ables. The predictive distribution for Rij can be found by using Monte-Carlo approximation (explained later). However, it is important to note the following major differences in our problem when compared with the previous work on MF [20, 24]. In product or movie rating prediction problems, an average (non-personalized) recommendation may be provided to a user who has not provided any preferences (not necessarily constant for all users). In our case, each image may require a different kind of parameter adjustment to create its enhanced version and thus no "average" adjustment exists. As explained before, the adjustment should depend on the image's features, which characterize that image (e.g. bright vs. dull, muted vs. vibrant). In our problem, it is particularly difficult to get a good generalizing performance on the testing set as we shall see later. The loss in performance of existing approaches on the testing set can
5 For

details, see supplementary material on author's website.

be attributed to the different requirements for parameter adjustments for each image. Thus it becomes necessary to include the information obtained from image features into the formulation. We show that simply concatenating the parameters and features and applying MF techniques presented in [20, 24] does not provide good performance, possibly because they lie in different regions of the feature space. To overcome this problem, we observe that the conditional distribution of each Ui factorizes with respect to the individual samples. We propose to express U as a linear function of F by using a convex optimization scheme. We then integrate it into the inference algorithm to find out the latent factors. The linear transformation can be expressed as, Ui = FiT P + Q,  i  {1, . . . , N },
L×1 D ×1 D ×D

(2)

and Q could be (trivially) obtained by just setting each entry of P to a very small value and letting a column of Q  Ui (which makes Fi redundant). Secondly, while testing for a new image, we would have to devise a strategy to determine the suitable value for Q. For example, we could take the column of Q that corresponds to the nearest training image. This adds unnecessary complexity and reduces generalization. By making Q a row vector, we consider that it may be possible to arrive to the space of enhancement parameters by linearly transforming the low-quality image features with a constant offset. In other words, we want P to transform the features into a region in the latent space where all the other high-quality images lie and Q provides an offset to avoid over-fitting. This is a joint 2,1 -norm problem which can be solved efficiently by reformulating it as convex. We thus reformulate Equation 3 as follows, inspired by [16]:

where Fi  R , Ui  R ,P  R and Q  R1×D . Note that to carry out this decomposition, we have to set D = L. This is not a severe limitation since L is usually large ( 1000) and as we have mentioned before, increasing D should decrease the approximation error at the cost of increased computation. Henceforth we assume that our feature extraction process generates Fi  RD×1 . Also, note that large L does not mean that the latent space is no longer low-dimensional, because L is still smaller as compared to all the possible combinations of parameters (e.g. 517770 ). We propose an iterative convex optimization process to determine coefficients P and Q of Equation 2. We propose the following objective function to determine them: q
N

min
P,Q

1 

N

||FiT P + Q - UiT ||2 + ||P||2,1 +
i=1

 ||Q||2 (4) 

The
2,1 (X)

2,1 -Norm M

of a matrix X  RM ×N is defined as,

=
i=1

||Xi ||2 . Also, for a row vector Q, we have

||Q||2 = ||Q||2,1 . Thus Equation 4 can be further written as:

min
P,Q

1 T ||F P + 1N Q - UT ||2,1 + ||P||2,1 +  ||Q||2,1 , (5) 

min
P,Q i=1

||FiT P + Q - UiT ||2 +  ||P||2,1 +  ||Q||2

(3)
 where  =  and 1N is a column vector of ones  RN . Now, put FT P + 1N Q -  E = UT . Thus Equation 5 becomes:

The objective function tries to reconstruct U using P, Q and F while controlling the complexity of coefficients. Let's concentrate on the structure of P (by neglecting the effect of Q momentarily). The columns of P act as coefficients for Fi . Ideally, we would want the elements of Ui to be determined by a sparse set of features, which implies sparsity in the columns of P. To this end, we impose 2,1 norm on P, which gives us a block-row structure for P. Let us consider the structure of Q along with P. Equation 2 shows that different columns of Ui depend on different image features Fi . Also, we expect that a different set of columns of P will get activated (take on large values) for different Fi . We add an offset Q  R1×D for regularization. Thus the offset introduced by Q remains constant across all the images but changes for each Fi,j . Making Q to be a row vector also forces P to play a major role in Equation 3. This in turn increases the dependence of Ui on Fi . If we were to define Q as the same size of U (which would mean different offsets for each image as well as its features), it would pose two potential disadvantages. Firstly, optimal P

E,P,Q

min ||E||2,1 + ||P||2,1 +  ||Q||2,1 ,

E,P,Q

min

s.t. FT P + 1N Q -  E = UT ,    (6) E E T - 1 N P s.t. - IN F  1  P  = UT Q 2,1 Q 

Equation 6 is now in the form of: min ||X||2,1 s.t. ZX =
X

B and thus convex. It can be iteratively solved by an efficient algorithm mentioned in [16]. We set  = 0.1 and  = 3. Once we have expressed U as a function of F, we use Gibbs Sampling to determine the latent factors P, Q, V and T [20]. As mentioned before, the predictive distribution ^ k is given by a multidimenfor a new parameter value R ij sional integral as:

Algorithm 1 Gibbs Sampling for Latent Factor Estimation Initialize model parameters {P(1) , Q(1) , V(1) , T(1) }. T Obtain U(1) = FT P(1) + Q(1) For y = 1, 2, . . . , Y · Sample the hyper-parameters according to the derivations 6 : (y)  p((y) |U(y) , V(y) , T(y) , R), (y ) (y ) (y ) (y ) U  p(U |U(y) ), V  p(V |V(y) ), (y ) (y ) (y ) T  p(T |T ) · For i = 1, ..., N , sample the latent features of an image (in parallel): (y +1) (y ) Ui  p(Ui |V(y) , T(y) , U , (y) , R) Determine P(y+1) and Q(y+1) using the iterative T optimization by substituting B = U(y+1) . ^ (y+1) Reconstruct U(y+1) : U
T

sample U, V, T and obtain P and Q. Note that it is required in the algorithm to reconstruct U(y+1) at every iteration since there will always be a small reconstruction ^ (y+1) - U(y+1) ||. The error occurs because we error ||U force Q to be a row vector, which makes the exact recovery of U(y+1) difficult. The reconstructed error causes adjustment of V and T. Once we obtain the four latent factors, our task is to predict the parameter values for M enhanced versions having K parameters each. Suppose Ft is the feature vector of a new image, then the param^ k can be simply obtained by computing, eter values R tj ^ k =< F T P + Q, Vj , Tk >  j  {1, . . . , M } and R t tj k  {1, . . . , K }. If the parameter value predictions lie beyond a certain range then a thresholding scheme can be used based on the prior knowledge. For example, to constrain the predictions between [0, 1], a logistic function may be used.

= FT P(y+1) + Q(y+1)

4. Experiments
We conduct two experiments to show the effectiveness of our approach. We did the first one on a synthetic data and compared it with: 1. BPMF 2. our own discrete version of BPTF, called D-BPTF. 3. multivariate linear regression (MLR) 4. twin Gaussian processes (TGP) [3] 5. Weighted k NN regression (WKNN). For D-BPTF, we make minor modifications in the original BPTF approach [24] by removing the temporal constraints on their temporal variable, since there are no temporal constraints in our case. The inference for their temporal variable is then done in the exactly same manner as the other non-temporal variables. This gave us a marginal boost in the performance. For MLR, We use a standard multivariate regression by maximum likelihood estimation method. Specifically, we use MATLAB's mvregress command. TGP is a generic structured prediction method. It accounts correlation between both input and output resulting in improved performance as compared to MLR or WKNN. The WKNN approach predicts the test sample as a weighted combination of the k -nearest inputs. The first two algorithms do not allow features inclusion. For MLR, TGP and WKNN, we j concatenate Ai and Fi , and use it to predict Ai . Even for our approach, we concatenate Ai and sample feature to form Fi . The intuition behind this concatenation is that the enhancement parameters should be a function of input parameters as well along with the features. We did observe performance boost after concatenating the features and parameters. The second experiment demonstrates the usefulness of this approach in a real-world setting where we have to predict paramters of the enhanced versions of an image (then generate those versions by applying predicted parameters to the input low-quality image) without using any information about the versions. We compare our approach with the

· For j = 1, ..., M , sample the latent features of the enhanced versions (in parallel): Vj
(y +1)

^ (y+1) , T(y) , (y) , (y) , R)  p(Vj |U V

· For k = 1, ..., K , sample the latent features of parameter (in parallel): Tk
(y +1)

^ (y+1) , V(y+1) ,  , (y) , R)  p(Tk |U T

(y )

k ^ ij p(R |R) =

k ^ ij p(R |Ui , Vj , Tk , )·

p(U, V, T, , U , V , T |R)· d(U, V, T, , U , V , T ).

(7)

We resort to numerical approximation techniques to solve the above integral. To sample from the posterior, we use Markov Chain Monte Carlo (MCMC) sampling. We use the Gibbs sampling as our MCMC algorithm. We can approximate the integral by,
Y k ^ ij p(R | R )  y =1 k ^ ij p R |Ui , Vj , Tk , (y) (y ) (y ) (y )

(8) Here we draw Y samples and the value of Y is set by observing the validation error. The sampling from U, V and T is simple since we use conjugate priors for the hyperparameters. Also, a random variable can be sampled in parallel while fixing others which reduces the computational complexity. Algorithm 1 shows how to iteratively
6 See supplementary material on author's website for detailed derivations.

Training RMSE for simulation

Testing RMSE for simulation

0.6 0.5 0.4 0.3 0.2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Proposed_Simulation PMF_Simulation D-BPTF_Simulation

0.6 0.55 0.5 0.45 0.4 0.35 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Proposed_Simulation PMF_Simulation D-BPTF_Simulation

Training RMSE for image enhancement

Testing RMSE for image enhancement

0.18 0.15 0.12 0.09 0.06 0.03 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Proposed_Enhancement PMF_Enhancement D-BPTF_Enhancement

0.18 0.16 0.14 0.12 0.1 0.08 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Proposed_Enhancement PMF_Enhancement D-BPTF_Enhancement

260 195 130 65 0
Image 1 Image 2 Image 3 Image 4 Image 5 Best image

KNN

Proposed Approach

Figure 1. Top plots: train and test RMSEs for both the experiments. Bottom plot: First 5 sets of bars show votes for version 1 to 5 of kNN vs. the best image of our approach. The last set of bars shows votes for the best image of both approaches. Please zoom in for better viewing. See in color.

competing 5 algorithms in addition to k NN-search as it is also used in [26, 5]. We also analyzed the effect of Q in our solution by: removing Q i.e. U = FT P.

linearly related to Ak,i  k,  m  {1, . . . , 4} and Fi . For example, Ak,i,m =  r1 1,i +
A 1 1+e-r2 A2,i
3 + Ar 3,i + (1 -

4.1. Data set description and experiment protocol
The synthetic data is carefully constructed by keeping the following task in mind. We are given a training set consisting of: 1. F  RD×N ; 2. A  RK ×N ; and 3. only parameters of M versions for each input sample A  RK ×N ×M . Our aim is to predict parameters for a set of M versions given a new Fi and Ai . In real-world problems, A and F are interdependent. The parameters of M versions are dependent on both A, F. Hence we construct the synthetic data as follows. Firstly, we generate a set of 3-D input parameters - A drawn from a uniform distribution [0, 1]. Then we generate a 50-D feature set F, where each element of Fi is related to all Ak,i  i = {1, . . . , 103 }, k = {1, 2, 3} by a nonlinA ear function. For example, Fj,i = r1 1,i + 1+e-1 r2 A2,i + 3 Ar ,  j  { 1 , . . . , 50 } and r , r , r are random numbers. 1 2 3 3,i The parameters of enhanced versions, Ak,i,m , are also non-

 ) · ||Fi ||2 . The contribution of Fi is decided by  . We perform 3-fold cross-validation. We predict the values of A in the test set (disjoint from training) using corresponding A and F. RMSE is computed between the predicted and actual A . The MIT-Adobe FiveK data-set contains 5000 highquality photographs taken with SLR cameras. Each photo is then enhanced by five experts to produce 5 enhanced versions. We extract average saturation, brightness and contrast for every image, which are parameters  A. We also extract 1274-D color histogram with 26 bins for hue, 7 bins each for saturation and value. We also calculate localized features of 144-D each for contrast, brightness and saturation. Finally, we append average saturation, brightness and contrast of the input low-quality image, which are its parameters. Thus we get a 1709-D (= 1274 + 3 × 144 + 3) representation for every image  F. We train using 4000 images and use 500 images each for validation and testing. We predict parameters for 5 versions in a 3 × 5 matrix for

Figure 2. Left: Original image, Middle: enhanced image by kNN and Right: proposed approach 7 . See in color.

each image in the testing set. An entry Ai,j denotes the value for ith parameter of j th enhanced version. To enable comparison with the expert-enhanced images of the data-set, we also compute parameters for 5 enhanced versions for each image, which we treat as ground-truth. We evaluate this experiment in two ways. Firstly, we calculate RMSE between the parameters of 5 expert-enhanced photos and the parameters of the predicted versions using five aforementioned algorithms. Secondly, we conduct a subjective test under standard test settings (constant lighting, position, distance from the screen). In this case, we compare our approach with the popular k NN-search-based approach. It first finds the nearest original image in the training set to the testing image - im - and then applies the same parameter transformation to im to generate 5 version. In our approach, we predict the parameters for enhanced versions using the proposed formulation. We threshold the parameter values as: Ak,i,m = min(Ak,i,m , Ak,i + k Ak,i ), Ak,i,m = max(Ak,i,m , Ak,i - k Ak,i ),

(9)

trast are:  = {0.4, 0.4, 0.05},  = {0.3, 0.3, 0.01}. As mentioned before, the clipping scheme in our formulation should be set using prior knowledge. Here, we know that the enhanced images usually have a larger increase (as compared to decrease) associated with their parameters. Also, changing contrast by a very small amount affects the image greatly. The predicted parameters are applied to the input image to obtain its enhanced versions. The procedure is the same for both the approaches and is as follows. First we change contrast till the difference between the updated and the predicted contrast is marginal. We update contrast first since changing it updates both brightness and saturation. We then update brightness and saturation till they come significantly closer to their corresponding predicted values. This gives us 5 versions for both approaches. To allow comparisons within a reasonable amount of time, we use a pre-trained ranking weight vector w (from [5]) to select the best image of our approach (im-proposed) and k NN-approach (imk NN ). For the subjective test, people are told to compare improposed with the 5 enhanced versions of k NN-approach as well as with im-k NN. Thus for every input image, people
7 See supplementary on author's website for additional full-resolution results.

where  and  are multipliers for the k th parameter. In our case, the multipliers for saturation, brightness and con-

perform 6 comparisons. The image order was randomized. We conducted the test with 11 people and 35 input images. Thus every person compared 210 pairs of images. They were told to choose a visually-appealing image. The third option of simultaneously preferring both images was also provided. This option has no effect on cumulative votes.

Table 1. Effect of varying  and 

4.2. Results
The parameters for the synthetic data were more accurately predicted by our approach than BPMF, D-BPTF, MLR, TGP and WKNN. It is worth noting that though the training error continues to decrease for our approach, BPMF and D-BPTF, the testing error starts increasing after only 5 and 8 iterations for BPMF and D-BPTF, respectively. However, testing error in our approach decreases rapidly for 4 iterations and then it decreases very slowly for the next 12, as shown in Fig. 1. The RMSE on test set for BPMF, D-BPTF, MLR, TGP, WKNN and the proposed approach is 0.4933, 0.4865, 0.6293, 0.4947, 0.8014 and 0.3644. The numbers show that our approach is able to effectively use the additional information provided by features and the interaction between A, F and all versions to provide better prediction. On the other hand, BPMF and D-BPTF start over-fitting quickly due to lack of sample feature information while MLR and WKNN fail to model the complex interaction between variables. TGP performs better because of its ability to capture correlations between input and output. However, TGP still treats each version independently and thus its performance still falls short of our approach. In the second experiment, the RMSE for BPMF, D-BPTF, MLR, TGP, WKNN and our approach is 0.1251, 0.1328, 1.2420, 0.1268, 0.1518 and 0.0820 respectively. The testing error starts increasing after only 3 and 5 iterations for BPMF and D-BPTF, respectively. It is important to note that we do not use the clipping scheme mentioned in Equation 9 in order to do a fair comparison of RMSEs between all the five approaches and the proposed appraoch. For the subjective evaluation, Fig. 1 shows cumulative votes obtained for ours and the k NN-based approach for comparison between 5 images chosen by k NN and the best image chosen by our approach. Fig. 1 also shows votes obtained for the best images chosen by both approaches. Fig. 2 shows two input images enhanced by both the approaches. The top row of Fig. 2 shows that k NN reduces the saturation while increasing the brightness. Our approach balances both of them to obtain a more appealing image. In the bottom row, however, both approaches fail to produce aesthetic images as images become too bright. It is probably due to the portion of the sky in the input image. For both the images, most people prefer images enhanced by our approach. Computationally, our approach is superior than k NN. Complexity of our approach is independent of data-set size at testing time whereas k NN searches the

Parameter setting  = 0.001,  = 6  = 0.01,  = 6  = 0.02,  = 0.1  = 0.2,  = 0.05  = 0.8,  = 0.05  = 0.1,  = 0.3  = 0.1,  = 0.8  = 0.1,  = 2

RMSE (lower the better) 0.3162 0.0962 0.0907 0.0930 0.0872 0.0820 0.0821 0.0820

entire data-set for the closet image and then applies its parameters. We reconstructed U = FT P and observed performance drop as it overfits. We get RMSE of 0.9305 and 0.3762 on enhancement and simulation data, respectively. We believe the real-world enhancement data has correlations naturally embedded in it unlike in synthetic data. Thus the performance drop is drastic in case of enhancement since the problem of recovering P only from U and F is ill-posed. We also analyzed the effect of varying  and  . Since our approach uses Bayesian probabilistic inference, small variations in  and  do not significantly affect the performance. Table 1 lists the various parameter settings and its effect on the performance of the second experiment (i.e. image enhancement):

5. Conclusion
In this paper, we introduced a novel problem of predicting parameters of enhanced versions for a low-quality image by using its parameters and features. We developed an MF-inspired approach to solve this problem. We showed that by modeling the interactions across low-quality images, its parameters and its versions, we can outperform five state-of-art models in structured prediction and MF. We proposed inclusion of feature information into our formulation through a convex 2,1 -norm minimization, which works in an iterative fashion and is efficient. Thus our approach utilizes information which helps characterize input image. This leads to better generalization and prediction performance. Since other approaches do not model interdependence between image features and parameters of their corresponding enhanced versions, they start over-fitting quickly and produce an inferior prediction performance on the test set. Experiments on synthetic and real data demonstrated superiority of our approach over other state-of-art methods. Acknowledgement: The work was supported in part by an ARO grant (#W911NF1410371) and an ONR grant (#N00014-15-1-2344). Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of ARO or ONR.

References
[1] L. Baltrunas, B. Ludwig, and F. Ricci. Matrix factorization techniques for context aware recommendation. In Proceedings of the fifth ACM conference on Recommender systems, pages 301­304. ACM, 2011. [2] F. Berthouzoz, W. Li, M. Dontcheva, and M. Agrawala. A framework for content-adaptive photo manipulation macros: Application to face, landscape, and global manipulations. ACM Trans. Graph., 30(5):120, 2011. [3] L. Bo and C. Sminchisescu. Twin gaussian processes for structured prediction. International Journal of Computer Vision, 87(1-2):28­52, 2010. [4] V. Bychkovsky, S. Paris, E. Chan, and F. Durand. Learning photographic global tonal adjustment with a database of input/output image pairs. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 97­ 104. IEEE, 2011. [5] P. S. Chandakkar, Q. Tian, and B. Li. Relative learning from web images for content-adaptive enhancement. In Multimedia and Expo (ICME), 2015 IEEE International Conference on, pages 1­6. IEEE, 2015. [6] C.-Y. Chen and K. Grauman. Inferring unseen views of people. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2011­2018. IEEE, 2014. [7] L. Chen, Q. Zhang, and B. Li. Predicting multiple attributes via relative multi-task learning. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 1027­1034. IEEE, 2014. [8] S. B. Kang, A. Kapoor, and D. Lischinski. Personalization of image enhancement. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 1799­1806. IEEE, 2010. [9] L. Kaufman, D. Lischinski, and M. Werman. Content-aware automatic photo enhancement. In Computer Graphics Forum, volume 31, pages 2528­2540. Wiley Online Library, 2012. [10] N. D. Lawrence and R. Urtasun. Non-linear matrix factorization with gaussian processes. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 601­608. ACM, 2009. [11] S. Li, S. Shan, and X. Chen. Relative forest for attribute prediction. In Computer Vision­ACCV 2012, pages 316­327. Springer, 2013. [12] H. Ma, H. Yang, M. R. Lyu, and I. King. Sorec: social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 931­940. ACM, 2008. [13] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King. Recommender systems with social regularization. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 287­296. ACM, 2011. [14] B. Marlin, R. S. Zemel, S. Roweis, and M. Slaney. Collaborative filtering and the missing at random assumption. arXiv preprint arXiv:1206.5267, 2012. [15] A. Mnih and R. Salakhutdinov. Probabilistic matrix factorization. In Advances in neural information processing systems, pages 1257­1264, 2007.

[16] F. Nie, H. Huang, X. Cai, and C. H. Ding. Efficient and robust feature selection via joint 2,1 -norms minimization. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1813­1821. Curran Associates, Inc., 2010. [17] D. Parikh and K. Grauman. Relative attributes. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 503­510. IEEE, 2011. [18] D. Parikh, A. Kovashka, A. Parkash, and K. Grauman. Relative attributes for enhanced human-machine communication. In AAAI, 2012. [19] J. D. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pages 713­719. ACM, 2005. [20] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo. In Proceedings of the 25th international conference on Machine learning, pages 880­887. ACM, 2008. [21] Y. Shi, M. Larson, and A. Hanjalic. Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges. ACM Computing Surveys (CSUR), 47(1):3, 2014. [22] Q. Song, J. Cheng, and H. Lu. Incremental matrix factorization via feature space re-learning for recommender system. In Proceedings of the 9th ACM Conference on Recommender Systems, pages 277­280. ACM, 2015. [23] S. Wang, J. Tang, Y. Wang, and H. Liu. Exploring implicit hierarchical structures for recommender systems. In International Joint Conference on Artificial Intelligence (IJCAI). IJCAI, 2015. [24] L. Xiong, X. Chen, T.-K. Huang, J. G. Schneider, and J. G. Carbonell. Temporal collaborative filtering with bayesian probabilistic tensor factorization. In SDM, volume 10, pages 211­222. SIAM, 2010. [25] J. Yan, S. Lin, S. B. Kang, and X. Tang. A learning-to-rank approach for image color enhancement. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2987­2994. IEEE, 2014. [26] J. Yan, S. Lin, S. B. Kang, and X. Tang. A learning-to-rank approach for image color enhancement. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2987­2994. IEEE, 2014.

Efficient Unsupervised Abnormal Crowd Activity Detection Based on a Spatiotemporal Saliency Detector
Yilin Wang Arizona State University Tempe, Arizona
ywang370@asu.edu

Qiang Zhang Samsung Pasadena, California
zhangtemplar@gmail.com

Baoxin Li Arizona State University Tempe, Arizona
baoxin.li@asu.edu

Abstract
Approaches to abnormality detection in crowded scene largely rely on supervised methods using discriminative models. In this paper, we presents a novel and efficient unsupervised learning method for video analysis. We start from visual saliency, which has been used in several vision tasks, e.g., image classification, object detection, and foreground segmentation. To detect saliency regions in video sequences, we propose a new approach for detecting spatiotemporal visual saliency based on the phase spectrum of the videos, which is easy to implement and computationally efficient. With the proposed algorithm, we also study how the spatiotemporal saliency can be used in two important vision tasks, saliency prediction and abnormality detection. The proposed algorithm is evaluated on several benchmark datasets with comparison to the state-of-the-art methods from the literature. The experiments demonstrate the effectiveness of the proposed approach to spatiotemporal visual saliency detection and its application to the above vision tasks.

1. Introduction
Automatic abnormality detection for online multimedia content has been an active area in recent years due to it potential applications for crowded surveillance[30], social media behavior monitoring[35, 37, 34]and event retrieval[7]. Early approaches [30, 8, 24] focus on either generating discriminative model for semantic indexing the video or decompose it into semantic parts. These approaches, which rely on frame-based video labels, have been shown effective on certain datasets. Unfortunately, frame-based labels are in general hard to obtain. Especially, for massive YouTube videos, it is too labor-and time-intensive to obtain labeled sets large enough for robust training. Thus, the unsupervised approach would be more desirable. This paper studies unsupervised video abnormality detection. 1

Typically, video features such as optical flow, motion trajectory, and spatialtemporal interest points, lack of semantic meanings required by the abnormality detection. In the supervised case, label information could be directly utilized to build the connection between video features and video labels. Thus, unsupervised video abnormality detection is inherently more challenging than its supervised counterpart. In this paper, we start from visual saliency, which has attracted a lot of interests in the vision community in recent years. One early work that is widely known is the approach by Itti et al. [19]. Since then, a lot of different models have been proposed for computing visual saliency. Moreover, visual saliency often depends on not only a static scene but also the changes in the scene. To this end, spatiotemporal saliency has been proposed, which tries to capture regions attracting visual attention in the spatiotemporal domain. Spatiotemporal saliency has been applied to vision tasks such as video summarization, human-computer interaction [18], and video compression. However, these approaches only focus on the video objects or foreground, but ignore irregular motion pattern changes, which is an essential part in abnormality event detection. On the other hand, the saliency information can be regarded as an abstract of the video frame (image) [32], which may be exploited to enable unsupervised abnormality detection. How to achieve this is the objective of our approach. In this paper, we study unsupervised video abnormality detection based on a spatiotemporal saliency detector by investigating two related challenges: (1) how to model the interaction between video content and spatiotemporal saliency systematically so as to augment video analysis using the information from saliency detection, and (2) how to use spatiotemporal saliency information to enable unsupervised video analysis. In addressing these two challenges, we propose a novel spatiotemporal visual saliency detector for video content analysis, based on the phase information of the video. With the saliency map computed using the proposed method, we analyze how it can be used for two fundamental vision tasks, namely saliency detec-

tion and abnormal event detection. We evaluate the performance of the proposed algorithm using several widely used datasets, with the comparison to the state-of-the-art in the literature. Our main contribution can be summarized as following: (1) A parameter free approach to enabling unsupervised video event detection. Neither normal examples nor abnormal examples are required for abnormality detection; (2) A novel and efficient framework for spatiotemporal saliency detection, which captures the global motion information and can be used to model complex activities. We demonstrate the complexity of the proposed algorithm is only O(N log N ), where N is the size of the input; and (3) Comprehensive comparisons and evaluations using several benchmark datasets on saliency detection and abnormality detection are used to demonstrate that the effectiveness of the proposed approach, suggesting its potential application for future video analysis tasks.

Figure 1. The spectrum analysis for normal videos. The first column is sample frame, the second column shows sampled video points in frequency domain. Please see the figures in color print.

2. The Proposed Method
2.1. Spectrum Analysis for Saliency Detection
There has been several explanations for why spectral domain based approach is able to detect saliency region from the image. For example, In [3], it has been shown that human visual system will select a subset of objects to focus. In other words, an attention competition exists among objects in the image. Only a small portion of objects, which are more distinctive, will be popped out, and rest of objects, which are usually in a uniform or common patterns, are suppressed. The spectral magnitude measures the total response of cells tuned to the specific frequency and orientation. According to lateral surround inhibition, similarly tuned cells will be suppressed depending on their total response, which can be modeled by dividing its spectral by the spectral magnitude [36]. [13] provided another explanation from sparse representation, which states that, if the foreground is sparse in spatial domain and background is sparse in DCT domain (e.g., periodic textures), the spectral domain based approach will highlight the foreground region in the saliency map. In a word, given an image (or 2D signal), f (x, y ), the saliency map can be calculated as: S (x, y ) = F -1 [h(m, n)  A(m, n) · e-iP (m,n) ] (1)

where h is a high pass filter and A, P represent amplitude and phase of Fourier transform F .

get object can be easily identified by human subjects. In this paper, we model the video abnormality detection as a spatiotemporal saliency detection problem, where normal video frame is regarded as non-salient and abnormality is perceptually salient. In [23, 13], it has been shown that, for natural image, the amplitude spectrum of background (non-salient region) has higher value at lower frequency. Essentially, our observation demonstrates that, in the temporal domains, the normal video frames, where object can be modeled in a uniform motion pattern , will have higher amplitude in lower frequencies than higher frequencies. Thus, the phase information of temporal domain can be used for abnormality modeling. We show that one can exploit such informative observation through spectrum analysis. In order to demonstrate the property of spectrum of normal videos, we generate two synthetic videos. The first video contains a uniform background (black) and second video with a moving object (33 by 33) which has its value uniformly distributed (white). Moreover, the motion trajectory of the object is followed as the red circle with same speed (we call it regular motion) in Figure 1. Specifically, two points, which are sampled from background and motion trajectory respectively, are further plotted in the frequency domain through the time period. From Figure 1, we can interpret the result as following: 1) if no global motion in the video, the background (even with dynamic scenes, we show later) has higher response in the lower frequency domain. 2)Since the result of the spectrum obeys the symmetry [2], the amplitude from the points within regular motion object also trend to be higher in lower frequencies.

2.2. Spectrum Analysis for Normal Videos
Spectrum analysis in spatiotemporal data, e.g., videos, is still in its infancy. In [16], the author studied how motion patterns contribute to saliency. It demonstrate that by setting the target object having different flicker rate, moving direction or motion velocity from other objects, the tar-

2.3. Modeling Video Abnormality via Saliency Detection
Based on the spectrum analysis of normal video, we have observed some potential properties. Then two research questions arises: 1) How to model the video abnormality only using the information from amplitude spectrum? 2)

How to automatically find the abnormality in a video? Answering these questions leads us to further analyze the amplitude spectrum with phase information. Given a signal f (x, y, t) it is first transformed to the frequency domain F f (x, y, t) -  F (m, n, k ), with the amplitude A(i, j, k ) = |F (m, n, k )| and phase P (m, n, k ) = angle(F (m, n, k ). Based on the Fourier Transform and inverse Fourier Transform, we have: f (x, y, t) = F -1 [F (m, n, k )] = F -1 A(m, n, k ) · ei·P (m,n,k)

2.4. Analysis
In this section, we provide evidence that, for a foreground object with irregular motion pattern, the proposed spatiotemporal saliency detector can approximately obtain its location in a video based on Parseval's theorem [2]. Parseval's theorem: The energy in u(t) equals the en+ ergy in U (f ), where u(t) = f =- U (f ) · ei2f t df Now, given a 3D signal and reconstruct it with only phase information: F (m, n, k ) ] |F (m, n, k )|

(2) S (x, y, t) = g  F -1 [ =gF
-1

Based on Parseval's theorem, we know the summation across all the dimensions of f (x, y, t) is equal to the summation across all the frequency component in the frequency domain. From the Eq 6, we can see that when only using S (x, y, t) = g  F -1 [h(k )  A(m, n, k ) · ei·P (m,n,k) ] (3) the phase information it is equal to replace the amplitude spectrum A(t) to a cube. In other words, all of the elements where h(k ) is the high-pass filter along the temporal diwhich have non-zero value in magnitude spectrum are set to rection in the frequency domain, and g is a low pass filter one. The region with repeat (regular) motion pattern creates in spatiotemporal domain, e.g., 3D Gaussian filter, which a high peak in the magnitude spectrum (Figure 1) is supsmooths the result. However, Eq 3 only considers the tempressed; while the region with salient (irregular motion patporal information for video abnormality detection, which tern instead corresponds to the spread-out magnitude specmay involve the noise from background if the video contrum will pop-out. Additionally, based on the proposition in tains global motion. To alleviate this issue, we further in[13], we can easily extend the sparse condition for saliency corporate the spatial saliency information to refine the dedetection in the spatial domain to the spatiotemporal dotection results. The improved model is described as below: main, which means the proposed method is also bounded with the ratio of salient region and non salient region. Due -1 i·P (m,n,k) to the space limits, we omit the proof. S (x, y, t) =g  F [h(k )  l(m, n)  (A(m, n, k ) · e )] To verify the correctness of the proposed model, we gen(4) erate one synthetic video to test the abnormality detection. where l(m, n) is the high pass filter along the spatial direcThe video contains a dynamic background with two movtion in the frequency domain. In frequency domain, setting ing squares with same texture. One of squares follows the the spectrum magnitude to uniform can achieve similar efred circle and moves steadily (we call normal object), anfect of high pass filter. In order to reduce the computation other moving square moving randomly (we call abnormal cost, we further relax Eq 4 (further analysis shown in Sec object). The motion trajectory of these square is defined as 2.4 ): following: S (x, y, t) = g  F -1 [ei·P (m,n,t) ] (5) F (m, n, k ) x(t) 128 + 64cos( t = g  F -1 ( ) 32 ) 1 (t) = = t |F (m, n, k )| y (t) 128 + 64sin( 32 ) t Eq 5 actually adopts the phase information of a video for x(t) 64 + 32cos( 32 ) +  2 (t) = = t saliency detection, it can be easily paralleled. The Fourier y (t) )+ 64 + 32sin( 32 transform for multiple dimensional data can be computed as a sequence of 1D Fourier transform on each coordinate of the data. Thus the computation cost of the proposed where  is a random variable uniformly draw from [0, 128]. spatiotemporal saliency detector is O(M N T log (M N T )) We view the object with trajectory 1 moving regularly. For when the input data size is X  RM ×N ××T . If the the Gauss filter used to smooth  the saliency map, we set the data has P feature channel, then the computational cost is standard deviation as 0.006 N 2 + M 2 and te filter size as 1 + 6 , where N × M is the size of each frame. O(P M N T log (M N T )).

In order to extract the video abnormality, and inspired by the saliency detection, we perform a high pass filtering on the frequency domain in temporal dimension, which will suppress the signals from normal videos. Then we model the abnormality in a saliency fashion:

(6) ]

[1(m, n, k ) · e

i·P (m,n,k)

In Fig. 2, we show some sample frames of the video (top), the results from the proposed method (middle) with the comparison to the results of the method proposed in [11] (bottom), where the frame differences of two adjacent frames are used as temporal information. From the figure, we can find the proposed method highlight the moving objects over the dynamic background; in addition, the object moving "irregularly" (i.e., with trajectory 2 ) gets higher values in the saliency map than the other object (the one with trajectory 1 ) does. In contrast, the method proposed in [11] not only has problem in segmenting the moving objects from changing background, but also can't discriminate the one moving "irregularly" from the one moving regularly. One explanation could be that simply the frame differences of two adjacent frames can't distinguish the object moving irregularly from the object moving reguarly. This reveals the potential of the proposed method to detect irregular events from the video (or abnormal events), as detailed in the next section.

abnormality detection. The performance of the proposed methods are compared with the existing methods, some of which are state-of-the-art methods.

3.1. Simulation Experiment
In this section, we evaluate the proposed method on synthetic data. In [16], how three properties of motion, namely flicker, direction and velocity, contribute to the saliency was studied. In this section, we generate the synthetic data according to the their protocol. The input data is a short clip where the resolution is 174 × 174 with 400 frames at the frame rate of 60 frames per second. We put 36 objects of size 5 × 13 in a 6 × 6 grid and a target object is randomly selected out of those 36 objects. All the objects are allowed to move within a 29 × 29 region centered at their initial position (and warped back, if they move out of this region). The video is black-and-white. We design the following three experiments: 1. Flicker: we set the objects on-off at a specified rate and the target object at a different rate from the other 35 objects; 2. Direction: we set the objects moving in a specified direction and the target object in a different direction. The velocity of all the objects are the same; 3. Velocity: we set the objects moving in a specified velocity and the target object moves in a different velocity. The moving direction of the all the objects are the same. All the other parameters are the same as used in [16]. According to [16], the target object could be easily identified by human subjects, when its motion property (e.g., flicker rate, moving direction, velocity) is different from the other objects. We also include some "blind" trials, where the target object has the same motion property as the other 35 objects. In this case, the target object can't be identified by the human subjects, i.e., there is no salient region. We apply the proposed method to the input data. For comparison, we also evaluate the method proposed in [4] and [13]. We use the area under receiver operating characteristic curve as the performance metric. The ground truth mask is generated according to the location of the target object. The experiment result is shown in Figure 3. From the experiment results, we can find that the proposed method detects the salient region much more accurately than [4] and [13] in all except the "blind" trials, which should be as lower as possible in terms of abnormality. However, [4] and [13] don't survive in those "blind" trials. Surprisingly, [4] and [13] achieves quite similar performances, though [4] was supposed to achieve better result as it include the differences of two adjacent frames as motion (temporal) information.

Figure 2. Top: some sample frames from the input video, where the red circle indicates the trajectory 1 ; middle: the corresponding saliency maps; and bottom: the saliency map computed with method described in [11]. For the saliency map, the warm color indicates high value and cold color for low value. The video can be found in the supplementary material. Please see the figures in color print.

3. Experiment
Since the proposed method is based on saliency detection, to verify the correctness in saliency detection, we first evaluate the proposed method on both syntheic data (Sec. 3.1) and two real-world video datasets (Sec. 3.2), CRCNSORIG and DIEM, for saliency detection. Then we evaluate the proposed method on several benchmark datasets on

Flicker
0.8 0.75 0.7 0.65 0.6 0.55 0.5 0.45 0.4 100 150 200 250 300 350 400 450 Absolute differences of the flicker rate 0.2 500 -10 0 0.6 0.7

Direction
0.7 0.65 0.6 0.55 0.5 0.4

Velocity

Proposed Bian[15] Hou[8]

0.5

0.3

Proposed Bian[15] Hou[8]
10 20 30 40 Absolute differences of the direction 50

0.45 0.4 0.35 60 -0.1 0

Proposed Bian[15] Hou[8]
0.1 0.2 0.3 0.4 Absolute differences of the velocity 0.5 0.6

Figure 3. The AUC on the synthetic data for the proposed method and two existing methods. For "Direction" and "Velocity", we also include some "blind" trials (X-axis has value 0), where the target object has exactly the same motion property as the other 35 objects. In those trials, the target object can't be identified by human subjects, i.e., there is no salient object [16].

Blind

Flicker

Direction

Velocity

Figure 4. Some visual sample of the synthetic data for different experiments.

3.2. Spatiotemporal Saliency Detection
In previous section, we test the proposed spatiotemporal saliency detector on synthetic videos, with the comparison to two other saliency detectors, where the proposed detector shows better performances in capturing the temporal information. In this section, we evaluate the proposed spatiotemporal saliency detector on two challenging video datasets for saliency evaluation, CRCNS-ORIG [20] and DIEM [29]. For this experiment, we first convert each frame into the LAB color space, then compute the spatiotemporal saliency in each channel independently and the final spatiotemporal saliency is the summation of the saliency maps of all three channels. CRCNS-ORIG includes 50 video clips from different genres, including TV programs, outdoor scenes and video games. Each clip is 6-second to 90-second long at 30 frames per second. The eye fixation data is captured from eight subjects with normal or correct-normal vision. In our experiment, we downsample the video from 640 × 480 to 160 × 120 and keep the frame rate untouched, then apply the our spatiotemporal saliency detector. To measure the performance, we compute the area under curve (AUC) and Fmeasure (harmonic mean of true positive rate and false positive rate). The experiment result is shown in Fig. 5, where the area under curve (AUC) is 0.6639 and F-measure is

0.1926. Tab. 1 compares the result of the proposed method with some state-of-art methods on CRCNS-ORIG, which indicates that our method outperforms them by at least 0.06 regarding AUC.
1 0.9

True Postive Rate

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.2 0.4

CRCNS AUC=0.6729 DIEM AUC=0.6715
0.6 0.8 1

False Postive Rate
Figure 5. The receiver operating characteristic curve of the propose method in CRCNS-ORIG dataset and DIEM dataset. The area under the curve is 0.6639 and 0.6896 accordingly.

DIEM dataset collects data of where people look during dynamic scene viewing such as film trailers, music videos, or advertisements. It currently consists of data from over

Method AWS [10] HouNIPS [15] Bian [4] IO SR [14] Torralba [33] Judd [21] Marat [27] Rarity-G [26] CIOFM [17] Proposed

AUC 0.6000 0.5967 0.5950 0.5950 0.5867 0.5833 0.5833 0.5833 0.5767 0.5767 0.6639

Method AWS [10] Bian [4] Marat [27] Judd [21] AIM [6] HouNIPS [15] Torralba [33] GBVS [12] SR [14] CIO [17] Proposed

AUC 0.5770 0.5730 0.5730 0.5700 0.5680 0.5630 0.5840 0.5620 0.5610 0.5560 0.6896

Method Optical flow [28] Social force [28] NN [9] Sparse reconstruction [9] Proposed

AUC 0.84 0.96 0.93 0.978 0.9378

Table 2. The result on UMN dataset. Note, we have cropped out the region which contains the text "abnormal", and results in frame resolution 214 × 320. Please note that, most of those methods, except the proposed one, need a training stage.

3.3. Abnormality Detection
In this section, we show how can we utilize the proposed spatiotemporal saliency detector to detect abnormality from the video.

Table 1. The result the proposed method compared with the results of the top ten existing methods on CRCNS dataset (left) and DIEM dataset (right) according to [5]. From this table, we can find that the propose method gets obvious better performances than the state-of-arts on both two datasets.

250 participants watching 85 different videos. Each video in DIEM dataset includes 1000 to 6000 frames at 30 frames per second. Similarly as CRCNS, we downsample the video to 1/4 (e.g., from 1280 × 720 to 320 × 180) while maintaining the aspect ratio and frame rate. We observe that each video in DIEM dataset is consisted of a sequence of short clips, where each clip has 30 to 100 frames. To properly detect the saliency from those videos, we apply the window function to our spatiotemporal saliency detector, where the size of the window (along temporal direction) is 60-frame. The experiment result is shown in Fig. 5 and Tab. 1, where the AUC is 0.6896 and F-measure is 0.35. From the table, we can find that the proposed method outperforms the stateof-arts by over 10%. To compare the performances of combining four visual cues via QFT and performances via summation of saliency maps of each visual cues, we design the following experiment. We run 1000 simulations and in each simulation we generate a r × c × 4 array, where r and c is a random number between [1, 1000] and 4 is the number of feature channels. We compute the saliency map with different methods then measures their similarities via cross-correlation, where 0.91 is reported for QFT and FFT. After smoothing the saliency map with a Gaussian kernel, the correlation is over 0.998. For natural image, we could expect an even higher correlation. This suggests that, we can compute the saliency map for each visual cue independently and then add them together, which will yield quite similar result by using quaternion Fourier transform. In addition, the proposed method other than QFT provides more flexibility, e.g., we can assign different weights to the visual cues as [21]. We also include the AUC of the proposed method for each video from the CRCNS-ORIG (Figure 6) and DIEM dataset (Figure 7).

Method Social force [28] MPPCA [22] MDT [25] Adam [1] Reddy [31] Sparse [9] Proposed

Ped1 31% 40% 25% 38% 22.5% 19% 27%

Ped2 42% 30% 25% 42% 20% N.A. 19%

Overall 37% 35% 25% 40% 21.25% N.A. 23%

Table 3. The frame level EER (the lower the better) for UCSD dataset. Please note that, most of those methods, except the proposed one, need a training stage. From the result, we can found that the proposed method, even without traing stage or training data, can still outperform social force, MPPCA.

For abnormality detection, we start with computing the saliency map for the input video as described above. The regions containing abnormalities can be detected by founding the region where the saliency value is above a threshold. Then the saliency score of a frame is computed as the average of saliency value of the pixels in that frame, i.e., s(t) = 1 NM X(i, j, t)
i j

(7)

where s(t) is the saliency score of tth frame, N × M is the size of one frame, i, j , t are row, column and frame index of the 3D saliency map accordingly. The frame with high saliency score would contain abnormality. To show the proposed method is not sensitive to the value of threshold, we choose the average value of the saliency in the video as threshold. We evaluate the proposed method for abnormality detection in videos from two datasets: UMN abnormal dataset1 and UCSD dataset [25]. Abnormal detection has attracted
1 http://mha.cs.umn.edu/Movies/Crowd-Activity-All.avi

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0

a lot efforts from the researchers. However, most of the existing works require training stage, e.g., social force [28], sparse reconstruction [9], MPPCA [22], i.e., they need training data to initialize the model. The proposed method, instead, does NOT need any training stage or training data. The result on UMN abnormal dataset is shown in Tab. 2, where we compute the frame-level true positive rate and false positive rate then compute the area under the ROC (Fig. 9). Fig. 10 shows the result for videos of three scenes, where we plot saliency value of each frame and show some sample frames. The result on UCSD dataset is shown in Tab. 3, where we report frame-level equal-error rate (EER) [25]. Fig. 11 shows the ROC for UCSD dataset with the proposed method; Fig. 8 shows eight samples frames, where red color highlights abnormal regions. We can find that, without training data, the proposed method

_p An eop le BBtarct _br C_ ica ook B w _l ly ad BC_ ildlif and n_1 ve w e_ sca r il e a t_b dlif ag pe am dve bc4 e_sp le_9 i rt_ _b e am _ib4 bra ees cial i_ 01 vi _ arc is10 0_c a_pa102 tic 00 los int ch _bea a_cl eup__ do illi_ rs ose 7 _ c do um plas 106 up_ c e t 6 do ume ntar ers_ x71 cu n y_ 12 ga men tary_ cora 80 m ta d l_ ga e_tr ry_p olph re ha me_ ailer lan ins ho iry_ tra _gh et_e me bi iler os k hu _m ers _wr tbu mm ovi _c ath e a _ mo ingb _Ch bba l v mo ie_ irds arliege _ _ v mu ie_t traile narr b ne sic rai r_a ato w _ ler li ne s_bered_ _qu ce_ a n ws e_ hot nt ne ews _she para _chi u ws _u rry sit li_ _w s_ _d es nig imb elec rink _7 pin htli ledo tion ing fe _ pingpon _in_n_m deb g g m pin po _a o acen gp ng_ ngl zam e o pla ng_ long _sh bi ne no _sh ot_ sc t_e _bo ot sp ottis arth die _9 or h_ _ s_ s t_b st jun 9 sp port arc arter gles o _ e sp rt_p foo lonas_12 ort ok tba _e sp _sla er_ ll_b xtr 1 sp ort_ m_d 280 est_ o w sp rt_w imb unk_ x640 or im le 1 ste ts_k ble don 280 wa en do _ba r do n_ l tv_ t_lee _12 ma t tv_ ket _12 80x gi un ch2 80 71 i_c _6 x7 ha 72 12 lle x5 ng 44 e_ fin

50

be beverl beve y01 beverrly03 beverlly05 ve y0 ga bev r 6 game erlly07 gamecub y08 gamecube02 gamecube04 gamecube05 gamecube06 gamecube13 cu e1 gamec be 6 me ub 17 mocube18 monice23 nica03 mo a0 nic samon a04 c i c c sta a a05 ete 6 standd standard0st a stand rd01 ndard 2 sta 03 standard0 a n r s a da d04 nd rd05 tvt- a 6 ac tv-tiord07 0 tv-adn s1 t tv- v-ads01 an tv-ads02 ouads03 tvn -m n 0 tv- usce04 tv-ne ic01 ws 1 tv-new 0 tv-ne s01 tv-news02 ws 3 tv-new 04 ne s0 tv- tv- news 5 tv-sp ws06 sports 09 tv- ort 01 tv-spo s0 2 tv-sp rts0 sports 3 o 0 tv- rts 4 ta 05 tv- tv-tallk01 k0 tv-talk 3 tal 04 k0 5
Figure 6. The AUC of the proposed method for each video from CRCNS-ORIG dataset.
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0

Figure 7. The AUC of the proposed method for each video from DIEM dataset.

still outperforms several state-of-arts in the literature, e.g., social force, MPPCA.

4. Conclusion and Discussion
In this paper, we proposed a novel approach for detecting spatiotemporal saliency, which was simple to implement and computationally efficient. The proposed approach was inspired by recent development of spectrum analysis based visual saliency approaches, where phase information was used for constructing the saliency map of the image. Recognizing that the computed saliency map captured the region of human's attention for dynamic scenes, we proposed two algorithms utilizing this saliency map for two important vision tasks. These approaches were evaluated on several well-known datasets with comparisons to the state-of-arts in the literature, where good results were demonstrated.

Peds1: Wheelchair

Peds1: Skater Scene 1

Peds1: Bike

Peds1: Cart

Scene 2 Peds2: Skater Peds2: Bike

Figure 8. Some sample results for the UCSD datasets, where the red color highlights the detected abnormal region, i.e., the saliency value of the pixel is higher than four times of the mean saliency value of the video. Please see the figures in color print.

Area under cuver: 0.937785
1

True Positive Rate

0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.2 0.4 0.6 0.8 1
0.9
X: 0.1978 Y: 0.8177

Scene 3
Figure 10. Some sample results for the UMN datasets, where we pick one video for each scene. The top is the saliency value (Yaxis) for each frame (X-axis) and bottom are sample frames picked from different frames (as shown by the arrow). Area under curve
1

False Positive Rate
Figure 9. The ROC for the UMN dataset computed with the propose method.

True Positive Rate

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
X: 0.2649 Y: 0.7456

X: 0.2792 Y: 0.7219

5. Acknowledgment
The work was supported in part by an ARO grant (#W911NF1410371) and an ONR grant (# N00014-15-12722). Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of ARO or ONR.

all: 0.8062 ped1: 0.7805 ped2: 0.8772
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

False Positive Rate
Figure 11. The ROC for the UCSD dataset computed with the propose method. [2] A. Antoniou. Digital signal processing. Toronto, Canada:, 2006. 2, 3 McGraw-Hill

References
[1] A. Adam, E. Rivlin, I. Shimshoni, and D. Reinitz. Robust real-time unusual event detection using multiple fixedlocation monitors. PAMI, 30(3):555 ­560, march 2008. 6

[3] D. M. Beck and S. Kastner. Stimulus context modulates com-

[4]

[5]

[6] [7]

[8]

[9]

[10]

[11]

[12] [13]

[14] [15] [16]

[17] [18]

[19] [20] [21]

[22]

petition in human extrastriate cortex. Nature neuroscience, 2005. 2 P. Bian and L. Zhang. Biological plausibility of spectral domain approach for spatiotemporal visual saliency. NIPS, pages 251­258, 2009. 4, 6 A. Borji, D. N. Sihite, and L. Itti. Quantitative analysis of human-model agreement in visual saliency modeling: a comparative study. 2012. 6 N. Bruce and J. Tsotsos. Saliency based on information maximization. In NIPS, pages 155­162, 2005. 6 L. Chen, Q. Zhang, P. Zhang, and B. Li. Instructive video retrieval for surgical skill coaching using attribute learning. In Multimedia and Expo (ICME), 2015 IEEE International Conference on, pages 1­6, June 2015. 1 X. Chen and K. S. Candan. GI-NMF: group incremental nonnegative matrix factorization on data streams. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014, Shanghai, China, November 3-7, 2014, pages 1119­1128, 2014. 1 Y. Cong, J. Yuan, and J. Liu. Sparse reconstruction cost for abnormal event detection. In CVPR 2011, pages 3449 ­3456, june 2011. 6, 7 A. Garcia-Diaz, X. R. Fdez-Vidal, X. M. Pardo, and R. Dosil. Decorrelation and distinctiveness provide with human-like saliency. In NIPS, pages 343­354. Springer, 2009. 6 C. Guo, Q. Ma, and L. Zhang. Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform. In CVPR, 2008. 4 J. Harel, C. Koch, and P. Perona. Graph-based visual saliency. In NIPS, pages 545­552, 2006. 6 X. Hou, J. Harel, and C. Koch. Image signature: Highlighting sparse salient regions. PAMI, pages 194­201, 2012. 2, 3, 4 X. Hou and L. Zhang. Saliency detection: A spectral residual approach. In CVPR, pages 1 ­8, 2007. 6 X. Hou and L. Zhang. Dynamic visual attention: Searching for coding length increments. NIPS, 21:681­688, 2008. 6 D. E. Huber and C. G. Healey. Visualizing data with motion. In Visualization, 2005. VIS 05. IEEE, pages 527­534. IEEE, 2005. 2, 4, 5 L. Itti and P. Baldi. Bayesian surprise attracts human attention. NIPS, 18:547, 2006. 6 L. Itti, N. Dhavale, and F. Pighin. Realistic avatar eye and head animation using a neurobiological model of visual attention. In Optical Science and Technology, pages 64­78. International Society for Optics and Photonics, 2004. 1 L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. PAMI, nov 1998. 1 R. Itti, Laurent; Carmi. Eye-tracking data from human volunteers watching complex video stimuli. Online, 2009. 5 T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to predict where humans look. In ICCV 2009, pages 2106 ­2113, 29 2009-oct. 2 2009. 6 J. Kim and K. Grauman. Observe locally, infer globally: A space-time mrf for detecting abnormal activities with incremental updates. In CVPR 2009, pages 2921 ­2928, june 2009. 6, 7

[23] J. Li, M. D. Levine, X. An, X. Xu, and H. He. Visual saliency based on scale-space analysis in the frequency domain. PAMI, pages 996­1010, 2013. 2 [24] X. Li, S. Huang, K. S. Candan, and M. L. Sapino. Focusing decomposition accuracy by personalizing tensor decomposition (PTD). In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014, Shanghai, China, November 3-7, 2014, pages 689­698, 2014. 1 [25] V. Mahadevan, W. Li, V. Bhalodia, and N. Vasconcelos. Anomaly detection in crowded scenes. In CVPR, pages 1975 ­1981, june 2010. 6, 7 [26] M. Mancas. Computational attention: Modelisation and application to audio and image processing. PhD thesis, PhD. Thesis, University of Mons, 2007. 6 [27] S. Marat, T. Ho Phuoc, L. Granjon, N. Guyader, D. Pellerin, and A. Gu´ erin-Dugu´ e. Modelling spatio-temporal saliency to predict gaze direction for short videos. IJCV, 82(3):231­ 243, 2009. 6 [28] R. Mehran, A. Oyama, and M. Shah. Abnormal crowd behavior detection using social force model. In CVPR 2009, pages 935 ­942, june 2009. 6, 7 [29] P. K. Mital, T. J. Smith, R. L. Hill, and J. M. Henderson. Clustering of gaze during dynamic scene viewing is predicted by motion. Cognitive Computation, 3(1):5­24, 2011. 5 [30] R. Raghavendra, A. Del Bue, M. Cristani, and V. Murino. Optimizing interaction force for global anomaly detection in crowded scenes. In Computer Vision Workshops (ICCV Workshops), pages 136 ­143, nov. 2011. 1 [31] V. Reddy, C. Sanderson, and B. Lovell. Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed, size and texture. In CVPRW, pages 55 ­61, june 2011. 6 [32] Q. Tian and B. Li. Simultaneous semantic segmentation of a set of partially labeled images. In IEEE Winter Conference on Applications of Computer Vision, 2016. 1 [33] A. Torralba. Modeling global scene factors in attention. JOSA A, 20(7):1407­1418, 2003. 6 [34] Y. Wang, Y. Hu, S. Kambhampati, and B. Li. Inferring sentiment from web images with joint inference on visual and social cues: A regulated matrix factorization approach. In Proceedings of the Ninth International Conference on Web and Social Media, ICWSM 2015, University of Oxford, Oxford, UK, May 26-29, 2015, pages 473­482, 2015. 1 [35] Y. Wang, S. Wang, J. Tang, H. Liu, and B. Li. Unsupervised sentiment analysis for social media images. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 2378­2379, 2015. 1 [36] L. Zhaoping and P. Dayan. Pre-attentive visual selection. Neural Networks, 19(9):1437­1439, 2006. 2 [37] D. Zhou, J. He, K. S. Candan, and H. Davulcu. MUVIR: multi-view rare category detection. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 2531, 2015, pages 4098­4104, 2015. 1

Diving deeper into mentee networks
Ragav Venkatesan Arizona State University
ragav.venkatesan@asu.edu

Baoxin Li Arizona State University
baoxin.li@asu.edu

arXiv:1604.08220v1 [cs.LG] 27 Apr 2016

Abstract
Modern computer vision is all about the possession of powerful image representations. Deeper and deeper convolutional neural networks have been built using larger and larger datasets and are made publicly available. A large swath of computer vision scientists use these pre-trained networks with varying degrees of successes in various tasks. Even though there is tremendous success in copying these networks, the representational space is not learnt from the target dataset in a traditional manner. One of the reasons for opting to use a pre-trained network over a network learnt from scratch is that small datasets provide less supervision and require meticulous regularization, smaller and careful tweaking of learning rates to even achieve stable learning without weight explosion. It is often the case that large deep networks are not portable, which necessitates the ability to learn mid-sized networks from scratch. In this article, we dive deeper into training these midsized networks on small datasets from scratch by drawing additional supervision from a large pre-trained network. Such learning also provides better generalization accuracies than networks trained with common regularization techniques such as l2 , l1 and dropouts. We show that features learnt thus, are more general than those learnt independently. We studied various characteristics of such networks and found some interesting behaviors.
Mentor

Mentee

Figure 1. Mentor mentoring mentee on the second hidden layer.

1. Introduction
With the proliferation of off-the-shelf, downloadable networks such as VGG-19, overfeat, R-CNN and several others in the caffe model zoo, it has become common practice in the computer vision community to simply fine-tune one of these networks for any task [21, 13, 8]. These networks are usually trained on a large dataset such as Imagenet and Pascal [20, 7]. The proponents of these networks argue that these networks have learnt image representations that are pertinent for most datasets that deal with natural images. Under the assumption that all these datasets are natural images and are derived from a similar distribution

this might as well be true. Even with such networks, features that are unique to each datasets do matter. While finetuning of an already trained network works to a certain extent, these features are not learnt in a traditional manner on the target dataset but are simply copied. There is also no guarantee that these features are the best representations for the target dataset, although there is some validity in expecting that such a representation might work well, since after all it was learnt from a large enough dataset. Most computer vision scientists do not attempt to train a new architecture from scratch (random initializations). Training even a mid-sized deep network with a small dataset is a notoriously difficult task. Training a deep network, even those with mid-level depth require a lot of supervision in order to avoid weight explosion. On most imaging datasets, with image sizes being 224X 224, the memory insufficiency of a typical GPU restricts the mini-batches to less than 100. Using small mini-batches and small datasets lead to very noisy and untrustworthy gradients. This leads to weight explosions unless the learning rates are made sufficiently smaller. With smaller learning rates, learning is slowed down. With smaller mini-batches learning is unstable. One way to avoid such problems is by using reg-

ularization. By regularizing we can penalize the gradients for trying to make the weights go higher and higher. Batch Normalization is another technique that is quite commonly used to keep weight explosion under check [12]. Even with these regularization techniques, the difficulty of training a deep network from scratch leads most computer vision scientists to use pre-trained networks. There are several reasons why one might favour a smaller or a mid-sized network even though there might be a better solution available using these large pre-trained networks. Large pre-trained networks are computationally intensive and often have a depth in excess of 20 layers. The computational requirement of these networks do not make them easily portable. Most of these networks require state-ofthe-art GPUs to work even in simple feed forward modes. The impracticality of using pre-trained networks on smaller computational form factors necessitates the need to learn smaller network architectures. The quandary now is that smaller networks architectures cannot produce powerful enough representations. Many methods have been recently proposed to draw additional supervision from large well-trained networks to regularize a new network while learning from scratch [23, 19, 2, 5]. All of these works were inspired from the Dark Knowledge (DK) approach [11]. All these techniques use at most one layer of supervision on top of the softmax supervision and try to use this technique to learn more deeper networks better. Figure 1 shows a conceptualization of this idea. In this paper, we try and make a shallower mentee network learn the same representation as a larger, well-trained mentor network at various depths of its network hierarchy. Mentorship happens by tagging on to the loss of the mentee network, a dissimilarity loss for each layer that we want mentored. To the best of our knowledge, there hasn't been any work that has regularized more than one layer this way. There also hasn't been any work that has trained a mid-sized network from a larger and deeper network from scratch. We study some idiosyncratic properties for some novel configurations of mentee networks. We argue that such mentoring avoids weight explosion. Even while using smaller minibatches, mentee networks get ample supervision and are capable of stable learning even at high learning rates. We show that mentee networks produce a better generalization performance than an independently learnt baseline network. We also show that mentee networks are better transferable than the independently learnt baselines and are also a good initializer. We also show that mentee networks can learn good representations from very little data and sometimes even without supervision from a dataset in an unsupervised fashion. The rest of the paper is organized as follows: section 2 discusses related works, section 3 details the mentored

learning, section 4 discusses designs for experiments, section 5 produces results and section 6 provides concluding remarks.

2. Related Works
Hinton et al., tried to make networks portable by learning the softmax outputs of a larger well-trained network along with the label costs [11]. This was previously explored using logits by Caruana et al., [1, 4]. By directly learning the softmax layers, they were forcing the softmax layer of a smaller network to mimic the same mapping as that of a larger network onto the label space. In a way they tried to learn a better second and third guesses. They called this dark knowledge, as the knowledge so learnt is only available to the larger network. By attempting to learn the softmax layer, they were able to transfer or distil knowledge between the two networks. The drawback of this work is that it only works as long as the larger network is already well-trained and stable. They relied upon the network's predictive softmax layer being learnt perfectly on the target dataset and propagate that knowledge. This also assumes that there are relationships between classes to be exploited. While this may work in cases where this is true, such as in character recognition or in voice recognition, it doesn't work in most object detection datasets where the relationship between classes is not a given in terms of its appearance features1 . They also distil only the softmax labels and not the representational space itself. This also requires that the smaller network is capable of training in a stable manner. Dark knowledge is extended upon by several previous works [23, 19, 2, 5]. One extension of this work that we generalize in this article is using layer-wise knowledge transfer for one layer in the middle of the network. This was used to show that thinner and deeper network can be trained with better regularization [19]. Another method uses a similar one-layer regularizer as knowledge transfer between a RNN and a CNN [5]. Mentored training has also been shown to be extremely useful when training LSTMs and RNNs with an independent mentor supervision [23]. All these methods discussed above are essentially the same technique as the dark-knowledge method extended beyond just the softmax layer. All of these methods have fixed one-layer regularizations and although trivial, we generalize this for many layers. Their mentee networks are typically much deeper and complex than their mentors and they use these as a means to build more complex models (albeit thinner as in the case of FitNets [19]). There has been no study to the best of our knowledge that builds less complex (both thinner and shallower) models with the same capability as larger models. Also, neither has there been a study that stud1 We tried this approach on Caltech101 and couldn't get reliable results.

ies various properties of these networks nor those that show the transferability and generality of these networks.

3. Generalized mentored learning
Let us first generalize all of the methods that use this knowledge transfer as follows: Consider a large mentor network with n layers Mn . Suppose we represent the k th neuron activations of the ith layer in the network as Mn (i, k ). Consider a smaller mentee network with m2 layers Sm . Suppose that M is already well-trained and stable on a general enough dataset D. Now consider that we are using S to learn classification3 on a newer dataset d1 which is less general and much smaller than D as determined a priori. Although this is not a constraint, having a smaller and less general dataset emphasizes the core need where such mentored learning is most useful.  l  n and j  m, we can define a probe as an error measure between Mn (l) and Sm (j ). This error can be modelled as an RMSE error as follows, 1 a
a

(l, j ) =

(Mn (l, i) - Sn (j, i))2 ,
i=0

(1)

where a is the minimum number of neurons between Mn (l, .) and S (j, .). If the neurons were convolutional, we consider element-wise errors between filters. By adding this cost to the label cost of the network S during back propagation, we learn not just a discriminative enough representation for the samples and labels of d1 , but also for layers j in a pre-determined set of layers, a representation closer to the one produced by M. Some implementations of such loses in literature tend to learn a regressor instead of simply adding the loss, but we concluded from our experiments that the computational requirements of such regressors do not justify their contributions. Adding a regressor would involve embedding the activations of the mentor and the mentee onto a common space and minimizing the distances between those embeddings. We quite simply circumvent that and consider the minimum number of matching neurons. This enables us to have a slimmer, fatter or same th sized mentee. Suppose db 1 is the b mini-batch of data from the dataset d1 and suppose we have a pre-determined set of probes B , which is a set of tuples of layers from (M, S ). The overall network cost is, e = t Ls (db 1 ) + t
(l,j )B
2 Although we adhere strictly to m < n, without losing any generality, we could have any m or n. In fact m > n with only one probe would be the special case of FitNets [19]. 3 Although we only consider the task of classification, the methods proposed are applicable to many forms of learning.

where L(.)s is the network loss of that mini-batch, t and t weighs the two losses together to enable balanced training and t is the weight of the probe between the two (temperature) softmax layers. t = g (t), t = g (t) and t = g (t) are annealing functions parametrized by the iteration t under progress. Although most methods in the literature use constants for t , t and t , we found it preferable to retain g (t) = 1, t throughout and anneal  and  linearly. We discuss the value and the need for these parameters in detail further. Since M is pre-trained and stable, the second and third terms of equation 2 are penalties for the activations of those layers in S not resembling the activations of the probed layer from M respectively. These losses as defined by equation 1 are functions of the weights of those layers from S only. They restrict the weights within a proximity or region, that produces activations that are known for the mentor to be better activations. This restricting behaviour acts as a guided regularization process, allowing the weights to explore in a direction that the mentor thinks is a good direction, while still not letting the gradients to explode or vanish. For a particular weight w  S at any layer, a typical update rule without the probe is, wt+1 = wt -   Ls , w (3)

where t is some iteration number,  is the learning rate and assuming t = 1, t. The update rule with mentored probes is, wt+1 = wt -  t t  Ls + w   (l, j ) + t (n, m) . (4) w w

(l,j )B

(l, j ) + t (n, m),

(2)

The last two terms add a guided version of a noise that decreases with each iteration. While at earlier stages of training, this allows the weights to explore the space, it also restricts the weights from exploding because the direction that the weights are allowed to explore is controlled by the mentor. The freedom to explore tightens up as the as learning proceeds, provided g (t) is a monotonically annealing function with respect to t. Note that even though to calculate these error gradients we need one forward propagation through M, we do not back propagate through M. This is a penalty on the weights, even though we are using the activations to penalize the weights indirectly. Although mentee networks can be further regularized with l2 , l1 , dropouts and batch normalizations, it is recommended that the mentee networks imposes additional regularizations mirroring the mentor networks for better learning.

Obedient Mentee 0.04 0.03 0.02 0.01 0 0
* * * 
Mentoring Phase Self-study Phase

Adamant Mentee 0.04 0.03 0.02 0.01 0 0
* * * 

50 Epochs

100

150

50 Epochs

100

150

Figure 2. Annealing ,  and  while learning for an obedient and an adamant network.

Different configurations of mentee networks Different combinations of ,  and  produces different characteristics of mentee networks. Equation 4 can be seen as learning with three different learning rates,    ,    and    . We can simulate using these three parameters, two idiosyncratic personalities of mentee networks: an obedient network and an adamant network. An obedient network is a network that focuses on learning the representation more than the label costs at the beginning stages and once a good representation is learnt, it focuses on learning the label space. It tends towards being over-regularized and its regularization relaxes with epochs. An adamant network is a network that focuses almost immediately on the labels as much as learning the representation, but its focus is positively towards learning the label only. The learning rates of these personalities are shown in figure 2. An independent network can be considered as a special case of the adamant network where probe weights are ignored ( = 0,  = 0, t). The other extreme case of an obedient network is perhaps a gullible network that learns just the embedding space of the mentor. Gullible networks are also a good way to initialize a network in an unsupervised mentoring fashion. Consider a dataset d2 , that does not have any labels. Neither the mentor nor the mentee could potentially learn any discriminative features. Using just the probes we could build an error function that could make the smaller mentee network still learn a good representation for the dataset. We use the information from the parent network to learn a good representation for d2 by simply back propagating the second term of equation 2 alone. These gullible mentees come in really handy when the dataset has considerably less samples to be supervised with. Unsupervised mentoring is also an aggressive way to initialize a network and is often helpful in learning large networks in a stable manner with a stable initialization. Typically the deeper one goes, the more difficult it becomes to learn the activations and the costs saturate quickly. The softmax layer is the most difficult to learn. To our surprise we find that probe costs converge much sooner than

the label costs, leading us to believe that the representations being mentored are indeed relevant as long as the datasets share common characteristics. There is a plethora of such configurations that could be tried and many unique characteristics discovered. In this article we limit ourselves to only those that enable us stability during learning and focus on those that help us with better generalizations. For learning large networks we prefer the use of obedient networks as obedient networks are heavily regularized at the beginning leading to careful initialization and stabilization of the network before learning of labels takes over. We call the stabilization phase as the mentoring phase and the rest, self-study phase. During the mentoring phase learning is slow but steady. In most cases,    is an increasing function due to the aggressive climb of . The annealing of these rates for a typical obedient mentee and an adamant mentee are shown in figure 2. We also find that typically the later layers are more stubborn in being mentored than earlier layers. Although this is typically to be expected, more obedience may be enforced by choosing higher  values for layers that are deeper in the network.

4. Design of experiments
We evaluate the effectiveness of mentorship through the following experiment designs:

4.1. Effectiveness
To demonstrate the effectiveness of learning, we first train a larger network on a dataset. Using this network as a mentor, we train the mentee network on the same dataset. Unlike those in literature, we choose mentee networks that are generally much smaller than the mentor. We show that this generalizes at least as well as an independent network of the exact same architecture regularized not by mentor, but by batch normalization, l2 and l1 norms and dropouts. Training mid-sized networks on small datasets are often difficult. To our best knowledge we have provided our best effort in meticulously learning all the networks. For learning an independent network often we spent additional effort in adjusting the learning rates at the opportune moments. We show that mentee networks outperforms the independent networks and even at the worst case performs as well as the independent networks.

4.2. Generality of the learnt representations
To demonstrate that the network learns a more general representation, we gather a pair of datasets of seemingly similar characteristics with one more general or larger than the other. We train the mentor with the more general dataset first and then fine tune it on the less general dataset. We then train both the independent and the mentee nets on the less general dataset and demonstrate again that at worst the mentee net performs the same as the independent net.

0.02 0.015 0.01 0.005 0 0

Conifguration For Learning VGG-19 using Caltech 101

Fine Tuning Begins Here

* * * 

10

20

30

40

50

60

70

80

90

100

Epochs

Figure 3. Annealing ,  and  while learning VGG-19 space for Caltech-101. We used an obedient network.

other works have been built. We try to learn the same 4096 dimensional representation of the VGG-19 network using ambitiously less number of layers. For the (Caltech-101-Caltech256) dataset pairs in all our experiments, there is no explicit mentor network that we learnt. We simply set g (t) = 0, t and learnt with probes without retraining the VGG-19 network. In a way we are attempting to learn VGG-19's view of the Caltech101 dataset and are probing into the representational frame of the VGG-19 network. We used a relatively obedient student as shown in figure 3 for this case.

4.4. Implementation details
The independent networks were all regularized with a l1 and l2 penalties with a weight of 1e-4 , which seems to give the best results. On all networks we also applied parametrized batch norm for both fully connected and convolutional layers and dropouts with rate of p = 0.5 for the fully connected layers [12, 22]. We find that dropout and bath norm together help in avoiding over-fitting. All our activation functions were rectified linear units [16]. For learning the mentee network we start with learning rates as high as 0.5, for the larger independent networks we are forced a learning rate of 0.001, while for the smaller experiments we were able to go as high as 0.01, since the batch sizes were larger. During training, if ever we ran into exploding gradients or NaNs, we reduce the learning rate by ten times, reset the parameters back to one epoch ago and continue training. We train until 75 epochs after which we reduce the learning rate by a hundred times and continue fine-tuning until early stopping. Unless early stopped, we train for 150 epochs. All our initializations were from a 0-mean Gaussian distribution, except the biases which were initialized with zeros. The experiment set-up was designed using Theano v0.8 and the programs were written by ourselves4 [3]. The experiments with MNIST datasets were conducted on a Nvidia GT 750M GPU, the others on an Nvidia Tesla K40c GPU, with cuDNN 3007 and Nvidia CUDA v7.5. The minibatch sizes for all the MNIST and cifar experiments were 500 (unless forced by small dataset size in which case we performed batch descent instead of the usual stochastic descent). The mini-batch sizes for all Caltech experiments were 36, with images resized to 224X 224 so as to the fit the VGG-19 requirement. Apart from normalization and meansubtraction, no other pre-processing were applied to any of the images. For the Caltech experiments we used Adagrad with Polyak's momentum [18, 9]. For the experiments that were smaller networks we used RMSprop with Nesterov's accelerated gradient [6, 17]. It is to be noted that we chose to use vanilla networks that are as simple as possible so as to enable us to compare against a baseline which is also vanilla. Since our aim is
4 Code

We then proceed to fine tune the classifier layer of both the mentee net and the independent net using the more general dataset but since the other layers are not allowed to change, the mentee net does not have any additional supervision. This tests the quality of the features learnt by these networks on a more general and more difficult dataset. For the sake of our experiments we consider the pairing of (Cifar-10 - Cifar-100) and (Caltech-101 - Caltech256) [14, 10]. We assume that Cifar-100 is more general than Cifar-10 and Caltech-256 is more general than Caltech101. Additionally, we conduct another experiment where we try to learn from a mentor network trained with the full MNIST dataset, a mentee network that only has supervision from a part of the dataset [15]. The independent network also in this case, learns with the same redacted dataset. We redact the dataset by only having p samples for each class in the dataset where p  {500, 250, 100, 50, 10, 1}. p = 1 is essentially an ambitious goal of 1-shot learning from scratch using a deep network. We also try this with a mentee network that is initialized by unsupervised mentoring from the same mentor network. We acknowledge that the comparison with unsupervised mentoring is unfair because the mentee net is initialized by the mentor with information filtered from data that is unavailable for the independent network. The latter results are to demonstrate that unsupervised mentoring could learn an effective feature space even without labels and with very less samples.

4.3. Learning the VGG-19 representation
In particular, while learning classification on the Caltech101 dataset, we try to learn the same representation as the popularly used VGG-19 network at various levels of the network hierarchy [21]. VGG-19 network's 4096 dimensional representation is one of the most coveted and iconic image features in computer vision at the time of the writing of this article. The VGG-19 network has 16 convolutional layers and 2 fully-connected layers the last of which produces the 4096 dimensions of features upon which many

is available at our GitHub page.

a) VGG-19 M entor

b) Gullible M entee

c) Obedient M entee

d) Adamant M entee

Figure 6. VGG-19 first layer filters and filters probed using Caltech101 for a Gullible, Obedient and an Adamant mentee after only one epoch of training. We recommend viewing this image on a computer monitor.

not to achieve state-of-the-art accuracies on any datasets, we didn't implement several techniques that are commonly applied to boost the network performances in modern day computer vision. The purpose of these experiments is to unequivocally demonstrate that among networks that learn from scratch, one that is mentored can perform better and learn more general features than one that is not.

5. Results
The results are split across two tables based on the network architectures. The smaller experiments on a 5 layer network are shown in figure 4 and the larger ones in figure 5. The  symbol shows which layers are probed and from where. In figure 4, the results clearly demonstrate the strong performance of the mentee networks over the independent networks. In the cifar experiments we under-weighted  purposely as we didn't want to propagate the 20% of error from the mentor network on to the mentee network. The results on Cifar 10 from scratch seem to indicate that both networks have reached the best possible performance for that architecture. We believe with the amount of supervision already provided from the 40,000 training images, mentoring is not as effective. When there is already ample supervision, men-

toring is ineffective, or rather unwanted, albeit it doesn't hurt. While fine-tuning on cifar 100, we find that there are great gains to be made. We find a similar trend with the MNIST experiments also. The less data there is, the higher the gain of the mentee networks. Note that even though mentee networks are regularized, care was taken to ensure that they both go through the exact same number of iterations at the exact same learning rate. We also found that unsupervised mentoring always keeps the learning at a very high standard although as was discussed in section 4.2 there was additional supervision on the entire dataset from the unsupervised mentoring, which is unfair. In the experiments with the Caltech101 datasets, we find that the mentee networks perform better than the vanilla network. The mentee network was also able to perform significantly better than the independent network when only the classifier/mlp sections were allowed to learn the Caltech256 dataset with representation learnt from Caltech101. This proves the generality of the feature space learnt. With an even obedient student, we were able to learn the feature space of the VGG-19 network to a remarkable degree. While with the first convolutional layer we were able to learn to a minimum rmse or 0.0023 from 6.54 at random. With the last two layers we were able to learn upto a rmse of 2.04 from 12.76 at random. Figure 6 shows the filters learnt after one epoch for a gullible network, an obedient network and an adamant network. All these networks were initialized with same random values at their inception. We can easily notice that the gullible network already sway towards the VGG-19 filters. In obedient mentee, we notice that most corner detector features are already swaying towards the mentee network but more complex features are not swaying as much as the gullible network. To our surprise we notice that even in an adamant network corner detectors are swaying towards VGG-19. This shows that even with low weights, the first layer features are learning the VGG-19's representation. It is to be noted that we are not learning the weights directly, but are learning the activations produced by the VGG-19 network for the Caltech101 dataset that leads us to learn the same filters as the VGG-19. This implies that corner features are more general among the Imagenet dataset, which VGG-19 was trained on, and the Caltech101 dataset, which explains why they are learnt earlier than others.

6. Conclusions
While the use of large pre-trained networks will continue to remain popular, because of the ease in just copying a network and fine-tuning the last layers, we believe that there is still a need for learning small and mid-sized networks from scratch. We also recognize the difficulty involved in reliably training deep networks with very few data sam-

Network
Convolutional Layers Activation: ReLU Stride: 1 Max Pooling: 2 Architecture Fully Connected Layers Activation: ReLU Dropout Input Rate: 0.5 Output Layer Trained from scratch Cifar 10 Fine-tuned last layer only Cifar 100 MNIST - 500

Mentor
Kernel Size: 5 Neurons: 20 Kernel Size: 3 Neurons: 50 Neurons: 800 Neurons: 800 Neurons: 10/100 79.36 % 41.21%

Mentee
Kernel Size: 5 Neurons: 20 Neurons: 800 Neurons: 800 Neurons: 10/100 68.5 % 33.2 % 97.73%
unsupervised mentoring

Independent
Kernel Size: 5 Neurons: 20 Neurons: 800 Neurons: 800 Neurons: 10/100 68.58 % 26.67% 97.71%

98.2%

MNIST-250 Accuracies

97.47 %
unsupervised mentoring

97.88%

96.89 %

MNIST-100 MNIST 99.59% MNIST-50

97.42%
unsupervised mentoring

96.01%

95.12%

92.95%
unsupervised mentoring

96.80%

90.96%

MNIST-10

78.5 %
unsupervised mentoring

96.7%

75.3%

MNIST - 1

48.5%
unsupervised mentoring

96.7%

41.5%

Figure 4. Architecture and results for the experiments with CIFAR and MNIST datasets.

ples. One way to meet the best of both worlds is by using a mentored learning approach. In our study, we find that a shallower mentee network was able to learn a new representation from scratch while being regularized by the mentor network's activations for the same input samples. We found that such mentoring provided much stabler training even at higher learning rates. We noted some special cases of these networks and recognize some idiosyncratic personalities. We extended one of these to be able to perform as an unsupervised initialization technique. We showed through compelling experiments, the strong performance and generality of mentor networks.

[3] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012. 5 [4] C. Bucilu, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535­541. ACM, 2006. 2 [5] W. Chan, N. R. Ke, and I. Lane. Transferring knowledge from a rnn to a dnn. arXiv preprint arXiv:1504.01483, 2015. 2 [6] Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for non-convex optimization. arXiv preprint arXiv:1502.04390, 2015. 5 [7] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303­ 338, 2010. 1 [8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition

References
[1] J. Ba and R. Caruana. Do deep nets really need to be deep? In Advances in neural information processing systems, pages 2654­2662, 2014. 2 [2] A. K. Balan, V. Rathod, K. P. Murphy, and M. Welling. Bayesian dark knowledge. In Advances in Neural Information Processing Systems, pages 3420­3428, 2015. 2

Network

Mentor (VGG-19)
Neurons: 64 Max Pooling: 1 Neurons: 64 Max Pooling: 2 Neurons: 128 Max Pooling: 1 Neurons: 128 Max Pooling: 2 Neurons: 256 Max Pooling: 1 Neurons: 256 Max Pooling: 1 Neurons: 256 Max Pooling: 1 Convolutional Layers Stride: 1 Kernel Size: 3 Activation ReLU Neurons: 256 Max Pooling: 2 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 2 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 1 Neurons: 512 Max Pooling: 2 Fully Connected Layers Activation: ReLU Dropout Input Rate: 0.5 Softmax Layer Trained from scratch on Caltech 101 Fine-tuned last layer only for Caltech 256 N/A N/A Neurons: 4096 Neurons: 4096

Mentee
Neurons: 64 Max Pooling: 2

Independent
Neurons: 64 Max Pooling: 2

Neurons: 128 Max Pooling: 2

Neurons: 128 Max Pooling: 2

Neurons: 256 Max Pooling: 2

Neurons: 256 Max Pooling: 2

Architecture

Neurons: 512 Max Pooling: 2

Neurons: 512 Max Pooling: 2

Neurons: 512 Max Pooling: 2

Neurons: 512 Max Pooling: 2

Neurons: 512 Max Pooling: 2 Neurons: 4096 Neurons: 4096 Neurons: 102/256 56.16% 66.12 %

Neurons: 512 Max Pooling: 2 Neurons: 4096 Neurons: 4096 Neurons: 102/256 45.46% 55.45%

Accuracies

Figure 5. Architecture and results for the experiments with Caltech datasets.

(CVPR), 2014 IEEE Conference on, pages 580­587. IEEE, 2014. 1 [9] S. Green, S. I. Wang, D. M. Cer, and C. D. Manning. Fast and

adaptive online training of feature-rich translation models. In ACL (1), pages 311­321, 2013. 5 [10] G. Griffin, A. Holub, and P. Perona. Caltech-256 object cat-

egory dataset. 2007. 5 [11] G. Hinton, O. Vinyals, and J. Dean. Dark knowledge. Presented as the keynote in BayLearn, 2014. 2 [12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 2, 5 [13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675­678. ACM, 2014. 1 [14] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images, 2009. 5 [15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998. 5 [16] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807­814, 2010. 5 [17] Y. Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372­376, 1983. 5 [18] B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1­17, 1964. 5 [19] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. 2, 3 [20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), pages 1­42, April 2015. 1 [21] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 1, 5 [22] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014. 5 [23] D. Wang, C. Liu, Z. Tang, Z. Zhang, and M. Zhao. Recurrent neural network training with dark knowledge transfer. arXiv preprint arXiv:1505.04630, 2015. 2

Neural Dataset Generality
Ragav Venkatesan Arizona State University
ragav.venkatesan@asu.edu

Vijetha Gattupalli Arizona State University
jgattupa@asu.edu

Baoxin Li Arizona State University
baoxin.li@asu.edu

arXiv:1605.04369v1 [cs.CV] 14 May 2016

Abstract
Often the filters learned by Convolutional Neural Networks (CNNs) from different datasets appear similar. This is prominent in the first few layers. This similarity of filters is being exploited for the purposes of transfer learning and some studies have been made to analyse such transferability of features. This is also being used as an initialization technique for different tasks in the same dataset or for the same task in similar datasets. Off-the-shelf CNN features have capitalized on this idea to promote their networks as best transferable and most general and are used in a cavalier manner in day-to-day computer vision tasks. It is curious that while the filters learned by these CNNs are related to the atomic structures of the images from which they are learnt, all datasets learn similar looking low-level filters. With the understanding that a dataset that contains many such atomic structures learn general filters and are therefore useful to initialize other networks with, we propose a way to analyse and quantify generality among datasets from their accuracies on transferred filters. We applied this metric on several popular character recognition, natural image and a medical image dataset, and arrived at some interesting conclusions. On further experimentation we also discovered that particular classes in a dataset themselves are more general than others.


2
3

1

4

5

Figure 1. Thought experiment to describe the dataset generality. S is the space of all possible atomic structures, D1 - D5 are the atomic structures present in respective datasets.

1. Introduction
Neural networks, particularly CNNs have broken all records recently in the computer vision research area. The growth of CNNs focused initially on the recognition of characters. Fukushima and LeCun were the initial pioneers. Independently they developed CNN based systems, some of which are still being used widely [7, 16]. Large networks are often trained with large number of data samples to achieve good accuracies [25, 14]. Still, scepticism over CNNs among the modern day computer vision scientists stems from the fact that one does not have a clear understanding of its inner working. Some studies show that a few (< 1%) nodes are all that are actively contributing to

classification [4]. They also suggest that large networks often overfit, but since the data is too large over-fitting often works as an advantage [20]. While it is reasonable to expect edge detectors and Gabor-like features in the lower-level filters and more sophisticated concepts at the higher levels, it is not clear as to why these filters adapt themselves in this manner. What is fairly clear though is that different datasets result in different sets of filters that are similar if the datasets are similar. It is only natural to ask, what role does the data itself play in such filters being learnt and how they compare with filters learnt from another dataset. In this paper we take the view that the filters learnt by networks when trained using a particular dataset represent the detectors for some atomic structure in the data itself. In which case each layer is a mapping form the previous layer to the next layer that is constructed using combinations of these atomic structures in the first layer in order to minimize a cost. Let us first define atomic structures to be the forms that CNN filters take by virtue of the entropy of the dataset it is learning on, analogous to dictionary atoms. Complex

datasets have more and varied atomic structures. Consider the following thought experiment: Let's assume that all possible atomic structures reside in an universe S . Suppose we have a set of three datasets D = {D1 , D2 , D3 } and D  S . Consider the system in figure 1. The figure describes the configuration of the elements of D. One would now recognize that D1 is a more general dataset with respect to D2 and D3 . It is so because, while D1 contains most of the atomic structures of D2 and D3 , the latter do not contain as many atomic structures of D1 . While this analysis is simplified for one layer, in typical CNNs, co-adaptation plays a major role in the learning of these atomic structures. Therefore, generality as defined by the overlap of areas in a layer-wise Venn diagram is impractical to obtain. In this paper we postulate that, the generalization performances of CNNs on one dataset re-trained on a network initialized by training using another, could be used to derive generality. We call this process of pre-training as prejudicing. By prejudicing on the first dataset, we froze and unfroze layers and retrained the networks on the second dataset. By freezing layers we are making a network more obstinate and we call this process obstination1 . The more the layers are frozen, the more obstinate the feature extractor is, therefore the harder the classifier has to work. If the prejudice was general enough, the classifier shall still generalize fairly well enough. What this means is that if the prejudicing dataset is more general than the re-train dataset, the classifier can generalize better than vice versa. We developed a generality metric by comparing the gain in performances of networks of various obstination. Using a generality such as the one proposed, it becomes clearer as to what kind of datasets are to be used to prejudice CNNs with during transfer learning. We even discovered that samples with particular labels within a dataset alone are general enough. So, if we begin by prejudicing the network on only those and then moved on to the rest of the labels, we were able to learn the rest of the dataset with considerably less training samples while achieving comparable generalization performances. Off-the-shelf networks such as VGG, overfeat and various published Caffe model weights are trained on large scale image datasets such as Imagenet or PASCAL [23, 12, 8, 22, 5]. For instance, while these may work on applications such as human pose recognition or vehicle detection, they do not necessarily work on tasks involving medical images. This is because the datasets on which they are trained are not general enough to adapt to the representational requirements of medical images, which is on a manifold unique and disjoint form the manifolds of natural images. This is visualized in D4 and D5 from figure 1. Even a large collection of natural images is not general enough
1 Obstinate layer or freezing implies that the weights were not changed during backprop. The layer remains prejudiced.

to have networks trained that are suitable to medical images. In these cases, the prejudiced network often fails. For instance, on the Colonoscopy dataset discussed later a 22 layer deep overfeat features, trained with a logistic regression performs poorer than a 3 layer deep CNN trained from random initialization, which is in turn outperformed when initialized by a network trained on an endoscopy dataset. In this article we considered popular offline character recognition datasets and arrived at some interesting analysis and generalities. We also show that within the MNIST dataset, classes [4, 5, 8] are general enough that we could learn the other classes with very few (even just one) samples, when prejudiced with networks trained on [4, 5, 8]. We also considered more sophisticated datasets such as Cifar 10 and Caltech 101 against some medical image datasets for colonoscopy video quality [13]. This study led us to two major research insights: 1. If one has very few data to learn from, which other dataset is better to prejudice the network with? The answer is particularly helpful when dealing with medical image datasets where data is very scarce and one can't simply use a network trained on VOC datasets as feature extractors as discussed above. 2. Among the various classes during the training procedure, if we prejudice with a certain general set of classes first and then move on to others later, generalization to all classes, even for those with few samples is better. This is particularly significant if the dataset has a lot of samples in certain classes and not as much of others. The rest of the paper is organized as follows: section 2 discusses related works, section 3 presents the design of our experiments, section 4 shows some results on the coreexperiment and section 5 provides concluding remarks.

2. Related work
One related work that this article shares with is the work by Yosinski et al [26]. In that article, the authors considered two tasks A and B that were essentially 500 classes each from the Imagenet dataset [22]. They trained an 8 layer network on one of the tasks (say A). They then initialized a new network carrying over the first n layers from the previous job while randomly initializing the others. This new network was used to retrain task B . Such a network was AnB + . They experimented by obstination of the carried over layers. Such a network was AnB . They also studied the specificity of each layer and their contributions to the overall performance. They also showed that networks working on similar tasks had a high memorability and that co-adaptation of layers increased the generalization performance.

While this analysis is interesting, it was performed on only one dataset: Imagenet. By design, the networks were forced to learn very general filters, so as to be best transferable. Since the images were all natural images, one would expect the layers to be more Gabor-like at earlier layers and have more label specific features at later layers, which was what was observed. Also, the paper analysed the transferability of the feature extractors from the perspective of the networks in terms of their fall in generalization performance. This analysis was not catered to the dataset's perspective, which is that the filters learned are a property of the dataset being trained on. This was not a problem for the authors as their datasets for tasks A and B occupied similar manifolds. This analysis also didn't explore re-training using the same network but rather went with re-initializing so that they could learn new co-adaptations. This is not interesting to the study of generality as we want to observe the effect of filters transferred from one dataset on another. The more general a dataset, the more variety of atomic structures it offers to the network to learn. We used this idea to define a generality metric between two datasets. To do so, we cannot follow the techniques used by Yosinski et al. Another closely related work is the work on dark knowledge by Hinton et al, [10]. Here the authors suggest that among the various classes in a dataset, there exists some amount of generalization knowledge that could be transferred. The authors construct a large network that learns all its classes. They then go on to train a smaller network with the same dataset (or with a dataset that is missing some of the classes altogether). While training this smaller network though, instead of using the the hard labels, they also use the softmax output from the large network also for backprop. This creates an effect of the larger network guiding the smaller network to not just generalize to the dataset, but also to generalize to unseen classes. This is because, as the argument goes, "the network learns the relationship between the classes" and "all the knowledge is among the relative probabilities or softmaxes that the network is almost certain is wrong" [10]. Although the author retrains an entire network that is randomly initialized using the softmax outputs from a trained network and uses this as prejudice, no information is actually being transferred in terms of actual filters. Ergo, this work, while interesting, also doesn't help in understanding generality of the data itself in a more direct manner. Some of the claims made by this article though were indirectly and independently verified by us through our generality results. The basic claim of their work is that among only a handful of classes, there is enough knowledge to generalize to other classes. Unless there exists some generality between classes, training on particular classes will not have been representational enough for the other classes to learn on. We directly verify this by showing that some classes

Figure 3. Samples of some of the datasets that we used in this analysis. From top to bottom: MNIST [17], MNISTrotated [15], MNIST-random-background [15], MNIST-rotatedbackground [15], Google street view house numbers [19], Char 74k English [3], Char 74k Kannada [3]. Last two rows, first five from left are CIFAR 10 and the rest are Caltech101 [13, 6]. The bottom row is the colonoscopy dataset.

alone have a high generalization to the rest of the dataset and make a similar conclusion from an entirely independent direction of research.

3. Design of experiments
Consider figure 3. Among the various datasets shown, it is natural to expect any network trained on MNIST to contain simpler filters than MNIST-rotated. This is because, while MNIST-rotated contains many structures from MNIST, due to the rotations, MNIST-rotated will contain additional structures that require the learning of more complicated filters. A network trained on MNIST-rotated on its first layers will be expected to additionally have filters for detecting sophisticated oriented edges than for MNIST. This would mean that prejudicing a network with MNIST to then re-train MNIST-rotated is much less helpful than vice versa. A network prejudiced with a general enough dataset is better to be retrained for it generalizes easily. A prejudice must come from a more general dataset if a prejudice transfers positive knowledge as shown in their generaliza-

0 1

0 1

0 1

0 1

9

9

9

9

Figure 2. Protocol of obstination: From left to right, all layers frozen, one, two and three layers unfrozen. Green represent unfrozen and red represent frozen. Note that the layers are always unfrozen from the end and that the softmax layer is always unfrozen and randomly initialized. This should be generalized similarly for more than three layers also.

tion performances. We use this simple intuition to argue that MNIST-rotated is a more general dataset with respect to MNIST. Our basic experiment is conducted between pairs of datasets Di and Dj . Firstly, we train (prejudice) a randomly initialized network with dataset Di . We call this network n(Di |r) or the base network (r implies random initialization). We then proceed to retrain n(Di |r) as per any of the setup shown in figure 2. nk (Dj |Di ) would imply that there are k degrees of freedom, or to be precise, k layers of filters that are allowed to learn by dataset Dj that is prejudiced by the filters of n(Di |r). nk (Dj |Di ) has N - k obstinate layers that carries the prejudice of dataset Di , where N is the total number of layers. Note that more degrees of freedom implies that the network is less obstinate to learn. Also note that these layers can be both convolutional or fully connected neural layers. Any idea expressed here can be extended to any type of parametrized layers. In fact while we perform operations such as batch normalizations, we even freeze and unfreeze the  and  of batch norm [11]. Obstination also includes the bias parameters. Layers learn in two facets. They learn some components that are purely their own and some that are co-adapted from previous layers that are allowed to learn as well. By freezing some layers we are making those layers a fixed functional transformation. Note that the performance gain from nk (Dj |Di ) and nk+1 (Dj |Di ) is not because of just the new layer k + 1 being allowed to learn, but of the combination of all k + 1 layers allowed to learn. Figure 2 shows the setup of our experiments and explains degrees of freedom. These are our obstination protocols. Notice that in all the various setup, the softmax layer remains non-obstinate. In fact the softmax layer is always randomly re-initialized because not all dataset pairs have the same number of labels. Also notice that the unfreezing of layers happen from the rear. We cannot unfreeze a layer that feeds into a frozen layer. This is because, while the unfrozen layer learns a new filter and therefore represents the image on new distributed domains, the latter layer is not adapting to such a transformation. When there are two

layers unfrozen, the two layers should be able to co-adapt together and must finally feed into an unfrozen classifier layer.

3.1. Dataset generality
Suppose the generalization performance of n(Dj |r) is (Dj |r) and the generalization performance of nk (Dj |Di ) is k (Dj |Di ). First order dataset generality or simply dataset generality of Di with respect to Dj at the layer k is given by, gk (Di , Dj ) = k (Dj |Di ) (Dj |r) (1)

This indicates the level of performance that is achieved by Dj using N - k layers worth of prejudice from Di and k layers worth of features from Di combined with k layers of novel knowledge from Dj together. Note that the generality is calculated for the base dataset as a measure of how the re-train performs with the prejudice of the base dataset. gk (Di , Dj ) > gk (Di , Dl ) indicates that at k layers, Di provides more general features to Dj than to Dl . Conversely, when initialized by n(Di |r), Dj has an advantage in learning than Dl . Note that, gk (Di , Di )  1 k . gk (Di , Dj ) for i = j might or might not be greater than 1. If gk (Di , Dj )  1 for i = j , it indicates that Dj is at least very similar to Di (such as the case considered by Yosinski et al.) and at most a perfect generalizer of Di [26].

3.2. Class generality
Di and Dj need not be entire datasets but can also be just disjoint class instances of the same dataset that is split in two. These generalities will tell us if particular classes are themselves more general than others. For instance, we divided the MNIST dataset into two parts. The first part contained the classes [4, 5, 8], the rest were contained by the second part2 . We performed the generality experiments
2 We chose this combination of classes strategically after trail and error as these are the most general among the classes and exaggerate the effect.

with MNIST[4, 5, 8] as base, which was trained over a random initialization. We re-trained this prejudiced network using the second part with the same experiment design as above. We defined class generality as the generality, of a class or a set of classes, retrained on the prejudice of the other mutually exclusive classes. We repeated this experiment several times with decreasing number of training samples per-class in the retrain dataset of MNIST [0, 1, 2, 3, 6, 7, 9]. All the while, the testing set remained the same size. This implies that the prejudiced network retrains on a much smaller dataset and tests on a much larger dataset. The re-train dataset had 7 classes. We created seven such datasets with 7p, p  [1, 3, 5, 10, 20, 30, 50] samples each. We now define subclass generality as the generality of these sub-sampled datasets (in each class we only consider a small random sample), retrained on the base of other mutually exclusive classes (MNIST[4, 5, 8]). . Initializing a network that was trained on only a small sub-set of well-chosen classes can significantly improve generalization performance on all classes, even if trained with arbitrarily few samples, even at the extreme case of one-shot learning.

batch sizes used in stochastic gradient descent of all the datasets used. No pre-processing were done on the images themselves except for cropping, resizing, normalizing. The images were all normalized to lie in [0, 1]. The character recognition datasets were all of a constant 28X 28 grayscale, the Caltech 101 vs. Cifar 10 experiments were performed ar 32X 32, RGB and the Caltech 101 vs. Colonsoscopy were at 128X 128, RGB. It is to be noted that the aim of the authors was not to set up the networks to achieve state-of-the-art. The authors did although try to achieve satisfactory performances on all base datasets involved before proceeding with the experimentation. Character Datasets. Our networks had three convolutional layers with 20, 20 and 50 kernels respectively. All the filters were 5 X 5 and were all stride 1 convolutions. The first layer didn't have any pooling. The second and the third layer maxpool by 2 subsampled. All the layers used rectified linear units (ReLU ) activations [18]. The classifier layer was a softmax layer and we didn't use any fully connected layers. We used a dropout of 0.5 only from the last convolutional layer to the softmax layer [24]. We optimized a categorical cross-entropy loss using an rmsprop gradient descent algorithm [2]. For acceleration we used Polyak Momentum that linearly increases in range [0.5, 1] from start to 100 epochs [21]. Unless early terminated, we ran 200 epochs. We also used a constant L1 and L2 regularizer co-efficients of 0.0001. Our learning rate was a 0.01 with a multiplicative decay of 0.0998. CIFAR10 Vs. Colonoscopy. Caltech101 and Caltech 101 vs

3.3. Datasets Used
We designed these experiments across three board categories of datasets: 1. Character datasets that included MNIST [17], MNIST-rotated [15], MNIST-randombackground [15], MNIST-rotated-background [15], Google street view house numbers [19], Char 74k English [3] and Char 74k Kannada [3] 2. Natural image datasets that includes Cifar 10 and Caltech 101 [13, 6] and 3. Natural images against medical images that included in addition to Caltech 101 a Colonoscopy video qualitty dataset. We leave it to the reader to find for themselves details about the datasets from the original articles, but the setup we have used can be found in table 1. Although we chose only a handful of datasets, the intention of this article was only to show that such generality measures could be made. The scope of this article was not to benchmark various publicly available popular datasets. Neither was it to make suggestions specific to types of datasets.

3.4. Network architecture and learning
We used one standard network architecture for all character datasets and experiments, one for Cifar 10 vs. Caltech 101 and another standard for Caltech 101 vs. Colonoscopy. The setup we have used can be found in table 1. The network architectures, learning rates and other details are provided below. The experiments were conducted on a Macbook Pro Laptop using an Nvidia GT 750M GPU, for character datasets and on an Nvidia Tesla K40 GPU for the others, with cuDNN v3 and Nvidia CUDA v7. Table 1 shows the train-test-validation splits and the

For this task, the networks had five convolutional layers with 20, 20, 50, 50 and 50 kernels respectively. We also had a last fully connected layer of 1800 nodes, which also had a dropout of 0.5. All the filters were 5 X 5 and were all stride 1 convolutions. Only the last layer maxpool by 2 subsampled. All the layers used rectified linear units (ReLU ) activations [18]. All CNN and MLP layers were also batch normalized [11].The classifier layer was a softmax layer and we didn't use any fully connected layers. We used a dropout of 0.5 only from the last convolutional layer to the softmax layer [24]. We optimized a categorical cross-entropy loss using an rmsprop gradient descent algorithm [2]. For acceleration we used Polyak Momentum that linearly increases in range [0.5, 0.85] from start to 100 epochs [21]. We use a learning rate of 0.001 for the first 150 epochs and then fine tune with a learning rate of 0.0001 for an additional 50 epochs unless early-terminated. Our learning decay of was subtractive 0.0005. Figure 4 shows more generality curves.

Dataset MNIST [17] MNIST-random-background [15] MNIST-rotated-background [15] NIST Special Dataset-19 [9] Google Street View House Numbers [19] Char 74k English [3] Char 74k Kannada [3] MNIST [4, 5, 8] MNIST [0, 1, 2, 3, 6, 7, 9] - p per-class CIFAR 10 [13] Caltech 101 [6] Colonoscopy3

Training 50,000 40,000 40,000 271,220 63,042 9,300 5,694 14,000 7p 40,000 5,080 2,700

Testing 10,000 12,000 12,000 271,220 63,042 3,355 1,314 2,500 7,000 10,000 3,048 900

Validation 10,000 10,000 10,000 271,220 63,042 305 1,752 2,500 7,000 10,000 1,016 100

Classes 10 10 10 62 10 62 100 3 7 10 102 2

Training Batch Size 500 500 500 191 399 305 438 500 500 500 254 100

Table 1. Datasets used and their properties.

4. Results and observations
4.1. Character Datasets
Figure 4 shows the generalities of MNIST-rotated-bg and Kannada prejudiced by all other the character datasets. For reference each plot also shows the generalization performance of a randomly initialized base convolutional network. The following are some observations of interest: While no dataset is qualitatively the most general, it is quite clear that MNIST dataset is the most specific. Rather, MNIST dataset is one that is generalized by all datasets very highly at all layers. Surprisingly, MNIST dataset actually gives better accuracy when prejudiced with other datasets, rather than when initialized with random, if all layers were allowed to learn. This is a strong indicator that all datasets contain all atomic structures of MNIST. NIST, Char74-English and Char74-Kannada follow similar generalization trends with almost all the datasets. With no degrees of freedom they all generalize rather poorly, but their generalities shoot up once one or many layers of the base networks are unfrozen. This indicates two properties: Firstly, these three datasets have similar manifolds. Secondly this also indicates that the last layers of the base datasets are extracting some particular quality of atomic structures that are present in the these datasets alone. Similarly, SVHN does not generalize in the first layer to most datasets, it generalizes much better in the latter layers. This is particularly noticeable in MNIST and Kannada. This further exemplifies the results. While initially one would have assumed that Kannada would be a general dataset, we observed the contrary. SVHN, Char74-English and Nist generalizes better to Kannada than even Kannada itself does. English characters seem to be a more general set than Kananda. While counter-intuitive, this result is immediately obvious when one pays close attention to the filers that are learnt and

300

Error Curve - Base Network:mnist-rotated-bg ; Retrained onmnist
1200

Error Curve - Base Network:mnist-rotated-bg ; Retrained onmnist-bg-rand

1000

800

250

600

400
Base Network -mnist-bg-rand 1-unfrozen 2-unfrozen 3-unfrozen all-frozen

200

200

0 0 20 40 60 80 100 120 140 160 180 200

150
Base Network -mnist 1-unfrozen 2-unfrozen 3-unfrozen all-frozen

100

50

0

50

100

150

200

Figure 5. Validation errors vs Epoch number for base-MNISTrotated-bg retrained on MNIST

the dataset itself. Kannada is dominated by predominantly curved edges only, whereas even MNIST has a multitude of unique atomic structures. Figure 5 demonstrates some interesting phenomenon that we discovered often. The gain in performance achieved, constantly decreases with increase in degrees of freedom. Through the epochs, unfreezing only the classifier layer, quickly converges. But while unfreezing, all layers converge at about the same number of epochs. We also observe, that MNIST retrained over MNIST-rotatedbackground, with the last degree of freedom does not learn antything at all. The error rate is within the statistical margin of error. This is a testament to the generality of MNISTrotated-background among the MNIST datasets. One might expect this because MNIST-rotated-background contains smooth background images (similar to natural image set) and MNIST characters that are rotated. These conditions provide for a good generality. For the intra-class experiment described in subsection 3.2 above, table 2 shows the accuracies. From the table one can observe that even with one-sample per class,

Retrainedmnist for different bases
1.02 1 0.98 1.1 1 0.9

Retrainedmnist-bg-rand for different bases

Generality

Generality

0.8 0.7 0.6 0.5 0.4
mnist-bg-rand MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

0.96 0.94 0.92 0.9
mnist MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

0

1

2

3

0

1

2

3

1.1 1 0.9

Number of Layers Unfrozen Retrainedmnist-rotated-bg for different bases

Number of Layers Unfrozen
1 0.9 0.8

Retrainedchar74-english for different bases

Generality

Generality

0.8 0.7 0.6 0.5 0.4 0.3 0 1
mnist-rotated-bg MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

0.7 0.6 0.5 0.4 0.3 0.2 0 1
char74-english MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

2

3

2

3

Number of Layers Unfrozen Retrainedchar74-kannada for different bases
1.2 1 0.8

Number of Layers Unfrozen
1 0.9 0.8

Retrainedchar74-english for different bases

Generality

Generality

0.7 0.6 0.5 0.4 0.3
char74-english MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

0.6 0.4 0.2 0

char74-kannada MNIST random background MNIST MNIST rotated background NIST Special Dataset-19 Google Street View House Numbers Char 74k English Char 74k Kannada

0

1

2

3

0.2

0

1

2

3

Number of Layers Unfrozen Retrained CALTECH101 for different bases
1.1 1 0.9
1.6 1.4 1.2

Number of Layers Unfrozen
Caltech 101 vs. Colonoscopy

Generalities

0.8 0.7 0.6 0.5 0.4 0.3 0 1 2 3 4 5 6
CALTECH101-base CALTECH101 CIFAR10

1 0.8 0.6 0.4 0.2
caltech 101 on caltech 101 caltech 101 on colonoscopy colonsoopy on colonoscopy colonoscopy on caltech101

0

1

2

3

4

5

6

Figure 4. Generalities of datasets not shown in the actual paper. The dark line represents the accuracy of n(D|r). Please zoom on a computer monitor for closer inspection.

a 7-way classifier could achieve 22% more accuracy than a randomly initialized network. It is note worthy that the

last row of table 2 still has 100 times less data than the full dataset and it already achieves close to state-of-the-art ac-

p 1 3 5 10 20 30 50

base Random MNIST[458] Random MNIST[458] Random MNIST[458] Random MNIST[458] Random MNIST[458] Random MNIST[458] Random MNIST[458]

k=0 73.07 83.61 90.98 91.55 95.52 96.5 96.38

k=1 73.91 87.2 92.98 93.71 95.52 97.34 97.40

k=2 76.37 85.7 92.6 93.82 97.07 97.35 97.71

k=3 55.61 77.52 73.34 87.6 83.32 92.07 81.31 95.08 87.77 96.78 88.62 97.45 90.78 97.38

4.2. CIFAR 10 vs. Caltech 101
From figure 4 we observe that Caltech 101 doesn't generalize to Cifar 10, which is surprising because Caltech 101 has a lot more classes. One would expect it to be more general. Its quite the opposite because Caltech 101 although has a lot of classes, the variability of each class is not as much as the variability in the Cifar 10 dataset. But it is altogether a serendipitous result that Cifar 10 is more general than Caltech 101 on the lower layers. However after three layers of obstination, we find that when the generalities crosses 1, the effect nullifies and reverses slightly. Even though the low-level features are more general in Cifar 10, Caltech 101 generalizes more on higher layers.

4.3. Caltech 101 vs. Colonoscopy
The colonoscopy dataset's labels identify if a image is deemed to be of a quality that is good enough so as to make a diagnosis on the pathology of that particular image. Figure 7 show the filters learnt by Caltech 101 base network and Colonoscopy base network for the exact same architecture from random initialization. Two things are immediately apparent from the learnt filters that while Caltech 101 learns more structured and organized shape features, Colonoscopy dataset learns at first sight what appears to be unstructured blob detectors and detectors for dark colors. These features still produce state-of-the-art accuracy on the dataset. On observation of the activations produced after the first layer, and from observations of images and their labels, one can immediately recognize that what the network is learning is indeed changes in brightness patterns. Most often the video quality in colonoscopy is affected because of saturation when too much light is thrown at a scene. The quality is also affected due to light reflection from bodily fluids that is also noticeable in the activations. As also can be noticed that most of the filter colors are yellowish or blueish. On an colonoscopy video most often the video is also labelled poor quality when these colors are present, as these colors are often present mostly because of scattering and reflections. Having made these observations one would arrive at the obvious conclusion that neither dataset generalizes the other. This was indeed the result observed from figure 4. Although, Caltech 101 seem to generalize a bit better for even though it predominantly learns shapes, it learns some color features also.

Table 2. Sub-sample experiment and its generalization accuracies for different layers of freezing. The re-train network was MNIST[0, 1, 2, 3, 6, 7, 9]. For obvious reasons random initializations are trained only with all layers unfrozen, hence the missing values.

Figure 6. Sub-class generalities for MNIST [4, 5, 8]

curacy even when no layer is allowed to change. This is a remarkably strong indicator that the classes [4, 5, 8] generalizes the entire dataset. Figure 6 mimics the same. We also observed that once initialized with a general enough subset of classes from within the same dataset, the generalities didn't vary among the layers like it did when we initialized with data from outside the mother dataset. We also observed that the more the data we used, more stable the generalities remained. Point of take away from this experiment is that if the classes are general enough, one may now initialize the network with only those classes and then learn the rest of the dataset even with very small number of samples.

4.4. Summary of results
From all these results and observations, we could summarize that one should prefer to initialize with a general dataset that might have a lot of variability or rather generality in data, when attempting to train with very few number of samples. Whenever possible one must initialize the network trained by a general dataset as this always boosts generalization performance. When there are biased datasets

Figure 7. From left to right, separated by a line are filters learnt by a base Caltech101 base colonoscopy, sample images from the colonoscopy dataset and their first activation for a filter that detects smooth areas of brightness.

with large number of samples in some classes and fewer in others, one should train the most general classes first. Once the network is well-prejudiced one should start introducing the classes with fewer number of and less general samples, provided the general class is general enough.

5. Conclusions
In this paper, we used the performance of CNNs on a dataset when initialized with the filters from other datasets as a tool to measure generality. We proposed a generality metric using these generalization performances. We used the proposed metric to compare popular character recognition datasets and found some interesting patterns and generality assumptions that add to the knowledge-base of these datasets. In particular, we noticed that MNIST data is one of the most specific dataset. We also found that Char74k Kannada is less general than English datasets. We also calculated generality on class-level within a dataset and conclude that a few well-chosen classes used as pre-training could build a network that is well-initialized that even with 100 times less samples, we could learn the other classes. We also provided some practical guidelines for a CNN engineer to adopt. After performing similar experiments on popular imaging datasets and medical datasets, we made similar serendipitous observations.

References
[1] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012. [2] Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for non-convex optimization. arXiv preprint arXiv:1502.04390, 2015. 5 [3] T. E. de Campos, B. R. Babu, and M. Varma. Character recognition in natural images. In Proceedings of the International Conference on Computer Vision Theory and Applications, Lisbon, Portugal, February 2009. 3, 5, 6

[4] V. Escorcia, J. C. Niebles, and B. Ghanem. On the relationship between visual attributes and convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1256­1264, 2015. 1 [5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303­ 338, 2010. 2 [6] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding, 106(1):59­70, 2007. 3, 5, 6 [7] K. Fukushima and N. Wake. Handwritten alphanumeric character recognition by the neocognitron. Neural Networks, IEEE Transactions on, 2(3):355­365, 1991. 1 [8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580­587. IEEE, 2014. 2 [9] P. J. Grother. NIST Special Database 19 Handprinted Forms and Characters Database. 1995. 6 [10] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 3 [11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 4, 5 [12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675­678. ACM, 2014. 2 [13] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images, 2009. 2, 3, 5, 6 [14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097­1105, 2012. 1 [15] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473­480. ACM, 2007. 3, 5, 6 Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541­551, 1989. 1 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998. 3, 5, 6 V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807­814, 2010. 5 Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5. Granada, Spain, 2011. 3, 5, 6 A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 427­436, 2015. 1 B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1­17, 1964. 5 O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), pages 1­42, April 2015. 2 K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 2 N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014. 5 C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014. 1 J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems, pages 3320­3328, 2014. 2, 4

