On a Preemptive Multi-Class Routing Scheme with
Protection Paths for WDM Networks
A. Sen, B. H. Shen, B. Hao and H. Jayakumar

S. Bandyopadhyay

Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287, USA
Email: {asen, bao, binhao, harishkumar}@asu.edu

School of Computer Science
University of Windsor
Windsor, Ontario, Canada
Email: subir@cs.uwindsor.ca

Abstract— A large optical network may carry multiple traffic
classes with different priorities and fault-tolerance requirement.
A higher priority traffic may require to have a backup path so
that the traffic can be switched quickly to this path in case of a
failure in the primary path. The lower priority traffic classes
may not have any such requirement. In the path protection
schemes currently in use for the WDM networks, a backup path
is computed for all traffic, whenever a primary path is established
between a source-destination pair. The resources needed for
communication using the backup path are reserved (or set aside)
for data communication between a source-destination pair, and
are utilized only when the primary path is unavailable due to a
failure in the network. The traffic carrying capacity of a network
can be increased, if the resources set aside for the backup paths
are utilized for data communication. In this paper we propose
a path protection scheme for networks with multiple classes of
traffic. The key features of our scheme are (i) Not all traffic
classes have backup paths - only higher priority classes have
backup paths, (ii) Primary paths of lower priority traffic share
wavelengths with secondary paths of higher priority traffic, and
(iii) Lower priority traffic can be preempted by higher priority
traffic in case of a failure.
The sharing of a wavelength between the primary path of
a lower priority communication with the secondary path of a
higher priority communication allows the network to satisfy more
call requests, thereby reducing the call blocking probability. We
provide a mathematical programming formulation for computing
the primary and backup paths for call requests in a dynamic
environment. We also compute the call blocking probability of
our scheme and compare it with the call blocking probability of
the conventional scheme through simulation. Our experimental
results show significant gain by the proposed scheme over the
conventional scheme.

I. I NTRODUCTION
Survivability of all-optical networks has become an important area of research in recent years as evidenced by
the increased attention the topic has received among the
researchers [5], [14], [17]. Two techniques, protection at the
WDM layer and restoration at the IP layer, have emerged
as the leading contenders for fault management in optical
networks [14], [15]. In particular, the protection scheme has
received considerable attention [2], [3], [4], [6], [11], [12],
[13].
In most of these studies, the researchers have focused
their attention on a single class of traffic. However, a large
network may have to carry traffic belonging to multiple service

classes with different priorities and survivability requirements.
In the currently existing schemes, on the arrival of a new call
request, the network manager examines if sufficient resources
(wavelengths) are available to establish both the primary and
the backup paths for this request. If sufficient resources are not
available, the call is not admitted in to the network (blocked).
It has been established in [7], [9] that such capacity-based
admission control (CAC) is less efficient than policy-based
admission control (PAC) where a policy based admission
criteria is used minimizing the significance of the order of
arrival.
In this paper we propose a policy based admission control
and fault-tolerant routing scheme for optical networks. The
key features of our scheme are:
•
•
•

•

Routing of multiple traffic classes with different priorities
and survivability requirements
Not all traffic classes have backup paths - only higher
priority classes have backup paths
Primary paths (the only paths) of lower priority traffic
share wavelengths with secondary paths of higher priority
traffic
If necessary, lower priority traffic can be preempted by
higher priority traffic

Our scheme excludes the possibility of multiple simultaneous failures, as such events are very rare.
In most of the studies on path protection in optical networks,
if the primary paths of two call requests do not share a link,
then the backup paths of these requests are allowed to share
a channel (wavelength). This assumption is reasonable as the
probability of multiple simultaneous failure is very small. Such
sharing is referred to as backup multiplexing in [12], [13].
For efficient use of network resources [12], [13] also suggest
primary-backup multiplexing where primary path of some call
request (say, R1) is allowed to share channels with the backup
path of some other call request (say R2). However allowing
such multiplexing to take place, one can no longer provide
100% guarantee for protection under a single-fault model. This
is true because in case of a failure in the primary path of
R2, there will be a contention between the backup path of
R2 and the primary path of R1, and only one of them can
succeed, resulting in premature termination of the other. The

0-7803-7802-4/03/$17.00 © 2003 IEEE

1417

A

B

C

D

E

λ1

λ2

λ1
F

λ1

Backup path

λ2
G

Primary path

H
λ2

Fig. 1.

Survivable routing in WDM networks

routing algorithm proposed in [12], [13] attempts to reduce
such events and shows significant performance gain at the
expense of reduced performance guarantee.
The scheme presented in this paper has one similarity with
the scheme in [12], [13] and that is the notion of primarybackup multiplexing. However, the schemes are different in
many respects:
• The scheme in [12], [13] consider single class traffic
whereas this paper considers multi-class traffic
• The primary-backup multiplexing used in this paper takes
place between requests belonging to two different classes
of traffic. Since [12], [13] consider only a single class
traffic, their primary-backup multiplexing takes place
between requests belonging to the same class.
• The scheme proposed in [12], [13] can only provide x%
guarantee where (x < 100%) for all traffic. The scheme
proposed here provides 100% guarantee for higher class
traffic while providing a best-effort service for lower class
traffic.
• In the scheme proposed in [12], [13], any call may be
pre-empted, whereas in this scheme higher class calls will
never be pre-empted under single-fault sceanrio.

II. P OLICY BASED A DMISSION C ONTROL AND
FAULT-T OLERANT ROUTING
In our model, we consider multiple classes of traffic,
C1 , . . . , Cn . However, these n classes can be divided into
two groups: A = {C1 , . . . , Cm }, the classes that will have
backup paths and B = {Cm+1 , . . . , Cn }, the classes that will
not have a backup path. The priority among the classes may
be C1 ≥ C2 . . . ≥ Cn . However, the path finding algorithm
will essentially be concerned with only two classes A and B,
as it will have to decide whether or not to establish a backup
path. Thus the multi-class traffic routing can essentially be
viewed as a two class traffic routing problem. We next state
the standard policy (Scheme 0), a variation of the standard
policy (scheme 1) and our proposed policy (Scheme 2):

Scheme 0
• Multi-class traffic is not supported
• All traffic will have two (primary and backup) paths
• Primary-secondary multiplexing is not allowed
• No preemption is allowed
Scheme 1
• Multi-class traffic is supported
• Not all traffic will have two paths - only the higher classes
(A) will have backup paths
• Primary-secondary multiplexing between higher and
lower class traffic is not allowed
• A newly arrived call in (A) cannot preempt an ongoing
call belonging to (B), just to establish the (A) call
Scheme 2
• Multi-class traffic is supported
• Not all traffic will have two paths - only the higher classes
(A) will have backup paths
• Primary-secondary multiplexing between higher and
lower class traffic is allowed
• A newly arrived call in (A) cannot preempt an ongoing
call belonging to (B), just to establish the (A) call
• An ongoing call in (A) can preempt an ongoing call in
(B) in case of a failure in the primary path of the (A)
call and if the backup path of (A) call shares a channel
with the (B) call.
The working of the scheme 2 is illustrated with the help
of an example shown in figure 1. For the sake of simplicity,
wavelength translation is not used in the figure. Suppose that
in this network each fiber is able to carry only two wavelengths
λ1 and λ2 and all communications in progress at time t, as
shown in figure 1 have high priority. If there is a low priority
request for communication from E to F requiring no backup
path, such a request can be satisfied by assigning the path E–
D–F and a wavelength λ2 . This low priority communication
using the path E–D–F may continue as long as the network
is fault-free. In the event of a failure of the link H–G the
high priority lightpath from H to F will be affected and the
backup path H–D–F must be used for H to F communication
using the wavelength λ2 . In this case the low priority lightpath
from E to F must be pre-empted so that the channels used
by that lightpath may be reclaimed and be allocated for the
establishment of the backup path of the H to F connection.
Our goal in this paper is to demonstrate the superior
performance of the proposed policy (Scheme 2) over the
standard policy (Scheme 0). Since Scheme 0 does not support
multi-class traffic whereas Scheme 2 does, it may not be very
meaningful to compare the performance of Scheme 2 against
Scheme 0. Accordingly, we have introduced a variation of the
standard policy that supports multi-class traffic (Scheme 1) and
measure the performance of Scheme 2 against it. It may be
noted that the key difference between Scheme 2 and Scheme
1 is that the first allows primary-backup multiplexing whereas
the second does not.
The blocking probability is a measure of the probability that
an incoming call request will be denied. It depends on factors

1418

3

III. P ROBLEM F ORMULATION

4

2

5

1

Primary Path
12

6

Backup Path
7

11

10

8
9

Fig. 2.

12 node Ring Network

such as offered traffic, network resources and connection
control algorithms [18]. The blocking probability can be used
as a metric for evaluation of various protection schemes. A
scheme which has a lower call blocking probability makes
better utilization of the network resources.
Before one deploys a new scheme, one would like to
perform a cost-benefit analysis of the scheme. One would
like to know the maximum benefit of the scheme and the cost
associated with it. The benefit in this context is the reduction
of the blocking probability in the proposed scheme (Scheme 2)
over Scheme 1. In the standard scheme, the resources allocated
for the backup path are locked and no other call request can
use these resources. Next we show that the potential benefit
of the proposed scheme is limited only by the topology of the
network and can be very significant. Also, the additional cost
is practically negligible.
Consider the ring network shown in figure 2. For the sake
of simplicity, we assume that each link supports only one
wavelength and all traffic in the network belongs to one of
two possible traffic classes - A and B, with type A being
the high priority traffic. In addition, the lower priority traffic
(type B) do not maintain a backup path. Suppose node 1 wants
to establish a type A connection to node 5. The primary
path can be 1 → 2 → 3 → 4 → 5 and the backup path
1 → 12 → 11 → 10 → 9 → 8 → 7 → 6 → 5. Suppose now
several new type B call requests arrive: 12 to 11, 11 to 10,
10 to 9, 9 to 8, 8 to 7, 7 to 6 and 6 to 5. In the scheme 2
all these call requests can be satisfied, as in this scheme type
B calls are allowed to share resources with the backup paths
of type A call from 1 to 5. In Scheme 1 all type B calls will
be blocked as in this scheme the resources allocated to the
backup paths of a type A call is locked and is not allowed to
share with any type B calls. Thus in scheme 1, 7 out of 8 calls
will be blocked, whereas in scheme 2 none of the calls will be
blocked. The gains of scheme 2 can be even more dramatic if
the ring is of larger size.
In the section on simulation results and discussion, we
analyze the additional computational cost of Scheme 2 over
Scheme 1 and show that it is practically negligible.

The physical topology of the network is represented by a
directed graph Gp = (Vp , Ep ), where Vp is the set of nodes
and Ep is the set of edges. It is assumed that each directed edge
representing a fiber link, can support up to W wavelengths.
In this paper, we consider a dynamic situation where call
requests arrive at different points in time and are allotted
network resources, if the call can be established. When the
call terminates, the resources used by the call are returned
to the network so that they can be reallocated to other call
requests. We assume that all nodes in the network have full
wavelength translators so that, to establish a new lightpath
from a source S to a destination D using path P, it is only
necessary to ensure that each fiber in the path P from S to D
has at least one channel not used by any existing lightpath.
In the previous section we indicated that a multi-class traffic
routing problem can essentially be viewed as a two class traffic
routing problem. Suppose that there are two types of call
requests. Type A (B) call requests have higher (lower) priority.
We require type A calls to have a primary path as well as a
protection path. Type B calls do not have a protection path
and may be dropped if necessary. A type B call is allowed
to use channels allocated to the protection paths of type A
calls. We consider the problem of establishing a primary path
and a backup path between the source destination node pair
(s, d) for a type A call request at time t. We note that several
type A and type B calls may already be in existence at time t.
Due to the existence of such traffic in the network, sufficient
resources may not always be available to establish the new
type A call request from s to d. Since the new request is of
type A, we need to find both the primary and the backup path
for this request. We are not separately stating the problem
of establishing a type B call since there is only one path to
establish and it is fairly straight forward.
We classify the W channels on the fiber for the link (i, j)
into 5 categories as follows:
(i) w((i,j),1) : the number of channels used to establish the
primary paths of type A
(ii) w((i,j),2) : the number of channels used to establish the
paths of type A only
(iii) w((i,j),3) : the number of channels shared by the backup
paths of type A calls and paths of type B calls
(iv) w((i,j),4) : the number of channels used by the backup
paths of type A calls only
(v) w((i,j),5) = W - w((i,j),1) - w((i,j),2) - w((i,j),3) w((i,j),4) : the unassigned (or free) channels
The mathematical programming methods of the next section
utilizes this classification of channels for path establishment.
IV. M ATHEMATICAL P ROGRAMMING S OLUTION
A mathematical programming [1] formulation to the routing
problem is presented in this section. This formulation seeks
to find the primary and the secondary paths for a type A
call request using the least number of channels, at a time
when a number of lightpaths have already been established

1419

in response to call requests made earlier. It may be noted
that the mathematical programming solution finds a primary
and backup path for a call request Rn (if sufficient resources
are available), when requests R1 , . . . , Rn−1 , made earlier
are already in progress (some of these calls may even have
terminated at the time of arrival of Rn ). The path computed
by the mathematical programming does not disturb the calls
already in progress. It tries to find paths for the new call using
only the available resources.
The integer linear programming (ILP) formulation for finding link disjoint primary and secondary paths between the
source-destination node pair (sp , dp ) is given in figure 3.
It is assumed that the paths (si , di ), 1 ≤ i ≤ p − 1 have
already been established and the p-th path is about to be
established. The index r is used to identify the primary or
the secondary path, r = 1 indicates a primary path and r = 2
indicates a secondary path. A binary indicator variable xk,r
i,j is
associated with each link (i, j) of the graph G = (V, E). If
the variable xk,r
i,j = 1, it indicates that the link (i, j) is a part
of the primary path of the (sk , dk ) path when r = 1 and the
secondary path when r = 2. If xk,r
i,j = 0 then it is not a part
of such a path.
The constraint (i) of the ILP establishes a path from the
source to the destination node. The constraint (ii) ensures
that the primary and the backup paths are link disjoint. The
constraint (iii) ensures that the primary path is established from
only the free channels. The binary variable yk in constraint
(iv) is used to indicate disjointness of the path (sp , dp ) with
the previously established paths (sk , dk ), 1 ≤ k ≤ p − 1.
The variable yk takes on a value 1 if the path (sp , dp ) is not
disjoint with the path (sk , dk ), 1 ≤ k ≤ p − 1. Otherwise,
yk can take any value 0 or 1. The constraint (v) restricts the
number of wavelengths available for the establishment of a
backup path on a link. The subtraction on the right hand side
of the constraint is due to the fact that protection path sharing
is not allowed when primary paths are not disjoint.
If the above ILP produces a solution, it implies that protection routing is possible for the new call request. If the ILP
fails to produce a solution, we can infer that such a primarybackup path-pair does not exist. It may be noted that shortest
path routing was used for the establishment of type B traffic.
V. S IMULATION E NVIRONMENT
We evaluated the relative performance of Scheme 1 and
Scheme 2 through extensive simulation. It may be noted that
the scheme 1 does not allow channel sharing between a type B
path and a type A backup path. The WDM network simulator
for the multi-class services environment was developed on
a SUN Ultra machine using programming language C and
optimization package CPLEX 6.5. The simulator consists of
three major components.
• Traffic Generator
• Path Finder
• Channel and Wavelength Manager (CWM)

M inimize

n 
n 
2


xp,r
i,j

i=1 j=1 r=1

Subject to the following constraints:

 −1

 p,r
 p,r 
1
(i)
xi,j −
xj,i =
0


{i|i∈V }
{i|i∈V }

(ii)

2

r=1

if j is the source
if j is the dest.
if j is any other
node

xp,r
i,j ≤ 1, ∀ (i, j) ∈ E

(iii) xp,1
i,j ≤ w((i,j),5) , ∀ (i, j) ∈ E
(iv)



{(i,j)∈E}

(v) xp,2
i,j ≤

k,1
xp,1
i,j xi,j ≤ | E | yk , 1 ≤ k ≤ p − 1

5

l=2

w((i,j),l) −

p−1


k=1

yk xk,2
i,j , ∀ (i, j) ∈ E

(vi) xp,r
i,j = 0/1, r = 0/1, ∀ (i, j) ∈ E
(vii) yk = 0/1, 1 ≤ k ≤ p − 1
Fig. 3.
problem

Mathematical programming formulation of the protection path

The component Traffic Generator generates streams of end-toend calls on the network based on a number of parameters such
as number of calls, ratio of type B to type A calls (traffic mix,
denoted by δ), call arrival rates, duration of type A and type
A calls. The call arrival is assumed to be a Poisson process
with inter-arrival times following an exponential distribution
with mean λ1 . The duration of type A and type B calls
were drawn from a uniform distribution with means λ2,1 and
λ2,2 respectively. For some of our simulation experiments we
assumed λ2,1 = λ2,2 and for some others λ2,1 = λ2,2 . These
five parameters are used to investigate the impact of offered
load (traffic patterns) [16] on the two schemes. In most our
experiments, the network consists of nodes up to 50 with
average degree at most 6. In the next section we present
simulation results on two specific network topologies: (i) a
ring network and (ii) an undirected version of the ARPANET
with 20 nodes and 32 links given in [18]. Each node in the
network has the same probability of initiating a connection
request. Once a call request from a source node s to destination
node d arrives, the second component of the simulator, the

1420

Path Finder, first determines the type of the call. Depending
on the type of the call (A or B), it either invokes CPLEX
with the mathematical program given in section 4 to find the
primary and the backup path for a type A call, or uses a much
simpler ILP to find the path for a type B call. The paths for the
call requests are established taking into account the available
resources (wavelengths) of the network at that point of time.
In most of the experiments, the ILP formulation to route a
call consists of 400 constraints and 15,000 variables or less.
The routes can be found in less than a second on a Sun Ultra
5 workstation. The additional cost of route computation for
scheme 2 over scheme 1 is negligible as they use the same
number of constraints and variables for the same set of calls
and network states. The Channel and Wavelength Manager
(CWM) is responsible for resource allocation/deallocation. It
keeps track of all calls in progress and manages the wavelength
assignment on all links of the network. Since the Path Finder
component only determines which links should be used for call
establishment, the CWM determines the type of wavelength
to be used for call establishment for each link on a route
subject to a predetermined priority rules. For a link (i, j) on a
protection path of type A call, the simulator searches for the
availability of wavelengths starting from w((i,j),5) , followed
by w((i,j),4) , w((i,j),3) , and w((i,j),2) respectively.
VI. S IMULATION R ESULTS AND D ISCUSSION
In this section we present the results of our simulation
experiments and draw conclusions from these results. The
parameters number of calls, traffic mix, λ1 , λ2,1 and λ2,2
(described in the previous section) were used to vary the
offered load to the network. In addition to these parameters,
we also have the number of wavelengths, (λ) available on
each link as a parameter. The simulation experiments were
conducted in two settings, (i)random and (ii) controlled. In the
random setting, random generators were used to generate offered load to the network by varying the parameters indicated
earlier. The results of the random setting experiments with
three different values of number of wavelengths λ = 4, 6, 8
for the 20 node, 32 link ARPANET topology given in [18]
are presented next. If the blocking probability in scheme
1 is X and the blocking probability in scheme 2 is Y ,
then the percentage performance improvement is computed as
follows: Improvement = (X−YX)×100 . It may be noted that
Improvement can at most be 100%, and this will happen when
X > 0 and Y = 0.
From the results shown in tables I we can make three
observations.
Observation 1: In every single experiment, Scheme 2 outperforms Scheme 1. Considering the fact that the additional
overhead for Scheme 2 is very small, it is clearly advantageous
to use Scheme 2 over Scheme 1.
Observation 2: The improvement made by Scheme 2 varied
from as low as 1.28% (for λ = 4) to as high as 80% (for
λ = 8). The average improvement turned out to be 23%
approximately.

Observation 3: The average improvement increases with
higher values of λ.
Since we observed a wide variation of improvement, we
wanted to find out the cases where the Scheme 2 would
perform significantly better than Scheme 1. In order to find
such cases we conducted experiments in controlled setting. In
this setting we generated traffic such that the distance between
the source-destination node pair is large for type A calls
and small for type B calls. Also the duration of the type A
calls were made longer than the duration of the type B calls.
The rationale for these modifications were to ensure that the
unutilized resources (resources in the backup path) in Scheme
1 is high so that Scheme 2 has the opportunity to perform
significantly better than Scheme 1. To ensure that the distance
between the source-destination node pair is large for type A
calls and small for type B calls, two threshold values were
used. A request for a type A call is considered valid only
if the distance between the source-destination pair is above
the first threshold. A request for a type B call is considered
valid only if the distance between the source-destination pair
is below the second threshold. To ensure that the duration
of a type A call is longer than the duration of a type B call,
they were drawn from two uniform distributions, one having a
much higher mean than the other. The results of the controlled
experiments for a 20 node ring topology and for the 20 node,
32 link ARPANET topology in [18]. is presented in tables II
and III.
TABLE II
C ONTROLLED C ASE FOR R ING T OPOLOGY WITH WAVELENGTH λ = 2

Calls
200
200
200
200

δ
0.9
0.9
0.9
0.9

Parameters
λ1
λ2,1
2
400
3
400
3
400
3
400

λ2,2
3
3
5
10

Sch. 2
0.135
0.130
0.145
0.290

Block. Prob.
Sch. 1
Improv.
0.325
58.46%
0.475
72.63%
0.250
42.00%
0.445
34.83%

TABLE III
C ONTROLLED C ASE FOR ARPANET WITH WAVELENGTH λ = 2

Calls
200
200
200
200

δ
0.9
0.9
0.9
0.9

Parameters
λ1
λ2,1
2
500
3
100
4
1000
5
1000

λ2,2
2
3
4
5

Sch 2
0.145
0.145
0.130
0.085

Block. Prob.
Sch 1
Improv.
0.275
47.27%
0.355
59.15%
0.280
53.57%
0.140
39.29%

For the results presented in tables II and III the ordering of
the call requests were also controlled. Several type A calls with
long call durations were generated before short durations type
B calls. The controlled experiment results consistently showed
significant improvement in blocking probability in Scheme 2.
This shows that Scheme 2 under appropriate conditions can
significantly outperform Scheme 1.
The simulation results demonstrate the superiority of
Scheme 2 over Scheme 1. Since Scheme 1 considers multiclass traffic and lower class traffic do not establish backup

1421

TABLE I
R ANDOM C ASE FOR ARPANET WITH WAVELENGTHS λ = 4, 6 AND 8

Calls
500
500
500
500
500
500
500
500
500
500
500
500
500
500
500

Parameters
Traf. Mix λ1 λ2,1
0.5
2
8
0.5
2
10
0.5
2
12
0.5
2
15
0.5
2
18
0.5
3
8
0.5
3
10
0.5
3
12
0.5
3
15
0.5
3
18
0.5
4
10
0.5
4
12
0.5
4
15
0.5
4
18
0.5
4
18

λ2,2
8
10
12
15
18
8
10
12
15
18
10
12
15
18
18

Bloc.
Sch. 2
0.260
0.292
0.334
0.396
0.462
0.192
0.241
0.310
0.389
0.411
0.134
0.217
0.243
0.324
0.358

Prob. (λ = 4)
Sch. 1 Improv.
0.280
7.14%
0.318
8.18%
0.352
5.11%
0.418
5.26%
0.468
1.28%
0.228 15.79%
0.279 13.62%
0.336
7.74%
0.416
6.49%
0.422
2.61%
0.186 27.96%
0.240
9.58%
0.264
7.95%
0.345
6.09%
0.370
3.24%

paths, the call blocking probability in this scheme will clearly
be lower than that of Scheme 0 (the standard scheme). This
is so, because Scheme 0 supports only single class traffic
where all call requests have a backup path. Since Scheme
2 demonstrates superior performance over Scheme 1, it can
be inferred that it will demonstrate superior performance over
Scheme 0.
VII. C ONCLUSION
In this paper we have proposed a new preemptive scheme
for survivable routing in WDM networks. The key contribution
of this paper is to combine the notion of primary-backup
multiplexing of [12], [13] with multi-class traffic of [7], [9],
[10]. The combination of these two ideas allows one to give
100% guarantee for high class traffic while providing besteffort service to the low class traffic. Through analysis and
extensive simulations, we have demonstrated the advantageous
of this scheme over the traditional one. For brevity, we are
unable to provide more detailed results. Such results along
with some other policies are available in [8].

Bloc.
Sch. 2
0.128
0.176
0.262
0.306
0.386
0.056
0.095
0.161
0.230
0.314
0.020
0.063
0.043
0.209
0.276

Prob. (λ = 6)
Sch. 1 Improv.
0.162 20.99%
0.220 20.00%
0.310 15.48%
0.364 15.93%
0.436 11.46%
0.083 32.53%
0.128 25.78%
0.203 20.69%
0.271 15.13%
0.352 10.80%
0.034 41.18%
0.085 25.88%
0.068 36.76%
0.245 14.69%
0.318 13.21%

Bloc.
Sch. 2
0.036
0.060
0.140
0.210
0.304
0.004
0.015
0.059
0.125
0.202
0.002
0.020
0.043
0.093
0.307

Prob. (λ = 8)
Sch. 1 Improv.
0.060 40.00%
0.082 26.83%
0.178 21.35%
0.242 13.22%
0.332
8.43%
0.010 60.00%
0.030 50.00%
0.080 26.25%
0.160 21.88%
0.246 17.89%
0.010 80.00%
0.012 60.00%
0.068 36.76%
0.124 25.00%
0.341
9.97%

[8] H. Jayakumar, “On a pre-emptive scheme for optical networks in a
differentiated services environment”, MS Thesis, Dept. of Computer
Science and Engineering, Arizona State University, August 2002.
[9] S. Jeon, R. T. Alber, J. A. Copeland and Y. Pan,“Path selection with
class distribution information in the integrated network”, IEEE Communications Letters, vol.6,no. 2, February 2002.
[10] Q. Ma and P. Steenkiste,“Supporting dynamic inter-class resource sharing: A multiclass QoS routing”, Proceedings of IEEE Infocom’99, 1999.
[11] M. Medard, S. Finn and R. A. Barry, “WDM Loop-back Recovery in
Mesh Networks”, Infocom’99, New York, March, 1999.
[12] G. Mohan and A. K. Somani, “Routing dependable connections with
specified failure restoration guarantees in WDM networks”, Infocom’00,
2000.
[13] G. Mohan, C. S. R. Murthy and A. K. Somani, “Efficient algorithms for
routing dependable connections in WDM optical networks”, IEEE/ACM
transactions on Networking, vol. 9, no. 5, October 2001.
[14] S. Ramamurthy and B. Mukherjee, “Survivable WDM Mesh Netwoks”,
Proc. of IEEE INFOCOM’99, pp. 744-751, March, 1999.
[15] S. Ramamurthy and B. Mukherjee, “Survivable WDM Mesh Netwoks:
Part II -Restoration,”, ICC ’99, Vancouver, CA, June, 1999.
[16] R. Ramaswami and K. Sivarajan, Optical Networks: A Practical Perspective, Morgan Kaufmann, 1998.
[17] A. Sen, B. Hao, B. H. Shen and G. Lin, “Survivable Routing in WDM
Networks: Logical Ring in arbitrary Physical Topology”, Proc. of IEEE
ICC’02, New York, NY, May 2002.
[18] T. E. Stearn and Krishna Bala, Multiwavelength Optical Networks - A
Layered Approach, Addison Wesley, 1999.

R EFERENCES
[1] R. K. Ahuja, T. L. Magnanti and J. B. Orlin, Network Flows, Prentice
Hall, 1993.
[2] O. Crochat and J. Y. Le Boudec, “Design Protection for WDM Optical
Networks”, IEEE JSAC, Vol. 16, No. 7, Sept. 1998.
[3] G. Ellinas, A. G. Hailemariam and T. E. Stern, “Protection Cycles in Mesh
WDM Networks”, IEEE Journal on Selected Areas in Communications,
vol. 18, no.10, October 2000.
[4] A. Fumagalli et.al, “Survivable networks based on optimal routing and
WDM self healing rings”, Proc. of IEEE INFOCOM’99, pp. 726-733,
March, 1999.
[5] O. Gerstel and R. Ramaswami, “Optical layer survivability: A services
perspective”, IEEE Communications Magazine, vol. 38, no. 3, pp. 104113, March 2000.
[6] W. D. Grover and D. Stamatelakis,“Cycle-Oriented Distributed Preconfiguration: Ring-like Speed with Mesh-like Capacity for Self-planning
Network Restoration”, Proceedings of IEEE ICC’98, pp. 537-543, Atlanta, June7-11.
[7] S. Herzog,“Signaled preemption priority policy element”, IETF, RFC
2751, January 2000.

1422

Optimal Routing for Fast Transfer of Bulk Data
Files in Time-Varying Networks
S. Ganguly
NEC Laboratories America, Inc.
Princeton, NJ 08540
Email: samrat@nec-labs.com

A. Sen, G. Xue, B. Hao and B. H. Shen
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287, USA
Email: {asen, xue, binhao, bao}@asu.edu

Abstract— Efficient transfer of bulk data requires routing that
minimizes the net transfer time instead of providing flow level
bandwidth guarantees. In this work, we consider a realistic
scenario where available bandwidth for each link in a given
network is time varying. In such a network and for a given file
size, we provide an optimal algorithm that minimizes the total
time required to transfer the file from source to destination.
We further consider the problem where path shifting is allowed
to increase the net throughput for bulk data transfer. For this
problem, we provide a dynamic programming based optimal
algorithm that minimizes the number of path shifts required
to transfer the file in the specified time. Solution to the above
problems addresses both the performance and scalability issues
that arise in large bulk data transfer for long duration of time.

I. I NTRODUCTION
A. Background and motivation
The growth of bulk data in near future has necessitated the
research in the area of fast bulk data distribution. The recent
emergence of bulk data is visible both in the entertainment,
business and scientific world as in content distribution, network attached storage, mirror site update, eScience collaboration. Specially, in the context of eScience, in a recent interview
in ACM queue [2], Turing award winner Jim Grey indicated
for his work with astronomers, he has to exchange 2 terabytes
of data regularly with his collaborators and the data size is
expected to grow to 40 terabytes soon.
In contrast to streaming or related real-time application, bulk
data applications are elastic and do not care about packet delay
or rate/flow level guarantees. Instead, the only performance
metric of importance to a bulk data is the net transfer time.
In this work, we concentrate on exactly the same metric and
try to explore a relevant routing problem in bulk data transfer.
Specifically, we consider a realistic case of a network where
the link bandwidth varies over time. Such a scenario is of
practical importance where we focus on large-scale data files
requiring hours of transfer time. For example, assuming 10
megabytes per second of sustained data transfer over a period
of time, it may take more than 2 days to transfer a file of size
2 terabytes. One cannot assume that link bandwidth remains
stationary for that duration.
Focusing on this important issue, we address two relevant
problems: 1) how to find an optimal path from source to destination in a graph with time-varying bandwidth that minimizes

IEEE Communications Society

the total data transfer time and 2) if we adapt our path with
time, how do we minimize the number of path shifts. One
must observe that in a network with stationary bandwidth,
the solution is trivial and corresponds to computing a shortest
widest path by modifying Dijkstra’s algorithm. However, in
time-varying bandwidth scenario, wideness of a given path
varies with time and therefore the challenge is in finding a
path that is optimal over a specified duration of time. The
second problem, that we address is relevant to the scalability
issues if we allow shifting of path based on the bandwidth
variation.
Application of our proposed solution can be used in the
context of overlay networks or private networks. In this work,
we assume that knowledge of the bandwidth variation is
available for routing purpose. In a advanced reservation or
time based resource broker framework as was proposed in [8],
the available bandwidth for different time slots can be known
a priori. Corollary of our solution can be directly applied to
advanced reservation framework for bulk-data transfer where
the amount of bandwidth to be reserved at different slots can
be computed.
B. Related work
Various point-to-point routing schemes were proposed in
the past mostly targeting real-time applications. These realtime applications have their bandwidth demand either fixed or
is adaptive over a specified range and requires packet level
delay/jitter guarantees. Routing of such applications in an
advanced reservation framework as discussed in [8] although
considers time variation of link bandwidth but significantly
differs in the following aspect. However, the focus of this
paper is on bulk data applications that do not have fixed
bandwidth requirement as realtime flows. Existing work in
bulk data transfer has also looked at reliability issues in using
UDP based transport as in [3].
In the context of overlay network, several schemes were
proposed that through coordination among overlay nodes
results in efficient transfer of bulk data or content. In [4],
authors propose establishing parallel connections from multiple sources with complementary data sets to increase the total
download speed. Similar work is also proposed in [6], where
authors proposed efficient scheduling of bulk data upload from

1182

0-7803-8533-0/04/$20.00 (c) 2004 IEEE

multiple source sites where these sites can coordinate among
themselves. In another recent work presented in [7], authors
propose multiple multicast trees to minimize the replication
time of bulk content to multiple sites. These work focus
more on coordinating the content distribution and not the
routing of bulk data. The closest work with similar goals
as ours is presented in [1], where optimal single point-topoint path is created based on network congestion or path
failures. However, optimality of the path is based on present
network condition and not conditioned over the entire duration
of transfer. Further, the above solution does not address the
scalability issue when switching of paths are used to better
adapt to network bandwidth variability. Frequent switching of
path can introduce significant signaling in setting up new path
and can also lead to route instability and flapping. In this work,
we address the issue of how to minimize the switching of paths
with a limited increase in the data transfer time. Shortest path
problems in time dependent networks have been studied in [5],
[9] and the reference therein.
II. P ROBLEM F ORMULATION
When the file size is so large that the data transmission is
likely to continue over an extended period of time. Since we
want the data to be transferred to the destination as early as
possible, we may use the widest path1 the path with the largest
residual bandwidth for data transfer (maximum bandwidth
path). The problem with this approach is, since the residual
bandwidth changes with time, the widest path between the
source and the destination may change with time. We give
a concrete example to illustrate this point. Suppose a large
file needs to be transferred from A to B starting at 9 AM.
The widest path from A to B at 9 AM may be through the
intermediate node C. At 10 AM the widest path from A to B
may be through the intermediate node D and at 11 AM through
the node E. If we want the file transfer to be completed at the
earliest possible time, we should use A-C-B path at 9 AM,
A-D-B path at 10 AM, A-E-B path at 11 AM and so on.
The first question is then which is the single optimal path
that we can use for the entire duration of the file transfer. If
we even allow path changes whenever there is a change in
the residual bandwidth, the signaling overhead in setting up
new path maybe significant. We next formally state the above
problems in details.
To capture the notion of time varying residual bandwidth on
each link, we divide the time into slots of some fixed duration.
We consider a graph G = (V, E), representing the network
and time slots 1, 2, 3, ... of equal size. The availability of
network resources such as the residual bandwidth is tracked
at the granularity of a slot. The elements bl [i] of the vector bl
= {bl [1], bl [2], ...,} associated with each link l ∈ E, represents
residual bandwidth of link l at time slot i.
• Routing problem: Given the above definition of graph
G with bl for each link l ∈ E, a file size F , source s and
1 A widest path from the source to the destination with respect to bandwidth
bl , associated with link l, is a path P ∗ that maximizes the value of minl∈P bl ,
over all path P i.e., P ∗ = argmaxP minl∈P wl

IEEE Communications Society

•

destination d, compute a path from s to d that minimizes
the number of slots required to transfer F .
Path switching problem Given the above definition
of graph G with bl for each link l ∈ E, a file size
F , source s, destination d and specified slots Ts , find
the minimum number of path switching and associated
switching positions (in slots) such that the given file can
be transferred in Ts number of slots.

In the context of the above problems, we define Tmin as
the minimum number of slots necessary to transfer a file of
size F from the source to the destination. Tmin corresponds
to the case where widest path in each time slot is used to
transfer the file. Therefore, in our first problem, number of
slots corresponding to the optimal path is always greater than
or equal to Tmin . Similarly, Ts ≥ Tmin for a valid solution to
the path switching problem.
In discussion of the algorithms, we define bandwidth for
a link in terms of number of bytes/slot since we assume
bandwidth does not vary during the slot duration. In that case,
link bandwidth b also means the amount of data that can
be transferred in a slot which we refer here as throughput.
Therefore, we use link bandwidth, file size in a given slot
and throughput interchangeably throughout the text. In the
next two sections, we provide the solutions to the above two
problems.
III. O PTIMAL ROUTING ALGORITHM
The algorithm takes as input a directed graph G = (V, E)
and n×n×T matrix B. i, j, k-th entry of the matrix stores the
residual bandwidth of the link from the node i to node j during
time slot k, (1 ≤ k ≤ T ). The specified source and destination
nodes, s and d respectively. Based on the inputs, we refer this
algorithm as Route(G, B). We use the term T hroughput on
a given time interval in slots to be size of the data that can be
transferred in that time interval.
Algorithm Route(G,B) uses an algorithm, called FileRoute(G,B,F,p,q) , which finds if there is a route in the slots
[p : q] where file with size F can be transferred, to find
out the minimum number of time slots needed to send F , in
the following way. The maximum number of slots Tmax can
be easily found by taking the minimum residual bandwidth
among all slots for each link in the graph and finding the
widest path. Therefore, we can safely say that the minimum
number of slots using the optimal path will belong in the range
[Tmin : Tmax ]. We next perform a binary search on this range
to find the exact number of slots and the corresponding path.
Before presenting the algorithm FileRoute(G,B,F,p,q) , we
first define certain notations and operations for ease of explanations. Let EG (p, α) denote the subset of the edges of
G, consisting of edges whose residual bandwidth for time
slot p is less than α. Let G = G − EG (p, α) be the
operation of deleting the edges in EG (p, α) from graph G.
Let Q(p) denotes a Queue for a given slot p with enqueue
and dequeue operations defined with the additional property
that all elements in Q is in sorted decreasing order.

1183

0-7803-8533-0/04/$20.00 (c) 2004 IEEE

A. An example
Suppose we have a 6-node network, Graph0, as in the Fig.
1 We want to know if it can transfer a file of size 8, from
node 1 to node 6, within 3 time slots by calling the function
F ileRoute(Graph0, 1, 3, 8).
Iteration 1: (G = Graph0, F = 8, p = 1, q = 3)
Recursion: 0
Get G = G (1) − 10 by deleting all edges less than 10 from
G (1). (shown in figure 2)

Initialize Q(p, f, G): {NOTE: a subroutine called by
algorithm FileRoute }
For all slot r; i ≤ r ≤ j
Q(p) ← enqueue distinct bandwidth values (less than equal
to f) for slot p in G in descending order
Algorithm: FileRoute
FileRoute(G, B, i, j, F )
begin
Initialize Q(i, F, G)
If (i == j)
begin
β = throughput of widest path P from s → d in G
If (β > F ) return yes and path P
else return no
end
While (Q(i)! = ∅)
begin
α = dequeue (Q(p))
G = G − EG (p, α)
If (∃ path P from s → d in G ),
F =F −α
If (F ≤ 0)
return yes and path P
test = FileRoute(G , i + 1, j, F − α)
If test = yes return yes and path P {NOTE: P is the
path found by the previous recursive call.}

2

Source

IEEE Communications Society

4

5,
3,
1

6 Destination

1
10
,6
,4

3

Fig. 1.

1,6,1

5

5
6,
5,

Graph1: G of Recursion#0
2

Bandwidth
1

4

,5
,1
11

5
1

1

6
10
,6
,4

5

3

Fig. 2.

Graph2: G of Recursion#0

2

Bandwidth
1

5,1,3

4

,5
,1
11

5
1

6
10
,6
,4

3

5

5
6,
5,

Graph1: G of Recursion#0
2

Bandwidth Bandwidth
2
1

5
1

5,
3,
1

1

Fig. 3.

The algorithm starts with the p-th slot. The algorithm
recursively calls itself by modifying the graph, advancing to
the next slot and adjusting the size of the file to be transmitted.
The idea is as follows: During the first recursive call, it asks
if the slots i + 1 through j will be able to transfer a file of size
F −α. If the answer to this question is yes, then it implies that
total file of size F can be completely transferred in the slot
range [i : j]. This is true because the minimum bandwidth
associated with a link in the graph at this time is α since
G has no edges less than value α and there exist a path from
s → d in G . Accordingly, the first slot will be able to transfer
a file of size α along with F − α sent over the slots i + 1
through j.
The terminating condition is invoked when the recursion
reaches the last slot j or when the queue gets empty any
slot. At the last slot, if widest path is found with throughput
β > F (F here is the remaining part of file that needs to be
transferred), then it returns yes.

5,1,3

5
1,
1,

,5
,1
11

1,
1,
5

Now, we present the algorithm F ileRoute(G, i, j, F )
where G is a capacitated graph, i, j are slots where i ≤ j and
F is the file size. The output of the algorithm is a decision:
yes or no based on if a single path exists such that file size
F can be transferred in the time range of slots [i : j] or
not. If the decision is yes , the algorithm also provides the
optimal path. Let us now formally present the algorithm with
G , i, j, F initialized to G, p, q, f .

4

1

5,
3,
1

1

6
10
,6
,4

3

Fig. 4.

5

5
6,
5,

Graph2: G of Recursion#0

Iteration 2: (G = Graph0, F = 8, p = 1, q = 3) Recursion
:0– Since there is no path from node 1 to 6 in G , dequeue
Q(1) gives 5 and G is updated accordingly as showing in fig.
3. Being there is a path from 1 to 6 with throughput 5 resulting
in F = 8 − 5 = 3. Next FileRoute(G , p = 2, q = 3, F = 3) is
called.
Iteration 3: (G = Graph2, F = 3, p = 2, q = 3) Dequeue
Q(2) giving 3 and resultant graph G after updating using
operation G − 3 is shown in fig. 4.

1184

0-7803-8533-0/04/$20.00 (c) 2004 IEEE

5
1

5,1,3

2

Bandwidth Bandwidth
2
1

4

1
1

6
10
,6
,4

5
6,
5,

5

3

Graph1: G of Recursion#0

Fig. 5.

5,1,3

2

Bandwidth
1

4

5,
3,
1

5
1,
1,

,5
,1
11

5
1

Iteration 8: (G = Graph5, F = 7, p = 2, q = 3) Recursion
:1– Dequeue Q(2) gives 7 which makes no 1 to 6 path in the
updated graph shown in fig. 7.
Iteration 9: (G = Graph5, F = 7, p = 2, q = 3)
Recursion :1– Further dequeue Q(2) gives 6 which results
in a path in updated G show in in fig. 8. F is updated to
F = 7 − 6 = 1 and F ileRoute(G , 3, 3, 1) is called.
Iteration 10: (G = Graph7, F = 1, p = 3, q = 3)
Recursion :2– Since p = q, compute the throughput in slot3
by using the widest path algorithm which returns 1 that is
equal to F , hence return yes. .
Solution:
The Path: 1-3-5-6, found by widest path as the terminating
stage of the algorithm, is the solution. Thus the contribution for
1,2 and 3 slots in terms of throughput is 1, 6 and 1 respectively
adding to 8 (the input file size.

5,
3,
1

,5
,1
11

6
10
,6
,4

1,
1,
5

1

3

1,6,1

5
6,
5,

5

Graph2: G of Recursion#0

Fig. 6.

IV. PATH SWITCHING ALGORITHM
2

Bandwidth Bandwidth
2
1

5
1

6
3
1

1

6

5

3

Fig. 7.

Graph1: G of Recursion#0
2

Bandwidth Bandwidth
2
1

5
1

6
3
1

The algorithms compute the paths for data transfer from
the source to the destination, such that a file of size F can
be transferred from the source to the destination with fewest
number of path switching and the number of time slots used
for data transfer will at most be Tmin + δ, where δ is a prespecified threshold value.
In figure 9, we show an example with 10 time slots. The
number of points on the time line, where the paths can change
are known as the switching points. In this example, there are
9 switching points. In general, there will be n − 1 switching
points corresponding to n slots.

4

4

1

6
10
,6
,4

3

1,6,1

5

5
6,
5,

Time Slots

Fig. 8.

Graph2: G of Recursion#0

Fig. 9.

Iteration 4: (G = Graph2, F = 3, p = 2, q = 3) Recursion
:1– There is no path from node 1 to 6 in G (Graph3), therefore, next dequeue Q(2) gives 1 and corresponding updating
G = G − 1 gives the graph as shown in fig. 5. Since there is
a path from 1 to 6 in G with throughput 1, F gets updated
to F = 3 − 1 = 2 and FileRoute(G , p = 3, q = 3, F = 2) is
called.
Iteration 5: (G = Graph4, F = 2, p = 3, q = 3) Recursion
:2– Since p = q, throughput can be computed using the widest
path algorithm (WidestPath).
WidestPath() algorithm in this case returns 1. Hence, return
NO to the upper level of recursion, Recursion#1. Reset F = 3.
Iteration 6: (G = Graph2, F = 3, p = 2, q = 3)
Recursion :1– Since Q(2) is empty, return NO to the upper
level recursion, Recursion 0. Reset F = 8.
Iteration 7: (G = Graph0, F = 8, p = 1, q = 3)
Recursion :0– Dequeue Q(1) giving 1 and G is updated as
shown in fig. 6 Update F = 8 − 1 = 7. Recursive call to
F ileRoute(G , 2, 3, 7).

IEEE Communications Society

1

Switching Points
1

2
2

3
3

4
4

5
5

6
6

7
7

8
8

9
9

10

Slots and Switching points

It may be noted that the throughput may be different if
path switching is allowed. We use the matrix Max Thp[p, q,
r] in our algorithms. The (p, q, r)-th entry in the Max Thp
matrix indicates the maximum size file that can be transferred
between the time slots p and q (both inclusive) with r path
switching.
First we describe the MaxThroughput algorithm which we
will subsequently use in the main algorithm that finds the
minimum number of paths. The algorithm MaxThroughput
computes the maximum file size that can be transferred from
the source to the destination using one path (i.e., without path
switching) within the specified time slots by repeated calls to
the FindRoute(G,p,q,F). We note that the maximum file size
Fmax that can be sent over the range [p : q] can be obtained
by summing the throughput obtained by running widest path
algorithm for each time slot in [p : q]. Let us now define
G which is derived from G with same nodes and edges, but
capacity of edge e in G is given by c(e) = minr∈[p:q] be (r),
where be (r) is the capacity of the edge in original graph G
for time slot r. Let the Fmin be the file size that can be
sent over G using widest path algorithm. Next we perform

1185

0-7803-8533-0/04/$20.00 (c) 2004 IEEE

a binary search within in the range Fmin : Fmax using the
algorithm FileRoute(G,p,q,F), to find maximum F for which
FileRoutegives a yes decision.
As noted earlier, we will use M ax T hp[p, q, r] to indicate
the maximum size file that can be transferred between the
time slots p and q (both inclusive) with r path switching.
In addition, we will use a matrix switch on such that the
(p, q, r)-th entry in the switch on matrix will indicate the
position of the first switching point where path switching
takes place for maximum data transfer between the slots
p through q with r path switching. M ax T hp[p, q, r] can
be computed in the following way. [M ax T hp[p, q, r] =
maxp≤s≤q−r [M ax T hp[p, s, 0] + M ax T hp[s + 1, q, r − 1]]
The algorithm presented next uses dynamic programming
technique and computes the value of M ax T hp[p, T, r], by
varying r, the number of switches used and p, the index of the
starting slot for file transfer. It may be noted that once, r and
p is fixed, there may be several switching positions in the slot
interval p to T , where path switching can take place. s∗ stores
the position of first path switching for realization of maximum
data transfer between the slots p and T with r path switchings.

Algorithm: PathSwitching
Using compute M ax
TT hp[i, i, 0]f or1 ≤ i ≤ T ;
TotalThroughput = i=1 M ax T hp[i, i, 0];
If T otalT hroughput < F exit
else
Compute M ax T hp[1, T, 0]
If M ax T hp[1, T, 0] ≥ F → No path switching
else
Compute M ax T hp[i, T, 0]f or1 ≤ i ≤ T − 1;
for r := 1 to T − 1 do
for p := 1 to T − r do
M ax T hp[p, T, r] = maxp≤s≤T −r M ax T hp[p, s, 0] +
M ax T hp[s + 1, T, r − 1]];
s∗ = the value of s that maximizes
[M ax T hp[p, s, 0] + M ax T hp[s + 1, T, r − 1]];
switch on[p, T, r] = s∗ ;
If M ax thp[p, T, r] ≥ F
return switch on[p, T, r]; EXIT;
A. Example of Path Switching Algorithm
We consider the network shown in figure 1. Suppose
that we want to transfer a file of size 10. In this
graph M ax T hp[1, 1, 0] = 5, M ax T hp[2, 2, 0] = 6 and
M ax T hp[3, 3, 0] = 5. Since T otalT hroughput (16) is
greater than the file size (10), algorithm MaxThroughput
is called to compute M ax T hp[1, 3, 0]. In this example
M ax T hp[1, 3, 0] = 8. Since M ax T hp[1, 3, 0] is less than
the file size, path switching will be necessary in this case.
M ax T hp[1, 3, 0] = 8, M ax T hp[2, 3, 0] = 7.
M ax T hp[1, 3, 1]
=
max{(M ax T hp[1, 1, 0] +
M ax T hp[2, 3, 0]), (M ax T hp[1, 2, 0] + M ax T hp[3, 3, 0])}
M ax T hp[1, 3, 1] = max{(5 + 7), (7 + 5)} = 12 and

IEEE Communications Society

switch on[1, 3, 1] = 1. This means two paths, one only over
the slot 1 and the other over the slots 2 and 3 will be used in
this case.
Computational
Complexity:
Finally,
Algorithm PathSwitching uses the Algorithm MaxThroughput
for computing the points where the paths have to
be switched. If m is the number of edges of the
network and T is the total number of slots, then
the complexity of the Algorithm ThroughputTest is
O(mT ). Since the Algorithm MaxThroughput executes
Algorithm ThroughputTest, O(log B) time, where B is the
sum of the maximum throughput in each time slot, the
complexity of the Algorithm MaxThroughput is O(mT log B).
The complexity of the Algorithnm PathSwitching is
O(mT + n2 + T 3 ).
V. C ONCLUSIONS
In this work, we considered the problem of fast bulk data
transfer in a network where bandwidth is varying. In that
respect, we proposed an algorithm that finds the optimal path
minimizing the total time slots required to transfer a given bulk
data size along with minimizing the path switching. As a future
work, we like to consider an interesting problem where bulk
data can be cached in intermediate nodes by which transfer
rate in each link can be decoupled.
ACKNOWLEDGMENT
The research of Guoliang Xue was supported in part by NSF
ITR grant ANI-0312635, ARO grant DAAD19-00-1-0377, and
CEINT.
R EFERENCES
[1] D. Anderson, H. Balakrishnan, M. Kaashoek and R. Morris, “Resilient
overlay networks”, Proc. 18th ACM SOSP conference, Oct 2001.
[2] D. Patterson, “A Conversation with Jim Gray”, ACM Queue, vol. 1, no.
4, June 2003.
[3] J. Byers, M. Luby, M. Mitzenmacher and A. Rege, “A digital fountain
approach to reliable distribution of bulk data”, Proc. of ACM SIGCOMM,
Vancouver, Sept 1998.
[4] J. W. Byers, M. Luby and M. Mitzenmacher, “Accessing multiple mirror
sites in parallel: using tornado codes to speed up downloads”, Proc. IEEE
Infocom 1999.
[5] X.Cai, T.Kloks, C.K.Wong, “Time-Varying Shortest Path Problems with
Constraints”, NETWORKS, Vol. 29 (1997), pp 141-149, John Wiley &
Sons Inc., 1997
[6] W. Cheng, C. Chou, L. Golubchik, S. Khuller and Y. Wan,“Large-scale
data collection: a coordinated approach”, Proc. of IEEE Infocom 2003,
San Francisco.
[7] L. Cherkasova and J. Lee, “FastReplica: Efficient Large File Distribution
within Content Delivery Networks”, Proc. of the 4-th USENIX Symposium
on Internet Technologies and Systems, Seattle, Washington, March 26-28,
2003.
[8] R. A. Guerin and A. Orda “Networks with Advance Reservations: The
Routing Perspective”, Proc. of IEEE Infocom, April, 2000.
[9] I.Ioachim, S.Gelinas, F.Soiumis, J.Desrosiers, “A Dynamic Programming
Algorithm for the Shortest Path Problem with Time Windows and Linear
Node Costs”, Vol. 31, pp. 193-204, John Wiley & Sons Inc., 1998.

1186

0-7803-8533-0/04/$20.00 (c) 2004 IEEE

Reader Scheduling for Tag Access in RFID Systems
Arunabha Sen, Arun Das, Chenyang Zhou, Anisha Mazumder

Nathalie Mitton, Abdoul Aziz Mbacke

CIDSE, Arizona State University
Email: {asen, arun.das, czhou24, amazumde}@asu.edu

Inria
Email: {nathalie.mitton, aziz.mbacke}@inria.fr

Abstract—“Reader” and “Tag” type devices are utilized in the
Radio-Frequency IDentification technology for identification and
tracking of objects. A tag can be “read” by a reader when the
tag is within the reader’s sensing range. However, when tags are
present in the intersection area of the sensing ranges of two or
more readers, simultaneous activation of the readers may cause
“reader collision”. In order to ensure collision-free reading, a
scheduling scheme is needed to read tags in the shortest possible
time. We study this scheduling problem in a stationary setting and
the reader minimization problem in a mobile setting. We show
that the optimal schedule construction problem is NP-complete
and provide an approximation algorithm and techniques that we
evaluate through simulation.
Keywords—RFID Radio Frequency IDentification ; reader anticollision problem ; ressource allocation scheduling ; distributed
algorithms

I.

I NTRODUCTION

Radio Frequency Identification (RFID) systems, comprising of readers and tags, are used extensively for identification
of objects with unique identifiers. In order to support complex
needs of RFID dependent business sectors, such as Supply
Chain Management and Transportation, a RFID system is
expected to allow readers fast and accurate access to tags
available in the environment. However, simultaneous transmissions by multiple readers and tags in close proximity may
cause signal interference and hinder accurate reading. Such
interference can be divided into three classes : tag-to-tag,
reader-to-tag, and reader-to-reader. To overcome these hindrances and achieve interference-free operation, development
of conflict resolution techniques are essential. Accordingly,
several conflict resolution techniques have been developed.
However, most of these techniques are developed for resolving
tag-to-tag collision, instead of reader-to-reader collision.
In this paper we study the optimal schedule construction
problem to avoid reader-to-reader collision. We formalize the
optimal schedule construction problem for RFID readers as
computation of interval chromatic number [1] of a RFIDconflict graph (a generalized version of Unit Disk Graphs),
and prove that the problem is NP-complete. We provide a
centralized and a distributed approximation algorithm for the
problem with a guaranteed performance bound.
The rest of the paper is organized as follows: In Section
II we outline the related work in this domain. In Section III
we show that the scheduling problem is NP-complete and
provide centralized and distributed approximation algorithms
for the problem. In Section IV we present the results of our
experiments, and finally Section V concludes the paper.
c 2016 IEEE
978–1–5090–4671–3/16/$31.00 

II.

R ELATED W ORK

Reader-reader anti-collision protocols from the literature
are of several kinds. Some, like HiQ-learning [2] use a hierarchical architecture to provide an online learning of collision
patterns of readers and assign frequencies to the readers
over time. Others, such as [3], [4] apply a carrier sense
multiple access (CSMA) based algorithms to detect collisions.
Centralized solutions [5] use an iterative procedure. A reader
is allocated a color in an order determined by the number of
neighboring readers already colored, choosing at each step, the
color with the smallest index. In Distributed Color Selection
(DCS) [6], each reader randomly selects a time slot in a frame
for transmission. The Variable-Maximum DCS (VDCS, or
colorwave) [7] allows adjustment of the maximum number of
colors. In [8], mobile readers communicate with a centralized
server which grants service to readers for tag identification on a
first-come-first-served basis. [9] proposed an Adaptive Color
based Reader Anti-collision Scheduling algorithm for 13.56
MHz RFID technology where every reader is assigned a set
of colors that allows it to read tags during a specific time slot
within a time frame. [10] also studied the slotted access model
to improve the read throughput of a multi-reader RFID system
by extending a centralized algorithm to a distributed one that
operates without location information of other readers.
Our model differs from the slotted access model as we assume
a reader can read only one tag per time unit. Also, a reader
fails to read any nearby tags if there is a reader collision, i.e.
a tag is present in the sensing range of two active readers.
III.

S CHEDULING P ROBLEM

Hardware requirements dictate that even in a collision-free
environment, if there is no tag collision, a reader requires t ×
Ntag seconds to read a set of Ntag tags in its sensing range
where t is the minimum amount of time needed to read one
tag (t is set to 5ms for 13.56 MHz tags [11]). Consequently,
to ensure that every tag available in a reader’s sensing range is
detected, the reader has to be turned ON during at least t×Ntag
consecutive seconds in order to let the underlying tag-tag anticollisions schemes to perform correctly and identify all tags
in its sensing range.
A. Problem Formulation
The inputs for the RFID scheduling problem are the
locations of the readers and the tags in the deployment area.
Suppose that there are n readers located in points {p1 , . . . , pn }
and m tags located in points {q1 , . . . , qm }. If the deployment
area is a two dimensional space, then each point pi (or qi )
is specified by its x, y co-ordinates (xi , yi ). We formulate
the RFID scheduling problem as computation of the Interval
Chromatic Number of a RFID Graph.

Definition: Interval Chromatic Number (ICN): An interval
coloring of a weighted graph maps each node v to an interval
of size w(v) such that intervals of adjacent nodes do not
intersect. The size of a coloring is the size of the union of these
intervals. The minimum possible size of an interval coloring
of a given weighted graph is its interval chromatic number [1].
We draw a circle of radius r, with each point pi , 1 ≤ i ≤ n
as the center, where r is the sensing range of the readers.
Corresponding to every point pi in the problem instance, we
create a node vi in the graph G = (V, E), and two nodes share
an edge if the intersection area of the sensing circles of the
corresponding points pi and pj covers at least one tag. Since
G is constructed from an instance of the RFID problem, we
will refer to it as a RFID Graph (RFIDG). It can be seen that
RFIDGs are a generalization of the UDG (when r = 1).
As a reader needs to be turned on for t × Ntag consecutive
time units to ensure that all tags in its sensing range is read,
for each node vi ∈ V we assign a weight wi = t × Ni , where
Ni is the number of tags available in the reader’s sensing
range. This way of assigning weights may be considered
somewhat inefficient as the tags that belong to the intersection
area of the sensing range of multiple readers will be read by
multiple readers. However, such duplication can be avoided if
the readers have sophisticated electronics to determine which
tag is being read by which reader, this is currently unavailable
in today’s commodity RFID readers. Accordingly, we use this
weight assignment rule to ensure that no tag is left unread.
We can now view the optimal schedule construction problem for the RFID problem as the Interval Coloring problem of
the corresponding RFID graph. We associate colors to readers
as communication tokens. A single color stands for a unit of
time. To ensure that every reader i successfully accesses all
tags in its sensing range, i has to be allocated Ni colors.
Incompatibility rule: Two readers are incompatible if there is
a tag in the intersection area of their sensing range.
Optimal Schedule Construction Problem (OSCP):
GIVEN: Two sets of points P = {p1 , . . . , pn } (locations of
readers) and Q = {q1 , . . . , qm } (locations of tags), the sensing
range r, from which the RFID Graph G = (V, E) can be
constructed, where V is the set of nodes corresponding to the
set of readers, and for every pair of nodes {u, v} ∈ V there
exists an edge (u, v) ∈ E if the readers represented by the
nodes (u, v) are incompatible.
QUESTION: Is it possible to assign an interval I(vi ) of size
|I(vi )| = wi to each node vi in V , such that the total span
of all the intervals does not exceed some predefined value B
(∪i |Ii | < B), and ∀i, jIi ∩ Ij = ∅ ? A schedule is said to be
“optimal” if the total span all the intervals is the smallest.
From our discussion, Optimal Schedule Construction Problem
(OSCP) for a RFID system is equivalent to the ICN computation problem of the corresponding RFID graph.
B. Algorithms and Analysis
We first prove that the OSCP is NP-complete. We then
provide an approximation algorithm and analyze it to establish a performance bound. Finally, we provide a distributed

implementation of our algorithm. As the solution to the OSCP
is equivalent to the computation of ICN of a RFID graph,
our algorithm essentially computes the ICN of a RFIDG.
Note that UDG is a special case of RFIDG. We prove that
the result produced by this algorithm will be bounded by a
factor of max(3, 2 + k) of the optimal solution for UDG, and
max(3α, 2 + k) of the optimal solution for RFIDG, where k
and α are parameters determined by reader and tag density
respectively. For our application we expect k ≤ 5 and α ≤ 5,
and later in this section we explain why we expect the two
parameters to satisfy these two bounds.
Theorem 1. The OSCP is NP-complete.
Proof: If we consider a case of the OSCP where weight w(vi )
of every node vi is 1, and the intersection area of every pair of
circles associated with the readers has a tag, OSCP becomes
equivalent to the computation of the Chromatic Number of a
Unit Disk Graph, a known NP-complete problem [12].
Notations:
N (vk ) : Set of vk ’s neighbors, ie nodes sharing an edge in G
with vk .
N l (vk ) (resp. N r (vk )) : Left (resp. right) neighbors of vk :
vi ∈ N (vk ) s.t. xi < xk (resp (xi > xk )).
|I(vi )| = wi : Length of I(vi ), where wi is the weight of vi
L(I(vi )): Left end point of I(vi ) on the Interval Line.
R(I(vi )): Right end point of I(vi ) on the Interval Line.
Definition: Lexicographic Ordering: The Lexicographic Ordering of a set of points in a plane is the ordering induced
by their (x, y) coordinates. The points are ordered by the
increasing values of their x coordinates and in case of a tie, are
ordered by the increasing values of their y coordinates [12].
Definition: Least Indexed Coloring (LIC): LIC scheme assigns an interval I(vi ) to each node vi , such that L(I(vi )) is
as small as possible, without violating any stated constraint.
Our interval coloring uses the LIC scheme on lexicographic
ordering of the nodes. Centralized and distributed algorithms
for computation of ICN are summed up in Algo. 1 and 2 resp.
Algorithm 1 Centralized ICN Algorithm for RFIDG/UDG
1:
2:

Arrange the nodes (readers) in Lexicographic Ordering
Sequentially apply LIC on the nodes till each node is
assigned an interval of size equal to its weight with no
overlap with intervals of adjacent nodes.

Algorithm 2 Distributed ICN Algorithm for RFIDG/UDG
(executed independently by each node vi in the graph)
1:
2:
3:
4:

5:
6:
7:

vi broadcasts xi , yi and Ni
while vi is not assigned color do
if Every node in N l (vi ) has assigned colors
then
vi chooses I(vi ) such that |I(vi )| = Ni , ∀vj ∈
N l (vi ), I(vi ) ∩ I(vj ) = ∅ and L(I(vi )) is the
smallest.
vi broadcasts I(vi )
end if
end while

For the distributed version of the coloring algorithm we
assume that the readers are aware of their own position as

well as the positions of its neighbors and the location of the
tags in its sensing range. As discussed earlier, each reader vi
must be assigned a set of wi consecutive colors to ensure that
every tag is read. To do so, we rely on [12] which proposes
a 3-approximated lexicographic order coloring. To illustrate
it, let us consider the conflict graph represented on Fig. 1(b).
Nodes represent readers with the number of colors they should
have. Each reader vi collects the position of its neighbors and
their colors. As readers are aware of their own position and
positions of their neighbors in the deployment area, they can
determine the “lexicographic order” (i.e. F , H, E, I, A, G, D,
C and B in Fig. 1(b)). Each reader will assign a set of colors
to itself, only after all its left neighbors have assigned colors to
themselves. In Fig. 1(b), reader G will wait till readers I and
A are colored. Reader B waits for D, G and C, but H and E
chooses independently as soon as F is colored as they do not
share an edge. F has no left neighbors, thus it chooses first and
takes the smallest set of 5 colors, i.e. colors 1, 2, 3, 4, 5 (as
shown on Fig.1(c)). Readers H and E follow by respectively
assigning colors 6-8 and 6-11, and so on. B cannot take colors
between 2-14, nor 21-28 as they have already been selected by
readers G, C and D. Although colors 14-21 are available in
the left neighborhood of reader B, it cannot utilize these colors
as it requires 9 consecutive colors. It thus assigns colors 28-37.
Finally, G assigns itself colors 2-6 as reader I is already using
colors 0-2. It may be noted that in this example, the centralized
and distributed algorithms provide the same solution.
1) Analysis of Algorithm: The input of both algorithms, are
the locations of the readers (nodes) and the weights assigned
to the nodes. In the following sections, we first analyze the
performance of our algorithms for a special case of RFID
graphs, known as Unit Disk Graphs (UDG). In UDGs every
pair of readers has at least one tag present in the intersection
area of the circles associated with the nodes.
Part I: Analysis of Algorithm for Unit Disk Graphs
Our interval coloring algorithm uses the LIC scheme
on lexicographic ordering of the nodes.If R(I(vi )) ≥
R(I(vj ))∀1 ≤ j ≤ n then the node vi is called a critical
node. Suppose that vk is a critical node when algorithm A
is applied on an instance of the RFID scheduling problem.
In this case, RA (I(vk )) is the solution to the instance of the
interval coloring problem using algorithm A. We will refer to
RA (I(vk )) as Approximate Interval Chromatic Number and
denote it by AICN . As per the interval layout shown in Fig.
1(c), the critical node is vk = B.
Intervals I(v1 ), I(v2 ), . . . , I(vk−1 ) associated with nodes
v1 , v2 , . . . , vk−1 are mapped on the Interval Line before the
interval I(vk ). For this reason, LA (I(vk )) may be greater than
zero as some of the nodes in the set {v1 , v2 , . . . , vk−1 } may
be adjacent to vk in G and the intervals associated with these
sets of nodes cannot overlap with the interval I(vk ).
If vl and vr are two nodes in N (vk ), such that LA (I(vl )) ≤
LA (I(vj )), and RA (I(vr )) ≥ RA (I(vj )), , the interval between LA (I(vl )) and RA (I(vr )) will be referred to as the
span of N (vk ) with algorithm A, and will be denoted by
SpA (N (vk )). The length of the span is the difference between
LA (I(vl )) and RA (I(vr )) and is denoted by |SpA (N (vk ))|.
In Fig. 1(c) SpA (N (vk )) is from 2 to 28, of length of the span
|SpA (N (vk ))| = 26.

𝑆1
𝑆2

60°

𝑣𝑘

𝑆3
𝑆𝑝1

𝑔1,0

𝑆𝑝2
𝑆𝑝3

𝑤1,1

𝑔2,0

𝑤3,1

1

𝑔1,1

𝑤2,1

𝑔3,1

𝑤1,2

𝑔2,1

𝑤3,2

(a)
𝑔1,2

𝑤2,2

𝑔3,2

𝑔1,3

𝑤1,3

𝑔2,2

𝑤2,3

𝑔2,3

𝑤3,3
𝑔3,3

𝑤3,4
𝑔3,4

18

2 3 4 5 6 7 8 9

11

13 14

𝑤+𝑔

𝑤

𝑔

𝑔1,4

𝑤1,4

𝑤3,5

𝐾1

𝐾2

𝐾3

20 21

(b)
Fig. 2: (a) Left side neighbors of the critical node vk , (b) Span of the
three segments associated with the critical node vk

It may be recalled that as our graph is a UDG, the
nodes v1 , . . . , vn correspond to points pi , . . . , pn on a two
dimensional plane. Suppose that we draw a semi-circle of
unit radius around the point pk corresponding to the node
vk (as shown in Fig. 2(a)), and divide the semi-circle into
three 60 degree segments, S1 , . . . , S3 , as shown in Fig. 2(a).
We denote by N i (vk ) the subset of N (vk ), comprised of
nodes corresponding to points in Si , ∀1 ≤ i ≤ 3. Due to
the construction rule of a UDG, the nodes corresponding to
the points that belong to segment Si , 1 ≤ i ≤ 3 form a
clique with the node vk in G = (V, E). As such, AICN ≥
|Sp(N i (vk ), A)|, ∀i, 1 ≤ i ≤ 3. Although Sp(N i (vk ), A)
and Sp(N j (vk ), A), j 6= i, need not be non-overlapping, in
the
P3 worst casei scenario |SpA (N (vk ))| may be as large as
of |SpA (N (vk ))|,
i=1 |SpA (N (vk ))|. The maximum value
P3
denoted by M ax SpA (N (vk )), can be i=1 |SpA (N i (vk ))|.
As points (nodes) that belong to segment Si , 1 ≤ i ≤ 3
form a clique with the node vk in G, the minimum value of
|SpA (N (vk ))|, denoted
by M in Sp
hP
i A (N (vk )), has to be at
least max1≤i≤3
|I(u)|
.
i
u∈N (vk )
It may be recalled that {v1 , v2 , . . . , vk−1 } were assigned
intervals on the Interval Line before the critical node vk . We
will denote the set {v1 , v2 , . . . , vk−1 } by Vk−1 . Thus, the set
of nodes in Vk−1 that are not adjacent to the node vk is
given by (called non-neighbors of vk , N N (vk )), N N (vk ) =
Vk−1 \ N (vk ). In the example of Fig. 1(a), the set N N (vk ) =
{A, E, F, H, I}, where vk = B. When the nodes in the set
N N (vk ) are assigned intervals on the Interval Line by the
algorithm A, some of these intervals may have overlap with the
interval span of the neighbors of vk , i.e., Sp(N (vk ), A). However, there may be some nodes in N N (vk ) whose assigned
intervals may not have any overlap with the Sp(N (vk ), A).
We will refer to this subset of N N (vk ) as non-overlapping
non-neighbor of vk and denote it by N O N N (vk ). In the
example of Fig. 1(a), the set N O N N (vk ) = {I}. The span of
non-overlapping non-neighbors of vk , Sp(N O N N (vk ), A)
can be at most |I(vk )|, as otherwise I(vk ) can be assigned
space in the interval line covered by the Sp(N O N N (vk ), A),
as such an assignment will not violate the non-overlapping

Lexicographic Ordering: F H E I A G D C B
5

F
5

H

H 8
3
E

C

I

H (3)

G
B

F

11

6

I 2

C (8)

I (2)

2

G (4)

21

A
10

6

G
4

E

A

F (5)

D

28

7

14

C
8

E (6)

(a) Deployment area

D

B (9)

B

37

9

A (10)

D (7)

0 2

14

(b) Unit Disk Graph

21

37

28

(c) Interval assignment

Fig. 1: (a) Readers (points) in the deployment area and their sensing range, (b) UDG graph constructed from the problem instance in Fig. 1(a),
(c) Interval assignment for the problem instance in Fig. 1(a)

requirement for adjacent nodes. The ICN will have three nonoverlapping intervals on the Interval Line corresponding to
Sp(N O N N (vk ), A), Sp(N (vk ), A) and I(vk ). As such, we
can conclude that AICN ≤ M ax Sp(N O N N (vk ), A) +
M ax Sp(N (vk ), A)
P3+ |I(vk )|.i As the maximum value of
Sp(N (vk ), A) is
i=1 |Sp(N (vk ), A)| and the maximum
value of Sp(N O N N (vk ), A) is |I(vk )|, it follows that:
AICN ≤

3
X

|SpA (N i (vk ))| + 2|I(vk )|

(1)

i=1

As nodes in N i (vk ) form a clique with vk , any optimal solution
to the Interval Chromatic Number of UDG (OICN ) must be
at least as large as:


X
OICN ≥ max 
|I(u)| + |I(vk )|
(2)
1≤i≤3

u∈N i (v

k)

Next,
we
examine
the
relationship
between
hP
i
P3
i
|Sp(N
(v
),
A)|
and
max
|I(u)|
.
i
k
1≤i≤3
i=1
u∈N (vk )
For any instance of the problem, the diagram of Sp(N (vk ))
will have the form shown in Fig. 2(b). Suppose the number of
nodes in the segments S1 , S2 , S3 are K1 , K2 , K3 . In that case
Sp1 , Sp2 , Sp3 will have K1 , K2 , K3 intervals with possibly
gaps between them as shown in Fig. 2(b). Suppose that the
weights of nodes in Si , 1 ≤ i ≤ 3 are wi,1 , wi,2 , . . . , wi,Ki . If
we draw vertical lines through the left and right end points of
every
the lines will intersect the Interval Line at most
Pinterval,
3
at 2 j=1 Kj points and divide the Interval Line into at most
P3
2 j=1 Ki + 1 sub-intervals. The sub-intervals are divided
into three disjoint groups – w-type, g-type and (w+g)-type,
depending on whether they include only w, g, or w and g type
of parts from the spans Spi . Examples of w-type, g-type and
(w+g)-type are shown in Fig. 2(b). The total space occupied
on the Interval Line by w and (w+g)
type sub-intervals
is at
i
P3 PKi
P3 h P
most: i=1 j=1 wi,j = i=1
u∈N i (vk ) |I(u)| . The size
of g-type sub-interval can be at most |I(vk )| as otherwise, the
critical node interval I(vk ) could have been inserted in the
gap. The number of g-type sub-intervals can be at most half
of the total number of sub-intervals on the Interval Line. As
noted earlier, there could be at most 2K + 1 sub-intervals and
as such, the number of g-type sub-intervals can be at most

K 0 = d(2K + 1)/2e = K + 1. Consequently:


3
3
X
X
X

|SpA (N i (vk ))| ≤
|I(u)| + (K + 1)|I(vk )|
i=1

i=1

u∈N i (vk )

Thus, from Equation (1), we have:


3
X
X

AICN ≤
|I(u)|+(K+1)|I(vk )|+2|I(vk )|, or
i=1

AICN ≤

u∈N i (vk )
3
X




X

|I(u)| + (K + 3)|I(vk )|


i=1

u∈N i (v

(3)

k)

Using Equations (2) and (3), we have:
i
P3 hP
i=1
u∈N i (vk ) |I(u)| + (K + 3)|I(vk )|
AICN
hP
i
≤
, or
OICN
max
|I(u)| + |I(v )|
i
1≤i≤3

u∈N (vk )

AICN
≤ max(3, (K + 3))
OICN

k

(4)

In the RFID application domain, it is unlikely that reader
density will be high. If there are no more than five readers
within a semi-circle of radius equal to the sensing range of a
AICN
reader, then K ≤ 5. In this case OICN
≤ 8.
Part II: Analysis of Algorithm for RFID Graphs
AICN
In Part I, we established that OICN
≤ max(3, (K + 3))
for Unit Disk graphs. In Part II, we extend that result to
RFID graphs, a generalized version of the UDG. In a UDG,
if the circles corresponding to points pi and pj intersect, then
corresponding nodes vi and vj share an edge in G. In RFIDG,
if the circles corresponding to points pi and pj intersect, then
corresponding nodes vi and vj share an edge if and only if their
sensing intersection area contains at least one tag. Unlike the
UDG case, the nodes that belong to each N i (vk ), 1 ≤ i ≤ 3
may no longer form a clique and we can no longer claim that
Equation (2) holds true. However, if we assume that at least a
fraction of the nodes in each N i (vk ), 1 ≤ i ≤ 3 form a clique
and the sum of the weights of the nodes in the clique is at
least a fraction ( α1 , α > 1), of the sum of the weights of all

ACKNOWLEDGMENT
This work was partially supported by a grant from
CPER/FEDER DATA, IPL CityLab@Inria and VITAL.
R EFERENCES
[1]
[2]

[3]

[4]

Fig. 3: Avg. % deviation of approx. schedule from the optimal.

nodes in each segment, we have:

X
OICN ≥ max 
1≤i≤3

u∈N i (vk )


|I(u)| 
+ |I(vk )|
α

Thus, from Equations (3) and (5), it follows:
max(3α, (K + 3)).

AICN
OICN

[5]

(5)
[6]

≤

In case of UDG we assume reader density determines that
K ≤ 5. In case of RFIDG, if we assume that the tag density
is such that the sum of the weights of the nodes in segments
S1 , S2 and S3 is at least 20% of the sum of the weights of all
nodes in segments S1 , S2 and S3 respectively, (i.e. α ≤ 5), we
AICN
≤ 15.
have OICN
IV.

[7]

[8]

[9]

E XPERIMENTAL R ESULTS

As primarily experiments, we implemented the distributed
and centralized coloring algorithms and compared them to the
optimal solution using WSNet [13], an event-driven simulator for large scale Wireless Sensor Networks. As to fairly
evaluate the performance under various network scenarios,
we considered a dense RFID system where 10 readers were
randomly deployed with uniform distribution on a square
network of dimension 100m × 100m. We set the reader-totag communication and sensing range to 10m, and set the
reader-to-reader communication range to 20m. For each of
the 100 simulations per scenario, we computed the optimal
solution and recorded the difference between the optimal and
our algorithm’s result. Fig. 3 presents the average percentage
deviation from the optimal solution for each scenario in terms
of efficiency. As we can observe, the centralized protocol
achieves performances close to the optimal and the more
readers, the closer to the centralized approach our distributed
solution is. These first results let us expect interesting behavior
in terms of throughput and fairness (evaluation left for future
work), especially in presence of high tag mobility.
V.

C ONCLUSION

In this paper we studied the scheduling for which we
provided centralized and distributed approximation algorithms.
Preliminary results are promising. For future work, we expect
to combine this approach with an intercorrelated problem
which is the reader minimization problem. In this environment,
not only would one like to know the minimum number of
mobile readers required to read all tags, but also the trajectories
of the readers to get a complete RFID systems.

[10]

[11]

[12]

[13]

M. Shalom, “On the interval chromatic number of proper interval
graphs,” Discrete Mathematics, vol. 338, no. 11, pp. 1907–1916, 2015.
S. S. J. Ho, D.W. Engels, “Hiq: a hierarchical q-learning algorithm
to solve the reader collision problem,” in Proc. of Int. Symposium on
Applications and the Internet Workshops (SAINT), USA, 2006.
S. Jain and S. Das, “Collision avoidance in a dense RFID network,” in
Proc. of ACM International Workshop on Wireless Network Testbeds,
Experimentation and CHaracterization (WiNTECH), USA, 2006.
G. P. Joshi, K. M. A. Mamun, and S. W. Kim, “A reader anti-collision
mac protocol for dense reader RFID system,” in Proc. of Int. Conference
on Communications and Mobile Computing (CMC), USA, 2009.
J. Riihijärvi, M. Petrova, and P. Mähönen, “Frequency allocation for
wlans using graph colouring techniques,” Ad Hoc & Sensor Wireless
Networks, vol. 3, no. 2-3, pp. 121–139, 2007.
J. Waldrop, D. W. Engles, and S. E. Sarma, “Colorwave: An anticollision algorithm for the reader collision problem,” in Proc. of IEEE
International Conference on Communications (ICC), 2003.
——, “Colorwave : A mac for RFID reader networks,” Proc. of IEEE
Conference on Wireless Communication and Networking (WCNC),
2003.
J. Eom and T.-J. Lee, “RFID reader anti-collision algorithm using
a server and mobile readers based on conflict-free multiple access,”
in Proc. of IEEE Int. Performance, Computing, and Communications
Conference (IPCCC), 2008.
E. Hamouda, N. Mitton, and D. Simplot-Ryl, “Reader Anti-Collision in
Dense RFID Networks With Mobile Tags,” in Proc. of IEEE International Conference on RFID-Technologies and Applications (RFID-TA,
Barcelona, Spain, Sep. 2011.
S. Tang, C. Wang, X.-Y. Li, and C. Jiang, “Reader activation scheduling
in multi-reader rfid systems: a study of general case,” in Proc. of IEEE
International Parallel & Distributed Processing Symposium (IPDPS).
IEEE, 2011, pp. 1147–1155.
M. Bolic, M. Latteux, and D. Simplot-Ryl, “Framed aloha based
anti-collision protocol for RFID tags,” in Proc. of ACM Workshop
on Convergence of RFID and Wireless Sensor Networks and their
Applications (SenseID), Australia, 2007.
M. Peeters, “On coloring j-unit sphere graphs,” Tilburg University,
School of Economics and Management, Research Memorandum FEW
512, 1991. [Online]. Available: http://EconPapers.repec.org/RePEc:tiu:
tiurem:0678289b-1798-4adb-9ca1-9ef5d608d166
E. B. Hamida, “Wsnet - an event-driven simulator for large
scale wireless sensor networks,” 2007. [Online]. Available: http:
//wsnet.gforge.inria.fr/

Identification of K Most Vulnerable Nodes in
Multi-layered Network Using a New Model of
Interdependency
Arunabha Sen, Anisha Mazumder, Joydeep Banerjee, Arun Das and Randy Compton
Computer Science and Engineering Program

arXiv:1401.1783v1 [cs.NI] 8 Jan 2014

School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {asen, amazumde, Joydeep.Banerjee, adas22, Randy.Compton}@asu.edu

Abstract—The critical infrastructures of the nation including
the power grid and the communication network are highly
interdependent. Recognizing the need for a deeper understanding
of the interdependency in a multi-layered network, significant
efforts have been made by the research community in the last
few years to achieve this goal. Accordingly a number of models
have been proposed and analyzed. Unfortunately, most of the
models are over simplified and, as such, they fail to capture the
complex interdependency that exists between entities of the power
grid and the communication networks involving a combination of
conjunctive and disjunctive relations. To overcome the limitations
of existing models, we propose a new model that is able to
capture such complex interdependency relations. Utilizing this
model, we provide techniques to identify the K most vulnerable
nodes of an interdependent network. We show that the problem
can be solved in polynomial time in some special cases, whereas
for some others, the problem is NP-complete. We establish that
this problem is equivalent to computation of a fixed point of a
multilayered network system and we provide a technique for its
computation utilizing Integer Linear Programming. Finally, we
evaluate the efficacy of our technique using real data collected
from the power grid and the communication network that span
the Maricopa County of Arizona.

I. I NTRODUCTION
In the last few years there has been an increasing awareness
in the research community that the critical infrastructures of
the nation are closely coupled in the sense that the well being
of one infrastructure depends heavily on the well being of another. A case in point is the interdependency between the electric power grid and the communication network. The power
grid entities, such as the SCADA systems that control power
stations and sub-stations, receive their commands through
communication networks, while the entities of communication
network, such as routers and base stations, cannot operate
without electric power. Cascading failures in the power grid,
are even more complex now because of the coupling between
power grid and communication network. Due to this coupling,
not only entities in power networks, such as generators and
transmission lines, can trigger power failure, communication
network entities, such as routers and optical fiber lines, can
also trigger failure in power grid. Thus it is essential that
the interdependency between different types of networks be
understood well, so that preventive measures can be taken to

avoid cascading catastrophic failures in multi-layered network
environments.
Recognizing the need for a deeper understanding of the
interdependency in a multi-layered network, significant efforts
have been made in the research community in the last few
years to achieve this goal [1], [2], [3], [4], [5], [6], [7], [8].
Accordingly a number of models have been proposed and
analyzed. Unfortunately, many of the proposed models are
overly simplistic in nature and as such they fail to capture
the complex interdependency that exists between power grid
and communication networks. In a highly cited paper [1], the
authors assume that every node in one network depends on one
and only one node of the other network. However, in a follow
up paper [2], the same authors argue that this assumption may
not be valid in the real world and a single node in one network
may depend on more than one node in the other network. A
node in one network may be functional (“alive”) as long as
one supporting node on the other network is functional.
Although this generalization can account for disjunctive
dependency of a node in the A network (say ai ) on more
than one node in the B network (say, bj and bk ), implying
that ai may be “alive” as long as either bi or bj is alive,
it cannot account for conjunctive dependency of the form
when both bj and bk has to be alive in order for ai to
be alive. In a real network the dependency is likely to be
even more complex involving both disjunctive and conjunctive
components. For example, ai may be alive if (i) bj and bk and
bl are alive, or (ii) bm and bn are alive, or (iii) bp is alive. The
graph based interdependency models proposed in the literature
[3], [4], [5], [9], [6], [7] including [1], [2] cannot capture
such complex interdependency between entities of multilayer
networks. In order to capture such complex interdependency,
we propose a new model using Boolean logic. Utilizing this
comprehensive model, we provide techniques to identify the
K most vulnerable nodes of an interdependent multilayered
network system. We show that the this problem can be solved
in polynomial time for some special cases, whereas for some
others, the problem is NP-complete. We also show that this
problem is equivalent to computation of a fixed point [10] and
we provide a technique utilizing Integer Linear Programming

2

to compute that fixed point. Finally, we evaluate the efficacy
of our technique using real data collected from power grid
and communication networks that span Maricopa County of
Arizona.

II. I NTERDEPENDENCY M ODEL
We describe the model for an interdependent network with
two layers. However, the concept can easily be generalized
to deal with networks with more layers. Suppose that the
network entities in layer 1 are referred to as the A type
entities, A = {a1 , . . . , an } and entities in layer 2 are referred
to as the B type entities, B = {b1 , . . . , bm }. If the layer 1
entity ai is operational if (i) the layer 2 entities bj , bk , bl
are operational, or (ii) bm , bn are operational, or (iii) bp
is operational, we express it in terms of live equations of
the form ai ← bj bk bl + bm bn + bp . The live equation for
a B type entity br can be expressed in a similar fashion
in terms of A type entities. If br is operational if (i) the
layer 1 entities as , at , au , av are operational, or (ii) aw , az
are operational, we express it in terms of live equations of
the form br ← as at au av + aw az . It may be noted that the
live equations only provide a necessary condition for entities
such as ai or br to be operational. In other words, ai or br
may fail independently and may be not operational even when
the conditions given by the corresponding live equations are
satisfied. A P
live equation
in general will have the following
Ti Qtj
form: xi ← j=1
y
k=1 j,k where xi and yj,k are elements
of the set A (B) and B (A) respectively, Ti represents the
number of min-terms in the live equation and tj refers to the
size of the j-th min-term (the size of a min-term is equal to the
number of A or B elements in that min-term). In the example
ai ← bj bk bl + bm bn + bp , Ti = 3, t1 = 3, t2 = 2, t3 = 1,
xi = ai , y2,1 = bm , y2,2 = bp .
We refer to the live equations of the form ai ← bj bk bl +
bm bn + bp also as First Order Dependency Relations, because
these relations express direct dependency of the A type entities
on B type entities and vice-versa. It may be noted however
that as A type entities are dependent on B type entities,
which in turn depends on A type entities, the failure of
some A type entities can trigger the failure of other A type
entities, though indirectly, through some B type entities. Such
interdependency creates a cascade of failures in multilayered
networks when only a few entities of either A type or B type
(or a combination) fails. We illustrate this with the help of
an example. The live equations for this example is shown in
table I.
Power Network
a1 ← b1 + b2
a2 ← b1 b3 + b2
a3 ← b1 b2 b3
a4 ← b1 + b2 + b3

Communication Network
b1 ← a1 + a2 a3
b2 ← a1 + a3
b3 ← a1 a2
−−

TABLE I: Live equations for a Multilayer Network

Entities
a1
a2
a3
a4
b1
b2
b3

t0
1
0
0
0
0
0
0

t1
1
0
0
0
0
0
1

Time Steps
t2
t3
t4
1
1
1
0
0
1
1
1
1
0
0
1
0
1
1
0
1
1
1
1
1

t5
1
1
1
1
1
1
1

t6
1
1
1
1
1
1
1

TABLE II: Time Stepped Cascade Effect for a Multilayer Network

Fig. 1: Cascading failures reach steady state after p time steps

As shown in table II, the failure of only one entity a1 at
time step t0 triggered a chain of failures that resulted in the
failure of all the entities of the network after by timestep t4 .
A table entry of 1 indicates that the entity is “dead”. In this
example, the failure of a1 at t0 triggered the failure of b3 at
t1 , which in turn triggered the failure of a3 at t2 . The failure
of b3 at t1 was due to the dependency relation b3 ← a1 a2
and the failure of a3 at t2 was due to the dependency relation
a3 ← b1 b2 b3 . The cascading failure process initiated by failure
(or death) of a subset of A type entities at timestep t0 , A0d and
a subset of B type entities Bd0 till it reaches its final steady
state is shown diagrammatically in figure 1. Accordingly, a
multilayered network can be viewed as a “closed loop” control
system. Finding the steady state after an initial failure in this
case is equivalent of computing the fixed point of a function
F(.) such that F(Apd ∪ Bdp ) = Apd ∪ Bdp , where p represents
the number of steps when the system reaches the steady state.
We define a set of K entities in a multi-layered network
as most vulnerable, if failure of these K entities triggers the
failure of the largest number of other entities. The goal of
the K most vulnerable nodes problem is to identify this set of
nodes. This is equivalent to identifying A0d ⊆ A, Bd0 ⊆ B, that
maximizes |Apd ∪Bdp |, subject to the constraint that |A0d ∪Bd0 | ≤
K.
The dependency relations (live equations) can be formed
either after careful analysis of the multilayer network along the
lines carried out in [8], or after consultation with the engineers
of the local utility and internet service providers.
III. C OMPUTATIONAL C OMPLEXITY AND A LGORITHMS
Based on the number and the size of the min-terms in the
dependency relations, we divide them into four different cases
as shown in Table III. The algorithms for finding the K most
vulnerable nodes in the multilayer networks and computation
complexity for each of the cases are discussed in the following
four subsections.
Case
Case I
Case II
Case III
Case IV

No. of Min-terms
1
1
Arbitrary
Arbitrary

Size of Min-terms
1
Arbitrary
1
Arbitrary

TABLE III: Equation Types for Dependency Relations

3

A. Case I: Problem Instance with One Min-term of Size One
In this case, a live equation in general will have the following form: xi ← yj where xi and yj are elements of the set A
(B) and B (A) respectively. In the example ai ← bj , xi = ai ,
y1 = bj . It may be noted that a conjunctive implication of
the form ai ← bj bk can also be written as two separate
implications ai ← bj and ai ← bk . However, such cases are
considered in Case II and is excluded from consideration in
Case I. The exclusion of such implications implies that the
entities that appear on the LHS of an implication in Case I
are unique. This property enables us to develop a polynomial
time algorithm for the solution of the K most vulnerable node
problem for this case. We present the algorithm next.
Algorithm 1
Input: (i) A set S of implications of the form of y ← x,
where x, y ∈ A ∪ B, (ii) An integer K.
Output: A set V 0 where |V 0 | = K and V 0 ⊂ A ∪ B such
that failure of entities in V 0 at time step t0 results in failure
of the largest number of entities in A ∪ B when the steady
state is reached.
Step 1. We construct a directed graph G = (V, E), where
V = A ∪ B. For each implication y ← x in S, where x, y ∈
A ∪ B, we introduce a directed edge (x, y) ∈ E.
Step 2. For each node xi ∈ V , we construct a transitive
closure set Cxi as follows: If there is a path from xi to some
node yi ∈ V in G, then we include yi in Cxi . It may be
recalled that |A| + |B| = n + m. So, we get n + m transitive
closure sets Cxi , 1 ≤ i ≤ (n + m). We call each xi to be the
seed entity for the transitive closure set Cxi .
Step 3. We remove all the transitive closure sets which are
proper subsets of some other transitive closure set.
Step 4. Sort the remaining transitive closure sets Cxi ,
where the rank of the closure sets is determined by the
cardinality of the sets. The sets with a larger number of entities
are ranked higher than the sets with a fewer number of entities.
Step 5. Construct the set V 0 by selecting the seed entities
of the top K transitive closure sets. If the number of remaining
transitive closure sets is less than K (say, K0 ), arbitrarily select
the remaining entities.
Time complexity of Algorithm 1: Step 1 takes O(n + m + |S|)
time. Step 2 can be executed in O((n+m)3 ) time. Step 3 takes
at most O((n + m)2 ) time. Step 4 sorts at most |S| entries, a
standard sorting algorithm takes O(|S| log |S|) time. Selecting
K entities in step 5 takes O(K) time. Since |S| ≤ n+m, hence
the overall time complexity is O((n + m)3 )
Theorem 1. For each pair of transitive closure sets Cxi and
Cxj produced in step 2 of algorithm 1, either Cxi ∩ Cxj = ∅
or Cxi ∩ Cxj = Cxi or Cxi ∩ Cxj = Cxj , where xi 6= xj .
Proof: Consider, if possible, that there is a pair of transitive
closure sets Cxi and Cxj produced in step 2 of algorithm 1,
such that Cxi ∩Cxj 6= ∅ and Cxi ∩Cxj 6= Cxi and Cxi ∩Cxj 6=

Cxj . Let xk ∈ Cxi ∩ Cxj . This implies that there is a path
from xi to xk (path1 ) as well as there is a path from xj to xk ,
(path2 ). Since, xi 6= xj and Cxi ∩Cxj 6= Cxi and Cxi ∩Cxj =
Cxj , there is some xl in the path1 such that xl also belongs to
path2 . W.l.o.g, let us consider that xl be the first node in path1
such that xl also belongs to path2 . This implies that xl has
in-degree greater than 1. This in turn implies that there are two
implications in the set of implications S such that xl appears in
the L.H.S of both. This is a contradiction because this violates
a characteristic of the implications in Case I. Hence, our initial
assumption was wrong and the theorem is proven.
Theorem 2. Algorithm 1 gives an optimal solution for the
problem of selecting K most vulnerable entities in a multilayer network for case I dependencies.
Proof: Consider that the set V 0 returned by the algorithm is
not optimal and the optimal solution is VOP T . Let us consider
there is a entity xi ∈ A ∪ B such that xi ∈ VOP T \ V 0 .
Evidently, (i) Cxi was either deleted in step 3 or (ii) |Cxi | is
less than the cardinalities of all the transitive closure sets with
seed entities xj ∈ V 0 , because our algorithm did not select
xi . Hence, in both cases, replacing any entity xj ∈ V 0 by xi
reduces the total number of entities killed. Thus, the number
of dead entities by the failure of entities in VOP T is lesser than
that caused by the failure of the entities in V 0 , contradicting
the optimality of VOP T . Hence, the algorithm does in fact
return the optimal solution.
B. Case II: Problem Instance with One Min-term of Arbitrary
Size
In this case, a liveQ equation in general will have the
q
following form: xi ← k=1 yj where xi and yj are elements
of the set A (B) and B (A) respectively, q represents the size
of min-term. In the example ai ← bj bk bl , q = 3, xi = ai ,
y1 = bj , y2 = bk , y3 = bk .
1) Computational Complexity: We show that computation
of K most vulnerable nodes (K-MVN) in a multilayer network
is NP-complete in Case II. We formally state the problem next.
Instance: Given a set of dependency relations between
A
Qqand B type entities in the form of live equations xi ←
k=1 yj , integers K and L.
Question: Is there a subset of A and B type entities of
size at most K whose “death” (failure) at time t0 , triggers a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached?
Theorem 3. The K-MVN problem is NP-complete.
Proof: We prove that the K-MVN problem is NP-complete
by giving a transformation for the vertex cover (VC) problem.
An instance of the vertex cover problem is specified by an
undirected graph G = (V, E) and an integer R. We want to
know if there is a subset of nodes S ⊆ V of size at most
R, so that every edge has at least one end point in S. From
an instance of the VC problem, we create an instance of the

4

K-MVN problem in the following way. First, from the graph
G = (V, E), we create a directed graph G0 = (V, E 0 ) by
replacing each edge e ∈ E by two oppositely directed edges
e1 and e2 in E 0 (the end vertices of e1 and e2 are same as
the end vertices of e). Corresponding to a node vi in G0 that
has incoming edges from other nodes (say) vj , vk and vl , we
create a dependency relation (live equation) vi ← vj vk vl . We
set K = R and L = |V |. The corresponding death equation is
of the form v¯i ← v¯j + v¯k + v¯l (obtained by taking negation
of the live equation). We set K = R and L = |V |. It can now
easily be verified that if the graph G = (V, E) has a vertex
cover of size R iff in the created instance of K-MVN problem
death (failure) of at most K entities at time t0 , will trigger a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached.
2) Optimal Solution with Integer Linear Programming:
In this case, we can find and optimal solution to the KMVN problem using Integer Linear Programming (ILP). We
associate binary indicator variables xi (yi ) to capture the state
of the entities ai (bi ). xi (yi ) is 1 when ai (bi ) is dead and
0 otherwise. Since we want find the set of K entities whose
failure at time step t0 triggers cascading failure resulting in the
failure of the largest number of entities, the
the
Pnobjective
Pof
m
ILP can be written as follows maximize
x
+
i
i=1
i=1 yi
It may be noted that the variables in the objective function
do not have any notion of time. However, cascading failure
takes place in time steps, ai triggers failure of bj at time
step t1 , which in turn triggers failure of ak in time step t2
and so on. Accordingly, in order to capture the cascading
failure process, we need to introduce the notion of time into
the variables of the ILP. If the numbers of A and B type
entities are n and m respectively, the steady state must be
reached by time step n + m − 1 (cascading process starts at
time step 0, t0 ). Accordingly, we introduce n + m versions
of the variables xi and yi , i.e., xi [0], . . . , xi [n + m − 1] and
yi [0], . . . , yi [n+m−1]. To indicate the state of entities ai and
bi at times t0 , . . . , tn+m−1 . The objective of the ILP is now
changed to
maximize

n
X
i=1

xi [n + m − 1] +

m
X

yi [n + m − 1]

i=1

Subject to the constraint that no more than K entities can
fail at time t0 .
Pn
Pm
Constraint 1:
i=1 yi [0] ≤ K In order
i=1 xi [0] +
to ensure that the cascading failure process conforms to
the dependency relations between type A and B entities,
additional constraints must be imposed.
Constraint 2: If an entity fails at time fails at time step p,
(i.e., tp ) it should continue to be in the failed state at all time
steps t > p. That is xi (t) ≥ xi (t − 1), ∀t, 1 ≤ t ≤ n + m − 1.
Same constraint applies to yi (t).
Constraint 3: The dependency relation (death equation)
a¯i ← b¯j +b¯k +b¯l can be translated into a linear constraint in the
following way xi (t) ≤ yj (t−1)+yk (t−1)+yl (t−1), ∀t, 1 ≤
t ≤ n + m − 1.

The optimal solution to K-MVN problem for Case II can be
found by solving the above ILP.
C. Case III: Problem Instance with an Arbitrary Number of
Min-terms of Size One
A live equation
Pq in this special case will have the following
form: xi ← j=1 yj where xi and yj are elements of the set
A (B) and B (A) respectively, q represents the number of minterms in the live equation. In the example ai ← bj + bk + bl ,
q = 3, xi = ai , y1 = bj , y2 = bk , y3 = bl .
1) Computational Complexity: We show that a special
case of the problem instances with an arbitrary number
of min-terms of size one is same as the Subset Cover
problem (defined below), which is proven to be NPcomplete. We define Implication Set(A) Pto be the
Ti
set of all implications of the form ai ←
j=1 bj and
ImplicationPSet(B) to be the set of all implications of the
Ti
form bi ←
j=1 aj . Now consider a subset of the set of
problem instances with an arbitrary number of min-terms
of size one where either Implication Set(A) = ∅
or Implication Set(B)
=
∅. Let A0
=
{ai |ai is the element on the LHS of an implication}
in the Implication Set(A). The set B 0 is defined
accordingly. If Implication Set(B) = ∅ then B 0 = ∅. In
this case, failure of any ai , 1 ≤ i ≤ n type entities will not
cause failure of any bj , 1 ≤ j ≤ m type entities. Since an
adversary can cause failure of only K entities, the adversary
would like to choose only those K entities that will cause
failure of the largest number of entities. In this scenario, there
is no reason for the adversary to attack any ai , 1 ≤ i ≤ n type
entities as they will not cause failure of any bj , 1 ≤ j ≤ m
type entities. On the other hand, if the adversary attacks
K bj type entities, not only those K bj type entities will
be destroyed, some ai type entities will also be destroyed
due to the implications in the Implication Set(A). As
such the goal of the adversary will be to carefully choose
K bj , 1 ≤ j ≤ m type entities that will destroy the largest
number of ai type entities. In its abstract form, the problem
can be viewed as the Subset Cover problem.
Subset Cover Problem
Instance: A set S = {s1 , . . . , sm }, a set S of m subsets of S,
i.e., S = {S1 , . . . , Sr }, where Si ⊆ S, ∀i, 1 ≤ i ≤ r, integers
p and q.
Question: Is there a p element subset S 0 of S (p < n) that
completely covers at least q elements of the set S? (A set S 0 is
said to be completely covering an element Si , ∀i, 1 ≤ i ≤ m
of the set S, if S 0 ∩ Si = Si , ∀i, 1 ≤ i ≤ m.)
The set S in the subset cover problem corresponds to the
set B = {b1 , . . . , bm }, and each set Si , 1 ≤ i ≤ r corresponds
to an implication in the ImplicationS et(A) and comprises of
the bj ’s that appear on the RHS of the implication. The goal
of the problem is to select a subset B 00 of B that maximizes
the number of Si ’s completely covered by B 00 .

5

Theorem 4. The Subset Cover problem is NP-complete.
Proof: We prove that the Subset Cover problem is NPcomplete by giving a transformation from the well known
Clique problem. It may be recalled that an instance of the
Clique problem is specified by a graph G = (V, E) and an
integer K. The decision question is whether or not a clique of
size at least K exists in the graph G = (V, E). We show that
a clique of size K exists in graph G = (V, E) iff the Subset
Cover problem instance has a p element subset S 0 of S that
completely covers at least q elements of the set S.
From an instance of the Clique problem, we create an
instance of the Subset Cover problem in the following way.
Corresponding to every vertex vi , 1 ≤ i ≤ n of the graph
G = (V, E) (V = {v1 , . . . , vn }), we create an element
in the set S = {s1 , . . . , sn }. Corresponding to every edge
ei , 1 ≤ i ≤ m, we create m subsets of S, i.e., S =
{S1 , . . . , Sm }, where Si corresponds to a two element subset
of nodes, corresponding to the end vertices of the edge ei . We
set the parameters p = K and q = K(K − 1)/2. Next we
show that in the instance of the subset cover problem created
by the above construction process, a p element subset S 0 of
S exists that completely covers at least q elements of the set
S, iff the graph G = (V, E) has a clique of size at least K.
Suppose that the graph G = (V, E) has a clique of size
K. It is clear that in the created instance of the subset cover
problem, we will have K(K − 1)/2 elements in the set S,
that will be completely covered by a K element subset of
the set S. The K element subset of S corresponds to the set
of K nodes that make up the clique in G = (V, E) and the
K(K − 1)/2 elements in the set S corresponds to the edges
of the graph G = (V, E) that corresponds to the edges of
the clique. Conversely, suppose that the instance of the Subset
Cover problem has K element subset of S that completely
covers K(K − 1)/2 elements of the set S. Since the elements
of S corresponds to the edges in G, in order to completely
cover K(K − 1)/2 edges, at least K nodes (elements of the
set S) will be necessary. As such, this set of K nodes will
constitute a clique in the graph G = (V, E).
2) Optimal Solution with Integer Linear
Programming: If
Pq
the live equation is in the form xi ← k=1 yj then the “death
equation” (obtained by taking negation
of the live equation)
Qq
will be in the product form x̄i ← j=1 ȳj . If the live equation
is given as ai ← bj + bk , then the death equation will be given
as a¯i ← b¯j b¯k .
By associating binary indicator variables xi and yi to
capture the state of the entities ai and bi , we can follow almost
identical procedure as in Case II, with only one exception.
It may be recalled that in Case II, the death equations such
as a¯i ← b¯j + b¯k was translated into a linear constraint
xi (t) ≤ yj (t − 1) + yk (t − 1), ∀t, 1 ≤ t ≤ n + m − 1. However
a similar translation in Case III, with death equations such as
a¯i ← b¯j b¯k , will result in a non-linear constraint of the form
xi (t) ≤ yj (t − 1)yk (t − 1), ∀t, 1 ≤ t ≤ n + m − 1. Fortunately,
a non-linear constraint of this form can be replaced a linear
constraint such as 2xi (t) ≤ yj (t − 1) + yk (t − 1), ∀t, 1 ≤

t ≤ n + m − 1. After this transformation, we can compute the
optimal solution using integer linear programming.
D. Case IV: Problem Instance with an Arbitrary Number of
Min-terms of Arbitrary Size
1) Computational Complexity: Since both Case II and Case
III are special cases of Case IV, the computational complexity
of finding the K most vulnerable nodes in the multilayer
network in NP-complete in Case IV also.
2) Optimal Solution with Integer Linear Programming:
The optimal solution to this version of the problem can be
computed by combining the techniques developed for the
solution of the versions of the problems considered in Cases
II and III.
IV. E XPERIMENTAL RESULTS
We applied our model to study multilayer vulnerability
issues in Maricopa County, the most densely populated county
of Arizona with approximately 60% of Arizonas population
residing in it. Specifically, we wanted to find out if some
regions of Maricopa County were more vulnerable to failure
than some other regions. The data for our multi-layered
network were obtained from different sources. We obtained
the data for the power network (network A) from Platts
(http://www.platts.com/). Our power network dataset consists
of 70 power plants and 470 transmission lines. Our communication network (network B) data were obtained from GeoTel
(http://www.geo-tel.com/). Our communication network data
consists of 2, 690 cell towers and 7, 100 fiber-lit buildings as
well as 42, 723 fiber links. Snapshots of our power network
data and communication network data are shown in figure 2. In
the power network snapshot of sub-figure(a), the orange markers show locations of powerplants while the yellow continuous
lines represent the transmission lines. In the communication
network snapshot of sub-figure (b) the pink markers show the
location of fiber-lit buildings, the orange markers show the
location of cell towers and the green continuous lines represent
the fiber links. In our dataset, ‘load’ in the Power Network is
divided into Cell towers and Fiber-lit buildings. Although there
exists various other physical entities which also draw electric
power and hence can be viewed as load to the power network,
as they are not relevant to our study on interdependency
between power and communication networks, we ignore such
entities. Thus in network A, we have the three types of Power
Network Entities (PNE’s) - Generators, Load (consisting of
Cell towers and Fiber-lit buildings) and Transmission lines
(denoted by a1 , a2 , a3 respectively). For the Communication
Network, we have the following Communication Network
Entities (CNE’s) - Cell Towers, Fiber-lit buildings and Fiber
links (denoted by b1 , b2 , b3 respectively). We consider the
Fiber-lit buildings as a communication network entities as they
house routers which definitely are communication network
entities. From the raw data we construct Implication Set(A)
and Implication Set(B), by following the rules stated below:
Rules: We consider that a PNE is dependent on a set of
CNEs for being in the active state (‘alive’) or being in the

6

(a) Snapshot of Power Network in Maricopa County

(b) Snapshot of Communication Network in Maricopa County

Fig. 2: Snapshots of power network and communication network in Maricopa County)

inactive state (‘dead’). Similarly, a CNE is dependent on a set
of PNEs for being active or inactive state. For simplicity we
consider the live equations with at most two minterms. For
the same reason we consider the size of each minterm is at
most two.

of the number of entities of the two networks A and B. Most
importantly, we find that the degree of vulnerability of all
the five regions considered in our study are close and no one
region stands out as being extremely vulnerable.

Generators (a1,i , 1 ≤ i ≤ p, where p is the total number
of generators): We consider that each generator (a1.i ) is
dependent on the nearest Cell Tower (b1,j ) or the nearest
Fiber-lit building (b2,k ) and the corresponding Fiber link (b3,l )
connecting b2,k and a1,i . Thus, we have
a1,i ← b1,j + b2,k × b3,l
Load (a2,i , 1 ≤ i ≤ q, where q is the total number of loads):
We consider that the loads in the power network do not depend
on any CNE.
Transmission Lines (a3,i , 1 ≤ i ≤ r, where r is the total number of transmission lines): We consider that the transmission
lines do not depend on any CNE.
Cell Towers (b1,i , 1 ≤ i ≤ s, where s is the total number
of cell towers): We consider the cell towers depend on the
nearest pair of generators and the corresponding transmission
line connecting the generator to the cell tower. Thus, we have
b1,i ← a1,j × a3,k + a1,j 0 × a3,k0
Fiber-lit Buildings (b2,i , 1 ≤ i ≤ t, where t is the total number
of fiber-lit buildings): We consider that the fiber-lit buildings
depend on the nearest pair of generators and the corresponding
transmission lines connecting the generators to the cell tower.
Thus, we have b2,i ← a1,j × a3,k + a1,j 0 × a3,k0
Fiber Links (b3,i , 1 ≤ i ≤ u, where u is the total number of
fiber links)): We consider that the fiber links do not depend
on any PNE.
Because of experimental resource limitation, we have considered 5 regions of Maricopa County for our experiments.
We used IBM CPLEX Optimizer 12.5 to run the formulated
ILP’s on the experimental dataset. We show our results in
the figure 3. We observe that in each of the regions there
is a specific budget threshold beyond which each additional
increment in budget results in the death of only one entity. The
reason for this behavior is our assumption that entities such
as the transmission lines and the fiberlinks are not dependent
on any other entities. We notice that all the entities of the
two networks can be destroyed with a budget of about 60%

Fig. 3: Experimental results of failure vulnerability across five regions
of Maricopa county

R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
“Catastrophic cascade of failures in interdependent networks,” Nature,
vol. 464, no. 7291, pp. 1025–1028, 2010.
[2] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, “Networks formed
from interdependent networks,” Nature Physics, vol. 8, no. 1, pp. 40–48,
2011.
[3] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, “Cascade of
failures in coupled network systems with multiple support-dependence
relations,” Physical Review E, vol. 83, no. 3, p. 036116, 2011.
[4] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, “Modelling interdependent infrastructures using interacting
dynamical models,” International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63–79, 2008.
[5] P. Zhang, S. Peeta, and T. Friesz, “Dynamic game theoretic model of
multi-layer infrastructure networks,” Networks and Spatial Economics,
vol. 5, no. 2, pp. 147–178, 2005.
[6] M. Parandehgheibi and E. Modiano, “Robustness of interdependent
networks: The case of communication networks and the power grid,”
arXiv preprint arXiv:1304.0356, 2013.
[7] D. T. Nguyen, Y. Shen, and M. T. Thai, “Detecting critical nodes in
interdependent power networks for vulnerability assessment,” 2013.
[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
“Power grid vulnerability to geographically correlated failures-analysis
and control implications,” arXiv preprint arXiv:1206.1099, 2012.
[9] J.-F. Castet and J. H. Saleh, “Interdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,” PloS one, vol. 8, no. 4, p. e60402, 2013.
[10] A. Fudenberg and J. Tirole, Game Theory. Ane Books, 2010.

281

MEEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 3, MARCH 1986

On Fault-Tolerant Distributor Communication Architecture
SUMANTA GUHA AND ARUNABHA SEN
Abstract -A new routing algorithm is proposed in this correspondence
for the network architecture developed in [1]. It has been shown that this
algorithm gives the shortest path between the source to destination, when
all the processors and the communication links of the network are nonfaulty. For the situation when some of the processors are faulty, a heuristic
function is given, which if used in the routing algorithm will give shortest
path from the source to destination, if such a path exists.
Index Terms -Heuristic function, match, matching, matching factor,
provisional matching factor.

used in the routing algorithm, yields the shortest path (when such a
path exists) even when some of the nodes are faulty.
SYSTEM TOPOLOGY
Suppose the source node is i and the destination node j [1]. Let
im-1im-2

iO

:Jm-1Jm-2

JO -

i

=

and
J

we call the substrings
a =

INTRODUCTION
Computer interconnection network design has received considerable attention from computer scientists in recent years [1]-[10].
Various network architectures have been proposed and their characteristics studied from the standpoint of interprocessor communication distance, number of I/O ports per processor, fault-tolerance
of network, and other parameters. In [ 1 ], the authors have proposed
a fault-tolerant communication architecture for distributed processing. In this architecture the number of nodes n of the network graph
is equal to rm where r and m are two integers. Internode connection
in the network is given by the following rule:
If (im-_Iim,-2, . , io) is the radix r representation of i,
0 < i - n - 1 and (j,-1,jm-29 *Jo) is the radix r representation
of j, 0 . j < n -1 then every node i is connected to node j, if
or i = j,+i

0<w

m -2.

The authors have given algorithms for message routing from node
to node both with and without the presence of faults in the network.
But neither the primary algorithm nor the alternate algorithm proposed give the shortest route from source to destination measured in
terms of the number of hops required. Consider the example given
in [1]. Source node is 6 and the destination node is 7. If the primary
algorithm is used the route followed will be

and
b =

6---> 3---> 7 or 6 ---> 11 ---> 7.

Assuming node 13 to be faulty, the alternate algorithm yields the

route

6---> 12---> 8---> 0--->
8 ---> 12 ---> 14 ---> 7
while again the routes

jyjy-1 ... jy-k+ 1

(0 s x, y c m - 1) of i and j, respectively, matching if
ix

=

jy, ix-I

=

jy-1,

,

ix-k+1

=

jy-k+l

-

We define the match of these two matching substrings a and b to
lx - yI + 2k.
Define the provisional matching factor N(i,j) of i and j to be
1) the maximum amongst the match of matching substrings of i and
j, if there exists at least one pair of matching substrings; 2) 0, if
there does not exist a pair of matching substrings of i and j (i.e., the
elements of the string i are distinct from those of string j). N(i,j)
may be determined by a pattern matching algorithm.
Define the matching factor of i andj to be
be

M(i,j)

=

max(N(i,j), m)

(note m is the string length).
Remark: The motivation for this definition lies in the proof of
Lemma 2.
Example: Consider the strings

i3i2iIiO = 0110
i = j3j2jIjO = 1101.
i =

6- - -> 13 - --> 11- - -> 7.

(note that the route 6 ---> 13 ---> 9 ---> 3 ---> 7 given in
[1] is erroneous) while there are, in fact, shorter routes

ix-k+I

ixlx-1

There are numerous pairs of matching substrings. Some of the
matching substrings and the corresponding match (indicated in parentheses) are given below.
(a) i2ili0 and131211
(7)
(b) i3i2 and jljo
(6)

(c) i2i1 andj3 j2
(d) iiio and]2]1
(e) i2 and jo

(5)
(5)
(4)

etc.

The maximum match is 7. Hence, N(i,j) = 7 and

M(i, j) - max(N(i, j), m)
= max(7, 4)

6 ---> 3 ---> 7 and 6 ---> 11 ---> 7 are available.

As the message delay and the transmission cost is directly propor= 7.
tional to the route length, it seems important to develop routing
algorithms which minimize the route length.
We now state a theorem.
We propose a new routing algorithm which gives the shortest
Theorem 1: Given the source node i and the destination node j,
route from the source node to destination when there is no faulty
the
shortest route from i to j requires 2m - M(i, j) hops.
node in the network. Then we derive a heuristic function which, if
The theorem follows easily from the following two lemmas.
Manuscript received February 24, 1984.
S. Guha is with the Department of Mathematics, Jadavpur University,
Calcutta, 700032, India.
A. Sen is with the Department of Computer Science, University of South
Carolina, Columbia, SC 29208.
IEEE Log Number 8406294.

Lemma 1:

M(*i,j)
and M(i *, j)
where * is an arbitrary digit [1].

0018-9340/86/0300-0281$01.00 ( 1986 IEEE

1 +

M(i,j)

1 + M(i, j)

IEEE

282

Lemma 2: If M(i,j) < 2m, then there exists a node i' such that

M(i',j) = 1 + M(ij)
where i' = *i or i' = i* for some *.
Proof of Lemma 1: We prove the first inequality, the proof of
the second being similar.
Suppose, if possible, for some i and *,

M(*i,j)> 1 + M(i,j).

(1)

We write i' = *i. Then
Hence, M(i',J) = N(i',j) > 1 + m and it follows from the definition of the provisional matching factor that there is a pair of matching substrings of if and j whose match is N(i', j).
Let the substrings be

iGiX_ l*

ix-k

jy-k+1 of]
+ 2k.

jyjy-iI

such that M(i,j) = Ix - yI
First suppose y ' x s m - 1,
=

tm-l'm-2

jm-ljm-2 .jyjy-1
Consider i' = i * with * arbitrary,

ix+lix

:h
i-l . m-2~* * .ixif
I =lm-iJm-2 .Jy Jy-iI

x-k+i * * *i

l

.

im-2im-3

x

.

k

.

.

Jy-k+l

ix+llx .

JO.

]i

x-k+I *

0*

ix_k+l)

ix;ixp-

are "matching" with "match" Jx - Y| + 2k + 1 = M(i',j) + 1.
Hence, M(i,j) . M(i',j) + 1.
This contradicts (1).
Next, suppose y ' x =. m - 1,
lxil-1

'x-k+1

Jm-lJm-2

3

.

.jyJy-WI

.

Jy-k+1

=

ixix- I . .ix-k+2ix-k+ 1

=

ixli>_2

.

.

* *

.

.

ilio

ioio

i

Then the substrings
ixix_ I

.

*. ix-k+ (= ix-lix-2
.

.

.

.

ix_k+l) of i

and
jy-ljy-2

.

.

.

of]

ixx-1

.
2...yj
jy
=]m-1]Jm--2Jy]y-V"]y-k+1
.

=

+ 1. Hence,

io

x-k-4-1

I

M(i,j)

.

.

.

k

lm

j JO
.

=

xix .-1
* *ix-k * * io

= y+ Iixix- I

.

.

.

ix-k+1

|x

YI + 2(k + 1)

-

1

.

.

.

il

]
and]jy+1y
=

M(i,)

.

+

.

jy-k+1 of j are

1.

Next, suppose y = x = m - 1. Then it can easily be seen (note
k < m) that M(*i,j) = M(i,j) + 1 for any *. The other cases
where y . x are similarly disposed.
Proof of Theorem 1: The theorem follows directly from the
lemmas with the observation that M(i,j) = 2m if and only if i = j.

]o

Hence,
i

*.0*

.

ix-k+1) of i

jy-k+1

Then the substrings i'i'-1 ... i'-k of i'
matching with a match equal to

jyjy-I .jy-k± Of j

=

io

Let i' = *i with * =jZ+

of i

and

i

=

i

ix-k±+2 (=

.

are matching with match lx -Y| + 1 + 2k =
= M(i,j) + 1.
Suppose next that y < x = m -1,

Then the substring
4x+lix

ik+2ix-k+l

M(i*,j)

ix-k+2 .. .iio

.

ix'ix-

*

Jo.

ix-k-41

.

jyjy-I

I

=

jy-k+I

and

Hence,
im-lim-2

ixix-iI

lo

Lx-k+2(= ixix1

,

=

ix+ lix'ix-.

i= im-1im2

x-k+1

Then the substrings

respectively, so that N(i',]) =j x - yj + 2k (note that, as
N(i' j) > I + m, k > 1).
We first suppose that y . x ' m - 1,

I

ixix- I

i

.

M

1986

and

= im-2im-3 .

]y]y- I . .iy-k+ I

= i mm-I i

MARCH

irix *I*.* ix-k+ I of i

I

and

i

3,

NO.

of M(i,j)). Now, suppose 2m > M(i,j) > m. It follows that there
is a pair of matching substrings

i

M(i',j) > 1 + m.

c-35,

TRANSACTIONS ON COMPUTERS, VOL.

jy-k+ 1 Of ]

are matching with "match" |x - y + 1 + 2(k - 1) =
M(i',j) - 1. Hence, M(i,j) . M(i',j) - 1 again contradicting (1).
The other cases when x < y are similarly disposed.
Proof of Lemma 2: If M(i,j) = m, then clearly
M(*i,j) = 1 + m where * = jo. (This fact motivated the definition

ALGORITHMS

Case 1: Without Node Failure in the Network
The construction of an algorithm to route a message through the
shortest path is now simple. We do not go into the details - as
should be clear, the route will depend on the disposition of the
matching substrings -but indicate the route in case of a particular
disposition. Suppose that there are a pair of matching substrings
ixi-_1

* * ix-k+ 1 of i

and
jyjy-I

]Y-k+Iofj

such that |x - yA + 2k = M(i,j) and y > x.
Given a node p let R(*) denote the hop from p to *p and L(*) the
hop from p to p *. Then a shortest path from i to j is given by the
sequence of hops (read from left to right)

EEEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 3, MARCH 1986

283

R(*)R(*) *.*.* R(*)L(jy-k)L(iy-k-) ..*L(jo)L(*)L(*) ... L(*)R(]jy+)R(jy+2)
x - k + 1 hops

m

-

y

- 1

hops

be arbitrary).
It tnay be observed that in [ 1] the primary algorithm does not yield
the shortest route as it restricts to hops of only one type, either L(*)
or R(*).
Case 2: With Node Failures in the Network
When some nodes are faulty, the message is constrained to move
on a route in a truncated graph obtained from the original graph by
deleting the faulty nodes and their adjacent edges.
Following [12], our routing is based on an ordered search algorithm for which we develop a heuristic function.
Let h(n) be the distance in the truncated graph of a node n from
the goal node j (distance being measured by the minimum number
of hops from n to j). In the original (untruncated) graph, by
Theorem 1, the distance from node n to goal node j is
(*

may

A

h(n)

=

2m -M(n,j).

Clearly h(n) s h(n) for all nodes in the truncated graph.
If we base an ordered search algorthm on an evaluation function
whose heuristic part is the function h(n), we find that the algorithm
is admissable, (Theorem 3-1 of [12]). That is, the algorithm will
find the shortest path from a source node i to a goal node j, provided
a path exists. If we make the reasonable assumption that no node has
a priori knowledge of the status (faulty or otherwise) of any node
other than those to which it is adjacent, then h(n) is clearly the most
informed heuristic function we may use.
To reduce the cost of transmission it is suggested that a probe
message be first transmitted which, moving according to the ordered
search algorithm, discovers the shortest route from source to destination. The actual message is then transmitted along this route.
CONCLUSION
A new routing algorithm is given from message transmission
from source to destination both in presence and absence of faults in
the network. It has been shown that this algorithm gives the shortest
route from source to destination. The maximum number of hops
required for message transmission when some of the processors are
faulty is presently under investigation [11].

REFERENCES
[1] D. K. Pradhan and S. M. Reddy, "A fault-tolerant communication architecture for distributed systems," IEEE Trans. Comput., vol. C-31, Sept.
1982.
[2] T. Feng, "A survey of interconnection networks," Computer, vol. 14,
Dec. 1981.

[3] J. Kuhl and S. M. Reddy, "Distributed fault-tolerance for large multiprocessor systems," in Proc. 7th Ann. Symp. Comput. Architecture, May
1980.

[4]

L.

Bhuyan

and D. P. Agrawal, "Generalized hypercube and hyperbus
a computer network," IEEE Trans. Comput., vol. C-33,

structures for

Apr. 1984.
[5] D. K. Pradhan, "Interconnection topologies for fault-tolerant parallel and
distributed architectures," in Proc. 1981 Int. Conf. Parallel Processing,
Aug. 1981.
[6] D. P. Agrawal, "Graph theoretic analysis and design of multistage interconnection networks," IEEE Trans. Comput., vol. C-32, July 1983.
[7] D. K. Pradhan, Z. Hanquan, and M. L. Schlumberger, "Fault-tolerant
multibus architectures for multiprocessors," in Proc. FTCS-14, June
1984.
[8] S. B. Akers and B. Krishnamurty, "Group graphs as interconnection
networks," in Proc. FTCS-14, June 1984.
[9] C. S. Raghavendra, M. Gerla, and A. Avizienis, "Reliable loop topologies for large local computer networks," IEEE Trans. Comput.,
vol. C-34, Jan. 1985.

...

R(jm-i)

[10] D. K. Pradhan, "Fault-tolerant multiprocessor link and bus network architectures," IEEE Trans. Comput., vol. C-34, Jan. 1985.
[11] A. Sengupta, A. Sen, and S. Bandyopadhyay, "An architecture for faulttolerant distributed systems," in Proc. Comput. Informat. Sci. Ass.
Conf., June 1985.
[12] N. J. Nilsson, Problem Solving Methods ofArtificial Intelligence. New
York: McGraw Hill, 1971.

Comments on "The Design of a Reliable Remote Procedure Call
Mechanism"
L. M. CASEY
Abstract lathe above correspondence' a remote procedure call mechanism is proposed that uses a system-wide sequence number to eliminate
duplicate and orphan messages. We show that the described mechanism
also discards valid messages. A brief description is provided of an alternative scheme that avoids this problem.
Index Terms Communication protocols, distributed systems, fault tollocal area networks, remote procedure cails.

erance,

I.

INTRODUCTION

In their correspondence "The Design of a Reliable Remote Procedure Call Mechanism,"' Shrivastava and Panzieri describe a mech,anism for performing procedure calls across nodes in a local area
network. They take the practical approach of devising a special
communication protocol purely for the implementation of remote
procedure calls. In the fault-free case the protocol involves two
messages: a ser-vice request message from a client to a server and a
reply message from the server back to the client.
Their design aims for exactly once semantics: if no error is signaled to the caller then a remote procedure call results in the procedure being executed once and only once, othe'rwise the procedure may be unexecuted, partially executed or executed once. They
predicate the existence of higher level software to roll back or otherwise recover from situations where an error is signaled.
The exactly once -semantics are achieved by detecting and discarding duplicate and orphan messages.2 The detection is performed
by using a system wide Sequence Number (SN) scheme based on
Lamport's loosely synchronized clocks [1].
II. A PROBLEM WITH GLOBAL SEQUENCE NUMBERS
Loosely synchronized clocks can be used to give a global total
ordering to events by giving each event an SN consisting of the time
the event occurred (as determined by the local node's loosely synchronized clock) appehded to local node's station identity. The
station number foims the low order part of the SN.
Shrivastava and Panzieri use these SN's to number service request messages. Each server keeps track of the last largest SN value
received and discards any incoming message unless it has a higher
SN. Each client discards result messages that do not have the same
SN as the last service request message it sent. Thus, servers that

Manuscript received October 4, 1982; revised October 14, 1983. The delay
in the publication of this work is due to an error by the IEEE Production Staff.
The author is with Bell-Northern Research, Ottawa, Ont. KIY 4H7, Canada.
'S. K. Shrivastava and F. Panzieri, "The design of a reliable remote procedure call mechanism," IEEE Trans. Comput;, vol., C-31, pp. 692-697, July
1982.
2Orphan messages are those destined for a previous incarnation of a
process they usually arise after a node crash.

0018-9340/86/0300-0283$01.00

C 1986 IEEE

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 15, NO. 1, FEBRUARY 2007

201

Finding a Path Subject to Many
Additive QoS Constraints
Guoliang Xue, Senior Member, IEEE, Arunabha Sen, Member, IEEE, Weiyi Zhang, Student Member, IEEE,
Jian Tang, Student Member, IEEE, and Krishnaiya Thulasiraman, Fellow, IEEE

Abstract—A fundamental problem in quality-of-service (QoS)
routing is to find a path between a source–destination node pair
that satisfies two or more end-to-end QoS constraints. We model
vertices and
edges with
this problem using a graph with
additive QoS parameters associated with each edge, for any
2. This problem is known to be NP-hard. Fully
constant
polynomial time approximation schemes (FPTAS) for the case of
= 2 have been reported in the literature. We concentrate on the
general case and make the following contributions. 1) We present a
+ log ) time -approximation algorithm
very simple (
that can be used in hop-by-hop routing protocols. 2) We present
an FPTAS for one optimization version of the QoS routing
problem with a time complexity of ( ( ) 1 ). 3) We present
an FPTAS for another optimization version of the QoS routing
problem with a time complexity of ( log + ( ) 1 )
when there exists an -hop path satisfying all QoS constraints.
When
is reduced to 2, our results compare favorably with existing algorithms. The results of this paper hold for both directed
and undirected graphs. For ease of presentation, undirected graph
is used.
Index Terms—Efficient approximation algorithms, multiple additive constraints, QoS routing.

I. INTRODUCTION
FUNDAMENTAL problem of routing in a network that
provides quality-of-service (QoS) guarantees is to find
a path between a specified source–destination node pair that
simultaneously satisfies multiple QoS constraints, such as cost,
delay, and reliability [3], [12], [16], [18], [22]. Such an environment is commonly modeled by a graph with vertices and
edges where the vertices represent computers or routers
edges represent links. Each edge has
weights
and the
associated with it, representing cost, delay, and reliability,
etc. Weights on edges extend to weights on paths in a natural
way. If the edge weights represent cost, delay, and reliability,
then the corresponding path weight is obtained by adding
(multiplying, in the case of reliability) the weights of the edges

A

Manuscript received February 1, 2005; revised October 1, 2005 and December 11, 2005; approved by IEEE/ACM TRANSACTIONS ON NETWORKING
Editor M. Ajmone Marsan. The work of G. Xue, W. Zhang, and J. Tang was supported in part by the Army Research Office under Grant W911NF-04-1-0385
and the National Science Foundation under Grants CCF-0431167 and
ANI-0312635. The work of K. Thulasiraman was supported in part by the
National Science Foundation under Grant ANI-0312435.
G. Xue, A. Sen, W. Zhang, and J. Tang are with the Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 852878809 USA (e-mail: xue@asu.edu; asen@asu.edu; weiyi.zhang@asu.edu; jian.
tang@asu.edu).
K. Thulasiraman is with the School of Computer Science, University of Oklahoma, Norman, OK 73019 USA.
Digital Object Identifier 10.1109/TNET.2006.890089

on the path. For this reason, such QoS parameters are said to
be additive. QoS parameters such as bandwidth are known as
bottleneck parameters where the corresponding weight of a
path is the smallest of the weights of the edges on the path [8],
[22]. Problems involving bottleneck constraints can be easily
solved by ignoring all edges whose weights are smaller than a
chosen value. Therefore, we restrict our attention to additive
parameters only.
)
It is well known that the multi-constrained path (
problem is NP-hard, even when the number of constraints
is two [22]. Recognizing the need for an efficient solution
to this fundamental problem, many researchers have studied
this problem in the last few years. Most of the existing works
problem with two additive constraints.
concentrate on the
This special case is known as the delay constrained least cost
problem where the two edge weights are cost and
path
delay, and one seeks a minimum cost path subject to a given
problem
delay constraint. Chen et al. [3] studied the
and proposed a polynomial time heuristic algorithm based on
scaling and rounding of the delay parameter so that the delay
parameter of each edge is approximated by a bounded integer.
Xue [25], [26] proposed to use a linear combination of the two
weights and presented a simple algorithm for finding a good
linear combination of the two weights. He also proved near
optimality properties of the two paths found. These heuristics
can find a good solution quickly, but do not provide any performance guarantee.
Warburton in [23] first developed a fully polynomial time ap[5] for the
problem on an
proximation scheme
s,
acyclic graph. Hassin in [9] presented two improved
, where is
one with a time complexity of
the approximation parameter, and the other with a time comwhere is an upper
plexity of
bound on the optimal solution value which is no more than
times the maximum edge-cost. Hassin’s algorithm, which
has a straightforward extension to general graphs, finds a delay
of
constrained path whose cost is within a factor of
that of the delay constrained least cost path. Lorenz and Raz
with a time complexity of
in [15] presented a faster
. In [7], Goel et al. presented an approximation algorithm for the single source all destinations delay
sensitive least cost path problem of time complexity
, where is the hop count of the computed path.
Note that the path computed by this algorithm does not necessarily satisfy the delay constraint: its delay is at most
times the delay constraint and its cost is at most that of the
delay constrained least cost path. In [6], Ergun et al. presented
for the case of acyclic graphs with a time complexity
an

1063-6692/$25.00 © 2007 IEEE

202

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 15, NO. 1, FEBRUARY 2007

TABLE I
SUMMARY OF COMPUTATIONAL COMPLEXITIES OF ALGORITHMS FOR THE MCP PROBLEM

of
. Orda and Sprintson [19] presented a precomputation scheme for QoS routing with two additive parameters.
Guerin and Orda [8] presented efficient approximation algorithms for QoS routing with inaccurate information. More recently [20], Orda and Sprintson presented efficient approximation algorithms for computing a pair of disjoint QoS paths. Applications of QoS in multiservice IP networks and their practical
significance may be found in [17].
problem with three or more constraints has also
The
been studied. In [14], Korkmaz and Krunz proposed a randomproblem. Using simulations they
ized heuristic for the
showed that their heuristic provides better performance than
other algorithms with comparable computational complexity. In
[28], Yuan presented a limited granularity heuristic and a limwhose
ited path heuristic. Xue et al. [27] presented an
running time depends on the size of the input.
In this paper, we concentrate on the general case with
being any fixed integer and make the following contributions.
1) We present a simple
time -approximation algorithm which can be easily used in hop-by-hop
for one optimizarouting protocols. 2) We present an
problem with a time complexity of
tion version of the
. 3) We present an
for another optiproblem with a time complexity
mization version of the
when there exists an -hop path
of
satisfying all QoS constraints. When reduced to the special
, the time complexities of our algorithms comcase of
pare favorably with existing algorithms. Table I summarizes
complexities of related algorithms for various versions of the
problems.
The rest of this paper is organized as follows. In Section II,
we define the problems and some notations. In Section III, we
present our simple -approximation algorithm. In Sections IV
s. In Section VI, we
and V, we present our two
present computational experiences. We conclude this paper in
Section VII.
II. DEFINITION OF PROBLEMS AND NOTATIONS
Throughout this paper, denotes an integer constant which
is greater than or equal to 2. All other constants, functions
and variables are assumed to have real values unless specified
otherwise.
We model a computer network by an edge weighted undi, where
is the set of
rected graph
vertices, is the set of
edges each with
weights, and
is the th weight of edge ,
,
.
,
Let be a path in . The th weight of , denoted by
is the sum of the th weights over the edges on . We assume

that is a connected graph. The decision version of the
problem
is defined in the following.
Definition 2.1
: INSTANCE: An undi, with
nonnegative real-valued
rected graph
,
, associated with each edge
edge weights
; a positive constant ; and a source–destination node
pair
. Question: Is there an – path such that
,
?.
satisfying all
QoS constraints is called
A path
. We say that
a feasible path of
is feasible if it has a feasible path, and
infeasible otherwise. Note that we could formulate the
problem in a seemingly more general form by replacing
with
independent positive constants
and replacing the
constraints
,
with
constraints:
,
.
the following
However, the two forms are equivalent because we can scale
the th weight (on edges, and thereby on paths) from
to
so that for any path ,
,
if and only if
,
.
Therefore, we choose to use the simpler form in this paper.
In the following, we define two optimization versions of this
NP-hard problem.
: INSTANCE: An undiDefinition 2.2
rected graph
, with
nonnegative real-valued
,
, associated with each edge
edge weights
; a positive constant ; and a source–destination node
. PROBLEM: Find an – path
such that
pair
,
, where
is the smallest real
such that there exists an – path satisfying
number
,
.
, we are treating all constraints
In the definition of
equally, where a single parameter
is applied to all constraints. This is slightly different from traditional optimization
problem, where we strictly enforce
versions of the
the delay constraint while approximating the minimum cost.
When the number of constraints is greater than 2, we have to
constraints, because finding a path
approximate at least
satisfying two or more additive constraints is itself an NP-hard
constraints
problem. This motives us to approximate all
simultaneously.
the optimal value of
We call
and call
an optimal path or an optimal solution of
. Note that
if and only if
is feasible. Since
is allowed to
also
be smaller than 1, our optimization problem
introduces a metric to compare two feasible solutions to
—the one with the smaller corresponding
value

XUE et al.: FINDING A PATH SUBJECT TO MANY ADDITIVE QOS CONSTRAINTS

is regarded as a better solution. When
, any opis a feasible path
timal solution of
, but the reverse is not true. Note
for
that when a feasible path for
is not
, we must have
an optimal solution of
. Therefore, we define another optimization version of
problem, named
.
the
Definition 2.3
: INSTANCE: An undi, with
nonnegative real-valued
rected graph
,
, associated with each edge
edge weights
; a positive constant ; and a source–destination node
pair
. PROBLEM: Find an – path
such that
,
, where
is the smallest real
such that there exists an – path satisfying
number
,
.
We call
the optimal value of
and call
an optimal path or an optimal solution of
. Note that
. Also note
if and only if
that
has a feasible path. When
, any optimal sois also a feasible path
lution of
. Also, any feasible path for
for
is guaranteed to be an optimal path
for
.
We note that every optimal solution to
is also an optimal solution to
, but not
is infeasible, every opvice versa. When
timal solution to
is also an optimal solu.
tion to
and
be a constant. If
Let be an – path in
,
(
,
, respectively), then is called a -approxi(to
, remation to
spectively). If is an algorithm that guarantees a -approximais called
tion, then is called a -approximation algorithm.
a fully polynomial time approximation scheme
, if for
,
is a
-approximation algorithm with
any fixed
running time bounded by a polynomial in the input size of the
instance, and in .
Our
s
for
and
need to solve instances of the
(where
is denoted by
following restricted version of
in the restricted version, and the edge weights take positive
integer values) repeatedly with is bounded by a polynomial
in .
: INSTANCE: An undiDefinition 2.4
, with positive integer-valued edge
rected graph
weights
,
, associated with each edge
;a
positive integer constant ; and a source–destination node pair
. Question: Is there an – path such that
,
?

203

edge weight functions). The path
is
(instead of using
.
guaranteed to be a -approximation of
Note that the auxiliary edge weights can be computed locally at
each node, and the shortest path can be computed using either
Dijkstra’s algorithm or Bellman–Ford’s algorithm. Therefore,
our -approximation algorithm can be implemented as either
a centralized or a distributed algorithm, and can be used by
existing routing protocols such as RIP and OSPF [10].
Algorithm 1

-

For each edge
weight

of

, compute an auxiliary edge
.

Compute a shortest – path
in , where the
distance is measured using the auxiliary edge
. Output
.
weighting function
found by
Theorem 3.1: The path
a
-approximation to
, i.e.,
,
, where
is the
optimal value of
. Moreover,
1) if Dijkstra’s algorithm is used, the time complexity of
is
;
2) if centralized Bellman–Ford algorithm is used, the time
is
;
complexity of 3) if distributed Bellman–Ford algorithm is used, the time
complexity (measured in terms of the number of rounds
is
.
of executions needed) of Proof: With a centralized algorithm the auxiliary weights
time as there are
edges. The
can be computed in
rest of the time analysis comes from well known results [5].
For distributed computation, each node can compute the auxiliary weights of the adjacent edges in one round of execution.
Bellman–Ford distributed shortest path algorithm will terminate
in at most rounds. So the overall time complexity of the dis.
tributed algorithm is
Recall
that
is
the
optimal
value
of
.
Therefore,
there
exists
an
– path
such that
,
.
This implies

is

(1)
We can rewrite (1) as
(2)
Summing (2) over all

possible values of , we have
(3)

III. A SIMPLE

-APPROXIMATION ALGORITHM FOR

A very simple
-approximation algorithm, named
, is presented in Algorithm 1. The algorithm comas the maximum of all
putes an auxiliary edge weight
edge weights
divided by . It then computes a shortest – path
using this auxiliary edge weight

Since for every edge

we have
, (3) implies
(4)

204

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 15, NO. 1, FEBRUARY 2007

Note that the left hand side of (4) is
with respect to
a shortest – path in
. Therefore,

. Since
is
, we must have
(5)

we have
, (5) implies

Since for every edge
,

(6)
We can rewrite (6) as
. This proves that
.
IV. AN

is a

,
-approximation to

FOR

In this section, we will present an
for any constant
.

for

A. A Pseudo-Polynomial Time Algorithm for
We first present an
time algorithm for
. The algorithm is named
and is listed as Algorithm 2. When the answer is
, our alwhich minimizes
gorithm also computes a feasible – path
among all – paths in .
Recall that all edge weights in
are positive integers.
Therefore, along any path, each hop increases each of the path
lengths by at least 1. Our algorithm uses a transformation from
. Each
an undirected graph to a directed acyclic graph
vertex
is associated with
vertices in
,
, where the integers
of the form
are used to record the th path length for
.
in , we have diTherefore, for each undirected edge
rected edges in
from
to
such that
,
; as well as dito
such that
rected edges from
,
. All such edges have length
. Therefore, a feasible solution to
corresponds to a path from
to
with
length no more than .
for
Theorem 4.1: Algorithm 2 computes a feasible path
if it has a feasible solution. The worst case
. In addition,
time complexity of the algorithm is
, the path
is a feasible solution to
when the answer is
, where is the smallest integer less than
or equal to such that
is feasible.
has
vertices and
Proof: Note that
edges (recall that
is connected and
is a constant). It follows from the construction of
that
has a directed path
from
to
with length if and only if
in (obtained by keeping only the
the corresponding path
) satisfies
first component in each vertex on
and
for
. Therefore,
has a feasible solution if and only if
to vertex
there is a directed path from vertex
in
with length at most . This proves the
correctness of Algorithm 2.

Algorithm 2
Construct a directed graph

with node set
and edge set
.
be an undirected edge in .
contains
Let
to vertex
directed edges from vertex
such that
,
; as well as directed edges from
to vertex
vertex
such that
,
.
. In
The length of all such edges is
addition,
also contains zero-length edges
to vertex
from vertex
where
if not all of
values
are equal,
if
the
.
Compute shortest paths
from vertex
to vertices
in
,
. If the length of path
is greater
does not have a
than , then
. STOP.
feasible solution. Output
Find the smallest integer
such that the
has length no more than
shortest path
. Output
, together with the path
corresponding to
, obtained by ignoring the
last
components within each node along the
.
path
We note that if a directed path
(from vertex
to vertex
in
) computed in
has length
for some integer
, then the path
at most
is also a feasible solution to
. The
reverse is also true. This proves that the path
returned by
,
Algorithm 2 is a feasible solution to
is the smallest integer less than or equal to
where
such that
is feasible, provided that
is feasible.
is a positive integer, for
and
Since each
, the existence of a directed edge from
to
in
implies that
,
. Therefore, the graph
is acyclic. As a result, it
time to compute the
takes
to all other vertices in
,
shortest paths from
since the single source shortest paths in an acyclic graph can be
computed in linear time (see [5, p. 592]).
B. Polynomial Time Approximate Testing
Our
uses the following polynomial time approximate
testing procedure [15]. For a given positive real number , we
which
construct an auxiliary graph
is the same as except that the edge weighting function
is
for every
,
changed to such that
which is called the th -scaled weighting function. For given
and
(we assume
), we define
real numbers
if

XUE et al.: FINDING A PATH SUBJECT TO MANY ADDITIVE QOS CONSTRAINTS

is feasible (where
) and define
otherwise. Using standard techniques of scaling and rounding
[9], [11], [15], [21], one can prove that
implies
and that
implies
. Recall that
is the optimal value of
. This is formally stated in the following
theorem.
be the optimal value of
Theorem 4.2: Let
. Let
and
be two fixed positive
numbers. Then:
implies
;
•
•
implies
.
is
Furthermore, the time complexity of
.
C. The

for

We may apply Algorithm 1 to compute an – path
. According to Theorem 3.1,
is a lower
and
is an upper bound for
. If
bound for
, we know that
is also an optimal solu. If
, we can use
tion to
the approximate testing procedure to generate a sequence of
lower bound-upper bound pairs so that the ratio of the upper
bound over the corresponding lower bound goes sufficiently
to obtain
close to 1, and then solve an instance of
an
-approximation to
. Following the ideas of
Lorenz and Raz [15], we say that
(
, respectively)
if
is an approximate upper bound for
(
, respectively). We set the initial lower bound
to
and initial approximate upper
of
to
. Our
is prebound of
sented in Algorithm 3.
Algorithm 3

205

Theorem 4.3: Algorithm 3 finds a
-approximation to
in
time.
and
denote the sequences
Proof: Let
of lower bounds and approximate upper bounds generated by
and
of the algorithm. We know that
(7)
is true for
. If

. Assume that (7) is true for
, we set
and

. If
, we set

and
that (7) is also true for
and
the sequences

. It follows from Theorem 4.2
. Also from the definition of
, we have

(8)
of the algorithm is executed no more than
times. However,
according to Theorem 3.1. As a result, the
and
of the
worst case running time required by
.
algorithm is bounded by
(as
In the rest of this proof, we will use to denote
in
) to simplify notations within the proof. Let
be an
, i.e.,
is an –
optimal solution to
path such that
for
. Since
for every
edge
, we have (noting that
has at most
edges)
Therefore,

(9)

Apply Algorithm 1 to compute a
to
.
,
optimal solution to

-approximation
Since

and

.

is an

(10)

.

Set

and set

solution to
. Therefore,
of the algorithm is guaranteed to find a feasible path.
Note that (9) also implies
This

;
;

implies

that

is

a

feasible

(11)

else
let

;
, set
, set

;
;

;

Set

always have integer values, (9) implies

. Apply Algorithm 2 to
the corresponding feasible path

and
.

Let
be the – path found in
of the algorithm. It follows from Theorem 4.1 that
is a feasible
, where
is the smallest
solution to
integer less than or equal to
such that
is feasible. Since
is optimal while
is only feasible, the maximum path weight of
cannot
:
exceed the maximum path weight of
(12)

206

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 15, NO. 1, FEBRUARY 2007

Combining (12) with (11), we obtain

. So for
is a number in the interval
, the feasibility of
guarantees Yuan’s reverse heuristic to find an – path such that
. When it fails to find an –
path, we can conclude that
is infeasible.
So Yuan’s reverse heuristic exhibits several advantages over
the original Yuan’s heuristic. We point out that compared with
has a lower time
Yuan’s reverse heuristic,
complexity and a better approximation performance.
,
runs in time
,
Note that for
of Lorenz and Raz [15] with a
which is faster than the
complexity of
. We point out that for
approximates both the
of [15] is designed to comcost and the delay while the
pute a path that minimizes cost under delay constraint.

(13)
On the other hand, we also have

(14)
Combining (13) and (14), we have

(15)
Some algebraic manipulations on (15) yield the following.

V. AN
(16)
Therefore,

is

an
approximation to
. It follows from Theorem 4.1
that the worst case time complexity of
is
.
is a constant, the time complexity of
Since
dominates the time complexity of all other steps. Therefore, the
.
overall time complexity of Algorithm 3 is
The limited granularity heuristic algorithm of Yuan [28]
is closest to our approximation scheme. Given a precision
, Yuan’s heuristic maintains a table of size
at each node and has a time complexity of
. We
would like to point out the difference between Yuan’s heuristic
. Yuan’s heuristic is designed for
and our
, the decision version of the problem. If
there is an – path such that every path weight of is no more
than
, Yuan’s heuristic is guaranteed to find a feasible
path. When the above condition is not true, the heuristic does
not guarantee finding a feasible path, even in the case where a
, checking the
feasible path exists. Note that for a given
existence of an – path with every path weight no more than
is itself an NP-hard problem. Therefore, when Yuan’s
heuristic fails to find an – path, we do not know whether
is feasible or infeasible. This is a character common to all heuristics. In contrast,
is
, an optimization version
designed for
is
of the problem. In the case where
feasible, FPTAS-SMCP always finds an – path such that
every path weight of is no more than
. Regardless
,
of the feasibility of
always finds a
-approximation to the optimal solution of
. In addition, if for the found path
we
have
, we can conclude that
is infeasible.
Note that Yuan’s heuristic can also be implemented with the
enlarged to
. To distinguish
constraint
from its original form, we call Yuan’s heuristic implemented
in this way Yuan’s reverse heuristic. Note that the feasibility
guarantees Yuan’s reverse heuristic
of
such that
.
to find an – path
,
, which
Note that for

FOR

Following the technique of [7], we present an
for
which guarantees finding a
-approximation to
. When
is feasible, our
finds
a
-approximation to
in
time, where
is the minimum
.
length (in hops) of any feasible path to
The worst-case time complexity of the algorithm (in all cases)
, which is asymptotically the same as the
is
. This
is named
time complexity of
and presented in Algorithm 4.
Algorithm 4

-

Apply Algorithm 1 to compute a
to
.
and
.
-approximation to

set
set

-approximation

is a
.

; set

;

;

Set

. Apply Algorithm 2 to
.
is infeasible)

(
set

;

Let

be the – path returned by Algorithm 2.
,
path
is a

and
.
-approximation to
.

;

XUE et al.: FINDING A PATH SUBJECT TO MANY ADDITIVE QOS CONSTRAINTS

207

(
set

;

path
is a

.
is infeasible
-approximation to
.

;

Since
have

is assumed to be 0 for notational purpose, in case
).
is a feasible solution to
, we

and

(18)
.

Theorem 5.1:
finds a
-approximation to the
problem in
time. In particular, when
is feafinds a
-approximation to the
sible,
problem in
time, where
is the minimum length (in hops) among all
.
feasible solutions to
, we
Proof: Due to the condition checking in
is a
-approximation to
know that the path
if the algorithm stops within
.
is a
-approximaSimilarly, we know that the path
if the algorithm stops within
tion to
. If Algorithm 4 stops within
, we know (from
the correctness proof of Algorithm 3) that the path
is a
-approximation to
, since the constraint in the instance of
is set to
(which is equivalent to
of
).
Next, we analyze the time complexity of Algorithm 2.
calls our -approximation algorithm, which has a time com. Suppose that
solves inplexity of
using Algorithm 2, with taking the values
stances of
. Then
for
and
. Therefore, the total
time required for these calls to Algorithm 2 is bounded by (reis a constant)
call that

also implies that
The feasibility of
. It follows from Theorem 3.1 that
.
It follows from the description of Algorithm 4 that we must
when the algorithm enters
. In
have
. Therefore,
other words, we have
the following inequality is true.
(19)
Assume that we enter
with
. We will
. Since
is a feasible solution to
have
with hop-length equal to , we have

(20)
. Therefore, (20) implies

It follows from (19) that

(21)
Since

is an integer, (21) implies
(22)

Therefore,

is

a

feasible
.

solution

to

It follows from (20) that
(23)

(17)

path returned by Algorithm 2 for
. Since
is optimal
and
has been proved to be feasible, we must have the
following.
Let

In the worst-case, we have
. Therefore, the time
complexity of Algorithm 4 is bounded by
in the worst case.
Finally, we prove the claim corresponding to the case where
is feasible. If the algorithm stops within
, the running time is
, which is bounded
by
. Therefore, we will assume that the
. Let
be a feasible soalgorithm does not stop within
which, among all feasible solution to
, has the minimum number of
lutions to
hops, denoted by
. We will prove that Algorithm
with
such that
4 will stop within

be the

–

(24)
It follows from the definition of
Therefore, (24) implies

that

.

(25)

208

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 15, NO. 1, FEBRUARY 2007

Fig. 1. Ratio of path weight versus constraint. (a) NSFNET. (b) ARPANET. (c) Italian National Network.

This proves that
is guaranteed to be a
-approximaand that Algorithm 4 must stop
tion to
is computed. Since this is the first time we have entered
after
with
, we conclude that
. Therefore,
.
the running time in this case is
We point out some features of
. When
is feasible, it runs very fast, often
. When
is
at the speed of infeasible, it still computes a provably good path, although
the running time could be larger than that of
.
When
is reduced to 2, its corresponding running time is
faster than that of [7] (again note that the goal here is slightly
different from that in [7]).
VI. NUMERICAL RESULTS
To verify the theoretical analysis of the algorithms presented
in this paper, we implemented ,
and
and compared them with Yuan’s heuristic
, Yuan’s reverse heuristic ( 2), as well
as Korkmaz and Krunz’s randomized heuristic
.
The tests were performed on a 2.4 GHz Linux PC with 1G
bytes of memory.
We used well known Internet topologies to verify the suitability of the algorithms, and randomly generated topologies
to verify the computational scalability of the algorithms. The
known Internet topologies include
(14 nodes and 21
edges) [4],
(20 nodes and 32 edges) [1], and Italian
National Network (33 nodes and 67 edges) [1]. As in [3], [7],
[13], [14], [28], the edge weights were uniformly generated in
a range (we used the range [1], [10]). From our analysis, one
should expect our algorithms to perform similarly on various
edge weights. For each topology, we used the same
pair
for all algorithms. For a fair comparison with the limited granularity heuristic, we used
as the parameter in [28]. For each
topology, we tested three different values of : a small value of
such that
is infeasible; a large value
of
such that
is feasible; and a larger
value of
such that
is feasible. We
performed the tests with
.
For
, we first set
to 5 (the small value of ).
,
and were all able

to find a path whose three path weights are 20, 20, 14, for each
. However,
, of
and 2 all failed to find any path. Next, we set
. This time,
was able to find a path with
path weights 20, 20, 14. 2 was able to find a path
with path weights 19, 21, 23 for
. For
,
2 was able to find a path with path weights 20, 20,
still failed to find a path. Finally,
14. However, we set
. This time, all algorithms were able to find a
path. Fig. 1(a) illustrates the quality of the paths found (with
), where the -axis measures the maximum ratio of
the path weight over the path constraint for the path found by
the different algorithms. In case no path was found, we treat
this ratio as infinity. Fig. 1(b) and (c) illustrate the results for
and Italian National Network respectively. We
can see that in all cases,
,
and
find a good path.
Fig. 2(a) illustrates the running times (in seconds) of the
different algorithms, as well as their dependency on the
value of , using the case of
for Italian National
Network. As expected, and
are always the fastest. Also, we observe that both
and
are much faster than and
2. As expected, the running time of
is independent of , while the running time of
is either slightly larger than that of
(when
is small), or very small (when
is large). We also observe
that the running times of and 2 may
increase with
slightly, but not significantly. This is due
to the fact that more edge relaxations may be performed by
and 2 for larger values of .
Fig. 2(b) illustrates the running times (in seconds) of our
two
s as functions of , using the case of
,
35 for Italian National Network. As expected, the running
times increase with
. Again, we note that the running time
of
is independent of
, while the running
time of
is either slightly larger than that of
(for small ) or very small (for large ).
To study the scalability of our
s with the network size,
we also tested
and
on large
network topologies generated by
, a well known Internet

XUE et al.: FINDING A PATH SUBJECT TO MANY ADDITIVE QOS CONSTRAINTS

209

Fig. 2. Running time versus various factors. (a) Running time versus W (b) Running time versus  (c) Running time versus n.

topology generator [2]. The values of
were chosen similarly
as in the case of well known topologies. We report results with
for all topology sizes for illustration.
BRITE provides several well-known models (including the
Waxman model [24]) for generating reasonable network topologies. We adopted the Waxman model (with default parameters
provided by BRITE) to generate random networks. We used five
different numbers of nodes: 80, 100, 120, 140, 160. Correspondingly, BRITE generated five network topologies with the following sizes: 1) 80 nodes with 314 edges; 2) 100 nodes with
390 edges; 3) 120 nodes with 474 edges; 4) 140 nodes with 560
edges; and 5) 160 nodes with 634 edges. For each topology, we
ran 10 test cases. For each test case, we randomly generated
a source–destination node pair
and used this pair for all
tested algorithms. For this node pair, we used a small value of
and a large value of
to test the algorithms (note that these
values of
may change when the node pair changes). The running times of our algorithms are shown in Fig. 2(c), where the
running time shown for each algorithm is the average over 10
cases, where the largest standard deviation is 1.29.
We observe that the running times of all algorithms (expect
that of
with large ) increase with the increase
of the network size. For the random networks generated,
is
approximately . Since we used
and
, the running times of both
and
should
be
for the cases tested. For small values
of ,
may have to solve instances of
with set to 7 H (note that
in this case) for
. Note that for the same setting,
only needed to solve one instance of
with set to
. Since the running time of
is proportional to
, we can use
as an estimate of the maximum of the ratio of the running time
of
over the running time of
.
Fig. 2(c) conforms to this analysis.
In terms of running time, most heuristic algorithms are fast,
but do not guarantee quality of solutions. There could be situations where they will produce solutions that deviate from the
optimal solutions to a considerable extent. So,
is
quite fast since it requires
shortest path computations.
is the fastest since it requires only one shortest path

produces solutions better
computation. We note that whenthan or as good as the solutions produced by
ever the latter is successful.
s take more time as the value
of chosen becomes smaller. In other words, the more accuracy
required for the solution, the more is the time taken. This is not
surprising since
s provide guarantees on the accuracy of
the solutions produced. This guarantee requirement is the reason
s. is a
for large running times taken by the
constant factor approximation algorithm with the constant equal
to the number of edge weights. However, simulation results in
comparison with those of our
s which guarantee accuracy of the solutions (specified arbitrarily by the value of ), provide evidence that performs quite well in practice.

VII. CONCLUSION
In this paper, we have studied the
problem with additive QoS constraints, where
is a fixed constant. We
presented a novel
time -approximation algorithm which uses a single auxiliary edge weight to compute
a shortest path. Because of this property, the algorithm is easily
implementable in a hop-by-hop environment. We also presented
two FPTASs for two slightly different versions of the problem
whose time complexities, when reduced to the case of
,
compare favorably with existing algorithms. To implement the
s proposed in this paper in the current networking environment, some careful modifications will be necessary. The
routing tables will have to store the next hop addresses for every
source destination pair for a few discrete values of . These
values of may be used to determine the traffic classes in the
network. It may be noted that the results presented in this paper,
although derived under the model of an undirected graph, are
equally valid for the case of directed graphs.

ACKNOWLEDGMENT
The authors wish to thank the associate editor and the
anonymous reviewers whose valuable comments on earlier
versions of this paper helped to improve the quality of the paper
significantly.

210

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 15, NO. 1, FEBRUARY 2007

REFERENCES
[1] R. Andersen, F. Chung, A. Sen, and G. Xue, “On disjoint path pairs
with wavelength continuity constraint in WDM networks,” in Proc.
IEEE INFOCOM’04, pp. 524–535.
[2] BRITE. [Online]. Available: http://www.cs.bu.edu/brite/
[3] S. Chen and K. Nahrstedt, “On finding multi-constrained paths,” IEEE
ICC’98, pp. 874–879.
[4] X. Chu and B. Li, “Dynamic routing and wavelength assignment
in the presence of wavelength conversion for all-optical networks,”
IEEE/ACM Trans. Netw., vol. 13, no. 3, pp. 704–715, Jun. 2005.
[5] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction
to Algorithms. New York: McGraw-Hill, 2001.
[6] F. Ergun, R. Sinha, and L. Zhang, “An improved FPTAS for restricted
shortest path,” Inf. Process. Lett., vol. 83, pp. 287–291, 2002.
[7] A. Goel, K. G. Ramakrishnan, D. Kataria, and D. Logothetis, “Efficient
computation of delay-sensitive routes from one source to all destinations,” in Proc. IEEE INFOCOM 2001, pp. 854–858.
[8] R. Guerin and A. Orda, “QoS routing in networks with inaccurate information: theory and algorithms,” IEEE/ACM Trans. Netw., vol. 7, no.
3, pp. 350–364, Jun. 1999.
[9] R. Hassin, “Approximation schemes for the restricted shortest path
problem,” Math. Oper. Res., vol. 17, pp. 36–42, 1992.
[10] C. Huitema, Routing in the Internet, 2nd ed. Englewood Cliffs, NJ:
Prentice-Hall PTR, 2000.
[11] O. H. Ibarra and C. E. Kim, “Fast approximation algorithms for the
Knapsack and Sum of Subset problems,” J. ACM, vol. 22, pp. 463–468,
1975.
[12] J. M. Jaffe, “Algorithms for finding paths with multiple constraints,”
Networks, vol. 14, pp. 95–116, 1984.
[13] A. Juttner, B. Szviatovszki, I. Mecs, and Z. Rajko, “Lagrange relaxation
based method for the QoS routing problem,” in Proc. IEEE INFOCOM
2001, pp. 859–868.
[14] T. Korkmaz and M. Krunz, “Mult-constrained optimal path selection,”
in Proc. IEEE INFOCOM 2001, pp. 834–843.
[15] D. H. Lorenz and D. Raz, “A simple efficient approximation scheme
for the restricted shortest path problem,” Oper. Res. Lett., vol. 28, pp.
213–219, 2001.
[16] Q. Ma and P. Steenkiste, “Quality-of-service routing for traffic with
performance guarantees,” Proc. IWQoS’97, pp. 115–126.
[17] Quality of Service in Multiservice IP Networks, Proc. QoS-IP 2003,
ser. Lecture Notes in Computer Science, Vol. 2601, M. Ajmone
Marsan, G. Corazza, M. Listanti, and A. Roveri, Eds. Berlin:
Springer, 2003.
[18] A. Orda, “Routing with end-to-end QoS guarantees in broadband
networks,” IEEE/ACM Trans. Netw., vol. 7, no. 3, pp. 365–374, Jun.
1999.
[19] A. Orda and A. Sprintson, “Precomputation schemes for QoS routing,”
IEEE/ACM Trans. Netw., vol. 11, no. 4, pp. 578–591, Aug. 2003.
[20] ——, “Efficient algorithms for computing disjoint QoS paths,” in Proc.
IEEE INFOCOM 2004, pp. 727–738.
[21] S. Sahni, “General techniques for combinatorial approximation,” Oper.
Res., vol. 25, pp. 920–936, 1977.
[22] Z. Wang and J. Crowcroft, “Quality-of-service routing for supporting
multimedia applications,” IEEE J. Sel. Areas Commun., vol. 14, no. 7,
pp. 1228–1234, Sep. 1996.
[23] A. Warburton, “Approximation of Pareto optima in multiple-objective,
shortest path problem,” Oper. Res., vol. 35, pp. 70–79, 1987.
[24] B. M. Waxman, “Routing of multipoint connections,” IEEE J. Sel.
Areas Commun., vol. 6, no. 9, pp. 1617–1622, Dec. 1988.
[25] G. Xue, “Primal-dual algorithms for computing weight-constrained
shortest paths and weight-constrained minimum spanning trees,” Proc.
IEEE IPCCC’2000, pp. 271–277.
[26] ——, “Minimum cost QoS multicast and unicast routing in communication networks,” IEEE Trans. Commun., vol. 51, no. 5, pp. 817–824,
May 2003.
[27] G. Xue, A. Sen, and R. Banka, “Routing with many additive QoS constraints,” in Proc. IEEE ICC’2003, pp. 223–227.
[28] X. Yuan, “Heuristic algorithms for multiconstrained quality-of-service
routing,” IEEE/ACM Trans. Netw., vol. 10, no. 2, pp. 244–256, Apr.
2002.

Guoliang (Larry) Xue (SM’99) received the B.S.
degree in mathematics and the M.S. degree in operations research from Qufu Teachers University, Qufu,
China, in 1981 and 1984, respectively, and the Ph.D.
degree in computer science from the University of
Minnesota, Minneapolis, in 1991.
He is a Full Professor in the Department of
Computer Science and Engineering at Arizona State
University, Tempe. He has held previous positions
at Qufu Teachers University (Lecturer, 1984–1987),
the Army High Performance Computing Research
Center (Postdoctoral Research Fellow, 1991–1993), the University of Vermont (Assistant Professor, 1993–1999; Associate Professor, 1999–2001). His
research interests include efficient algorithms for optimization problems in
networking, with applications to fault tolerance, robustness, and privacy issues
in networks ranging from WDM optical networks to wireless ad hoc and sensor
networks. He has published over 130 papers in these areas. His research has
been continuously supported by federal agencies including NSF and ARO.
Dr. Xue received the Graduate School Doctoral Dissertation Fellowship
from the University of Minnesota in 1990, a Third Prize from the Ministry of
Education of P.R. China in 1991, an NSF Research Initiation Award in 1994,
and an NSF-ITR Award in 2003. He is an Editor of Computer Networks, an
Editor of IEEE Network, and an Associate Editor of the IEEE TRANSACTIONS
ON CIRCUITS AND SYSTEMS-I: REGULAR PAPERS, and the Journal of Global
Optimization. He has served on the executive/program committees of many
IEEE conferences, including INFOCOM, Secon, Icc, Globecom and QShine.
He served as the General Chair of the IEEE International Performance,
Computing, and Communications Conference in 2005, and will serve as a
TPC co-chair of IEEE Globecom’2006 Symposium on Wireless Ad Hoc and
Sensor Networks, as well as a TPC co-chair of IEEE ICC’2007 Symposium
on Wireless Ad Hoc and Sensor Networks. He also serves on many NSF grant
panels and is a reviewer for NSERC (Canada). He has been a member of the
ACM since 1993.

Arunabha (Arun) Sen (M’90) received the Bachelor degree in electronics and telecommunication engineering from Jadavpur University, Kolkata, India,
and the Ph.D. degree in computer science from the
University of South Carolina, Columbia.
He is currently an Associate Professor in the
Department of Computer Science and Engineering,
Arizona State University, Tempe. He also serves
as the Associate Chairman of the department responsible for Graduate Programs and Research.
His research interest is in the area of resource
optimization problems in wireless and optical networks. He primarily studies
the algorithmic issues related to the problems in these domains and utilize
graph theoretic and combinatorial optimization techniques to find solutions.
He has published over 80 research papers in peer-reviewed journals and
conferences on these topics. He has served many IEEE and ACM workshops
and conferences either as a Program Committee member or as the Chair of the
Program Committee.

Weiyi Zhang (S’02) received the B.E. and M.E. degrees from Southeast University, China, in 1999 and
2002, respectively. Currently, he working toward the
Ph.D. degree in the Department of Computer Science
and Engineering at Arizona State University, Tempe.
His research interests include reliable communication in networking, protection and restoration in
WDM networks, and QoS provisioning in communication networks.

XUE et al.: FINDING A PATH SUBJECT TO MANY ADDITIVE QOS CONSTRAINTS

Jian Tang (S’04) received the B.E. and M.E. degrees
from Beijing University of Posts and Telecommunications, China, in 1998 and 2001, respectively. Currently, he is working toward the Ph.D. degree in the
Computer Science and Engineering Department of
Arizona State University, Tempe.
His research interest is in the area of wireless
networking and mobile computing with emphases
on routing, scheduling and cross-layer design in
wireless networks.

Krishnaiyan Thulasiraman (F’90) received the
Bachelor’s degree and Master’s degree in electrical
engineering from the University of Madras, India, in
1963 and 1965, respectively, and the Ph.D. degree
in electrical engineering from Indian Institute of
Technology (IIT), Madras, in 1968.
He holds the Hitachi Chair and is Professor in
the School of Computer Science at the University of
Oklahoma, Norman, where he has been since 1994.
Prior to joining the University of Oklahoma, he was
Professor (1981–1994) and Chair (1993–1994) of
the Electrical and Computer Engineering Department, Concordia University,
Montreal, Canada. He was on the faculty in the Electrical Engineering and
Computer Science Departments of the IIT Madras during 1965–1981. His
research interests have been in graph theory, combinatorial optimization,
algorithms and applications in a variety of areas in computer science and

211

electrical engineering, including electrical networks, VLSI physical design,
systems level testing, communication protocol testing, parallel/distributed
computing, telecommunication network planning, fault tolerance in optical
networks, interconnection networks, and more. He has published more than
100 papers in archival journals, coauthored with M. N. S. Swamy the textbooks
Graphs, Networks, and Algorithms (Wiley Inter-Science, 1981) and Graphs:
Theory and Algorithms (Wiley Inter-Science, 1992), and authored two chapters
in the Handbook of Circuits and Filters (CRC and IEEE, 1995) and a chapter on
graphs and vector spaces for the Handbook of Graph Theory and Applications
(CRC Press, 2003).
Dr. Thulasiraman has received several awards and honors, including the
Endowed Gopalakrishnan Chair Professorship in Computer Science at IIT,
Madras (Summer 2005), elected member of the European Academy of Sciences (2002), IEEE Circuits and Systems (CAS) Society Golden Jubilee Medal
(1999), Fellow of the IEEE (1990), and Senior Research Fellowship of the
Japan Society for Promotion of Science (1988). He has held visiting positions
at the Tokyo Institute of Technology, University of Karlsruhe, University of
Illinois at Urbana-Champaign, and Chuo University, Tokyo. He has been Vice
President (Administration) of the IEEE CAS Society (1998, 1999), Technical
Program Chair of ISCAS (1993, 1999), Deputy Editor-in-Chief of the IEEE
TRANSACTIONS ON CIRCUITS AND SYSTEMS I (2004–2005), Co-Guest Editor of
a special issue on Computational Graph Theory: Algorithms and Applications
(IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS, March 1988), Associate
Editor of the IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS (1989–1991,
1999–2001), and Founding Regional Editor of the Journal of Circuits, Systems,
and Computers and an editor of the AKCE International Journal of Graphs
and Combinatorics. Recently, he founded the Technical Committee on Graph
Theory and Computing of the IEEE CAS Society. He has been a member of
the ACM since 1995.

619

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 5, MAY 1987
*

*
*

*

*

.

.

*

x

X

.

.

*

X

*

X

*

X

.

.

*

X

x *

m=6 n=4

m=n=4
Points of L' are represented by "X"
Points of R' are represented by "*"
Fig. 7.

X

[11] W. L. Miranker and A. Winkler, "Space-time representations of
computational structures," IBM Res. Rep., RC 9775, 1982.
. [12] D. I. Moldovan, "On the analysis and synthesis of VLSI algorithms,"
IEEE Trans. Comput., vol. C-31, pp. 1121-1126, Nov. 1982.
* ([13] F. P. Preparata and J. Vuillemin, "Area-time optimal VLSI networks
for multiplying matrices," Information Processing Lett., vol. 11, no.
*
2, pp. 77-80, 1980.
[14] P. Quinton, "The systematic design of systilic arrays," INRIA Res.
Rep. 216, Rennes, 1983.
Y. Robert and M. Tchuente, "Une approche divide-and conquer pour
.......[15]
des algorithmes systoliques," Rapport de Recherche IMAG, 393,
Grenoble, Sept. 1983.
[16] U. Weiser and A. Davis, "Mathematical representation for VLSI
arrays," Univ. Utah, Salt Lake City, Rep. UUCS-80-1 11, Sept. 1980.

opposite directions. We have shown that optimal arrays, i.e., those
which can support algorithms with running time Minp,m T(p, -m) =
3n - 2, are of size at least n x n (resp. n x (n + 1)) if n is odd
(resp. even).
Since the complexity results are based on the assignment of the
On an Optimally Fault-Tolerant Multiprocessor Network
coefficients c,j to region An, the method can trivially be applied to the
Architecture
computation of the product of rectangular matrices.
In practical situations, especially when n is large, it is not realistic
A. SENGUPTA, A. SEN, AND S. BANDYOPADHYAY
to assume that the size of the array can be adjusted to the size n x n
of the matrices. Therefore, the study of optimal algorithms associated
with arrays of size p x m where p or m is much smaller than n, is an
Abstract-This correspondence presents a class of optimally fault
important and interesting question. In [11], we have applied the tolerant multiprocessor network architecture, based on the networks
combinatorial approach introduced here to derive optimal algorithms proposed earlier by Pradhan [71, where the networks are represented by
associated with arrays of size n x 2 and n x 3.
regular digraphs. Because of optimal fault tolerapce, the number of
In all optimal algorithms exhibited here, and which compute the connections per node is precisely related to the degree of fault tolerance
product of two dense square matrices of order n in time T 3 n - 2, the network is designed to provide. The routing of messgges in presence
on an array of size n x n, some data variables a,k, bkj must be input Qf faults is adaptive and unless the number of faults is equal to the degree
several times to the array. We conjecture that this multiple reading of of fault tolerance the increase in routing delay in presence of faults is
some data variables, is a necessary condition for the designing of minimal.
algorithms which run in time T = 3 n - 2 on arrays of size S <=
Index Terms-Connectivity, diameter of graphs, fault-tolerant netn(n + 1). Since in all the methodologies developed in the literature
for the systematic synthesis of systolic arrays [6]-[8], [11], [121, work, multiprocessor network, regular graphs, shuffle-exchange graph.
[14], [16], any input of the problem is represented by a single
I. INTRODUCTION
variable, this conjecture implies that these methodologies need to be
extended in order to yield arrays of size S <'= n(n + 1) which
With the recent developments in technology, it has been possible to
compute the product of two dense square matrices of order n, in time interconnect a large number of computing elements to form one
T = 3n - 2.
integrated system with processing, control, and information being
distributed among these elements. The system might be represented
REFERENCES
by a number of computing elements with physical communication
[1] A. L. Fischer, H. T. Kung, L. M. Monier, H. Walker, and Y. Dohi, links between the elements. Whenever some element wants to
"Design of the PSC: A programmable systolic chip," in Proc. Third communicate with some other element, it communicates through
Caltech Conf. VLSI, R. Bryant Ed., March 1983.
other elements unless there exists a direct physical link between the
[2] L. J. Guibas, H. T. Kung, and C. D. Thompson, "Direct VLSI two. Several researchers proposed different topologies for the
implementation of combinatorial algorithms," in Proc. Caltech Conf. interconnections between the computing elements [l]-[20] for such
VLSI: Architecture, Design, Fabrication, Caltech, Jan. 1979, pp.
multiprocessor systems. Major key considerations in the design of the
509-521.
[3] H. T. Kung, "Why systolic architectures," Computer, vol. 15, pp. topology of the network are the overall reliability and fault tolerance,
the delay in routing messages, the message routing schemes with and
37-46, Jan. 1982.
[4] H. T. Kung and P. L. Lehman, "Systolic VLSI arrays for relational without the presence of faults and the increase of routing delay in
data base operations, " in Proc. Int. Conf. Management Data, A CM- presence of faults. Usually a measure of the fault tolerance of the
SIGMOD, 1980, pp. 105-116.
overall system is given by the maximum number of computing
[5] H. T. Kung and C. E. Leiserson, "Systolic arrays for VLSI," in elements of the network which can fail simultaneously without
Introduction to VLSI Systems, C. A. Mead and L. A. Conway, prQhibiting the ability of every fault-free element to communicate
Eds. Reading, MA: Addison-Wesley, 1980, sect. 8.3, pp. 37-46.
[6] H. T. Kung and W. T. Lin, "An algebra for VLSI algorithm design,"
Tech. Rep., Carnegie-Mellon Univ., April 1983.
Manuscript received December 5, 1985; revised December 18, 1985. S.
[7] C. E. Leiserson and J. B. Saxe, "Optimizing synchronous systems," J. Bandyopadhyay is supported by a grant from the Natural Science and
VLSI Comput. Syst., vol. 1, no. 1, pp. 41-47.
Engineering Research Council of Canada.
A. Sengupta is with the Department of Computer Science, University of
[8] G. J. Li and B. W. Wah, "The design of optimal systolic algorithms,"
IEEE Trans. Comput., vol. C-34, pp. 66-77, Jan. 1985.
South Carolina, Columbia, SC 29208.
A. Sen was with the Department of Computer Science, University of South
[9] L. Melkemi and M. Tchuente, "Programmation du produit matriciel
sur un reseau systolique rectangulaire," TSI, vol. 4, no. 5, pp. 459- Carolina, Columbia, SC 29208. He is now with the Department of Computer
470, 1985.
Science, Arizona State University, Tempe, AZ.
[10]
, "I/O-time tradeoffs for matrix product on programmable
S. Bandyopadhyay is with the Department of Computer Science, University
arrays," in Proc. Parallel Computing 85, M. Feilmeir et al., Eds. of Windsor, Windsor, Ont., Canada.
Amsterdam, The Netherlands: North Hol., 1986, pp. 187-192.

IEEE Log Number 8612066.

0018-9340/87/0500-0619$01.00 © 1987 IEEE

620

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 5, MAY 1987

messages with every other fault-free element. The maximum delay in
routing messages is measured by the maximum value of the shortest
path between all pairs of elements. Obviously, a good design of the
topology should aim at increasing the fault tolerance as far possible,
reducing the message routing delay, reducing the increase in routing
delay in presence of faults. However, the fault tolerance cannot be
increased indefinitely, since the fault tolerance is bounded above by
one less than the minimum number of connections per computing
element in case of bidirectional links and by one less than the
minimum of the number of incoming and outgoing connections per
node in case of unidirectional links. A network will be referred to as
optimally fault tolerant if the fault tolerance is equal to this upper
bound. Usually a topology is referred to as a regular topology if the
number of connections for each element is same for the entire
network. Most of the discussed regular networks either have a fixed
fault tolerance (cannot be changed by changing the number of
connection per element) as in [4] and [5] or are not optimal [2], [3],
[61, [8], [10]. Two topologies are discussed in [9], one of them uses
unidirectional links and is represented by a digraph and is optimally
fault tolerant. The other uses bidirectional links and has not been
shown to be optimally fault tolerant.
In this correspondence, we will present a new topology of
interconnections between the computing elements of a network and
address all the above key considerations. The proposed topology is
based on that proposed by Pradhan [7] which has been shown to be
fault tolerant in most of the cases. Hpwever, the proposed topology is
different from [7] in that it uses unidirectional links instead of
bidirectional links as in [7]. The topology is regular in the sense that
each computing element has the same number of message incoming
and outgoing links. However, the presence of unidirectional links
does not increase the message routing delay. We will show that with
this topology, the network possesses the maximal fault tolerance (the
topology given in [7] is not always maximally fault tolerant), the
message routing delay is small (of the order of logarithm of the
number of elements in the network). An interesting feature of this
topology is that unless the number of faulty elements is equal to the
fault tolerance of the network, the increase in message routing delay
in presence of faults is minimal.
II. THE PROPOSED NETWORK

The network having unidirectional links between different computing elements will be represented by a digraph where each computing
element will be represented by a node of the graph and the
communication link between a pair of computing elements will be
represented by a directed edge between the two nodes representing
the computing elements. Thus, the"fault tolerance of the network is
given by the node connectivity of the graph minus one and the
maximum message routing delay is given by the diameter of the
graph. As discussed in [7] and [9], the same graphical representation
can be used for multiprocessor multibus network where processorbus communication is unidirectional and the nodes represent buses
while the edges represent processors.
Let the digraph G be given by (V, E) where V is the set of nodes
andE is the set of directed edges. Throughout our discussion, we will
use G and the network interchangeably. We will assume that there are
rm nodes in Vfor some positiveintegers rand m. Each node i C Vis
* , m -1) where i E
represented by anm-dimensional vector (io, il
{O, 1,2,2. , r - 1} for all 0 j] . m - 1. Conversely, for each
m-dimensional vector of this type, there exists a node in V. We will
refer io and im as the most and the least significant digits of the
vector representation, respectively. The topology of the interconnections between different nodes is defined by the edge setE as follows.
There is a directed edge from the node ito the nodej if any of the
following conditions is satisfied.
a) ik = jk for all 0 k . m - 2.
b) 4k = Jk-I for all Ic kc m - 1 and io = j -1.
c) Ifik= ik+lforallO . k < m - 2,thenJk = lk+ 1 (modr)
for allO c k . m - 1.
As an example, the topology of the network with 8 nodes having r

000

-OOj

oil

Fig. 1. An example of the topology with r = 2 and m = 3.

= 2 and m

=

3 is shown in Fig. 1. The digraph G constructed

above, effectively means that a node i will have a direct communication link to another nodej iff the vector representation of i differ from
that of j only in the least significant digit position or if the vector
representation ofj is one digit 'rotate left version of that of i. For r =

2, the graph is quite similar to shuffle-exchange graphs [22]-[24].
Theorem 1: The digraph G constructed as above is a regular graph
with each node having indegree r and outdegree r.
Proof: Consider a node i E Vhaving the m-dimensional vector
representation as (io, il, -, 4 - l)- It has r - 1 incoming-edges from
and r - 1 outgoing edges to each of the nodes having vector
representations (io,4', *im-2, x) where 0 . x s r - 1 and x *
ihn-i If ij = i4+I for all j, 0 c j . m - 2, then i has another
incoming edge from the node having vector representation (k, k, *
k) where k = io - 1 (mod r) and has another outgoing edge to the
node having vector representation (q, q, ***, q) where q = io + 1
(mod r). Ifij * ij, for allj, then i has another incoming edge from
the node having the vector representation (m-i,' io, il, *, im-2)
which is distinct from i. Also this i will-have another outgoing edge to
the node having vector representation (1, k, * m*,
i -i, io) which is
distinct from i. Hence, the theorem.
The following properties of the digraph G will be shown in this
correspondence.
i) G has diameter of 2m - 1.
ii) G has connectivity r, the maximum possible connectivity.
Hence, the fault tolerance of the network topology given by G is r -

iii) If no more than r - 2 nodes are removed from G and
accordingly all the edges to and from these nodes are deleted from G,
the resulting graph can have a maximum increase in diameter by 2.
Thus, unless all the r - 1 nodes are faulty, the diameter increases by
only 2.
If an undirected version of the above graph G is considered by
omitting the direction of the edges and assuming them to be
undirected edges, then this. undirected version happens to be a
subgraph of the topology considered in [7]. However, the topology
considered in [7] has fault tolerance r. - 1, same as above and has the
same diameter as G above (this will be shown later). In the case of
fewer than r - 1 faults, the topology in [7] exhibits an increase of
message routing delay by lI t where t is the number of.faulty nodes
and is no more than (r - l)/2. This is obviously a weaker result as
compared to Property iii).
In order to prove the Property i), consider any source node s who
needs to comm.nunicate message with any other destination node d. Let,
the vector representations of s and d be, respectively, (so, s,i
sm-I) and (do, di, * dm
d l). Consider the following path'of length
no more than 2m - 1 from s to d which we will henceforth refer to as
the normal path np(s, d) between s and d. In the following np(s, d),
the nod& having vector representation (do, dl,
S, -2) will be
referred-to as the penultimate node in np(s, d).

621

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 5, MAY 1987

(so,si, S*,
(SOi, s2,

(SIg S29

' *

-

, Sm-29

do, di)

(Sm-2, do,

di,

dm-2)

.

.

Paths 3 to r: np(ni, ni,k) followed by np(nik", n1)

Sm..2, d0,2 so)
Sm_29 do, QO

(Si, S2,

(do, di,

Path 2: np(ni, n1)

o)

*
.

* * *,

for all k.i and kitEj.

dm -2, Sm-2)

(dog di,

dm_29 dm- 0In absence of any faults, using the path np(s, d) of length no more

than 2m - 1, any node s can communicate with any other node d. (It
might be noted that the path np(s, d) is quite similar to that referred
in [7]. However, considering the typographical error in [7] which was
subsequently corrected in [21], this is the same path as in [7] after
correction.) The routing of message is also quite simple if we assume
the message format to be such that it contains the destination address.
Any intermediate node in the path, by looking into the destination
address, can precisely determine which node it should forward the
message to.
Ill. FAULT TOLERANCE OF THE NETWORK
In this section, we will prove Properties ii) arid iii) of the network.
First, we will show that the network has fault tolerance r. - 1 and in
order to show that, we will prove that for any source node s and any
destination node d, even if there are r - 1 faulty nodes in the
network, there exists a path from s to d containing no faulty node. Let
us assume that ni represents the node whose vector representation is
,j
(j, j,
9 D
Lemma' 1: For any d, the paths given by np(ni, d) and np(n1, d)
are node disjoint whenever i * j.
Proof: Each node in the path np(nJ, d) has, in its vector
representation, j appearing as the most significant digit, excepting the
penultimate node which has j as the least significant digit in its vector
representation. Thus, if the paths are not node disjoint, then the
common node must be the penultimate node in one path and not in the
other. Let us assume that such a node exists, i.e., the paths are not
node disjoint and it appears as the penultimate node in np(n', d) and
after p hops from n' in np(ni, d), p < 2m - 2. Thus, its vector
representation in the two paths are respectively as follows and these
two vectors must be the samne.

do di

*-

i i

...

dm-q-2
i

dm-q
do

...

...

dmq2 X

dq-l

x

where p = 2q or 2q + 1 depending on whether p is even or odd and
x = i if p is even and x = dq if p is odd. Now, p cannot be even as
that would imply i =j. Since q . m - 2, each d's in the second row
above appears in the first row also. The equality of the vectors imply,

=dq = d2q-m+ I

=d(a +)q-ami+a

for any positive integer a such that (a + l)q - am + a . 0.
Assume a - Q(2(q + 1) - m)/(m - q + 1)], for which (a + l)q
- am + a . m - q - 2and hence d(a+I)q -am+a = iimplyingi =
j, which is a contradiction and hence the lemma.
Using similar arguments, we can prove the following lemma.
Lemma 2: For any s, the paths given by np(s, n i) and np(s, n1) are
node disjoint whenever i ]j.
Lemma 3: For any i,j,suchthati * j and 0 . i, j. r - 1, there
exists r node disjoint paths from n1 to ni.
Proof: Let the node whose vector representation is (i, i, i,
k, i) be represented by n1K Obviously the path np(ni, ni.k) is of
length 2. Consider the following r paths, where additions are mod r.
Path 1: ni-ni

I

*

nj- -+nj

Consider the Path 1, each of the nodes in this path has its vector
representation containing the same digit in all the m positibns. On the
other hand, each intermediate node in Path 2 contains both i and j in
its vector representation and each intermediate node in any of the
Paths 3 thru r contains i and j and the corresponding k in its vector
representation. Thus, nodes in Path1 'are disjoint from the nodes in
any other path. Similarly, only i and j appear in the vector
representations of the nodes in Path 2 while k appears in those of the
nodes in Paths 3 thru r. Thus, the nodes in Path 2 are disjoint from
those in Paths 3 thru r. The Paths 3 thru r are node disjoint since k is
distinct for each of them and it appears in all the vector representations. Hence the lemma.
Combining the results of Lenimas 1 to 3, we find that for any
source s and destination d, there exist r node disjoint paths from s to d
and hence we have the following theorem.
Theorem 2: The network given by the graph G is optimally fault
tolerant.
In presence of faults, the message routing scheme for any given s
and d can be specified as follows.
Step 1: If none of the nodes in np(s, d) is faulty, use np(s, d).
Step 2: If all the faulty nodes are of the form nm for some j, then
find some k such that nk is not faulty, use np(s, nk) followed by
np(nk, d). The routing delay increases by at most 2m - 1.
Step 3: If all the nodes of the form nJ are fault free, then find some
k and I such that np(s, n k) and np(n , d) are fault free paths. If k = 1,
the routing delay increases by at most 2m - 1. If k * 1, use np(s,
nk) followed by the Path 1 referred in Lemma 3 from nk to n1
followed by np(n', d). The routing delay increases by at most 2m +
r - 2.
Step 4: Find k and I as in Step 3 and use np(s, nM) followed by a
fault free path from one of the r paths referred in Lemma 3 followed
by np(n', d). The routing delay increases by at most 4m.
The message routing scheme in presence of faults is more or less
similar to that in [7]. Apparently, in presence of faults the routing
delay increases by a factor of three in the worst case. However, if
there are no more than r - 2 faults, the message might be routed with
a minimal increase of routing time. For a given s and d, consider the
subgraph G' of G as shown in the Fig. 2, where s and d are
represented by the vectors (so, si, * * *, sm- 1) and (do, di, * ),
dmrespectively, and the intermediate nodes are shown in their corresponding vector- representations. This subgraph shows r paths from s
to d. Unfortunately, these r paths are not node disjoint. We will refer
these paths by path 0, path 1, *
path r - 1. A path is said to be
blocked if at least one node in that path is faiulty.
Lemma 4: In the graph G', the same,node cannot appear in more
than two distinct paths.
Proof: If possible suppose the same node appears in three paths:
path uo, path uI and path u2, respectively, in the levels io, jo, ko. We
will show that u0, U1, u2 cannot be distinct. Let i = Lio/2i, j = Ljo/
2j, and k = Lko/2j. Since in the graph G', the nodes appearing in the
same level are all distinct and also the nodes appearing in distifict
paths at levels x and y are distinct whenever, Lx/2j = Ly/2j, we must
have i, j, k all distinct. Let us assume that i < j < k. Let k -j = q
andj - i = p. The vector representations of these nodes in the three
levels in the three paths are given by A), B), and C) below.
A) ssi
5i+I.sm-p-q

B)

s3js+ ...
*.. Sm-q
U2

5m-p
... U1

...

..
di-3 a1o
dp .. dj_s aio
dp+q-i dp+q * '*dk-3 ako

U0

do

..dq-i
C)
Sk+l
where a, = SLx/2J if x is even and ax = dLx/2j if x is odd and is other
than 1. If io = 1, then a1o = uo0.
Since these three vectors represent the same node, all these vectors
are the same. Comparing A) and B), uto = dp -1 and comparing B)
and C), U2 = Smn-q, Since k . m and i < jand q = k - j, m - q

622

IEEE TRANSACTIONS ON

LUevel

Path 0

Path 1

COMPUTERS, VOL. C-36, NO. 5, MAY 1987
Path r-1

...

..

Source node

1

(s

sI......s 0I)
0'
osi"'Sm-2'

(s

I is).*

1 0)
.m.2

(s,ls
(s 1'

5

(s

2m-1

2m

2,

"

Is

1',) 2

'rr-2'
IS

0'

(1,dotdl

(do,d1,...d

(do,dI...d

2 )

(do0d1

..

(r-l,do ,d 1

Idm-2)

I1

*

.

d

d)

m-2 )

(d,d '...,d

2,1)

*...*dm-1

m2, r-ld0

"I

.s .2 **sm-2) r-l,dC

'I

(0,do)dd.*d m-2 )

1, 0)
sm-2'r

(s2..sm-2, r-1,do,s1)

m-2,1 ,dols 1)..

(s2 .5m-2' 1,dod 1)...

ms-_2',do,d1)

1I

I

d, s)

*s

2'"
(s 2, .

d0's1)

. . s
(s,s
(s 2,'sm-2'
'0Od,do)

(sl5s2.,

t-2'0

I

4

(sO,S1, ....,S 2,r-1)

s
1)
1
.a
0' lS,..'"' m-2'

A

Destination node

Fig. 2. A subgraph G' of the topology G.
> i and hence Sm_q will appear in A). Comparing A) and B)

Sm-q =dm-q-(p+q-m- 1) = dp-

I

=

Sm + (a+ I)q- bp or
d(a1+l)q- bp - 1-

UO.

Thus, UO,ul,u2 cannot be all distinct. Hence the lemma.
Lemma 5: If there exist two nodes ca and ,B appearing in G' such
that 'a appears in path u0 and in path uI and ,B appears in path u2 and irt
path u3, then u0, ul, U2, U3 cannot be all distinct.
Proof: Suppose ca appears at the level io of path u0 and at the
level jo of path u1 and (3 appears at the level ko of path u2 and at the
level lo of path U3. Let i = Lio/2i, i = LJo/2i, k = Lko/2i, and I =

d(a+ I)q- bp- I

appears in B) above and Sm +aq- bp =

Claim ii): If for some integers a and b, daq - bp -1 appears in
B) above, then either daq - (b+ l)p-1 appears in B) and daq - bp -1 =
daq- b+1)p- I or d(a+ l)q-bp- I appears in B) and daq-bp- I =
d(a+ l)q-bp- I or Sm+aq-(b+1)p appears in C) and daq-bpl =

Sm+aq-(b+ l)p-

D),

Proof of Claim i): If Sm +aq-bp appears in C) then from C) and

sm +aq-bp- d(a+l)q_bp-1 otherwise.
Llo/21. Without any loss of generality, let us assume that j > i and k
Sm+(a+l)q-bp
> landj - i 2 k - 1. Letp = j - iandq = k - 1. The vector
representations of the nodes in the ioth and joth levels of paths uo and If aq - bp . 1 - q, d(a+ 1)q-bp- i appears in D) and hence (a + l)q
- bp - 1 < q - 1 <p - 1. Thusd(a+l)q-bp-lappearsinB). For
ul, respectively, are given by A) and B) as follows:
the otherwise part, since k > 1, every s appearing in D) appears in C)
A) s, si+l -** Sm-p-i Sm-p Sm-p+1 ... Sm -U
do*o
* di-3 aj0 also and since Sm + (a + 1)q - bp appears in D), it appears in C) too.
B) sjsj+I
... dp-2 dp-1 dp d-_3 ajo
sm-I u1 do
Proof of Claim ii): If daq -bp- 1 appears in B), then from A) and
B),
Obviously these two vectors must be the same. Similarly, the vector
representations of the nodes in the koth and loth levels of paths u2 and

U3, respectively, are given by C) and D) as follows and these two
vectors must be the same.
C) SkSk+I
Sm-q ISm-q Sm-q+1
D) s, SI+ I ... Sm_i U3 do

...

Sm-1 U2

do

**

dk-3ako

if aq-bp,1 -q

f

d

daq-(b+l)p-1 ifaq-bp.p+1

-

If aq - bp 2 p + 1,

Sm + aq - (b +1)p otherwise.

daq-(b+1)p1 appears- in A) and since j >

i,

al0. every d appearing A) appears in B) also. Thus daq(b+ l)p- 1 appears
in B). For the otherwise part, if Sm + aq - (b+ l)p appears in C), we are
(In A), B), C), D), ax is defined the same way as in the proof of done. If not, then m + aq - (b + l)p c I - 1, i.e., aq - bp - I
Lemma 4.) Now, consider two cases.
c I - m + p - 2 < I - 2, asp < m. Thus, daq-bp-I appears in
Case J:-p = q. From A) and B), uo = dp = dq.Iasp = q. But C) also. From C) and D), daq-bp-1 = d(a+l)q-bp-1. Since
from C) and D), dq -1 = U2. Hence u0, u2 are not distinct.
Sm±+ aq -(b+ 1)p does not appear in C), m + aq - (b + l)p < 1, hence,
Case 2: p > q. Before we prove the lemma for p > q, we make (a + l)q - bp - 1 <j - i - 2 <j - 2 (since, k < m). Thus,
the following claims and prove them.
d(a + 1)q - bp- 1 appears in B) and we have shown before that daq - bpClaim. i): If for some integers a and b, Sm + aq - bp appears in C) = d(a+ 1)qbp-,. This completes the proof of Claim ii).
Now we go back to the Case 2 of Lemma 5- Let L = lcm (p, q)
above, then either Sm+(a+l)q-bp appears in C) and sm + aq - bp =

dq-2 dq-i dq
I

*-.

d,

3

623

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 5, MAY 1987

and K = L/p and M= L/q, M > K. From C) and D), U2 = dq_I
and since q < p, d_1 appears in B). Thus applying the results of
Claim i) and ii) repeatedly we will have

either U2 = dq -I

= Sm+ (M-1 )q- Kp

[10]
[11]

or U2=dq-1=dMq-(K-1)p-1.

[12]

U3 and

dMq-(K-1)p-1 = dp-l = UO.
Thus either U2, U3 are nondistinct or u2, u0 are nondistinct. Hence the

[13]

lemma.
On the basis of Lemmas 4 and 5, we can prove that if no more than
r - 2 nodes are faulty, then the tnessage routing delay increases
minimally. This is given by the following theorem.
Theorem 3: If no more than r - 2 nodes are faulty, then at least
one path of G' is not blocked.
Proof: Let S be the set of faulty nodes appearing in G'. LetS =
Sa U Sb, Sa n Sb = ', such that every node in S, appears in two
paths in G' and every node in Sb appears in one path of G'. We are
done if we can prove that the nodes in S, can block at the most Sal +
1 paths of G'. Let n1, n2, * *, nIsal be the nodes in Sa. By Lemma 4,
n1 can block at the tnost two paths of G'. Consider some Sc C S,.
Let us assume that the nodes in Sc can block at the most 152 + 1
paths of G'. Choose any n, E Sa - S,. By Lemma 5, of the two
paths in which ni appears in G', at least one is blocked by the nodes
in S,. Thus the nodes in Sc U {ni} can block at the most IS2 + 2
paths of G' and hence the theorem follows by induction.
In presence of no more than r - 2 faults, the routing scheme can
be implemented by using the graph G' which shows that the delay
increases by maximum two units. The adaptive routing scheme can be
formulated as follows. In absence of any fault use np(s, d) with
sources and destination d. In presence of no more than r - 2 faults,
use the graph G'. In presence of r - 1 faults, use Steps 1) thru 4) as
discussed before.
IV. CONCLUSION
In this correspondence, we have presented a regular digraph
topology for a multiprocessor system which has been proved to be
optimally fault tolerant. The routing delay has been shown to be of
the logarithmic order of the number of processors in the system. In
presence of faults, unless the number of faults is equal to the
maximum number of faults the system can tolerate the routing delay
increases marginally. The undirected version of the topology is a
subgraph of a topology presented in [7].

[14]

But Sm+(M-1)q-Kp

=

Sm-q

=

[15]
[16]
[17]

[18]

[19]
[20]
[21]

[22]
[23]

multibus architecture for multiprocessors," in Proc. 14th Int. Conf.
Fault-Tolerant Comput., 1984, pp. 400-408.
M. Imase, T. Soneoka, and K. Okada, "Connectivity of regular
directed graphs with small diameters," IEEE Trans. Comput., vol.
C-34, pp. 267-273, Mar. 1985.
S. B. Akers, "On the construction of (d, k) graphs," IEEE Trans.
Electron. Comput., vol. EC-15, pp. 253-254, Apr. 1966.
I. Koren, "A reconfigurable and VLSI multiprocessor array," in Proc.
8th Ann. Symp. Comput. Arch., Minneapolis, MN, May 1981.
J. P. Hayes, "A graph model for fault-tolerant computing systems,"
IEEE Trans. Comput., vol. C-25, pp. 876-884, Sept. 1976.
D. K. Pradhan, "Interconnection topologies for fault-tolerant parallel
and distributed architecture," in Proc. 1981 Int. Conf. Parallel
Processing, Aug. 1981.
S. B. Akers and B. Krishnamurthy, "Group graphs as interconnection
networks," in Proc. FTCS-14, Orlando, FL, June 1984.
A. Sengupta, A. Sen, and S. Bandyopadhyay, "An architecture for
fault-tolerant distributed system," in Proc. Comput. Inform. Sci.
Ass. Conf., Montreal, Canada, 1985.
S. M. Reddy, J. G. Kuhl, and S. H. Hosseini, "On digraphs with
minimum diameter and maximum connectivity," in Proc. 20th Ann.
Allerton Conf., Oct. 1982.
H. Lee, "On design of least vulnerable networks of minimum delay,"
Dep. Comput. Sci., Northwestern Univ., Evanston, IL, Tech. Rep.
L. W. Hawkes, "A regular fault-tolerant architecture for interconnection networks," IEEE Trans. Comput., vol. C-34, pp. 677-680, July
1985.
A. H. Esfahanian and S. L. Hakimi, "Fault-tolerant routing in
DeBruijn communication networks," IEEE Trans. Comput., vol. C34, pp. 777-789, Sept. 1985.
D. K. Pradhan, "Corrections to 'Fault-tolerant multiprocessor link and
bus architecture'," IEEE Trans. Comput., vol. C-35, p. 94, Jan.
1986.
H. S. Stone, "Parallel processing with perfect shuffle," IEEE Trans.
Comput., vol. C-20, pp. 153-161, Feb. 1971.
F. T. Leighton, "Layouts for shuffle-exchange graphs and lower bound
techniques for VLSI," Ph.D. dissertation, Massachusetts Inst. Tech-

nol., Cambridge, MA, 1981.[24] C. D. Thompson, "A complexity theory for VLSI," Ph.D. dissertation, Carnegie-Mellon Univ., Pittsburgh, PA, Aug. 1980.

General Criterion for Essential Nonfault
Logical Functions

REFERENCES
[1] J. R. Armstrong and F. G. Gray, "Fault diagnosis in Boolean n-cube
array of microprocessors," IEEE Trans. Comput., vol. C-30, pp.
590-596, Aug. 1981.
[2] J. Kuhl and S. M. Reddy, "Distributed fault tolerance for large
multiprocessor systems," in Proc. 7th Ann. Symp. Comput. Arch.,
May 1980, pp. 23-30.
[3] L. Bhuyan and D. P. Agrawal, "Generalized hypercube and hyberbus
structure for a computer network," IEEE Trans. Comput., vol. C-33,
Apr. 1984.
[4] F. P. Preparata and J. Vuillemin, "The cube connected cycles: A
versatile network for parallel computation," in Proc. 20th Ann. IEEE
Symp. Foundations Comput. Sci., 1979.
[5] E. Horowitz and A. Zorat, "The binary tree as an interconnection
network: Applications to multiprocessor systems and VLSI," IEEE
Trans. Comput., vol. C-30, pp. 247-253, Apr. 1981.
[6] M. L. Schlumberger, "DeBruijn communication networks," Ph.D.
dissertation, Stanford Univ., Stanford, CA, 1974.
[7] D. K. Pradhan, "Fault-tolerant multiprocessor link and bus network
architecture," IEEE Trans. Comput., vol. C-34, pp. 3345, Jan.
1985.
[8] D. K. Pradhan and S. M. Reddy, "A fault-tolerant communication
architecture for distributed systems," IEEE Trans. Comput., vol. C31, Sept. 1982.
[9] D. K. Pradhan, Z. Hanquan, and M. L. Schlumberger, "Fault-tolerant

Locatability of

FRANTISEK KREMLA
Abstract-This correspondence presents a solution of a well-known
essential problem of diagnostics open till now. The method of solution is
based on a straight application of mathematical structures theory 1121,
[131 and on a topological representation of Boolean algebras [41-l71, [141.
The principle idea of this approach is as follows. The distribution of
diagnostic information is controlled in Boolean combinational circuits
(CC) by two relations generally, the first is a congruence relation and the
other is a tolerance relation. This fact is given by a real construction of
CC. The Boolean calculus is evidently a formal interpretation of one of
these relations only. Hence, the Boolean calculus cannot be used to solve
entirely such problems which are related to this distribution. If we want to
disclose the essence, we have to use a more general mathematical
apparatus to interpret both relations mathematically. The outline of
mathematical model of CC is presented and a trace of model on its
underlying set is employed to solve the problem.

Manuscript received November 11, 1985; revised May 12, 1986.
The author is with the Research Institute for Mathematical Machines,
Prague, Czechoslovakia.
IEEE Log Number 8612067.

0018-9340/87/0500-0623$01.00

© 1987 IEEE

Milcom 2016 Track 2 - Networking Protocols and Performance

Budget Constrained Relay Node Placement Problem
for Maximal “Connectedness”
Anisha Mazumder, Chenyang Zhou, Arun Das and Arunabha Sen
School of Computing, Informatics and Decision System Engineering
Arizona State University, Tempe, Arizona 85287
Email: {anisha.mazumder, czhou24, arun.das, asen}@asu.edu

Abstract—The relay node placement problem in the wireless
sensor network have been studied extensively in the last few
years. The goal of most of these problems is to place the fewest
number of relay nodes in the deployment area so that the network
formed by the sensors nodes and the relay nodes is connected.
Most of these studies are conducted for the unconstrained budget
scenario, in the sense that there is an underlying assumption that
no matter however many relay nodes are needed to make the
network connected, they can be procured and deployed. However,
in a fixed budget scenario, the expenses involved in procuring the
minimum number of relay nodes to make the network connected
may exceed the budget. Although in this scenario, one has to
give up the idea of having a network connecting all the sensor
nodes, one would still like to have a network with high level
of “connectedness”. In the paper we introduce two metrics for
measuring “connectedness” of a disconnected graph and study
the problem whose goal is to design a network with maximal
“connectedness”, subject to a fixed budget constraint. We show
that both versions of the problem are NP-complete and provide
heuristics for their solution. We show that the problem is nontrivial even when the number of sensor nodes is as few as three.
We evaluate the performance of heuristics through simulation.

I. I NTRODUCTION
The relay node placement problem, because of its importance in wireless sensor networks, has been studied fairly
extensively in the last few years [1-7]. The study of this
problem is conducted in a scenario where a number of
sensors (nodes) have been placed in a deployment area and
often the objective is to place the fewest number of relay
nodes in the deployment area such that the resulting network
comprising of sensor and relay nodes is connected. As the
deployment of relay nodes involves cost, it may not be
possible to acquire and deploy the number of relay nodes
necessary to make the entire network connected, particularly
when one has to operate under a fixed budget. Although
in this scenario, one has to give up the idea of having a
network connecting all the sensor nodes, one would still like
to have a network with high level of “connectedness”. In
this paper we introduce the notion of “connectedness” for a
disconnected graph and provide two metrics to measure it.
The first metric to measure connectedness of a disconnected
graph is the number of connected components of the graph.
A lower number of connected components in a disconnected
graph is an indicator of a higher degree of connectedness
of the graph. The second metric to measure connectedness
of a disconnected graph is the size of the largest connected

978-1-5090-3781-0/16/$31.00 ©2016 IEEE

component of the graph. A larger size of the largest connected
component in a disconnected graph is an indicator of a higher
degree of connectedness of the graph. In this paper we study
the problem whose goal is to design sensor networks with
relay nodes to maximize “connectedness” subject to a fixed
budget constraint. Although resource constrained version of
relay node placement problems have been studied in literature
[5-7], to the best of our knowledge, the problems investigated
in this paper have not been studied earlier.
The problem scenario studied in this paper is depicted
diagrammatically in Fig 1. By communication range, we refer
to the upper bound on transmission range. Consider a set of
twenty-three sensor nodes (shown as blue circles) deployed
as shown in Fig. 1(a). Since the mathematical abstraction
of the relay node placement problem corresponds to the
Geometric Steiner Tree Problem, and the terms Steiner Points
and terminal points are used in the abstraction, where the
Steiner Points and terminal points correspond to the locations
of the relay and sensor nodes respectively, in this paper we
have used the terms “sensor nodes” and “terminal points”
interchangeably. In Fig. 1(a) there are three clusters – the first
one with ten terminal points, the second one with eight, while
the third with five. The intra-cluster distances are within the
communication range, whereas the inter-cluster ones are not.
Suppose that the maximum inter cluster distance is less than
twice the communication range, and as such only one relay
node is sufficient for connecting any two clusters. If we have
the option of placing two relay nodes (shown as red squares),
then under both metrics of connectedness, the placement of
relay nodes as shown in Fig. 1(b) is an optimal solution.
However, if we have a budget of only one relay node, the
solution shown in Fig. 1(c) is an optimal solution under budget
constraint according to the first metric of connectedness. This
is true as there are exactly two connected components which
is the best that can be achieved with only one relay node.
However, in this solution, the largest connected component
has thirteen nodes and is not optimal as per the second
metric. Fig. 1(d) shows the optimal placement of the relay
node under budget constraint for the second metric, where the
largest connected component has eighteen terminal points. It
may be noted that this placement also results in an optimal
solution under budget constraint according to the first metric.
In this paper, we have reported our findings using these
two metrics. However, we have also considered a unified

Milcom 2016 Track 2 - Networking Protocols and Performance

(a) Deployment of terminal
points

(b) Optimal solution with a
Budget of 2 relay nodes

(c) Optimal solution for objective 1, but not for objective 2

(d) Optimal solution for both
objectives 1 and 2

Fig. 1: Figure showing variation in placing relay nodes for different objectives and budget constraints

metric that measures “connectivity” of a disconnected graph
by combining the two metrics discussed here. Due to lack of
space, we are unable to report our findings using this unified
metric in this paper.
II. P ROBLEM F ORMULATION
As discussed earlier, the goal of this study is to enhance (or
maximize) the “connectedness” of a wireless sensor network
with the deployment of a limited number of relay nodes.
As a first step in this direction, we formalize the notion
of “connectedness” in two different ways, and accordingly,
formalize two separate problems. In both problems, we are
given: (i) the locations of a set of sensor nodes (terminal
points) P = {p1 , p2 , . . . , pn } in the Euclidean plane, (ii) the
communication range R of the sensor and relay nodes, and (iii)
a budget B on the number of relay nodes that can be deployed
in the sensing field. From the set of points P and communication range R, we construct a graph G = (V, E) in the
following way. Corresponding to each point pi ∈ P we create
a node vi ∈ V and two nodes vi and vj have an edge ei,j ∈ E
if the distance between the points pi and pj is at most R. It
may be noted that the graph G = (V, E) so constructed may be
disconnected (i.e., it might comprise of a number of connected
components). The purpose of deploying the relay nodes is to
make the augmented graph, G0 = (V 0 , E 0 ), (comprising of
sensor and relay nodes) connected. Suppose that the B relay
nodes are deployed at points Q = {q1 , q2 , . . . , q|B| }. Corresponding to every point qi ∈ Q there is a node vi ∈ V 0 − V
and there is an edge between vi and a node vj ∈ V 0 if
the distance between the corresponding points qi and pj is
at most R (vj corresponds to pj ). With unlimited budget B,
obviously this goal can be achieved. However, if the budget is
smaller than the minimum number of relay nodes necessary
to make the graph G0 = (V 0 , E 0 ) connected, this goal is
unachievable. However, in this scenario, one would like to
have the graph G0 = (V 0 , E 0 ) with as much connectedness as
possible. This gives rise to the “connectedness” maximization
problem. The goal of creating the graph G0 = (V 0 , E 0 ) with
as much connectedness as possible, can be achieved by (i)
deploying the relay nodes in a fashion that minimizes the
number of connected components of G0 = (V 0 , E 0 ), or (ii)
deploying the relay nodes in a fashion that maximizes the
size of the largest connected components of G0 = (V 0 , E 0 ).

We refer to (i) as Budget Constrained Relay node Placement
with Minimum Number of Connected Components (BCRPMNCC) problem, and (ii) as Budget Constrained Relay node
Placement for Maximizing the Largest Connected Component
(BCRP-MLCC) problem. In BCRP-MNCC, a smaller number
of connected components is an indicator of a higher level
of connectedness of the network. While in BCRP-MLCC, a
larger size of the largest connected component is an indicator
of a higher level of connectedness of the network. We formally
define these two problems as follows:
Budget Constrained Relay node Placement with Minimum
Number of Connected Components (BCRP-MNCC)
Given the locations of n sensor nodes in the Euclidean plane
P = {p1 , p2 , . . . , pn }, positive integers R, C, and a budget B1
on the number of available relay nodes, is it possible to find a
set of Q = {q1 , q2 , . . . , q|B1 | } points in the same plane where
relay nodes can be deployed, so that the number of connected
components in the graph G0 = (V 0 , E 0 ) corresponding to the
point set P and Q is at most C?
Budget Constrained Relay node Placement with Maximum
size of Largest Connected Component (BCRP-MLCC)
Given the locations of n sensor nodes in the Euclidean plane
P = {p1 , p2 , . . . , pn }, positive integer R, C, and a budget
B2 on the number of available relay nodes, is it possible
to find a set of Q = {q1 , q2 , . . . , q|B2 | } points in the same
plane where relay nodes can be deployed, so that the size of
the largest connected component in the graph G0 = (V 0 , E 0 )
corresponding to the point set P and Q is at least C?
The authors in [8] have shown that the Steiner Tree Problem
with Minimum Number of Steiner Points (STP-MSP) is NPcomplete. As STP-MSP problem is a special case of both
BCRP-MNCC and BCRP-MLCC problems, and STP-MSP is
NP-complete, we can conclude that both BCRP-MNCC and
BCRP-MLCC problems are NP-complete. In the following,
we elaborate on this point, starting with the formal statement
of the STP-MSP problem.
Steiner Tree Problem with Minimum Number of Steiner Points
(STP-MSP): Given a set of n terminals points (location of
sensor nodes) X = {p1 , p2 , ..., pn } in the Euclidean plane,
and positive integers R and B3 , is there a tree T spanning a
superset of X such that each edge in the tree has a length of
no more than R and the number C(T ) of points other than

Milcom 2016 Track 2 - Networking Protocols and Performance

those in X, called Steiner points is at most B3 ? [8]
It may be observed that a special case of the BCRP-MNCC
problem where B1 = B3 and C = 1 is equivalent to the STPMSP problem. Similarly, it may be observed that a special
case of the BCRP-MLCC problem where B2 = B3 and C =
n + B2 , is equivalent to the STP-MSP problem. Since both
BCRP-MNCC and BCRP-MLCC problems are generalization
of the STP-MSP problem, we can conclude that both BCRPMNCC and BCRP-MLCC problems are NP-complete.
III. P ROBLEM S OLUTION
The budget unconstrained version of the relay node placement problem is equivalent to the STP-MSP problem discussed
earlier. The authors in [8] have shown that the problem is
NP-complete and provided an approximation algorithm with a
performance bound of 5. A follow-up paper has since reduced
the factor to 3 [9]. The approximation algorithm in [8] follows
a Minimum Spanning Tree (MST) based approach. Although
such an approach provides a constant factor approximation
algorithm for the budget unconstrained version of the relay
node placement problem, such an approach cannot provide
a constant factor approximation algorithm for the budget
constrained version of the problem as shown in the example
of Fig. 2. In the figure there are three adjacent squares where
length of each side is R0 +  and the distance from the
circumcenter of the square to a corner point is R0 .

row and n/2 nodes on the bottom row as shown in Fig. 2 and a
budget of n/2−1, whereas the optimal placement will produce
1 connected component, the MST based approach will produce
n/2+1 components. As the the number of sensor nodes n can
be arbitrarily large, the ratio between the approximate to the
optimal solution of the BCRP-MNCC problem for the MST
based approximation algorithm can also grow arbitrarily large.
We next show in subsection III-A that the computation of
the optimal solution of the BCRP-MLCC problem even when
the number of sensor nodes is as few as three is non-trivial.
And in subsection III-B we provide heuristic solutions for
both the BCRP-MNCC and BCRP-MLCC problems where the
number of sensor nodes can be arbitrarily large.
A. Optimal Solution for a special case of the BCRP-MLCC
When the number of nodes is 2, i.e., n = 2, the BCRPMLCC problem can be solved trivially. Consider a special case
of the BCRP-MLCC problem where n = 3, and the distance
between each of these nodes is more than the transmission
range R. W.l.o.g, assume that transmission range for a relay
node is 1 unit, i.e. R = 1 otherwise we can always divide the
length of each side by R. Then, for any two nodes u, v on the
two-dimensional plane, let Iu,v be the interval formed by u, v
as end points, and let |Iu,v | be the length of the interval, the
subsequent observation and lemmas follow:
Observation 1. If we want to make u communicate with v (in
isolation w.r.t. other nodes), let the minimum number of relay
nodes we need to place be f (u, v), then f (u, v) = d|Iu,v |e−1.
Lemma 1. Let u, v, x, y be four nodes on the two-dimensional
plane, if |Iu,v | ≥ |Ix,y |, then f (u, v) ≥ f (x, y).
Lemma 2. If |Iu,v | is an integer and |Iu,v | − 1 < |Ix,y | ≤
|Iu,v |, then f (u, v) = f (x, y).

Fig. 2: Example to demonstrate that the ratio between the approximate
to optimal can be O(n) for any MST based approximation algorithm
for BCRP-MNCC problem

In the BCRP-MNCC problem, the goal is to minimize
the number of connected components subject to the budget
constraint. If we only consider the square with points 1 through
4 and the budget is 1, the optimal number of connected components will be 1 by placing the relay node at the circumcenter
of the square. However, in this case the MST based approach
will produce 3 connected components as only two of the nodes
(1, 2), (1, 3), (2. 4) or (3, 4) can be connected by a single
relay node, if the location of the relay node is constrained to
be on a line of the MST. Using the same argument, when the
locations of the sensor nodes is points 1 through 6 and the
budget is 2, the the optimal number of connected components
will be 1 whereas the MST based approach will produce 4
connected components. If the locations of the sensor nodes is
points 1 through 8 and the budget is 3, the optimal number
of connected components will be 1 whereas the MST based
approach will produce 5 connected components. In general,
such a placement of sensor nodes with n/2 nodes on the top

Given three nodes A, B, C on the two-dimensional plane,
we want to find the minimum number M of relay nodes
such that A, B, C can communicate with each other. If B2
is at least M , then the optimal solution is 3. Otherwise,
the optimal solution is at most 2 and can be computed
trivially. Here, we assume A, B, C are not on a straight line,
otherwise, the problem can be solved easily by considering
two intervals. Therefore, we consider the setting that A, B, C
forms a triangle. It may be also noted that if the length of
the smallest side of the triangle is at most 1, the problem
becomes trivial as well. W.l.o.g, we say that the side A, B is
shorter than 1, then A, B can communicate with each other
directly. So we only need to consider link A, C or B, C. From
Observation 1 the solution will be min{f (A, C), f (B, C)} =
min{d|IA,C |e−1, d|IB,C |e−1}. Hence, we consider scenarios
where all side lengths are greater than 1. Evidently, in such
a scenario, we need to place at least one relay node and we
should place all relay nodes within the triangle area.
Claim 1. There exists an optimal solution which contains a
relay node D, such that all the other relay nodes are located
on the intervals IA,D and IB,D and IC,D . In other words, the
resulting solution looks like a star as shown in Fig. 3(a).

Milcom 2016 Track 2 - Networking Protocols and Performance

(a) Figure depicting the point D.

(b) Figure with paths P and Q.

(a) Location of point D

(b) Claim 2 Proof Construction

Fig. 3: Constructions for proof of Claim 1

Fig. 4: Scenario 1

Proof. Given any optimal solution, we know the location of
all relay nodes. Since A, B can communicate with each other
(Fig. 3(b)), there must be a path A − B, path P = (A =
v1 , v2 , ..., vn = B) using relay nodes as intermediate vertices.
Similarly, there is an A − C path Q = (A = u1 , u2 , ..., un =
C). It can be noted that there is no other relay node that is
not in P ∪ Q as A, B, C is already connected.
We say D is the common node of P, Q, in addition, D has
the largest index on P . Such a D exists since A = v1 = u1
is a candidate. After obtaining D, we divide P into two subpaths A − D and D − B. Since our objective is to minimize
the number of relay nodes, both of these sub-paths should be
intervals. We consider the same for path Q, and the resulting
shape looks like a star (in some cases, the resulting shape
overlaps two sides of the triangle when D is located at the
same location as one of A or B or C.

then D is on the circumference of CIRA,|IA,D | as well as
CIRB,|IB,D | as shown in Fig. 4(b). Suppose |IC,D | is not an
integer, say |IC,D | = M − , M ∈ N+ . Then we can move D
along circumference of CIRA,|IA,D | a very small distance,

such that ∠BAD0 < α and ∠D0 AD ≤ min{α, |IA,D
| }.
0
0
By triangular
inequality
,
|I
|
<
|I
|
+
|I
|
<
C,D
C,D
D,D
_
|IC,D | + |DD’| =≤ M . According to Lemma 2, f (C, D) =
f (C, D0 ). Next we consider IA,D0 . By the construction of D0 ,
|IA,D | = |IA,D0 | which implies f (A, D) = f (A, D0 ). Finally,
we consider IB,D0 . Since CIRA,|IA,D | intersects CIRB,|IB,D |
at D, D0 must be within CIRB,|IB,D | , hence |IB,D0 | < |IB,D |
and f (A, D0 ) + f (B, D0 ) + f (C, D0 ) < f (A, D) + f (B, D) +
f (C, D). However, based on our choice of D and α, this is
a contradiction. So, such a D0 does not exist and |IC,D | must
be an integer.

For any triangle, w.l.o.g, say (B, C) is the longest side with
length L. Then, it takes at least Θ(dLe) time to compute the
coordinates of all the relay nodes. Next we will present an
algorithm that finds the minimum number of required relay
nodes in O(L2 ) time. The main idea behind the algorithm
is to consider the possible options for the optimal location
of D. We see that once the location of D is fixed, the other
relay nodes can be placed greedily at unit distance apart (since
R = 1) from each other along IA,D , IB,D and IC,D and we can
conclude upon the required minimum number of relay nodes
for this choice of location of D. We categorize the different
options of location of D into three major ‘Scenarios’ which
are further divided into different cases. For each setting, we
compute the optimal location of D and the total number of
relay nodes needed for that choice of D. We finally consider
the location of D which minimizes the total required number
of relay nodes over all categories to obtain the solution
for BCRP-MLCC when n = 3. The three major scenarios
considered are as follows:
Scenario 1: D is located inside the triangle (not on a side).
Scenario 2: D is located on side AC.
Scenario 3: D is located on either side BC or AB.
We describe Scenario 1 in details and omit the descriptions
and analysis of Scenarios 2 and 3 due to lack of space.
Scenario 1: As mentioned earlier, Scenario 1 is when D is
located inside the triangle (not on a side) shown in Fig. 4(a).
Claim 2. In scenario 1, there is an optimal solution such that
|IC,D | is an integer.
Proof. We pick an optimal solution such that α = ∠BAD is
the smallest. Since D is located inside the triangle, α > 0.
Let CIRP,r be the circle whose centre is P with radius R,

Next we show that in Scenario 1 (i) either |IA,D | is an
integer, or (ii) one of ∠ADC and ∠ADB is π2 .

(a) Case (i)

(b) Case (ii)

Fig. 5: Constructions for Case (i) and Case (ii) under Scenario 1

Case (i): |IA,D | is integral: In this case, as presented in
Algorithm 1, we enumerate |IA,D | and |IC,D | (since both are
integers), the intersection point (if there are two intersection
points, we pick the one inside the triangle) of CIRA,|IA,D | and
CIRC,|IC,D | will be the candidate of D (Fig. 5(a)). Among
all candidates, the one that minimizes f (A, D) + f (B, D) +
f (C, D) is the final candidate.
Algorithm 1: Algorithm to compute scenario 1.(i)
1: for i = 0 to bLc do
2:
for j = 0 to bLc do
3:
Compute intersection point, say D, of CIRA,i
and CIRC,j if two circle intersects.
4:
if the intersection point is inside triangle then
5:
Compute f (A, D) + f (B, D) + f (C, D)
using f (A, D) = d|IA,D |e − 1 etc.
6:
end if
7:
end for
8: end for
9: Choose D that minimize f (A, D) + f (B, D) + f (C, D),
call it D1 .

Milcom 2016 Track 2 - Networking Protocols and Performance

Case (ii): |IA,D | is not integral: By the choice of D, |IA,D |
cannot be extended. There could be only two reasons for
this: either ∠ADC = π2 , i.e., AD is a tangent line of circle
CIRC,|IC,D | ; or ∠ADB = π2 , i.e., AD is a tangent line of
circle CIRB,|IB,D | . We omit the proof of this claim due to
space limitations. This gives rise to the following sub-cases:

(a) Sub-Case 1

(b) Sub-Case 2

Fig. 6: Constructions for Scenario 1, Case (ii), Sub-Cases I and II

Sub-Case I: ∠ADC = π2 : As presented in Algorithm 2, we
can enumerate over integer values of |IC,D |. Then we can
compute a tangent AD to CIRC,|IC,D | and get coordinates of
D (Fig. 6(a)). Among all different Ds, choose the one that
minimize minimizes f (A, D) + f (B, D) + f (C, D) as final
candidate.

all terminal points with weights on the edges. The weight of
an edge e connecting nodes vi and vj is equal to the Euclidean
distance between the corresponding terminal points pi and
pj divided by R, where R is the communication range, i.e.,
e. This weight w(e) represents the number
w(e) = d length(e)
R
of relay nodes that will be needed to enable communication
between the sensor nodes at the two ends of this edge. We then
compute an MST on this graph. If the length of an edge of the
MST is at most R, the two sensor nodes connected by this edge
do not need any relay node for communication. However, if
the length of an edge of the MST is greater than R, some relay
nodes will be needed for communication between the sensor
nodes connected by this edge. We place the relay nodes on the
MST edge (i.e., the line connecting the terminal points) and
the number of relay nodes needed to enable communication
between two sensor nodes will be equal to w(e).
If the budget on
P the number of available relay nodes is
sufficient, i.e., if e∈E(T 0 ) w(e) ≤ B1 , the number of connected component is one and we directly output the solution.
Otherwise, we are short of relay nodes and we successively
remove some of the edges of T 0 till such time that the number
of required relay nodes becomes less than or equal to the
budget. It may be noted, removal of one edge from the MST
increases the number of connected components by exactly one.
We follow a greedy approach for edge removal sequence in
that at every stage of the removal process, we remove the
highest weighted edge, breaking ties arbitrarily (Algorithm 4).

Algorithm 2: Algorithm to compute scenario 1.(ii).I
1: for i = 0 to bLc do
2:
Compute tangent line AD to circle CIRC,i .
3:
if the intersection point is inside triangle then
4:
Compute f (A, D) + f (B, D) + f (C, D).
5:
end if
6: end for
7: Choose D that minimize f (A, D) + f (B, D) + f (C, D), Algorithm 4: Heuristic for BCRP-MNCC problem
call it D2 .
1: Create an MST T 0 on the set of given terminal points P .
2: Assign each edge e of T 0 a weight of


w(e) =P length(e)
−1
R
Sub-Case II: ∠ADB = π2 : Let E be the mid point of AB,
3: while
w(e)
> B1 do
0
e∈E(T )
then by knowledge of geometry, D lies on the circumference of
4:
Remove the edge that has the maximum weight
CIRE, |AB| . As presented in Algorithm 3, again we enumerate
from T 0 ; breaking ties arbitrarily
2
over integral values of IC,D and compute intersection point of
5: end while
CIRE, |AB| and CIRC,|IC,D | (Fig. 6(b)).
6: Return the resulting forest obtained from T 0
2
Algorithm 3: Algorithm to compute scenario 1.(ii).II
C. Heuristic Solution for BCRP-MLCC with Arbitrary Num1: for i = 0 to bLc do
ber of Sensor Nodes
2:
Compute intersection point of CIRC,i and
CIRE, |AB| where E is mid point of side AB.
Our heuristic for the BCRP-MLCC is based on the k-MST
2
problem, where one is given an undirected graph G with non3:
if the intersection point is inside triangle then
negative costs c(e) for the edges e ∈ E(G) and an integer
4:
Compute f (A, D) + f (B, D) + f (C, D).
k, and the problem is to find the minimum-cost tree in G
5:
end if
that spans at least k vertices. Computation of k-MST is a
6: end for
7: Choose D that minimize f (A, D) + f (B, D) + f (C, D), well studied problem. [10] and [11] present 1 +  approximate
solutions for the k-MST problem. Our heuristic (Algorithm
call it D3 .
5) for the BCRP-MLCC computes k-MST with decreasing
value of k starting with k = n, where n is the number of
B. Heuristic Solution for BCRP-MNCC with Arbitrary Num- terminal nodes. Once the k-MST is computed, our algorithm
ber of Sensor Nodes
computes the minimum number of relay nodes that will be
Our heuristic solution for the BCRP-MNCC problem is necessary to make the k-MST connected by using the same
based on a Minimum Spanning Tree (MST) on the terminal technique as in the BCRP-MLCC problem. If this number
points (sensor nodes). First we construct a complete graph on does not exceed the budget, our procedure stops and outputs

Milcom 2016 Track 2 - Networking Protocols and Performance

the nodes of the k-MST as the largest connected component.
Otherwise it computes k-MST once again with the value of k
decremented by one and then checks if the number of relay
nodes needed is within the specified budget.
Algorithm 5: Heuristic for solving BCRP-MLCC problem
1:
2:
3:
4:
5:
6:
7:
8:

for k ∈ n to 2 do
Create an approximate k-MST T 0 on the set of
given target points P
To each edge e ofT 0 , assign the weight
length(e)
−1
w(e)
R
P=
if e∈E(T 0 ) w(e) ≤ B2 then
Return T’ as the solution of BCRP-MLCC
end if
end for
Return any arbitrary terminal point as solution

IV. E XPERIMENTAL R ESULTS
In this section, we present the results of our experimental
evaluations of Algorithms 4 and 5. To compute the MST for
BCRP-MNCC, we use Prim’s algorithm and for k-MST for
BCRP-MLCC, we use algorithm presented in [12]. In order to
evaluate the performance of the heuristics for BCRP-MNCC
and BCRP-MLCC problems presented in Algorithms 4 and
5, we need to know both the approximate and the optimal
solution for the problem instances. Whereas, approximate
(heuristic) solution to the problem instances can be obtained by
running Algorithms 4 and 5, optimal solution to the problem
instances is not obvious using Integer Linear Programming
(which is often used in similar problems scenarios) as the
placement of a relay node can be at any point in the deployment area and the number of such points are infinite. To
overcome this constraint, we created data sets by placing the
sensor nodes at specific locations in the deployment area so
that we can compute the optimal solution for a specified budget
easily. We manually created 33 datasets, each with 20 sensor
nodes and a fixed communication range, varying the (i) the
sensor node deployment pattern and the (ii) relay node budget,
in a way that we know the optimal solution for these problem
instances. The ratio between the heuristic to optimal solution
for the BCRP-MNCC and BCRP-MLCC problems are shown
in Fig. 7. On the X-axis of Fig. 7 we have data sets 1 through
33 and on the Y-axis have the ratio of the heuristic to the
optimal solution for problem instance (i.e., a specific data set).
It may be observed that the ratio between heuristic to optimal
was never lower than 0.5 for the BCRP-MLCC problem but
for the BCRP-MNCC problem this rato was as large as 11. As
we have observed earlier in section III, an MST based solution
to the BCRP-MNCC problem can perform poorly, if the sensor
nodes have some specific (bad) deployment pattern. The poor
performance of the BCRP-MNCC heuristic for some problem
instances can be explained by this observation.

(a) Plot of experimental results for (b) Plot of experimental results for
BCRP-MNCC problem
BCRP-MLCC problem

Fig. 7: Experimental results plotting the ratio of the heuristic to the
optimal solutions for different datasets for the BCRP-MNCC and the
BCRP-MLCC problems.

V. C ONCLUSION
In this paper, we have studied the relay node placement
problem under budget constraint using two different metrics.
We prove that the problems using both the metrics are NPcomplete and provide heuristic solutions for them. We report
our experimental results on synthetic data sets. This study has
led us to the development of the notion of “connectivity of
a disconnected graph” or “disconnectivity”, which we believe
opens up a new area of research.
R EFERENCES
[1] E. L. Lloyd and G. Xue, “Relay node placement in wireless sensor
networks,” Computers, IEEE Transactions on, vol. 56, no. 1, pp. 134–
138, 2007.
[2] S. Khuller and B. Raghavachari, “Improved approximation algorithms
for uniform connectivity problems,” in Proceedings of the twenty-seventh
annual ACM symposium on Theory of computing. ACM, 1995, pp. 1–
10.
[3] W. Zhang, G. Xue, and S. Misra, “Fault-tolerant relay node placement
in wireless sensor networks: Problems and algorithms,” in INFOCOM
2007. 26th IEEE International Conference on Computer Communications. IEEE. IEEE, 2007, pp. 1649–1657.
[4] X. Han, X. Cao, E. L. Lloyd, and C.-C. Shen, “Fault-tolerant relay
node placement in heterogeneous wireless sensor networks,” Mobile
Computing, IEEE Transactions on, vol. 9, no. 5, pp. 643–656, 2010.
[5] S. Misra, S. D. Hong, G. Xue, and J. Tang, “Constrained relay node
placement in wireless sensor networks: Formulation and approximations,” IEEE/ACM Transactions on Networking (TON), vol. 18, no. 2,
pp. 434–447, 2010.
[6] J.-Y. Chang and Y.-W. Chen, “A cluster-based relay station deployment
scheme for multi-hop relay networks,” Communications and Networks,
Journal of, vol. 17, no. 1, pp. 84–92, 2015.
[7] P. Li, C. Huang, and Q. Liu, “Bcdp: Budget constrained and delaybounded placement for hybrid roadside units in vehicular ad hoc
networks,” Sensors, vol. 14, no. 12, pp. 22 564–22 594, 2014.
[8] G.-H. Lin and G. Xue, “Steiner tree problem with minimum number of
steiner points and bounded edge-length,” Information Processing Letters,
vol. 69, no. 2, pp. 53–57, 1999.
[9] D. Chen, D.-Z. Du, X.-D. Hu, G.-H. Lin, L. Wang, and G. Xue,
“Approximations for steiner trees with minimum number of steiner
points,” Journal of Global Optimization, vol. 18, no. 1, pp. 17–33, 2000.
[10] J. S. Mitchell, “Guillotine subdivisions approximate polygonal subdivisions: A simple polynomial-time approximation scheme for geometric
tsp, k-mst, and related problems,” SIAM Journal on Computing, vol. 28,
no. 4, pp. 1298–1309, 1999.
[11] S. Arora, “Polynomial time approximation schemes for euclidean tsp and
other geometric problems,” in Foundations of Computer Science, 1996.
Proceedings., 37th Annual Symposium on. IEEE, 1996, pp. 2–11.
[12] F. A. Chudak, T. Roughgarden, and D. P. Williamson, “Approximate
k-msts and k-steiner trees via the primal-dual method and lagrangean
relaxation,” Mathematical Programming, vol. 100, no. 2, pp. 411–421,
2004.

2010 IEEE Annual Symposium on VLSI

An Analytical Framework with Bounded Deflection
Adaptive Routing for Networks-on-Chip
Pavel Ghosh1 , Arvind Ravi2 , and Arunabha Sen1
1

Computer Science Program; Computing, Informatics and Decision Systems Engineering; Arizona State
University, Tempe, AZ, 85281, {pavel.ghosh, asen}@asu.edu
2
Electrical Engineering Department; School of Electrical, Computer and Energy Engineering; Arizona State
University, Tempe, AZ, 85287, aravi2@asu.edu

the studies involving the design of NoC based systems have
been based on extensive simulations using synthetic traffic
and traffic traces of real applications. These kinds of studies,
although useful for the comparison of the design outputs,
provide little insight into the effect of individual components.
Moreover, simulations are very slow for large systems. It
is, therefore, desired to have an analytical model to study
the system components, which will facilitate the selection of
design parameters. In this paper, we concentrate on the routing
problem as it itself involves several design considerations and
have significant impact on the overall power consumption,
end-to-end delay, traffic congestion and creation of hotspots.
Wormhole switching is used as the on-chip packet switching
technique as the available alternatives store-and-forward and
virtual cut-through pose significant buffering and delay overheads. Routing can be classified in several different ways, as
static vs. dynamic, distributed vs. source routing and minimal
vs. non-minimal [2], etc. One important requirement in the
design of the routing algorithm is its freedom from deadlock.
Different strategies can be used to avoid deadlock, such as, use
of virtual channels, over-allocation of buffers, turn-prohibition
models, etc. Whereas, the use virtual channels and oversized
buffers have inherent disadvantages in terms of chip area, complex arbitration mechanism and power consumption, the turnprohibition models have the tradeoff of reduced adaptivity. In
this paper, we develop an adaptive routing algorithm, which
allows non-minimal paths using detours away from congested
links. One drawback of non-minimal routing is the potential
livelocks, which is avoided in our routing algorithm by using
a specified upper bound on the number of deflections taken by
any packet. Our proposed routing algorithm can be classified
as dynamic, distributed and non-minimal, which does not use
virtual channels and is free from deadlocks and livelocks.
Traffic analysis of the targeted applications and the knowledge based on the analysis of the system components can
also help the buffer allocations of the routers and the design
of routing algorithms. We used two different models for the
analytical study of NoC components. While a stochastic analytical model can be leveraged to gain knowledge of average
delay and average buffer requirements, more deterministic
studies can be performed to compute the worst case delay and
buffer requirements. Depending on the traffic characteristics,
we use a stochastic analytical model [4], where the traffic

Abstract—In a Multi-Processor System-on-Chip (MPSoC)based embedded system with Network-on-chip (NoC) as the
communication architecture, routing of the communication traffic
among the Processing Elements (PEs) contributes significantly to
the overall latency, throughput and energy consumption. Design
of an efficient routing algorithm for NoC requires a thorough
understanding of the role of individual components of NoC.
Simulation based studies are time-consuming and do not provide
adequate insight into the design parameters for performance
improvement. In this paper, we provide a framework for the
analytical study of the NoC components and design an adaptive
routing algorithm. Based on the traffic pattern of the communication traffic among PEs, we perform analytical studies based
on network calculus and probabilistic analysis. Analytical study
relates the design parameters with the worst case and average
case latency and buffer requirements. Knowledge obtained from
the analytical study is utilized for resource allocation of NoC,
which further constitutes the design philosophy of the proposed
Bounded Deflection Adaptive Routing (BDAR) algorithm. Our
routing algorithm is deadlock-livelock free and efficiently reacts
to link congestions. Experimental results based on simulations
show that our routing algorithm performs significantly better
than some existing static and dynamic routing in terms of link
utilization, average and maximum end-to-end latency.

I. I NTRODUCTION
In order to cope with the growing complexity of consumer
embedded products and the emerging communication and
multimedia standards, future MPSoCs are predicted to have
hundreds of PEs on a single chip. With the increasing amount
of communication traffic among the PEs, on-chip communication architectures based on shared bus or point-to-point
links are no longer adequate. A paradigm shift in the on-chip
communication architecture was necessary and NoC emerged
as one of the most promising alternatives [1], [2], [3].
Each PE in an NoC is connected to a router through a
network interface. The routers are connected among themselves using on-chip links according to the communication
topology. Communication traffic between PEs is routed over
the NoC links. Satisfying performance requirements in terms
of delay, throughput and minimizing the power/energy consumption constitutes a few of the most challenging issues of
system design. In order to improve system performance, it is
necessary to understand the impact of each design decision
and individual NoC components, namely, topology selection,
routing strategies, buffer allocation in the routers, etc. Most of
978-0-7695-4076-4/10 $26.00 © 2010 IEEE
DOI 10.1109/ISVLSI.2010.90

363

arrival process is poisson, and an analytical model based on
network calculus [5], [6] where the traffic follows a bursty
nature. The knowledge gained from these models is used for
the resource allocation which constitutes the design philosophy
of our routing algorithm. The contribution made in this paper
can be listed as follows:
• We provide a framework for the analytical study of the
NoC components and their impact on the worst case and
average case delay and buffer requirements.
• Knowledge based on the analytical models is utilized in
allocating resources (e.g., buffer, etc.).
• We develop an adaptive routing algorithm which reacts
to the resource allocations and avoids congested paths.
• Extensive simulations are used to study the impact of our
routing algorithm on delay, congestion and is compared
with standard static and dynamic routing algorithms.

Fig. 1.
•

II. R ELATED W ORK
In the recent past, there has been a lot of focus on developing deadlock-free adaptive routing algorithms in the NoC
domain. To ensure freedom from deadlock, virtual channels
(VCs) can be introduced into the network with an overhead
introduced in terms of buffers, control logic and arbitration.
Consequently, deadlock avoidance based on the turn model [7]
and direction restriction model [8] gained popularity. The OddEven turn based adaptive routing algorithm proposed in [9]
overcame the shortcomings of previous turn based algorithms
by providing more even routing adaptiveness under nonuniform traffic. The authors in [10] presented DyAD, a routing
concept based on minimum odd-even routing that selects
between deterministic routing and adaptive routing based on
the network load. In [11], the authors present a methodology useful in designing topology agnostic and deadlockfree adaptive routing algorithms for NoC architectures. Such
algorithms, called Application Specific Routing Algorithms
(APSRA) are developed with the goal of maximizing adaptivity given a mapped and scheduled application. In [12], an
adaptive stochastic routing (ASR) is proposed based on the
NoRC platform [13], wherein a self learning mechanism is
implemented in each router.
More recently, research efforts have been spent into the
development of analytical models for NoC. Two different
methodologies exist, both assuming a deterministic routing.
Models like the one in [4] are based on stochastic and
probabilistic approaches and aid in average case NoC performance analysis. Models based on network calculus utilize
application-specific traffic information in establishing worst
case performance bounds [6]. We utilize these concepts in our
proposed framework for resource allocation and the design of
our adaptive routing algorithm.

•

•

•
•

•
•
•

Solution Framework

Communication Trace Graph (CTG) GP (VP , EP ), where
each vertex ui ∈ VP represents a processing element
(PE), and a directed edge tij = (ui , uj ) ∈ EP represents
communication flow fk between PEs ui and uj .
Undirected NoC topology graph GR (VR , ER ), where
each vertex vi ∈ VR represents a router of the NoC
architecture, and undirected edge eij = (vi , vj ) ∈ ER
represents a communication link of the network.
Given the dimension of the mesh dimX × dimY , each
node vi ∈ VR of the NoC topology graph is represented
with coordinates (xi , yi ), where 0 ≤ xi ≤ dimX − 1 and
0 ≤ yi ≤ dimY − 1.
The mapping function M : VP → VR , which maps the
PEs onto the routers of the NoC architecture.
For tij = (ui , uj ) ∈ EP , M (ui ) ∈ VR is defined as
the source sk of flow fk , and similarly M (uj ) ∈ VR
is defined as the destination dk of the flow fk . The
coordinates of source sk and destination dk are denoted
by (sxk , syk ) and (dxk , dyk ), respectively.
A traffic injection rate rk associated with each communication flow fk , specified in the CTG.
Uniform link bandwidth bw of the NoC links.
Buffer size B associated with each router of NoC.

Find routing of all the communication flows over the links
of the NoC. This can be formally defined as a mapping
Route that maps flow fk as a series of NoC routers and
links, i.e., Route(fk ) = {vk1 , ek1 , vk2 , . . . , vkl−1 , ekl−1 , vkl },
where sk = vk1 , dk = vkl and ekp = (vkp , vkp+1 ) ∈ ER for
1 ≤ p ≤ l − 1.
The objective is to minimize average delay λ of the network
such that link bandwidth constraints are satisfied. The routing
algorithm should be deadlock-free, livelock-free and should be
able to avoid congested links. This will in turn enhance the
system performance and avoid creating hotspots.
The flowchart of our solution framework is shown in Fig. 1.
Both the analytical models, based on stochastic analysis and
network calculus, assumes a deterministic routing, such as,
XY -routing, for the communication flows. Worst case or
average case delay and buffer requirements are obtained from
the analytical models. The NoC resource allocations (such as,
router buffers) are performed based on this knowledge. Our

III. P ROBLEM F ORMULATION AND S OLUTION
F RAMEWORK
In this section, we provide a formal definition of the
NoC routing problem, and give an overview of the solution
framework proposed in this paper. Given:
364

routing algorithm reacts to these resource allocations, and decides the routing paths which tries to avoid the congested links,
and buffers. Finally using simulations, we get an estimate of
the latency for our algorithm.

where Λ, C and R are the matrices of the arrival rates,
the contention matrix and the residual matrix, respectively.
Equation (5) can be used to compute the buffer utilization of
each router. For a single queue model (P = 1),

IV. A NALYTICAL M ODELS

N=

Most of the research in the literature uses probabilistic
methods to design analytical models for NoC [4] to estimate
the average end-to-end delay, average buffer requirements, etc.
Deterministic approach to find analytical model for NoC using
network calculus has been used [6] to determine the worst case
end-to-end delay, worst-case buffer requirements, etc. For the
first model, the traffic arrival is considered to be a poisson
process, whereas for analysis based on network calculus, it
is required that the traffic satisfies burstiness constraint [14].
In this section, based on the traffic pattern (poisson arrival,
bursty traffic with predicted arrival rate, etc.) we discuss both
avenues for the analytical study of NoC.

0

cij =

P
X

cjk Nk



P
X

pim pjm

(7)

m=1

Performance Analysis: We can utilize this model to compute
average buffer utilization in the router and average packet
latency. The arrival rate φij at port j of router i is given by:
X
φij =
rk D(f, i, j)
(8)
∀k∈F

where D(f, i, j) is a decision variable which is 1 if flow f
is routed through port j of router i and 0, otherwise. The
blocking probability at port j, pb (φj , T, buf ) is found using
an M/G/1 queuing model, following which the forwarding
probabilities are computed. If we have the arrival rates φij ,
contention matrix C and the service time T , we can compute
N , the average number of packets in every buffer using (5).
The average buffer size Bavg is the mean of all the computed
buffer utilization values. Applying Little’s Theorem, average
waiting time Wij of a packet at port j of router i is:

(2)

Wij = Nij /φij

where τj is the average time an incoming packet spends in the
queue j. It is composed off the residual service time seen by
an incoming packet, the service times of the packets waiting
in the same buffer and the packets queued in the other buffers
of the same router that are served before the current packet.
Thus, the value of τj can be substituted into (2) to obtain:
Nj = φj R + T Nj + T

(6)

where k is the current router and k is the neighboring
router connected to port j. The contention probability can be
calculated from equation (6) as:

Analytical model of a single NoC router (hence, subscript i
is dropped) is built by considering it as a set of buffers, each
connected to one of the router ports j ∈ P . For port j,
Nj = φj τj

1 2
φT
2

φki
φ 0
pij = PP
∗ PP k i
0
i=1 φki
i=1 φk i

In this subsection, we use a stochastic model similar to
the one provided in [4] for probabilistic analysis of the
NoC components, and compute the average delay and buffer
requirements for a given set of communication flows routed by
a specific deterministic routing algorithm and using wormhole
switching. We use P as the number of router ports, S as a
random variable denoting packet size, Hs as function of time
taken by a packet to traverse the router tr and traverse a link
tl , φij as traffic arrival rate of the header flits to the input
port j of router i, and Nij as the average no. of packets in
input buffer at port j of router i. Service time of a packet T
(excluding queuing delay) can be defined as (where, bw = link
bandwidth):
T = Hs + dS/bwe
(1)

i.e.,

where R =

We need to define and compute the forwarding probabilities
in order to compute the contention matrix. Let us define pij
as the probability that a packet arrives at port i and leaves the
router through port j. Mathematically, it is the product of the
Probability that a packet arrives at port i and the Probability
that a packet leaves through port j. Thus,

A. Average Case Analysis based on Stochastic Model

φj = Nj /τj

φR
,
1 − Tφ

(9)

Eq. 9 provides us with the average packet latency at each
router. Using this value, we canQobtain the average latency
experienced by every flow f . Let f be the set of routers and
their corresponding buffers the flow f traverses. The average
latency experienced by the packets of f can be expressed as:
X
L f = Ws +
(Wij + T )
(10)

(3)

(i,j)∈

k=1,k6=j

Q

f

where Ws is the queuing delay experienced at the source as
computed using the M/G/1/m model, applicable as the buffers
are finite. Thus the overall packet latency or the average delay
Davg can be obtained using the latency computed in (10).
P
∀f rf Lf
Davg = P
(11)
∀f rf

In (3), cjk is the contention probability between ports j and k
contending for the same output port. This can be expressed as
a row vector Cj = [cj1 cj2 . . . cjP ] where cjj = 1. Therefore:

Nj = φj R + T Cj N
(4)
This is the equilibrium condition for the buffer at input port j.
If we extend the above model to the entire router, (4) becomes
−1
N = T CΛN + ΛR =⇒ N = 1 − T ΛC
ΛR
(5)

This stochastic model enables us to analyze the network under
deterministic routing approaches and compute Davg and Bavg .
365

Therefore, the worst-case delay Dmax can be computed as
taking the maximum over all the flows. Similarly, the worst
case buffer requirement Bmax can be computed as taking the
maximum over all the routers.

B. Worst Case Analysis based on Network Calculus (NC)
NC is a mathematical modeling technique that was developed [14] to comprehend network elements in isolation
and their inter-relations in determining the system performance. It was mainly used for studying QoS services of
internet, network processors used for embedded networking
applications, etc. In [6], NC has been used as a means of
developing analytical model of NoC. The underlying theory
of NC is based on (min, +) algebra. In (min, +) algebra,
given two wide sense increasing functions f and g with
f (0) = g(0) = 0, their convolution is defined as (f 
g)(t) = inf0≤s≤t {f (t − s) + g(s)}, and their deconvolution
as (f  g)(t) = sups≥0 {f (t + s) − g(s)}. The input flow
received by a component is defined by function R(t), known
as the input function, which represents the cumulative data
units arriving at the component in time interval [0, t]. Similarly,
the output function is represented as the cumulative function
R∗ (t). For lossless systems, as is the case for NoC, the backlog
x(t) and virtual delay d(t) at time t are given by:
x(t)

= R(t) − R∗ (t)

d(t)

= inf {τ ≥ 0 : R(t) ≤ R∗ (t + τ )}

V. B OUNDED D EFLECTION A DAPTIVE ROUTING (BDAR)
In this section, we describe our proposed routing algorithm
for NoC. Before going into the details of our design, we give
an overview of NoC routing algorithms. Although described
here for regular 2D mesh topology, the proposed strategies can
be readily applicable for any other NoC topologies as well.
In the rest of the section, we use the terms node and router
interchangeably. Routing algorithms, specifically for NoC, can
be classified in three ways [2]:
• static vs. dynamic - based on whether the sourcedestination paths are fixed or can change over time.
• distributed vs. source routing - based on the node at which
the routing decisions are taken.
• minimal vs. non-minimal - based on whether the sourcedestination path is guaranteed to be minimal or not.
Whereas, static routing is advantageous for predicted traffic
patterns, dynamic routing is more useful for more general
cases. Although source routing can benefit from the minimal
routing logic required at the intermediate routers, distributed
routing can more efficiently react to the dynamic nature of the
network state. Minimal routing is always desirable, but in case
of heavy traffic loads, the strict restriction of minimal paths
may overlook uncongested non-minimal paths.
Each packet of a flow consists of a number of flow control
units (called flit). Flits are generally classified as header, body
and tail. It needs to be mentioned here that routing decision
is made only for the header flit of a packet. The rest of the
flits in the packet follow the same path taken by the header
towards the destination. This is typically implemented in the
router logic as a reservation table, which reserves the output
port for the packet to which its header flit is forwarded to, and
is released only after the tail flit is forwarded.
Typical performance metrics for routing algorithms are delay, throughput, adaptivity, power consumption, etc. Moreover,
they are expected to adapt to the dynamic network conditions
and thus avoid congestions and hotspots. Routing algorithms
should be deadlock (when resources, such as, buffers, wait
for each other to be free in a cyclic dependent manner) and
livelock (when packets are routed continuously never reaching
the destination) free. Assuming the x-axis and y-axis running
from west to east, and from north to south, respectively, let us
define ∆xi = dxi − sxi and ∆yi = dyi − syi . The following
turn conditions are used to prevent livelocks and unbounded
non-minimal paths: ∆x > 0 → No West, ∆x < 0 → No East,
∆y > 0 → No North. ∆y < 0 → No South.
Our BDAR algorithm, details of which follow, can be
classified as a dynamic, distributed and non-minimal routing
algorithm, which is deadlock and livelock-free.
Algorithm 1 returns the direction dir for next hop forwarding for a header flit. It first calculates the distance hi
between the source and destination of flow fi (line 1). Flows
with higher traffic injection rate λi are desired to have shorter

The arrival curve of the component is defined as a wide-sense
increasing function α defined for t ≥ 0. The input flow R
is said to be constrained by α if and only if R(t) − R(s) ≤
α(t−s) for all s ≤ t. It is said that R has α as an arrival curve,
or R is α-smooth. The service curve β for the component
is defined as a non-decreasing function such that β(0) = 0
and ∀t ≥ 0, R∗ (t) ≥ inf0≤s≤t {R(s) + β(t − s)}, or using
(min, +) algebra R∗ ≥ Rβ. Similar to the relation between
the arrival curve α and the input flow R, the output flow R∗
can be constrained by output curve α∗ (equivalently, R∗ is
α∗ -smooth), where α∗ (t) = (α  β)(t) [5]. A typical example
of α and β can be given by α(t) = b + rt (affine function)
and β(t) = R(t − T )+ (rate-latency function), where r is the
rate, b the burst size of the input traffic, R ≥ r the guaranteed
service rate of the component and T the maximum delay in
service. Using this example, the bounds on backlog and delay
can be given by, B = b + rT and D = Rb + T , respectively.
Now, let us consider an NoC with 2D mesh topology. Let
us assume that the source and destination of all the flows of
the NoC and the routing paths are already known. Considering
router i, let us denote its north, south, east, west -neighbors
as N (i), S(i), E(i) and W (i), respectively. Each router is
attached with a PE. Considering the traffic injection rate of
the PE as λi , the arrival curve of the router i can computed
as the sum of the local traffic λi and the output traffic from
its neighbors, given by:
∗
∗
∗
∗
αi (t) = λi (t)+αN
(i) (t)+αS(i) (t)+αE(i) (t)+αW (i) (t) (12)

Knowing the service curve βi of this router, the output curve
can computed as:
αi∗ (t) = αi (t)  βi (t)

(13)

Using (12) and (13), we can compute B and D for each
component. Overall delay for each flow can be computed
as the sum of the delays in the components along its path.
366

Algorithm 1 Bounded Deflection Adaptive Routing (BDAR)

P1

Input: n flows, their traffic injection rates λ1 , λ2 , . . . , λn , the corresponding
source-destination (s-d) pairs, (sxi , syi ), (dxi , dyi ), for 1 ≤ i ≤ n,
threshold parameter η, the mesh dimensions dimx and dimy, upper
bound on number of detours ud = 2
Output: Next hop for the traffic of flow fi , returned as the direction dir ∈
{N, S, E, W }
1: Calculate the distance hi (in number of hops) for the source-destination
pair of flow fi , given by hi =| sxi − dxi | + | syi − dyi |
2: if (λi hi ≥ η) then
3:
Add directions only along the minimal paths from current node.
4:
Select direction dir such that, Ndir (i) (neighbor of i in direction dir)
has not been visited before, and buf Occ[dir] is minimum among the
pushed directions.
5: else
6:
if (detourCount ≥ ud ) then
7:
Add directions only along the minimal paths from current node.
8:
else
9:
Add all directions that satisfies turn conditions.
10:
end if
11:
Select direction dir such that, Ndir (i) has not been visited before,
and buf Occ[dir] is minimum among the pushed directions.
12:
if (dir is non-minimal) then
13:
Increment detourCount
14:
end if
15: end if
16: return direction dir, which identifies the next hop

s

•

P6

d
P5

P7

P3

Fig. 2.

Paths with detourCount ≤ 2 for a specific s − d pair

TABLE I
C OMPARISON OF A DAPTIVITY BETWEEN XY, O DD E VEN AND BDAR

XY
OddEven
BDAR

P1

P2

P3

P4

P5

P6

P7

Adaptivity

3
3
3

7
3
3

7
3
3

7
7
3

7
7
3

7
7
3

7
7
3

1
3
7

can never indefinitely travel away from the destination.
Deadlock: Although we do not provide a formal proof of
deadlock avoidance, but in practice, it is highly unlikely
to occur for BDAR for the following reasons. Firstly,
deadlock can happen only when buffers become full.
BDAR always tries to avoid the highly occupied buffer
spaces. Secondly, our buffer allocation is performed based
on the feedback from analytical models based on XYrouting. The buffer utilization for XY-routing is far from
being uniform, and thus for BDAR, which utilizes buffers
in a much more uniform fashion, it is highly unlikely to
have the buffers full. This is also supported by experimental results.
Features of BDAR include:
• Low latency: This is achieved by avoiding congested links
using buffer availability information. Although, it may
introduce non-minimal paths and thus introducing longer
hop delays, but this is done for avoiding longer queueing
delays in the buffers.
• High throughput: BDAR achieves higher throughput by
using inherent prioritization of the flows, and then making
the decision of minimal and non-minimal paths based on
the network state, i.e., buffer free space.
• High Adaptivity: Adaptivity can be defined as the number
of distinct paths allowed by a routing algorithm for a
specific source-destination pair. Without having any strict
restriction of guaranteeing minimal paths, or prohibiting
specific turns, as followed in the turn prohibition models,
it allows more number of potential paths for a specific
source-destination pair. For the simple example shown in
Fig. 2, the permissible paths and adaptivity values are
shown in Table I for different routing algorithms.
•

routing path since, higher traffic over longer paths not only
creates potential congestions, but also contributes to higher
power consumption. Also, flows for which hi is higher (higher
source-destination hop counts), are desired to have minimal
routing paths in order to prevent them from taking even longer
routes contributing to higher delay. Both of these aspects
are taken care of by comparing the product λi hi with a
specified threshold value η (line 2), and thus relative priorities
are assigned to flows. Routing is performed in two steps
(Fig. 1): first the potential next directions are added into a
data structure, and secondly, the most suitable of them is
selected. For flows with higher priority (λi hi ≥ η), only
directions on the minimal paths are added (line 3). The header
flit maintains the record of already visited nodes along the
routed path so far in order to prevent it to see the same node
again and again. The direction dir, which is not yet visited
and whose occupied buffer space is minimum is selected for
forwarding (line 4). The lower priority flows (λi hi < η) are
handled in lines 5 − 15. Number of detours (directions away
from destination) detourCount is compared with maximum
allowed (ud ), and accordingly only minimal or all directions
satisfying the turn conditions are added (line 7 and 9). In
line 11, the next hop direction is selected in similar fashion.
In case of non-minimal direction selected, detourCount is
incremented (line 13). Selected direction dir for forwarding
the packet is returned in line 16.
The undesirable scenarios of congestion, livelock and deadlock are avoided by BDAR algorithm in the following fashion:
•

P4

P2

VI. S IMULATION R ESULTS
In this section we present the simulation results to illustrate
the effectiveness of the BDAR algorithm. We compare its
performance against two standard routing algorithms, the
minimal static XY routing algorithm [2] and the OddEven
adaptive routing algorithm [9]. The simulations are performed
using a cycle-accurate simulator (Noxim)[15] wherein we have
implemented the BDAR algorithm. The feedback obtained
from the analytical models enabled us to set the sizes of the
buffers attached to the router ports.

Congestion: BDAR avoids congestion in two ways.
Firstly, it always selects the direction from the permissible
ones which has the maximum buffer space free. Secondly,
having an inherent prioritization of flows, it decides
whether or not to take a detour.
Livelock: BDAR avoids livelock by having an upper
bound ud on the number of detours taken. Thus, a packet
367

7000

OddEven
XY
BDAR

10000
8000
6000
4000
2000
0

4 X 4 Mesh

5 X 5 Mesh

10 X 10 Mesh

(a) Random Traffic

OddEven
XY
BDAR

6000
5000
4000
3000
2000
1000
0

3 X 3 Mesh

4 X 4 Mesh

(b) Transpose Traffic

5000
Average Delay (in cycles)

12000

Average Delay (in cycles)

Average Delay (in cycles)

14000

4000

OddEven
XY
BDAR

3000
2000
1000
0

3 X 3 Mesh

4 X 4 Mesh

(c) Hotspot Traffic

Fig. 3. Average Delay for XY, OddEven and BDAR Algorithms. It should be noted that average delay is affected by the number of communication flows,
and not solely by the dimension of the mesh

A. Simulation Setup

is an inherent versatility associated with BDAR, brought about
with parameters like M nM , that is lacking from other routing
algorithms.

We consider 2D meshes ranging from 3 × 3 upto a 10 × 10,
and simulate synthetic traffic comprising of random, transpose
and hotspot traffic [9]. Both fixed packet sizes (4 to 8 flits),
and variable packet sizes are considered. The simulation time
is also varied to test the response of the network to heavy
and persistent traffic. We set the value of the threshold η
discussed in the previous section, which we name as M nM ,
short for Minimal-non-Minimal. This affects the number of
non-minimal routes in BDAR. It is a function of the average
packet injection rates and source to destination hop count of
all the flows.

VII. C ONCLUSION
In this paper, we have proposed a framework to perform deterministic and probabilistic analysis of NoC components. We
developed an adaptive routing algorithm, which is deadlocklivelock free and efficiently avoids the congested paths. Using
simulations, we have shown that our algorithm performs
significantly better in achieving shorter end-to-end delay compared to a few existing static and dynamic routing algorithms.
R EFERENCES

B. Observations and Inferences

[1] A. Jantsch and H. Tenhunen, Eds., Networks On Chip.
Kluwer
Academic Publishers, 2003.
[2] G. De-Micheli and L. Benini, Networks On Chips. Morgan Kaufmann,
2006.
[3] W. Dally and B. Towles, “Route Packets, Not Wires: On-Chip Interconnection Networks,” in Proceedings of Design Automation Conference,
Las Vegas, NV, USA, June 2001, pp. 684–689.
[4] U. Y. Ogras and R. Marculescu, “Analytical Router Modeling for
Networks-on-Chip Performance Analysis,” in Proceedings of the Conference on Design, Automation and Test in Europe, 2007, pp. 1096–1101.
[5] J.-Y. L. Boudec and P. Thiran, Network Calculus: A Theory of Deterministic Queueing Systems for the Internet, ser. LNCS 2050. Springer,
2001.
[6] M. Bakhouya, S. Suboh, J. Gaber, and T. El-Ghazawi, “Analytical
Modeling and Evaluation of On-Chip Interconnects using Network Calculus,” in Proceedings of the 3rd ACM/IEEE International Symposium
on Networks-on-Chip, 2009, pp. 74–79.
[7] C. J. Glass and L. M. Ni, “The Turn Model for Adaptive Routing,”
SIGARCH Comput. Archit. News, vol. 20, no. 2, pp. 278–287, 1992.
[8] Y. M. Boura and C. R. Das, “A class of partially adaptive routing
algorithms for n dimensional meshes,” in ICPP 9́3: Proceedings of the
1993 International Conf. on Parallel Processing, 1993, pp. 175–183.
[9] G. M. Chiu, “The odd-even turn model for adaptive routing,” IEEE
Trans. Parallel Distrib. Syst., vol. 11, no. 7, pp. 729–738, 2000.
[10] J. Hu and R. Marculescu, “Dyad: smart routing for networks-on-chip,”
in Proceedings of the 41st annual Design Automation Conference, 2004,
pp. 260–263.
[11] M. Palesi, R. Holsmark, S. Kumar, and V. Catania, “Application specific
routing algorithms for networks on chip,” Parallel and Distributed
Systems, IEEE Trans. on, vol. 20, no. 3, pp. 316 –330, March 2009.
[12] W. Song, D. Edwards, J. L. Nunez-Yanez, and S. Dasgupta, “Adaptive
stochastic routing in fault-tolerant on-chip networks,” in Proceedings of
the 2009 3rd ACM/IEEE International Symposium on Networks-on-Chip,
2009, pp. 32–37.
[13] J. L. Nunez-Yanez, D. Edwards, and A. M. Coppola, “Adaptive routing
strategies for fault-tolerant on-chip networks in dynamically reconfigurable systems,” IET Computers & Digital Techniques, vol. 2, no. 3, pp.
184–198, 2008.
[14] R. L. Cruz, “A Calculus for Network Delay, Part I: Network Elements
in Isolation,” IEEE Transaction on Information Theory, vol. 37, no. 1,
pp. 114–131, January 1991.
[15] F. Fazzino, M. Palesi, and D. Patti, “Noxim: Network-on-Chip
Simulator,” 2008. [Online]. Available: http://noxim.sourceforge.net

Figures 3(a), 3(b) and 3(c) show the Average Delay in cycles
plotted for meshes of varying sizes and traffic patterns. In
the case of random traffic, the reduction in average delay
in BDAR vs OddEven routing and XY routing ranges from
5 − 30% as can be seen from Fig. 3(a). BDAR experiences
the same delay as OddEven routing even for a 10 × 10
mesh, which is the network dimension considered in [9]. This
reinforces its adaptive capability. In the case of transpose
traffic (Fig. 3(b)), our routing algorithm performs better than
the OddEven algorithm by about 5 − 10% and matches, if
not betters the static XY routing, which traditionally provides the least delay for such traffic due to the absence of
hotspots. Same is the case when hotspot traffic is considered
(Fig. 3(c)) as we observe a 5 − 10% improvement in the
average delay. A major improvement in performance was
noticed with increased traffic. It was observed that BDAR
delivered more flows as compared to both XY and OddEven,
which successfully routed only half the flows that BDAR did.
While the delays of the undelivered packets/flows should be
considered as approaching to infinity, Noxim does not consider
those flows in the delay calculation, thus misinterpreting it as
lower average delays for XY and OddEven routings. Further
investigation indicated that BDAR dynamically re-routed a
large number of flows around the hotspot regions and thereby
not only utilized the link bandwidths better, but also displayed
a larger saturation throughput. The trends for the Maximum
Delay were almost identical to the ones explained above and
hence have been omitted due to space constraint. Although we
do not explicitly consider power consumption, but many of
our routing decisions, such as, bounded deflection, intuitively
lead to power-efficient designs. The simulations clearly bear
testimony to the performance of our routing algorithm. There
368

Partitioning Signed Bipartite Graphs for
Classification of Individuals and Organizations
Sujogya Banerjee, Kaushik Sarkar, Sedat Gokalp,
Arunabha Sen, and Hasan Davulcu
Arizona State University
P.O. Box 87-8809, Tempe, AZ, 85281 USA
{sujogya,kaushik.sarkar,sedat.gokalp,asen,hdavulcu}@asu.edu

Abstract. In this paper, we use signed bipartite graphs to model opinions expressed by one type of entities (e.g., individuals, organizations)
about another (e.g., political issues, religious beliefs), and based on the
strength of that opinion, partition both types of entities into two clusters. The clustering is done in such a way that support for the second
type of entity by the ﬁrst within a cluster is high and across the cluster
is low. We develop an automated partitioning tool that can be used to
classify individuals and/or organizations into two disjoint groups based
on their beliefs, practices and expressed opinions.

1

Introduction

The goal of the Minerva1 project, currently underway at Arizona State University is to increase understanding of movements within Muslim communities
actively working to counter violent extremism. As a part of this study, we have
collected over 800,000 documents from web sites various organizations in Indonesia. Based on the support and opposition of certain beliefs and practices, we can
partition the set of organizations O into two groups O1 and O2 and the set of
beliefs and practices B into two groups, B1 and B2 , such that organizations in O1
support B1 and oppose B2 , while the organizations O2 support B2 and oppose
B1 . With the domain knowledge of the social scientists in our team regarding
the beliefs and practices of Indonesian community, we can then label one group
as being radical and other as counter-radical.
Although the motivation for our work was driven by Minerva, the the problem
that is being addressed in this paper is much broader in nature. In the mathematical sociology community, the problem is known as the Signed two-mode network
partitioning problem [1]. In its mathematical abstraction, the problem is speciﬁed by a bipartite graph G = (U ∪ V, E) and label function σ : E → {P, N }.
The node sets U and V may be representing the set of organizations O and the
set of beliefs B respectively. If the label of an edge from oi ∈ O to bj ∈ B is
1

A project sponsored by the U.S. Department of Defense.

S.J. Yang, A.M. Greenberg, and M. Endsley (Eds.): SBP 2012, LNCS 7227, pp. 196–204, 2012.
c Springer-Verlag Berlin Heidelberg 2012


Partitioning Signed Bipartite Graphs for Classiﬁcation

(a)

197

(b)

Fig. 1. Partitioning of the node set U and V with the desired goal

P , it implies oi supports (or has positive opinion) about bj . If the label of an
edge is N , it implies oi opposes (or has negative opinion) about bj . The goal of
the partitioning problem is to divide the node sets U and V into two subsets
(U1 , U2 ) and (V1 , V2 ) respectively, such that
1. number of P edges (positive opinion or support) between nodes within block
1 (P11 between U1 and V1 ) and block 2 (P22 between U2 and V2 ) is high,
2. number of P edges between nodes across block 1 and block 2 (edges P12
between U1 and V2 and P21 between U2 and V1 ) is low,
3. number of N edges (negative opinion or opposition) between nodes within
block 1 (N11 between U1 and V1 ) and block 2 (N22 between U2 and V2 ) is
low and
4. number N edges between nodes across block 1 and block 2 (edges N12 between U1 and V2 and N21 between U2 and V1 ) is high.
The goal of partitioning is depicted in Fig. 1, where the green edges indicate
support (i.e, P edges) and the red edges indicate opposition (i.e, N edges). We
can realize these goals by maximizing [(P 11 + P 22 + N 12 + N 21) − (P 12 + P 21 +
N 11 + N 22)].
Signed two-mode network partitioning problem can be applied in a multitude
of domains, where the node sets U and V can represent diﬀerent entities. For
example, (i) U and V may represent the members of the U.S. Senate/House of
Representatives and the bills before the senate/house of representatives where
they cast their votes, either supporting or opposing the bill; (ii) U and V may
represent the political blogs/bloggers and various issues confronting the nation,
where they express their opinions either supporting or opposing issues. Clearly,
availability of an automated tool that will co-cluster the entities represented by
U and V , will be valuable to individuals and organizations that need a coarse
grain (two-modal) partitioning of the data set represented by the node set U
and V . This tool can help classify individuals or organizations as radicals vs.
counter-radicals, or liberals vs. conservatives or violent vs. non-violent, etc.
The main contribution of this eﬀort is the development of a fast automated
tool (and associated algorithms) for co-clustering the entities represented by the
node sets U and V . We ﬁrst compute an optimal solution of the partitioning
problem using an integer linear program to be used as a benchmark for our

198

S. Banerjee et al.

heuristic solution. We then develop a heuristic solution and compare its performance using three real data sets. The real data sets include voting records
of the Republican and Democratic members of the 111th US Congress and the
opinions expressed in top twenty two liberal and conservative blogs. In all these
data sets our partitioning tool produces high quality solution (i.e., with low misclassiﬁcation) at a low cost (in terms of computation time). To the best of our
knowledge, our Minerva research group is the ﬁrst to present an eﬃcient computational technique for partitioning of signed bipartite graph and apply it to
some real data sets.

2

Related Works

As the literature on clustering, classiﬁcation and partitioning is really vast, due
to page limitations, we only refer to the ones that are most relevant to this paper
[1,2,3,4,8,7]. The two key features of the partitioning problem addressed in this
paper are (i) the graph is bipartite and (ii) the weights on the edges are signed
(i.e., the weights are both positive and negative). Simultaneous clustering of two
sets of entities (represented by two sets of nodes in the bipartite graph) was
considered in the context of document clustering in [4,8]. In these studies one
set of entities are the documents and the other set is terms or words. Although
these eﬀorts study the bipartite graph partition problem, they are distinctly
diﬀerent from our study in one respect. In our study, the edge weights are signed,
whereas the edges weights considered in [4,8] are unsigned. Graph partitioning
problem with signed edge weights was studied in [2,3]. However, these studies are
also distinctly diﬀerent from our study in that, while they focus on partitioning
general (i.e., arbitrary) graphs, we focus our attention to partitioning bipartite
graphs. The study that comes closest to our study is [1,7], where attention is
focused on partitioning of a signed bipartite graphs. However, neither [1] nor
[7] present any eﬃcient algorithm to solve the partitioning problem in signed
bipartite graph.

3

Problem Formulation

In this section we formally deﬁne the partitioning problem.
Signed Bipartite Graph Partition Problem (SBGPP): An edge labeled weighted
bipartite graph G = (U ∪ V, E) where U = {u1 , u2 , . . . , un } represents entities of type I and V = {v1 , v2 , . . . , vm } represents entities of type II. Each edge
(u, v) ∈ E has two functions associated with it: (i) label function σ : E → {P, N },
which indicates the type of opinion (positive or negative), and (ii) weight function w : E → Z+ , which indicates the strength of that opinion. AN = [wn (u, v)]
and AP = [wp (u, v)] are the weighted adjacency matrix for edges with label N
and P respectively. If the node set U is partitioned into U1 and U2 and V is
partitioned into V1 and V2 , the strength of the positive and negative opinions of
the entities of type I regarding the entities of type II are deﬁned as follows:

Partitioning Signed Bipartite Graphs for Classiﬁcation

199

For all edges (u, v) ∈ E,
 
 
 
P11 =
wp (u, v), P12 =
wp (u, v), P22 =
wp (u, v)
u∈U1 v∈V1

P21 =

 

u∈U1 v∈V2

wp (u, v), N11 =

u∈U2 v∈V1

 

u∈U2 v∈V2

wn (u, v), N12 =

u∈U1 v∈V1

N22 =

 

wn (u, v), N21 =

u∈U2 v∈V2

 

wn (u, v)

u∈U1 v∈V2

 

wn (u, v)

u∈U2 v∈V1

Problem: Find a partition of the node set U into U1 and U2 and V into V1 and V2
such that [(P 11 + P 22 + N 12 + N 21) − (P 12 + P 21 + N 11 + N 22)] is maximized.

4

Computational Techniques

In this section we give a mathematical programming technique to ﬁnd the optimal solution for the SBGPP. Since computational time for ﬁnding optimal solution for large graphs is unacceptably high, we present a heuristic in subsequent
section to solve the SBGPP.
4.1

Optimal Solution for SBGPP

The goal of the SBGPP is to partition U into two disjoint sets U1 and U2
(similarly V into V1 and V2 ). For each node in u ∈ U and each partition Ui , i =
1, 2, we use a variable bui . bui is 1 iﬀ in u is in Ui . Similarly we deﬁne variable
pvi for all v ∈ V . We will refer B1 = U1 ∪ V1 and B2 = U2 ∪ V2 as blocks 1 and
2 respectively.
V ariables: For each node u ∈ U, v ∈ V and each partition Ui , Vi , i = 1, 2


1, if node u is in partition Ui
1, if node v is in partition Vi
bui =
pvi =
0, otherwise.
0, otherwise.
The mathematical programming formulation is given as follows:
max

L=

2  


(wp (u, v) − wn (u, v))bui pvi

i=1 u∈Ui v∈Vi

+

2

 

(wn (u, v) − wp (u, v))bui pvj

i,j=1 u∈Ui v∈Vj
i=j

s.t bu1 + bu2 = 1,

∀u ∈ U

(1)

pu1 + pu2 = 1,

∀p ∈ V

(2)

200

S. Banerjee et al.

The objective function computes the objective value given by the expression L.
We want to maximize L. It may be noted that the above quadratic objective
function can easily be changed into a linear function by simple variable transformation [6]. Constraint 1 and 2 ensures that each node in U and V belongs to
one particular block.
4.2

Move-Based Heuristics

We present a move-based heuristic to ﬁnd an approximate solution of SBGPP.
The move-based heuristic is a variant of well known FM algorithm [5] for partitioning graphs. The algorithm starts with a random initial partition and iteratively moves nodes from one block to another such that the value of the objective
function is improved. The “gain” of a node is deﬁned as the value by which the
objective function increases if the node is moved from one block to the other.
In each iteration the node with the highest gain is moved from one block to
the other. In case of a tie a node is chosen arbitrarily. After a node is moved,
it is locked and is not moved until the next pass. The heuristic is presented in
Algorithm 1. It should be noted that original FM algorithm will not work for our
problem as SBGPP relates to signed bipartite graphs with a completely diﬀerent
objective function and doesn’t have any size constraints. As a result the node
gain computation routine Algorithm 2 is considerably diﬀerent from the original
Algorithm 1: Move-based Heuristic (MBH)

13

Input : A weighted signed bipartite graph H = (U ∪ V, E)
Output: A partition of the nodes U1 , U2 and V1 , V2 such that objective value L
is maximum
L ←− 0;
for i ←− 1 to r do
Generate a random partitioning of the nodes in U into U1 and U2 and nodes
in V into V1 and V2 ;
repeat
Compute gains of all nodes using Algorithm 2 ;
repeat
Among all the unlocked nodes select the node of highest gain. Move
the node to the other block and call it base node. Lock the base
node. If the objective function value is best of the all the values seen
so far in this iteration then save this partition;
Update the node gains of all the free neighbors of the base node;
until Until all the nodes are locked ;
Change the current partition into a new partition that has the largest
value of the objective function in this pass ;
Unlock all the nodes;
until If the objective value L improves during the last pass;
if L ≥ L then L ← L and save the current partition

14

return L and the final partition of nodes

1
2
3
4
5
6
7

8
9
10
11
12

Partitioning Signed Bipartite Graphs for Classiﬁcation

201

Algorithm 2: Node Gain Computation
Input : A weighted signed bipartite graph G = (U ∪ V, E)
Output: Gains of all nodes
foreach node u ∈ U ∪ V do
gain(u) ←− 0;
// FBlock = "from block" of node u, ToBlock = "to block" of node
u, w(e) = weight of edge e and # = number
foreach edge e ∈ E with l(e) = N of node u do
if # nodes of e in ToBlock is 0 then gain(u) ← gain(u) + 2 ∗ w(e);
if # nodes of e in FBlock is 1 then gain(u) ← gain(u) − 2 ∗ w(e);

1
2

3
4
5

foreach edge e ∈ E with l(e) = P of node u do
if # nodes of e in ToBlock is 0 then gain(u) ← gain(u) − 2 ∗ w(e);
if # nodes of e in FBlock is 1 then gain(u) ← gain(u) + 2 ∗ w(e);

6
7
8

FM algorithm. Algorithm 1 runs for r diﬀerent initial random partition of the
nodes to avoid the possibility of being stuck at a local maxima. In practice the
heuristic converges very fast, mostly in 2 to 3 passes.

5

Experimental Results and Discussions

To validate the eﬀectiveness of our heuristic and benchmark its performance
we tested the heuristic on real world data. The real world data consists of US
Congress (SENATE, REP) and political blogosphere (BLOG) data sets.
5.1

US Congress Data [SENATE, REP]

The US Congress has been collecting data since the very ﬁrst congress of the US
history. This data has been encoded as XML ﬁles and publicly shared through
the govtrack.us project2 . From various types of data available at the project
site, we collected the roll call votes for the 111th US Congress which includes
The Senate and The House of Representatives and covers the years 2009-2010.
The 111th Senate data contains information about 108 senators and their votes
on 696 bills3 . The 111th Congress has 451 representatives and the data contains
their vote on 1655 bills.
We extracted the SENATE and REP data in adjacency matrices A|U|×|V | ,
with U vertices representing the congressmen, and the V vertices representing
the bills. The edge (ui , vj ), ui ∈ U, vj ∈ V has weight 1 if the congressman
ui votes ‘Yea’ for the bill vj , −1 if the congressman votes ‘Nay’, and 0 if he
did not attend the session. We have the original classiﬁcation vector for both
the congressmen and the bills in terms of which party they represent (or which
2
3

http://www.govtrack.us/data
Normally, each congress has 100 senators (2 from each state), however in many of
the congresses, there are unexpected changes on the seats caused by displacements
or deaths.

202

S. Banerjee et al.

party sponsored the bill). The ﬁrst two columns of Table 1 provide information
about this data as well as the partitioning accuracies of the algorithms. Figure
2 depicts the partitioned vote matrices of the 111th US Congress data, where
rows representing the congressmen and the columns representing the bills. Also,
the light green color represents ‘Yea’ votes, and dark red represents ‘Nay’ votes.
5.2

Blog Data [BLOG]

As Web 2.0 platforms gained popularity, it became easy for web users to be a
part of the web and express their opinions, mostly through blogs. Most blogs
are maintained by individuals, whereas there are also professional blogs with a
group of authors. In this study, we focus on a set of popular political liberal or
conservative blogs that have a clearly declared positions. These blogs contain
discussions about social, political, economic issues and related key individuals.
They express positive sentiment towards individuals whom they share ideologies
with, and negative sentiment towards the others. In these blogs, it is also common
to see criticism of people within the same camp, and also support for people from
the other camp.

(a) 111th US House

(b) 111th US Senate

Fig. 2. Vote matrix of US Congress after partitioning
Table 1. Descriptive summaries of the graphs for each dataset with the Heuristic
accuracy

Vertices in V

111th US Senate
64 Democrat
42 Republican
Senator
696 Bills

111th US House
268 Democrat
183 Republican
Representatives
1655 Bills

Graph Density
Heuristic accuracy

88.36 %
100.00%

91.23 %
99.56%

Vertices in U

Political Blogosphere
13 Liberal
9 Conservative
Blogs
20 Liberal
14 Conservative People
39.04 %
98.21%

In this experiment, we collected a list of 22 most popular liberal and conservative blogs from the Technorati4 rankings. For each blog, we fetched the posts
for the period of 6 months before the 2008 US presidential elections (May - October, 2008). We expected to have high intensity of the debates and discussions
4

http://technorati.com

Partitioning Signed Bipartite Graphs for Classiﬁcation

203

and resulting in a bipolar clustering in the data. Table 2 shows the partial list
of blogs with their URLs, political camps and the number of posts for the given
period.
We use AlchemyAPI5 to run a named entity tagger to extract the people
names mentioned in the posts, and an entity-level sentiment analysis which provided us with weighted and signed sentiment (positive values indicating support,
and negative indicating opposition) for each person. This information was used
to synthesize a signed bipartite graph (the BLOG data), where the blogs and
people correspond to the two sets of vertices U and V . The aij values of the adjacency matrix A are the cumulative sum of sentiment values for each mention
of the person vj by the blog ui .
To get a gold standard list of the most inﬂuential liberal and conservative
people, we used The Telegraph List6 for 2007. The third column of Table 1
provides information about this data as well as the partitioning accuracies of
the algorithm.
Table 2. Political Blogs
Blog name
URL
Huﬃngton Post
http://www.huﬃngtonpost.com/
Daily Kos
http://www.dailykos.com/
Boing Boing
http://www.boingboing.net/
Crooks and Liars
http://www.crooksandliars.com/
Firedoglake
http://www.ﬁredoglake.com/
Hot Air
http://hotair.com/
Reason - Hit and Run http://reason.com/blog
Little green footballs http://littlegreenfootballs.com/
Atlas shrugs
http://atlasshrugs2000.typepad.com/
Stop the ACLU
http://www.stoptheaclu.com/
Wizbangblog
http://wizbangblog.com/

6

Political view
Liberal
Liberal
Liberal
Liberal
Liberal
Conservative
Conservative
Conservative
Conservative
Conservative
Conservative

Size
3959
1957
1576
1497
1354
1579
1563
787
773
741
621

Conclusion

In this paper we study the problem of partitioning signed bipartite graph with
relevant application in political, religious and social domains. We provided a fast
heuristic to ﬁnd the solution for this problem. We tested the high accuracy of
our heuristic on three sets of real data collected from political domain.

References
1. Andrej, M., Doreian, P.: Partitioning signed two-mode networks. Journal of Mathematical Sociology 33, 196–221 (2009)
2. Bansal, N., Blum, A., Chawla, S.: Correlation clustering. Machine Learning,
238–247 (2002)
5
6

http://www.alchemyapi.com
The-top-US-conservatives-and-liberals.html

204

S. Banerjee et al.

3. Charikar, M., Guruswami, V., Wirth, A.: Clustering with qualitative information.
In: Proceedings of the 44th Annual IEEE FOCS (2003)
4. Dhillon, I.S.: Co-clustering documents and word using bipartite spectral graph partitioning. In: Proceedings of the KDD. IEEE (2001)
5. Fiduccia, C., Mattheyses, R.: A linear-time heuristic for improving network partitions. Papers on Twenty-Five Years of Electronic Design Automation, pp. 241–247.
ACM (1988)
6. Sen, A., Deng, H., Guha, S.: On a graph partition problem with application to vlsi
layout. Inf. Process. Lett. 43(2), 87–94 (1992)
7. Zaslavsky, T.: Frustration vs. clusterability in two-mode signed networks (signed
bipartite graphs) (2010)
8. Zha, H., He, X., Ding, C., Simon, H., Gu, M.: Bipartite graph partitioning and data
clustering. In: Proceedings of the 10th International Conference on Information and
Knowledge Management, pp. 25–32. ACM (2001)

On the Entity Hardening Problem in Multi-layered
Interdependent Networks
Joydeep Banerjee, Arun Das, Chenyang Zhou, Anisha Mazumder and Arunabha Sen

arXiv:1412.6686v1 [cs.NI] 20 Dec 2014

Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {joydeep.banerjee, arun.das, czhou24, anisha.mazumder, asen}@asu.edu
Abstract—The power grid and the communication network
are highly interdependent on each other for their well being.
In recent times the research community has shown significant
interest in modeling such interdependent networks and studying
the impact of failures on these networks. Although a number
of models have been proposed, many of them are simplistic in
nature and fail to capture the complex interdependencies that
exist between the entities of these networks. To overcome the
limitations, recently an Implicative Interdependency Model that
utilizes Boolean Logic, was proposed and a number of problems
were studied. In this paper we study the “entity hardening”
problem, where by “entity hardening” we imply the ability of the
network operator to ensure that an adversary (be it Nature or
human) cannot take a network entity from operative to inoperative
state. Given that the network operator with a limited budget
can only harden k entities, the goal of the entity hardening
problem is to identify the set of k entities whose hardening will
ensure maximum benefit for the operator, i.e. maximally reduce
the ability of the adversary to degrade the network. We show
that the problem is solvable in polynomial time for some cases,
whereas for others it is NP-complete. We provide the optimal
solution using ILP, and propose a heuristic approach to solve the
problem. We evaluate the efficacy of our heuristic using power
and communication network data of Maricopa County, Arizona.
The experiments show that our heuristic almost always produces
near optimal results.

I.

I NTRODUCTION

The critical infrastructures of a nation form a complex symbiotic ecosystem where individual infrastructures are heavily
interdependent on each other for being fully functional. Two
such critical systems that rely heavily on each other for their
well being are the power and communication network infrastructures. For instance, power grid entities such as SCADA
systems, that are used to remotely operate power generation
units, receive their control commands over the communication
network infrastructure, while communication network entities
such as routers and base stations are inoperable without electric
power. Thus, failure introduced in the system either by Nature
(hurricanes), or man (terrorist attacks), can trigger further
failures in the system due to interdependencies between the
entities of the two infrastructures.
Although a number of models have been proposed for modeling and analysis of interdependent multi-layered networks
[1], [2], [3], [4], [5], [6], [7], [8], many of these models are simplistic in nature and fail to capture the complex interdependencies that exists between the entities of these networks. As noted
in [9], these models fail to model complex interdependencies

that may exist between network entities, such as when entity ai
is operational, if entities (i) bj and bk and bl are operational, or
(ii) bm and bn are operational, or (iii) bp is operational. Graph
based interdependency models proposed in the literature such
as [3], [4], [5], [10], [6], [7] including [1], [2] cannot capture
such complex interdependency involving both conjunctive and
disjunctive terms between entities of multi-layer networks. To
overcome these limitations, an Implicative Interdependency
Model that utilizes Boolean Logic, was recently proposed in
[9], and a number of problems including computation of K
most vulnerable nodes [9], root cause of failure analysis [11],
and progressive recovery from failures [12], were studied using
this model.
In this paper we study the “entity hardening” problem in
the interdependent power-communication network using the
Implicative Interdependency Model (IIM). By “entity hardening”, we imply the ability of the network operator to ensure
that an adversary (be it Nature or human), cannot take a network entity from an operative (operational) to an inoperative
(failed) state. We assume that the adversary is clever and
is capable of identifying the most vulnerable entities in the
network that causes maximum damage to the interdependent
system. However, the adversary does not have an unlimited
budget and has the resources to destroy at most K entities
of the interdependent network. The network operator is also
aware of adversary’s target entities for destruction. Since we
assume that once an entity is “hardened” by the network
operator it cannot be destroyed by the adversary, if all K
targets of the adversary are hardened by the network operator,
then the adversary cannot induce any failure in the network.
However, if due to resource limitations the network operator
is able to strengthen only k entities, where k < K, these k
entities have to be carefully chosen. The goal of the entity
hardening problem is to identify the set of k entities whose
hardening will ensure maximum benefit for the operator, i.e.
maximally reduce the ability of the adversary to degrade the
network.
We classify the entity hardening problem into four different
cases depending on the nature of the interdependency relationships. We show that the first case can be solved in polynomial
time, and all other cases are shown to be NP-complete. We
provide an inapproximability result for the second case, an
approximation algorithm for the third case, and a heuristic
for the fourth (general) case. We evaluate the efficacy of our
heuristic using power and communication network data of
Maricopa County, Arizona. The experiments show that our

2

heuristic almost always produces near optimal results.
The paper is organized as follows, the IIM model is
presented in Section II, in Sections III and IV we formally
state the entity hardening problem and analyze its computational complexity, Section V outlines the optimal and heuristic
solutions to the problem, Section VI shows the experimental
results, and finally Section VII concludes this paper.
II.

I NTERDEPENDENCY M ODEL

We now present an overview of the underlying IIM interdependency model [9]. IIM uses Boolean Logic to model
the interdependencies between network entities, these interdependent relationships are termed as Implicative Interdependency Relations (IDRs). We represent this interdependent network setting as I(A, B, F (A, B)), where sets A
and B are the power and communication network entities
respectively, and F (A, B) is the set of dependency relations,
or IDRs. Table I represents a sample interdependent network I(A, B, F (A, B)), where A = {a1 , a2 , a3 , a4 }, B =
{b1 , b2 , b3 } and F (A, B) is the set of IDRs (dependency
relations) between the entities of A and B. In this example,
the IDR b1 ← a1 a3 + a2 implies that entity b1 is operational
when both the entities a1 and a3 are operational, or entity a2
is operational. The conjunction of entities, such as a1 a3 , is
also referred to as a minterm.
Power Network
a1 ← b1 b2
a2 ← b1 + b2
a3 ← b1 + b2 + b3
a4 ← b1 + b3

Comm. Network
b1 ← a1 a3 + a2
b2 ← a1 a2 a3
b3 ← a1 + a2 + a3
−−

TABLE I: Implicative Interdependency Relations of a sample network

Given a set of inoperable (failed) entities, a time stepped
failure cascade can be derived from the dependency relationships outlined in the IDR set. For example, for the interdependent network outlined in Table I, Table II shows the failure
propagation when entities {a2 , b3 } fail at the initial time step
(t = 0). It may be noted that the model assumes that dependent
entities fail immediately in the next time step, for example,
when {a2 , b3 } fail at t = 0, b2 fails at t = 1 as b2 is dependent
on a2 for its survival. The system reaches a steady state when
the failure propagation process stops. In this example, when
{a2 , b3 } fail at t = 0, the steady state is reached at time step
t = 4.
Entities
a1
a2
a3
a4
b1
b2
b3

0

1

0
1
0
0
0
0
1

0
1
0
0
0
1
1

Time Steps (t)
2
3
4
1
1
0
0
0
1
1

1
1
0
0
1
1
1

1
1
1
1
1
1
1

5

6

1
1
1
1
1
1
1

1
1
1
1
1
1
1

TABLE II: Failure cascade propagation when entities {a2 , b3 } fail at
time step t = 0. A value of 1 denotes entity failure, and 0 otherwise

A primary consideration for using this model is the accurate
formulation of the IDRs that is representative of the underlying
physical power and communication network infrastructures.
This can either be done by careful analysis as done in [8], or
by consultation with experts of these infrastructures. We utilize
IIM to model the interdependency between the two networks
and analyze the entity hardening problem in this setting.

III.

P ROBLEM F ORMULATION

Before we make a formal statement of the entity hardening
problem in the IIM setting, we explain it with the help of an
example. Consider an interdependent system as outlined in the
IDR set shown in Table I. It may be easily checked that when
the adversary budget is K= 2, the most vulnerable entities
of this system are {a2 , b3 }. If the network operator doesn’t
harden any one of the entities a2 or b3 , then in this example
all the network entities eventually fail, as seen from the fault
propagation in Table II. When the network operator chooses
to harden both a2 and b3 then none of the entities in the
network fail if the adversary restricts the attack only to the two
most vulnerable entities of the network, which in this example
happens to be {a2 , b3 }. If the network operator has resources
to harden only one entity and the operator chooses to harden
a2 , the destruction of b3 by the adversary will eventually lead
to the failure of no other entities of the network, as shown in
Table III(a). If on the other hand, the network operator chooses
to harden b3 , destruction by the adversary of a2 will eventually
lead to the failure of the entities {a2 , b2 , a1 , b1 } as shown in
Table III(b). Clearly in this scenario the operator should harden
a2 instead of b3 .
Definition: Kill Set of a set of Entities(S): The kill set of a
set of entities S, is the set of all entities that will eventually
fail due to failure of S and the interdependencies between the
entities of the network as given by the set of IDR’s. The kill
set of a set of entities S is denoted by KillSet(S).
It may be noted that the search for k entities to be hardened
is restricted to the KillSet(S), where S is the set of K
most vulnerable entities in the network, because hardening any
entity not in KillSet(S) does not provide any benefit to the
network operator. In this study we also assume that the set of
K most vulnerable entities in the network is unique.
Entities
0
a1
a2
a3
a4
b1
b2
b3

0
∗
0
0
0
0
1

Time Steps (t)
1
2
3
0
∗
0
0
0
0
1

0
∗
0
0
0
0
1

0
∗
0
0
0
0
1

(a) Entity a2 is hardened

Entities
4
0
∗
0
0
0
0
0

0
a1
a2
a3
a4
b1
b2
b3

0
1
0
0
0
0
∗

Time Steps (t)
1
2
3
0
1
0
0
0
1
∗

1
1
0
0
0
1
∗

1
1
0
0
1
1
∗

4
1
1
0
0
1
1
∗

(b) Entity b3 is hardened

TABLE III: Failure cascade propagation with entity hardening. Entities {a2 , b3 } are attacked at time step t = 0. A value of 1 denotes
entity failure, 0 otherwise. ∗ denotes a hardened entity.

We now proceed to formulate the entity hardening
problem formally. Given an interdependent network system
I(A, B, F (A, B)), and the set of K most vulnerable entities
of the system A′ ∪ B ′ , where A′ ⊆ A and B ′ ⊆ B:
The Entity Hardening (ENH) problem
INSTANCE: Given:
(i) An interdependent network system I(A, B, F (A, B)),
where the sets A and B represent the entities of the two
networks, and F (A, B) is the set of IDRs.
(ii) The set of K most vulnerable entities of the system
A′ ∪ B ′ , where A′ ⊆ A and B ′ ⊆ B
(iii) Two positive integers k, k < K and EF .

3

QUESTION:Is there a set of entities H = A′′ ∪ B ′′ , A′′ ⊆
A, B ′′ ⊆ B, |H| ≤ k, such that hardening H entities results
in no more than EF entities to fail after entities A′ ∪ B ′ fail
at time step t = 0.
We note some of the assumptions for the ENH problem:
First, we assume that once an entity is hardened, it is always
operational and does not fail at any time step of the observation, even when the entity is part of the K most vulnerable
entities. Second, we assume that k < K, as otherwise the
selection of K entities for hardening ensures that no entities
fail at all. Finally, as noted earlier, we assume that the set of
K most vulnerable entities in the network is unique. We now
proceed to analyze the computational complexity of the ENH
problem.
IV.

C OMPUTATIONAL C OMPLEXITY A NALYSIS

For an interdependent network I(A, B, F (A, B)) the IDRs
can be represented in four different forms. We analyze the
computational complexity of the ENH problem for each of
these cases separately.
A. Case I: Problem Instance with One Minterm of Size One
The IDRs of Case I have a single minterm of size 1. This
can be represented as xi ← yj , where xi and yj are entities of
network A(B) and B(A) respectively. We show that the ENH
problem for Case I can be solved optimally in polynomial time.
Algorithm 1: Entity Hardening Algorithm for systems
with Case I type interdependencies

1
2
3
4
5
6
7

8
9
10

Data: An interdependent network I(A, B, F(A, B)), set of
K most vulnerable entities A′ ∪ B ′ , A′ ⊆ A, B ′ ⊆ B,
hardening budget k and a set H = ∅.
Result: Set of hardened entities H.
begin
For each entity xi ∈ (A′ ∪ B ′ ) compute the set of kill sets
C = {Cx1 , Cx2 , ..., CxK }, where Cxi = KillSet(xi ) ;
Create a copy D = {Dx1 , Dx2 , ..., DxK } of set C ;
for (i=1; i ≤ K; i++) do
for (j=1, j 6= i; j ≤ K; j++) do
if Cxj ⊂ Cxi then
Dxi ← Dxi \ Dxj ;
Choose the top k sets from D with highest cardinality ;
For each of the Dxi ⊆ D sets chosen in Step 8,
H ← H ∪ xi ;
return H

Theorem 1. Algorithm 1 solves the Entity Hardening problem
for Case I optimally in polynomial time.
Proof: It is shown in [9] that the kill set for all entities in
the interdependent network can be computed in O(n3 ) where
n = |A| + |B|, thus computing the kill sets for K entities takes
O(Kn2 ). Step 4-7 of the algorithm runs in O(K2 ). Choosing
the k highest cardinality sets can be found using any standard
sorting algorithm in O(Klog(K)). Hence Algorithm 1 runs in
O(Kn2 ).
For two kill sets Cxi and Cxj it can be shown that either
Cxi ∩ Cxj = ∅ or Cxi ∩ Cxj = Cxi or Cxi ∩ Cxj = Cxj

[9]. So with two entities {xi , xj } ∈ A′ ∪ B ′ and Cxi ∩ Cxj =
Cxj i.e, Cxj ⊂ Cxi , if xi is hardened it prevents the failure
of Cxi − Cxj entities (provided that none of the entities in
Cxi − Cxj − {xi } are in A′ ∪ B ′ ). With this assertion, for
an entity xi ∈ A′ ∩ B ′ , steps 4-7 of Algorithm 1 finds the
actual entities for which failure is prevented by hardening xi .
The set D = {Dx1 , Dx2 , ..., DxK } comprises of these set of
entities for each hardened entity xi .
To prove that Algorithm 1 finds the optimal solution we
make the following two assertions: First, consider any two sets
Dxi and Dxj . It is implied from step 6 of Algorithm 1 that
/ A′ ∪ B ′ is
Dxi ∩ Dxj = ∅. Second, consider an entity xp ∈
hardened. If xp fails when entities in A′ ∪B ′ fails initially then
it would belong to some set Dxi . Thus hardening xp results
in preventing the failure of entities that is a proper subset of
Dxi . Hence the entities to be hardened must belong to A′ ∪ B ′
only. Owing to the two assertions it directly follows that with
a given budget k, hardening k highest cardinality sets from the
set D ensures prevention of failure for the maximum number
of entities.
B. Case II: Problem Instance with One Minterm of Arbitrary
Size
The IDRs of Case II have a single
Qpminterm of arbitrary
size. This can be represented as xi ← j=1 yj , where xi and
yj are entities of network A(B) and B(A) respectively and the
size of the minterm is p. The Entity Hardening problem with
respect to Case II is NP-complete and is proved in Theorem
2. An inapproximability proof for this case of the problem is
given in Theorem 3
Theorem 2. The Entity Hardening problem for Case II is NP
Complete
Proof: The Entity Hardening problem for case II is proved
to be NP complete by giving a reduction from the Densest pSubhypergraph problem [13], a known NP-complete problem.
An instance of the Densest p-Subhypergraph problem includes
a hypergraph G = (V, E), a parameter p and a parameter M .
The problem asks the question whether there exists a set of
vertices |V ′ | ⊆ V and |V ′ | ≤ p such that the subgraph induced
with this set of vertices has at least M hyperedges. From an
instance of the Densest p-Subhypergraph problem we create
an instance of the ENH problem in the following way. For
each vertex vi and each hyperedge ej an entity bi and aj are
added to the set B and A respectively. For each hyperedge ej
with ej = {vm , vn , vq } (say) an IDR of form aj ← bm bn bq is
created. It is assumed that the value of K is set of |V |. The
values of k and EF are set to p and |V | + |E| − p − M (where
|A| = |V | and |B| = |E|) respectively.
In the constructed instance only entities of set A are
dependent on entities of set B. Additionally the dependency
for an entity ai consists of conjunction of entities in set B.
Hence for an entity ai ∈ A to fail, either it itself has to fail
initially or all entities to which ai is dependent on has to fail.
It is to be noted that the entities in set B has no induced failure
i.e., there is no cascade. Following from this assertion, with
K = p, the solution A′ = ∅ and B ′ = B would fail all entities
in set A ∪ B. Moreover this is the single unique solution to
the problem instance. This is because by including one entity

4

ai in the initial failure set would result in not failing at least
one entity bj for a given budget K = p. Hence it won’t fail
the entire set of entities in A ∪ B.
If an entity in set A is hardened then it would have no effect
in failure prevention of any other entities. Whereas hardening
an entity bm ∈ B might result in failure prevention of an entity
ai ∈ A with IDR aj ← bm bn bq provided that entities bn , bq
are also defended. With k = p (and K ≤ |V | = |B|) it can be
ensured that entities to be defended are from set B ′ .
To prove the theorem consider that there is a solution to the
Densest p-Subhypergraph problem. Then there exist p vertices
which induces a subgraph which has at least M hyperedges.
Hardening the entities bi ∈ B ′ for each vertex vi in the solution
of the Densest p-Subhypergraph problem would then ensure
that at least M entities in set A are protected from failure.
This is because the entities in set A for which the failure
is prevented corresponds to the hyperedges in the induced
subgraph. Thus the number of entities that fail after hardening
p entities is at most |V | + |E| − p − M , solving the ENH
problem. Now consider that there is a solution to the ENH
problem. As previously stated, the entities to be hardened will
always be from set B ′ . So defending p entities from set B ′
would result in failure prevention of at least M entities in set
A such that EF ≤ |V | + |E| − p − M . Hence, the vertex
induced subgraph would have at least M hyperedges when
vertices corresponding to the entities hardened are included
in the solution of the Densest p-Subhypergraph problem, thus
solving it.
Theorem 3. For an interdependent network I(A, B, F (A, B))
with n = |A ∪ B| and F (A, B) having IDRs of form Case II,
it is hard to approximate the ENH problem within a factor of
1
for some λ > 0.
log(n)λ
2

Proof: From Theorem 2, Densest p-Subhypergraph problem has been shown to be a special case of the ENH problem
with IDRs of form Case II. Densest p-Subhypergraph problem
1
is proved to be inapproximable within a factor of log(n)
λ
2
(λ > 0) in [13]. Hence the theorem follows.
C. Case III: Problem Instance with an Arbitrary Number of
Minterm of Size One
The IDRs of Case III have arbitrary number
P of minterm of
size 1. This can be represented as xi ← pq=1 yq , where xi
and yq are entities of network A(B) and B(A) respectively
and the number of minterms are p. The ENH problem with
respect to Case III is NP-complete and is proved in Theorem
4.
Theorem 4. The ENH problem for Case III is NP Complete
Proof: The ENH problem for case III is proved to be NP
complete by giving a reduction from the Set Cover Problem,
a well known NP-complete problem. An instance of the Set
Cover problem includes a set S = {x1 , x2 , ..., xn }, a set S =
{S1 , S2 , ..., Sm } where Si ⊆ S and a positive integer M . The
problem asks the question whether there exists at most M
subsets from set S whose union would result in the set S. From
an instance of the set cover problem we create an instance of
the ENH problem in the following way. For each element xi
in set S we add an entity ai in set A. For each subset Si in

set S we add an entity bi in set B. For all subsets in S, say
Sp , Sm , Sn , which has the element xi there is an IDR of form
ai ← bm + bn + bl . The values of positive integers k and EF
are set to M and m − M respectively. It is assumed that the
value of K = m.
With similar reasoning as that of Case II it can be shown
that for K = m the maximum number of node failures (i.e.
failure of all entities in A ∪ B) would occur if A′ = ∅ and
B ′ = B. This is also the single unique solution to the problem
instance.
The constructed instance also ensures that the entities to
be hardened are from set B ′ (A′ not considered as it is equal
to ∅). This is because protecting an entity ai ∈ A would only
result in prevention of its own failure whereas protecting an
entity bj ∈ B would result in failure prevention of its own and
all other entities in set A for which it appears in its IDR.
To begin with the proof, consider that there is a solution
to the Set Cover problem. Then there exist M subsets (or
elements in set S) whose union results in the set S. Hardening
the entities in set B corresponding to the subsets selected
would ensure that all entities in set A are prevented from
failure. This is because for the dependency of each entity
ai ∈ A there exist at least one entity (in set B) that is hardened.
Hence the number of entities that fails after hardening is m−M
which is equal to EF , thus solving the ENH problem. Now,
consider that there is a solution to the ENH problem. As
discussed above the entities to be hardened should be from
set B ′ . To achieve EF = m − M with k = M , no entities
in the set A must fail. Hence for each entity ai ∈ A at least
one entity in set B that appears in its IDR has to be hardened.
Thus, it directly follows that the union of subsets in set S
corresponding to the entities hardened is equal to the set S,
solving the Set Cover Problem.
1) Approximation Scheme for Case 3: In this subsection we
provide an approximation algorithm for Case 3 of the problem.
For an interdependent network I(A, B, F (A, B)) with the
initial failed set of entities as A′ ∪ B ′ we define Protection
Set of each entity as follows.
Definition: For an entity xi ∈ A ∪ B the Protection Set is
defined as the entities that would be prevented from failure
by hardening the entity xi when all entities in A′ ∪ B ′ fails
initially. This is represented as P (xi |A′ ∪ B ′ ).
The Protection Set of each entity can be computed in
O((n + m)2 ) where n and m are the number of entities and
number of minterms respectively in an interdependent network
I(A, B, F (A, B)) .
Theorem 5. For two entities xi , xj ∈ A ∪ B, P (xi |A′ ∪ B ′ ) ∪
P (xj |A′ ∪ B ′ ) = P (xi , xj |A′ ∪ B ′ ) when IDRs are of form
Case III.
Proof: Assume that defending two entities xi and xj
would result in preventing failure of P (xi , xj |A′ ∪ B ′ ) entities
with |P (xi |A′ ∪ B ′ ) ∪ P (xj |A′ ∪ B ′ )| < |P (xi , xj |A′ ∪ B ′ )|.
Then there exist at least one entity xp ∈
/ P (xi |A′ ∪ B ′ ) ∪
P (xj |A′ ∪ B ′ ) such that it’s failure is prevented only if xi and
xj is protected together. So two entities xm and xn (with xm ∈
P (xi |A′ ∪B ′ ) and xn ∈ P (xj |A′ ∪B ′ ) or vice versa) have to be

5

present in the IDR of xp . As the IDRs are of form Case III so if
any one of xm or xn is protected then xp is protected, hence
a contradiction. On the other way round P (xi , xj |A′ ∪ B ′ )
contains all entities which would be prevented from failure
if xi or xj is defended alone. So it directly follows that
|P (xi |A′ ∪ B ′ ) ∪ P (xj |A′ ∪ B ′ )| > |P (xi , xj |A′ ∪ B ′ )| is
not possible. Hence the theorem holds.
Theorem 6. There exists an 1 − 1e approximation algorithm
that approximates the ENH problem for Case III.
Proof: The approximation algorithm is constructed by
modeling the problem as Maximum Coverage problem. An
instance of the maximum coverage problem consists of a
set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where
Si ⊆ S and a positive integer M . The objective of the problem
is to find a set S ′ ⊆ S and |S ′ | ≤ M such that ∪Si ∈S Si
is maximized. For a given initial failure set A′ ∪ B ′ with
|A′ |+|B ′ | ≤ K, let P (xi |A′ ∪B ′ ) denote the protection set for
each entity xi ∈ A ∪ B. We construct a set S = A ∪ B and for
each entity xi a set Sxi ⊆ S such that Sxi = P (xi |A′ ∪ B ′ ).
Each set Sxi is added as an element of a set S. The conversion
of the problem to Maximum Coverage problem can be done
in polynomial time. By Theorem 5 defending a set of entities
X ⊆ S would result in failure prevention of ∪xi ∈X Sxi entities.
Hence, with the constructed sets S and S and a positive integer
M (with M = k) finding the Maximum Coverage would
ensure the failure protection of maximum number of entities in
A ∪ B. This is same as the ENH problem of Case III. As there
exists an 1 − 1e approximation algorithm for the Maximum
Coverage problem hence the theorem holds.
D. Case IV: Problem Instance with an Arbitrary Number of
Minterms of Arbitrary Size
The IDRs of Case IV have arbitrary number of minterm
of
Pp arbitrary
Qqj1 size. This can be represented as xi ←
j2 =1 yj2 , where xi and yj2 are entities of network
j1 =1
A(B) and B(A) respectively and there are p minterms each
of size qj1 .
Theorem 7. The Entity Hardening problem for Case IV is NP
Complete
Proof: Case II and Case III are special cases of Case
IV. Hence following from Theorem 2 and Theorem 4 the
computational complexity of the Entity Hardening problem is
NP-complete in Case IV.
V.

S OLUTIONS TO

THE

E NTITY H ARDENING P ROBLEM

A. Optimal Solution using Integer Linear Programming
We propose an Integer Linear Program (ILP) that solves
the Entity Hardening problem optimally. Let [G, H] with
G = {g1 , g2 , ..., gn } and H = {h1 , h2 , ..., hm } denote the
entities in set A and B respectively with hi = 0 (gj = 0)
if entity ai (bj ) is alive and hi = 1 (gj = 1) otherwise.
Given an integer k let [G, H] be the solution (with value of 1
corresponding to entities failed initially) that cause maximum
number of entity failure. Two variables xid and yjd are used
in the ILP with xid = 1 (yjd = 1), when entity ai ∈ A
(bj ∈ B) is in a failed state at time step d, and 0 otherwise.
The number of entities to be defended is considered to be k.

It is to be noted that the maximum number cascading steps is
upper bounded by |A| + |B| − 1 = m + n − 1. The objective
function can now be formulated as follows:
min

n
m

X
X
yj(m+n−1)
xi(m+n−1) +

(1)

j=1

i=1

The objective in (1) minimizes the number of entities failed
after the cascading failure with the respective constraints for
the Entity Hardening problem as follows:
Constraint Set 1:

n
P

i=1

qxi +

m
P

qyj = k , with qxi , qyj ∈ [0, 1].

j=1

If an entity xi (yj ) is defended then qxi = 1 (qyj = 1) and 0
otherwise.
Constraint Set 2: xi0 ≥ gi − qxi and yi0 ≥ hi − qyi .
This constraint implies that only if an entity is not defended
and gi (hi ) is 1 then the entity will fail at the initial time step.
Constraint Set 3: xid ≥ xi(d−1) , ∀d, 1 ≤ d ≤ m + n − 1, and
yid ≥ yi(d−1) , ∀d, 1 ≤ d ≤ m + n − 1, in order to ensure
that for an entity which fails in a particular time step would
remain in failed state at all subsequent time steps.
Constraint Set 4: Modeling of the constraint to capture
the cascade propagation for IIM is similar to the constraints
established in [9]. A brief presentation of this constraint is
provided here. Consider an IDR ai ← bj bp bl + bm bn + bq of
type Case IV. The following steps are enumerated to depict
the cascade propagation:
Step 1: Replace all minterms of size greater than one with a
variable. In the example provided we have the transformed
minterm as ai ← c1 + c2 + bq with c1 ← bj bp bl and
c2 ← bm bn (c1 , c2 ∈ {0, 1}) as the new IDRs. Note that after
transformation, the original IDR is in the form of Case III and
the introduced IDRs are in the form of Case II.
Step 2: For each variable c, a constraints is added to capture
the cascade propagation. Let N be the number of entities
in the minterm on which c is dependent. In the example
for the variable c1 with IDR c1 ← bj bp bl , constraints
y
+y
+yl(d−1)
c1d ≥ j(d−1) p(d−1)
and c1d ≤ yj(d−1) + yp(d−1) +
N
yl(d−1) ∀d, 1 ≤ d ≤ m + n − 1 are introduced (with
N = 3 in this case). If IDR of an entity is already in
form of Case II, i.e.,ai ← bj bp bl then constraints xid ≥
yj(d−1) +yp(d−1) +yl(d−1)
− qxi and xid ≤ yj(d−1) + yp(d−1) +
N
yl(d−1) ∀d, 1 ≤ d ≤ m + n − 1 are introduced (with N = 3).
These constraints satisfies that if the entity xi is hardened
initially then it is not dead at any time step.
Step 3: Let M be the number of minterms in the transformed IDR as described in Step 1. In the given example
with IDR ai ← c1 + c2 + bq constraints of form xid ≥
c1(d−1) + c2(d−1) + yq(d−1) − (M − 1) − qxi and xid ≤
c1(d−1) +c2(d−1) +yq(d−1)
∀d, 1 ≤ d ≤ m + n − 1 are introduced.
M
These constraints ensures that even if all the minterms of xi
has at least one entity in dead state then it will be alive if the
entity is hardened initially. For all IDRs of type Case I and
Case III, the constraint discussed in this step is used.

6

B. Heuristic

Algorithm 2: Heuristic Solution to the ENH Problem

In this subsection we provide a greedy heuristic solution to
the Entity Hardening problem. For an interdependent network
I(A, B, F (A, B)) with the initial failed set of entities as
A′ ∪ B ′ , Protection Set of each entity has been defined in
the approximation scheme of Case III. To design the heuristic
we define Minterm Coverage Number of each entity in A ∪ B
as follows:

1
2
3
4

Definition: For an entity xi ∈ A ∪ B the Minterm Coverage
Number is defined as the number of minterms that can be
removed from F (A, B) without affecting the cascading process
by hardening the entity xi when all entities in A′ ∪ B ′ fails
initially. This is represented as M (xi |A′ ∪ B ′ ).
Similar to the computation of Protection Set the Minterm
Coverage Number of each entity can be computed in O((n +
m)2 ). With these definitions the heuristic is given in Algorithm
2. The algorithm takes in as input an interdependent network
I(A, B, F (A, B)) with S = A∪B. Step 4-5 is done to reduce
the search space as it directly follows that the set of entities
in Q wouldn’t effect the hardening process. In each iteration
of the while loop an entity xd is greedily selected which
when hardened would prevent failure of maximum number of
entities. This ensures that at each step the number of entities
failed is minimized. In case of a tie, among all entities involved
in the tie, the entity having the highest Minterm Coverage
Number is included in the solution. This gives a higher priority
to the entity which when hardened, has more impact on failure
minimization in subsequent iterations of the while loop. The
interdependent network I(A, B, F (A, B)) is updated in steps
13-16 of the algorithm. This takes into account the effect of
hardening an entity in the current iteration on entities hardened
in the following iterations.
Run Time Analysis of Algorithm 2: For this analysis we
consider n to be the total number of entities and m to be
the total number of minterms. Updates in step 4 can be done
in O(m) and step 5 in O(n). The while loop iterates for k
times. In each iteration of the while loop step 7 and step 8 takes
at most O((n + m)2 ) and O(nlog(n)) time respectively. On
branching in step 9, step 10 and step 11 takes O((n + m)2 )
and O(nlog(n)) time respectively. Updates in step 13 takes
O(n) time and in step 14 takes O(n + m) time. Step 12,
16 and 17 runs in constant time. Hence Algorithm 2 runs in
O(k(n + m)2 ) time.
VI.

E XPERIMENTAL R ESULTS

In this section we present the experimental results of the
Entity Hardening problem by comparing the optimal solution
computed using an ILP, and the proposed heuristic algorithm.
The experiments were conducted on real world power grid data
obtained from Platts (www.platts.com), and communication
network data obtained from GeoTel (www.geo-tel.com) of
Maricopa County, Arizona. The data consisted of 70 power
plants and 470 transmission lines in the power network, and
2, 690 cell towers, 7, 100 fiber-lit buildings and 42, 723 fiber
links in the communication network. We identified five nonintersecting geographical regions from the data set and labeled
them from regions 1 through 5. For each of the regions, the
entities of the power and communication network that were

5
6
7
8
9
10
11
12

Data: An interdependent network I(A, B, F(A, B)) (with
S = A ∪ B), set of entities A′ ∪ B ′ failed initially
causing maximum failure in the interdependent network
with |A′ | + |B ′ | = K and hardening budget k.
Result: Set of hardened entities H.
begin
Initialize S ′ ← A′ ∪ B ′ ;
Initialize H = ∅;
Update F(A, B) as follows — (a) let Q be the set of
entities that does not fail on failing K entities, (b) remove
IDRs corresponding to entities in set Q, (c) remove from
minterm of entities not in set Q all entities which are in
set Q ;
Update S = S \ Q ;
while (k entities are not hardened) do
For each entity xi ∈ S compute the Protection Set
P (xi |S ′ );
Choose the entity xd with highest cardinality of the
set |P (xd |S ′ )|;
if (more than one entity has the same highest
cardinality value) then
For each such entity xj compute the Minterm
Coverage Number M (xj |S ′ ) ;
Choose the entity xd with highest Minterm
Coverage Number. ;
In case of a tie choose arbitrarily;

16

Update S ← S − P (xd |S ′ );
Update F(A, B) by removing (i) IDRs corresponding
to all entities in P (xd |S ′ ), and (ii) occurrence of
these entities in IDRs of entities not in P (xd |S ′ );
if (xd ∈ S ′ ) then
Update S ′ ← S ′ − {xd };

17

Update H = H ∪ xd ;

13
14

15

18

return H ;

located within the geographic region formed the set A and B
respectively. Each region was represented by an interdependent
network I(A, B, F (A, B)). We use the IDR construction rules
as defined in [9] to generate F (A, B).

In all of our simulations IBM CPLEX Optimizer 12.5 to
solve ILPs and Python 3 for heuristic is used. To analyze
the Entity Hardening problem the value of K was set to 8.
The ILP in [9] was used to compute the K most vulnerable
nodes in the network, and the set of failed entities due to
the failure of the K entities was also computed. For the five
regions, when the K = 8 most vulnerable nodes failed, the
total number of failed entities in the network were 28, 23, 28,
28 and 27 respectively. With the K most vulnerable nodes and
final set of failed nodes as input, the ILP and heuristic of the
Entity Hardening problem are compared with k = 1, 3, 5, 7.
The results of these simulations are shown in Figure 1. It is
observed that the heuristic solution differs more from optimal
at higher values of k (factor of 0.5 and 0.67 for Regions 1
and 3 respectively with k = 7). This is primarily because of
the greedy nature of Algorithm 2. However on an average the
heuristic solution differs by a factor of 0.13 from the optimal.

7

14 13

ILP solution
Heuristic

10
8

7
6

6

4

4

3
2

2
0

1
1

14

13

ILP solution
Heuristic

12
10
8

7

4

3

Number of entities failed

3

2

1
1

1

13

ILP solution
Heuristic

12

12
10

8

8
6

6

5

4

3

2
1

3
5
7
Number of entities hardened

(b) Region 2
11

ILP solution
Heuristic

10
8
6
5

4

4

3
2

2

1
1

(c) Region 3

8

11

6

3
1

0

3
5
7
Number of entities hardened

(a) Region 1

0

7

6

0

3
5
7
Number of entities hardened

12

Number of entities failed

13
12

3
5
7
Number of entities hardened

(d) Region 4

Number of entities failed

12

Number of entities failed

Number of entities failed

14

7

ILP solution
Heuristic

7

6
5

5

4
3

3

2
1
0

1

1

3
5
7
Number of entities hardened

(e) Region 5

Fig. 1: Comparison chart of the optimal solution (ILP) with the heuristic by varying number of entities hardened for each identified region

VII.

C ONCLUSION

In this paper we studied the entity hardening problem
in multi-layer networks. We modeled the interdependencies
shared between the networks using IIM, and formulated the
the Entity Hardening problem in this setting. We showed that
the problem is solvable in polynomial time for some cases,
whereas for others it is NP-complete. We evaluated the efficacy
of our heuristic using power and communication network data
of Maricopa County, Arizona. Our experiments showed that
our heuristic almost always produces near optimal results.
R EFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

[7]

S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
“Catastrophic cascade of failures in interdependent networks,” Nature,
vol. 464, no. 7291, pp. 1025–1028, 2010.
J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, “Networks formed
from interdependent networks,” Nature Physics, vol. 8, no. 1, pp. 40–48,
2011.
J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, “Cascade of
failures in coupled network systems with multiple support-dependence
relations,” Physical Review E, vol. 83, no. 3, p. 036116, 2011.
V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, “Modelling interdependent infrastructures using interacting
dynamical models,” International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63–79, 2008.
P. Zhang, S. Peeta, and T. Friesz, “Dynamic game theoretic model of
multi-layer infrastructure networks,” Networks and Spatial Economics,
vol. 5, no. 2, pp. 147–178, 2005.
M. Parandehgheibi and E. Modiano, “Robustness of interdependent
networks: The case of communication networks and the power grid,”
arXiv preprint arXiv:1304.0356, 2013.
D. T. Nguyen, Y. Shen, and M. T. Thai, “Detecting critical nodes in
interdependent power networks for vulnerability assessment,” 2013.

[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
“Power grid vulnerability to geographically correlated failures-analysis
and control implications,” arXiv preprint arXiv:1206.1099, 2012.
[9] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, “Identification of k most vulnerable nodes in multi-layered network using a new
model of interdependency,” in Computer Communications Workshops
(INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014, pp.
831–836.
[10] J.-F. Castet and J. H. Saleh, “Interdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,” PloS one, vol. 8, no. 4, p. e60402, 2013.
[11] A. Das, J. Banerjee, and A. Sen, “Root cause analysis of failures in
interdependent power-communication networks,” in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910–915.
[12] A. Mazumder, C. Zhou, A. Das, and A. Sen, “Progressive recovery from
failure in multi-layered interdependent network using a new model of
interdependency,” in Conference on Critical Information Infrastructures
Security (CRITIS), 2014. Springer, 2014.
[13] M. Hajiaghayi, K. Jain, K. Konwar, L. Lau, I. Mandoiu, A. Russell,
A. Shvartsman, and V. Vazirani, “The minimum k-colored subgraph
problem in haplotyping and dna primer selection,” in Proceedings of the
International Workshop on Bioinformatics Research and Applications
(IWBRA). Citeseer, 2006.

2014 International Conference on Computing, Networking and Communications (ICNC)
1

On the Impact of Coding Parameters on Storage
Requirement of Region-based Fault Tolerant
Distributed File System Design
Sujogya Banerjee, Arun Das, Anisha Mazumder, Zahra Derakhshandeh and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {sujogya, adas22, amazumde, zderakhs, asen }@asu.edu

are created from the original file F and are stored in N
nodes of the network (one coded segment per node). The
advantage of this storage scheme is that the original file F can
be reconstructed by any user in the network just by retrieving
and decoding any K out of N segments.
We consider the scenario where failure of nodes (storing
data segments) might disconnect (fragment) the network.
While (N , K) codes ensure that as long as K file segments
survive the file can be reconstructed, this condition alone is
not sufficient for successful file reconstruction in a distributed
network file storage environment. In order to recreate the file
a node should be able to access K file segments by contacting
the nodes storing these segments. In order to achieve this, the
node must be connected to the other nodes storing the K file
segments. Survival of nodes storing K coded segments alone
in the network may not be very useful in the scenario where
the network splits up into two or more connected components
and although more than K segments survive, none of the
components have more than K − 1 segments. To be able to
reconstruct the file, one has to ensure that at least one of
the connected components has at least K segments. In this
paper we require that a largest connected component of the
fragmented network must have at least K segments.
Although quite a few studies ([7], [8], [9], [10]) have
proposed the use of (N , K) coding for reliable distributed
file system design, to the best of our knowledge, no one has
investigated the impact of the choice of the coding parameters
(N , K) on the storage that will be necessary to provide an all
region fault tolerant distributed file system. By all region fault
tolerant system we imply a system that has the data segments
stored in different network nodes in such a way that no
matter which region of the network fails, the largest connected
component of the residual network will have K segments with
which to reconstruct the entire file. In this paper we present
analytical results demonstrating that the choice of the coding
parameters N and K may have significant impact on storage
that will be necessary to achieve reliability. We present a
polynomial time algorithm for optimal storage allocation in a
mesh network with (i) specified size of the mesh, (ii) specified
size of the fault region, and (iii) coding parameters N and K.
We conduct extensive experimentation to evaluate the impact
of the coding parameters N and K on the storage requirement

Abstract—Advances in technology have resulted in Internetscale deployment of storage systems such as peer-to-peer storage
and cloud storage, where data is distributed over multiple storage
nodes in a networked environment. In these environments the
storage nodes are often commodity machines and are susceptible
to failure. The notion of fault domain, introduced by Microsoft
Azure, captures the fault-tolerance aspects of a data center. A
fault domain is defined as a set of servers all of which become
inaccessible when a single fault (such as the failure of a switch
or a router) occurs in the data center. As such a fault domain
can be viewed as a spatially correlated or region based failure.
In order to enhance reliability through redundancy, maximum
distance separable (MDS) codes such as Reed-Solomon codes and
(N , K) codings are utilized. In this paper we present analytical
results demonstrating that the choice of the coding parameters
N and K may have significant impact on storage that will be
necessary to achieve reliability. We present a polynomial time
algorithm for optimal storage allocation in a mesh network and
we conduct extensive experimentation to evaluate the impact of
the coding parameters N and K on the storage requirement to
provide all region fault tolerance with varying size of the mesh
and the fault region.

I. I NTRODUCTION
Advances in technology have resulted in Internet-scale
deployment of storage systems such as peer-to-peer storage
and cloud storage in a networked environment. In these
environments the storage nodes are often commodity machines
and are susceptible to failure. It leads us to the notion of fault
domain, introduced by Microsoft Azure [1], which captures
the fault-tolerance aspects of a data center. In a network
spanning a large geographical area, the faulty nodes may be
spatially correlated (i.e., confined to a region) [2], [3], [4].
Such failures are often encountered in diaster situations, either
natural (earthquake, forest fire, flood or hurricane) or manmade (EMP attack or enemy bomb), where only the nodes
in the disaster zone are affected. A similar scenario can be
envisioned in a data center environment as well. When a
router or a switch malfunctions in a data center, all the servers
connected to that switch or router become inaccessible. Thus,
these failures can also be viewed as region based faults.
In order to enhance reliability through redundancy, maximum distance separable (MDS) codes such as Reed-Solomon
codes and (N , K) codings are utilized [5], [6]. In a (N , K)
code based storage system, N coded segments of size |F |/K

978-1-4799-2358-8/14/$31.00 ©2014 IEEE

78

2014 International Conference on Computing, Networking and Communications (ICNC)
2

is less than or equal to αi , (ii)
Pneach set Si ∈ S has at least K
distinct colors and (iii) σ = i=1 ρi is minimum.
If we take αi = 1, the DDP reduces to the following
problem.

to provide all region fault tolerance with varying size of the
mesh and the fault region.
II. P ROBLEM F ORMULATION

Set Coloring Problem (SCP): Given a set of elements S =
{s1 , . . . , sn }, another set S = {S1 , . . . , St }, where Si ⊆
S, 1 ≤ i ≤ t, integers K and N , find the smallest subset
S 0 ⊆ S, such that the elements of S 0 can be colored in such a
way that each Si , 1 ≤ i ≤ t, receives at least K distinct colors
and the number of colors used does not exceed N . (note: not
every element in Si ⊆ S, 1 ≤ i ≤ t has to be colored.)

In this section we provide a formal statement of the data
distribution problem. A file F of length M is split into
K segments each of length M/K. Using erasure coding
techniques like maximum distance separable (MDS) codes,
Reed-Solomon (RS) codes, etc., these K segments can be
encoded to get altogether N segments (N ≥ K). According
to the erasure coding theory retrieving any K segments out of
the N segments is sufficient to reconstruct the whole file F .
The network is represented by a graph G = (V, E) where V
is the set of n nodes in the network and E is the set of links
between them. Each node has a maximum storage capacity
for storing the file segments. For the file F , a node vi ∈ V
can store maximum of αi file segments. Nodes in the network
are assumed to have both storage and processing unit. So any
node can recreate the file F by downloading and decoding K
coded segments from other nodes.
We assume that the faults are region based. Due to any
region-fault the residual network may become fragmented. Our
objective is to distribute at most N encoded segments of file
F among the n storage locations of the network in such a way
that a largest component of the residual network has at least
K distinct segments to reconstruct the file. Let Si be the set of
nodes in largest residual component in network G after regionfault Ri , where i = 1, . . . , t. Let set S = {S1 , . . . , St } represent the set of t largest residual components of network G.
Let us consider that after distribution of the coded segments,
each node vi receives ρi segments, such that 0 ≤ ρi ≤ αi .
Since storing each file segments involves a cost, we want to
minimize the total storage required to make the network all
region fault tolerant.
The data distribution problem can be viewed as a color
assignment problem. Let C = {1, 2, . . . , N } be the set of
N distinct colors. We formulate the problem as a color
assignment problem where each color represents a segment
and assigning a color cj , 1 ≤ j ≤ N to a node vi implies
that file segment j is stored in node vi . If the storage capacity
of node vi is αi then at most αi colors can be assigned to
node vi .
Formally the data file segment distribution problem can be
stated as:

If we take K = 1, the SCP reduces to the following problem.
Hitting K-Set Problem : Given a set of elements S =
{s1 , . . . , sn }, another set S = {S1 , . . . , St }, where Si ⊆
S, 1 ≤ i ≤ t, integer K, find the smallest subset SH ⊆ S,
such that for each set Si , 1 ≤ i ≤ t, |SH ∩ Si | ≥ K.
It may be noted that instances of the Hitting K-Set problem
are a subset of the instances of Set Coloring problem. Suppose
that for a given instance ISCP of the Set Coloring Problem,
the size of the smallest Hitting K-Set of this instance is
β, i.e. |SH | = β. This implies that minimizing total storage
requirement is equivalent to minimizing the number of nodes
where one segment of data has to be stored.
III. I MPACT OF C ODING PARAMETERS N AND K ON
R EQUIRED S TORAGE σ
In this section, we first provide an example that shows
that the choice of the coding parameters N and K can have
considerable impact on the minimum storage requirement σ.
We then prove a theorem to formally establish how significant
this difference can be.
Example 1: Consider an instance of SCP where S =
{1, 2, . . . , 20} and the subsets S={S1 , S2 , ..., S28 }, Si ⊆ S,
1 ≤ i ≤ 28, as shown in Table I. The solution to SCP is
the smallest S 0 ⊆ S such that the elements of S 0 can be
colored in a way that each Si , 1 ≤ i ≤ t, receives at least
K = 2 distinct colors and the number of colors used does not
exceed N = 8, 4, 2. The cardinality of the smallest S 0 , for
N = 8, 4, and 2, are – 8 (where elements 1 through 8 of the
set S are assigned a color, in other words file segments are
placed in nodes 1 through 8), 12 (where elements 1 through
8, and 10, 13, 16, and 18 of the set S are assigned a color)
and 20 (where elements 1 through 20 of the set S are assigned
a color) respectively. It may be noted that the cardinality of
S 0 represents the number of nodes where one segment of the
data file has to be stored. The colors assigned to the nodes are
represented by alphabets A, B, .... in Table I.
In the following discussion we establish a theorem that
formalizes our observations from the previous example, that
there can be a significant difference in σ depending on the
choice of N and K.
Lemma 1: In SCP problem, if N ≥ β, then σ is at most β.
Proof: If N ≥ β, then we first solve the Hitting K-Set problem
on the instance of SCP problem and assign each of the β node
of the solution set SH a distinct color. It is easy to check that

Data Distribution Problem (DDP):
INSTANCE: Given
(i) a graph G = (V, E) where V = {v1 , . . . , vn } and E =
{e1 , . . . , em } are the sets of nodes and links respectively,
(ii) maximum of αi colors can be assigned to a node vi ,
(iii) region R defined in an appropriate fashion (may be a
circular or square area),
(iv) a set S = {S1 , . . . , St } where Si ⊆ V , 1 ≤ i ≤ t,
(v) a set C = {1, 2, . . . , N } of distinct colors and parameter
K.
QUESTION: Assign at most N colors to the nodes in V in
such a way that (i) number of colors ρi assigned to node vi

79

2014 International Conference on Computing, Networking and Communications (ICNC)
3

TABLE I: Set coloring with different values of N

this will satisfy the coloring constraint for the SCP problem as
each subset Si , 1 ≤ i ≤ t will have K different colors. Thus,
the number of nodes that need to be colored in this case, and
hence the storage requirement σ, is at most β.
Lemma 2: In SCP problem if N < β, then σ can be as
large as 2β .
Proof: Let us consider an instance of SCP such that K = 2,
n = 2β . W.l.o.g we assume si = i where si ∈ S and
1 ≤ i ≤ n. Again w.l.o.g we assume that first β elements
of the set S constitute the solution of Hitting K-Set problem,
i.e. SH = {1, 2, . . . , β}. The subsets of SCP are as follows:
S = {(i, j, k)|(i, j) ⊂ {1, 2, . . . , β}, k ∈ {β + 1, β +
2, . . . , n}}. It may be noted that SH is the solution of the
Hitting K-Set problem for this instance. All the nodes of SH
must be assigned a color to satisfy the coloring constraint.
Since N < β, at least a pair of nodes in SH will have the
same color. Suppose that nodes i and j are assigned the same
color. Because of this assignment the following set of subsets
will violate the coloring constraint (i.e., each subset must have
at least two distinct colors)

Color
Subset S1
Color
Subset S2
Color
Subset S3
Color
Subset S4
Color
Subset S5
Color
Subset S6
Color
Subset S7
Color
Subset S8
Color
Subset S9
Color
Subset S10
Color
Subset S11
Color
Subset S12
Color
Subset S13
Color
Subset S14
Color
Subset S15
Color
Subset S16
Color
Subset S17
Color
Subset S18
Color
Subset S19
Color
Subset S20
Color
Subset S21
Color
Subset S22
Color
Subset S23
Color
Subset S24
Color
Subset S25
Color
Subset S26
Color
Subset S27
Color
Subset S28

{i, j, β + 1}, {i, j, β + 2}, . . . , {i, j, n}
In order to satisfy the coloring constraint, not only nodes
1, . . . , β need to be colored, nodes β + 1, β + 2, . . . , n must
also be assigned a color. Since only one data segment is stored
in a node and in order to satisfy the coloring constraint each
node must receive a color, the storage requirement σ will be
equal to n. Since we assumed that n = 2β , the number of
nodes that has be colored can be as large as 2β .
Theorem 1: The number of nodes that need to be colored
to satisfy the coloring constraint (σ) could be as small as β
or as large as 2β , depending on whether N ≥ β or N < β
respectively.
Proof: It follows from the Lemmas 1 and 2.
IV. O PTIMAL A LGORITHM FOR DATA D ISTRIBUTION IN A
M ESH N EWORK
A. Optimal solution for network with regular structure
In [10] it was proved that for arbitrary networks DDP is
a NP-complete problem. But it is possible to solve DDP
optimally in polynomial time for networks with specific structure with a specific definition of region. In this section we
present a polynomial time optimal algorithm for solving DDP
in a regular two dimensional grid network of size (n × n)
where failures of the nodes are assumed to be confined to
a region defined by a smaller grid of size (r × r) with
r < n and n being a multiple of r. In a (n × n) two
dimensional grid network, node uij is located at the position
(i, j), ∀1 ≤ i ≤ n, 1 ≤ j ≤ n, on the grid (Fig. 1). Faults
are assumed to be confined within a smaller grid of size
(r × r) completely contained within the grid network. The
fault region can be anywhere inside the grid network, and
it is assumed all the nodes in the fault region becomes non
operational. The storage capacity of each node in this case is
assumed to be 1 unit. One important property of grid networks
is that the residual network will always remain connected even
after any region fault. Given a value of (N , K), Algorithm

A
1
A
1
A
1
A
1
A
1
A
1
A
1
B
2
B
2
B
2
B
2
B
2
B
2
C
3
C
3
C
3
C
3
C
3
D
4
D
4
D
4
D
4
E
5
E
5
E
5
F
6
F
6
G
7

N =8
B
2
C
3
9
D
4
E
5
10
F
6
G
7
11
H
8
C
3
D
4
12
E
5
F
6
13
G
7
H
8
14
D
4
E
5
15
F
6
G
7
16
H
8
E
5
F
6
17
G
7
H
8
18
F
6
G
7
19
H
8
G
7
H
8
20
H
8

A
1
A
1
A
1
A
1
A
1
A
1
A
1
B
2
B
2
B
2
B
2
B
2
B
2
C
3
C
3
C
3
C
3
C
3
D
4
D
4
D
4
D
4
A
5
A
5
A
5
B
6
B
6
C
7

K=2
N =4
B
2
C
3
D
4
A
5
B
6
C
7
D
8
C
3
D
4
A
5
B
6
C
7
D
8
D
4
A
5
B
6
C
7
D
8
A
5
B
6
C
7
D
8
B
6
C
7
D
8
C
7
D
8
D
8

9

B
10

11

12

A
13

14

15

B
16

17

A
18

19

20

A
1
A
1
A
1
A
1
A
1
A
1
A
1
B
2
B
2
B
2
B
2
B
2
B
2
A
3
A
3
A
3
A
3
A
3
B
4
B
4
B
4
B
4
A
5
A
5
A
5
B
6
B
6
A
7

N =2
B
2
A
B
3
9
B
4
A
B
5
10
B
6
A
B
7
11
B
8
A
3
B
A
4
12
A
5
B
A
6
13
A
7
B
A
8
14
B
4
A
B
5
15
B
6
A
B
7
16
B
8
A
5
B
A
6
17
A
7
B
A
8
18
B
6
A
B
7
19
B
8
A
7
B
A
8
20
B
8

1 (DDG) shows a technique to assign colors to minimum
number of nodes in this network such that for any region
fault, the residual network (which is also the largest connected
component) will have at least K different colors.
The algorithm DDG starts by assigning color 1 to node
u1,1 . The two inner for-loops (step 10-31) ensures that colors
assigned to nodes in each iteration do not fall within the same
region. In each iterations the two outer for-loops (step 7-33)
appropriately shifts x and y index of the new node to be

80

2014 International Conference on Computing, Networking and Communications (ICNC)
4

Algorithm 1 Data Distribution on Grid Network (DDG)

colored so that in that iteration maximum number of uncolored
nodes can be colored. During the coloring process r0 keeps
track of the number of nodes colored so far which falls within
a single region. The goal is to distribute the colored nodes in
such a way that r0 is minimum. Each of these outer iterations
increase r0 by one. If N ≥ K + d d n eK2 −1 e, the algorithm
r
terminates when number of nodes colored ρ is equal to K +r0 .
If N < K + d d n eK2 −1 e the algorithm will have to reuse
r
some of the already assigned colors to color new nodes. The
algorithm reuses colors in such a way that nodes with same
color do not fall in the same region (step 14-18). The algorithm
then terminates when ρ = 2K − (N − K)(d nr e2 − 2). In
worst case the algorithm assigns color to each node. The time
complexity of the algorithm is O(|U |) i.e., O(n2 ).

1:

2:

3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

15:
16:
17:
18:
19:

Fig. 1: An example of a regular grid network of size 6 × 6 and a
region fault of size 3 grid. Given N = 8 and K = 5, the example
shows how the algorithm colors the node in the network. According
to the algorithm colors assigned to node u1,1 is 1, u1,4 is 2, u4,1 is
3, u4,4 is 4, u1,2 is 5, u1,5 is 6 and u4,2 is 7

20:
21:

Theorem 2: Given a value of N , K Algorithm 1 (DDG)
gives the optimal solution.
Proof: Here, we assume that n is a multiple of r. First of all,
when K > n2 − r2 or N > K then no solution is feasible.
Step 3 of DDG takes care of this infeasibility condition. We
find the optimal solution for two different cases:
CASE I ( N ≥ K + d d n eK2 −1 e): Assume that in the optimal
r
solution nodes are assigned colors in such a way that the
maximum number of colored nodes falling within one region
is given by k 0 . Maximum number of non-overlapping regions
that can exist simultaneously in a (n×n) grid is given by d nr e2 .
Thus, even if one of these regions fail, the optimal solution
should ensure that at least K colored nodes will be available
in rest of the d nr e2 − 1 regions. Accordingly, the following
equation holds: k 0 d nr e2 ≥ K + k 0 or k 0 ≥ d n eK2 −1 . Optimal
r
solution will find minimum value of k 0 , i.e., k 0 = d d n eK2 −1 e.
r
So if N ≥ K + d d n eK2 −1 e the total number of nodes that will
r
be colored by optimal solution is equal to K + d d n eK2 −1 e.
r
The DDG algorithm colors at most d nr e2 nodes in the inner
for-loops (steps 10-31).The algorithm stops after r0 iterations
of outer for-loops (step 7-33). So maximum number of nodes
colored in r0 iterations is ρ = r0 d nr e2 . The algorithm stops

22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:

INPUT:
1) The network G = (U, E) with a two dimensional
grid topology of size n × n where node uij ∈ U ,
2) Maximum 1 color can be assigned to ui,j , ∀ui,j ∈
U,
3) Color set C = {i, . . . , N },
4) Region grid length r,
5) Parameter K.
OUTPUT: Assignment of colors ci,j ∈ C to each node
ui,j , ∀1 ≤ i ≤ n such that for any region faults of size
r ×r the residual network G0 has at least K distinct colors
and number of nodes colored ρ is minimum.
if K > n2 − r2 OR N < K then
No feasible solution exists
end if
c = 0, ρ = 0, r0 = 0
for l = 0 → 2r − 1 do
for i = d l+1
r e − 1 → l do
j = l − i; r0 = r0 + 1
for p = 0 → d nr e − 1 do
for q = 0 → d nr e − 1 do
x = pr + i + 1 and y = qr + j + 1;
if x ≤ n and y ≤ n then
if Color (c mod C) + 1 has not been used in
any node within (r −1)-hop neighbor of node
(x, y) then
Put color (c mod C) + 1 in node (x, y);
c = c + 1;
else
Put color c in node (x, y);
end if
ρ = ρ + 1;
if N ≥ K + d d n eK2 −1 e then
r
if ρ ≥ K + r0 then
Output ρ;
end if
else
if ρ ≥ 2K − (N − K)(d nr e2 − 2) then
Output ρ;
end if
end if
end if
end for
end for
end for
end for

when ρ = K + r0 . This gives r0 = d d n eK2 −1 e. So total number
r
of nodes colored (ρ) is equal to K + d d n eK2 −1 e which is equal
r
to the optimal when N ≥ K + d d n eK2 −1 e.
r

CASE II (K ≤ N ≤ K + d d n eK2 −1 e): In this case some of
r
the assigned colors have to be reused. This condition can be
divided this case into two subcases
(i) When N = K, in order to satisfy the color constraint, all

81

2014 International Conference on Computing, Networking and Communications (ICNC)
5

(a) 12x12 grid, 4x4 fault region, vary- (b) 20x20 grid, 5x5 fault region, vary- (c) 30x30 grid, 10x10 fault region, (d) 30x30 grid, 5x5 fault region, varying N over K’s
ing K over N’s
varying N over K’s
ing K over N’s

Fig. 2: Experimental results showing impact of coding parameters N and K over storage requirement σ

N distinct colors should be available in the residual network.
In order to ensure that optimal solution will have a copy
of each color assigned to two nodes far enough not to get
destroyed by one region fault. In this case total number of
nodes that will be colored is ρ = 2K. The DDG algorithm (in
step 14-18) ensures that each color is used at most twice and
the nodes with same color do no fall within same region. So
total number of nodes colored by the algorithm in this case is
2K.
(ii) When N − K > 0, then each additional available color
(k = N − K) can be treated as a copy of an already assigned
color as mentioned in previous case. Since there are total d nr e2
non-overlapping regions, each additional color assigned to a
node in one of these regions can be treated as a copy of a
color assigned to a node in rest of (d nr e2 − 1) non-overlapping
regions. So total number of nodes that need to be colored
in this case is ρ = 2K − k(d nr e2 − 1) + k or ρ = 2K −
(N − K)(d nr e2 − 2). Since the algorithm terminates when ρ =
2K − (N − K)(d nr e2 − 2) nodes are colored altogether, in this
case also, the algorithm produces the optimal solution.
A similar technique can also be used to prove the optimality
of the algorithm when n is not a multiple of r. Due to space
constraints, we omit the tedious and lengthy proof.

to observe that the rate of increase in σ as K approached N
was much steeper when K was closer to N , in other words, the
rate of increase in storage cost σ was not uniform, as shown
in Figures 2(b) and 2(d).
VI. C ONCLUSION
We conclude through analysis and experimentation that
coding parameters (N,K) have significant impact on storage
requirement σ.
R EFERENCES
[1] Manage availability of virtual machines. [Online].
Available: http://www.windowsazure.com/en-us/manage/
windows/common-tasks/manage-vm-availability/
[2] A. Sen, S. Murthy, and S. Banerjee, “Region-based
connectivity-a new paradigm for design of fault-tolerant
networks,” in Proceedings of IEEE HPSR, 2009, pp. 1–7.
[3] S. Neumayer and E. Modiano, “Network reliability with
geographically correlated failures,” in INFOCOM, 2010
Proceedings IEEE. IEEE, 2010, pp. 1–9.
[4] S. Banerjee, S. Shirazipourazad, and A. Sen, “Design and
analysis of networks with large components in presence
of region-based faults,” in Proceedings of IEEE ICC,
2011. IEEE, 2011, pp. 1–6.
[5] A. Dimakis, V. Prabhakaran, and K. Ramchandran, “Decentralized erasure codes for distributed networked storage,” IEEE/ACM Transactions on Networking (TON),
vol. 14, no. SI, pp. 2809–2816, 2006.
[6] A. G. Dimakis, K. Ramchandran, Y. Wu, and C. Suh,
“A survey on network codes for distributed storage,”
Proceedings of the IEEE, vol. 99, no. 3, pp. 476–489,
2011.
[7] M. Naor and R. Roth, “Optimal File Sharing in Distributed Networks,” SIAM Journal on Computing, vol. 24,
pp. 158–183, 1995.
[8] A. Jiang and J. Bruck, “Network file storage with
graceful performance degradation,” ACM Transactions
on Storage (TOS), vol. 1, no. 2, pp. 171–189, 2005.
[9] M. Sardari, R. Restrepo, F. Fekri, and E. Soljanin,
“Memory allocation in distributed storage networks,” in
Proceedings IEEE International Symposium on Information Theory (ISIT), 2010, 2010, pp. 1958–1962.
[10] S. Banerjee, S. Shirazipourazad, and A. Sen, “On regionbased fault tolerant design of distributed file storage
in networks,” in INFOCOM, 2012 Proceedings IEEE.
IEEE, 2012, pp. 2806–2810.

V. E XPERIMENTAL R ESULTS AND D ISCUSSIONS
Using the proposed DDG optimal algorithm we now present
extensive simulation results that demonstrate the impact of
the choice of parameters N and K on grid networks. The
experiments were performed on n×n grid networks with n
of size 12, 20, and 30, with 4 different fault region sizes
for each n. For each of the grids, and fault regions chosen,
the experiments were categorized and analyzed in two sets,
namely – (i) varying parameter N keeping K constant, where
K values ranged from 50 to 70 in steps of 5, and (ii) varying
parameter K keeping N constant, where N values ranged from
30 to 70 in steps of 10.
In our experiments we were able to observe that as N
increases from initial value of K, i.e. the ratio between (N , K)
increases, storage cost σ decreases, whereas when K increases,
i.e. the ratio between (N , K) decreases, σ increases. We were
also able to observe that when N = K, σ was 2K confirming
CASE II(i) of Theorem 2. Also, for a given value of K, our
experiments revealed the presence of a threshold value (T ) of
N ≤ n beyond which σ becomes constant for all values of
N ≥ T , as shown in Figures 2(a) and 2(c). We were also able

82

Wireless Networks 7, 565–566, 2001
 2001 Kluwer Academic Publishers. Manufactured in The Netherlands.

Introduction: Discrete Algorithms and Methods for Mobility
AMOTZ BAR-NOY
AT&T Research Labs, 180 Park Ave., Florham Park, NJ 07932, USA

DANNY KRIZANC
Department of Mathematics & Computer Science, Wesleyan University, Middletown, CT 06459, USA

ARUNABHA SEN
Department of Computer Science & Engineering, Arizona State University, Tempe, AZ 85287, USA

Mobility in computing and communications has become a necessity of everyday life. The introduction of mobility
raises many new questions that are often best addressed using techniques developed by algorithmic and combinatorial researchers. Examples of this phenomenon range from coloring algorithms used for frequency allocation in cellular networks
to scheduling algorithms used in satellite communications. In 1997 the first International Workshop on Discrete Algorithms
and Methods for Mobile Computing and Communications (DIAL M for Mobility) was held in association with the ACM
International Conference on Mobile Computing and Networking (MOBICOM) in Budapest, Hungary. This issue contains
papers chosen from the second DIAL M held in Dallas, Texas in 1998 and the third DIAL M held in Seattle, Washington,
in 1999. Together these workshops received more than forty submissions, from which twenty-one papers were selected for
presentation. Out of these twenty-one papers a total of eight were chosen for inclusion in this issue. All eight went through
the rigorous reviewing process required of this journal. The topics of the papers range from channel assignment in cellular
networks to routing in ad hoc wireless networks. What they have in common is the application of discrete algorithmic
methods to their respective domains.
Two of the papers concern the channel assignment problem in cellular networks. In “A Graph Theoretic Approach for
Channel Assignment in Cellular Networks” by Mihaela Iridon, David Matula and Cheng Yang, the channel assignment
problem is modeled using a graph model that includes information concerning overlapping cell segments. The main result
of the paper is a theorem which characterizes precisely when the problem is solvable in this model. They further show that
the simplicity and regularity of the graphs generated by their model make them ideal for further theoretical and simulation
investigations. The second channel assignment paper, “Models and Approximation Algorithms for Channel Assignment in
Radio Networks” by Sven Krumke, Madhav Marathe and S.S. Ravi, considers the channel assignment problem on networks
modeled by graphs with certain geometric structures such as planar graphs, graphs with bounded genus, etc. Even for these
restricted classes of graphs, it is known that the channel assignment problem is NP-hard. For these cases, the authors give
polynomial time approximation algorithms which guarantee a solution to within a constant of the optimal.
Jennifer Walter, Jennifer Welch and Nitin Vaidya in their paper “A Mutual Exclusion Algorithm for Ad Hoc Mobile
Networks” combine techniques from a number of previous approaches to the problem of mutual exclusion, in order to
develop a fault-tolerant mutual exclusion algorithm for an environment where frequent and unpredictable topology changes
occur, such as an ad hoc mobile network. Their experimental results suggest that for such an environment, a dynamic
algorithm that adjusts to the network conditions is a better approach than using a static mutual exclusion algorithm running
on top of an ad hoc routing protocol.
M. Satyanarayanan and D. Narayanan introduce an interesting notion of multi-fidelity algorithms in their paper “MultiFidelity Algorithms for Interactive Mobile Applications”. Their definition of a multi-fidelity algorithm is “a sequence of
computing steps that terminates, yielding a result that falls within a range of acceptable output specifications”. As a measure
of fidelity, they give an example of sorting a sequence of N numbers. If instead of the entire sequence, only the M leading
elements are sorted, then the ratio M/N can be used as a measure of the fidelity of the result.
P. Bose, P. Morin, I. Stojmenovic and J. Urrutia in their paper “Routing with Guaranteed Delivery in Ad Hoc Wireless
Network” consider ad hoc networks modeled as unit disk graphs. They provide distributed algorithms that guarantees packet
delivery at the destination without requiring duplication of the packets or buffers at the nodes. In addition, they provide a
simple distributed protocol for extraction of a planar subgraph of a unit disk graph.
Ravi Prakash in his paper “A Routing Algorithm for Wireless Ad Hoc Networks with Unidirectional Links” points out
that although most routing algorithms for ad hoc networks assume that all wireless links are bidirectional, in reality some
of the links may be unidirectional. The presence of such unidirectional links in an ad hoc network may have a significant
performance consequence for the distance vector routing algorithm. The paper suggests modifications to distance vector
routing algorithms so that it does not suffer from performance degradation in presence of unidirectional links.

566

INTRODUCTION

In the paper “Flooding for Reliable Multicast in Multi-Hop Ad Hoc Networks”, C. Ho, K. Obraczka, G. Tsudik and
K. Viswanath consider mobile networks with fast moving nodes. They show that traditional approaches to multicasting
may not be very effective in such an environment. They suggest the use of flooding for multicasting and through simulation
illustrate its impact on the performance. An interesting observation of their simulation is that in a highly mobile environment
even flooding may not be very effective.
T. Kunz, A. A. Siddiqi and J. Scourias in their paper “The Peril of Evaluating Location Management Proposals through
Simulations” very effectively point out the importance of the mobility model used for the evaluation of various location
management schemes. Using two different mobility models, the activity-based and random, they demonstrate that the
random mobility model may not reflect the relative performance in a deployed system.
Taken together, these papers provide valuable insights into the application of discrete algorithmic techniques to the issues
raised by adding mobility to computing and communications. They also reflect the cooperation between theoreticians and
practitioners engendered by workshops such as DIAL M. We hope that you benefit from these contributions as much as we
benefited from bringing them to you.
Amotz Bar-Noy received the B.Sc. degree in 1981 in mathematics and computer science and the Ph.D. degree in 1987 in computer science, both from
the Hebrew University, Israel. From 1987 to 1989 he was a post-doc fellow in Stanford University, California. From 1989 to 1996 he was a Research
Staff Member with IBM T.J. Watson Research Center, New York. In 1995 he became an Associate Professor with the Electrical Engineering Systems
Department of Tel Aviv University, Israel. Since 1999 he has been with AT&T Research Labs in New Jersey. His main areas of interest are: communication
networks, combinatorial optimization and algorithms, parallel and distributed computing.
E-mail: amotz@research.att.com

Danny Krizanc received his BSc from University of Toronto in 1983 and his PhD from Harvard University in 1988, both degrees in computer science.
He held positions at the Centruum voor Wiskunde en Informatica, Amsterdam, The Netherlands, the University of Rochester, Rochester, New York, and
Carleton University in Ottawa, Canada, before coming to Wesleyan University as an Associate Professor in 1999. His research focus is the design and
analysis of algorithms, especially as applied to distributed computing and networking.
E-mail: dkrizanc@wesleyan.edu

Arunabha (Arun) Sen received his B.E.Tel.E. degree from Jadavpur University, Calcutta, India, and Ph.D. in computer science
from University of South Carolina, USA. Currently he is an Associate Professor in the Department of Computer Science and
Engineering at Arizona State University. He is also the Associate Chairman of the department responsible for graduate programs
and research. His research interest is in the area of optimization problems in telecommunication networks. He has published
extensively in the areas of routing, scheduling and topological design of optical and wireless network.
E-mail: asen@asu.edu

The 31st Annual IEEE International Conference on Computer Communications: Mini-Conference

On Region-based Fault Tolerant Design of
Distributed File Storage in Networks
Sujogya Banerjee, Shahrzad Shirazipourazad and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {sujogya, sshiraz1, asen}@asu.edu

Abstract—Distributed storage of data files in different nodes of
a network enhances the reliability of the data by offering protection against node failure. In the (𝒩 , 𝒦), 𝒩 ≥ 𝒦 file distribution
scheme, from a file 𝐹 of size ∣𝐹 ∣, 𝒩 segments of size ∣𝐹 ∣/𝒦 are
created in such a way that it is possible to reconstruct the entire
file, just by accessing any 𝒦 segments. For the reconstruction
scheme to work it is essential that the 𝒦 segments of the file
are stored in nodes that are connected in the network. However
in case of node failures the network might become disconnected
(i.e., split into several connected components). We focus on node
failures that are spatially-correlated or region-based. Such failures
are often encountered in disaster situations or natural calamities
where only the nodes in the disaster zone are affected. The goal
of this research is to devise a file segment distribution scheme
so that, even if the network becomes disconnected due to any
region fault, at least one of the largest connected components will
have at least 𝒦 distinct file segments with which to reconstruct
the entire file. The distribution scheme will also ensure that
the total storage requirement is minimized. We provide an
optimal solution through Integer Linear Programming and an
approximation solution with a guaranteed performance bound
of 𝑂(ln 𝑛) to solve the problem for any arbitrary network. The
performance of the approximation algorithm is evaluated by
simulation on two real networks.

I. I NTRODUCTION
In order to enhance fault-tolerance, security and load balancing capability, error-correcting codes have been used extensively in data storage systems and server clusters - such
as RAID [1] and DPSS [2]. One well known scheme for this
purpose is the use of (𝒩 , 𝒦) erasure codes [3], [4], [5], [6].
In (𝒩 , 𝒦) maximum distance separable (MDS) erasure code
based storage system 𝒩 coded segments of size ∣𝐹 ∣/𝒦 are
created from the original file 𝐹 and is stored in 𝒩 nodes of the
network (one coded segment per node). The advantage of this
storage scheme is that the original file 𝐹 can be reconstructed
by any user in the network just by retrieving and then decoding
any 𝒦 out of 𝒩 segments. Clearly location of the storage
nodes within the network will have an impact on ease of
retrieval of the segments. The focus of our research is to design
a robust file distribution scheme that takes into account the
network topology - particularly in the scenario when one or
more of the network nodes are unavailable due to failure.
We consider the scenario where failure of nodes (storing
data segments) might disconnect the network. (𝒩 , 𝒦) codes
ensure that as long as 𝒦 file segments survive the file can be

978-1-4673-0775-8/12/$31.00 ©2012 IEEE

reconstructed. However this condition alone is not sufficient
for successful file reconstruction where the network splits up
into two or more connected components and although more
than 𝒦 segments survive, none of the components have more
than 𝒦 − 1 segments. To be able to reconstruct the file, one
has to ensure that at least one of the connected components
has at least 𝒦 segments. The file distribution scheme presented
in this paper utilizes (𝒩 , 𝒦) erasure codes and ensures that
even in the event of a network disconnection due to node
failures, one of the largest connected components will have
at least 𝒦 distinct file segments, with which to reconstruct
the entire file. Since storage of file segments involves cost,
the distribution scheme presented here also ensures that the
total storage required over the network is minimized. In our
model a node represents a user with some storage capacity.
Our rationale for ensuring that a largest connected component
has sufficient number of segments to reconstruct the file, is
that such a capability will benefit the largest number of users.
Our goal of minimization of storage requirement is driven by
the fact that although storage has become less expensive in
recent times, cost of storing large data sets (of the order of
petabytes, exabytes or higher)is still significantly high.
In a network spanning a large geographical area, the faulty
nodes may be spatially correlated (i.e., confined to a region).
Such failures are often encountered in diaster situations, either
natural (earthquake, forest fire, flood or hurricane) or manmade (EMP attack or enemy bomb), where only the nodes
in the disaster zone are affected. A network spanning across
a large area might get disconnected due to such massive
but localized failures. As a consequence, design of a data
distribution scheme robust against such failures is extremely
important. It may be noted that a region may be defined in
terms of the topology or the geometry (i.e., the layout of
the network in a two dimensional plane) of the network. An
example of a region in terms of topology could be subgraph
with a specified diameter of 𝑑. An example of a region in
terms of geometry could be a circular area of radius 𝑟. In this
paper we focus on region-based faults in a geometric setting.
We provide a robust file distribution scheme utilizing (𝒩 , 𝒦)
coding that ensures that even when the network is fractured
into components due to region-based node and link failures,
at least one of the largest component will have at least 𝒦

2806

segments to reconstruct the file. The scheme also minimizes
the total data storage (𝜎) required over the network.
An example in Fig. 1 shows how coding can help in reducing storage requirement for designing robust data distribution
scheme. We use the term “robust” to imply that the distribution
scheme enables the non-faulty nodes of a largest connected
component to reconstruct the entire file after a region-based
fault strikes the network. The example shows distribution of
three uncoded file segments 𝐴, 𝐵 and 𝐶 of a file 𝐹 . Both
the schemes, with and without coding (Fig. 1 (a) and (b)), are
robust against region-based faults. Here a region is defined to
be a subgraph of diameter one. It is assumed that the storage
capacity of each node is one. As shown in Fig. 1 (a), the
uncoded segments 𝐴, 𝐵 and 𝐶 must be stored in at least six
nodes of the network, in order to make it robust against a
region-based fault. However, as shown in Fig. 1(b),
(𝒩 , 𝒦)
⊕ if⊕
coding is⊕
used and an extra coded segment (𝐴 𝐵 𝐶) is
created ( represents an XOR operation), (𝒩 = 4 and 𝒦 = 3
in this example),
it is sufficient to store the data segments 𝐴, 𝐵,
⊗ ⊗
𝐶 and (𝐴 𝐵 𝐶) in at most four nodes of the network, in
order to make it robust against a same size region-based fault.

and use erasure codes to solve the repair problem of node, (i.e
how to replace the data on a failed node). However the problem
under study in this paper is considerably different from the one
in [3], [5]. While [3], [5] focus on the repair problem and study
the trade-off between storage and bandwidth requirement, our
study is directed towards development of a robust scheme that
allows reconstruction of file after a few nodes disappear due
a region-based failure. Moreover, our technique accounts for
the topology of the network while designing such scheme.
Distribution of coded file segments among different nodes in
the network, taking topology of the network into account, has
been studied in [15], [16], [17], [18], [19], [4]. The focus of
this line of research is in developing a file distribution scheme
so that each node in the network can recreate the original
file by accessing 𝒦 file segments from nodes within their 𝑟
hop neighborhoods. Jiang et al. solves this problem optimally
for networks with special structure such as trees [18] or torus
[15]. This paper is along the same line as that of [17] and [18]
and our objective is also to minimize the total storage in the
network. However, there exists a major distinction between our
research and that of [17] and [18]. In [17], [18], [19] authors
solve the problem when there is no fault in the network,
whereas we discuss the file segment distribution scheme on
network of arbitrary topology considering a region-based fault
in the network.
III. P ROBLEM F ORMULATION

Fig. 1. A data storage network with a region-based fault. Fig (a) uses no
coding scheme. Storage used is 6 in this case. Fig (b) uses (𝒩 , 𝒦) coding
scheme and uses only 4 storage units.

The rest of the paper is organized as follows: in section II
we discuss prior research work on related topics; in section III
we provide problem formulation; in section IV, we present an
optimal and a polynomial time approximation solution for any
arbitrary network. In section V we present results and analysis
of our evaluation of two real backbone networks and finally
section VI concludes the paper.
II. R ELATED W ORK
The networking research community in recent times has
seen a heightened level of interest in spatially correlated or
region-based faults in the network [7], [8], [9], [10], [11],
[12], [13]. Although all these studies delve into some aspects
of robustness, to the best of our knowledge none of them
focus of robust and optimal data storage problem in presence
of region-based faults.
Error-correcting codes have been used extensively in enhancing the performance, reliability and fault tolerance capability in data storage systems [3], [5], [6], [14], [15], [16], [17],
[18], [19]. In [3], [5] Dimakis et. al. consider node failures,

In this section we provide a formal statement of the data
distribution problem. A file 𝐹 of length ℳ is split into
𝒦 segments each of length ℳ/𝒦. Using erasure coding
techniques like maximum distance separable (MDS) codes,
Reed-Solomon (RS) codes, etc., these 𝒦 segments can be
encoded to get altogether 𝒩 segments (𝒩 ≥ 𝒦). According
to the erasure coding theory, retrieving any 𝒦 segments out of
𝒩 segments is sufficient to reconstruct the whole file 𝐹 .
The network is represented by a graph 𝐺 = (𝑉, 𝐸) where
𝑉 is the set of 𝑛 nodes in the network and 𝐸 is the set of
links between them. Each node has a storage capacity for
storing the file segments. For the file 𝐹 , a node 𝑣𝑖 ∈ 𝑉 can
store maximum of 𝛼𝑖 file segments. Nodes in the network
are assumed to have both storage and processing unit. So any
node can recreate the file 𝐹 by downloading and decoding 𝒦
appropriate coded segments from other nodes. Let the layout
of 𝐺 on a 2-dimensional plane be 𝐿𝐺 = (𝑃, 𝐿) where
𝑃 = {𝑝1 , . . . , 𝑝𝑛 } and 𝐿 = {𝑙1 , . . . , 𝑙𝑚 } are the sets of
points (representing nodes) and straight lines (representing
links) respectively. We consider a region to be a circular area
𝑅 on this plane of radius 𝑟. Also we assume due to a single
region-fault all the nodes and links confined in the region are
destroyed. Total number of such distinct regions that need to
be considered for a graph layout 𝐿𝐺 on 2-dimensional plane is
𝑡 = 𝑂(𝑛2 ) for wireless networks [7] and 𝑡 = 𝑂(𝑛4 ) for wired
networks [12]. The difference in the number of distinct regions
in wireless and wired network is because of the absence of any
physical links in wireless network. Let ℛ = {𝑅1 , 𝑅2 , . . . , 𝑅𝑡 }
be the set of distinct regions.

2807

Due to a region-fault the residual network might get disconnected. We want to distribute at most 𝒩 encoded segments
of file 𝐹 among the 𝑛 storage locations of the network in
such a way that the largest component of the residual network
has at least 𝒦 distinct segments to reconstruct the file. Let
𝑆𝑖 be the set of nodes in a largest residual component in
network 𝐺 after region 𝑅𝑖 fails, where 𝑖 = 1, . . . , 𝑡. Let
set 𝒮 = {𝑆1 , . . . , 𝑆𝑡 } represents the set of 𝑡 largest residual
components of network 𝐺. Lets say after distribution of the
coded segments , each node 𝑣𝑖 receives 𝜌𝑖 segments, such that
0 ≤ 𝜌𝑖 ≤ 𝛼𝑖 . Since storing each file segments involves a cost,
our objective in this paper is to minimize the total storage used
in the whole network. Let 𝒞 = {1, 2, . . . , 𝒩 } be a set of 𝒩
distinct colors. We formulate the problem as color assignment
problem where each color represents a segment and assigning
a color 𝑐𝑗 , 1 ≤ 𝑗 ≤ 𝒩 to a node 𝑣𝑖 implies that file segment
𝑗 is stored in node 𝑣𝑖 . If the storage capacity of node 𝑣𝑖 is
𝛼𝑖 then at most 𝛼𝑖 colors can be assigned to node 𝑣𝑖 . In this
paper we use the terms color capacity and storage capacity of
a node interchangeably.
Formally the data file segment distribution problem can be
stated as:

Then constraint (1) means node 𝑣𝑖 can be assigned at most
𝛼𝑖 colors. Constraints (2) and (3) together means that each
set 𝑆𝑝 should have at least 𝒦 distinct colors. Then minimizing
∑𝑛 ∑ 𝒩
𝑖
𝑗 𝑥𝑖𝑗 minimizes the total number of colors assigned to
different nodes. In other words, the objective is minimizing
the total storage space used all over the networks. The ILP
will solve the problem in exponential time.
Variables: For each node 𝑣𝑖 and color 𝑐𝑗
{
1, if node 𝑣𝑖 is assigned color 𝑐𝑗
𝑥𝑖𝑗 =
0, otherwise.
For each set 𝑆𝑝 and color 𝑐𝑗
{
1, if set 𝑆𝑝 has at least one node of color 𝑐𝑗
𝑧𝑝𝑗 =
0, otherwise.
min

𝑛 ∑
𝒩
∑

𝑠.𝑡.

𝑁
∑

𝑥𝑖𝑗 ≤ 𝛼𝑖 ,

𝑗=1

Data Distribution Problem (DDP)

𝑧𝑝𝑗 ≤

INSTANCE: Given
(i) a graph 𝐺 = (𝑉, 𝐸) where 𝑉 = {𝑣1 , . . . , 𝑣𝑛 } and 𝐸 =
{𝑒1 , . . . , 𝑒𝑚 } are the sets of nodes and links respectively,
(ii) the layout of 𝐺 on a two dimensional plane 𝐿𝐺 = (𝑃, 𝐿)
where 𝑃 = {𝑝1 , . . . , 𝑝𝑛 } and 𝐿 = {𝑙1 , . . . , 𝑙𝑚 } are the sets of
points and lines on the 2-dimensional plane,
(iii) maximum of 𝛼𝑖 colors can be assigned to a node 𝑣𝑖 ,
(iv) region 𝑅 defined to be a circular area of radius 𝑟,
(v) a set 𝒮 = {𝑆1 , . . . , 𝑆𝑡 } where 𝑆𝑖 ⊆ 𝑉 , 1 ≤ 𝑖 ≤ 𝑡,
(vi) a set of 𝐶 = {1, 2, . . . , 𝒩 } distinct colors and parameter
𝒦.
QUESTION: Assign at most 𝒩 colors to the nodes in 𝑉 in
such a way that (i) 𝜌𝑖 ≤ 𝛼𝑖 , where 𝜌𝑖 is the number of colors
assigned to node 𝑣𝑖 , (ii)
∑𝑛each set 𝑆𝑖 ∈ 𝒮 has at least 𝒦 distinct
colors and (iii) 𝜎 = 𝑖=1 𝜌𝑖 is minimum.
Data distribution problem (DDP) can be shown to be NPcomplete by transforming Hitting Set, a known NP-complete
problem[20], to a special case of DDP. Due to space constraint
we omit the proof.
IV. A LGORITHMS FOR DATA D ISTRIBUTION P ROBLEMS
In this section we an optimal algorithm solution and a polynomial time approximation algorithm for the data distribution
problem in any arbitrary network.
A. Optimal solution for DDP in arbitrary networks
We formulate an integer linear program to solve the DDP
optimally. For each node 𝑣𝑖 ∈ 𝑉 and each color 𝑐𝑗 ∈ 𝐶 we
use a variable 𝑥𝑖𝑗 . 𝑥𝑖𝑗 is 1 if and only if the node 𝑣𝑖 is given
the color 𝑐𝑗 . Similarly, for each set 𝑆𝑝 ∈ 𝒮 and each color
𝑐𝑗 ∈ 𝐶 we have a variable 𝑧𝑝𝑗 . The variable 𝑧𝑝𝑗 is 1 if and
only if the set 𝑆𝑝 has at least one node assigned color 𝑐𝑗 .

𝑥𝑖𝑗

𝑖=1 𝑗=1

∑

∀𝑖 = 1, ⋅ ⋅ ⋅ , 𝑛

𝑥𝑖𝑗 ,

(1)

∀𝑝 = 1, . . . , 𝑡, 𝑗 = 1, . . . , 𝒩 (2)

𝑖:𝑣𝑖 ∈𝑆𝑝
𝒩
∑

𝑧𝑝𝑗 ≥ 𝒦,

∀𝑝 = 1, . . . , 𝑡

(3)

𝑗=1

𝑥𝑖𝑗 ∈ {0, 1}, 𝑧𝑝𝑗 ∈ {0, 1},

∀𝑥𝑖𝑗 and 𝑧𝑝𝑗

(4)

B. Approximation Solution for DDP in arbitrary networks
Next we present an approximation (Algorithm 1) (DDA) to
solve DDP in polynomial time for any arbitrary network. Step
1 and step 2 of DDA computes all the distinct regions of radius
𝑟 on the graph layout 𝐿𝐺 and finds the largest component
of the residual graph for each region fault. In [7], [12] it is
shown that number of such distinct circular regions (𝑡) in a
geographical network is bounded by a polynomial function of
𝑛 and can be computed in 𝑂(𝑛6 ). Set 𝐶𝑖 maintains the list
of colors assigned to node 𝑣𝑖 and 𝛼𝑖 is the maximum color
capacity of node 𝑣𝑖 . 𝑘𝑗 represents the the number of distinct
colors required for set 𝑆𝑗 . Initially, 𝐶𝑖 for all nodes 𝑣𝑖 ∈ 𝑉
is empty and 𝑘𝑗 is 𝒦 for all sets 𝑆𝑗 ∈ 𝒮. The value of 𝒩 is
taken to be greater than total storage
available in the largest
∑
component 𝑆𝑗 , i.e., 𝒩 = max𝑗 𝑣𝑖 ∈𝑆𝑗 𝛼𝑖 .
In each iteration, the while-loop from step (7-18) assigns
a new color to a node. 𝒯 is a set of integers 𝐻𝑖 , where 𝐻𝑖
denotes the number of times 𝑣𝑖 appears in all the remaining
sets in 𝒮. A high value of 𝐻𝑖 means 𝑣𝑖 is a good candidate
for assigning a new color. In each iteration, step 11 picks up
the node 𝑣𝑝 for which value of 𝐻𝑝 is maximum among all the
nodes in 𝒯 with at least unit color capacity left. The for-loop
from step 13-16 runs over all the sets 𝑆𝑗 that contains 𝑣𝑝 .
Step 14 chooses the set of colors 𝐷𝑗 which has not been
assigned to any node in 𝑆𝑗 . Step 17∩chooses the color 𝑐
which is minimum over all the colors 𝑆𝑗 :𝑣𝑝 ∈𝑆𝑗 𝐷𝑗 selected

2808

within the for-loop and assigns 𝑐 to the node 𝑣𝑝 . Note that this
intersection will be non-empty
because total number of colors
∑
available is
𝒩
=
max
𝑗
𝑣𝑖 ∈𝑆𝑗 𝛼𝑖 . Also choosing the lowest
∩
color 𝑐 ∈ 𝑆𝑗 :𝑣𝑝 ∈𝑆𝑗 𝐷𝑗 ensures that all the sets 𝑆𝑗 considered
in the for-loop get a new distinct color. So the value of 𝑘𝑗 is
decreased in step 15 for all 𝑆𝑗 ∋ 𝑣𝑝 . If in this process any
𝑆𝑗 satisfies its all color requirement it is removed from 𝒮.
Step 18 reduces the maximum color capacity of node 𝑣𝑝 by
1. If at any iteration 𝛼𝑝 = 0, ∀𝑣𝑝 ∈ 𝑉 , the step 9 implies
that all the nodes have been assigned colors up to its capacity.
However there are still some sets left 𝑆𝑗 ∈ 𝒮 which has not
satisfied the coloring constraints of having 𝒦 distinct colors.
It implies no feasible solution is possible for that instance and
the algorithm terminates. At the end, steps from 20-24 perform
deletion of the any redundant assigned colors. The double forloop iterates through the list of the colors assigned to the nodes
in reverse order and deletes a color from a node if it turns out
that removing the color from that node still produces a feasible
solution.
Theorem 1: The time complexity of DDA is 𝑂(𝑛6 ) .
Proof: In step 1 and 2 of Algorithm 1, the largest components of all the distinct regions for the layout 𝐿𝐺 of the graph
𝐺 can be computed in 𝑂(𝑛6 ) time using the method stated in
[12]. Number of distinct regions 𝑡 in wired network is 𝑂(𝑛4 )
[12]. The inner for-loop satisfies the color requirement for at
least one set 𝑆𝑗 in worst case and reduces 𝑘𝑗 by 1. The outer
while-loop terminates when color requirements of all the sets
are satisfied. So in worst case the outer while-loop will run
two for-loops in the reverse
for 𝑡𝒦 times which is 𝑂(𝑛4 ). The∑
𝑛
delete steps (20-24) will run for 𝑖 𝜌𝑖 times which is 𝑂(𝑛).
For checking feasibility of the solution one will have to go
through all the sets 𝑆𝑗 in 𝒮. So it will take 𝑂(𝑡) or 𝑂(𝑛4 )
time. So altogether the complexity of the algorithm is 𝑂(𝑛6 ).
Theorem 2: Approximate solution produced by DDA is at
most 𝑂(ln(𝐾) + 4 ln(𝑛)) times the optimal solution.
Proof: The outer while-loop (step 7-18) of algorithm DDA
returns a set of nodes 𝐴 ⊆ 𝑉 and a set of colors 𝐶𝑖 assigned
to every node 𝑣𝑖 ∈ 𝐴. Since total number 𝒩 distinct colors
available
in color set 𝒞 is greater than total color capacity
∑
𝑣∈𝑆𝑖 𝛼𝑣 , ∀𝑆𝑖 ∈ 𝒮, in worst case all the colors assigned to
the nodes in 𝐴 will be distinct. Assigning a color to a node
is equivalent to selecting a storage element from that node.
So at the end of the while-loop the algorithm will return a set
of storage elements 𝒰 = {𝑢𝑖𝑗 ∣∀𝑣𝑖 ∈ 𝐴, 1 ≤ 𝑗 ≤ 𝜌𝑖 } such
that each 𝑆𝑗 , ∀1 ≤ 𝑗 ≤ 𝑡, has at least 𝒦 elements from 𝒰 .
This can be viewed as if there are 𝒦 copies of set 𝑆𝑗 given
as 𝑆𝑗𝑘 , 1 ≤ 𝑘 ≤ 𝒦 and set 𝒰 hits these sets at least once. So
altogether there are 𝜆 = 𝒦∣𝒮∣ = 𝒦𝑡 sets are hit by 𝒰 . Since
the algorithm follows the greedy approach of general hitting
set problem, the final approximation ratio of DDA is 𝑂(𝒦𝑡),
i.e., 𝑂(ln(𝐾) + 4 ln(𝑛)) [21].
V. E XPERIMENTAL R ESULTS AND D ISCUSSIONS
In this section we present extensive simulation results that
demonstrate the efficacy of the proposed approximation algorithm DDA by comparing it’s solution to the optimal number

Algorithm 1: Data Distribution Algorithm (DDA)

1
2
3
4
5
6
7
8

9
10
11
12
13
14
15
16
17
18
19

20
21
22
23

Input :
1) The layout of graph 𝐺 = (𝑉, 𝐸) on a two dimensional plane
𝐿𝐺 = (𝑃, 𝐿),
2) Maximum 𝛼𝑖 colors can be assigned to 𝑣𝑖 , ∀𝑣𝑖 ∈ 𝑉 ,
3) Color set 𝒞 = {𝑐𝑖 , . . . , 𝑐𝒩 },
4) Region radius 𝑟,
∑
5) Parameter 𝒦 and 𝒩 = max𝑖 𝑣∈𝑆𝑖 𝛼𝑖 .
Output: Assignment of colors 𝐶𝑖 = {𝑐1 , . . . , 𝑐𝜌𝑖 } to each
node 𝑣𝑖 , ∀1 ≤ 𝑖 ≤ 𝑛 such that 𝐶𝑖 ⊆ 𝒞,
∣𝐶𝑖 ∣ = 𝜌𝑖 ≤ 𝛼𝑖 , for any region faults of radius 𝑟 the
largest component of the residual
graph 𝐺′ has at least
∑
𝒦 distinct colors and 𝜎 = 𝑛
𝜌
𝑖 𝑖 is minimum.
Compute all the regions ℛ = {𝑅1 , . . . , 𝑅𝑡 } of radius 𝑟 in 𝐿𝐺
using the method described in [12];
Find the largest component 𝑆𝑖 of the residual graph for each
region fault 𝑅𝑖 ∈ ℛ. Let 𝒮 = {𝑆1 , . . . , 𝑆𝑡 };
𝒬 = {𝛼𝑖 , . . . , 𝛼𝑛 };
𝐴 ← ∅;
𝐶𝑖 ← ∅, ∀𝑖 = 1, . . . , 𝑛;
𝑘𝑗 = 𝒦, ∀𝑗 = 1, . . . , 𝑡;
while 𝒮 ∕= ∅ do
Compute 𝒯 = {𝐻1 , . . . , 𝐻𝑛 } for all 1 ≤ 𝑖 ≤ 𝑛, where 𝐻𝑖
is the total number of times node 𝑣𝑖 appears in the
remaining sets of 𝒮;
if 𝛼𝑝 = 0 for all 𝑣𝑝 ∈ 𝑉 then
No feasible solution exist; return;
Pick 𝑣𝑝 ∈ 𝑉 with 𝛼𝑝 ∕= 0 such that corresponding 𝐻𝑝 is
maximum in 𝒯 ;
𝐴 ← 𝐴 ∪ 𝑣𝑝 ;
forall 𝑆𝑗 such that 𝑣𝑝 ∈ 𝑆𝑗 do
Let 𝐷𝑗 ⊆ 𝒞 be the set of colors available in set 𝑆𝑗
such that 𝐷𝑗 ∩ 𝐶𝑖 = ∅ for all 𝑣𝑖 ∈ 𝑆𝑗 ;
𝑘𝑗 ← 𝑘𝑗 − 1;
if 𝑘𝑗 = 0 then 𝒮 ← 𝒮 ∖ 𝑆𝑗 ;
∩
Let 𝑐 be the minimum color in 𝑆𝑗 :𝑣𝑝 ∈𝑆𝑗 𝐷𝑗 , then
𝐶𝑝 ← 𝐶𝑝 ∪ 𝑐;
𝛼𝑝 ← 𝛼𝑝 − 1;
Let nodes in 𝐴 are added in the order {𝑣 1 , 𝑣 2 , . . . , 𝑣 ∣𝐴∣ } and
𝜌𝑖 colors have been assigned to node 𝑣 𝑖 in the order
𝐶𝑖 = {𝑐1 , 𝑐2 , . . . , 𝑐𝜌𝑖 }
for 𝑖 = ∣𝐴∣ to 1 do
for 𝑗 = 𝜌𝑖 to 1 do
if 𝐶𝑖 ∖ 𝑐𝑗 gives a feasible solution then
𝐶 𝑖 ← 𝐶 𝑖 ∖ 𝑐𝑗

of storage required in the network. All the experiments are
performed on two real fiber backbone networks of a major
network provider [22]: (i) USA network (147 nodes) and (ii)
Europe network (46 nodes). The (𝑥, 𝑦) coordinate of a node
on the network layout is taken to be the latitude and longitude
of the corresponding city in the map. A fiber link between two
cities in the network map is taken as a straight line between
the corresponding nodes in the network layout. All distance
units in this section are in latitude and longitude coordinates
(i.e, one unit is approximately 60 miles).
We consider two parameters that impact the comparison
results - (𝑖) radius of the circular fault region 𝑟, and (𝑖𝑖)
the number of file segments required to reconstruct the file

2809

Number of storage required in network

(𝒦). In all the experiments the storage capacity of each node
(measured in terms of number of segments it can store) is
taken to be a random value between 1 and 𝒦/2. The optimal
solutions were obtained by solving Integer Linear Programs
using CPLEX Optimizer 10.0. In each set of experiments the
solution of the approximation algorithm is compared with the
optimal solution.
In the first set of experiments, we study the impact of change
in the radius 𝑟 of circular fault region on the total storage
requirement, keeping 𝒦, 𝒩 and storage capacity of each node
constant. In these experiments, 𝒦 = 10 and 𝒩 = 20 for both
the networks. The radius 𝑟 is varied from 0.5 unit to 2.5 unit
in steps of 0.5 (i.e, from 30 miles to 150 miles). Figure 2
shows the result. In most of the cases DDA solution is equal
the optimal solution and in other cases the difference between
the two is less than 2. While the ILP takes several hours to
find the optimal solution when 𝑟 is large DDA takes only a
few seconds to find a solution.
In the second set of experiments, we study the impact of
change in the value of 𝒦 on the total storage requirement,
keeping 𝑟 and 𝒩 constant. The radius 𝑟 is set to 1.5 unit (90
miles) and 𝒩 is set to 20 for both the networks. The parameter
𝒦 is varied from 2 to 10 in steps of 2. Figure 3 shows the
result. In all cases the DDA solution matches exactly with the
optimal solution. The increase in the storage requirement in
the network with increase in 𝒦 is quite intuitive. Higher values
of 𝒦 implies that a node requires a larger number of distinct
file segments to reconstruct the file. This requirement results
in higher storage need for the file segments.
24
22

Heuristic Solution (US Network)
Opt Solution (US Network)
Heuristic Solution (Europe Network)
Opt Solution (Europe Network)

20
18
16
14
12
10
0

0.5

1

1.5

2

2.5

3

Region radius (r)

Number of Storage required in the network

Fig. 2. Network storage vs Region Radius in a US and Europe fiber network
when the value of 𝒦 = 10

22
20
18
16

Heuristic Sol (Europe Network)
Opt Sol (Europe network)
Heuristic Sol (US network)
Opt Sol (US network)

14
12
10
8
6
4
2
0

2

4

6

8

10

K

Fig. 3. Network storage vs 𝒦 in a US and Europe fiber network when the
value of 𝑟 = 1.5 units

VI. C ONCLUSION
Utilizing (𝒩 , 𝒦) erasure coding, we have presented a file
distribution scheme for a data storage network which is robust
against region-based faults. It ensures that at least one largest
component of the residual network has at least 𝒦 file segments
to reconstruct the file. We have presented an approximation
algorithm for the problem for any arbitrary network.
ACKNOWLEDGEMENT
The authors are thankful to Dr. Salim El Rouayheb for
helpful discussion.
R EFERENCES
[1] D. Patterson, G. Gibson, and R. Katz, “A case for redundant arrays of
inexpensive disks (RAID),” in Proceedings of ACM SIGMOD International conference on Management of data, 1988, pp. 109–116.
[2] Q. Malluhi and W. Johnston, “Coding for high availability of a
distributed-parallel storage system,” IEEE Transactions on Parallel and
Distributed Systems, vol. 9, no. 12, pp. 1237–1252, 1998.
[3] A. Dimakis, V. Prabhakaran, and K. Ramchandran, “Decentralized erasure codes for distributed networked storage,” IEEE/ACM Transactions
on Networking (TON), vol. 14, no. SI, pp. 2809–2816, 2006.
[4] A. Jiang and J. Bruck, “Diversity coloring for distributed storage in
mobile networks,” Tech. Rep., 2001.
[5] A. Dimakis, K. Ramchandran, Y. Wu, and C. Suh, “A Survey on
Network Codes for Distributed Storage,” Proceedings of the IEEE,
vol. 99, no. 3, 2011.
[6] S. Pawar, S. El Rouayheb, and K. Ramchandran, “On secure distributed
data storage under repair dynamics,” in Information Theory Proceedings
(ISIT), 2010 IEEE International Symposium on, 2010, pp. 2543–2547.
[7] A. Sen, B. Shen, L. Zhou, and B. Hao, “Fault-tolerance in sensor
networks: A new evaluation metric,” in Proceedings of IEEE INFOCOM,
2006.
[8] S. Neumayer, G. Zussman, R. Cohen, and E. Modiano, “Assessing the
vulnerability of the fiber infrastructure to disasters,” in Proceedings of
IEEE INFOCOM, 2008, pp. 1566–1574.
[9] A. Sen, S. Murthy, and S. Banerjee, “Region-based connectivity-a new
paradigm for design of fault-tolerant networks,” in Proceedings of IEEE
HPSR, 2009, 2009, pp. 1–7.
[10] S. Neumayer and E. Modiano, “Network reliability with geographically
correlated failures,” in Proceedings of IEEE INFOCOM, 2010.
[11] P. Agarwal, A. Efrat, S. Ganjugunte, D. Hay, S. Sankararaman, and
G. Zussman, “The resilience of wdm networks to probabilistic geographical failures,” in Proceedings of IEEE INFOCOM, 2011.
[12] S. Banerjee, S. Shirazipourazad, P. Ghosh, and A. Sen, “Beyond connectivity - new metrics to evaluate robustness of networks,” in Proceedings
of IEEE HPSR, 2011.
[13] S. Banerjee, S. Shirazipourazad, and A. Sen, “Design and analysis of
networks with large components in presence of region-based faults,” in
Proceedings of IEEE ICC, 2011.
[14] S. El Rouayheb and K. Ramchandran, “Fractional repetition codes for
repair in distributed storage systems,” in Communication, Control, and
Computing (Allerton), 2010 48th Annual Allerton Conference on.
[15] A. Jiang, M. Cook, and J. Bruck, “Optimal t-interleaving on tori,” in
Proceedings. International Symposium on Information Theory, 2004.
ISIT 2004., 2004, p. 22.
[16] M. Sardari, R. Restrepo, F. Fekri, and E. Soljanin, “Memory allocation
in distributed storage networks,” in Proceedings IEEE International
Symposium on Information Theory (ISIT), 2010, 2010, pp. 1958–1962.
[17] M. Naor and R. Roth, “Optimal File Sharing in Distributed Networks,”
SIAM Journal on Computing, vol. 24, pp. 158–183, 1995.
[18] A. Jiang and J. Bruck, “Network file storage with graceful performance
degradation,” ACM Transactions on Storage (TOS), vol. 1, no. 2, pp.
171–189, 2005.
[19] ——, “Memory allocation in information storage networks,” in Proceedings. IEEE International Symposium on Information Theory.
[20] M. Garey and D. Johnson, Computers and Intractability: A Guide to the
Theory of NP-completeness, 1979.
[21] V. Vazirani, Approximation algorithms. Springer Verlag, 2001.
[22] Level 3 Communications, Network Map. [Online]. Available: http://www.level3.com/Resource-Library/Maps/Level-3-NetworkMap.aspx

2810

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

Design and Analysis of Networks with Large
Components in Presence of Region-Based Faults
Sujogya Banerjee, Shahrzad Shirazipourazad, and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {sujogya, sshiraz1, asen}@asu.edu

Abstract—Connectivity κ(G) of a network G is traditionally
considered to be the primary metric for evaluation of its faulttolerance capability. However, connectivity as a metric has several
limitations - e.g., it has no mechanism to distinguish between
localized and random faults. Also it does not provide any
information about the network state, if the number of failures
exceed κ(G). The network state information that might be of
interest in such a scenario is the size of the largest connected
component. In this paper, we address both these limitations and
introduce a new metric called region-based largest component size
(RBLCS), that provides the largest size of the component in which
the network decomposes once all the nodes of a region fail. We
study the computational complexity of finding RBLCS for a given
network. In addition, we study the problem of least cost design
of a network with a target value of RBLCS. We prove that the
optimal design problem is NP-complete and present a heuristic
to solve the problem. We evaluate our heuristic by comparing
its solutions with the optimal solutions. Experimental results
demonstrate that our heuristic produces near optimal solution
in a fraction of time needed to find the optimal.

I. I NTRODUCTION
The node/link connectivity κ(G) is traditionally considered
to be the primary metric for evaluation of the fault-tolerance
capability of both wired and wireless networks. However connectivity metric has two major limitations: (i) it only considers
node/link failures that are random in nature, i.e, the probability
of a node or link failing is independent of its location in the
deployment area (this is true because connectivity has no way
of capturing the notion of the locality or spatial correlation
of the faults), (ii) the connectivity metric fails to provide any
information about network state (i.e, the number and size of
the components) once the number of failures exceed κ(G).
Most of the traditional studies of fault tolerance in wired and
wireless networks [1], [2] assume that the node/link failures
are random in nature. However, the assumption of random
node/link failure is not valid in many scenarios. This is particularly true in a military environment, where an enemy bomb
can destroy a large number of nodes confined in a limited
area. This situation is shown in Fig. 1(a) where the shaded part
indicates the fault region. The networking research community
in recent past has shown considerable interest in studying
localized, i.e., spatially correlated or region-based faults in
both wired and wireless networks [3]–[9]. The limitation (i)
of connectivity metric can be overcome by utilizing the metric
region-based connectivity introduced in [5], [8]. A region may

be defined either with reference to the network graph (i.e, the
topological relationship between the nodes) or with reference
to the network geometry (i.e, layout of the nodes and links
in a geographical area). There may be many different ways
of defining a region with respect to a network graph. For
example, a region in a network graph G = (V, E) may be
defined as (i) a subgraph of G with diameter d (the maximum
of the shortest path distance between a pair of nodes, taken
over all source-destination node pairs), or (ii) a subgraph of
G, centered in some v ∈ V and radius r. With reference to
network geometry, a region may be defined as a circular area
in the network layout covering a set of nodes and links.
We elaborate limitation (ii) of connectivity metric with
an example shown in Fig. 1(b). It can be noted that the
connectivity of both the linear array and the star networks
is 1. Therefore no distinction between these networks can be
made regarding their robustness (or fault-tolerance capability)
using connectivity as the metric. However, the following observations can be made regarding the state of these two networks
after failure of one node: (i) the linear array network can break
up into at most two components and the size of at least one
component will be ≥ n/2, (ii) the star network can break
up into (n − 1) components and the size of these components
can be as small as 1, where n denotes the number of nodes
in the network. In the unfortunate event of a network being
disconnected after a failure, it is certainly desirable to have
a few, large connected components than a large number of
small connected components. From operational point of view,
a linear array network will certainly be preferable to a star
network as it offers the possibility of a graceful performance
degradation instead of a catastrophic failure.
This motivates us to introduce a new metric for network
fault tolerance called region-based largest component size
(RBLCS). It is formally defined as follows:
Definition : Suppose {R1 , . . . , Rk } is the set of all possible
regions of a graph G. Consider a k-dimensional vector CL
whose i-th entry, CL [i], indicates the size of the largest connected component in which G decomposes when all nodes in
Ri fails. Then, region-based largest component size (RBLCS)
γR (G) of graph G with region R is defined as
γR (G) = min CL [i]
1≤i≤k

In order to have graceful degradation in performance, we

U.S. Government work not protected by U.S. copyright

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

may want to design networks with a high value of γR (G). The
giant or (largest) connected component of a random graph in
fault free environment has been studied in [10], [11]. In this
paper we are considering giant components for any arbitrary
graph when its layout in a 2-dimensional plane is given and
faults are confined to a region. The contributions of the paper
are as follows:
• We introduce a new metric to capture network state where
the traditional metric connectivity is inadequate.
• We provide a polynomial time algorithm for computing
region-based largest component size (RBLCS)
• We consider a network design problem with a target value
of RBLCS and show that the problem is NP-complete.
• We provide a heuristic for the design problem .
• We conduct the experimental evaluation of the heuristic
and show that it produces near-optimal solutions in a
fraction of time needed to find the optimal solution.

(a)

(b)

a region (i.e., a circular area in the plane) correspond to a
set of points in the plane. We say that region R intersects
or covers line li , if R ∩ li = ∅. Although there could be an
infinite number of circular regions in the plane, for the purpose
of computation of RBLCS, we only need to consider a finite
number of them. Two regions are said to be indistinguishable
if they cover the same set of links and nodes. Otherwise, they
are distinguishable or distinct. For computing RBLCS, we only
need to evaluate the distinct regions. Since there are n nodes
and m links in the network, there could be at most 2n+m
distinct regions. We will show next that the number of distinct
regions that needs to be considered is bounded by a polynomial
function of n and m. Two indistinguishable regions are shown
in Fig. 2.

Fig. 2.

Region 1 and Region 2 are indistinguishable

Fig. 1. (a) Network with a Fault Region, (b) Linear Array and Star Network

II. N ETWORK ROBUSTNESS A NALYSIS
In this section, we provide an algorithm that computes the
RBLCS of a graph G = (V, E) when its layout in a plane is
given as input and the region R is defined to be a circular area
with radius r. The algorithm computes γR (G) in O(n6 ) time,
where |V | = n. The inputs to the algorithm are the following:
(i) a graph G = (V, E) where V = {v1 , . . . , vn } and E =
{e1 , . . . , em } are the sets of nodes and links respectively, (ii)
the layout of G on a 2-dimensional plane LG = (P, L) where
P = {p1 , . . . , pn } and L = {l1 , . . . , lm } are the sets of points
and straight lines on the 2-dimensional plane (note: (a) there is
a one-to-one correspondence between the nodes and points in
V and P , (b) a one-to-one correspondence between the edges
and lines in E and L, (c) each li , 1 ≤ i ≤ m connects two
points pj and pk in P and does not pass through a third point
pq ), (iii) a region is defined as a circular area R of radius r.
A. Ideas behind the RBLCS Algorithm
Before we present the algorithm, we make a few observations on which the algorithm is based. Our algorithm is valid
for both wired and wireless networks. In a wired network,
a physical link connects two nodes. If a node is destroyed
due to failure of a region, all links incident on that node are
also destroyed. However, it is possible that failure of a region
destroys a link without destroying the nodes at its end points.
In a wireless network, there is no physical link, and as such the
possibility of a fault destroying a link does not arise. There
could potentially be infinite number of circular regions that
covers the 2-dimensional plane where the nodes and links are
deployed. It may be noted that a node corresponds to a point
in this plane and a link (i.e., a straight line in the plane) and

Fig. 3.

Vulnerability zone of a link and a node

Definition - Node Vulnerability Zone (NVZ): The circular area
of radius r centered at the location of a node in the network
layout is defined as the NVZ (see Fig. 3(ii)). Any region fault
occurring in this area will destroy the node.
Definition - Link Vulnerability Zone (LVZ): Let lk be a line
of length Lk in network layout corresponding to link eij in
the graph. The rectangular area of length Lk and width 2r (as
shown in Fig. 3(i)) is defined as the LVZ for this link. This
area is called LVZ because if the center of the fault region lies
within this area, it will destroy the link. It may be noted that
a link can also be destroyed if the center of the fault region
lies within the NVZ of a node on which the link is incident.
However, we do not include this area as part of LVZ, as it is
already considered as part of NVZ.
Each node and link vulnerability zone can be represented by
a set of polynomials P = {P1 , . . . , Ps } in R2 with degree(d)
at most 2 [12], [13]. Note that number of such polynomials
s is O(n + m). The arrangement arr(P) [12], [13] of the
polynomials P is the subdivision of the plane into I-points,
arcs and cells, where I-points are the intersection points of the
boundaries of node and link vulnerability zones [5], and the
arcs are the maximally connected portions of the boundaries
between the I-points and cells are the maximally connected

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

Fig. 4.

A Region, intersection of vulnerability zones and a Principal Region

regions bounded by the arcs [12], [13]. The cells can be
described by a constant number of polynomial inequalities of
constant maximum degree d = 2 [12], [13]. In Fig. 4(a) a
region that covers 2 nodes and 2 links (at least partially) is
shown. Fig. 4(b) shows the arrangement of the vulnerability
zones of these set of nodes and the links. A cell is highlighted
in Fig. 4(b).
Definition - C-point: A C-point is an arbitrarily selected point
within a cell in arr(P).
Definition - Principal Regions: Any region centered at a Cpoint will be referred to as a Principal Region (Fig. 4(c)).
Note that Principal Region defined in [5] is a region centered
at an I-point. Considering the regions centered only at the
I-points will not include all distinct regions. For example
consider a network with two nodes n1 and n2 . Let the NVZs of
these nodes intersect at two points I1 and I2 . Regions centered
at I1 and I2 will cover both the nodes. But if we consider
principal regions to be centered at the C-point corresponding
to the three cells made by the NVZs then there will be 3
Principal regions - one covering n1 , one covering n2 and other
covering both the nodes. So Principal regions considered in
this paper is a superset of the Principal regions considered
in [5]. Also Principal regions centered at the C-points is the
actual set of all possible distinct regions in the network. The
previous definition of Principal Region in [5] worked because
only limited number of distinct regions needed to be examined
for computing region-based connectivity.
Observation 1: Given a region R, the intersection area of the
vulnerability zones of the nodes and links within region R is
non-empty [5].
Observation 2: If a region R covers a set of nodes and links
and R is not centered at one of the C-points, there must be
at least one other region centered at one of the C-points that
covers all the nodes and links covered by R. Accordingly this
region will be indistinguishable from R. The proof follows the
proof of Observation 2 in [5].
Observation 3: For computing the RBCDN/RBSCS/RBLCS of
the network graph G where the layout LG of G is given as
input, only a limited number of distinct regions, i.e., only the
Principal Regions need to be examined [5].
Observation 4: The maximum number of Principal Regions is

O(n4 ).
Proof: By definition, a Principal Region is a region centered
at a C-point and number of C-points is equal to the number
of cells in the arrangement arr(P). As the maximum degree
of the set of polynomials P = {P1 , . . . , Ps } defined over R2
is 2 [12], [13], the number of cells in arr(P) is O(s2 ). Since
s is O(m), there can be at most O(m2 ) or O(n4 ) Principal
Regions.
Observation 5: All the C-points can be computed in O(n6 )
time.
Proof: Using the results presented in [12], [13], we can
compute a set of points C such that each cell in arr(P)
contains at least one point from C, in time O(s3 ) (or O(m3 )).
As a consequence the overall time-complexity to compute all
the C-points is O(m3 ) (or O(n6 )).
B. The RBLCS Algorithm
From the observations above, it is clear that we need to
examine only the Principal Regions (i.e., the regions whose
centers are at C-points) to compute the RBLCS of a network
G (with a layout LG on a 2-dimensional plane) and a circular
region R of radius r. Since there are only O(n4 ) of such
regions, we can develop a polynomial time algorithm to
compute RBLCS.

Algorithm 1 Algorithm for Computing RBLCS (γR (G)) of a
network graph G = (V, E) with region R
1: INPUT: G = (V, E), Graph layout LG = (P, L) and r
2: OUTPUT: γR (G)
3: Find the set of C-points using the algorithm sketched in
Observation 5
4: For each C-point rj , find Gj = (Vj , Ej ) a subgraph of
G formed by removing the nodes and edges covered by
region Rj centered at rj
5: For each such graph Gj = (Vj , Ej ) find the largest
connected component LCj using depth-first search [14].
Let LCSj be the size of LCj For graph Gj = (Vj , Ej )
6: γR (G) = minj LCSj
Theorem 1: The complexity of the Algorithm for computing RBLCS is O(n6 ).

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

Proof: As mentioned in Observation 5, time complexity of
finding all the C-points is of O(n6 ), where n is number of
nodes. So, time complexity of Step 3 is O(n6 ). In Step 2,
we have to check all edges in E and all nodes in V if they
have intersections with the fault region Rj . So, complexity of
Step 4 is O(n2 ). Step 5 uses depth-first search to compute
the number of connected components and has complexity of
O(| V | + | E |) = O(n2 ). Step 5 is repeated O(n4 ) of times.
Therefore, the complexity of the Algorithm is O(n6 ).
III. ROBUST N ETWORK D ESIGN
In the previous section we presented a polynomial time
algorithm to compute the RBLCS of a graph G = (V, E),
when its layout LG = (P, L) on a 2-dimensional plane and
the radius r of a circular region R is provided as input. In this
section, we study a complementary problem, where the goal
is to have the least cost augmentation of an existing network,
so that it attains a specific target value of RBLCS. Formal
description of the decision version of this problem is given
below.

RBLCS Augmentation Problem (RBLCS-AP )
INSTANCE: Given
(i) a graph G = (V, E) where V = {v1 , . . . , vn } and E =
{e1 , . . . , em } are the sets of nodes and links respectively,
(ii) the layout of G on a two dimensional plane LG = (P, L)
where P = {p1 , . . . , pn } and L = {l1 , . . . , lm } are the sets of
points and lines on the 2-dimensional plane,
(iii) region R defined to be a circular area of radius r,
(iv) cost function c(e) ∈ Z+ , ∀e ∈ Ē, where Ē is complement
of the link set E (i.e., Ē is comprised of the links not present
in E, but can be added to the graph G = (V, E)),
(v) integers C and K (K ≤ n).
QUESTION: Is it possible to increase the RBLCS of G by K
by adding edges to G (from the set Ē) so that the total cost
of the added links is at most C?
A. NP-Completeness Proof of RBLCS-AP
We prove that RBLCS-AP is NP-complete by a transformation from the Hamiltonian Cycle in Planar
Graph Problem (HCPGP) which is known to be NPcomplete [15]. A Hamiltonian Cycle in an undirected graph
G = (V, E) is a simple cycle that includes all the nodes. A
graph is a planar if it can be embedded in a plane by mapping
each node to a unique point in the plane and each edge is a
line connecting its endpoints, so that no two lines meet except
at a common endpoint [15].
Hamiltonian Cycle in Planar Graph Problem
(HCPGP)
INSTANCE: Given an undirected planar graph G = (V, E).
QUESTION: Does G contains a Hamiltonian Cycle?
Theorem 2: RBLCS-AP is NP-complete.
Proof: It is easy to verify whether a set of additional edges of
total cost ≤ C increases the RBLCS of graph G with region
R from γR (G) to γR (G) + K. Therefore RBLCS-AP is in NP.

Fig. 5.

Transformation of a HCPGP instance to a RBLCS-AP instance

From an instance of the HCPGP (a planar graph G =
(V, E)) we create an instance of the RBLCS-AP (the layout
LG = (P  , L ) of a graph G = (V  , E  )) in the following
way. First, we do a straight line embedding of the planar graph
G on a plane so that lines corresponding to links in G do not
intersect each other. Such an embedding can be carried out in
polynomial time [16]. We call this layout LG = (P  , L ).
We create the layout LG = (P  , ∅), by setting P  = P 
and L = ∅. The graph G corresponding to the layout
LG = (P  , L ) is the instance of RBLCS-AP created from
the instance of HCPGP. We define a region R to be circular
area of sufficiently small radius r, such that if a region fails, it
can only destroy (i) a single node with all links incident on it,
or (ii) a single link. Since the created instance of RBLCS-AP
has no links (E  = L = ∅), the RBLCS of G with region R
is 1. We set the parameters C and K of the instance of the
RBLCS-AP to be equal to n and n − 2 respectively. We assign
costs to the links of Ē  in the following way. The cost of a
link c(e) = 1, if e ∈ (E ∩ Ē  ) and c(e) = ∞, if e ∈ (Ē ∩ Ē  ).
If the instance of the HCPGP has a Hamiltonian Cycle, we
can use the set of links that make up the cycle, to augment the
link set E  of the instance G = (V  , E  ) of the RBLCS-AP.
The augmented G , (Gaug ), is now a simple cycle that involves
all the nodes. With the given definition of region R (a small
circle of radius r), only one node can be destroyed when a
region fails. Accordingly RBLCS of Gaug is n − 1. It may be
recalled that RBLCS of G is 1. Accordingly, augmentation of
the link set of G augmented its RBLCS by n − 2. Due to the
specific cost assignment rule of the links, the total cost of link
augmentation is n. Therefore, if the HCPGP instance has a
Hamiltonian Cycle, the RBLCS of the instance of RBLCS-AP
can be augmented by K with augmentation cost ≤ C.
Suppose that it is possible to augment the RBLCS of the
instance of RBLCS-AP by K with augmentation cost being at
most C. This implies that the RBLCS of G can be increased
from 1 to n − 1 (as K = n − 2) when it is augmented with
additional links with total cost at most n (as C = n). In order
for the RBLCS of Gaug to be n − 1, the node connectivity of
Gaug must be at least 2. A n node graph that has the fewest
number of links and yet is 2-connected, is a cycle that includes
all the nodes. As G had no links, this implies at least n links
must have been added to create the augmented graph Gaug .
Given that the cost of a link c(e) = 1, if e ∈ (E ∩ Ē  ) and
c(e) = ∞, if e ∈ (Ē ∩ Ē  ), and total cost of link augmentation
is at most n, it is clear that the links used in augmenting G

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

must be from the set (E ∩ Ē  ). These links are part of the
edge set of the instance of HCPGP. Accordingly, the instance
of HCPGP must have a Hamiltonian Cycle.
IV. A H EURISTIC FOR RBLCS-AP
In this section, we propose a heuristic for RBLCS-AP.The
notations used in the heuristic are as follows:
Gi : The subgraph induced from G = (V, E) by removing
links and the nodes covered by the region Ri .
PE i : The potential link set PE i ⊆ Ē, is the set of links, such
that any link in PE i can be used to connect two components
of Gi .
Ci : The set of connected components of Gi . For each connected component p ∈ Ci let size(p) be the number of nodes
in the component, AE(p) be the set of potential links used to
make the component p and cost(p) be a cost assigned to the
component depending on the set AE(p). Initially, AE(p) is
empty and cost(p) is zero.
LC(i): The largest component of the Gi .
LCS(i): The largest component size of the Gi .
B: The desired largest component size for all regions, i.e.
B = γR (G) + K.
l: The number of regions Ri for which the graph Gi has
LCS(i)
B.
<
l
PE = i=1 PE i .


PE
 : PE ⊆ PE is the output of the Heuristic such that
e∈PE  c(e) is minimum and Gi has at least one component
of size ≥ B, ∀1 ≤ i ≤ l.
ϕ(e): The cost per unit size of the new components (across
all regions) that can be created by adding e.
cpi (e): The composite component resulted by adding the
edge e connecting two components p and q of Gi . Then,
cost(cpi (e)) = (size(p) + size(q)) × ϕ(e) and AE(cpi (e)) =
AE(p) ∪ AE(q) ∪ {e}.
The Heuristic runs through a number of iterations. In each
iteration, an edge e ∈ PE is selected in the following way.
First for each edge e ∈ PE, ϕ(e) is computed,

c(e) + {i:e∈PE i } (cost(pi ) + cost(qi ))

ϕ(e) =
,
{i:e∈PE i } (size(pi ) + size(qi ))
where pi and qi are the two components of Gi that can be
connected by the edge e. In Algorithm 3 we describe how
ϕ(e) can be calculated for each edge e. Then an edge eb with
minimum cost per unit size is selected. For all regions, where
eb ∈ PE i the corresponding set Ci is updated. If eb was connecting two components p and q in Ci , then Ci is updated by
replacing p and q with the combined component cpi (eb ). For
each newly created component cpi (eb ) in the current iteration
we set cost(cpi (eb )) as size(cpi (eb )) × ϕ(eb ). If the updated
LCSi ≥ B then the edges in this new composite component
is added to PE  and the potential edges of this region is not
considered in subsequent iterations. Before continuing to the
next iteration the edges in every remaining PE i , connecting
the same pair of components as eb , are removed. Since after
adding eb these edges connect the nodes within one component
and cannot increase the size of the components of Gi anymore.

Algorithm 2 Heuristic for RBLCS-AP
1: INPUT:
1) The desired RBLCS value (B),
2) A set of all potential edges PE = {e1 , . . . , em },
3) a cost function c(e) associated with edge e ∈ PE,
4) S = {PE 1 , . . . , PE l },
5) Ci : The set of components of Gi ∀i.

2: OUTPUT: PE

3: PE ← ∅
4: while S = ∅ and PE = ∅ do
5:
Find the edge eb ∈ PE with minimum ϕ using Algorithm 3
6:
for all Ri , ∀1 ≤ i ≤ l do
7:
if eb ∈ PE i and connecting two components p and q
in Ci then
8:
Update Ci by replacing the two components p and
q with cpi (eb )
9:
Remove from PE i all the other potential edges
which connect the components p and q
10:
if size(cpi (eb )) > LCSi then
11:
LCSi ← size(cpi (eb )) and LCi ← cpi (eb )
12:
end if
13:
if LCSi ≥ B then
14:
PE  ← PE  ∪ AE(LCi ) and S ← S − {PE i }
15:
end if
16:
end if
17:
end for
18:
PE ← PE − {eb }
19: end while
Algorithm 3 Finding ϕ(e) for the edge e
1: Cost ← c(e), Size ← 0
2: for all Rj , 1 ≤ j ≤ l do
3:
if e ∈ PE j and connects components p and q then
4:
Cost ← Cost + cost(p) + cost(q)
5:
Size ← Size + size(p) + size(q)
6:
end if
7: end for
8: ϕ(e) ← Cost
Size

The algorithm continues until all regions has a component of
size larger than B.
It should be noted that even if after adding all the edges in
PE to the graph G there exists some regions that do not have a
component with size ≥ B then it is infeasible to make RBLCS
≥ B. In the worst case the heuristic will add all the edges in
PE. So it will always find a solution to the RBLCS-AP if a
feasible solution exists.
Theorem 3: The time complexity of the Heuristic is O(n8 ).
Proof: In order to find ϕ(e) for an edge e we have to consider
all the l distinct regions which is O(n4 ). So, finding the edge
with minimum ϕ (line 5) is O(|PE|×n4 ) = O(n6 ). In RBLCSAP |E| = O(n2 ), the maximum number of links that can be
added to the graph G = (V, E). The f or-loop in line 6 is
repeated l times. Also, the while-loop in line 4 is repeated at

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

an algorithm for computing RBLCS for a given network. We
study the problem of designing a network with a target value
of RBLCS and present a heuristic for its solution. Future work
of this study includes possible reduction of time-complexity
O(n8 ) of the heuristic so that the heuristic can be applied
on very large networks. Also the work can be extended by
studying the use of other metrics, like region-based component
decomposition number and region-based smallest component
size, in measuring the robustness of a network in presence of
region-based fault.
R EFERENCES
Fig. 6. Comparison of the solution of the Heuristic with the Optimal by
taking its ratio to the optimal.

most |PE| times. Therefore, the complexity of the Heuristic
will be O(n2 (n6 + n4 )) = O(n8 ).
V. E XPERIMENTAL R ESULTS
In this section we present the experimental results of the
Heuristic proposed in Section IV. We compare the results
of the heuristic with the optimal solution through extensive
simulations. We find the optimal solution through exhaustive
search method. We generate several random instances of
network layout in 2-dimensional plane. The number of nodes,
n, in these instances are varied from 10 to 30 in a step of 5. For
each value of n we create 10 instances where in every instance
the node locations are uniformly and randomly distributed on
a square deployment area of side length 100 units. The edge
between every two nodes is added with probability p = 0.5.
The heuristic finds a solution within a wide range of choices
of p and n. However, when p < 0.5 and n > 30 the optimal
algorithm takes an unacceptably long time (several days) to
complete. This is due to the fact that when p < 0.5 and
n > 30 the set of the potential edges becomes large. Since
the complexity of the optimal algorithm is exponential, with
a large set of potential edges, it takes an unacceptably long
computation time. Since our goal was to compare performance
of the heuristic with the optimal, we restricted our experiments
to p = 0.5 and n < 30.
For every instance, we find the set of R-points, Gi , Ci and
PE i by using the Algorithm for Computing RBLCS presented
in Section II. For each instance we execute the algorithm for
values of K (number by which RBLCS needs to be augmented)
equal to 1 and 2. For n > 20 and p = 0.5, almost all the
instances do not have a feasible solution for values of K ≥ 3.
Therefore we restrict the value of K to be 1 and 2 only. The
results of the experiments are shown in Fig. 6. We plot the
average of the ratios of the cost of the solution of the heuristic
to the optimal cost for each value of n and K. It can be
observed that in all of these cases, the ratios are less than
1.4, indicating the superior quality of the heuristic solution.
The heuristic executes in a small fraction of time needed to
compute the optimal solution in all our experiments.
VI. C ONCLUSION
In this paper we introduced, region-based largest component
size, a new metric of fault tolerance of a network. We presented

[1] K. Eswaran and R. Tarjan, “Augmentation problems,” SIAM Journal on
Computing, vol. 5, p. 653, 1976.
[2] G. Frederickson and J. JaJa, “Approximation algorithms for several
graph augmentation problems,” SIAM Journal on Computing, 1981.
[3] M. Bakkaloglu, J. J. Wylie, C. Wang, and G. R. Ganger, “On correlated
failures in survivable storage systems,” Carnegie Mellon University,
Tech. Rep. CMU-CS- 02-129, 2002.
[4] W. Cui, I. Stoica, R. H. Katz, and Y. H. Katz, “Backup path allocation
based on a correlated link failure probability model in overlay networks,”
in 10th IEEE ICNP, 2002.
[5] A. Sen, B. Shen, L. Zhou, and B. Hao, “Fault-tolerance in sensor
networks: A new evaluation metric,” in Infocom, 2006.
[6] J. Fan, T. Chang, D. Pendarakis, and Z. Liu, “Cost-effective configuration of content resiliency services under correlated failures,” in
Proceedings of the International Conference on DSN, 2006.
[7] S. Neumayer, G. Zussman, R. Cohen, and E. Modiano, “Assessing
the vulnerability of the fiber infrastructure to disasters,” Columbia
University, EE, Tech. Rep, pp. 08–26, 2008.
[8] A. Sen, S. Murthy, and S. Banerjee, “Region-based connectivity: a new
paradigm for design of fault-tolerant networks,” in HPSR, 2009.
[9] S. Neumayer and E. Modiano, “Network reliability with geographically
correlated failures,” in Proceedings of the 29th conference on Information communications. IEEE Press, 2010.
[10] B. Bollobas, Random Graphs. Cambridge Univ Pr, 2001.
[11] M. Molloy and B. Reed, “The size of the giant component of a random
graph with a given degree sequence,” Combin. Probab. Comput, 1998.
[12] A. Vigneron, “Geometric optimization and sums of algebraic functions,”
2009.
[13] S. Basu, R. Pollack, and M. Roy, “On Computing a Set of Points Meeting
Every Cell Defined by a Family of Polynomials on a Variety* 1,” Journal
of Complexity, vol. 13, no. 1, pp. 28–37, 1997.
[14] J. Hopcroft and R. Tarjan, “Efficient algorithms for graph manipulation,”
1971.
[15] M. Garey and D. Johnson, Computers and intractability. A guide to the
theory of NP-completeness., 1979.
[16] H. Fraysseix, J. Pach, and R. Pollack, “How to draw a planar graph on
a grid,” Combinatorica, vol. 10, no. 1, pp. 41–51, 1990.

Progressive Recovery from Failure
in Multi-layered Interdependent Network
Using a New Model of Interdependency
Anisha Mazumder(B) , Chenyang Zhou, Arun Das, and Arunabha Sen
School of Computing, Informatics and Decision System Engineering,
Arizona State University, Tempe, AZ 85287, USA
{anisha.mazumder,czhou24,adas22,asen}@asu.edu

Abstract. A number of models have been proposed to analyze interdependent networks in recent years. However most of the models are unable
to capture the complex interdependencies between such networks. To
overcome the limitations, we have recently proposed a new model. Utilizing this model, we provide techniques for progressive recovery from
failure. The goal of the progressive recovery problem is to maximize
the system utility over the entire duration of the recovery process. We
show that the problem can be solved in polynomial time in some special
cases, whereas for some others, the problem is NP-complete. We provide two approximation algorithms with performance bounds of 2 and 4
respectively. We provide an optimal solution utilizing Integer Linear Programming and a heuristic. We evaluate the eﬃcacy of our heuristic with
both synthetic and real data collected from Phoenix metropolitan area.
The experiments show that our heuristic almost always produces near
optimal solution.
Keywords: Critical infrastructure · Multi-layer networks
Inter-dependence · Progressive recovery · Modeling · Analysis

1

·

Introduction

In recent years the research community is becoming increasingly aware of the
fact that the critical infrastructures of a nation are heavily interdependent for
being fully functional. Let us consider the complex interdependencies that exist
between the electric power grid and the communication network. The power grid
entities, such as the SCADA systems control power stations and sub-stations.
Such SCADA systems receive the critical commands for proper functioning
through communication networks. On the other hand, electric power is imperative for communication network entities, such as routers and base stations, to
operate.
This research is supported in part by a grant from the U.S. Defense Threat Reduction
Agency under grant number HDTRA1-09-1-0032 and by a grant from the U.S. Air
Force Oﬃce of Scientiﬁc Research under grant number FA9550-09-1-0120.
c Springer International Publishing Switzerland 2016

C.G. Panayiotou et al. (Eds.): CRITIS 2014, LNCS 8985, pp. 368–380, 2016.
DOI: 10.1007/978-3-319-31664-2 38

Progressive Recovery from Failure in Multi-layered Interdependent Network

369

In order to understand the nuances of the interdependencies between multilayered networks, the research community has made signiﬁcant eﬀorts over the
past few years [1,2,4–6]. Although, quite a few models have been proposed to
analyze such interdependent networks, most of the models are too simplistic.
Thus, unfortunately these models fail to fully capture the complexities pertaining
to the interdependence of power grid and communication networks. In [1], the
authors assume that each entity in a network depends on exactly one entity of
the other network. However, in a follow up paper [2], the same authors modify
this assumption of theirs, simply because in the real world, an entity of a network
can in fact depend on multiple entities of the other network.
The generalized model of [2] can account for disjunctive dependency of an
entity in network A (say ai ) on multiple entities in the network B (say, bj and
bk ), which implies that ai may be “alive” (functional) if either bi or bj is alive
(functional). However, their model still cannot account for conjunctive dependency of the form that for ai to be “alive”, both bj and bk must be alive. Furthermore, it is quite likely, that in a real world network the dependency might
be even more complex being a combination of both disjunctive and conjunctive
components. For e.g., ai may be alive if (i) bj and bk and bl are alive, or (ii) bm
and bn are alive, or (iii) bp is alive. The graph based interdependency models
proposed in [1,2,4–7], cannot capture such complex interdependency. In order
to overcome these shortcomings of the models in the existing literature, we have
recently proposed the Implicative Interdependency Model (IIM) [10] which uses
Boolean logic to capture such complexities.
It may be noted that as entities of network A are dependent on entities
of network B, which in turn depend on entities of network A, the failure of a
small number of type A or B entities can trigger a cascade of failures in multilayered networks resulting in a failure of a large number of entities. Suppose that
V (A) = {a1 , . . . , an } is the set of entities of network A and V (B) = {b1 , . . . , bm }
O
is that of network B. Further, AO
f ⊆ V (A), Bf ⊆ V (B) represent the subset
of A and B type entities respectively whose failure originally, results in the
failure of Acf ∪ Bfc through the cascading failure process. In this case, the set
O
AO
f ∪ Bf , must be repaired to take the system back from its degraded state
O
to its pre-failure state. Suppose that AO
f = {a1 , . . . , as } and Bf = {b1 , . . . , bt }.
O
O
Every time an element of Af or Bf is repaired, the system moves towards its
pre-failure state. However, improvement of system utility (formally deﬁned in
Sect. 3) after repair of an element ai ∈ AO
f (say), may be quite diﬀerent from
that after repair of another element aj ∈ AO
f . Accordingly, the sequence in which
O
and
B
are
repaired
have
signiﬁcant impact on system utility
the elements of AO
f
f
during the recovery process. The goal of the Progressive Recovery Problem is to
O
ﬁnd the repair sequence of the elements of AO
f ∪ Bf , so that the system utility is
maximized over the entire recovery process. The problem is described in detail
in Sect. 3.
Utilizing the IIM model, we study the progressive recovery problem in an
interdependent multi-layered networked system. We show that this problem can
be solved in polynomial time for some special cases, whereas for some others,

370

A. Mazumder et al.

the problem is NP-complete. We provide two approximation algorithms for two
special cases of the problem with a performance bound of 2 and 4 respectively.
For the most general version of the problem, we provide an optimal solution
utilizing Integer Linear Programming and as well a heuristic. Finally, we evaluate the eﬃcacy of our heuristic using both synthetic data and real power grid
and communication network data collected from Phoenix metropolitan area.
The experiments show that our heuristics almost always produce near optimal
solution.

2

Implicative Interdependency Model (IIM)

In Sect. 1, we indicated that the Implicative Interdependency Model (IIM) [10]
was proposed to overcome the limitations of the earlier models [1,2]. If the
network A entity ai is operational (“alive”) if (i) the network B entities bj , bk , bl
are operational, or (ii) bm , bn are operational, or (iii) bp is operational, we express
this in terms of live implications of the form ai ← bj bk bl + bm bn + bp . Similarly,
we can express the live implication for a B type entity br . We refer to the live
implications of the form ai ← bj bk bl + bm bn + bp also as First Order Implicative
Dependency Relations (IDRs), because these relations express direct dependency
of the A type entities on B type entities and vice-versa. It may be noted however
that as A type entities are dependent on B type entities, which in turn depends
on A type entities, the failure of some A type entities can trigger the failure
of other A type entities, though indirectly, through some B type entities. Such
an interdependency creates a cascade of failures in multi-layered networks when
only a few entities of either A type or B type (or a combination) fail. It may
be observed that the IIM model is essentially a Boolean [3]. However, to the
best of our knowledge, such modeling has not been previously used in analyzing
progressive recovery techniques in interdependent infrastructure networks.
The IDRs can be formed either through a power-ﬂow analysis of the multilayer network (similar to the ones carried out by the engineers at FERC [11], and
also by the researchers at Columbia University [12] for the power grid), or by
consultation with the engineers of the local utility and Internet service providers.
It may be noted that it is possible that A type entities may depend on A type
entities themselves, similarly, B type entities may depend on B type entities
too. The IIM model can deal with such a scenario by not distinguishing between
A and B type entities and treating them as a third type entity C. Moreover,
the concept can easily be generalized to deal with networks with more than two
layers.

3

Progressive Recovery Problem

O
Let AO
f ⊆ V (A), Bf ⊆ V (B) represent the subset of A and B type entities
respectively whose failure initiates a cascade of failures and let Acf ∪Bfc represent
O
the entities that failed due to the cascading process. So, the set AO
f ∪Bf , must be

Progressive Recovery from Failure in Multi-layered Interdependent Network

371

Table 1. IDRs for a power Table 2. SU OT [T ] for Table 3. SU OT [T ] for
communication network
repair sequence (a1 , a2 )
repair sequence (a2 , a1 )
Power Net. Comm Net.
a1 ← φ

b 1 ← a1 a2

a2 ← φ

b 2 ← a1 + a 2

...

b 3 ← a1

Timestep (t) 0 1

2

Timestep (t) 0 1

2

SU IT (t)

0 40 110

SU IT (t)

0 80 110

SU OT [T ]

0 40 150

SU OT [T ]

0 80 190

repaired to take the system from its degraded state back to its normal functioning
O
state where all entities should be functional (alive). We call such a set AO
f ∪Bf as
the set of original failures and for notational simplicity denote it by DO . W.l.o.g,
we assume that no IDR has an entity di ∈ DO on the LHS. Also, the entire set of
O
c
c
failed entities i.e., (AO
f ∪ Bf ) ∪ (Af ∪ Bf ) is denoted by Df . Suppose that DO =
O
{d1 , d2 , . . . , dp }, where di ∈ AO
f ∪Bf . Obviously, the p entities of the set DO must
be repaired to take the system back to its normal functioning state. Suppose that
only one failed entity di ∈ DO can be repaired in one unit of time. Since the
real world utilities of the failed entities of Df may be diﬀerent, the sequence
in which the entities in DO are repaired becomes important. We illustrate this
with the help of an example. Suppose that the IDRs of an interdependent powercommunication network are as given in Table 1.
In this two layer network, when a1 , a2 fail, we see that b1 , b2 , b3 also fail. In
order to return the system to its normal operational state both a1 and a2 must
be repaired. However, whether a1 is repaired ﬁrst and then a2 , or the other way
around, will have an impact on system utility. Suppose that the utility of an
entity ai is denoted by u(ai ) and is deﬁned as the beneﬁt obtained when the
entity ai is operational. Similarly, we deﬁne utility u(bj ) for entity bj . Also, let
xai (t) be the indicator variable for entity ai such that xai (t) = 1 if the entity ai is
operational at time t and 0 otherwise. Indicator variable xbj (t) is deﬁned similarly
deﬁne System Utility atInstance of Time t, denoted by SU IT (t)
for entity bj . We
as: SU IT (t) = ai ∈V (A) u(ai )xai (t) + bj ∈V (B) u(bj )xbj (t), and System Utility
T
Over Time interval 0 to T as SU OT [T ] as: SU OT [T ] = t=0 SU IT (t).
In this example, DO = {a1 , a2 } and Df = {a1 , a2 , b1 , b2 , b3 }. Let the utilities of the entities be as follows: u(a1 ) = 10, u(a2 ) = 10, u(b1 ) = 20, u(b2 ) = 30,
u(b3 ) = 40. In our analysis, we assume that if an entity di ∈ DO is ﬁxed at timestep
t, then all the entities ﬁxed due to the cascade initiated by ﬁxing of di are also ﬁxed
at timestep t, i.e., we ignore the cascade propagation time. If the repair sequence is
a2 followed by a1 , then a2 and b2 are operational at t = 1, and all of a1 , a2 , b1 , b2 , b3
are operational at t = 2. If on the other hand, the repair sequence is a1 followed
by a2 , we have that a1 , b2 , b3 are operational at t = 1 and all of a1 , a2 , b1 , b2 , b3
are operational at t = 2. The SU IT (t) and SU OT [T ] values at diﬀerent time
steps, corresponding to the two diﬀerent repair sequences are shown in Tables 2
and 3. From this example, it is clear that the sequence in which the failed entities
are repaired has an impact on the system utility over time SU OT [T ]. The system utility over time, SU OT [T ], for the second sequence (a1 , a2 ) is 190, whereas

372

A. Mazumder et al.

the SU OT [T ] for the ﬁrst sequence (a2 , a1 ) is 150. Clearly, the second sequence is
preferable over the ﬁrst. The goal of the progressive recovery problem is to identify
the repair sequence in such that the system utility over time SU OT [T ] is maximized.
Algorithm 1. Polynomial Algorithm for Progressive Recovery Problem in
Case 1
Input : (i) A set S of IDR’s of implications of the form of x ← y, where
x, y ∈ V (A) ∪ V (B), (ii) A set of original fault entities
DO ⊆ V (A) ∪ V (B), (iii) A set of failed entities Df ⊆ V (A) ∪ V (B),
(iv) utility of each entity in Df
Output: An ordering σ(DO ) such that if the entities of DO are activated in
that order, the value of SUOT[T] is maximized.
1: We construct a directed graph G = (V, E), where V = V (A) ∪ V (B). For each
IDR x ← y in S, where x, y ∈ V (A) ∪ V (B), we introduce a directed edge
(y, x) ∈ E.
2: For each node di ∈ DO , we construct a transitive closure set Cdi as follows: If
there is a path from di to some node x ∈ V in G, then we include x in Cdi .
We call each di to be the seed entity for the transitive closure set Cdi . This
physically means that if di fails, all elements in Cdi fail.
3: Sort the transitive closure sets Cdi  s, where the ranks of the closure sets are
determined by the sum of the utilities of the failed entities belonging to each
closure set. The sets with a larger sum of utilities of failed entities are ranked
higher than the sets with a smaller sum. Return the seed entities of the sorted
transitive closure sets as the required ordering of the entities of DO .

4
4.1

Computational Complexity and Solutions
Case 1: Problem Instance with One Minterm of Size One

In this case, an IDR in general has the following form: xi ← yj where xi and yj
belong to networks A (B) and B (A) respectively. For e.g., in the IDR ak ← bl
belonging to Case 1, xi = ak , yj = bl . It may be noted that a conjunctive
implication of the form ai ← bj bk can also be written as two separate implications
ai ← bj and ai ← bk . However, such cases are considered in Case 3 and is
excluded from consideration in Case 1. The exclusion of such implications implies
that the entities that appear on the LHS of a set of IDRs in Case 1 are unique.
So, the in-degree is unity for each node v ∈ V (G), G being the graph created by
Algorithm 1. This property enables us to develop a polynomial time algorithm
for the solution of the Progressive Recovery Problem for this case. We present
the algorithm next. It may be noted that the description and analysis of this
algorithm are similar to those of Algorithm 1 of [10].
Time complexity of Algorithm 1: Step 1 takes O(n + m + r) time, where
|V (A)| = n, |V (B)| = m, |S| = r. Step 2 can be executed in at most O((n + m)3 )

Progressive Recovery from Failure in Multi-layered Interdependent Network

373

time. A standard sorting algorithm in step 3 takes O(|DO |log|DO |) time. Since,
|DO | ≤ n + m, hence the overall time complexity is O((n + m)3 ).
Theorem 1. For each pair of transitive closure sets Cdi and Cdj produced in
step 2 of Algorithm 1, Cdi ∩ Cdj = ∅ where di 	= dj , di , dj ∈ DO .
Proof: Consider, if possible, that there is a pair of transitive closure sets Cdi and
Cdj where Cdi ∩ Cdj 	= ∅. If Cdi ∩ Cdj = Cdi or Cdi ∩ Cdj = Cdj , it means that
di ∈ DO or dj ∈ DO appears on the LHS of an IDR - this is a contradiction to
our assumption that DO is the set of original failures. So, let Cdi ∩ Cdj 	= Cdi
and Cdi ∩ Cdj 	= Cdj . Let dk ∈ Cdi ∩ Cdj . This implies that there is a path from
di to dk (path1 ) as well as there is a path from dj to dk , (path2 ). Since, di 	= dj ,
there is some entity, say dl , in the path1 such that dl also belongs to path2 . It
may be noted that dl may be dk , yet dl can not be di or dj because in the latter
cases either Cdi ∩ Cdj = Cdi or Cdi ∩ Cdj = Cdj . W.l.o.g, let us consider that dl
be the ﬁrst node in path1 such that dl also belongs to path2 . This implies that
dl has in-degree greater than 1. This in turn implies that there are two IDRs
in the set of implications S such that dl appears in the LHS of both. This is a
contradiction because this violates the characteristic of the IDRs in Case 1.
Theorem 2. Algorithm 1 gives an optimal solution for the Progressive Recovery
Problem in a multi-layer network for Case 1 dependencies.
Proof: We match the solution σ  of Algorithm 1 with the optimal ordering σOP T .
We say that there is a mismatch at position r, when comparing σ  with σOP T ,
the rth entity in σ  , while dj is the rth entity in σOP T and
such that di is 


x∈Cdi u(x) 	=
y∈Cdj u(y). So, when comparing σ and σOP T , if there are no
mismatch as deﬁned, we say that the greedy Algorithm 1 does as good as the
optimal solution. Otherwise, let r be the ﬁrst position of mismatch from the
left. By Theorem 1, we know that Cdi ∩ Cdj = ∅ where di 	= dj , di , dj ∈ DO .
Since,the greedy algorithm
did not choose dj and chose di instead, it means

that
x∈Cd u(x) >
y∈Cd u(y). So, replacement of di with dj reduces the
i

j

total number of entities ﬁxed at the rth selection of an entity in DO to be ﬁxed.
This means that the SUOT[T] value as achieved by greedy will be more than
the optimal solution - this is a contradiction. So, the algorithm in fact returns
an optimal solution.
4.2

Case 2: Problem Instance with Arbitrary Number of Minterms
of Size One
k
In this case, the IDRs to be considered are in the general form of xj ← i=1 yi ,
such that xj belongs to network A(B) and yi belongs to network B(A). For e.g.,
ap ← bq + br + bs is an IDR belonging to Case 2.

374

A. Mazumder et al.

1. Proof of Hardness. We can show that the min sum set cover (mssc) [9]
problem, which is shown to be N P − hard, can be reduced to a special case of
the Progressive Recovery Problem if all the IDRs are in Case 2. This indicates
our Progressive Recovery Problem is also NP-hard if all the IDRs are in Case 2.
Following is a brief discussion of the reduction.
Min sum set cover (mssc). Viewing the input as a hypergraph H(V, E), a
linear ordering is a bijection f from V to {1, ..., |V |}. For a hyperedge e and
linear ordering f , deﬁning f (e) as the minimum
of f (v) over all v ∈ e. The goal

is to ﬁnd a linear ordering that minimizes e f (e).
For any instance I in mssc, say H(V, E) is the input hypergraph. For each
node v ∈ V , we create an entity bv with u(bv ) = 0, and let DO = {bv |v ∈ V }.
For each edge e ∈ E, we 
create an entity ae with u(ae ) = 1. Also, we create an
IDR of the form as ae = v∈e bv . Clearly, we construct a Progressive Recovery
Problem instance in polynomial time with respect to the input size |V | and |E|.
We denote this instance by L. It is easy to check that if we can solve L optimally,
we can also obtain optimal solution for I, since their objectives are equivalent.
Hence, unless P = N P , it is impossible to solve Progressive Recovery Problem in
polynomial time even if all IDRs belong to Case 2, which means it is N P − hard.
2. Optimal Solution Using Integer Linear Programming. Here we provide an ILP formulation for the Progressive Recovery Problem. Let statetai (similarly statetbj ) be the indicator variable capturing the state of entity ai of network A (similarly bj belonging to network B) at timestep t. Let statetai = 0
if entity ai is dead at timestep t, and statetai = 1 if entity ai is indeed alive
at timestep t,1 ≤ t ≤ |DO |. The state variables for entities bj of network B
are deﬁned likewise. Let the indicator variable ut , 1 ≤ t ≤ |DO | give the value
of SU IT (t) (deﬁned in Sect. 3). The objective of the ILP can be written as
|DO |
|DO |
maximize
t=1 ut where
t=1 ut gives the value of SU OT [|DO |] (SU OT [T ]
is deﬁned in Sect. 3). It may be recalled that the objective of the Progressive
Recovery Problem is to ﬁnd the optimal ordering in which the entities in DO
should be activated such that SU OT [T ] is maximized. The constraints of the
ILP are as follow:

1. ut = di ∈Df u(di ) × statetdi , 1 ≤ t ≤ |DO |: This constraint computes
the value of ut , 1 ≤ t ≤ |DO | as the sum of the utilities of all the entities
which are alive in timestep t. Here, u(di ) gives the utility value for the entity
di , 1 ≤ i ≤ |Df | and is provided as input to the problem.
Now, for each entity di ∈ DO , let the indicator variable acttdi = 1 if di is
activated at timestep t and acttdi = 0 otherwise, where 1 ≤ t ≤ |DO |. So, we
have the following constraints:
|DO |
t
2.
t=1 actdi = 1, ∀di ∈ DO : This constraint ensures that each entity
di ∈ DOis activated exactly once during the time interval t = 1 to t = |DO |.
t
3.
di ∈DO actdi = 1, 1 ≤ t ≤ |DO |: This constraint ensures that in each
timestep 1 ≤ t ≤ |DO |, exactly one entity di ∈ DO is activated.
4. state0di = 0∀di ∈ Df : This constraint ensures that at timestep t = 0, all
entities in Df are in dead condition.

Progressive Recovery from Failure in Multi-layered Interdependent Network

375

t
5. statetdi = statet−1
di + actdi , ∀di ∈ DO , 1 ≤ t ≤ |DO |: This constraint
ensures that the state of an entity di ∈ DO at timestep t must be the same
as that in timestep t − 1 unless di is activated at timestep t. Also, if an entity
di ∈ DO is alive at timestep t, it remains alive in timesteps t + 1, t + 2, . . . , |DO |.
k
Also, in general form, for each IDR of Case 2, say xj ← i=1 yi , we have the
following linear constraints. These two constraints ensure that entity xj is alive
only when at least one of yi , 1 ≤ i ≤ k is alive.
6.a statetxj ≥ statetyi , 1 ≤ i ≤ k, 1 ≤ t ≤ |DO |
k
6.b statetxi ≤ i=1 yi , 1 ≤ t ≤ |DO |
For e.g., if we have an IDR of the form a1 ← b1 + b2 , for an instance of
the Progressive Recovery Problem having |DO | = 2, then this IDR, leads to
the following constraints: state1a1 ≥ state1b1 , state1a1 ≥ state1b2 , state2a1 ≥ state2b1 ,
state2a1 ≥ state2b2 , state1a1 ≤ state1b1 + state1b2 , state2a1 ≤ state2b1 + state2b2 . Thus,
given an instance of the Progressive Recovery Problem, we can compute the
optimal sequence in which the entities of DO should be activated by solving this
ILP.

3. Approximation Algorithm for a Special Subcase. In our Progressive
Recovery Problem, we can transform the IDRs such that the RHS of each IDR
consists of only entities of DO . Considering the subcase of our problem such that
utilities of all the entities are equal, the objective of this subcase of our problem
is identical to that of the mssc problem [9] for which the authors provide a
4−approximation algorithm.
4.3

Case 3: Problem Instance with One Minterm of Arbitrary Size
k
In case 3, the general form of the IDRs to be considered is given by xj ← i=1 yi ,
such that xj and yi are entities belonging to network A(B) and B(A) respectively.
For e.g., ap ← bq × br × bs is an IDR belonging to Case 3.
1. Proof of Hardness. It is possible to show that the Minimum Latency Set
Cover Problem (MLSC) [8], proven as NP hard, can be reduced to a special case
of the Progressive recovery problem if all the IDRs are belong to Case 3.
The Minimum Latency Set Cover Problem (M LSC). The problem is
deﬁned as follows: Let J = {J1 , J2 , ...., Jm } be a set of jobs to be processed by
a factory. Each job Ji has a non-negative weight wi . Let T = {t1 , t2 , . . . , tn }
be a set of tools. Job j is associated with a nonempty subset Sj ⊆ T . In each
time unit, a single tool can be installed by the factory. Once the entire tool
subset Sj has been installed, job j can be processed instantly. The problem is to
determine the order of tool installation in order to minimize the weighted sum
of job completion times.
Similar construction scheme from Case 2 can be used. Let I be an instance of
M LSC and J, T be the corresponding input. For each t ∈ T , we create an entity
bt with u(bt ) = 0. For each j ∈ J, we create an entity aj with u(aj ) = w(j).

376

A. Mazumder et al.


Also, for each Sj , we create an IDR of the form as aj = t∈Sj bt . By arguments
similar to those given in Case 2, we know that even Case 3 alone is N P − hard.
2. Optimal Solution Using Integer Linear Programming. The Integer
Linear Programming formulation for the Progressive Recovery Problem when
the IDRs belong to Case 3 is almost identical to that when the IDRs are belong
to Case 2. The objective function along with constraints one through ﬁve remain
unchanged. Only constraint 6 changes to account for the change in the 
form of
k
IDR from Case 2 to Case 3. An IDR in Case 3, in general form, say, xj ← i=1 yi
k
t
t
can be represented by the linear constraints k × statexj ≤ i=1 stateyi , 1 ≤ t ≤
|DO |. These constraints ensure that the entity xj can be alive only when all the
entities yi , 1 ≤ i ≤ k are alive. For e.g., let us again consider that we have an
IDR a1 ← b1 × b2 and the instance of the problem has |DO | = 2, then the linear
constraints arising from this IDR are 2×state1a1 ≤ state1b1 +state1b2 ,2×state2a1 ≤
state2b1 + state2b2 . Solving this ILP gives the optimal solution for this case.
3. Approximation Algorithm for a Special Subcase. If ∀di ∈ DO , u(di )
are equal, we transform the IDRs such that the RHS of all the IDRs are subsets
of DO . Then the objective of our problem is identical to that of the M LSC
problem. A 2−approximation algorithm for the M LSC problem is given in [8].
4.4

Case 4: Problem Instance with Arbitrary Minterm of Arbitrary
Size

In the most
k setting, an IDR belongs to Case 4 and has the general form
l general
of xj ← m=1 i=1 ymi , where, as before, xj and ymi are entities belonging to
network A(B) and B(A) respectively. For e.g., ap ← bq × br + bs × bt is an IDR
belonging to Case 4.
1. Proof of Hardness. Because the IDRs belonging to Case 2 and 3 are special
cases of the general case i.e., Case 4 and the Progressive Recovery Problem has
been proven to be NP-complete when IDRs belong to Case 2 and 3, so evidently
the problem remains NP-Complete when the IDRs belong to Case 4 as well.
2. Optimal Solution Using Integer Linear Programming. When an IDR
belongs to Case 4, it can be expressed in terms of linear constraints by applying
a combination of techniques used to translate IDRs belonging to Cases 2 and 3
as discussed in the previous subsections. For e.g., if we have an IDR such as
a1 ← b1 × b2 + b3 × b4 , we can re-write it as a1 ← c1 + c2 (and translate it into
constraints as discussed in Case 2), where c1 ← b1 × b2 and c2 ← b3 × b4 (these
are IDRs belonging to Case 3).

Progressive Recovery from Failure in Multi-layered Interdependent Network

377

Algorithm 2. Heuristic Algorithm for Progressive Recovery Problem in
Case 4
1: set ans = 0
2: for i = 1 to n do
3:
for each node di that is not activated yet in DO do
4:
Compute inf luencedi
5:
Compute supportdi
6:
end for
7:
Choose an entity di ∈ DO with the highest inﬂuence value inf luencedi .
If there is a tie, choose the one with the larger support value supportdi .
Choose one arbitrarily if tie still exists. Let e ∈ DO denote the entity
chosen ﬁnally.
8:
Activate e and allow the cascade to occur. Remove any IDR from the set
of IDRs if the entity on the LHS is ﬁxed at this time step.
9:
ans = 2 ∗ ans + inf luencee
10:
σ =σ+e
11: end for
12: Output ans and σ.

3. Heuristic Solution. Our heuristic algorithm is a greedy one, i.e., we always
want to obtain as much utility as possible in each time step. For an entity
di ∈ DO , we deﬁne inf luencedi as the total gain (utility) obtained when
entities get ﬁxed following the cascade initiated by activating entity di alone.
For instance, let us consider the following IDRs: a0 ← b1 , a1 ← b4 + b2 , a2 ←
b1 + b2 × b3 , b4 ← a0 ,u(a0 ) = u(a1 ) = u(a2 ) = u(b4 ) = 1,b1 , b2 , b3 ∈ DO .
Then inf luenceb1 = 4 since by activating b1 , all of a0 , a1 , a2 and b4 are
ﬁxed after cascading. inf luenceb2 = 1, for only a1 is ﬁxed upon activation
of b2 and inf luenceb3 = 0. Entities with higher inﬂuence are preferred during each timestep, however, there could be a tie when multiple entities have
the same inﬂuence value. In order to distinguish, we introduce another variable supportdi . For each di ∈ DO , we deﬁne supportdi to be the total number
of appearances of di on the RHS among all IDRs. For instance, if we have
a0 = b1 × b2 , a1 = b1 × b3 , a2 = b1 × b4 , it is easy to see that all of bi has
inﬂuence 0. However, supportb1 = 3 since it appears thrice on the RHS and
supportb2 = supportb3 = supportb4 = 1. In particular, if one IDR has the form
of a1 = b1 × b2 + b1 × b3 , then supportb1 = 1 for such an IDR. So, whenever
there is a tie, we will choose the entity with larger support value. If a tie further
exists, we break the tie arbitrarily. Algorithm 2 gives the pseudocode for the
heuristic algorithm. Input consists of a set of entities DO which must be activated, a failed set of entities Df with utility function u(), and IDRs. W.l.o.g,
let |DO | = n, |Df | = m and r is the total number of minterms in the IDR set.
Let σ be the activation order obtained from Algorithm 2 and ans be the total
system utility and we recall that we want to maximize the total system utility.
The running time of the algorithm is O(n2 (m + r)). We need to consider n
time steps. During each time step, every entity in DO is considered. Given an

378

A. Mazumder et al.

entity di ∈ DO , it takes O(m) time to compute its support value and O(r) time
to compute its inﬂuence value. Hence the total running time is O(n2 (m + r)).

5

Experimental Result

To study the performance of the heuristic solution for Case 4, we have conducted
experiments both on real world data for Phoenix metropolitan area which is the
most densely populated area of Arizona, U.S.A, as well as some synthetic data.
An overview of the two types of data used in our experiments is as follows:
1. To consider a multi-layer network in a real world setting, we consider the
dataset used in our work [10]. We have obtained the data for the power network
of Phoenix metropolitan area from Platts (http://www.platts.com/) and that for
the communication network from GeoTel (http://www.geo-tel.com/). The power
network entities considered are powerplants and transmission lines while the
communication network entities considered are ﬁber-lit buildings, cell towers and
ﬁber links. The dataset consists of 70 power plants, 470 transmission lines, 2, 690
cell towers, 7, 100 ﬁber-lit buildings and 42, 723 ﬁber links. Due to experimental
resource limitation, we have considered ﬁve regions of interest in the Phoenix
metropolitan area. For each of these ﬁve regions, we have constructed a set of
IDRs from the power and communication network data using the set of rules
described in [10]. For completeness, we describe the set of rules used: (a) For
each generator to be alive, either the geographically nearest cell tower should be
alive or the nearest ﬁber-lit building and the corresponding ﬁber link connecting
the generator with the ﬁber-lit building must be alive, (b) To be alive, the ﬁber-lit
buildings and the cell towers must have at least one of the two nearest generators
and the corresponding connecting transmission lines alive, (c) The transmission
lines and the ﬁber links are independent of any other entities.
2. We have also consider twenty datasets of synthetic data. Because of computational resource limitation, for each of these datasets we have considered

(a) Figure comparing optimal and (b) Figure comparing optimal and
heuristic solutions for the data for the heuristic solutions for the randomly
Phoenix metropolitan area
generated synthetic data
Fig. 1. Figure showing experimental comparison of the optimal and heuristic solutions

Progressive Recovery from Failure in Multi-layered Interdependent Network

379

(1) a random number chosen among {2, 3, . . . , 10} for the size of DO , (2) a random size for the set Df \ DO which failed due to cascade, with the sizes varying
from ten to twenty, (3) a random number of minterms of random sizes for each
IDR. The number of minterms in each IDR is chosen randomly from {1, 2, 3}.
The size of each minterm is randomly chosen from {1, 2, . . . , 8}.
We have used IBM CPLEX optimizer 12.5 to implement the formulated ILP.
We show the results of our experiments both on the real world data as well
as the synthetic data in Fig. 1. We observe that in the real world data, the
heuristic attains optimal solution in each of the ﬁve datasets. The reason for
such a result is that the IDRs considered are quite simple because of the simple
rules [10] as discussed previously- each IDR has at most two minterms and the
size of each minterm does not exceed two. In case of the synthetic data, we have
considered much more complex IDRs with much bigger sizes for minterms and
much bigger failed set Df . However, even in the case of the synthetic data, the
heuristic attains near optimal solution in all the cases, with the ratio between the
optimal and heuristic solution never exceeding 1.2 in any of the twenty datasets.
In Fig. 1(b), we compare the optimal and the heuristic solutions for the cases
where the latter deviates the most from the optimal solution. It can be thus seen
that the heuristic performs quite well in our experimental setup.

6

Conclusion

In this paper, we study the Progressive Recovery Problem to maximize the
system utility over the time when recovery of failed entities takes place in an
inter-dependent network. We show that the problem can be solved in polynomial
time in some cases, while in others it is NP-complete. We also provide two
approximation algorithms and a heuristic to solve the problem in diﬀerent cases.
Experimental evaluations show that the heuristic attains near optimal solution
in almost all cases.

References
1. Buldyrev, S., Parshani, R., Paul, G., Stanley, H., Havlin, S.: Catastrophic cascade
of failures in interdependent networks. Nature 464(7291), 1025–1028 (2010)
2. Gao, J., Buldyrev, S., Stanley, H., Havlin, S.: Networks formed from interdependent
networks. Nat. Phys. 8(1), 40–48 (2011)
3. Kauﬀman, S.A.: Metabolic stability and epigenesis in randomly constructed nets.
J. Theor. Biol. 22(3), 437–467 (1969)
4. Rosato, V., Issacharoﬀ, L., Tiriticco, F., Meloni, S., Porcellinis, S., Setola, R.:
Modelling interdependent infrastructures using interacting dynamical models. Int.
J. Crit. Infrastruct. 4(1), 63–79 (2008)
5. Parandehgheibi, M., Modiano, E.: Robustness of Interdependent Networks: The
case of communication networks and the power grid. arXiv preprint (2013).
arxiv:1304.0356
6. Nguyen, D., Shen, Y., Thai, M.: Detecting critical nodes in interdependent power
networks for vulnerability assessment. IEEE Trans. Smart Grid 4(1), 151–159
(2013)

380

A. Mazumder et al.

7. Castet, J., Saleh, J.: Interdependent multi-layer networks: modeling and survivability analysis with applications to space-based networks. PloS one 8(4), e60402
(2013)
8. Hassin, R., Levin, A.: An approximation algorithm for the minimum latency set
cover problem. In: Brodal, G.S., Leonardi, S. (eds.) ESA 2005. LNCS, vol. 3669,
pp. 726–733. Springer, Heidelberg (2005)
9. Feige, U., Lovsz, L., Tetali, P.: Approximating min sum set cover. Algorithmica
40(4), 219–234 (2004)
10. Sen, A., Mazumder, A., Banerjee, J., Das, A., Compton, R.: Identiﬁcation of k most
vulnerable nodes in multi-layered network using a new model of interdependency.
In: Presented at the International Workshop on Network Science for Communication Networks (INFOCOM workshop). arXiv preprint (2014). arxiv:1401.1783
11. Smith, R.: U.S. Risks National Blackout From Small-Scale Attack. Wall
Street Journal (2012). http://online.wsj.com/news/articles/SB10001424052702304
020104579433670284061220
12. Bernstein, A., Bienstock, D., Hay, D., Uzunoglu, M., Zussman, G.: Power grid vulnerability to 3 geographically correlated failures-analysis and control implications.
arXiv preprint (2012). arxiv:1206.1099

Routing with Many Additive QoS Constraints
Guoliang Xue, Arunabha Sen and Rakesh Banka
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287–5406
Email: {xue, arunabha.sen, rakesh}@asu.edu
Abstract— A fundamental problem in QoS routing is to find
a path between a specified source-destination node pair that
satisfies a set of end-to-end quality-of-service constraints. We
study this problem in a communication system where there
are multiple additive quality of service parameters associated
with each link. It is well-known that the multi-constrained path
selection problem (MCPS) is NP-complete. In this paper, we
present a fully polynomial time approximation scheme for an
optimization version of the MCPS problem. This means that
for any given  > 0, we can compute, in time bounded by a
polynomial of the input size of the problem and in 1 , a solution
whose cost is at most (1 + ) of that of the optimal solution.
Keywords Quality of service routing, multiple additive constraints, efficient approximation algorithms.

I. I NTRODUCTION
A fundamental problem of routing in a network that provides QoS guarantees is to find a path between a specified
source-destination node pair that simultaneously satisfies multiple QoS constraints, such as cost, reliability, delay, and delayjitter. Such an environment is modeled by a graph where
each link has multiple weights associated with it, representing
among other things, cost, reliability, delay, and delay-jitter.
Since each link has multiple weights associated with it, each
path also has multiple “path lengths” associated with it. If
the weights represent cost, reliability, delay and delay-jitter,
then the path length can be obtained by adding up the weights
associated with the links on the path (although the reliability
of a path is the product of the reliabilities of the links, the
logarithm of the reliability of the path is the sum of the
logarithms of the reliabilities of the links on the path). For
this reason such parameters are known as additive quality of
service parameters. Another kind of QoS parameters (such
as bandwidth) are known as bottleneck parameters where
the corresponding weight of a path is the smallest of the
weights of the links on the path. Problems involving bottleneck
constraints can be easily solved by considering subgraphs with
only those links whose weights are greater than or equal to a
particular chosen value. Therefore we restrict our attention to
only additive parameters.
Recognizing the need of an efficient solution of this fundamental problem, many researchers in the networking and
operations research domain have studied this problem in the
last few years [1], [2], [3], [7], [8], [6], [9], [10], [11], [13],
[14], [15], [16], [18], [19]. It is well known that the multiconstrained path selection problem (MCPS) is NP-complete,

even when the number of constraints is two [5]. Many researchers have used heuristic techniques to find a solution
to this problem. Through experimental evaluations, they have
claimed the efficacy of their heuristics. The limitation of this
approach is that it fails to provide any performance guarantee
for the solution. Warburton in [17] first developed a fully
polynomial time approximation scheme for the MCPS problem
with two constraints (MCPS/2) on an acyclic graph. Hassin in
[7] improved the efficiency of the algorithm for the MCPS/2
problem to O(|E|(n2 /) log(n/)), where n is the number
of nodes in the network. The algorithm guarantees to find
a path of length at most (1 + ) times the length of an
optimal path. Lorenz and Raz [11] further improved this to
O(|E|n(log log n + 1/)). To the best of our knowledge no
fully polynomial time approximation scheme for the MCPS/K
problem for arbitrary K, has appeared in the literature before.
In this paper we present a fully polynomial time approximation
scheme for the general MCPS/K problem for any arbitrary
K ≥ 2. The following are some of the advantages of our
result.
1) Our algorithm is guaranteed to find a path within (1+)
of optimal while none of the previous algorithms has this
guarantee.
2) The worst-case running time of our algorithm is bounded
by a polynomial in the problem input size and 1/,
which is the best one can expect for an NP-hard problem.
We present simulation results which show that our algorithm
can find 1.5-approximations within reasonable time. We also
present an efficient heuristic algorithm for this problem.
The rest of the paper is organized as follows. In Section II,
we define the problems and some notations. In Section III, we
present our main result–a fully polynomial time approximation
scheme for an optimization version of the MCPS problem.
In Section IV, we discuss advantages of our algorithm over
a previous algorithm. In Section V, we present an efficient
heuristic algorithm. In Section VI, we present computational
experiences. We conclude this paper in Section VII.
II. D EFINITIONS AND N OTATIONS
All constants, functions and variables are assumed to
have integer values unless specified otherwise. We model
a network by an edge weighted undirected graph G =
(V, E, ω 1 , . . . , ω K ), where V is the set of n vertices, E is
the set of m edges, and ω i (e) ≥ 0 is the ith weight of edge
e. Let p be a path in G. The ith weight of p, denoted by

0-7803-7802-4/03/$17.00 © 2003 IEEE

223

ω i (p), is the sum of the ith weights over the edges on p.
The decision version of the multi-constrained path selection
(DMCPS) problem is the following.
DMCPS(s, t, W)
Instance: Given a graph G = (V, E), with K edge weights,
ω i (e), 1 ≤ i ≤ K associated with edge e, a set of K
constraints W = {W1 , W2 , . . . , WK } and a source-destination
node pair (s, t).
Question: Is there a path p from s to t such that ω i (p) ≤
Wi , ∀1 ≤ i ≤ K?
It is well known that this problem is NP-hard for K ≥ 2
[5], [16]. For K = 2, FPTAS exists [7], [11] which minimizes
ω 1 (p) subject to ω 2 (p) ≤ W2 . To the best of our knowledge,
no provably good approximation is previously known for
K ≥ 3. Since finding a path satisfying two or more additive
constraints is NP-hard, we study the following optimization
version of the MCPS problem:
OMCPS(s, t, W)
Instance: Given a graph G = (V, E) with edge weights
ω i (e), 1 ≤ i ≤ K associated with edge e, a set of K
nonnegative constants W = {W1 , . . . , WK } and a sourcedestination node pair (s, t).
Problem: Find the a path p from s to t such that ω i (p) ≤
ζ · Wi , ∀1 ≤ i ≤ K and ζ is minimum over all possible paths
from s to t.
We show that there is a fully polynomial time approximation
scheme (FPTAS) for OMCPS(s, t, W). In other words, for any
given  > 0, our algorithm finds an s–t path p in polynomial
(in input size and 1/) such that ω i (p) ≤ (1 + )ζopt Wi , 1 ≤
i ≤ K, where ζopt is the optimal value of OMCPS(s, t, W).
Our approximation scheme makes use of the following
MinMax path problem, which is a special case of the decision
version of the MCPS problem.
DMMPS(s, t, W )
Instance: Given a graph G = (V, E), with K edge weights,
ω i (e), 1 ≤ i ≤ K associated with edge e, an integer W and a
source-destination node pair (s, t).
Question: Is there a path p from s to t such that ω i (p) ≤
W, ∀1 ≤ i ≤ K?
DMMPS(s, t, W ) is a special case of DMCPS(s, t, W)
where all constraints are the same. We call this problem the
MinMax path problem because we are looking for a path
whose maximum path weight satisfies the given constraint
W . One can easily prove that DMMPS(s, t, W ) is NP-hard
for K ≥ 2. In the next section, we will present a pseudo
polynomial time algorithm for solving DMMPS(s, t, W ), with
a running time bounded by a polynomial in n, m, and W .
Our approximation scheme for OMCPS(s, t, W) will solve
instances of DMMPS repeatedly with the constraint value W
polynomially bounded.
III. P OLYNOMIAL T IME A PPROXIMATION S CHEME
In this section, we will present a fully polynomial time
approximation scheme (FPTAS) to OMCPS(s, t, W) for any
constant K ≥ 2. We will need a pseudo-polynomial time

algorithm for DMMPS and an approximate testing procedure
for OMCPS before presenting our FPTAS.
A. A Pseudo-Polynomial Time Algorithm for DMMPS
We present an O(mW K−1 + nW K−1 log(nW K−1 )) time
algorithm for the MinMax path problem. When the answer is
yes, our algorithm also computes a feasible path.
Algorithm 1 Solving the MinMax path problem.
Step 1 Construct a directed graph GW with node set
VW = V · {0, 1, . . . , W }K−1 and edge set EW .
If (u, v) is an undirected edge in E, then EW
contains directed edges from (u, C2 , C3 , . . . , CK )
to (v, D2 , D3 , . . . , DK ) such that Di = Ci +
ω i (u, v), i = 2, 3, . . . , K. The length of all such
edges is ω 1 (u, v). In addition, EW also contains
zero length edges from vertex (t, C2 , C3 , . . . , CK ) to
(t, D2 , D3 , . . . , DK ), where Di = Ci for all i =
2, 3, . . . , K, except one of them, say j, where Dj =
Cj + 1.
Step 2 Compute the shortest paths from (s, 0, 0, . . . , 0) to all
other nodes in GW .
Step 3 If there is a path p from (s, 0, 0, . . . , 0) to a node in
the form (t, W, W, . . . , W ) whose length is no more
than W , the decision version of the MinMax path
problem has a feasible solution that can be obtained
from p by retaining only the first component in each
vertex on the path. Otherwise, the decision version of
the MinMax path problem does not have a feasible
solution.
Theorem 3.1: Algorithm 1 is correct. The worst case
time complexity of the algorithm is O(mW K−1 +
nW K−1 log(nW K−1 )).
P ROOF. Note that GW has nW K−1 vertices and O(mW K−1 )
edges. It follows from the construction of GW that
GW has a directed path pW from (u, C2 , C3 , . . . , CK ) to
(v, D2 , D3 , . . . , DK ) with length  if and only if the corresponding path p in G (obtained by keeping only the first component in each vertex on pW ) satisfies ω 1 (p) =  and ω i (p) ≤
Di − Ci for i = 2, 3, . . . , K. Therefore DMMPS(s, t, W ) has
a feasible solution if and only if there is a directed path
from (u, 0, 0, . . . , 0) to (v, W, W, . . . , W ) with length at most
W . This proves the correctness of Algorithm 1. The time
complexity of the algorithm follows from Dijkstra’s algorithm
with Fibonacci heaps [4].
✷
B. Polynomial Time Approximate Testing
Our FPTAS depends on the following polynomial time
approximate test procedure. For a positive real number θ, we
will use ωθi to denote the scaled ith weighting function such
that ωθi (e) = ω i (e) · θ/Wi 
 for every e ∈ E. For given real
numbers C > 0 and  > 0, we define TEST(C, ) = YES
if the decision version of MinMax problem with weight coni
straint  n−1

 
 and scaled weighting functions (using ωθ with

224

θ = n−1
C·
 ) is feasible and define TEST(C, ) = NO otherwise.
Let ζopt be the optimal value of the OMCPS(s, t, W) problem.
It follows from standard techniques of scaling and rounding
[7], [11] that TEST(C, ) = NO implies ζopt > C and that
TEST(C, ) = YES implies ζopt < C · (1 + ).
Theorem 3.2: Let ζopt be the optimal value of the
OMCPS(s, t, W) problem. Let C and  be two fixed positive
numbers. Then
• TEST(C, ) = YES implies ζopt < C · (1 + );
• TEST(C, ) = NO implies ζopt > C.
P ROOF. Assume that TEST(C, ) = YES. This means that
there exists an s–t path p such that ωθi (p) ≤  n−1

 
 for i =
1, 2, . . . , K. It follows from the definition of θ and ωθi that for
each edge e and i = 1, 2, . . . , K, we have
ωθi (e) = ω i (e) ·

n−1

.
C ·  · Wi

Since ωθi (p) ≤

n−1

 ,

n−1
− (n − 1), i = 1, 2, . . . , K.
CWi

(2)

(2) implies

ω (p) < (1 + ) · C · Wi , i = 1, 2, . . . , K.
i

(3)

Therefore ζopt < C · (1 + ). This proves the first claim of the
theorem.
To prove the second claim of the theorem, assume that
ζopt ≤ C. Therefore there exists an s–t path p such that
ω i (p) ≤ ζopt · Wi ≤ C · Wi for i = 1, 2, . . . , K. It follows
from (1) that
ωθi (p) ≤ ω i (p) ·

n−1
n−1
≤
, i = 1, . . . , K.
C ·  · Wi


Step 1 set LB := LB[0] and UB := UB[0] ;
Step 2 if UB ≤2·LB then
goto Step 3;
else
√
let C := UB · LB;
if TEST(C, 1) = NO, set LB = C;
if TEST(C, 1) = YES, set UB = C;
goto Step 2;
endif
Step 3 Use θ := (n−1)
LB·
 to scale the edge weighting functions; bisect [LB, 2UB · θ
] to find the smallest
integer ζ such that DMMPS(s, t, ζ) is feasible and
output the corresponding feasible path pθ .

(1)

Since p has at most n − 1 edges, equation (1) implies
ωθi (p) > ω i (p)

Algorithm 2 FPTAS for OMCPS(s, t, W).

(4)

Since ωθi (p) always has integer values, (4) also implies
ωθi (p) ≤  n−1
= 1, 2, . . . , K. This implies

 
 for i
TEST(C, ) = YES. Therefore TEST(C, ) = NO implies
✷
ζopt > C.

is true for i = 
0. Assume that (5) is true for
i ≥ 0. If 
TEST( LB[i] · UB[i] , 1) = NO, we set
[i+1]
LB
LB[i] · UB[i] and UB[i+1] = UB[i] . If
=
[i]
TEST( LB · UB[i] , 1) = YES, we set LB[i+1] = LB[i] and
UB[i+1] = LB[i] · UB[i] . It follows from Theorem 3.2 that
(5) is also true for i + 1. Also from the definition of the
sequences {LB[i] } and {UB[i] }, we have

log UB[i] − log LB[i]
, i = 0, 1, 2, . . . .
2
(6)
Therefore Step 2 of the algorithm is executed no more than
log(log UB[0] −log LB[0] ) times. The worst case running time
required by the first two steps of the algorithm is bounded by
O((mnK−1 + nK log(nK )) log(log UB[0] − log LB[0] )).
In the rest of this proof, we will assume that θ is fixed as in
Step 3. Let p be an optimal solution to OMCPS(s, t, W), i.e.,
p is an s–t path such that ω i (p) ≤ ζopt ·Wi for i = 1, 2, . . . , K.
n−1
n−1
Since ωθi (e) = ω i (e) · LB·
·W

 ≤ ω i (e) · LB·
·W
for every
i
i
edge e, we have
log UB[i+1] −log LB[i+1] =

ωθi (p) ≤ ω i (p) ·

C. Polynomial Time Approximation Scheme for OMCPS
In O(km + n) time, we can check whether the OMCPS
problem has a zero-cost feasible solution. If it does, we have
an optimal solution. Otherwise, LB[0] ≡ 1 is an initial lower
bound for ζopt . Let the initial approximate upper bound for
ζopt be UB[0] which is 12 times the sum of all the km edge
weights. UB[i] is an approximate upper bound for ζopt in the
sense that 2 · UB[i] is a true upper bound for ζopt . Our FPTAS
is presented in Algorithm 2.

Theorem 3.3: Algorithm 2 finds an (1 + )-approximation
1
to OMCPS(s, t, W) in O((mnK−1 + knK log n) · ( 
K−1
+
[0]
K
UB
n
1
log log LB[0] ) + 
K−1 log 
K−1 ) time.
P ROOF. Let {LB[i] } and {UB[i] } denote the sequences of lower
bounds and approximate upper bounds generated by Step 2
of the algorithm. We know that
LB[i] ≤ ζopt ≤ 2 · UB[i]

(5)

n−1
n−1
≤ 2UB · θ. (7)
≤ ζopt ·
LB · 
LB ·  · Wi

Therefore Step 3 of the algorithm is guaranteed to find a
feasible path.
Let pθ be the s–t path found in Step 3 of the algorithm.
Since pθ is at least as good as p for the scaled weighting
functions, we have

n−1
≥ ωθi (pθ ), i = 1, 2, . . . , K.
LB · 
Since pθ has at most n − 1 edges, we also have
ζopt ·

ωθi (pθ ) ≥ ω i (pθ ) ·

(8)

n−1
− (n − 1), i = 1, . . . , K. (9)
LB ·  · Wi

Combining (8) and (9), we have

ω i (pθ ) ≤ ζopt ·Wi +LB··Wi ≤ ·(1+)·ζopt ·Wi , i = 1, . . . , K.
(10)
Therefore pθ is an (1+) approximation to OMCPS(s, t, W). It
follows from Theorem 3.1 that the worst case time complexity

225

K−1

K

K

n
n
of Step 3 is O(m n
K−1 + 
K−1
· log( 
K−1
)). Therefore the
overall time complexity of Algorithm 2 is O((mnK−1 +
K−1
nK
nK log(nK )) log(log UB[0] − log LB[0] ) + m n
K−1 + 
K−1
·
K
n
log( 
K−1
)).
✷

IV. D ISCUSSIONS
We first note that the running time of our algorithm is fully
polynomial (in the sense that the running time is bounded by
a polynomial in the size of the problem and in 1/). This
is the first approximation algorithm with a provably good
performance guarantee for the MCPS problem with three or
more additive quality of service parameters. We also note that
the time complexity of our FPTAS is relatively high, compared
with that of many existing heuristics. We want to point out
that our FPTAS always computes a path with a guaranteed
quality of service, while existing heuristics do not provide such
performance guarantee.
s

0
1
1
0
0
1

2, 2

A
2, 0

2, 2

2, 2

2, 2

Fig. 1.

2, 2

B

2, 2

t

2, 0
2, 4

u1
u2

2, 4

2, 4

2, 0

uL

Limitation of randomized routing algorithm

Recently Korkmaz and Krunz in [10] has proposed a randomized algorithm for the MCPS/K problem. Using simulations they have showed that their algorithm provides better
performance than other comparable algorithms with same
computational complexity. Although the fully polynomial approximation scheme being proposed in this paper is not
directly comparable with the solution in [10], we make some
comparison with that algorithm, primarily because the complexity of the solution in [10] is less than the one being
proposed in this paper. We show that in some networks, even
if a path satisfying all the K constraints exists, the probability
of the randomized algorithm finding such a path may be very
small. The randomized algorithm maintains several labels for
each node u, B i [u, v], L[u, v], Di [v] and π[u] for all i, 1 ≤
i ≤ K, where the meaning of the labels are as follows:
B i [u, v]: the length of the shortest path from u to every
possible destination v w.r.t link weight ω i , L[u, v]: same as
B i [u, v] with optimization being done w.r.t ω 1 +ω 2 +. . .+ω K ,
Di [v]: the length of a path from s to u w.r.t ω i , 1 ≤ i ≤ K,
and π[u]: the predecessor of u. The algorithm uses a breadthfirst search to find the next node on the path from the source
to the destination. If u is a node on the partially constructed
path from the source to the destination, then v can be the next
node on the path if the following two conditions are satisfied:
(i) Di [u] + ω i (u, v) + Bi [v, t] ≤ Wi , ∀i, 1 ≤ i ≤ K, and

K
K
(ii) i=1 (Di [u] + ω i (u, v)) + L[v, t] ≤ i=1 Wi . Consider
MCPS/2 problem for the network shown in Figure 1 with
two link weights on each link. Suppose that W1 = W2 = 7.
Consider the situation when u = S and v = u1. In this case,
D1 [u] = D2 [u] = 0, ω 1 [u, v] = ω 2 [u, v] = 2, B1 [v, t] =
B2 [v, t] = 4 and L[v, t] = 10. In this case, both conditions
(i) and (ii) will be satisfied and the algorithm will conclude
that u1 is the next node on the path to the destination and
put it in the queue for further exploration. However, it can
be seen from the network that no path from s to t through
u1 will be feasible satisfying the constraints W1 = W2 = 7.
Therefore, if the algorithm chooses u1 as the next node from
s, it will fail to find a feasible path, even though a feasible path
s → A → B → t exists in the network. Such failure will result
if the randomized algorithm chooses any node u1, . . . uL. The
algorithm will not fail only if it chooses A as the next node.
Since from s, it can go to any one of the adjacent L + 1
nodes, only one of which will produce a feasible path, the
1
, which may
probability of choosing the “right” node is L+1
be small if L is large. Therefore, in such situations, the ability
of the randomized algorithm of finding a feasible path will be
fairly small. Our approximation algorithm, on the other hand,
will find the optimal path in such situations, if  is chosen in
the interval (0, 17 ).
Let us again consider the network given in Figure 1. This
time, we assume that the quality of service constraints are
W1 = W2 = 5. Clearly, there is no s–t path satisfying both
constraints. The randomized heuristic algorithm will not find
a feasible solution, without knowing that no such path exists.
With  ∈ (0, 71 ), our approximation algorithm will produce the
s–t path (s, A, B, t), which violates each constraint by a factor
of 51 . In this case, we also know that a feasible solution does
not exist because the existence of a feasible path would imply
1
1
7 ≥ 5 . These examples show that our FPTAS, although with
a relatively high time complexity, provides quality of service
guarantees that existing heuristics do not provide.
V. A S IMPLEX H EURISTIC
Our FPTAS can be used to compute a provably good
approximation. However, almost all provably good approximation algorithms require relatively longer running times. In
this section, we propose a heuristic algorithm which is very
fast and very effective.
K
Let Λ = {(λ1 , . . . , λK )| i=1 λi = 1, ∀i, 0 ≤ λi ≤ 1}.
For each λ = (λ1 , · · · , λK ) ∈ Λ, define a new edge weighting
K
ω i (e)
function by ωλ (e) = i=1 λi W
. Using this edge weighting
i
function, we may compute a shortest s–t path pλ . Define
i
(pλ )
f (λ) = max1≤i≤K { ω W
}. Note that for a given λ, pλ and
i
f (λ) can be computed by solving a shortest path problem.
If f (λ) ≤ 1, then clearly pλ is a solution to
DMCPS(s, t, W). If 1 < f (λ) < f (λ ), then pλ is a better
approximate solution to DMCPS(s, t, W) than pλ . Therefore
minimizing f (λ) over Λ leads to a heuristic algorithm for
solving OMCPS(s, t, W). In [20], Xue demonstrated that in
the case of K = 2, the corresponding heuristic is very efficient
and effective. For K ≥ 3, the well-known simplex method of

226

Nelder and Mead [12] for nonlinear minimization turns out to
be very efficient and effective to this problem. Due to space
limitations, we will refer the readers to [12] for details of
the simplex method. Our simplex heuristic applies the NelderMead simplex method to compute λopt ∈ Λ such that f (λopt )
is close to min{f (λ)|λ ∈ Λ}. It then output the path pλopt as
an approximate solution to OMCPS(s, t, W).
VI. C OMPUTATIONAL E XPERIENCES
We have implemented all three algorithms and tested on
randomly generated graphs on a 1.7GHz P4 with 512Mb of
memory running redhat linux. The algorithms are random [9],
simplex, and FPTAS. The performance is quite stable. Table
I shows the result on a graph with 80 nodes and 160 edges
with K = 3 edge weights randomly distributed in [0, 10], with
 = 0.5.
TABLE I
S IMULATION RESULT ON A RANDOM GRAPH

(W1 ,W2 ,W3 )
(7, 2, 5)
(18, 3, 13)
(7, 7, 7)
(8, 11, 11)

random
Not Found
Not Found
Not Found
(8, 11, 11)

(8,
(8,
(8,
(8,

simplex
11, 11)
11, 11)
11, 11)
11, 11)

3.92
3.31
6.25
5.45

FPTAS
(18, 3,
(18, 3,
(8, 11,
(8, 11,

13)
13)
11)
11)

The first row of the table shows the results with (7, 2, 5) as
constraints. Algorithm random quits, claiming that it cannot
find a path. No other information is provided. Algorithm simplex finds a path with weights (8, 11, 11). Algorithm FPTAS
finds a path with weights (18, 3, 13). This path violates the
constraints by a factor of 2.6. Since it is a 1.5-approximation
to the optimal solution, we conclude that there is no path
satisfying all three constraints. The running time of FPTAS
is 3.92 seconds. The running times of the two heuristics are
negligible. Other rows are similar.
VII. C ONCLUSIONS

R EFERENCES
[1] S. Chen and K. Nahrstedt, On finding multi-constrained paths, IEEE
ICC’98, pp. 874–879.
[2] S. Chen and K. Nahrstedt, An overview of quality-of-service routing for
the next generation high-speed networks: problems and solutions, IEEE
Network, Vol. 12, No. 6(1998), pp. 64–79.
[3] F. Ergun, R. Sinha and L. Zhang, QoS routing with performancedependent costs, IEEE INFOCOM’00, pp. 137–146.
[4] M.L. Fredman and R.E. Tarjan, Fibonacci heaps and their uses
in improved network optimization algorithms, Journal of the ACM,
Vol. 34(1987), pp. 596–615.
[5] M.R. Garey and D.S. Johnson, Computers and Intractability: A Guide
to the Theory of NP-Completeness, W.H. Freeman, 1979.
[6] A. Goel, K.G. Ramakrishnan, D. Kataria and D. Logothetis, Efficient
computation of delay-sensitive routes from one source to all destinations,
IEEE INFOCOM’01, pp. 854–858.
[7] R. Hassin, Approximation schemes for the restricted shortest path
problem, Mathematics of Operations Research, vol. 17(1992), pp. 36–
42.
[8] J.M. Jaffe, Algorithms for finding paths with multiple constraints,
Networks, Vol. 14(1984), pp. 95–116.
[9] T. Korkmaz and M. Krunz, A randomized algorithm for finding a path
subject top multiple QoS requirements, IEEE INFOCOM’01, pp. 834–
843.
[10] T. Korkmaz and M. Krunz, Mult-constrained optimal path selection,
Computer Networks, Vol. 36(2001), pp. 251–268.
[11] D.H. Lorenz and D. Raz, A simple efficient approximation scheme
for the restricted shortest path problem, Operations Research Letters,
Vol. 28(2001), pp. 213–219.
[12] J.A. Nelder and R. Mead, A simplex method for function minimization,
The Computer Journal, Vol. 7(1965), pp. 308–313.
[13] R. Ravindran, K. Thulasiraman, A. Das, K. Huang, G. Lou and G. Xue,
Quality of service routing: heuristics and approximation schemes with
a comparative evaluation, IEEE ISCAS’02, accepted for publication.
[14] H.F. Salama, D.S. Reeves and Y. Viniotis, A distributed algorithm for
delay-constrained unicast routing, IEEE INFOCOM’97, pp. 84–91.
[15] Z. Wang and J. Crowcroft, Bandwidth-delay-based routing algorithms,
IEEE GLOBECOM’95, pp. 2129–2133.
[16] Z. Wang and J. Crowcroft, Quality-of-service routing for supporting
multimedia applications, IEEE Journal on Selected Areas in Communications, Vol. 14(1996), pp. 1228–1234.
[17] A. Warburton, Approximation of Pareto optima in multiple-objective,
shortest path problem, Operations Research, Vol. 1(1987), pp. 70–79.
[18] X. Xiao and L.M. Ni, Internet QoS: a big picture, IEEE Network, vol. 13,
no. 2, 1999, pp. 8–18.
[19] X. Yuan and X. Liu, Heuristic Algorithms for Multi-constrained QoS
routing, IEEE INFOCOM’01, pp. 844–853.
[20] G. Xue, Primal-dual algorithms for computing weight-constrained
shortest paths and weight-constrained minimum spanning trees, IEEE
IPCCC2000, pp. 271–277.

In this paper, we have presented a fully polynomial time
approximation scheme for the multi-constrained path selection
problem with K ≥ 2 additive quality of service constraints.
We note that several other researchers have done excellent
work along the line. In [19], a polynomial time heuristic are
presented for K ≥ 2 by changing the granularity of K − 1
constraints. In [6], a polynomial time approximation schemes
for the case of K = 2 is designed by approximating both
constraints. Combining our scaling technique and the method
of [6], we can improve the time complexity presented in this
paper. Such extensions will be presented in the journal version
of this paper.
ACKNOWLEDGEMENT
The research of Guoliang Xue and Rakesh Banka was
supported in part by ARO grant DAAD19-00-1-0377.

227

2016 12th Int. Conference on the Design of Reliable Communication Networks (DRCN 2016)

A Network Planning and Management Tool for
Mitigating the Impact of Spatially Correlated
Failures in Infrastructure Networks
Arun Das∗ , Arunabha Sen∗ , Chunming Qiao† , Nasir Ghani‡ , Nathalie Mitton§

∗ School of Computing, Informatics and Decision System Engineering, Arizona State University, Tempe, Arizona 85287, USA
† Department of Computer Science and Engineering, SUNY at Buffalo, Buffalo, NY 14201, USA
‡ Department of Electrical Engineering, University of South Florida, Tampa, FL 33620, USA
§ Inria, 40 Avenue Halley, 59650 Villeneuve D’ASCQ, France

Email: arun.das@asu.edu, asen@asu.edu, qiao@computer.org, nghani@usf.edu, nathalie.mitton@inria.fr
Abstract—Current practices of fault-tolerant network design
ignore the fact that most network infrastructure faults are
localized or spatially correlated (i.e., confined to regions). Network
operators require new tools to mitigate the impact of such
region based faults on their infrastructures. Utilizing the support
from the U.S. Department of Defense, and by consolidating a
wide range of theories and solutions developed in the last few
years, the authors of this paper have developed an advanced
Network Planning and Management Tool (NPMT) that facilitates
the design and provisioning of robust and resilient networks.
The tool provides multi-faceted network design, evaluation and
simulation capabilities for network planners. Future extensions of
the tool currently being worked upon not only expand the tool’s
capabilities, but also extend these capabilities to heterogeneous
interdependent networks such as communication, power, water
and satellite networks.

I.

I NTRODUCTION AND M OTIVATION

It is extremely important that planners for large wide area
networks have the right tools to design robust and resilient
networks that can effectively withstand large scale geographically correlated failures in their networks. Such failures can
be triggered by nature (hurricane or earthquake) or humans
(nuclear attack or conventional weapon attack over a large
geographical area). With research support from the U.S. Defense Threat Reduction Agency, an agency whose mission is to
protect the U.S. against Weapons of Mass Destruction (WMD),
such as nuclear, biological or chemical attacks, the authors
of this paper, over the last six years have developed a wide
ranging set of concepts and techniques for enhancing network
robustness against spatially correlated or region based faults.
We have recently incorporated these concepts and techniques
into a Network Planning and Management Tool (NPMT) [1]
for the benefit of network designers, planners and operators.
In this paper, we first describe the novel concepts developed
to design networks that are robust against region based faults,
and then describe how these concepts have been incorporated
into the NPMT. The goal of this paper is to bring to the
attention of the networking research community, and the
audience of the DRCN conference in particular, the existence
of NPMT as a tool that consolidates a large body of work on
spatially correlated failures, and as a tool that can be used by
This work was supported in part by the NSF grant 1441214, and by grant
HDTRA1-14-C-0015 from the U.S. Defense Threat Reduction Agency

978-1-4673-8496-4/16/$31.00 ©2016 IEEE

71

the community to meet the needs for robust network design
against spatially correlated failures. In essence, this paper’s
contribution should not be measured in terms of new analytical
findings, but in terms of service to the networking community.
We use the term WMD attack to imply a large scale
geographically correlated failure such as failures caused by an
earthquake, hurricane or nuclear attack. The characteristic of a
WMD attack is massive but localized faults. The connectivity
of a network [2] is generally accepted as a metric for evaluating
the fault-tolerance capability of a network [3]. If a network’s
connectivity is k + 1, then the network can tolerate up to k
faults, implying that the surviving network remains connected
even after k failures. The connectivity metric, however, has no
way of capturing locality, i.e., the faulty nodes/edges may be
close or far away from each other. Thus, the connectivity metric cannot distinguish between faults that are geographically
correlated (a WMD fault characteristic), and faults that are not.
Connectivity as a metric also fails to capture other important
structural properties of the network such as the number or
size of the connected components [2] into which a network
disintegrates when the number of failed nodes/edges exceeds
the node/edge connectivity of the network.
Recognizing the limitations of connectivity as a metric
for capturing the special characteristics of geographically correlated failures, the authors of [4] introduced the notion of
region-based connectivity. A region may be defined either with
reference to the network graph or to the network geometry (i.e.,
layout of the network in a two or three dimensional space). For
example, a region may be defined as a subgraph with diameter
d (where the diameter of a graph is defined as the maximum
of the shortest path distance between a pair of nodes, taken
over all source-destination node pairs). Or, a region may also
be defined as a collection of nodes and edges in the network
graph layout that is covered by a circular area in that layout.
Figure 1(a) shows an example of a circular region-based fault.
The NPMT described in this paper is intended to support
design and analysis of single layered and multi-layered interdependent heterogeneous networks. In essence, the NPMT
is particularly suitable for planning and design of critical
infrastructures. For example, from the single network layer
perspective, the NPMT enables backbone communication network providers, such as, AT&T, Sprint, Qwest and Level 3
Communications, to (i) identify the most vulnerable parts of

2016 12th Int. Conference on the Design of Reliable Communication Networks (DRCN 2016)

(a)

(b)

(c)

Fig. 1: (a) Network with circular fault region, (b) Optical fiber network of a major U.S. provider, (c) Optical Fiber network of a major European
provider disrupted by a WMD attack

their network against a WMD attack, and (ii) reinforce the
network with least cost to eliminate or significantly reduce
the threat of network disruption due to a WMD attack. Figure
1(b) shows the backbone network of a major U.S. provider
and Figure 1(c) shows how the backbone network of a major
European provider can potentially be disrupted by a WMD
attack. From a multi-layer perspective the NPMT can be used
for design and analysis of smart cities, where heterogeneous
networks ranging from disparate telecom networks (such as
2G, 3G, WiFi, Bluetooth, etc.) to water, electricity and gas
distribution networks, form a complex interdependent ecosystem. Subsequently, failures in one network, for example a leak
in the water distribution network, may deteriorate other nearby
(spatially correlated) infrastructures such as gas or electricity
whose pipes and cables may get affected due to the leak. In this
context, a tool like NPMT can be an asset for utility companies
and smart city planners to quickly perform (i) root cause
analysis of failure, and (ii) forecast fault evolution, to direct
repairs and maintenance towards specific network components
and restrict fault propagation. To the best of our knowledge no
such tool is available today that supports features for planning
and designing of single layer and multi-layer interdependent
networks in the presence of spatially correlated faults.
Several studies in the network research community have
focused on different aspects of spatially correlated or regionbased faults in networks [5-11], however, to the best of our
knowledge there does not exist an executable platform that
consolidates the findings and techniques of these studies into
a readily usable tool. The NPMT is intended to fill that gap
and be such a platform that can incorporate the outcomes
developed in [5-11] into executable modules to be integrated
into the NPMT. This will allow network designers, planners
and operators to use the results of these studies in their real
world operational networks.
The rest of the paper is organized as follows: In Section
II we present an overview of the underlying concepts and
theoretical results that the NPMT operates on. In Section III we
outline the capabilities of the NPMT and Section IV concludes
this paper.
II.

C ONCEPTS , M ETRICS AND S OLUTION T ECHNIQUES

In this section we give a brief overview of the underlying
concepts, metrics and solution techniques that the NPMT

978-1-4673-8496-4/16/$31.00 ©2016 IEEE

72

utilizes to carry out its functional operations. The NPMT is
built as a modular execution engine that can execute smaller
reusable modules to perform desired operations on a network
topology. In this respect, the current version of the NPMT
comprises of different modules that deal with both static and
dynamic aspects of robust and resilient network design. The
modular approach allows design, development and testing of
these modules to be carried out independently, and defers
integration into the NPMT until a module meets it’s functional
requirements. In the following sub-sections we give a brief
overview of the analytical foundations of these modules. It may
be noted that, as of writing this paper not all modules have
been implemented and integrated into the NPMT. Accordingly,
we highlight our ongoing work in the discussion below.
A. Region-Based Fault Metrics Computation Module
As outlined in Section I, connectivity as a metric fails to
capture several characteristics of the network in presence of
spatially correlated failures. For instance, the number or size of
the connected components into which a network disintegrates
in the presence of a spatially correlated fault is not captured by
the traditional connectivity metric. In order to overcome these
gaps and capture such network state characteristics, several
metrics and their computation techniques have been proposed
by the research community. For a given network topology, the
NPMT can analyze the network and compute metrics pertinent
to network state in the presence of spatially correlated faults.
The following metrics are supported by the NPMT:
Region-based Connectivity Metric Computation
Region based connectivity can be considered under two fault
models – (i) Single Region Fault Model (sRFM) where faults
are confined to a single region [4], and (ii) Multiple Region
Fault Model (mRFM) where faults are confined to k regions
for some specified k [12].
Formally, in sRFM, the single-region-based (node) connectivity of graph G with a specified definition of region R,
sκR (G), is defined as follows: Suppose that {R1 , . . . , Rk } is
the set of all possible regions of the graph G. Consider a
k-dimensional vector T whose i-th entry, T [i], indicates the
number of nodes in region Ri whose failure will disconnect
the graph G. If the graph G remains connected even after the
failure of all nodes of the region Ri then T [i] is set equal to

2016 12th Int. Conference on the Design of Reliable Communication Networks (DRCN 2016)

∞. The region-based connectivity of a graph G with region
R, is then computed as follows:
sκR (G) = min T [i]
1≤i≤k

In mRFM, the multi-region-based (node) connectivity of graph
G with a specified definition of region R, mκR (G), is defined
as the minimum number of regions whose removal (i.e.,
removal of all nodes in the regions and edges incident on them)
will disconnect the graph.
Polynomial time algorithms to compute region-based connectivity in sRFM was presented in [4]. The NPMT contains an
implementation of this algorithm that can be used to compute
the Region-based Connectivity for a given network topology.
Region-based
Component
Decomposition
(RBCDN) Metric Computation

Number

Proposed by the authors of [13], the Region-Based Component Decomposition Number, or RBCDN of graph G = (V, E)
with a specified definition of region R is defined the following
way: Suppose that {R1 , . . . , Rk } is the set of all possible
regions of the graph G. Consider a k-dimensional vector C
whose i-th entry, C[i], indicates the number of connected
components in which G decomposes when all entities in Ri
fails. RBCDN of a graph G with region R is defined as follows:
δR (G) = max C[i]
1≤i≤k

RBCDN as a metric provides an insight into the worst case
scenario on how fragmented a network can become in the
presence of a spatially correlated fault. In [13] the authors
propose techniques to compute the RBCDN and the NPMT
provides an implementation of this algorithm that can be used
on user selected network topologies.
Region-based Smallest/Largest Component Size Metric
Computation
The Region-Based Smallest (Largest) Component Size, or
RBSCS/RBLCS was proposed in [14], and is defined for
a graph G = (V, E) with a specified definition of region
R, as follows: Suppose that {R1 , . . . , Rk } is the set of all
possible regions of the graph G. Consider a k-dimensional
vector CS (CL ) whose i-th entry, CS [i] (CL [i]), indicates the
size of the smallest (largest) connected component in which G
decomposes when all nodes in Ri fails. The RBSCS αR (G)
and RBLCS βR (G) of graph G with region R is defined as:
αR (G) = min CS [i] and βR (G) = min CL [i]
1≤i≤k

1≤i≤k

The RBLCS and RBSCS metrics provide insights on how well
a network’s performance degrades in the presence of regionbased faults. Depending on the needs of graceful performance
degradation, networks designers may choose to design networks that have a small value of RBCDN (δR (G)) and a high
value of either RBLCS (αR (G)) or RBSCS (βR (G)). The
NPMT allows the user to compute the RBLCS and RBSCS
metrics for a chosen network topology.
B. Distinct Regions Computation Module
It may be noted that all the previously defined metrics
operate on a given graph and a set of regions. Thus, there

978-1-4673-8496-4/16/$31.00 ©2016 IEEE

73

is a need for techniques to compute the set of regions, given
a network and some fault specification. In [14], given a graph
G’s layout on a two-dimensional plane and a fault radius r,
the authors provide a polynomial time algorithm to compute all
distinguishable or distinct circular regions with radius r. Two
fault regions are considered indistinguishable if they contain
the same set of links and nodes. The authors considered both
wired networks, where nodes and edges can be part of a failure
region, and wireless networks, where only nodes can be part
of a failure region. It was shown in [14] that the number of
distinct regions in wireless and wired networks are O(n2 )
and O(n4 ) respectively, and that all distinct regions can be
computed in O(n6 ) time, where n is the number of nodes.
The NPMT is bundled with an implementation of the
technique outlined in [14]. Given a network topology and a
fault radius, the NPMT can compute all distinct regions of
the network which can then be used by other modules of
the NPMT, such as the Metric Computation Module and the
Region-disjoint Path Computation Module (discussed next).
C. Region-disjoint Paths Computation Module
For a graph G = (V, E), a set of region-disjoint paths
P between a source node s and destination node d with a
specified definition of region R, is defined as follows: Suppose
that {R1 , . . . , Rk } is the set of all possible regions of graph
G and path Pu ∈ P contains a set of nodes and edges from
G such that Pu forms a path from s to d, {s, d} ∈ V . Then,
for every pair of paths {Pu , Pv } ∈ P, u 6= v, Pu and Pv are
region-disjoint, i.e. there is no region in R that both the paths
traverse. Formally, region-disjoint paths are defined as follows,
for all i = 1, . . . , k:
|(Pu ∩ Ri ) ∩ (Pv ∩ Ri )| = 0, ∀{Pu , Pv } ∈ P, u 6= v
Although region-disjoint path computation has been addressed
in [8], the authors consider a model where faults do not cause
edges to fail unless a failed edge is associated with a failed
node. This assumption is considerably restrictive and possibly
unusable for designers of larger networks where spatially
correlated faults can affect nodes and edges independently.
In order to overcome this limitation the NPMT supports
computation of region-disjoint paths in the presence of circular
faults using an Integer Linear Program (ILP) that doesn’t
presuppose any such restrictions. The NPMT is capable of
computing two region-disjoint paths from given source and
destination nodes such that the sum of lengths of the two
paths is minimum. Also, as the source (destination) node is
part of a region that is traversed by both paths (as both paths
have the same starting and ending points), no region disjoint
path may exist. To accommodate this situation the NPMT
accommodates the use of no-fault zones – a circular area
around the source and destination nodes that is immune to
faults. Future extensions of this module include computing
more than two paths, and including other selection criteria such
as minimizing the maximum path length.
D. Region-based Fault Tolerant Distributed File Storage Module
In the preceding discussions the importance of a node
in keeping the network connected is emphasized, however,
individual nodes can also act as data stores of the network

2016 12th Int. Conference on the Design of Reliable Communication Networks (DRCN 2016)

and the removal of a node from a network (due to a regionbased fault), may not only cause connectivity losses, but also
data losses. To address such data loss risks, distributed storage
techniques are often employed that enhances data survivability
in the presence of faults. One such technique is redundancy,
such as by (i) storing multiple copies of the entire file, or
(ii) storing different fragments of the same file at different
nodes in the network. In the popular (N, K), N ≥ K file
distribution scheme, from a file F of size |F |, N segments
of size |F |/K are created in such a way that it is possible to
reconstruct the entire file by accessing any K segments. For
such a reconstruction scheme to work, it is essential that the
K segments of the file are stored in nodes that are connected
to each other in the network. However, in the event of failures,
the network may become disconnected (i.e., split into several
connected components) and K segments may not be accessible
in the residual network to reconstruct the file F .
From the context of data survivability in the presence of
spatially correlated faults in networks, the NPMT supports
a “Region-based Distributed File Storage Module” that implements an algorithm proposed in [11] that ensures that:
(i) even when the network is fractured into disconnected
components due to a region-based fault, at least one of the
largest components will have access to at least K distinct file
segments with which to reconstruct the entire file, and (ii)
the total storage requirement is minimized. As of writing this
paper, this module is currently under development and will be
part of the NPMT upon its completion.
E. Robust Multi-layer Interdependent Network Design Module
In today’s world, a multitude of heterogeneous interconnected networks form a symbiotic ecosystem that supports all
of the economic, political and social aspects of human life.
For example, the critical infrastructures of the nation such
as the power grid and the communication network are highly
interdependent on each other, and any adverse effects on one
network can affect the other network. Thus, isolated network
analysis is no longer sufficient to design and operate such
interconnected and interdependent network systems.
Recognizing this need for a deeper understanding of the
interdependency in such multi-layered network systems, significant efforts have been made by the research community in the
last few years, and accordingly, a number of analytical models
have been proposed to analyze such interdependencies [15-17].
However, most of these models are simplistic and fail to
capture the complex interdependencies that may exist between
entities of the power grid and communication networks. To
overcome the limitations of existing models, the authors of
[18] have proposed the Implicative Interdependency Model
(IIM) that is able to capture such complex interdependencies.
Utilizing this model, several problems on multi-layer interdependent networks have been studied, such as (i) identification
of the K most vulnerable nodes [18], (ii) root cause analysis
of failures [19], (iii) the entity hardening problem [20], (iv)
the smallest pseudo-target set identification problem [21], and
(v) the robustness analysis problem [22].
This module will support modeling of interdependent networks using IIM, and analysis of such networks using the
techniques proposed in [18-22]. The module is currently under
development and will be part of the NPMT upon completion.

978-1-4673-8496-4/16/$31.00 ©2016 IEEE

74

F. Module for Progressive Recovery from Region-based Failures
With this module, the NPMT addresses post-fault recovery
techniques in the aftermath of region-based faults on multilayer interdependent networks. To restore an interdependent
network system from a post-fault scenario to its pre-failure
state, all the faulty network entities (nodes/edges) have to be
repaired or replaced. However, resource limitations may prevent simultaneous restoration of all failed units of the network.
Accordingly, the failed units have to be restored in a sequenced
manner. As each network entity in its operational state adds
some utility value to the interdependent network system, when
a unit recovers from a failed state to an operational state,
the unit starts providing some “benefit” to the system. Since
different units have different utility values to the system, the
sequence in which the failed units are restored is important as
the recovery sequence determines the cumulative system utility
during the recovery process.
As discussed in Section II-E, the IIM provides a powerful
technique for modeling dependencies in multi-layer interdependent networks. Using this model the authors of [23] studied
the progressive recovery problem in interdependent networks
with the objective of maximizing system utility during the
system recovery process. This module implements the progressive recovery algorithm of [23], and can be used to sequence
recovery of network entities from a post-fault to a pre-fault
network state that maximizes system utility during the recovery
process. The module is currently under development and will
be part of the NPMT upon its completion.
III.

A RCHITECTURE AND S YSTEM C APABILITIES

In this section we first outline the system architecture, and
then discuss the different capabilities of the NPMT.
A. System Architecture
View

Service

Fault Analyzer

Path Analyzer

Topology Manager

Profile Manager

Traffic and Fault Simulator

Core Modules:
Network Topology Manager
Region-Based Fault Analysis

Controller

Disjoint Path Analysis

Visualization Engine

Simulation Engine

Execution Engine

Common Modules:

Repository

N/W Fault Impact Analyzer

Path Planning Algorithms

Model
Simulation Data

User/Roles

Path Archive

Fault Generation Engine

N/W Topologies

Fault Archive

Library Faults

Request Generation Engine

Fig. 2: The NPMT High-Level Architecture

The NPMT is implemented as a web-application that
allows the user to remotely connect and operate the tool from
a browser. The web-application follows the standard three-tier
architecture and has a client tier, application tier, and database
tier. The tool has been developed following the Model-ViewController (MVC) design pattern. Figure 2 outlines the high
level architecture and some of the components of the tool.
The tool is currently accessible from Arizona State University’s WAN, and runs from our testbed server. The tool’s webapplication is deployed on an Apache Tomcat 7 instance, and

2016 12th Int. Conference on the Design of Reliable Communication Networks (DRCN 2016)

(a)
(b)
Fig. 3: (a) Topology Manager – create, edit and manage network topologies, (b) Fault Analyzer – generic fault analysis, metric computations

the repository used is MySQL. The application tier business
logic for operations on network topologies, such as RegionBased Fault Analysis and Region Disjoint Path Analysis, are
implemented in Java. Additional packages and libraries, such
as IBM ILOG CPLEX Optimization Studio libraries (required
for solving Integer Linear Programs), are setup and made
available on the testbed server. Our testbed server is a 64bit Intel Core 2 Quad Core (2.66 GHz) system with 8 GB of
RAM running an Ubuntu 14.04 instance.
B. System Capabilities
The NPMT is designed to be used by following a three
step workflow comprising of (i) Network Creation, (ii) Network Analysis, and (iii) Network Simulation. Accordingly, the
individual features and the executable modules of the NPMT
are built around these three workflow steps. The following list
enumerates the current high-level features of the tool and the
corresponding workflow step that each feature emulates:
1)
2)
3)
4)

Topology Management (Network Creation)
Fault Analysis (Network Analysis)
Path Analysis (Network Analysis)
Traffic and Fault Impact Simulation (Network Simulation)

Each of the above features are accessible from a tabbed
interface of the tool and can be navigated to from any part of
the web-application. In the following subsections we discuss
each of the features and provide a brief functional overview.
Topology Management
Network Creation is the first step of the NPMT workflow
and the Topology Manager tab allows the user to create, edit,
save and delete network topologies. The Topology Manager
displays a geographical map interface that can be accessed
to manage network topologies. The displayed map tiles are
rendered from OpenStreetMap [24]. The NPMT uses the
OpenLayers API to support an user interactive map interface.
To create the topology and place nodes and edges on the
map, the user can either point-and-click on the map itself,
or can type in specific latitude and longitude coordinates and
then proceed to add the network entity. Capacities for each

978-1-4673-8496-4/16/$31.00 ©2016 IEEE

75

edge (in Gigabits per second), can also be specified during the
edge creation process. Once a network topology is created, the
topology must be saved to be used for Network Analysis and
Network Simulation. The topologies are saved on the NPMT
server and can be loaded back into the Topology Manager to
edit any entity or attribute of the network.
Figure 3(a) shows a screen grab of the Topology Manager.
As seen in the figure, the map interface is on the right and the
user interact-able menu is on the left. The user can click on the
map to to add nodes and edges, or can alternatively type in the
latitude and longitude coordinates in the input fields available
on the menu. The menu also lists the nodes and edges that are
part of the topology. Selecting an edge or node from these lists
highlights the network entity on the map (in yellow), and the
user can then proceed to remove the entity from the network if
necessary. The displayed map overlays can be toggled from a
dropdown menu available on the map (in blue in Figure 3(a)).
Finally, as seen in Figure 3(a), options for saving, loading, and
deleting topologies are available to the user directly below the
displayed map’s dimensions.
Fault Analysis
Once network topologies are created from the Topology Manager, the Fault Analyzer can be used to analyze the created
networks for their resilience in the presence of spatially correlated faults. In the context of the NPMT, network resiliency
is measured by how well the network performs when benchmarked against the metrics outlined in Section II-A. It may
be noted that the metrics of Section II-A emphasize resilience
from the aspect of connectivity in the presence of a spatially
correlated fault. For example, the more number of disconnected
components a network has due to a fault, the worse is the
network’s resilience (as captured by the metric RBCDN). It
may be noted that, for the purpose of this analysis the NPMT
assumes that any network entity (nodes/edges), that fall within
the fault area are all rendered inoperable, i.e. the fault model
is deterministic and if a network entity falls within the fault
region, it necessarily fails. To carry out this analysis, the user
first selects a network topology and can then choose to either
perform a generic fault analysis, or a specified fault analysis.
These analyses are described below.

2016 12th Int. Conference on the Design of Reliable Communication Networks (DRCN 2016)

(a)
(b)
Fig. 4: Fault Analyzer - Specified Fault Analysis. (a) User specified fault coordinates, (b) Fault impact of the user defined fault and an imported
library fault (coordinates for the state of California, USA)

Generic Fault Analysis: In the generic fault analysis, for a
selected network topology, the user specifies a fault feature and
the tool computes the values of the individual metrics listed
in Section II-A. The NPMT can generically analyzes circular
faults, and the supported fault feature is the fault radius r.
As shown in Figure 3(b), the user can specify the fault
radius r from the left menu. The tool then performs the
generic fault analysis by (i) computing all the distinct regions with radius r using the techniques implemented in
the module “Distinct Regions Computation Module” (Section
II-B), and (ii) computes the individual metrics using the
techniques implemented in the module “Region-Based Fault
Metrics Computation Module” (Section II-A). The results
are subsequently reported back to the user. For the network
selected in Figure 3(b) and radius r = 500 km., the computed
Region-based Component Decomposition Number (RBCDN)
is 2, the Region-based Largest Component Size (RBLCS) is 9
and the Region-based Smallest Component Size (RBSCS) is
1. Finally, the number of distinct regions computed is 112.

from to simulate fault impact on a network. The current set
of library faults consist of the coordinates of the 50 states
of the USA. The inclusion of a fault library in the NPMT is
to provide the user with pre-defined fault scenarios based on
known fault patterns, faults centered at a target of interest, or
recorded faults. For example, fault impact zones of Level 4
hurricanes such as hurricane Katrina or hurricane Sandy.
As shown in Figure 4(a), to specify the exact coordinates of
the fault region the user can either type in the exact coordinates
of the fault region coordinates, or can click on the map to add
such coordinates. The user also has the option for importing
library faults. Once all the fault regions are defined, the NPMT
can simulate the impact of the fault on the selected network.
In Figure 4(b), apart from the user specified fault region,
the boundary of the state of California has been imported
from the fault library and the selected network has been
analyzed for these two fault regions. The updated map shows
the impacted nodes and edges in gray, while the operable
nodes and edges are shown in green and black respectively.
The connected components are shown with a green region. As
seen in Figure 4(b) the menu displays impact statistics such
as, the number of surviving nodes/edges and the number of
connected components. The user is provided with the options
to save the analysis results and the defined fault regions.

As shown in Figure 3(b), the user is also presented with
sample worst case fault scenarios where a distinct fault causes
the network to fragment into the same number of components
as the RBCDN. Selecting one of the listed faults updates
the displayed network with the fault’s impact. In Figure 3(b)
the fault centered at 36.249◦ N , −85.696◦ E is selected. The
nodes and edges rendered inoperable by the fault are grayed
out, while the surviving nodes and edges are shown in green
and black respectively. The connected components in the
fragmented network are highlighted by a light-green region. In
this example, the loss of the two grayed out nodes causes the
network to fragment into two disconnected components: one
with 9 components, and the other with 1 component. Options
for saving the analysis results are available from the menu.

The Path Analyzer allows the user to compute paths between
source and destination nodes that provide protection against
spatially correlated faults. As in the Fault Analyzer, the Path
Analyzer allows the user to specify a fault feature, and then
computes paths between a given source and destination node
pair such that (i) at least one of the paths survive in the
presence of one or more spatially correlated faults, and (ii)
satisfy other network resource constraints.

Specified Fault Analysis: In the specified fault analysis, the
user can provide the exact coordinates of one or more faults
and visualize the impact of these faults on the selected network.
The user has the option to save and load faults to visualize the
impact of a fault on different networks. The NPMT also comes
bundled with a set of library faults that the user can choose

In the current version of the tool circular faults are considered and the supported user specified fault feature is the fault
radius r. The number of spatially correlated faults considered
for path analysis is one, and the number of paths computed is
two, i.e. the NPMT computes two paths such that if a single
circular fault with radius r occurs anywhere in the network,

978-1-4673-8496-4/16/$31.00 ©2016 IEEE

76

Path Analyzer

2016 12th Int. Conference on the Design of Reliable Communication Networks (DRCN 2016)

(a)
(b)
Fig. 5: Path Analyzer - Region disjoint paths between a source and destination nodes for given fault radius (r) and no-fault zone radius (nfr )
(a) r = 100 km., nfr = 300 km. (b) r = 120 km., nfr = 300 km.

at least one of the two paths computed will be unaffected by
the fault. The network resource constraint supported is that the
sum of lengths of the two paths computed must be minimum.
It may be noted that a single fault can also render inoperable the source node and/or the destination node. Thus there
always exists a fault region such that no region disjoint paths
exists as the source (destination) node is impacted due to the
fault. To accommodate this case when the source and/or destination nodes themselves are part of the fault region, the NPMT
supports a “No-Fault Zone” parameter. The user can specify a
no-fault zone radius nfr for the source and destination nodes
that reserves two circular areas with radius nfr centered at
the source and destination nodes such that network entities, or
parts of an entity (such as an edge segment), that falls within
this no-fault zone are immune to faults.
Figures 5(a) and 5(b) show screen grabs of the path
analyzer computation for different input values of fault radius
(r). The no-fault zone is set to a radius of nfr = 300 km. and
is shown as white circular regions centered at the source and
destination nodes. The computed paths are shown in orange
and blue, and the lengths of each of these to paths are reported
in the left menu. The effect of the path selection criteria, i.e.
the sum of the lengths of the two paths must be minimum,
is also visible in Figures 5(a) and 5(b). In Figure 5(a) when
r = 100 km., the sum of lengths of the two paths is 5793.24
km., however in Figure 5(b) increasing r to 120 km. the
previously computed paths are no longer feasible as a region
fault exists that can impact both these paths. Hence, new paths
are computed and the sum of the new lengths is 5921.69 km.
Traffic and Fault Impact Simulation
For a selected network, the Traffic and Impact Simulator allows
users to generate traffic and faults to analyze the impact of
faults on a load bearing network. To perform this analysis,
a simulation schedule consisting of bandwidth requests and
faults is generated by the NPMT using user provided simulation parameters. Parameters such as total number of time steps
in the schedule, total number of requests in the schedule, minimum/maximum request bandwidth and minimum/maximum
request hold times can be specified by the user. The source and
destination nodes for each request can be generated randomly,

978-1-4673-8496-4/16/$31.00 ©2016 IEEE

77

or can be user specified. For introducing faults in the schedule,
the user can specify the number of faults to introduce and can
either specify the exact fault coordinates, or introduce random
circular faults from the set of all possible distinct circular faults
for a specified fault radius. Time intervals of the faults can be
user specified, or can be randomly generated by the NPMT.
Using the request and fault settings, the NPMT then generates
a time stepped simulation schedule of requests and faults. The
user can then select the routing algorithm to be used in the
simulation and then proceed to run the simulation.
As shown in the screen grabs of Figures 6(a) and 6(b), the
left menu of the Traffic and Impact Simulator contains the fault
and simulation parameter fields that can be used to generate
the schedule and run the simulation. The tables below the
map’s dimensions allow the user fine grained control over the
requests and faults that will be simulated. Once the simulation
is complete, for each time interval the network state can be
visualized from the “Event Simulation Results” table. The user
can click on a row of this table to visualize the network state
on the map for that specific time interval. The user can also
“play” the simulation results and the NPMT will iterate over
all the time steps and update the map with the network state
at each step. In Figures 6(a) and 6(b) the impact of a fault
and the corresponding response of the network is shown. In
Figure 6(a) the network is fault free, but in Figure 6(b) a fault
is introduced and an edge is rendered inoperable. It can be
seen that the red and yellow flows of Figure 6(a) are impacted
by the fault, however, as bandwidth is available, in Figure 6(b)
the flows are rerouted in response to this fault.
Performance Analysis: For our preliminary performance analysis of the tool, we used a 40 node test network where each
node had a degree of at least 3. Using two users, the Path
Analysis, Fault Analysis and Traffic and Impact Simulator
modules were executed concurrently on the test network using
different input parameters. In these tests it was observed that
for both users, computations were completed within a minute
of the user request. The Path Analysis module, however,
required variable computation time depending on the source
and destination nodes chosen. This is due to the fact that the
Path Analysis computation is performed as an ILP, however,
in all our tests, the path computations took at most 3 minutes.

2016 12th Int. Conference on the Design of Reliable Communication Networks (DRCN 2016)

(a)
(b)
Fig. 6: Traffic and Fault Impact Simulator (a) Pre-Fault network state, (b) Post-Fault network state with rerouted red and yellow flows

IV.

C ONCLUSION

In this paper we presented a summary of the work done
towards developing a Network Planning and Management Tool
(NPMT), intended to support design and analysis of single
layer and multi-layer networks in the presence of spatially
correlated faults. We highlighted that the NPMT is particularly
suitable for planning and design of critical infrastructures.
We described the underlying novel concepts that have been
developed to enhance robustness of networks in presence of
region based faults, and then described how those concepts
have been incorporated into the NPMT. The goal of this
paper was to bring to the attention of the networking research
community, and to the audience of the DRCN conference
in particular, about the existence of NPMT as a tool that
consolidates a large body of work on spatially correlated faults.
To the best of our knowledge no such tool is available today
that supports planning and designing of single layer and multilayer networks in the presence of spatially correlated faults.
R EFERENCES
[1] NEXT Lab, Arizona State University. The Network Planning and
Management Tool. [Online]. Available: http://netsci.asu.edu/networktool/
[2] R. Diestel, Graph Theory. Springer, 2005.
[3] E. Ganesan and D. K. Pradhan, “The Hyper-deBruijn Networks: Scalable
Versatile Architecture,” IEEE Transactions on Parrallel and Distributed
Systems, vol. 4, no. 9, September 1993.
[4] A. Sen, B. H. Shen, L. Zhou, and B. Hao, “Fault-tolerance in Sensor
Networks: A New Evaluation Metric,” in Proceedings of IEEE Infocom,
Barcelona, Spain, April 2006, pp. 1–12.
[5] S. Neumayer and E. Modiano, “Network reliability with geographically
correlated failures,” in INFOCOM, 2010 Proceedings IEEE. IEEE,
2010, pp. 1–9.
[6] Y. Cheng, M. T. Gardner, J. Li, R. May, D. Medhi, and J. P. Sterbenz,
“Optimised heuristics for a geodiverse routing protocol,” in 10th International Conference on the Design of Reliable Communication Networks
(DRCN), 2014, 2014, pp. 1–9.
[7] P. Agarwal, A. Efrat, S. Ganjugunte, D. Hay, S. Sankararaman, and
G. Zussman, “The resilience of wdm networks to probabilistic geographical failures,” in Proceedings of IEEE INFOCOM, 2011.
[8] S. Trajanovski, F. Kuipers, A. Ilic, J. Crowcroft, and P. Van Mieghem,
“Finding critical regions and region-disjoint paths in a network,”
IEEE/ACM Transactions on Networking, vol. 23, no. 3, pp. 908–921,
2015.
[9] S. Banerjee, A. Das, A. Mazumder, Z. Derakhshandeh, and A. Sen, “On
the impact of coding parameters on storage requirement of region-based
fault tolerant distributed file system design,” in Computing, Networking
and Communications (ICNC), International Conference on. IEEE, 2014,
pp. 78–82.

978-1-4673-8496-4/16/$31.00 ©2016 IEEE

78

[10] A. Mazumder, A. Das, C. Zhou, and A. Sen, “Region based fault-tolerant
distributed file storage system design under budget constraint,” in Reliable Networks Design and Modeling (RNDM), 2014 6th International
Workshop on. IEEE, 2014, pp. 61–68.
[11] A. Sen, A. Mazumder, S. Banerjee, A. Das, C. Zhou, and S. Shirazipourazad, “Region-based fault-tolerant distributed file storage system
design in networks,” Networks, vol. 66, no. 4, pp. 380–395, 2015.
[12] A. Sen, S. Murthy, and S. Banerjee, “Region-based connectivity-a new
paradigm for design of fault-tolerant networks,” in High Performance
Switching and Routing, 2009. HPSR 2009. International Conference on.
IEEE, 2009, pp. 1–7.
[13] S. Banerjee, S. Shirazipourazad, P. Ghosh, and A. Sen, “Beyond
connectivity-new metrics to evaluate robustness of networks,” in High
Performance Switching and Routing (HPSR), 2011 IEEE 12th International Conference on. IEEE, 2011, pp. 171–177.
[14] S. Banerjee, S. Shirazipourazad, and A. Sen, “Design and analysis of
networks with large components in presence of region-based faults,” in
International Conference on Communications (ICC). IEEE, 2011.
[15] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
“Catastrophic cascade of failures in interdependent networks,” Nature,
vol. 464, no. 7291, pp. 1025–1028, 2010.
[16] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, “Networks formed
from interdependent networks,” Nature Physics, vol. 8, no. 1, pp. 40–48,
2011.
[17] J.-F. Castet and J. H. Saleh, “Interdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,” PloS one, vol. 8, no. 4, p. e60402, 2013.
[18] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, “Identification of k most vulnerable nodes in multi-layered network using a
new model of interdependency,” in NetSciCom Workshop (INFOCOM
WKSHPS), Conference on Computer Communications. IEEE, 2014,
pp. 831–836.
[19] A. Das, J. Banerjee, and A. Sen, “Root cause analysis of failures in
interdependent power-communication networks,” in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910–915.
[20] J. Banerjee, A. Das, C. Zhou, A. Mazumder, and A. Sen, “On the entity
hardening problem in multi-layered interdependent networks,” in WIDN
Workshop (INFOCOM WKSHPS), 2015 IEEE Conference on Computer
Communications. IEEE, 2015, pp. 648–653.
[21] A. Das, C. Zhou, J. Banerjee, A. Sen, and L. Greenwald, “On the smallest
pseudo target set identification problem for targeted attack on interdependent power-communication networks,” in Military Communications
Conference, (MILCOM) 2015 IEEE. IEEE, 2015, pp. 1015–1020.
[22] J. Banerjee, C. Zhou, A. Das, and A. Sen, “On robustness in multilayer interdependent networks,” in Conference on Critical Information
Infrastructures Security (CRITIS). Springer, 2015.
[23] A. Mazumder, C. Zhou, A. Das, and A. Sen, “Progressive recovery from
failure in multi-layered interdependent network using a new model of
interdependency,” in Conference on Critical Information Infrastructures
Security (CRITIS). Springer, 2014.
[24] OpenStreetMap Contributors. OpenStreetMap. [Online]. Available:
www.openstreetmap.org

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

gStreams: A new technique for fast recovery with
capacity efficient protection in WDM mesh
networks
Arunabha Sen and Sudheendra Murthy

Subir Bandyopadyay

Dept. of Computer Science & Engineering
Arizona State University
Tempe, Arizona 85281
Email: {asen, sudhi}@asu.edu

School of Computing Science
University of Windsor
Windsor, Ontario N9B 3P4, Canada
Email: subir@uwindsor.ca

Abstract—In a recent paper, Kim and Lumetta [6] proposed a
capacity efficient protection scheme that provides fast recovery
in WDM mesh networks. They introduced the notion of a stream
that is utilized for this purpose. In this paper, we introduce
the concept of gStream, which is a more generalized form of
stream and develop efficient algorithms for maximizing capacity
utilization without sacrificing the benefit of fast recovery. We
show that the problem of finding the set of gStreams that
maximizes capacity utilization is NP-complete. We present (i) an
optimal solution for formation of streams and gStreams and (ii)
a heuristic solution. The results of our experimental evaluation
show that our heuristic provides near optimal solution to almost
all instances of the problem in a fraction of the time needed for
finding the optimal solution.

I. I NTRODUCTION
Survivability of all-optical networks has become an important area of research in recent years as evidenced by
the increased attention the topic has received among the
researchers [3], [9], [10]. Two techniques, protection at the
WDM layer and restoration at the IP layer have emerged to
be the leading contenders for fault management in optical
networks [4], [7]–[9]. Each of these two schemes has its
own advantages and disadvantages. In general, restoration is
slow but capacity efficient, whereas protection is fast but
less efficient in terms of network capacity utilization. This
is due to the fact that the protection schemes compute a
backup path and reserve network capacity (wavelength) on
the backup path whenever a primary path between a sourcedestination node pair is established. The protection scheme
can be further divided into two groups - path protection and
link protection. The path protection scheme can be further subdivided into i) (1+1) scheme and ii) (1:1) scheme. In (1+1)
scheme, the traffic is sent from the source to the destination
through the primary and the backup path all the time. As such,
this scheme provides fast recovery with minimal loss of data
due to the failure of a primary path. In the (1:1) scheme,
network capacity is reserved on the backup path but is not used
to transfer data, unless there is a failure on the primary path.
Supported in part by ARO grant W911NF-06-1-0354

Thus, the resources on the optical fiber links can be shared
among multiple backup paths as long as their primary paths
are disjoint. Moreover, the reserved capacity can be utilized
to carry low priority traffic that can be preempted in case of
failure on a primary path. In Dedicated Path Protection (DPP)
schemes, the backup paths do not share network resources. In
Shared Path Protection (SPP) schemes, two backup paths are
allowed to share network resources when the corresponding
primary paths are edge disjoint. The photonic cross-connects
(PXCs) are not configured ahead of a failure in SPP schemes,
as they are in DPP schemes. As a result, the SPP scheme
is slower in comparison with the DPP scheme. However, in
terms of capacity utilization, the SPP scheme is considerably
more efficient than the DPP scheme as it allows for sharing
of resources on the backup paths.
Clearly, there is a trade-off between time-to-recover and
capacity utilization when we compare the SPP scheme with
the DPP scheme. Recently in [6], the authors proposed a
scheme where they introduced a notion of stream, which is
a collection of overlapping backup paths. The stream scheme
can be conceptualized as a member of a virtually shared DPP
scheme. In this scheme, all the PXCs are preconfigured (just
like DPP schemes) and in case of a failure, the traffic is sent
through the backup path that forms a part of a stream. This
scheme results in fast recovery as the PXCs are not required
to spend any time for decision making at the time of a failure.
The speed of recovery in this scheme is much faster than that
of SPP schemes as no signalling or configuration of PXCs in
the intermediate nodes lying on the backup path is needed after
a failure [6]. The resource utilization in this scheme is much
better than that of DPP schemes as it allows some sharing of
resources on the backup paths between multiple connections.
Thus, the scheme retains some of the advantages of both the
DPP and the SPP. In the current taxonomy of schemes for
survivability in optical networks, one can be view a stream as
a scheme that lies somewhere between the SPP and the DPP.
In addition to introducing the concept of stream, Kim
and Lumetta in [6] proposed algorithms for the formation
of streams. Through extensive simulations they demonstrated

1-4244-0353-7/07/$25.00 ©2007 IEEE
2211

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

the efficacy of their methods. In this paper, we propose an
expanded definition of streams called gStreams. The new
definition subsumes the definition of stream given in [6]. The
contributions of this paper are as follows.
- We introduce a novel concept of a gStream that generalizes
the notion of a stream for better resource utilization.
- We show that the problem of finding the optimal number
of gStreams (or streams) that maximizes capacity utilization
is NP-complete even when the primary and backup paths are
fixed in the network.
- We give a mathematical programming formulation for finding
the optimal streams and gStreams.
- We provide a heuristic solution that computes near optimal
streams and gStreams to most of the problem instances.
- Through extensive simulation, we demonstrate the benefits of
this new concept and the efficacy of our heuristics in finding
near optimal solutions.
Since the algorithm proposed by Kim and Lumetta is for
a dynamic scenario where the streams are being formed as
and when the call requests arrive and our heuristic is for a
static scenario, the results produced by these two methods are
not directly comparable. However, our results can be used as
an upper bound on the capacity savings that can achieved for
the generated set of backup paths. In some sense, Kim and
Lumetta provide an online algorithm for the problem, whereas
ours is an offline algorithm.

is no need to wait until the occurrence of a failure. Irrespective
of the failure of the primary path from 1 to 3 or 5 to 8, the
PXC at 4 has to forward the traffic to node 3. The key idea of
the streams scheme is to form the backup paths in such a way
that divergence of the paths are not necessary. The absence of
divergence of overlapping backup paths eliminates the need
to wait until the occurrence of a failure on the primary path
to configure a PXC thereby ensuring faster recovery. In this
way, the use of the concept of stream can provide performance
comparable to that of a DPP scheme. In the example of figure
1 path 1 → 5 → 4 → 3 → 8 is a stream formed by the
overlapping backup paths 1 → 5 → 4 → 3 and 5 → 4 →
3 → 8. In order to get DPP performance for fast recovery,
with the backup paths 1 → 5 → 4 → 3 and 5 → 4 → 8
(without stream formation), five units of network resources
(wavelengths) have to be reserved: λ1 on the links (1, 5), (5, 4)
and (4, 3) and λ2 on the links (5, 4) and (4, 8). However, if a
stream, 1 → 5 → 4 → 3 → 8 is formed, it will require only
four units of resources: λ1 on the links (1, 5), (5, 4), (4, 3)
and (3, 8). Thus, the formation of streams can provide fast
recovery (comparable to DPP) with better capacity utilization.
We will refer to the difference between the resource usage in
a non-stream solution (5 units to the stream solution (4 units)
as the gain. The objective will be to form the streams in such
a way that will maximize the gain.

II. S TREAM P ROTECTION S CHEME

The stream formation algorithm given in [6] assumes a
dynamic scenario where call requests arrive at random time
intervals. A set of shortest paths, P (s, d) between each sourcedestination node pair is precomputed. In addition, corresponding to every pi ∈ P (s, d), a set of backup paths B(pi , h) is also
precomputed. The parameter h is used to restrict the length of
potential backup paths to at most h hops more than the length
of the shortest paths. As soon as a request for connection from
a source node s to a destination node d arrives, the traffic
manager chooses one primary and one backup path from the
set of precomputed primary and backup paths from s to d.
The primary and backup path-pair is chosen in such a way
that the backup path is “compatible” with the existing streams
and hence, will impose least increase in “cost” for the new
connection request. A detailed description of the algorithm
can be found in [6].

Although a stream was not formally defined in [6], the
authors implied it to be a directed path from node vi1 to vik
(vi1 → vi2 → vi3 → . . . → vik−2 → vik−1 → vik ) formed
by overlapping backup paths that connect multiple sourcedestination node pairs. For example, the stream from vi1 to vik
may be formed by the concatenation of the overlapping backup
paths (i) vi1 → vi2 → vi3 → . . . → vik−2 , (ii) vi2 → vi3 →
vi4 → . . . → vik−1 and (iii) vi3 → vi4 → vi5 → . . . → vik .
The notion of backup paths sharing network resources is not
new and is the key feature of the SPP schemes.
However, arbitrary sharing of
backup paths may lead to a situation, in which preconfiguration
of the PXCs on the backup paths
may not be possible. Consider the
network shown in Figure 1.

2

1

3

4
5
Fig. 1.

6

7

8

A. Current approach to stream formation

B. Limitations of the current approach

Stream formation

Let the primary paths connecting the nodes (1, 3) and (5, 8)
be 1 → 2 → 3 and 5 → 6 → 7 → 8 respectively. Let the
respective backup paths be 1 → 5 → 4 → 3 and 5 → 4 →
8. Suppose the two backup paths use the same wavelength,
then the PXC at node 4 cannot be preconfigured before the
occurrence of a fault on the primary paths, since node 4 cannot
decide ahead of a fault whether to forward the traffic coming
on the backup lightpath to node 3 or 8. However, instead of
using 5 → 4 → 8 as the backup path from 5 to 8, if 5 → 4 → 3
→ 8 is used, the PXC at node 4 can be preconfigured as there

The advantage of the stream formation scheme as given
in [6] is its simplicity. It takes a simple greedy approach to
the formation of the streams with the backup paths. However,
the disadvantage of such a simple scheme is that the solution
produced by this approach may be far from optimal. In
the following, we provide two examples where the simple
approach fails to realize the full benefit of stream formation.
Case 1: Suppose that at an instance of time t, there are only
two non-overlapping streams in existence S1 : vi → vi+1 →
. . . → vj → vj+1 → . . . → vk and S2 : va → va+1 → va+2 →
. . . → vb → vb+1 . . . → vc . If at this time a new backup path

2212

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

TABLE I
A N EXAMPLE DEMONSTRATING THE BENEFITS OF G S TREAMS OVER STREAMS

Backup Paths

Streams

gStreams

S1 (B1 +B2 +B3 ): v1 → v2 → v3 → v4 →
v5 → v6 → v7 → v8 → v9 , Gain = 3
S2 (B4 +B5 ): v11 → v10 → v5 → v6 , Gain = 0
S3 (B6 +B7 ): v12 → v13 → v14 → v15 , Gain = 1
S4 (B8 ): v16 → v14 → v15 , Gain = 0
Total Gain = 4

gS1 := B1 +B2 +B3 +B4 +B5 , Gain = 4
gS2 := B6 +B7 +B8 , Gain = 2

B1 : v1 → v2 → v3 → v4
B2 : v2 → v3 → v4 →
v5 → v6 → v7 → v8
B3 : v7 → v8 → v9
B4 : v10 → v5 → v6
B5 : v11 → v10
B6 : v12 → v13 → v14
B7 : v13 → v14 → v15
B8 : v16 → v14 → v15
(Primary paths P1 , P2 , . . . , P8
are all mutually disjoint)

vj → vj+1 → . . . → vk → vk+1 → . . . → va → va+1 →
. . . → vb is selected, the algorithm proposed in [6] combines
this backup path either with the stream S1 or with S2 . It may
be noted however that as the new backup path shares links
with both S1 and S2 , the two streams can be combined into
a single stream S3 : vi → vi+1 → . . . → vj → vj+1 → . . . →
vk → vk+1 → . . . → va → va+1 → . . . → vb → vb+1 →
. . . → vc with the help of the newly arrived backup path,
thereby increasing capacity utilization.
Case 2: Suppose that at an instance of time t, there is only one
stream in existence S1 : v1 → v2 → v3 → v4 → v5 formed by
only one backup path B1 . At this time, a second backup path
B2 : v3 → v4 → v5 → v6 is selected. The stream formation
technique proposed in [6] combines the newly arrived backup
path with the stream S1 to form a new stream S2 : v1 → v2 →
v3 → v4 → v5 → v6 . Suppose a third backup path B3 : v2 →
v3 → v4 → v5 → v7 is selected at this time. B3 cannot be
combined with the stream S2 as this will lead to divergence at
node v5 . The net gain of combining S1 with B2 will be 2 units
as S1 and B2 have overlapping path segment v3 → v4 → v5 .
However, if S1 had not been combined with B1 , but combined
with B3 to form the stream S3 : v1 → v2 → v3 → v4 → v5 →
v7 , the gain would have been 3 units since S1 and B3 have
overlapping path segment v2 → v3 → v4 → v5 .
C. Definition of gStream
As noted earlier, although stream was not formally defined
in [6], the authors implied it to be a directed path formed by
the concatenation of overlapping backup paths. Two backup
paths B1 and B2 can form a stream as long as there is no
divergence between these two paths. For example, backup
paths B1 : v1 → v2 → v3 and B2 : v2 → v3 → v4 can
form a stream S1 : v1 → v2 → v3 → v4 . However, paths
B1 : v1 → v2 → v3 and B2 : v4 → v2 → v5 cannot be
concatenated to form a stream as it leads to diverge at node v2 .
We make a key observation regarding the formation of streams.
We note that in order to avoid divergence, combination of

Total Gain = 6

two overlapping backup paths B1 and B2 need not always
form a path. Divergence can still be avoided if for example,
B1 : v1 → v2 → v3 and B2 : v4 → v2 → v3 . Although
the combination of B1 and B2 does not form a path, it still
retains the important property that all nodes in this combined
structure can be preconfigured. We refer to such a structure as
generalized streams(gStreams). We give a formal definition of
a gStream next.
• Outdegree-1 Graph (OD1G): A directed graph is called
an Outdegree-1 Graph if the outdegree of each node is at
most one.
• gStream: A connected OD1G is called a generalized
stream.
The example in Table I shows the streams and gStreams
formation with 8 backup paths. In the example, suppose that
the primary paths are all mutually disjoint and there are at
least 2 wavelengths that can be used on all the backup paths.
The directed path definition of streams results in the formation
of 4 streams (second column) resulting in a total gain of 4
units. However, the OD1G definition of gStreams results in
2 gStreams (third column) with a net total gain of 6 units.
Since a OD1G has outdegree at most one, there never is any
divergence. Therefore, we can combine backup paths to form
gStream as long as the combined backup paths form a OD1G.
It may be noted that a directed path, directed cycle, ReverseBranching and Reverse-Arborescence [5] are examples of
Outdegree-1 Graphs. Thus, the definition of stream as a path
used in [6] is a special case of gStream.
III. P ROBLEM F ORMULATION
In the most general form of the problem, we have a
collection of source-destination node pairs (si , di ), 1 ≤ i ≤ k
and corresponding to each (si , di ), we would like to establish
a primary path Pi and a backup path Bi (node disjoint from
Pi ) such that the network resources used for the establishment
of these paths is minimized. The channels on optical fiber
links are the network resources that we plan to optimize. If

2213

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

a primary path Pi is established over three optical fiber links
using wavelength λ, we say that the path used three units of
network resources. Under SPP scheme, two backup paths Bi
and Bj can share a channel over a link if the corresponding
primary paths are disjoint. We assume that no wavelength
converters are available and as such, wavelength continuity
constraint must be satisfied by all primary and backup paths.
In this paper, we do not address the issues related to the
selection of primary and backup paths. Rather, we address
the issue of stream formation from the set of backup paths
such that resource utilization is maximized. We are assuming
that based on some considerations, the primary and the backup
paths between the source-destination node pairs have already
been selected and our job is to form the streams in a way
that maximizes resource utilization. We prove that even when
the set of wavelengths Λi that can be used to establish a
backup path Bi is equal to Λ, the set of wavelengths available
on a fiber link, the stream formation for maximum resource
utilization problem is NP-complete.
A. Definitions
•

•

•

•
•

Compatible backup paths: Two backup paths Bi and
Bj are said to be compatible (denoted by Bi ⇔ Bj ) if
(i) their primary paths Pi and Pj are node disjoint (ii)
there is no divergence in the structure created by their
concatenation and (iii) Bi , Bj can be realized using the
same wavelength λ (wavelength continuity). It may be
noted that compatibility is not transitive.
Gain of a pair of backup paths: If two compatible
backup paths Bi and Bj are combined to form a stream,
then the gain associated with this pair of paths denoted
by g(i, j) is equal to the number of common arcs between
them. The gain of a pair of compatible paths is a nonnegative number. If Bi  Bj , then g(i, j) = −∞.
Path Intersection Graph (PIG): The path intersection
graph H(VH , EH , GH , LH ) corresponding to a set of
backup paths {B1 , B2 , . . . , Bn } is an edge-weighted,
completely connected, undirected graph, in which each
vertex vi represents a backup path Bi . GH is the set of
gain values g(i, j) for every edge (vi , vj ) ∈ EH . LH
is the set of wavelength lists Λi for vi ∈ VH where
Λi represents the set of wavelengths that can be used
throughout on backup path Bi .
Gain of a vertex set: If V ⊆ VH , then
the gain associated
with V denoted by g(V ) is given by vi ,vj ∈V g(vi , vj ).
Gain of a graph partition: If V = {V1 , V2 , . . . , Vk }
represents a partition of the node set VH of a graph, then
the gain associatedwith this partition denoted by g(V) is
k
given by g(V) = i=1 g(Vi ).

B. Graph Partitioning for Gain Maximization (GPGM) Problem
Instance:
Given
a
Path
Intersection
Graph
H(VH , EH , GH , LH ) with VH , EH , GH and LH as
defined before and a positive integer R.

Question: Is there a partition of VH into V = {V1 , V2 , . . . , Vk }
with g(V) ≥ R such that
1) there is at least one wavelength that can be assigned to
all vertices belonging
 to the same component Vi . That
is, ∀l, 1 ≤ l ≤ k, vi ∈Vl Λi 	= ∅, and
2) two backup paths whose concatenation results in divergence cannot be assigned the same wavelength.
It may be noted that in the partition V that has the maximum
gain, a component Vi ∈ V does not contain −∞ gain edges.
Each component represents a connected OD1G of the backup
paths in the original network and thus, a gStream.
IV. C OMPUTATIONAL C OMPLEXITY OF GPGM P ROBLEM
Consider the restricted version of the GPGM problem, in
which all wavelengths in the network are available for use at
every backup path. That is, ∀vi ∈ VH , Λi =Λ. This assumption
eliminates the wavelength constraints in the GPGM problem.
We prove that this simplified version of the GPGM problem
termed as RGPGM problem itself is NP-complete.
A. Restricted Graph Partitioning for Gain Maximization
(RGPGM) Problem
Instance: Given a Path Intersection Graph H(VH , EH , GH )
with VH , EH and GH as defined before, positive integer R.
Question: Is there a partition of VH into V = {V1 , V2 , . . . , Vk }
such that g(V) ≥ R ?
To prove the NP-completeness of the RGPGM problem,
we transform Exact Cover by 3-Sets (X3C) [2] into RGPGM
problem.
Theorem 1: RGPGM problem is NP-complete.
Proof: Clearly, RGPGM problem is in NP since a nondeterministic algorithm need only guess a partition V of vertex
set VH and verify in polynomial time whether g(V) ≥ R.
Suppose a finite set X with |X| = 3q and a collection
C = {c1 , . . . , ck } of 3-element subsets of X make up the instance of X3C. From this instance of X3C, we will construct an
instance of RGPGM such that a partition V of the node set VH
with g(V) ≥ R exists if and only if C contains an exact cover.
We use local replacement technique for the NP-completeness
proof. Corresponding to each subset ci ∈ C (ci = {xi , yi , zi }),
we will construct a subgraph of G with 12 nodes and 18
edges, as shown in figure 2. Since this construct corresponds
to the 3-element subset ci , we will refer to the edges of this
construct as Ei . Thus, the instanceof the RGPGM will have
|C|
VH vertices where VH = X ∪ i=1 {aji : 1 ≤ j ≤ 9}.
The instance of the RGPGM will have two types of edges,
|C|
EA and EB , where EA = ∪i=1 Ei . It may be recalled that
the instance of the RGPGM problem is an edge-weighted,
completely connected undirected graph. If two nodes vi and
vj are not connected by an edge e ∈ EA , then there must be
an edge e ∈ EB connecting these two nodes. The weight on
all edges e ∈ EA is 1 and the weight on all edges e ∈ EB is
−∞. Finally, we set R = 3q + 9|C|. The number of vertices
of the instance of RGPGM will be |X| + 9|C| = 3q + 9|C| i.
e, |VH | = 3q + 9|C| = 3q  , where q  = q + 3|C|. It is easy to

2214

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

see that this construction procedure can be done in polynomial
time.
a3i
a1i
xi

a9i

a6i

a2i 4
ai

a5i

a8i

a7i
zi

yi

Fig. 2. Local replacement for ci = {xi , yi , zi } ∈ C for transforming X3C
to RGPGM.

a31
u1

a83
a93

a71
a51
u3
u2

a21 4
a1

a11
a73 5
a3

a33

u4

u6

a43
a23

a91

a61

u5
a13 a82

a22
a42

a81

size 1, K2 cliques of size 2 and K3 cliques of size 3. In this
case, |VH | = 3q + 9|C| = 1.K1 + 2.K2 + 3.K3 and g(V) =
0.K1 + 1.K2 + 2.K3 < |VH |. Thus, the only way the instance
of RGPGM can be partitioned with gain g(V) = 3q + 9|C|
is when it can be partitioned into cliques of size 3 with all
edge weights equal to 1. If V = V1 ∪ V2 ∪ · · · ∪ Vq is a
partition of H into cliques of size 3 with edge weight 1, then
the corresponding exact cover can be computed by choosing
those subsets ci ∈ C such that {a3i , a6i , a9i } = Vj for some
j, 1 ≤ j ≤ q  . It can be easily be verified that the subsets
chosen this way will constitute an exact cover for the X3C
problem.

a12

V. O PTIMAL S OLUTION THROUGH M ATHEMATICAL
P ROGRAMMING

a32

a52 a62
a72
a92

Fig. 3.
Local replacement for 3 subsets c1 = {u1 , u2 , u3 }, c2 =
{u3 , u4 , u5 } and c3 = {u1 , u5 , u6 }

Claim: There is a partition V of the instance of the RGPGM
problem with g(V) ≥ R, if and only if the instance of the
X3C has an exact cover.
Suppose that c1 , c2 , . . . , cq are the 3-element subsets from
C that make up an exact cover of X. In this case, we
can partition the node set VH into V1 ∪ V2 ∪ · · · ∪ Vq by
taking {a1i , a2i , xi }, {a4i , a5i , yi }, {a7i , a8i , zi }, {a3i , a6i , a9i } from
the vertices of Ei , whenever ci = {xi , yi , zi } is in the exact
cover, and by taking {a1i , a2i , a3i }, {a4i , a5i , a6i }, {a7i , a8i , a9i }
from the vertices meeting Ei , whenever ci is not in the exact
cover. Thus if there is an exact cover in the instance of X3C,
the instance of the RGPGM can be partitioned into q  subsets
V1 , . . . , Vq , where each Vi , 1 ≤ i ≤ q  is a completely
connected subgraph (clique) of size 3. Since the edge weights
associated with each edges that form the clique is 1, the gain
associated with each clique Vi , 1 ≤ i ≤ q  is 3. Since VH has
been partitioned into q  cliques, the gain of this partition is
3q  = 3q + 9|C|. Thus, if there is an exact cover, there is a
partition with gain R.
Conversely, suppose that there exists a partition V of the
instance of RGPGM such that g(V) = R = 3q  = 3q + 9|C|.
First we show that if such a partition exists, it must be possible
to partition the instance of RGPGM into cliques of size 3, such
that all edges belonging to the cliques have weight 1. Then,
we show that if such a partition into cliques of size 3 with
all edge weights 1 exists, the instance of X3C must have an
exact cover.
It may be noted that the way the instance of the RGPGM
was created, the graph does not contain a clique of size 4
or higher with all edge weights equal to 1. Suppose that the
graph H is partitioned into cliques of size 1, 2 and 3 and
suppose that the partition V is composed of K1 cliques of

In this section, we provide an Integer Linear Program
formulation [1] for finding the gain maximizing partition of the
path intersection graph H(VH , EH , GH , LH ). Let the network
graph be G and Λ be the set of all wavelengths in the network.
Λi is the set of wavelengths associated with vertex vi ∈ VH as
specified by LH . g(i, j) is the gain of edge (vi , vj ) ∈ EH as
specified by GH . Let |VH | = n. Suppose the optimal partition
of VH is V with components {V1 , V2 , . . .}. There can be at
most n components in the optimal partition. The indicator
variables are defined as follows. ∀v ∈ VH , 1 ≤ i ≤ n, xiv = 1
i
=1
if vertex v ∈ Vi , 0 otherwise. ∀u, v ∈ VH , 1 ≤ i ≤ n, yu,v
if vertices u, v ∈ Vi , 0 otherwise. ∀v ∈ VH , 1 ≤ c ≤ |Λ|,
zv,c = 1 if vertex v is assigned wavelength c, 0 otherwise.
The objective function is to maximize
total gain of the
n n the
n
i
.
partition. That is, M aximize i=1 u=1 v=1 g(u, v)yu,v
The set of constraints is given below.
- Each vertex must be in 
exactly one component of the
n
partition. That is, ∀v ∈ VH , i=1 xiv = 1.
- Each vertex must be assigned exactly one wavelength
from
 wavelengths. That is, ∀v ∈ VH ,
 its list of available
z
=
1
and
c∈Λv v,c
c∈Λ
/ v (v) zv,c = 0.
- An edge (u, v) is in component i if and only if both u
i
= xiu xiv . This quadratic
and v are in i. Mathematically, yu,v
constraint can be replaced by two linear constraints. ∀u, v ∈
i
i
and xiu + xiv − 1 ≤ yu,v
.
VH , 1 ≤ i ≤ n, xiu + xiv ≥ 2yu,v
- All vertices belonging to a component must be assigned the
same wavelength. Define for convenience, an indicator variable
for each vertex pair u, v and wavelength c as follows. ∀u, v ∈
VH , 1 ≤ c ≤ |Λ|, scu,v = 1 if vertices u, v are both assigned
the same wavelength c, 0 otherwise. The quadratic constraint
scu,v = zu,c zv,c can be replaced by the two linear constraints:
∀u, v ∈ VH , 1 ≤ c ≤ |Λ|, zu,c + zv,c ≥ 2scu,v and zu,c + zv,c −
1 ≤ scu,v . Now, the constraint that all vertices in a component
must be assigned the same wavelength
can be represented as:
l
i
∀u, v ∈ VH , 1 ≤ i ≤ n, yu,v
≤ c=1 scu,v .
- Two backup paths whose combination causes divergence
cannot be assigned the same wavelength. That is, ∀u, v ∈ VH
such
g(u, v) = −∞ & Bu , Bv are not disjoint in G,
l that
c
s
c=1 u,v = 0.

2215

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

VI. H EURISTIC FOR COMPUTING G S TREAMS

VII. S IMULATION E NVIRONMENT, R ESULTS AND
D ISCUSSION

Algorithm 1 heuristic streams(H(VH , EH , GH , LH ), set
Bi of backup paths in G)

We conducted simulations to evaluate the gain for stream
and gStream partition. In both cases, we compared the optimal
gain value obtained by solving the Integer Linear Program
with that of the heuristic. The experiments were conducted
on two network graphs namely, National network (24 nodes,
44 links) and NSFNET (14 nodes, 21 links). The experiments
were conducted for different number of wavelengths (λ). For
each network and wavelength, n source-destination (s − d)
pairs were randomly selected. The shortest path between each
(s−d) pair was chosen to be the primary path between (s−d).
A randomly selected wavelength that can be used throughout
on the path was assigned to all the nodes on the path. For the
backup path between (s − d), the first 5-shortest paths that are
disjoint with the primary path were computed. One of these
paths was selected uniformly at random to be the backup path
between (s − d). From the set of backup paths, compatibility
and gain values between the paths for forming streams and
gStreams were computed. The algorithm to compute the gain
value that can be obtained by the concatenation two backup
paths is provided in the appendix.

1: k ← 1
2: while ∃ edge (vi , vj ) ∈ EH such that g(i, j) ≥ 0, do
3:
(va , vb ) ⇐ (vi , vj ) such that Λi ∩Λj = ∅ and g(i, j) is largest
4:
5:

6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

among all (vi , vj ) ∈ EH
Vk ← {va , vb }

Select vertex vt ∈ VH \ Vk such that
vs ∈Vk g(s, t) is
positive, maximum and Λ(Vk ) ∩ Λt = ∅. If no such vertex
exists, go to step 8.
Vk ← Vk ∪ {vt }.
Remove vertex vt and all incident edges from H. Go to step
5.
Select wavelength λ from Λ(Vk ) uniformly at random and
assign it to all vertices of Vk .
∀vi ∈ VH \ Vk and ∃vj ∈ Vk such that Bi ∩ Bj = ∅, Λi ←
Λi \ λ
k ←k+1
end while
if VH = ∅ then
while VH = ∅ do
Select at random, a vertex vi ∈ VH into component Vk .
Select wavelength λ from Λi uniformly at random and
assign it to v.
∀vj ∈ VH \ vi such that Bi ∩ Bj = ∅, Λi ← Λi \ λ
VH ← VH \ vi , k ← k + 1
end while
end if
return {V1 , V2 , . . . Vk−1 }

In this section, we present the heuristic to compute gStreams
from the Path Intersection Graph H = (VH , EH , GH , LH ).
Recall that Λi ∈ LH represents the set of wavelengths
available for vertex vi ∈ VH . Let Λ(S) for S ⊆ VH be
defined as the set ofwavelengths common to all vertices of
S. That is, Λ(S) = vi ∈S Λi . The heuristic first selects two
vertices va and vb into a component such that edge (va , vb )
has the largest gain in the path intersection graph and va ,
vb have at least one available wavelength in common. The
heuristic proceeds by selecting vertices from the rest of the
graph that have at least one wavelength in common with the
vertices in the component and result in the largest component
gain. When no such vertex can be found, this component is
complete and the heuristic assigns a wavelength randomly
from the list of wavelengths common to all vertices in the
component. These vertices (and the incident edges) are then
removed from the graph. The wavelength that was assigned (if
present) is removed from the set of available wavelengths of
those vertices in the graph whose backup path in the network
graph shares edges with the backup paths corresponding to
the vertices in the component. This process is repeated until
there are no edges with positive gain left in the graph. When
the graph has no positive gain edges, the heuristic partitions
the remaining vertices in the graph into separate partitions
and assigns wavelengths randomly from their list of available
wavelengths. The heuristic runs in O(n2 l) time, where n is the
number of backup paths and l is the number of wavelengths.

Fig. 4. Comparison of gain of streams and gStreams for National network
with λ = 13

Fig. 5. Comparison of gain of streams and gStreams for NSFNET network
with λ = 13

All the simulations were conducted on a Pentium-4 3.2 GHz
computer with 2 GB RAM. The ILPs were solved optimally
using ILOG CPLEX 10.0. In the experiments of Figures 4
and 5, we set out to study the effect of increasing the number
of backup paths on the gain value in case of streams and
gStreams in National and NSFNET networks respectively. We
varied the number of backup paths from 5 to 25, while the
total number of wavelengths in the network was 13. In both the
networks, CPLEX took less than 30 minutes to solve for 20 or

2216

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

Fig. 6.
Comparison of gain of streams and
gStreams obtained from heuristic for large number
of backup paths for National network with λ = 15

Fig. 7.
Comparison of gain of streams and
gStreams obtained from heuristic for large number
of backup paths for NSFNET network with λ = 15

less backup paths, while for 25 backup paths, it typically took
17 hours to compute the optimal solution. The time taken by
heuristic for all the cases was in the order of a few seconds. In
88% of the test cases, the gain value obtained by the heuristic
differed with the optimal gain value by at most 1. This strongly
suggests that our heuristic provides near optimal solution to
most instances of the problem in a fraction of the time needed
for finding the optimal solution. In the experiments of Figures
6 and 7, we increased the number of backup paths to 45.
Because of the large execution times for solving the ILP in
each instance, we resorted to the heuristic. We varied the
number of backup paths from 5 to 50 and computed the
heuristic gain values of streams and gStreams. It can be seen
that the gain value of gStreams becomes significantly higher
than that of streams as the number of backup paths increase.
This is because, gStream with its OD1G structure can take
better advantage of more number of backup paths than streams
which is be made up of only directed paths. In figure 8, we
studied the effect of increasing the number of wavelengths
in the network on the gain value of streams and gStreams.
When more number of wavelengths are available, gStreams
gain is significantly higher than streams gain since increase
in number of wavelengths relaxes the wavelength continuity
constraint and allows more gStream formations. From all these
experiments, the superior benefits of gStreams are evident.
VIII. C ONCLUSION
In this paper, we presented a generalized notion of streams
and developed efficient algorithms for maximizing capacity
utilization. Through our extensive simulation, we demonstrated that the gStream scheme provides significantly better
resource utilization compared to streams.
R EFERENCES
[1] M. Bazaraa, J. Jarvis, and H. Sherali, “Linear Programming and Network
Flows (3rd Edition)”, John Wiley & Sons, 2005.
[2] M. R. Garey and D. S. Johnson, “Computers and Intractability: An
Introduction to the Theory of NP-Completeness”, W. H. Freeman, 1979.
[3] O. Gerstel and R. Ramaswami, “Optical layer survivability: A services
perspective”, IEEE Communications Magazine, vol. 38, no. 3, March
2000.
[4] W. D. Grover and D. Stamatelakis,“Cycle-Oriented Distributed Preconfiguration: Ring-like Speed with Mesh-like Capacity for Self-planning
Network Restoration”, IEEE ICC, Atlanta, 1998.
[5] N.-F. Huang, and T.-H. Huang, “On the Complexity of Some Arborescences Finding Problems on a Multihop Radio Network”, BIT, vol. 29,
no. 2, 1989.

Fig. 8. Effect of increasing the number of wavelengths on the gain for streams and gStreams for
National network with 20 backup paths

[6] S. Kim and S. S. Lumeta,“Capacity-efficient protection with fast recovery
in optically transparent mesh networks”, Proceedings of Broadnets, 2004.
[7] M. Medard, S. Finn and R. A. Barry, “WDM Loop-back Recovery in
Mesh Networks”, Infocom, New York, March 1999.
[8] G. Mohan, C. S. R. Murthy and A. K. Somani, “Efficient algorithms for
routing dependable connections in WDM optical networks”, IEEE/ACM
transactions on Networking, vol. 9, no. 5, October 2001.
[9] S. Ramamurthy and B. Mukherjee, “Survivable WDM Mesh Netwoks”,
Parts I and II Proc. of IEEE INFOCOM, March 1999, and ICC,
Vancouver, CA, June 1999.
[10] A. Sen, B. Hao, B. H. Shen and G. Lin, “Survivable Routing in WDM
Networks: Logical Ring in arbitrary Physical Topology”, IEEE ICC, New
York, May 2002.

A PPENDIX : G AIN C OMPUTATION
Algorithm 2 compute gain(Bi , Bj )
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

Initialize gain← 0, E = ∅, δi− ← 0, δi+ ← 0, 1 ≤ i ≤ nb
for all edges (u, v) ∈ Bi do
E ← E ∪ {(u, v)}
δu+ ← 1, δv− ← 1
end for
for all edges (u, v) ∈ Bj do
if (u, v) ∈ E then
gain ← gain + 1
else
E ← E ∪ {(u, v)}
end if
δu+ ← δu+ + 1, δv− ← δv− + 1
if δu+ > 1 or δv− > 1 then
return −∞
end if
end for
return gain

The function compute gain takes as arguments two backup
paths Bi and Bj and determines the gain value that can be
obtained by including Bi and Bj in a stream. If Bi and Bj are
incompatible, then this function returns −∞. Let nb represent
the number of vertices in Bi ∪ Bj . Let E be the set of unique
edges in Bi ∪ Bj . Let δv− and δv+ represent the indegree and
outdegree of vertex v respectively in Bi ∪ Bj . Computing the
gain value for gStreams requires only a small modification
to the above procedure. Recall that in the case of gStreams,
vertices with indegree > 1 are possible. Hence for gStreams,
we need not compute and check δv− in the above procedure.

2217

Efficient Mapping and Voltage Islanding Technique for
Energy Minimization in NoC under Design Constraints
Pavel Ghosh

Arunabha Sen

Computing, Informatics and Decision Systems
Engineering, Arizona State University
Tempe, AZ, USA, 85287

Computing, Informatics and Decision Systems
Engineering, Arizona State University
Tempe, AZ, USA, 85287

pavel.ghosh@asu.edu

asen@asu.edu

ABSTRACT

1.

Voltage islanding technique in Network-on-Chip (NoC) can
significantly reduce the computational energy consumption
by scaling down the voltage levels of the processing elements
(PEs). This reduction in energy consumption comes at the
cost of the energy consumption of the level shifters between
voltage islands. Moreover, from physical design perspective
it is desirable to have a limited number of voltage islands.
Considering voltage islanding during mapping of the PEs to
the NoC routers can significantly reduce both the computational and the level-shifter energy consumptions and the
communication energy consumption on the NoC links. In
this paper, we formulate the problem as an optimization
problem with an objective of minimizing the overall energy
consumption constrained by the performance in terms of
delay and the maximum number of voltage islands. We provide the optimal solution to our problem using Mixed Integer Linear Program (MILP) formulation. We also propose
a heuristic based on random greedy selection to solve the
problem. Experimental results using E3S benchmark applications and some real applications show that the heuristic
finds near-optimal solution in almost all cases in a very small
fraction of the time required to achieve the optimal solution.

In recent years Multi-Processor System-on-Chip (MPSoC)
design has become extremely challenging due to the increasing complexities in processor and semiconductor technologies. Due to the growing complexity of consumer embedded products, and the complexity of new communication
and multimedia standards, future MPSoCs are predicted to
contain several hundreds of processing elements (PEs) communicating among themselves at very high-speed rates. In
order to meet the increasing requirements of performance,
scalability and flexibility, shared bus based communication
infrastructure is no longer adequate for MPSoCs. Networkon-Chip (NoC) provides an alternative to the bus-based onchip communication that can overcome the problems of performance, scalability and flexibility. In order to handle the
increasing complexities in the MPSoC designs, the necessity
for NoC has been discussed by several researchers [2, 3, 8].
As the number of PEs on an MPSoC and the data traffic
between them continue to grow, minimization of energy consumption subject to the performance constraint has become
one of the most important objectives. Power consumption of
VLSI circuits can be roughly broken down into two components: static power and dynamic power. While static power
mainly relates to the leakage current, dynamic power Pd is
a result of the switching activities of the circuit, given by:

Categories and Subject Descriptors

INTRODUCTION

2
Pd = kCVdd
f

B.7.2 [Integrated Circuits]: Design Aids—Placement and
Routing, Layout; C.3 [Special-purpose and Applicationbased Systems]; G.1.6 [Numerical Analysis ]: Optimization—Linear Programming

General Terms
Algorithms, Design, Experimentation, Theory

Keywords
Multi-Processor System-on-Chip (MPSoC), Network-on-Chip
(NoC), voltage islanding, integer linear program, greedy randomized heuristic

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SAC’10 March 22-26, 2010, Sierre, Switzerland.
Copyright 2010 ACM 978-1-60558-638-0/10/03 ...$10.00.

535

where k is the switching rate, C is the load capacitance,
Vdd is the supply voltage and f is the clock frequency. As
dynamic power is proportional to the square of supply voltage Vdd , reducing Vdd can significantly reduce the dynamic
power consumption. Among the various approaches taken
to reduce power consumption of MPSoCs, the use of multisupply voltages (MSV) has gained popularity among the researchers. The performance critical PEs generally require
high supply voltage to meet the performance requirements,
while the non-crictical PEs can be slowed down using lower
supply voltages and thus gaining in terms of power consumption. Utilizing the power-performance tradeoff, the idea of
multi-voltage islanding was first proposed in [9]. Voltage island on a chip is defined as a cluster of adjacent PEs all
operating at the same voltage level. Although scaling down
the voltage levels of PEs is favorable for reduction of energy consumption, excessive number of voltage islands may
be detrimental from the perspective of physical design [14]
as it creates voltage island fragmentation of the chip and
increases the complexity of layout of the power delivery network. Therefore, voltage islanding needs to be considered

during the mapping of the PEs to the NoC routers. This way
computational energy consumption can be reduced, without creating excessive number of voltage islands. Moreover,
mapping of heavily connected PEs in adjacent locations will
reduce communication energy consumption on NoC links.

1.1

<
=
0
>
1
2
?
3
@
;4
5
6
/7
8
9
:.A
-B
+
,C
*#
D
$
%
)"E
(F
!&
G
'K
JH
L
M
IN
~
O
}P
|m
Q
u
w
v{zyxtR
sln
ko
p
q
rS
T
jU
g
h
V
ià
b
d
ce
W
f_
^]X
\Y
Z
[

Related Work

In [10], the authors have developed a dynamic programming based voltage island partitioning, level shifter insertion
and power network aware floorplanning for power optimization within timing constraints. The authors in [15] have
proposed dynamic programming based approach for voltage
selection and island creation to minimize overall SoC power,
area and floorplanner runtime. In [12], the authors optimize total power consumption and power network complexity without compromising wirelength and chip area. In [11],
the authors have proposed an α2 -approximation algorithm
for the voltage islanding problem, where α is the ratio of the
maximum and minimum voltage values. In [17], the objective of optimizing power consumption with limited design
cost and number of level shifters has been considered. The
authors in [6] develops a Simulated Annealing based framework with cost function combining the number of voltage
islands, power consumption and area overhead. In [7], the
authors propose a temperature-aware voltage islanding and
floorplanning to minimize the peak and average temperature across SoC, area, wirelength and power budget. In [13],
the authors consider voltage islanding in NoC to minimize
energy consumption based on solving a non-linear problem
formulation. These approaches either have the disadvantage
of allowing voltage islanding only on existing placements of
PEs, and thus suffer from early design decisions, or they
do not consider the energy consumption factor on the level
shifters. Also, some of these approaches involve exploring
the entire design space exhaustively, and thus become less
practical for increasing number of PEs.
It has been mentioned in [13], that the energy consumption on level shifters can be significantly high. In [1], the
authors design a voltage level converter circuit, with an estimated energy consumption proportional to the difference
of the square of the voltage levels of two end-points. We follow the similar characterization in this paper. To the best of
our knowledge, our paper is the first that considers the voltage islanding problem for NoC at an early phase of design
taking into account all the contributing factors for energy
consumption, including the level shifters’ overhead.

1.2

<
=
0
>
1
2
?
3
@
;4
5
6
/7
8
9
:.A
-B
+
,C
*#
D
$
%
)"E
(F
!&
G
'K
JH
L
M
IN
~
O
}P
|m
Q
u
w
v{zyxtR
sln
ko
p
q
rS
T
jU
g
h
V
ià
b
d
ce
W
f_
^]X
\Y
Z
[

<
=
0
>
1
2
?
3
@
;4
5
6
/7
8
9
:.A
-B
+
,C
*#
D
$
%
)"E
(F
!&
G
'K
JH
L
M
IN
~
O
}P
|m
Q
u
w
v{zyxtR
sln
ko
p
q
rS
T
jU
g
h
V
ià
b
d
ce
W
f_
^]X
\Y
Z
[

A

B

A

B

A

D

C

D

C

D

B

C

(a) Mapping with 4 (b) Mapping with 2 (c) Mapping with 2
voltage islands

voltage islands, but voltage islands and
higher energy con- lower energy consumption
sumption

Figure 1: The effect of mapping on final voltage islanding and energy consumption (light green/ light
gray) indicating lower voltage level V1 and (dark
green/dark gray) indicating higher voltage level V2
• Experimental results evaluate the quality of the heuristic as compared to the optimal.

1.3

Motivational Example

The motivation behind our approach can be shown in
the following example. It shows that considering voltage
islanding as part of mapping of the PEs to router nodes,
can be advantageous in terms of saving energy consumption. Let us consider a simple communication trace graph
consisting of 4 PEs, A, B, C and D. The edges in the graph
(A, B), (A, C), (B, D) and (C, D) represent the communication among the PEs. We define allowable voltage levels for
each PE, as the set of voltage levels, operating at each of
which the task executing at the PE will execute within its
performance bound. Let us consider that the allowable voltage levels for PEs A and D are V1 = 1.77 V and V2 = 2.5 V ,
whereas, the allowable voltage levels for PEs B and C are
only V2 = 2.5 V . Now, let us consider placing the PEs in
a regular 2 × 2 mesh NoC architecture topology. Without
voltage islanding in mind, we may map the communicating
PEs to the adjacent router nodes of the NoC, and the mapping is shown in Fig. 1(a). If we operate all the PEs at the
lowest possible voltage levels, there will be 4 voltage islands
requiring 4 level shifters as shown in the figure. If the design
constraint specifies the maximum number of voltage islands
to be 2, we need to raise the voltage level of either A or D,
as shown in Fig. 1(b). This will lead higher computation
energy consumption due to the higher voltage assignment to
one of those PEs (shown as D in figure). On the other hand,
if the voltage islanding was considered during mapping of the
PEs, we could have achieved the mapping as shown in Fig.
1(c). In this mapping, all the PEs can be operated at their
lowest possible voltage levels, and still satisfying the design
constraint. As shown in the Fig. 1(c), the number of voltage
islands created is 2, and requires only 2 level shifters.
From this example, it is clear that considering the voltage islanding technique for energy minimization at an early
phase of design leads to better energy reduction. In our
paper, we have combined all three components of energy
consumption (computation, level shifter and link communication) in the objective function, and included the constraint
of maximum number of voltage islands.

Key Contributions

The contributions of our paper can be listed as follows:
• The voltage islanding problem for NoC is considered as
part of mapping of the PEs to the NoC router nodes.
• All three components of energy consumption, namely
- computational energy consumption, communication
energy consumption on links and level shifter energy
consumption have been considered.
• The mapping and voltage islanding problem is formulated as an optimization problem and Mixed Integer
Linear Program (MILP) is developed for optimal solution of the problem.

1.4

• Efficient heuristic based on random greedy selection is
provided for good quality solutions.

System Model and Assumptions

Our targeted system can be classified as application spe-

536

cific standard products (ASSP) [3]. The traffic patterns and
bandwidth requirements are known a priori in this kind of
systems. We consider the routers to follow static source
routing, where the routing of the data paths remains static
based on the routing table at the source node. Delay analysis is based on static analysis technique described in [5].
The characterization of allowable voltage levels can be done
independently for each PE as in [6, 7, 12, 15]. In this paper,
we considered regular mesh NoC topology, but our approach
can be used for any other regular NoC topologies. Each of
the routers have 5 ports. One of the ports is used for connecting it to a PE and the other four are used for connection to the neighboring routers. Voltage level shifter energy
consumption model is taken from [1]. Link communication
energy consumption parameters have been taken from [16].

1.5

The objective is to place the PEs on the nodes of the
NoC mesh topology such that, the cost function is minimized
without violating any design constraints. The cost function
for this problem has three components:
(1) Computation energy consumption
(2) Level shifter energy consumption
(3) Communication energy consumption

3.

Paper Outline

The rest of the paper is organized as follows. Section 2
gives a formal definition of the problem and formulate it as
an optimization problem. In section 3 we develop Mixed
Integer Linear Program (MILP) to obtain optimal solution
to the problem. We develop efficient heuristic to solve the
problem in section 4. The experimental results are discussed
in section 5. Finally, we conclude the paper in section 6.

2.

OPTIMAL SOLUTION

In this section, we use the mathematical programming
techniques to solve the mapping and voltage islanding problem. We formulate the problem as a Mixed Integer Linear
Program (MILP). The objective function combines all three
components of energy consumption. We shall first define the
variables used in the formulation, followed by the definition
of the objective function and the constraints.
Variables:
1. ∀pi ∈ VP , ∀k ∈ Li ,

1, if pi is operating at voltage level k
xik =
0, otherwise
2. ∀pi ∈ VP , ∀up ∈ VR ,

1, if pi is placed at location up
δip =
0, otherwise

PROBLEM FORMULATION

In this section, we give the formal definition of the mapping and voltage islanding problem. Considering regular
mesh topologies, the problem can be stated as follows: Given,

3. ∀pi , pj ∈ VP , ∀k ∈ Li , ∀l ∈ Lj ,

1, if xik = 1 and xjl = 1
yijkl =
0, otherwise

• A mesh topology for the NoC, GR (VR , ER ), with dimension M × N , where VR is the set of NoC routers
and ER is the set of NoC links.

4. ∀pi , pj ∈ VP , ∀up , uq ∈ VR ,

1, if δip = 1 and δjq = 1
βijpq =
0, otherwise

• Each node of the mesh topology is represented by a
tuple (xci , yci ), representing the x−y coordinate of the
node, i.e., ∀ui ∈ VR , there is an associated (xci , yci ),
where 1 ≤ xci ≤ M and 1 ≤ yci ≤ N .

5. ∀pi , pj ∈ VP , ∀up , uq ∈ VR , ∀k ∈ Li , ∀l ∈ Lj ,

1, if yijkl = 1 and βijpq = 1
kl
γijpq
=
0, otherwise

• Distance between ui , uj ∈ VR is denoted by dij (in
number of hops: dij = |xci − xcj | + |yci − ycj |.

• Communication volume (in number of bytes) associated with each eij ∈ EP , and denoted by cij .

6. Considering m =| VP |, in the extreme case there can
be at most m number of voltage islands. We number
the islands in sequence (this formulation will be utilized to design the constraint on maximum number of
voltage islands). Then, ∀pi ∈ VP , 1 ≤ k ≤ m,

1, if pi is in island k
θik =
0, otherwise

• Each pi ∈ VP is associated with an allowable set of
voltage levels, Li = {vi1 , vi2 , . . . , vini }, operating at each
of which the task assigned at PE pi finishes within the
specified deadline.

7. ∀k, 1 ≤ k ≤ m,

1, if there are nonzero elements in island k
tk =
0, otherwise

• Communication delay bound λij for eij (pi , pj ) ∈ EP .

8. ∀pi , pj ∈ VP ,
8
< 1, if pi , pj are placed at adjacent locations of
mesh
ωij =
:
0, otherwise

• Communication trace graph GP (VP , EP ) (|VP | ≤ |VR |).
• Each pi ∈ VP represents a Processing Element (PE)
and an edge eij ∈ EP represents a communication
trace between PEs pi and pj , where eij = (pi , pj ).

• Maximum number of allowable voltage islands (κ).
• ηiv = computation energy consumption of pi ∈ VP at
voltage level v ∈ Li .

9. ∀pi , pj ∈ VP ,

1, if pi , pj are operating at the same voltage
ρij =
0, otherwise

• αv1 v2 = level shifter energy consumption between two
adjacent PEs operating at voltage levels v1 and v2 .
• ψl = power consumption of the links (/mm/M bps).

537

10. ∀pi , pj ∈ VP ,

1, if pi , pj reside in the same voltage island
ζij =
0, otherwise

be zero in case tk is zero. Also, to enforce tk to be 0
when the RHS is 0, we need
X
∀k, 1 ≤ k ≤ m :
θik ≥ tk
pi ∈VP

11. ∀pi , pj ∈ VP , 1 ≤ k ≤ m,

1, if θik = 1 and θjk = 1
0
θijk
=
0, otherwise

6. We can consider the delay bound λij specified in number of hops. Therefore
X
∀eij = (pi , pj ) ∈ EP :
dpq βijpq ≤ λij

Objective Function:
The objective function consists of three components: the
computational energy consumption on the PEs E1 , the energy consumption on the level shifters E2 and the communication energy consumption on the NoC links. Each of these
can be calculated as:
X X k
E1 =
ηi xik

up ,uq ∈VR

7. The following two constraints are required to ensure
that yijkl = 1 if and only if xik = 1 and xjl = 1, i.e.,
∀pi , pj ∈ VP , k ∈ Li , l ∈ Lj :
xik + xjl ≥ 2yijkl

E2

=

X

X

αkl yijkl βijpq

(up ,uq )∈ER pi ,pj ∈VP k∈Li ,l∈Lj

=

X

X

X

δip + δjq ≥ 2βijpq

kl
αkl γijpq

(up ,uq )∈ER pi ,pj ∈VP k∈Li ,l∈Lj

E3

=

X

X

cij dpq βijpq ψl

kl
yijkl + βijpq ≥ 2γijpq

Therefore, the objective function can be written as
minimize

and

δip + δjq − 1 ≤ βijpq

kl
9. The following two constraints ensure that γijpq
= 1
if and only if yijkl = 1 and βijpq = 1. We formulate
∀pi , pj ∈ VP , k ∈ Li , l ∈ Lj , up , uq ∈ VR :

eij ∈EP up ,uq ∈VR

obj :

xik + xjl − 1 ≤ yijkl

8. The following two constraints ensure that βijpq = 1
if and only if δip = 1 and δjq = 1, i.e., ∀pi , pj ∈
VP , up , uq ∈ VR :

pi ∈VP k∈Li

X

and

E 1 + E2 + E3

and

kl
yijkl + βijpq − 1 ≤ γijpq

10. This constraint ensures the definition of ωij .
X
∀pi , pj ∈ VP : ωij =
βijpq

Constraints:
1. Each PE will be assigned to exactly one voltage level
in its allowable voltage levels
X
∀pi ∈ VP :
xik = 1

(up ,uq )∈ER

11. This constraint ensures the definition of ρij
X
∀pi , pj ∈ VP : ρij =
yijvv

k∈Li

v∈Li ∩Lj

2. Each PE will be placed to exactly one location of the
mesh and no two PEs are placed in the same location
of mesh, i.e.,
X
X
∀pi ∈ VP :
δip = 1 and ∀up ∈ VR :
δip ≤ 1
up ∈VR

0
12. Since ζij = 1 if and only if θijk
= 1 for some k, 1 ≤
k ≤ m, we can write this constraint as follows:

∀pi , pj ∈ VP :

pi ∈VP

∀pi ∈ VP :

m
X

0
θijk

k=1

3. Each PE resides in exactly one voltage island
m
X

ζij =

13. At the same time, two PEs can reside in the same
island if they are placed at adjacent locations and operating at the same voltage level, i.e., ζij = 1 if ωij = 1
and ρij = 1. Also if ρij = 0, then ζij has to be 0. This
is ensured by the following:

θik = 1

k=1

4. The number of voltage islands created is within the
allowable maximum limit κ
m
X
tk ≤ κ

∀pi , pj ∈ VP :

ωij + ρij − 1 ≤ ζij

and

ρij ≥ ζij

0
14. This constraint ensures the definition of θijk
, i.e., ∀pi , pj ∈
VP , ∀k, 1 ≤ k ≤ m:

k=1

0
θik + θjk ≥ 2θijk

From the definition of tk , the LHS of the above inequation is the number of voltage islands created.

4.

5. For a large constant M (which can be set here to be
equal to m =| VP |)
X
∀k, 1 ≤ k ≤ m : M tk ≥
θik

and

0
θik + θjk − 1 ≤ θijk

HEURISTIC SOLUTION BASED ON RANDOM GREEDY SELECTION APPROACH

Most of the efforts in developing algorithm for mapping
and voltage assignments have been based on heuristics due
to the hardness of the problems. In this section, we provide
a greedy heuristic to solve the mapping and voltage islanding problem defined in section 2. One drawback of greedy

pi ∈VP

The RHS of the above inequation indicates the number
of PEs in island k. The inequation above force this to

538

Algorithm 1 Random Greedy Selection Heuristic

policy is that it may quite easily get stuck into local optimal
solutions. In order to avoid this, we introduce randomness
in our greedy selection. Before we explain the heuristic,
we give an overview of the major factors considered in the
heuristic:

Input: Given problem formulation in section 2, selection parameter a (0 ≤ a ≤ 1), and two iteration limits N1 and N2 .
Output: Solution S consisting of the physical locations and the
voltage level assignments for all PEs pi ∈ VP .
1: Initialize solution set X as empty.
2: for (i = 1 to N1 ) do
3: Randomly place the PEs on distinct locations of up ∈ VR .
4: for (j = 1 to N2 ) do
5:
∀eij ∈ EP , calculate the set F of f (i, j) values.
6:
Sort F according to f (i, j) values.
7:
From the a× | F | highest values of F select f (x, y) ∈ F
randomly.
8:
while (px and py are already adjacently placed) do
9:
F = F \ {f (x, y)}
10:
Select a new f (x, y) ∈ F
11:
end while
12:
∀pw ∈ neighbor(px ) and ∀pz ∈ neighbor(py ), calculate
the set H of common voltage cardinality values, i.e., |
Lx ∩ Lw | and | Ly ∩ Lz |.
13:
Select the minimum value element from H, which is due
to neighbor pu .
14:
if (pu is neighbor of py ) then
15:
Swap position of pu with px
16:
else
17:
Swap position of pu with py
18:
end if
19:
Set Voltage level of px and py at v = min(Lx ∩ Ly ), and
do not change them within the inner iteration
20: end for
21: Calculate number of islands numIslands
22: while (numIslands > κ) do
23:
Find PE px operating at the lowest voltage level and
minimum number of neighbors at the same voltage
24:
Voltage assignment of px is increased to the next higher
voltage level of neighbors
25: end while
26: Save solution as Si , i.e., X = X ∪ Si
27: end for
28: Select S ∈ X such that C(S) = minSi ∈X (C(Si ))
29: return S

• Communication volume cij - More is the value of cij
for a pair of pi , pj ∈ VP (eij ∈ EP ), it is desirable to
place them closer to each other in order to reduce the
communication energy consumption.
• Common allowable voltage levels Li ∩ Lj - More is the
value of | Li ∩ Lj |, the pair of nodes pi , pj ∈ VP can
be more likely operated at the same lower voltage level
when placed close to each other. This will reduce both
the voltage island fragmentation (and thus reducing
the number of voltage islands), and the energy consumption due to computation and level shifters.
• Delay bound λij - Less is the value of λij for an edge
eij ∈ EP , it is desirable to place them closer to each
other in order to satisfy the delay bounds.
• Distance dij - More is value of dij between pi and pj
in the current placement, it may be required to move
them closer.
Based on this observation, we define a function f (i, j) for
every eij ∈ EP , which is directly proportional to cij , | Li ∩
Lj | and dij , while inversely proportional to λij . Then we
can claim that higher the value of f (i, j), it is desirable to
place pi and pj closer to each other. Before defining the
function, we need to scale all the constituting factors into
the same order of magnitude, so that none of them dominate
the value of f (i, j) alone. It should be noted that only the
dij factor changes from solution to solution and thus scale
the effect of the other factors accordingly in deciding the
movement of the PEs. Now, we can define f (i, j) as:
∀eij = (pi , pj ) ∈ EP :

f (i, j) =

cij × | Li ∩ Lj |
× dij
λij

For the comparison of the solutions we use the following cost
function associated with each solution:

this selection as f (x, y). While px and py are are already adjacently placed, we remove them from F and select another
pair px , py corresponding to f (x, y) (line 8-11). Among all
neighbors of x and y, the one pu having minimum overlapping voltage range with px or py is chosen (line 12-13), and
swapped positions with px or py accordingly (line 14-18).
Now, px and py will be adjacently placed, and we set their
voltage level to the lowest common one (line 19). After N2
such perturbations are performed, we calculate the number
of islands in the current solution, numIslands (line 21). If
this value is greater than the maximum limit on the number
of voltage islands κ, we choose the PE which is operating
at the lowest voltage level and having minimum number of
neighbors at the same voltage level. We change the voltage level of this PE to a higher one which is minimum of
all its neighbors. This is performed until numIslands ≤ κ
(line 22-25). The current solution is appended in X as Si
(line 26). At the end of N1 iterations, the solution having
the minimum cost value is returned (line 29). It is to be
noted here that all Si ∈ X are feasible solutions in terms
of maximum allowable voltage islands constraint. Also, the
cost function C introduces a high penalty for solutions violating delay bounds, and thus remove them from the final
consideration.

C = E1 + E2 + E3 + φpen × DV
where E1 is the computation energy consumption, E2 is the
level shifter energy consumption, and E3 is the communication energy consumption. DV is the number of delay bound
violations in the solution. We set φpen to be a constant of
considerably higher order of magnitude. This way, solutions
with delay bound violations can be easily distinguished from
others, as they will have a higher order of magnitude of cost
than others, and will be less likely to be returned as the
final solution. The details of the algorithms are as follows:
In line 1 of the algorithm, the current solution set is initalized as empty. The for-loop (line 2 to 27), iterates N1
times, each returning a solution Si . In each iteration, first
we set the placement of the PEs randomly (line 3). In the
for-loop (line 4 to 20), this solution is perturbed N2 number
of times. First we calculate the set F of f (i, j) values for
all eij ∈ EP , and sort them (line 5-6). Based on the user
specified parameter a, we select a× | F | highest values of
the set F and select one of them randomly (line 7). For
example, if a = 0.3, then 30% of the highest values of F are
considered and one of them is selected randomly. We call

539

Table 1: Number of Nodes and Edges in the Applications
Application
Nodes Edges
24
12
13
5
12
12
16

@A2"4:&B#8>

Auto-Industry
Consumer
Networking
Office-Automation
MPEG4
MWD
OPD

#55)A4!27>#C2>)#3
27>#!)3:78>9;
34>$#9D)3*
A#387C49
C?4*.
C$:
#?:

+,0

21
12
9
5
13
11
17

+,/

+,.

+,-

+
!"#$%&"#$'

5.

!"#$%&()*('
!()*(%&"#$'
123*4&#5&62"748&5#9&!:4"2;&<#73:%&=6#">2*4&)8"23:8&<#73:'&?2)9

!()*(%&()*('

Figure 2: Optimal solution cost (scaled) for all appli-

EXPERIMENTAL RESULTS

cation CTGs with varying range of constraints (delay
bound, voltage island limit)

In this section, we present the results obtained from the
experiments performed. We analyze the effect of several
parameters on the solution costs. The experiments are performed using Communication Trace Graphs (CTG) for applications (auto-industry, consumer, networking and officeautomation) from the E3S benchmark suite [4] and three real
applications MPEG4, MWD (Multi-Window Display) and
OPD (Object Place Decoder). The number of nodes and
communication edges of the graphs are shown in Table 1.
We used five discrete choices of voltage levels as V0 = 3.6V ,
V1 = 3.3V , V2 = 2.5V , V3 = 2.3V and V4 = 1.9V . Power
consumption of the tasks on the PEs are based on the information provided in the benchmark. Using static delay
analysis, and using the power consumption variations with
voltage change from the processor vendors’ datasheet mentioned in the benchmark, we assign allowable voltage levels
for the PEs. Delay constraint parameters are varied in the
range of small values and in the range of large values. The
upper limit on the number of voltage islands is also set at
both low (between 3 to 5) and high (between 7 and 9) values.
We want to analyze the effects of the variations of the following parameters on the solution cost: N1 , N2 - the number
of iterations and perturbations per iteration in the heuristic,
a - value of the randomization parameter used by heuristic,
and λij , κ - which are the bounds on delay and number of
voltage islands in the problem specification, respectively.
We used values of N1 as 20, 100, 200, 500 and values of N2
as 5, 20, 40, 70. Three different values for the randomization
parameter a used are 0.3, 0.5 and 0.7. All the experiments
were performed on a Pentium-4 3.2 GHz processor with 1
GB RAM. The heuristic is implemented in C++. The MILP
execution for generation of optimal solution was done using
ILOG CPLEX 10.0 Concert technology on the same machine. Considering the 7 CTGs, 4 possible combination of
λij and κ (low-low, low-high, high-low and high-high), 4
values of N1 , N2 and 3 different values of a, in total 336
experiments were performed. Since, the values of the cost
function for different datasets (CTGs) are quite different, we
scale them in order to plot them in the same graph.
From Fig. 2, it can be seen that the value of the cost functions for all the seven test-cases follow similar trend, with
variations of λij and κ. This is according to our expectation,
since in case (λij , κ) = (low, low), the constraints become
stricter, leading to highest value of the cost functions. In
case of (λij , κ) being either (low, high) or (high, low), one
of the constrains is relaxed, while the other one still being
strict. In this case, the value of cost function is smaller than
the previous case. In case of (λij , κ) = (high, high), both
the constraints are relaxed, and thus leading to lowest value
of the cost function. In Fig. 2, we have plotted optimal value

1.4
Optimal
Heuristic

Scaled Cost

1.2

0.8

0.4

0

office−auto auto−indust networking

consumer

mpeg4

mwd

opd

Application test Cases − (delay bound, #voltage island bound) = (low, low)

Figure 3: Optimal vs. Heuristic solution for (delay
bound, voltage island bound) = (low, low)
1.4
optimal

heuristic

1.2

Scaled Cost

1
0.8
0.6
0.4
0.2
0

office−auto auto−indust networking

consumer

mpeg4

mwd

opd

Application Test Cases − (delay bound, #voltage island bound) = (low, high)

Figure 4: Optimal vs. Heuristic solution for (delay
bound, voltage island bound) = (low, high)
of the cost functions, for all seven datasets, and varying the
range of (λij , κ).
Comparing the value of the cost function at different values of the parameters N1 , N2 , a, it was observed that for
number of iterations N1 as 500, and the number of perturbations N2 as 70 in each iteration, we get very good quality
solutions. Although we do not expect our random greedy
selection approach to follow strictly any pattern based on
the value of a, but it can be seen that in majority of the
cases, it gives good result for a set at 0.5, since this is a
good tradeoff between the strictly greedy (when a is very
low) and strictly random (when a is very high) approaches.
In Figures 3, 4, 5, 6, we compare the results obtained by
the heuristic with that of the optimal. Following our above
observations we set (N1 , N2 ) as (500, 70) and a as 0.5 while
executing our heuristic. It can be seen from the plots in
these figures that, for all seven CTGs and for all four values
of (λij , κ), the solution cost of the heuristic is very close to

540

1.4

Optimal

[2] W. Dally and B. Towles. Route Packets, Not Wires:
On-Chip Interconnection Networks. In Proc. of DAC,
pages 684–689, Las Vegas, Nevada, USA, June 2001.
[3] G. De-Micheli and L. Benini. Networks On Chips.
Morgan Kaufmann, 2006.
[4] R. Dick. Embedded System Synthesis Benchmarks
Suite(E3S).
[5] J. Hu and R. Marculescu. Energy-Aware Mapping for
Tile-based NoC Architectures Under Performance
Constraints. In Proceedings of ASPDAC Conf.,
Kitakyushu, Japan, January 2003.
[6] J. Hu, Y. Shin, N. Dhanwada, and R. Marculescu.
Architecting Voltage Island in Core-based
System-on-a-Chip Designs. In Proc. of ISLPED, pages
180–185, 9-11 Aug. 2004.
[7] W. L. Hung, G. M. Link, Y. Xie, N. Vijaykrishnan,
N. Dhanwada, and J. Corner. Temperature-Aware
Voltage Islands Architecting in System-on-Chip
Designs. In Proc. of ICCAD, pages 689–695, 2-5 Oct.
2005.
[8] A. Jantsch and H. Tenhunen, editors. Networks On
Chip. Kluwer Academic Publishers, 2003.
[9] D. E. Lackey, P. S. Zuchowski, and T. R. Bednar.
Managing Power and Performance for System-on-Chip
Designs using Voltage Islands. In Proc. of ICCAD,
pages 195–202, November 10-14 2002.
[10] W. Lee, H. Y. Liu, and Y. W. Chan. Voltage Island
Aware Floorplanning for Power and Timing
Optimization. In Proc. of ICCAD, pages 389–394, San
Jose, CA, November 5-9 2006.
[11] H.-Y. Liu, W.-P. Lee, and Y.-W. Chang. A Provably
Good Approximation Algorithm for Power
Optimization Using Multiple Supply Votages. In Proc.
of DAC, pages 87–890, San Diego, CA, June 4-8 2007.
[12] W. K. Mak and J. W. Chen. Voltage Island
Generation under Performance Requirement for SoC
Designs. In Proc. of ASPDAC, pages 798–803, 2007.
[13] U. Y. Ogras, R. Marculescu, P. Choudhary, and
D. Marculescu. Voltage-Frequency Island Partitioning
for GALS-based Networks-on-Chip. In Proc. of DAC,
San Diego, California, USA, June 2007.
[14] M. Popovich, E. G. Friedman, M. Sotman, and
A. Kolodny. On-Chip Power Distribution Grids with
Multiple Supply Voltages for High Performance
Integrated Circuits. In Proc. of GLSVLSI, pages 2–7,
Chicago, Illinois, USA, April 2005.
[15] D. Sengupta and R. A. Saleh. Application-Driven
Voltage-Island Partitioning for Low-Power
System-on-Chip Design. IEEE Transactions on
Computer-Aided Design of Integrated Circuits and
Systems, 28(3):316–326, March 2009.
[16] K. Srinivasan, K. S. Chatha, and G. Konjevod.
Linear-Programming-Based Techniques for Synthesis
of Network-on-Chip Architectures. IEEE Transactions
on VLSI Systems, 14(4):407–420, April 2006.
[17] H. Wu and M. D. F. Wong. Improving Voltage
Assignment by Outlier Detection and Incremental
Placement. In Proc. of DAC, pages 459–464, San
Diego, CA, June 4-8 2007.

Heuristic

Scaled Cost

1

0.6

0.2

0

office−auto auto−indust networking

consumer

mpeg4

mwd

opd

Optimal vs. Heuristic Solution for (delay bound, #voltage island bound) = (high, low)

Figure 5: Optimal vs. Heuristic solution for (delay
bound, voltage island bound) = (high, low)
1.4

optimal

heuristic

Scaled Cost

1.2

0.8

0.4

0

office−auto auto−indust networking

consumer

mpeg4

mwd

opd

Application Test Cases − (delay bound, #voltage island bound) = (high, high)

Figure 6: Optimal vs. Heuristic solution for (delay
bound, voltage island bound) = (high, high)
that of the optimal. For all test-cases, the proposed heuristic
finishes execution within a few seconds, whereas the CPLEX
solver was executed for hours in order to achieve the optimal
solution. Also, out of all the 336 experiments, only once the
heuristic was unable to return a feasible solution. This was
determined by the value of the cost function being a order
of magnitude higher due to the penalty associated for delay
violation. Moreover, our proposed heuristic is much more
efficient in terms of solution time than the traditional local
search heuristics, such as, simulated annealing. In most of
the test-cases for our experiments, the optimal solution was
also found in reasonable amount of time. But, whereas for
our experiments the maximum mesh size used was 5 × 5, in
real-life very soon MPSoCs are predicted to have hundreds of
cores in a single chip. In this scenario, even for design phase,
the use of the optimal solution will become unrealistic due
to the exponential growth in solution time.

6.

CONCLUSION

We have considered the mapping and voltage islanding
problem for NoCs in a unified fashion. Considering the voltage islanding at an early phase of design, during mapping of
the PEs onto the NoC routers, can be beneficial in restricting
the number of voltage islands within a certain limit and also
minimizing the overall energy consumption. In this paper,
we have formulated the mapping and voltage islanding problem as an optimization problem. We provide both optimal
and heuristic solution to the problem. Experimental results
evaluate the quality of the heuristic solution as compared
with that of the optimal.

7.

REFERENCES

[1] T. D. Burd and R. W. Brodensen. Design Issues for
Dynamic Voltage Scaling. In Proc. of ISLPED, pages
9–14, Rapallo, Italy, 2000.

541

1

Architecture and Algorithms for an Airborne
Network†
Arunabha Sen1 , Pavel Ghosh1 , Tiffany Silva1 , Nibedita Das1 , and Anjan Kundu2

arXiv:1009.4499v1 [cs.NI] 22 Sep 2010

1

Department of Computer Science and Engineering, Arizona State University, Tempe, AZ, 85281
{Pavel.Ghosh, asen, tsilva, nmaulik}@asu.edu
2
Saha Institute of Nuclear Physics, Kolkata 700064, India
anjan.kundu@saha.ac.in

Abstract—The U.S. Air Force currently is in the process of
developing an Airborne Network (AN) to provide support to
its combat aircrafts on a mission. The reliability needed for
continuous operation of an AN is difficult to achieve through
completely infrastructure-less mobile ad hoc networks. In this
paper we first propose an architecture for an AN where airborne
networking platforms (ANPs - aircrafts, UAVs and satellites) form
the backbone of the AN. In this architecture, the ANPs can be
viewed as mobile base stations and the combat aircrafts on a
mission as mobile clients. Availability of sufficient control over the
movement pattern of the ANPs, enables the designer to develop a
topologically stable backbone network. The combat aircrafts on
a mission move through a space called air corridor. The goal of
the AN design is to form a backbone network with the ANPs with
two properties: (i) the backbone network remains connected at
all times, even though the topology of the network changes with
the movement of the ANPs, and (ii) the entire three dimensional
space of the air corridor is under radio coverage at all times by
the continuously moving ANPs.
In addition to proposing an architecture for an AN, the contributions of the paper include, (i) development of an algorithm
that finds the velocity and transmission range of the ANPs so that
the dynamically changing backbone network remains connected
at all times, (ii) development of a routing algorithm that ensures
a connection between the source-destination node pair with the
fewest number of path switching, (iii) given the dimensions of
the air corridor and the radius of the coverage sphere associated
with an ANP, development of an algorithm that finds the fewest
number of ANPs required to provide complete coverage of the
air corridor at all times, (iv) development of an algorithm that
provides connected-coverage to the air corridor at all times, and
(v) results of experimental evaluations of our algorithms, (vi)
development of a visualization tool that depicts the movement
patterns of the ANPs and the resulting dynamic graph and the
coverage volume of the backbone network.

I. I NTRODUCTION
Efforts are currently underway in the U.S. Air Force to
utilize a heterogeneous set of physical links (RF, Optical/Laser
and SATCOM) to interconnect a set of terrestrial, space and
highly mobile airborne platforms (satellites, aircrafts and Unmanned Aerial Vehicles (ANPs)) to form an Airborne Network
(AN). The design, development, deployment and management
of a network where the nodes are mobile are considerably
more complex and challenging than a network of static nodes.
† This research is supported in part by a grant from the U.S. Air Force
Office of Scientific Research under grant number FA9550-09-1-0120.

This is evident by the elusive promise of the Mobile Ad-Hoc
Network (MANET) technology where despite intense research
activity over the last fifteen years, mature solutions are yet to
emerge [1], [2]. One major challenge in the MANET environment is the unpredictable movement pattern of the mobile
nodes and its impact on the network structure. In case of an
Airborne Network (AN), there exists considerable control over
the movement pattern of the mobile platforms. A senior Air
Force official can specify the controlling parameters, such as
the location, flight path and speed of the ANPs to realize
an AN with desired functionalities. Such control provides the
designer with an opportunity of develop a topologically stable
network, even when the nodes of the network are highly
mobile. We view the AN as an infrastructure (a wireless
mesh network) in the sky formed by mobile platforms such
as aircrafts, satellites and UAVs to provide communication
support to its clients such as combat aircrafts on a mission.
Just as an Airborne Warning and Control System (AWACS)
aircraft plays a role in a mission by providing communication
support to fighter aircrafts directly engaged in combat, we
believe that the aircrafts and ANPs forming the AN will
provide similar support to the combat aircrafts over a much
larger area. As shown in Fig. 2(a), the combat air crafts on
a mission fly through a zone referred to as an air corridor.
In addition to forming a connected backbone network, the
ANPs are also required to provide complete radio coverage
in the air corridor so that the combat aircrafts, irrespective of
their locations within the air corridor, have access to at least
one backbone node (i.e., an ANP) and through it, the entire
network. Accordingly, the AN is required to have two distinct
properties: (1) the backbone network formed by the ANPs
must remain connected at all times, even though the topology
of the network changes with the movement of the ANPs, and
(2) the entire three dimensional space of the air corridor is
covered at all times by the continuously moving ANPs. To
the best of our knowledge this is the first paper that proposes
an architecture for an AN and provide solutions for the all time
connected-coverage problem of a three-dimensional space with
mobile nodes.

One of the pioneering results in three dimensional coverage
problem for sensor networks was presented by Haas et al.

2

ight path

Air Corridor

client aircrafts
on mission

(a) Air Corridor and the combat aircrafts on a mission
with planned flight paths
circular orbit
of ANPs

Lac

ANP

Wac
Hac

Section of Air Corridor

(b) A section of air corridor

Fig. 1.

A schematic view of the Airborne Network

[3], [4] in which they concluded that the truncated octahedron
has the highest volumetric quotient (the ratio of the volume
of a polyhedron to the volume of its circumsphere) among
all the space filling polyhedrons and utilized this to develop
placement strategies for three dimensional underwater sensor
networks. Their scheme is a centralized one. Distributed protocol of achieving three dimensional space coverage is found
in the research of Tezcan et al. [5]. Poduri et al. [6] later on
introduced the notion of N ET graphs and used it to obtain
three dimensional sensor coverage. Similar research aiming at
the coverage problem in 3D was also presented by [7]–[9].
However none of these researchers put any emphasis on the
problem of obtaining coverage while the constituting nodes
are mobile in a three dimensional space. The mobile nature of
the ANPs in airborne networks add yet another dimension of
difficulty to the 3D coverage problem.
In this paper we first propose an architecture for an AN
where airborne networking platforms (ANPs - aircrafts, UAVs
and satellites) form the backbone or mobile base stations of
the AN, and the combat aircrafts on a mission function as
mobile clients. We then proceed to determine the the number
and initial location of the ANPs, their velocity and transmission range, so that the dynamically changing network retains
properties (1) and (2) mentioned in the previous paragraph.
The rest of the paper is organized as follows. In Section II, we
provide the system model and an architecture of an Airborne
Network. Section III formally states the connectivity problem
for an AN. In Section IV, we provide an algorithm that finds
the velocity and the transmission range of the ANPs so that
the dynamically changing network remains connected at all
times. Section V presents a routing algorithm that ensures a
connection between the source-destination node pair with the
fewest number of path switching. Given the dimensions of the
air corridor and the radius of the coverage sphere associated
with an ANP, Section VI formulates the coverage problem for
the air corridor. Section VII presents an algorithm that finds the
fewest number of ANPs required to provide complete coverage

(c) Circular orbits of
ANPs (black dots)
placed at the top
surface of air corridor

Fig. 2. Air Corridor, rectangular parallelopiped section,
client airplanes, ANPs in circular orbits

of the air corridor at all times. The Section VIII combines
results of Sections IV and VII and presents an algorithm to
provide connected-coverage to the air corridor at all times.
In Section IX we briefly describe a visualization tool that
we developed to demonstrate the movement patterns of the
ANPs and its impact on the resulting dynamic graph and
the coverage volume of the backbone network. The results
of experimental evaluations of our algorithms and related
discussion is presented in Section X. Section XI concludes
the paper.
II. S YSTEM M ODEL AND A RCHITECTURE
A schematic diagram of our view of an AN is shown in
Fig. 1. In the diagram, the black aircrafts are the Airborne
Network Platforms (ANP), the aircrafts that form the infrastructure of the AN (although in Fig. 1, only aircrafts are shown
as ANPs, the UAVs and satellites can also be considered as
ANPs). We assume that the ANPs follow a circular flight path.
The circular flight paths of the ANPs and their coverage area
(shaded spheres with ANPs at the center) are also shown in
Fig. 1. Thick dashed lines indicate the communication links
between the ANPs. The figure also shows three fighter aircrafts
on a mission passing through space known as air corridor,
where network coverage is provided by ANPs 1 through 5.
When the fighter aircrafts are at point P1 on their flight path,
they are connected to ANP4 because point P1 is covered by
ANP4 only. As the fighter aircrafts move along their flight
trajectories, they pass through the coverage area of multiple
ANPs and there is a smooth hand-off from one ANP to another
when the fighter aircrafts leave the coverage area of one ANP
and enter the coverage area of another. The fighter aircrafts are
connected to an ANP as long as they are within the coverage
area of that ANP. At points P1, P2, P3, P4, P5 and P6 on their
flight path in Fig. 1, the fighter aircrafts are connected to the
ANPs (4), (2, 4), (2, 3, 4), (3), (1, 3) and (1), respectively.
One major difference between the wireless mesh networks
deployed in many U.S. cities [10] and the ANs is the fact that,

3

while the nodes of the wireless mesh networks deployed in the
U.S. cities are static, the nodes of an AN are highly mobile.
However, as noted earlier, the AN designer has considerable
control over the movements of the mobile platforms forming
the AN. She can decide on the locality where the aircraft/ANPs
should fly, its altitude, flight path and speed of movement.
Control over these four important parameters, together with
the knowledge of the transmission range of the transceivers on
the flying platforms, provides the designer with an opportunity
for creating a fairly stable network, even with highly mobile
nodes. In this paper, we make a simplifying assumptions that
two ANPs can communicate with each other whenever the
distance between them does not exceed the specified threshold
(transmission range of the onboard transmitter). We are well
aware of the fact that successful communication between
two airborne platforms depends not only on the distance
between them, but also on various other factors such as (i)
the line of sight between the platforms [11], (ii) changes in
the atmospheric channel conditions due to turbulence, clouds
and scattering, (iii) the banking angle, the wing obstruction and
the dead zone produced by the wake vortex of the aircraft [12]
and (iv) Doppler effect [13] . Moreover, the transmission range
of a link is not a constant and is impacted by various factors,
such as transmission power, receiver sensitivity, scattering loss
over altitude and range, path loss over propagation range, loss
due to turbulence and the transmission aperture size [12].
However, the distance between the ANPS remains a very
important parameter in determining whether communication
between the ANPs can take place, and as the goal of this
research is to understand the basic and fundamental issues of
designing an AN with twin invariant properties of coverage
and connectivity, we feel such simplifying assumptions are
necessary and justified. Once the fundamental issues of the
problem are well understood, factors (i) through (iv) can be
incorporated into the model to obtain a more accurate solution.
III. D ESIGN FOR C ONNECTIVITY - P ROBLEM
F ORMULATION
It is conceivable that even if the network topology changes
due to movement of the nodes, some underlying structural
properties of the network may still remain invariant. A
structural property of prime interest in this context is the
connectivity of the dynamic graph formed by the ANPs. We
want the ANPs to fly in such a way, that even though the
links between them are established and disestablished over
time, the underlying graph remains connected at all times.
Although we give connectedness of the graph as an example
of a structural property, many other graph theoretic properties
P can be specified as design requirements for the network.
The problem can be described formally in the following way.
Consider n nodes (flying platforms) in an m-dimensional
space Rm (for ANP network scenario m = 3). We denote by
xi (t) ∈ Rm the coordinates of the node i at time t, where
by convention xi is considered a m × 1 column vector, and
T
by x(t) = [x1 T (t), . . . , xn T (t)] , the mn vector resulting
from stacking the coordinates of the nodes in a single vector.
Suppose that the dynamics of node i (for all i ∈ {1, 2, . . . , n}),

is given by ẋi (t) = ui (t), where ui (t) is the control vector
taking values in some set U ⊆ Rm . In vector notation, the
system dynamics become
ẋ(t) = u(t)

(1)

where ẋ(t) = [ẋT1 (t), . . . , ẋTn (t)]T and u(t) =
[u1 T (t), . . . , un T (t)]T are mn × 1 vectors, respectively. The
network of flying platforms described by system (1), gives rise
to a dynamic graph G(x(t)).
G(x(t)) = (V, E(x(t))) is a dynamic graph consisting of
• a set of nodes V = {1, 2, . . . , n} indexed by the set of
flying platforms, and
• a set of edges E(x(t)) = {(i, j) | dij (x(t)) < δ} with
dij (x(t)) =k xi (t) − xj (t) k as the Euclidean distance
between the platforms i and j and δ > 0 is a constant.
Since we have control over the node dynamics, the question
that naturally arises is whether we can control the motion of
the ANPs so that G(x(t)) retains graph-theoretic properties
of interest P for all time t > 0. A graph G is connected
if there exists a path between any two nodes of the graph G.
Often times the property P will correspond to the requirement
that the graph G remains connected at all times. Formally the
problem can be stated as follows. Suppose that Cn,P is the
set of all graphs on n nodes with property P. Is it possible
to find a control law u(t) such that if G(x(0)) ∈ Cn,P then
G(x(t)) ∈ Cn,P for all t ≥ 0?
Although a few researchers have studied problems in this
domain [14]–[16], many important questions still remain unanswered. For example, in our study of the movement pattern
of the ANPs to create a connected network, we assume that
the flight paths of the mobile platforms are already known and
we want to find out the speed at which these platforms should
move, so that the resulting dynamic graph remains connected
at all times. The studies undertaken in [14]–[16] do not address
such issues. Although the movement of the airborne platforms
will be in a three dimensional space, in a simplified version
of the problem in two dimension (i.e., when all the aircrafts
are flying at the same altitude) the problem can be stated as
follows:
Mobility Pattern for Connected Dynamic Graph (MPCDG):
This problem has five controlling parameters:
(i) a set of points {p1 , p2 , . . . , pn } on a two (or three)
dimensional space (representing the centers of circular flight
paths of the platforms),
(ii) a set of radii {r1 , r2 , . . . , rn } representing the radii of
circular flight paths,
(iii) a set of points {l1 , l2 , . . . , ln } representing the initial
locations (i.e., locations at time t = 0) of the platforms on
the circular flight paths,
(iv) a set of velocities {v1 , v2 , . . . , vn } representing the speeds
of the platforms, and
(v) transmission range Tr of the transceivers on the airborne
platforms.
IV. D ESIGN FOR C ONNECTIVITY - S OLUTION
In the MPCDG problem scenario, any structural property
P of the resulting dynamic graph will be determined by the

4

y

y
i(0)
•

ci
•

! i (0)
R

r!i (t)
s!ij (t)

! i (t)
R

θi (t)

αci

θi (0)
x

O

Fig. 4.

•
j
!j (t)
R

αci

O

cj
•
r!j (t)

r!cj

αcj

βi

r!ci

•
i
r!ci

ci
•

Initial phase angle βi of point i; at time 0 point is shown as i(0)

distance between them does not exceed the communication
threshold distance D. This implies that the link between the
nodes i and j is alive (or active) when

θj (t)
x

~i (t) and R~j (t)) of two points i and j at
Fig. 3. Vector representations (R
time t moving along two circular orbits: rci = 15, rcj = 27, ∠ci Ox =
αci = π3 , ∠cj Ox = αcj = π6

problem parameters (i) through (v). The problems that arise
in this formulation are as follows: Given any four of the five
problem parameters, how to determine the fifth one, so that
the resulting dynamic graph retains property P at all times?
Most often we would like to know that given (i), (ii), (iii) and
(v), at what speed the ANPs should fly so that the resulting
graph is connected at all times. Alternately, we may want to
determine the minimum transmission range of the ANPs to
ensure connectivity. In this case, the problem will be specified
in the following way. Given (i), (ii), (iii) and (iv), what is the
minimum transmission range of the ANPs so that the resulting
graph is connected at all times? In order to answer these
questions, we first need to able to answer a simpler question.
Given all five problem parameters including the speed of the
ANPs, how do you determine 1if the resulting dynamic graph
is connected at all times? We discuss this problem next.
A. Connectivity checking for AN when all control parameters
specified
In this subsection we describe our technique to find answer
to the question posed in the previous paragraph. Suppose
that two ANPs, represented by two points i and j (either in
two or in three dimensional space, the two dimensional case
corresponds to the scenario where the ANPs are flying at same
altitute) are moving along two circular orbits with centers at
ci and cj with orbit radius ri and rj as shown in Fig. 3 with
velocities vi and vj (with corresponding angular velocities ωi
and ωj ), respectively.
~ i (t)
A moving node i is specified by the radius vector R
~
directed from some origin point O, and similarly Rj (t) for
point j. Therefore the distance sij (t) between the nodes i − j
at time t is given by:
~ i (t) − R
~ j (t))2 = R2 (t) + R2 (t) − 2R
~ i (t) · R
~ j (t)
s2ij (t) = (R
i
j
(2)
As mentioned earlier, we have assumed that the communication between the ANPS is possible if and only if the Euclidean

sij (t) ≤ D

(3)

In the analysis that follows, we have assumed that ANPs
are flying at the same altitude, i.e., we focus our attention
to the two dimensional scenario. However, this analysis can
easily be extended to the three dimensional case to model the
scenario where the ANPs are flying at different altitude. In this
case we can view the ANPs as points on a two-dimensional
plane moving along two circular orbits, as shown in Fig. 3.
In Fig. 3, the vectors from the origin O to the centers of
the orbits ci and cj are given as r~ci and r~cj . The cartesian
co-ordinates of the centers can be readily obtained as r~ci =
(rci cos αci , rci sin αci ) and r~cj = (rcj cos αcj , rcj sin αcj ).
~ i (t) can be expressed in polar coordinates:
Accordingly, R
Ri (t), θi (t) with respect to origin point O, as shown in Fig. 3,
~ i (0)
and similarly for R~j (t). The initial location of the points R
and R~j (0) are given. From Fig.1 4, the phase angle βi for node
i with respect to the center of orbit ci , can be calculated as
(by taking projection on the axes):
tan βi =
Since from Fig. 3,

Ri (0)cos θi (0) − rci cos αci
Ri (0)sin θi (0) − rci sin αci

(4)

~ i (t) = ~rc + ~ri (t)
R
i

(5)

where ~ri (t) = (ri cos (βi + ωi t), ri sin (βi + ωi t)) (since
angle made by i at time t w.r.t. ci is given by (βi + ωi t)).
Therefore, the angle between ~ri (t) and ~rci is (βi − αci + ωi t).
Hence,
Ri2 (t) = rc2i + ri2 + 2rci ri cos (βi − αci + ωi t)

(6)

~ i (t) = ~rc + ~ri (t) on the x and
Now taking the projection of R
i
y axes, we get
Ri (t) cos θi (t)

= rci cos αci + ri cos (βi + ωi t), (7)

Ri (t) sin θi (t)

= rci sin αci + ri sin (βi + ωi t)

(8)

Recalling cos(A − B) = cos A cos B + sin A sin B, and
simplifying we get
Ri (t)Rj (t) cos(θi (t) − θj (t)) = rci rcj cos αci cj

+ri rj cos(βij + (ωi − ωj )t) + rci rj cos(αci − βj − ωj t)
+rcj ri cos(αcj − βi − ωi t)

(9)

5

where αcij = αci −αcj and βij = βi −βj . Combining equation
2 with equations 6 and 9, we have:
s2ij (t)

=
+

rc2i
rc2j

+ ri2
+ rj2

+ 2rci ri cos(βi − αci + ωi t)

+ 2rcj rj cos(βj − αcj + ωj t)

+ rci rcj cos αci cj + ri rj cos(βij + (ωi − ωj )t)
+ rci rj cos(αci − βj − ωj t)

+ rcj ri cos(αcj − βi − ωi t)

(10)

In equation 10, all parameters on the right hand side are
known from the initial state of the system, and thus the
distance sij (t) between the nodes i − j at any time t can be
obtained. If the ANPs move at same velocity, i.e., ωi = ωj = ω
for all i, j and the radius of the circular orbits are identical, i.e.,
ri = rj = r for all i, j, and the above expression simplifies
to:
s2ij (t)

=
+

rc2i + r2 + 2rci r cos(βi − αci + ωt)

rc2j + r2 + 2rcj r cos(βj − αcj + ωt)

+

rci rcj cos αci cj + r2 cos βij

+

rci r cos(αci − βj − ωt)

+

rcj r cos(αcj − βi − ωt)

(11)

If the problem parameters (i) through (v) are specified, we
can check if the dynamic graph is connected at all times
following these two steps. In the first step, we determine the
lifetime (active/inactive) of a link between a pair of nodes i
and j in the following way.
Algorithm 1: Link Lifetime Computation
1. begin
2. Using equation (10), compute and plot the distance
between a pair nodes i and j, sij (t), as a function
of time; ( See Fig. 5(a))
3. Draw a horizontal line in the sij (t) versus t plot with
sij (t) = D, where D is the communication threshold,
i.e., communication between i and j is possible
if sij (t) ≤ D and impossible otherwise.
Call this line communication threshold line, CTL.
4. The CTL is divided in to segments corresponding
to the parts where sij (t) ≤ D and where sij (t) > D.
5. Projections of the CTL segments on the x-axis
(i.e., the time line) indicates the times when
the link between i and j is alive and
when it is not. ( See Fig. 5(b))
6. end
Using Algorithm 1, we can compute the life time of every
link (i.e., every pair of nodes) in the network. In step 2, using
Algorithm 2 (given below) we divide the time line into smaller
intervals and determine exactly the links that are active during
each of these interval. For each of the intervals we check if the
AN graph is connected during that interval using connectivity
checking algorithm in [17]. The algorithm is described in
detail next.

Algorithm 2: Checking Connectivity of Airborne Network
between time t = t1 and t = t2
1. begin
2. Using the Algorithm for Link Lifetime Computation,
compute the lifetimes of links between all node pairs
and plot them over time line. (See Fig. 6)
3. Draw a vertical line through start and finish
time of each interval associated with a link on the
x-axis (time line)
4. Repeat step 3 for each link of the network
5. The x-axis (time line) is divided into a number of
smaller intervals. (See Fig. 6, intervals are
numbered from 1 through 17). From the figure,
we can identify all the links that are alive
during any one interval.
6. Check if the AN graph is connected with the set
of live links during one interval. This can be done
with the connectivity testing algorithm in [17]
7. Repeat step 6 for all the intervals between t = t1
and t = t2 .
8. If the AN graph remains connected for all intervals,
conclude that the AN remains connected during
the entire duration between t = t1 and t = t2 ,
otherwise conclude that the specified problem
parameters does not ensure a AN that remains
connected during the entire time interval
between t = t1 and t = t2 .
6. end
An example of a plot of equation (10) (generated using
MATLAB) is shown in Fig. 5(a) with communication threshold distance D = 18. This implies that the link between the
nodes i and j exists, when the distance between them is at
most 18 and the link does not exist otherwise. This is shown
in Fig. 5(b). The red part indicates the time interval when the
link is inactive(or dead) and the blue part indicates when it is
active (or live).
Thus using equation (10) and comparing the distance between any two nodes with the threshold distance D, we
can determine active/inactive times of all links. This can be
represented as intervals on a time line as shown in Fig. 6. By
drawing projections from the end-points of the active/inactive
times of each link on the time line, we can find out all the
links that are active during a interval on time line. As shown
in Fig. 6, links 1, 2 and 3 are active in interval 1; links 1 and
3 are active in interval 2, links 1, 2 and 3 are active in interval
3 and so on. Once we know all the links that are active during
a time interval, we can determine if the graph is connected
during that interval using any algorithm for computing graph
connectivity [18]. By checking if the graph is connected at all
intervals, we can determine if the graph is connected at all
times, when the ANPs are moving at specified velocities.
B. Finding the velocity of the ANPs to ensure a connected AN
during operational time between t = t1 and t = t2
In subsection A, we have described a technique to determine
if the AN remains connected during the entire operational time

6

Edge exists between i and j

20

15

10

5

0
0

50

100

150

200

250
Time

300

350

400

450

500

(a) Distance between two points i and j as a function of time
Fig. 5.

No edge between i and j

25
Distance between nodes i and j

Distance Between Nodes i and j

25

D = 24

20
D = 18

15

10

5
D=4

0
0

50

100

150

200

250
Time

300

350

400

450

500

(b) Active (Blue)/Inactive (Red) times of the link between i and j with
transmission range Tr = 18

Effect of the distance between nodes on the existence of the communication link between them

between t = t1 and t = t2 ), when all problem parameters,
(i)-(v) are specified. In this subsection we try to determine
the problem parameter (iv) (i.e., velocity of the ANPs) that
we ensure a connected AN during the entire operational time
between t = t1 and t = t2 ) when all other problem parameters
have already been specified. The minimum and maximum
operating velocities of the ANPs (vmin , vmax ) are known. By
conducting a binary search on this range, we can compute
the minimum velocity at which the ANPs should fly, so that
the AN remains connected during the entire operational time.
Alternately, we can also try to determine the velocity at which
the ANPs should fly, so that the AN remains connected during
the entire operational time and fuel consumption by the ANPs
is minimized. If it is known that the fuel consumption is
minimized when the ANPs fly with velocity voptimal , we can
find the velocity that is closest to voptimal and also ensures
connectivity of the AN during entire operational time by a
targeted search within the range (vmin , vmax ).
C. Finding the transmission range of the ANPs to ensure a
connected AN during operational time between t = t1 and
t = t2
In this subsection we try to determine the problem parameter
(v) (i.e., transmission range of the ANPs) that we ensure a
connected AN during the entire operational time between t =
t1 and t = t2 ) when all other problem parameters have already
been specified. The maximum transmission range of an ANP
is known in advance (Tmax . By conducting a binary search
within the range 0 − Tmax , we can determine the the smallest
transmission range that will ensure a connected AN during
the entire operational time when all other problem parameters
have already been determined.
V. ROUTING WITH M INIMUM PATH S WITCHING
In the previous section we described a procedure to determine the velocity of the ANPs so that the resulting dynamic
graph is connected at all times. Although the graph remains
connected at all times, as the links come and go (alive or dead)
a path between a source-destination pair may not exist for the
entire duration of communication. Suppose that a node s has to
communicate with another node d from time t = 5 to t = 12.
Since the graph is connected at all times, at least a path, say

Links Active

Links Dead

Link 1:
Link 2:

Link 3:

Timeline

1

2

3 4 5

6

7 8

9

10

11 12 13 14 15 16 17

Fig. 6. Active/Inactive time interval of each link and interval intersection
projections on the time line

P1 , exists from s to d at t = 5. However, this path may
not exist till t = 12. Suppose that as one of its link dies, P1
breaks at t = 7. Clearly P1 cannot be used for communication
between s and d at t = 7. Since the graph is connected at all
times, there must exist at least one path, say P2 , between s
and d at t = 7. Therefore data can be transferred from s to d
using P2 at t = 7. However, P2 can break at t = 10, in which
case a yet another path, say P3 (which is guaranteed to exist
because the graph is connected at all times) can be used for
communication between s and d from t = 10 to t = 12. In
such a scenario, the path sequence P1 → P2 → P3 is used
for communication between s and d in the time interval t = 5
to t = 12. In this scenario the path has to be switched two
times, once from P1 → P2 and the other time from P2 → P3 .
However, it is possible that communication between s and d in
the time interval t = 5 to t = 12 could have been achieved by
only one path switching, using a path P4 from t = 5 to t = 9
and a path P5 from t = 9 to t = 12. Since path switching
involves a certain amount of overhead, it is undesirable and
as such we would like to accomplish routing for the duration
of communication with as few path switching as possible.
In Fig. 6 we showed how “lifetime” of a link (i.e.,
alive/dead) can be computed. Since paths comprise of links,
a path between a source-destination node pair will also be
alive/dead at different points of time. From the lifetime of
links, we can compute the lifetime of paths in the following
way. If the number of nodes (i.e., ANPs) in the network is n,
there exists n(n − 1)/2 links with each having an individual
lifetime. If a path P is made up of links l1 , l2 , . . . , lk , the
path P is “alive” when all the links l1 through lk are alive.

7

P1

t=0

t=t1

I1,3

t=t2
Ti1,j1

I2,1

P2

Pr

I1,2

I1,1

Ir,1

I2,2

I2,3

Ir,2

t=t1

Solution
Produced
by Optimal
Algorithm

I2,4

Ii1,j1

Ii2,j2

Pi1

Pi2
Ti1',j1'

Ir,3
t=t2

Fig. 7.
Lifetime of r Paths between a source-destination pair, and the
corresponding time intervals when they are alive. s and d need to communicate
between time t = t1 and t = t2

Therefore, similar to Fig. 6 that shows the“lifetime” of a link,
we can construct a figure for “lifetime” of a path (Fig. 7). Once
we have knowledge of lifetimes of paths, we can construct a
route from the source node s to the destination node d with
the fewest number of path switching in the following way.
The lifetimes of paths between a source-destination node
pair s and d is shown in Fig. 7. The time intervals
during which a path is alive is shown by solid lines in
Fig. 7. We use the notation Pj = {Ij,1 , Ij,2 , . . . , Ij,jk },
to indicate that the path Pj is alive during the jk time
intervals {Ij,1 , Ij,2 , . . . , Ij,jk }, as shown in the Fig. 7. In
the application scenario that we are considering, we want a
communication channel to be open between the source node
s and the destination node d for the entire duration of time
from t = t1 to t = t2 . Since it is possible that no single path
between s to d remains alive for the entire duration from t = t1
to t = t2 , a set of paths P may constitute a communication
channel from s to d for the duration, where each path in P
is alive only for a fraction of the time interval from t = t1 to
t = t2 .
Next we focus on the number of paths between s and d
that we need to consider. Since the graph has n nodes and
n(n − 1)/2 links, there could be as many as (1 + (n − 2) +
(n − 2)(n − 3) + . . . + (n − 2)(n − 3)(n − 4) . . . 2 = O(nn ))
paths corresponding to 1-hop, 2-hop, . . ., (n − 1)-hop paths
between s and d. It may also be noted that each of these
paths will have a lifetime associated with it. Since examining
O(nn ) paths and their lifetimes will be too time consuming,
we restrict our attention to only those paths between s and d
whose number of hops is at most k, for some specified value
of k. Restricting the number of hops in a path to at most k
(from n − 1), we reduce the computational complexity from
O(nn ) to O(nk ). Suppose the set of paths of at most k hops
is denoted at Pk . The minimum path switch routing algorithm
given below finds a subset Pk0 ⊆ Pk so that paths in Pk0
maintain a communication channel between s and d for the
entire time duration from t = t1 to t = t2 with the fewest
number of path switchings.
Minimum Path-Switch Routing Algorithm
Input: The set Pk of paths between s and d with length at most
k-hops, and associated lifetimes of each each path Pi ∈ Pk ,
in the form of live intervals of Pi . If there are r live intervals
of Pi in the time interval between t = t1 to t = t1 , we denote

Solution
Produced by
Minimum Path
Switch Routing
Algorithm

Ii1',j1'
Pi1'

Ti(x-1),j(x-1)

Ti2,j2

Ii2',j2'
Pi2'

....

Tix,jx

Iix,jx
Pix

Ti2',j2'

Ti(y-1)',j(y-1)'

....

Tiy',jy'

Iiy',jy'
Piy'

Fig. 8. Solution Produced by the Optimal Algorithm and the Minimum Path
Switch Routing Algorithm

Pi = {Ii,1 , Ii,2 , . . . , Ii,r }.
Output: A subset Pk0 ⊆ Pk so that paths in Pk0 maintain a
communication channel between s and d for the entire time
duration from t = t1 to t = t2 with the fewest number of path
switchings.
Comments: The algorithm uses a greedy (locally optimum)
approach to find the paths needed to have one live path from
s to d during the entire time interval t = t1 and t = t2 .
In Theorem 1, we prove that this locally optimum greedy
approach indeed finds the globally optimal solution.
1.
2.
3.
4.
5.

begin
Pk0 = ∅;
tstart = t1 ;
tf inish = t1 ;
While (tf inish < t2 ) do
begin
(i) for all Pi ∈ Pk do
if ((start time (Ii,j ) ≤ tstart )
&& (tf inish < f inish time(Ii,j ) for some j)
begin
(i) tf inish = f inish time(Ii,j )
(ii) Pk0 = Pk0 ∪ Pi ;
end
tstart = tf inish ;
end
6. end
Theorem 1: The Minimum Path-Switch Routing Algorithm
finds a set of paths so that a communication channel is open
during the entire duration from t = t1 to t = t1 with the
fewest number of path switches.
Proof: Suppose that an optimal algorithm selected the paths
{Pi1 , Pi2 , . . . , Pix } and the Minimum Path-Switch Routingl
algorithm selected the paths {Pi01 , Pi02 , . . . , Pi0y }, where x < y.
The live intervals of the paths {Pi1 , Pi2 , . . . , Pix } that were
selected by the optimal algorithm are {Ii1 ,j1 , Ii2 ,j2 , . . . , Iix ,jx }
respectively. Similarly, the live intervals of the paths
{Pi01 , Pi02 , . . . , Pi0x } that were selected by the Minimum PathSwitch Routing algorithm are {Ii01 ,j10 , Ii02 ,j20 , . . . , Ii0y ,jy0 } respectively. The paths and intervals chosen by the two algorithms are shown in Fig. 8. As shown in Fig. 8, the
finish times of the intervals {Ii1 ,j1 , Ii2 ,j2 , . . . , Iix ,jx } are

8

denoted as {Ti1 ,j1 , Ti2 ,j2 , . . . , Tix ,jx } and the finish times
of the intervals {Ii01 ,j10 , Ii02 ,j20 , . . . , Ii0y ,jy0 } are denoted as
{Ti01 ,j10 , Ti02 ,j20 , . . . , Ti0y ,jy0 }.
Since the Minimum Path-Switch Routing Algorithm
chooses the path Pi ∈ Pk such that Pi is live at tstart and the
finish time of the live interval containing t1 is largest among
the finish times of the intervals associated with all the paths,
we can conclude that Ti01 ,j10 > Ti1 ,j1 . Therefore, replacing the
path Pi1 from the optimal soultion by the path Pi01 we will have
a new optimal solution {Pi01 , Pi2 , . . . , Pix }. Because of nature
of the path selection criteria of the Minimum Path-Switch
Routing Algorithm, we can conclude that Ti02 ,j20 > Ti2 ,j2 .
Therefore, replacing the path Pi2 from the new optimal
soultion by the path Pi02 we will have yet another optimal
solution {Pi01 , Pi02 , . . . , Pix }. Continuing this process, we can
get an optimal solution {Pi01 , Pi2 , . . . , Pi0x }. This implies that
the Minimum Path-Switch Routing Algorithm will select only
x paths instead of y, (x < y) to have an open communication
channel for the enire duration of t = t1 to t = t2 , and since
x is the optimal number of paths needed for this purpose,
the Minimum Path-Switch Routing Algorithm produces an
optimal solution.
VI. D ESIGN FOR C OVERAGE - P ROBLEM F ORMULATION
In this section, we discuss the coverage model of the
network formed by the ANPs. As shown in Fig. 2(a), an
air corridor through which the combat aircrafts fly towards
their destination can be modeled as a collection of rectangular
parallelepipeds. As the combat aircrafts must have access to
the AN as they fly through the air corridor, all points inside
the air-corridor must have radio coverage at all times. As
the shape of an air corridor can be quite complex, we view
that the complex shape can be approximated with rectangular
parallelepipeds as shown in Fig. 2(b).
The length, width and height of this section of the aircorridor are denoted as Lac , Wac and Hac , respectively. The
radius of the circular orbit of the ANPs is denoted by ro and
the number of ANPs in each such orbit is denoted by n. We
assume that the ANPs move around in their orbit with uniform
velocities. The coverage volume of each ANP is defined as
a spherical volume of radius rs with the ANP being at the
center of the sphere. We assume that the orbits of the ANPs are
located at the top surface of the air corridor so that they do not
cause any hindrance in the flight path of the combat aircrafts.
This is shown in Fig. 2(c), where two circular orbits, each of
them containing 5 ANPs, are located at the top surface of the
air corridor. The number of orbits located at the top surface
of the air corridor section is denoted as m, and accordingly
the total number of ANPs in the network is given by mn.
The goal of the coverage problem is to provide complete
coverage at all times of the entire air corridor with the fewest
number of ANPs. In this problem, a complete coverage must
be provided irrespective of the locations of the ANPs as they
move continuously in their respective orbits.
In Figs. 9, 10, 11, we can see the 3D view, top-view and
front-view of 5 ANPs in circular orbits and the volume covered
by them for three different cases. In case I (Fig. 9), the value of

orbit radius (ro ) is greater than that of the spherical coverage
volume (rs ) of each ANP. It can be clearly seen from Fig. 9(b),
that there is an open space inside the orbit, that is not covered
by any of the ANPs as they move in the orbit. To increase the
intersection volume, ro can be at most rs (case II). In Fig. 10,
ro = rs , and we can see from Fig. 10(b), that the spheres meet
at a single point inside the orbit. We call the intersection of
adjacent spheres as leaf, the top view of which is visible in
the figure. To increase the intersection of the spheres further,
we need to decrease ro even more, and thus bringing in the
ANPs even more closer to each other. This is case III (shown
in Fig. 11), where ro < rs .
The coverage problem can be stated as follows: Given the
rectangular parallelepipeds in terms of Lac , Wac and Hac and
the radius of the coverage sphere associated with an ANP, rs ,
find the radius of the orbit of the ANPs ro , and the number of
ANPs in each orbit (n), the entire volume of the air corridor
is covered at all times with the fewest number of ANPs.
Intersection of coverage spheres of the ANPs create a
coverage volume. For two intersecting spheres, the intersection
volume is shown in Fig. 12. As the ANPs move in their
orbits, the associated coverage spheres move with them and
consequently the volume that is covered by the moving ANPs
also changes. As a consequence some volume will be covered
only a part of the time. However, a part of the intersection
volume will be covered at all times irrespective of the positions
of the ANPs as they move in their orbits. This is defined as the
invariant coverage volume. We would like to use this invariant
coverage volume as building blocks in order to fill up the air
corridor modeled in the form of a rectangular parallelopiped.
Since the invariant coverage volume is irregular-shaped, it is
difficult to use it as a building block. For ease of coverage
using a building block with a regular shape, we extract a
cylindrical volume out of this invariant volume and use it to
fill up the rectangular parallelopiped. Such a cylinder is shown
in Fig. 12. Different views of such a cylindrical section for 5
intersecting ANPs in a circular orbit are shown in Fig. 13.
As we have decided to use a cylinder as the building block
to cover the air corridor, we need to know the height and radius
of the circular surface of such cylindrical blocks, denoted by
2hc and rc , respectively. The height and radius of the invariant
coverage cylinder are determined by (i) the orbit radius (ro ),
(ii) the number of ANPs per orbit (n) and (iii) the radius of the
spherical coverage volume of each ANP (rs ). As mentioned
earlier, in this design the ANPs and their orbits are placed on
the top surface of the air corridor. As a consequence, the top
half of the invariant coverage cylinder cannot be utilized and
only the bottom half of the cylindrical volume (of height hc )
will be used for the coverage of the rectangular parallelopiped.
Therefore, in order to cover the height of air corridor, one must
satisfy the constraint hc ≥ Hac (Fig. 14). Once this constraint
is satisfied, the problem reduces to cover the plane defined by
Lac × Wac with circles of radius rc with a goal to minimize
the total number of ANPs required (mn).

We investigate the structure of the coverage volume. In
Fig. 15, we have shown the top view through the center

9

(a) 5 ANPs moving in a circular orbit
Fig. 9.

(b) Top View of the coverage spheres and circular orbit of 5 ANPs

(c) Front View of the coverage spheres
and circular orbit of 5 ANPs

Case II: orbit radius of ANPs (ro ) = radius of coverage sphere (rs )

(a) 5 ANPs moving in a circular orbit
Fig. 11.

(c) Front View of the coverage spheres
and circular orbit of 5 ANPs

Case I: orbit radius of ANPs (ro ) > radius of coverage sphere (rs )

(a) 5 ANPs moving in a circular orbit
Fig. 10.

(b) Top View of the coverage spheres and circular orbit of 5 ANPs

(b) Top View of the coverage spheres and circular orbit of 5 ANPs

(c) Front View of the coverage spheres
and circular orbit of 5 ANPs

Case II: orbit radius of ANPs (ro ) < radius of coverage sphere (rs )

of orbit of three consecutive spheres (out of total n of
them) intersecting with each other and moving around in the
circular orbit with center at O. The center of the spheres are
denoted as C1 , C2 and C3 . The radius of the circular orbit is
OC1 = OC2 = OC3 = ro . All the spheres are of uniform
radius rs , and moving in a uniform velocity. The term leaf is
used to refer to the intersection between two adjacent spheres.

In the top view, it can be seen as the intersection of two circular
˘
˘ . The length of the leaf P Q is denoted
arcs P
SQ and QRP
by 2hl (P T = T Q = hl ). Distance from the center of the
orbit center O to the end point of the leaf Q is denoted by y.
Therefore,
T O = T Q − OQ = hl − y

10

(a) 3-D view
Fig. 12. Intersection volume of two spheres; cylindrical
volume cut from the intersection volume

(b) Front View

Fig. 13. Cylindrical volume cut from the intersection of the coverage spheres of ANPs
(rs > ro )

Hac

Fig. 14. Air-corridor being filled up with cylindrical sections - the cylinder
shown is formed due to the ANPs in a particular orbit on the top surface

The width of the leaf (T S) is denoted by wl . Angle between
two adjacent leaves ∠P OM is given by θ. For n number of
spheres moving in the orbit, θ is given by 2π
n . Therefore,

Fig. 15.
Orbit

Top View of Three Consecutive Intersecting Spheres Moving in a

Fig. 16.

Side View of Intersecting Spheres and the Leaves

π
θ
=
2
n
Now, in Fig. 16, the side view of two intersecting spheres
and leafs are shown. The intersecting spheres are shown using
dashed line, whereas the intersecting leaves are shown using
solid lines. We cut the largest cylindrical volume from the
intersecting region such that this is covered by at least one
sphere at all times, as the spheres move around in the orbit.
From Fig. 15:
∠P OC2 = ∠C2 OM =

PT

2

i.e., h2l
i.e., h2l
hl
Also, from 4C2 T O:

2

= P C2 − C2 T

2

= rs2 − (C2 S − T S)2

= rs2 − (rs − wl )2
»
wl (2rs − wl )
=

(12)

θ
C2 T = T Otan
2
π
rs − wl = (hl − y)tan
(13)
n
θ
C2 T = C2 Osin
2
θ
rs − wl = ro sin
2
π
wl = rs − ro sin
(14)
n
From equations 12, 13 and 14, it is clear that all variables
hl , wl and y can be represented through variables ro and n
only. Now from Fig. 16, it is clear that the cylindrical volume
with largest height that remains covered during the movement

of the spheres, has radius of the circular surface (rc ) equal to
length V A. This can be calculated as :
VA
i.e., rc

=

OU

=

2(T Q − OQ)

=

2(hl − y)

(15)

The height of the cylinder is given by V W = 2V O = 2hc ,
calculated as:
VO

2

=

h2c

=

hc

=

2

V T − TO

2

h2l − (hl − y)2
»
y(2hl − y)

(16)

Therefore, given the radius of the orbit ro , the number of
spheres moving in each orbit n, and radius of each sphere rs ,
the cylinder can be determined by using the above equations

11

10

10

5

20

8

cylinder volume (vc)

15

height of cylinder (hc)

Radius of Cylinder (rc)

5000

9

20

7
6
5
4
3
2

4000

3000

2000

1000

20

15
0
0

1

10
1

0
0

5

2

3

4

5

6

orbit radius (r )

7

8

9

10

0

1

10
2

3

4

(a) Radius of Cylinder

5

6

orbit radius (ro)

number of spheres per orbit (n)

o

Fig. 17.

20
15

=

2(hl − y)

=

2(rs − wl )cot

(from equation 15)
θ
2

(from equation 13)

θ
θ
2ro sin cot
2
2
π
θ
= 2ro cos = 2ro cos
2
n

h2c

(from equation 14)
(17)

= h2l − (hl − y)2

θ
(rs2 − (rs − wl )2 ) − ((rs − wl )cot )2
2
1 2
2
)
= rs − ((rs − wl )
sin θ2

=

hc

= rs2 − ro2
p
=
rs2 − ro2

(18)

The volume of cylinder is given by:
vc

=

πrc2 hc

(19)

The change of the radius (rc ), height (hc ) and volume (vc )
of the cylindrical region with the values of ro and n for fixed
value of rs can be seen in the plots in Figs. 17(a), 17(b) and
17(c). The coverage problem for an airborne network can be
formally defined as follows. Given:

•

10

0

number of spheres per orbit (n)

10
1

2

3

4

5

6

5
7

orbit radius (ro)

8

9

10

0

number of spheres per orbit (n)

(c) Volume of Cylinder

VII. D ESIGN F OR C OVERAGE - S OLUTION

Using equations 12, 13, 14 and 16, we can express hc as :

•

9

15
0
0

Change of Cylinder Radius, Height and Volume with Orbit Radius and Number of Spheres per Orbit with fixed Sphere Radius = 10 units

=
rc

8

(b) Height of Cylinder

as follows:
rc

7

5

The length, width and height of the air corridor as Lac ,
Wac and Hac .
The radius of each coverage sphere associated with each
ANP rs .

Find (i) the orbit radius, ro , (ii) the number of ANPs in each
orbit, n, (iii) the number of orbits m required to cover the
air corridor, and (iv) the placement of the center of the orbits
(xi , yi ), 1 ≤ i ≤ m, such that:

1) The orbits are placed only on the top surface of the air
corridor.
2) All points in the rectangular area defined by Lac and Wac
is covered by at least one circle of radius rc with center
at (xi , yi ), 1 ≤ i ≤ m.
3) hc ≥ Hac
4) Total number of spheres required = total number of orbits
× number of spheres per orbit = mn is minimized.

In order to minimize the objective function subject to the
constraints, we need to find ro and n. This will determine the
values of rc and hc . It can be seen from equations (17) and
(18), that decreasing ro , increases hc , but decreases rc . Intuitively, with smaller value of rc , we will need more orbits (m)
to cover the rectangular parallelopiped, which will eventually
increase the number of ANPs required (mn). Therefore, for
given rs we need to set ro at the highest possible value that still
satisfies the constraint hc ≥ Hac . For the corresponding rc , m
will be determined by the placement strategy of the orbits on
the top surface of the air corridor. The overall objective of the
coverage problem is to minimize mn subject to the constraint
hc ≥ Hac . We use the following two strategies for placement
of circular orbits on the top surface of the air corridor:
Strategy 1: The largest square that can be inscribed in the
circular surface of the cylinder is used as the building block
to cover the rectangular region defined by Lac and Wac . With
rc being the radius
√ of the cylinder, the length of each side a of
the square is 2rc . Therefore, total number of orbits can be
Wac
ac
calculated as m = d √L2r
e × d√
e. Hence, the optimization
2rc
c
problem following strategy 1 can be formally stated as:
Lac
Wac
minimize mn = d √ e × d √ e × n
2rc
2rc
Lac
Wac
=d √
e×d √
e×n
2 2ro cos nπ
2 2ro cos nπ
p
subject to : hc = rs2 − ro2 ≥ Hac

(20)
(21)

Strategy 2: The largest rectangle that can be inscribed in the
circular surface of the cylinder, and that has the same length
to width ratio as that of the rectangular region defined by Lac
and Wac , is used as the building block for covering the entire
region. Let a and b be the length and width of such a building
block. rc being the radius of the cylinder, we get the following:
a
Lac
=
and (2rc )2 = a2 + b2
b
Wac
From the above two relations we can get:
a= p

2rc Lac
2rc Wac
, b= p
2
2
2
Lac + Wac
L2ac + Wac

Since rc = 2ro cos nπ , total number of orbits (cylinders)

12

Lac

Lac

Lac
Wac

Wac

Wac

(a) Placement using Strategy 1

(b) Placement using Strategy 2

Fig. 18. Covering Lac × Wac plane with circles of radius rc using strategy
1 and 2

required m to cover the entire region can be calculated as:
Wac
Lac
e×d
e
m = d
a
b
p
p
2
2
L2ac + Wac
L2ac + Wac
= d
e×d
e
π
π
4ro cos n
4ro cos n
Accordingly, the optimization problem following placement
strategy 2 can be formally stated as:
p
2
L2ac + Wac
minimize mn = d
e2 × n
(22)
π
4ro cos n
p
subject to : hc = rs2 − ro2 ≥ Hac
(23)
The diagrams for the placement of orbits, and hence the cylindrical regions following the above two strategies, are shown
in Fig. 18(a) and 18(b), respectively. Hence, the location of he
center of the orbits (xi , yi ) can be easily determined. It may
be observed both placement strategy 1 and 2 formulates the
coverage problem as a non-linear optimization problem. We
used the non-linear constrained program solver Nimbus [19]
to solve optimization problems following strategies 1 and 2.
The results obtained from Nimbus is discussed in Section X.
VIII. D ESIGN FOR C ONNECTED C OVERAGE
In Section VI, we discussed the three dimensional air corridor coverage problem with the ANPs. As the ANPs are mobile,
the coverage volume associated one ANP is continuously
changing with time. In Section VII we described techniques
to find least cost solution to the air corridor coverage problem
with mobile nodes. Earlier in Section IV, we discussed how
to determine the velocity and subsequently the transmission
range of the of the ANPs, so that resulting backbone network
formed by the ANPs remain connected at all times. In this
section, we discuss how to design a network of ANPs, so that
(i) it remains connected at all times and (ii) it provides 100%
coverage to the air corridor at all times.
We provide a two phase solution to the connected coverage
problem. In the first phase, using the techniques described
in Section VII, we determine the number and orbit of the
ANPs that will provide 100% coverage to air corridor at all
times. Once that is accomplished, in the second phase, using
the techniques described in Section IV, we determine the the
velocity and the transmission range of the ANPs so that the
backbone network formed by the ANPS remains connected at
all times.

Fig. 19.

Snapshot of Visualization Tool with Three Moving Objects

IX. V ISUALIZATION T OOL FOR A IRBORNE N ETWORK
D ESIGN
The visualization tool was designed for observing the
movement of objects along circular orbits in a 3D plane.
The Euclidean distance between every pair of objects keeps
changing due the their movement. Each pair of objects has a
threshold value specified. If the Euclidean distance between
this pair of objectss is within the threshold, then they are
connected by a link. As soon as the pairwise distance goes
beyond the threshold, the link is broken. The tool was designed
using OpenGL and C++. OpenGL is a 3D graphics API that
works with C++ that provides dynamic interaction with the
user. Mostly it is used for game programming and creating
3D scenes. For this program, we utilized some of the basic
features of the API to create an interactive application to
control the variables of each particular orbiting objects. One
of the features of OpenGL is the ability to alter the camera
view. This allows us to see every angle of the orbiting objects
and rotate around the scene. A snapshot of the visualization
tool with three moving points is shown in Fig. 19.
The floor is based on a 24×24 grid. The center of each orbit
has the ability to be moved along the X axis, −12 ≤ X ≤ 12,
and the Z axis −12 ≤ Z ≤ 12. The Y -axis, which represents
the height above the floor, allows the object to increase in
height in the range 1 ≤ Y ≤ 10. The radius of each circular
orbit can be modified in the range 1 ≤ R ≤ 10. Each object
has a connectivity threshold to each other. Each threshold,
A−B, B −C, and C −A has the following range 1 ≤ T ≤ 20.
Using the standard distance equation between two points in 3D
space, a link will appear to declare if the objects are within
the given threshold limit. The speed of each object is based
on the system clock, which will vary between each computer.
The speed of each object can be increased up to 5 units. Prior
to each object being set in motion, they can be positioned
strategically around their own orbit. The motion of all objects
can be paused to analyze a specific situation.
X. E XPERIMENTAL R ESULTS AND D ISCUSSION
In this section we present the experimental evaluation results
of two strategies proposed in Section VII. The goal of these
experiments were to find the impact of change of (i) radius
of the coverage sphere (rs ) and (ii) height of the air corridor

13

200

60

Lac = 100, Wac = 70, Hac = 10

80

Lac = 100, Wac = 180, Hac = 10

Strategy 2 Objective

Strategy 2 Objective
ro

150

r

o

n

40

Lac = 100, Wac = 100, Hac = 10

Objective

Strategy 1 Objective

Strategy 1 Objective

50

r

o

60

n

n

30

100

40

50

20

20
10
0
15

20

25

30

rs

35

0
15

40

(a) Lac = 100, Wac = 70, Hac = 10

20

25

30

rs

35

0
15

40

(b) Lac = 100, Wac = 180, Hac = 10

20

25

rs

30

35

40

(c) Lac = 100, Wac = 100, Hac = 10: Strategy
1 becomes the same as Strategy 2

Fig. 20. Variation of the ro , n and the objective function for Strategy 1 and 2 (all values on y axis) with variable rs and fixed value of Lac , Wac and Hac
80

Strategy 2 Objective
Strategy 1 Objective

60

120

250

Lac = 100, Wac = 70, rs = 20

Lac = 100, Wac = 180, rs = 20

200

Strategy 2 Objective

r

Objective
80

Strategy 1 Objective

o

150

n

Lac = 100, Wac = 100, rs = 20

100

ro

ro

40

n

60

n
100

40

20
50

0
5

10

15

20
Hac

25

30

20

0
5

35

10

15

20
H

25

30

0
5

35

10

15

20

ac

(a) Lac = 100, Wac = 70, rs = 20

(b) Lac = 100, Wac = 180, rs = 20

H

ac

25

30

35

(c) Lac = 100, Wac = 100, rs = 20: Strategy 1
becomes the same as Strategy 2

25

n /cos2(!/n)

20

15

10

5
2

4

6

(a)
Fig. 22.

8

10

n

n

π)
cos2 ( n

12

14

versus n

16

18

20

7

x 10

8

10

4
3.5

7

10
3

mn (in log scale)

Total number of spheres required (mn)

Fig. 21. Variation of the ro , n and the objective function for Strategy 1 and 2 (all values on y axis) with variable Hac and fixed value of Lac , Wac and rs

2.5
2
1.5
1

6

10

5

10

4

10
0.5
0
10

3

8

6

4

2

orbit radius (ro)

0

2

4

6

8

10

12

14

16

18

number of spheres per orbit (n)

(b) Change of objective function with n and ro

20

10
10

9

8

7

6

5

4

orbit radius (ro)

3

2

1

0

0

5

10

15

20

number of spheres per orbit (n)

(c) Objective function in log scale

The change of objective function and its components with n and ro

(Hac ) on (a) radius of the circular orbit of the flying ANPs
(ro ), (b) the number of ANPs in each orbit (n), and (c) the total
number of ANPs (mn) needed to provide complete coverage
for the air corridor, specified by its length, width and height
parameters Lac , Wac , Hac , respectively. Fig. 20 show impact
of changing rs on ro , n and mn for different sets of values for
Lac , Wac , Hac and for two different strategies 1 and 2. Fig. 21
show impact of changing Hac on ro , n and mn for different
set of values for Lac , Wac , rs and for two different strategies
1 and 2. The parameter values for Lac , Wac , Hac and rs used
for the experimentation are indicated in the figures.
Since the optimal coverage problem turned out to be a
non-linear optimization problem (equations (20) to (23)), we
used the non-linear constrained program solver Nimbus [19] to
solve it using two different ANP orbit placement strategies 1
and 2. In the following we discuss some experimental results,
some of which are intuitive, some others are not.
Observation 1: From Fig. 20, it can be seen that increase in
rs results in increase in ro and decrease in mn for both the
strategies 1. This is somewhat intuitive as it is only natural to

expect that as the radius of the coverage sphere increases, the
radius of the circular orbit of the ANPs will increase and the
total number of ANPs needed to cover the entire air corridor
will decrease. It may also be noted that when rs is too small
compared to Hac , there may not be a feasible solution.
Observation 2: From Fig. 21, it can be seen that increase in
Hac results in decrease in ro and increase in mn for both
the strategies 1 and 2. This is also somewhat intuitive, as the
height of the air corridor increases, the radius of the circular
orbit of the ANPs has to decrease (please see discussion in
Section VII) and the total number of ANPs needed to cover
the entire air corridor must increase.
Observation 3: From Figs. 20 and 21, it can be seen that, n, the
number of ANPs in an orbit remains a constant irrespective of
changes in Lac , Wac , Hac and rs . This result is not at all obvious. However, on closer examination of the objective function
in equations (20) and (22), one can find an explanation for
this phenomenon. The plot of n/cos2 ( nπ ) versus n is shown
in Fig. 22(a). This factor is present in the objective function
for both the strategies. From Fig. 22(a), n/cos2 ( nπ ) reaches its

14

minimum value when n = 5. Therefore the objective functions
in equations (20) and (22) are minimized when n = 5. This
nature of n also explains the fact in observation 1, where
mn decreases when m decreases (i.e., when ro increases).
Similarly, it also explains the fact in observation 2, where mn
increases when m increases (i.e., when ro decreases).
Observation 4: From the Figs. 20(a), 20(b), 21(a), 21(b), it can
be seen that the cost of the solution (i.e., the number of ANPs
needed to provide complete coverage of the air corridor) using
strategy 1 is less than that of strategy 2. Although, the reason
for this phenomenon may not be obvious at a first glance, on
closer examination, we can explain the phenomenon. Given the
2
fact that L2ac + Wac
≥ 2Lac Wac and presence of these two
terms in objective functions of strategies 1 and 2 (equations
(20) and (22) in page 8), it is not surprising that cost of the
solution strategy 1 is less than that of strategy 2.
From our experiments we learn that (i)strategy 1 performs
better than strategy 2 in all cases, except where Lac = Wac ,
for which both the strategies are identical, (ii)the number of
ANPs in an orbit remains a constant (5) irrespective of the
values of Lac , Wac , Hac and rs , when the objective function
is specified by equations (20) or (22), and (iii)to optimize the
objective function, the radius of the circular orbit of the ANPs
should be made as large as possible subject to the constraint
that the height of the corresponding invariant coverage cylinder
is at least as large as the height of the air corridor Hac .
XI. C ONCLUSION
Existence of sufficient control over the movement pattern
of the mobile platforms in Airborne Networks opens the
avenue for designing topologically stable hybrid networks. In
this paper, we discussed the system model and architecture
for Airborne Networks (AN). We studied the problem of
maintaining the connectivity in the underlying dynamic graphs
of airborne networks with control over the mobility parameters
and developed an algorithm to solve the problem.
R EFERENCES
[1] J. L. Burbank, P. H. Chimento, B. K. Haberman, and W. T. Kasch, “Key
Challenges of Military Tactical Networking and the Elusive Promise

[2]
[3]
[4]
[5]
[6]
[7]

[8]
[9]
[10]
[11]

[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]

of MANET Technology,” IEEE Communication Magazine, November
2006.
M. Conti and S. Giardano, “Multihop ad-hoc Networking: the Reality,”
IEEE Communications Magazine, April 2007.
S. M. Alam and Z. J. Haas, “Coverage and Connectivity in ThreeDimensional Underwater Sensor Networks,” Wireless Communications
and Mobile Computing, vol. 8, no. 995-1009, 2008.
——, “Coverage and Connectivity in Three-Dimensional Networks,” in
International Conference on Mobile Computing and Networking, 2006.
H. Tezcan, E. Cayirci, and V. Coskun, “A Distributed Scheme for 3D
space Coverage in Tactical Underwater Sensor Networks,” in MILCOM,
2004.
S. Poduri, S. Pattem, B. Krishnamachari, and G. S. Sukhatme, “Sensor
Network Configuration and the Curse of Dimensionality,” in EmNets,
2006.
F. Chen, P. Jiang, and A. Xue, “An Algorithm of Coverage Control for
Wireless Sensor Networks in 3D Underwater Surveillance Systems,”
Advanced Intelligent Computing Theories and Applications, vol. 5226,
no. 1206-1213, 2008.
R. Lei, L. Wenyu, and G. Peng, “A Coverage Algorithm for ThreeDimensional Large-Scale Sensor Network,” in Intelligent Signal Processing and Communication Systems, 2007.
C. Huang, Y. Tseng, and L. Lo, “The Coverage Problem in ThreeDimensional Wireless Sensor Network,” in GLOBECOM 2004, 2004.
“City-wide
Wi-fi
Projects.”
[Online].
Available:
http://www.seattlewireless.net,http://www.wirelessphiladelphia.org,
http://www.waztempe.com
A. Tiwari, A. Ganguli, and A. Sampath, “Towards a Mission Planning
Toolbox for Airborne Networks: Optimizing Ground Coverage Under
Connectivity Constraints,” in IEEE Aerospace Conference, March 2008,
pp. 1–9.
B. Epstein and V. Mehta, “Free Space Optical Communications Routing
Performance in Highly Dynamic Airspace Environments,” in IEEE
Aerospace Conference Proceedings, 2004.
“Doppler
Effect.”
[Online].
Available:
http://en.wikipedia.org/wiki/Doppler effect
M. Mesbahi, “On State-dependent Dynamic Graphs and their Controllability Properties,” IEEE Transactions on Automatic control, vol. 50,
no. 3, pp. 2473–2478, 2005.
——, “Controlling Connectivity of Dynamic Graphs,” in 44th IEEE
Conference on Decision and Control, December 2005.
M. M. Zavlanos and G. J. Papas, “Potential Fields for Maintaining
Connectivity of Mobile Networks,” IEEE Transactions on Robotics,
vol. 23, no. 4, August 2007.
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction
to Algorithms. McGraw Hill, 2001.
R. Diestel, Graph Theory. Springer, 2005.
“Nimbus: Interactive multi-objective optimization system.” [Online].
Available: http://wwwnimbus.it.jyu.fi/N4/index.html

On Sparse Placement of Regenerator Nodes in
Translucent Optical Networks†
Arunabha Sen and Sudheendra Murthy

Subir Bandyopadhyay

Department of Computer Science and Engineering
Arizona State University
Tempe, Arizona 85281
Email: {asen, sudhi}@asu.edu

School of Computing Science
University of Windsor
Windsor, Ontario N9B 3P4, Canada
Email: subir@uwindsor.ca

Abstract—Since the optical reach (the distance an optical signal
can travel before its quality degrades to a level that necessitates
regeneration) ranges from 500 to 2000 miles, regeneration of
optical signals is essential to establish lightpaths of lengths greater
than the optical reach. In a translucent optical network, the optical
signal is regenerated at selected nodes of the network before
the signal quality degrades below a threshold. Given the optical
reach of the signal, to minimize the overall network design cost,
the goal of the regenerator placement problem is to find the
minimum number of regenerators necessary in the network, so
that every pair of nodes is able to establish a lightpath (either
transparent or translucent) between them. In this paper, we study
the regenerator placement problem and prove that the problem is
NP-complete. We formulate the regenerator placement problem
as a Connected Dominating Set problem in a Labeled Graph
(LCDS) and provide a procedure for computing it. We evaluate
the effectiveness of our approach using a number of networks.

I. I NTRODUCTION
In all-optical networks, optical-bypass is used to carry the
traffic from a source s to a destination t entirely in the
optical domain so that no Optical-Electrical-Optical (O/E/O)
conversion is needed at any intermediate node in the path
from s to t. All-optical networks are also referred to as
transparent networks in contrast to opaque networks that use
all-electronic switching techniques [1]–[3]. As the opticalbypass technology reduces both capital and operating costs,
it is increasingly deployed in backbone networks. In a widearea backbone network, spanning a large geographical area,
all end-to-end connections, using current technology, cannot
be established entirely in the optical domain since factors
such as optical noise, chromatic dispersion, nonlinear effects,
polarization mode dispersion (PMD) and cross-talk degrade
optical signal quality as it propagates through a fiber network
[1], [2]. The notion of translucent networks, introduced by
Ramamurthy et al. in [3], [4], have features of both transparent
and opaque networks. In a translucent network, the optical
signal is regenerated at the regeneration points (typically a
subset of the network nodes with the regeneration capability)
to carry the signal over long distances. Optical reach (the
distance an optical signal can travel before its quality degrades
† This material is based upon work supported by, or in part by, the U.
S. Army Research Laboratory and the U. S. Army Research Office under
contract/grant number W911NF-06-1-0354.

to a level that necessitates regeneration) usually ranges from
500 to 2000 miles [1]. To transmit an optical signal beyond
this distance, it is essential to re-amplify, reshape and re-time
(a process often called 3R regeneration) it [2].
An example of a long haul network with distances between
the nodes in miles is shown in Figure 1. If the optical reach is
2000 miles, it is clear that an optical signal from node A cannot
reach node D without regeneration. However, communication
between A and D can be established by placing a regenerator
either at B or C. The Regenerator Placement Problem (RPP)
problem to find i) the minimum number of regenerators and
ii) their locations, so that a communication path can be
established between every pair of source-destination nodes in
the network. For this network 2 regenerators are needed for the
RPP problem and one solution is to place these regenerators
at nodes C and E. When designing a translucent network, the
RPP problem must be solved before tackling the problem of
lightpath allocation (either static or dynamic).

Fig. 1. Long haul optical network with distances between the nodes in miles

The RPP has been studied by a number of researchers [1]–
[3]. However, the problem remains important and efficient
solutions are still in demand. This is underscored by a U.S.
Patent award to Telcordia Technologies Inc. as recently as
October 2007 [5]. An associated problem is the Routing with
Regenerators Problem (RRP). In this problem, the locations
of the regenerators are known and the objective is to compute
a path between a source-destination node pair using as few
regenerators as possible. The RRP problems has also been
studied quite extensively [4]. In many cases, researchers have
tried to solve both RPP and RRP simultaneously. However,
as noted in [5], most of the published methods of locating
O/E/O capability operate by iteratively improving previously
computed routes until they become feasible. These methods

978-1-4244-2324-8/08/$25.00 © 2008 IEEE.

1

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2008 proceedings.

usually generate a path between each pair of nodes and then
place O/E/Os, as needed, along those paths to make them
feasible. However, this approach usually results in placing
a significantly higher number of O/E/Os than are needed to
ensure that a path can be established between every sourcedestination node pair. The connected dominating set (CDS)
approach used by the authors in [5] for their U.S. Patent
overcomes the shortcomings of the routing-based approach to
the RPP.
As noted earlier, the degradation of the optical signal
as it travels through the network depends on a number of
factors. Actual regeneration depends heavily on the underlying
technology of the particular system and factors such as the
spacing between regenerators, the loss between regenerators
and the type of fiber used. However as noted in [1], the details
of the fiber layout are generally not provided at the initial
stages of design and distance-based regeneration is often used
as a rough approximation. In the rest of the paper, we consider
distance-based regeneration only.
In this paper we first show that the approach in [5] may
lead to invalid path computation and present a new technique
to compute valid paths. In this paper we
1. show that the RPP can be formulated as the minimum
Labeled Connected Dominating Set (LCDS) problem.
2. prove that the RPP problem is NP-complete.
3. provide a novel procedure for computation of LCDS.
4. evaluate the performance of our solution technique on
five representative graphs.
II. P ROBLEM F ORMULATION
Let Gn = (Vn , En ) be a connected edge-weighted network
graph with edge-weights representing distances between the
corresponding nodes. Given a source node s, a destination
node t and a subset Vn ⊆ Vn , a subpath P S of a path from
s to t (henceforth called a s − t path) is referred to as a
path segment, if the end-points of P S are in Vn ∪ {s, t}
and no intermediate node is in Vn . As a first step towards
formally defining the regenerator placement problem, we view
the problem as follows:
Unrestricted Regenerator Placement Problem (URPP):
Given Gn = (Vn , En ) and an optical reach distance R, the
problem is to find the smallest Vn ⊆ Vn such that there exists
a path between every pair of nodes s, t ∈ Vn with no path
segment of the s − t path having a length more than R.
Need for edge-disjointness among path segments
Although at first glance the URPP appears to accurately
capture the problem at hand, a closer inspection reveals that
it fails to capture a key aspect of the problem in the optical
domain. For example, let the optical reach be 3250 miles in
the network graph Gn = (Vn , En ) shown in Figure 2. The
smallest subset Vn that solves the URPP contains only node
D. In this case, we can establish a path A → B → C →
D → F → G → B → C → H from node A to H, with the
optical signal regenerated at node D. Two path segments that
make up the path from A to H (A → B → C → D) and

(D → F → G → B → C → H) will not violate the optical
reach constraint. However, this may not be an acceptable
solution when we consider free wavelengths available on each
of the fiber links. Let λ1 be the only wavelength available on
the fiber links A → B and C → H and let wavelengths
{λ1 , λ2 , . . .} be available on the remaining links. In this
scenario both the path segments (A → B → C → D) and
(D → F → G → B → C → H) must use wavelength
λ1 to set up the lightpath. This will not be an acceptable
solution since the link B → C appears in both path segments.
The solution technique presented in [5] essentially solves the
URPP. However, as shown in this example, a solution to the
URPP may include overlapping path segments, leading to
invalid paths. To address this problem, we modify the URPP
to the RPP described below.
H
2000
A

750

B

1000

C

100
G

1500

D

3000

E

100
50

F

Fig. 2. Example illustrates need for edge-disjointness among path segments

Regenerator Placement Problem (RPP): Given Gn =
(Vn , En ), the problem is to find the smallest Vn ⊆ Vn such
that there exists a path between every pair of nodes {s, t} ∈ Vn
where (i) no path segment of the s − t path has a length more
than R and (ii) the path segments of the s−t path are mutually
edge-disjoint.
III. D EFINITIONS AND N OTATIONS

1) Labeled
Graph: Given an alphabet set
of symbols,

= {s1 , s2 , . . . , sk }, a graph G = (V, E) is called a
labeled graph if each edge e ∈ E 
has a label l(e) (a
string formed with
the
alphabet
set
) associated
with
∗
∗
for all e ∈ E, where
represents
it, i.e., l(e) ∈

the set of all strings over the alphabet .
= (Vn , En ), (En
=
The network graph Gn
{e1 , . . . , em }), becomes a labeled graph if we assign
a label
 l(ei ) to each edge ei . For instance, we may
use
= {σ1 , . . . , σm }, where σ1 , . . . , σm are distinct
symbols, and assign l(ei ) = σi , ∀ei , 1 ≤ i ≤ m.
2) Reachability Graph: Given a network graph Gn =
(Vn , En ) with edge weights representing the distances
between the nodes, and an optical reach distance R,
the Reachability Graph Gr = (Vr , Er ) corresponding
to Gn will be constructed as follows: Vr = Vn and
corresponding to every path of length at most R between
two nodes vi and vj in Gn , there will be an edge
between vi and vj in Gr . Since there can be multiple
paths of lengths at most R between the nodes vi and
vj in Gn , multiple edges may exist between vi and vj
in Gr (in other words Gr , in general, is a multi-graph).
Moreover, if an edge between the nodes vi and vj in Gr

978-1-4244-2324-8/08/$25.00 © 2008 IEEE.

2

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2008 proceedings.

e

3)

4)

5)

6)

7)

8)

9)

10)

11)

e

e

1
2
3
corresponds to the path Pi,j = vi,j,1 →
vi,j,2 →
vi,j,3 →
ek−1
. . . →vi,j,k−1 → vi,j,k in Gn , then the edge between vi
and vj in Gr , corresponding to path Pi,j , will be assigned
) = σ1 · σ2 · · · σk−1 .
a label l(e1 ) · l(e2 ) · · · l(ek−1
String Intersection operator : If l(ei ) and l(ej ) represent two strings then l(ei ) l(ej ) is the set of symbols
that appear in both the strings l(ei ) and l(ej ).
e1
e2
e3
vi,j,2 →
vi,j,3 →
Valid Path: A path Pi,j = vi,j,1 →
ek−1
. . . →vi,j,k−1 → vi,j,k in a labeled graph G = (V, E),
with labels l(e1 ), l(e2 ), . . . , l(ek−1 ) associated with the
edges e1 , e2 , . . . , ek−1 respectively, will be referred to
as a valid
 path if, for all edge pairs ea and eb in Pi,j ,
l(ea ) l(eb ) = ∅.
=
Interior Node Set INS(Pi,j ): If Pi,j
vi,j,1 →vi,j,2 → . . . →vi,j,k−1 →vi,j,k is a path in a
graph G, then the set of nodes in the path Pi,j
except the source and the destination is referred to
as Interior Node Set and is defined as INS(Pi,j ) =
{vi,j,2 , vi,j,3 , . . . , vi,j,k−1 }.
Nodeset Constrained Path: Given a graph G = (V, E),
a set of nodes U (U ⊆ V ) and a source-destination
e1
e2
vi,j,2 →
node pair (vi , vj ), a path Pi,j = vi,j,1 →
e
e3
k−1
vi,j,3 →
. . . →vi,j,k−1 → vi,j,k in G, is referred to
as a Nodeset (U ) Constrained (vi , vj ) Path and denoted
by NCP(U, vi , vj ), if INS(Pi,j ) ⊆ U .
Connected Dominating Set (CDS): Given a graph G =
(V, E), V  ⊆ V is called a Connected Dominating Set,
if (i) V  is a dominating set, i.e., if every node in the set
V − V  is adjacent to at least one node in V  and (ii) the
the subgraph induced by V  is connected [6].
Labeled Connected Dominating Set (LCDS): Given a
labeled graph G = (V, E), V  ⊆ V is called a Labeled
Connected Dominating Set in G, if (i) V  is a dominating
set of V and (ii) there exists a valid NCP(V  , vi , vj ) for
every pair of nodes vi , vj ∈ V .
ek−1
e1
e2
Path Label: If Pi,j = vi,j,1 →
vi,j,2 →
. . . →vi,j,k−1 →
vi,j,k is a path in a labeled graph G and l(ei ) is
the label associated with edge ei , 1 ≤ i ≤ k − 1,
the label associated with the path L(Pi,j ) is given by
l(e1 ) · l(e2 ) · · · · l(ek−1 ), (i.e., the label of Pi,j is obtained
by concatenating the labels of the edges of Pi,j ).
Regular Language Constrained Path: Given a labeled
graph G and a regular language L [7], a path Pi,j in
G is said to be a regular language L constrained path if
L(Pi,j ) ∈ L [8].
SD(X, Y ): The difference between the sets X and Y , i.e.,
SD (X, Y ) = X\Y , where X and Y are two sets of nodes
in the graph G.

IV. C OMPUTATIONAL C OMPLEXITY
First we show that the URPP for the network graph Gn
is equivalent to the Minimum Connected Dominating Set
(MCDS) problem of the corresponding Reachability Graph
Gr . Since MCDS problem is known to be NP-complete [6],
the URPP must also be NP-complete.

Theorem 1. URPP is NP-complete.
Proof: We ignore the edge labels of the reachability
graph Gr since they are not needed here. From the way we
constructed Gr , it is clear that if the edge (vi , vj ) ∈ Er the
node vj is reachable from vi in Gn without any regeneration.
Let the solution to the URPP be the node set U ⊆ Vr . We
make the following two claims with respect to U .
Claim 1.1: The node set U must be a dominating set of Vr .
Claim 1.2: The subgraph induced by U in Gr must be
connected.
Proof of Claim 1.1: Let the claim be false so that the node
set U is not a dominating set in the reachability graph Gr =
(Vr , Er ). This implies that there exists at least one node vi in
Vr \ U that is not adjacent to any node in U . Consider a node
vj ∈ U . Since vi is not adjacent to any node in U , in order to
reach vj from vi , the path has to visit at least one other node
vk in Vr \ U . Since there is no edge in Gr between vi and vj ,
it implies that the length of the shortest path between these
two nodes in Gn is greater that the optical reach R. Although
there is a path from vi to vj through vk , the length of this path
is greater than the optical reach R. Therefore the optical signal
from vi will not be able to reach vj without regeneration and
hence, the node set U cannot be a solution to the URPP.
Proof of Claim 1.2: Let the claim be false so that the graph
induced by the node set U in Gr is not connected. It implies
that there exists a pair of nodes (vi , vj ) in U such that there is
no path from vi to vj using the nodes of U alone. However,
there may be a path vi → · · · → vk−1 → vk → vk+1 →
· · · → vj , where all nodes except vk are in U . Since this path
is using the subpath vk−1 → vk → vk+1 and there is no edge
between vk−1 and vk+1 in Gr , it implies that the length of
the shortest path between these two nodes in Gn is greater
that the optical reach R. Accordingly, vi → · · · → vk−1 →
vk → vk+1 → · · · → vj , path cannot be used to transmit an
optical signal from vi to vj because the length of the subpath
vk−1 → vk → vk+1 is longer than the optical reach R.
The necessity of the node set U to be a MCDS of Gr follows
from Claims 1 and 2. Sufficiency of node set U to be a MCDS
of Gr can easily be verified.
The example in figure 2 demonstrated the necessity of making
the path segments of a path edge disjoint. However, overlapping path segments do not pose any problem in path computation if they use different wavelengths. Therefore, if we make a
physically unrealistic but mathematically plausible assumption
that an infinite (or very large) number of wavelengths are
available on each fiber link, then it is no longer necessary
to make sure that path segments are edge disjoint. This is
true because in this scenario one can always assign a unique
wavelength to each path segment. The RPP problem reduces
to the URPP problem if the edge disjointness requirement is
removed. Thus the RPP reduces to the URPP when an infinite
number wavelengths are available on each optical link. Since
the URPP is NP-complete and the RPP reduces to the URPP
when an infinite number wavelengths are available on each
optical link, we claim that RPP must also be NP-complete.

978-1-4244-2324-8/08/$25.00 © 2008 IEEE.

3

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2008 proceedings.

Theorem 2. Except in the case where the optical reach
distance R is so large that no regeneration is necessary, the
solution to the RPP can be obtained by computing the smallest
LCDS of the reachability graph Gr = (Vr , Er ) corresponding
to the network graph Gn = (Vn , En ) with optical reach
distance R.
Proof: The proof of this theorem is similar to the proof
of theorem 1. However, in this case, we no longer ignore the
labels associated with the edges of Gr . Let the solution to the
RPP be the node set U ⊆ Vr .
Claim 2.1: The node set U must be a dominating set of Vr .
Claim 2.2: There exists at least one valid NCP(U, vi , vj ) path
for every pair of nodes vi , vj ∈ Vr .
Proofs of Claims 2.1, 2.2: The proof of claim 2.1 is identical
to the proof of claim 1.1 and the proof of 2.2 is similar to
the proof of claim 1.2, except that in the proof of claim 2.2,
the labels associated with the paths have to be considered to
determine if it is a valid NCP(U, vi , vj ) path. We omit the
details due to space limitations.
V. C OMPUTATION OF L ABELED C ONNECTED D OMINATING
S ET
We first provide a high level description of the procedure
and then explain in detail how each of these steps will
be executed. For ease of understanding, we demonstrate the
execution of the steps of the procedure with an example.
Procedure 1 Regenerator Location Computation
Input: Network Graph Gn = (Vn , En )
Output: Vn ⊆ Vn , the locations of the regenerators.
1: Compute the Reachability Graph Gr = (Vr , Er ) from the
network graph Gn = (Vn , En ). (Vr = Vn and Er ⊇ En ).
2: Compute a dominating set U in Gr .
3: Check if a valid NCP(U, vi , vj ) exists for every node pair
(vi , vj ) in Gr .
4: If a valid NCP(U, vi , vj ) exists for every node pair
(vi , vj ) in Gr , then set Vn = U , return Vn and Stop.
5: Augment dominating set U with additional nodes from
Vr \ U to form a new dominating set U  such that a valid
NCP(U, vi , vj ) exists for every node pair (vi , vj ) in Gr .
Set Vn = U  , return Vn and Stop.
• Reachablity Graph Computation
There exists a number of algorithms to compute k shortest
paths between a source-destination node pair in a graph [9].
The algorithm presented in [9] can also be utilized to list
all shortest paths between a node pair whose distance is
shorter than a given threshold length. Accordingly, we use the
algorithm in [9] to find all paths between a source-destination
node pair with distances shorter than the optical reach R. If the
number of such paths is very large, to reduce the computational
time, only k shortest paths between the source-destination
node pair with distance shorter than R can be computed, for
some suitable k.
• Dominating Set Computation

It is well known that the dominating set problem (i.e., the
computation of a dominating set of the smallest size) is an NPcomplete problem [6]. Accordingly, we use a greedy heuristic
for the computation of a dominating set.
Procedure 2 Dominating Set Computation
Input: Reachability Graph Gr = (Vr , Er )
Output: A dominating set Vr of Gr = (Vr , Er )
1: Vr , U ← φ
2: while U = Vr do
3:
vm ← highest degree node in Gr = (Vr , Er )
4:
Vr ← Vr∪ {vm }
5:
U ← Vr N eighbor(Vr )
6:
Vr ← V r \ U
7:
Er ← Er \ {(vi , vj ) | either vi ∈ U or vj ∈ U }
8: end while
9: return Vr
where N eighbor(Vr ) is the set of nodes that are adjacent
to the nodes Vr .
• Checking for Valid Nodeset Constrained Paths
From the definitions of Valid Paths and Nodeset Constrained
Paths, it is clear that given a pair of source-destination nodes
(vi , vj ) and a subset Vr of the nodes of the Reachablity Graph
Gr = (Vr , Er ), a valid path between vi and vj does not imply
that it is a N CP (Vr , vi , vj ). Conversely, an N CP (Vr , vi , vj )
does not imply that it is valid. Our technique for checking
Valid Nodeset Constrained Paths N CP (Vr , vi , vj ) proceeds in
two phases. In the first phase we generate all (or k shortest)
valid paths between the nodes vi and vj . In the second
phase we check if any one of the generated valid paths is
N CP (Vr , vi , vj ).
Phase 1: Generation of Valid Paths
Before we describe a technique for computing all (or k
shortest, if computation of all valid paths is time consuming)
valid paths between any source-destination node pair in the
Reachability Graph G = (V  , E  ), we prove the following
theorem.
Theorem 3. The language comprised of labels of valid paths
in the Reachability Graph Gr = (Vr , Er ) is regular.
Proof: If Pi,j is a path in Gr and L(Pi,j ) = σ1 · σ2 · · · σk
is the label (string of symbols) associated it, then Pi,j is a
valid path if and only if the symbols of the string σ1 · σ2 · · · σk
are unique (i.e., not repeated). Each symbol in the alphabet
set Σ = {σ1 , σ2 , . . .} corresponds to an edge of the network
graph Gn = (Vn , En ). Since the size of the edge set En is
finite, so is the size of the alphabet set Σ. Suppose that Lunique
is a language over the alphabet set Σ formed by the strings
with unique symbols. Since the number of such strings is finite
with a fixed alphabet set Σ the language Lunique is finite. If
Lvalid is the language comprised of labels of valid paths in
Gr = (Vr , Er ) then Lvalid ⊆ Lunique . Since Lunique is finite,
Lvalid must also be finite and hence regular [7].

978-1-4244-2324-8/08/$25.00 © 2008 IEEE.

4

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2008 proceedings.

Barret et.al [8] study regular language constrained path problems. Since theorem 3 proves that the language comprised
of labels of valid paths in the Reachablity Graph is regular,
we utilize the algorithm in [8] for generation of valid paths
in the Reachablity Graph. For terminologies such as nondeterministic finite automata (NFA) from formal language
theory, we refer the reader to [7]. The basic idea in finding
all (or k shortest) valid paths in the Reachability Graph
Gr = (Vr , Er ) is the following.
Let Munique = (S1 , Σ, δ1 , p0 , F1 ) represent the NFA
corresponding to the language Lunique . Construct an NFA
MGr = (S2 , Σ, δ2 , q0 , F2 ) corresponding to the Reachability
Graph Gr = (Vr , Er ) as follows.
1) S2 = Vr , q0 = a source node vi ∈ Vr , F2 = a destination
node vj ∈ Vr
2) Σ is the set of all labels that are used to label the edges
in Gn , i.e., Σ = {l(e1 ), . . . , l(em )}
3) vj ∈ δ2 (vi , a) if and only if there is an edge (vi , vj )
in Gr with label a, where a is a string of one or more
symbols from Σ.
It is clear that a string x ∈ Lunique represents a label where
the symbols do not repeat and a string y ∈ LGr (the language
corresponding to MGr ) corresponds to a path in Gr from a
node vi to a node vj . Therefore, Lunique ∩ LG corresponds to
paths in Gr , such that the symbols of the labels associated with
these paths are not repeated. In other words, if z ∈ Lunique ∩
LG , then it corresponds to a valid path between some source
node vi to some destination node vj .
=
Let Munique = (S1 , Σ, δ1 , p0 , F1 ) and MGr
(S2 , Σ, δ2 , q0 , F2 ) be two NFAs. The product NFA is defined
as Munique × MGr = (S1 × S2 , Σ, δ, (p0 , q0 ), F1 × F2 ), where
∀a ∈ Σ, (p2 , q2 ) ∈ δ(p1 , q1 ), a) if and only if p2 ∈ δ1 (p1 , a)
and q2 ∈ δ1 (q1 , a). It can be easily verified that L(Munique ×
MGr ) = L(Munique ) ∩ L(MGr ) = Lunique ∩ LGr , where
L(M ) represents the language recognized by the NFA M .
Procedure 3 Generation of k shortest Valid Paths between a
source-destination node pair in the Reachablity Graph
Input: Labeled Reachablity Graph Gr = (Vr , Er ), source
node vi and destination node vj .
Output: k shortest Valid Paths between vi and destination
node vj .
1: Construct an NFA Munique = (S1 , Σ, δ1 , p0 , F1 ) corresponding to Lunique .
2: Construct an NFA MGr = (S2 , Σ, δ2 , q0 , F2 ) corresponding to the reachability graph Gr = (Vr , Er ).
3: Construct an NFA Munique × MG .
4: Generate k shortest paths from (p0 , vi ) to (f, vj ) where
f ∈ F1 in the graph Munique × MG using the algorithm
in [9]. Length of a path in this case can be measured in
terms of the number of hops.
The reader is referred to [8] for correctness of procedure 3.
Phase 2: Checking for Nodeset Constraint (N CP (Vr , vi , vj ))
Once we have the valid paths between vi and vj we can easily

check if they are they are nodeset Vr constrained. In order for
a valid path between vi and vj to be nodeset Vr constrained,
all nodes in that path except vi and vj must be in Vr
• Augmentation of Dominating Set
If valid (N CP (Vr , si , ti )) paths exist for all sourcedestination pairs (si , ti ) in G then, regenerators can be placed
in the node set Vr . In this case, the optical reach constraint
is satisfied and the path segments are mutually edge-disjoint.
However, if there exists at least one source-destination node
pair (si , ti ) for which no (N CP (Vr , si , ti )) paths exists, then
the dominating set v  has to be augmented with additional
nodes so that at least one (N CP (Vr , si , ti )) path exist between
every (si , ti ) pair. Suppose a set of paths Pi , connecting nodes
si to ti , (1 ≤ i ≤ k), are violating the nodeset (Vr ) constraint,
i.e., in each of these paths there exists at least one node that
is not in the node set Vr .
There may be li different valid Pi paths (connecting nodes
si to ti , 1 ≤ i ≤ k) such that none of them satisfy
(N CP (Vr , si , ti )) requirement. We denote these paths as
(Pi,1 , Pi,2 , . . . , Pi,li ) and write them as follows:
Pi,1 : si → vi,1,1 → vi,1,2 → . . . → vi,1,qi,1 → ti
Pi,2 : si → vi,2,1 → vi,2,2 → . . . → vi,2,qi,2 → ti
...
Pi,li : si → vi,li ,1 → vi,li ,2 → . . . → vi,li ,qi,li → ti
In order to make a path Pi,j , 1 ≤ j ≤ li from the above set
of paths satisfy (N CP (Vr , si , ti )) requirement, we need to add
(SD(INS(Pi,j ), Vr )) nodes to the set Vr . In order to make all
Pi s, 1 ≤ i ≤ k, satisfy (N CP (Vr , si , ti )) we need to add the
set of nodes corresponding to the smallest minterm obtained
after simplification of the following boolean expression to
current Dominating set Vr .
((SD(INS(P1,1 ), Vr ))
OR
(SD(INS(P1,2 ), Vr ))
OR
···
OR
(SD(INS(P1,l1 ), Vr ))) AND (SD(INS(P2,1 ), Vr )) OR (SD(INS(P2,2 ), Vr ))


OR · · · OR (SD(INS(P2,l2 ), Vr ))) AND · · · AND (SD(INS(Pk,1 ), Vr )) OR
(SD(INS(Pk,2 ), Vr )) OR · · · OR (SD(INS(Pk,lk ), Vr )))

The simplification can be performed in polynomial time of
the number of valid paths, which depends on the value of k
chosen in Procedure 3.
VI. I LLUSTRATIVE E XAMPLE
In this section, we demonstrate the execution of procedure
1 on the example network graph Gn = (Vn , En ) of figure 1.
Step 1 of the algorithm computes the labeled Reachability
Graph. For an optical reach of 2000 miles, the computed
labeled Reachability Graph Gr = (Vr , Er ) is shown in Figure
3. In Gr , solid edges indicate links that are part of the network
graph Gn and dashed edges represent paths of at least two hops
in Gn whose length is at most 2000 miles. Each edge (solid
or dashed) in Gr has two attributes – i) a number denoting
the length of the corresponding path and ii) edge (s) in Gn
specifying the corresponding path in the network graph Gn .
Step 2 of the algorithm computes a dominating set in G =
(V  , E  ) using Procedure 2. The dominating set chosen by this
algorithm for this example is the node set Vr = {C, F }.
Step 3 of the algorithm checks if a valid NCP(Vr , vi , vj ) for
every node pair (vi , vj ) in Gr . In order to do this, first it gen-

978-1-4244-2324-8/08/$25.00 © 2008 IEEE.

5

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2008 proceedings.

Fig. 3.

Reachability graph of the network graph of figure 1

erates all (or k shortest) valid paths between every node pair
(vi , vj ) in Gr using Procedure 3. Among 21 possible sourcedestination node pairs in Gn , 17 have a valid NCP(Vr , vi , vj )
path with Vr = {C, F }. These paths are shown in Table I.
The remaining 4 paths (A-F, B-F, C-F, F-G) do not have a
single valid NCP(Vr , vi , vj ) path between them. Table II gives
a subset of the valid paths between B and F. These paths are
obtained as output of Procedure 3. It can be verified easily
that none of these paths are NCP(Vr , vi , vj ) constrained. For
space limitations, we omit results for A-F, C-F and F-G paths.
Since Step 3 concludes that there exists some (vi , vj )
pairs for which no NCP(Vr , vi , vj ) paths exist, Step 5 of the
algorithm is executed to augment the current dominating set
Vr = {C, F }. The boolean expression of the previous section
is evaluated, which for this example is as follows.
(D ⊕ DEB ⊕ E ⊕ DEGB ⊕ EB ⊕ EGB ⊕ DE ⊕ . . . ⊕ DG ⊕ DGB) 
(D ⊕ EA ⊕ E ⊕ DE ⊕ EGA ⊕ . . . ⊕ EG ⊕ DG ⊕ DA ⊕ DEGA) 
(D ⊕ DEA ⊕ E ⊕ DEGA ⊕ DEGAB ⊕ EA ⊕ . . . ⊕ DAB ⊕ DEAB) 
(DEA ⊕ D ⊕ DEB ⊕ E ⊕ EAB ⊕ EA ⊕ EB ⊕ . . . ⊕ DAB ⊕ DEAB)

where the symbols 
 and ⊕ represents AND and OR operations respectively. On simplification, the boolean expression
reduces to D ⊕ E. This implies that augmenting the current
dominating set Vr = {C, F } with either the node D or E we
will get Vr  = {C, F, D} or Vr  = {C, F, E} as the solution
to the regenerator placement problem. As mentioned in Section
I, the optimal number of regenerators for this problem is two,
with the regenerators placed at C and E.

network of Figure 1, NJLATA (11 nodes, 23 links), COSTsmall (11, 24), ARPANET (20, 32) and NATIONAL (24, 44)
[10]. The optical reach was set to be 2000 miles. We compared
the execution time and the number of regenerators produced by
the heuristic with the optimal solution. The optimal solution
was obtained by solving the Integer Linear Program (ILP)1
using CPLEX-10 optimizer. The numbers on the bars indicate
the execution time in seconds. The results for larger networks
are not provided here since the ILP failed to produce the
optimal solution in meaningful amount of time. It is evident
from the results of figure 4 that the heuristics produce nearoptimal solution in a fraction of time needed to find the optimal
when the number of nodes exceeds 10.

Fig. 4.
Number of regenerators produced by optimal and the proposed
heuristics for different networks.

VIII. C ONCLUSION
In this paper, we formulated the regenerator placement
problem in translucent optical networks as the problem of
computing minimum connected dominating set in labeled
graphs (LCDS). In contrast with the existing literature, the
LCDS problem takes into account edge-disjointness among
path segments. We provide NP-completeness results and a
novel procedure for computing LCDS. The simulation results
show that the proposed heuristic produces near-optimal solution in a fraction of time needed to find the optimal solution.
R EFERENCES

TABLE I
N ODE PAIRS HAVING NCP(Vr , vi , vj ) CONSTRAINED PATHS
A-B ⇒ e1
A-G ⇒ e1 , e6
B-G ⇒ e6
D-E ⇒ e9 , e10
E-G ⇒ e2 , e3

A-C ⇒ e2 , e8
B-C ⇒ e4
C-D ⇒ e4 , e5
D-E ⇒ e9 , e10

A-D ⇒ e1 , e5
B-D ⇒ e5
C-E ⇒ e8
D-G ⇒ e5 , e6

A-E ⇒ e2
B-E ⇒ e4 , e8
C-G ⇒ e4 , e6
E-F ⇒ e10

TABLE II
VALID PATHS BETWEEN B AND F OF THE (F IG . 3) R EACHABILITY G RAPH
Path

Path Labels

U = INS(PB,F,∗ )

PB,F,1
PB,F,2
PB,F,3
PB,F,4
...
PB,F,24
PB,F,25

σ5 , σ 9
σ4 .σ7 , σ9
σ4 , σ 7 , σ 9
σ1 , σ2 , σ10
...
σ5 , σ7 , σ4 .σ6 , σ3 , σ2 , σ10
σ6 , σ 3 , σ 2 , σ 8 , σ 7 , σ 9

D
D
CD
AE

SD(U, Vr )
D
D
D
EA

...

...

DCGAE
GAECD

DEGA
DEGA

VII. S IMULATION R ESULTS
We conducted extensive experimentation to study the efficacy of our heuristics using 5 different realistic networks -

[1] J. Simmons, “Network design in realistic all-optical backbone networks,”
IEEE Comm. Magazine, vol. 44, 2006.
[2] G. Shen and R. S. Tucker, “Translucent optical networks: the way
forward,” IEEE Comm. Magazine, vol. 45, 2007.
[3] X. Yang and B. Ramamurthy, “Sparse regeneration in translucent
wavelength routed optical networks: architecture, network design and
wavelength routing,” Photonic Network Comm., vol. 10, 2005.
[4] ——, “Dynamic routing in translucent wdm optical networks: the
interdomanin case,” Jour. of Lightwave Tech., vol. 23, March 2005.
[5] T. Carpenter, D. Shallcross, J. Gannett, J. Jackel, and A. Lehmen,
“Method and system for design and routing in transparent optical
networks,” U.S. Patent 7,286,480 B2, Oct., 2007.
[6] M. Garey and D. Johnson, Computers and Intractability: A Guide to the
Theory of NP-Completeness. Freeman Press, 1979.
[7] J. Hopcroft, R. Motwani, and J. Ullman, Introduction to Automata
Theory, Languages, and Computation. Addison Wesley, 2000.
[8] C. Barrett, R. Jacob, and M. Marathe, “Formal-language-constrained
path problems,” SIAM J. Comput., vol. 30, no. 3, 2000.
[9] D. Eppstein, “Finding the k shortest paths,” Tech. Rep., 1994. [Online].
Available: http://www.ics.uci.edu/ eppstein/pubs/Epp-TR-94-26.pdf
[10] S. Kim, X. Zhang, and S. Lumetta, “Rapid and efficient protection for
wdm mesh networks,” IEEE Jour. of Selected Areas in Comm., 2007.
1 ILP

formulation not provided due to space limitations

978-1-4244-2324-8/08/$25.00 © 2008 IEEE.

6

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2008 proceedings.

Proceedings of the Third International ICWSM Conference (2009)

A Social Identity Approach to Identify Familiar Strangers in a Social Network
Nitin Agarwal, Huan Liu, Sudheendra Murthy, Arunabha Sen, and Xufei Wang
School of Computing and Informatics
Arizona State University
{Nitin.Agarwal.2, Huan.Liu, sudhi, asen, Xufei.Wang}@asu.edu

distribution it is quite likely that they may not know each
other. Aggregating such familiar strangers could form a critical mass such that (1) the understanding of one member
gives us a sensible and representative glimpse to others, (2)
more data about familiar members can be collected for better
customization and services (e.g., personalization and recommendation), (3) the nuances among them suggest new business opportunities, and (4) knowledge about them can facilitate predictive modeling and trend analysis in new product/market development. Connecting them to form a critical mass can potentially expand their social network, i.e.,
job searching, special interest group formation. Aggregating familiar strangers can encourage participation due to the
crowd effect (Kumar et al. 2004). People usually trust those
with similar interests. Knowledge transfer or information
ﬂow among friends and acquaintances becomes smoother
and more receptive.
Identifying familiar strangers in online social networks is
interesting and involves several key challenges. Individuals
have only local view, i.e., individuals know their contacts but
may not know their contacts’ contacts and so on. Searching for all the contacts of a node, his contacts’ contacts and
so on, to identify familiar strangers incurs an exponential
cost. Each individual is associated with some content or
attributes. The challenge lies in intelligently putting that
information to the beneﬁt of searching familiar strangers.
Evaluation and validation of the proposed approaches is a
big issue due to the absence of an established ground truth.

Abstract
We present a novel problem of searching for ‘familiar
strangers’ in a social network. Familiar strangers are individuals who are not directly connected but exhibit some similarity. The power-law nature of social networks determines that
majority of individuals are directly connected with a small
number of fellow individuals, and similar individuals can be
largely unknown to each other. Moreover, the individuals of
a social network have only a local view of the network, which
makes the problem of aggregating these familiar strangers a
challenge. In this work, we formulate the problem, show why
it is signiﬁcant to address the challenge, and present an approach that innovatively employs the social identities of the
individuals with competitive approaches. A blogger and citation network are used to showcase technical details and empirical results with related issues and future work.

Introduction
Familiar strangers as deﬁned by Stanley Milgram (Milgram
1972) in physical world are those individuals who do not
know each other but share some common attributes like interests, occupation, location etc. For instance, people taking
the same train daily ﬁnd familiar faces but do not know each
other. Analogous to physical world, it is equally interesting and challenging to deﬁne and study the existence of familiar strangers in virtual or online world. Social networks
represent a complex set of human relations through interactions expressed via a spectrum of social media websites
like blogs, online friendship networks, wikis, media sharing
websites, social tagging websites etc. In an online world, familiar strangers could be deﬁned as those individuals who
are not friends with each other, i.e., they are not in each
other’s social network, but they share some common set of
attributes like hobbies, community afﬁliations, workplace,
location, etc. A more formal deﬁnition is given later.
Identifying familiar strangers has profound applications
in online social networks. Since the online social networks
are shown to have long tail distribution, i.e., most of the
members have very few contacts and very few members
have a large number of contacts, which means that most of
these members do not know each other. Although many of
them could have a lot in common but due to the long tail

Problem Formulation
Here we deﬁne familiar strangers and formulate the problem
of searching them using local information. Given a social
network G where V is the set of vertices (nodes) or the members of the social network. The nodes are associated with an
attribute. The attribute can take one or more values from a
domain D = {a1 , a2 , ..., al }. We call this the attribute-value
set of a node and is denoted by Au for a node u (u ∈ V ).
Each node u has a local view of the network (also known
as an egocentric view (Wasserman and Faust 1994)), that
means the node only knows its adjacent nodes denoted by
Cu = {m1 , m2 , ..., my | edge(u, mp ) = 0, 1 ≤ p ≤ y},
also known as u’s contacts. Here edge(c, d) = 0 denotes
an edge between nodes c and d. This is similar to a scenario where one knows his/her friends but doesn’t know

c 2009, Association for the Advancement of Artiﬁcial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

2

v2

A: Interest
D: {Academic, Arts, Blogging, Business, Computers, Exercise, History, Internet,
Music, News, Personal, Political, Recreation, Technology, Travel}
u: Seed Blogger
Cu: {v1,v2,v3,v4}
Au: {Exercise, History, Recreation}
Av1: {Internet, News}
Local knowledge
Av2: {Blogging, Internet}
that u has of the
network
Av3: {Blogging, Internet, Technology}
Av4: {Recreation, Travel}

v4

v3
u

v1

Find Tu: familiar strangers of u given goal : Sports = {Exercise, Recreation}

Figure 1: Searching familiar strangers for a node u given the local network information that u has and the goal γ.
his/her friends’ friends and so on. In order to deﬁne familiar
strangers of u, it is essential to deﬁne the notion of similarity.

Linear Programming (ILP) formulation to solve the Steiner
tree problem optimally. Given the undirected social network graph G = (V, E), we ﬁrst construct the corresponding directed graph H = (V, F ), in which two directed
edges {(vi , vj ), (vj , vi )} ∈ F for each undirected edge
(vi , vj ) ∈ E. Let the number of required nodes be denoted by n, i.e., |V  | = n and let an arbitrary vertex say,
the node u ∈ V  be designated as the root node. The ILP
views the directed graph H as a ﬂow graph, in which (n−1)
units of ﬂow are routed from the root u towards the nodes in
V  \ {u} through minimum number of edges. Each node
in V  \ {u} consumes exactly one unit of ﬂow. The edges
of graph H through which a positive (unit) ﬂow exists form
the minimum-edge arborescence2 in H spanning the vertices
V  . The undirected edges in graph G corresponding to the
arborescence edges forms the required Steiner tree in G.
Let indicator variables xvi vj = 1, if edge (vi , vj ) belongs
to the required minimum-edge arborescence T in H, otherwise, xvi vj = 0. Let variables fvi vj ≥ 0 represent nonnegative ﬂow on the edges. The variables xvi vj and fvi vj
are deﬁned for all edges (vi , vj ) ∈ F . The objective is to
minimize the number of edges in the arborescence in H,

xvi vj
M inimize

Deﬁnition 1 (Similarity) Nodes u and v are similar iff Av ∩
γ = ∅, where γ is a goal described as γ ⊆ Au .
Deﬁnition 2 (Familiar Strangers) Given u and γ, Tu is
the set of familiar strangers of u iff (1) for all the nodes
v ∈ Tu , edge(u, v) = 0 i.e., all the nodes v are non-adjacent
to u - stranger1 and (2) all the nodes v are similar to u with
respect to γ as deﬁned above - familiar.
The problem of searching for familiar strangers given a
node u can be illustrated in Figure 1 where a blogger social network is presented in the left, snippet of which is presented in the middle. Here the attribute A is “Interest” and
D is the domain for the values of “Interest”. Cu represents
the contacts of u and Au represents the attribute-value set
of u. Av1 , Av2 , Av3 , Av4 represent the attribute-value sets
of v1 ,v2 ,v3 , and v4 respectively. We need to ﬁnd Tu , familiar strangers of u for the goal γ (“Sports”) deﬁned by the
combination of “Exercise” and “Recreation”.
The challenge lies in searching for familiar strangers efﬁciently, i.e., in minimum number of edge traversals with local information. To compute the lower bound on the search
space for ﬁnding the familiar strangers, consider the centralized version of the problem, in which the node u has global
or whole view of the network and the objective is to ﬁnd the
smallest set of edges that will connect all the nodes in Tu
starting at node u. This centralized version of the familiar
strangers problem corresponds to the Steiner tree problem.
Given a subset of nodes V  ⊂ V in a graph G = (V, E), the
Steiner tree (T ) spans the node set V  with least number of
edges. The node set V  is referred to as the required nodes
or terminal nodes and the set of nodes in V \ V  is referred
to as the optional nodes or Steiner vertices. It may be noted
that tree T contains all the nodes in set V  and zero or more
nodes in set V \ V  . The Steiner tree in a social network that
spans the node u and the familiar strangers Tu provides the
least number of edges that need to be traversed to ﬁnd all the
familiar strangers of u and thus provides a lower bound on
the search space of the familiar strangers problem.
The problem of ﬁnding the Steiner tree is known to be
NP-complete (Du and Hu. 2008). We provide an Integer

(vi ,vj )∈F

• There are exactly (n − 1) units of ﬂow emanating out of
the root node u and 0 units of ﬂow going into it. That is,


fuvj = n − 1,
fvj u = 0
(u,vj )∈F

(vj ,u)∈F

• Every other required node, i.e., vi ∈ V  \ {u} consumes
1 unit of ﬂow. That is,


∀vi ∈ V  \ {u},
fvj vi −
fvi vj = 1
(vj ,vi )∈F

(vi ,vj )∈F

• A positive ﬂow exists on an edge, iff the edge is selected
in the arborescence which is ensured by:
∀(vi , vj ) ∈ F, fvi vj ≤ (n − 1)xvi vj
Because solving ILP in general takes exponential time, we
employ a 2-Approximation algorithm based on Minimum
2
An arborescence T of a graph H is a directed, rooted tree subgraph of H in which all edges point away from the root.

1
This deﬁnition of stranger nodes is borrowed from the famous
concept of weak ties (Granovetter 1973).

3

Spanning Tree approach (Du and Hu. 2008) for computing
Steiner trees. The 2-Approximation algorithm produces a
solution that is guaranteed to be within 2 times the optimal
solution in terms of the edge traversals.

stop words and stemming. However, this is a very sparse
and high-dimensional vector. So this sparse vector could be
transformed to concept space vector using latent semantic
analysis (Deerwester et al. 1990). The transformed vector is
less sparse and low dimensional.
Clustering of the contacts could be performed either ofﬂine or online while searching. We perform the clustering
ofﬂine. So the social identity of the nodes of the network
are constructed a priori to speedup the search process. Online clustering takes care of the dynamics of the network,
nevertheless, it increases the response time while searching.
We can also bypass the construction of social identities to
search for familiar strangers. Perhaps this would mean that
the search phase will look at all the contacts of a node to
ﬁnd the most relevant nodes to propagate the search. However, by constructing social identity we cluster the contacts
and pick the relevant cluster, hence pruning the search space
early on. Since clustering is done ofﬂine, which does not
incur clustering overhead costs while searching.

Social Identity Theory
Real-world social networks of people have been shown to
exhibit properties of searchability, which means a target can
be found quickly even in the absence of global network
view (Watts, Dodds, and Newman 2002). Searchability in
social networks has been attributed to the tendency of people to cluster their contacts into meaningful groups based
on different attributes and selecting relevant cluster of contacts to advance the search at each hop which would take the
search closer to the destination. This arrangement of neighbors in groups gives a sense of social identity (Tajfel 1978).
Social identity theory has been widely studied in realworld social networks in terms of observing searchability
property of the network. In this paper, we attempt to utilize
the social identity theory in online social networks to identify familiar strangers, which is the ﬁrst of its kind to the best
of our knowledge. Directly connected neighbors of a node
form the set of its contacts and the attribute-value set of the
nodes are used to construct the social identity. More details
on social identity construction using the attribute-value set
is described in the Social Identity Construction.

Example 1: To illustrate with an example, refer to Figure 1,
where we need to ﬁnd the familiar strangers of the node u
with respect to the goal, γ = {Exercise, Recreation}. We
can either search all his contacts viz., v1 , v2 , v3 , v4 to ﬁnd
the contacts that are similar to the γ. This would result in v4
as the contact whose attribute-values match with the γ. Or
we can cluster the contacts ofﬂine and pick the relevant cluster. Clustering resulted in two clusters one with v1 , v2 , v3
and the other with v4 . Now the second cluster with v4 is
more similar to the γ, so we pick the contacts in this cluster,
which in this case is v4 . The latter strategy greatly prunes the
search space, especially when the nodes have much larger
number of contacts4 and the clustering is performed ofﬂine.

Approaches for Egocentric View
Here we present strategies to ﬁnd familiar strangers Tu of a
node u given goal γ using an egocentric view of the network.

Social Identity Approach
According to social identity theory, people cluster their contacts into meaningful groups and pick the cluster that has
maximum similarity with the goal γ. So we prune some
contacts at each level and propagate the search with the selected cluster of contacts to ensure that the search remains
closer to the speciﬁed γ instead of wandering away.
Social Identity Construction Social identity based
search relies on the ability of a node u to cluster its contacts.
Each node in the network is represented as a vector space
model of its attributes and simple cosine similarity based
measures could be used to compute afﬁnity matrix between
contacts of node u. Then conventional clustering algorithms
like k-means could be used to cluster the contacts of node
u. The clustering approach could be more sophisticated if
more data is available about the nodes of the network besides the attribute-value set. For a blogger social network
dataset along with the blogger network and their attributevalue set3 we also have their blog posts and the metadata
associated with the blogger like tags, categories, and blog
post text. This rich metadata about the bloggers is used to
construct the vector space model for each of the contact s of
a node u in blogger network. Here the terms of the vector
space model are the words in the vocabulary after removing

Social Identity based Search for familiar strangers of a
node u and γ can be summarized in the pseudo code in Algorithm 1. Given a node: u, its contacts: Cu , its contacts’
attribute-value set: BCu , and γ as input, it outputs a set of
node(s) Tu that are the familiar stranger(s) for u. Algorithm
ﬁrst clusters the contacts Cu of node u and selects the cluster
that has maximum similarity with γ, i.e., Cu . Then among
the node(s) in Cu , node(s) whose attribute-value set matches
with γ are selected and we call this set of node(s) Cu . The
node(s) in Cu are then added to a data structure Q. For each
node t in Q search is repeated by ﬁrst clustering the contacts
Ct of node t and then selecting the cluster that has maximum
similarity with γ, i.e., Ct . Then further ﬁlter Ct by selecting
the node(s) whose attribute-value set matches with γ. Assign these node(s) to the set Ct . Node(s) in Ct are added
to the set Tu and Q. Q is a FIFO data structure to ensure a
breadth-ﬁrst search. We do not add Cu to Tu since these are
the adjacent contacts of u and not strangers.
A social network could be a cyclic graph so a person
might get multiple requests to search his contacts to ﬁnd familiar strangers. We assume that a node searches his contacts only once. This is realistic because once a person has

3
Bloggers’ attribute-value set construction is explained in more
detail in the BlogCatalog section.

4
It has been found that on average people have approximately
over 150 contacts, also known as the Dunbar number (Bialik 2007).

4

Table 1: Summary of BlogCatalog and DBLP datasets.

Input : Node: u,
−
→
Contacts of u: Cu ,
Attribute-value set of contacts of u: BCu ,
A goal: γ
Output: Set of nodes Tu familiar strangers to node u
1
2
3
4
5
6
7
8
9
10
11

Statistics
Number of nodes
Number of node-node links
Link density
Average degree of nodes
Diameter of the network
Attribute name
Size of domain of the attribute
Average size of attribute-value
set per node

Tu ← ∅;
Cluster the contacts Cu of node u;
Cu ← select the cluster of contacts that has maximum
similarity with γ;
Cu ← select the nodes from Cu whose attribute-value set
match with γ;
Add selected nodes Cu to a FIFO data structure Q;
Set participatedFlag for u ← true;
while Q = ∅ do
t ← dequeue a node from Q;
if participatedFlag for node t = f alse then
Ct ← All contacts of t;
Cluster the contacts Ct of node t;
Ct ← select the cluster of contacts that has
maximum similarity with γ;
Ct ← select the nodes from Ct whose
attribute-value set match with γ;
Add selected nodes Ct to Q;
Add Ct to Tu ;
Set participatedFlag for node t ← true;
end
end
Return Tu ;

BlogCatalog
23,566
1,165,622
0.002
98
5
Categories
60
1.6

DBLP
35,001
1,067,447
0.0009
9
10
Venues
3198
28.7

of the contacts of a node, and (2) no intelligent selection of
contacts in random search approach. Exhaustive search is a
special case of random search where σ = 1.

Datasets

Here a node explores all his contacts and his contacts explores all their contacts and so on to search for the nodes
that have maximum similarity with the goal γ. This procedure continues till all the familiar strangers Tu of the node
u are found. This exhaustive search procedure incurs an exponential computational cost. Approximately, for an average degree d of the network, and h hops needed to ﬁnd all
the familiar strangers, the total number of edges the exhaustive approach needs to traverse is O(dh ) which is exponential
to the search depth. However, exhaustive search guarantees
that all the familiar strangers of a node are found.

A blogger network, BlogCatalog5 and citation network
DBLP6 is used for the evaluation of different approaches
BlogCatalog A blog in BlogCatalog is associated with
various information pieces like the categories the blog is
listed under, blog level tags, snippets of 5 most recent blog
posts, and blog post level tags. A blogger also speciﬁes his
social network of other bloggers. A blogger’s interests could
be gauged by the categories he publishes his blogs in. There
are in total 60 categories in BlogCatalog. Each blogger
could list his blog under more than one categories. On average each blogger lists their blog under 1.6 categories. All
the categories his blog has been published are agglomerated
to construct his proﬁle vector. This proﬁle vector forms the
attribute-value set for this blogger. However, in case where
the category information is unavailable, we can use various
existing author-topic model extraction approaches (RosenZvi et al. 2004) to extract topics of the author from the text
in blog posts, tags, and comments. Note that the blogger’s
social network vector is extremely sparse as also depicted by
the average degree of nodes and link density in Table 1.
DBLP dataset presents information on computer science
publications. We construct social network of authors using
the co-author relation. Two authors are connected through
an edge if they have collaborated on at least one paper. So
all the co-authors of an author constitute his social network.
Each author publishes his work in the choice of his venue,
which also tells us about his interests. Based on the venue
information of the publications we construct the attributevalue set of each author. We use a part of DBLP dataset
which is the largest connected component of the graph generated using the co-author relation. The average degree of
the author social network is 9. This shows that on average
an author collaborates with 9 authors, much smaller than the
BlogCatalog dataset, due to which the diameter is twice as
large as the BlogCatalog as summarized in Table 1.

Random Search

Dataset Characteristics

The search starts from u and propagates by randomly selecting some nodes at each hop. A user-speciﬁed selectivity
fraction σ ∈ R and σ ∈ [0, 1] controls the number of contacts randomly selected at each hop. This is different than
the social identity based search because of (1) no clustering

For BlogCatalog and DBLP, we investigate characteristics
like power-law degree distribution and small-world assump-

12
13
14
15
16
17
18
19

Algorithm 1: Searching familiar strangers of u.

searched his contacts and forwarded the search request to his
contacts he has no incentive to do it again. This is realized
by associating a participatedFlag to each node which
is set to false by default and is set to true once the node gets a
search request and forwards it to his contacts (line 16 in the
Algorithm 1). A node checks the participatedFlag
before searching its contacts and propagating the search to
the next hop (line 9 in the Algorithm 1).

Exhaustive Search Approach

5
6

5

http://www.blogcatalog.com
http://kdl.cs.umass.edu/data/dblp/dblp-info.html

Table 2: Clustering coefﬁcient results for both datasets.





	




BlogCatalog
DBLP



Actual Network
0.51
0.69

Random Network
0.001 ± 0.0002
0.001 ± 0.0002



high clustering coefﬁcient implies that the two datasets indeed exhibit small- world characteristics.





Experiments - Constructing Social Identity



Social identities of the nodes are not available in the online social networks, so we construct the social identities
of the nodes using conventional clustering algorithm - kMeans. BlogCatalog dataset has very rich metadata for the
bloggers, including blog posts and tags. We construct the
social identities of the bloggers using the metadata as mentioned in the section on Social Identity Construction. The
DBLP dataset doesn’t have any details about the authors besides their venues. So we cluster the contacts of an author
using the venue information.
Here we present the results of social identity construction of the nodes of the blogger network from BlogCatalog dataset. To avoid the high-dimensionality and synonymy and polysemy issues we use latent semantic analysis to transform the term space vector to concept space as
mentioned before in the Social Identity Construction section. Since we use k-Means algorithm to construct the clusters, we need to ﬁnd the optimal value of k to compute the
clusters. To determine the cluster number k, we try to maximize the following ratio:

 

1
2
Cosine(v
,
v
)
m
n
ci ci ×(ci −1)
vm ∈ci ,vn ∈ci
k





2
1
Cosine(v
,
v
)
m
n
ci ,cj ,i<j ci ×cj 
vm ∈ci
vn ∈cj
k(k−1)
(3)
s.t. 2 ≤ k ≤ D
In the above formula, ci , cj represent two different clusters i and j. vm , vn are two different vectors representing
two different bloggers. k varies from 2 to the number of
contacts a node has, i.e. D. Cosine(bi , bj ) gives the
cosine similarity between the two bloggers, bi , bj . Each
blogger has two vectors: the content vector (bci , bcj ) and tag
vector (bti , btj ). We compute the cosine similarity between
the two bloggers by linearly combining the cosine similarity of each of the two corresponding vectors by assigning
0.3 and 0.7 weight to content and tag vector respectively7 .
The numerator is the average similarity within the clusters,
and the denominator is the average similarity between different clusters. We call them Within Similarity and Between Similarity, respectively. We plot the differential i.e.,
d W ithinSimilarity
dk BetweenSimilarity for different values of k averaged over
100 nodes in Figure 3. We ﬁt a polynomial trendline to help
visualize the trend of the increase in the ratio of Within Similarity and Between Similarity. It is evident from Figure 3
that after a certain value of k (= 30), the increment in this
ratio is small. This means that the ratio increases faster when








	







Figure 2: Degree distribution for BlogCatalog.
tion which are necessary for searchability in the network
with local information (Watts, Dodds, and Newman 2002).
Degree Distribution We study the degree distribution of
the nodes in BlogCatalog and DBLP dataset. We display the
log-log graph of this distribution with log(degree) on the xaxis and log(f requency) on the y-axis, for BlogCatalog in
Figure 2. We omit the degree distribution plot for DBLP due
to space constraints. We observe that both BlogCatalog and
DBLP dataset follow power law distribution P (x) ∼ x−k
with scaling exponent k of 1.1693 and 2.7896, respectively.
Small-World Assumption Networks conforming to
small world assumption are characterized by short average
path lengths and high clustering coefﬁcient (Watts and Strogatz 1998). The distance between any two nodes in the network is deﬁned as the number of edges along the shortest
path connecting them. Average path length of a network is
deﬁned as follows (Watts and Strogatz 1998):

1
lG =
×
d(vi , vj )
(1)
n × (n − 1)
i,j
where n is the number of vertices in the graph G and
d(vi , vj ) denotes the shortest path between two nodes vi and
vj . For BlogCatalog and DBLP, we computed the average
path length using the above formulae and was found to be
2.379 and 5.083, respectively.
Clustering coefﬁcient is a common property of social networks representing circles of friends in which every member
knows every other member. If a node v in graph G is connected to kv other nodes then the clustering coefﬁcient of
node v is deﬁned as (Watts and Strogatz 1998):
2Ev
Cv =
(2)
kv (kv − 1)
where Ev is the actual number of edges that exist between
the kv vertices. We compute Cv for all the vertices v of the
graph G and compute the average value. We compare the
clustering coefﬁcient values of the two datasets with that of
random networks generated using the same set of nodes as in
BlogCatalog and DBLP but the edges are rewired according
to Erodös-Rényi model(Erdös and Rényi 1959). We report
the results for clustering coefﬁcient for both the datasets and
their random network counterparts in Table 2, which shows
that clustering coefﬁcient values for the original datasets
(Actual Networks) is much higher than their random counterparts (Random Networks). Low average path length and

7
These values of weights give the best result. Due to space constraint we do not present the results with different weight values.

6

Table 3: Within Similarity and Between Similarity by different clustering methods



	
	



	


Within Similarity
Between Similarity



k-Means
0.71
0.51

Random
0.52
0.52



Table 4: Comparison of the approaches in terms of Accuracy
and Search Space Complexity for BlogCatalog dataset.


















Figure 3: Differential of the ratio of Within Similarity and
Between Similarity vs. k.
k is small, and the trend becomes ﬂat for larger values of k.
We simply set the number of clusters to 30.
To evaluate the effectiveness of k-Means, we cluster the
contacts by k-Means and random partition, by setting the k
to 30. For k-Means, we randomly choose the nodes to start
clustering. For random partition, the contacts are distributed
into 30 clusters randomly. The average Within Similarity
and Between Similarity values are computed for the clusters
obtained from both k-Means and random partition. Table 3
shows the average Within Similarity and Between Similarity
values for k-Means and random partition method over 100
runs. It is evident from Table 3 that k-Means clustering gives
dense or cohesive and well-separated clusters as implied by
higher Within Similarity and lower Between Similarity as
compared to random partition.

Approach (E)

Accuracy (%)

Steiner Tree
Exhaustive
Random
Social Identity

100%
100%
1.0283% ± 0.862
79.2908% ± 9.052

Search Space Complexity (edge traversals)
3, 565 ± 560
4, 531, 967 ± 891, 831
1, 823 ± 1, 833
6, 032 ± 2, 117

a node u from the given network such that the attribute values Au of u and the goal γ are similar. This constraint is
realized by setting γ ⊆ Au as also deﬁned in the Problem
Formulation section. Recall that this is the same γ that was
used to generate the ground truth of familiar strangers using Steiner Tree based approach. Then we use the strategy
E to generate the familiar stranger nodes for u denoted by
E
. We repeat this process for all such possible nodes and
Vu,γ
aggregatethe familiar strangers identiﬁed for each node, deE
noted by u∈V,γ⊆Au Vu,γ
. Then accuracy for approach E is
computed as the intersection between the ground truth computed by using Steiner Tree based approach and the familiar
strangers identiﬁed by E for the γ normalized by the total number of the familiar strangers identiﬁed by the Steiner
Tree based approach as the ground truth. Mathematically,
we can represent accuracy of an approach E with respect to
a goal γ as,

E
)|
|VγF S ( u∈V,γ⊆Au Vu,γ
E
Accγ =
(4)
F
S
|Vγ |

Experiments - Searching Familiar Strangers
In this section we compare the proposed social identity
based search approach with other alternatives, viz., Steiner
tree approach, exhaustive search approach and random
search approach. We compare these approaches in terms of
accuracy and search space complexity as explained next.

Search Space Complexity We deﬁne the search space
complexity of an approach E as the number of hops traversed to ﬁnd
stranger nodes with respect
the set of familiar
E
to a goal γ ( u∈V,γ⊆Au Vu,γ
). Since Steiner Tree based approach ﬁnds the set of familiar stranger nodes with respect
to a goal γ by traversing minimum number of edges. We exploit this property to establish the lower bound on the search
space complexity for various approaches.

Evaluation Criteria
To compare the above-mentioned approaches we need to establish a ground truth. As mentioned in the Problem Formulation section, Steiner Tree based approach has the global
view of the network G with V vertices, so we construct the
ground truth using Steiner Tree based approach. For a given
goal γ, Steiner Tree based approach extracts a subgraph Gγ
from the original graph containing nodes that share a part or
whole of the γ (required vertices), Vγ , as well as some nodes
that do not share γ at all (Steiner vertices or optional vertices), VγSV . This subgraph could be used to identify the familiar strangers of any node which is a part of this subgraph.
Basically, the required nodes that are not directly connected
to a node u in this subgraph are the familiar strangers of u
or Tu and forms the ground truth, denoted by VγF S and is
computed as Vγ − VγSV .

Results and Analysis
In our experiments we test for 1000 goal (γ) values. For each
value of γ we generate the set of familiar stranger nodes using the approaches mentioned above. We compute the accuracy for each of the mentioned approaches as explained in
the section on Accuracy and also compute the search space
complexity in terms of the hops traversed as described in the
section on Search Space Complexity. We average the accuracy values over all the goals, i.e., 1000 γ values. We report the average accuracy values along with the search space
complexity for all the approaches in Tables 4 and 5.
From the Tables 4 and 5 it can be observed that, though
exhaustive approach gives 100% accuracy it bears an over-

Accuracy To evaluate an approach E (where E could be
one of the social identity based search approach, random
search approach, and exhaustive search approach), we pick

7

whelming search cost to discover all the familiar stranger
nodes. On the other hand social identity based search approach achieves 79.2908% accuracy for BlogCatalog and
91.3495% accuracy for DBLP dataset. However, the social
identity based approach searches approximately 0.1331%
of the space as searched by exhaustive search approach in
BlogCatalog and 0.1136% for DBLP. This shows a phenomenal reduction in the search space using social identity of the
nodes while searching for familiar strangers.
We present the results for Steiner Tree based approach as
a lower bound for search space complexity in Tables 4 and 5.
Since Steiner Tree based approach assumes global information of the network, it can discover all the familiar stranger
nodes, hence it achieves an accuracy of 100% for both BlogCatalog and DBLP dataset in minimum search steps. However, social identity based search approach, which does not
have global information about the network, searches only a
couple of factors more of the search space (precisely, 1.69
and 2.56 for BlogCatalog and DBLP, respectively). Since
the social identity based search approach has egocentric
view of the network, it cannot achieve 100% accuracy, but
it still performs reasonably well as compared to Steiner tree
approach for both the datasets.
Deeper analysis explains the reason for such a drastic reduction in search space complexity. We computed the average number of contacts selected at each hop for the social
identity based search, which comes out to be 0.030 ± 0.006
and 0.039 ± 0.011 for BlogCatalog and DBLP dataset, respectively. This means that as few as 3% and 4% of nodes
are selected on average at each hop that propagates the
search at the next hop, respectively for BlogCatalog and
DBLP datasets. This extremely small fraction of nodes selected at each hop is the reason why social identity based
search approach has such a small search space complexity.
To test the effectiveness of the social identity based approach we compare it with the random search approach.
Random search approach selects a percentage of nodes at
each hop randomly and propagates the search to the next
hop. This doesn’t involve any intelligent selection of the
contacts. For a fair comparison we assigned σ (the selectivity parameter for random search approach) as the selectivity
for social identity based search approach, which was found
to be 0.030 ± 0.006 and 0.039 ± 0.011 for BlogCatalog and
DBLP dataset, respectively. A comparison of accuracy values between the random search approach and social identity
based search approach (in Tables 4 and 5) clearly shows that
intelligent selection of contacts based on social identity theory improves the accuracy phenomenally. Note that random
search approach selects the contacts randomly at each hop
so for each goal γ value we run the random search 1000
times and report average accuracy and search space complexity results for a particular γ. Finally for all the 1000
goal γ values we compute the average accuracy and search
space complexity results.
Next we compare the various approaches at different accuracy values in terms of search complexity. This experiment is performed to observe the search space complexity
behavior as we attempt to ﬁnd increasingly larger number
of familiar strangers. We report the results in Figure 4 for

Table 5: Comparison of the approaches in terms of Accuracy
and Search Space Complexity for DBLP dataset.
Approach (E)

Accuracy (%)

Steiner Tree
Exhaustive
Random
Social Identity

100%
100%
2.304% ± 0.1264
91.3495%±4.4398

Search Space Complexity (edge traversals)
4, 752 ± 907
909, 543 ± 162, 651
58 ± 159
12, 182 ± 4, 716

BlogCatalog dataset. Note that since the random search
approach does not give reasonable accuracy (< 10%) in
both the datasets, we do not include it in these experiments.
It is evident from the ﬁgure that exhaustive search based
approach has an exponential behavior. The overwhelming
search space complexity of the exhaustive search approach
overshadows the search space complexity behavior for social identity and Steiner Tree approach. To observe the
search space complexities of social identity and Steiner Tree
based search approach we plot accuracy vs. log of search
steps in Figure 5 for BlogCatalog. It shows that social identity and Steiner Tree based search approach are comparable
in terms of search space complexity. However, exhaustive
search approach is almost 2-3 orders of magnitude higher
than both the social identity and Steiner Tree based approach
for both the datasets. This shows that social identity based
search is closer to Steiner Tree based search approach in
terms of search space complexity although social identity
based search assumes only egocentric view unlike Steiner
Tree based search approach that assumes global view of the
network. Similar behavior is observed for DBLP, however
due to space constraints the results are not presented here.

Related Work
To the best of our knowledge no work uses the social identity
theory to search for familiar strangers, so we review extant
literature in identifying latent friends and clustering nodes
of a social network.
Identifying Latent Friends Authors in (Schwartz and
Wood 1993) use Social Network Analysis (SNA) to discover groups of individuals sharing the same connectivity
properties of networks. Since this does not consider the textual information of the entities, it limits the applications of
SNA. Authors in (Rosen-Zvi et al. 2004)(McCallum et al.
2005) use LDA and its variations to mine relationships between people based on the content. These approaches develop topic models on the documents submitted by the authors. Authors may produce several documents often with
coauthors, making it unclear how the topics generated for
these documents might be used to describe the interests of
the authors. Moreover, it is challenging to learn the parameters in these approaches even though well-established approximation techniques exist. Considering the limitations of
author-topic model based approaches to identify latent relations, authors in (Shen et al. 2006) train an SVM to predict the topics for bloggers from external topic taxonomies.
Based on the topic similarity, further reﬁned by the cosine
similarity of actual blog content, similar bloggers can be recommended. As topic taxonomies keep evolving, it requires

8









	


	




















#

#

#

#

#

$

#

$

$

	





$

!$

$



	
	




Figure 4: Accuracy vs. Search Steps for BlogCatalog.








Figure 5: Accuracy vs. Log of Search Steps for BlogCatalog.

Acknowledgments

re-training the classiﬁer that adds complexity to the solution.
Moreover, detecting bloggers true interests in some of their
writings could be a big challenge at times. Unlike the familiars strangers, the latent bloggers identiﬁed by (Shen et
al. 2006) could possibly know each other. Other key differences are, the constraint of egocentric network view and use
of social identity theory in searching familiar strangers.
Clustering in Social Networks
Girvan and Newman (Girvan and Newman 2002) proposed a divisive algorithm by measuring “edge betweenness” based on the observation that the inter cluster edges have a large “edge betweenness” value if the communities are loosely interconnected. (Radicchi et al. 2004) improves the former work by
considering the “edge-clustering coefﬁcient” as the number
of triangles to which a given edge belongs, divided by the
number of triangles that might potentially include it, which
is similar to the deﬁnition of “clustering coefﬁcient” ﬁrst introduced by (Watts and Strogatz 1998). Another measure to
detect the community is modularity(Newman 2006) which
estimates the fraction of in-links in a community minus the
expected value of in-links in a network with the same community structure but random connections between the nodes.
Unlike above methods which search for the non-overlapping
communities, (Palla et al. 2005) explores overlapping communities based on the idea that a community consists of several complete subgraphs that share several nodes.

This work is in part supported by AFOSR FA95500810132
and ONR N000140810477, N000140910165 grants.

References
Bialik, C. 2007. Sorry, you may have gone over your limit of
network friends. The Wall Street Journal Online.
Deerwester et al., S. 1990. Indexing by latent semantic analysis.
JASIS 41(6):391–407.
Du, D., and Hu., X. 2008. Steiner tree problems in computer
networks. World scientiﬁc publishing.
Erdös, P., and Rényi, A. 1959. On random graphs,i. Publications
Mathematicae 6:290–297.
Girvan, M., and Newman, M. 2002. Community structure in
social and biological networks. PNAS 99(12).
Granovetter, M. 1973. The Strength of Weak Ties. American
Journal of Sociology 78(6):1360.
Kumar et al., R. 2004. Structure and evolution of blogspace.
Communications of the ACM 47(12):35–39.
McCallum et al., A. 2005. Topic and role discovery in social
networks. In IJCAI.
Milgram, S. 1972. The familiar stranger: An aspect of urban
anonymity. Division 8, Newsletter.
Newman, M. E. J. 2006. Modularity and community structure in
networks. PNAS 103(23):8577–8582.
Palla et al., G. 2005. Uncovering the overlapping community
structure of complex networks in nature and scociety. Nature 435.
Radicchi et al., F. 2004. Deﬁning and identifying communities in
networks. PNAS 101(9).
Rosen-Zvi et al., M. 2004. The author-topic model for authors
and documents. In Proceedings of UAI.
Schwartz, M., and Wood, D. 1993. Discovering shared interests
using graph analysis. Commun. ACM 36(8).
Shen, D.; Sun, J.-T.; Yang, Q.; and Chen, Z. 2006. Latent friend
mining from blog data. In ICDM’06.
Tajfel, H. 1978. Differentiation between social groups: studies in
the social psychology of intergroup relations. Academic Press.
Wasserman, S., and Faust, K. 1994. Social Network Analysis.
Cambridge University Press.
Watts, D., and Strogatz, S. 1998. Collective dynamics of ‘smallworld’ networks. Nature 393(6684).
Watts, D.; Dodds, P.; and Newman, M. 2002. Identity and Search
in Social Networks. Science 296(5571):1302–1305.

Conclusion
In this paper, we studied the familiar strangers in online social networks and identify the numerous research opportunities and business advantages of identifying and aggregating
the familiar strangers. We formulate the problem and propose a social identity theory based solution with other alternatives. We also show that under certain circumstances, the
problem of identifying familiar strangers can be reduced to a
well-known np- complete Steiner tree problem and study its
2-approximation solution to estimate the lower bound on the
search space. The Steiner tree solution is also used to generate the ground truth. We performed extensive experiments
on a real world blogger social network dataset, BlogCatalog
and citation network dataset, DBLP to show that the proposed social identity based approach outperforms the other
alternative approaches and is quite close to the Steiner tree
based search approach in terms of search space complexity.

9

On Robustness in Multilayer
Interdependent Networks
Joydeep Banerjee(B) , Chenyang Zhou, Arun Das, and Arunabha Sen
School of Computing, Informatics and Decision System Engineering,
Arizona State University, Tempe, AZ 85287, USA
{joydeep.banerjee,czhou24,arun.das,asen}@asu.edu

Abstract. Critical Infrastructures like power and communication networks are highly interdependent on each other for their full functionality.
Many signiﬁcant research have been pursued to model the interdependency and failure analysis of these interdependent networks. However
most of these models fail to capture the complex interdependencies that
might actually exist between the infrastructures. The Implicative Interdependency Model that utilizes Boolean Logic to capture complex interdependencies was recently proposed which overcome the limitations of
the existing models. A number of problems were studied based on this
model. In this paper we study the Robustness problem in Interdependent Power and Communication Network. The robustness is deﬁned with
respect to two parameters K ∈ I + ∪ {0} and ρ ∈ (0, 1]. We utilized the
Implicative Interdependency Model to capture the complex interdependencies between the two networks. The problem is solved using an Integer Linear Program and the solution is used to study the robustness of
power and communication interdependent network of Maricopa County,
Arizona, USA.
Keywords: Implicative Interdependency Model
networks · Robustness

1

·

Interdependent

Introduction

Critical infrastructures (or networks) of a nation are heavily interdependent
on each other for their full functionality. Two such infrastructures that engage
in a heavy symbiotic dependency are the power and communication networks.
For analysis of these infrastructures it is imperative to model their interdependencies. A number of such models have been proposed [2–4,6]. However, most
of these models fail to account for the complex interdependencies between the
networks [1]. In [5] the authors described the need to address complex interdependencies. They introduce a model (Implicative Interdependency Model (IIM))
that capture the interdependencies using boolean logic. We use the (IIM) model
to study the “Robustness” problem in interdependent power and communication network. An Integer Linear Program (ILP) is provided for the problem.
The ILP is used to analyze the robustness of power and communication network
of Maricopa County, Arizona, USA.
c Springer International Publishing Switzerland 2016

E. Rome et al. (Eds.): CRITIS 2015, LNCS 9578, pp. 247–250, 2016.
DOI: 10.1007/978-3-319-33331-1 21

248

2

J. Banerjee et al.

Problem Formulation

An Interdependent Network (IDN) is represented as I(A, B, F(A, B)) where A is
the set of power entities, B is the set of communication entities and the function
F(A, B) captures the interdependencies between the two networks. The IIM
model is described by an example. Consider A = {a1 , a2 } and B = {b1 , b2 , b3 }
and F(A, B) consisting of the relations a1 ← b1 + b2 b3 , b1 ← a1 a2 and b2 ← a2 .
The dependency or relation is termed as Implicative Interdependency Relation
(IDR). In the given example, an IDR a1 ← b1 + b2 b3 implies that the entity a1
is operational if entity b1 or entity b2 and b3 are operational. In the IDR each
conjunction term e.g. a1 a2 is referred to as minterm. Initial failure of a set of
entities would cause the failure to cascade. The failures are assumed to happen
at unit time steps denoted by t with the initial failure occurring at t = 0. With
initial failure of entities b1 and b3 at t = 0 the entity a1 would fail at t = 1 and
b2 at time step t = 2 signifying the cascade of failures. The entities failed after
the initial failure are termed as induced failure.
Using the IIM model described above we define the Robustness in Interdependent Network (RIDN) problem. The RIDN problem consists of an integer
K ∈ I + ∪ {0} and a real valued parameter ρ ∈ R with 0 < ρ ≤ 1. An IDN
I(A, B, F(A, B)) is (K, ρ) robust if a minimum of K + 1 entities need to fail
initially for a final failure of at least ρ(|A| + |B|) entities. We formally state the
optimization version of the RIDN problem as follows —.
The Robustness in Interdependent Network (RIDN) Problem
Instance— An IDN I(A, B, F(A, B)) , an integer K ∈ I + and a real valued
parameter ρ ∈ R with 0 < ρ ≤ 1.
Optimization Version— Find the minimum set of entities SI which when
failed initially causes a final failure of at least ρ(|A| + |B|) entities. The IDN is
then referred to as (|SI | − 1, ρ) robust

3

Integer Linear Program to the RIDN Problem

We formulate an ILP that for a given parameter ρ ∈ (0, 1] and an IDN computes
the minimum number of entities that need to fail initially for a final failure of
ρ(|A| + |B|) entities. Let K  be the solution to the ILP. Then the IDN is (K, ρ)
robust with K = K  − 1. The ILP works with two variables xid and yid for each
entity xi ∈ A and yi ∈ B respectively. The parameter d in the variable denotes
the time step. xid =1 (or yid = 1) if at time step d the entity xi (yi ) is in a
failed state and 0 otherwise. With these definitions the objective function can
be formulated as follows:
m
n


min xi0 +
yj0
(1)
i=1

j=1

In the above objective function m and n denote the size of the networks A and
B respectively. The constraints of the ILP are formally described as follows:

On Robustness in Multilayer Interdependent Networks

249

Constraint Set 1: xid ≥ xi(d−1) , ∀d, 1 ≤ d ≤ tf and yid ≥ yi(d−1) , ∀d, 1 ≤ d ≤ tf ,
where tf denotes the final time step. The constraint satisfies the property that
if the entity xi fails at time step d it should remain to be in the failed state for
all subsequent time steps [5].
Constraint Set 2: A brief overview of the constraint set to model the failure propagation through cascades is presented here. Consider an IDR of form
ai ← bj + bk bl + bm bn bq . This corresponds to the general case or Case IV as
discussed earlier. The constraints created to capture the failure propagation are
described in the following steps —
Step 1: We introduce new variables to represent minterms of size greater than
one. In this example two new variables c1 and c2 are introduced to represent the
minterms bk bl and bm bn bq respectively. This is equivalent of adding two new IDRs
c1 ← bk bl and c2 ← bm bn bq along with the transformed IDR ai ← bj + c1 + c2 .
Step 2: For each IDR corresponding to the c type variables and untransformed
IDRs of form Case II we introduce a linear constraint to capture the failure
propagation. For an IDR c2 ← bm bn bq the constraint is represented as c2d ≤
ym(d−1) + yn(d−1) + yq(d−1) , ∀d, 1 ≤ d ≤ tf .
Step 3: Similarly, for each transformed IDR and untransformed IDRs of form
Case III we introduce a linear constraint to capture the failure propagation.
For an IDR ai ← bj + c1 + c2 the constraint is represented as N × xid ≤
yj(d−1) + c1(d−1) + c2(d−1) , ∀d, 1 ≤ d ≤ tf . Here N is the number of minterms in
the IDR (in this example N = 3).
Constraint Set 3: We must also ensure that at time step tf at least ρ(|A| +
m

|B|) entities fail. This can be captured by introducing the constraint
xi(tf ) +
n

j=1

i=1

yj(tf ) ≥ ρ(|A| + |B|).

So with the objective in (1) and set of constraints the ILP finds the minimum
number of entities K  which when failed initially causes at least ρ(|A| + |B|)
entities to fail at tf .

4

Experimental Results

We performed experimental evaluation of the optimal solution of the RIDN
problem in this section. Real world data sets were used for the experiments. The
communication network data was obtained from GeoTel (www.geo-tel.com). The
dataset contains 2, 690 cell towers, 7, 100 fiber-lit buildings and 42, 723 fiber links
of Maricopa County, Arizona, USA. The power network data was obtained from
Platts (www.platts.com). It contains 70 power plants and 470 transmission lines
of the same county. We took three non overlapping regions of the Maricopa
county. It is to be noted that the union of the regions does not cover the entire

250

J. Banerjee et al.

space. The entities of the power and communication network for these three
regions were extracted. As per notation, set A and B contain entities of the power
and communication network respectively. The number of entities in set A and B
are 29 and 19 for Region 1, 29 and 20 for Region 2, and 29 and 19 for Region 3.
The regions were represented by an interdependent network I(A, B, F(A, B)).
For these regions F(A, B) was generated using the IDR construction rule as
defined in [5] (Fig. 1).
30

25

25

15
10

Robustness (K)

20

15

10

5

5
0
0

30

20
Robustness (K)

Robustness (K)

25

0.2
0.4
0.6
0.8
Minimum fraction of entities killed(ρ)

(a) Region 1

1

0
0

20
15
10
5

0.2
0.4
0.6
0.8
Minimum fraction of entities killed(ρ)

(b) Region 2

1

0
0

0.2
0.4
0.6
0.8
Minimum fraction of entities killed(ρ)

1

(c) Region 3

Fig. 1. Robustness parameter K returned by the optimal solution by varying parameter
ρ for three regions in Maricopa County, Arizona, USA

IBM CPLEX Optimizer 12.5 is used to get the optimal solution using the
ILP. For all the regions the network robustness is less than 10 for ρ = 0.6 with
a gradual increase for higher ρ values. Hence it can be inferred that for most
regions of Maricopa county the IDN is comparatively more vulnerable for lower
values of ρ.

References
1. Banerjee, J., Das, A., Sen, A.: A survey of interdependency models for critical
infrastructure networks. NATO Sci. Peace Secur. Ser. D Inf. Commun. Secur. 37,
1–16 (2014)
2. Buldyrev, S.V., Parshani, R., Paul, G., Stanley, H.E., Havlin, S.: Catastrophic cascade of failures in interdependent networks. Nature 464(7291), 1025–1028 (2010)
3. Parandehgheibi, M., Modiano, E.: Robustness of interdependent networks: The
case of communication networks and the power grid (2013). arXiv preprint
arXiv:1304.0356
4. Rosato, V., Issacharoﬀ, L., Tiriticco, F., Meloni, S., Porcellinis, S., Setola, R.: Modelling interdependent infrastructures using interacting dynamical models. Int. J.
Crit. Infrastruct. 4(1), 63–79 (2008)
5. Sen, A., Mazumder, A., Banerjee, J., Das, A., Compton, R.: Identiﬁcation of k most
vulnerable nodes in multi-layered network using a new model of interdependency.
In: 2014 IEEE Conference on Computer Communications Workshops (INFOCOM
WKSHPS), pp. 831–836. IEEE (2014)
6. Zhang, P., Peeta, S., Friesz, T.: Dynamic game theoretic model of multi-layer
infrastructure networks. Netw. Spat. Econ. 5(2), 147–178 (2005)

On Robustness in Multilayer Interdependent
Network

arXiv:1702.01018v1 [cs.NI] 24 Jan 2017

Joydeep Banerjee, Chenyang Zhou, and Arunabha Sen
School of Computing, Informatics and Decision System Engineering
Arizona State University, Tempe, Arizona 85287
Email: {joydeep.banerjee, czhou24, arun.das, asen}@asu.edu

Abstract. Critical Infrastructures like power and communication networks are highly interdependent on each other for their full functionality.
Many significant research have been pursued to model the interdependency and failure analysis of these interdependent networks. However
most of these models fail to capture the complex interdependencies that
might actually exist between the infrastructures. The Implicative Interdependency Model that utilizes Boolean Logic to capture complex interdependencies was recently proposed which overcome the limitations of
the existing models. A number of problems were studies based on this
model. In this paper we study the Robustness problem in Interdependent
Power and Communication Network. The robustness is defined with respect to two parameters K ∈ I + ∪ {0} and ρ ∈ (0, 1]. We utilized the
Implicative Interdependency Model model to capture the complex interdependency between the two networks. The model classifies the interdependency relations into four cases. Computational complexity of the
problem is analyzed for each of these cases. A polynomial time algorithm
is designed for the first case that outputs the optimal solution. All the
other cases are proved to be NP-complete. An in-approximability bound
is provided for the third case. For the general case we formulate an Integer Linear Program to get the optimal solution and a polynomial time
heuristic. The applicability of the heuristic is evaluated using power and
communication network data of Maricopa County, Arizona. The experimental results showed that the heuristic almost always produced near
optimal value of parameter K for ρ < 0.42.

Keywords: Implicative Interdependency Model, Interdependent Networks, Robustness.

1

Introduction

Critical infrastructures (or networks) of a nation are heavily interdependent
on each other for their full functionality. Two such infrastructures that engage
in a heavy symbiotic dependency are the power and communication networks.
For example, consider entities in the power network such as SCADA systems.
The primary function of a SCADA system is to control power generation units
remotely. This operation relies on the entities of the communication network
for receiving control commands. On the other hand communication network

entities are dependent on electric power to function properly. The power and
communication networks are highly vulnerable to nature induced and man made
(terrorist attack) failure. Considering a set of entities in either network failing
initially, further failures may be triggered due to the interdependencies between
them.
For analysis of these infrastructures it is imperative to model their interdependencies. A number of such models have been proposed [4], [7], [14], [12], [15],
[11], [10], [3]. However, most of these models fail to account for the complex interdependencies between the networks [1]. In [13] the authors described the need to
address complex interdependencies as the one in the following example. Consider
an entity ai in power network and entities bj , bk , bl , bm , bn in the communication
network. For the entity ai to be operational (i) entities bj and bk needs to be
operational, or (ii) entities bk and bm needs to be operational, or (iii) entity
bn needs to be operational. The constructed interdependency cannot be represented by graph based interdependency models as described in [14], [12], [15],
[5], [11], [10], [4], [7]. Graph based models fall short in capturing the disjunctive
and conjunctive dependencies that exists in real world systems. Authors in [13]
proposed an Implicative Interdependency Model that overcome these limitations.
The model uses Boolean Logic to characterize the complex interdependencies.
This model was used to study a number of problems on interdependent critical
systems [13], [6], [9] and [2].
We use the Implicative Interdependency Model (IIM) to study the “Robustness” problem in interdependent power and communication network. An Interdependent Network (IDN) is formally denoted as I(A, B, F (A, B)). Here A
and B are set of entities in power and communication network respectively and
function F (A, B) capturing the interdependency between them through the IIM
model (discussed later). “Robustness” of an interdependent system can be formulated with respect to two parameters K ∈ I + ∪{0} and ρ ∈ R with 0 < ρ ≤ 1.
An interdependent system is (K, ρ)-robust if a minimum of K + 1 entities need
to fail for failure of at least ρ(|A| + |B|) entities. This robustness value can be
treated as a metric to determine the quality of an IDN when it is set up initially.
In existing systems this value determines the need and importance of introducing
additional measures for improving their robustness.
The interdependencies using the IIM model can be categorized into four cases
(namely case I, II, III and IV). The robustness problem is separately studied for
each individual cases. We show that for case I there exists a polynomial time
solution to the problem whereas all the other cases are NP-complete. Additionally, we provide an in-approximability bound for case III, an optimal solution
using integer linear program for case IV (the general case) and a heuristic for
the same. The heuristic is compared with the optimal solution using power and
communication network data of Maricopa County, Arizona. From the experimental results we infer that almost always our heuristic produces near optimal
solution for ρ < 0.42.
The rest of the paper is organized as follows. In Section 2 the IIM model is
presented, formal definition of the robustness problem and analysis of its com-

putational complexity are done in Section 3 and 4 respectively, the heuristic and
optimal solution to the problem is provided in Section 5 with the corresponding
experimental results in Section 6, and we conclude the paper is Section 7.

2

The Implicative Interdependency Model

In this section we describe the IIM model [13]. Consider an IDN I(A, B, F (A, B)).
The set A and B consisits of entities {a1 , a2 , a3 } and {b1 , b2 , b3 , b4 } respectively.
The function F (A, B) giving the set of dependency equations are represented in
Table 1. We call the dependency equation of each entity as Implicative Dependency Relation (IDR). In the given example, an IDR a1 ← b3 + b1 b4 implies that
entity a1 is operational if entity b3 or entity b1 and b4 are operational. In the
IDR each conjunction term e.g. a2 a3 is referred to as minterms.
Power Network
a 1 ← b2 + b4
a 2 ← b1 b3
a 3 ← b3 + b1 b4
−−

Comm. Network
b1 ← a 1 + a 2
b2 ← a 1 a 2 a 3
b3 ← a 1 + a 2 a 3
b4 ← a 2

Table 1: Sample Implicative Interdependency Relations of an IDN
Initial failure of an entity set in A ∪ B would cause the failure to cascade
until a steady state is reached. The event of an entity failing after the initial
failure is termed as induced failure. The cascade is assumed to occur in time
steps of unit length. Each time step captures the effect of entities killed in all
previous time steps. We demonstrate the cascading failure for the interdependent
network outlined in Table 1 through an example. Consider that the entity a1 fails
at time step t = 0. Table 2 represents the cascade of failure in each subsequent
time steps. In Table 2, for a given entity and time step ′ 0′ represents the entity
is operational and ′ 1′ non operational. In this example a steady state is reached
at time step t = 4 when all entities are non operational. The IIM model also
assumes that the dependent entities of all failed entities are killed immediately
at the next time step. For example at time step t = 1 entities a1 , b2 and b4 are
non operational. Due to the IDR a1 ← b2 + b4 entity a1 is killed immediately at
time step t = 2.
Entities Time Steps (t)
012345 6
a1
a2
a3
b1
b2
b3
b4

0
1
0
0
0
0
0

0
1
0
0
1
0
1

1
1
0
0
1
0
1

1
1
0
1
1
1
1

1
1
1
1
1
1
1

1
1
1
1
1
1
1

1
1
1
1
1
1
1

Table 2: Failure cascade propagation when entity {a2 } fail at time step t = 0. A
value of 1 denotes entity failure, and 0 otherwise

The main challenge of the IIM model is accurate formulation of the IDRs.
Two possible ways of doing this would be 1) careful analysis of the underlying infrastructures as in [3], 2) Consultation with domain experts. However we
only utilize the IIM model to analyze the Robustness problem and refrain from
addressing the mentioned challenge.

3

Problem Formulation

As described before we define the Robustness in Interdependent Network (RIDN)
problem with respect to an integer K ∈ I + ∪ {0} and a real valued parameter
ρ ∈ R with 0 < ρ ≤ 1. An IDN I(A, B, F (A, B)) is (K, ρ) robust if a minimum
of K + 1 entities need to fail initially for a final failure of at least ρ(|A| + |B|)
entities. For example, the IDN described in Table 1 is (0,ρ) robust for any value
of ρ ∈ (0, 1]. This is because initial failure of entity a2 causes all the entities to
fail in the steady state. The output of the RIDN problem is the parameter K
for a given IDN and a value of ρ. We formally state the decision version of the
RIDN problem as follows —.
The Robustness in Interdependent Network (RIDN) problem
Instance— An IDN I(A, B, F (A, B)), an integer K ∈ I + and a real valued
parameter ρ ∈ R with 0 < ρ ≤ 1.
Decision Version— Does there exist a set of entities SI ⊆ A ∪ B and |SI | ≤ K
which when failed initially causes a final failure of at least ρ(|A| + |B|) entities.
It is to be noted that a solution to the decision version of RIDN problem would
ensure that the IDN is not (K, ρ) robust. We use this notion in our computational complexity proofs.

4

Computational Complexity Analysis

The IDRs in the IIM model can be represented in four different forms (1) one
minterm of size one, 2) one minterm of arbitrary size, 3) arbitrary number of
minterms of size one, and 4) arbitrary number of minterms of arbitrary size
(general case). For each of the forms we separately analyze the computational
complexity of the RIDN problem.
4.1

Case I: Problem Instance with One Minterm of Size One

The IDRs in the set F (A, B) have minterms of size 1. With two entities xi and
yj of network A(B) and B(A) respectively the IDR xi ← yj represents this case.
Additionally any entity can appear at most once on the left side of the IDR. We
provide a polynomial time algorithm (Algorithm 1) and prove its optimality (
Theorem 1) that solves the RIDN problem for Case I.
To develop the algorithm we use the notion of Kill Set of an entity xi ∈ A∪B
as in [13]. Kill Set Cxi of an entity xi are the set of entities failed due to initial
failure of xi alone. Using the concept of Kill Set Algorithm 1 is developed.

Algorithm 1: RIDN Algorithm for IDNs with Case I type interdependencies

1
2

3
4
5
6
7
8
9
10

Data: An interdependent network I(A, B, F(A, B)) and a real valued
parameter ρ ∈ (0, 1].
Result: A set of entities E in I(A, B, F(A, B)).
begin
For each entity xi ∈ (A ∪ B) compute the set of kill sets
C = {Cx1 , Cx2 , ..., Cx|A|+|B| }, where Cxi = KillSet(xi ) ;
Initialize D = ∅ and E = ∅ ;
while |D| < ρ(|A| + |B|) do
Let xj be the entity having highest |Cxj |, in case of a tie choose
arbitrarily ;
Add xj to set E ;
Update D = D ∪ Cxj ;
for (i = 1; i ≤ |A| + |B|; i + +) do
Cxi = Cxi \Cxj ;
return E ;

Theorem 1. Algorithm 1 solves the RIDN problem for Case I optimally in polynomial time.
Proof. Computation of Kill Sets for all A ∪ B entities can be done in O((|A| +
|B|)3 ) [13]. The while loop runs for maximum of |A| + |B| times when ρ = 1
and Kill Set of each entity is only composed of the entity itself. The highest
cardinality Kill Set among all Kill Sets can be found in O(|A|+|B|). The for loop
iterates for |A|+|B| times with computation inside it taking O(|A|+|B|) time per
iteration. Hence, the time complexity of the while loop in total is O((|A|+|B|)3 ).
So the overall time complexity of Algorithm 1 is O((|A| + |B|)3 ).
We claim that Algorithm 1 returns the optimal value of robustness parameter
K = |E| − 1 of an IDN I(A, B, F (A, B)) with set E containing the minimum
number of entities that causes failure of at least ρ(|A| + |B|) entities. The claim
is proved by contradiction. Let EOP T be the optimal set that causes failure of
at least ρ(|A| + |B|) entities and xn be an entity in EOP T \E. It is proved in [13]
that in Case I for any two entities xi and xj , Cxi ∩ Cxj = ∅ or Cxi ∩ Cxj = Cxi or
Cxi ∩ Cxj = Cxj where xi 6= xj . At any iteration of the while loop the entity xj
with highest cardinality Kill Set is selected. Inside the for loop all entities having
Cxi ∩ Cxj = Cxi and the entity itself would have its Kill Set updated to ∅. Hence
the Kill Set of the entity xn would either be set to ∅ at some iteration of the while
loop or didn’t have the highest cardinality at any iteration. Hence adding xn to
optimal solution would have made no difference or reduce the number of failed
entities. Hence a contradiction. So Algorithm 1 returns the optimal number of
entities that causes failure of at least ρ(|A| + |B|) entities.

4.2

Case II: Problem Instance with One Minterm of Arbitrary Size

Case II is composed of IDRs having a single minterm of arbitrary size. A minterm
of size p with entities xi and Q
yj belonging to network A(B) and B(A) respectively
p
can be represented as xi ← j=1 yj . Thus killing any one entity (or more) from
the product term would kill xi . In Theorem 2 we prove that the decision version
of the RIDN problem for Case II is NP complete.
Theorem 2. The decision version of the RIDN problem for Case II is NPcomplete.
Proof. We prove the NP-completeness by giving a transformation from the Hitting Set Problem. An instance of the hitting set problem consists of a set of
elements S and a set S = {S1 , S2 , S3 , .., Sn } where Si ⊆ S, ∀Si ∈ S. The question asked in the problem is given an integer M does there exist a set S ′ ⊆ S
with |S ′ | ≤ M such that each subset in S contains at least one element from
S ′ . From an instance of the hitting set problem we create an instance of the
RIDN problem as follows. For each element xi ∈ S we add an entity bi ∈ B.
Similarly for each subset Si ∈ S we add an entity ai ∈ A. For each subset
Si = {xm , xn , xp } (say) we create an IDR ai ← bm bn bp . The value of K is set
M+|S|
to M and ρ is set to |S|+|S|
. It is to be noted that there wont be any cascading
failure due to absence of dependency relations of B type entities.
Let there exists a solution to the hitting set problem. So each subset Si ∈
S has at least one element from set S ′ (with |S ′ | = M ). Hence killing the
corresponding B type entities from the constructed instance would kill all A
M+|S|
= ρ solving the RIDN
type entities. Thus the fraction of entities killed is |S|+|S|
problem.
On the other way round let there exist a solution to the RIDN problem. It
can be shown that the initial failure set would always be chosen from set B to
M+|S|
fail ρ = |S|+|S|
fraction of entities. This is because failure of any A type entity
cannot trigger failure of any other entity. Moreover the total number of entities
in final failure set is M + |S| (as |S| = |A|). Thus the failure set must contain all
A type entities except for M other entities which has to be chosen from set B.
So a solution to RIDN problem consisting of entities B ′ ⊆ B would ensure that
for each entity ai ∈ A at least one entity in its IDR is killed initially. So the set
of elements in S ′ corresponding to the entities in B ′ would solve the hitting set
problem. Hence proved
4.3

Case III: Problem Instance with an Arbitrary Number of
Minterm of Size One

Case III is composed of IDRs having arbitrary number of minterms of size 1.
With entities xi and yq belongingP
to network A(B) and B(A) respectively this
case can be represented as xi ← pq=1 yq . The given example has p minterms
each of size 1. Thus to kill xi all entities in its IDR must be killed. In Theorem
3 we prove that the decision version of the RIDN problem for Case III is NP
complete.

Theorem 3. The decision version of the RIDN problem for Case III is NPcomplete.
Proof. We prove that the problem is NP-complete by giving a reduction from
the Densest p-Subhypergraph problem [8], a known NP- complete problem. An
instance of the Densest p-Subhypergraph problem includes a hypergraph G =
(V, E), a parameter p and a parameter M . The problem asks the question
whether there exists a set of vertices |V ′ | ⊆ V and |V ′ | ≤ p such that the
subgraph induced with this set of vertices has at least M completely covered hyperedges. From an instance of the Densest p-Subhypergraph problem we create
an instance of the RIDN problem as follows. For each vertex vi ∈ V we add an
entity bi ∈ B. Similarly for each hyperedge ej ∈ E we add an entity aj ∈ A. For
each hyperedge ej with ej = {vm , vn , vq } (say) an IDR of form aj ← bm + bn + bq
is created. The value of K is set to p and ρ is set to |Vp+M
|+|E| . It is to be noted
that there wont be any cascading failure due to absence of dependency relations
of B type entities.
Let there exist a solution to the Densest p-Subhypergraph problem. Then
there exist a set V ′ ⊆ V and |V ′ | = p that covers completely at least M hypedges
in E. Thus killing the B type entities corresponding to the vertices in V ′ would
cause at least M A type entities to fail. Hence the fraction of entities killed is
≥ |Vp+M
|+|E| = ρ. So the solution of the Densest p-Subhypergraph problem solves
the Robustness problem.
For the created instance of the RIDN problem all entities in set B can only
fail initially. The A type entities can either fail initially or through induced failure
of failing B type entities. Hence initial failure of entities from set B would have
the most impact on final number of entities failed. Let us assume that there
exists one or many solutions to the RIDN problem. Then at least one solution
would have entities only from set B. For this solution the number of entities
killed on initial failure of p B type entities is at least p + M . The additional M
entities killed belongs to set A. So the vertices in V corresponding to the entities
in B would completely cover at least M hyperedges. Thus the solution of RIDN
problem solves the Densest p-Subhypergraph problem. Hence proved.
Theorem 4. The RIDN problem for Case III is hard to approximate within a
1
factor log(n)
λ (where n = |A ∪ B|) for some λ > 0.
2

Proof. In [8] it is proved the Densest p-Subhypergraph problem is hard to ap1
proximate within a factor of 2log(n)
λ with λ > 0. For IDRs of form Case III it is
shown in Theorem 3 that Densest p-Subhypergraph problem is a special case of
the RIDN problem. Hence proved.
4.4

Case IV (General Case): Problem Instance with an Arbitrary
Number of Minterms of Arbitrary Size

Case IV is composed of IDRs having arbitrary number of minterms of arbitrary
size. With entities xi and yq belonging to network A(B) and B(A) respectively

P
Qq
this case can be represented as xi ← pj1 =1 j2j1=1 yj2 . The given example has
p minterms each of size qj1 . In Theorem 5 we prove that the decision version of
the RIDN problem for Case IV is NP complete.
Theorem 5. The decision version of the RIDN problem for Case IV is NPcomplete.
Proof. As IDRs in form of Case II and Case III are subsets of the general case
so the RIDN problem for Case IV is NP-complete.

5

Solutions to the RIDN Problem

We propose an optimum solution to the RIDN problem using Integer Linear
Programming (ILP) in 5.1, and a heuristic in section 5.2
5.1

Optimal Solution for the RIDN problem

We formulate an ILP that for a given parameter ρ ∈ (0, 1] and an IDN computes
the minimum number of entities that need to fail initially for a final failure of
ρ(|A| + |B|) entities. Let K ′ be the solution to the ILP. Then the IDN is (K, ρ)
robust with K = K ′ − 1. The ILP works with two variables xid and yid for each
entity xi ∈ A and yi ∈ B respectively. The parameter d in the variable denotes
the time step. xid =1 (or yid = 1) if at time step d the entity xi (yi ) is in a
failed state and 0 otherwise. With these definitions the objective function can
be formulated as follows:
n
m
X
X
yj0
(1)
min xi0 +
i=1

j=1

In the above objective function m and n denote the size of the networks A and
B respectively. The constraints of the ILP are formally described as follows:
Constraint Set 1: xid ≥ xi(d−1) , ∀d, 1 ≤ d ≤ tf and yid ≥ yi(d−1) , ∀d, 1 ≤ d ≤ tf ,
where tf denotes the final time step. The constraint satisfies the property that
if the entity xi fails at time step d it should remain to be in the failed state for
all subsequent time steps [13].
Constraint Set 2: A brief overview of the constraint set to model the failure
propagation through cascades is presented here. Consider an IDR of form ai ←
bj + bk bl + bm bn bq . This corresponds to the general case or Case IV as discussed
earlier. The constraints created to capture the failure propagation are described
in the following steps —
Step 1: We introduce new variables to represent minterms of size greater than
one. In this example two new variables c1 and c2 are introduced to represent the
minterms bk bl and bm bn bq respectively. This is equivalent of adding two new IDRs
c1 ← bk bl and c2 ← bm bn bq along with the transformed IDR ai ← bj + c1 + c2 .

Step 2: For each IDR corresponding to the c type variables and untransformed
IDRs of form Case II we introduce a linear constraint to capture the failure
propagation. For an IDR c2 ← bm bn bq the constraint is represented as c2d ≤
ym(d−1) + yn(d−1) + yq(d−1) , ∀d, 1 ≤ d ≤ tf .
Step 3: Similarly, for each transformed IDR and untransformed IDRs of form
Case III we introduce a linear constraint to capture the failure propagation.
For an IDR ai ← bj + c1 + c2 the constraint is represented as N × xid ≤
yj(d−1) + c1(d−1) + c2(d−1) , ∀d, 1 ≤ d ≤ tf . Here N is the number of minterms in
the IDR (in this example N = 3).
Constraint Set 3: We must also ensure that at time step tf at least ρ(|A| + |B|)
m
P
xi(tf ) +
entities fail. This can be captured by introducing the constraint
n
P

j=1

i=1

yj(tf ) ≥ ρ(|A| + |B|).

So with the objective in (1) and set of constraints the ILP finds the minimum
number of entities K ′ which when failed initially causes at least ρ(|A| + |B|)
entities to fail at tf .
5.2

Heuristic Solution for the RIDN problem

A heuristic solution for the RIDN problem is provided in this subsection. Along
with the definition of Kill Set, we introduce the notion of Total Minterm Hit Set
of an entity to design the heuristic. Before formal definition of Total Minterm
Hit Set we first define Minterm Hit Set of an entity as follows —
Definition 1. The Minterm Hit Set for an entity xj ∈ A ∪ B in an interdependent network I(A, B, F (A, B)) is denoted as M HS(xj ). M HS(xj ) contains the
set of all minterms that has the entity xj in it.
Definition 2. The Total Minterm Hit Set for an entity xj ∈ A ∪ B is denoted
as T M HS(xj ). It is defined as union of Minterm Hit Set of all entities in Cxj
(Kill Set of xj ).
Using these definitions a heuristic is formulated in Algorithm 2. For each
iteration of the while loop in the algorithm, the operational entity having highest
cardinality Kill Set is selected. This ensures that at each step the number of
entities failed is maximized. In case of a tie, the entity having highest cardinality
Total Minterm Hit Set among the set of tied entities is selected. This causes the
selection of the entity that has the potential to kill maximum number of entities
in the subsequent steps. Thus, the heuristic greedily minimizes the set of entities
which when killed initially fails at least ρ fraction of total entities in the IDN.
The heuristic overestimates the parameter K while determining the robustness
(K, ρ) of an IDN. The value of the parameter K is equal to |KH | − 1 which is the
output of Algorithm 2. Algorithm 2 runs in polynomial time, more specifically
the run time is ρn(n + m)2 (where n = |A| + |B| and m = Number of minterms
in F (A, B)).

Algorithm 2: RIDN Algorithm for IDNs with Case I type interdependencies

1
2
3
4
5
6
7
8
9
10
11
12

13
14
15
16

17

6

Data: An interdependent network I(A, B, F(A, B)) and a real valued
parameter ρ ∈ (0, 1].
Result: An integer |KH | − 1 where KH is a set of entities that when killed
initially fails at least ρ(|A| + |B|) entities
begin
Initialize D = ∅ and KH = ∅ ;
while |D| < ρ(|A| + |B|) do
For each entity xi ∈ (A ∪ B)\D compute the kill set Cxi ;
For each entity xi ∈ (A ∪ B)\D compute T M HS(xi );
Let xj be the entity having highest |Cxj | ;
if There exists multiple entities having highest cardinality Kill Set then
Let xp be an entity having highest T M HS(xp ) with xp in the set of
entities having highest cardinality Kill Set;
If there is a tie choose arbitrarily;
Add xp to set KH ;
Update D = D ∪ Cxp ;
Update F(A, B) by removing all IDRs corresponding to entities in
Cxp and all minterms in T M HS(xp );
else
Add xj to set KH ;
Update D = D ∪ Cxj ;
Update F(A, B) by removing all IDRs corresponding to entities in
Cxj and all minterms in T M HS(xj );
return |KH | − 1 ;

Experimental Results

We performed experimental comparison between the heuristic and the optimal
solution of the RIDN problem. Real world data sets were used for the experiments. The communication network data was obtained from GeoTel (www.geotel.com). The dataset contains 2, 690 cell towers, 7, 100 fiber-lit buildings and
42, 723 fiber links of Maricopa County, Arizona, USA. The power network data
was obtained from Platts (www.platts.com). It contains 70 power plants and 470
transmission lines of the same county. We took four non overlapping regions of
the Maricopa county. It is to be noted that the union of the regions does not
cover the entire space. The entities of the power and communication network for
these four regions were extracted. As per notation, set A and B contain entities
of the power and communication network respectively. The number of entities
in set A and B are 29 and 19 for Region 1, 29 and 20 for Region 2, 29 and 19 for
Region 3, and 33 and 20 for Region 4. The regions were represented by an interdependent network I(A, B, F (A, B)). For these regions F (A, B) was generated
using the IDR construction rule as defined in [13].

40
ILP solution
Heuristic

Robustness (K)

Robustness (K)

40
30
20
10
0

0

0.2

0.4

0.6

0.8

ILP solution
Heuristic

30
20
10
0

1

Minimum fraction of entities killed(ρ)

0

0.2

(a) Region 1

0.8

1

40
ILP solution
Heuristic

Robustness (K)

Robustness (K)

0.6

(b) Region 2

40
30
20
10
0

0.4

Minimum fraction of entities killed(ρ)

0

0.2

0.4

0.6

0.8

1

Minimum fraction of entities killed(ρ)

(c) Region 3

ILP solution
Heuristic

30
20
10
0

0

0.2

0.4

0.6

0.8

1

Minimum fraction of entities killed(ρ)

(d) Region 4

Fig. 1: Robustness parameter K returned by the optimal solution and the heuristic by varying parameter ρ for four regions in Maricopa County, Arizona, USA
IBM CPLEX Optimizer 12.5 is used to get the optimal solution using the
ILP. The simulation for the heuristic was done in Python 3. In a given region
the minimum fraction of entities killed (ρ) was varied from 0.02 to 1 in steps of
0.02. For each value of ρ the robustness parameter (K) was obtained from the
optimal solution and the heuristic. Figures 1a to 1d shows the result obtained
from the simulations for Region 1 to 4. It can be seen from the figures that the
heuristic solution performs almost same as the ILP till ρ = 0.42. For values of ρ
higher than 0.42 there results in an overestimation of the robustness parameter
K. The maximum overestimation is 3 for Region 1, 4 for Region 2 and 3 and 5
for Region 4.

7

Conclusion

In this paper we propose the Robustness problem in Multilayer Interdependent
Network. Robustness in an IDN is defined with respect to two parameters K
and ρ. The Implicative Interdependency Model is utilized to model the interdependency. The IIM model segregates the interdependency relations into four

different cases. Analysis of computational complexity of the Robustness problem
is done with respect to these four cases. For the general form of interdependency
relation, the problem is found to be NP-complete. The optimal solution for the
general case is obtained from an ILP. A heuristic is designed that returns an
overestimated K parameter value for a given ρ. Finally we compare the efficacy
of the heuristic with the optimal solution using real data set of Maricopa County,
Arizona. The heuristic produced optimal or near optimal solution for ρ < 0.42.

References
1. Banerjee, J., Das, A., Sen, A.: A survey of interdependency models for critical infrastructure networks. NATO Science for Peace and Security Series -D: Information
and Communication Security 37, 1–16 (2014)
2. Banerjee, J., Das, A., Zhou, C., Mazumder, A., Sen, A.: On the entity hardening problem in multi-layered interdependent network. IEEE Workshop on InterDependent Networks (2015)
3. Bernstein, A., Bienstock, D., Hay, D., Uzunoglu, M., Zussman, G.: Power grid
vulnerability to geographically correlated failures-analysis and control implications.
arXiv preprint arXiv:1206.1099 (2012)
4. Buldyrev, S.V., Parshani, R., Paul, G., Stanley, H.E., Havlin, S.: Catastrophic
cascade of failures in interdependent networks. Nature 464(7291), 1025–1028 (2010)
5. Castet, J.F., Saleh, J.H.: Interdependent multi-layer networks: Modeling and survivability analysis with applications to space-based networks. PloS one 8(4), e60402
(2013)
6. Das, A., Banerjee, J., Sen, A.: Root cause analysis of failures in interdependent
power-communication networks. In: Military Communications Conference (MILCOM), 2014 IEEE. pp. 910–915. IEEE (2014)
7. Gao, J., Buldyrev, S.V., Stanley, H.E., Havlin, S.: Networks formed from interdependent networks. Nature Physics 8(1), 40–48 (2011)
8. Hajiaghayi, M., Jain, K., Konwar, K., Lau, L., Mandoiu, I., Russell, A., Shvartsman, A., Vazirani, V.: The minimum k-colored subgraph problem in haplotyping
and dna primer selection. In: Proceedings of the International Workshop on Bioinformatics Research and Applications (IWBRA). Citeseer (2006)
9. Mazumder, A., Zhou, C., Das, A., Sen, A.: Progressive recovery from failure
in multi-layered interdependent network using a new model of interdependency.
In: Conference on Critical Information Infrastructures Security (CRITIS), 2014.
Springer (2014)
10. Nguyen, D.T., Shen, Y., Thai, M.T.: Detecting critical nodes in interdependent
power networks for vulnerability assessment (2013)
11. Parandehgheibi, M., Modiano, E.: Robustness of interdependent networks:
The case of communication networks and the power grid. arXiv preprint
arXiv:1304.0356 (2013)
12. Rosato, V., Issacharoff, L., Tiriticco, F., Meloni, S., Porcellinis, S., Setola, R.:
Modelling interdependent infrastructures using interacting dynamical models. International Journal of Critical Infrastructures 4(1), 63–79 (2008)
13. Sen, A., Mazumder, A., Banerjee, J., Das, A., Compton, R.: Identification of k most
vulnerable nodes in multi-layered network using a new model of interdependency.
In: Computer Communications Workshops (INFOCOM WKSHPS), 2014 IEEE
Conference on. pp. 831–836. IEEE (2014)

14. Shao, J., Buldyrev, S.V., Havlin, S., Stanley, H.E.: Cascade of failures in coupled
network systems with multiple support-dependence relations. Physical Review E
83(3), 036116 (2011)
15. Zhang, P., Peeta, S., Friesz, T.: Dynamic game theoretic model of multi-layer
infrastructure networks. Networks and Spatial Economics 5(2), 147–178 (2005)

1

On Connectivity of Airborne Networks
Shahrzad Shirazipourazad, Pavel Ghosh and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University

arXiv:1403.0982v1 [cs.NI] 5 Mar 2014

Email: {sshiraz1, pavel.ghosh, asen }@asu.edu

Abstract
Mobility pattern of nodes in a mobile network has significant impact on the connectivity properties
of the network. One such mobile network that has drawn attention of researchers in the past few years is
the Airborne Networks (AN) due to its importance in civil and military purpose and due to the several
complex issues in these domains. Since the nodes in an airborne network (AN) are heterogeneous
and mobile, the design of a reliable and robust AN is highly complex and challenging. In this paper a
persistent backbone based architecture for an AN has been considered where a set of airborne networking
platforms (ANPs - aircrafts, UAVs and satellites) form the backbone of the AN. End to end connectivity
of the backbone nodes is crucial in providing the communication among the hosts. Since ANPs are prone
to failure because of attacks like EMP attack or jamming, another important issue is to improve the
robustness of the backbone network against these attacks. Such attacks will impact specific geographic
regions at specific times and if an ANP is within the fault region during the time of attack, it will
fail. This paper focuses on connectivity and fault-tolerance issues in ANs and studies algorithms to
compute the minimum transmission range of ANPs in fault free and faulty scenarios to ensure network
connectivity all the times. It also considers the scenarios where the network may have to operate in a
disconnected mode for some part of time and data transmissions may be tolerant to some amount of
delay. Hence, ANPs may not need to have end-to-end paths all the time but they should be able to
transmit data to each other within bounded time.

I. I NTRODUCTION
An Airborne Network (AN) is a mobile ad hoc network that utilizes a heterogeneous set of
physical links (RF, Optical/Laser and SATCOM) to interconnect a set of terrestrial, space and
highly mobile airborne platforms (satellites, aircrafts and Unmanned Aerial Vehicles (UAVs)).
Airborne networks can benefit many civilian applications such as air-traffic control, border patrol,
and search and rescue missions. The design, development, deployment and management of a

2

Fig. 1.

A schematic view of an Airborne Network

network where the nodes are mobile are considerably more complex and challenging than a
network of static nodes. This is evident by the elusive promise of the Mobile Ad-Hoc Network
(MANET) technology where despite intense research activity over the previous years, mature
solutions are yet to emerge [1], [2]. One major challenge in the MANET environment is the
unpredictable movement pattern of the mobile nodes and its impact on the network structure.
In case of an AN, there exists considerable control over the movement pattern of the mobile
platforms. A senior Air Force official can specify the controlling parameters, such as the location,
flight path and speed of the ANPs, to realize an AN with desired functionalities. Such control
provides the designer with an opportunity to develop a topologically stable network, even when
the nodes of the network are highly mobile. It is increasingly being recognized in the networking
research community that the level of reliability needed for continuous operation of an AN may
be difficult to achieve through a completely mobile, infrastructure-less network [3]. In order
to enhance reliability and scalability of an AN, Milner et al. in [3] suggested the formation
of a backbone network with Airborne Networking Platforms (ANPs). In order to deal with the
reliability and scalability issues in an AN, we consider an architecture for an AN where a set of
ANPs form the backbone of the AN. This set of ANPs may be viewed as mobile base stations
with predictable and well-structured flight paths and the combat aircrafts on a mission as mobile
clients. We want that the backbone network remains connected all the times even though the
topology of the network changes with the movement of the ANPs. Network connectivity can be

3

easily achieved if the transmission range of the ANPs is very large. However large transmission
range also implies high power consumption. In order to minimize power consumption and hence
extend network lifetime, we would like to find the smallest transmission range to ensure network
connectivity. We define the critical transmission range (CT R) to be the minimum transmission
range of the ANPs to ensure that the dynamic network formed by the movement of the ANPs
remains connected at all times. We present an algorithm to compute CT R when the flight paths
are known. As a part of design of this algorithm, we develop techniques to compute the dynamic
topology of the AN at any instance of time.
Using CT R as the transmission range of all nodes, the network is connected as long as
all the network nodes (i.e., the ANPs) are operational. However, the ANPs are vulnerable to
Electromagnetic Pulse (EMP) attacks or jamming. Such an attack will impact specific geographic
regions at specific times and if an ANP is within the fault region during the time of attack, it
will not be able to carry out its normal communication functions. We will refer to these ANPs as
faulty nodes of the network. In this research, we also consider the AN scenario where some of
the network nodes are faulty. We consider faulty nodes are spatially correlated (or region-based),
that is faulty nodes due to an attack are confined to a region. We want that the network remains
connected irrespective of location of the fault region and the time of failure.
We define critical transmission range in faulty scenario (CT Rf ) to be the smallest transmission range necessary to ensure network connectivity, irrespective of (a) the location of the fault
region and (b) the time of the failure. We would like to find CT Rf . As a part of design of this
algorithm, we develop techniques to (i) compute all the fault regions that need to be considered
to ensure overall connectivity at all times, (ii) compute the set of dynamic nodes that might be
affected by the failure of a specific region at a specific time, and finally, (iii) compute CT Rf .
In previous problems the connectivity requirement is very strict and the backbone network is
needed to be connected all the times. However, it may not be possible to equip the ANPs with
radios with transmission range at least as large as the CT R. In such a scenario the network
may operate in a disconnected mode for some part of time. On the other hand, based on the
type of data that should be transmitted between ANPs, data transmissions may be tolerant to
some amount of delay. Hence, ANPs may not need to have end-to-end paths all the time but
they should be able to transmit data to each other within bounded time. These requirements lead
us to study the problem of computation of critical transmission range in delay tolerant airborne

4

networks. More specifically, the critical transmission range in delay tolerant network (CT RD )
is defined to be the minimum transmission range necessary to ensure that every pair of nodes in
the backbone network can communicate with each other within a bounded time. In this paper
we formulate CT RD and propose a solution to compute CT RD .
The rest of the paper is organized as follows. We discuss related work in section II. In
section III we present the AN architecture. We present dynamic topology computation of the
AN in section IV. In section V we present an algorithm to compute CT R in fault free scenario.
We discuss the faulty scenario and propose an algorithm to compute CT Rf in section VI.
Connectivity problem in delay tolerant airborne network is formulated and studied in section
VII. Experimental results are presented in section VIII. The section IX concludes the paper.
II. R ELATED W ORKS
Due to the Joint Aerial Layer Networking (JALN) activities of the U.S. Air Force, design of a
robust and resilient Airborne Network (AN) has received considerable attention in the networking
research community in recent years. It has been investigated that the flat ad hoc networks have
limitations with respect to data transmission, distance, interference and scalability [3], [4], [5].
Accordingly, [6], [3], [4] suggested the addition of a mobile wireless backbone of base stations
(analogous to cellular telephony or the Internet backbones), in which topologies and mobility
can be controlled for purposes of assured communications.
There exists considerable amount of studies on topology control using power control in
MANETs [7], [8], [9], [10], [4]. The goal of the proposed algorithms is to assign power values
to the nodes to keep the network connected while reducing the power consumption. The authors
of [7], [8] proposed distributed heuristics for power minimization in mobile ad hoc networks
and offer no guarantees on the worst case performance. Santi in [10] studied the minimum
transmission range required to ensure network connectivity in mobile ad hoc networks. He
proved that the critical transmission range for connectivity (CTR) is c

q

ln n
πn

for some constant

c where mobility model is obstacle free and nodes are allowed to move only within a certain
bounded area. In these studies the mobility patterns are not known unlike the problems studied
in this paper where it is assumed that the flight paths of ANPs are predictable. Moreover, this
research studies the computation of minimum transmission range in presence of region-based
faults and in delay tolerant scenario where it is not the case in previous studies.

5

In recent times, there has been considerable interest in studying localized, i.e., spatially
correlated or region-based faults in networks [11], [12], [13], [14]. In order to capture the
notion of locality in measuring the fault-tolerance capability of a network, a new variant of
connectivity metric called region-based connectivity was first introduced by Sen et. al. [11].
Region-based connectivity, for multiple spatially correlated faults, has been studied in [12]. The
region-based connectivity of a network can be informally defined to be the minimum number
of nodes that has to fail within any region of the network before it is disconnected. Neumayer
et. al [13] gave an analysis on identifying the most vulnerable parts of the network when the
faults are geographically correlated. That is, the analysis gives locations of disasters that would
have the maximum disruptive effect on the network in terms of capacity and connectivity. In [14]
Neumayer et. al. evaluates average two-terminal reliability of a fiber-optic network in polynomial
time under the presence of such geographically correlated faults. The networks studied in [11],
[12], [14], [13] are all static. However, ANs under study in this research are dynamic.
There may be times that the backbone network may have to operate in a disconnected mode.
The last few years have seen considerable interest in the networking research community in
delay tolerant networks (DTN) design [15]. The authors of [16] survey challenges in enhancing
the survivability of mobile wireless networks. This paper mentions that one of the aspects that
can significantly enhance network survivability is the design for end-to-end communication in
environments where the path from source to destination is not wholly available at any given
instant of time. In this design adjusting the transmit power of the nodes plays an important role.
Existing DTN research mainly focuses on routing problem in DTN [17], [18]. The paper [19]
provides a survey on routing algorithms for DTN. For the routing algorithms to be effective,
every pair of nodes should be able to communicate with each other over time. Therefore, the time
evolving DTN should be connected over time. Few papers [20], [21] have studied the problem of
topology control in DTNs. In these papers, the time evolving network is modeled by space-time
graph and it is assumed that the space-time graph is initially connected and the problem is to
find the minimum cost connected subgraph of the original graph.
These papers have not studied the computation of minimum transmission range of nodes in
DTN networks such that the time evolving network is connected over time and to the best of
our knowledge, this is the first paper that studies this problem.

6

III. S YSTEM M ODEL AND A RCHITECTURE
In the previous section, we argued that the level of reliability needed for continuous operation
of an AN may be difficult to achieve through a completely mobile, infrastructure-less network
and wherever possible a backbone network with Airborne Networking Platforms (ANPs) should
be formed to enhance reliability. In order to achieve this goal, we propose an architecture of an
AN where a set of ANPs form a backbone network and provide reliable communication services
to combat aircraft on a mission. In this architecture, the nodes of the backbone networks (ANPs)
may be viewed as mobile base stations with predictable and well-structured flight paths and the
combat aircrafts on a mission as mobile clients. A schematic diagram of this architecture is
shown in Fig. 1. In the diagram, the black aircrafts are the ANPs forming the infrastructure of
the AN (although in Fig. 1, only aircrafts are shown as ANPs, the UAVs and satellites can also
be considered as ANPs). We assume that the ANPs follow a circular flight path. The circular
flight paths of the ANPs and their coverage area (shaded spheres with ANPs at the center) are
also shown in Fig. 1. Thick dashed lines indicate the communication links between the ANPs.
The figure also shows three fighter aircrafts on a mission passing through space known as air
corridor, where network coverage is provided by ANPs 1 through 5. As the fighter aircrafts
move along their flight trajectories, they pass through the coverage area of multiple ANPs and
there is a smooth hand-off from one ANP to another when the fighter aircrafts move from the
coverage area of one ANP to that of another. At points P1, P2, P3, P4, P5 and P6 on their flight
path in Fig. 1, the fighter aircrafts are connected to the ANPs (4), (2, 4), (2, 3, 4), (3), (1, 3)
and (1), respectively.
In this paper, we make a simplifying assumption that two ANPs can communicate with each
other whenever the distance between them does not exceed the specified threshold (transmission
range of the on board transmitter). We are well aware of the fact that successful communication
between two airborne platforms depends not only on the distance between them, but also on
various other factors such as (i) the line of sight between the platforms [22], (ii) changes in
the atmospheric channel conditions due to turbulence, clouds and scattering, (iii) the banking
angle, the wing obstruction and the dead zone produced by the wake vortex of the aircraft [23]
and (iv) Doppler effect. Moreover, the transmission range of a link is not a constant and is
impacted by various factors, such as transmission power, receiver sensitivity, scattering loss over

7

altitude and range, path loss over propagation range, loss due to turbulence and the transmission
aperture size [23]. However, the distance between the ANPs remains a very important parameter
in determining whether communication between the ANPs can take place, and as the goal of this
research is to understand the basic and fundamental issues of designing an AN with twin invariant
properties of coverage and connectivity, we feel such simplifying assumptions are necessary and
justified. Once the fundamental issues of the problem are well understood, factors (i) - (iv) can
be incorporated into the model to obtain a more accurate solution.
For simplicity of analysis, we make two more assumptions. We assume that (i) all ANPs
are flying at the same altitude and (ii) they follow a circular flight path. The first assumption
allows us to reduce the problem from three dimension to two. However, none of these two
assumptions are critical and our analysis technique can easily be extended to scenarios where
the ANPs are not flying at the same altitude and they are not following a circular flight path. As a
consequence of assumption (i), we can view the n backbone nodes (ANPs) as moving points on
a 2 dimensional plane. Let (xi (t), yi (t)) be the coordinates of the node i at time t. The network
of flying ANPs gives rise to a dynamic graph G(t) = (V, E(t)) where V = {1, 2, . . . , n} is
the set of nodes indexed by the ANPs and E(t) is the set of edges at time t. There is an edge
between two nodes if their Euclidean distance, sij is less than the transmission range T r at time
t, i.e., E(t) = {(i, j)|sij (t) < T r}. It may be noted that the dynamic graph G(t) = (V, E(t)) is
completely defined by the following five controlling parameters.
1) a set of points {c1 , c2 , . . . , cn } on a two dimensional plane (representing the centers of
circular flight paths),
2) a set of radii {r1 , r2 , . . . , rn } representing the radii of circular flight paths,
3) a set of points {p1 , p2 , . . . , pn } representing the initial locations of the platforms
4) a set of velocities {v1 , v2 , . . . , vn } representing the speeds of the platforms, and
5) transmission range T r of the transceivers on the airborne platforms.
In next section we explain the computation of dynamic topology of graph G(t) = (V, E(t))
when all five controlling parameters are given.
IV. DYNAMIC T OPOLOGY C OMPUTATION
In this section we answer the following question. Given all five problem parameters including
the transmission range of the ANPs, how do you determine if the resulting dynamic graph is

8
y

ci
•

r!i (t)

y

•
i

s!ij (t)

! i (t)
R

r!ci

cj
•

i(0)
•

r!j (t)

θi (t)

! i (0)
R

•
j

r!cj
!j (t)
R

αci

r!ci

θj (t)

αcj

αci
x

O

θi (0)
x

O

(a)
Fig. 2.

βi

ci
•

(b)

~ i (t) and R
~j (t)) of
(a) Initial phase angle βi of point i; at time 0 point is shown as i(0), (b) Vector representations (R

two points i and j at time t moving along two circular orbits: rci = 15, rcj = 27, ∠ci Ox = αci =

π
,
3

∠cj Ox = αcj =

π
6

connected at all times?
Suppose that two ANPs, represented by two points i and j (either in two or in three dimensional
space, the two dimensional case corresponds to the scenario where the ANPs are flying at same
altitude) are moving along two circular orbits with centers at ci and cj with orbit radius ri and
rj as shown in Fig. 2(a) with velocities vi and vj (with corresponding angular velocities ωi and
1

ωj ), respectively.
~ i (t) directed from some origin point O,
A moving node i is specified by the radius vector R
~j (t) for point j. Therefore the distance sij (t) between the nodes i − j at time t
and similarly R
is given by:
~ i (t) − R
~ j (t))2 = R2 (t) + R2 (t) − 2R
~ i (t) · R
~ j (t)
s2ij (t) = (R
i
j
1

(1)

As mentioned earlier, we have assumed that the communication between the ANPs is possible if
and only if the Euclidean distance between them does not exceed the communication threshold
distance T r. This implies that the link between the nodes i and j is alive (or active) when
sij (t) ≤ T r

(2)

In the analysis that follows, we have assumed that ANPs are flying at the same altitude, i.e.,
we focus our attention to the two dimensional scenario. However, this analysis can easily be
extended to the three dimensional case to model the scenario where the ANPs are flying at

9

different altitude. In this case we can view the ANPs as points on a two-dimensional plane
moving along two circular orbits, as shown in Fig. 2(a). In Fig. 2(a), the vectors from the origin
O to the centers of the orbits ci and cj are given as r~ci and r~cj . The cartesian co-ordinates of the
centers can be readily obtained as r~ci = (rci cos αci , rci sin αci ) and r~cj = (rcj cos αcj , rcj sin αcj ).
~ i (t) can be expressed in polar coordinates: (Ri (t), θi (t)) with respect to origin
Accordingly, R
~j (t). The initial location of the points R
~ i (0)
point O, as shown in Fig. 2(a), and similarly for R
~j (0) are given. From Fig. 2(b), the phase angle βi for node i with respect to the center of
and R
orbit ci , can be calculated as (by taking projection on the axes):
tan βi =

Ri (0)cos θi (0) − rci cos αci
Ri (0)sin θi (0) − rci sin αci

(3)

~ i (t) = ~rc + ~ri (t)
R
i

(4)

From Fig. 2(a),

where ~ri (t) = (ri cos (βi + ωi t), ri sin (βi + ωi t)) (since angle made by i at time t w.r.t. ci is
given by (βi + ωi t)). Therefore, the angle between ~ri (t) and ~rci is (βi − αci + ωi t). Hence,
Ri2 (t) = rc2i + ri2 + 2rci ri cos (βi − αci + ωi t)

(5)

~ i (t) = ~rc + ~ri (t) on the x and y axes, we get
Now taking the projection of R
i
Ri (t) cos θi (t) = rci cos αci + ri cos (βi + ωi t),

(6)

Ri (t) sin θi (t) = rci sin αci + ri sin (βi + ωi t)

(7)

Recalling cos(A − B) = cos A cos B + sin A sin B, and simplifying we get
Ri (t)Rj (t) cos(θi (t) − θj (t)) = rci rcj cos αci cj + ri rj cos(βij + (ωi − ωj )t)
+ rci rj cos(αci − βj − ωj t) + rcj ri cos(αcj − βi − ωi t) (8)
where αcij = αci − αcj and βij = βi − βj . Combining equation 1 with equations 5 and 8, we
have:
s2ij (t)

=

rc2i + ri2 + 2rci ri cos(βi − αci + ωi t) + rc2j + rj2 + 2rcj rj cos(βj − αcj + ωj t)

−2[ rci rcj cos αci cj + ri rj cos(βij + (ωi − ωj )t)
+rci rj cos(αci − βj − ωj t) + rcj ri cos(αcj − βi − ωi t)]

(9)

10

Edge exists between i and j
Distance between nodes i and j

Distance Between Nodes i and j

20

15

10

5

0
0

50

100

150

200

250
Time

300

350

400

450

500

(a)
Fig. 3.

No edge between i and j

25

25

D = 24

20
D = 18

15

10

5
D=4

0
0

50

100

150

200

250
Time

300

350

400

450

500

(b)

Effect of the distance between nodes on the existence of the communication link between them; (a)Distance between

two points i and j as a function of time, (b)Active (Blue/Light gray)/Inactive (Red/ Dark gray) times of the link between i and
j with transmission range T r = 18

In equation 9, all parameters on the right hand side are known from the initial state of the
system, and thus the distance sij (t) between the nodes i − j at any time t can be obtained. If
the ANPs move at the same velocity, i.e., ωi = ωj = ω for all i, j and the radius of the circular
orbits are identical, i.e., ri = rj = r for all i, j, and the above expression simplifies to:
s2ij (t)

=

rc2i + r2 + 2rci r cos(βi − αci + ωt) + rc2j + r2 + 2rcj r cos(βj − αcj + ωt)

−2[ rci rcj cos αci cj + r2 cos βij
+rci r cos(αci − βj − ωt) + rcj r cos(αcj − βi − ωt)]

(10)

An example of a plot of equation (9) (generated using MATLAB) is shown in Fig. 3(a) with
communication threshold distance T r = 18. This implies that the link between the nodes i and
j exists, when the distance between them is at most 18 and the link does not exist otherwise.
This is shown in Fig. 3(b). The red(dark gray) part indicates the time interval when the link is
inactive(or dead) and the blue(light gray) part indicates when it is active (or live).
Thus using equation (9) and comparing the distance between any two nodes with the communication threshold T r, we can determine active/inactive times of all links. This can be represented
as intervals on a time line as shown in Fig. 4. By drawing projections from the end-points of the
active/inactive times of each link on the time line, we can find out all the links that are active
during an interval on time line. As shown in Fig. 4, links 1, 2 and 3 are active in interval 1;
links 1 and 3 are active in interval 2, links 1, 2 and 3 are active in interval 3 and so on. Once
we know all the links that are active during a time interval, we can determine if the graph is

11
Links Active

Links Dead

Link 1:
Link 2:

Link 3:

Timeline

Fig. 4.

1

2

3 4 5

6

7 8

9

10

11 12 13 14 15 16 17

Active/Inactive time interval of each link and interval intersection projections on the time line

connected during that interval using any algorithm for computing graph connectivity [24]. By
checking if the graph is connected at all intervals, we can determine if the graph is connected
at all times, when the ANPs are moving at specified velocities.
We note that based on equation (9), sij is periodic if every pair of velocities ωi and ωj are
commensurate, i.e. ωi /ωj is a rational number [25]. Therefore, the network topologies will be
repeated periodically and it is enough to check network connectivity in one period.
If the problem parameters (1) through (5) are specified, we can check if the dynamic graph
is connected at all times following these two steps. In the first step, we determine the lifetime
(active/inactive intervals) of every link between every pair of nodes i and j by comparing sij (t)
with T r and finding the time points that the state of a link changes. Let L(T r) = {e1 , e2 , . . . , el }
denote the set of events ei s that state of a link changes when transmission range is T r; L(T r)
is sorted in increasing order of the time of the events. Hence, between two consecutive events
ei and ei+1 that happen at times ti and ti+1 the set of active links is unchanged. Algorithm 1
shows the details of computing L(T r). In the second step we check the graph connectivity in
each interval [ti , ti+1 ) for all 0 ≤ i ≤ l − 1 using connectivity checking algorithm in [26]. t0
shows current time (starting point). Step 2 is described in detail in Algorithm 2.

12

Algorithm 1 Link Lifetime Computation
Input: (i) a set of points {c1 , c2 , . . . , cn } representing the centers of circular flight paths, (ii) a set of radii
{r1 , r2 , . . . , rn } representing the radii of circular flight paths, (iii) a set of points {p1 , p2 , . . . , pn } representing the
initial locations of the platforms, (iv) a set of velocities {v1 , v2 , . . . , vn } representing the speeds of the platforms.
Output: L(T r): an ordered set of events that the state of a link changes from active to inactive or inactive to
active.
1: L(T r) ← ∅
2: for all pairs i, j do

Compute l to be the set of time points t such that sij (t) = T r (equation 9) over a period of time, to find

3:

the instances of times t where the state of the link (i, j) changes. If sij (t) = T r and is sij (t) increasing at
t, it implies that the link dies at t, and if sij (t) decreasing at t, it implies that the link becomes active at t.
4:
5:

for all lk ∈ l do
Find the position of lk in L(T r) using binary search and Add the event into L(T r). (L(T r) is sorted in
increasing order)

6:

end for

7: end for

Algorithm 2 Checking Connectivity of Airborne Network
Input: L(T r)
Output: true if graph is connected all the time; otherwise false.
1: for all li ∈ L(T r) do
2:

Check if the AN graph is connected with the set of live links during interval [li , li+1 ). This can be done
with the connectivity testing algorithm in [26]

3:

if AN graph is not connected, return f alse

4: end for
5: return true

Let n be the number of ANPs. The first loop of Algorithm 1 is executed for O(n2 ) times. The
number of iterations of the inner loop depends on the number of the solutions of sij (t) = T r. For
the case that ANPs move at the same velocity, i.e., ωi = ωj = ω it is obvious that equation (10) is
periodic and length of one periodic interval is 2π/ω. So, it is enough to execute Algorithm 1 for
one period [t0 , t0 + 2π/ω). In this case, equation (10) can be written as A cos(ωt) + B sin(ωt) =
√
A2 + B 2 sin(φ + ωt) where A, B and φ are constants and can easily be obtained from equation

13

(9). In this case, the equation sij (t) = T r can have at most two solutions and the solutions
can be found in constant time. Therefore, for every link, the timeline is divided into at most
three segments in one period and the size of the set of intervals, |L(T r)| is O(n2 ); also, the

time complexity of the binary search is O(log n2 ). So, the total time complexity of Algorithm
1 is O(n2 log n). Even when the velocities of the ANPs are different, sij remains periodic if
every pair of velocities ωi and ωj are commensurate, i.e. ωi /ωj is a rational number [25]. In
this case also we need to solve sij (t) = T r for one period only. Otherwise, equation (9) is not
periodic and we need to consider a period of time between t0 and finish time tf and find the

solutions in that period. For the sake of simplicity, in this paper we assume that the ANPs move
at the same speed. The running time of connectivity testing algorithm in [26] is O(n2 ). Also,
as |L(T r)| = O(n2 ) time complexity of Algorithm 2 is O(n4 ).
A. Predictable Ill-Structured Flight Path
In this subsection we consider ANPs following predictable ill-structured flight paths. Predictable Ill-structured flight paths are defined as pre-defined equations in the 3D space (or in
2D space, in case the aircrafts are all moving at the same altitude). Using the same assumption
as in the earlier sections, we assume that there exists a communication link between two nodes
in such an AN if they are within the specified threshold distance D from each other. Positions
of nodes of the network at any time can be found by using the parametric representation as
Ä

ä

r~i (t) = xi (t), yi (t), zi (t) , for each node i = 1, 2, . . . , N , where t is the time and xi (t), yi (t),
and zi (t) represent the x, y and z co-ordinates of the node in the 3D space at time t. Using
the analysis described in section IV, we can compute the link lifetimes for all pairs of nodes.
Then, using the similar techniques as described in section IV we can check the connectivity of
the dynamic graph formed by the moving ANPs.
V. C OMPUTATION OF C RITICAL T RANSMISSION R ANGE IN FAULT F REE S CENARIO
It is conceivable that even if the network topology changes due to movement of the nodes,
some underlying structural properties of the network may still remain invariant. A structural
property of prime interest in this context is the connectivity of the dynamic graph formed by the
ANPs. We want the ANPs to fly in such a way, that even though the links between them are
established and disestablished over time, the underlying graph remains connected at all times.

14

We define critical transmission range (CTR) to be the smallest transmission range necessary to
ensure network graph G(t) is always connected. We would like to determine CT R. In this case,
the problem will be specified in the following way. Given controlling parameters 1, 2, 3 and 4,
what is the minimum transmission range of the ANPs so that the resulting graph is connected
at all times?
In the previous section we explained how we check network connectivity when all five
parameters are given. The maximum transmission range of an ANP T rmax is known in advance.
In order to compute CT R we can conduct a binary search within the range 0 − T rmax and we
can determine the smallest transmission range that will ensure a connected AN during the entire
operational time when all other problem parameters have already been determined. The binary
search adds a factor of log T rmax to the complexity of Algorithms 1 and 2.
VI. C OMPUTATION OF C RITICAL T RANSMISSION R ANGE IN FAULTY S CENARIO
The CTR computed in previous section may not guarantee the connectivity of backbone
network when some of ANPs fail. In this section, we consider the AN scenario where some of the
network nodes are faulty and we compute critical transmission range in faulty scenario(CTRf )
which is defined to be the smallest transmission range necessary to ensure network connectivity,
irrespective of (a) the location of the fault region and (b) the time of the failure. First we describe
the fault model used in this paper. Also, we identify the challenges that one has to confront, in
order to find the CTRf .
A. Fault Model
As we mentioned before, our focus is on spatially correlated (or region-based) faults such as
Electromagnetic Pulse (EMP) attacks or jamming. Spatially correlated or region-based faults
imply that the faulty nodes due to an attack are confined to a geographic area. In a two
dimensional deployment area, a region can be viewed as a circular area with radius R (in
three dimensional space it can be viewed as a sphere with radius R). In our model, when a
region is under attack and consequently fails at time t, some or all the ANPs within that region
at time t also fail. In this version of the model, we also make an assumption that only one region
can fail at any one time. Fig. 5 shows five ANPs moving on a two dimensional plane and a
faulty region (red circle, centered at point P ) at time t. Since ANPs 4 and 5 are within the fault

15

region at time t, we assume that these nodes are damaged and no longer can be viewed as part
of the backbone network. It may be noted that both the location of the center of the fault circle,
P , as well as the time of attack, t, play a critical role in determining the impact of the attack
on the backbone network.
ANP2
ANP1 c1

c2

c5
ANP3

ANP5

P

ANP4
c3

Fig. 5.

c4

ANPs on a circular flight path on a 2D-plane with a fault region

B. Problem Formulation and Design Challenges
In faulty scenario the connectivity problem is defined in the following way: Given the controlling parameters 1, 2, 3 and 4 (defined in Section III) as well as the radius of a region R,
what is the smallest transmission range necessary to ensure network connectivity, irrespective of
(a) the location of the fault region and (b) the time of the failure. In other words, the problem
is how to compute CTRf .
One can easily recognize the complexity of the problem by noting that potentially there could
be an infinite number of locations for point P and infinite choices for attack time t. In our
analysis we show that although there could be an infinite number of choices of P and t, we
need to consider only a small subset of them to correctly determine CTRf . The tasks that need
to be performed before a solution to the problem is found can be listed as follows:
•

Computation and comprehension of the dynamic topology of the backbone network (in a
fault-free scenario) as it changes with movements of the ANPs.

•

How many regions (locations of point P ) and instances of attack time t should be considered?

•

How to determine the ANPs that are damaged when an attack takes place in location P at
time t?

16

In section IV we described the computation of the dynamic topology of the backbone network
(in a fault-free scenario). In the following subsections we describe our techniques to deal with
the second and third challenges and to compute CTRf .
C. Regions to Examine
The authors in [11] introduced the notion of region-based faults and introduced a new metric,
region-based connectivity, to measure the fault-tolerance capability of a network under the regionbased fault model. Region-based connectivity of a network is defined to be the minimum number
of nodes that has to fail in any region of the network before it is disconnected. In this study,
a region is defined to be a circle of radius R. With this definition of a region, the number of
potential regions could be infinite. The authors in [11] proved that in a static wireless network,
only a limited number of distinct regions need to be examined to compute the region-based
connectivity. They showed that it is enough to consider the regions centered at the intersection
points of the circles centered at the nodes with radius R. Although the AN is dynamic, if we
take a snapshot of the network at some instance of time t, the AN can be viewed as a static
network with a specific topology and nodes in specific locations on the plane. The vulnerability
zone of a node i, V Zi (t), is defined to be a circular region centered at the location of node i
at time t with radius R. The motivation for this definition of the vulnerability zone of node i is
the following. If the center of the fault region is within the vulnerability zone of ANPi (node i),
then the ANPi is likely to be damaged. The vulnerability zones of ANPs are shown in Fig. 6.
Since there is no discernible difference between a static sensor network considered in [11] and
a snapshot of an AN at a specific instance of time t, using the analysis presented in [11], we
can conclude that it is enough to examine only the regions centered at the intersection points
(I-points) of the vulnerability zones of the ANPs. The vulnerability zones of two ANPs and
their intersection points are shown in Fig. 6. If a V Zi does not have intersection with any other
node’s vulnerability zone, an I-point is considered at the location of the node i.
Since the ANPs are mobile, the location of the intersection points of their vulnerability
zones also changes with time. Each pair of ANPs will have at most two intersection points.
Since there are only n(n − 1)/2 pairs of nodes, at most n(n − 1) intersections points can
exist at any given time (it may be noted that depending on flight path of a pair of ANPs,
their vulnerability zones may never intersect). We define a set of n(n − 1) + n I-points, I =

17

1
2
1
2
1
2
1
2
{I(1,2)
, I(1,2)
, I(1,3)
, I(1,3)
, . . . , I(n−1,n)
, I(n−1,n)
, I1 , I2 , . . . , In }, where the I(i,j)
and I(i,j)
are the

1
intersection points of the vulnerability zones V Zi and V Zj . We will use the notation I(i,j)
(t)
2
1
2
and I(i,j)
(t) to denote the locations of I(i,j)
and I(i,j)
at time t. Similarly, Ii (t) will denote the

location of node i at time t. Based on the results presented in [11], it is known that at any point
of time t it is sufficient to examine only the regions centered at the I-points in I. In the rest of

1
2
the paper we will use I(i,j) to denote both I(i,j)
or I(i,j)
.

For every two nodes i and j, V Zi (t) and V Zj (t) intersect iff sij (t) ≤ 2R. In this case we
say that the region centered at intersection point I(i,j) exists at time t; otherwise, it does not,
i.e., there exists no region that can cover both nodes i and j at time t. It may be noted that
due to the mobility of the ANPs, I(i,j) may exist at some point of time t and may not exist
at some other point of time t0 . By checking the condition sij (t) ≤ 2R, we can determine the
intervals on the timeline when I(i,j) exists for each pair of nodes i and j; i.e., we can compute
k
existence intervals of each I-point on the timeline. Let T (f ) = {(t1f , t2f ), . . . , (tk−1
f , tf )} be the

set of existence intervals of I-point f ∈ I where the first element in every pair (tjf , tj+1
f ) is

the start time and the second one is the finish time of the j- th existence interval. If in a time
interval (tjIi , tj+1
Ii ), V Zi does not have intersection with any other ANP’s vulnerability zone then
a region centered at Ii should be considered, i.e, (tjIi , tj+1
Ii ) ∈ T (Ii ). Without loss of generality
we can assume that the region centered at the point Ii exists all the time and it only covers
node i. The computation of the intervals on the timeline when I(i,j) exists (or does not exist),
for each pair of nodes i and j, can be carried out by an algorithm similar to Alg. 1 presented
earlier. The only differences are (i) the value of t that satisfies the equation sij (t) = 2R should
be computed instead of the value of t that satisfies the equation sij (t) = T r, (ii) since there is
no need to combine existence interval information of one pair of nodes (I(i,j) ) with another pair,
the binary search in step 5 of Alg. 1 is not needed.
D. Computation of the Damaged ANPs in a Fault Region
After finding the existence intervals of I-points we want to find the set of nodes that might
be damaged by the failure of a region centered at an I-point when it exists. A node might be
damaged by failure of a region if the Euclidean distance between the center of the region and
the node is less than R. As explained in part VI-C the regions centered at I-point Ii ∈ Is only
can destroy node i. Since, we know the locations of each pair of nodes i and j at time t, we can

18

orbit
Iij1

j

center of orbit
ci

VZj

cj

i
VZi

Fig. 6.

R

Iij2

1
2
Iij
and Iij
are intersection points of V Zi and V Zj at time t.

1
2
compute V Zi (t) and V Zj (t), and hence I(i,j)
(t) and I(i,j)
(t), the intersection points of V Zi (t)

and V Zj (t) at time t.
Once the location of each intersection I(i,j) in its existence intervals ∈ T (I(i,j) ) are known,
we can find the nodes that might be damaged if the region centered at I(i,j) fails. For ease
of notation we denote the set of I-points ∈ I as F = {f1 , f2 , . . . , fl }. Dik (t) denotes the
distance between I-point fi ∈ F and node k at time t. For every I-point fi ∈ F in its existence
interval ∈ T (fi ), we find Dik (t) for all k ∈ V . Since we know the position of the nodes
and I-points at any point of time, Dik (t) can be computed easily. If Dik (t) ≤ R, then the
node k may be damaged due to the region failure fi . From this calculation, we can find out
the time interval when node k is vulnerable to a failure fi . In other words, we can find out
the time intervals when a node k is covered by the region centered at fi (i.e., Dik (t) ≤ R).
It may be noted that this time interval will be subinterval of the intersection points existence
time interval. Accordingly, every existence interval (tjfi , tj+1
fi ) ∈ T (fi ) is divided into a set of
smaller subintervals such that each of these intervals identify a specific set of nodes that may
be damaged if the region centered at fi fails. Suppose that tm be the mth interval of T (fi ).
We define a set N T (fi , tm ) = {(tm1 , Nm1 ), (tm2 , Nm2 ), . . . , (tmj , Nmj )}as the set of subintervals
into which tm is divided, where tmj denotes the start time of the jth subinterval of tm where
at least a node enters the region or leaves the region and Nmj denotes the set of nodes within
the region centered at fi in its jth subinterval. We need to compute N T (fi , tm ) for every region
fi ∈ F and for all of its existence intervals. Based on N T (fi , tm ) we can draw a timeline,
region-coverage timeline for each region centered at an I-point fi ∈ F . Fig. 7 shows an example
in which N T (f1 , t1 ) = {(t11 , {1, 2}), (t12 , {1, 2, 3}), (t13 , {1, 2})}.

19

N12 =N21={1,2,3}

N23={1,2,4}

ANP4
ANP3
ANP2
ANP1

f1=I(1,2)

Fig. 7.

t2

t1
t11

t12

Timeline
t13

t14

t21 t22 t23

t24

Region coverage timeline of the region centered at f1 = I(1,2) ; The first timeline shows the availability intervals of

f1 ; i.e, T (f1 ) = {t1 , t2 }.

E. Computation of Critical Transmission Range in Faulty Scenario (CTRf )
In this section we propose an algorithm to find CTRf .
The transmission range T r is one of the parameters that determines the number of active
links at any given time. Similarly, the location of the center of the fault region is one of the
parameters that determines the number of ANPs that can potentially be damaged by the fault.
For a specific region centered at an I-point fi , and a transmission range T r, we define an interval
on the timeline as static interval, if the set of potentially damaged nodes due to a region fault at
location fi and the set of alive links with transmission range T r remain unchanged. We can find
static intervals using the timeline region-coverage(fi ) and the timeline link-lifetime L(T r). In
order to find the static intervals for I-point fi and transmission range T r, we define four events
during the time interval when fi exists: (i) a dead link comes alive, (ii) a live link dies, (iii) an
ANP node comes within coverage area of fi and (iv) an ANP node moves out of the coverage
area of fi . The instance of time at which any of the four events takes place is the instance of
the start time of a new static interval. Let SI(fi , T r) be a sorted list of events resulting from
combining the sorted list L(T r) and N T (fi , tm ) for all tm ∈ T (fi ). Therefore, between any two
consecutive elements in SI(fi , T r) neither the topology nor the region coverage changes.
Once the nodes within a region (or nodes covered by a region) and the set of active links
during a static interval are known, we can use Algorithm-2 in [11] in order to find the region
based connectivity of the network with respect to I-point fi . Region based connectivity with
respect to an I-point fi (RBC(fi )) is defined to be the minimum number of nodes in the region
centered at fi whose failure disconnects the network. If the number of nodes that can be damaged

20

due to a region based fault at fi is ni (i.e., the fault at fi covers ni nodes), we would like the
ANPs to have enough transmission range, so that the region based connectivity of the graph is at
least ni + 1. This will ensure that the network will remain connected if any subset of the covered
nodes fails. Using Algorithm-2 of [11], and applying binary search within the range 0 − T rmax
we can find the minimum transmission range necessary in each static interval, to ensure that
the network remains connected when a region fi fails (during the interval when it fi is exists).
We define err to be the maximum acceptable difference between the smallest transmission
range necessary to maintain connectivity and the smallest transmission range computed by the
algorithm to maintain connectivity. In our algorithm, we set the maximum possible transmission
range to be equal to diameter of the deployment area. The algorithm computes the minimum
transmission range necessary to maintain connectivity for each static interval. The maximum of
these minimum values computed is the critical transmission range (CTRf ). Alg. 3 provides all
the details.
In Alg. 3, line 1 takes O(n2 ). In order to compute T (fi ) we need to solve sij = 2R. As
described in Alg. 1, for the case that ANPs move at the same velocity, ω, this equation can
be solved easily in constant time and it has two solutions in one period. So, |T (fi )| ≤ 2. In
line 5, we have to solve Div (t) = R for all v ∈ V . For one node v, this equation also in one
period can have a constant number of solutions since it can easily be converted to a single
variable polynomial equation with degree 6. So, computation of N T (fi , tm ) takes O(n) and
P

tm

|N T (fi , tm )| = O(n). Consequently, lines 2-7 have complexity of O(n3 ). The while loop

is repeated for log T rmax (binary search complexity). As it is discussed in Alg. 1, computation
of L(T r) takes O(n2 log n). Computation of SI(fi , T r) need sorting the sorted lists N T (fi , tm )
and L(T r) which takes O(n2 ). Clearly, |SI(fi , T r)| = O(n2 ). Computing RBC(fi ) takes O(n4 )
[11]. Therefore, the time complexity of Alg. 3 is O(n8 log T rmax ).

VII. C OMPUTATION OF C RITICAL T RANSMISSION R ANGE IN D ELAY T OLERANT A IRBORNE
N ETWORKS CTRD
In previous sections we explained the computation of critical transmission range in fault free
(CTR) and faulty scenarios (CTRf ). However, it may not be possible to equip the ANPs with
radios that have coverage of radius CTR. Therefore, the backbone network cannot be connected
all the times. On the other hand, based on the type of data that should be transmitted between

21

Algorithm 3 Computing CT Rf
1: Compute sij (t) for all pair of ANPs i and j
2: for all I-points fi ∈ I
3:

Compute T (fi ) = {(t1f , t2f ), . . . , (tk−1
, tkf )}
f

4:

for all tm ∈ T (fi )

Compute N T (fi , tm ) = {(tm1 , Nm1 ), . . . , (tmp , Nmp )}

5:

6: error = T rmax , tra = 0, trb = T rmin = T rmax
7: while error > err
8:

error = error/2, T r = (tra + trb )/2

9:

Find L(T r) = {e1 , e2 , . . . , et } using Alg. 1

10:
11:

for all I-points fi ∈ I
SI(fi , T r) ← Sort the lists N T (fi , tm ) and L(T r) based on time
of the events (considering all tm ∈ T (fi ))

12:
13:

for all event ∈ SI(fi , T r)
Update the graph G(t) (by adding or removing the links) or the
region coverage of fi

14:

RBC(fi ) ← Using Alg. 2 in [11] Compute the region-based
connectivity considering only one region centered at I-point fi

15:

if (RBC(fi ) ≥ ni + 1) NextSI ← TRUE

16:

else NextSI ← FALSE; break;

17:

if (NextSI = FALSE) tra = T r; break;

18:

if (NextSI = TRUE) trb = T r; T rmin = T r

19: return T rmin

ANPs, data transmissions may be tolerant to some amount of delay. Hence, ANPs may not
be needed to have end-to-end paths all the times but they should be able to transmit data to
each other in some limited time through intermediate nodes in different network topologies. In
this section we investigate the problem of computation of minimum transmission range in delay
tolerant airborne networks.
We consider that the trajectories and the distance function sij (t) of the nodes are periodic over
time. As a consequence, the network topologies are repeated periodically. However, periodicity
is not an underlying assumption and our results can be utilized in non-periodic scenario as long
as the node trajectories for the whole operational duration of a network are given. In Section IV
we explained how we can compute link lifetime timeline and accordingly the network topologies
caused by ANPs mobility in a time period when all five controlling parameters are given. We

22

represent the set of topologies in a periodic cycle starting from time t0 (starting time of network
operation) by the set G = {G1 , G2 , . . . , Gl }. Each network topology Gi exists for a time duration
of Ti . As the focus of this section is study of the delay caused by network disconnection (which
may be viewed as delay due to queuing at an intermediate node), we assume that other delays
due to transmission and propagation are negligible.
In Fig. 8, an example of a dynamic graph with two topologies G1 and G2 in one periodic
cycle is shown. G1 and G2 last for T1 and T2 time units respectively. It can be observed
that there is no end-to-end path from A to C in either G1 or G2 . However, A can transmit
data to B in G1 , and B can forward it to C in G2 . In this case we say that A can reach
C through a temporal path with delay equal to the lifetime of G1 , i.e. T1 ; and the temporal
path is completed in G2 . We define a temporal path from node s to d to be a set of tuples
{(t1 , (v1 , v2 )), (t2 , (v2 , v3 )), . . . , (tk , (vk , vk+1 ))} such that v1 = s, vk+1 = d, vi ∈ V and for
every tuple (ti , (vi , vi+1 )) , edge (vi , vi+1 ) is active at time ti , and ti ≥ ti−1 for all 1 ≤ i ≤ k.
Moreover, without loss of generality, we assume that ti corresponds to the starting time of a
topology in G. Then the path delay is defined to be tk − t0 where t0 is the starting time of G1 in
the first periodic cycle. We note that all path delays are computed with respect to starting point
t0 but we later show that we can modify the starting point to any time.

A

Fig. 8.

B

A

B

C

C

G1

G2

A dynamic graph with two topologies G1 and G2

We note that existence of a path from node i to j with some delay does not guarantee the
existence of a path from j to i with the same delay. For example, in Fig. 8 the path from C to A
has a delay of T1 + T2 while the path delay from A to C is equal to T1 . We say that a dynamic
graph G(t) is connected with delay D if there exists a temporal path from every node i ∈ V to
every node j ∈ V − {i} with delay smaller than D. In a network, if the transmission range T r
is too small, ANPs may not be able to reach each other at all; i.e. there is no temporal path of
finite delay between the ANPs. We define critical transmission range in delay tolerant network

23

(CTRD ) to be the minimum transmission range necessary to ensure that the dynamic graph is
connected with delay D. We define the connectivity problem in delay tolerant networks as the
problem of computation of CTRD given the first four controlling parameters defined in Section
III, and the delay threshold D.
In order to find the value of CTRD , first we explain an algorithm to check whether a
transmission range T r is adequate for having a connected dynamic network with delay D.
Using Algorithm 1 in Section IV we can compute the different network topologies and their
lifetime in one periodic cycle. Before describing the rest of the algorithm, first we propose an
observation.
Observation 1. For a given transmission range T r, there is a temporal path from every node
u to every node v with finite delay iff the superimposed graph Gc = {V,

Sl

i=1

Ei }, where Ei is

the set of edges in Gi , is connected.
Although a transmission range T r may be enough to result in a connected superimposed
graph Gc , it may not be sufficient for the existence of a temporal path between every pair of
nodes with delay smaller than a threshold D even if D is as large as

Pl−1

i=1

Ti . Fig. 9 depicts an

AN with three topologies in one period. It can be observed that A cannot have a temporal path
from A to D in the first period. Actually the fastest path includes edges (A, B) in G3 in first
period, (B, C) in G2 in the second period and (C, D) in G1 in the third period. Therefore, the
path delay is 2(T1 + T2 + T3 ). Generally, in the worst case in every period just a subpath (a set
of consecutive edges) in one topology is used and therefore the maximum delay of a temporal
path will be Dmax = (l − 1)

Pl

i=1

Ti . Hence, if D ≥ Dmax , examining the connectivity of Gc is

enough to decide whether for a transmission range there exists a temporal path of delay smaller
than D between every pair of nodes in the dynamic network.

Next, we explain the algorithm that checks for a given value of transmission range T r whether
a network is connected with delay D where D < Dmax . Let N (u) denotes the set of nodes that
are reachable from u ∈ V with delay smaller than D. Initially N (u) = {u}. The algorithm starts
by computing the connected components in every topology Gi . Let Ci = {Ci,1 , Ci,2 , . . . Ci,qi }
represents the set of connected components in Gi where Ci,j is the set of nodes in jth component

24

A

B

A

B

A

B

D

C

D

C

D

C

G1
Fig. 9.

G2

G3

A dynamic graph with three topologies G1 , G2 and G3

of Gi and qi = |Ci |. Let g and h be the quotient and remainder of PlD

i=1

Ti

respectively, and t0 +h

is the time where the network topology is Gp for a p, 1 ≤ p ≤ l. Therefore, the topologies in
time duration t0 to t0 +D includes G1 to Gl for g number of cycles and G1 to Gp in last periodic
cycle. Starting from first topology G1 in first period, in each topology Gi , if a node v is in the
same connected component with a node w ∈ N (u), then v can be reachable from u through a
temporal path which is completed in Gi ; hence, N (u) is updated to N (u) ∪ (

S

k:N (u)∩Cik 6=∅

Cik ).

In this step the algorithm goes through all the topologies from t0 to t0 + D. In the end, if
N (u) = V for all u ∈ V then the transmission range T r is sufficient for having a connected
network with delay D. In Algorithm 4 the steps of checking the connectivity of a dynamic graph
with delay D is proposed.

25

Algorithm 4 Checking Connectivity of Airborne Network with delay D
Input: G(t) = {G1 , G2 , . . . , Gl } and delay threshold D
Output: true if dynamic graph G(t) is connected with delay D; otherwise false.
1:

Initialize N (u) = {u} for every u ∈ V

2:

for all topologies Gi , 1 ≤ i ≤ l

3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

Compute Ci = {Ci,1 , Ci,2 , . . . Ci,qi }, the set of connected components of Gi
for all periods 1 to g
for all topologies Gi , 1 ≤ i ≤ l
for all node u ∈ V
N (u) ← N (u) ∪ (

S

k:N (u)∩Ci,k 6=∅

Ci,k )

for all topologies Gi , 1 ≤ i ≤ p (the topologies in the last period)
for all node u ∈ V
N (u) ← N (u) ∪ (
for all node u ∈ V

[

Ci,k )

k:N (u)∩Ci,k 6=∅

if N (u) 6= V , return f alse
return true

As we explained in section IV, number of topologies, l in one period is O(n2 ). The computation
of the connected components of a graph Gi = (V, Ei ) needs using either breadth-first search or
depth-first search with time complexity of O(|V | + |Ei |) = O(n2 ). Hence, step 2-4 takes O(n4 ).
This algorithm is used for the case that D < (l − 1)

Pl

i=1

Ti . Therefore, number of periods

g < l − 1 and g = O(n2 ). Computation of Step 7 also needs O(n2 ) since |N (u)| and total size
of all components in Gi is O(n). Finally, we can conclude that total time complexity of the
algorithm is O(n7 ).
As we mentioned before, in Algorithm 4 the delays are computed with respect to t0 . We can
easily extend it to any time in the network operation duration, by repeating Algorithm 4 for
every ti , 1 ≤ i ≤ l where ti is the starting time of topology Gi . The complexity increases by a

factor of l = O(n2 ). We note that in this case even if a node starts communication at some time

instances t where ti ≤ t ≤ ti+1 , the delay will be smaller than the case it starts at ti . Hence it
is enough to just consider the time points in which a topology change happens.
Similar to the computation of CTR and CTRf , in order to compute CTRD we can conduct a

26

binary search within the range 0 − T rmax and we can determine the smallest transmission range
that will ensure the AN is connected with delay D during the entire operational time. The binary
search adds a factor of log T rmax to the complexity of Algorithm 4.
VIII. S IMULATIONS
The goal of our simulation is to compare critical transmission range in different scenarios of
non faulty, faulty and delay tolerant and investigate the impact of various parameters, such as the
number of ANPs, the region radius and delay on critical transmission range. In our simulation
environment, the deployment area is a 1000 × 1000 square mile area. The centers of the orbits
of the ANPs are chosen randomly in such a way that the orbits do not intersect with each other.
In our simulation, we assume that all the ANPs move at the same angular speed of ω = 20
radian/hour. Hence a period length is 0.1π hour. One interesting point to note is that, in this
environment where all the ANPs are moving at the same angular speed on circular paths, the
value of CT R is independent of the speed of movement of the ANPs. This is true because
changing the angular speed ω effects just the time at which the events, such as a link becomes
active or a link dies, take place. If we view the dynamic topology of the backbone network
over one time period as a collection of topologies G = {G1 , G2 , . . . , Gl }, where Gi morphs into
Gi+1 , 1 ≤ i ≤ l at some time, by increasing or decreasing the angular speed of all the ANPs, we
just make the transitions from Gi to Gi+1 faster or slower, without changing the topology set
G. Similarly, the set of ANPs that are damaged to failure of a region at a certain time, remains
unchanged.
In our first set of experiments we compute CT R, CT Rf when R = 20, 60, and CT RD when
D = 0.5period, 2period for different values of number of nodes, n. Fig. 10 depicts the result of
these experiments. In these experiments, for each value of n we conducted 30 experiments and
the results are averaged over the 30 different random initial setups. We set orbit radius = 10. We
observe that expectedly an increase in the number of nodes results in a decrease in CT R, CT Rf
and CT RD . Moreover, CT RD ≤ CT R ≤ CT Rf for all instances. In all of the experiments, we
compute CT RD with respect to all times (corresponding to beginning of a new topology) not
only t0 .
In the second set of experiments, we examined the impact of change of the region radius R
on the transmission range. We conducted these experiments for two values of orbit radii, 10

27

and 30, and n = 35 in both the cases. For each value of R, we conducted 100 experiments and
the results are averaged over them. Fig. 11(a) shows the results. It may be observed that increase
in the value of R leads to increase in CTR. This observation is quite expected as larger regions
can destroy more nodes at a time. Moreover, it may be noted that for larger values of orbit
radii the transmission range also increases. The reason is that for a specific number of nodes
in a bounded deployment area, larger orbit radii result in larger distance between the nodes.
Accordingly, larger transmission range is necessary, particularly in the case of larger Rs.
600

CTR_f (R=60)
CTR_f (R=20)
CTR
CTR_D (D=0.15)
CTR_D (D=0.6)

550

Transmission Range

500
450
400
350
300
250

200
150
100
0

Fig. 10.

20

40
60
80
Number of Nodes (n)

100

120

Transmission Range vs. Number of Nodes

700

300

n=30
n=50
n=70

650

280

600
550

260

CTRD

CTRf

500
450

400
350

220
200

300

OrbitRadius=30

250

180

OrbitRadius=10

200
0

20

40
60
80
Region Radius (R)

(a)
Fig. 11.

240

100

120

160
0

0.2

0.4
Delay (D)

0.6

0.8

(b)

(a) Transmission Range (CT Rf ) vs. Region Radius, n = 35; (b) Transmission Range (CT RD ) vs. Delay

Finally, we conducted experiments to investigate the impact of delay D on the value of CT RD .
Fig. 11(b) depicts the results. We observe that when value of delay D is zero the value of CT RD

28

is equal to CT R and by increasing delay, CT RD decreases and the interesting observation is that
when delay becomes greater than 2period the decrease in the value of CT RD is unnoticeable
or even zero.
IX. C ONCLUSION
Existence of sufficient control over the movement pattern of the mobile platforms in Airborne
Networks opens the avenue for designing topologically stable airborne networks. In this paper,
we discussed the system model and architecture for Airborne Networks (AN). We studied the
problem of maintaining the connectivity in the underlying dynamic graphs of airborne networks
when trajectories of nodes are given. We developed techniques to compute the dynamic topology
of the AN at any instance of time and proposed an algorithm to compute critical transmission
range when all nodes are operational. Motivated by the importance of robustness and fault
tolerance capability of ANs, we have also investigated the region-based connectivity of the ANs
and proposed an algorithm to find the minimum transmission range necessary to ensure that
the surviving nodes of the network remain connected, even when all or some nodes of region
fail due to an enemy attack. In the process of computing the minimum transmission range in
faulty scenario, we developed techniques to (i) compute all the fault regions that need to be
considered to ensure overall connectivity at all times and (ii) compute the set of nodes that
might be damaged by the failure of a specific region at a specific time. Moreover, we defined
and formulated the critical transmission range in delay tolerant airborne networks CT RD and
proposed an algorithm to compute CT RD . Through simulations, we have illustrated the impact
of the number of nodes, the region radius in faulty scenario and delay in delay tolerant networks
on the minimum transmission range. In future we plan to develop more efficient algorithms
to compute critical transmission range in different scenarios and also to study the environment
where the ANPs take unpredictable flight paths.
R EFERENCES
[1] J. L. Burbank, P. H. Chimento, B. K. Haberman, and W. T. Kasch, “Key Challenges of Military Tactical Networking and
the Elusive Promise of MANET Technology,” IEEE Communication Magazine, November 2006.
[2] M. Conti and S. Giardano, “Multihop ad-hoc Networking: the Reality,” IEEE Communications Magazine, April 2007.
[3] S. Milner, S. Thakkar, K. Chandrashekar, and W. Chen, “Performance and scalability of mobile wireless base-stationoriented networks,” ACM SIGMOBILE MC2 R , vol. 7, 2003.

29

[4] S. Milner, J. Llorca, and C. Davis, “Autonomous reconfiguration and control in directional mobile ad hoc networks,”
Circuits and Systems Magazine, IEEE, vol. 9, no. 2, pp. 10 –26, quarter 2009.
[5] P. Gupta and P. Kumar, “The capacity of wireless networks,” IEEE Transactions on Information Theory, vol. 46, no. 2,
pp. 388 –404, mar 2000.
[6] N. R. C. Committee on Evolution of Untethered Communications, The Evolution of Untethered Communications.

The

National Academies Press, 1997. [Online]. Available: $http://www.nap.edu/openbook.php?record id=5968$
[7] R. Ramanathan and R. Rosales-Hain, “Topology control of multihop wireless networks using transmit power adjustment,” in
INFOCOM 2000. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings.
IEEE, vol. 2, 2000, pp. 404 –413 vol.2.
[8] J. Cabrera, R. Ramanathan, C. Gutierrez, and R. Mehra, “Stable topology control for mobile ad-hoc networks,”
Communications Letters, IEEE, vol. 11, no. 7, pp. 574 –576, july 2007.
[9] J. Wu and F. Dai, “Mobility-sensitive topology control in mobile ad hoc networks,” IEEE Transactions on Parallel and
Distributed Systems, vol. 17, no. 6, pp. 522 –535, june 2006.
[10] P. Santi, “The critical transmitting range for connectivity in mobile ad hoc networks,” IEEE Transactions on Mobile
Computing, vol. 4, 2005.
[11] A. Sen, B. Shen, L. Zhou, and B. Hao, “Fault-tolerance in sensor networks: A new evaluation metric,” in Infocom, 2006.
[12] A. Sen, S. Murthy, and S. Banerjee, “Region-based connectivity: a new paradigm for design of fault-tolerant networks,”
in HPSR, 2009.
[13] S. Neumayer, G. Zussman, R. Cohen, and E. Modiano, “Assessing the vulnerability of the fiber infrastructure to disasters,”
in Infocom, 2009.
[14] S. Neumayer and E. Modiano, “Network reliability with geographically correlated failures,” in Infocom, 2010.
[15] K. Fall, “A delay-tolerant network architecture for challenged internets,” in Proceedings of the 2003 Conference on
Applications, Technologies, Architectures, and Protocols for Computer Communications, ser. SIGCOMM ’03.

New

York, NY, USA: ACM, 2003, pp. 27–34. [Online]. Available: http://doi.acm.org/10.1145/863955.863960
[16] J. P. G. Sterbenz, R. Krishnan, R. R. Hain, A. W. Jackson, D. Levin, R. Ramanathan, and J. Zao, “Survivable
mobile wireless networks: Issues, challenges, and research directions,” in Proceedings of the 1st ACM Workshop
on Wireless Security, ser. WiSE ’02.

New York, NY, USA: ACM, 2002, pp. 31–40. [Online]. Available:

http://doi.acm.org/10.1145/570681.570685
[17] S. Jain, K. Fall, and R. Patra, “Routing in a delay tolerant network,” in Proceedings of the 2004 Conference on
Applications, Technologies, Architectures, and Protocols for Computer Communications, ser. SIGCOMM ’04.

New

York, NY, USA: ACM, 2004, pp. 145–158. [Online]. Available: http://doi.acm.org/10.1145/1015467.1015484
[18] J. Alonso and K. Fall, “A linear programming formulation of flows over time with piecewise constant capacity and transit
times,” Intel Research Technical Report IRB-TR-03-007, 2003.
[19] Y. Cao and Z. Sun, “Routing in delay/disruption tolerant networks: A taxonomy, survey and challenges,” Communications
Surveys Tutorials, IEEE, vol. 15, no. 2, pp. 654–677, 2013.
[20] M. Huang, S. Chen, Y. Zhu, B. Xu, and Y. Wang, “Topology control for time-evolving and predictable delay-tolerant
networks,” in 2011 IEEE 8th International Conference on Mobile Adhoc and Sensor Systems (MASS), 2011, pp. 82–91.
[21] M. Huang, S. Chen, Y. Zhu, and Y. Wang, “Cost-efficient topology design problem in time-evolving delay-tolerant
networks,” in Global Telecommunications Conference (GLOBECOM 2010), 2010 IEEE, 2010, pp. 1–5.

30

[22] A. Tiwari, A. Ganguli, and A. Sampath, “Towards a Mission Planning Toolbox for Airborne Networks: Optimizing Ground
Coverage Under Connectivity Constraints,” in IEEE Aerospace Conference, March 2008, pp. 1–9.
[23] B. Epstein and V. Mehta, “Free Space Optical Communications Routing Performance in Highly Dynamic Airspace
Environments,” in IEEE Aerospace Conference Proceedings, 2004.
[24] R. Diestel, Graph Theory.

Springer, 2005.

[25] Olmsted, J. M. H., and C. G. Townsend, “On the Sum of Two Periodic Functions,” The Two-Year College Mathematics
Journal, vol. 3, 1972.
[26] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms.

McGraw Hill, 2001.

arXiv:1012.2142v1 [cs.CC] 9 Dec 2010

NP-completeness Proof: RBCDN Reduction
Problem
Sujogya Banerjee, Shahrzad Shirazipourazad, Pavel Ghosh and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {sujogya, sshiraz1, pavel.ghosh, asen }@asu.edu

June 19, 2013
Suppose {R1 , . . . , Rk } is the set of all possible regions [?] of graph G. Consider a k-dimensional vector C whose i-th entry, C[i], indicates the number of
connected components in which G decomposes when all nodes in Ri fails. Then,
region-based component decomposition number (RBCDN) of graph G with region
R is defined as αR (G) = max1≤i≤k C[i].
Suppose the RBCDN of G with region R is αR (G). If αR (G) is considered
to be too high for the application and it requires RBCDN of the network not to
exceed αR (G) − K, for some integer K. Assuming each additional link li that
can be added to the network has a cost c(i) associated with it, find the least cost
link augmentation to the network so that its RBCDN is reduced from αR (G) to
αR (G) − K. Formal description of the decision version of this problem is given
below.
RBCDN Reduction Problem (RBCDN-RP )
INSTANCE: Given
(i) a graph G = (V, E) where V = {v1 , . . . , vn } and E = {e1 , . . . , em } are the
sets of nodes and links respectively,
(ii) the layout of G on a two dimensional plane LG = (P, L) where P =
{p1 , . . . , pn } and L = {l1 , . . . , lm } are the sets of points and lines on the 2dimensional plane,
(iii) region R defined to be a circular area of radius r.
(iv) cost function c(e) ∈ Z+ , ∀e ∈ Ē, where Ē is complement of the link set E.
(v) integers C and K (K ≤ αR (G), where αR (G) is the RBCDN of G).
QUESTION: Is it possible to reduce the RBCDN of G by K by adding edges
to G (from the set Ē) so that the total cost of the added links is at most C?

1

Figure 1: Transformation of a HCPGP instance to a RBCDN-RP instance

1

NP-Completeness Proof of RBCDN-RP

We prove that RBCDN-RP is NP-complete by a transformation from the Hamiltonian
Cycle in Planar Graph Problem (HCPGP) which is known to be NP-complete
[?]. A Hamiltonian Cycle in an undirected graph G = (V, E) is a simple cycle
that includes all the nodes. A graph is a planar if it can be embedded in a
plane by mapping each node to a unique point in the plane and each edge is
a line connecting its endpoints, so that no two lines meet except at a common
endpoint [?].
Hamiltonian Cycle in Planar Graph Problem (HCPGP)
INSTANCE: Given an undirected planar graph G = (V, E).
QUESTION: Does G contains a Hamiltonian Cycle?
Theorem 1 RBCDN-RP is NP-complete.
Proof: It is easy to verify whether a set of additional edges of total cost ≤ C
reduces the RBCDN of graph G with region R from αR (G) to αR (G) − K.
Therefore RBCDN-RP is in NP.
From an instance of the HCPGP (a planar graph G = (V, E)) we create an
instance of the RBCDN-RP (the layout LG0 = (P 0 , L0 ) of a graph G0 = (V 0 , E 0 ))
in the following way. First, we do a straight line embedding of the planar graph
G on a plane so that lines corresponding to links in G do not intersect each
other. Such an embedding can be carried out in polynomial time [?]. We call
this layout LG00 = (P 00 , L00 ). We create another layout LG0 = (P 00 , ∅), by setting
P 0 = P 00 and L0 = ∅. The graph G0 corresponding to the layout LG0 = (P 0 , L0 )
is the instance of RBCDN-RP created from the instance of HCPGP. We define
a region R to be circular area of sufficiently small radius r, such that if a
region fails, it can only destroy (i) a single node with all links incident on it,

2

or (ii) a single link. Since the created instance of RBCDN-RP has no links
(E 0 = L0 = ∅), the RBCDN of G0 with region R is n where n = |V 0 | = |P 0 |.
We set the parameters C and K of the instance of the RBCDN-RP to be equal
to n and n − 1 respectively. We assign costs to the links of Ē 0 in the following
way. The cost of a link c(e) = 1, if e ∈ (E ∩ Ē 0 ) and c(e) = ∞, if e ∈ (Ē ∩ Ē 0 ).
If the instance of the HCPGP has a Hamiltonian Cycle, we can use the set
of links that make up the cycle, to augment the link set E 0 of the instance
G0 = (V 0 , E 0 ) of the RBCDN-RP. The augmented G0 , (G0aug ), is now a simple
cycle that involves all the nodes. With the given definition of region R (a
small circle of radius r), only one node can be destroyed when a region fails.
Accordingly RBCDN of G0aug is 1. It may be recalled that RBCDN of G0 is
n. Accordingly, augmentation of the link set of G0 reduced its RBCDN by
n − 1. Due to the specific cost assignment rule of the links, the total cost of
link augmentation is n. Therefore, if the HCPGP instance has a Hamiltonian
Cycle, the RBCDN of the instance of RBCDN-RP can be reduced by K with
augmentation cost ≤ C.
Suppose that it is possible to reduce the RBCDN of the instance of RBCDNRP by K with augmentation cost being at most C. This implies that the
RBCDN of G0 can be reduced from n to 1 (as K = n − 1) when it is augmented
with additional links with total cost at most n (as C = n). In order for the
RBCDN of G0aug to be 1, the node connectivity of G0aug must be at least 2. A
n node graph that has the fewest number of links and yet is 2-connected, is a
cycle that includes all the nodes. As G0 had no links, this implies at least n
links must have been added to create the augmented graph G0aug . Given that
the cost of a link c(e) = 1, if e ∈ (E ∩ Ē 0 ) and c(e) = ∞, if e ∈ (Ē ∩ Ē 0 ), and
total cost of link augmentation is at most n, it is clear that the links used in
augmenting G must be from the set (E ∩ Ē 0 ). These links are part of the edge
set of the instance of HCPGP. Accordingly, the instance of HCPGP must have
a Hamiltonian Cycle.

3

Resource Mapping and Scheduling for Heterogeneous
Network Processor Systems
Liang Yang

Tushar Gohad

Pavel Ghosh

Department of Computer
Science and Engineering
Arizona State University
Tempe, AZ 85287

MontaVista Software
Tempe, AZ 85282

Department of Computer
Science and Engineering
Arizona State University
Tempe, AZ 85287

tgohad@mvista.com

Liang.Yang@asu.edu

pavel.ghosh@asu.edu

Devesh Sinha

Arunabha Sen

Andrea Richa

Department of Computer
Science and Engineering
Arizona State University
Tempe, AZ 85287

Department of Computer
Science and Engineering
Arizona State University
Tempe, AZ 85287

Department of Computer
Science and Engineering
Arizona State University
Tempe, AZ 85287

devesh@asu.edu

asen@asu.edu

aricha@asu.edu

ABSTRACT

General Terms

Task to resource mapping problems are encountered during
(i) hardware-software co-design and (ii) performance optimization of Network Processor systems. The goal of the
first problem is to find the task to resource mapping that
minimizes the design cost subject to all design constraints.
The goal of the second problem is to find the mapping that
maximizes the performance, subject to all architectural constraints. To meet the design goals in performance, it may
be necessary to allow multiple packets to be inside the system at any given instance of time and this may give rise to
the resource contention between packets. In this paper, a
Randomized Rounding (RR) based solution is presented for
the task to resource mapping and scheduling problem. We
also proposed two techniques to detect and eliminate the
resource contention. We evaluate the efficacy of our RR approach through extensive simulation. The simulation results
demonstrate that this approach produces near optimal solutions in almost all instances of the problem in a fraction of
time needed to find the optimal solution. The quality of the
solution produced by this approach is also better than often
used list scheduling algorithm for task to resource mapping
problem. Finally, we demonstrate with a case study, the results of a Network Processor design and scheduling problem
using our techniques.

Design Performance

Keywords
Network Processor, HW-SW Partitioning, Randomized Rounding, Codesign

1.

INTRODUCTION

To meet twin goals of performance and flexibility, Network Processors (NP) were introduced by several vendors a
few years ago. Evolution in Network Processor family in the
last few years has seen increasing heterogeneity in the architectural design. This is evidenced by the introduction of
specialized co-processors for classification and security and
content addressable memory (CAM) for faster search. Such
heterogeneity contributes to added complexity for two problems, (i) hardware-software co-design of Network Processors
and (ii) mapping of application components to appropriate
resources for optimal performance. Network Processors are
required to support multiple applications, such as header
parsing, table lookup, encryption/decryption for virtual private networks (VPN), network address translation (NAT)
and voice processing. In the first problem, one needs to find
the optimal architecture that can support all the specified
applications. In the second problem, given the architecture,
one needs to find the optimal mapping of the application
components (task) to the available resources. In both the
cases, one needs to find a task-to-resource mapping. The
goal of the first problem is to find the task-to-resource mapping that minimizes the design cost subject to all design
constraints. The goal of the second problem is to find the
task-to-resource mapping that maximizes the performance
subject to all architectural constraints.
The first problem was studied extensively by Thiele et al.
in [11]. In their model of design space exploration problem,
each application (e.g, encryption/decryption, voice processing) is represented by a directed acyclic graph (DAG). The
nodes of this graph represents the application components

Categories and Subject Descriptors
B.8.2 [Hardware]: [Performance Analysis and Design Aids]
This research was supported in part by the Center for Embedded Systems
(CES) at Arizona State University.
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, to republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee.
ANCS’05, October 26–28, 2005, Princeton, New Jersey, USA.
Copyright 2005 ACM 1-59593-082-5/05/0010 ...$5.00.

19

(or tasks). A set of resources, such as general purpose
processors, co-processor for classification (classifier), co-processor
for security, micro-engines and others are available for execution of the tasks. The execution time of each task on
each resource is known. The goal of the task-to-resource
mapping problem is to find the optimal mapping that satisfies all the design and performance constraints. Figure 1
shows the DAGs corresponding to three common Network
 	 


  
   
 

%&'() *+(&,)- 
./0 /1 
23456( 

7'+8 .1 
4;)(8>0%)-?'&+
5+, ;)5,)- 6)+@A; 

7'+8 .1 
4;)(8 >0%)-?'&+
5+, ;)5,)- 6)+@A; 

465??'DC

"
2.0 7&&89:  4;)(8//7
#
4;)(8<9= 
B)(-)=)+A //7
4;)(8<9=


7'+8 /1 


.&9A) 7&&89:
!

465??'DC


23456(


B)A)-='+) 4&<
.&9A) 7&&89:

!

2.07&&89: $
7'+8 /1
3)5,)- *+(-C:A 

7'+8/1


Figure 3: DAG and Mapping Solutions

7&&89:E&<
4& ,
 6 9
!

have to be mapped to two different resources, a General
Purpose Processor(GPP) and a Co-Processor (CP). The CP
is a specialized unit designed to execute the task b and it
takes 2 units of time for the execution. Because the CP is
a specialized processor for the task b, it may not be able
to execute tasks a and c. This is captured in the bipartite
graph model by assigning a weight of infinity to the directed
edge from a to CP and c to CP. The execution times of three
tasks on the GPP are 6, 12 and 10 respectively (shown in
the Figure 3). The communication costs from GPP to CP
are 4 and 20 time units for task pairs (a, b) and (b, c),
respectively. Communication cost is assumed to be zero for
pair of tasks executing on the same processor. The table in
Figure 3 shows the mapping and start, finish times of the
tasks using the list scheduling algorithm [10], the optimal
solution and the randomized rounding (RR) technique based
solution. In case of the list scheduling algorithm, task b
will be mapped to the CP (since, communication cost(4) +
processing cost on CP(2)=6 < processing cost on GPP(12)).
Thus the greedy list scheduling technique will be trapped in
a local minimal point. In this case, this local optimal point
(finishing time = 42) is much worse than the global optimal
point (finishing time = 28) obtained from the RR algorithm.
Clearly this approach does not find the optimal solution
and in fact the difference between solution produced by this
approach and the optimal solution can be arbitrarily large.
For this reason, in this paper we present a Randomized
Rounding (RR) based approach for the solution of the taskto-resource mapping and scheduling problem. We evaluate
the efficacy of our RR approach through extensive simulation. The simulation results demonstrate that this approach
produces near optimal solutions in almost all instances of
the problem in a fraction of time needed to find the optimal
solution.
The mapping and scheduling problems in embedded systems have been studied extensively by the research community in recent years due to its importance in determining the
performance and the cost of the system. Most of the studies
in this arena have focused their attention on the optimal
design of systems that supports a single application [1, 6,
7]. However, a Network Processor System (NPS) may have
to support multiple applications of the type mentioned ear-

B)+C

Figure 1: Task graphs for different flows
Processor applications, Voice-over-IP, IPv4 Forwarding and
Best Effort Quality-of-Service. The bipartite graph of Figure 2 shows the execution time of a task Ti on a resource
Rj . The execution time of task Ti on a resource Rj is shown
on directed edge of the bipartite graph from Ti to Rj .

Figure 2: Task-Resource Mapping Graph
The second problem was studied by Ramaswamy et al. in
[10]. In the first part of their paper they conduct extensive
analysis of the application data to build the DAG that will
be used for mapping. The nodes of this DAG is a set of
instructions that may be viewed as the tasks in the first
problem. In the second part of the paper [10], the authors
provide an algorithm for mapping application DAGs to NP
Architectures. This is a greedy algorithm based on widely
known list scheduling scheme. Unfortunately, this greedy
scheme can produce solutions those are far from optimal.
Consider the DAG example shown in Figure 3. The DAG
is comprised of three nodes (tasks), a, b and c. The tasks

20

lier. Although a vast majority of the mapping and scheduling studies focus on single application systems and as such
they are not particularly useful in the Network Processor
domain. There have been only a few studies, where the issues of multi-application systems have been addressed [5,
8]. Since our focus is on the design of Network Processor systems, we will discuss efforts undertaken by others in
multi-application domain, but will refrain from discussion
of single-application systems. Mapping and scheduling of
multi-application systems may be defined in one of the following two ways [5]:

the resources on which the tasks can be executed and the
time needed for execution of the tasks on the resources is
shown in Figure 4(a). From the figure, the task T11 can be
executed on resources R1 , R2 and R3 and it takes 7, 9 and
15 nanoseconds respectively. With Kalavade’s assumption
of only one packet inside the system at any given time, there
will be no feasible solution. On increasing the deadline from
18ns to 36ns, all the tasks can be processed. However, increasing the deadline will be result in existence of a second
packet in the system before the first one has left. There
may be resource contention between the packets within the
system now. Kalavade’s Extended GCLP (Global Criticality Local Phase)approach [5] does not provide any safeguard
against this as shown in 4(c).If the packets belong to application 2 (specified by task graph T2), the first packet will
use resource R3 from time 0ns to 28ns (0 to 15 for executing T21 and 15 to 28 for T22 ) and R1 from 28ns to 36ns.
The second packet that enters the NP system at time 18ns,
will require the resource R3 between the times 18ns to 46ns,
(for processing T21 from 18 to 33 and T22 from 33 to 46).
As the first packet needs R3 from 0ns to 28ns and the second packet needs it from 18ns to 46ns, there is resource
contention between these two packets. In section 3.2, we
provide mechanisms for detection and elimination of such
resource contentions.
The contribution of this paper is twofold. First, we propose a methodology for finding the least cost mapping of
tasks to resources, satisfying all the constraints. As a first
step toward partitioning, an Integer Linear Programming
(ILP) formulation is developed from the system level specification. To reduce computational time of ILP, the integrality
constraints are relaxed and the corresponding LP is solved.
Randomized rounding technique is used to convert the fractional values assigned to the variables by the LP to integral
values. Second, the initial solution is evaluated for resource
contentions. If this initial solution is resource contention
free, we are done. If this solution is not free from resource
contention, then we change the deadline for completion of
tasks in a systematic way and repeat the entire process.
This process is discussed in detail in section 3. Finally, we
evaluate the efficacy of our methodology through extensive
simulation using the software package TGFF (Task Graph
For Free) [2].
It may be noted that we propose no new technique to
build the task graph (DAG). Accordingly, any method, including the one proposed in [10], can be utilized for this
purpose. The mapping problem discussed in this paper assumes that the task graph is provided as a part of the input
specification.

Definition 1: Given a set AP P = {AP P1 , AP P2 ,
..., AP Pk } of applications each specified by a
DAG, where each application AP Pj has a set
of constraints (e.g., timing constraint Dj , area
constraint Aj , etc.), find the mapping that minimizes the design cost (in monetary terms) while
satisfying all the design constraints.
Definition 2: Given a set AP P = {AP P1 , AP P2 ,
..., AP Pk } of applications each specified by a
DAG, where each application AP Pj has a set
of constraints (e.g., timing constraint Dj , area
constraint Aj , etc.), find the mapping that maximizes system performance while satisfying all the
design constraints.
The first definition can be used before the system is designed and the second definition can be used after the system
is designed. A profile-guided automated mapping compiler
was developed for runtime performance enhancement in [12].
In addition to [10], this may be viewed as an example of the
second definition.
In addition to supporting multiple applications, Network
Processor systems have to process a stream of packets continuously arriving at its input ports. To meet performance
goals, it may be necessary to allow more than one data unit
to be inside the embedded system at any given time. A data
unit is smallest chunk of data on which the system operates.
In a networking application, the data unit may be a packet.
Although the notion of a packet is associated with the networking domain, we will use it in a much broader sense in
this paper and will use the terms data unit and packet interchangeably. The possibility of multiple data units being
inside the embedded system at the same time adds to the
complexity of the design process, as it opens up the possibility of resource contention between successive data units.
We propose two techniques for detection and elimination of
such resource contention. To the best of our knowledge,
such resource contention issues were not studied in earlier
papers.
Kalavade et al.[5] were one of the earliest researchers to
study partitioning and scheduling problem for multi-function
(multi-application) systems. In their model they assumed
that only one data unit (or packet) will be inside the embedded system at any given time. However, this is a strong
assumption and it may be difficult to meet in practice. To
elaborate the issue, we provide the following example.
We assume a network processor has to process data at the
line rate of 20 Gbps and the packet size is 45 Bytes with no
inter-packet arrival gap. The NP has only 18 nanoseconds
to process each packet. This scenario is illustrated in Figure 4. The task graphs corresponding to two applications,

2.

MAPPING AND SCHEDULING APPLICATION DAGS TO NP SYSTEMS

This section describes randomized rounding approach followed by the methods handling resource contention among
packets.

2.1

Randomized Rounding Technique

As the first step of solving the mapping and scheduling
problem for application DAGs to NP systems, we set up the
Integer Liner Program (ILP) formulation of the problem.
The ILP formulation of the problem is provided in Appendix. As it is well known, the solution of ILP may take consid-

21

Figure 4: Resource Contention in GCLP Sol.
erable amount of time, specially if the application DAG has
large number of nodes. For this reason, we do not solve the
ILP. We relax the integrality constraints on the variables, i.e
allowing the solution to have fractional (non-integer) values
for the variables, and solve the corresponding Linear Program (LP). Solution of the LP can be obtained much faster
than the solution of the ILP as LP is solvable in polynomial
time. Relaxing the integrality constraint may give rise to
fractional (non-integral) solution. However, fractional values associated with task-to-resource mapping for variables
may not have any physical meaning. In this section, we describe the randomized rounding approach [9] to convert the
fractional values to integer values.

2.1.1

variable x and if x is rounded, it determines the values of y
and z as well. We chose to round variable d before variable
x.
The fractional value of variables is used as the probability to round them to the closest integer. For example, a
binary variable p has a fraction value 0.35 after some ILP
relaxation. In order to determine the value of p (0 or 1), we
generate a random number between 0 and 1. If the number
happens to be greater than or equal to 0.35, p is set to 1
and 0 otherwise.

2.1.2

Variable Fixing

The relaxed version of the ILP (i.e., the LP) may still produce some variables with integer (or binary) values. During
the next iteration of the LP, we assign integer values (obtained during the previous iteration) to these variables and
treat them as constants. We call this process variable fixing. By preceding variable fixing with variable rounding,
the number of variables to be rounded is reduced and thus
time can be saved later in the rounding process. However,
the variable-fixing process may not always lead to a situation where all variables end up having integer values. In
that case, we still need to use randomized rounding to turn
the left non-integer variables into integers. the same priority order is followed during variable fixing process as in the
variable rounding process.

Rounding Scheme

One of the key issues in a randomized rounding approach
is to maintain the feasibility of the solution as we round
fractional variables. The selection of an appropriate rounding scheme and the adoption of some randomization strategies helps reduce the probability of constraint violations and
speed up the randomization procedure. To maintain feasibility, one should carefully understand the inter-dependencies
amongst the variables. For a constraint of the form
a+b+c+d≤1
where a, b, c and d are variables in the range [0, 1], we know
that only one of them is to be rounded up to one, while the
remaining variables should be zero. Thus, we consider variables in groups, and define an ordering for rounding those
variables as explained in the following sections.
Among the five decision variables defined in appendix,
x, y, z and d are 0/1 variables and s is a positive integer
variable. The variables d and s are related (constraint (4))
and they determine the execution order and scheduling time
of each flow. Accordingly, they can be grouped and d is
rounded first. The variables x, y and z are responsible for
task-to-resource mapping and inter-task relationship. These
variables can be placed in another group. From constraints
(5) and (6), the variables y and z are determined by the

2.1.3

Rollback Point selection

Some of the constraints may not be satisfied even after
randomized rounding of some variables. In this case, we
need to roll back and undo the applied randomized rounding
steps. If during the rounding process, rollback has to be
done several times, it will significantly increase the execution
time. Thus selection of a good rollback point is critical to
the efficient performance of the algorithm. When we choose
rollback point, we need to consider the context in which the
constraint violation takes place. For example, as indicated
earlier, there are two randomized rounding stages involved in
solving the problem. The variables d’s are rounded first and

22

the variables x’s are done next. If any constraint violation is
detected during the first rounding stage, we remove all the
rounded d from the problem file and restart the randomized
rounding procedure. However, if any constraint violation is
detected in the second stage, the rounded d values from the
first stage can be preserved and we only require to roll back
to the beginning of the second stage.

2.1.4

Rounding Step Size

The rounding step size is defined as the number of variables being rounded in each iteration. As the computation
time is related to the number of iterations, lesser number of
iterations will imply lesser computation time. However, the
probability of some constraint being violated is also large
when several variables are rounded simultaneously. Obviously, such violations imply increased execution time. In
our experiments, we found that the computation time is essentially independent of the number of variables rounded
in each iteration. We choose only one constraint at random
and only one variable in that constraint is rounded at a time.

2.2 Resource Contention Problem

Figure 5: Range of Solutions

The total system cost in dollar value can be reduced further by relaxing the deadline for each packet and at the same
time allowing multiple packets inside the system to satisfy
the throughput constraint. Allowing multiple packets inside the system may introduce the problem of resource contention as already discussed in section 1, since more than one
packet may access the same resource at the same instance
of time. We have devised a technique to handle resource
contention amongst tasks belonging to same application.

2.2.1

finishes all the tasks scheduled for a packet on it. For each
resource r ∈ V − {s, t} the resource cycle time is defined as:
y(e)

(1)

x(e)=a

where a = mine∈δ− (r) (x(e)), and b = maxe∈δ− (r) (x(e)), and
δ − (r) is the set of incoming edges to node r.
Maximum Cycle Time: It is defined as the maximum of all
the resource cycle times, i.e.,

Exploration of Solution Space

The solution space of mapping and scheduling problem for
NPSs can be described on a one-dimensional range as shown
in Figure 5. If the deadline constraint is too strict, the ILP
model may not return a feasible solution. On the other
hand, making the deadline too relaxed will give a feasible
solution with a lower cost as the ILP tries to allocate more
than one task to a resource. This increases the chance of
resource contention amongst packets. The cost generally
increases on decreasing the deadline value, as the ILP has
to include faster (more expensive) resources. The goal is
to find a point in the solution space, which is feasible and
resource contention-free with lowest possible dollar cost.

2.2.2

X

x(e)=b

Tcr =

Tc =

max

{Tcr }.

r∈V −{s,t}

(2)

Packet flow graph construction from a given solution of ILP
and the calculation of maximum cycle time is shown in Figure 6. Figure 6(a) shows a particular task graph and 6(b)
has a corresponding solution given by the ILP. Figure 6(c)
shows the corresponding PFG. The calculation of maximum
cycle time is shown in 6(d).
In order to detect the resource contention in the allocation and scheduling corresponding to the ILP solution, the
packet-flow graph is built first. Since the maximum cycle
time in the PFG is a measure of the maximum timespan
a resource is busy processing a packet, there must exist at
least one resource contention amongst packets in the system if the maximum cycle time is greater than the packet
arrival rate. However, this technique may fail to detect an
existing resource contention if two consecutive packets follow different branches in the same task graph. In order to
detect the resource contention in this scenario, Gantt chart
method is used. Resource allocation time-lines are drawn for
each path. Any overlapping allocation of the same resource
for more than one packet is considered to be a resource contention. An iterative approach to resource contention detection and elimination is described next.

Resource Contention Detection

On account of the existence of multiple packets inside
the system, resource contention may be caused amongst the
packets following the same path or different paths in a task
graph. To describe the method for resource contention detection, the following terms are defined first:
Packet Flow Graph (PFG): It is defined as G(V, E), where
V is the subset of resources allocated by the ILP, with additional entry s and exit t nodes. An edge e = (u, v) ∈ E
indicates an allocation sequence between resource u and v.
There is an weight function w(e) associated with each edge.
For each w(e) = (x(e), y(e)); x(e) indicates the sequence
number of allocation of the following resource and y(e) indicates the corresponding execution time on the resource.
Resource Cycle Time: It indicates the maximum amount of
timespan for which a resource is busy in executing a set of
tasks for a packet, i.e. a resource is not available until it

2.2.3

Resource-Contention Elimination

To speed up the exploration of solution space, an iterative procedure is used. It finds the least cost feasible solution

23

we present a case study for design of a Network Processing
system that will support a designated set of applications.

3.1

Performance of Randomized Rounding

The quality of randomized mapping can be evaluated based
on the solution time and the objective value. To test the effectiveness of our technique, we use the task graphs shown
in figure 1 as the problem input which has 737 variables and
868 constraints in the ILP formulation. We vary the timing
deadline and conduct ten independent experiments for each
deadline. Figure 8 is the solution time comparison between
the randomized rounding (RR) approach and ILP technique.
Figure 9 reflects the deviation of heuristic solution from the
optimal objective value obtained by ILP. The figures show
that our approach produces a solution with near optimal
objective value in a fraction of the time needed to find an
optimal solution.
Figure 6: PFG Construction

ZQJQK[\HQ JWQ ]\O[QJQKI G^ OSS JWQ JOIP _KOVWI`
FWGGIQ JWQ [Oa\[L[ bcdG^ JWQ ]\O[QJQKI
efg h i jklmn opqnr s jt u q vwr
efg x i jh y kr z { i h
efg ¦f§¦¨©ªf« ¬ i jx ­ {r s ®
¯§°¦± §°f²e³´g± §°f µ§°g©g©³ª©ª¶
§ª¦ e·¸f¦¹¨©ª¶
¿ºµ ºµ»¼§ª¦³½©¾f¦ ¼³¹ª¦©ª¶
Figure 8: Solution Time Comparison

|H^QOI\}SQ
~




R\KIJ
\JQKOJ\GH
~


 ÉÅÊ
Ë ÅÁÌ ÁÅÃ
 N
N
 

FGHIJKLMJ NOMPQJ RSGT UKOVWI XNRUIY
FOSMLSOJQ Oa\[L[ FMSQ \[Q  OH] JGJOS
QaQMLJ\GH J\[Q  GQK OSS JWQ NRUI
ÀÁ
  h 

~
ÂÃ ÀÄÃÅÆ


 ¡ ¢£¤£¤¥£
efg x i 


Ç È
~



 Ã 
N

  efg { i


 
N

Ç È
~


Figure 9: Objective Value Deviation



3.2

Task-to-Resource Mapping Case Study

To demonstrate the validity of the model, we applied our
mapping method to design an IXP2400-like system. We
designed the system for 3 common and representative applications for NPSs defined by the Network Processor Forum
benchmarking implementation agreements [3].
The applications are shown in figure 10: Ingress and egress
processing for the Ethernet IPv4 unicast forwarder and Diffserv on the ingress path. IPv4 packet forwarding is based on
RFC1812 is the core in many NPS applications. Diffserv is
a method of facilitating end-to-end quality of service (QoS)
over an existing IP network.
In all three applications, the tasks to be used were decided
based on the Intel Application Building Blocks Design Guide

Figure 7: Iterative Improvement
which is free from resource contention. A flowchart representation of the iterative improvement process is shown in
figure 7.

3. EXPERIMENTAL RESULTS
We discuss the quality of solution provided by the randomized rounding approach compared to the ILP solution. Later

24

[4]. A resource set was chosen to be the set of resources
in IXP2400. This helped us have cycle-accurate simulation
model available for all the resources. The resource set with
associated design parameters is shown in table 1.

with its cycle-accurate simulator and transactor to get the
timing values. Communication delays were obtained based
on the bus speeds and the amount of data being transferred
between tasks. Communication delays are not shown here.
ILOG CPLEX 8.1 was used to solve the constraint system of the ILP formulation on an Intel XEON 1.5GHZ with
1GB RDRAM memory running RedHat Linux 8.0. The Intel IXP2400 workbench was configured for 600MHz microengine speed.
For the task graph in figure 10 with 22 tasks, 16 resources,
and 3 applications, the ILP generated 202 variables and 297
constraints. Packets were set to arrive every 205ns with no
inter-packet gap. The value 205ns corresponds to an OC48 configuration with packet size 64 bytes. The maximum
length of a path in the task graph specification was 14, thus
the initial relaxed deadline was set to (14 * 205 =) 2870ns.
The solution was found in 6 seconds after 7 iterations
of binary search for a feasible solution. The deadline was
relaxed from 51ns to 556ns. The total cost of hardware was
97 units. The mapping and schedule generated is shown in
table 2.

Figure 10: Task Graphs based on NPF Benchmark
Applications for NPSs

Resource
RISC Processors
Hash Unit
CRC Unit
CAM Accelerator
Route Lookup Engine
Checksum Unit
POS PHY
CSIX

Label
R1 ..R8
R9
R10
R11
R12
R13
R14
R15

Cost
7
11
9
17
13
12
9
7

Area
6
3
2
4
4
2
3
3

Task

Resource

T1
T2
T3
T4
T5
T6
T7
T8
T9
T10
T11
T12
T13
T14
T15
T16
T17
T18
T19
T20
T21

R14
R1
R7
R6
R7
R12
R7
R6
R13
R15
R15
R7
R9
R11
R6
R3
R6
R14
R7
R15
R6

IPv4
Ingress
0
15
93
146
146
189
238
240
242
434
468
123
–
–
–
–
–
–
240
–
–

Start times
Diffserv
IPv4
Ingress Egress
–
–
10
10
88
–
298
–
392
–
341
–
390
–
392
–
394
–
–
434
468
468
118
–
141
–
201
–
270
–
215
–
230
–
0
–
–
–
–
0
–
414

Table 2: Output Mapping

We compared our mapping against the task-resource mapping provided by Intel in the preconfigured applications of
[4]. Aggregate throughput, end-to-end packet latency and
resource utilization were the parameters used to compare
the models. We found our results to be within 7-10% of the
Intel mapping. Our method used less resources with better
resource utilization and achieved close to desired throughput
figures.
The important aspect of the mapping produced by our
approach is the cost per performance. Intel’s mapping costs
134 for the same performance achieved with our mapping
with cost 97. Moreover, our mapping method generated
the mapping in less than 7 seconds while Intel’s hand-tuned
mapping must have taken days to arrive at.
Figures 11 through 14 give the comparison of the performance of the two mappings.

Table 1: Set of Resources
Each possible implementation for a task was profiled in
the IXP2400 cycle accurate simulation environment. To obtain average execution cycles per task, the application was
tested with worst case input traffic. An instance of each
implementation of a task was run on the hardware and software resource options with the appropriate traffic. The line
rate was set to 2.5Gbps and packet size was set to minimum 64 bytes (size of mpacket for IXP2400) to simulate
the worst case. The Intel Developer Workbench was used

25

Diffserv Ingress Resource Utilization
90.00%

Throughput Results for IPv4 Ingress

80.00%
70.00%

4500

60.00%
Utilization

4000

Throughput (Mbps)

3500
3000

50.00%

Intel

40.00%

Our Approach

30.00%

2500

Intel

2000

Our Approach

20.00%

1500

10.00%

1000

0.00%
M1

500

M2

M3

M4

M5

M6

M7

M8

Microengine id

0
64

65

128

129

256

512

1518

Frame Size (bytes)

Figure 14: Diffserv Ingress Resource Utilizn

Figure 11: IPv4 Ingress Throughput

4.

IPv4 Ingress Resource Utilization
80.00%
70.00%

Utilization

60.00%
50.00%

5.

Intel

40.00%

Our Approach

10.00%
0.00%
M1

M2

M3

M4

M5

M6

M7

M8

Microengine id

Figure 12: IPv4 Ingress Resource Utilization

Throughput Results for Diffserv
4000
3500
3000
2500
Intel

2000

Our Approach

1500
1000
500
0
64

65

128

129

256

512

REFERENCES

[1] K. Chatha and R. Vemuri. Hardware-software
partitioning and pipelined scheduling of
transformative applications. IEEE Transactions on
Very Large Scale Integration (VLSI) Systems,
10:193–208, 2002.
[2] R. Dick, D. Rhodes, and W. Wolf. Tgff: task graphs
for free. International Workshop Hardware/Software
Codesign, pages 97–101, Mar 1998.
[3] N. P. Forum. Benchmark implementation agreements.
Network Processing Forum,
http://www.npforum.org/techinfo/IA.
[4] Intel. Intel application building blocks design guide.
http://www.intel.com/design/network/products/npfamily/sdk.htm.
[5] A. Kalavade and P. A. Subrahmanyam.
Hardware/software partitioning for multi-function
systems. IEEE Transactions on CAD of ICs and
Systems, 17(9):516–521, Sep 1998.
[6] R. Niemann and P. Marwedel. Hardware/software
partitioning using integer programming. Electronic
Design & Test Conference, pages 473–479, 1996.
[7] M. Palesi and T. Givargis. Multi-objective design
space exploration using genetic algorithms. Tenth
International Symposium on Hardware/Software
Codesign, pages 67–72, May 2002.
[8] A. Prasad, W. Qui, and R. Mahapatra. Hardware
software partitioning of multifunction systems. Design
Automation for Embedded Systems, Dec 2002.
[9] P. Raghavan and C. Thompson. Randomized
rounding: A technique for provably good algorithms
and algorithmic proof. Combinatorica, 7:365–374,
1987.

30.00%
20.00%

Throughput (Mbps)

CONCLUSION

In this paper we have presented a new methodology for
task to resource mapping problem. We show that our methodology produces near optimal solution in a fraction of time
needed to find the optimal solution. In addition, we have
identified the resource contention problem that can arise in
case multiple packets are allowed to be within the system
at the same time. We have provided two different solution
techniques for this resource contention problem. Finally, we
provided a case study of Network Processor system design
using our tool.

1518

Frame size (bytes)

Figure 13: Diffserv Ingress Throughput

26

[10] R. Ramaswamy, N. Neng, and T. Wolf. Application
analysis and resource mapping for heterogeneous
network processor architectures. Proc. of Third
Workshop on Network Processors and Applications
(NP-3) in conjunction with Tenth International
Symposium on High Performance Computer
Architecture (HPCA-10), pages 103–119, Feb 2004.
[11] L. Thiele, S. Chakraborty, M. Gries, and S. Kunzli.
Design space exploration of network processor
architectures. First Workshop on Network Processors
at the 8th International Symposium on
High-Performance Computer Architecture (HPCA8),
pages 30–41, 2002.
[12] H. Vin, J. Mudigonda, J. Jason, E. Johnson, R. Ju,
A. Kunze, and R. Lian. A programming environment
for packet-processing systems: Design considerations.
In the Workshop on Network Processors &
Applications - NP3 in conjunction with The 10th
International Symposium on High-Performance
Computer Architecture, Feb 2004.

Exclusive resource constraint: In each flow, if two tasks
occupy the same resource, they have to be scheduled sequentially, i.e.,

X
Gu

∀f, ∀k, ∀u&v, sfu +

X X
N

k
Tu,j
xk,f
u,j +

j=1

X

′

,f k,k′ ,f
zu,i

∀i,u→i k′ 6=k

X
Gv

Gu

−sfv ≤ (3 − dfu,v −

k,k
Cu,i

xk,f
u,j −

j=1

xk,f
v,j )T

j=1

(6)
Communication delay constraint: In each flow, a communication delay may be defined when two adjacent tasks use
different resources.

X

k,k
zu,v

′

,f

≥

xk,f
u,j +

j=1

X

X
Gv

Gu

∀f, ∀k&k′ , ∀u&v,

′

xkv,j,f − 1

j=1

Gu

k,k
zu,v

′

,f

≤

xk,f
u,j

(7)

j=1

X
Gv

k,k
zu,v

APPENDIX (ILP Formulation)

∀f, ∀u&v,

(3)

Timing constraint: In each flow, every task should finish
before the timing deadline, i.e.
Gi

(4)

k=1 j=1

Unique task constraint: In each flow, every task runs exactly once, i.e.

XX
N

Gi

xk,f
i,j = 1

+

Gu

j=1

X X
N

k
Tu,j
xk,f
u,j

+

k,k′ ,f k,k′ ,f
Cu,i
zu,i

u,u→i k′ 6=k

(9)

k=1

∀f, ∀i,

(8)

≤ sfv

Ck yk

k k,f
Ti,j
xi,j ≤ T

sfu

k=1

N

N

0
X X
N

The ILP model enforces the following constraints:
Area constraint: The total area occupied by the resources
in the target architecture should not exceed the maximum
allowed area,i.e.

XX

xk,f
i,j ≤ yk

Task dependency constraint: If two tasks (in the same
path)have to be scheduled sequentially, the task starting
earlier should finish before another one begins. For two adjacent tasks using different resources, the communication
delay should also be included in the total running time of
the earlier one, i.e.,

k=1

∀f, ∀i, sfi +

X
j=1

N

Ak yk ≤ A

′

xkv,j,f

Gi

∀f, ∀i, ∀k,

The objective of this ILP model is to find a task-to-resource
mapping with minimum cost:

X

≤

Task-to-resource mapping constraint: A resource is included in the target architecture if and only if it is used
by at least one task, i.e.

The ILP model has the following decision variables:
xk,f
i,j is equal to 1, when task i runs on resource k with algorithm j in flow, 0 otherwise yk is equal to 1, when resource
k is used in the target architecture, 0 otherwise dfu,v is equal
to 1, when task u starts no later than task v in flow f , 0
k,k′ ,f
otherwise zu,v
is equal to 1, when in flow f , task u runs
on resource k and task v runs on resource k′ , 0 otherwise sfi
is the starting time of task i in flow f

X

,f

j=1

k
Gi is the number of candidate algorithms for task i. Ti,j
is the running time for task i with algorithm j on resource
k. Each resource k is associated with an area Ak , whereas
k,k′ ,f
A is the maximum allowable area for the system. Ci,i
′
represents the communication time, if task i (in flow f ) is
assigned to resource k and task i′ (in flow f ) is assigned to
resource k′ and task i′ follows task i in the task graph associated with flow f . Each flow f is associated with a deadline
T and all the tasks in a flow must be completed with the
deadline.

Minimize

′

(5)

k=1 j=1

27

1
A

IEEE ICC 2013 - Optical Networks and Systems

Analysis of On-line Routing and Spectrum
Allocation in Spectrum-sliced Optical Networks
Shahrzad Shirazipourazad, Zahra Derakhshandeh and Arunabha Sen
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {sshiraz1, zderakhs, asen}@asu.edu

Abstract—The orthogonal frequency division multiplexing
(OFDM) technology provides an opportunity for efﬁcient resource utilization in optical networks. It allows allocation of
multiple sub-carriers to meet trafﬁc demands of varying size.
Utilizing OFDM technology, a spectrum efﬁcient and scalable optical transport network called SLICE was proposed recently. The
SLICE architecture enables sub-wavelength, super-wavelength
resource allocation and multiple rate data trafﬁc that results
in efﬁcient use of spectrum. However, the beneﬁt is accompanied
by additional complexities in resource allocation. In SLICE
architecture, in order to minimize utilized spectrum, one has
to solve the routing and spectrum allocation (RSA) problem, a
generalization of the routing and wavelength allocation (RWA)
problem. In this paper, we focus our attention to the on-line
version of RSA problem and provide an algorithm for the ring
network with a competitive ratio of min{O(log(dmax )), O(log(k))}
where k is the total number of requests and dmax is the maximum
demand in terms of the number of sub-carriers. Moreover, we
provide a heuristic for the network with arbitrary topology
and measure the effectiveness of the heuristic with extensive
simulation.

I. I NTRODUCTION
It is being increasingly recognized by the optical network
designers that in order to meet the challenges posed by the
explosive growth of the network trafﬁc, the networks must
be operated in the most innovative and efﬁcient manner. The
traditional WDM network operates at the granularity of a
wavelength, which may lead to inefﬁcient use of resources
as some connection requests may not have enough trafﬁc
to utilize the full capacity of a wavelength. However such
wastage of networking resources can be avoided if the optical
network can be made to operate at a ﬁner grain (i.e., subwavelength level) instead of the current practice of course
grain operation (i.e., wavelength level). Recent introduction of
Orthogonal Frequency Division Multiplexing (OFDM) technology in optical networks [1] offers an opportunity for
operating optical networks at a much ﬁner grain than what
is currently possible. The advantages offered by the OFDM in
terms of ﬂexibility and scalability originate from the unique
multicarrier nature of this technology [1].
Utilizing the OFDM technology, a spectrum efﬁcient and
scalable optical transport network called spectrum-sliced elastic optical path network (SLICE) was proposed recently [2].
Just as the ability to operate at a granularity ﬁner than a
wavelength (i.e., a sub-wavelength) will enable the network
operator to manage resources more efﬁciently, the same is

978-1-4673-3122-7/13/$31.00 ©2013 IEEE

true if the operator is provided with capability to operate at
super-wavelength granularity. Such a capability will be useful
for the network operator to meet large trafﬁc demand. The
goal of SLICE architecture is to allocate variable sized optical
bandwidths that matches with the user trafﬁc demands. It
achieves that goal by slicing off spectral resources of a route
and allocating only the requested amount to establish an endto-end optical path.
Although the sub-wavelength (sub-carrier) level allocation
capability of SLICE leads to more effective resource utilization, it also leads to additional complexities in network control
and management. First, if a call requests for d sub-carriers, the
network controller must allocate d consecutive sub-carriers to
this request. Second, if the paths corresponding to two requests
R1 and R2 share a ﬁber link, not only the set of carriers
allocated to R1 and R2 must be disjoint, in order to avoid
interference, they must be separated from each other in the
spectrum domain by a few carriers, known as guard carriers
or guard bands. The ﬁrst and the second constraints are known
as the sub-carrier consecutiveness constraint and the guardcarrier constraint respectively [3]. The introduction of the subcarrier consecutiveness constraint signiﬁcantly increases the
complexity of the Routing and Spectrum Assignment (RSA)
problem that needs to be solved in SLICE. The RSA problem may be informally deﬁned as follows: Given a network
topology and a set of call requests with varying demands (in
terms of the number of sub-carriers) ﬁnd a route for each
request and allocate a number of sub-carriers to each request
(equal to their requested demand), so that the utilized part
of the spectrum span is minimized. It may be noted that if
the demand of each request is one sub-carrier, then the RSA
problem reduces to the Routing and Wavelength Assignment
(RWA) problem, which has been studied extensively. One can
conceive of two different versions of the RSA problem - offline and on-line. In the off-line version all the requests are
known ahead of time before path and spectrum allocation for
any request is carried out. In the on-line version, the requests
come in a sequence and path and spectrum allocation for a
request has to carried out at the time of arrival of that request.
Because the off-line version has the luxury of knowing all
the requests, it can carry out better optimization of utilized
spectrum span than its on-line counterpart.
In this paper we study the on-line version of RSA problem.
Previous studies on the on-line version of the RSA problem,

3899

[4]–[9] primarily focus on the development of efﬁcient heuristics for the problem. The effectiveness of these heuristics are
primarily evaluated through simulation. To the best of our
knowledge, very little analytical results are available in the
literature regarding the performance of these heuristics. In
this paper we present analytical results relating to the on-line
version of the RSA problem when the network topology is
a ring. The performance of an on-line algorithm is measured
in terms of the metric competitive ratio. In this metric, the
performance of an on-line algorithm is compared with the
performance of an optimal off-line algorithm that knows the
sequence of requests in advance. The maximum ratio between
their respective performances, taken over all sequences, is
known as the competitive ratio of the algorithm [10].
In this paper, we provide an algorithm for the on-line version
of the RSA problem for the ring network with a competitive
ratio of min{O(log(dmax )), O(log(k))} where k is the total
number of requests, dmax = max1≤i≤k di , and di is the
demand in terms of the number of sub-carriers associated with
request Ri . Moreover, we provide a heuristic for the network
with arbitrary topology and measure the effectiveness of the
heuristic with extensive simulation.
The rest of the paper is organized as follows. We discuss
related works in section II. In section III we introduce deﬁnitions and notations. We present problem statement for the
on-line RSA problem in section IV. Analytical results for
on-line RSA in rings is presented in section V. A heuristic
and experimental results for the arbitrary network topology is
presented in section VI. Section VII concludes the paper.
II. R ELATED W ORK
Utilizing the optical OFDM technology, the SLICE architecture proposes a novel scheme for slicing off the spectral
resources of a route, resulting in more efﬁcient utilization [2].
The fact that the sub-carriers in the SLICE architecture have
to be assigned in a contiguous manner, led to the formulation
of the RSA problem. To the best of our knowledge, the
RSA problem was originally introduced in [4], [11], [12].
Since then a few other papers, [3], [13] have also studied
the RSA problem. In most of these studies [3], [12]–[14], the
authors propose an integer linear program based solution and a
heuristic solution for the off-line RSA problem. Based on the
experimental results, the authors claim effectiveness of their
heuristics.
The on-line version of RSA problem has been studied in
[4]–[9]. In all of these papers, the objective of the on-line
RSA problem is to maximize the number of requests that
can be satisﬁed and minimize the blocking probability. In
this version of on-line RSA problem, the number of available
spectrum sub-carriers is limited. The authors of these papers
proposed heuristic solutions mainly by modifying the Dijkstra
shortest path algorithm or using K-shortest path algorithm
accompanying with the First-Fit algorithm. To the best of
our knowledge none of these papers consider the objective
of minimizing the utilized spectrum while satisfying all the
requests. It may be the case that all the requests should be
satisﬁed while the utilized spectrum is minimized. In this paper

we propose a new heuristic for arbitrary network graphs. We
also modify the K-shortest path approach for this version of
the on-line RSA and through simulations we evaluate their
performance.
Most of the studies both on on-line and off-line RSA do
not present any analytical results for the RSA problem, even
for the simplest optical network topologies such as rings. The
ring topology is of particular importance in the optical domain
because of its application in metro networks and in some
long haul networks. A major thrust of our effort is to present
analytical results for the on-line RSA for optical networks with
ring structure.
III. D EFINITIONS AND N OTATIONS
Spectrum Slice/Interval: A number of consecutive sub-carriers
from ai to bi denoted by [ai , bi ], that is allocated to a speciﬁc
request Ri to establish a connection between (si , ti ) with di
sub-carriers. The length of this slice is bi − ai + 1 = di .
Spectrum Span/Spread: The total amount of spectrum used
for allocating a slice to all the requests; If Ri , 1 ≤ i ≤ k is
allocated the spectrum interval [ai , bi ] then the spectrum span
is [ min ai , max bi ].
1≤i≤k

1≤i≤k

Chromatic Number: The Chromatic Number, χ(G), of a graph
G = (V, E) is the fewest number of colors necessary to color
the nodes of the graph, such that no two adjacent nodes have
the same color.
Interval Chromatic Number (ICN): Consider a weighted graph
G∗ = (V, E, w) with a strictly positive integer weight w(v)
associated with each node v ∈ V . An interval t-coloring of
G∗ = (V, E, w) is a function c from V to {1,2, . . . , t} such
that c(x) + w(x) − 1 ≤ t and if both c(x) ≤ c(y) and (x, y) ∈
E then c(x) + w(x) − 1 < c(y). We can view an interval
coloring c of G∗ as assigning an interval [c(v), . . . , c(v) +
w(v) − 1] of w(v) consecutive colors to each vertex v so
that the intervals of colors assigned to two adjacent vertices
(i.e., the pair of nodes that has an edge between them) do not
overlap. If interval t-coloring is feasible for a graph G∗ then
G∗ is said to be interval t-colorable. The interval chromatic
number of G∗ , denoted by χint (G∗ ) is the least t such that
G∗ has a interval t-coloring [15].
Interval Graph: Let F be a family of non-empty sets. The
intersection graph of F is obtained by representing each set
in F by a node and connecting the two nodes with an edge, if
and only if the corresponding sets intersect. The intersection
graph of a family of intervals on a linearly ordered set (such
as the real line) is called Interval Graph.
Path Intersection Graph: Consider a graph G = (V, E) and
a set of paths P = {P1 , . . . , Pk }, where each Pi is a path
between a node pair (si , ti ), ∀i, 1 ≤ i ≤ k. A graph G =
(V  , E  ) is a Path Intersection Graph corresponding to P, if
each vertex pi ∈ V  corresponds to a path Pi ∈ P and two
nodes pi and pj in V  have an edge between them, if the
corresponding paths Pi and Pj in P have at least one common
edge in E.
IV. P ROBLEM F ORMULATION
In this section we provide a formal statement of the on-line
routing and spectrum allocation problem.

3900

On-line Routing and Spectrum Allocation (RSA) Problem:
A graph G = (V, E) representing the network topology is
given. The connection requests arrive in a sequence one by one
where k is the total number of requests. The ith connection
request is denoted by a triple Ri = (si , ti , di ), 1 ≤ i ≤ k,
where si represents a source node, ti represents a destination
node, and di represents the demand between si and ti in terms
of sub-carriers. Once a request Ri arrives without knowledge
of the future requests, assign a path Pi from si to ti and
assign a spectrum interval Ii = [ai , bi ] of length di to Pi ,
such that for every pair of requests i and j, j ≤ i the intervals
Ii and Ij do not overlap if the corresponding paths Pi and Pj
share an edge between them in G = (V, E). Moreover, if the
paths Pi and Pj overlap, not only the corresponding intervals
Ii and Ij must be non-overlapping, these two intervals must
be separated by a ﬁxed number of sub-carriers, known as
the guard band. The objective is to minimize spectrum span,
I = [min1≤i≤k ai , max1≤i≤k bi ]. Without loss of generality,
we number the ﬁrst available sub-carrier one and the rest are
numbered accordingly.
We note that guard-band constraint can be satisﬁed by
increasing the demand values by guard-band value g. In other
words, in an instance of RSA problem, RSA1 with guard-band
g1 > 0 and requests {Ri = (si , ti , di )|1 ≤ i ≤ k}, we can
increase the demand values in each request by g1 and consider
another instance of RSA, RSA2 where guard-band g2 = 0 and
for every request Ri in RSA1 , request Ri = (si , ti , di + g1 )
is added to RSA2 . Then the optimal solution of RSA2 can
be used to create the optimal solution of RSA1 by removing
the last g1 sub-carriers from each spectrum slice assigned to
each request (for the proof, reader is referred to the proof of
the Observation 4 in [16]). As a result, from this point onward
we assume that guard-band is zero.
The RSA problem has two distinct components - the routing
component and the spectrum allocation component. When
routing is given and the paths for the requests are known
then interval chromatic number (ICN) of the intersection graph
of request paths ﬁnds the solution of the SA problem. Let
G = (V  , E  , w) be the weighted path intersection graph of
paths of all requests where V  = {p1 , p2 , . . . , pk } and each
node pi corresponds to the path of request Ri and the weight
of pi is di ; i.e., w(pi ) = di . Let χint (G ) be the ICN of graph
G . In computation of χint (G ), each node pi ∈ V  is assigned
an interval [ai , bi ] of colors with length w(pi ) = di where the
intervals of two adjacent vertices do not intersect and total
number of distinct colors used is minimum. Therefore, interval
[ai , bi ] can be allocated to the path Pi in G and no two paths
with common edge intersect in their spectrum intervals. Hence,
the spectrum span of χint (G ) is sufﬁcient for the spectrum
allocation of requests in G with predeﬁned set of paths P.
Moreover, χint (G ) is the minimum spectrum span needed in
the SA problem; otherwise, it contradicts with χint (G ) being
the minimum interval chromatic number of G . It is known
that computation of ICN of interval graphs is NP-complete
(Problem SR2 in [17]).
Fig. 1 shows an example of SA instance where the network
graph is a ring with 8 nodes and requests are {R1 =

p1

1
8

2

p3

p4

3

7

6

4

p2

p5

5

(a)

(b)

Fig. 1. (a) An example of SA instance where the network graph is a ring
(b) Path intersection graph G of SA instance in (a)

(1, 3, 15), R2 = (1, 6, 6), R3 = (2, 5, 6), R4 = (2, 8, 6), R5 =
(4, 7, 12)}. Dashed lines show the paths for the requests. Fig.
1(b) depicts G , the path intersection graph of these paths
where w(p1 ) = 15, w(p2 ) = 6, w(p3 ) = 6, w(p4 ) = 6
and w(p5 ) = 12. In this example, χint (G ) is 24 where
the requests R1 to R5 are assigned intervals [1, 15], [13, 18],
[16, 21], [19, 24] and [1, 12] respectively.
V. O N - LINE ROUTING AND S PECTRUM A LLOCATION
P ROBLEM IN R INGS
Theorem 1: RSA problem (the off-line case) is NPComplete when the optical network topology is a Ring.
Proof: If the demands of the requests in the off-line RSA
instance are all equal to one, then RSA problem becomes RWA
problem. In [18], it is proven that the RWA problem for optical
networks with a ring topology is NP-complete. Since RWA
problem is a special case of the RSA problem, it follows that
the RSA problem for optical networks with a ring topology is
also NP-complete.
Next, we propose an on-line algorithm for RSA problem
when network topology is a ring. In this algorithm, ﬁrst we use
cut-one-link approach and after removing one link the induced
graph is a chain. In the chain for every request there exists
just one path. Therefore routing is trivial. For the spectrum
assignment, we use First-Fit technique that ﬁnds the ﬁrst free
spectrum interval ﬁt the demand of the current request. The
steps of the algorithms are explained in Algorithm 1.
Algorithm 1 On-line RSA in Ring
1: Remove an edge e ∈ E randomly; Let Gp be the induced
chain;
2: while A new request arrives do
3:
Find the path for the request in graph Gp ;
4:
Compute the ﬁrst free spectrum interval ﬁt the demand
of the current request
5: end while
Theorem 2: Algorithm 1 has competitive ratio of
min{O(log(dmax )), O(log(k))} where k is total number of
requests and dmax = max1≤i≤k di .
Proof: In order to compute the competitive ratio we need to
compare the spectrum span of Algorithm 1 with the optimal
spectrum span of off-line RSA where the sequence of requests
is known in advance. After removing one edge randomly
from G = (V, E) in Algorithm 1, the induced graph Gp
is a chain. Let OP T and OP Tp be the optimal spectrum

3901

span in RSA problem when network graph is G and Gp ,
respectively, and I be the size of the spectrum computed by
Algorithm 1. Clearly, the intersection graph of paths of the
requests in Gp is an interval graph (a path from node i to
node j in Gp can be interpreted as an interval from i to j).
Let Gp be the path intersection graph. Therefore, minimum
spectrum needed to satisfy requests in Gp is equivalent to the
χint (Gp ). Based on the paper [10], First-Fit algorithm will
have competitive ratio of min{O(log(dmax )), O(log(χGp ))}
for on-line interval coloring in Gp . Also it is obvious χGp ≤ k
(i.e., chromatic number of Gp is at most as large as the number
of nodes in Gp that is number of requests). Hence, we have (1)
I ≤ min{O(log(dmax )), O(log(k))} · OP Tp . We denote the
set of paths in the optimal solution of RSA (off-line) when
network graph is G by POP T . The paths in POP T can be
partitioned into two subsets, Pe1 and Pe2 such that Pe1 is the set
of paths that include edge e and the paths in Pe2 do not include
edge e. Let OP Te1 and OP Te2 be the ICN of the intersection
graph of paths in Pe1 and Pe2 respectively. Then we have (2)
OP T ≥ max(OP Te1 , OP Te2 ). Since all the paths in Pe1 have
intersection in edge e, their intervals do not intersect. Clearly,
(3) OP Tp ≤ OP Te1 + OP Te2 . The reason is that in the worst
case, all requests that were routed through edge e in POP T
are routed the other way in Gp and now they at most need
OP Te1 spectrum span not intersecting the spectrum allocated
to the paths in Pe2 . Therefore, using relations in (2) and (3)
we have OP Tp ≤ 2OP T . Also, based on relation (1) we can
conclude I ≤ min{O(log(dmax )), O(log(k))} · OP T .
VI. A H EURISTIC AND R ESULTS FOR G ENERAL G RAPHS
In this section ﬁrst we present our heuristic for on-line
RSA problem in general graphs. Then we present the results
of our extensive simulation that demonstrate the efﬁcacy of
our heuristic for the on-line RSA problem by comparing it
against (i) the optimal solution and (ii) the solution obtained
by executing the heuristic based on K-shortest path and FirstFit technique.
Minimum Sub-Carrier Path Heuristic (MSCP): The main
idea in our heuristic is that it tries to ﬁnd disjoint paths for
routing the requests to increase the reuse of sub-carriers in
spectrum allocation. Of this concern, we deﬁne a new weight
function on the edges (ﬁbers) of the network where weight
of an edge e ∈ E, w(e) will be largest sub-carrier number
that is used in that edge. We also deﬁne the weight of a path,
P from node s to node t to be maxe∈P {w(e)}. For each
new request, M SCP selects the path with minimum weight.
The minimum weight path can be computed by modifying the
distance function in Dijkstra algorithm so that it considers
the new weight function as the distance. After ﬁnding the
path, M SCP uses First-Fit algorithm to ﬁnd the ﬁrst available
spectrum slice with the length of the request demand in all
the edges of the path. Then, M SCP updates the weight of
every edge in the path to the largest sub-carrier so far used
in that edge. For each request, time complexity of minimumweight path computation is O(|V |2 ) and First-Fit algorithm
takes O(k|V |2 ) where k is the number of requests. Hence,
time complexity of M SCP is O(k 2 |V |2 ).

K-Shortest Path Heuristic (KSP): In this heuristic, initially
K shortest paths are computed between every pair of nodes
in the network using [19] algorithm with O(k|V |3 ). When
a request Ri arrives, for every path in the K shortest paths
between si and ti we compute First-Fit algorithm to ﬁnd the
ﬁrst available spectrum slice [ai , bi ] with the length of di .
Then we select the path whose ﬁrst available spectrum slice
[ai , bi ] has the smallest bi . This algorithm takes O(Kk 2 |V |2 )
for satisfying all the k requests.
We perform our experiments on the NSFnet (Fig. 2(a)) and
the ﬁber network of Level-3 that spans Europe (Fig. 3(a)) [20].
We view the NSFnet and Level-3 networks as examples of a
small and a large network respectively.
In Fig. 2(b), we present the results obtained from ILP ,
M SCP and KSP when executed on the NSFnet. We ﬁnd the
optimal solution of the RSA problem (off-line) by solving an
ILP using the software package CPLEX. Since computing the
optimal solution by ILP takes considerable amount of time,
we need to do this set of experiments for small number of
requests. In this set of experiments, the number of requests,
k, is varied from 2 to 6 with step of one. For each value of
k, we generate 10 instances. In each instance we generate k
requests randomly and consider them one at a time. For this
set of experiments all the demand values are at most 5, (i.e.,
dmax ≤ 5). The average spectrum span computed by each of
the three methods is shown in Fig. 2(b). It may be observed
that the average spectrum span of M SCP is closest to the
ILP almost in all cases. The ratio of the average spectrum
span of M SCP to ILP is at most 1.28 demonstrating the
closeness of the M SCP to the optimal. The results in these
experiments also show that M SCP works better than KSP
algorithm in almost all the cases even when number of paths
in KSP is K = 3. We repeat similar experiments for larger
value of k, where k = 10 and we change the value of dmax
from 5 to 25. The result of these experiments is depicted in
Fig. 2(c). It can be observed that spectrum span in M SCP is
at least 12% smaller than the span in KSP where K = 1 and
it is even smaller than the one in KSP where K = 2. When
K = 3 in KSP , KSP needs smaller spectrum span than
M SCP but its time complexity is at least 3 times M SCP .
We perform our next set of experiments on the Level-3
network shown in Fig. 3(a). In these experiments, ﬁrst, we
vary k from 10 to 60 with step of 10. For a speciﬁc value of k
we generate 10 instances. In all these instances, the maximum
demand is limited to 10 (i.e., dmax ≤ 10). The average utilized
spectrum span is shown in Fig. 3(b). These results show that
M SCP efﬁcacy with respect to utilized spectrum span is
almost the same as KSP when K = 2. We also conduct
experiments for the case that values of dmax is varied from 5
to 25 with step of 5, while keeping the number of requests
k constant at 20. We compute the average spectrum span
over 10 random instances for each value of dmax . The results
are shown in Fig. 3(c). According to these results, M SCP ’s
performance is better than KSP when K = 2 especially for
larger values of dmax . According to the last experiments we
may conclude that M SCP outperforms KSP when K = 2
for larger values of dmax .

3902

50

NJ

IL

UT

CA1

PA

NE
CO
MD

dmax ≤ 5

7
6

KSP (K=1)
KSP (K=2)
KSP (K=3)
MSCP
ILP

5
4

3

Average Spectrum Span

NY

WA

Average Spectrum Span

8
MI

2

CA2

30
KSP (K=1)
KSP (K=2)
KSP (K=3)
MSCP

20
10
0

1

TX

k = 10

40

2

GA

3
4
5
Number of requests (k)

(a)

6

7

0

5

(b)

10
15
20
25
Maximum demand (dmax)

30

(c)

Fig. 2. (a) The 14-node NSF Network, (b) The average spectrum span in NSF Network for different values of k where dmax ≤ 5, (c) different values of
dmax where k = 10
180

160

dmax ≤ 10

160

100
80

KSP (K=1)

60

KSP (K=2)

40

MSCP

20
0

(a)

120
100
KSP (K=1)
KSP (K=2)
MSCP

80
60
40
20

0

Fig. 3.

k = 20

140

120

Average Spectrum Span

Average Spectrum Span

140

20

40
60
Number of requests (k)

(b)

80

0
0

10
20
30
Maximum demand (dmax)

40

(c)

(a) Level-3 network over Europe, (b) The average spectrum span in Level-3 network for dmax ≤ 10, (c) k = 20

VII. C ONCLUSION
In this paper we study on-line version of Routing and
Spectrum Allocation problem in OFDM-based optical networks. We propose an algorithm for the ring network with
a competitive ratio of min{O(log(dmax )), O(log(k))} where
k is the total number of requests and dmax is the maximum
demand. In addition, we provide a heuristic for networks with
arbitrary topology and measure its effectiveness with extensive
simulation. In future, we plan to develop efﬁcient algorithms
for on-line RSA in networks with tree and grid topologies. We
also would like to extend our results to the case that different
modulation models can be used.
Acknowledgement: The research was supported in part by
the DTRA grant HDTRA1-09-1-0032 and the AFOSR grant
FA9550-09-1-0120.
R EFERENCES
[1] W. Shieh, “Ofdm for ﬂexible high-speed optical networks,” Journal of
Lightwave Technology, vol. 29, no. 10, pp. 1560 –1577, 2011.
[2] M. Jinno, H. Takara, B. Kozicki, Y. Tsukishima, Y. Sone, and S. Matsuoka, “Spectrum-efﬁcient and scalable elastic optical path network: architecture, beneﬁts, and enabling technologies,” IEEE Communications
Magazine, vol. 47, no. 11, pp. 66 –73, 2009.
[3] Y. Wang, X. Cao, and Y. Pan, “A study of the routing and spectrum allocation in spectrum-sliced elastic optical path networks,” in INFOCOM,
2011, pp. 1503–1511.
[4] M. Jinno, B. Kozicki, H. Takara, A. Watanabe, Y. Sone, T. Tanaka, and
A. Hirano, “Distance-adaptive spectrum resource allocation in spectrumsliced elastic optical path network [topics in optical communications],”
IEEE Communications Magazine, vol. 48, no. 8, pp. 138 –145, 2010.
[5] K. Christodoulopoulos, I. Tomkos, and E. Varvarigos, “Dynamic bandwidth allocation in ﬂexible ofdm-based networks,” in OFC/NFOEC,
2011.
[6] X. Wan, N. Hua, and X. Zheng, “Dynamic routing and spectrum assignment in spectrum-ﬂexible transparent optical networks,” IEEE/OSA
Journal of Optical Communications and Networking, vol. 4, no. 8, pp.
603 –613, 2012.

[7] T. Takagi, H. Hasegawa, K. Sato, Y. Sone, B. Kozicki, A. Hirano,
and M. Jinno, “Dynamic routing and frequency slot assignment for
elastic optical path networks that adopt distance adaptive modulation,”
in OFC/NFOEC, 2011.
[8] A. Castro, L. Velasco, M. Ruiz, M. Klinkowski, J. P. Fernández-Palacios,
and D. Careglio, “Dynamic routing and spectrum (re)allocation in future
ﬂexgrid optical networks,” Compututer Networks, vol. 56, no. 12, pp.
2869–2883, 2012.
[9] G. Shen and Q. Yang, “From coarse grid to mini-grid to gridless: How
much can gridless help contentionless?” in OFC/NFOEC, 2011.
[10] M. G. Luby, “Tight bounds for dynamic storage allocation,” SIAM
Journal on Discrete Mathematics, vol. 9, no. 1, pp. 155–166, Feb. 1996.
[11] A. N. Patel, P. N. Ji, J. P. Jue, and T. Wang, “Routing, wavelength
assignment, and spectrum allocation in transparent ﬂexible optical wdm
(fwdm) networks,” in Photonics in Switching, 2010.
[12] K. Christodoulopoulos, I. Tomkos, and E. A. Varvarigos, “Routing
and spectrum allocation in ofdm-based optical networks with elastic
bandwidth allocation,” in GLOBECOM, 2010, pp. 1–6.
[13] M. Klinkowski and K. Walkowiak, “Routing and spectrum assignment
inspectrum sliced elastic optical path network,” IEEE Communications
Letters, vol. 15, no. 8, pp. 884–886, 2011.
[14] K. Christodoulopoulos, I. Tomkos, and E. Varvarigos, “Elastic bandwidth allocation in ﬂexible ofdm-based optical networks,” Journal of
Lightwave Technology, vol. 29, no. 9, pp. 1354 –1366, 2011.
[15] H. A. Kierstead, “A polynomial time approximation algorithm for
dynamic storage allocation,” Discrete Mathematics, vol. 87, no. 2-3,
pp. 231–237, 1991.
[16] S. Shirazipourazad, C. Zhou, Z. Derakhshandeh, and A. Sen. On
routing and spectrum allocation in spectrum-sliced optical networks.
[Online]. Available: http://www.public.asu.edu/∼sshiraz1/RSA.pdf
[17] M. R. Garey and D. S. Johnson, Computers and Intractability; A Guide
to the Theory of NP-Completeness. W. H. Freeman & Co., 1990.
[18] T. Erlebach and K. Jansen, “The complexity of path coloring and call
scheduling,” Theoretical Computer Science, vol. 255, no. 1-2, pp. 33–50,
2001.
[19] J. Y. Yen, “Finding the k shortest loopless paths in a network,”
Management Science, vol. 17, no. 11, pp. 712–716, 1971.
[20] Level 3 Communications, Network Map. [Online]. Available: http:
//www.level3.com/en/resource-library/maps/level-3-network-map/

3903

2013 Proceedings IEEE INFOCOM

On Routing and Spectrum Allocation in
Spectrum-sliced Optical Networks
Shahrzad Shirazipourazad, Chenyang Zhou, Zahra Derakhshandeh and Arunabha Sen
School of Computing, Informatics and Decision System Engineering
Arizona State University
Email: {sshiraz1, czhou24, zderakhs, asen}@asu.edu

Abstract—The orthogonal frequency division multiplexing
(OFDM) technology provides an opportunity for efﬁcient resource utilization in optical networks. It allows allocation of
multiple sub-carriers to meet trafﬁc demands of varying size.
Utilizing OFDM technology, a spectrum efﬁcient and scalable optical transport network called SLICE was proposed recently. The
SLICE architecture enables sub-wavelength, super-wavelength
resource allocation and multiple rate data trafﬁc that results
in efﬁcient use of spectrum. However, the beneﬁt is accompanied
by additional complexities in resource allocation. In SLICE
architecture, in order to minimize the utilized spectrum, one
has to solve the routing and spectrum allocation problem (RSA).
In this paper, we focus our attention to RSA and (i) prove that
RSA is NP-complete even when the optical network topology
is as simple as a chain or a ring, (ii) provide approximation
algorithms for RSA when the network topology is a binary
tree or a ring, (iii) provide a heuristic for the network with
arbitrary topology and measure the effectiveness of the heuristic
with extensive simulation. Simulation results demonstrate that
our heuristic signiﬁcantly outperforms several other heuristics
proposed recently for RSA.

I. I NTRODUCTION
The phenomenal growth of the Internet trafﬁc in the last
few years and its anticipated growth in the next few years,
necessitates introduction of innovative and efﬁcient technology
solutions in optical networks of the future. The traditional
WDM network operates at the granularity of a wavelength,
which may lead to inefﬁcient use of resources as some
connection requests may not have enough trafﬁc to utilize
the full capacity of a wavelength. However such wastage of
networking resources can be avoided if the optical network can
be made to operate at a ﬁner grain instead of the current practice of course grain operation. Recent introduction of OFDM
technology in optical networks [1] offers an opportunity for
operating optical networks at a much ﬁner grain than what is
currently possible. The advantages offered by the OFDM in
terms of ﬂexibility and scalability originate from the unique
multicarrier nature of this technology [1].
Utilizing the OFDM technology, a spectrum efﬁcient and
scalable optical transport network called spectrum-sliced elastic optical path network (SLICE) was proposed recently [2].
Just as the ability to operate at a granularity ﬁner than a
wavelength (i.e., a sub-wavelength) will enable the network
operator to manage resources more efﬁciently, the same is
true if the operator is provided with capability to operate at
super-wavelength granularity. Such a capability will be useful

978-1-4673-5946-7/13/$31.00 ©2013 IEEE

for the network operator to meet large trafﬁc demand. The
goal of SLICE architecture is to allocate variable sized optical
bandwidths that matches with the user trafﬁc demands. It
achieves that goal by slicing off spectral resources of a route
and allocating only the requested amount to establish an endto-end optical path.
Although the sub-wavelength (sub-carrier) level allocation
capability of SLICE leads to more effective resource utilization, it also leads to additional complexities in network control
and management. First if a call requests for d sub-carriers, the
network controller must allocate d consecutive sub-carriers to
this request. Second, if the paths corresponding to two requests
R1 and R2 share a ﬁber link, not only the set of carriers
allocated to R1 and R2 must be disjoint, in order to avoid
interference, they must be separated from each other in the
spectrum domain by a few carriers, known as guard carriers
or guard bands. The ﬁrst and the second constraints are known
as the sub-carrier consecutiveness constraint and the guardcarrier constraint respectively [3]. The introduction of the
sub-carrier consecutiveness constraint signiﬁcantly increases
the complexity of the Routing and Spectrum Assignment
problem (RSA) that needs to be solved in SLICE. RSA may
be informally deﬁned as follows: Given a network topology
and a set of call requests with varying demands (in terms of
the number of sub-carriers) ﬁnd a route for each request and
allocate a number of sub-carriers to each request (equal to their
requested demand), so that the utilized part of the spectrum
is minimized. It may be noted that if the demand of each
request is one sub-carrier, then RSA reduces to the Routing
and Wavelength Assignment problem (RWA), which has been
studied extensively. RSA is signiﬁcantly more complex than
RWA, as it is NP-complete even for a simple network topology,
such as a chain. It may be noted that RWA for the same
topology can be solved with a low order polynomial time
algorithm.
To the best of our knowledge, RSA was originally introduced in [4], [5], [6]. Since then a few other papers, [3],
[7] have also studied RSA and proposed solution techniques.
In most of these studies [3], [6], [7], the authors propose
an integer linear program based solution and a heuristic
solution. Based on the experimental results, the authors claim
effectiveness of their heuristics. Most of these studies do not
present any analytical results for RSA, even for the simplest
optical network topologies such as trees and rings. The ring

385

2013 Proceedings IEEE INFOCOM

topology is of particular importance in the optical domain
because of its application in metro networks and in some
long haul networks. A major thrust of our effort is to present
analytical results for RSA for optical networks with special
structures, such as binary trees and rings.
In this paper we study RSA for networks with arbitrary
structure as well as the networks with speciﬁc structure, such
as a tree and a ring. We provide NP-completeness proof and
approximation algorithms for RSA when the network topology
is a chain, binary tree or a ring. We also present a heuristic for
networks with arbitrary topology and measure the effectiveness
of the heuristics with extensive simulation.
The rest of the paper is organized as follows. In section
II we introduce deﬁnitions and notations. We present formal
problem statement for RSA and a few preliminary observations
in sections III and IV respectively. Analytical results for
chains, trees and rings for RSA is presented in section V.
Experimental results for RSA with arbitrary network topology
is presented in section VI. The section VII concludes the paper.
II. D EFINITIONS AND N OTATIONS
Spectrum Slice/Interval: A number of consecutive sub-carriers
from ai to bi denoted by [ai , bi ], that is allocated to a speciﬁc
request Ri to establish a connection between (si , ti ) with di
sub-carriers. The length of this slice is bi − ai + 1 = di .
Spectrum Span/Spread: The total amount of spectrum used
for allocating a slice to all the requests; If Ri , 1 ≤ i ≤ k is
allocated the spectrum interval [ai , bi ] then the spectrum span
is [ min ai , max bi ].
1≤i≤k

1≤i≤k

Interval Chromatic Number (ICN): An interval t-coloring of
a weighted graph G∗ = (V, E, w) is a function c from V
to {1,2, . . . , t} such that c(x) + w(x) − 1 ≤ t and if both
c(x) ≤ c(y) and (x, y) ∈ E then c(x) + w(x) − 1 < c(y). We
can view an interval coloring c of G∗ as assigning an interval
[c(v), . . . , c(v) + w(v) − 1] of w(v) colors to each vertex v so
that the intervals of colors assigned to two adjacent vertices
(i.e., the pair of nodes that has an edge between them) do not
overlap. If interval t-coloring is feasible for a graph G∗ then
G∗ is said to be interval t-colorable. The interval chromatic
number of G∗ , denoted by χint (G∗ ) is the least t such that
G∗ has a interval t-coloring [8].
Interval Graph: Let F be a family of non-empty sets. The
intersection graph of F is obtained by representing each set
in F by a node and connecting the two nods with an edge, if
and only if the corresponding sets intersect. The intersection
graph of a family of intervals on a linearly ordered set (such
as the real line) is called Interval Graph.
Chordal Graph: An undirected graph G = (V, E) is called
a Chordal Graph if every cycle of length strictly greater than
3 posses a chord, that is, an edge joining two nonconsecutive
nodes in the cycle.
Path Intersection Graph: Consider a graph G = (V, E) and
a set of paths P = {P1 , . . . , Pk }, where each Pi is a path
between a node pair (si , ti ), ∀i, 1 ≤ i ≤ k. A graph G =
(V  , E  ) is a Path Intersection Graph corresponding to P, if
each vertex pi ∈ V  corresponds to a path Pi ∈ P and two

p1

1
8

2

p3

p4

3

7

6

4

p2

p5

5

(a)

(b)

Fig. 1. (a) An example of SA instance where the network graph G is a ring
(b) Path intersection graph G of paths in SA instance in (a)

nodes pi and pj in V  have an edge between them, if the
corresponding paths Pi and Pj in P have at least one common
edge in E.
III. P ROBLEM S TATEMENT
In this section we provide a formal statement of RSA. We
formulate RSA as it is deﬁned in [6], [7].
Routing and Spectrum Assignment Problem (RSA): Given
a graph G = (V, E) representing the network topology, and
a set of request triples Ri = (si , ti , di ), 1 ≤ i ≤ k, where
si represents a source node, ti represents a destination node,
and di represents the demand between si and ti in terms of
sub-carriers, ﬁnd (i) a set of paths P = {P1 , . . . , Pk }, such
that each Pi , 1 ≤ i ≤ k is a path from si to ti and (ii)
assign a spectrum interval Ii = [ai , bi ] of length di to each
Pi , such that all the intervals Ii , 1 ≤ i ≤ k can be ﬁtted
within a smallest interval I = [ min ai , max bi ] (spectrum
1≤i≤k

1≤i≤k

span) such that the intervals Ii and Ij do not overlap if the
corresponding paths Pi and Pj share an edge between them
in G = (V, E). Moreover, if the paths Pi and Pj overlap, not
only the corresponding intervals Ii and Ij be non-overlapping,
these two intervals must be separated by a ﬁxed number of subcarriers, known as the guard band. Without loss of generality,
we number the ﬁrst available sub-carrier one and the rest are
numbered accordingly.
IV. P RELIMINARY O BSERVATIONS
RSA has two distinct components - the routing component
and the spectrum allocation component. It is tempting to think
that the complexity of RSA arises due to the fact that it
has to deal with two interdependent subproblems at the same
time. However, this is not necessarily true. In certain types of
network topology (e.g., a tree), there exists only one path that
connects the source node si to the destination node ti for all
i, 1 ≤ i ≤ k. This implies that routing is a trivial problem
in these networks. However, even in these networks, spectrum
allocation (SA) is not only non-trivial, it is computationally
hard. In the following, we make two observations. In the ﬁrst
observation, we assume there is no guard carrier constraint, i.e.
size of the guard band is zero. Then, in the second observation
we explain how we deal with guard carrier constraint.
Observation 1: Interval Chromatic Number (ICN) ﬁnds the
solution to the SA problem.

386

2013 Proceedings IEEE INFOCOM

In the SA problem a set of paths P on graph G is associated to
the set of requests {Ri , 1 ≤ i ≤ k}. Let G = (V  , E  , w) be
the weighted path intersection graph of paths in P where V  =
{p1 , p2 , . . . , pk } and each node pi corresponds to a path Pi ∈
P and the weight of pi is di ; i.e., w(pi ) = di . Let χint (G ) be
the ICN of graph G . In computation of χint (G ), each node
pi ∈ V  is assigned an interval [ai , bi ] of colors with length
w(pi ) = di where the intervals of two adjacent vertices do not
intersect and total number of distinct colors used is minimum.
Therefore, interval [ai , bi ] can be allocated to the path Pi in G
and no two paths with common edge intersect in their spectrum
intervals. Hence, the spectrum span of χint (G ) is sufﬁcient
for the spectrum allocation of requests in G with predeﬁned
set of paths P. Moreover, χint (G ) is the minimum spectrum
span needed in the SA problem; otherwise, it contradicts with
χint (G ) being the minimum interval chromatic number of G .
Fig. 1 shows an example of SA instance where the network
graph is a ring with 8 nodes and requests are {R1 =
(1, 3, 15), R2 = (1, 6, 6), R3 = (2, 5, 6), R4 = (2, 8, 6), R5 =
(4, 7, 12)}. Dashed lines show the paths for the requests.
Fig. 1(b) depicts G . In this example, χint (G ) is 24 where
the requests R1 to R5 are assigned intervals [1, 15], [13, 18],
[16, 21], [19, 24] and [1, 12] respectively.
Observation 2: An algorithm that ﬁnds the optimal solution
for RSA when the guard band g = 0, can be utilized to ﬁnd
the optimal solution for RSA when g > 0.
Proof: We note that guard-band constraint can be satisﬁed
by increasing the demand values by guard-band value g. In
other words, in an instance of RSA, RSA1 with guard-band
g1 > 0 and requests {Ri = (si , ti , di )|1 ≤ i ≤ k}, we can
increase the demand values in each request by g1 and consider
another instance of RSA, RSA2 where guard-band g2 = 0 and
for every request Ri in RSA1 , request Ri = (si , ti , di + g1 )
is added to RSA2 . Then the optimal solution of RSA2 can
be used to create the optimal solution of RSA1 by removing
the last g1 sub-carriers from each spectrum slice assigned to
each request (for the proof, reader is referred to the proof of
the Observation 4 in [9]). As a result, from this point onward
we assume that g = 0.
V. ROUTING AND S PECTRUM A LLOCATION P ROBLEM
A. RSA in Chains
Theorem 1: RSA is NP-Complete when G is a chain.
Proof: Reader is referred to the proof of the Theorem 1 in [9].
Theorem 2: There exists an approximation algorithm with
a performance bound of 2 +  for RSA when G is a chain.
Proof: When the graph G = (V, E) is a chain, then there
exists only one path for each request and routing is trivial.
Let P = {P1 , . . . , Pk } be the set of paths of requests in G.
Clearly, intersection graph of paths in P, G = (V  , E  ) is an
interval graph. Therefore, RSA in a chain will be equivalent
to the computation of ICN of interval graph G . In [10], an
approximation algorithm with performance bound of 2 +  for
computation of ICN of interval graphs is proposed. Therefore,
the same algorithm will have an approximation ratio of 2 + 
for RSA when graph G is a chain.

B. RSA in Binary Trees
Theorem 3: There exists an approximation algorithm with
a performance bound of O(log k) for RSA when G is a binary
tree.
Proof: When network topology is a tree, then there exists
only one path for each request. Let P be the set of paths of
requests. It has been shown in [11] that the intersection graph
of paths in a binary tree is a chordal graph. Hence, RSA when
network is a binary tree is equivalent to the computation of
ICN of the path intersection graph which is a chordal graph.
In [12] an approximation algorithm with performance bound
of O(log k) is proposed for computation of ICN in chordal
graphs when k is the number of nodes. Therefore, the same
algorithm will ﬁnd a solution for RSA with approximation
ratio of log k where k is the number of requests.
C. RSA in Rings
In [13], it is proven that RWA for networks with a ring
topology is NP-complete. Since RWA is a special case of RSA,
it follows that RSA for a ring network is also NP-complete.
We propose an approximation algorithm called RSA-R,
for RSA in a ring network. In RSA-R, we use cut-one-link
approach and take advantage of the approximation algorithm
for computation of ICN in interval graphs proposed in [10].
Algorithm 1 RSA-R
1: Remove an edge e ∈ E randomly; Let Gp be the induced
chain;
2: Compute the set of paths P  for the requests in graph Gp ;
3: Compute and return the approximated ICN of intersection
graph of paths in P  using algorithm in [10];
Theorem 4: Algorithm 1 has performance bound of 4 + 2.
Proof: After removing one edge randomly from G =
(V, E), the induced graph Gp is a chain. Let OP T and OP Tp
be the optimal spectrum span in RSA when network graph is
G and Gp , respectively, and I be the size of the spectrum
computed by Algorithm 1. Based on Theorem 2 we have (1)
I ≤ (2 + )OP Tp . We denote the set of paths in the optimal
solution of RSA when network graph is G by POP T . The paths
in POP T can be partitioned into two subsets, Pe1 and Pe2 such
that Pe1 is the set of paths that include edge e and the paths in
Pe2 do not include edge e. Let OP Te1 and OP Te2 be the ICN
of the intersection graph of paths in Pe1 and Pe2 respectively.
Then we have (2) OP T ≥ max(OP Te1 , OP Te2 ). Since all
the paths in Pe1 have intersection in edge e, their intervals
do not intersect. Clearly, (3) OP Tp ≤ OP Te1 + OP Te2 . The
reason is that in the worst case, all requests that were routed
through edge e in POP T are routed the other way in Gp and
now they at most need OP Te1 spectrum span not intersecting
the spectrum allocated to the paths in Pe2 . Therefore, using
relations in (2) and (3) we have OP Tp ≤ 2OP T . Also, based
on relation (1) we can conclude I ≤ (4 + 2)OP T .
D. RSA Problem in General Graphs
Since RSA is NP-Complete, computation of the optimal
solution in a reasonable amount of time may not be possible

387

2013 Proceedings IEEE INFOCOM

unless P=NP. As such we need to ﬁnd a heuristic solution for
RSA. In this section we propose a heuristic called DP H For
RSA for general networks. We evaluate the performance of
DP H in section VI using simulations on some real optical
networks. The main idea in our heuristic is that it tries to ﬁnd
disjoint paths for routing the requests to increase the reuse of
sub-carriers in spectrum allocation. First the requests are sorted
in decreasing order of their demands. In each iteration, a set
of disjoint paths is found. Then, spectrum allocation of these
requests is computed. Since the paths found in one iteration
are disjoint their spectrum interval can intersect. Also, DP H
checks if any of the paths can reuse the sub-carriers used in
previous iterations trying to minimize the spectrum span.
Algorithm 2 DP H, Heuristic for RSA in general graphs
1: Sort the requests in decreasing order of the demands; Let
R = {R1 , R2 , . . . Rk } be the sorted set.
2: Deﬁne I to be a set of intervals, Ii = [ai , bi ], 1 ≤ i ≤ k
allocated to Ri s where the intervals in I are sorted in
decreasing order of bi s. Initially, I includes an interval
I0 = [1, 1] corresponding to an empty path P0 .
3: while there is a request that is not allocated a path do
4:
D = ∅; D is a set of disjoint paths sorted in decreasing
order of demands.
5:
Pf = shortest path between sf and tf in G where Rf
is the ﬁrst request in R not allocated spectrum interval;
6:
Add Pf to D;
7:
GT = (V, ET ) where ET = E − {e|e ∈ Pf };
8:
for all Ri , f ≤ i ≤ k that is not allocated a path do
9:
if There exists a path between si and ti in GT then
10:
Pi = the shortest path between si to ti in GT ;
11:
Add Pi to D; and GT = (V, ET − {e|e ∈ Pi });
12:
end if
13:
end for
14:
for all Paths Pi ∈ D do
15:
for all Intervals Ij ∈ I do
16:
if paths Pi and Pj do not intersect in any edge and
Ij is the last interval in I then
17:
ai = aj ;
18:
else if paths Pi and Pj do not intersect in any edge
and Il is the interval right after Ij in I then
19:
ai = bl + 1;
20:
else
21:
ai = bj + 1; break;
22:
end if
23:
end for
24:
Ii = [ai , ai + di − 1]; Add Ii to I such that I remains
sorted in decreasing order of bi .
25:
end for
26: end while
VI. E XPERIMENTAL R ESULTS AND D ISCUSSION
In this section we present results of our extensive simulation
that demonstrate the efﬁcacy of our heuristic for RSA by
comparing it against (i) the optimal solution and (ii) the
solutions of the two heuristics proposed in [3].

We ﬁnd the optimal solution of RSA by solving an ILP
using the software package CPLEX. In order to minimize the
used spectrum, our heuristic exploits disjoint paths to establish
routes between source and destination node pairs. One of the
heuristic proposed in [3] uses shortest path with maximum
spectrum reuse and the other uses balanced load spectrum
allocation. We will refer to the optimal solution as ILP , our
heuristic as DP H and the two heuristics in [3] as SP SR and
BLSA respectively.
We perform our experiments on the NSFnet (Fig. 2(a))
and the ﬁber network of Level-3 that spans the continental
United States (Fig. 3(a)) [14]. We view the NSFnet and Level3 networks as examples of a small and a large network
respectively. In Fig. 2(b), we present the results obtained
from ILP , SP SR, BLSA and DP H when executed on the
NSFnet. In this set of experiments, the number of requests, k,
is varied from 2 to 6 with step of one. For each value of k,
we generate 10 instances where each instance has k random
requests. For this set of experiments all the demand values
are at most 5, (i.e., dmax ≤ 5). The average spectrum span
computed by each of the four methods is shown in Fig. 2(b).
It may be observed that the average spectrum span of DP H
is closest to the ILP . The ratio of the average spectrum span
of DP H to ILP is at most 1.2 demonstrating the closeness
of the DP H to the optimal. When the number of requests
is increased to larger values, such as 10, 15, 20 and beyond,
the CPLEX could no longer ﬁnd the optimal solution within a
reasonable amount of time. For large request sets, we compare
the performance of SP SR, BLSA and DP H and present the
results in Fig. 2(c). It may be observed that as the size of the
request set increases, DP H consistently outperforms SP SR
and BLSA and the average reduction of spectrum span is
more than 18% when the request set size exceeds 10.
We perform our next set of experiments on the Level-3
network shown in Fig. 3(a). In these experiments, ﬁrst, we
vary k from 5 to 25 with step of 5. For a speciﬁc value
of k we generate 100 instances. In all these instances, the
maximum demand is limited to 5 (i.e., dmax ≤ 5). We
present the comparative results of spectrum span computed by
DP H, SP SR and BLSA in Table I. The comparative results
between DP H and SP SR is shown in DP H − SP SR and
comparative results between DP H and BLSA is shown in
DP H − BLSA part of the Table I. In columns W , L and T ,
the results under % shows the percentage of times that DP H
Wins, Loses and Ties against the SP SR and BLSA heuristics.
We say that DP H “Wins” against the SP SR (BLSA), if
the spectrum span computed by DP H is smaller than the
span computed by SP SR (BLSA). For example, when the
number of requests, k, is 20, DP H wins 94% of times against
SP SR and 89% of times against BLSA. Moreover, average
savings in the number of sub-carriers (spectrum span) using
DP H against SP SR is 4.44 and against BLSA is 3.81, when
k = 20. It may be noted that in the very few cases, where the
DP H loses against SP SR or BLSA, the amount (measured
in terms of the number of sub-carriers) by which it loses is
much smaller than the large number of cases when it wins.

388

2013 Proceedings IEEE INFOCOM

20

NJ

IL

UT

PA

NE
CO
MD
CA2

dmax ≤ 5

8
7

6

SPSR

5

BLSA

4

DPH
ILP

3
TX

0

GA

2

4
6
Number of requests (k)

(a)
Fig. 2.

(b)

35

dmax ≤ 5

Average spectrum span

30
25
20

10

SPSR
BLSA

5

DPH

0
0

5

10
15
20
Number of requests (k)

25

(c)

10

0

5

10

15

20

Number of requests (k)

25

The research was supported in part by the DTRA grant
HDTRA1-09-1-0032 and the AFOSR grant FA9550-09-10120.
30

R EFERENCES

(a)
(b)
Fig. 3. (a) Level-3 ﬁber network over US, (b) The average spectrum span
in Level-3 network for dmax ≤ 5
TABLE I
R ESULTS FOR USA NETWORK , dmax ≤5
DP H-SP SR
W
L
AVG
%
AVG
2.26
0
3.1
0
4.04
2
1.5
4.44
6
1.5
5.91
0

T
%
30
17
5
0
2

%
35
77
86
89
93

DP H-BLSA
W
L
AVG
%
AVG
1.43
7
1.29
2.23
6
1.33
3.26
3
1.5
3.81
9
2
4.88
1
2

that RSA is NP-complete when the network topology is a
chain or a ring and provide approximation algorithms for
RSA in the network with these topologies. We also provide
a heuristic for networks with arbitrary topology and measure
the effectiveness of the heuristic with extensive simulation.
ACKNOWLEDGEMENT

SPSR
SPSR'
BLSA
BLSA'
DPH

15

5

%
70
83
93
94
98

dmax ≤ 5
15

(a) The 14-node NSF Network, (b) The average spectrum span in 14-node NSF Network for k ≤ 6 and dmax ≤ 5 (c) k ≥ 5 and dmax ≤ 5

The average loss by DP H against SP SR is 1.5 and against
BLSA is 2, when k = 20.

No.
of
Req.
5
10
15
20
25

8

Average spectrum span

NY

WA

CA1

Average spectrum span

9
MI

T
%
58
17
11
2
6

The dramatic improvement in performance by DP H against
SP SR and BLSA might lead one to believe that the efﬁcacy
of DP H is derived primarily from its SA phase. To evaluate
if this is indeed correct, we modiﬁed the SA part of both
SP SR and BLSA and used the same spectrum allocation
technique as in DP H. We denote the modiﬁed heuristics by
SP SR and BLSA . Fig. 3(b) depicts the average spectrum
span used in all ﬁve heuristics on the same sets of instances
used in previous experiments. We observe that DP H is more
efﬁcient than all the other heuristics even though SP SR and
BLSA use the same spectrum allocation technique as DP H.
Modiﬁcation of the spectrum allocation technique in SP SR
and BLSA slightly improves their performance. However, for
k ≥ 15, DP H still performs better than BLSA and SP SR
by at least 11% and 12.4% respectively. These results clearly
demonstrate that the routing scheme used in DP H plays a
signiﬁcant role in improving its performance over SP SR and
BLSA.
VII. C ONCLUSION
In this paper we study the Routing and Spectrum Allocation
problem (RSA) in OFDM-based optical networks. We prove

[1] W. Shieh, “Ofdm for ﬂexible high-speed optical networks,” Journal of
Lightwave Technology, vol. 29, no. 10, pp. 1560 –1577, 2011.
[2] M. Jinno, H. Takara, B. Kozicki, Y. Tsukishima, Y. Sone, and S. Matsuoka, “Spectrum-efﬁcient and scalable elastic optical path network: architecture, beneﬁts, and enabling technologies,” IEEE Communications
Magazine, vol. 47, no. 11, pp. 66 –73, 2009.
[3] Y. Wang, X. Cao, and Y. Pan, “A study of the routing and spectrum allocation in spectrum-sliced elastic optical path networks,” in INFOCOM,
2011, pp. 1503–1511.
[4] A. N. Patel, P. N. Ji, J. P. Jue, and T. Wang, “Routing, wavelength
assignment, and spectrum allocation in transparent ﬂexible optical wdm
(fwdm) networks,” in Photonics in Switching, 2010.
[5] M. Jinno, B. Kozicki, H. Takara, A. Watanabe, Y. Sone, T. Tanaka, and
A. Hirano, “Distance-adaptive spectrum resource allocation in spectrumsliced elastic optical path network [topics in optical communications],”
IEEE Communications Magazine, vol. 48, no. 8, pp. 138 –145, 2010.
[6] K. Christodoulopoulos, I. Tomkos, and E. A. Varvarigos, “Routing
and spectrum allocation in ofdm-based optical networks with elastic
bandwidth allocation,” in GLOBECOM, 2010, pp. 1–6.
[7] M. Klinkowski and K. Walkowiak, “Routing and spectrum assignment
inspectrum sliced elastic optical path network,” IEEE Communications
Letters, vol. 15, no. 8, pp. 884–886, 2011.
[8] H. A. Kierstead, “A polynomial time approximation algorithm for
dynamic storage allocation,” Discrete Mathematics, vol. 87, no. 2-3,
pp. 231–237, 1991.
[9] S. Shirazipourazad, C. Zhou, Z. Derakhshandeh, and A. Sen. On
routing and spectrum allocation in spectrum-sliced optical networks.
[Online]. Available: http://www.public.asu.edu/ sshiraz1/RSA.pdf
[10] A. L. Buchsbaum, H. Karloff, C. Kenyon, N. Reingold, and M. Thorup,
“Opt versus load in dynamic storage allocation,” SIAM Journal on
Computing, vol. 33, no. 3, pp. 632–646, 2004.
[11] M. C. Golumbic, M. Lipshteyn, and M. Stern, “Representing edge
intersection graphs of paths on degree 4 trees,” Discrete Mathematics,
vol. 308, no. 8, pp. 1381 – 1387, 2008.
[12] S. V. Pemmaraju, S. Penumatcha, and R. Raman, “Approximating
interval coloring and max-coloring in chordal graphs,” Journal of
Experimental Algorithmics, vol. 10, 2005.
[13] T. Erlebach and K. Jansen, “The complexity of path coloring and call
scheduling,” Theoretical Computer Science, vol. 255, no. 1-2, pp. 33–50,
2001.
[14] Level 3 Communications, Network Map. [Online]. Available:
http://nsssc.superb.net/img/l3-usmap.gif

389

Fault-Tolerance in Sensor Networks: A New
Evaluation Metric
Arunabha Sen, Bao Hong Shen, Ling Zhou and Bin Hao
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287–5406
Email: {asen, bao, ling.zhou, binhao}@asu.edu

Abstract— A new metric for measuring the fault-tolerance
capability of a multihop wireless sensor network is introduced
in this paper. Most of the studies on fault-tolerance in sensor
networks use connectivity as the metric of fault-tolerance. If the
underlying network is k-connected, it can tolerate up to k − 1
failures. In measuring fault tolerance in terms of connectivity,
no assumption regarding the locations of the failed sensor nodes
is made - they may be very close to each other or very far
from each other. In other words, the connectivity metric fails
to capture any notion of locality of faults. However, in sensor
networks, it is highly likely that the faults will be localized. This is
particularly true in military applications, where an enemy bomb
may inflict massive but localized damage to the sensor network. To
capture the notion of locality in fault-tolerance, we introduce the
notion of region-based connectivity. The region-based connectivity
of a network may be informally defined to be the minimum
number of nodes within a region whose failure will disconnect
the network. Obviously, the notion of region-based connectivity
is tied to the notion of a region. A region may be defined in
several different ways and they are discussed in detail in the
paper. The attractive feature of the region-based connectivity as
the fault-tolerance metric is that it can achieve the same level
of fault-tolerance as the traditional metric - connectivity, but
requires much lower transmission power for the sensor nodes.
We provide both analytical as well as extensive simulation results
to support our claim.

I. I NTRODUCTION
A wireless sensor network (WSN) is composed of a large
number of sensor nodes distributed in a geographic region.
Each sensor node comprises two components: the sensing
component and the communication component. The sensing
component gathers information from the surrounding area and
the communication component, which consists of a radio transceiver, is responsible for transmitting the gathered information
to the appropriate location for processing.
Sensor networks are being extensively studied by the researchers for the last few years due to its wide application
potential - military, environmental, just to name a few [1].
Among various factors, such as scalability, fault-tolerance, and
production cost, which impact the design of sensor networks,
fault-tolerance plays a very important role - particularly in military applications. The sensor network should always remain
connected so that the nodes can communicate for data fusion
and send gathered data to the sink node. In graph theoretical
terms, the (node/link) connectivity of a graph is the minimum

number of nodes (links) that has to be removed before the
graph is disconnected. If the sensor network is k-connected or
k-link-connected, it will remain connected even after failure
of any k − 1 nodes or links. Such a network is said to be able
to tolerate up to k − 1 failures.
A sensor network with connectivity 1 may not be acceptable
for most of the applications, as it may not be able to tolerate
a single failure. In order to keep the topology connected after
the failure of a single node (link), the sensor network should at
least be 2-connected. Studies show that the additional cost of
having a 2-connected network is amply justified by the benefits
derived. By designing a k-connected network topology, failure
up to k−1 nodes/links can be tolerated. Although such designs
ensure that the network remains connected after the failure
of some nodes and links, they fail to take into account two
significant characteristics of faults in sensor networks. Faults
in sensor networks are most likely massive failures of nodes
and links in a limited area. This is specifically true in military
applications, where an enemy bomb may destroy a large
number of sensors confined in a particular area. This situation
is shown in figure I, where the shaded region shows the fault
region. The significant body of literature available for the faulttolerant design of sensor networks [15] [19], fails to take into
account these two characteristics, and as such fails to design
networks, which meant to operate under the condition of
massive but localized failure. To capture the notion of locality
in measuring the survivability of the network, we introduce
a new notion of connectivity called region-based connectivity.
The region-based connectivity of a network may be informally
defined to be the minimum number of nodes (links) that has to
fail within any region of the network before it is disconnected.
Obviously, the notion of region-based connectivity is tied to
the notion of region. A region may be defined in several
different ways and they are discussed in section III of the
paper. The attractive feature of the region-based connectivity
as the fault-tolerance metric is that it can achieve the same
level of robustness as the traditional metric - connectivity, but
requires much lower transmission power for the sensor nodes.
We provide both analytical and extensive simulation results to
support our claim.
The contributions of the paper are as follows: (i) introduction of the notion of region-based connectivity, (ii) devel-

1-4244-0222-0/06/$20.00 (c)2006 IEEE
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

Fault Region

(a) Sensor Nodes in the Plane
Fig. 1.

(b) Sensor Network

(c) Sensor Network with Fault Region

(a): sensors in a geographic area; (b): communication network formed by the sensors; (c): Fault region and the affected sensors

opment of two polynomial time algorithms to compute the
region-based connectivity of a sensor network, (iii) development of a polynomial time algorithm to minimize the transmission power of the sensor nodes, which ensures the resulting sensor network region-based connectivity K with region
diameter d, (iv) demonstration through analytical techniques
and simulation that the new metric is much more cost effective
(in terms of power consumption) than the conventional metric
in achieving the same level of fault-tolerance in the sensor
network.
II. R ELATED W ORK
Fault-tolerance and connectivity of communication networks formed by the sensor nodes or multihop packet radio
network have been studied by many researchers [4], [5], [9],
[15], [18], [19], [22], [28], [30], [31], [34], [35], [39]. The
authors in [9] are among the early researchers who studied
the relationship between the density of the radio transceivers
in a geographic region, their radius of transmission and the
connectivity of the network formed with these two parameters.
Research along this line was further refined in [30], [31],
where the authors established the conditions for the radio
network being connected (with high probability) for a given
transmission range and distribution density of the transceivers.
The work in [23] incorporated routing protocol into the notion
of connectivity and analyzed the node density requirement
to satisfy the routing dependent connectivity. In [32], the
authors studied the minimum transmission power required to
maintain connectivity of the network when each node can
independently choose a transmission power level. The authors
in [13] considered the effect of link dynamics on topology
control and connectivity in sensor networks. Utilizing some
fundamental results presented by Penrose in [28], Bettstetter
[4] recently developed an analytical expression for computing
the range of the transceivers (with the assumption that they are
uniformly distributed in a plane) so that the resulting network
is k-connected with high probability. The authors in [6] studied
how to deploy and repair a sensor network to guarantee kconnectivity. In [5] the authors refine the radio transmission
model in [4] by considering shadow fading environment.
Wang et al considered both coverage and connectivity of
a sensor network in [39] and established the relationship
between the sensing range and transmission range so that
the sensor nodes cover the entire region and at the same

time the resulting network is connected. The authors in [36]
extended the work of [39] by considering the relationship
between sensing range and communication range to maintain
both coverage and connectivity in the case when there are
initial sensing holes due to random deployment. In [22], the
authors considered the problem of optimal node placement
for connected coverage of the sensor networks. If the sensor
network is k-connected, multiple paths (potentially k disjoint
paths) can be utilized for data transmission. Minimum energy
disjoint path routing in wireless ad-hoc network was studied in
[34]. The authors in [15] studied the redundancy in deployment
of sensor networks and made some interesting observations
regarding the minimum and maximum number of neighbors
needed by each sensor node to provide complete redundancy.
The author in [24] studied the time at which the failure will
lead to breakdown in connectivity among the surviving nodes
of the network. Effect of connectivity on routing algorithm
and tradeoff among power consumption, data rate and various
sleeping strategies were studied in [27], [40], [25].
In addition to the efforts from the networking research
community, the issues related to the connectivity of graphs
have been extensively studied by mathematicians with an
interest in graph theory. Many different variations of connectivity have been studied, e.g. distance connectivity [2], average
connectivity [3], line connectivity [7], l-connectivity [12],
supper connectivity [14], path connectivity [20], algebraic
connectivity [21], cyclic connectivity [29] and conditional
connectivity [38]. The concept of local connectivity studied in
[37], at a first glance may appear to be similar to the notion of
region-based connectivity introduced in this paper. However,
on closer examination one can find out that the concepts are
totally different. To the best of our knowledge, neither the
networking research community nor the graph theory research
community has developed any notion comparable to the notion
of region-based connectivity proposed in this paper.
III. R EGION - BASED C ONNECTIVITY
In the previous section we indicated that connectivity as
a metric of fault-tolerance in sensor networks is somewhat
limited, as it fails to capture the notion of locality of faults.
To address this limitation, we introduce the notion of regionbased connectivity.
Consider a set of sensor nodes distributed over a geographic
area. The transceivers of the sensor nodes form a network

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

graph, which indicates the radio link connection between
sensor nodes. In addition to the network graph, we also have
the layout of the sensor nodes. We refer to the layout of
the sensor nodes as the network geometry. A region may be
defined either with reference to the network graph or with
reference to the network geometry. There may be two different
ways to define a region with respect to a network graph: (i)
A region in a network graph G(V, E) may be defined as a
subgraph of G with diameter d (the diameter of a graph is
the maximum of the shortest path distance between a pair
of nodes, taken over all source-destination node pairs); (ii)
A region in a network graph G(V, E) may be defined as a
subgraph of G, centered at some node v ∈ V and radius
r. With reference to the network geometry, a region may be
defined as a collection of nodes and links covered by a circular
area in the network layout. It may be noted that although
the number of such circular areas may be infinite, since the
number of nodes and links are finite, the number of regions
will also be finite. In this paper, we denote V (R) as the set
of nodes covered by the region R.
The region-based connectivity of the layout of a graph
G, with a circular region of diameter d may be defined
in terms of either nodes or links. Formally, region-based
connectivity (node) κd (G) may be defined as follows: suppose
that {R1 , . . . , Rk } is the set of all possible regions of the graph
G. Let min cut(Ri ) indicate the number of nodes in region
Ri whose failure will disconnect the graph G. If the graph G
remains connected even after the failure of all nodes of the
region Ri , min cut(Ri ) is set equal to ∞. Then region-based
connectivity of the layout of a graph G with a circular region
of diameter d is
κd (G) = min min cut(Ri )
1≤j≤k

The region-based connectivity with respect to the network
graph can be defined in a similar way.
The connectivity of a graph G and its region-based connectivity may be widely different. Consider the graph shown in
figure 2. Clearly, the (node) connectivity of the graph is 2, as
failures of nodes u and v will disconnect the graph. However,
if the region is defined to be a subgraph of diameter 2, nodes
u and v cannot fail simultaneously, as their distance is greater
than the specified diameter of the region. In this situation, the
graph will be disconnected if the node set {u, b1 , . . . , bm } or
{u, y1 , . . . , ym } or {v, a1 , . . . , am } or {v, x1 , . . . , xm } fails.
Please note that the distance between any two nodes in any
above fault set does not exceed the fault diameter 2. Therefore,
for this example the minimum number of nodes (within
distance of 2) whose failure will disconnect the graph is m+1.
Accordingly, the region-based connectivity of this graph (with
region being a subgraph of diameter 2) is m + 1. Since m can
be arbitrarily large, the difference between the region-based
connectivity and connectivity can also be arbitrarily large.
A sensor network G is said to be fault-tolerant for some
specified d, if its region-based connectivity κd (G) = ∞. A
sensor network G is said to be fault-tolerant of degree p for

u

a1

a2

am

x1

x2

xm
Km,m

Km,m
b1

b2

y1

bm

y2

ym

v
Fig. 2. A network with connectivity 2 and region-based connectivity m + 1

some specified d, if its region-based connectivity κd (G) =
p. In the following text, we only consider the region-based
connectivity with respect to the network geometry.
A. Power Saving Using Region-based Connectivity
In this paper we argue that if some prior knowledge is
available on the maximum size of the region in which faults
will be restricted, instead of ignoring that piece of knowledge,
it should be utilized for efficient design of sensor networks. In
a battlefield scenario, one can have a reasonably good estimate
of the size of region that will be affected by an enemy bomb. It
is true that enemies can drop multiple bombs affecting multiple
areas. In order to deal with that situation, we can extend the
notion of region-based connectivity proposed in this paper to
k-region-based connectivity, where k is the maximum number
of regions that can be affected by faults. However, due to
page limitation, in this paper we present results related only
to region-based connectivity, which is a special case of kregion-based connectivity with k = 1.
Suppose that the locations of the sensor nodes are already
determined, as shown in figure 3 (the distances between sensor
nodes are also shown). The design requirements specify that
the network should be able to tolerate failure of up to 2
sensor nodes. Moreover, it is also given that the faults will
be confined to any circular region of diameter 5 meters. Since
the design requirements specify that the network should be
able to tolerate up to 2 faults, one can design a network
with connectivity 3. However, in this case, the information
regarding the locality of faults (that the faults will be confined
to any circular region of diameter 5 meters) has not been
used yet. Suppose the network is designed such that the
transmission range of each node is 10 meters, it will result
in the topology shown in figure 3. It may be noted that
the connectivity of this network is 2, as failure of nodes
u and v will disconnect the network. However, since the
distance between nodes u and v is 10 meters, they cannot fail
simultaneously. Accordingly, this network with each sensor
node having transmission range of 10 meters is sufficient to
meet the design goal of tolerating up to 2 faults, under the
condition that the fault will be confined to a circular region
of diameter 5 meters.
If the information about the locality of faults is ignored,

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

u
o
60 o 60

1 0m

10
m

e

m

b

h

f

10
10 m

m

10
o
60 o 60

m

3m

3m

3m

3m

c

3m

3m

d

10

3m

a
3m

1 0m

g
10 m

v
Fig. 3.

An example: cost effective design using region-based connectivity

one must design a network whose connectivity is 3 to meet
the design requirements. In this example, it implies that the
sensor nodes should have sufficient transmission range so that
node b can communicate with node h. √
This means that the
transmission range should be at least 10 3 meters instead of
10 meters. This is more than 71% increase in transmission
range. Since the power consumption is proportional to the
square of the transmission range, there has to be 250% increase
in power requirements. It may be noted that this increase in
transmission power does not provide any additional benefit,
since the robustness of the network against failure would have
been the
√ same if the transmission range was 10 meters instead
of 10 3 meters. One can conclude from this example that
the goal in this case should be to design a network whose
region-based connectivity is 3 instead of connectivity being 3.
Making the connectivity equal to 3 results in over design of
the network with higher cost but no additional benefit.
IV. C OMPUTATION OF R EGION - BASED C ONNECTIVITY IN
S ENSOR N ETWORKS
In this section, we provide two different algorithms to
compute the region-based connectivity of a graph generated by
a set of sensor nodes distributed in a plane. Both algorithms
compute the region-based connectivity in polynomial time.
The computational complexity of the second algorithm is
superior to the first one. However, it is also more complex.
To ease the understanding, we present both algorithms in this
paper.
The notion of Point graphs was introduced by Sen and
Huson in connection with packet radio networks in [33].
Given a set of points P = {p1 , . . . , pn } in an m-dimensional
space, each specified by a vector (x1 , . . . , xm ), and a range ri
associated with each pi , the point graph constructed from this
set of points is a graph G(V, E), where each node vi represents
a point pi and there is a directed edge from node vi to node
vj if d(pi , pj ) ≤ ri , where d(pi , pj ) is the Euclidean distance
between points pi and pj . Planar point graphs are point graphs
constructed from a set of points on a two dimensional plane
(m = 2). Planar point graphs, where the range associated with
each point is same, are known as the unit disk graphs [11].

The Location-Range representation of an n-point graph is a
set of n tuples (li , ri ), where li represents the location of the
point pi and ri represents the range associated with that point.
If pi is a point on a two dimensional plane, its location li is
specified by a vector (xi , yi ).
In our model, we view each sensor node as a point pi on
a two dimensional plane. As indicated earlier, there may be
many definitions of a region. In this paper, by region, we imply
a circular area of a specified diameter d. A region covering
four sensor nodes is shown in figure 4.
We will refer to the distribution of sensor nodes on a
two dimensional plane as sensor layout (figure 1(a)) and
the network generated from these sensors as sensor networks
(figure 1(b)).
Theorem 1: Given the location-range representation of a sensor
network G(V, E)(|V | = n, |E| = m), and the region diameter
d, it is possible to compute the region-based connectivity of
the graph G, κd (G), in polynomial time.
Proof: The proof is by construction. We provide an algorithm,
prove its correctness and analyze its time complexity to show
that the algorithm computes the region-based connectivity of
the graph G in polynomial time.
A. Region-Based Connectivity Algorithm 1
Before we present the algorithm, we make a few observations on which the algorithm is based.
Observation 1: There exists only a finite number of distinct
regions.
Proof: If the region is defined to be a circular area of diameter
d, potentially, there may be an infinite number of regions that
can cover part of the sensor layout. This is true because if
one slides the circular region slightly on the plane, it may be
considered as a different region. This is shown in figure 5,
where two different regions are shown. However, we do not
need to distinguish between these two regions because they
cover the same set of sensor nodes. Since there exists a finite
number of sensor nodes in the plane and each region will

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

Sensor node
Center of region
E-point
I-point

Principal region

(a)

(b)
Fig. 4.

(c)

A Region, intersection of Regions, Principal Region

include a distinct subset of the sensor nodes, we only need to
consider a finite number of regions.
Sensor node
Center of region

centered at p0 with diameter d includes p1 , . . . , pn , circles
centered at pi , 1 ≤ i ≤ n with diameter d must include p0 .
Therefore the intersection area of the regions centered all the
sensor nodes in R includes the point p0 , and it must be nonempty.
Observation 3: If a region R is not centered at one of the
I-points, there must be at least one region centered at one of
the I-points that includes all the sensor nodes included in R.

c1
region 1

Fig. 5.

c2
region 2

Redundant regions covering the same set of sensor nodes

Definition: I-Point: In figure 4-a a region and four sensor
nodes included in that region are shown. Figure 4-b shows
this region and four other regions centered at each of the
sensor nodes. The intersection point of the boundaries of two
different regions centered at different sensor nodes is referred
to as an I-point. The number of I-points in figure 4-b is 12.
If a region centered at sensor node (xi , yi ) does not intersect
with any other region, the location (xi , yi ) is considered to be
an I-point.
Definition: E-Point: The figure 4-b shows a non-empty area
that lies at the intersection of all the regions centered at
different sensor nodes. The extreme points of the area are
referred to as E-points. The number of E-points in figure 4-b
is 4. It may be noted that an E-point is also an I-point.
Definition: Principal Region: Any region centered at an I-point
is referred to as a Principal Region.
Observation 2: Given a region R, the intersection area of the
regions centered at the sensor nodes in R is non-empty.
Proof: Suppose that a region R includes sensor nodes
p1 , . . . , pn . Recall that a region is a circle of diameter d.
Suppose that the center of region R is p0 . Since a circle

Proof: Suppose that the region R includes a set of sensor nodes
and R is not centered at any of the I-points. By observation
2, we know that the intersection area of the regions centered
at the sensor nodes in R will be non-empty. This implies that
there will be a positive number of E-points. Any region R
centered at any of these E-points will include all the sensor
nodes included in the region R. In figure 4-c, the Principal
Region that includes all the sensor nodes of the original region
is shown.
Observation 4: For the purpose of computing the region-based
connectivity of the sensor network G(V, E), only a limited
number of distinct regions, i.e., the Principal Regions need to
be examined.
Proof: Observation 3 states that if a region R includes a set
of sensor nodes, those nodes are also included in another
Principal Region R . Since R includes all the sensor nodes
of R, considering R should be sufficient for the purpose of
computing the region-based connectivity of the graph G.
Observation 5: The maximum number of Principal Regions is
O(n2 ).
Proof: A Principal Region is a region centered at an I-point.
Therefore the number of Principal Regions is same as the
number of I-points. Every pair of sensor nodes will give at
most 2 I-points. Since there are only n(n − 1)/2 pairs of
sensor nodes, there can be at most n(n−1) I-points and hence
n(n − 1) Principal Regions.
Definition: min cut(R): The minimum number of sensor nodes

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

within region R, whose removal will disconnect the network
G(V, E).
Definition: V(R): The set of sensor nodes in the sensor network
G(V, E), which is included in the region R.
1) Main Idea of Algorithm 1: Before formally presenting
the algorithm, we briefly describe the main idea of the
algorithm. For each region R, we compute min cut(R).
The min cut(R) is computed by constructing an auxiliary
graph G∗R from G as follows: First replace each connected
components [10] of G \ V (R) by a clique of size | V (R) | +1
; then connect each node in the clique to every neighbor of
the component. The connectivity of the graph G∗R is equal to
min cut(R).
The pseudo-code of the algorithm is listed below. The
algorithm takes as input, the location-range representation of
the sensor network G(V, E) and the region diameter d, and
computes the region-based connectivity κd (G).
Connected component 1

V(R)

Connected component 2

Algorithm 1:
Step 0. Identify all the Principal Regions R1 , R2 , . . . , Rk
from the location-range representation of the sensor network
G(V, E) and the region diameter d.
Step 1. For each Ri ,i ∈ {1, . . . , k} in G and do the following
steps:
Step 1.1. Construct a graph GRi as follows. Let C1 , . . . , Ch
be the connected components of G \ V (Ri ). Shrink each
component Cj to a single node cj , while keeping all the links
between Cj and V (Ri ). Refer to figure 6.
Step 1.2. Let M =| V (Ri ) |, the number of nodes included in
the region Ri . Construct a graph G∗Ri from GRi by replacing
each cj as a clique Kj of size M + 1, and connecting each
node in the clique to all the neighbors of cj . Refer to figure
7.
Step 1.3. Compute the connectivity κ(G∗Ri ) of G∗Ri . If
κ(G∗Ri ) ≤ M , set min cut(Ri ) = κ(G∗Ri ); otherwise, set
min cut(Ri ) = ∞.
Step 2. Set the region-based connectivity of G, κd (G) =
min {min cut(Ri )}.

1≤i≤k

Connected component 3

Fig. 6.

Constructed Graph GR

V(R)

Fig. 7.

Constructed Graph G∗R

2) Correctness of Algorithm 1: Based on the definition of
region-based connectivity in Section III, to prove the correct-

ness of Algorithm 1, it suffices to show that the connectivity
κ(G∗Ri ) of G∗Ri computed in Step 1.3 is exactly the same as
min cut(Ri ), the minimum number of sensor nodes within
region Ri , whose removal will disconnect G(V, E).
We start with the easier case when κ(G∗Ri ) > M . Since
M is the total number of sensor nodes in region Ri , this
case means that even after the failure of all the sensor nodes
in region Ri , the network G will still remain connected.
Therefore we set min cut(Ri ) = ∞ according to Section
III.
For the case when κ(G∗Ri ) ≤ M , we prove by contradiction
that the set S of sensor nodes with size κ(G∗Ri ), whose
removal will disconnect G(V, E), must all fall into the region
Ri . Without the loss of generality, let S  = S ∩ Kj and
S  = ∅. Since the size of any clique is M + 1, Kj \ S  = ∅.
Since by the construction in Step 1.2, we connect each node
in the clique Kj to all the neighbors of the corresponding
connected component, removing only part of nodes in the
clique won’t have any effect on the connectedness of the graph
G∗Ri (also G(V, E)). Therefore if removing all the nodes in
S will disconnect G∗Ri , removing only nodes in S \ S  will
also disconnect G∗Ri , which is a contradiction to the minimum
property of connectivity. Therefore all the nodes in set S
must fall into the region Ri . And thus κ(G∗Ri ) is equal to
min cut(Ri ), the minimum number of sensor nodes within
region Ri , whose removal will disconnect G(V, E). Therefore,
Algorithm 1 computes the region-based connectivity of the
sensor network G(V, E) correctly.
3) Time Complexity of Algorithm 1: It is not difficult to
see that Algorithm 1 is a polynomial-time algorithm. Now we
analyze its time complexity in detail.

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

Assume that the number of sensor nodes in the network is n.
Step 1 is repeated for every Principal Region. All the Principal
Regions are computed in Step 0 and as noted in observation 5,
the maximum number of Principal Regions is O(n2 ). Hence,
Step 1 will repeat O(n2 ) times. Inside each iteration of Step
1, Step 1.1 takes O(n2 ) of time; Step 1.2 takes O(n2 ) of
time; In Step1.3, if we use the algorithm presented in [26]
to compute the connectivity of the graph G∗Ri , it will take
O(| G∗Ri |5 ) = O(n10 ) of time. Thus, Step 1 takes O(n12 ) of
time. In Step 2, κd (G) is computed by taking the minimum of
min cut(Ri ) over all the Principal Regions. Since the number
of Principal Regions is O(n2 ), the computational complexity
of step 2 is O(n2 ).
Therefore, the overall time complexity of Algorithm 1 is
O(n12 ).
The computational complexity of Algorithm 1 is high due
to the high computational complexity of the algorithm to
compute the connectivity of a graph. The complexity of Algorithm 1 will be lower if a more efficient algorithm to compute
the connectivity of a graph, instead of the one presented in
[26], is used.
B. Region-Based Connectivity Algorithm 2
Algorithm 1 has proved Theorem 1, i.e. region-based connectivity of sensor networks with location-range representation and region diameter d can be computed in polynomial
time. However, the computational complexity of Algorithm 1
O(n12 ) is high. In the following, we develop Algorithm 2,
which is more efficient than Algorithm 1 and has computational complexity of O(n6 ).
1) Main Idea of Algorithm 2: If one takes a close look at
the time complexity analysis of Algorithm 1, it may be noticed
that Step 1.3 dominates the running time. And the main reason
that makes the time complexity high is that the cardinality of
G∗Ri is O(n2 ), and calling the connectivity algorithm on G∗Ri
(running Max-Flow algorithm on each s − t pair of nodes)
makes the running time rise up to O(n10 ).
So, Algorithm 2 tries to avoid the construction of graphs
with high cardinality, and reduce the number of s − t pairs,
which need to be examined to compute the minimum number
of nodes in each region, whose removal will disconnect the
graph G.
The notations used in Algorithm 2 and its pseudocode are
listed below.
κd (G): Region-based connectivity of graph G, with its
geometry layout and region diameter d.
min cut(R): The minimum number of nodes in the region
R, whose failure will disconnect graph G.
min cutG
S [s, t]: The minimum number of nodes in the set
S ⊂ V (G), whose failure will separate s and t in graph G.
2) Correctness of Algorithm 2: In order to show the correctness of Algorithm 2, it suffices to show the following two
claims.
Claim 1. The max flow value f ∗ computed in Step 1.2 is
GR
equal to min cutV (Ri i ) [s, t].

Algorithm 2:
Step 0. Identify all the Principal Regions R1 , R2 , . . . , Rk
from the location-range representation of the sensor network
G(V, E) and the region diameter d.
Step 1. For each region Ri , 1 ≤ i ≤ k in the graph G, do the
following steps:
Step 1.1. Construct a graph GRi as follows. Let C1 , . . . , Ch
be the connected components of G \ V (Ri ). Shrink each
component Cj to a single node cj , while keeping all the links
between Cj and V (Ri ).
Step 1.2. In graph GRi , fix a source node s = c1 (actually
s can be any cj ). For each v ∈ GRi − {s}, set the destination node t = v, and do the following steps to compute
GR
min cutV (Ri i ) [s, t]:
Step 1.2.1. If there is an edge between s and t, set
GR
min cutV (Ri i ) [s, t] = ∞.
Step 1.2.2. Else construct a directed weighted graph GRi
from GRi as follows:
For each node v ∈ GRi \ {s, t}, expand v into a pair
of nodes vin and vout , adding one edge between them. If
v ∈ V (Ri ), assign the capacity of (vin , vout ) as 1; otherwise,
assign the capacity of (vin , vout ) as | V (Ri ) |.
For each edge (u, v) ∈ E(GRi ), add two edges (uout , vin )
and (vout , uin ) in E(GRi ), and assign capacity ∞ to them.
For each edge (s, v) ∈ E(GRi ), add two edges (s, vin )
and (vout , s) in E(GRi ), and assign capacity ∞ to them.
For each edge (v, t) ∈ E(GRi ), add two edges (vout , t)
and (t, vin ) in E(GRi ), and assign capacity ∞ to them.
Use Max-Flow algorithm to compute the s − t max
flow in GRi . Suppose the value of max flow is f ∗ , set
GR
min cutV (Ri i ) [s, t] = f ∗ .
Step 1.3. Set
min cut(Ri ) =

min

GR

{min cutV (Ri i ) [s, t]}

t∈GRi −{s}

.
Step 2. Set the region-based connectivity of G, κd (G) =
min {min cut(Ri )}.

1≤i≤k

Proof of Claim 1: If there is an edge connecting s and t,
no matter how many nodes are removed from V (R) \ {t}, s
and t will still be connected. Hence, in this case Step 1.2.1 set
GR
min cutV (Ri i ) [s, t] as ∞. Actually, this case can only happen
when t ∈ V (Ri ), since if t ∈
/ V (Ri ), which means t = ci
for some i, then both s and t correspond to some connected
components in G \ V (Ri ), which implies that there is no edge
between them.
If there is no edge connecting s and t in GRi , by the
construction of graph GRi in Step1.2.2, we know that for any
v ∈ V (Ri ), since it is expanded into a pair of nodes vin and
vout , and the directed edge (vin , vout ) is assigned weight of
1, it is true that in computing the s − t max flow, there is
at most one unit of in-flow going into node vin and at most

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

one unit of out-flow going out of node vout , and the in-flow
value equals the out-flow value. Similarly, for any v ∈
/ V (Ri ),
since the weight of directed edge (vin , vout ) is assigned as
| V (Ri ) |, there can be f units of in-flow going into node vin
and f units of out-flow going out of vout , and f ≤| V (Ri ) |.
The reason we assign different weight to the edge (vin , vout )
based on the different location of node v (inside the region or
outside the region) is that we want to ensure that each node
inside the region is only went through by one single unit of
flow, while the node outside the region is went through by
at most | V (Ri ) | units of flow. Figure 8 and figure 9 show
the construction of GRi from GRi . Then it is true that the
computed s − t max flow value is equivalent to the maximum
number of region-node-disjoint paths from s to t. By regionnode-disjoint path, we mean that for any pair of paths from s
to t, they have no common node inside the region, but they
can have common nodes outside the region. Then it is not hard
to find out that the computed s − t max flow value is equal to
the minimum number of nodes in region Ri , whose removal
GR
will separate s and t, namely f ∗ = min cutV (Ri i ) [s, t]. For
example, in figure 9 the value of the s − t max flow is 2,
which is exactly the same as the minimum number of nodes
in region Ri , whose removal will separate s and t in figure 8.
The Claim 1 is proved.

A

B

Ain 1

Bout

s

t
Cout

Din

C in 1

1

E in

Fig. 9.

4

Dout

E out

Graph GRi by using transformation of step 1.2.2 on Graph GRi

On the other hand, according to the definition of
min cut(Ri ), after appropriately deleting min cut(Ri ) nodes
from region Ri , GRi will be cut into at least two components
G1 and G2 . Since we only delete nodes from V (Ri ), s cannot
be deleted. Without loss of generality, assume s ∈ G1 , then
for any node u ∈ G2 , these min cut(Ri ) nodes separate
GR
s and u. According to the definition of min cutV (Ri i ) [s, u],
which means the minimum number of nodes in set V (Ri ),
whose failure will separate s and u in graph GRi , we have
GR
min cutV (Ri i ) [s, u] ≤ min cut(Ri ).
Therefore, it is true that
min

t∈GRi −{s},s=c1

s

Bin 1

Aout

GR

{min cutV (Ri i ) [s, t]}

GR

≤ min cutV (Ri i ) [s, u]
≤ min cut(Ri )

t
Hence, the following claim

C

min cut(Ri ) =

D

Fig. 8. Graph GRi after shrinking each connected component of G \ V (Ri )

Claim 2.
min

t∈GRi −{s},s=c1

GR

{min cutV (Ri i ) [s, t]}

Proof of Claim 2: It may appear to the reader that
in order to compute min cut(Ri ), we need to compute
GR
min cutV (Ri i ) [s, t] for each pair of s and t, s, t ∈ GRi . Now,
we show that it suffices to consider only the pairs of s and t,
such that s is fixed to some cj , 1 ≤ j ≤ h. Without loss of
generality, we set s = c1 and t = v, v ∈ GRi − {s}.
Clearly, we have
min cut(Ri ) =
≤

GR

min {min cutV (Ri i ) [s, t]}

s,t∈GRi

min

GR

{min cutV (Ri i ) [s, t]}

is proved. This completes the proof of correctness of Algorithm 2.

E

min cut(Ri ) =

min

t∈GRi −{s},s=c1

t∈GRi −{s},s=c1

GR

{min cutV (Ri i ) [s, t]}

3) Time Complexity of Algorithm 2: Now, we analyze the
time complexity of Algorithm 2. Similar to Algorithm 1, Step
0 takes O(n2 ) of time to find all the Principal Regions. Step
1 is repeated O(n2 ) times, since there are O(n2 ) Principal
Regions.
It is not hard to see that inside each iteration of Step 1, Step
1.2 dominates the running time. Hence, it suffices to compute
the running time of Step 1.2. Suppose there are h connected
components in G \ V (Ri ). Then GRi has | V (Ri ) | +h =
O(n) nodes, and m = O((| V (Ri ) | +h)2 ) = O(n2 ) edges.
Time complexity for the s−t max flow algorithm in Step 1.2.2
is O(n3 ). Since there are totally | V (Ri ) | +h−1 = O(n) s−t
pairs considered, Step 1.2 takes O(n4 ) of time, which implies
that the running time of Step 1 is O(n2 × n4 ) = O(n6 ). Step
2 will take time O(n2 ).
Therefore, the time complexity of Algorithm 2 is O(n6 ).

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

V. D ESIGN OF S ENSOR N ETWORKS WITH A S PECIFIED
R EGION -BASED C ONNECTIVITY
In this section we present an algorithm for designing a
sensor network that attains a specified value of region-based
connectivity with minimum power consumption. The algorithm takes as input (i) locations of the sensor nodes in a
two-dimensional plane, (ii) diameter of the (circular) region,
d and (iii) required region-based connectivity value, K. The
output of the algorithm is the minimum transmission range for
each sensor node, which will result in a network with regionbased connectivity K.
This algorithm can use either Algorithm 1 or Algorithm 2
(see section IV) to compute the region-based connectivity of
a sensor network. Since computational complexity of Algorithm 2 is less than that of Algorithm 1, we use Algorithm 2
to solve this problem. The algorithm (Alg. MinTranRange)
calls the procedure Binary Search(L, 1, length(L), 0, K) to
do a binary search on the list L of possible transmission
ranges, until a minimum transmission range which can ensure
region-based connectivity at least K is found, or value 0 is
returned, which means the required region-based connectivity
value is unattainable even by setting the transmission range as
the longest distance of sensor nodes.
Algorithm (Alg. MinTranRange):
Step 0. Compute the distance between any two sensor nodes,
and restore them in a list L in an increasing order.
Step 1. Execute a binary search on L by calling
Binary Search(L, 1, length(L), 0, K).

Binary Search(L, lef t, right, previous, K)
Base Case: If (lef t ≥ right)
Use L[lef t] as transmission range to derive a network
G(V, E) from the sensor layout.
If G(V, E) is connected, use Algorithm 2 to compute the
region-based connectivity Kd of G(V, E), otherwise set Kd
as 0.
If Kd ≥ K, return L[lef t], otherwise if previous is not
0, return L[previous], otherwise return 0.
Recursive Case: If (lef t < right)
Set mid = lef t + (right − lef t)/2.
Use L[mid] as transmission range to derive a network
G(V, E) from the sensor layout.
If G(V, E) is connected, use Algorithm 2 to compute
the region-based connectivity Kd of G(V, E), otherwise set
Kd as 0.
If Kd ≥ K, return Binary Search(L, lef t, mid − 1,
mid, K).
If Kd < K, return Binary Search(L, mid + 1, right,
previous, K).

Since the computational complexity of Algorithm 2 is
O(n6 ), and it will be called at most O(log n) time in the
Binary Search procedure, the computational complexity of the
algorithm Alg. MinTranRange is O(n6 log n).
VI. S IMULATION R ESULTS AND D ISCUSSION
In section III, we have demonstrated with a concrete example that substantial power savings can be achieved by
using the new metric: region-based connectivity, instead of
the conventional metric: connectivity, to set the transmission
range of sensor nodes. By proving the following theorem, we
claim that using region-based connectivity as metric, power
saving can be achieved in general case.
Theorem 2: For a sensor network G(V, E), the minimum
transmission range Rt required to make the connectivity K
is always larger than or equal to the minimum transmission
range Rt required to make the region-based connectivity K.
Proof: The proof is by contradiction. Let’s suppose Rt < Rt .
Transmission range Rt making the connectivity K means after
removing any K−1 sensor nodes from G(V, E), the remaining
sensor nodes will still form a connected graph, which also
means that after removing any K − 1 sensor nodes from any
region, the remaining sensor nodes will still form a connected
graph. Therefore the minimum transmission range Rt to make
the region-based connectivity K cannot exceed Rt , which is
a contradiction with the assumption of Rt < Rt . Therefore
Theorem 2 is proved, and power saving can be achieved in
general case with region-based connectivity as design metric.
In the following content, we will validate Theorem 2 with
results of extensive simulations.
In our simulation setting, the x and y coordinates of n sensor
nodes are uniformly generated in a 500×500m2 field. We will
compare the transmission range required to ensure the desired
connectivity with the transmission range required to ensure
the desired region-based connectivity in different experimental
settings. There are three parameters, which can influence the
comparison results: (i) the number of nodes n, (ii) the region
diameter d, and (iii) the desired connectivity or region-based
connectivity K. For each of these parameters, we do a set of
experiments to find its effect on the comparison results.
In the first set of experiments, we note the effect of changing
connectivity requirement K on the transmission range, while
the number of nodes n and the region diameter d are kept
unchanged. This set of experiments were conducted for two
sets of values of n and d. For the first set n = 100 and
d = 100, and for the second set n = 100 and d = 150.
In the both sets of experiments, the connectivity parameter
K is changed from 2 to 25. The results are presented in
figures 10 and 11. Each experiment is repeated 10 times,
and the results are averaged over 10 randomly generated
topologies. It may be observed in both figures 10 and 11 that
the transmission range for both connectivity and region-based
connectivity increases with the increasing value of parameter
K. However, the rate of increase in transmission range for

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

It may be observed in figures 14 and 15 that both the transmission range to maintain connectivity K and the transmission
range to maintain region-based connectivity K decreases with
the increase in the number of nodes in the sensing field. This is
quite expected, as higher node density implies closer proximity
of nodes, and closer proximity allows the sensor network to
remain connected even with shorter transmission range. The
difference between transmission range for connectivity and
region-based connectivity also decreases with the increase in
the number of nodes. This is expected because with higher
node density, each node v will have a larger number of neighbors. These nodes have to fail before v gets disconnected from
the network. In a sense the failures have to be localized. This
is exactly the same situation that the region-based connectivity
tries to capture. For this reason, with a large number of nodes
in the sensing field, the transmission power requirement for
both connectivity and region-based connectivity approaches
the same value.
It may be noted that in all of our experiments, the regionbased connectivity needs smaller transmission range (and
hence lower power requirement) than the connectivity metric.
In some situations, the requirements for both the metrics
approach the same value, but in no case did the connectivity
metric require a smaller transmission range than required by
the metric region-based connectivity. From these sets of experiments, one can conclude that in comparison with connectivity
as the metric for fault-tolerance, region-based connectivity will
result in substantial power savings for most of the situations.
300

250

200

Transmission Range R

connectivity is much higher than the corresponding increase
for region-based connectivity. The difference in transmission
range (and hence power requirement) for connectivity and
region-based connectivity grows with the increasing value of
K, implying that there exists great potential for power saving
for larger values of K.
Both figures 10 and 11 show that the transmission range
requirement saturates for region-based connectivity, once the
value of K reaches a certain threshold value (12 in figure
10 and 7 in figure 11). This saturation effect is observed
because the region diameters d are set equal to 100 and 150
in these two sets of experiments. Once the transmission range
has exceeded the value of d, even failure of all the nodes
in a region may not disconnect the graph. As a result, no
further increase in transmission range is needed for regionbased connectivity, since the region-based connectivity of the
graph becomes ∞ at this time.
In the second set of experiments, we note the effect of
changing the region diameter d on the transmission range,
while the number of nodes n and the desired connectivity or
region-based connectivity K are kept unchanged. This set of
experiments were conducted for two sets of values of n and
K. For the first set n = 100 and K = 5, and for the second
set n = 100 and K = 10. In both sets of experiments, the
region diameter d is changed from 30 to 150. The results are
presented in figures 12 and 13. As in the previous experiment
sets, each experiment is repeated 10 times, and the results are
averaged over 10 randomly generated topologies.
It may be observed that the value of the parameter d has
practically no effect on the transmission range to maintain
the specified connectivity (5 and 10 in our experiments),
while it has considerable impact on the transmission range
to maintain the specified region-based connectivity 5 and
10. It can be seen from figures 12 and 13 that in order to
maintain the specified region-based connectivity, the transmission range has to increase with the region diameter d. Both
the observations regarding the behaviors of connectivity and
region-based connectivity are quite expected. As the region
diameter d increases, the restriction on faults being localized
is also relaxed, and as such transmission range requirement for
region-based connectivity tends to approach the transmission
range requirement for connectivity. It may be noted that
when the region diameter d is sufficiently large, there will
be no difference between the region-based connectivity and
connectivity.
In the third set of experiments, we note the effect of
changing the number of nodes n on the transmission range,
while the region diameter d and the desired connectivity or
region-based connectivity K are kept unchanged. This set
experiments was conducted for two sets of values of d and
K. For the first set d = 100 and K = 5, and for the second
set d = 100 and K = 10. In both these sets of experiments, the
number of nodes n is changed from 100 to 200. The results are
presented in figures 14 and 15. As in the previous cases, each
experiment is repeated 10 times, and the results are averaged
over 10 randomly generated topologies.

150

100

50
Trans Range requi red to ensu re Graph Conne ctivity K
Trans Range requi red to ensu re Region Conne ctivity K
0
2

3

4

5

6

7

8

9

10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

Desired Graph/ Region Conne ctivity K

Fig. 10. Transmission Range vs. Connectivity K, with n = 100 and d = 100

VII. C ONCLUSION
In this paper we have introduced a new metric for evaluation
of fault tolerance capability of wireless sensor networks. The
conventional metric - connectivity cannot capture the notion of
locality of faults. The new metric - region-based connectivity

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

200

300
Trans Range requi red to ensu re conne ctivity K

180

Trans Range requi red to ensu re region-based
conne ctivity K
250

160

140

Transmission Range R

Transmission Range R

200

150

120

100

80

60

100

Trans Range requi red to ensu re Graph
Conne ctivity K

40

Trans Range requi red to ensu re Region
Conne ctivity

50

20

0
30

0
2

3

4

5

6

7

8

40

50

60

70

Desired Graph/ Region Conne ctivity K

Fig. 11. Transmission Range vs. Connectivity K, with n = 100 and d = 150

80

90

100

110

120

130

140

150

Region Diameter d

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

Fig. 13.
K = 10

Transmission Range vs. Region Diameter d, with n = 100 and

160

160

140
140

120
120

Transmission Range R

100

Transmission Range R

100

80

80

60

60

40
Trans Range required to ensure connectivity K

40
Trans Range required to ensure region-based
connectivity K

Trans Range to ensu re graph conne ctivity K

20

Trans Range to ensu re region-based conne ctivity
20

0
100

0
30

40

50

60

70

80

90

100

110

120

130

140

110

120

130

140

150

160

170

180

190

200

Number of Node N

150

Region Diameter d

Fig. 12.
K=5

Transmission Range vs. Region Diameter d, with n = 100 and

alleviates this problem by integrating the notion of region
where faults are confined. We have shown that if there is
prior knowledge (or estimate) of the largest area that might
be affected by faults, that information can be very effectively
utilized to design a lower cost network, which can provide
the same level of robustness as conventional design. We have
provided polynomial time algorithms for (i) computing regionbased connectivity of sensor networks and (ii) design of sensor
networks with a specified region-based connectivity. Through
both theoretical analysis and extensive simulations, we have
shown that using the new metric will result in power saving
over the traditional metric in most cases.
In order to deal with faults in multiple regions, we have
already extended the notion of region-based connectivity to

Fig. 14.
K=5

Transmission Range vs. Number of Nodes n, with d = 100 and

k-region-based connectivity, where k is the maximum number
of regions that can be affected by faults. However, due to page
limitations, we are unable to present those results in this paper,
which will be presented in a future paper.
R EFERENCES
[1] I.F. Akyildiz, W. Su, Y. Sankarasubramaniam and E. Cayirci, Wireless
Sensor Networks: a Survey, Computer Networks, vol. 38, pp. 393-422,
2002.
[2] M. C. Balbuena, A. Carmona, M. A. Fiol, Distance connectivity in graphs
and digraphs, Journal of Graph Theory, vol. 22, issue 4, pp. 281-292,
1998.
[3] L. W. Beineke, O. R. Oellermann, and R. E. Pippert, The average
connectivity of a graph, Discrete Math., vol. 252, issues 1-3, pp. 31-45,
May, 2002.
[4] C. Bettstetter, On the minimum node degree and connectivity of a wireless
multihop network, MOBIHOC, 2002.

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

200

180

160

Transmission Range R

140

120

100

80

60
Trans Range requi red to ensu re Graph Conne ctivity K
40
Trans Range requi red to ensu re Region-based Conne ctivity K
20

0
100

110

120

130

140

150

160

170

180

190

200

Number of Nodes N

Fig. 15. Transmission Range vs. Number of Nodes n, with d = 100 and
K = 10
[5] C. Bettstetter and C. Hartmann, Connectivity of wireless multihop networks in a shadow fading environment, MSWiM’03, pp. 28-32, Sep. 2003.
[6] Jonathan L. Bredin, Erik D. Demaine, MohammadTaghi Hajiaghayi,
Daniela Rus, Deploying sensor networks with guaranteed capacity and
fault tolerance, Proceedings of the 6th ACM international symposium on
Mobile ad hoc networking and computing’05.
[7] F. Cao, D.Z. Du, D. F. Hsu, L. Hwang, W. Wu, Super line-connectivity of
consecutive-d digraphs, Discrete Mathematics, vol 183, pp. 27-38, 1998.
[8] X. Cheng, D.Z. Du, L. Wang, B. Xu, Relay sensor placement in
wireless sensor networks, submitted for publication. Also available at
http://citeseer.nj.nec.com/cheng01relay.html.
[9] Y.-C. Cheng and T.G. Robertazzi, Critical connectivity phenomena in
multihop radio models, IEEE Trans. on Communications, vol. 37, no. 7,
pp. 770-777, July 1989.
[10] T. H. Cormen, C.E. Leiserson, R. Rivest and C. Stein, Introduction to
Algorithms, Second Edition, McGraw Hill, 2001.
[11] Brent N. Clark, Charles J. Colbourn, and David S. Johnson, Unit Disk
Graphs, Discrete Mathematics, vol 86, pp. 165-177, 1990.
[12] D. P. Day, O. R. Oellermann and H. C. Swart, Bounds on the size of
graphs of given order and l-connectivity, Discrete Math., vol 197/198,
pp. 217-223, 1999.
[13] Andrs Farag, On the fundamental limits of topology control,Proceedings
of the 2004 joint workshop on Foundations of mobile computing .
[14] M. A. Fiol, The superconnectivity of large digraphs and graphs, Discrete
Math., vol. 124, issue 1-3, pp. 67-78, January 1994.
[15] Y. Gao, K. Wu and F. Li, Analysis on the Redundancy of Wireless
Sensor Networks, WSNA’03, pp. 108-114, Sep. 2003.
[16] M. Garey and D. Johnson, Computers an Intractability: A Guide to the
Theory of NP-Completeness, W. H. Freeman, 1979.
[17] H.Gupta, Samir R. Das, Q. Gu, Connected sensor cover: selforganization of sensor networks for efficient query execution, Proceedings
of ACM MOBIHOC’2003, pp. 189–200.
[18] P. Gupta and P.R. Kumar, Critical power for asymptotic connectivity
in wireless networks, in Stochastic Analysis, Control, Optimization and
Applications, pp. 547-566, Birkhauser, 1998.
[19] G. Gupta and M. Younis, Fault-tolerant clustering of wireless sensor
networks, Wireless Communications and Networking, vol. 3, pp. 15791584, Mar. 2003.
[20] Y. Guo, Path-connectivity in local tournaments, Discrete Math., vol.
167/168, pp. 353-372, 1997.
[21] F. Juhasz, The asymptotic behaviour of Fiedler’s algebraic connectivity
for random graphs.
[22] K. Kar and S. Banerjee, Node Placement for Connected Coverage in
Sensor Networks, Proc. of WiOpt’03, Mar. 2003.
[23] Kulkarni S.S., Iyer A., Rosenberg C., Kofman D.; Routing dependent

node density requirements for connectivity in multi-hop wireless networks, IEEE GLOBECOM ’04, Vol. 5, pp. 2890-2896, Nov. 2004.
[24] Kunniyur S.S., Venkatesh S.S., Sensor network devolution and breakdown in survivor connectivity, Proceedings. International Symposium on
Information Theory’04, pp. 82, Jun. 2004.
[25] Xin Lin, Srikant, R., An information-theoretic view of connectivity in
wireless sensor networks, IEEE SECON’04, pp.508-516, Oct. 2004.
[26] J. A. McHugh, Algorithmic Graph Theory, Prentice Hall, Englewook
Cliffs, New Jersey, 1990.
[27] Pishro-Nik, Chan K.S., Fekri F., On connectivity properties of largescale sensor networks, IEEE SECON’04, pp. 498-507, Oct. 2004.
[28] M.D. Penrose, On k-connectivity for a geometric random graph, Random
Structures and Algorithms, Wiley, vol. 15, no. 2, pp. 145-164, 1999.
[29] B. Peroche, On several sorts of connectivity, Discrete Math. vol.46, pp.
267-277, 1983.
[30] T. J. Philips, S. S. Panwar and A. N. Tantawi, Connectivity properties of
packet radio network model, IEEE Transactions on Information Theory,
vol. 35, no. 5, pp. 1044-1047, Sept. 1989.
[31] P. Piret, On the connectivity of radio networks, IEEE Trans. on Information Theory, vol. 37, no. 5, pp. 1490-1492, Sep. 1991.
[32] Balaji Rangarajan, Chen J.K., Sanjay Shakkottai, Rappaport T.S., Connectivity of sensor networks with power control, The Thirty-Seventh
Asilomar Conference on Signals, Systems and Computers’03, Vol. 2, pp.
1691-1693, Nov. 2003.
[33] A. Sen and M. Huson, A new model for scheduling packet radio networks, Proc. of IEEE Infocom 1996, pp. 1116-1124. Also in ACM/Baltzer
Journal Wiress Networks in 1997.
[34] A. Srinivas and E. Modiano, Minimum Energy Disjoint Path Routing in
Wireless Ad-hoc Networks, MobiCom’03, pp. 122-133, Sep. 2003.
[35] J. Tang, B. Hao and A. Sen, Relay Node Placement in Large Scale
Wireless Sensor Networks, To appear in Computer Communications
Journal.
[36] Di Tian, Georganas N.D.,Connectivity maintenance and coverage preservation in wireless sensor networks, Canadian Conference on Electrical
and Computer Engineering’04, vol. 2, pp. 1097-1100, May 2004.
[37] D. W. Vanderjagt, Graphs with Prescribed local connectivity, Discrete
Mathematics, vol. 10, pp. 391-395, 1974.
[38] M. Wang, Q. Li, Conditinal edge connectivity properties reliability
comparisons and transitivity of graphs, Discrete Math., vol 258, pp. 205214, 2002.
[39] X. Wang, G. Xing, Y. Zhang, C. Lu, R. Pless and C. Gill, Integrated
Coverage and Connectivity Configuration in Wireless Sensor Networks,
SenSys’03, pp. 28-39, Nov. 2003.
[40] Jin Zhu, Papavassiliou S., On the connectivity modeling and the tradeoffs
between reliability and energy efficiency in large scale wireless sensor
networks, IEEE WCNC’03, Vol. 2, pp. 1260 - 1265, Mar. 2003.

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the Proceedings IEEE Infocom.

2014 IEEE 15th International Conference on High Performance Switching and Routing (HPSR)

On Shortest Single/Multiple Path Computation
Problems in Fiber-Wireless (FiWi) Access Networks
Chenyang Zhou∗ , Anisha Mazumder∗ , Arunabha Sen∗ , Martin Reisslein† and Andrea Richa∗
∗ School of Computing, Informatics and Decision Systems Engineering
† School of Electrical, Computer, and Energy Engineering

Arizona State University
Email: {czhou24, anisha.mazumder, asen, reisslein, aricha}@asu.edu

Abstract—Fiber-Wireless (FiWi) networks have received considerable attention in the research community in the last few
years as they offer an attractive way of integrating optical and
wireless technology. As in every other type of networks, routing
plays a major role in FiWi networks. Accordingly, a number of
routing algorithms for FiWi networks have been proposed. Most
of the routing algorithms attempt to ﬁnd the “shortest path”
from the source to the destination. A recent paper proposed
a novel path length metric, where the contribution of a link
towards path length computation depends not only on that link
but also every other link that constitutes the path from the
source to the destination. In this paper we address the problem
of computing the shortest path using this path length metric.
Moreover, we consider a variation of the metric and also provide
an algorithm to compute the shortest path using this variation.
As multipath routing provides a number of advantages over
single path routing, we consider disjoint path routing with the
new path length metric. We show that while the single path
computation problem can be solved in polynomial time in both
the cases, the disjoint path computation problem is NP-complete.
We provide optimal solution for the NP-complete problem using
integer linear programming and also provide two approximation
algorithms with a performance bound of 4 and 2 respectively.
The experimental evaluation of the approximation algorithms
produced a near optimal solution in a fraction of a second.

I. I NTRODUCTION
Path computation problems are arguably one of the most
well studied family of problems in communication networks.
In most of these problems, one or more weight is associated
with a link representing, among other things, the cost, delay or
the reliability of that link. The objective most often is to ﬁnd
a least weighted path (or “shortest path”) between a speciﬁed
source-destination node pair. In most of these problems, if a
link l is a part of a path P , then the contribution of the link
l on the “length” of the path P depends only on the weight
w(l) of the link l, and is oblivious of the weights of the links
traversed before or after traversing the link l on the path P .
However, in a recent paper on optical-wireless FiWi network
[5], the authors have proposed a path length metric, where the
contribution of the link l on the “length” of the path P depends
not only on its own weight w(l), but also on the weights
of all the links of the path P . As the authors of [5] do not
present any algorithm for computing the shortest path between
the source-destination node pair using this new metric, we
present a polynomial time algorithm for this problem in this
paper. This result is interesting because of the nature of new

978-1-4799-1633-7/14/$31.00 ©2014 IEEE

metric proposed in [5], one key property on which the shortest
path algorithm due to Dijkstra is based, that is, subpath of a
shortest path is shortest, is no longer valid. We show that even
without this key property, not only it is possible to compute
the shortest path in polynomial time using the new metric, it
is also possible to compute the shortest path in polynomial
time, with a variation of the metric proposed in [5].
The rest of the paper is organized as follows. In section
III, we present the path length metric proposed for the FiWi
network in [5] and a variation of it. In section IV we provide
algorithms for computing the shortest path using these two
metrics. As multi-path routing offers signiﬁcant advantage
over single path routing [6], [7], [8], [9], we also consider
the problem of computation of a pair of node disjoint paths
between a source-destination node pair using the metric proposed in [5]. We show that while the single path computation
problem can be solved in polynomial time in all these cases,
the disjoint path computation problem is NP-complete. The
contributions of the paper are as follows;
• Polynomial time algorithm for single path routing (metric
1) in FiWi networks
• Polynomial time algorithm for single path routing (metric
2) in FiWi networks
• NP-completeness proof of disjoint path routing (metric
1) in FiWi networks
• Optimal solution for disjoint path routing (metric 1) in
FiWi networks using Integer Linear Programming
• One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 4 and
computation complexity O((n + m)log n)
• One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 2 and
computation complexity O(m(n + m)log n)
• Experimental evaluation results of the approximation
algorithm for disjoint path routing in FiWi networks
II. R ELATED W ORK
Fiber-Wirelss (FiWi) networks is a hybrid access network
resulting from the convergence of optical access networks
such as Passive Optical Networks (PONs) and wireless access
networks such as Wireless Mesh Networks (WMNs) capable of providing low cost, high bandwidth last mile access.

131

Because it provides an attractive way of integrating optical
and wireless technology, Fiber-Wireless (FiWi) networks have
received considerable attention in the research community in
the last few years [1], [2], [3], [4], [5], [8], [9]. The minimum
interference routing algorithm for the FiWi environment was
ﬁrst proposed in [4]. In this algorithm the path length was
measured in terms of the number of hops in the wireless
part of the FiWi network. The rationale for this choice was
that the maximum throughput of the wireless part is typically
much smaller than the throughput of the optical part, and
hence minimization of the wireless hop count should lead to
maximizing the throughout of the FiWi network. However,
the authors of [5] noted that minimization of the wireless
hop count does not always lead to throughput maximization.
Accordingly, the path length metric proposed by them in
[5] pays considerable importance to the trafﬁc intensity at a
generic FiWi network node. The results presented in this paper
are motivated by the path length metric proposed in [5].
III. P ROBLEM F ORMULATION
In the classical path problem, each edge e ∈ E of the graph
G = (V, E), has a weight w(e) associated with it and if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 →1 v1 →2 v2 →3 v3 . . . →k vk
then the path length or the distance between the nodes v0 and
vk is given by
w(Pv0 ,vk ) = w1 + w2 + · · · + wk
However, in the path length metric proposed in [5] for
optical-wireless FiWi networks [1], [2], [3], the contribution
of ei to the path length computation depends not only on
the weight wi , but also on the weights of the other edges
that constitute the path. In the following section, we discuss
this metric and a variation of it. We also also formulate the
multipath computation problem using this metric.
The Optimized FiWi Routing Algorithm (OFRA) proposed
in [5] computes the “length” (or weight) of a path P from v0
to vk using the following metric




(wu ) + max (wu )
w (Pv0 ,vk ) = min
P

∀u∈P

∀u∈P

where wu represents the trafﬁc intensity at a generic FiWi
network node u, which may be an optical node in the ﬁber
backhaul or a wireless node in wireless mesh front-end. In
order to compute shortest path using this metric, in our
formulation, instead of associating a trafﬁc intensity “weight”
(wu ) with nodes, we associate them with edges. This can easily
be achieved by replacing the node u with weight wu with two
nodes u1 and u2 , connecting them with an edge (u1 , u2 ) and
assigning the weight wu on this edge. In this scenario, if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 →1 v1 →2 v2 →3 . . . →k vk
then the path length between the nodes v0 and vk is given by

w+ (Pv0 ,vk )

=
=

w1 + w2 + . . . + wk + max(w1 , w2 , . . . wk )
k

wi + maxki=1 wi
i=1

In the second metric, the length a path Pv0 ,vk :
v0 →v1 →v2 → . . . →vk , between the nodes v0 and vk is given
by

w̄(Pv0 ,vk )

=
=

k

i=1
k


wi + CN T (Pv0 ,vk ) ∗ max(w1 , w2 , . . . wk )
wi + CN T (Pv0 ,vk ) ∗ maxki=1 wi

i=1

where CN T (Pv0 ,vk ) is the count of the number of times
max (w1 , w2 , . . . wk ) appears on the path Pv0 ,vk . We study the
shortest path computation problems in FiWi networks using
the above metrics and provide polynomial time algorithms for
solution in subsections IV-A and IV-B.
If wmax = max(w1 , w2 , . . . wk ), we refer to the corresponding edge (link) as emax . If there are multiple edges having
the weight of wmax , we arbitrarily choose any one of them as
emax . It may be noted that both the metrics have an interesting
property in that in both cases, the contribution of an edge e
on the path length computation depends not only on the edge
e but also on every other edge on the path. This is so, because
if the edge e happens to be emax , contribution of this edge
in computation of w+ (Pv0 ,vk ) and w̄(Pv0 ,vk ) will be 2 ∗ w(e)
and CN T (Pv0 ,vk ) ∗ w(e) respectively. If e is not emax , then
its contribution will be w(e) for both the metrics.
As multipath routing provides an opportunity for higher
throughput, lower delay, and better load balancing and resilience, its use have been proposed in ﬁber networks [6],
wireless networks [7] and recently in integrated ﬁber-wireless
networks [8], [9]. Accordingly, we study the problem of
computing a pair of edge disjoint paths between a sourcedestination node pair s and d, such that the length of the
longer path (path length computation using the ﬁrst metric)
is shortest among all edge disjoint path pairs between the
nodes s and d. In subsection IV-C we prove that this problem
is NP-complete, in subsection IV-D, we provide an optimal
solution for the problem using integer linear programming,
in subsections IV-E and IV-F we provide two approximation
algorithms for the problem with a performance bound of 4
and 2 respectively, and in subsection IV-F we provide results
of experimental evaluation of the approximation algorithms.
IV. PATH P ROBLEMS IN F I W I N ETWORKS
In this section, we present (i) two different algorithms for
shortest path computation using two different metrics, (ii)
NP-completeness proof for the disjoint path problem, (iii)
two approximation algorithms for the disjoint path problem,
and (iv) experimental evaluation results of the approximation
algorithms.

132

It may be noted that, in both metrics w+ (Pv0 ,vk ) and
w̄(Pv0 ,vk ), we call an edge e ∈ Pv0 ,vk crucial, if w(e) =
maxki=1 w(e ), ∀e ∈ Pv0 ,vk .
A. Shortest Path Computation using Metric 1
It may be recalled that the path length
k metric used in this
case is the following: w+ (Pv0 ,vk ) = i=1 wi + maxki=1 wi . If
k
the path length metric was given as w(Pv0 ,vk ) = i=1 wi ,
algorithms due to Dijkstra and Bellman-Ford could have been
used to compute the shortest path between a source-destination
node pair. One important property of the path length metric
that is exploited by Dijkstra’s algorithm is that “subpath of a
shortest
path is shortest”. However, the new path length metric
k
k
i=1 wi +maxi=1 wi does not have this property. We illustrate
this with the example below.
Consider two paths P1 and P2 from the node v0 to v3 in the
w
w
w
graph G = (V, E), where P1 : v0 →1 v1 →2 v2 →3 v3 and P2 :
w4
w5
w3
v0 → v4 → v2 → v3 . If w1 = 0.25, w2 = 5, w3 = 4.75, w4 =
2, w5 = 4, the length of the path P1 , w+ (P1 ) = w1 +w2 +w3 +
max(w1 , w2 , w3 ) = 0.25 + 5 + 4.75 + max(0.25, 5, 4.75) = 15
and the length of the path P2 , w+ (P2 ) = w4 + w5 + w6 +
max(w4 , w5 .w6 ) = 2 + 4 + 4.75 + max(2, 4, 4.75) = 15.5.
Although P1 is shortest path in this scenario, the length of
w
w
its subpath v0 →1 v1 →2 v2 is 0.25 + 5 + max (0.25, 5) =
10.25, which is greater than the length of a subpath of P2
w
w
v0 →4 v4 →5 v2 2 + 4 + max (2, 4) = 10, demonstrating that
the assertion that “subpath of a shortest path is shortest” no
longer holds in this path length metric.
As the assertion “subpath of a shortest path is shortest” no
longer holds in this path length metric, we cannot use the
standard shortest path algorithm due to Dijkstra in this case.
However, we show that we can still compute the shortest path
between a source-destination node pair in polynomial time by
repeated application of the Dijkstra’s algorithm. The algorithm
is described next.
For a given graph G = (V, E), w.l.o.g, we assume |V | = n
and |E| = m. Deﬁne Ge as subgraph of G by deleting edges
whose weight is greater than w(e).
Also, as Dijkstra’s algorithm does, we need to maintain
distance vector. We deﬁne distv be distance (length of shortest
path) from s to v, Πv be predecessor of v and maxedgev be
weight of the crucial edge from s to v via the shortest path,
ansv be optimal solution (length) from s to v.
Different Ge can be treated as different layers of the
G. For any path P , we deﬁne the function e∗ (P ) as the
crucial edge along P . It is easy to observe that if
Pd is the
optimal path from s to node d then w(Pd ) =
e∈P w(e)
and w+ (Pd ) = w(Pd ) + w(e∗ (Pd )). It may be noted that
henceforth, we shorten Ps,d to Pd , because we consider that
the source is ﬁxed while the destination d is variable.
Lemma 1. w(Pd ) is minimum in Ge∗ (Pd ) .
Proof: It is obvious that Pd still exists in Ge∗ (Pd ) , since
edges on Pd are not abandoned. Suppose Pd is not shortest,
then there must be another path Pd s.t. w(Pd ) < w(Pd ).
Noting that the crucial edge on Pd , namely e , is no longer than

Algorithm 1 Modiﬁed Dijkstra’s Algorithm
1: Initialize ansv = ∞ for for all v ∈ V
2: sort all edges according to w(e) in ascending order
3: for i = 1 to m do
4:
Initialize distv = ∞, Πv = nil, maxedgev = 0 for
all v ∈ V
5:
dists = 0
6:
Q = the set of all nodes in graph
7:
while Q is not empty do
8:
u = Extract-Min(Q)
9:
for each neighbor v of u do
10:
if eu,v ∈ E(Gei ) then
11:
t = MAX {maxedgeu , w(eu,v )}
12:
if distu + w(eu,v ) < distv then
13:
distv = distu + w(eu,v )
14:
maxedgev = t
15:
Πv = u
16:
else if distu + w(eu,v ) == distv then
17:
if maxedgev > t then
18:
maxedgev = t
19:
Πv = u
20:
end if
21:
end if
22:
end if
23:
end for
24:
end while
25:
for each node v do
26:
ansv = min{ansv , distv + maxedgev }
27:
end for
28: end for
e∗ (Pd ) since they both belong to Ge∗ (Pd ) . Hence w+ (Pd ) =
w(Pd )+w(e ) < w(Pd )+w(e∗ (Pd )) = w+ (Pd ), contradicting
Pd is optimal.
Lemma 2. Modiﬁed Dijkstra’s Algorithm (MDA) computes
shortest path while keeping the crucial edge as short as
possible in every iteration.
Proof: Line 4 to 24 works similar to the standard Dijkstra’s algorithm does. Besides, when updating distance, MDA
also updates the crucial edge to guarantee that it lies on the
path and when there is a tie, MDA will choose the edge with
the smaller weight.
Theorem 1. Modiﬁed Dijkstra’s Algorithm computes optimal
solution for every node v in O(m(n + m)logn) time.
Proof: Lemma 1 indicates for any node v ∈ V , optimal
solution can be obtained by enumerating all possible crucial
edges e∗ (Pv ) and computing shortest path on Ge∗ (Pv ) . By sorting all edges in nondecreasing order, every subgraph Ge∗ (Pv )
is considered and it is shown in lemma 2, MDA correctly
computes shortest path for every node v in every Ge∗ (Pv ) .
Then optimal solution is obtained by examining all shortest
path using the w() metric plus the corresponding crucial edge.
Dijkstra’s algorithm runs O((n + m)logn) time when using

133

binary heap, hence MDA runs in O(m(n+m)logn) time when
considering all layers.
B. Shortest Path Computation using Metric 2
Given a path P , let e∗ (P ) be the crucial edge along the
P and CN T (P ) be the number of occurrence of such edge.
Now
a path Q, such that w̄(P ) =
 our objective becomes to ﬁnd
∗
w(e)
+
CN
T
(Q)
∗
w(e
(Q))
is minimum.
e∈Q
The layering technique can also be used in this problem.
However, shortest path under a ceratin layer may not become
a valid candidate for optimal solution. Here, we introduce a
dynamic programming algorithm that can solve the problem
optimally in O(n2 m2 ) time.
Input is a weighted graph G = (V, E), |V | = n, |E| = m
with a speciﬁed source node s. In this paper, we only
consider nonnegative edge weight. As shown before, we use
Ge to represent the residue graph by deleting edges longer
than e in G. Different from MDA1, in order to consider
the number of crucial edges, distv is replaced by an array
dist0v , dist1v , ....distnv . One can think distcv be the shortest
distance from s to v by going through exactly c crucial edges
and possibly some shorter edges. Similarly, we replace Πv by
Πcv , 0 ≤ c ≤ n. Each Πcv records predecessor of v for the
path corresponding to distcv . Lastly, ansv is used as optimal
solution from s to v.
Lemma 3. If Pv is the best path from s to v, i.e., w̄(Pv ) is
minimum among all s-v path, then Pv is computed in Ge∗ (Pv )
CN T (Pv )
and distv
= w(Pv ).
Proof: By deﬁnition, Pv exists in Ge∗ (Pv ) and
CN T (Pv ) ≥ 1 since any path should go through at least one
crucial edge. Noting w̄(Pv ) = w(Pv )+CN T (Pv )∗w(e∗ (Pv )),
on one hand if we treat CN T (Pv ) as a ﬁxed number, then we
need to keep w(Pv ) as small as possible. Inspired by idea
of bellman-ford algorithm, we can achieve it by enumerating
|Pv |, i.e., number of edges on Pv . On the other hand, we need
to keep tracking number of crucial edges as well. Hence, distcv
is adopted to maintain such information, superscript c reﬂects
exact number of crucial edges. From line 12 to line 25, distcv
is updated either when it comes from a neighbor who has
already witnessed c crucial edges or it comes from a neighbor
with c − 1 crucial edges and the edge between is crucial. In
either case, node v gets a path, say P  , with exact c crucial
edges on it and w(P ) is minimum. At last, Pv can be selected
by enumerating number of crucial edges and that is what line
30 to 32 does.
Lemma 4. Maxedge Shortest Path Algorithm(MSPA) runs in
O(n2 m) time for each Ge .
Proof: We can apply similar analysis of bellman-ford
algorithm. However, we need to update distcv array, it takes
extra O(n) time for every node v in every iteration when
enumerating |Pv |. Hence, total running time is O(n2 m).
Theorem 2. MSPA computes optimal path for every v ∈ V
in O(n2 m2 ) time.

Algorithm 2 Maxedge Shortest Path Algorithm
1: Initialize ansv = ∞ for for all v ∈ V
2: sort all edges according to w(e) in ascending order, say
e1 , e2 , ..., em after sorting
3: for i = 1 to m do
4:
Initialize distcv = ∞, Πcv = nil for all v ∈ V and all
0≤c≤n
5:
dist0s = 0
6:
for j = 1 to n − 1 do
7:
for k = 0 to j do
8:
for every node v ∈ V do
9:
if distkv = ∞ then
10:
continue
11:
end if
12:
for every neighbor u of v do
13:
if w(eu,v ) > w(ei ) then
14:
continue
15:
else if w(eu,v ) == w(e∗ ) then
16:
if distkv + w(eu,v )
<
then
distk+1
u
17:
distk+1
= distkv +
u
w(eu,v )
18:
Πk+1
=v
u
19:
end if
20:
else
21:
if distkv + w(eu,v ) < distku
then
22:
distku = distkv +
w(eu,v )
23:
Πku = v
24:
end if
25:
end if
26:
end for
27:
end for
28:
end for
29:
end for
30:
for i = 1 to n − 1 do
31:
ansv = min{ansv , distiv + i ∗ w(ei )}
32:
end for
33: end for

Proof: By Lemma 3, if Pv is obtained when computing
Ge∗ Pv . Then, by considering all possible Ge∗ , we could get
Pv in one of these layering. It takes O(m) to generate all Ge∗ ,
by Lemma 4, MSPA runs in v ∈ V in O(n2 m2 ) time.
C. Computational Complexity of Disjoint Path Problem
In this section, we study edge disjoint path in optical
wireless network. By reduction from well known Min-Max
2-Path Problem, i.e., min-max 2 edge disjoint path problem
under normal length measurement, we show it is also
NP-complete if we try to minimize the longer path when w+
length is applied. Then we give an ILP formulation to solve
this problem optimally. At last, we provide two approximation
algorithm, one with approximation ratio 4, running time

134

O((m + n)logn), the other one with approximation ratio 2
while running time is O(m(m + n)logn).

D. Optimal Solution for the Disjoint Path Problem
Here, we give an ILP formulation for MinMax2OWFN.
ILP for MinMax2OWFN

Min-Max 2 Disjoint Path Problem (MinMax2PP)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e ∈ E, a source node
s ∈ V , a destination node t ∈ V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to d in G such that w(P1 ) ≤ w(P2 ) ≤ X?

min
s.t.

MP



The MinMax2PP problem is shown to be NP-complete in
[10]. With a small modiﬁcation, we show NP-completeness
still holds if w+ length measurement is adopted.

fi,j,1 −

fj,i,1 =

(i,j)∈E

(j,i)∈E





fi,j,2 −

(i,j)∈E

Min-Max 2 Disjoint Path Problem in Optical Wireless
Networks (MinMax2OWFN)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e ∈ E, a source node
s ∈ V , a destination node t ∈ V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to t in G such that w+ (P1 ) ≤ w+ (P2 ) ≤ X  ?



fj,i,2 =

(j,i)∈E

⎧
⎪
⎨
⎪
⎩
⎧
⎪
⎨
⎪
⎩

1
−1
0

i=s
i=t
otherwise

1
−1
0

i=s
i=t
otherwise

fi,j,1 + fi,j,2 ≤ 1
w1 ≥ fi,j,1 ∗ w(i, j)

∀(i, j) ∈ E

w2 ≥ fi,j,2 ∗ w(i, j)

fi,j,1 ∗ w(i, j)
M P ≥ w1 +

∀(i, j) ∈ E

∀(i, j) ∈ E

(i,j)∈E



M P ≥ w2 +

Theorem 3. The MinMax2OWFN is NP-complete

fi,j,2 ∗ w(i, j)

(i,j)∈E

Proof: Evidently, MinMax2OWFN is in NP class, given
two edge joint path P1 and P2 , we can check if w+ (P1 ) ≤
w+ (P2 ) ≤ X  in polynomial time.
We then transfer from MinMax2PP to MinMax2OWFN.
Let graph G = (V, E) with source node s , destination t and
an integer X be an instance of MinMax2PP, we construct an
instance G’ of MinMax2OWFN in following way.
1) Create an identical graph G with same nodes and edges
in G.
2) Add one node s0 to G .
3) Create two parallel edges e01 , e02 between s0 and s,
w(e01 ) = w(e02 ) = maxe∈G(E) w(e)
4) Choose s0 to the source node in G and t to be the
destination.
5) Set X  = X + 2w(e01 )
It is easy to see, the construction takes polynomial time.
Now we need to show a instance of MinMax2OWFN have
two edge disjoint paths from s0 to t with length at most X  if
and only if the corresponding instance have two edge disjoint
paths from s to t with length at most X.
Suppose there are two edge disjoint paths P1 and P2 from
s0 to t in G , such that w+ (P1 ) ≤ w+ (P2 ) ≤ X  . By the
way we construct G , P1 and P2 must go through e01 and
e02 . W.l.o.g. we say e01 ∈ P1 and e02 ∈ P2 . Since w(e01 ) =
w(e02 ) = maxe∈E(G ) {w(e)}, therefore e01 and e02 are the
crucial edge on P1 and P2 respectively. Hence, P1 − e01 and
P2 −e02 are two edge disjoint path in G, with length no greater
than X  − 2w(e01 ) = X.
Conversely, now suppose P1 and P2 are two edge joint paths
in G satisfying w(P1 ) ≤ w(P2 ) ≤ X. We follow the same
argument above, P1 + e01 and P2 + e02 are two desired paths,
with length not exceeding X + 2w(e01 ) = X  .

fi,j,1 = {0, 1},

fi,j,2 = {0, 1}

∀(i, j) ∈ E

The following is a brief description of this ILP formulation.
The ﬁrst two equation represent ﬂow constraint as normal
shortest path problem does. fi,j,1 = 1 indicates path P1 goes
through edge (i, j), and 0 otherwise. So it is with fi,j,2 and
path P2 . Constraint 3 ensures two edges are disjoint, since
fi,j,1 and fi,j,2 cannot both be 1 at the same time. w1 , w2 act
as the weights of the crucial edges on P1 and P2 respectively.
Finally, we deﬁne M P to be the maximum of w+ (P1 ) and
w+ (P2 ) and therefore try to minimize it.
E. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 4
Next we propose a 4-approximation algorithm which runs
in O((n + m)logn) time.
Given G = (V, E) with source s and destination t, the idea
of approximation algorithm is to ﬁnd two disjoint P1 and P2
such that w(P1 ) + w(P2 ) is minimized. Such P1 and P2 can
be found either using min cost max ﬂow algorithm or the
algorithm due to Suurballe presented in [11]. And we need to
show both w+ (P1 ) and w+ (P2 ) are at most four times of the
optimal solution.
Algorithm 3 MinMax2OWFN Approximation Algorithm 1 (MAA1)
1: Run Suurballe’s algorithm on G, denote P1 , P2 be two
resulting path.
2: Compute w + (P1 ) and w + (P2 ).
3: Output max{w + (P1 ), w + (P2 )}.

135

Lemma 5. For any path P , w+ (P ) ≤ 2w(P ).
Proof: By deﬁnition, w+ (P ) = w(P ) + w(e∗ (P )). Since
w(e (P )) ≤ w(P ), then w+ (P ) ≤ 2 ∗ w(P ).
∗

Lemma 6. If P1 and P2 are two edge joint path from s to
t such that w(P1 ) + w(P2 ) is minimum, then w+ (P1 ) and
w+ (P2 ) are at most four times of the optimal solution.
Proof: Say opt is the optimal value of a
M inM ax2OW F N instance and Q1 ,Q2 are two s − t
edge disjoint path in one optimal solution. W.l.o.g, we may
suppose w+ (P1 ) ≥ w+ (P2 ) and w+ (Q1 ) ≥ w+ (Q2 ). Let
w(P1 ) + w(P2 ) = p and w(Q1 ) + w(Q2 ) = q, by assumption,
p ≤ q. Also, we have w+ (P1 ) = w(P1 ) + e∗ (P1 ) ≤ 2p,
opt = w+ (Q1 ) = w(Q1 ) + e∗ (Q1 ) > 2q . Hence,
+
w+ (P2 )
(P1 )
2p
≤ w opt
< q/2
≤4
opt
Theorem 4. MAA1 is a 4-approximation algorithm running
in O((n + m)logn) time and 4 is a tight bound.
Proof: By Lemma 5 and 6, MAA1 has approximation
ratio at most 4.Then we show MAA1 has approximation at
least 4 for certain cases. Consider the following graph.

Algorithm 4 MinMax2OWFN Approximation Algorithm
2(MAA2)
1: set ans = ∞
2: for every Ge of G do
3:
Run Suurballe’s algorithm on Ge , denote P1 , P2 be
two resulting path.
4:
Compute w+ (P1 ) and w+ (P2 ).
5:
ans = min{ans, max{w+ (P1 ), w+ (P2 )}}.
6: end for
7: Output ans.
w(P1 )+w(e )
max{w+ (Q1 ), w+ (Q2 )} < 2. We
w(P1 )+w(e )
Suppose max{w
+ (Q ), w + (Q )} ≥ 2,
1
2

w(e ). It sufﬁces to show

prove

it by contradiction.

then

w(P1 ) + w(e ) ≥ w+ (Q1 ) + w+ (Q2 )
Which follows,
w(P1 ) + w(e ) ≥ w(Q1 ) + w(e∗ (Q1 )) + w(Q2 ) + w(e∗ (Q2 ))
By deﬁnition, e is one of e∗ (Q1 ), e∗ (Q2 ). Hence,
w(P1 ) > w(Q1 ) + w(Q2 )
It is impossible since w(P1 ) + w(P2 ) is minimum in layer
G e .
Theorem 5. MAA2 is a 2-approximation algorithm running
in O(m(n + m)logn) time and 2 is a tight bound.

It is easy to check, P1 = {s → t}, P2 = {s → r → t} are
two edge disjoint path with minimum length 2k+2, w+ (P1 ) =
4k > w+ (P2 ) = 3. However, let Q1 = {s → u1 → u2 →
... → uk−1 → uk → r → t}, Q2 = {s → r → v1 → v2 →
... → vk−1 → vk → t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
4k
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 ≈ 4 when k is
sufﬁciently large. Hence, 4 is a tight bound for MAA1.
We need O((n + m)logn) time running Suurballe’s algorithm and O(n) time computing w+ (P1 ) and w+ (P2 ).
Therefore total running time is O((n + m)logn).
F. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 2
In MAA1, layering technique is not used and we only
consider the original graph. However, by taking all Ge of G
into account, we can have a better approximation ratio.
Say Q1 , Q2 are two disjoint paths in one optimal solution.
Let e = max{e∗ (Q1 ), e∗ (Q2 )} and P1 , P2 be the resulting
paths when computing layer Ge ; w.l.o.g, we may assume
w(P1 ) > w(P2 ). Also, let anse = max{w+ (P1 ), w+ (P2 )}.
Lemma 7. anse < 2 max{w+ (Q1 ), w+ (Q2 )}.
Proof: Noting that w(e∗ (P1 )) ≤ w(e ) and w(e∗ (P2 )) ≤
w(e ) since they both belong to Ge . Then anse ≤ w(P1 ) +


Proof: By Lemma 7, in one of the layer, we guarantee
to have a 2-approximation solution. Since we take minimum
outcome among all layers, the ﬁnal result is no worse than
twice of the optimal solution. Now we need to show there
exists certain case, such that MAA2 is no good than twice of
the optimal solution. Consider the following graph

There is only one layer, and P1 = {s → x1 → x2 →
... → x2k−1 → x2 k → t}, P2 = {s → r → t} are two
edge disjoint path with minimum length 2k + 3, w+ (P1 ) =
2k + 2 > w+ (P2 ) = 3. Again, set Q1 = {s → u1 → u2 →
... → uk−1 → uk → r → t}, Q2 = {s → r → v1 → v2 →
... → vk−1 → vk → t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
2k+2
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 ≈ 2 when k is
sufﬁciently large. Hence, 2 is a tight bound for MAA2.
Finally, it is easy to see that the running time is O(m(n +
m)logn).

136

S
node
14
18
1
18
20
10
1
14
20
10
18
1
20
14
10
20
5

D
node
2
8
6
4
3
3
11
6
7
5
12
20
13
19
17
16
11

Opt
Sol
47
46
28
50
40
27
35
50
38
36
22
46
26
29
36
29
40

Approx
Sol 1
55
46
28
58
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 1
1.17
1
1
1.16
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Approx
Sol 2
55
46
28
57
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 2
1.17
1
1
1.14
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Fig. 1.

TABLE I
C OMPARISON OF THE A PPROXIMATE SOLUTIONS WITH THE O PTIMAL
SOLUTION FOR THE ARPANET GRAPH

The ARPANET graph with 20 nodes and 32 links

approximation algorithms. Both the approximation algorithms
have a constant factor approximation bound. However, there is
a trade-off between the quality of the solution (approximation
bound) and the execution time. Finally, we show that both the
approximation algorithms obtain near optimal results through
simulation using the ARPANET topology.

G. Experimental Results for the Disjoint Path Problem
In this section, we present the results of simulations for
comparing the performance of our approximation algorithms
with the optimal solution when w+ () metric is applied.
The simulation experiments have been carried out on the
ARPANET topology (as shown in Fig 1 with nodes and links
shown in black) which has twenty nodes and thirty two links.
The weights of the links have been randomly generated and
lie in the range of two and eleven (as shown in red in Fig
1) and we consider the graph to be undirected. The results of
the comparison is presented in Table I. We have compared the
lengths of the longer of the two edge disjoint paths computed
by the optimal and the approximate solutions for seventeen
different source-destination pairs. It may be noted that for
almost 65% of the cases, the approximate algorithms obtain
the optimal solution. In the remaining cases, the approximate
solutions lie within a factor of 1.2 of the optimal solution
Thus, even though the approximation ratio in the worst case
are proven to be 4 and 2, in practical cases, it is within 1.2.
From these experimental results, we can conclude that the
approximation algorithms produce optimal or near optimal
solutions in majority of the cases. It may be noted that the
two approximation algorithms perform in a similar fashion
in the ARPANET graph, however, as proven theoretically,
the two approximation algorithms differ in their worst case
approximation ratio.
V. C ONCLUSION

R EFERENCES
[1] N. Ghazisaidi, M. Maier, and C. M. Assi, “Fiber-Wireless (FiWi) Access
Networks: A Survey”, IEEE Communications Magazine, vol. 47, no. 2,
pp 160-167, Feb. 2009.
[2] N. Ghazisaidi, and M. Maier, “Fiber-Wireless (FiWi) Access Networks:
Challenges and Opportunities”, IEEE Network, vol. 25, no. 1, pp 36-42,
Feb. 2011.
[3] Z. Zheng, J. Wang, X. Wang, “ONU placement in ﬁber-wireless (FiWi)
networks considering peer-to-peer communications”, IEEE Globecom, 2009.
[4] Z. Zheng, J. Wang, X. Wang, “A study of network throughput gain
in optical-wireless (FiWi) networks subject to peer-to-peer commuincations”, IEEE ICC, 2009.
[5] F. Aurzada, M. Levesque, M. Maier, M. Reisslein, “FiWi Access
Networks Based on Next-Generation PON and Gigabit-Class WLAN
Technologies: A Capacity and Delay Analysis”, IEEE/ACM Transactions
on Networking, to appear.
[6] A. Sen, B.Hao . B. Shen , L.Zhou and S. Ganguly, “On maximum
available bandwidth through disjoint paths”, Proc. of IEEE Conf. on
High Performance Switching and Routing, 2005.
[7] M. Mosko, J.J. Garcia-Luna-Aceves, “Multipath routing in wireless
mesh networks”, Proc. of IEEE Workshop on Wireless Mesh Networks, 2005.
[8] J. Wang, K. Wu, S. Li and C. Qiao ,“Performance Modeling and Analysis
of Multi-Path Routing in Integrated Fiber-Wireless (FiWi) Networks”,
IEEE Infocom mini conference, 2010.
[9] S. Li, J. Wang, C. Qiao, Y. Xu ,“Mitigating Packet Reordering in
FiWi Networks”, IEEE/OSA Journal of Optical Communications and
Networking, vol. 3, pp.134-144, 2011.
[10] C. Li, S.T. McCormick and D.Simchi-Levi, “Complexity of Finding Two
Disjoint Paths with Min- Max Objective”, Discrete Applied Mathematics, vol. 26, pp. 105-115, 1990.
[11] J. W. Suurballe, “Disjoint paths in a network”, Networks, vol. 4, pp. 125145, 1974.

In this paper, we study the shortest path problem in FiWi
networks. Based on the path length metrics proposed in [3],
[5], we present polynomial time algorithms for the single
path scenario. In the disjoint path scenario, we prove that the
problem of ﬁnding a pair of disjoint paths, where the length
of the longer path is shortest, is NP-complete. We provide an
ILP solution for the disjoint path problem and propose two

137

On Auxiliary Entity Allocation Problem in
Multi-layered Interdependent Critical
Infrastructures

arXiv:1703.06744v1 [cs.NI] 24 Jan 2017

Joydeep Banerjee, Arunabha Sen and Chenyang Zhou
School of Computing, Informatics and Decision System Engineering
Arizona State University, Tempe, Arizona 85287
Email: {joydeep.banerjee, asen, czhou24}@asu.edu

Abstract. Operation of critical infrastructures are highly interdependent on each other. Such dependencies causes failure in these infrastructures to cascade on an initial failure event. Owing to this vulnerability
it is imperative to incorporate efficient strategies for their protection.
Modifying dependencies by adding additional dependency implications
using entities (termed as auxiliary entities) is shown to mitigate this issue to a certain extent. With this finding, in this article we introduce
the Auxiliary Entity Allocation problem. The objective is to maximize
protection in Power and Communication infrastructures using a budget in number of dependency modifications using the auxiliary entities.
The problem is proved to be NP-complete in general case. We provide
an optimal solution using Integer Linear program and a heuristic for a
restricted case. The efficacy of heuristic with respect to the optimal is
judged through experimentation using real world data sets with heuristic
deviating 6.75% from optimal on average.

Keywords: Interdependent network, IIM Model, Auxiliary Entity, Dependency
Modification, K Most Vulnerable Entities.

1

Introduction

Critical infrastructures like power, communication, transportation networks etc.
interact symbiotically to carry out their functionalities. As an example there
exists strong mutual interactions between the power and communication network or infrastructure (in this article the term infrastructure and network are
used interchangeably). Entities in the power network like generators, substations,
transmission lines etc. relies on control signals carried over by communication
network entities like routers, fiberoptic lines etc. Similarly all entities in the communication network relies on power supply from the power network to drive their
functionalities. To capture this kind of dependencies the critical infrastructure
can be modeled as a multilayered interdependent network. Failure of entities in
either infrastructure impacts the operation of its own infrastructure as well as
the other infrastructure. Owing to these dependencies the initial failure might

result in cascade of failures resulting in disastrous impact. This has been observed in power blackouts which occurred in New York (2003) [1] and India
(2012) [11].
To study the nature of failure propagation in these interdependent networks
it is imperative to model their dependencies as accurately as possible. Recent
literature consists of a plethora of these models [4], [10], [5], [5], [10], [8], [12],
[7], [6], [3]. However each of these models have their own shortcoming in capturing the complex dependencies that might exist. For example consider a scenario
with one power network entity a1 and three communication entities b1 , b2 , b3 .
The entity a1 is operational provided that both entities b1 and b2 are operational or if entity b3 is operational (note that the italicized words represent
logical operations). None of the above models can accurately model this kind
of a dependency. Sen et. al. in [9] proposed a model that uses boolean logic to
capture these interdependencies. This model is referred to as the Implicative
Interdependency model (IIM)). To express the dependency of an entity on other
entities it uses implications which are disjunction(s) and conjunction(s) of logical
terms (denoting entities of the network). With respect to the example considered above the dependency implication for the entity a1 can be represented as
a1 ← b1 b2 + b3 . The boolean implication depicting the dependency is termed as
Inter-Dependency Relation. Our approach in designing solutions and analyzing
the problem addressed in this paper is based on the IIM model.
We restrict our attention to an interdependent power and communication
network in this paper. However the solutions can be extended to any two interdependent networks. As discussed earlier initial failure of a certain entity set
in power and communication network may trigger cascading failure resulting in
loss of a large number of entities. Authors in [2] proposed the Entity Hardening problem to increase the reliability of these interdependent systems. They
assumed that an entity when hardened would be resistant to both initial and
cascading failure. Given a set of entities that failed initially (that is at time t = 0)
the problem was to find a minimal set of entities which when hardened would
prevent failure of at least a predefined number of entities. On situations where
entity hardening is not possible alternative strategies needs to be employed to
increase the system reliability. Adding additional dependencies for entities in
interdependent infrastructure can be beneficial in this regard. We elaborate this
with the help of an example. Consider the dependency rule a1 ← b1 b2 + b3 . With
this dependency entity a1 would fail if entities (b1 , b3 ) or (b2 , b3 ) fails. Now consider an entity b4 is added as a disjunction to the IDR (with the new dependency
being a1 ← b1 b2 + b3 + b4 ). For entity a1 to fail, either (b1 , b3 , b4 ) or (b2 , b3 , b4 )
should fail. This increases the reliability compared to the previous dependency
for a1 . Hence adding additional dependency can be employed as a strategy when
entity hardening is not possible. Any entity added to modify a dependency relation is termed as an auxiliary entity. However due to system, cost and feasibility
constraints the number of such modifications are restricted. Hence when the
number of IDR modifications are restricted one has to find which IDRs to modify and with what entities so that the impact of failure is minimized. We term
this problem as the Auxiliary Entity Allocation Problem. It is to be noted that
in both Entity Hardening Problem and Auxiliary Entity Allocation Problem the

IDRs of the interdependent system are changed but these changes are carried
out differently.
The rest of the paper is organized as follows. A brief explanation of the
IIM model with formal problem definition is provided in Section 2. The computational complexity of the problem and proposed solutions are discussed in
Sections 3 and 4 respectively. We discuss the experimental results in Section 5
and conclude the paper in Section 6

2

Problem Formulation using the Implicative
Interdependency Model

We briefly describe the IIM model introduced in [9]. Two sets A and B represent entities in power and communication network. The dependencies between
these set of entities are captured using a set of interdependency denoted as
F(A, B). Each function in the set F(A, B) is termed as an Inter-Dependency
Relation (IDR). We describe an interdependent network which composes of the
entity sets A and B and the interdependency relations F(A, B) and denote it by
I(A, B, F(A, B)). Through an example we explain this model further. Consider
an interdependent network I(A, B, F(A, B)) with A = {a1 , a2 , a3 , a4 , a5 } and
B = {b1 , b2 , b3 }. The set of IDRs (F(A, B)) for the interdependent network are
provided in Table 1. Consider the IDR a3 ← b2 + b1 b3 in Table 1. It implies that
the entity a1 is operational if entity b2 or entity b1 and b3 are operational. As
evident, the IDRs are essentially disjunction(s) of entity (entities) in conjunctive form. We refer to each conjunctive term, e.g. b1 b3 , as minterm. The example
considers dependencies where an entity in network A(B) is dependent on entities
in network B(A) i.e. inter-network dependency. However this model can capture
intra-network dependencies as well.

Power Network
a1 ← b1 + b2
a2 ← b1 b2
a3 ← b2 + b1 b3
a4 ← b3
a5

Comm. Network
b1 ← a2
b2 ← a2
b3 ← a4
−−
−−

Table 1: IDRs for the constructed
example

Entities Time Steps (t)
01234567
a1
00011111
a2
01111111
a3
01111111
a4
01111111
a5
00000000
b1
00111111
b2
11111111
b3
11111111

Table 2: Cascade propagation when
entities {b2 , b3 } fail initially. 0 denotes the entity is operational and
1 non-operational

The cascading procedure is described with respect to the interdependent
network captured by IDRs in Table 1. The cascade proceeds in unit time steps

(denoted by t). Consider two entities b2 and b3 are attacked and made non
operational by an adversary at time step t = 0 (initial failure). Owing to the
IDRs a2 ← b1 b2 , a3 ← b2 + b1 b3 and a4 ← b3 the entities a2 , a3 , a4 becomes non
operational at t = 1. Subsequently entities b1 (b1 ← a2 ) and a1 (a1 ← b1 + b2 )
seize to operate at time step t = 2 and t = 3 respectively. The failure of entities
after t = 0 is termed as induced failure. The cascade is represented in Table 2.
It is to be noted that the maximum number of time steps in the cascade for any
interdependent network is |A| + |B| − 1 (assuming initial time step as t = 0).
Hence in Table 2 with number of entities being 8 the state (operational or nonoperational) of all entities are shown till t = 7. Construction of these IDRs is
a major challenge of this model. Possible strategies are (i) deep investigation of
physical properties and flows in the interdependent network and (ii) consultation
with domain experts. The methodology to construct these IDRs is ongoing and
is expected to be addressed in future. The problem in this article assumes that
the IDRs are already constructed for a given interdependent network.
Authors in [9] introduced the K most vulnerable entities problem. The problem used the IIM model to find a set of K (for a given integer |K|) entities in
an interdependent network I(A, B, F(A, B)) whose failure at time t = 0 (initial
failure) would result in failure of the largest number of entities due to induced
failure. For the example provided in this section consider the case where |K| = 2.
Failing entities b2 and b3 at t = 0 make entities {a1 , a2 , a3 , a4 , b1 , b2 , b3 } not operational by t = 3. Hence K = {b2 , b3 } are one of the 2 most vulnerable entities
in the interdependent network (it is possible to have multiple K most vulnerable
entities in an interdependent network). The set of entities failed when K most
vulnerable entities fail initially is denoted by A0 ∪ B 0 with A0 ⊆ A and B 0 ⊆ B.
Here A0 = {a1 , a2 , a3 , a4 } and B 0 = {b1 , b2 , b3 }.
For a given K most vulnerable entities of an interdependent network, the
system reliability can be increased (i.e. entities can be protected from failure)
by Entity Hardening [2]. On scenarios where entity hardening is not possible it
is imperative to take alternative strategies. The number of entities failing due
to induced failure can be reduced by modifying the IDRs. One way of modifying
an IDR is adding an entity as a new minterm. For example, consider the interdependent network with IDRs given by Table 1 and b2 and b3 being the 2 (when
K = 2) most vulnerable entities (as discussed above). Let the IDR b1 ← a2 be
modified as b1 ← a2 + a5 . Hence the new interdependent network is represented
as I(A, B, F 0 (A, B)) with the same set of IDRs as that in Table 1 except for IDR
b1 ← a2 + a5 as the sole modification. The entity a1 introduced is termed as an
auxiliary entity. It follows that after the modification, failure of entities b2 and
b3 at time t = 0 would trigger failure of entities a2 , a3 and a4 only. Thus before
modification the failure set would have been {a1 , a2 , a3 , a4 , b1 , b2 , b3 } and after
the modification it would be {a2 , a3 , a4 , b2 , b3 }. Thus the modification would lead
to a fewer number of failures.
We make the following assumptions while modifying an IDR —
– It is possible to add an auxiliary entity as conjunction to a minterm. However
it is intuitive that this would have no impact in decreasing the number of
entities failed due to induced failure. Hence we modify an IDR by adding
only one auxiliary entity as a disjunction to a minterm

– An auxiliary entity does not have the capacity to make an entity operational
which fails due to initial failure. So to prune the search set for obtaining a
solution we discard entities in (A0 ∪ B 0 ) as possible auxiliary entities.
– If an IDR D is modified then it is done by adding only one entity not in
A0 ∪ B 0 ∪ ED where ED is a set consisting of all entities (both on left and
right side of the equation) in D. For any IDR D ∈ F(A, B) we denote
AU X = (A ∪ B)/(A0 ∪ B 0 ∪ ED ) as the set of auxiliary entities that can be
used to modify D.
We quantify the number of modifications done as the number of IDRs to
which minterms are added as a disjunction. It should also be noted than an attacker only have information about the initial interdependent network I(A, B, F(A, B)).
Hence with a budget of |K| it attacks and kills the K most vulnerable entities to
maximize the number of entities killed due to induced failure. Any modification
in the IDR is assumed to be hidden from the attacker.
With these definitions the Auxiliary Entity Allocation Problem (AEAP) is
defined as follows. Let K be the most vulnerable entities (already provided as input) of an interdependent network I(A, B, F(A, B)). With a budget S in number
of modifications, the task is to find which are the S IDRs that are to be modified
and which entity should be used to perform this modification such that number
of entities failing due to induced failure is minimized. A more formal description
given below.
The Auxiliary Entity Allocation Problem (AEAP)
Instance — An interdependent network I(A, B, F(A, B)), K most vulnerable
entities for a given integer |K| and two positive integers S and Pf .
Decision Version — Does there exist S IDR auxiliary entity tuple (D, xi )
such that when each IDRs D ∈ F(A, B) is modified by adding auxiliary entity
xi ∈ AU X as a disjunction it would protect at least Pf entities from induced
failure with K vulnerable entities failing initially.

3

Computational Complexity Analysis

In this section we analyze the computational complexity of the AEAP problem.
The computational complexity of the problem depends on nature of the IDRs.
The problem is first solved by restricting the IDRs to have one minterm of size
1. For this special case a polynomial time algorithm exists for the problem. With
IDRs in general form the problem is proved to be NP-complete.
3.1

Special Case: Problem Instance with One Minterm of Size One

The special case consist of IDRs which have a single minterm of size 1 and each
entity appearing exactly once on the right hand side of the IDR. With entities
ai ’s and bj ’s belonging to network A(B) and B(A) respectively, the IDRs can
be represented as ai ← bj . The AEAP problem can be solved in polynomial
time for this case. We first define Auxiliary Entity Protection Set and use it to
provide a polynomial time heuristic in Algorithm 1. The proof of optimality is
not included due to space constraint.

Definition 1. Auxiliary Entity Protection Set: With a given set of K most
vulnerable entities failing initially the Auxiliary Entity Protection Set is defined
as the number of entities protected from induced failure when an auxiliary entity xi is added as a disjunction to an IDR D ∈ F(A, B). It is denoted as
AP (D, xi |K).
Algorithm 1: Algorithm solving AEAP problem for IDRs with minterms
of size 1

1
2

3
4
5

6
7
8
9
10
11

3.2

Data: An interdependent network I(A, B, F(A, B)) and set of K vulnerable
entities
Result: A set Dsol consisting of IDR auxiliary entity doubles (D, xi ) (with
|Dsol | = S and Pf (denoting the entities protected from induced
failure)
begin
For each IDR D ∈ F (A, B) and each entity xi ∈ AU X (where
AU X = A ∪ B/(A0 ∪ B 0 ∪ ED ) as discussed in the previous section)
compute the Auxiliary Entity Protection Set AP (D, xi |K) ;
Initialize Dsol = ∅ and Pf = ∅;
while S 6= 0 do
Choose the Auxiliary Entity Protection Set with highest AP (xi , D|K).
In case of tie break arbitrarily. Let Dcur be the corresponding IDR and
xcur the auxiliary entity;
Update Dsol = Dsol ∪ (Dcur , xcur ) and add auxiliary entity xcur as a
disjunction to the IDR Dcur ;
Update Pf = Pf ∪ AP (Dcur , xcur |K);
for ∀ IDR D0 ∈ F (A, B) and xi ∈ AU X of D0 do
Update AP (D0 , xi |K) = AP (D0 , xi |K)\AP (Dcur , xcur |K);
S ← S − 1;
return Dsol and Pf ;

General Case: Problem Instance with an Arbitrary Number of
Minterms of Arbitrary Size

The IDRs in general are composed of disjunctions of entities in conjunctive form
i.e. arbitrary number of minterms of arbitrary size. This case can be represented
Pp
Qjk1
as ai ← k1 =1 j=1
bj where entities ai and b0j s belong to network A(B) and
B(A) respectively. The given example has p minterms each of size jk1 . In Theorem 1 we prove that the decision version of the AEAP problem for general case
is NP complete.
Theorem 1. The decision version of the AEAP problem for Case IV is NPcomplete.
Proof. The hardness is proved by a reduction from Set Cover problem. An instance of a set cover problem consists of a universe U = {x1 , x2 , ..., xn } of elements and set of subsets S = {S1 , S2 , ..., Sm } where each element Si ∈ S is

a subset of U . Given an integer X the set cover problem finds whether there
are ≤ X elements in S whose union is equal to U . From an instance of the
set cover problem we create an instance of the AEAP problem. For each subset
Si we create an entity bi and add it to set B. For each element xj in U we
add an entity aj to a set A1 . We have a set A2 of entities where |A2 | = |B|. Let
A2 = {a21 , a22 , ..., a2|B| } where there is an association between entity bj and a2j .
Additionally we have a set of entities A3 with |A3 | = X which does not have
any dependency relation of its own. The set A is comprised of A1 ∪ A2 ∪ A3 . The
IDRs are created as follows. For an element xi that appears in subsets Sx , Sy , Sz ,
an IDR ai ← bx + by + bz is created. For each entity bj ∈ B an IDR bj ← a2j is
added to F(A, B). The cardinality of K most vulnerable node is set to |A2 | and
it directly follows that K = A2 comprises the set of most vulnerable entities. The
value of S (number of IDR modifications) is set to X and Pf is set to S + |A1 |.
Let there exist a solution to the set cover problem. Then there exist at least
X subsets whose union covers the set U . For each subset Sk which is in the
solution of the set cover problem we choose the corresponding entity bk . Let B 0
be all such entities. We arbitrarily choose and add an entity from A3 to each IDR
bk ← a2k with bk ∈ B 0 to form S = X distinct IDR auxiliary entity doubles. As
A3 type entities does not have any dependency relation thus all the entities in
B that correspond to the subsets in the solution will be protected from failure.
Additionally protecting these B type entities would ensure all entities in A1
does not fail as well (as there exists at least one B type entity in the IDR of A1
type entities which is operational). Hence a total of X + |A1 | are protected from
failure.
Similarly let there exist a solution to the AEAP problem. It can be checked
easily that no entities in B ∪ A1 ∪ A2 has the ability to protect additional entities
using IDR modification. Hence set A3 can only be used as auxiliary entities. An
entity from A3 for the created instance can be added to an IDR of A1 type entity
or B type entity. In the former strategy only one entity is protected from failure
whereas two entities are operational when we add auxiliary entity to IDRs of
B type entities. Hence all the auxiliary entities are added to the B type IDRs
with a final protection of X + |A1 | entities. For each IDR of the B type entity to
which the auxiliary entity is added, the corresponding subset in S is chosen. The
union of these subsets would result in U as the solution of the AEAP problem
protects the failure of all A1 type entities. Hence solving the set cover problem
and proving the hardness stated in Theorem 1.

4

Solutions to the AEAP Problem

We consider the following restricted case where there exists at least S entities in
the interdependent network which does not belong to any of the failing entities.
This comprise the set of auxiliary entities that can be used. It is also imperative
to use such set as auxiliary entities because they never fail from induced or
initial failure when the K most vulnerable entities fail initially. The problem still
remains to be NP compete for this case as in Theorem 1 the set of entities A3
belong to such class of auxiliary entities. With these definition of the special
case let A denote a set of such auxiliary entities which can be used for IDR

modifications with A ⊂ A ∪ B/(A00 ∪ B 00 ) (where A00 ∪ B 00 are the entities that
fails due to failing entities A0 cupB 0 initially). Hence we loose the notion of
IDR auxiliary entity doubles in the solution as any auxiliary entity from set A
would produce the same protection effect. Let A denote all such entities that
can be used as auxiliary entities as defined above. We additionally assume that
|A| ≥ S, i.e., there are enough auxiliary entities to suffice the AEAP budget S.
So in both the solutions we only consider the IDRs that needs to be modified and
disregard which auxiliary entity is used for this modification. We first propose an
Integer Linear Program (ILP) to obtain the optimal solution in this setting. We
later provide a polynomial heuristic solution to the problem. The performance
of heuristic with respect to the ILP is compared in the section to follow.
4.1

Optimal solution to AEAP problem

We first define the variables used in formulating the ILP. Two set of variables
G = {g1 , g2 , ..., gc } and H = {h1 , h2 , ..., hd } (with c = |A| and d = |B|) are
used to maintain the solution of K most vulnerable entities. Any variable gi ∈ G
(hj ∈ H) is equal to 1 if ai ∈ A (bj ∈ B) belongs to K and is 0 otherwise.
For each entity ai and bj a set of variables xid and yjd are introduced with
0 ≤ d ≤ |A| + |B| − 1. xid (yid ) is set to 1 if the entity ai (bj ) is non operational
at time step d and is 0 otherwise. Let P denote the total number of IDRs in
the interdependent network and assume each IDR has a unique label between
numbers from 1 to P . A set of variables M = {m1 , m2 , ..., mP } are introduced.
The value of mi is set to 1 if an auxiliary node is added as a disjunction to the
IDR labeled i and 0 otherwise. With these definitions we define the objective
function and the set of constraints in the ILP.
|A|
|B|

X
X
xi(|A|+|B|−1) +
yj(|A|+|B|−1)
(1)
min
i=1

j=1

The objective function defined in 1 tries to minimize the number of entities
having value 1 at the end of the cascade i.e. time step |A| + |B| − 1. Explicitly
this objective minimizes the number of entities failed due to induced failure.
The constraints that are imposed on these objective to capture the definition of
AEAP are listed below —
Constraint Set 1: xi0 ≥ gi and yj0 ≥ hj . This imposes the criteria that if entity
ai (bj ) belongs to the K most vulnerable entity set then the corresponding variable xi0 (yj0 ) is set to 1 capturing the initial failure.
Constraint Set 2: xid ≥ xi(d−1) , ∀d, 1 ≤ d ≤ |A|+|B|−1, and yid ≥ yi(d−1) , ∀d, 1 ≤
d ≤ |A|+|B|−1,. This ensures that the variable corresponding to an entity which
fails at time step t would have value 1 for all d ≥ t.
Constraint Set 3: We use the theory developed in [9] to generate constraints
to represent the cascade through the set of IDRs. To describe this consider an
IDR ai ← bj bp bl +bm bn +bq in the interdependent network. Assuming the IDR is
labeled v it is reformulated as ai ← bj bp bl + bm bn + bq + mv with mv ∈ M . This
is done for all IDRs. The constraint formulation is described in the following
steps.

Step 1: All minterms of size greater than 1 are replaced with a single virtual
entity. In this example we introduce two virtual entities C1 and C2 (C1 , C2 ∈
/
A∪B) capturing the IDRs C1 ← bj bp bl and C2 ← bm bn . The IDR in the example
can be then transformed as ai ← C1 + C2 + bq + mv . For any such virtual entity
Ck a set of variables ckd are added with ckd = 1 if Ck is alive at time step d
and 0 otherwise. Hence all the IDRs are represented as disjunction(s) of single
entities. Similarly all virtual entities have IDRs which are conjunction of single
entities.
Step 2: For a given virtual entity Ck and all entities having a single midterm
of arbitrary size, we add constraints to capture the cascade propagation. Let
N denote the number of entities in the IDR of Ck . The constraints added is
described through the example stated above. The variable c1 with IDR C1 ←
y
+y
+yl(d−1)
bj bp bl , constraints c1d ≥ j(d−1) p(d−1)
and c1d ≤ yj(d−1) + yp(d−1) +
N
yl(d−1) ∀d, 1 ≤ d ≤ m + n − 1 are added (with N = 3 in this case). This ensures
that if any entity in the conjunction fails the corresponding virtual entity fails
as well.
Step 3: In the transformed IDRs described in step 1 let n denote the number of entities in disjunction for any given IDR (without modification). In the
given example with IDR ai ← C1 + C2 + bq + mv , constraints of form xid ≥
c
+c
+yq(d−1) +mv
c1(d−1) +c2(d−1) +yq(d−1) +mv −(n−1) and xid ≤ 1(d−1) 2(d−1)
∀d, 1 ≤
n
d ≤ m + n − 1 are added. This ensures that the entity ai will fail only if all the
entities in disjunction become non operational.
Constraint Set 4: To ensure that only S auxiliary entities are added as disPP
junction to the IDRs constraint v=1 mv = S is introduced.
4.2

Heuristic solution to the AEAP problem

In this section we provide a polynomial heuristic solution to the AEAP problem.
We first redenote Auxiliary Entity Protection Set as AP (D|K) as it is immaterial
which entity is added as an auxiliary entity since no auxiliary entity can fail due
to any kind of failure. Along with the definition of Auxiliary Entity Protection
Set, we define Auxiliary Cumulative Fractional Minterm Hit Value (ACFMHV)
for designing the the heuristic. We first define Auxiliary Fractional Minterm
Hit Value (AFMHV) in Definition 2 which is used in defining ACFMHV (in
Definition 3).
Definition 2. The Auxiliary Fractional Minterm Hit Value of an IDR D ∈
F(A,
is denoted by AF M HV (D|K). It is calculated as AF M HV (D|K) =
Pm B)
1
.
i=1 |si | Let xj denote the entity in the right hand side of the IDR D and
m denotes all the minterms in which the entity xj appears over all IDRs. The
parameter si denotes ith such minterm with |si | being its size. If an auxiliary
entity is placed at D then the value computed above provides an estimate implicit
impact on protection of other non operational entities.
Definition 3. The Auxiliary Cumulative Fractional Minterm Hit Value of an
IDR D ∈ F(A, B) is denoted by ACF M HV (D). It is computed as ACF M HV (D) =

P

∀xi ∈AP (D|K) AF M HV (Dxi |K) where Dxi is the IDR for entity xi ∈ AP (D|K).
The impact produced by the protected entities when IDR D is allocated with an
auxiliary entity over set A ∪ B is implicitly provided by this definition.

Algorithm 2: Heuristic solution to the AEAP problem

1

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

21

Data: An interdependent network I(A, B, F(A, B)), set of K vulnerable
entities, set A of auxiliary entities and budget S
. Result: A set Dsol consisting of IDRs (with |Dsol | = S to each of which an
auxiliary entity is added as a disjunction and Pf (denoting the entities
protected from induced failure)
begin
Initialize Dsol = ∅ and Pf = ∅;
while S 6= 0 do
For each IDR D ∈ F (A, B) compute the Auxiliary Node Protection Set
AP (D|K) ;
if There exists multiple IDRs having same value of highest cardinality of
the set AP (D|K) then
For each IDR D ∈ F (A, B) compute the Auxiliary Cumulative
Fractional Minterm Hit Value ACF M HV (D) ;
Let Dp be an IDR having highest ACF M HV (Dp ) among all Di ’s in
the set of IDRs having highest cardinality of the set AP (Di |K);
If there is a tie choose arbitrarily;
Update Dsol = Dsol ∪ Dp and add an auxiliary entity from A as a
disjunction to the IDR Dp ;
Update Pf = Pf ∪ AP (Dp );
Update A by removing the auxiliary entity added ;
S ← S − 1;
else
Let Dp be an IDR having highest cardinality of the set D ∈ F(A, B);
Update Dsol = Dsol ∪ Dp and add an auxiliary entity from A as a
disjunction to the IDR Dp ;
Update Pf = Pf ∪ AP (Dp |K);
Update A by removing the auxilary entity added ;
S ← S − 1;
Prune the interdependent network I(A, B, F(A, B) by removing the
IDRs for entities in AP (Dp |K) and removing the same set of entities
from A ∪ B ;
return Dsol and Pf ;

The heuristic is provided in Algorithm 2. At any given iteration the auxiliary
entity is placed at the IDR which protects the most number of entities from
failure. In case of a tie the entity having highest ACFMHV value is chosen. At any
given iteration the algorithm greedily maximize the number of entities protected
from induced failure. Algorithm 2 runs in polynomial time, more specifically the
time complexity is O(Sn(n + m)2 ) (where n = |A| + |B| and m = Number of
minterms in F(A, B)).

(a) Region 1

(b) Region 2

(c) Region 3

(d) Region 4

Fig. 1: Comparison of the number of entities protected in optimal solution (ILP)
and heuristic in each of the 5 identified regions with |K| = 8 and number of
auxiliary entities (or modifications) varied as 1, 3, 5, 7

5

Experimental Results

The solution of the heuristic is compared with the ILP to judge its efficacy. We
perform the experiments on real world data sets with the IDRs generated artificially based on some predefined rules. Platts (www.platss.com) and GeoTel
(www.geo-tel.com) provided the power and communication network data respectively. The power network data consisted of two types of entity — 70 power plants
and 470 transmission lines. There are three types of entity in the communication network data — 2,690 cell towers, 7,100 fiber-lit buildings and 42,723 fiber
links. The data corresponds to the Maricopa county region of Arizona, USA.
To perform the experimental analysis we picked four non overlapping regions
in Maricopa county. They are labelled as Region 1, 2, 3, and 4 respectively. It
is to be noted that the union of these regions does not cover the entry county.
For each region we filtered out the entities from our dataset and constructed the
IDRs based on rules defined in [9].
The cardinality of K most vulnerable nodes was set to 8 and was calculated
using the ILP described in [9]. The number nodes failed in each region due to
initial failure of the most vulnerable nodes are 28, 23, 28, 28 respectively. We vary
the number of auxiliary entities placed (or modifications) from 1 to 7 in steps
of 2. For each region and modification budget the number of entities protected
from failure for the heuristic was compared with the ILP solution and is plotted
in Figure 1. The maximum possible percentage difference of the heuristic from
optimal for any region and modification budget pair is observed to be a 11.76%
in Region 3 with 5 auxiliary entities (Figure 1c). On an average the heuristic
performed very near to the optimal with a difference of 6.75%.

6

Conclusion

In this paper we introduce the auxiliary entity allocation problem in multilayer
interdependent network using the IIM model. Entities in multilayer network can
be protected from an initial failure event when auxiliary entities are used to
modify the IDRs. With a budget on the number of modifications, the problem
is proved to be NP-complete. We provide an optimal solution using ILP and
polynomial heuristic for a restricted case of the problem. The optimal solution
was compared with the heuristic on real world data sets and on an average
deviates 6.75% from the optimal.

References
1. Andersson, G., Donalek, P., Farmer, R., Hatziargyriou, N., Kamwa, I., Kundur,
P., Martins, N., Paserba, J., Pourbeik, P., Sanchez-Gasca, J., et al.: Causes of the
2003 major grid blackouts in north america and europe, and recommended means
to improve system dynamic performance. Power Systems, IEEE Transactions on
20(4), 1922–1928 (2005)
2. Banerjee, J., Das, A., Zhou, C., Mazumder, A., Sen, A.: On the entity hardening
problem in multi-layered interdependent networks. In: Computer Communications
Workshops (INFOCOM WKSHPS), 2015 IEEE Conference on. pp. 648–653. IEEE
(2015)
3. Bernstein, A., Bienstock, D., Hay, D., Uzunoglu, M., Zussman, G.: Power grid
vulnerability to geographically correlated failures-analysis and control implications.
arXiv preprint arXiv:1206.1099 (2012)
4. Buldyrev, S.V., Parshani, R., Paul, G., Stanley, H.E., Havlin, S.: Catastrophic
cascade of failures in interdependent networks. Nature 464(7291), 1025–1028 (2010)
5. Gao, J., Buldyrev, S.V., Stanley, H.E., Havlin, S.: Networks formed from interdependent networks. Nature Physics 8(1), 40–48 (2011)
6. Nguyen, D.T., Shen, Y., Thai, M.T.: Detecting critical nodes in interdependent
power networks for vulnerability assessment (2013)
7. Parandehgheibi, M., Modiano, E.: Robustness of interdependent networks:
The case of communication networks and the power grid. arXiv preprint
arXiv:1304.0356 (2013)
8. Rosato, V., Issacharoff, L., Tiriticco, F., Meloni, S., Porcellinis, S., Setola, R.:
Modelling interdependent infrastructures using interacting dynamical models. International Journal of Critical Infrastructures 4(1), 63–79 (2008)
9. Sen, A., Mazumder, A., Banerjee, J., Das, A., Compton, R.: Identification of k most
vulnerable nodes in multi-layered network using a new model of interdependency.
In: Computer Communications Workshops (INFOCOM WKSHPS), 2014 IEEE
Conference on. pp. 831–836. IEEE (2014)
10. Shao, J., Buldyrev, S.V., Havlin, S., Stanley, H.E.: Cascade of failures in coupled
network systems with multiple support-dependence relations. Physical Review E
83(3), 036116 (2011)
11. Tang, Y., Bu, G., Yi, J.: Analysis and lessons of the blackout in indian power
grid on july 30 and 31, 2012. In: Zhongguo Dianji Gongcheng Xuebao(Proceedings
of the Chinese Society of Electrical Engineering). vol. 32, pp. 167–174. Chinese
Society for Electrical Engineering (2012)
12. Zhang, P., Peeta, S., Friesz, T.: Dynamic game theoretic model of multi-layer
infrastructure networks. Networks and Spatial Economics 5(2), 147–178 (2005)

The 2011 Military Communications Conference - Track 5 - Communications and Network Systems

On Connectivity of Airborne Networks in Presence
of Region-based Faults
Shahrzad Shirazipourazad, Pavel Ghosh and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Email: {sshiraz1, pavel.ghosh, asen }@asu.edu
Abstract—The U.S. Air Force is currently in the process of
building an Airborne Network (AN), where the nodes are a set
of heterogeneous, highly mobile, Airborne Networking Platforms
(ANPs) - such as satellites, airplanes and unmanned aerial
vehicles. Mobility pattern of nodes in a mobile network has
significant impact on the coverage and connectivity properties
of the network. The level of reliability needed for continuous
operation of an AN may be difficult to achieve through a
completely infrastructure-less mobile ad hoc networks. In an
earlier paper, we proposed an architecture for an AN where
a set of ANPs form the backbone of the AN. In this architecture,
the ANPs may be viewed as mobile base stations with predictable
and well-structured flight paths and the combat aircrafts on a
mission as mobile clients. In this paper we consider the AN
scenario where a part of the network might not be operational
due to enemy attack and/or jamming. We consider faults that
are spatially correlated (or region-based), that is faults due to an
enemy attack are confined to a region. The goal is to design a
robust AN so that no matter which region in the deployment area
fails and at what time, the surviving nodes of the network will
remain connected and be able to communicate with each other.
We propose an algorithm that finds the minimum transmission
range necessary to ensure network connectivity irrespective of
location of the fault region and the time of the failure.

I. I NTRODUCTION
Due to the Joint Aerial Layer Networking (JALN) activities of the U.S. Air Force, design of a robust and resilient
Airborne Network (AN) has received considerable attention
in the networking research community in recent years [1]–[4].
Mobility pattern of nodes in a mobile network has significant
impact on the connectivity of the network. It is increasingly
being recognized in this community that the level of reliability
needed for continuous operation of an AN may be difficult
to achieve through a completely mobile, infrastructure-less
network [2], [5]. In order to enhance reliability and scalability
of an AN, Milner et al. in [2] suggested the formation of
a backbone network with Airborne Networking Platforms
(ANPs). Similar conclusion about unreliabilty of a completely
mobile, infrastructure-less network was independently arrived
by the Boeing engineers engaged in development of an AN
[5]. Our own efforts in video streaming over a mobile ad-hoc
This research is supported in part by a grant from the U.S. Defense Threat
Reduction Agency under grant number HDTRA1-09-1-0032 and by a grant
from the U.S. Air Force Office of Scientific Research under grant number
FA9550-09-1-0120.

978-1-4673-0081-0/11/$26.00 ©2011 IEEE

network formed by a group of soldiers engaged in combat in
an urban environment also led us to the same conclusion.
In order to deal with the reliability and scalability issues in
an AN, in an earlier paper [6], we proposed an architecture
for an AN where a set of ANPs form the backbone of the AN.
This set of ANPs may be viewed as mobile base stations with
predictable and well-structured flight paths and the combat
aircrafts on a mission as mobile clients. In [6] we presented
techniques to compute the minimum transmission range of
the ANPs to ensure that the dynamic network formed by the
movement of the ANPs remains connected at all times.
The results presented in [6] are valid as long as all the
network nodes (i.e., the ANPs) are operational. However, the
ANPs are vulnerable to Electromagnetic Pulse (EMP) attacks
or jamming. Such an attack will impact specific geographic
regions at specific times and if an ANP is within the fault
region during the time of attack, it will not be able to carry
out its normal communication functions. We will refer to
these ANPs as faulty nodes of the network. In this paper,
we consider the AN scenario where some of the network
nodes are faulty. We consider faulty nodes are spatially
correlated (or region-based), that is faulty nodes due to an
attack are confined to a region. In recent times, there has
been considerable interest in studying localized, i.e., spatially
correlated or region-based faults in networks [7]–[10]. The
networks studied in [7]–[10] are all static. However, ANs
under study in this paper are dynamic. We want this dynamic
network to remain connected irrespective of location of the
fault region and the time of failure. Network connectivity can
be easily achieved if the transmission range of the ANPs is
very large. However large transmission range also implies high
power consumption. In order to minimize power consumption
and hence extend network lifetime, we would like to find the
smallest transmission range to ensure network conncetivity.
Santi in [11] has studied the minimum transmission range
required to ensure network connectivity in mobile ad hoc
networks. The networks studied in [11] is infrastructure-less
and no failure is considered. We define critcal transmission
range (CTR) to be the smallest transmission range necessary
to ensure network connectivity, irrespective of (a) the location
of the fault region and (b) the time of the failure. We would
like to find CTR. As a part of design of this algorithm, we
develop techniques to (i) compute the dynamic topology of the

1997

AN at any instance of time, (ii) compute all the fault regions
that need to be considered to ensure overall connectivity at all
times, (iii) compute the set of dynamic nodes that might be
affected by the failure of a specific region at a specific time,
and finally, (iv) compute CTR.
The rest of the paper is organized as follows: In section II,
we describe the system architecture and the fault model. Also,
we describe the challenges that have to be overcome in order to
determine CTR. In section III, we propose our apparoaches to
overcome these challenges. Section IV presents the simulation
results and section V concludes the paper.

•
•
•
•
•

a set of points {c1 , c2 , . . . , cn } on a two dimensional
plane (representing the centers of circular flight paths),
a set of radii {r1 , r2 , . . . , rn } representing the radii of
circular flight paths,
a set of points {p1 , p2 , . . . , pn } representing the initial
locations of the platforms
a set of velocities {v1 , v2 , . . . , vn } representing the
speeds of the platforms, and
Radius of a region, R.

II. S YSTEM A RCHITECTURE AND FAULT M ODEL
In this section, we describe the system architecture and the
fault model used in this paper. Also, we identify the challenges
that one has to confront, in order to find the CTR.
A. System Architecture
In the previous section, we argued that the level of reliability
needed for continuous operation of an AN may be difficult
to achieve through a completely mobile, infrastructure-less
network and wherever possible a backbone network with
Airborne Networking Platforms (ANPs) should be formed to
enhance reliability. In order to achieve this goal, we propose
an architecture of an AN where a set of ANPs form a backbone network and provide reliable communication services to
combat aircraft on a mission. In this architecture, the nodes
of the backbone networks (ANPs) may be viewed as mobile
base stations with predictable and well-structured flight paths
and the combat aircrafts on a mission as mobile clients. A
schematic diagram of this architecture is shown in Fig. 1.
In the diagram, the black aircrafts are the ANPs forming
the backbone of the AN. The circular flight paths of the
ANPs and their coverage area (shaded spheres with ANPs
at the center) are also shown in Fig. 1. Thick dashed lines
indicate the communication links between the ANPs. The
diagram shows that the ANPs follow a circular flight path.
However, as long as they have a predictable flight path, our
analysis technique can be utilized. For simplicity of analysis,
we make two assumptions. We assume that (i) all ANPs are
flying at the same altitude and (ii) they follow a circular flight
path. The first assumption allows us to reduce the problem
from three dimension to two. However, none of these two
assumptions are critical and our analysis technique can easily
be extended to scenarios where the ANPs are not flying at
the same altitude and they are not following a circular flight
path. As a consequence of assumption (i), we can view the n
backbone nodes (ANPs) as moving points on a 2 dimensional
plane. Let (xi (t), yi (t)) be the coordinates of the node i at
time t. The network of flying ANPs gives rise to a dynamic
graph G(t) = (V, E(t)) where V = {1, 2, . . . , n} is the set
of nodes indexed by the ANPs and E(t) is the set of edges at
time t. There is an edge between two nodes if their Euclidean
distance, dij is less than the transmission range tr at time t,
i.e., E(t) = {(i, j)|dij (t) < tr}. It may be noted that the
dynamic graph G(t) = (V, E(t)) is completely defined by the
following five parameters.

Fig. 1.

A schematic view of an Airborne Network

B. Fault Model
In the previous section we indicated that our focus is on spatially correlated (or region-based) faults. Spatially correlated
or region-based faults imply that the faulty nodes due to an
attack are confined to a geographic area. In a two dimensional
deployment area, a region can be viewed as a circular area
with radius R (in three dimensional space it can be viewed as
a sphere with radius R). In our model, when a region is under
attack and consequently fails at time t, some or all the ANPs
within that region at time t also fail. In this version of the
model, we also make an assumption that only one region can
fail at any one time. Fig. 2 shows five ANPs moving on a two
dimensional plane and a faulty region (red circle, centered at
point P ) at time t. Since ANPs 4 and 5 are within the fault
region at time t, we assume that these nodes are damaged and
no longer can be viewed as part of the backbone network. It
may be noted that both the location of the center of the fault
circle, P , as well as the time of attack, t, play a critical role in
determining the impact of the attack on the backbone network.
ANP2
ANP1 c1

c2

c5
ANP3

ANP5

P

ANP4
c3

Fig. 2.

c4

ANPs on a circular flight path on a 2D-plane with a fault region

C. Design Challenges
One can easily recognize the complexity of the problem by
noting that potentially there could be an infinite number of

1998

locations for point P and infinite choices for attack time t. In
our analysis we show that although there could be an infinite
number of choices of P and t, we need to consider only a
small subset of them to correctly determine CTR. The tasks
that need to be performed before a solution to the problem is
found can be listed as follows:
• Computation and comprehension of the dynamic topology of the backbone network (in a fault-free scenario) as
it changes with movements of the ANPs.
• How many regions (locations of point P ) and instances
of attack time t should be considered?
• How to determine the ANPs that are damaged when an
attack takes place in location P at time t?
In the following section we describe our techniques to deal
with these challenges and to compute CTR.

Ri (t)Rj (t) cos(✓i (t) ✓j (t)) = rci rcj cos ↵ci cj
+ri rj cos( ij + (!i !j )t) + rci rj cos(↵ci
+rcj ri cos(↵cj
!i t)
i

where ↵cij = ↵ci ↵cj and ij =
(1) with eq. (3) and (4), we have:
s2ij (t) = rc2i + ri2 + 2rci ri cos(
+rc2j + rj2 + 2rcj rj cos(

Active Intervals of Link3

~ i (t) · R
~ j (t) (1)
2R

i

~ i (t) = ~rc + ~ri (t)
R
i

(2)

where ~ri (t) = (ri cos ( i + !i t), ri sin ( i + !i t)) So, the
angle between ~ri (t) and ~rci is ( i ↵ci + !i t). Hence,
i

Inactive Intervals of Link3

Link 2

As mentioned earlier, we have assumed that the link between
the nodes i and j is alive (or active) when sij (t)  tr. In
Fig. 3(a), the vectors from the origin O to the centers of
the orbits ci and cj are given as r~ci and r~cj . The cartesian
coordinates of the centers can be readily obtained as r~ci =
(rci cos↵ ci , rci sin↵ ci ) and r~cj = (rcj cos↵ cj , rcj sin↵ cj ).
~ i (t) can be expressed in polar coordinates:
Accordingly, R
(Ri (t), ✓i (t)) with respect to origin point O, as shown in
Fig. 3(a), and similarly for R~j (t). The initial location of
~ i (0) and R~j (0) are given. From Fig. 3(b), the
the points R
phase angle i for node i with respect to the center of orbit
ci , can be calculated as (by taking projection on the axes):
R (0)cos✓ (0) r cos↵
tan i = Rii(0)sin✓ ii (0) rcci sin↵ cci . From Fig. 3(a),

Ri2 (t) = rc2i + ri2 + 2rci ri cos (

!j )t) +
!i t))
(5)

Link 3

In this section, we describe how we can compute the
topology of the dynamic backbone network, G(t), formed by
the ANPs when transmission range is tr.
Suppose that two ANPs, represented by two points i and j
are moving along two circular orbits with centers at ci and cj
with orbit radii ri and rj as shown in Fig. 3(a) with velocities
vi and vj (with corresponding angular velocities !i and !j ),
respectively. A moving node i is specified by the radius vector
~ i (t) directed from some origin point O, and similarly R~j (t)
R
for point j. Therefore the distance sij (t) between the nodes
i j at time t is given by:

i

↵ci + !i t)
↵cj + !j t)

In eq. (5), all parameters on the right hand side are known
from the initial state of the system, and thus the distance sij (t)
between the nodes i j at any time t can be obtained.

In this section, we propose our approach to the solution of
the design challenges posed in the previous section.
A. Dynamic Topology Computation

~ j (t))2 = Ri2 (t) + Rj2 (t)
R

!j t)
(4)

Combining eq.

2(rci rcj cos ↵ci cj + ri rj cos( ij + (!i
rci rj cos(↵ci
!j t) + rcj ri cos(↵cj
j
i

III. S YSTEM D ESIGN

~ i (t)
s2ij (t) = (R

i
j

j.

i

j

↵ci + !i t)

(3)

~ i (t) = ~rc + ~ri (t) on the x and
Now taking the projection of R
i
y axes, we get Ri (t) cos ✓i (t) = rci cos ↵ci + ri cos ( i + !i t)
and Ri (t) sin ✓i (t) = rci sin ↵ci + ri sin( i + !i t). Recalling
cos(A B) = cos A cos B + sin A sin B, we get

Link 1

l1

Fig. 4.

l2 l3

l4 l5 l6

l7 l8 l9

l10

l11 l12

Timeline

Active/Inactive time intervals of each link (link-lifetime timeline)

Using eq. (5) we can compute the intervals where sij 
tr. From this comparison, we can identify exactly the time
intervals during which the link between the nodes i and j is
alive (active) and when it is dead (inactive). By repeating this
test (sij  tr) for every pair of nodes i and j, we can find out
the lifetime of every link in the network. This is shown in Fig.
4. We refer to the event of a link coming alive as birth and
the event of a link dying as death. Let L(tr) = {l1 , l2 , . . . , lt }
denote the ordered instances of times where a birth/death event
has taken place for at least one link (see Fig. 4). The set
L(tr) of points (instances of times) on the timeline divides the
timeline into intervals [lj , lj+1 ], so that the number of active
links during that interval remains unchanged. Fig. 4 shows an
example of link-lifetime timeline.
Alg. 1 describes the computation of link lifetime timeline
in details. Let n be the number of ANPs. The first loop is
executed for O(n2 ) times. The number of iterations of the
inner loop depends on the number of the solutions of sij (t) =
tr. For the case that ANPs move at the same velocity, i.e.,
!i = !j = ! it is obvious that eq. (5) is periodic and its period
is 2⇡/!. So, it is enough to execute Alg. 1 for one period. In
this
p case, eq. (5) can be written as A cos(!t) + B sin(!t) =
A2 + B 2 sin( + !t) where A, B and are constants and
can easily be obtained from eq. (5). In this case, the equation
sij (t) = tr can have at most two solutions and the solutions
can be found in constant time. Therefore, for every link, the
timeline is divided into at most three segments in one period
and the size of the set of intervals, |L(tr)| is O(n2 ); also,
the time complexity of the binary search is O(log n2 ). So, the
total time complexity is O(n2 log n).
B. Regions to Examine
The authors in [7] introduced the notion of region-based
faults and introduced a new metric, region-based conenctivity,
to measure the fault-tolerance capability of a network under

1999

y

ci
•

r~i (t)

y

•
i

s~ij (t)

~ i (t)
R

r~ci

i(0)
•

r~j (t)

✓i (t)

~ i (0)
R

•
j

r~cj
~j (t)
R

↵ ci
↵ cj

cj
•

Iij1

↵ ci
x

cj

Iij2

x

O

Algorithm 1 Link Lifetime Computation
of a link changes from active to inactive or inactive to active.
L(tr)
;
for all pairs i, j
l
Find the values of t such that sij (t) = tr (eq. 5) over a
period of time, to find the instances of times t where the state
of the link (i, j) changes. If sij (t) = tr and sij (t) is increasing
at t, it implies that the link
dies at t, and if sij (t) decreasing
1
at t, it implies that the link comes to life at t.
for all lk 2 l
Find the position of lk in L(tr) using binary search
Add lk into L(tr). (L(tr) is sorted in increasing order)

1

the region-based fault model. Region-based connectivity of
a network is defined to be the minimum number of nodes
that has to fail in any region of the network before it is
disconnected. In this study, a region is defined to be circle
of radius R. With this definition of a region, the number of
potential regions could be infinite. The authors in [7] proved
that in a static wireless network, only a limited number of
distinct regions need to be examined to compute the regionbased connectivity. They showed that it is enough to consider
the regions centered at the intersection points of the circles
centered at the nodes with radius R. Although the AN is
dynamic, if we take a snapshot of the network at some instance
of time t, the AN can be viewed as a static network with a
specific topology and nodes in specific locations on the plane.
The vulnerability zone of an ANPi , V Zi (t), is defined to be
a circular region centered at the location of ANPi at time
t with radius R. The motivation for this definition of the
vulnerability zone of ANPi is the following. If the center of
the fault region is within the vulnerability zone of ANPi , then
the ANPi is likely to be damaged. The vulnerability zones of
ANPs are shown in Fig. 3(c). Since there is no discernible
difference between a static sensor network considered in [7]
and a snapshot of an AN at a specific instance of time t,
using the analysis presented in [7], we can conclude that it is
enough to examine only the regions centered at the intersection
points (I-points) of the vulnerability zones of the ANPs. The
vulnerability zones of two ANPs and their intersection points
are shown in Fig. 3(c). If a V Zi does not have intersection with

i

(c)
of point AN Pi at time 0; point is shown as

any other node’s vulnerability zone, an I-point is considered
at the location of the ANPi .

1: Input: The parameters difined in part II-A
2: Output: L(tr): A sorted list of instances of times when the state

6:
7:

VZj

i
VZi

✓i (0)

(a)
(b)
~i (t) and R~j (t)) of two points i and j at time t, (b)Initial phase angle
Fig. 3. (a) Vector representations (R
1 and I 2 are intersection points of V Z and V Z at time t.
i(0), (c) Iij
i
j
ij

3:
4:
5:

R
j

center of orbit
ci

r~ci

✓j (t)

O

ci
•

orbit

i

Since the ANPs are mobile, the location of the intersection
points of their vulnerability zones also changes with time.
Each pair of ANPs will have at most two intersection
points. Since there are only n(n
1)/2 pairs of nodes,
at most n(n
1) intersections points can exist at any
given time (it may be noted that depending on flight path
of a pair of ANPs, their vulnerability zones may never
intersect). We define a set of n(n 1) + n I-points, I =
1
2
1
2
1
2
, I(1,2)
, I(1,3)
, I(1,3)
, . . . , I(n
{I(1,2)
1,n) , I(n 1,n) , I1 , I2 , . . . , In },
1
2
where the I(i,j)
and I(i,j)
are the intersection points of the
vulnerability zones V Zi and V Zj . We will use the notation
1
2
1
(t) and I(i,j)
(t) to denote the locations of I(i,j)
and
I(i,j)
2
I(i,j) at time t. Similarly, Ii (t) will denote the location of
ANPi at time t. Based on the results presented in [7], it is
known that at any point of time t it is sufficient to examine
only the regions centered at the I-points in I. In the rest of
1
2
or I(i,j)
.
the paper we will use I(i,j) to denote both I(i,j)
For every two nodes i and j, V Zi (t) and V Zj (t) intersect
iff sij (t)  2R. In this case we say that the region centered
at intersection point I(i,j) exists at time t; otherwise, it does
not, i.e., there exists no region that can cover both nodes i
and j at time t. It may be noted that due to the mobility
of the ANPs, I(i,j) may exist at some point of time t and
may not exist at some other point of time t0 . By checking
the condition sij (t)  2R, we can determine the intervals on
the timeline when I(i,j) exists for each pair of nodes i and j;
i.e., we can compute existence intervals of each I-point on the
timeline. Let T (f ) = {(t1f , t2f ), . . . , (tkf 1 , tkf )} be the set of
existence intervals of I-point f 2 I where the first element
in every pair (tjf , tj+1
f ) is the start time and the second one
is the finish time of the j- th existence interval. If in a time
interval (tjIi , tj+1
Ii ), V Zi does not have intersection with any
other ANP’s vulnerability zone then a region centered at Ii
should be considered, i.e, (tjIi , tj+1
Ii ) 2 T (Ii ). Without loss
of generality we can assume that the region centered at the
point Ii exists all the time and it only covers node i. The
computation of the intervals on the timeline when I(i,j) exists
(or does not exist), for each pair of nodes i and j, can be
carried out by an algorithm similar to Alg. 1 presented earlier.
The only differences are (i) the value of t that satisfies the

2000

equation sij (t) = 2R should be computed instead of the value
of t that satisfies the equation sij (t) = tr, (ii) since there is
no need to combine existence interval information of one pair
of nodes (I(i,j) ) with another pair, the binary search in step 6
of Alg. 1 is not needed.
C. Computation of the Damaged ANPs in a Fault Region
After finding the existence intervals of I-points we want to
find the set of nodes that might be damaged by the failure of
a region centered at an I-point when it exists. A node might
be damaged by failure of a region if the Euclidean distance
between the center of the region and the node is less than
R. As explained in part III-B the regions centered at I-point
Ii 2 Is only can destroy node i. Since, we know the locations
of each pair of nodes i and j at time t, we can compute V Zi (t)
1
2
(t) and I(i,j)
(t), the intersection
and V Zj (t), and hence I(i,j)
points of V Zi (t) and V Zj (t) at time t.
Once the location of each intersection I(i,j) in its existence
intervals 2 T (I(i,j) ) are known, we can find the nodes
that might be damaged if the region centered at I(i,j) fails.
For ease of notation we denote the set of I-points 2 I as
F = {f1 , f2 , . . . , fl }. Dik (t) denotes the distance between Ipoint fi 2 F and node k at time t. For every I-point fi 2 F in
its existence interval 2 T (fi ), we find Dik (t) for all k 2 V .
Since we know the position of the nodes and I-points at any
point of time, Dik (t) can be computed easily. If Dik (t)  R,
then the node k may be damaged due to the region failure fi .
From this calculation, we can find out the time interval when
node k is vulnerable to a failure fi . In other words, we can
find out the time intervals when a node k is covered by the
region centered at fi (i.e., Dik (t)  R). It may be noted that
this time interval will be subinterval of the intersection points
existence time interval. Accordingly, every existence interval
(tjfi , tj+1
fi ) 2 T (fi ) is divided into a set of smaller subintervals
such that each of these intervals identify a specific set of
nodes that may be damaged if the region centered at fi fails.
Suppose that tm be the mth interval of T (fi ). We define a set
N T (fi , tm ) = {(tm1 , Nm1 ), (tm2 , Nm2 ), . . . , (tmj , Nmj )}as
the set of subintervals into which tm is divided, where tmj
denotes the start time of the jth subinterval of tm where
at least a node enters the region or leaves the region and
Nmj denotes the set of nodes within the region centered at
fi in its jth subinterval. We need to compute N T (fi , tm )
for every region fi 2 F and for all of its existence intervals. Based on N T (fi , tm ) we can draw a timeline, regioncoverage timeline for each region centered at an I-point
fi 2 F . Fig. 5 shows an example in which N T (f1 , t1 ) =
{(t11 , {1, 2}), (t12 , {1, 2, 3}), (t13 , {1, 2})}.
N12 =N21={1,2,3}

N23={1,2,4}

ANP4
ANP3
ANP2
ANP1

f1=I(1,2)

t2

t1
t11

t12

t13

t14

t21 t22 t23

Timeline
t24

Fig. 5. Region coverage timeline of the region centered at f1 = I(1,2) ; The
first timeline shows the availability intervals of f1 ; i.e, T (f1 ) = {t1 , t2 }.

D. Computation of Critical Transmission Range (CTR)
In this section we propose an algorithm to find CTR.
The transmission range tr is one of the parameters that
determines the number of active links at any given time.
Similarly, the location of the center of the fault region is one
of the parameters that determines the number of ANPs that
can potentially be damaged by the fault. For a specific region
centered at an I-point fi , and a transmission range tr, we
define an interval on the timeline as static interval, if the set
of potentially damaged nodes due to a region fault at location
fi and the set of alive links with transmission range tr remain
unchanged. We can find static intervals using the timeline
region-coverage(fi ) and the timeline link-lifetime L(tr). In
order to find the static intervals for I-point fi and transmission
range tr, we define four events during the time interval when
fi exists: (i) a dead link comes alive, (ii) a live link dies, (iii) an
ANP node comes within coverage area of fi and (iv) an ANP
node moves out of the coverage area of fi . The instance of
time at which any of the four events takes place is the instance
of the start time of a new static interval. Let SI(fi , tr) be
a sorted list of events resulting from combining the sorted
list L(tr) and N T (fi , tm ) for all tm 2 T (fi ). Therefore,
between any two consecutive elements in SI(fi , tr) neither
the topology nor the region coverage changes.
Once the nodes within a region (or nodes covered by a
region) and the set of active links during a static interval
are known, we can use Algorithm-2 in [7] in order to find
the region based connectivity of the network with respect to
I-point fi . Region based connectivity with respect to an Ipoint fi (RBC(fi )) is defined to be the minimum number of
nodes in the region centered at fi whose failure disconnects
the network. If the number of nodes that can be damaged
due to a region based fault at fi is ni (i.e., the fault at fi
covers ni nodes), we would like the ANPs to have enough
transmission range, so that the region based connectivity of
the graph is at least ni + 1. This will ensure that the network
will remain connected if any subset of the covered nodes fails.
Using Algorithm-2 of [7], and applying binary search within
the range 0 T rmax we can find the minimum transmission
range necessary in each static interval, to ensure that the
network remains connected when a region fi fails (during the
interval when it fi is exists). We define err to be the maximum acceptable difference between the smallest transmission
range necessary to maintain connectivity and the smallest
transmission range computed by the algorithm to maintain
connectivity. In our algorithm, we set the maximum possible
transmission range to be equal to diameter of the deployment
area. The algorithm computes the minimum transmission range
necessary to maintain connectivity for each static interval. The
maximum of these minimum values computed is the critcal
transmission range (CTR). Alg. 2 provides all the details.
In Alg. 2, line 1 takes O(n2 ). In order to compute T (fi )
we need to solve sij = 2R. As described in Alg. 1, for the
case that ANPs move at the same velocity, !, this equation
can be solved easily in constant time and it has two solutions
in one period. So, |T (fi )|  2. In line 5, we have to solve

2001

over them. Fig. 7 shows the results. It may be observed that
increase in the value of R leads to increase in CTR. This
observation is quite expected as larger regions can destroy
more nodes at a time. Moreover, it may be noted that for larger
values of orbit radii the transmission range also increases. The
reason is that for a specific number of nodes in a bounded
deployment area, larger orbit radii result in larger distance
between the nodes. Accordingly, larger transmission range is
necessary, particularly in the case of larger Rs.
600
500
Transmission Range

Div (t) = R for all v 2 V . For one node v, this equation
also in one period can have a constant number of solutions
since it can easily be converted to a single variable polynomial
equation with
P degree 6. So, computation of N T (fi , tm ) takes
O(n) and tm |N T (fi , tm )| = O(n). Consequently, lines 27 have complexity of O(n3 ). The while loop is repeated for
log T rmax (binary search complexity). As it is discussed in
Alg. 1, computation of L(tr) takes O(n2 log n). Computation
of SI(fi , tr) need sorting the sorted lists N T (fi , tm ) and
L(tr) which takes O(n2 ). Clearly, |SI(fi , tr)| = O(n2 ).
Computing RBC(fi ) takes O(n4 ) [7]. Therefore, the time
complexity of Alg. 2 is O(n8 log T rmax ).
Algorithm 2 Computing CTR

12:
13:
14:
15:
16:
17:
18:
19:

Compute sij (t) for all pair of ANPs i and j
for all I-points fi 2 I
Compute T (fi ) = {(t1f , t2f ), . . . , (tkf 1 , tkf )}
for all tm 2 T (fi )
Compute N T (fi , tm ) = {(tm1 , Nm1 ), . . . , (tmp , Nmp )}

300
200
100

0

error = T rmax , tra = 0, trb = T rmin = T rmax
while error > err
error = error/2, tr = (tra + trb )/2
Find L(tr) = {l1 , l2 , . . . , lt } using Alg. 1
for all I-points fi 2 I
SI(fi , tr)
Sort the lists N T (fi , tm ) and L(tr) based on time
of the events (considering all tm 2 T (fi ))
for all event 2 SI(fi , tr)
Update the graph G(t) (by adding or removing the links) or the
region coverage of fi
RBC(fi )
Using Alg. 2 in [7] Compute the region-based
connectivity considering only one region centered at I-point fi
if (RBC(fi ) ni + 1) NextSI
TRUE
else NextSI
FALSE; break;
if (NextSI = FALSE) tra = tr; break;
if (NextSI = TRUE) trb = tr; T rmin = tr
return T rmin

IV. S IMULATION
The goal of our simulation is to investigate the impact of
various parameters, such as the number of ANPs and the
region radius on determining the CTR. In our simulation
environment, the deployment area is a 1000 ⇥ 1000 square
mile area. The center of the orbits of the ANPs and the radius
of the orbits are chosen in such a way that the orbits do
not intersect with each other. In our simulation, we assume
that all the ANPs move at the same angular speed of ! = 20
radian/hour.
Fig. 6 shows the effect of number of nodes on the transmission range. In these experiments, for each value of n we
conducted 40 experiments and the results are averaged over
the 40 different random initial setups. In these experiments
orbit radius = 10 and R = 30. It may be observed that an
increase in the number of nodes results in a decrease in CTR.
Although, more nodes may be destroyed by failure of a larger
region, larger node density ensures that a smaller transmission
range is sufficient to maintain the connectivity of the network.
In the second set of experiments, we examined the impact
of change of the region radius R on the transmission range.
We conducted these experiments for two values of orbit radii,
10 and 30, and n = 35 in both the cases. For each value of R,
we conducted 100 experiments and the results are averaged

0

10

Fig. 6.

20

30

40

50

60

70

80

90

100

Number of Nodes

Transmission Range vs. Number of Nodes

700
Transmission Range

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

400

600
500
400
OrbitRadius=10

300

OrbitRadius=30

200
100
0
0

Fig. 7.

10 20 30 40 50 60 70 80 90 100 110

Region Radius

Transmission Range vs. Region Radius, n = 35

V. C ONCLUSION
Motivated by the importance of robustness and fault tolerance capability of ANs, we have investigated the region-based
connectivity of the ANs and proposed an algorithm to find
the minimum transmission range necessary to ensure that the
surviving nodes of the network remain connected, even when
all or some nodes of region fail due to an enemy attack.
R EFERENCES
[1] J. Rohrer and et al., “Airborne telemetry networks: Challenges and
solutions in the ANTP suite,” in Milcom, 2010.
[2] S. Milner and et al., “Performance and scalability of mobile wireless
base-station-oriented networks.”
[3] K. Kwak and et al., “IANetServ: Design and implementation of robust
and auto-configurable network service for airborne network,” in Milcom,
2010.
[4] C. Danilov and et al., “Experiment and field demonstration of a 802.11based ground-UAV mobile ad-hoc network,” in Milcom, 2009.
[5] C. Danilov, Presentation at the Network Mathematics Workshop, Harvard University, November 2009.
[6] A. Sen and et al. Architecture and algorithms for airborne networks.
[Online]. Available: http://netsci.asu.edu/drupal/node/5#tech
[7] ——, “Fault-tolerance in sensor networks: A new evaluation metric,” in
Infocom, 2006.
[8] ——, “Region-based connectivity: a new paradigm for design of faulttolerant networks,” in HPSR, 2009.
[9] S. Neumayer and et al., “Assessing the vulnerability of the fiber
infrastructure to disasters,” in Infocom, 2009.
[10] S. Neumayer and E. Modiano, “Network reliability with geographically
correlated failures,” in Infocom, 2010.
[11] P. Santi, “The critical transmitting range for connectivity in mobile ad
hoc networks,” IEEE Transactions on Mobile Computing, vol. 4, 2005.

2002

On Targeted Entity Hardening Problem in
Multi-layered Interdependent Networks
Joydeep Banerjee, Chenyang Zhou, and Arunabha Sen
School of Computing, Informatics and Decision System Engineering

arXiv:1701.07106v1 [cs.NI] 24 Jan 2017

Arizona State University
Email: {jbanerje, czhou24, asen}@asu.edu
Abstract—Critical infrastructures like power and communication networks are highly interconnected. Hence it is imperative
to have an accurate model that captures these interdependencies. A number of models were proposed but each has their
own limitations. Some limitations of the existing models were
addressed by the Implicative Interdependency Model which uses
Boolean Logic to represent these interdependencies. In this paper,
we study the Targeted Entity Hardening problem. Some entities
in an interdependent network might have a higher priority to
be protected from failure. This can be achieved by hardening
a set of entities. A hardened entity is assumed to have the
capacity to remain operational on being attacked by any kind
of adversary. But hardening an entity would usually entail a
high cost. So it is essential to minimize the number of hardened
entities which would protect all entities in the higher priority
set from failure. We study this problem utilizing the Implicative
Interdependency Model. We provide a polynomially solvable
restricted case and prove that the problem is NP-complete. An
Integer Linear program is provided to obtain the optimal solution.
A heuristic is proposed whose efficacy is judged by comparing it
with the optimal solution using real world data.
Keywords—Interdependent Network, Entity Hardening, Targeted Entity Hardening, Power Network, Communication Network

I.

I NTRODUCTION

A nation’s critical infrastructures (like power, communication, transportation networks) are heavily interdependent on
each other for their functionality. As an example the power
grid and communication network engage in a heavy symbiotic
relationship among each other. To explain this dependency
consider entities in power network such as Supervisory Control
and Data Acquisition System (SCADA). The SCADA systems
primarily control the electricity generation and flow in the
power grid. These controls are essentially carried out by signals
from the communication network. On the other way round
every entity in the communication network require power to
be operational. These dependencies causes failure in any of
these two network to have its impact on the other which may
eventually lead to cascade of failures. The initial failure of
entities in either network are driven by their vulnerability to
nature induced and man made (terrorist attack, cyber-attack)
failure. Failures in power or communication network have
disastrous effects (as seen in power blackouts which occurred
in New York (2003) [1] and India (2012) [2]).
This topic is important in communication network perspective in the same way it is for power grid. Owing to the
dependencies stated above, the working of the communication
network can get affected due to failures in power network.
Previous work on this topic [8] has also been accepted in
communication conference (GLOBECOM). Hence it can be
claimed that modeling and analysis of interdependent powercommunication network is relevant among communication
network research community.

Modeling these complex interdependencies and analysis of
failure in these infrastructures are considered to be highly important. A number of models have been proposed that capture
these interdependencies [3], [5], [4], [6], [7], [8], [9], [10].
However each of these models have their own shortcomings in
bringing out the complex nature of the interdependencies that
might exist [11]. Authors in [12] brings out the need to address
the complex interdependency which can explained through
the following example. Let ax (which can be a generator,
substation, transmission line etc.) be a power network entity
and bw , by , bz (which can be a router, end system etc.) a set
of communication network entities. Consider the dependency
where the entity ax is operational if (i) entities bw and (logical
AND) by are operational, or (logical OR) (ii) entity bz is
operational. Models in [3], [5], [4], [6], [7], [8], [9], [10]
fails to capture this kind of interdependency. Owing to the
above nature of dependencies, graph based models might
lack the capacity to represent the same. Motivated by these
findings and limitations of the existing models, the authors
in [12] proposed a Boolean logic based dependency mode
termed as Implicative Interdependency Model (IIM). For the
example stated above the dependency of ax on bw , by , bz can
be represented as ax ← bw by +bz . This equations representing
the dependency of an entity is termed as Interdependency
Relation (IDR). Using this model a number of problems were
studied on interdependent power and communication infrastructure system [12], [13], [14], and [15]. The Targeted Entity
Hardening problem discussed in this article is a restricted
version of the Entity Hardening Problem [14]. We use IIM
to study this problem with respect to interdependent power
and communication network.
An entity xi when hardened is resistant to both initial and
induced (failing of entities in the cascading process) failures. In
physical world an entity can be hardened with respect to cyber
attacks (say) by having strong firewall. Similarly some entities
can be hardened by strengthening their physical structures for
protection from natural disaster. There exist multiple such ways
to harden an entity from different kind of failures. Even though
there may be circumstances under which an entity cannot be
hardened, in this paper we relax such possibilities and assume
that there always exist a way to harden a given entity.
For a positive integer K consider the most vulnerable set of
K entities [12] are known. Initial failure of these vulnerable
entities would maximize the total number of failed entities
(through induced failure) in the interdependent network. The
Entity Hardening Problem [14] takes as input an interdependent network, the set of K vulnerable entities and a budget H
on the number of entities that can be hardened. With these K
vulnerable entities failing initially, the problem finds a set of
H entities which when hardened minimizes the cardinality of
the final failure set.
For an interdependent power and communication infrastructure certain entities might have higher priority to be

2

protected. There might exist entities whose non-functionality
poses higher economic or societal damage as compared to
other entities. For example, power and communication network
entities corresponding to office buildings running global stock
exchanges, transportation sectors like airports etc. presumably
are important to have higher protection. Let F denote the
failed set of entities (including initial and induced failure)
when the K most vulnerable entities fail initially. We define
a set P (with P ⊆ F ) of entities which have a higher
priority to be protected. P contains all those entities which
have a higher priority to be protected. The Targeted Entity
Hardening problem finds the minimum set of entities which
when hardened would ensure that none of the entities in set P
fail. In our initial attempt to solve this problem we consider
a restricted class of IDRs. For this setting the problem can
be solved optimally in polynomial time and an algorithm
solving the same is provided. However the problem is found
to be NP-complete for IDRs in general form. Owing to the
hardness we provide an Integer Linear program that solves the
problem optimally and a non optimal heuristic with polynomial
run time. Using power and communication network data of
Maricopa County, Arizona we ran experiments to compare the
heuristic with the optimal solution.
II.

I MPLICATIVE I NTERDEPENDENCY M ODEL AND
P ROBLEM F ORMULATION

In this section we describe the IIM model [12]. An interdependent network is represented as I(A, B, F(A, B)) with
A being the set of entities in power network, B representing
the set of entities in communication network and the function
F(A, B) consisting of the IDRs that capture the dependencies
between these two networks. Consider an example where the
set A and B consist of entities {a1 , a2 , a3 } and {b1 , b2 , b3 , b4 }
respectively. The function F(A, B) giving the set of dependency equations are provided in Table I. In the given example,
an IDR b3 ← a2 + a1 a3 implies that entity b3 is operational
if entity a2 or entity a1 and a3 are operational. In the IDRs
each conjunction term e.g. a1 a3 is referred to as minterms. It is
also to be noted that the IDRs can have dependencies between
entities in the same network.
Power Network
a1 ← b2
a2 ← b2
a3 ← b4
−−

Comm. Network
b1 ← a1 + a2
b2 ← a1 a2
b3 ← a2 + a1 a3
b4 ← a3

TABLE I: IDRs for the constructed example

Initial failure of entities in A ∪ B would cause the failure
to cascade until a steady state is reached. The event of an
entity failing after the initial failure is termed as induced
failure. The cascade is assumed to occur in time steps of unit
length. Each time step captures the effect of entities killed in
all previous time steps. We demonstrate the cascading failure
for the interdependent network outlined in Table I through
an example. Consider that the entity a2 and a3 fail at time
step t = 0. Table II represents the cascade of failure in each
subsequent time steps. In Table II, for a given entity and
time step, 0 00 represents the entity is operational and 0 10 non
operational. In this example a steady state is reached at time
step t = 3 when all entities are non operational. The IIM model
also assumes that the dependent entities of all failed entities are
killed immediately at the next time step. For example at time
step t = 1 entities a2 , a3 , b2 , b3 and b4 are non operational.
Due to the IDR a1 ← b2 entity a1 is killed immediately at
time step t = 2. At t = 3 the entity b1 is killed due to the IDR
b1 ← a1 +a2 thus reaching the steady state. It can be followed

that with K = 2, {a2 , a3 } represents a set of 2 vulnerable
entities.
Entities
a1
a2
a3
b1
b2
b3
b4

0
0
1
1
0
0
0
0

1
0
1
1
0
1
1
1

Time
2
1
1
1
0
1
1
1

Steps
3
1
1
1
1
1
1
1

(t)
4
1
1
1
1
1
1
1

5
1
1
1
1
1
1
1

6
1
1
1
1
1
1
1

TABLE II: Failure cascade propagation when entities {a2 , a3 } fail at
time step t = 0. A value of 1 denotes entity failure, and 0 otherwise

Before describing the problem in IIM setting we note some
of the challenges in modeling the IDRs. The main challenge
is accurate formulation of the IDRs. Two possible ways of
doing this would be (i) careful analysis of the underlying
infrastructures as in [10], (ii) consultation with domain experts.
The formulation of IDRs from the interdependent network
is an ongoing research and the problem is solved under the
assumption that these IDRs can be developed.
With this example we develop the notion implied by the
Targeted Entity Hardening (TEH) problem. As mentioned
earlier, when an entity xi ∈ A ∪ B is hardened then it
is protected from both initial and induced failure. Consider
the problem where we have to choose minimum number of
entities to harden such that at least the entity {b4 } is protected
from failure. With {a2 , a3 } being the two vulnerable entities,
hardening entity a2 (with a3 failing) would prevent failure
of entities a1 , a3 , b1 , b1 , b3 . Similarly hardening the entity a3
(with a2 failing) would prevent the failure of entity b4 . Even
though hardening a2 prevent failure of more entities than
hardening a3 , owing to the problem description a3 has to
be hardened which is a solution to the TEH problem in this
scenario. It is to be noted that other entities might also be
protected from failure when a set of entities are hardened to
protect a given set of entities.
The TEH problem is formally stated below accompanied
with a descriptive diagram provided in Figure 1 —
INSTANCE: Given:
(i) An interdependent network system I(A, B, F(A, B)),
where the sets A and B represent the entities of the two
networks, and F(A, B) is the set of IDRs.
(ii) The set of K most vulnerable entities of the system
A0 ∪ B 0 , where A0 ⊆ A and B 0 ⊆ B.
(iii) The set F ⊆ A ∪ B contains all the entities failed due to
initial failure of A0 ∪ B 0 entities.
(iv) A positive integer k and k < K.
(v) A set P ⊆ F .
DECISION VERSION: Is there a set of entities
H = A00 ∪ B 00 , A00 ⊆ A, B 00 ⊆ B, |H| ≤ k, such that
hardening H entities would result in protecting all entities in
the set P after entities A0 ∪ B 0 fails at the initial time step.
OPTIMIZATION VERSION: Find the minimum set of
entities in A ∪ B to harden that would result in protecting all
entities in the set P after entities A0 ∪ B 0 fails at the initial
time step.
The TEH problem solutions are based on the following
assumptions — (i) A hardened entity is always operational
and at any given time step it does not fail, even if the entity
belongs to the K most vulnerable set of entities, (ii) the
condition k < K is assumed as with k ≥ K hardening the

3

Other Entities Protected
from Failure

Direct
Failure
of set 𝒀
with
|𝒀| = K
Set of
entities 𝑨
and 𝑩
Hardened
set of
entities 𝑯
with
|𝑯| = 𝐤

Interdependency
Relations
F(𝑨, 𝑩)

Entities in
Target
Set 𝑷
protected
from
failure

Failed entities

Fig. 1: Figure describing the Targeted Entity Hardening problem

K vulnerable entities would ensure that there are no induced
and initial failure, (iii) The set of K vulnerable entities for an
interdependent network i.e. the attackers strategy is assumed
to be provided. The necessity of this assumption can be
described through the example provided in this section. For
the interdependent network with IDRs in table I both the set
{b2 , b4 } and {a2 , a3 } represent the 2 vulnerable entities. If we
worked with the former set of entities the solution obtained
for the TEH problem with P = {b4 } will be different. Hence
for a given K if the attacker has a choice of different set of
K vulnerable entities, with the current problem setting it is
not possible to find the set of entities to harden. Solving the
problem by relaxing the assumption (iii) is considered to be a
potential future work.
III.

as an entity is essentially a vertex in G. It can be shown that
G is either (a) Directed Acyclic Graph (DAG) with maximum
in-degree of at most 1 or, (b) contain at most one cycle with
no incoming edge to any vertex in the cycle and maximum indegree of at most 1, or (c) collection of graphs (a) and/or (b).
Consider a vertex xi ∈ V . Let G0 = (V 0 , E 0 ) be a subgraph
of G with V 0 consisting of xi and all the vertices that has
a directed path from xi . Moreover The edge set E 0 consists
of all edges (x, y) ∈ E with x, y ∈ V 0 except for any edge
(y, xi ) with yi ∈ V 0 . Such a subgraph G0 would be a directed
tree with (i) one or more entities in V 0 \{xi } is in A0 ∪ B 0 . Let
X denote the set of such entities which satisfy this property,
or (ii) no entities in V 0 \{xi } is in A0 ∪ B 0 . If the entity xi
is hardened then for case (i) all the entities in V 0 would be
protected from failure except for entities in all subtrees with
roots in X. The set of entities in such subtrees are contained in
a set Z (say). For this condition if xj ∈ V 0 \Z then P S(xj |A0 ∪
B 0 ) ⊂ P S(xi |A0 ∪ B 0 ). Else if xj ∈ Z then P S(xi |A0 ∪ B 0 ) ∩
P S(xj |A0 ∪ B 0 ) = ∅. For case (ii) for any entity xj ∈ V 0 the
condition P S(xj |A0 ∪ B 0 ) ⊆ P S(xi |A0 ∪ B 0 ) always holds
(the equality holds for graphs of type (b) as stated above).
This property holds for all entities in the entity set A ∪ B.
Hence proved.
Algorithm 1: Algorithm for TEH problem with IDRs of
minterms with size 1

C OMPUTATIONAL C OMPLEXITY A NALYSIS

In this section we provide (i) a polynomial time algorithm
that solves the Targeted Entity Hardening Problem optimally
with IDRs restricted to single minterm of size 1 and (ii) prove
the problem is NP-complete with IDRs in general form.
A. Solution to the Targeted Entity Problem with IDRs having
single minterm of size 1

1
2
3
4
5
6

With two entities xi and yj of network A and B respectively, IDRs of form xi ← yj represents this special case.
Additionally an entity can appear at most once on the left side
of the IDR. Consider a simple interdependent network with
the following two properties — (i) When an entity fails it has
the capacity to make one or more entity non operational, (ii)
However each entity can be made non operational by failure
of at most one entity. Presence of this kind of dependency in
the physical word encompass the IDRs of this special case. To
develop the solution for this case consider an interdependent
network I(A, B, F(A, B)). Let A0 ∪ B 0 (with A0 ⊆ A and
B 0 ⊆ B) the K most vulnerable entities of this interdependent
network. We first define Protection set in Definition 1. Using
the result in Theorem 2 we design an algorithm (Algorithm 1)
that solves the problem for this case optimally in polynomial
time (proved in Theorem 3).
Definition 1. P S(xi |A0 ∪ B 0 ) denotes a set of entities which
would not fail due to induced failure when the entity xi is
hardened with entities in A0 ∪ B 0 failing initially. P S(xi |A0 ∪
B 0 ) is termed as Protection Set of entity xi .
Theorem 2. For any entity xi and xj with xi 6= xj either (a)
P S(xi |A0 ∪ B 0 ) ⊆ P S(xj |A0 ∪ B 0 ), (b) P S(xj |A0 ∪ B 0 ) ⊆
P S(xi |A0 ∪B 0 ), or (c) P S(xi |A0 ∪B 0 )∩P S(xj |A0 ∪B 0 ) = ∅.
Proof: Consider a directed graph G = (V, E). The vertex
set V consists of a vertex for each entity in A ∪ B. For each
IDR of form y ← x there is a directed edge (x, y) ∈ E. In
this proof the term vertex and entity is used interchangeably

7
8
9

10

Data: An interdependent network I(A, B, F(A, B)), set of
K vulnerable entities and the set P of entities to be
protected from failure.
Result: A set of entities H to be hardened.
begin
For each entity xi ∈ (A ∪ B) compute the Protection Sets
P S(xi |A0 ∪ B 0 ) ;
Initialize H = ∅ ;
while P 6= ∅ do
Choose the Protection Set with highest
|P S(xi |A0 ∪ B 0 ) ∩ P |;
Update H = H ∪ {xi } ;
Update P = P \P S(xi |A0 ∪ B 0 );
for all xj ∈ A ∪ B do
P S(xj |A0 ∪ B 0 ) =
P S(xj |A0 ∪ B 0 )\P S(xi |A0 ∪ B 0 );
return E ;

Theorem 3. Algorithm 1 solves the Targeted Entity Hardening
problem with IDRs having single minterms of size 1 optimally
in polynomial time.
Proof: The Protection Sets of the entities can be found
in a similar way as that of computing Kill Sets defined in
[12]. Due to lack of space we did not provide the algorithm to
compute the Protection Sets. It can be shown that computing
these sets for all entities in A∪B can be done in O(n3 ) where
n = |A| + |B|. The while loop in Algorithm 1 iterates for a
maximum of n times. Step 5 can be computed in O(n2 ) time.
The for loop in step 8 iterates for n times. For any given xj
and xi , P S(xj |A0 ∪ B 0 ) = P S(xj |A0 ∪ B 0 )\P S(xi |A0 ∪ B 0 )
can be computed in O(n2 ) time with the worst case being the
condition when |P S(xi |A0 ∪ B 0 )| = |P S(xj |A0 ∪ B 0 )| = n.
As step 9 is nested in a for loop within the while loop this
accounts for the most expensive step in the algorithm. The
time complexity of this step is O(n4 ). Thus Algorithm 1 runs
polynomially in n with time complexity being O(n4 ).
In Algorithm 1 the while loop iterates till all the entities
in P are protected from failure. In step 5 the entity xi with

4

protection set P S(xi |A0 ∪ B 0 ) having most number of entities
belonging to set P is chosen to be hardened. Correspondingly
the entity xi is added to the hardening set H. The set P is
updated by removing the entities in P S(xi |A0 ∪ B 0 ). Similarly
all the protection sets are updated by removing the entities in
P S(xi |A0 ∪ B 0 ).
We use the result from Theorem 2 to prove the optimality
of Algorithm 1. An entity xi is selected to be hardened at any
iteration of the while loop has maximum number of entities
in P S(xi |A0 ∪ B 0 ) ∩ P . All entities xj with P S(xj |A0 ∪
B 0 ) ⊆ P S(xi |A0 ∪ B 0 ) would have P S(xj |A0 ∪ B 0 ) ∩ P ⊆
P S(xi |A0 ∪ B 0 ) ∩ P . Moreover there exist no entity xk for
which P S(xi |A0 ∪B 0 ) ⊂ P S(xk |A0 ∪B 0 ) otherwise xk would
have been hardened instead. Hence there exist no other entity
that protect other entities in P including P S(xi |A0 ∪ B 0 ) ∩ P .
So Algorithm 1 selects the minimum number of entities to
harden that protects all entities in P .
B. Computation Complexity of Targeted Entity Hardening with
IDRs in general case
In general case the IDRs can have arbitrary number of
minterms of arbitrary size. With entities xi and yq belonging
to network A(B) and
Pp B(A)
Qq respectively this case can be
represented as xi ← j1 =1 j2j1=1 yj2 . The given example has
p minterms each of size qj1 . We prove that the TEH problem
is NP-complete (in Theorem 4) when the IDRs are logical
disjunctions with minterms of size 1. With entities xi and yq
belonging to network A(B) P
and B(A) respectively this case
p
can be represented as xi ← q=1 yq . The given example has
p minterms each of size 1. This class of IDRs is a subset of
the general case and hence proves that TEH problem is NPcomplete for the general case as well. In Theorem 5 we prove
that the TEH problem is solvable with an approximation bound
of O(log(|P |) when IDRs are logical disjunctions of minterms
with size 1.
Theorem 4. The TEH problem is NP-complete
Proof: We proof that the Targeted Entity Hardening is NP
complete by a reduction from Set Cover problem. An instance
of the Set Cover problem consists of (i) a set of elements U =
{x1 , x2 , . . . , xn }, (ii) a set of subsets S = {S1 , S2 , . . . , Sm }
with Si ⊆ U ∀Si ∈ S, and (iii) a positive integer M . The
0
problem asks the question
S whether there is a subset S of S
0
with |S | ≤ M such that Sk ∈S 0 Sk = U . From an instance of
the Set Cover problem we create an instance of the Targeted
Entity Hardening Problem as follows. For each element xj in
U we add an entity aj in set A. Similarly for each subset Si
in set S we add an entity bi in set B. For each element xi ∈ U
which appears in subsets Sm , Sn , Sp ∈ S (say) we add an IDR
ai ← bm + bn + bp . There are no IDRs for entities in set B
which prevents any cascading failure. The value of K is set to
|S|. This ensures that all entities in set B are killed initially for
final failure of maximum number of entities. The maximum
number of entities failed in this scenario are all entities in
A ∪ B. The set P of entities to be protected is set to U and k
is set to M .
Consider there exists a solution to the Set Cover problem. Then there exist a set S 0 of cardinality M such that
S
0
Sk ∈S 0 Sk = U . For each subsets Sk ∈ S we harden the
entity bk ∈ B. So in each IDR of the A type entities there
exist a B type entity that is hardened. Hence all A type entities
will be protected from failure thus solving the Targeted Entity
Hardening problem.
On the other way round consider there is a solution to the
Targeted Entity Hardening problem. This ensures either that
for each entity aj ∈ A (i) aj itself is hardened, or (ii) at least

one entity from set B in aj ’s IDR is hardened. For scenario (i)
arbitrarily select an entity bp in aj ’s IDR and include it in set
C. For scenario (ii) include the hardened entities in the IDR
of aj into set C. This is done for each entity aj ∈ A. For each
entity in set C select the corresponding subset in set S. The
union of these set of subsets would result in the set U . Thus
solving the set cover problem. Hence the theorem is proved.
Theorem 5. The Targeted Entity Hardening Problem is
O(log(|P |) approximate when IDRs are logical disjunctions
of minterms with size 1.
Proof: We first compute the protection set P S(xi |A0 ∪B 0 )
for all entities xi ∈ A ∪ B. Each protection set is pruned by
removing entities that are not in set P . Now the Targeted Entity
Hardening Problem can be directly transformed into Minimum
Set Cover problem by setting U = P and S = {P S(x1 |A0 ∪
B 0 ), P S(x2 |A0 ∪ B 0 ), ..., P S(x| A| + |B||A0 ∪ B 0 )}. Selecting
the corresponding entities of the protection sets that solve the
Minimum Set Cover problem would also solve the Targeted
Entity Hardening problem. There exists an approximation ratio
of order O(log(n)) (where n is the number of elements in set
U ) for the Set Cover problem. Hence the same ratio holds for
the Targeted Entity Hardening problem with n = |P |. Hence
proved.
IV.

S OLUTIONS TO THE E NTITY H ARDENING P ROBLEM

In this section we provide optimal solution to the Targeted
Entity Hardening problem using an Integer Linear program
and a non optimal heuristic solution with polynomial time
complexity.
A. Optimal solution to the Targeted Entity Hardening problem
We propose an Integer Linear Program (ILP) that solves
the TEH problem optimally. Let G = [g1 , g2 , ..., gn ] and
H = [h1 , h2 , ..., hm ] be two boolean arrays with gi = 0
(hj = 0) if entity ai ∈ A (bj ∈ B) is in a failed state and
gi = 1 (hj = 1) otherwise. Given an integer K let [G, H]
be the solution to the K most vulnerable node problem
(with gi = 1 (hj = 1) if entity ai (bj ) belongs to the set of
vulnerable entities). A set of variables xid and yjd are used in
the ILP with xid = 1 (yjd = 1), when entity ai ∈ A (bj ∈ B)
is in a failed state at time step d, and 0 otherwise. It is to
be noted that the maximum number of cascading steps is
upper bounded by |A| + |B| − 1 = m + n − 1. The variables
qxi and qyi are used to denote entity hardening. If an entity
xi ∈ A (yj ∈ B) is hardened then qxi = 1 (qyj = 1) and 0
otherwise.The objective function can now be formulated as
follows:
min

m
X
i=1

qxi +

n

X
qyj

(1)

j=1

The set of constraints are described below:
Constraint Set 1: xi0 ≥ gi − qxi and yi0 ≥ hi − qyi . This
constraint implies that only if an entity is not hardened and
gi (hi ) is 1 then the entity will fail at the initial time step.
Constraint Set 2: xid ≥ xi(d−1) , ∀d, 1 ≤ d ≤ m + n − 1, and
yid ≥ yi(d−1) , ∀d, 1 ≤ d ≤ m + n − 1, in order to ensure
that for an entity which fails in a particular time step would
remain in failed state at all subsequent time steps.
Constraint Set 3: Modeling the constraints to capture

5

the cascade propagation in IIM is similar to the constraints
established in [12]. A brief presentation of this constraint is
provided here. Consider an IDR ai ← bj bp bl + bm bn + bq in
general form. The following steps are enumerated to depict
the cascade propagation:
Step 1: Replace all minterms of size greater than one with
a variable. In the example provided we have the transformed
minterm as ai ← c1 +c2 +bq with c1 ← bj bp bl and c2 ← bm bn
(c1 , c2 ∈ {0, 1}) as the new IDRs.
Step 2: For each variable ck , constraints are added to capture
the cascade propagation. Let N be the number of entities
in the minterm on which ck is dependent. In the example, for the variable c1 with IDR c1 ← bj bp bl , constraints
+y
+yl(d−1)
y
c1d ≥ j(d−1) p(d−1)
and c1d ≤ yj(d−1) + yp(d−1) +
N
yl(d−1) ∀d, 1 ≤ d ≤ m + n − 1 are introduced (with N = 3
in this case). If IDR of an entity is already in form of
a single minterm of arbitrary size, i.e., ai ← bj bp bl then
y
+y
+yl(d−1)
− qxi and xid ≤
constraints xid ≥ j(d−1) p(d−1)
N
yj(d−1) +yp(d−1) +yl(d−1) ∀d, 1 ≤ d ≤ m+n−1 are introduced
(with N = 3). These constraints satisfies that if the entity xi
is hardened initially then it is not dead at any time step.
Step 3: Let M be the number of minterms in the
transformed IDR as described in Step 1. In the given
example with IDR ai ← c1 + c2 + bq , constraints of form
xid ≥ c1(d−1) + c2(d−1) + yq(d−1) − (M − 1) − qxi and
c
+c
+yq(d−1)
xid ≤ 1(d−1) 2(d−1)
∀d, 1 ≤ d ≤ m + n − 1 are
M
introduced. These constraints ensures that even if all the
minterms of xi has at least one entity in dead state then it
will be alive if the entity is hardened initially.
Constraint Set 4: For all entities xi , yj ∈ P , xi(m+n−1) = 0
and yj(m+n−1) = 0. This ensures that all the entities in set P
are protected from failure at the final time step.
With these constraints the objective in (1) minimizes the
number of hardened entities that results in protection of all
entities in set P .
B. Heuristic Solution o the Targeted Entity Hardening problem
A heuristic solution is provided in this subsection. Along
with the definition of Protection Set, we introduce the notion
of Cumulative Fractional Minterm Hit Value of an entity to
design the heuristic. Before formal definition of Cumulative
Fractional Minterm Hit Value (in Definition 7) we first define
Fractional Minterm Hit Value of an entity in Definition 6.
Definition 6. The Fractional Minterm Hit Value for an
entity xj ∈ A ∪ B in an interdependent network
I(A, B, F(A, B)) is denoted as
F M HV (xj , X). It is calPm
culated as F M HV (xj , X) = i=1 |s1i | . In the formulation
m are the minterms in which xj appears over all IDRs except
for the IDRs of entities in set X. The parameter si denotes ith
such minterm. If an entity xj is hardened (or protected from
failure) the value computed provides an estimate impact on
protection of other non operational entities.
Definition 7. The Cumulative Fractional Minterm Hit
Value for an entity xj ∈ A ∪ B is denoted as
CF
M HV (xj ) =
P M HV (xj ). It is computed as CF
0
0
∀xi ∈P S(xj |A0 ∪B 0 ) F M HV (xi , P S(xi |A ∪ B )). This gives
a measure of impact on protecting non functional entities when
the entity xj is hardened.
Using these definitions a heuristic is formulated in Algorithm 2. For each iteration of the while loop in the algorithm,

Algorithm 2: Heuristic solution to the TEH problem

1
2
3
4
5
6

7

8
9
10
11
12

13
14
15
16
17
18

19

Data: An interdependent network I(A, B, F(A, B)), set of
K vulnerable entities and the set P of entities to be
protected from failure.
Result: A set of entities H to be hardened.
begin
Initialize D = ∅ and H = ∅ ;
while P 6= ∅ do
For each entity xi ∈ (A ∪ B)\D compute the
Protection Sets P S(xi |A0 ∪ B 0 ) ;
For each entity xi ∈ (A ∪ B)\D compute
CF M HV (xi );
if There exists multiple entities having same value of
highest cardinality of the set P S(xi |A0 ∪ B 0 ) ∩ P
then
Let xp be an entity having highest
CF M HV (xp ) among all xp ’s in the set of
entities having highest cardinality of the set
P S(xi |A0 ∪ B 0 ) ∩ P ;
If there is a tie choose arbitrarily;
Add xp to set H ;
Update D = D ∪ P S(xp |A0 ∪ B 0 );
Update P = P \P S(xp |A0 ∪ B 0 );
Update F(A, B) by removing entities in
P S(xp |A0 ∪ B 0 ) both in the left and right side of
the IDRs ;
else
Let xi be an entity having highest cardinality of
the set P S(xi |A0 ∪ B 0 ) ∩ P ;
Add xj to set H ;
Update D = D ∪ P S(xi |A0 ∪ B 0 );
Update P = P \P S(xi |A0 ∪ B 0 );
Update F(A, B) by removing entities in
P S(xj |A0 ∪ B 0 ) both in the left and right side of
the IDRs ;
return H ;

the entity having highest cardinality of the set P S(xi |A0 ∪
B 0 ) ∩ P is hardened. This ensures that at each step the number
of entities protected in set P is maximized. In case of a tie,
the entity having highest Cumulative Fractional Minterm Hit
Value among the set of tied entities is selected. This causes
the selection of the entity that has the potential to protect
maximum number of entities in subsequent iterations. Thus,
the heuristic greedily minimizes the set of entities hardened
which would cause protection of all entities in P . The heuristic
overestimates the cardinality of H from the optimal solution.
Algorithm 2 runs in polynomial time, more specifically the
time complexity is O(|P |n(n + m)2 ) (where n = |A| + |B|
and m = Number of minterms in F(A, B)). It is to be noted
if the Algorithm returns H with |H| ≥ K then we harden
entities given by the K most vulnerable set. This is because
hardening these K vulnerable entities would protect all entities
in the interdependent network from failure.
V.

E XPERIMENTAL R ESULTS

In this section we compared the heuristic solution with
the optimal obtained from the Integer Linear program. The
simulations were performed with power network data obtained
from Platts (www.platts.com) and communication network
data obtained from GeoTel (www.geo-tel.com). The power
network consists of 70 power plants and 470 transmission lines
and communication network constitutes 2, 690 cell towers,
7, 100 fiber-lit buildings and 42, 723. All this data pertains to
Maricopa County, Arizona, USA. Five non overlapping regions

6

(a) Region 1

(b) Region 2

(d) Region 4

(c) Region 3

(e) Region 5

Fig. 2: Comparison of the number of entities hardened in optimal solution (ILP) and heuristic in each of the 5 identified regions with protection
set cardinalities 5, 10, 15, 20

of the county were identified (union of these five region does
not span over the entire Maricopa county). The entities in
the power and communication network for these regions were
extracted and added to set A (for power network entities) and
set B (for communication network entities). The cardinality
of the entity sets A and B were 29 and 19 for Region 1, 29
and 20 for Region 2, 29 and 19 for Region 3, 33 and 20 for
Region 4 and 29 and 20 for Region 5. For each of these regions
an interdependent network I(A, B, F(A, B)) was constructed.
The function F(A, B) was computed for each interdependent
network I(A, B, F(A, B)) using the IDR construction rules
defined in [12].
IBM CPLEX optimizer was used to obtain the optimal
solutions. The heuristic solutions were obtained using Python.
The value of K (K most vulnerable entity) was set to 8 which
resulted in failure of 28, 23, 28, 28 and 27 entities over the five
regions. For each region an arbitrary set P was constructed
using 5, 10, 15, 20 entities from the respective failed set of
entities. The set P gives the entities to be protected from
failure. The number of entities required to harden for a given
region and set P was obtained for the optimal and heuristic
solution. The results are provided in Figure 2a - Figure 2e. For
the given set of simulations the heuristic solution deviates by a
maximum of 50% with the optimal when |P | = 15 for Region
5. This variance can be accounted for the greedy nature of
Algorithm 2. On an average the heuristic solution varied by
18.31% from the optimal.
VI. C ONCLUSION
In this paper we studied the Targeted Entity Hardening
problem in multi-layer networks. We modeled the interdependencies shared between the networks using IIM, and formulated the Targeted Entity Hardening problem in this setting.
We showed that the problem is solvable in polynomial time
for a special case, whereas for IDRs in general form it
is NP-complete. We evaluated the efficacy of our heuristic
by comparing it with the optimal solution (obtained from
an Integer Linear program) using power and communication
network data of Maricopa County, Arizona.
R EFERENCES
[1] G. Andersson, P. Donalek, R. Farmer, N. Hatziargyriou, I. Kamwa,
P. Kundur, N. Martins, J. Paserba, P. Pourbeik, J. Sanchez-Gasca et al.,
“Causes of the 2003 major grid blackouts in north america and europe,
and recommended means to improve system dynamic performance,”
Power Systems, IEEE Transactions on, vol. 20, no. 4, pp. 1922–1928,
2005.

[2] Y. Tang, G. Bu, and J. Yi, “Analysis and lessons of the blackout in indian
power grid on july 30 and 31, 2012,” in Zhongguo Dianji Gongcheng
Xuebao(Proceedings of the Chinese Society of Electrical Engineering),
vol. 32, no. 25. Chinese Society for Electrical Engineering, 2012, pp.
167–174.
[3] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
“Catastrophic cascade of failures in interdependent networks,” Nature,
vol. 464, no. 7291, pp. 1025–1028, 2010.
[4] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, “Cascade of failures
in coupled network systems with multiple support-dependence relations,”
Physical Review E, vol. 83, no. 3, p. 036116, 2011.
[5] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, “Networks formed
from interdependent networks,” Nature Physics, vol. 8, no. 1, pp. 40–48,
2011.
[6] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, “Modelling interdependent infrastructures using interacting
dynamical models,” International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63–79, 2008.
[7] P. Zhang, S. Peeta, and T. Friesz, “Dynamic game theoretic model of
multi-layer infrastructure networks,” Networks and Spatial Economics,
vol. 5, no. 2, pp. 147–178, 2005.
[8] M. Parandehgheibi and E. Modiano, “Robustness of interdependent
networks: The case of communication networks and the power grid,”
arXiv preprint arXiv:1304.0356, 2013.
[9] D. T. Nguyen, Y. Shen, and M. T. Thai, “Detecting critical nodes in
interdependent power networks for vulnerability assessment,” 2013.
[10] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
“Power grid vulnerability to geographically correlated failures-analysis
and control implications,” arXiv preprint arXiv:1206.1099, 2012.
[11] J. Banerjee, A. Das, and A. Sen, “A survey of interdependency models
for critical infrastructure networks,” NATO Science for Peace and Security Series -D: Information and Communication Security, vol. 37, pp.
1–16, 2014.
[12] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, “Identification of k most vulnerable nodes in multi-layered network using a new
model of interdependency,” in Computer Communications Workshops
(INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014, pp.
831–836.
[13] A. Das, J. Banerjee, and A. Sen, “Root cause analysis of failures in
interdependent power-communication networks,” in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910–915.
[14] J. Banerjee, A. Das, C. Zhou, A. Mazumder, and A. Sen, “On the
entity hardening problem in multi-layered interdependent networks,”
in Computer Communications Workshops (INFOCOM WKSHPS), 2015
IEEE Conference on. IEEE, 2015, pp. 648–653.
[15] A. Das, C. Zhou, J. Banerjee, A. Sen, and L. Greenwald, “On the smallest
pseudo target set identification problem for targeted attack on interdependent power-communication networks,” in Military Communications
Conference, MILCOM 2015-2015 IEEE. IEEE, 2015, pp. 1015–1020.

On some topological properties of Hypercube, Incomplete Hypercube and Supercube

Arunabha Sen
Department of Computer Science
Arizona State University
Tempe, AZ 85287

Abhijit Sengupta
Department of Computer Science
University of South Carolina
Columbia, SC 29208

Abstract : Hamiltonian properties of Hypercube, Incomplete Hypercube and Supercube are examined in
this paper. It is known that in a nonfaulty hypercube
there are a t least n! Hamiltonian cycles. We extend
this result showing that the lower bound is at least
2n-3n!. We show that with at most n-2 faulty links a
faulty hypercube has at least 2(n-2)! Hamiltonian cycles. We establish that an incomplete hypercube with
odd (even) number of nodes has (n-2)! Hamiltonian
paths (cycles). We show that a supercube has at least
( n - l)! Hamiltonian cycles and when the number of
nodes is 2"-' +2"-', then the number of Hamiltonian
cycles is at least as high as 2(n - l)!.

I. Introduction
The hypercube interconnection network is probably the most well studied interconnection network with
many interesting properties known [l-4,6,11]. In this
paper, we examine the Hamiltonian properties of a
hypercube and two of its derivative networks, the incomplete hypercube [5] and the supercube [7-lo]. The
existence of Hamiltonian cycles (HCs) in a hypercube
is important as it relates to the embedding of rings.
This problem has been examined before [l-3,6,11]. It
was shown in [4] that a nonfaulty n-dimensional hypercube Qn has at least n! HCs. In this paper, we
extend the result to show that the lower bound is at
least 2n-3n!. It is shown in [l] that a Qn with at
most n - 2 faulty links has an HC. It is trivial to see
that a Qn with more unrestricted faulty links cannot
have an HC. In [ 3 ] ,it has been shown that a Qn with
at most 2n - 5 faulty links will have an HC if every
node has at least two nonfaulty links incident on it.
In this paper, we show that a Qn with at most n - 2
faulty links has a t least 2(n - 2)! HCs. There is no
known result on the Hamiltonian properties of hypercube derived networks like incomplete hypercubes [5]

636

1063-713Y93$3.00 0 1993 IEEE

Subir Bandyopadhyay
School of Computer Science
University of Windsor
Windsor, Ontario N9B 3P4

and supercubes [7-lo]. In this paper, we establish that
an incomplete hypercube with odd (even) number of
nodes has (n-2)! Hamiltonian paths (cycles). We also
show that a supercube has at least ( n - l ) ! HCs and
when the number of nodes is 2"-'
2"-', there are at
least 2(n - l)! HCs.

+

11. Definitions and Notations
An interconnection network will be represented
by an undirected graph G = (V, E ) where the set of
nodes (processors) is V = {vo, V I , ...,V N - ~ }and the set
of edges (communication links) is E = { e l , e 2 , ..., em}.
Each edge will also be denoted by an unordered pair
of nodes, the endpoints of the edge. Let n = [ l o g z N ] .
Each node can be represented by a binary vector of
a n dimensions. Any node V i and its binary vector
representation will be used synonymously. Without
loss of generality, we will assume that vi is the binary
representation of the integer i. Note that if N < 2",
there will exist binary vectors not representing any
node of G. A node v j will be called the image of a node
vi if the vector representations of vi and v j differ only
in the most significant bit. The image of a node vi will
be denoted by I(vi). The Hamming distance between
the nodes vi and v j is the number of bit positions where
they differ and will be denoted by H(vi, vj). Obviously,
H(vi, U,) is the number of ones in the bitwise exclusiveor of vi and v j . G is an n-dimensional hypercube,
denoted by Qn, if N = 2" and ( V i , Vj) E E iff H(vi, vj)
= 1. If vi and v j differ only in bit position d (counting
starts from 1 and from left to right), then they are
called neighbors along the dimension d and the edge
between them is called the edge in dimension d. G is an
n-dimensional incomplete hypercube when (vi, v j ) E E
iff H(vi, v j ) = 1. For an incomplete hypercube, N need
not be 2". Suppose the nodes of a graph G with N
nodes is divided into subsets U , V1, V2, V3 as follows:
U is the set of nodes having most significant bit 0, V3

is the set of nodes having most significant bit 1 and
V1 is the set of nodes that are images of the nodes in
V3. V2 = U - V1 . G is a supercube when (vi, v j ) E E
iff H ( v i , v j ) = 1 or H ( v i , V j ) = 2 when vi E V2 and
vj E V3. In case, N is a power of two, both supercube
and incomplete hypercube are complete hypercubes.

Proof: The proof is by induction. For j = 2, sj =
2122x1 and hence the path P ( v i , s j ) is given by the
e,,vi + e , , which is
node sequence vivi +e,,vi +e,,
a HP in the subcube formed by the dimensions x1,22
and containing vi. Also vi +e,, is adjacent to vi in dimension e,,, thus the lemma holds for j = 2. Assume
that the lemma holds for j = m.
Now s,+1 = s m x m + l s m . Hence P ( u , s m + l ) is
given by following node sequence P( vi,Sm) F( V i , Sm)
e,m+l P ( F ( v j ,s m ) e,
, sm). Note that by induction hypothesis, P ( v i , s m ) contains all the nodes of
the subcube formed by the dimensions 2 1 , x2, ...,xm
and containing vi and F ( v i , sm)is vi + e$, . Also note
that P ( F ( v i ,sm)+eZ,+,, sm)contains all the nodes of
the subcube, formed by the dimensions X I , x2, ..., xm
and containing F(vi, s m ) e,,,, = vi e,,
e,,,,.
Thus P ( v i , s m + l ) contains all the nodes of the subcube
formed by the dimensions tl,2 2 , ...,I,, xm+l and con)
,
s
= vi
taining vi. Since F ( F ( v ( ,Sm)
e,
e,,+,
e,
= Vj e,,,,,
the lemma holds
for j = m + 1. By induction argument, the lemma
follows.
Corollary 1 : For any vi and for any permutation
x on Z , P(vi, D,) is an HC in Q n .
Corollary 2 :
For any v j and for any permutation x = x1x 2...2, on Z, P(vj,D,) has 2"-i edges in
dimension xi, 1 5 i 5 n - 1 and two edges in the
dimension 2".
From corollary 2, it follows directly that for any
given U,, and for two different permutations x1 and x2,
P ( v j , D r l ) and P(vj, D T 2 are
) distinct HCs unless the
permutations x1 and a;!have the same prefix of length
n-2. For any vj and any permutation x1 = x 1 ~ 2 . . . 2 ~ ,
since the HC P ( v j , O x , )has two edges in dimension
t n - l as well as in dimension t n iit is possible that
P ( v j , DT1)and P ( v k , DTl)are non-distinct for distinct
vj and vk. Also it is possible that for two distinct permutations x1 and 7 r 2 , P(vj,D,,) and P(vk,D,,) are
non-distinct for distinct vj and v k . We call two permutations 7rl and 172 to be homologous to each other
if they differ only in the last two symbols. Given a
permutation 171 = ~ 1 x ...
2 tn,we call two nodes of Qn
to be homologous with respect to the permutation x1
if they differ only in the bit positions zn-l and xn.
Thus if nodes vi and vj are homologous with respect to
some permutation T I , then they are homologous with
respect to some another permutation xg, such that ?rl
and 7r2 are homologous.
We observe that there are 2"n! HCs given by
P ( v i , D,) for 2" possible choices of vi and n! possible

+

111. Hamiltonian cycles in hypercubes

+

+

It has been shown [4] that corresponding to n!
permutations of the dimensions of a Q n , there are n!
distinct gray codes, each of which corresponds to a distinct HC and hence a Qn must have a t least n! distinct
HCs (two HCs are non-distinct if their edge sequences
are the same with or without circular rotation). In
this section, we show that there are many more distinct HCs in a fault-free hypercube Q n .

+

+

Let Z={1,2,3, ...,n} be the set of dimensions of
a Qn. Any string of symbols chosen from Z will be
called a dimension sequence (DS). Suppose we use the
following steps to form a DS.

+

1. Choose any x1 from Z and remove it from Z.
2. Define a string s1 = t l . (SI is a string of length 1)
3. For every j , 2 5 j 5 n , choose a xj from Z, remove
it from Z and define sj as sj = sj-1 * x j * si-1 where
* is concatenation operation
4. Set U t sn * x,;
Let e, denote the binary vector having only zth
bit 1 and let + denote the modulo 2 addition. For
any permutation x = ~ 1 x ...
2 xn on Z, the string U obtained by the above steps is a DS and will be denoted
by D, . Given any node vi and a DS y, let P(vi, y) denote the walk traversed as one starts from the node vi
and traverses the dimensions of y in the order they appear in y and F(vi, y) denote the node finally reached
at the end of the walk. For any x-dimensional edge
appearing in y, let us denote T(vi, y, x) = { vjl tth bit of v j is zero and the edge (vj, vj+e,) appears
in P(vi, y) ). Given distinct x 1 , 2 2,..., z q E 2 , let
L ( I ~ , z ...,
~ , z q ) denote the set of vectors in the subspace spanned by e,, , e,,, ...,e,. and for any vector w
of Qn, let C(t1,22,..., z , ; w ) be the coset [12] induced
X t~q ,; w ) =
by w on the subspace. Hence C ( X ~ , ...,
{w+cyJa E L ( z l , x 2 , ..., zg)}.Thus, for two distinct wl
and w2, C ( z l , t 2 , ...,z q ; w l ) and C ( t l , x 2 , ...,xq;w2)
are same or disjoint.
Lenima 1 :
For any node vi and for any s j ,
2 5 j 5 n , as constructed above, F ( v i , s i ) is the neighbor of vi along the dimension xj and P ( v i , si) gives a
Hamiltonian path (HP) starting from ui in the subcube
formed by the dimensions t 1 , 2 2 , ...,x j .

637

+

+

+

+

+

IV. Hamiltonian cycles in faulty hypercubes

choices for x . However, not all such HCs are distinct.
Suppose we partition all such HCs in n!/2 blocks (2"+'
HCs in each block) such that for any pair of nodes
vj and vk (not necessarily distinct), P ( v j , D , , ) and
p ( V k , Dr,)
are in the same block if and only if ?rl and
?r2 are homologous. As noted earlier, the HCs given by
P ( v j , On,)
and P(vk, D x , ) are distinct if ?rl and a2 are
non-homologous. If there are at least Ii' distinct HCs
in each block then Qn has at least (n!/2)K distinct
HCs. We will show that for n > 3 , K 2 2"-'.
Lemma 2 : The HCs P ( v j , D , , ) and P ( V k , & , )
are non-distinct for distinct vj and vk if and only if vj
and vk are homologous with respect to r l .

Proof : Let ?r1 = 21x2 ...c,. Then the dimension
sequence U traversed in the P ( v j , D , , ) is given by
U = s , - ~ z , s , - ~ z , ;sj = s j - 1 t j s j - 1 , 2 5 j 5 n - 1,
and s1 = 21. The HCs P ( v j , D , , ) and P ( V k , & , )
could be non-distinct if and only if starting from vk
in P ( V k , D,,),the node v j is reached either right before traversing the dimension t, (the first or the second time) or right afler traversing the dimension t,
for the first time (note that the dimension I, is traversed twice and the node reached after traversing it
for the second time in P(?&,or,)
is the node v k ) . It
may be observed that the nodes reached immediately
before traversing the dimension E , in P ( V k , D n l ) are
given by vk +e,,, and vk e,,,-l and the node reached
immediately afler traversing the dimension I, for the
. Thus the lemma
first time is given by vk +e,,, +e,,-,
follows.
Lemma 3 : For two homologous permutations x1
and 7 2 , the HCs P ( V j , D , , ) and P ( V k , D r a )are nondistinct for distinct vj and vk if and only if Vj and
vk + e,,-, are homologous with respect to ?rl (or x 2 ) .

+

Proof: Let xl = z 1 1 2 ...x,. Then, 1 2 = z 1 t 2 . . . z n - 2
z n z n - l . The HCs P(vj, D n l ) and P ( V k , D,,) could
be non-distinct iff starting from V k in P(Vk, Dn,),
the
node vj is reached right before or afler traversing the
dimension 2,. This dimension is traversed twice and
the nodes reached before or after traversing the dimension 2 , are homologous to vk e,,,-, with respect to
?rl (or ~ 2 ) Hence
.
the lemma.
Theorem 1 : A Q, has 2"%! distinct HCs.

Since an HC in a Q, needs 2" edges and a Q,
has n2"-' edges, it seems possible that a Q, will have
HCs even when some of the edges are not available,
because the corresponding communication links are
faulty. This has been confirmed by Bruck et.al. [l]
and Chan et. al. in [3]. In this section, we show that
so long as the number of faulty edges is at most n - 2,
there are at least 2(n-2)! HCs in such a faulty Q,.
In the previous section, we have seen that for any
node vj and any permutation x on 2, P ( v j , D , ) is
an HC. If vj = vo (UO is zero binary vector), then
we will call P ( v j , Dn)as the canonical HC for A. We
will show that if the number of faulty edges is at most
n - 2 , then the faulty Q, has a t least 2(n-2)! canonical
HCs. Let Qi be the set of canonical HCs when z1 = i.
The canonical HCs have some interesting properties as
follows.
Observation 1 : Starting from 210, the first edge of
each HC in Ei is the edge between vo and v2, (recall
that 00 is the node given by the zero vector).
Observation 2 : Starting from 210, the odd numbered
edges of any HC in a oi are in the same dimension and
any odd numbered edge in any HC in Qi appear as an
odd edge of every HC of Ej.
Proof : The odd edges of an HC are in dimension X I .
Fromcorollary 2 , we know that dimension zl edges will
appear 2"-' times in an HC. Since there are only 2"-'
edges in dimension 21, all edges in that dimension will
be present in the HC. The observation follows since all
HCs in Si have the same 21.

All edges in dimension i appear in

Corollary 3 :
1

Gi.

For the sake of simplicity, for any permutation
the canonical HC P(vo,D,) will be denoted by
H ( x ) . Consider any permutation x = t 1 z 2 ...t,. For
any j, 2 5 j 5 n - 1, and for any node v i , since
S j = S j - 1 t j s j - 1 , when the edge in dimension x j is traversed in P ( v i , S j ) , the traversal is from F ( v i , sj-1) to
F ( v i , s j - l ) + e , , , which is from vi+e,,-, to vi+e,,-, +
e,, (by lemma 1). Hence, from definition, T(v,,s j , z j )
= {vi e = , - , } .
Lemma 4 : For k > j,
T ( v i , s k , X j ) = C(+j+lrX j + Z , ' ' ' 9 x k ; vi
e,,-,).
Proof: We will prove by induction. Since sj+l = sj
t j + l s j , by lemma 1, F(vi, Sj+l) = vi+e,,+,
and hence
F ( v i , s j . z j + l ) = Vi+e,,+, + e , , . Hence T ( v i , ~ j + l , t j )
- {vi e , > - , , vi e,,,, ez,- ,} = C(tj+l;vi e=,- ,).
Thus the lemma is true for A: = j 1. Assume that
K,

+

+

Proof : By lemmas 2 and 3 , it follows that in each
block, for a given x l , four different choices of U , give
the same P ( v j , D , , ) and the same HC is given by
P ( V k , Dr,)
for four different choices of V k for ?r2 homologous to X I . Hence each block has at least 2"+'/8 =
271-2 distinct HCs. Since there are n ! / 2 blocks, there
are at least 2n-2(n!)/2 = 2"-3n! distinct HCs in a Q n .

+

+

638

+

+

+

+

the lemma is true for k = m

>j.

Hence T ( v i , S m ,Zj)

= C ( x j + l , 2 j + 2 , ...,dm; vi + e Z j - , ) . Since s m + l = sm
Xm+l sm,by lemma 1, F ( v i , s m + l ) = vi + e,,,,
and
hence F ( v i , s m . t m + l ) = vi+e,,+,
+e,,.
Hence T ( v i ,
= T ( v i , s m , x j ) U T(vi+ez,+e,,+,,s,,zj)
e Z j - , ) U C(zj+l,t j + 2 , ...,
2,; vi+e,,+e,,+,+e,j-,)
= C(xj+l,z j + 2 , -..,xm; vi+
U C(zj+l,t j + 2 1 . . . , z m ; vi
e,,+,
ezj-1) =
C(zj+l,zj+2, ..., t m r2m+1; vi
(because e,.,,
is one of the bases of the subspace L ( z l , 2 2 , ..., Zm)).
Thus the lemma holds for k = m+ 1. Hence the lemma
follows by induction.
Given any node v j , let Zj be the set of bit positions where the vector representation of vj has a one.
Given an edge ( v j , vj + e i ) in dimension i (without loss
of generality, assume, i 4 Zj), of Q,,suppose we want
to find how many of the n! canonical HCs include the
given edge ( v i , v, + e i ) . Let us assume that Zj is not
empty.
Lemma 5 : For a permutation K = 2 1 2 2 . . . 2 n r and
an edge ( v j , vj + e i ) in dimension i such that i 4 Z j ,
and 2 1 E Zj, the edge ( v j , vj e i ) appears in H ( K ) iff
2 2 = i. (Note that 11 cannot be i , since 2 1 E Zj and
vj has i-th bit 0)
Proof: Suppose x2 = i , then by lemma4, T(vo,Sn, x 2 )
= C(2324 ...2,; vo+e,,) = C ( t 3 ~...
4 zn;e , , ) . Note that
vj E C ( t 3 ~...
4 z,; e,,) because 2 2 = i 4 2 , . Hence the
edge ( v j , vj + e i ) appears in the traversal given by 8.,
Suppose 2 2 # i. Thus x k = i for k > 2. Then, by
lemma4, since T(vj,s,, z k = C ( z k + l , zk+2, ..., t n ;v j +
e,,-,) and all the nodes in C(zk+l,2k+2, ...,2,; V j
has 21th bit 0 and hence the endpoints of an
i-dimensional edge in H ( K ) has 21th bit 0 and since
2 1 E Zj, the edge cannot appear in H(?r).
Corollary 4 : An edge ( v j , vj e i ) in dimension i ,
i 4 Zj, appears in ( n - 2)! canonical HCs given by the
permutations having 11 E Zj and 2 2 = i .
Lemma 6 :
An edge ( v j , vj + e i ) in dimension i,
i 4 Zj, appears in H ( a ) , where K = ~ 1 x ...
2 2, and
21 $! Zj and t l # i , iff for some k, such that k # n
and 2 k = i , (1) z k - 1 E Zj and (2) Zj - (Zk-1) E
sm+1,2j)

= C ( x j + l , t j + 2 , ...,xm; vi

+

+

+

+

+

+

+

{ ~ ~ +2lkr+ 2 , . - - ,+n}.

Proof: Assume both conditions hold.
= T(vo,
Then vj E C ( ~ k + l , ~ t + .2. .,, z n ; v o
s,, zt) = T(v0,s,, i ) . Hence the edge (vj, vj e , ) appears in H ( T ) .

+

+

Suppose t k - 1 $! Zj. Then V j 4 C ( Z k + i , Z k + 2 , ..., X n ; Y o
+e,,-,)
because vj has 2k-lth bit 0 while every node
in C ( ~ k + 1 , 2 k + 2...,
,
2,;vo
e,,-,) has xk-lth bit 1.

+

639

Hence v, is not the endpoint of an i-dimensional edge
traversed in H ( K ) . Suppose (2) does not hold, then
some element Of Zj does not belong to { x k - 1 ) U{ t k + l ,
q + 2 , ...,t
,
} and thus a cannot be in C ( t k + l , 2 k + 2 , ...,
x,; W O +
implying vj is not the endpoint of an
i-dimensional edge traversed in H ( x ) .
Lemma 7 : An edge ( v j , vj e i ) in dimension i,
i $! Z,, appears in H ( K ) , where K = 31x2 ...t n - l i iff

+

zj

= {Zn-1}.

Proof : Since in K , i appears as the last symbol,
by corollary 2, only two edges in dimension i appear
in H ( n ) . One of them is the last edge of the HC
as one starts from vo and hence is given by ( v 0 , e i ) .
This cannot be the edge ( v j , v j e , ) , because Zj is
non-empty. The other dimension i edge in the HC is
( F ( v 0 ,sn-l),F(vo, s,-i)+ei). Hence the edge ( v j , vj+
e i ) can appear in the HC is iff vj = F ( v o , s , - ~=
)
vo+e,,-,
(by lemma 1) and this implies Zj = {2,-1}.

+

+

Given an edge ( v j , vj e i ) in dimension i, i 4 Zj,
suppose we want to find how many canonical HCs contain this edge. All these HCs do not exist in a €aulty
Q, if the edge is faulty. From the above discussions,
it follows that the canonical HC given by the permutation K = 2 1 2 2 ...E , contains the edge iff
Case I : 2 1 = i , by corollary 3 (this means the edge
appears in all the (n - l)! HCs of Gj), or
Case 11: 11 E Zj and 2 2 = i, by corollary 4 (this
means the edge appears in (n - a)! HCs of each Ga,
CY E Zj, where 2 2 = i ) , or
Case 111: if X k = i, 2 € k C n , then one element of
Zj must appear as z k - 1 and all the other elements of
zj must appear after X k in K , by lemma 6 or
Case IV: if I , = i , then either Zj = q5 or 2, =
{ t n - l } , by lemma 7.
Note that for a given

/3 4 Zj, a canonical HC in

Gp will contain the edge iff the corresponding permutation K satisfies the case 111 or IV above. The number
of canonical HCs in Gp for a given /3 4 Zj can be computed as follows using case I11 and IV. Let lZjl = q.

To form the permutation K , note that t l = /3 and if i
appears as X k , 2 < k < n,then to have the condition of
case I11 satisfied, we must have 3 5 k 5 n - q + 1. Now
that we have 2 1 and 2 k chosen, and r k - 1 must be one
element of Zj (which can be done in q possible ways),
we must have the n - k positions after zk containing
the remaining q - 1 elements of Zj , while the remaining n - q - 2 elements of Z arranged in ( n - q - 2)!
ways. Hence the number of canonical HCs in G p for a
given p 4 Zj is given by

is at most (r(n-l)+t(n-r))(n-2)! 5 ( t ( n - l ) + t ( n t ) ) ( n- 2)! = t(2n -t - l)(n - 2)!. Thus the number of
canonical HCs present in the faulty Qn is at least n! t(2n-t-l)(n-2)! = n! - (n-2)(2n-n+2-l)(n-2)!
= 2(n - 2)!.

The above discussions can be summarized as follows. An edge ( w j ,v j e i ) in dimension i , i # Zj and
2, # 4, appears in all ( n- l ) ! canonical HCs in Gi and
in ( n - 2)! canonical HCs in every other Gas.
If, however, Zj = 4, then v j = vo and hence the
edge ( v j , v j e i ) in dimension i can appear in H ( r )
iff the edge is the first or the last edge in the HC as
one starts from wo. This means i is the first or the last
symbol of x . If i is the first symbol in x , then the HC
is in Gi and by corollary 2 we know that the edge is
in all (n - l ) ! canonical HCs of Q i . In every other G,,
there will be exactly (n - 2)! canonical HCs containing
the edge, these are the HCs given the AS having first
symbol cr and last symbol i . Thus, we have proved the
following theorem.

+

V. Hamiltonian paths in incomplete
hypercubes
An incomplete hypercube with N nodes as defined in (51 might not have a Hamiltonian cycle (for
example, a five node incomplete hypercube), because
there exists node of degree one. However, it always
has a Hamiltonian path. We denote a n incomplete hypercube by IH and refer to an IH as odd IH (OIH)
if it has odd number of nodes and an even IH (EIH)
otherwise. . We show that an OIH with N nodes has
at least least ( n - 2)! Hamiltonian paths (HPs) while
an EIH with N nodes has at least (n - 2)! HCs where
n = [logzNl.

+

+

Theorem 2 : Any i dimensional edge ( v j , vj e,),
i Z j , appears in (n - l)!canonical HCs of Gi and in
( n - 2)! canonical HCs of every other Gj
The above theorem will be useful in finding a
lower bound on the number of HCs in a faulty Qn.
It is simple to note that a faulty Qn cannot have an
HC if more than n - 2 unrestricted edge failures occur.
However, in presence of at most n - 2 unrestricted edge
faults, a Qn still remains Hamiltonian as shown by the
following theorem.
Theorem 3 : If Qn has a t most n - 2 faulty edges,
there are at least 2(n - 2)! canonical HCs in the faulty
Qn.

Proof : Let t be the number of faulty edges, t 5
n - 2. Let f i be the number of faulty edges in dimension i. Without loss of generality, assume that the
faulty edges are only in dimensions 1, 2, ..., r . Thus
fi = t and r 5 t . Also by theorem thenumber, all
canonical HCs in G I , Gz, ..., Gr contain faulty edge(s)
and are not available in the faulty Qn. Also by theorem thenumber, each of the faulty edges makes (n-2)!
canonical HCs in each of G+I,G r + 2 , ..., Gn unavailable. Hence the total number of unavailable canonical
HCs in !&+I, Gr+2, ..., 1;, is a t most t ( n - r ) ( n - 2)!.
Thus the total number of unavailable canonical HCs
in Qn is at most r ( n - l ) !+ t ( n - r ) ( n- 2)! = ( r ( nl)+t(n-r))(n-2)!. If r < t , then r ( n - l ) + t ( n - r ) i
t ( n - l ) + t ( n - t ) , otherwise t 2 n - 1 , violating the assumption that t 5 n - 2. Therefore, r(n - 1) + t ( n - r )
5 b(n - 1) + t ( n - t ) , (equality holds when t = r ) .
Thus, the total number of unavailable canonical HCs

640

Let us consider a Qn first. I n section IV, we described how to generate n! canonical HCs in a Qn by
using permutations on the dimension set 2. Consider
the class of permutations x = z1z2...zn, where z1 = 1,
2 2 = n and other z i s are arbitrary (recall that in
the vector representation of a node, dimensions are
counted from left to right and starts from 1). Evidently, there are (n-2)! such permutations. Suppose
xi is one such permutation and consider the H ( A ~in
)
Qn as defined in section IV. The H ( x j ) can be considered as a circular list of nodes of Qn.
Since we have chosen 2 1 = 1, starting from node
wo in the H ( x i ) , all the odd numbered edges are in
dimension 1, as given by the observation 2. Hence in
,
consecutive
the node sequence given by H ( A ~ )three
nodes cannot have 1 as the leftmost bit in the binary
representation of all of them. Let U and v be two
consecutive nodes in the node sequence given by H ( x ; )
and having a 1 in the leftmost bit of both of them, Let
p , U ,U ,q be a part of the node sequence containing U
and w. Since both U and w have the leftmost bit 1
the edge between them is not in dimension 1, thus the
edge between them must be an even numbered edge,
implying that the edge between p and U must be in
dimension 1 and so also the edge between v and q.
Since U and w differ exactly in one bit position, so do p
and q . Thus, if from the node sequence given by H ( x i ) ,
both the nodes U and w are removed so that the node
sequence p , U , U , q now becomes p , q , the modified node
sequence gives a valid walk in the Qn.
Given an EIH with N nodes, suppose the nodes

are numbered 0 through N - 1 (that is the highest
node number is an odd integer). Suppose we mark
every node from the node sequence given by H ( n i ) ,
whenever the node is non-existent in the EIH, that is,
it is numbered more than N - 1. Obviously, such a
marked node will have leftmost bit 1 in its vector representation. Suppose a node a is marked in the node
sequence and let p and 7 be the nodes appearing next
to a in the node sequence, that is, par is part of the
node sequence containing a. Without loss of generality, assume ,6 and CY differ in the leftmost bit. Then
starting from V O , dimension 1 edges must have been
traversed even number of times to reach p, and this
implies the edge between a and 7 must be in dimension 2 2 = n. Since the highest node number is an odd
integer, y must also have a node number larger than
N- 1. Hence 7 is also marked. If all the marked nodes
are removed from the node sequence given by H ( ? r i )
as mentioned above, then the modified node sequence
remains valid HC in the EIH.
Suppose we consider an EIH with 12 nodes numbered from 0 to 11, then n = 4. Considering ni = 1423,
the node sequence given by H(*i) is 0, 8, 9, 1, 5, 13,
12, 4, 6, 14, 15, 7, 3, 11, 10, 2, 0. The marked nodes
are nodes numbered 12 through 15. After node removal, the modified node sequence is 0, 8, 9, 1, 5, 4,
6, 7, 3, 11, 10, 2, 0 which is an HC in the EIH.
In the case of an OIH with N nodes, a similar
process of marking the nodes in the node sequence of
H ( n i ) given by the same ni can be used. In the case of
an EIH, as we found above, if a node is marked then either its predecessor or successor in the node sequence is
also marked and removal of the marked nodes did not
affect the walk. When the same technique is applied
for a OIH, it works exactly the same way, excepting
the fact the node numbered N gets marked but neither
its predecessor nor its successor in the node sequence.
Hence from the HC given by H ( n i ) , the HP can be
constructed by starting from N - 1 and following the
node sequence given by the HC and ending at the node
which was consecutive to N in the HC.
Considering an OIH with 11 nodes numbered from
0 through 10, the marked nodes are nodes numbered
11 through 15. After node removal, the node sequence
is 0, 8, 9, 1, 5, 4, 6, 7, 3, 10, 2, 0, which can be viewed
as a HP in the OIH as 3, 7, 6, 4, 5, 1, 9, 8, 0, 2, 10.
Since there are (n-2)! possible ways to choose the
permutation, we have proved the following theorem.
Theorem 4 : An OIH with N nodes has (n - 2)!
Hamiltonian paths (HPs) and an EIH with N nodes

has ( n - 2)! Hamiltonian cycles (HCs) where n =
r10g2NI.

VI. Hamiltonian cycles in supercubes
In this section, we show that a supercube network

N nodes has at least (n - l ) ! HCs, where n = [ f o g 2N I .
For some specific values of N , the number of HCs are
even higher. Like the incomplete hypercubes, here also
we start with the Qn and use the similar node marking procedure. Assume that the nodes are numbered
0,1,2,...,N - 1.
Theorem 5 : A supercube with N nodes has (n-l)!
HCs where n = rfogzN1.
Proof: We prove this result using similar arguments
used in the proof of theorem 4. Consider the class of
permutations ?r = 21x2 ...2, where X I = 1 and other
xis are chosen arbitrarily. Clearly there are (n - l)!
such permutations. Construct the canonical HCs given
by H ( n ) in Qn and in the node sequence given by
the HC, mark a node if it is numbered larger than
N - 1 (same marking procedure as considered in the
case of an incomplete hypercube). Using the same
arguments as before, if two consecutive nodes in the
node sequence with 1 in their leftmost bits are marked
then the modified node sequence after removing the
marked nodes still remains a cycle. Consider the case
when only one of two consecutive nodes with leftmost
bit 1 is marked. Note that because we chose X I = 1,
there cannot exist in the node sequence, three consecutive nodes each having leftmost bit 1. Suppose a
substring of the node sequence was p , U , v,q in which
U and v have 1 in their leftmost bit positions, while p
and q have 0. Also assume that the node U was marked
but v was not. and the substring became p-v-q. Since
p has leftmost bit 0 and is adjacent to U (numbered
larger than N - 1, hence leftmost bit 1) in Qn p must
be in V2 (refer to definition of V1, V2 and V3 in a supercube). Also w has leftmost bit 1 and hence belongs
to V3. Since H ( p , v) = 2, p and v are adjacent in the
supercube. Thus after removal of the marked nodes,
the modified sequence remains to be a valid path in
the supercube. Hence the modified sequence gives an
HC in the supercube. Corresponding to (n - l)! different ways choosing the permutations with x1 = 1,
there will be (n - l)! different HCs in the supercube.

+

Theorem 6 : A supercube with 2,-'
2"-' nodes
h a s a t least 2(n - l)! HCs.
Proof: Consider the of permutations n = 21x2 ...zn
where 21 = 2 and the other x i s are chosen arbitrarily.
Construct the canonical HCs given by H ( n ) in Qn and

641

in the node sequence given by the HC, mark a node
if it is numbered larger than N - 1 (same marking
procedure as considered in the case of an incomplete
hypercube). Note that because we chose 21 = 2, there
cannot exist in the node sequence, three consecutive
nodes each having the second bit 1 . Consider a substring of the node sequence p , U ,v , q in which two nodes
U and v have 1 in their second bit positions, while p and
q have 0. If both U and v are marked then, removal
of them from the node sequence keeps the modified
node sequence a valid path, because p and q are adjacent. Assume U is marked but v is not. Note that
since N = 2"-' 2n-2, p and q are not marked. Since
U is marked but U is not, the edge between U and v
must be in dimension 1. Because every odd dimensional edge is in dimension xl = 2, the edge between p
and U must be dimension 2 and so is the edge between
v and q. Since U has 1 in bit position 1, so does p .
Also because U is marked, p must be in V3 (refer to
the definition of V3). Again, v has 0 in bit position 1
( v is unmarked) and has 1 in bit position 2, hence v
appears in V2. Since H ( p , v ) = 2, by definition, p and
v are adjacent in the supercube. Thus, after removal
of marked nodes, the modified node sequence is a valid
path in the supercube and hence gives an HC. Using
the result of theorem 5, we note that all the modified
node sequences obtained from the canonical HCs of Qn
choosing the permutations to have 2 1 = 1 or z1 = 2
give HCs in the supercube. Hence there are at least
2(n - l)! HCS.

References

1. J . Bruck, R. Cypher and D. Soroker, Running
ascend-descend algorithms on faulty hypercubes,
Research Report RJ 8339 (76062), IBM T. J .
Watson Research Center, September 9, 1991.
2. J . Bruck, R. Cypher and D. Soroker, Tolerating
faults in hypercubes usin Subcube partitioning,
Research Report RJ 8142 (74555), IBM T . J .
Watson Research Center, May 31, 1991.

3. M. Y. Chan and S. J. Lee, On the existence of
Hamiltonian circuits in faulty hypercubes, SIAM
Journal of Discrete Mathematics, vol. 4, pp.
511-527, 1991.

+

4. M. S . Chen and K. G. Shin, Processor allocation
in an N-cube multiprocessor using gray codes,
IEEE Trans. Computers, vol. C-36, pp. 1396-

1407, 1987.
5. H.P. Katseff, Incomplete hypercubes, IEEE Trans.
Computers, vol. C-37, pp. 604-608, 1988.
6. Y. Saad and M. H. Schultz, Topological properties of hypercubes, IEEE Trans. on Computers,
vol. C-37, pp. 867-872, 1988.
7. A. Sen, Supercube: An optimally fault-tolerant
network architecture, Acta Informatica, vol. 26,
pp. 741-748, 1989.
8. A. Sen, A. Sengupta and S. Bandyopadhyay, Generalized supercube: an incrementally expandable
interconnection network, Journal of Parallel and
Distributed Computing, vol. 13, pp. 338-344,
1991.

VII. Conclusion
In this paper we have presented some new Hamiltonian properties of hypercube and two hypercube derived networks, the incomplete hypercube and supercube. It was known that an n dimensional hypercube
has at least n! Hamiltonian cycles. In this paper we
show that the number of Hamiltonian cycles in an n
dimensional hypercube is actually much higher. We
show that an n dimensional cube has a t least 2"-3n!
distinct Hamiltonian cycles. We show that at least
2 ( n - 2)! Hamiltonian cycles can be embedded in a
faulty hypercube as long as the number of faulty edges
is a t most n - 2. We establish that in an incomplete
hypercube if the number of nodes is odd then it has
( n - 2)! Hamiltonian paths and if the number of nodes
is even, it has ( n - 2)! Hamiltonian cycles. We show
that a supercube has a t least (n - l ) ! Hamiltonian cycles and when the number of nodes N = 2 " - l + 2"-2
then the number of Hamiltonian cycles is at least as
high as 2(n - l)!.

9. A. Sen, A. Sengupta and S. Bandyopadhyay, On
fault tolerant routing in supercubes, Information
Processing Letters, vol. 42, pp. 39-46, 1992.
10. S. M. Yuan, Topological properties of supercube,
Information Processing Letters, vol. 37, pp. 241246, 1991.
11. A. Wang and R. Cypher, Fault-tolerant embeddings of rings, meshes and tori in hypercubes,
Proceedings of International Parallel and Distributed Processing Symposium, Dallas, 1992.

12. C. L. Liu, Elements of Discrete Mathematics,
McGraw Hill, 1977.

642

2011 IEEE 12th International Conference on High Performance Switching and Routing

Beyond Connectivity - New Metrics to Evaluate
Robustness of Networks
Sujogya Banerjee, Shahrzad Shirazipourazad, Pavel Ghosh, Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85281
Email: {sujogya, sshiraz1, pavel.ghosh, asen}@asu.edu

Abstract—Robustness or fault-tolerance capability of a network is an important design parameter in both wired and
wireless networks. Connectivity of a network is traditionally
considered to be the primary metric for evaluation of its faulttolerance capability. However, connectivity κ(G) (for random
faults) or region-based connectivity κR (G) (for spatially correlated
or region-based faults, where the faults are confined to a region
R) of a network G, does not provide any information about
the network state, (i.e., whether the network is connected or
not) once the number of faults exceeds κ(G) or κR (G). If the
number of faults exceeds κ(G) or κR (G), one would like to
know, (i) the number of connected components into which G
decomposes, (ii) the size of the largest connected component, (iii)
the size of the smallest connected component. In this paper, we
introduce a set of new metrics that computes these values. We
focus on one particular metric called region-based component
decomposition number (RBCDN), that measures the number of
connected components in which the network decomposes once
all the nodes of a region fail. We study the computational
complexity of finding RBCDN of a network. In addition, we
study the problem of least cost design of a network with a target
value of RBCDN. We show that the optimal design problem is
NP-complete and present an approximation algorithm with a
performance bound of O(log K + 4log n), where n denotes
the number of nodes in the graph and K denotes a target
value of RBCDN. We evaluate the performance of our algorithm
by comparing it with the performance of the optimal solution.
Experimental results demonstrate that our algorithm produces
near optimal solution in a fraction of time needed to find an
optimal solution.

I. I NTRODUCTION
The node/link connectivity of a graph is defined as the
fewest number of nodes/links whose deletion disconnects the
graph [1]. Traditionally it is used as the primary metric for
evaluation of the fault-tolerance capability of a network. If the
node/link connectivity of the graph G = (V, E) is κ(G), it can
tolerate failures of up to κ(G) − 1 nodes or links, in the sense
that the graph induced by the non-faulty nodes still remains
connected. Traditional studies in augmenting node and edge
connectivity of wired or logical networks [1]–[4] or wireless
network [5], [6] assume that the faults are random in nature,
i.e., the probability of a node or link failing is independent of
This research is supported in part by a grant from the U.S. Defense Threat
Reduction Agency under grant number HDTRA1-09-1-0032 and by a grant
from the U.S. Air Force Office of Scientific Research under grant number
FA9550-09-1-0120.

U.S. Government work not protected by U.S. copyright

its location in the deployment area. However, the assumption
of random node failure is not valid in many scenarios. This
is particularly true in military environment, where an enemy
bomb can destroy a large number of nodes confined in a
limited area. This situation is shown in Fig. 1(a) where the
shaded part indicates the fault region.
In order to address this limitation, the networking research
community over the last decade has shown considerable interest in studying localized i.e., spatially correlated or regionbased faults in various types of networks - storage networks
[7], overlay networks [8], wide area monitoring services [9],
sensor networks [10], content resiliency service networks [11]
and fiber-optics networks [12], [13]. To capture the notion
of locality in measuring the fault-tolerance capability of a
network, a new variant of connectivity called region-based
connectivity was introduced in [10]. Region-based connectivity
for multiple spatially correlated faults has been studied in [14].
The region-based connectivity of a network can be informally
defined to be the minimum number of nodes (links) that has to
fail within any region of the network before it is disconnected.
The notion of region-based faults is tied to the notion of a
region. Consider a set of nodes distributed over a geographical
area. These nodes form a network through wired or wireless
links. By network graph we imply the topological relationship
between the nodes. In addition to the network graph, we may
also have a layout of the nodes and links in the geographical
area. We refer to the layout of the nodes and links as the
network geometry. With reference to network geometry, a
region may be defined as a circular area in the network layout
covering a set of nodes and links. The shaded area in Fig. 1(a)
shows one such region.
Although the metric region-based connectivity incorporates
the notion of locality of faults, both connectivity κ(G) and
region-based connectivity κR (G) (where R is the region in
which the faults are confined) of a graph G suffers from yet
another limitation. These metrics provide information about
the network state (i.e., if the network is connected or not)
as long as the number of faults do not exceed κ(G) or
κR (G), respectively. Neither κ(G) or κR (G) provide any
information about the network state, if the number of faults
exceeds these numbers. The network state information that
we are interested in such a scenario are (i) the number of

171

Fault Region

(a)
Fig. 1.

(b)

(a) Network with a Fault Region, (b) Linear Array Network and Star Network

connected components into which G decomposes, (ii) the
size of the largest connected component, (iii) the size of the
smallest connected component. We elaborate these concepts
with the help of an example shown in Fig. 1(b). It may be
noted that the connectivity of both the linear array and the star
networks is 1. Therefore no distinction between these networks
can be made regarding their robustness (or fault-tolerance
capability) using connectivity as the metric. However, the
following observations can be made regarding the state of
these two networks after failure of one node: (i) the linear
array network can break up into at most two components and
the size of at least one component will be at least ⌊n/2⌋, (ii)
the star network can break up into (n − 1) components and
the size of these components can be as small as 1, where n
denotes the number of nodes in the network. In the unfortunate
event of a network being disconnected after a failure, it is
certainly desirable to have a few, large connected components
than a large number of small connected components. From
operational point of view, a linear array network will certainly
be preferable to a star network as it offers the possibility of
a graceful performance degradation instead of a catastrophic
failure. Unfortunately, the metric connectivity is incapable of
making any distinction between these two networks.
In order to address this limitation, we introduce three
new metrics for measuring the robustness (or fault tolerance
capability) of a network. Suppose {R1 , . . . , Rk } is the set of
all possible regions of graph G.
Definition 1: Consider a k-dimensional vector C whose i-th
entry, C[i], indicates the number of connected components in
which G decomposes when all nodes in Ri fail. Then, regionbased component decomposition number (RBCDN) of graph
G with region R is defined as αR (G) = max1≤i≤k C[i].
Although, RBCDN measures the number of connected components, it does not capture the sizes of these components.
For graceful degradation in performance, we may want that
(i) we have at least one large component, or (ii) the size of
even the smallest component is larger than a certain threshold
value. In order to capture the size aspect of the disconnected
components, we introduce two additional metrics.
Definition 2: Consider a k-dimensional vector CS (CL ) whose
i-th entry, CS [i] (CL [i]), indicates the size of the smallest
(largest) connected component in which G decomposes when
all nodes in Ri fail. Then, region-based smallest component

size (RBSCS) βR (G) and region-based largest component size
(RBLCS) γR (G) of graph G with region R is defined as
βR (G) = min CS [i] and γR (G) = min CL [i]
1≤i≤k

1≤i≤k

In order to have graceful degradation in performance, we want
to design networks with a small value of αR (G) and a high
value of βR (G) and/or γR (G). It may be noted that although
the above metrics are defined for scenarios where faults are
localized (i.e., faults confined to a region), these concepts can
easily be generalized for the scenario where the faults are not
localized. We study the following two problems in this paper.
Problem 1 (Analysis): Given the geometric layout of a graph
G = (V, E) on a 2-dimensional plane, and a region defined
as a circular area of radius r, compute the RBCDN of G.
Problem 2 (Design): Suppose that the RBCDN of G with
region R is αR (G). Suppose αR (G) is considered to be too
high for the application and it requires the RBCDN of the
network not to exceed αR (G) − K, for some integer K.
Assuming each additional link li that can be added to the
network has a weight (cost) w(i) associated with it, find the
least cost link augmentation to the network so that its RBCDN
is reduced from αR (G) to αR (G) − K.
It may be noted that the authors in [12] provide an analysis
for finding the most vulnerable region in a wired network with
respect to a few network robustness metrics. A significant
difference between the analysis presented in [12] and our
results is that, while the robustness metrics considered in
[12] are monotonic in nature (i.e., the value of the metric
either diminishes or increases as the number of faulty nodes
increases), the metrics introduced in this paper are nonmonotonic and thereby more challenging from the analysis
point of view.
The contributions of the paper are as follows:
• We introduce several new metrics to capture the network
state where the traditional metric connectivity is inadequate.
• For the geometric setting, we provide a polynomial time
algorithm for computing the metric region-based component decomposition number (RBCDN).
• We consider a network design problem with a target value
of RBCDN and show that the problem is NP-complete.
• We provide an approximation algorithm for the design
problem with guaranteed performance bound.

172

•

We evaluate the performance of our algorithms experimentally to show that they produce near-optimal solutions
in a fraction of time needed to find the optimal solution.
II. N ETWORK ROBUSTNESS A NALYSIS

In this section, we provide an algorithm that computes the
RBCDN, RBSCS and RBLCS of a graph G = (V, E) when its
layout in a plane is given as input and the region R is defined
to be a circular area with radius r. The algorithm computes
αR (G), βR (G) and γR (G) in O(n6 ) time, where |V | = n.
Specifically, the input to the algorithm are the following:
(i) a graph G = (V, E) where V = {v1 , . . . , vn } and E =
{e1 , . . . , em } are the sets of nodes and links respectively, (ii)
the layout of G on a 2-dimensional plane LG = (P, L) where
P = {p1 , . . . , pn } and L = {l1 , . . . , lm } are the sets of noncollinear points and straight lines on the two dimensional plane
(note: (a) there is a one-to-one correspondence between the
nodes and points in V and P , (b) a one-to-one correspondence
between the edges and lines in E and L, (c) each li , 1 ≤ i ≤ m
connects two points pj and pk in P , (iii) a region is defined
as a circular area R of radius r.
Given the input parameters (i), (ii) and (iii), we provide an
algorithm, prove the correctness of the algorithm and analyze
the algorithm to show that it computes αR (G), βR (G) and
γR (G) in O(n6 ) time.
A. Insight to the RBCDN/RBSCS/RBLCS Problems
First we make a few observations regarding the nature of
the RBCDN/RBSCS/RBLCS problems and then utilize these
insights to develop the algorithm. Our algorithm is valid for
both wired and wireless networks. In a wired network, a
physical link connects two nodes. If a node is destroyed due
to failure of a region, all links incident on that node are also
destroyed. However, it is possible that failure of a region
destroys a link without destroying the nodes at its end points.
In a wireless network, there is no physical link, and as such the
possibility of a fault destroying a link does not arise. There
could potentially be infinite number of circular regions that
covers the 2-dimensional plane where the nodes and links are
deployed. It may be noted that a node corresponds to a point
in this plane and a link (i.e., a straight line in the plane) and
a region (i.e., a circular area in the plane) correspond to a
set of points in the plane. We say that region R intersects
line li , if R ∩ li 6= ∅. Although there could be an infinite
number of circular regions in the plane, for the purpose of
computation of RBCDN, RBSCS and RBLCS, we only need to
consider a finite number of them. Two regions are said to be
indistinguishable if they cover the same set of links and nodes.
Otherwise, they are distinguishable or distinct. For computing
RBCDN, RBSCS and RBLCS, we only need to evaluate the
distinct regions. Since there are n nodes and m links in the
network, there could be at most 2n+m distinct regions. We will
show next that the number of distinct regions that needs to be
considered is bounded by a polynomial function of n and m.
Two indistinguishable regions are shown in Fig. 2(a). Similar

analysis has been done in [10] in order to compute regionbased connectivity for wireless network. But that analysis
will not hold for computing RBCDN, RBSCS and RBLCS in
wired network. This is because in [10] only limited number
of distinct regions are considered for computing region-based
connectivity. But here the assumption is that all the nodes
inside the region fail. So in this analysis we have to consider
all possible distinct regions. It will become clearer later in this
section.
Definition - Node Vulnerability Zone (NVZ): The circular area
of radius r centered at the location of a node in the network
layout is defined as the NVZ (see Fig. 2(b)(ii)). Any region
fault occurring in this area will destroy the node.
Definition - Link Vulnerability Zone (LVZ): Let lk be a line
of length Lk in network layout corresponding to link eij in
the graph. The rectangular area of length Lk and width 2r (as
shown in Fig. 2(b)(i)) is defined as the LVZ for this link. This
area is called LVZ because if the center of the fault region lies
within this area, it will destroy the link. It may be noted that
a link can also be destroyed if the center of the fault region
lies within the NVZ of a node on which the link is incident.
However, we do not include this area as part of LVZ, as it is
already considered as part of NVZ.
Each node and link vulnerability zone can be represented by
a set of polynomials P = {P1 , . . . , Ps } in R2 with degree(d)
at most 2 [15], [16]. Note that number of such polynomials
s is O(n + m). The arrangement arr(P) [15], [16] of the
polynomials P is the subdivision of the plane into I-points,
arcs and cells, where I-points are the intersection points of the
boundaries of node and link venerability zones [10], and the
arcs are the maximally connected portions of the boundaries
between the I-points and cells are the maximally connected
regions bounded by the arcs [15], [16]. The cells can be
described by a constant number of polynomial inequalities of
constant maximum degree d = 2 [15], [16]. In Fig. 3(a) a
region that covers 2 nodes and 2 links (at least partially) is
shown. Fig. 3(b) shows the arrangement of the vulnerability
zones of these set of nodes and the links. A cell is highlighted
in Fig. 3(b).
Definition - C-point: A C-point is an arbitrarily selected point
within a cell in arr(P).
Definition - Principal Regions: Any region centered at a Cpoint will be referred to as a Principal Region (Fig. 3(c)).
Note that Principal Region defined in [10] is a region
centered at an I-point. Considering the regions centered only
at the I-points will not include all distinct regions. For example
consider a network with two nodes n1 and n2 . Let the NVZs of
these nodes intersect at two points I1 and I2 . Regions centered
at I1 and I2 will cover both the nodes. But if we consider
principal regions to be centered at the C-point corresponding
to the three cells made by the NVZs then there will be 3
Principal regions - one covering n1 , one covering n2 and other
covering both the nodes. So Principal regions considered in
this paper is a superset of the Principal regions considered in
[10]. Also Principal regions centered at the C-points is the

173

(a)
Fig. 2.

(b)

(a) Region 1 and Region 2 are indistinguishable, (b) Vulnerability zone of (i) a link and (ii) a node

Fig. 3.

A Region, arrangements of vulnerability zones and a Principal Region

actual set of all possible distinct regions in the network. The
previous definition of Principal Region in [10] worked because
only limited number of distinct regions needed to be examined
for computing region-based connectivity.
Observation 1: Given a region R, the intersection area of the
vulnerability zones of the nodes and links within region R is
non-empty [10].
Observation 2: If a region R covers a set of nodes and links
and R is not centered at one of the C-points, there must be
at least one other region centered at one of the C-points that
covers all the nodes and links covered by R. Accordingly this
region will be indistinguishable from R. The proof follows the
proof of Observation 2 in [10].
Observation 3: For computing the RBCDN/RBSCS/RBLCS of
the network graph G where the layout LG of G is given as
input, only a limited number of distinct regions, i.e., only the
Principal Regions need to be examined [10].
Observation 4: The maximum number of Principal Regions is
O(n4 ).
Proof: By definition, a Principal Region is a region centered
at a C-point and number of C-points is equal to the number
of cells in the arrangement arr(P). As the maximum degree
of the set of polynomials P = {P1 , . . . , Ps } defined over R2
is 2 [15], [16], the number of cells in arr(P) is O(s2 ). Since

s is O(n + m), there can be at most O((n + m)2 ) or O(n4 )
Principal Regions.
Observation 5: All the C-points can be computed in O(n6 )
time.
Proof: Using the results presented in [15], [16], we can
compute a set of points C such that each cell in arr(P)
contains at least one point from C, in time O(s3 ) (or O(m3 )).
As a consequence the overall time-complexity to compute all
the C-points is O(m3 ) (or O(n6 )).

B. The RBCDN/RBSCS/RBLCS Algorithm
From the observations in the previous subsection, it is clear
that we need to examine only the Principal Regions (i.e.,
the regions whose centers are at C-points) to compute the
RBCDN/RBSCS/RBLCS of a graph G (with a layout LG on a
2-dimensional plane) and a circular region R of radius r. Since
there are only O(n4 ) of such regions, we can develop a polynomial time algorithm to compute RBCDN/RBSCS/RBLCS.
Algorithm 1 computes RBCDN, RBSCS and RBLCS of a
network G(V, E) with circular region R with radius r.

174

Algorithm 1 Computing RBCDN, RBSCS and RBLCS of a
network graph G = (V, E) with region R
1: Input: G = (V, E), Graph layout LG = (P, L) and r
2: Output: αR (G), βR (G), γR (G)
3: Step 1: Find the set of C-points using the algorithm sketched in
Observation 5
4: Step 2: For each C-point cj , find G′j = (Vj′ , Ej′ ) a subgraph of
G formed by removing the nodes and edges covered by region
Rj centered at cj .
5: Step 3: For each such graph G′j = (Vj′ , Ej′ ) find the largest
connected component LCj , smallest connected component SCj
and number of connected component CNj using depth-first
search [17]. Let LCSj be the size of LCj and SCSj be the
size of SCj for graph G′j = (Vj′ , Ej′ ).
6: Step 4: αR (G) = maxj CNj , γR (G) = minj LCSj and
βR (G) = minj SCSj

Theorem 1: The complexity of the Algorithm 1 is O(n6 ).
Proof: As noted in Observation 5, time complexity of finding
all the C-points is of O(n6 ), where n is number of nodes.
So, time complexity of Step 1 is O(n6 ). In Step 2, we have
to check all edges in E and all nodes in V if they have
intersections with the fault region Rj . So, time complexity
of Step 2 is O(n2 ). Step 3 uses depth-first search to compute
the number of connected components and has complexity of
O(| V | + | E |) = O(n2 ). Step 2 and 3 is repeated O(n4 ) of
times. Therefore, the total time complexity of the Algorithm
is O(n6 ).
III. ROBUST N ETWORK D ESIGN
In the previous section we presented a polynomial time
algorithm to compute the RBCDN of a graph G = (V, E),
when its layout LG = (P, L) on a 2-dimensional plane and
the radius r of a circular region R is provided as input. In this
section, we study a complementary problem, where the goal is
to have least cost augmentation of an existing network, so that
it attains a specific target value of RBCDN. Formal description
of the decision version of this problem is given below.
RBCDN Reduction Problem (RBCDN-RP)
INSTANCE: Given
(i) a graph G = (V, E) where V = {v1 , . . . , vn } and E =
{e1 , . . . , em } are the sets of nodes and links respectively,
(ii) the layout of G on a two dimensional plane LG = (P, L)
where P = {p1 , . . . , pn } and L = {l1 , . . . , lm } are the sets of
points and lines on the 2-dimensional plane,
(iii) region R defined to be a circular area of radius r,
(iv) weight (cost) function w(e) ∈ Z+ , ∀e ∈ Ē, where Ē is
complement of the link set E,
(v) integers W and K (K ≤ αR (G), where αR (G) is the
RBCDN of G).
QUESTION: Is it possible to reduce the RBCDN of G by K
by adding edges to G (from the set Ē) so that the total weight
(cost) of the added links is at most W ?
We can prove that RBCDN-RP is NP-complete by a transformation from the Hamiltonian Cycle in Planar
Graph Problem (HCPGP) which is known to be NPcomplete [18]. Due to space limitation we omit the proof here.
Interested readers are referred to [19] for the proof.

A. Approximation Algorithms for the RBCDN-RP Problem
We propose an approximation algorithm for RBCDN-RP
problem with an approximation factor of O(ln(K)+4 ln(n))),
where n is the number of nodes in the network graph G =
(V, E) and K is an integer by which RBCDN αR (G) of
G has to be reduced. We formulate the problem as follows:
Let G′i be the subgraph induced from G by removing the
links and the nodes intersecting with the region Ri and
CN (i) be the number of connected components in graph
G′i . For each region Ri if CN (i) > αR (G) − K then at
least ki = CN (i) − (αR (G) − K) links should be added to
G′i to reduce CN (i) to αR (G) − K. Let l be the number
of the regions where if fault strikes and the nodes of the
region become inoperative, the graph decomposes to more than
αR (G) − K components.
As such only these l regions have to be considered for
decreasing the RBCDN of G from αR (G) to αR (G)−K. Let
PE i be the set of the potential links that can be added between
the connected components of G′i to decrease its CN (i). The
potential links of G′i are defined as the links ∈
/ E whose
corresponding lines in the layout of the network do not have
any intersection with region Ri . We partition the links in PE i
into disjoint subsets PE ij , 1 ≤ j ≤ d(i), where each subset
is non-empty and includes only the links between a pair of
connected components of G′i . The number d(i) indicates the
′
number of such disjoint subsets
 of PE i for Gi . The maximum
CN (i)
value of d(i) can be
.
2
In addition to the notations introduced in section II-B we
use the following notations in our algorithms.
K: The integer by which αR (G) of graph G with region R
should be reduced.
ki : The integer by which CN (i) of G′i should be reduced.
l: The number of regions Ri for which graph G′i has CN (i) >
αR (G) − K.
PE i : The set of the potential links that can be added between
the connected components of G′i to decrease its CN (i).
Sl
E = i=1 PE i ;
PE ij : PE i,j ⊆ PE i .
It may be noted that more than one link e ∈ PE ij can connect
the same pair of components in G′i .
d(i): The number of subsets PE ij present in PE i .
Sd(i)
It should be noted j=1 PE ij = PE i and ∀j, k, with j 6=
k, PE ij ∩ PE ik = ∅.
Input and output for the algorithm are as follows:
Input:
1) A set of all potential links E = {e′1 , . . . , e′m′ },
2) a weight (cost) function w(e) associated with link e ∈ E,
3) S = {PE 1 , . . . , PE l }, PE i ⊆ E,
4) a partition of each PE i into d(i) subsets, 1 ≤ i ≤ l,
5) a number ki associated with each PE i , 1 ≤ i ≤ l.
P
Output: E ′ ⊆ E such that e∈E ′ w(e) is minimum and E ′
hits at least ki subsets of the partition of PE i
Greedy Algorithm (GA) for RBCDN-RP
In the following we describe the approximation algorithm for
RBCDN-RP. In each iteration, the algorithm chooses the most

175

cost effective potential link. It may be noted that a potential
link may appear in more than one PE i . The number of times
a potential link e appears in different PE i ∈ S is called its hit
number for that iteration and is denoted by He . The potential
link whose weight (cost) (w(e)) to hit number (He ) ratio is
the smallest is considered to be the most cost effective link.
If the link et , chosen in iteration t, has a hit in set PE i then
adding this link in graph G will reduce the CN (i) of G′i by 1.
Therefore, ki is decreased by 1. Adding two edges ep and eq
from same subset PE ij ⊆ PE i will not decrease the number
of components of G′i more than 1. As such, the subset PE ij
of PE i which contains et is removed in iteration t. When G′i
attains its desired component number (i.e., ki = 0), it is not
considered any more and PE i is removed from S.
Algorithm 2 Greedy Algorithm (GA) for RBCDN-RP
1: E ′ ← ∅
2: while S 6= ∅ do
3:
Compute T = {H1 , . . . , Hm′ } such that ∀1 ≤ t ≤ m′ Ht is

the number of PE i s, PE i ∈ S that contains link et ∈ E

4:
Pick et ∈ E such that w(et )/Ht is minimum
5:
E ′ ← E ′ ∪ et
6:
for all PE i such that et ∈ PE i do
7:
PE i ← PE i \ PE ij , such that, et ∈ PE ij
8:
ki ← ki − 1
9:
if ki = 0 then
10:
S ← S \ PE i
11:
end if
12:
end for
13: end while

Theorem 2: The time complexity of GA is O(Kn6 ).
Proof: The inner f or-loop in GA runs for at most l times.
In each iteration of outer while-loop one link is selected and
removed from all PE i . So outer while-loop will only run at
most m′ times. So the complexity of GA2 will be O(lm′ ).
Since in the worst case l is O(n4 ) and m′ is O(n2 ), the
complexity of GA is O(n6 ).
Theorem 3: Approximate solution produced by GA is at
most O(ln(K) + 4 ln(n)) times the optimal solution.
Proof: The subset E ′ ⊆ E is chosen in such a way that E ′
hits at least ki subsets
Pl in the partition of PE i ∀1 ≤ i ≤ l.
So altogether λ = i=1 ki subsets are hit by E ′ . Let us order
the subsets of each PE i , i.e., [PE ij , 1 ≤ i ≤ l, 1 ≤ j ≤ ki ]
in the order in which they were hit by the elements of E ′
in the algorithm, resolving ties arbitrarily. Let the ordering
be S ′ = {PE ′ 1 , PE ′ 2 , . . . , PE ′ λ } where each PE ′i is some
PE ij , 1 ≤ i ≤ l, 1 ≤ j ≤ ki .
We assign a price for each subset PE ′ i ∀1 ≤ i ≤ λ such that
if PE ′ i is hit by link et in some iteration the price(PE ′ i ) =
w(et )
where Ht is the number of subsets hit by et in that
Ht
iteration. Let OP T be the total cost of the optimal solution.
So at any iteration the leftover edges of the optimal solution set
will hit the remaining sets of S ′ at a cost of at most OP T . In
the worst case PE ′ i will be hit by an edge in the i-th iteration.
In i-th iteration at least λ − i + 1 elements remain in S ′ . So
the value of Ht in i-th iteration cannot be greater λ − i + 1.
Since PE ′ i will be hit by the most cost-effective link in this

OP T
iteration, price(PE ′ i ) ≤ λ−i+1
. The cost of each link picked
is distributed among the
new
subsets
covered,
the total cost
P
Pλ
′
of the link picked is
w(e
)
=
price(PE
′
t
i) ≤
et ∈E
i=1

1
1
1 + 2 + . . . + λ OP T = O ln(λ) OP T . Also, λ ≤ K × l
and l is of the order of O(n4 ). So the approximation ratio for
GA is O(ln(Kn4 )).

IV. E XPERIMENTAL R ESULTS
In this section we compare the results of the approximation
algorithm with the optimal solution found using ILP for 10
random instances of network layout in a 2-dimensional plane.
The number of nodes, in these instances are varied from 10
to 90 in a step of 10. In every instance the node locations are
uniformly and randomly distributed on a square deployment
area of side length 100 units. We followed Erdos-Renyi model
of random graphs for our simulation. The probability of having
a link between two nodes was chosen to be 0.3, so that the
resulting graph do not become too dense or too sparse. For all
the simulation experiments, the fault region was considered to
be a circular area of radius 25 units.
For every problem instance, we first compute the set of Cpoints C = {C1 , . . . , CT } and then using Algorithm 1, we
compute the RBCDN . Also, ∀Ci ∈ C we find G′i , CN (i),
PE i and d(i) (please refer to section 2.1 for the notations).
For each instance we execute the algorithm for values of K
(number by which RBCDN needs to be reduced) as 1 and 2.
For this case study, the results of the experiments are shown
in Fig. 4. We plot the ratio of the cost of the solutions of the
greedy algorithm to the optimal cost for different values of n
and K. It can be seen that in most of the cases, the ratio is
1, indicating that the greedy algorithm produces the optimal
solution. In other cases the ratio is close to 1, indicating
the greedy algorithm produces a near optimal solution. In all
of these cases the greedy algorithm takes a fraction of time
required to find the optimal solution. It may be noted that
for those instances of RBCDN-RP, where ∃Ci ∈ C such that
ki > d(i), there exists no feasible solution. The instances with
no feasible solutions can be seen as blank columns in the plots
of Fig. 4.
V. C ONCLUSION
We define three new metrics RBCDN, RBSCS and RBLCS
to overcome some of the limitations of connectivity, the
traditional metric of fault-tolerance in a network. We present
a polynomial time algorithm to compute RBCDN, RBSCS and
RBLCS, when the layout of the graph on a two dimensional
plane is given as input. We prove that the least cost network
design problem to achieve a target value of RBCDN is NPcomplete. We provide an approximation algorithm for design
problem with guaranteed performance bound. Experimental
evaluation of the approximation algorithms shows that they
almost always produce near optimal solution.

176

R EFERENCES
[1] K. Eswaran and R. Tarjan, “Augmentation problems,” SIAM Journal on
Computing, vol. 5, p. 653, 1976.

Greedy Algorithm/Optimal

Ratio of greedy Algorithm
Solutions to Optimal

Ratio of greedy Algorithm
Solutions to Optimal

1.4 k=1
1.2
1
0.8
0.6
0.4
0.2
0

10

20

30

40

50

60

70

80

1.4 k=2
1.2
1
0.8
0.6
0.4
0.2
0

90

Number of nodes

Greedy Algorithm/Optimal

10

20

30

40

50

60

70

80

90

Number of Nodes

(a) K = 1

(b) K = 2

Fig. 4. Comparison of the Solution of the Greedy Algorithms with the Optimal. The greedy algorithms are compared by taking their ratios to the optimal. On
the y-axis, ratio value 1 indicates that the greedy algorithm achieves optimal solution. Blank columns indicate the infeasibility of the corresponding instances.

[2] G. Frederickson and J. JaJa, “Approximation algorithms for several
graph augmentation problems,” SIAM Journal on Computing, 1981.
[3] T. Watanabe and A. Nakamura, “Edge-connectivity augmentation problems,” Journal of Computer and System Sciences, vol. 35, 1987.
[4] J. Cheriyan and R. Thurimella, “Fast algorithms for k-shredders and
k-node connectivity augmentation,” in Proceedings of the 28th annual
ACM STOC, 1996.
[5] M. Penrose, “On k-connectivity for a geometric random graph,” Random
structures and Algorithms, 1999.
[6] C. Bettstetter, “On the connectivity of ad hoc networks,” The Computer
Journal, vol. 47, no. 4, p. 432, 2004.
[7] M. Bakkaloglu, J. J. Wylie, C. Wang, and G. R. Ganger, “On correlated
failures in survivable storage systems,” Carnegie Mellon University,
Tech. Rep. CMU-CS- 02-129, 2002.
[8] W. Cui, I. Stoica, R. H. Katz, and Y. H. Katz, “Backup path allocation
based on a correlated link failure probability model in overlay networks,”
in 10th IEEE ICNP, 2002.
[9] S. Nath, H. Yu, P. B. Gibbons, and S. Seshan, “Tolerating correlated
failures in wide-area monitoring services,” Intel Corporation, Tech. Rep.
IRP-TR-04-09, 2004.
[10] A. Sen, B. Shen, L. Zhou, and B. Hao, “Fault-tolerance in sensor
networks: A new evaluation metric,” in Proc. IEEE INFOCOM, 2006.
[11] J. Fan, T. Chang, D. Pendarakis, and Z. Liu, “Cost-effective con-

[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]

177

figuration of content resiliency services under correlated failures,” in
Proceedings of the International Conference on DSN, 2006.
S. Neumayer, G. Zussman, R. Cohen, and E. Modiano, “Assessing the
vulnerability of the fiber infrastructure to disasters,” in Proc. IEEE
INFOCOM, 2009.
S. Neumayer and E. Modiano, “Network reliability with geographically
correlated failures,” in Proc. IEEE INFOCOM, 2010.
A. Sen, S. Murthy, and S. Banerjee, “Region-based connectivity-a new
paradigm for design of fault-tolerant networks,” in Proceedings of IEEE
HPSR, 2009, 2009, pp. 1–7.
A. Vigneron, “Geometric optimization and sums of algebraic functions,”
in Proceedings of the Twenty-First Annual ACM-SIAM SODA, 2010.
S. Basu, R. Pollack, and M. Roy, “On Computing a Set of Points Meeting
Every Cell Defined by a Family of Polynomials on a Variety* 1,” Journal
of Complexity, vol. 13, no. 1, pp. 28–37, 1997.
J. Hopcroft and R. Tarjan, “Efficient algorithms for graph manipulation,”
1971.
M. Garey and D. Johnson, Computers and intractability. A guide to the
theory of NP-completeness., 1979.
S. Banerjee, S. Shirazipourazad, P. Ghosh, and A. Sen. NPcompleteness Proof: RBCDN Reduction Problem. [Online]. Available:
http://arxiv.org/abs/1012.2142v1

A Novel Mechanism to Dynamically Switch Speed and
Accuracy in SystemC based Transaction Level Models
Zhu Zhou

Dharmin Parikh

Pradnyesh Gudadhe

Arunabha Sen

Intel Corporation
5000 W Chandler Blvd
Chandler, AZ-85226
+ 001 480 552 2753

Intel Corporation
5000 W Chandler Blvd
Chandler, AZ-85226
+ 001 480 552 0933

Arizona State University
910 E Lemon St, #6
Tempe, AZ-85281
+ 001 480 406 4363

Arizona State University
CSE Department, #5406
Tempe, AZ-85287
+ 001 480 965 6153

zhu.zhou@intel.com

dharmin.y.parikh@intel.com

pgudadhe@asu.edu

asen@asu.edu

architecture analysis. Since performance models implement detailed
micro-architecture features, they run much slower than their
functional counterparts.

ABSTRACT
OSCI’s TLM-2.0 standard enables the simulation of functionality
and timing of a system by defining two coding styles namely
Loosely-timed (LT) and approximately timed (AT). Without
dynamic switching between the two modes, a user interested in
performance analysis is forced to execute the model in AT mode for
the entire duration of simulation. A run-time switching mechanism
enables user to execute uninteresting simulation portions (e.g.
operating system boot) in the high speed LT mode and switch to
detailed AT model only when one needs to carry out detailed microarchitectural analysis (e.g. benchmark execution). In this paper, we
introduce a comprehensive switching mechanism that addresses all
the potential issues during LT-to-AT and AT-to-LT transitions. We
test this switching methodology on one Intel proprietary
Interconnect Bus model and demonstrate a ~24X speedup over ATonly simulations.

While TLM-2.0 enables LT and AT implementations to coexist in
the same model, it does not specify a method to dynamically switch
execution from LT to AT mode and vice versa. Hence, a user
interested in conducting performance analysis of a given system
component has to run entire simulation in AT mode. With
increasing complexity of SoC designs, AT simulations tend to run
very slow and thus limit the scope and efficiency of performance
analysis. As an example, to run benchmarks on an Intel processor
based SoC model, one would first need boot operating system; but
doing so in AT mode would take several hours. Instead, a dynamic
switching mechanism could allow one to quickly boot operating
system using model’s LT mode and then switch to AT mode before
executing the benchmark. In addition, simulations can also “fast
forward” any uninteresting phases (i.e. switch back to LT mode) to
get to future time period if needed. In this paper, we introduce a
comprehensive switching mechanism that enables the end user to
dynamically switch between the LT and AT modes, and ultimately
improve turn-around time for performance analysis.

Categories and Subject Descriptors
I.6.5 [Model Development]: Modeling methodologies.

General Terms
Performance modeling, Functional modeling, Architectural analysis,
Simulation infrastructure, Dynamic switching.

1.1 Related Work
Models of different abstraction levels are efficiently developed in
timed and un-timed manner in [2]. Performance modeling and
power modeling of SoCs has been presented with a new multiaccuracy Transaction Level Modeling approach, which utilizes the
concept of dynamically switching abstraction levels. This approach
uses the concept of multilevel channel to achieve the switching
ability. The methodology is not generic and lacks interoperability.
Dynamic switching of Hardware Abstraction Models is achieved
using shared data structure in [3], but has some limitations and lacks
interoperability.

1. INTRODUCTION
Simulation based architectural exploration and early software
development have become critical with increasing complexity in
SoC designs and shortening time to market window. SystemC, the
IEEE-1666 standard, has become a popular modeling language.
SystemC based TLM-2.0 package [1] standardizes communication
interfaces and data structures to facilitate modeling of interoperable
chip components for specification, simulation, verification,
implementation, and evaluation purposes. In addition, the standard
defines two coding styles: loosely timed (LT) for functional
simulation and approximately timed (AT) for architectural design.
Functional models are used to determine functional correctness of a
system. Performance models emphasize timing accuracy for micro-

1.2 Paper Organization
This paper is organized as follows: Section 2 describes the LT and
AT coding styles supported by SystemC based TLM-2.0 standard.
Section 3 identifies some potential issues for the dynamic switching.
Section 4 provides a detailed description of the novel LT/AT
switching mechanism. Section 5 analyzes data obtained by running
the simulation with the switching mechanism enabled on one
proprietary interconnect Model. Section 6 concludes with future
work targeted towards improving the switching mechanism.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
GLVLSI’09, May 10–12, 2009, Boston, Massachusetts, USA.
Copyright 2009 ACM 978-1-60558-522-2/09/05…$5.00.

405

2. SYSTEMC TLM-2.0 MODELS

Master 1

Loosely-timed (LT) models have a loose dependency between
timing and data, and are able to provide timing information and
requested data whenever a transaction is initiated. Normally,
resource contention and arbitration are not modeled using this style.
Due to the limited dependencies and minimal context switches,
these models can run very fast and are particularly useful for doing
software development on a Virtual Platform. Reaching simulation
speeds of 1-50M Transactions per second allows software
developers to boot an OS and run test code in seconds.

Interconnect

Figure 1: Simplified SoC Scenario with Central Controller
Several masters may access the same target memory range. To
guarantee data safety, all the masters need to flush out all the
outstanding transactions before switching to LT.

4. SWITCHING MECHANISM
We define a central controller and the communication protocols that
handle multiple run-time switches between LT and AT modes in
SystemC TLM-2.0 based simulation system. Such a dynamic
switching mechanism ensures system and data integrity during the
switches.

Switching from LT to AT is relatively simple as blocking transport
call finishes once it returns. The switch can be done when all
masters complete their blocking functions. On the other hand AT
style normally models several phases for each transaction. It is
likely at any simulation time point, there are multiple outstanding
transactions issued from any master that reside at different phases.
To maintain data integrity, the switching mechanism has to wait for
all the outstanding transactions to complete before starting the new
transaction in LT mode. Figure-1 below shows a simplified SoC
scenario which includes several masters, targets, bridges connected
by an interconnect bus. Within the system context, there are few
challenges that need to be addressed:

•

Each master can have multiple outstanding transactions when
running at AT mode. Those outstanding transactions reside at
different phases. For AT to LT switch, each master has to
complete all issued transactions to avoid data corruption.

•

Bridge shown in the diagram receives transactions master1
and/or master2 initiated. In handling those transactions, bridge
may generate new transactions targeting another component
(Target2 in this case). Hence, a bridge also needs to be treated
as a master.

Target 2

Central Controller

3. DYNAMIC SWITCHING CHALLENGES

Masters are asynchronous in terms of injecting traffic into the
system.

Bridge

Master 2

Approximately-timed (AT) models have stronger dependency
between timing and data. Each transaction normally goes through a
sequence of phases mimicking hardware protocol for timing
accuracy. Resource contention and arbitration can be modeled
easily with this style. Since these models must synchronize/order
the transactions before processing them, they are forced to trigger
multiple context switches in the simulation, resulting in performance
penalties. AT style models generally achieve the speed of 50 K –
100 K transactions per second.

•

Target 1

Central controller accepts the switching command user enters during
the simulation. All master components that initiate transactions
register themselves to central controller before the simulation starts.
Communication protocols (described below) are defined between
controller and all the masters to make sure that the system reaches a
stable state before switching.

4.1 AT to LT switching
Figure-2 depicts a flow chart of AT-to-LT switch process. When a
user thread or any modeling entity requests a mode switch (step-1),
the central controller triggers switching mechanism by sending a
mode switch notification to every registered unit (step-2).
When the central controller notifies masters of the mode-switch
request (AT to LT in this case), every master first stops issuing any
new AT transactions (step-3). Masters also allow the outstanding
transactions to proceed through their phases in AT mode. During the
AT mode, each master (master1, master 2 and bridge as shown in
the diagram) usually has a queue that tracks all the outstanding
transactions. Each master pushes every transaction it creates into the
tracking queue. When the transaction finishes execution, the
originating master is notified of the completion event. As a result,
the master removes transaction’s corresponding score-boarded entry
from the tracking queue (step-4).
Eventually all the outstanding transactions complete in AT mode
and the tracking queues are emptied in the masters. The Central
controller polls each master periodically (the inter-polling interval is
configured) until all the masters report empty tracking queue (step5). Only when all masters (bridge is registered as master too since it
initiates new transactions) are clean, the controller issues “switch”
command to let masters start new transactions in LT mode (step-7).

Similar to hardware behavior that normally is modeled in AT style,
masters need to track the outstanding transactions (e.g. push it into a
queue at start time, pop it out once the whole transaction is done).
The transaction can be completed out of order. Hence, some match
mechanism needs to be in place (e.g. unique transaction ID). They
need to be notified when the transaction is done to make sure that
any expected actions (e.g. memory read, write) have actually taken
place before the transaction gets removed from the queue.

The Central controller uses polling approach (controller periodically
checks the tracking queue status of every master) instead of a push
approach (each master notifying controller when it is clean). In
every round of polling, if any queue in any master is not empty, the
controller stops polling and re-polls all masters after the
programmable interval period (step-6).

406

It must be noted that LT-to-AT protocol implements push approach
(master sends clean notification to controller) rather than the pull
approach used in AT-to-LT switching mechanism.

This polling mechanism addresses a potential problem faced in the
push approach. In Figure-1, the bridge (registered as a master) may
notify the controller its cleanness even when master1 and/or master2
have a few outstanding transactions. Since master1 and/or master2
are not clean yet, the on-going transaction originated from
master1/master2 may cause bridge to generate a new transaction
even after the bridge indicates its cleanness to controller. In a push
approach, the central controller would have already marked the
bridge clean and would not detect generation of new transactions. In
a complicated system, to identify such causality and enforce the
sequence of checking the cleanness of each master is impractical.
The polling approach solves this problem. When all the masters in
the simulation model are clean, the entire system is empty and ready
to be switched into LT mode.
8A – AT to LT
Done. Start LT

8A – LT to AT
Done. Start AT

Master-A

4A – Master
Clean

Master-Z
4A – Continue
finish Trans
from queue

Master-Z

3A – Complete
current Trans

8Z – AT to LT
Done. Start LT

Master-A

8Z – LT to AT
Done. Start AT

2A – Notify

7A – Start AT

3Z – Complete
current Trans

4Z – Master
Clean
7Z – Start AT
2Z – Notify

4Z – Continue
finish Trans
from queue

3A –stop issuing
AT Trans

Master – A

3Z –stop issuing
AT Trans

………
Master – Z

5A – poll queue

Not Clean

5 – Update
LT Table

Clean

Controller

5Z – poll queue

2A – Notify

2Z – Notify

6 – Check if
entire table is
clean

7Z – Start LT

1 – LT to AT
switch request

7A – Start LT

Figure 3: LT to AT Switching Protocol

6 – If any non-empty master queue
detected, re-poll (5) all masters after interpolling period. Stop when all masters are
clean.

5. SIMULATION RESULTS
The proposed methodology has been tested on the proprietary
interconnect model. The interconnect model is connected to various
masters and targets in the SoC. Components like PCIe, multi-core
multi-threaded engines can play the role of masters connected to the
bus whereas components like PCIe, DRAM controller, and scratch
memory can play the role of targets. The bus has three independent
channels. Also split-transaction and pipelining are used to improve
the throughput.

Controller

1 – AT to LT
switch request

Figure 2: AT to LT Switching Protocol

The test platform has two masters and two targets connecting
through the bus model. The test cases include 100% read, 100%
write and 50% read-50% write operations. The simulation speed of
the model has been monitored for all tests. We have marked average
of the values obtained for all three test cases. As shown in the graph
below in figure 4, the test system ran in LT mode first. The average
simulation speed during LT mode has been observed to be
1,143,500 transactions per second. At one point, model got switched
from LT to AT mode. Model runs in AT mode for a period of time.
The average simulation speed observed during AT mode is 45,737
transactions per second. As seen the figure below, the simulation
speed has been dropped by approximately 25X as AT mode
contains much more architectural details. The simulation switches
back to LT mode later on and regains the simulation speed with
average 1,129,500 transactions per second.

4.2 LT to AT switching
Figure-3 depicts a flow chart of the LT-to-AT switch process. When
a user thread or any modeling entity requests a mode switch (step1), the central controller triggers switching mechanism by sending a
mode switch notification to every registered unit (step-2). If a
master is executing a transaction in LT mode when LT-to-AT
switch occurs, the master ensures completion of the transaction and
then notifies the central controller (step-3 & step-4). On receiving
clean notification, the central controller updates a table that tracks
cleanness of each master (step-5). Only when all masters are clean
(step-6), the central controller broadcasts “start AT” message (step7). At this point, all the masters can start execution in AT mode
(step-8). 1. This mechanism ensures that a clean master does not
start issuing AT transactions until all masters complete execution in
LT mode.

407

Simulation Speed with Switching Mechanism on models with variable number of
masters and targets
900000
800000

Transactions/sec

700000
600000
500000
400000
300000
200000
100000
0
0

39

46

95

120

180

time (secs)
4 Masters, 4 Targets

Figure 4: Simulation speed of test platform with LT <-> AT
switching enabled.

6. CONCLUSIONS
Dynamic switching capability enables architects to optimize microarchitecture using realistic workload. Traditionally there are two
independent modeling infrastructures, one for software development
while the other for architectural exploration. The attempt to combine
both to leverage real traffic generated by the software has been
either too complicated to implement or too slow to comprehend all
the test cases. TLM-2.0 provides a mechanism to encapsulate both
blocking and non-blocking interfaces into one single socket so that
user can implement models with both LT and AT styles. However,
there are several challenges given the intrinsic different nature of LT
and AT coding style.

Table 1: Speedup for variable number of master and targets

Speedup

2 Master, 2 Targets

24.9X

3 Master, 3 Targets

22.1X

4 Master, 4 Targets

23.7X

2 Masters, 2 Targets

Figure 5: Simulation speed with switching mechanism on
models with different number of masters and targets

As seen in the figure 4, the pattern obtained from all the three test
cases is approximately similar. One observation is that the switch
latency while switching from AT to LT is relatively longer than that
from LT to AT switching. When switching from LT to AT mode,
each master (and bridge) can have at most 1 blocking transaction
running in the system. The controller waits till all the masters finish
their blocking transactions and then switches to the AT mode. On
the other hand, the masters (and bridges) can have numerous nonblocking transactions in different phases while switching from AT
to LT mode. The controller waits until all these non-blocking
transactions are finished and then switches to the LT mode.
Therefore the transition period is longer when switching from AT to
LT.

Number of Masters and Targets

3 Masters, 3 Targets

This paper proposed a generic switching mechanism based on the
latest SystemC TLM-2.0 standard. The potential problems during
the transition from LT to AT and AT to LT are addressed by
introducing a central controller and elaborated communication
protocols between the controller and all the masters. The prototype
on a real project proves the robust of the protocol. This specific bus
protocol shows approximately 24x speedup when switching from
AT to LT. More speedup is expected for complicated systems where
more detailed micro-architecture modeling makes running in AT
mode much slower than running in LT mode. The switching
mechanism gives the users the freedom to fast forward to any
interesting period in LT mode before switching to AT for detailed
architectural study. The implementation overhead on each master
module is minimal and a generic SystemC TLM-2.0 based C++
library has been released for reference.

Number of masters and targets has been increased to test the
scalability of the switching mechanism. Figure 5 shows switches
from LT to AT and back to LT with varying number of components
(masters and targets) in the system. Adding more components
obviously slows down the simulation speed as more threads need
more computing resources and the bus congestion increases.
However, the speedup from AT to LT remains the same as shown in
the table 1 above.

7. REFERENCES
[1] OSCI TLM-2.0 User Manual, Open SystemC Initiative
(OSCI), Version JA22, June 2008
[2] Jerome Cornet, Florence Maraninchi, Laurent Mailet-Contoz.
A Method for the Efficient Development of Timed and
Untimed Transaction-Level Models of System-on-Chip.
[3] Pin-Hsien Lu. Dynamically Switching Between Hardware
Abstraction Models for Rapid Embedded Software
Development. Master degree thesis submitted to National
Chung Cheng University, August 2006.

408

A memory-efficient scheme for Address Lookup
using Compact Prefix Tries
Anand Sarda and Arunabha Sen
Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ 85287
Email: {asarda, asen}@asu.edu

Abstract— In this paper we present a new memory-efficient
scheme for address lookup that exploits the caching support
provided by general-purpose processors. We propose Compact
Prefix Tries, in which prefixes occurring at multiple levels of a
subtrie are compressed into a single node that fits in a single
cache line. The scheme performs well in compressing dense as
well as sparse tries. For an IP core router (Mae-West) database
with 93354 prefixes, the simulation results for Compact Prefix
Tries show up to 70% improvement in lookup performance and
up to 33% reduction in memory when compared with LC-T ries.
In fact, the entire forwarding table for Mae-West required only
829 KB space. Measurements for Compact Prefix Tries, when
compared with most existing schemes, show better results in
terms of memory usage as well as lookup speeds. Moreover, as
the memory usage is significantly less and sparse tries with long
paths can be compressed into only a few nodes, this scheme is
particularly attractive for IPv6.

aggregated into a single 2k -ary branching node. This scheme
maintains a good balance of memory usage, search speed and
update times. Another trie compression scheme in hardware is
presented in [4]. Multibit tries are also used for compressing
levels in a trie. Multibit tries [2] [14] [1] speedup the lookup
speed of tries by inspecting many bits simultaneously. A
technique for expanding and compressing multibit tries is
presented in [2]. The Lulea scheme [3] compresses multibit
trie nodes to reduce storage to fit in the cache. In the worstcase, O(W) memory accesses are required, but these accesses
are to fast cache memory. Controlled Prefix Expansion [14]
optimizes the multibit trie using dynamic programming. The
scheme reduces memory, improves performance and the authors claim it to be very tunable.

I. IP ADDRESS LOOKUP PROBLEM

III. C OMPACT P REFIX T RIES

The primary role of a router is to route the packet to its
destination. In order to do so, for each packet it receives, the
router must determine the address of the next hop where it
should be forwarded. Router maintains a table called forwarding table that stores the forwarding information. Each entry in
the routing table has a network address, length and an output
port identifier or next hop address. The pair of address and
its length is called as a prefix. When a packet is received, the
router extracts the destination address from the packet header.
It is then matched with the prefixes in the routing table using
some lookup algorithm to find the next hop address. This
operation is called as address lookup. Since the prefixes are of
different lengths in the router tables, multiple prefixes match
a given address. So, in order to find the next hop address for
the destination address, the router has to find the most specific
prefix or the longest matching prefix. The router then forwards
the packet from incoming port to corresponding outgoing port.
This is called as switching.

This section will present the new scheme designed for
compressing tries for address lookups. While designing the
data structure for this scheme, the primary goals were to
reduce the memory required and memory accesses. Both these
goals go hand in hand. Reducing the number of memory
accesses is important because they are relatively slow as
compared to processor speeds and are usually the bottleneck
of lookup procedures. Reducing the size of data structure
allows the data structure to fit entirely in the cache memory.
This means that accessing the data will be extremely fast as
compared to accessing it from relatively slow main memory
(DRAM/SDRAM).
Even if the entire forwarding table dose not fit into cache, it
is beneficial to group correlated information together, so that a
large fraction of it will reside in cache. Other important factor
is locality of reference. Locality observed in traffic patterns
will keep the most frequently used pieces of the data structure
in cache, so that most lookups will be fast.
A. Basic Idea

II. P REVIOUS W ORK
Several algorithms for efficient prefix matching lookups
have been presented in technical literature in recent years. The
classical solution for IP address lookup is using tries. A trie [5]
is a tree-based data structure in which prefixes are organized
on digital basis using the bits of prefixes to decide the
branching. Another method used for compressing the height
of trie is Level Compression [10][11]. In this technique, for
any given prefix length, dense areas with common ancestor are

GLOBECOM 2003

Path compression reduces the height of trie by compressing
single child nodes. This only works if the trie structure is
sparse. Level Compression reduces the height by a significant
factor, but is more effective in the denser areas of the trie.
Multibit tries work in a similar way as Level Compressed
tries, but prefixes of intermediate length have to be expanded
and they require exponential memory 2k , where k is the length
by which the prefix is expanded.

- 3943 -

0-7803-7974-8/03/$17.00 © 2003 IEEE

The basic idea of Compact Prefix Tries is to group the
prefixes occurring at multiple levels into a single compressed
node that can fit into a cache line. Instead of using the
prefix expansion like in multibit tries, they are expanded using
Boundary Prefix Expansion [13]. Only the boundary ranges
of a prefix are stored. The advantage of doing this is that if a
prefix has to be expanded by length, only 2 entries are required
instead of 2k , as in case of multibit tries. This scheme was
originally proposed in [7]. For matching a prefix inside the
compressed node, a binary search is performed on the set of
Boundary Expanded prefixes.
The Figure 1 (a) shows a Trie as a large triangle. The
maximum height of the trie is equal to maximum levels it
has, usually the size of destination address. This trie is then
partitioned into smaller subtries of different heights, such that
each subtrie fits into a node equal to the size of cache line.
These nodes shall be called as Compressed SubTrie Nodes or
CSTnodes. The number of levels a subtrie covers is called
as stride. A subtrie contains prefixes as well as links. A link
points to a subtrie at the next level. All prefixes in a subtrie are
expanded and then compressed along with links into a single
CSTnode. The Compact Prefix Trie can be viewed as a tree of
n-ary CSTnodes which are nothing but compressed subtries.
After all the levels are compressed the Trie structure looks as
shown in the Figure 1 (b).

stored as an expanded entry. A key is searched in these entries
by performing binary search. The third part stores the pointers
to next hop table and nodes at the next level. Size required to
store pointers is p bytes. The elements in second and third part
are mapped 1:1. For every expanded entry in second part, the
corresponding pointer information is stored in the third part.
Once the binary search on entries stops, the corresponding
pointers in third part are used to find the next hop and node
at next level. Total size of the CSTnode is denoted by P .
C. Compressing a Trie into Compact Prefix Trie
Like path, level compressed or multibit tries, Compact
Prefix Tries can be created by first building a binary trie and
then compressing it. The four steps involved in compressing
the trie are described below.
1) Start from the root node and find level l, such that the
subtrie up to level l will fit into a CSTnode.
2) Expand the prefixes in the subtrie using Boundary Prefix
Expansion.
3) Fill the node after compacting boundary expanded prefixes.
4) For each subtrie below level l repeat step 1.
Algorithms used at each step are explained below.
Prefixes
0*
A
0000* B
01011* C
1*
D
1000* E
1001* F
1010* G
1011* H
11*
I

a
a
b

c
b

d

e

h

c

f

g

d

h

i

0

f

1

i

j

k

1

0

0

1

j
1

0 1

B

l

(a)

0

I
0

0

l

k

D

0

g
e

1

A

E

0 1

F

G

H

1

(b)

C

Fig. 1.

Compressed Trie after Partitioning
Fig. 3.

Metadata

1 2

…...

E

s
m

Sample Trie

Pointer Information

Expanded Entries
1

2

.....

E

Level 0

p
E*s

0
Level 1

E*p

1

A

P

l

0

D
1

r

0

l

1

r

Level 2

Fig. 2.

I

CSTnode Structure

l

0

0

l 0

1

1

0

l
Level 4

B. Node Structure

1

A

A

l

0 1

E

0 1
F

G

H

0
I

nrp nlp np
0
0
1

ent
1

0

0

2

2

2

2

2

2

2

3

0

4

2

3

2

3

2

7

2

3

7

6

4

9

7

6

8

7

1

14

1
I

1

A CSTnode is used to store a subtrie. Each CSTnode has 3
parts as shown in Figure 2. The first part is called Metadata
part, where the information like, type of node, number of
entries, stride of the subtrie is stored. m bytes are required
to store Metadata. Second and third parts are arrays of length
E, where E is the maximum entries that can be stored in a
CSTnode. In the second part expanded entries are stored. Each
entry is of size s bytes. The maximum length of the expanded
entry that can be stored in this CSTnode is equal to number
of bits in s. If s = 1 byte, prefix of maximum length 8 can be

GLOBECOM 2003

B

1

lp
0

1

Level 3
0

rp
0

Level 5

C

Fig. 4.

Finding Exact Levels

1) Algorithm to Find Exact Levels: We present an algorithm
that finds the exact number of levels that can be compressed.
Five variables are maintained as the search proceeds in Breadth
first manner. The variables rp and lp count the number of right
paths (high endpoint of a range) and left paths (low endpoint
of a range) originating from prefixes at previous levels. For

- 3944 -

0-7803-7974-8/03/$17.00 © 2003 IEEE

example, in Figure 4 all the nodes from level 2, for path 0000,
belong to the prefix A’s (0*) left path. nrp and lrp count the
right and left paths for the next level. Links or prefixes that
do not belong to any right or left path are counted by np. Let
EN T be the number of entries found at each level that can
be stored into a CSTnode. Flag r or l is used to indicate if
the node is part of right path or left path for a prefix. In the
above example, nodes at paths 00, 000 and 0000 are marked
as l indicating that they belong to left path for prefix A. The
algorithm is for finding exact number of levels is given below.

to 1111 and I from 1100 to 1111. But since prefix I is of larger
length than D, the destination address falling into the range
1100 and 1111 will find ‘I’ as the longest matching prefix.
While extracting the entries, care has to be taken to preserve
the spans of each prefix, as well as mark the Start and End
for each span. Depth first search can be used for finding and
expanding the prefixes. The algorithm is as follows:
1) Start from the root node. Follow Depth first search towards
left.
2) If a node is not marked, mark the node as ‘visited’ and do
a) If the length of the prefix is equal to the stride length,
store its bit string and mark it as ‘Point Range’ (Prefixes
B, E, F, G and H).
b) Else, if the node is a valid prefix, expand its prefix by
appending 0’s, store and mark the bit string as ‘Low
range’ (Prefix A, D and I).
3) If the node is already marked as ‘visited’
a) If the node’s height is equal to the stride length and it is
not a valid prefix, then store the bit string for that node
and mark it as a ‘Link’ (Node at path 0101).
b) If it is a valid prefix, expand it by appending 1’s and
mark the string as ‘High Range’ (Prefixes A, I and D).

procedure F indingExactLevels()
1: unmark all the nodes in the trie and start breadth first search from the
root node
2: lvl ← 0, rp ← 0, lp ← 0, nrp ← 0, nlp ← 0 and EN T ← 0
3: while EN T ≤ max do
4: np ← 0
5: for each node v in the subtrie at level l do
6:
if v is a prefix then
7:
if v is unmarked then
8:
nrp ← nrp + 1
9:
nlp ← nlp + 1
10:
np ← np + 1
11:
else
12:
if mark(v) = r then
13:
nlp ← nlp + 1
14:
else if mark(v) = l then
15:
nrp ← nrp + 1
16:
end if
17:
end if
18:
if v has children then
19:
mark left child as l and right child as r
20:
end if
21:
else
22:
if v is unmarked then
23:
np ← np + 1
24:
else
25:
if mark(v) = r then
26:
mark right child as r
27:
else if mark(v) = l then
28:
mark left child as l
29:
end if
30:
end if
31:
end if
32: end for
33: EN T ← rp + lp + np
34: lvl ← lvl + 1
35: rp ← nrp
36: lp ← nlp
37: end while
38: return lvl − 1

The value rp + lp + np calculates the exact number of
entries that will be stored in the CSTnode for level lvl. This
is done by adding all the right and left paths originating from
previous levels (rp and lp) and the links (np) that point the
subtries below level lvl. Since this algorithm keeps track of
the extended paths of all the prefixes at intermediate levels, it
finds the exact number of entries at each level that can fit into
one node. Also, as Breadth First search is used, each node in
the subtrie is accessed only once. Hence the complexity for a
subtrie with N nodes is O(N ).
2) Extracting Entries: Once the number of levels that can
be compressed is known, the next step is to extract and expand
the prefixes occurring at intermediate levels. Figure 5 shows
the spans for prefixes in the sample trie from Figure 3. Span
for prefix A is from 0000 to 0111. Prefix D spans from 1000

GLOBECOM 2003

After all the nodes in the subtrie are traversed, a sorted array
of bit strings is obtained with the each bit string marked as
either Low Range, High Range, Point Range or Link. The bit
strings extracted from the sample trie are shown in Figure 5.
The markers High Range (Hi) and Low Range (Lo) are used
to determine the span of a prefix.
Since Depth First Search is used, again the time complexity
is O(N ).
3) Filling Nodes: Now, the expanded set of bit strings
in Figure 5 should be compacted by removing the duplicate
strings. The strings are processed one by one, from lowest
value to the highest, in a similar way as described in [7].
Finally the compacted entries look like in Figure 6.
The Type indicates the maximum stride length allowed
for this CSTnode. As we process at most 2N entries, the
complexity for this operation is also O(N ). The size of the
CSTnode is dependent on number of entries after compacting,
and is never greater than the maximum allowed size. The node
manager, which manages the creation, alignment, assignment
and maintenance of the nodes, is requested for a CSTnode
of the particular size. When a free CSTnode is assigned, the
metadata, prefix strings and the information about next hop
and next node pointers are filled into it.
D. Extensions and Optimizations
Some extensions and optimizations can be applied the
scheme to increase the lookup speed and to make it more
suitable for IPv6. These are explained below.
1) Initial array for 16 bits: It is possible to reduce the
number of memory accessing by having a table for first 16
bits. The size of this table will be 216 , i.e. 65535 entries. The
16 bits can be used to index into the array. Each entry in the
array stores the corresponding longest matching prefix and a
link to the nodes of Compact Prefix Tries is also stored. Since
indexing into the initial array requires just one memory access,
the total number of accesses required for lookup is reduced.

- 3945 -

0-7803-7974-8/03/$17.00 © 2003 IEEE

0

of destination address is 128 bits and tries are sparse. The
disadvantage is that the branching inside the lookup function
increases.

1

A

D

0

1

0

1
I

0

0

0

1

1

B
0
0
0
0
Lo

0
0
0
0
P

0

1

0 1

A

A

0
1
0
1
Link

0
1
1
1
Hi

B
A

E
1
0
0
0
Lo

0

0

0 1
F

1
0
0
0
P

E
A D

Fig. 5.

1

G

F

G

H

The Search procedure for finding the best matching prefix
is described below. The same procedure can be used for both
IPv4 and IPv6 prefixes.

1

I

I

1
1
0
0
Lo

1
1
1
1
Hi

H
1
0
1
1
P

1
0
1
0
P

1
0
0
1
P

F. Searching

1

I
D

I

Extracting Prefixes from Levels

0000

0101

0111

1000

1001

1010

1011

1100

1111

=
B
>
A

=
>
A

=
A
>
-

=
E
>
D

=
F
>
D

=
G
>
D

=
H
>
D

=
I
>
I

=
I
>
-

0 0
Entries = 9
0 1
Stride = 4
0 0
Type = 8
0 1
4 bytes

0
1
1
1

1
0
0
0

1
0
0
1

1) If the Initial array is present, index into the it using the
first 16 bits of the destination address.
2) If the pointer to CSTnode at next level is null, return
the next hop pointer stored at that index. Else go to the
next node.
3) Read the metadata from the node and get the stride
length and number of entries. Extract bits to be matched
from the destination address. Search for a match within
the prefix entries using binary search. If an exact match
is found, use the (=) next hop and assign it to current
next hop. If the key falls in any range, use the (>) next
hop pointer and return it. If the next node pointer is not
null, repeat the same step, else return the current next
hop.

1
1
1
1
Hi

1
0
1
0

1
0
1
1

1
1
0
0

9 bytes

Fig. 6.

Since that size of the data structure is designed to be equal
or in multiples of cache line width, most parts of the node will
be in cache after the first bytes are accessed. Also the locality
in traffic patterns will keep most frequently used nodes in
cache, so most lookups will be fast.

1
=B =- =A =E =F =G =H =I =I
1
>A >A >- >D >D >D >D >I >1
Null Link Null Null Null Null Null Null Null
1
36 bytes

Node after filling the entries

IV. S IMULATION R ESULTS AND D ISCUSSION
2) Increasing the size of CSTnodes: Increasing the size
CSTnodes reduces the depth of the tree. But cache line
accesses required for lookup are also increased. It is possible
to reduce the maximum number of cache line accesses by
intelligently arranging the data inside the node. The bit strings
require less space and hence requires less cache lines for
storing. The accesses for binary search will be within cache
lines containing the bit strings only. If all the bit string cache
lines are available in cache, only one memory access per node
is required for retrieving pointer information. Thus, even if
the size of the complete data structure is larger than the size
of total cache available, the scheme will perform better if a
small part of the structure (Metadata and bit string entries) is
in cache.
E. Larger stride lengths
In some cases when the tries are very sparse, it might
happen that even though the maximum allowed length is
reached, the node might be not be completely filled. In these
cases, it is possible to store more levels in same CSTNode by
decreasing the total entries that can be stored. The advantage of
using larger stride length is that more number of levels can fit
into a node. If the trie is sparse, this reduces the average length
of the trie, thus reducing average number of memory accesses
required. This is especially useful for IPv6 where the length

GLOBECOM 2003

A. Environment
The lookup operations were simulated on two Pentium
based platforms running Windows 2000. Pentium II, 450 Mhz
and Pentium 4, 2.4 Ghz system with 512 KB cache were
used. The programs were written in C and compiled using
‘Maximum Speed’ optimization in Visual Studio 6. The code
for Level Compressed tries was obtained from the authors and
was compiled on the same platform with same settings.
B. Prefix Databases
The prefix databases used for simulations were obtained
from [6]. The largest database was MaeWest with 93354
prefixes, while the smallest was MaeEast with 18360 prefixes.
MaeWest database was very large in size on 27th April 2001,
but its size reduced afterwards. It was used for simulations
to study the behavior of Compact Prefix Tries for very large
databases. While prefix databases are publicly available, it
is not the case with traffic traces. The traffic was simulated
assuming that every prefix has the same probability of being
accessed. This assumption allows us to measure of the worstcase lookup time. In order to reduce the effects of cache
locality, random permutations of all the entries in the forwarding table were used. The entries were extended to 32 bits by
adding zeros.

- 3946 -

0-7803-7974-8/03/$17.00 © 2003 IEEE

C. Comparison with Other Schemes
1) Memory usage comparison: The memory requirements
of Compact Prefix Tries are compared with various other
schemes in the Table I. Since the databases used by other
schemes were of different sizes, the comparison is not without
flaws. The number of prefixes is not exactly same but, roughly
40000 prefixes in all the measurements.
Table I describes a comparison of the various schemes in
terms of memory usage. The first 8 schemes in the table used
the prefix size of approximately 40000 prefixes. The memory
usage values reported in the literature are used for comparison.
As can be seen from the table, Extended Compact Prefix Tries
are second best in terms of memory requirements. The last row
shows the memory required by Extended Compact Prefix Trie
for MaeWest database with 93354 entries. Only 829 KB of
space was required.
2) Lookup Performance Comparison: Ruiz-Sanchez et. al
[12] have compared some schemes and there lookup times
on a 200 MHz, Pentium-Pro based computer with 512 KB
cache. They ran the simulations using MaeEast database with
47113 prefixes. As it was not possible to get the exact prefix
database, a database of similar size from PacBell (48578
entries) was used for our simulations. The lookup performance
was measured in same way as measured by Ruiz-Sanchez
et. al. In our simulations, the time required for accessing
the prefixes at each level of the Compact Prefix Tries was
measured on Pentium II, 450 MHz system and scaled to 450
MHz clock. This comparison is also not without flaws because
scaling up the clock dose not necessarily speed up lookup
times by the same factor because memory access times do not
speed up with faster clock.
Table II shows the lookup time variability for six different
schemes. The lookup times for first five schemes are borrowed
from [12]. Full expansion/compression scheme was the fastest
scheme as it required just 3 memory accesses in the worst
case. Extended Compact Prefix Trie performed much better
than other trie compression schemes LC Trie and Multibit Trie.
3) Experimental comparison with LC-Tries: As the source
code for LC-Tries was available, an experimental comparison
was performed with them. Since the sources of other state of
art software implementations [14] are not publicly available,
a direct comparison with them could not made.
The lookup performance of LC Tries and Extended Compact
Prefix Tries (ECP Tries) is compared in Table III. Results
are shown only for Pentium 4, 2.4 GHz system. The fill
factor for LC Tries was 0.5. ECP Tries are Compact Prefix
Tries after using the extensions (initial array and large sized
CSTNodes) discussed in Section III-D applied. The lookup
performance was measured by randomly searching the prefixes
in the database.
It can be seen from the table, that the Extended Compact
Prefix Tries can perform lookup operations almost twice fast,
and also require up to 33% less memory then LC-T ries.
Note: For brevity we are unable to provide detailed results
of our analysis of this new scheme. Such details can be found
in [13].

GLOBECOM 2003

Scheme
Patricia Trie
6-way search on prefixes
Binary search on hash tables
Full expansion/compression
Lulea scheme
Controlled Prefix Expansion
LC Trie
Extended Compact Prefix Trie
Extended Compact Prefix Trie

Entries
38816
38816
38816
43524
32732
38816
44168
44168
93354

Size (KB)
3262
950
1600
1057
160
640
708
533
829

TABLE I
M EMORY REQUIREMENT COMPARISON WITH OTHER SCHEMES
Scheme
BSD Trie
Full expansion/compression
Binary Search on pref. len.
Multibit trie
LC Trie
ECP Trie

10th
percentile
2050
115
484
364
422
177

50th (median)
precentile
2640
213
702
591
569
422

99th
percentile
3964
373
3146
1328
880
635

TABLE II
P ERCENTILES OF LOOKUP TIMES (ns)
Database

Prefixes

Mae-east
Aads
PacBell
Mae-west

18360
31283
44168
93354

LC Tries
Size
Mlps
444 11.77
585
5.41
708
4.22
1259
3.13

CP Tries
Size
Mlps
406
9.82
541
8.02
606
6.73
1025
3.85

ECP
Size
373
456
533
829

Tries
Mlps
11.76
10.00
8.83
5.43

TABLE III
M EMORY AND L OOKUP P ERFORMANCE COMPARISON WITH LC T RIES
(Size is in KB and Mlps is million lookups per second)

R EFERENCES
[1] G. Cheung and S. McCanne, Optimal Routing Table Design for IP
Address Lookups Under Memory Constraints, Infocom, 1999.
[2] P. Crescenzi and L. Dardini and R. Grossi. IP Address Lookup Made
Fast and Simple, European Symposium on Algorithms, 1999, pp 65-76.
[3] M. Degermark, A. Brobnik, S. Carlsson and S. Pink, Small Forwarding
Tables for Fast Routing Lookups, Proceedings of ACM SIGCOMM,
October 1997.
[4] W. N. Eatherton, Hardware-Based Internet Protocol Prefix Lookups,
Master’s Thesis, Washington University, May 1999.
[5] E. Fredkin, Trie Memory, Communications of the ACM, 1960.
[6] IPMA. Routing Table Snapshots, http://www.merit.edu/ipma/.
[7] B. Lampson, V. Srinivasan and G. Varghese, IP Lookups using Multiway
and Multi-column search, IEEE/ACM TON June 1999.
[8] Lmbench,
Tools
for
Performance
Analysis,
http://www.bitmover.com/lmbench/.
[9] D. R. Morrison. PATRICIA - Practical Algorithm to Retrieve Information
Coded in Alphanumeric, J. ACM, vol 15, Oct. 1968, pp 514-34.
[10] S. Nilsson and G. Karlsson, Fast Address Look-Up for Internet Routers,
Proceedings of IEEE Broadband Communications 98, April 1998.
[11] S. Nilsson and G. Karlsson, IP-Address Lookup Using LC-Tries, IEEE
Journal on Selected Areas in Communications, June 1999.
[12] M. Ruiz-Sanchez, E. Biersack and W. Dabbous, Survey and Taxonomy
of IP Address Lookup Algorithms, IEEE Network, March/April 2001.
[13] A. Sarda, A memory-efficient scheme for IP Address Lookup using
Compact Prefix Tries, MS Thesis, Dept. of Computer Science and
Engineering, Arizona State University, February 2003.
[14] V. Srinivasan and G. Varghese. Faster IP Lookups using Controlled
Prefix Expansion, Proc. ACM Sigmetrics, 1998.

- 3947 -

0-7803-7974-8/03/$17.00 © 2003 IEEE

Power Efficient Voltage Islanding for
Systems-on-Chip from a Floorplanning Perspective
Pavel Ghosh and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision Systems Engineering
Arizona State University
Tempe, AZ, USA, 85287
{pavel.ghosh, asen}@asu.edu
Abstract—Power consumption can be significantly reduced in
Systems-on-Chip (SoC) by scaling down the voltage levels of the
Processing Elements (PEs). The power efficiency of this Voltage
Islanding technique comes at the cost of energy and area overhead
due to the level shifters between voltage islands. Moreover, from
the physical design perspective it is not desirable to have an excessive number of voltage islands on the chip. Considering voltage
islanding at an early phase of design as during floorplanning
of the PEs can address various of these issues. In this paper,
we propose a new cost function for the floorplanning objective
different from the traditional floorplanning objective. The new
cost function not only includes the overall area requirement,
but also incorporates the overall power consumption and the
design constraint imposed on the maximum number of voltage
islands. We propose a greedy heuristic based on the proposed cost
function for the floorplanning of the PEs with several voltage
islands. Experimental results using benchmark data study the
effect of several parameters on the outcome of the heuristic.
It is evident from the results that power consumption can be
significantly reduced using our algorithm without significant area
overhead. The area obtained from the heuristic is also compared
with the optimal, and found to be within 4% of the optimal on
average, when area minimization is given the priority.

I. I NTRODUCTION
In recent years Multi-Processor System-on-Chip (MPSoC)
design has become extremely challenging due to the increasing
complexities in processor and semiconductor technologies.
As the number of PEs on an MPSoC and the data traffic
between them continue to grow, minimization of energy/power
consumption subject to the performance constraint has become
one of the most important objectives. As dynamic power
consumption of VLSI circuits is proportional to the square of
supply voltage Vdd , reducing Vdd can significantly reduce the
dynamic power consumption. Among the various approaches
taken to reduce power consumption of MPSoCs, the use of
multi-supply voltages (MSV) has gained popularity among the
researchers. The performance critical PEs generally require
high supply voltage to meet the performance requirements,
while the non-critical PEs can be slowed down using lower
supply voltages and thus gaining in terms of power consumption. Utilizing the power-performance tradeoff, the idea
of multi-voltage islanding was first proposed in [1]. Voltage
island on a chip is defined as a cluster of adjacent PEs all
operating at the same voltage level. Although scaling down
the voltage levels of PEs is favorable for reduction of power

978-3-9810801-6-2/DATE10 © 2010 EDAA

consumption, excessive number of voltage islands may be
detrimental from the perspective of physical design [2] as it
creates voltage island fragmentation of the chip and increases
the complexity of layout of the power delivery network.
All these factors have compelled the researchers to consider
the voltage islanding technique at an early phase of system
design. In traditional floorplanning problem, the objective is to
minimize the area of the rectangular bounding box containing
all the PEs and the wiring area cost. On the other hand, from
power optimization point of view it may be beneficial to place
the non-critical PEs close to each other. This will enable
to operate these PEs at a lower voltage level in an attempt
to minimize power consumption, without creating excessive
number of voltage islands. These two objectives often conflict
with each other, and hence the cost function of traditional
floorplanning needs to be modified in order to incorporate the
power consumption and voltage island fragmentation issues.
In [3], the authors have developed a dynamic programming
based voltage island partitioning, level shifter insertion and
power network aware floorplanning for power optimization
within timing constraints. In [4], the authors optimize total
power consumption and power network complexity without
compromising wirelength and chip area. In [5], the authors
have proposed an α2 -approximation algorithm for the voltage
islanding problem, where α is the ratio of the maximum
and minimum voltage values. The authors in [6] propose
algorithms for optimizing power consumption with limited
design cost and number of level shifters. The authors in [7]
develops a Simulated Annealing based floorplanning framework with cost function combining the number of voltage
islands, power consumption and area overhead. In [8], the
authors propose a temperature-aware voltage islanding and
floorplanning to minimize the peak and average temperature
across SoC, area, wirelength and power budget. In [9], the
authors consider voltage islanding in NoC to minimize energy
consumption. Most of these techniques are not practical for
growing number of PEs since they involve multiple iterations
of floorplanning. Moreover, some of them do not consider the
power consumption overhead of level shifters, and some of
them operate only on existing floorplans. The floorplanning
problem in VLSI has a similar counterpart in the Operations
Research domain, known as the rectangle packing problem.
Given the dimensions of the blocks, the objective of the

D

C

B

D

A

A

(a) Islanding done after
floorplanning: Area = 96
sq. units, Number of voltage islands = 4
Fig. 1.

C

B

B
C

D

(b) Higher voltage level
for C, Number of voltage
islands = 2; Higher power
consumption

A

(c) Islanding as part of
floorplanning: Area =
99 sq. units, Number
of voltage islands = 2

Islanding with Floorplanning
TABLE I
D IMENSION AND A LLOWABLE VOLTAGE L EVELS OF PE S
PE
A
B
C
D

Dimension
8×3
6×5
6×3
4×3

power consumption. On the other hand, if voltage islanding is
considered during floorplanning, we can achieve a floorplan
as shown in Fig. 1(c), with total area of 99 sq. units (3%
increase). This floorplan requires only 2 voltage islands (satisfying the design constraint) and 2 level shifters, and reduces
the power consumption since all the PEs are operating at their
lowest possible voltage level.
The rest of the paper is organized as follows. Section II
gives a formal definition of the problem and formulate it as
an optimization problem. We develop efficient heuristic to
solve the problem in section III. The experimental results are
discussed in section IV. Finally, we conclude the paper in
section V.

Allowable Voltage Level
V1
V2
V1
V2

= 1.77V, V2 = 2.5V
= 2.5V
= 1.77V, V2 = 2.5V
= 2.5V

problem is to pack all the blocks in a non-overlapping fashion
such that the overall area is minimized. The researchers in this
domain have proposed several heuristics for the problem [10],
[11], [12]. These approaches cannot be readily applied while
considering floorplanning and the voltage islanding of the PEs,
as the power consumption information is not relevant in the
major application domain targeted for the rectangle packing
problem and therefore have been completely ignored.
In [13], the authors designed a voltage level converter
circuit, with an estimated energy consumption proportional to
the difference of the square of the voltage levels of two endpoints. We follow the similar characterization for the power
consumption of level shifters in this paper. To the best of our
knowledge, our paper is the first that considers the voltage
islanding problem for SoC at an early phase of design taking
into account the overall power consumption, area requirements
and design constraint on the maximum number of voltage
islands.
The following example shows that considering the voltage
islanding for power minimization during floorplanning, we can
achieve better results within the system design constraints than
considering it later. Let us consider a simple communication
trace graph (CTG) consisting of four PEs A, B, C and D,
and with the edges (A, B), (A, C), (B, D) and (C, D). We
define allowable voltage levels for each PE, as the set of
voltage levels, operating at each of which the task executing
at the PE will meet the performance bound. The dimensions
of the PEs and their allowable voltage levels are shown in
Table I. Following the objective of traditional floorplanning,
where we want to minimize the overall area requirement, the
floorplanning achieved will be as shown in Fig. 1(a), with total
area = 96 sq. units. In order to minimize power consumption
according to this floorplanning, we will set the voltage level
of A and C at V1 (shown in light green/gray), and voltage
levels of B and D at V2 (shown in dark green/gray). This will
create 4 voltage islands and requiring 4 level shifters (shown
as red/dark gray strips) as shown in Fig. 1(a). If the design
constraint specifies the maximum number of voltage islands
to be 2, we need to scale up the voltage level of either A
or C to V2 , as shown in Fig. 1(b), and thus increasing the

II. P ROBLEM F ORMULATION
In this section, we give a formal definition of the voltage
islanding problem with combined area and power consumption
objective, constrained by the maximum number of voltage
islands. The floorplanning and voltage islanding problem can
be defined as follows: Given • A Communication Trace Graph (CTG) GP (VP , EP ),
where each pi ∈ VP represents a Processing Element
(PE) and an edge eij ∈ EP represents a communication
trace between PEs pi and pj , where eij = (pi , pj ).
• Communication volume (in number of bytes) associated
with each eij ∈ EP , and denoted by cij .
• Each pi ∈ VP is associated with an allowable set of
voltage levels, Li = {vi1 , vi2 , . . . , vini }, operating at each
of which the task being executed at PE pi finishes within
its specified deadline.
• Each pi ∈ VP is associated with a specified height hi and
width wi . Rotations are allowed during floorplanning.
• Maximum number of voltage islands allowed κ.
• User specified power-area weightage parameter σ.
v
• ηi = computational power consumption of pi ∈ VP at
voltage level v ∈ Li .
• αv1 v2 = level shifter power consumption between two
adjacent PEs operating at voltage levels v1 and v2 .
Find :
• Co-ordinates (xi , yi ) assigned to the bottom left corner
for the placement of each pi ∈ VP
• Assign voltage vi ∈ Li to each PE pi
Such that:
• There is no overlap among the PEs in their placement.
• Number of voltage islands created ≤ κ.
• The objective function (combination of power consumption and area) is minimized.
The objective function is defined as:
obj = σ(E1 + E2 ) + (1 − σ)A
where, E1 = computational power consumption of all the
PEs, E2 = level shifter power consumption, A = area of the
rectangular bounding box consisting of all the PEs according
to their placement. When σ = 0, the objective reduces
to traditional floorplanning objective, where the goal is to
minimize the area. When σ = 1, the objective is to minimize
the power consumption without any consideration of the area.

III. G REEDY H EURISTIC FOR THE P ROBLEM
In this section, we propose a heuristic based on greedy
selection in order to solve the problem defined in the previous
section. We define a function f for each pi ∈ VP . Our heuristic
selects the PE pi with highest value of f and fix their locations
in order. Before going into the details of the algorithm, we
provide here the philosophy behind the greedy approach. We
identify the following factors that should be considered during
the placement of the PEs:
(1) Communication volume cij - Higher is the value of the
communication volume cij between the PEs pi and pj , more
it is desirable to have physical proximity in the floorplanning
and placement of the PEs pi and pj on the chip.
(2) Common allowable voltage levels Li ∩ Lj - Higher is the
overlapping range of allowable voltage levels of PEs pi and
pj , it is desirable to have them placed closer to each other.
(3) Area and Dimension of PE wi and Ai - In order to reduce
the chip area, it is better to place the PEs with large values
of dimensions and areas before the others having smaller
dimensions and areas.
Based on this observation, for all pi ∈ VP , we define the
function f in the following way:
X
f (i) = σ(
gij )2 + (1 − σ)wi Ai , if pi is not placed
j∈placed

=

0, otherwise

where for all pairs of PEs pi and pj , gij is defined as:
gij

= cij × | Li ∩ Lj |

where σ is a user specified power-area weightage parameter
(0 ≤ σ ≤ 1). The first term in the function f (i) gives priority
to the PEs having high communication volume and high range
of overlapping allowable voltage with the already placed PEs.
The second term in the function f (i) gives priority to the PEs
having higher dimension wi and area Ai . Initially, the set of
placed PEs is empty, and therefore the first term in f (i) is zero.
As more and more PEs are placed, more priority is given to the
PEs based on their communication volume and overlapping
allowable voltage range with the already placed PEs. When
the value of σ is 0, the first term in f (i) vanishes, and the
problem reduces to the traditional floorplanning problem with
area minimization objective. On the other hand, with σ being
1, the objective is to minimize power consumption without
any consideration for the area overhead. The details of the
algorithm are given below. To reduce the chip-area, we try to
reduce the height of the area for a specified width W .
IV. E XPERIMENTAL R ESULTS
In this section, we present the results of the experiments
performed. In the absence of the benchmarks which provide
simultaneously the power consumption and area/dimension
information of the PEs/blocks we created the following two
setups in order to evaluate the quality of our heuristic, and
also investigate the effect of the parameters on the outcome.
Setup I: We used four applications from the E3S benchmark
suite [14], and the another three real-applications as the

Algorithm 1 Greedy Heuristic
Input: The problem instance defined in section II and specified maximum
width W .
Output: The placement, orientation (without overlap) and voltage assignment
of each PE pi ∈ VP such that the number of voltage islands does
not exceed κ, and the combined goal of area and power consumption
is minimized.
1: Sort the PEs according to the f (i) values
2: while (placement NOT complete) do
3:
Find lowest gap in the rectangular region
4:
if (PE found to be filled in this gap) then
5:
Place PE in the leftmost position
6:
else
7:
Raise the height of the gap to the lowest neighbor
8:
end if
9:
Update f (i) values and sort the PEs according to updated f (i) values
10: end while
11: Assign minimum possible voltage level to each PE
12: Calculate number of islands numIslands
13: while (numIslands > κ) do
14:
Select pi having highest number of neighbors with different voltage
levels
15:
Raise voltage level of pi to the next lowest voltage level of a
neighboring PE
16:
calculate numIslands
17: end while
18: return Solution with placement and voltage assignments
TABLE II
N UMBER OF B LOCKS IN THE A REA B ENCHMARK I NSTANCES
Test Instance

No. of Blocks

Test Instance

C1: P1, P2, P3
C2: P1, P2, P3
C3: P1, P2, P3
C4: P1, P2, P3
C5: P1, P2, P3
C6: P1, P2, P3
C7: P1, P2, P3

16, 17, 16
25
28, 29, 28
49
73
97
196, 197, 196

N1, N2
N3, N4
N5, N6
N7, N8
N9, N10
N11, N12
N13

No. of Blocks
10, 20
30, 40
50, 60
70, 80
100, 200
300, 500
3152

input communication trace graphs. We generated the area and
dimensions of each of the PEs referring to several processor
vendor datasheets. The number of nodes (PEs) and the number
of edges (communication links between PEs) in each of these
applications task graphs are as follows: Auto-industry - 24
nodes, 21 edges; Consumer - 12 nodes, 12 edges; Networking
- 5 nodes, 5 edges; Office-Automation - 13 nodes, 9 edges;
MPEG-4 - 12 nodes, 13 edges; MWD - 12 nodes, 11 edges;
OPD - 16 nodes, 17 edges.
Setup II: We used two different sets of area/dimension benchmark data for rectangular modules/PEs mentioned in [10],
[15]. The first set contains 7 categories of data, each containing 3 problems, thus 21 in total. The second contains
13 different test instances. The number of nodes in each
of these instances is mentioned in Table II. We added the
communication volume and power consumption information
in this benchmark instances similar to those found for the E3S
benchmark applications.
For both the experimental setups, we vary the value of κ
(maximum number of voltage islands allowed) from 3 to 5.
Also, in both the cases the the value of the user-specified
parameter σ is set at 0, 0.2, 0.5, 0.8 and 1. We study the
variation of the power consumption and area requirement for
different parameter settings. We used five discrete voltage
levels V0 = 1.9V , V1 = 2.3V , V2 = 2.5V , V3 = 3.3V

power consumption (mW)

90
80
70
60
0

0.2
0.5

Power−area weightage0.8
Parameter (")

1

4

3

5

6

7

Maximum number of
voltage islands (!)

Area (sq. Units)

Fig. 2. Power Consumption variation for different values of σ and κ for
auto-industry application

5000
4800
4600
4400
4200
7

6

5

Maximum number of 4 3
voltage levels (")

0

1
0.8
0.5
0.2 power−area weightage

parameter (!)

Fig. 3. Variation of Area for different values of σ and κ for auto-industry
application
1.25
Heuristic Area/Optimal Area

from the heuristic with that of the optimal. The comparison is
shown in Fig. 4. Over the set of the 20 instances, on average
the area obtained from the heuristic differs from the optimal
by 4%. Therefore, our algorithm can be easily switched to a
traditional floorplanning algorithm for area minimization by
setting the value of σ = 0.
We also observe that increasing the value of σ we gain
significantly in terms of power consumption reduction (on
average 10% reduction), without significant change in the area
value (on average less than 5%). Therefore, power consumption minimization using voltage islands and being considered
as part of floorplanning can be very beneficial without paying
a significant cost.

1

R EFERENCES

0.75
0.5
0.25
0

V. C ONCLUSION
We considered the voltage islanding problem for SoC from a
floorplanning perspective. We introduced a new cost function
for floorplanning which not only includes the overall area,
but also incorporates the power consumption component.
Our formulation also incorporates the maximum number of
voltage islands as a design constraint. We proposed an efficient
heuristic to solve the problem. Experimental results based
on benchmark data shows that our approach reduces power
consumption significantly with negligible area overhead.

C1 to C7, and N1 to N13

Fig. 4. Ratio of the Heuristic Area (with σ = 0) to the Optimal Area for
benchmark data C1 to C7 and N1 to N13

and V4 = 3.6V . From the experimental results we make the
following observations. For lack of space, we showed here the
plots for the auto-industry application. Similar nature of the
plots is observed for the other applications as well.
Observation 1: The value of power consumption decreases
with increasing value of σ and also with increasing value κ as
can be seen in Fig. 2. When, σ increases more priority is given
to power consumption reduction, than area reduction, as can
be seen from the definition of function f . Moreover, if κ has
lower value, more number of PE’s voltage level are needed to
be raised in order to meet the voltage island constraint, and
thus leads to more power consumption.
Observation 2: The area value decreases with increasing value
σ, but is not affected with the value of κ as can be seen in Fig.
3. Since, with increasing value of σ more priority is given to
power reduction than area reduction, the area value is lower
obtained for lower values of σ. Since, the value of κ does not
affect the placement and thus the area value according to our
approach in Algorithm 1, the area value does not change along
the variation on the axis of κ.
In the area benchmark instances used in this paper for setupII, the optimal area for each instance is also given. We set the
value of σ to 0, and then compare the area value obtained

[1] D. E. Lackey, P. S. Zuchowski, and T. R. Bednar, “Managing Power
and Performance for System-on-Chip Designs using Voltage Islands,”
in Proc. of ICCAD, November 10-14 2002, pp. 195–202.
[2] M. Popovich, E. G. Friedman, M. Sotman, and A. Kolodny, “OnChip Power Distribution Grids with Multiple Supply Voltages for High
Performance Integrated Circuits,” in Proc. of GLSVLSI, Chicago, Illinois,
USA, April 2005, pp. 2–7.
[3] W. Lee, H. Y. Liu, and Y. W. Chan, “Voltage Island Aware Floorplanning
for Power and Timing Optimization,” in Proc. of ICCAD, San Jose, CA,
November 5-9 2006, pp. 389–394.
[4] W. K. Mak and J. W. Chen, “Voltage Island Generation under Performance Requirement for SoC Designs,” in Proc. of ASPDAC, 2007, pp.
798–803.
[5] H.-Y. Liu, W.-P. Lee, and Y.-W. Chang, “A Provably Good Approximation Algorithm for Power Optimization Using Multiple Supply Votages,”
in Proc. of DAC, San Diego, CA, June 4-8 2007, pp. 87–890.
[6] H. Wu and M. D. F. Wong, “Improving Voltage Assignment by Outlier
Detection and Incremental Placement,” in Proc. of DAC, San Diego, CA,
June 4-8 2007, pp. 459–464.
[7] J. Hu, Y. Shin, N. Dhanwada, and R. Marculescu, “Architecting Voltage
Island in Core-based System-on-a-Chip Designs,” in Proc. of ISLPED,
9-11 Aug. 2004, pp. 180–185.
[8] W. L. Hung, G. M. Link, Y. Xie, N. Vijaykrishnan, N. Dhanwada, and
J. Corner, “Temperature-Aware Voltage Islands Architecting in Systemon-Chip Designs,” in Proc. of ICCAD, 2-5 Oct. 2005, pp. 689–695.
[9] U. Y. Ogras, R. Marculescu, P. Choudhary, and D. Marculescu, “VoltageFrequency Island Partitioning for GALS-based Networks-on-Chip,” in
Proc. of DAC, San Diego, California, USA, June 2007.
[10] E. K. Burke, G.‘Kendall, and G. Whiwell, “A New Placement Heuristic for the Orthogonal Stock-Cutting Problem,” Operations Research,
vol. 52, no. 4, pp. 655–671, 2004.
[11] D. Zhang, Y. Kang, and A. Deng, “A new heuristic recursive algorithm
for the strip rectangular packing problem,” Computers and Operations
Research, vol. 33, no. 8, pp. 2209–2217, August 2006.
[12] W. Huang and D. Chen, “An efficient heuristic algorithm for rectanglepacking problem,” Simulation Modelling Practive and Theory, vol. 15,
pp. 1356–1365, 2007.
[13] T. D. Burd and R. W. Brodensen, “Design Issues for Dynamic Voltage
Scaling,” in Proc. of ISLPED, Rapallo, Italy, 2000, pp. 9–14.
[14] R. Dick, “Embedded System Synthesis Benchmarks Suite(E3S).”
[Online]. Available: http://www.ece.northwestern.edu/ dickrp/e3s/
[15] E. Hopper and C. H. Turton, “An Empirical Investigation of Metaheuristic and Heuristic Algorithms for a 2D Packing Problem,” European
Journal of Operational Research, vol. 128, no. 1, pp. 34–57, 2000.

90

IEEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 1, JANUARY

free function from each of the remaining (22n - 1) functions for an
n-input circuit. In view of LSI/VLSI technology, the difficult problem of generating test sets for detecting bridging faults can be
circumvented by resorting to a syndrome testable design which
takes care of detecting short circuit faults as well [10]. Better comprehension of ICFF and its application to testable design with an
objective of having higher fault coverage in LSI/VLSI circuits
would be a challenging area of future research.
REFERENCES
[1] K. C. Y. Mei, "Bridging and stuck-at faults," IEEE Trans. Comput.,
vol. C-23, pp. 720-727, July 1974.
[2] A. D. Friedman, "Diagnosis of short circuit faults in combinational circuits," IEEE Trans. Comput., vol. C-23, pp. 746-752, July 1974.
[3] S. Xu and S. Y. H. Su, "Testing feedback bridging faults among internal,
input and output lines by two patterns," in Proc. IEEE Int. Conf. Circuits
Comput., ICCC, 1982, pp. 214-217.
[4] M. Abramovici and P. R. Menon, "A practical approach to fault simulation and test generation for bridging faults," in Proc. IEEE Int. Test
Conf., 1983, pp. 138-142.
[5] P. Lamoureux and V. K. Agrawal, "Nonstuck-at fault detection in nMOS
circuits by region analysis," in Proc. IEEE Int. Test Conf., 1983,
pp. 129-137.
[6] A. Isoupvicz, "Optimal detection of bridge faults and stuck-at faults in
two-level logic," IEEE Trans. Comput., vol. C-27, pp. 452-455, May
1978.
[7] K. L. Kodandapani and D. K. Pradhan, "Undetectability of bridging
faults and validity of stuck-at fault tests," IEEE Trans. Comput.,
vol. C-29, pp. 55-59, Jan. 1980.
[8] M. Karpovsky and S. Y. H. Su, "Detection and location of input and
feedback bridging faults among input and output lines," IEEE Trans.
Comput., vol. C-29, pp. 523-527, June 1980.
[9] J. Galiay et al., "Physical versus logical fault models in MOS LSI circuits: impact on their testability," IEEE Trans. Comput., vol. C-29,
pp. 527-531, June 1980.
[10] B. B. Bhattacharya and B. Gupta, "Syndrome testable design of combinational networks for detecting stuck-at and bridging faults," in Proc.
IEEE Int. Test Conf., 1983, pp. 446-452.
[11] J. P. Hayes, "On realizations of Boolean functions requiring a minimal
number of tests," IEEE Trans. Comput., vol. C-20, pp. 1506-1513,
Dec. 1971.
[12]
, "Transition count testing of combinational circuits," IEEE Trans.
Comput., vol. C-25, pp. 613-620, June 1976.
[13] H. Fujiwara, "On closedness and test complexity of logic circuits," IEEE
Trans. Comput., vol. C-30, pp. 556-562, Aug. 1981.
[14] B. B. Bhattacharya and B. Gupta, "Anomalous effect of a stuck-at fault
in a combinational logic circuit," Proc. IEEE, vol. 71, pp. 779-780,
June 1983.
[15] S. L. Hurst, Logical Processing of Digital Signals. New York: Crane
Russak, 1978.
[16] J. Savir, "Syndrome testable design of combinational circuits," IEEE
Trans. Comput., vol. C-29, pp. 442-457, June 1980.
[17] A.K. Susskind, "Testing by verifying Walsh coefficients," in Proc.
FTCS-J1, June 1981, pp. 206-208.
[18] B. B. Bhattacharya and B. Gupta, "Logical modeling of physical failures
and their inherent syndrome testability in MOS LSI/VLSI networks," in
Proc. IEEE Int. Test Conf., 1984, pp. 847-855.

On System Diagnosability in the Presence of Hybrid Faults
A. SENGUPTA, A. SEN, AND S. BANDYOPADHYAY
Abstract -This correspondence deals with the problem of testing the
diagnosmbllity of a system in presence of hybrid faults (that is, when some

Manuscript received July 9, 1983; revised July 2, 1984.
A. Sengupta and A. Sen are with the Department of Computer Science,
University of South Carolina, Columbia, SC 29208.
S. Bandyopadhyay is with the Department of Computer Science, University
of Windsor, Windsor, Ont., N9B 3P4, Canada.
IEEE Log Number 8406293.

1986

of the units of the system have failed intermittently and some have failed
permanently). Presence of intermittent faults can lead to incomplete diagnosis and usually complicates the diagnosis problem in comparison to
permanent fault situation. Alternative characterizations for hybrid fault
diagnosability of a system to those originally presented in [3] are derived
in this paper. It is shown that these conditions lead to the testing of the
hybrid diagnosability of a system with fewer computations than that in [3].
Index Terms - Hybrid faults, intermittent faults, permanent faults, system level diagnosis.

I. INTRODUCTION
The study of diagnosable systems under different models
[1], [7]-[11] received considerable attention in recent years. The
model introduced in [1] is possibly the most well-studied model in
connection with diagnosability analysis under different measures.
This model assumes a system to be composed of a number of units,
each of which is tested by some other units of the system. Most
studies on this model [1], [6]-[9], [12] assume that whenever any
unit of the system is faulty, the fault is of permanent nature. In
general, the fault situation is hybrid in the sense that some of the
faulty units can fail intermittently while the others fail permnanently.
The problem of diagnosability of a system under intermittent or
hybrid fault situation was studied in [2] and [3].
A system modeled as in [1] can be represented as follows. A
system S having n units or components is represented by a directed
graph G with n vertices. Each unit of the system is tested by some
of the other units but never by itself. The feature of testing of each
unit by other units is represented by directed edges of G. If the ith
unit of the system tests the jth unit, then G will have a directed edge
from the ith vertex vi to the jth vertex vj and this edge will be

represented by (vi, vj). Throughout this correspondence, the
vertices of G and the corresponding units of S will be used interchangeably. The result of testing of one unit by another is represented by labeling the edge representing the testing. An edge (vi, vj)
is labeled 0 if vi "finds" vj good and is labeled 1 if vi "finds" vj
faulty. If vi is faulty, intermittently or permanently, its finding is
unreliable and thus the labeling of (vi, vj) does not convey any
information about the status of vj. If vi is fault free, then its finding
about vj is reliable if vj is fault free or permanently faulty. If vj is
intermittently faulty, the fault can escape detection by vi if vj was
working in a fault free manner when vi tested it. Thus, if vj is
intermittently faulty, the finding of vi about vj could be unreliable
even when vi is fault free. Because the intermittently faulty units
can escape detection by fault free units, the identification of faulty
units from the test results will usually lead to incomplete identification. One way to reduce the incompleteness is possibly to repeat
the testing several times with an expectation that the intermittently
faulty units will show some evidence of fault during this repetition
and will be ultimately detected. A system is said to be diagnosable
if all the permanently faulty units and possibly some of the intermittently faulty units can be uniquely identified from the test results. In
this correspondence, we will derive an alternative characterization
to those originally presented in [3] for the diagnosability of a system
assuming upper bounds on the number of faulty units and the number of intermittently faulty units. It will be shown that these characterizations lead to the testing of the diagnosability of a system with
fewer computations than that in [3].
II. DEFINITIONS AND NOTATIONS
Let G = (V, E) represent a system S where V is the set of vertices
and E is the set of directed edges of G, such that each vertex of V
represents an unit of S and each edge represents the testing feature
of one unit by another. Every element of E is an ordered pair of
vertices, that is, E C V x V. Let X(vi) represent the set of vertices
tested by vi, i.e., X(vi) = {vjj(vj,vj) E E} and X-1(vi) =
{v;l(vj, vi) E E}. For any V, C V, we define, X(V1) = UiEV
X(vi) - V1 and X-'(V1) = U iEV, X 1(vi) - V1.

0018-9340/86/O100-0090$01.00 © 1986 IEEE

91

IEEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 1, JANUARY 1986

If there are q edges in G, then at any time of testing, there are
q possible test outcomes considering all the edges. A set of these
q outcomes will be referred to as a syndrome. Depending on the
status of the testing and tested units, a test outcome will be 0 or 1.
The labeling of the edge (vi, vj) will be O if both vi and vj are fault
free, 1 if vi is fault free and vj is permanently faulty. If vi is faulty
or if vj is intermittently faulty, then either of 0 and 1 is a possible
labeling of (vi, vj). Thus, if V1 C V is the set of faulty units of the
system, usually there will exist a set of syndromes each of which is
a possible syndrome under this fault situation. Given a syndrome s,
we will say that s is (VI, V2)-compatible ifs is a possible syndrome
when all the units of V1 are permanently faulty and all the units of
V2 are intermittently faulty. This fault situation will be referred to as
a (V1, V2) fault situation. A syndrome s will be called permanent
fault compatible, if there exists some V1 C V, such that s is (VI, 0)compatible. It has been shown [3] that every (VI, V2)-compatible
syndrome is not permanent fault compatible.
A system is t/ti diagnosable if for every permanent fault compatible syndrome s, whenever s is (V2, 0)-compatible, for some
V2 C V, 1V21 . t, V2 is unique, and if there exist VI,, V12 C V,
with 1V12l s ti, |V1I U V121 < t, such that s is also (VI,, V12)-compatible, then VI, C V2 C V11 U V12.
Thus, a system is tlti diagnosable if from every permanent fault
compatible syndrome, all the permanently faulty units can be
uniquely identified and possibly some of the intermittently faulty
units too, provided no more than t units are faulty and no more than
ti units are intermittently faulty. However, some of the intermittently faulty units may escape diagnosis possibly because they never
behaved in a faulty manner when the testing was carried out. In this
sense, the diagnosis may be incomplete but never incorrect in a tlti
diagnosable system. Because of the presence of intermittently faulty
units in the system, if the testing is carried out several times, the
outcomes of testing, in general, might be changing with time. For
example, if either vi or vj is intermittently faulty, then the labeling
of the edge (vi, vj) might be changing, in general, as the testing is
repeated. Thus, to identify the faulty units of the system, it may be
necessary to repeat the testing several times and each time the
syndrome is to be modified in view of the previous syndromes to
retain the fault information contained in them, i.e., if at any instance
of testing, the labeling of any edge is 1 then this labeling remains 1
in all the modified syndromes obtained by subsequent testing. Obviously, at any instance of testing, the modified syndrome will provide
either more or identical information (but never less) about faulty
vertices of G than the syndrome obtained at that instant. This was
referred to as updated syndrome in [3]. We will discuss the t/t,
diagnosability of a system S, assuming, as in [3], that the diagnosis
of the faulty units is always made on the basis of permanent fault
compatible modified syndrome.
An ordered pair of sets of vertices of G, (V1, V2) where IVI,
' t, will be referred to as a critical pair if V2 X VI, otherwise
IV21
it is a noncritical pair.
III. t/ti DIAGNOSABILITY OF A SYSTEM
Based on the above discussions, we now present the necessary
and sufficient conditions for the t/ti diagnosability of a system.
Theorem 1: A system S is t/ti diagnosable iff for every critical
pair (VI, V2) of G at least one of the following conditions is satisfied:

Cl: IX(V- V1 - V2) n (V, - V2)1 > ti

C2:

IX(V - V1

- v2) n (v2

--v1)l >

.

In order to prove the Theorem 1, we will use the necessary and
sufficient conditions for the t/ti diagnosability of a system
presented in [3], which is included here for completeness.
Theorem 2 [3]: A system S is t/ti diagnosable iff for every
V1 C V where jVj ' t, at least one of the following conditions holds
relative to every other V, C V, such that I/y1 t and V.fn v = 0.

mmi:
MM2:

X-I(V.)!Z VY

IX(V

VI'

Proof of Theorem 1: Necessity. Suppose S is t/ti diagnosable
and (V1I, V2) is a critical pair. Then V2 !t V1. Form Vy = V1 and V. =
V2 - Vl. Obviously,
S t, V - V1 - V2 = V - V, -Vy
and v1 f Vy = 0. Let us denote V - V1 - Vy by V,,. Since S is t/ti
diagnosable, at least one of MMI and MM2 is satisfied by V, Vy. If
MM1 is satisfied, X(11) nf 11
0 and hence C2 is satisfied by
(V1, V2) as V. = V2- 1V and V, = V - V1 -V2. If MM1 is not
satisfied, then X(V,) n VX = 0. Hence, jx(v11) n (v1 - o.)I
= 0.
Now v1 n v2 =12 - V, and hence v11 n v21 < tsince
V21 c t and V, C V2. Thus, IX(V,) n (V flnV2)1
v lnV21 C t - IV1I. But if MMI is not satisfied, then MM2 is
satisfied and hence, t + ti - I1v/l < IX(v1)I < lX(v11) n
(V1 - V2)1 + t - V11, since lX(v,,) n (12 - v1)l = 0. This implies

IVxI,l

|v,I

> ti satisfying Cl.
Ix(v,) n (V, - V2)1Suppose
S is not

t/ti diagnosable. Then there
Sufficiency:
exist some V,, Vy, such that V1,, VyCV,IV,I, IVyl t andv1, n
vyf =
0 and V,, Vy do not satisfy MMI nor MM2. Let V1 denote VV1 - Vy as before. Form V1 = Vy and V2 = V, U 1K where Va C
X(Vy) n vyl). Obviously,
x(v,) n Vy and 1Val = Min(t V1- V, - Vy = V -1Vl-1V2, IVIl, IV21 < t and sinceV n vy=
0, V2 V1. Thus, (V1, V2) is a critical pair, since MMI is not satisfied by V1,Vy,X-'(V.1) C Vy. Since V, = V - V, - Vy = V V- V2 and V1 = V2 - V,, C2 is not satisfied by (Vi, V2) pair. Now

lVyI,

v1a = 1Vl V2andsince1Va c x(v )x(v(1) n (vln v2) = vln v2,
IX(V,) n (vln v2)1 = v1al. If 1val = lX(v1) n vyl,
x(v1,) n (vy - Va) = 0 and since V, = V - V, - V2 and Vy Va = V1 - V2, C2 is not satisfied by (V1I, V2). IfIVaI = t- IVA, then
since MM2 is not satisfied by V,, Vy, IX(V,)l < t + ti - 1V/, i.e.,
Ix(v1,) n (v1 - .2)1 + t - IXI 5 t + ti - IVxl (since X(v,) n
(V2 - V1) = 0 and IX(v1) n (vln v2)1 = v1al = t - Iv,1J) implyand hence

ing that (V1, V2) does not satisfy Cl. This completes the proof.
Corollary: If for every critical pair (VI, V2), the condition Cl or
C2 is satisfied, then every unit is tested by at least t other units.
Proof: Suppose vi E V is tested by fewer than t other units.
Form V, = X 1(vi) and V2 = {v,} U VI. Since IV,|I < t, (V1, V2) is a
critical pair. But this critical pair does not satisfy Cl as V, - V2 = 0
nor C2 since the only unit in V2 - V, is tested by units in VI.
Given a system S, the testing for its t/ti diagnosability does not,
however, need the checking of the validity of the conditions of
Theorem 1 for all possible critical pairs. We will show that if
(V,, V2) is a critical pair and (V2, V1) is not, then the critical pair
(V1, V2) need not be considered at all. If, however, both (V1, V2) and
(V2, V1I) are critical pairs, then the consideration of any one of them
is enough. Consider a critical pair (V1, V2) for which the condition
Cl of Theorem I is satisfied. Then, if (V2, VI) is a critical pair too,
the condition C2 of Theorem 1 is satisfied for this critical pair as
ti ' 0. It might be noted that since the t/ti diagnosability includes
t/0 diagnosability, all the necessary conditions for t/0 diagnosability are necessary conditions for t/ti diagnosability as well.
Hence, a system is t/ti diagnosable only if every unit is tested by at
least t other units of the system.
Lemma 1: If every unit of a system is tested by at least t other
units of the system, then for every critical pair (V1, V2) whenever
(V2, V1) is a noncritical pair, (VI, V2) satisfies C2 of Theorem 1.
Proof: If (VI, V2) is a critical pair and (V2, VI) is not, V, C V12.
Consider any vj E V2 - VI. Since each vertex is tested by at least
t other vertices and 11V21 ' t, and V, C V2, at least one vertex of
V - V, - V2 tests vj. Hence, every vertex of V12-1V, is tested by
at least one vertex of V - V, - V2, that is, X(V - V, - v2) n
(V2 - V,) = V2- 1V. Since 1V2 - V1,I > 0, the condition C2 is
satisfied by (V1, V2).
The necessary and sufficient conditions for the diagnosability of
a system can be respecified in the following form, using Lemma 1
and Theorem 1, without using any ordering of the pair (V1, V2).
Theorem 3: A system is t/ti diagnosable, iff for every VI, V2
such that both (V1I, V2) and (V2, VI) are critical pairs, at least one of
the conditions CCI and CC2 below is satisfied.

CC 1:

VY)l > t + ti ly'l.

a) JX(V

V,

(vi v2) > ti
or b) JX(V
VI v2)

v2)

n

n

(v2

vi)l > ti

IEEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 1, JANUARY 1986

92

CC2. a)

IX(V

- VI - V2) n (v1 - v2)1 > 0
and b) |X(V - V1 - V2) n (V2 - v)l > 0.

If IVjI < p, form V1 = V, U V, where V, C V - V - V, and
V I = p - IV,l. Clearly, V, C V - V, IV,I = p, and
V - V, - VcI = 2t - 2p. If possible, let IVyll ' Min(t, ti + p).
If lVyl . p, and since V, C Vy, IVy - Vjl c Min(t - p, ti). If
|Vyl < p, then Vy C V,, and hence, IVy - VI = 0. In any case,
IVy - VcI ' Min(t - p, ti). Divide V - V, - V, into two equal
parts VIa and V2a each containing t - p units, such that
Vy- V1 C VIa. Let VI = Via U V, and V2 = V2a U V,. then
1VIl = 1V21 = t and V - VI - 12 = V,. Thus,

Proof: Necessity. Sup p ose some (V1, V2) does not satisfy CC1
and CC2. Suppose IV,l 1V21; hence V2 X1V1. Since CCl is not
satisfied, the condition Cl of Theorem 1 cannot be satisfied by the
critical pair (V1, V2). If (V1I, V2) does not satisfy CC2(b), then the
critical pair (V1, V2) does not satisfy the condition C2 of Theorem 1
and hence by Theorem 1, the system is not tlti diagnosable. If
(V1, V2) does satisfy CC2(b) but not CC2(a) nor CC1, then the
Ix(v - vi - v2) n (vi - v2)1 = Ix(v,) n vial
ordered pair (V2, V1) having V1 X V2 (since IVlI . 1V21) is a critical
= |Vy - Vcl < ti and
pair and does not satisfy the conditions Cl and C2 of Theorem 1.
Thus, the system is not diagnosable. If IV21 > IV,1, the same arguIX(V - v1 - v2) n (v2 - vi)l = IX(VX) n v2.1
ment follows by switching the names of the sets.
= Ivy n v2a1 = 0.
Sufficiency: Suppose the system is not diagnosable. Thus, by
Theorem 1, there exists a critical pair (VI, V2) such that (V1, V2) does
not satisfy the conditions Cl and C2 of Theorem 1. Thus, the un- Thus, neither of the conditions of Theorem 3 is satisfied by (VI, V2)
ordered pair (Vi, V2) does not satisfy CC2. Since the condition Cl of and hence the system is not tlti diagnosable.
Suficiency: Suppose the condition holds but the system is not
Theorem 1 is not satisfied by the ordered pair (V1, V2), the unordered
pair (VI, V2) does not satisfy CCI(a). Again, since the ordered pair diagnosable. We will show that this leads to a contradiction. Be(VI, V2) does not satisfy condition C2 of Theorem 1, the condi- cause the system is not tlti diagnosable, there exists some pair
tion CC1 (b) cannot be satisfied by the unordered pair (V1, V2), as (VI, V2) such that, IVI = IV21 = t and
ti 2 0. Hence the theorem.
Given a system S, in order to check for the t/ti diagnosability of A) IX(V - VI - v2) n (v - v2)1 ti
the system, we need not, however, consider all the possible choices
and B) IX(V - VI - v2) n (V2 - V)l = o.
of V1I, V2 such that both (V1, V2) and (V1, V2) are critical pairs. The
following lemma specifies that consideration of only those (VI, V2) Let Vc =V1nV2. Then, |Vcl = p, O ' p < t. IV-VI-V21 =
n-2t + p. Let V, = V - V, -V2. Then, X(Vx) S 1 U 12 and
for which IV, = IV21 = t, is sufficient.
Lemma 2: A system is tlti diagnosable iff each unit is tested by rK(V1)I > Min(t, ti + p).
Case 1: t < ti + p: For this case, IX(V,)l > t. Since,
at least t other units and for any two distinct vertex sets V1I, V12,
= 1V21 = t, at least one of the conditions CCl and CC2 of X(Vx) C VI U V2 and IX(V.)l > t, X(V1) g: VI. Thus, at least one
IVlI
unit of V2 - Vi is tested by V,. This contradicts B.
Theorem 3 is satisfied.
Case 2: ti + p < t: For this case, IX(Vx)l > ti + p. If B is satisProof: Necessity. This follows directly from Theorem 3.
Sufficiency: Consider any two vertex sets V1 and 12 where I1, , fied, then X(Vx) C V1. As IX(Vx)I > t, + p and Vc1 = p, we have,
I1V21 t. According to Lemma 1, we need consider the cases when ix(vX) n (v1i - Vc)l > ti, i.e., IX(V,) n (v, - V2)1 > ti. This conboth (V1, V2) and (V2, V1) are critical pairs. Without any loss of tradicts A. Hence the theorem.
Theorem 4 can be regarded as the generalization of the results
generality let us assume that VI . IV21. Then either IV,I < 1V21 < t
and V1 g V2 or 1VIl = 1V21 < t. Choose V, C V2 - VI such that presented in [6]. An assumption of ti = 0 leads to the permanent
|V. + 1V/I = 1V21. Choose Vy C V- V - V2 such that IV21 + fault situation and a substitution of ti = 0 produces the same necesVy t. Form V2a=1V2U Vy and VlaVI U Vx U Vy. ThenV - sary and sufficient conditions as in [6]. Theorem 4 also leads to the
VIa -V2a C V - VI - V2 and V2a - VIa C V2 - Vi, and VIa- checking for the tlti diagnosability of a system with fewer comV2a VI - V2. Now |Vial = 1V2aI t and if at least one of the putations than that presented in [3]. According to Theorem 4, in
conditions of Theorem 3 is satisfied for Vla, V2a then they are satis- order to check the tlti diagnosability of a system having n units, the
number of subsets V, (as given by Theorem 4) to be considered is
fied for Vi, V2 also. This completes the proof of the lemma.
An interesting observation might be made for ti > 0. For such a
situation, if for every vertex set pair (VI, V2) where IV/I = 1V21 = t,
at least one of the conditions CCl and CC2 of Theorem 3 is satisXEt + x}
P=o 2t - p}
fied, then every unit is tested by at least t other units. To prove this,
consider any Y C V, such that IYI = t + 1 and any two vertices According to the results in [3] as given by Theorem 2, a similar
checking needs the following number of ordered subset pairs to be
v E Y and v2 E Y. Form VI = Y - {vi} andV2 = Y- {v2}. Then
considered
(according to Theorem 2, the ordering of Vx, V, in the
=
=
t
and
=
=
1.
Since
>
ti
0,
1VIl 1V21
I1VI V21 I12 Vil
(VI, V2) cannot satisfy the condition CCI of Theorem 3. Then the pair (Vx, V,) is important).
condition CC2 must be satisfied. Thus, vi and v2 each must be tested
j (n)[ (n ; )]
by at least one vertex from V - Y. Hence any vertex in Y is tested
by at least one vertex from V - Y. Suppose that for some vi,
IX-1(vi)l = t - k < t. Then let Y consist of vi, X-'(vi), and any
other k vertices. From the above argument vi E Y must be tested by
IV. DIsCUSSION
at least one vertex from V - Y and this is impossible because of the
The problem of testing the diagnosability of a system in the presC Y. Hence, for all vi, IX- I(vi)l 2 t.
fact X '(vi)
According to Lemma 2, given a system consisting of n units, the ence of both intermittent and permanent failures has been contesting for tlti diagnosability apparently needs the consideration of sidered in this correspondence. A new necessary and sufficient
N(N - 1)/2 pairs of vertex sets where N is the possible number of condition for the diagnosability of a system has been derived. It has
vertex sets each containing t vertices. However, with much less been shown that this new condition results in an algorithm for
computational effort, the checking might be carried out. This is testing the diagnosability of a system with fewer computations than
that resulting from the necessary and sufficient conditions presented
given by the following theorem.
Theorem 4: A system consisting of n units is tlti diagnosable iff in [3]. Throughout this correspondence as well as in [2] and [3], the
every unit is tested by t other units and for every V, C V where fact that the different units of a system might have different probabilities of intermittent and permanent failure has not been taken into
IVII = n - 2t + p, 0 'p < t, IX(V,)l > Min(t,ti + p).C
Proof: Necessity. Let X(V,) = Vy. Then Vy V - V. account. The problem of diagnosability of a system when this fact
IV - V,I = 2t - p. If IVyl ' p, choose Vc C Vy such thatIVcl = p. is taken into account needs further investigations.
-

93

EEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 1, JANUARY 1986

REFERENCES

[1] F. P. Preparata, G. Metze, and R. T. Chien, "On the connection assignment problem of diagnosable systems," IEEE Trans. Comput.,
vol. C-16, pp. 448-454, Dec. 1967.
[2] S. Mallela and G. M. Masson, "Diagnosable systems for intermittent
faults," IEEE Trans. Comput., vol. C-27, pp. 560-566, June, 1978.
[3] -, "Diagnosis without repair for hybrid fault situations," IEEE Trans.
Comput., vol. C-29, pp. 461-470, June, 1980.
[4] M. A. Breuer, "Testing for intermittent faults in digital circuits," IEEE
Trans. Comput., vol. C-22, pp. 241-246, Mar. 1973.
[5] S. Kamal and C. V. Page, "Intermittent faults: A model and a description
procedure," IEEE Trans. Comput., vol. C-23, pp. 713-719, July, 1974.
[6] S. L. Hakimi and A. T. Amin, "Characterization of the connection assignment of diagnosable systems," IEEE Trans. Comput., vol. C-23,
pp. 86-88, Jan. 1974.
[7] S. Karunanithi and A. D. Friedman, "Analysis of digital systems using a
new measure of system diagnosis," IEEE Trans. Comput., vol. C-28,
pp. 121-133, Feb. 1979.
[8] S. N. Maheshwari and S. L. Hakimi, "On models of diagnosable systems
and probabilistic fault diagnosis," IEEE Trans. Comput., vol. C-25,
pp. 228-236, Mar. 1976.
[9] F. Barsi, F. Grandoni, and P. Maestrini, "A theory of diagnosability of
digital systems," IEEE Trans. Comput., vol. C-25, pp. 585-593, June,
1976.
[10] C. R. Kime, "An analysis model for digital system diagnosis," IEEE
Trans. Comput., vol. C-19, pp. 1063-1073, Nov. 1970.
[11] J. D. Russell and C. R. Kime, "System fault diagnosis: Closure and
diagnosability with repair," IEEE Trans. Comput., vol. C-24,
pp. 1078-1088, Nov. 1975.
[12] T. Kameda, S. Toida, and F. Allan, "A diagnosing algorithm for networks," Informat. Contr., vol. 29, pp. 141-148, 1975.

Comments on "The Diogenes Approach to Testable
Fault-Tolerant Arrays of Processors"
ISRAEL KOREN
The above paper1 presents an approach to the design of faulttolerant processor arrays. In Section IV of this paper (related work
on fault-tolerant networks) the author criticizes a previously published approach presented by Koren [1] and by Gordon, Koren and
Silberman [2]. In [1], an algorithm for structuring a linear array on
a rectangular grid of processing elements (PE's), some of which
may be faulty, is presented. Since similar structuring algorithms for
other structures like square arrays and binary trees (in the presence
of faulty PE's) are more complicated, a different strategy has been
suggested in [ 1 ]. According to it, all PE's in the same row or column
of the faulty processor will turn into connecting elements (CE's) and
will not participate in any future processing task. The remaining
PE's still constitute a rectangular grid with one less row and one less
column. Consequently, the same structuring algorithms (for faultfree arrays) can be used and in many cases the grid will admit
the same size of a binary tree as before [1]. If the communication
link between two neighboring processors fails, only the processors
within the corresponding single row or column will be declared
CE's. In [2], a similar strategy has been applied to hexagonal arrays.
The author of the above paper argues, based on a private communication by Kedem and Zorat, that with "given current models of
faults in arrays, all of Koren's PE's are likely to be converted to
connecting elements." Once again we see a very common mistake
which is the result of not making the necessary distinction between
manufacturing defects and operational faults.
Manuscript received January 6, 1984; revised January 17, 1985.
The author is with the Department of Electrical Engineering, TechnionIsrael Institute of Technology, Haifa 32000, Israel.
IEEE Log Number 8406295.
'A. L. Rosenberg, IEEE Trans. Comput., vol. C-32, pp. 902-910, Oct.
1983.

These two types of faults differ in probability of occurrence and
in associated costs. Production flaws are numerous resulting in low
yields for VLSI chips and consequently increasing the production
cost. In contrast, operational faults have a very small probability
of occurrence and their impact is on the system operational cost.
Clearly, a method which is cost-effective for handling the first type
of fault, is not necessarily a cost-effective one for the'second type
of fault.
In [1] and [2] only operational faults are considered. A low failure
rate is projected for such faults in a VLSI chip and consequently, the
probability of two or more operational faults is almost negligible.
Hence, it has been argued in [1] that it is beneficial to have a simple
strategy to handle faulty PE's rather than insist on 100 percent
utilization of the remaining PE's. The latter might require either
more complicated restructuring algorithms or a more complex
switching mechanism, both may increase the silicon area of each PE
and a smaller number of PE's will fit into the same chip area. The
suggested strategy, although wasteful in PE's when an operational
fault does occur, has the advantage of simplicity and low overhead.
The existence of faulty PE's is made transparent to the outside world
and as a result the same restructuring algorithms (for fault-free
arrays) can be employed. This tums the suggested simple strategy
attractive although not optimal.
It seems, following the argument in Section IV of the Rosenberg
paper, that this simple strategy is inappropriate to handle the large
number of production flaws. However, in a recent paper by Koren
and Breuer [3], it is shown that such simple strategies might be
effective even for manufacturing defects. Employing practical defect distributions and not just asymptotic results, it has been shown
in [3] that the effective yield (i.e., the total number of chips that are
acceptable from a given wafer) might even increase when a faulttolerant array of processors with the above reconfiguration strategy
is designed. This phenomenon results from the fact that the probability of getting a chip with only a single or two defects is high and
if the chip can successfully handle such a small number of defects
the overall yield is increased.
In contrast to [1], Rosenberg's main emphasis is on manufacturing flaws. He configures a linear array, a tree and a pyramid on
a physical collinear layout which may contain a large number of
defective processors. His design achieves 100 percent utilization of
the fault-free PE's but a fault on the global busses might be fatal.
Another drawback of his scheme is the fact that completely different
switching circuits are needed for a linear array, a binary tree and a
pyramid. Consequently, a particular configuration must be chosen
at design time and cannot be altered later.
REFERENCES
[1] I. Koren, "A reconfigurable and fault-tolerant VLSI multiprocessor array,"
in Proc. 8th Ann. Symp. Comput. Architecture, May 1981, pp. 425-441.
[2] D. Gordon, I. Koren, and G. M. Silberman, "Fault-tolerance in VLSI
hexagonal arrays," Dep. Elec. Eng., Technion-Israel Inst. Technol.,
Haifa, Israel, Tech. Rep., 1984.
[3] I. Koren and M. A. Breuer, "On area and yield considerations for faulttolerant VLSI processor arrays," IEEE Trans. Comput., vol. C-33,
pp. 21-27, Jan. 1984.

Author's Reply2
ARNOLD L. ROSENBERG
Professor Koren's comments on my paper raise a number of
points that merit more attention than they typically receive in the
literature.

2Manuscript received March 6, 1984.
The author is with the Department of Computer Science, Duke University,
Durham, NC 27706.

IEEE Log Number 8406296.

0018-9340/86/0100-0093$01.00 © 1986 IEEE

On Topological Design of Service
Overlay Networks
Arunabha Sen1 , Ling Zhou1 , Bin Hao1 , Bao Hong Shen1 , and Samrat Ganguly2
1
Dept. of Computer Science,
Arizona State University, Tempe, AZ 85287, USA
{asen, ling.zhou, binhao, bao}@asu.edu
2
Dept. of Broadband & Mobile Networks,
NEC Laboratories, USA
samrat@nec-lab.com

Abstract. The notion of service overlay network (SON) was proposed
recently to alleviate diﬃculties encountered in providing end-to-end quality of service (QoS) guarantees in the current Internet architecture. The
SONs are able to provide QoS guarantees by purchasing bandwidth from
individual network domains and building a logical end-to-end data delivery infrastructure on top of existing Internet. In this paper, we consider a generalized framework for SON, which is categorized based on
three diﬀerent characteristics: a) single-homed/multi-homed end-system
b) usage-based/leased cost model and c) capacitated/uncapacitated network. We focus on the algorithmic analysis of the topology design problem for the above generalized SON. We prove that for certain case,
polynomial-time optimal algorithm exists, while for other cases, the topology design problem is NP-complete. For the NP-complete cases, we provide approximation algorithms and experimental results.

1

Introduction

The Internet today comprises multiple independently operated networks (autonomous systems or domains) joined at the peering points. The independently
operated networks (often Internet Service Providers, ISPs) may have an interest
in providing QoS guarantees within their own network, but they do not have
any incentive to provide service guarantees to customers of other remote ISPs.
The notion of service overlay network (SON) was proposed in [3] to overcome
this problem, so that end-to-end guarantees can be provided to the customers
of diﬀerent ISPs. Service overlay network is an outcome of the recent studies on
overlay networks such as Detour [10], Resilient Overlay Network [1] and Internet
Indirection Infrastructure [11].
The SONs are able to provide end-to-end QoS guarantees by building a logical
delivery infrastructure on top of the existing transport network by purchasing
bandwidth from individual network domains. The SONs provide various ﬂexibilities in deploying and supporting new services by allowing the creation of
H. de Meer and N. Bhatti (Eds.): IWQoS 2005, LNCS 3552, pp. 54–68, 2005.
c IFIP International Federation for Information Processing 2005


On Topological Design of Service Overlay Networks

End system 1

55

Provider
Node 1

Provider
Node 2

Provider
Node 3

Provider
Node 4

Provider
Node 5

ISP 1

ISP 2

ISP 3

ISP 4

ISP 5

End System
1

End System
2

End System
3

End System
4

End System
5

End system 2

PN 2
ISP 1

End system 3
ISP 2

PN 3

PN 1
ISP 3
ISP 5

ISP 4

PN 4

PN 5

Provider Node
End system 5

End system
End system 4

Fig. 1. Service Overlay Network Model

Fig. 2. Relationship between Provider
Nodes, End-systems and ISPs

service-speciﬁc overlay network without incorporating changes in the underlying network infrastructure. This mechanism can be utilized to support applications for fault-tolerance, multi-cast communication, security, ﬁle sharing and
QoS [1, 2, 7].
We consider the SON model described in [8], where it is constructed on
top of an infrastructure of ISPs and is capable of providing QoS guarantees to
a set of customers. Because of this capability, the network is referred to as a
QoS Provider Network or Provider Network. The provider network comprises
a collection of provider nodes, and a set of customers referred to as the endsystems or enterprises. The provider nodes and the end-systems gain access to
the Internet through ISPs. An illustration of the SON is given in Figure 1. The
provider nodes are connected to each other through ISPs and the end-systems are
also connected to the provider nodes through ISPs. Two provider nodes are said
to be connected to each other, if they are connected to the same ISP. Similarly,
an end-system is said to be connected to a provider node if they are connected
to the same ISP. Figure 2 illustrates the relationships between provider nodes,
ISPs and end-systems. The provider node buys services (guaranteed bandwidth)
from ISPs and sells them to the end-systems with end-to-end service guarantees.
Currently, there exists at least one commercial service overlay network (Internap
[6]) that closely resembles the model used in this paper as well as in [8].
The topology design problem of a SON can be described as follows: Given
a set of end-systems, provider nodes, access cost of traﬃc from an end-system
to a provider node, transport cost of traﬃc among provider nodes, traﬃc demand for each pair of end-systems, ﬁnd the least cost design that satisﬁes the
traﬃc bandwidth demand between each pair of end-systems. Our work is motivated by the recent study done by Vieira et.al. [8] on topology design problem for a speciﬁc SON model. In this paper, we introduce a generalized framework for SON, which provides a comprehensive view of the overall topology
design space. We categorize the generalized SON model based on the following
scenarios:

56

A. Sen et al.

1
2

2

2
1

2

2

20

1
2

20

2

2
1

1
2

2

2

2
1

2

2

15
20
10

20

10
8

8

2

4

20

3

4

2

2

4

8
3

2

4

3

2
3

4

2
3

4

3

Provider Node

Provider Node

Provider Node

End system

End system

End system

Fig. 3. Service Overlay Net- Fig. 4. Single-homed Solu- Fig. 5. Multi-homed Solution for SON Design
tion for SON Design
work Model

– Single-homed vs multi-homed: The term multihoming is generally used
to indicate that an end-system is connected to multiple ISPs [12]. In the
context of SON, we extend this notion and let multihoming refer to the
scenario where one end-system can be connected to multiple provider nodes
instead of multiple ISPs. In a multi-homed environment, an end-system has
more ﬂexibility in connecting to a set of provider nodes. This ﬂexibility
enables the designer to ﬁnd a lower cost solution. Figures 4 and 5 show the
solution of the same problem in single-homed and multi-homed scenarios,
where the cost of the single-homed design is 26 and that of the multi-homed
is 18.
– Usage-based vs leased(fixed) cost model: In the usage-based cost model,
the cost of the link is proportional to the volume of data sent through the
link. In a leased or ﬁxed cost model, we assume that each link has an associated cost that is independent of the traﬃc sent through it. Such ﬁxed cost
scenario is often applicable to enterprises who buy leased lines from ISPs at
a ﬂat rate.
– Capacitated vs uncapacitated network: In case of a capacitated network, we assume that any link in the SON has a capacity bound that cannot
be exceeded. While in an uncapacitated case, there exist no such constraints.
It may be noted that the authors in [8] provide solution only for the singlehomed, uncapacitated network with usage-based cost model. In this paper, we
provide results of our comprehensive study of the SON design problem. The key
contributions of this paper are as follows:
– We prove that the SON topology design problem with (a) multi-homed enterprise, (b) usage-based cost model and (c) uncapacitated network can be
solved in polynomial time.
– We prove that the SON topology design problem with (a) single-homed
enterprise, (b) usage-based cost model and (c) uncapacitated network is NPComplete.
– We prove that the SON topology design problem with (a) single-homed/multihomed enterprise and (b) ﬁxed cost model is NP-Complete in both capaci-

On Topological Design of Service Overlay Networks

57

Table 1. Complexity results for diﬀerent versions of SON design problem
Uncapacitated Network
Capacitated Network
Cost Model
Single-homed Multi-homed Single-homed Multi-homed
Usage-based cost
NPC
Poly. Solution
NPC
NPC
Fixed cost
NPC
NPC
NPC
NPC

tated and uncapacitated network scenarios. We present approximation algorithms for the solution of uncapacitated version of these problems.
– We show that all the four problems in the capacitated version of the SON
design problem are NP-Complete.
A summary of the complexities involved in the topology design problem for the
various cases is shown in Table 1.

2

Problem Formulation

The optimal topology design problem of a SON is described in the previous
section. We consider diﬀerent versions of the problem based on diﬀerent application environments: (i) single-homed or multi-homed end-system, (ii) usage-based
or fixed cost model [9], and (iii) finite or infinite capacity links. The notations
used in this paper are same as in [8] and are given in Table 2.
The access cost αij of an access link connecting an end-system ESi to a
provider node P Nj refers to the cost of transmitting one unit of data over that

Table 2. Basic Notations
ESi
P Nj
M
N
αij
α
lij
L
bij
B
ωij
Ω

end-system i
Provider node i
Number of end-systems
Number of provider nodes
Access cost (per unit of reserved bandwidth) for traﬃc from ESi to P Nj
Access cost matrix for traﬃc from all ESi to all P Nj
Transport cost (per unit of reserved bandwidth) for traﬃc on the transport
link from P Ni to P Nj
Transport cost matrix for traﬃc on the transport link from all P Ni to all P Nj
Cost of least-cost route (per unit of reserved bandwidth) for traﬃc between
P Ni to P Nj
Cost of least-cost route matrix for traﬃc between all P Ni to all P Nj
Reserved bandwidth for traﬃc from ESi to ESj
Reserved bandwidth matrix for traﬃc from all ESi to all ESj

58

A. Sen et al.

link in usage-based cost model and the cost of transmitting any number of units
of data in ﬁxed cost model. In case ESi can be connected to P Nj through more
than one ISP, αij represents the cheapest way of connecting ESi to P Nj . If ESi
cannot reach P Nj through any ISP, access cost αij = ∞. The transport cost lij
of a transport link connecting P Ni to P Nj refers to the cost of transmitting one
unit of data over that link in usage-based cost model and the cost of transmitting
any number of units of data in ﬁxed cost model. In case P Ni can be connected to
P Nj through more than one ISP, lij represents the cheapest way of connecting
P Ni to P Nj . If P Ni cannot reach P Nj through any ISP, transport cost lij = ∞.
From the set of input data, we construct a graph GESP N = (VESP N , EESP N ),
where the vertex set VESP N consists of two diﬀerent types of nodes, VP N and
VES , representing the provider nodes and the end-systems respectively. Similarly, the edge set EESP N consists of two diﬀerent types of edges, EP N,P N and
EES,P N . For any vi , vj ∈ VP N , there is an edge in EP N,P N connecting them
with an associated weight lij . lij values for all pairs of provider nodes are denoted by matrix L. The length of the shortest path between vi and vj is denoted
by bij . bij values for all pairs of provider nodes are denoted by matrix B. For
any vi ∈ VES and vj ∈ VP N , there is an edge in EES,P N connecting them with
an associated weight αij . αij values for all end-system to provider node pairs
are denoted by matrix α. For any vi , vj ∈ VES , there is a traﬃc demand ωij
associated with this ordered pair of nodes (ωij may be zero). Traﬃc demands
for all pairs of end-systems are denoted by matrix Ω. An illustration of such a
graph is shown in Figure 3. In this example, there is a non-zero traﬃc demand
for the pairs (ES1 , ES3 ), (ES1 , ES4 ) and (ES2 , ES3 ). In the ﬁxed cost model,
the actual bandwidth request by each pair is not relevant. The optimal solutions
for the single-homed and multi-homed versions of the SON design problem are
shown in Figures 4 and 5 respectively. The optimal cost of the single-homed
version is 26, whereas the multi-homed version is 18.
The main diﬀerence between usage-based model and ﬁxed cost model is how
the access and transport costs are calculated, especially when the same edge
appears on more than one path between end-system pairs. For example, if an
edge epq ∈ EES,P N is used for transferring ωi,j amount of data from ESi to
ESj and also used for transferring ωi,k amount of data from ESi to ESk , then
the cost of using this link will be αpq (ωi,j + ωi,k ) in usage-based cost model and
only αpq in the ﬁxed cost model. Similarly, If an edge epq ∈ EP N,P N is used for
transferring ωi,j amount of data from ESi to ESj and ωr,s amount of data from
ESr to ESs , then the cost of using this link will be lpq (ωi,j +ωr,s ) in usage-based
cost model and only lpq in the ﬁxed cost model.

3

SON Topology Design - Algorithms and Complexities

We consider eight diﬀerent versions of the SON topology design problem. We
show that only one of the four diﬀerent versions with uncapaciated network
model is polynomial-time solvable, and the other three are NP-complete. Since
uncapacitated version of the problem is just a special case of the capacitated

On Topological Design of Service Overlay Networks

59

version, the NP-Completeness of the capacitated version will follow from the
NP-Completeness of the uncapacitated version. The complexity results of various
versions are summarized in Table 1.
3.1

SON Design Problem with Multi-homed Enterprise,
Uncapacitated Network and Usage-Based Cost Model
(SONDP-MHE/UN/UBC)

Instance: Given a graph GESP N = (VESP N , EESP N ) with matrices L, α, Ω,
and a positive integer K.
Question: Is it possible to construct a SON topology with total cost less than
or equal to K so that all traﬃc demands given in matrix Ω are satisﬁed?
Theorem 1. SON design problem with MHE/UN/UBC can be solved in polynomial time.
Proof: From the special properties of this problem, it is no hard to see that the
cost of establishing a path to transmit wij units of data from ESi to ESj is independent of the cost of establishing paths for other pairs. Therefore, we can minimize the total cost by establishing a shortest path for each pair of end-systems
separately in GESP N , and thus obtain the optimal solution for this problem.
The computation complexity of this algorithm is O(k(|VESP N | log |VESP N |+
|EESP N |)), where k is the number of end-systems pairs that need to transfer data
between each other.
3.2

SON Design Problem with Single-Homed Enterprise,
Uncapacitated Network and Usage-Based Cost Model
(SONDP-SHE/UN/UBC)

From the transport cost matrix L (in Table 2), we compute the least-cost route
matrix B. The problem instance is described in terms of matrix B.
Instance: Given a graph GESP N = (VESP N , EESP N ) with matrices B, α, Ω,
and a positive integer K.
Question: Is it possible to construct a SON topology with total cost less than
or equal to K so that all traﬃc demands given in matrix Ω are satisﬁed, and
meanwhile each end-system is connected to only one provider node?
Theorem 2. SON design problem with SHE/UN/UBC is NP-complete.
Proof: We can restate the question more formally in the following way:
Question: Is there a function g : {1, 2, ..., M } → {1, 2, ..., N }, such that
M 
M

i=1 j=1

wij (αig(i) + bg(i)g(j) + αjg(j) ) ≤ K?

(1)

60

A. Sen et al.

Clearly SONDP-SHE/UN/UBC is in NP. We prove its NP-Completeness by
reduction from the Matrix Cover problem [4]. The reduction maps a Matrix
Cover instance (an n × n matrix A = {aij }, K) to a SONDP-SHE/UN/UBC

instance (Ω,
α, K ), so that there is a function f : {1, 2, ..., n} → {−1, +1}
B,
n n
such that
i=1
j=1 aij f (i)f (j) ≤ K if and only if there is a function g :
M M
{1, 2, ..., M } → {1, 2, ..., N } such that i=1 j=1 wij (αig(i) +bg(i)g(j) +αjg(j) ) ≤

K . Given any instance of Matrix Cover: An n × n matrix A = {aij } with
nonnegative integer entries, and an integer K, we construct the instance for
SONDP-SHE/UN/UBC problem as follows:
1. Let M = n. For the M × M bandwidth reservation matrix Ω = {wij },
∀1 ≤ i ≤ M, 1 ≤ j ≤ M , let wij = 1.
2. Let N = 2M . For the N × N transport matrix B = {bij }, ∀1 ≤ k ≤ M, 1 ≤
l ≤ M , let
akl + alk
+ max;
2
akl + alk
+ max;
=−
2

b2k,2l =
b2k,2l−1

akl + alk
+ max;
2
akl + alk
+ max;
=−
2

b2k−1,2l−1 =
b2k−1,2l

(2)

where max is the maximum element of matrix A in the instance of Matrix
Cover. It is added to make sure that bij is nonnegative.
3. For the M × N Access Cost matrix α = {αij }, ∀1 ≤ i ≤ M

αij =

0 if j = 2i − 1, or 2i
∞ otherwise

(3)

4. Let K  = K + M 2 · max = K + n2 · max.
The construction can be done in polynomial time. To complete the proof,
we show that this transformation is a reduction. Suppose for the instance of
matrix
nproblem, there is a function f : {1, 2, .., n} → {−1, +1} such
ncover
that
i=1
j=1 aij f (i)f (j) ≤ K, then we can build the corresponding g :
{1, 2, .., M } → {1, 2, ..., N } for the instance of SONDP-SHE/UN/UBC as follows:
∀1 ≤ i ≤ M (M = n, N = 2M )

2i
if f (i) = +1
g(i) =
2i − 1 if f (i) = −1

(4)

Due to the process of construction, there exists a relationship between the objective functions of the two problems, as shown in the following table. Therefore,
given the g function we have build, it is true that

On Topological Design of Service Overlay Networks

61

Table 3. Relationship between two objective functions
f (i)
+1
−1
+1
−1

f (j) aij f (i)f (j) aji f (j)f (i) g(i) g(j)
bg(i)g(j)
bg(j)g(i)
aij +aji
aij +aji
+1
aij
aji
2i
2j
+
max
+ max
2
2
a +a
aij +aji
−1
aij
aji
2i − 1 2j − 1 ij 2 ji + max
+ max
2
a +a
a +a
−1
−aij
−aji
2i 2j − 1 − ij 2 ji + max − ij 2 ji + max
a +a
a +a
+1
−aij
−aji
2i − 1 2j − ij 2 ji + max − ij 2 ji + max
M
M 


wij (αig(i) + bg(i)g(j) + αjg(j) )

i=1 j=1

=

=

M 
M

i=1 j=1
n 
n


bg(i)g(j)

(wij = 1, αig(i) = αjg(j) = 0)

aij f (i)f (j) +

i=1 j=1

n 
n


(5)
max

i=1 j=1

= ≤ K + n2 · max
=K



Conversely, suppose that for the instance we have built for the SONDPSHE/UN/UBC problem, there is a function g : {1, 2, .., M } → {1, 2, ..., N } such
M M

that i=1 j=1 wij (αig(i) + bg(i)g(j) + αjg(j) ) ≤ K . Then ∀1 ≤ i ≤ M , g(i)
must be equal to 2i or 2i − 1, otherwise αig(i) will be equal to ∞. Then we can
build the corresponding f : {1, 2, .., n} → {−1, +1} for the instance of Matrix
Cover as follows:
∀1 ≤ i ≤ n(n = M, N = 2M );

+1 if g(i) = 2i
f (i) =
−1 if g(i) = 2i − 1

(6)

Similarly, due to the relationship shown in Table 3, it is true that
n 
n


aij f (i)f (j) =

i=1 j=1

M 
M


bg(i)g(j) −

i=1 j=1

=

M 
M


wij (αig(i) + bg(i)g(j) + αjg(j) ) −

n 
n

i=1 j=1

≤ K − n · max
=K
This proves the theorem.

max

i=1 j=1

i=1 j=1


n 
n


2

max (7)

62

A. Sen et al.

3.3

SON Design Problem with Multi-homed/Single-Homed
Enterprise, Uncapacitated Network and Fixed Cost Model
(SONDP-MHE/UN/FC)

In this section we consider both the multi-homed and single-homed versions of
the uncapacitated network with ﬁxed cost model, which are described as follows:
Instance: Given a graph GESP N = (VESP N , EESP N ) with matrices L, α, Ω,
and a positive integer K.
Question: Is it possible to construct a SON topology with total cost (under
ﬁxed cost model) less than or equal to K so that all traﬃc demands given in
matrix Ω are satisﬁed?
Instance: Given a graph GESP N = (VESP N , EESP N ) with matrices L, α, Ω,
and a positive integer K.
Question: Is it possible to construct a SON topology with total cost (under
ﬁxed cost model) less than or equal to K so that all traﬃc demands given in
matrix Ω are satisﬁed, and meanwhile each end-system is connected to only one
provider node?
Theorem 3. SON design problems with MHE/UN/FC and SHE/UN/FC are
NP-complete.
Proof. Clearly, the SONDP-SHE/UN/FC problem belongs to NP. We prove its
NP-Completeness by reduction from the Steiner Tree Problem. Given any instance of Steiner Tree Problem: undirected graph G(V, E), weights c : E(G) →
R+ , the set of terminals S ⊆ V (G), and a positive integer K, we construct an
instance (G , L, α, Ω, K  ) for SONDP-SHE/UN/FC problem as follows: G =
(V ∪ U, E ∪ E  ), where V is the set of provider nodes; U = {u|u is new added
node adjacent to v, ∀v ∈ S} is the set of end-systems; E  = {(u, v)|∀v ∈ S};
∀e ∈ E, l(e) = c(e); ∀e ∈ E  , α(e) = 0; ∀ui , uj ∈ U, ω(ui , uj ) = 1; and K  = K.
The construction can be done in polynomial time.
Now we show that this transformation is a reduction. Suppose for the instance
of Steiner Tree problem, there is a Steiner tree T = (VST , EST ) for S in G with
total cost c(EST ) less than or equal to K, then we can construct a solution T  for
SONDP-SHE/UN/FC problem as follows: T  = (VST ∪ U, EST ∪ E  ), where T 
connects all the end-systems, which means the bandwidth requirement for each
pair of end-systems is satisﬁed. In addition, each end-system is connected to only
one provider node, and the cost of T  is less than or equal to K  . Similarly, given
the solution T  = (V  ∪ U, E  ∪ E  ) for the instance of the SONDP-SHE/UN/FC
problem, we can construct a corresponding solution T for Steiner Tree problem
as T = (V  , E  ), by removing all the end-systems and the associated edges. T
is a solution for the Steiner Tree Problem, since all the terminals are connected
and c(E(T )) is no greater than K. Therefore, the transformation is a reduction
and the SONDP-SHE/UN/FC problem is NP-Complete.

On Topological Design of Service Overlay Networks

Minimize

M 
N


αi,j yi,j +

i=1 j=1
N


Subject to

k,l
qi,j
>= 1,

N
−1


N


li,j zi,j

63

(8)

i=1 j=i+1

for 1 ≤ i ≤ M , wk,l > 0; (9)

j=1
N

l=1

i,k
xj,l
i,k + qk,j −

N


i,k
xl,j
i,k − qi,j = 0,

for 1 ≤ j ≤ N , wi,k > 0; (10)

l=1
M
M 


k,l
qi,j
≤ M 2 × yi,j ,

for 1 ≤ i ≤ M , 1 ≤ j ≤ N ; (11)

k=1 l=1
N 
N 



l,j
2
xj,l
i,k + xi,k ≤ 2N × zj,l ,

for 1 ≤ j < l ≤ N ; (12)

i=1 k=1

yi,j = 0/1,

for 1 ≤ i ≤ M , 1 ≤ j ≤ N ; (13)

zj,l = 0/1,

for 1 ≤ j < l ≤ N ; (14)

k,l
qi,j

= 0/1,

for 1 ≤ i, k, l ≤ M , 1 ≤ j ≤ N ; (15)

xj,l
i,k

= 0/1,

for 1 ≤ i, k ≤ M , 1 ≤ j, l ≤ N ; (16)

Fig. 6. ILP for SONDP-MHE/UN/FC Problem

It’s true that the instance of SONDP-SHE/UN/FC problem we constructed
can also be seen as a special instance for SONDP-MHE/UN/FC problem, since
in the problem description, there is no constraint on the number of provider
nodes each end-system can connect to. Therefore, a similar proof can show that
SONDP-MHE/UN/FC problem is also NP-Complete.
3.4

Optimal Solution for SON Topology Design Using Integer
Linear Programming

In this section, we provide a 0-1 integer linear programming formulations for both
SONDP-SHE/UN/FC and SONDP-MHE/UN/FC problems. The formulation
for multi-homed problem is shown in Figure 6. For single-homed problem, we
only need to add one more set of constraints for
the ILP to ensure that exactly
N
one access link is used for each end-system, i.e. j=1 yi,j = 1 for 1 ≤ i ≤ M.
The variable yi,j = 1 indicates that bandwidth is reserved on the access
link from end-system i to provider node j. The variable zj,l = 1 indicates that
bandwidth is reserved on the transport link between provider node j and provider
k,l
= 1 indicates that the traﬃc from end-system k to endnode l. The variable qi,j
system l is using the access link between end-system i and provider node j,where
i is equal to k or l. The variable xj,l
i,k = 1 indicates that traﬃc from end-system
i to end-system k is using transport link between provider node j and provider
node l.

64

A. Sen et al.

The objective function in Figure 6 is the sum of the costs of access links
and transport links. Constraint (9) ensures that at least one access link is used
to connect an end-system to the overlay network. Constraint (10) is for ﬂow
conservation at each provider node. No traﬃc is initiated or terminated at a
provider node. Constraints (11) and (12) determine the access links and the
transport links used by the solution.

4

Approximate Algorithms for SON Topology Design

In this section we present approximate algorithms for the solution of SHE/UN/FC
and MHE/UN/FC problems. It may be noted that we have shown that the
MHE/UN/UBC problem is polynomial-time solvable, and approximate solution
for the SHE/UN/UBC problem has been presented in [8]. Since in the ﬁxed cost
model, the cost of each link is independent of the amount of data transmitted
on it, the amount of reserve bandwidth ωij between end-systems ESi and ESj
can be ignored. If ωij > 0, then ESi and ESj should be connected in the resulting topology, otherwise, they don’t need to be connected. Therefore, from
the reserve bandwidth matrix Ω, we construct a connectivity requirement set
R = {(s1 , t1 ), . . . , (sk , tk )}, where each (si , ti ) is an ordered pair of end-systems
which has positive bandwidth demand.
We provide three diﬀerent heuristics for the solution of SHE/UN/FC problem: (i) Randomized Heuristic, (ii) Gain-based Heuristic and (iii) Spanning Tree
based heuristic. It may be noted that by shortest path between any two nodes
(end-systems or provider nodes), we implies the shortest path that only uses
provider nodes as intermediate nodes. In analyzing the computational complexity of each heuristic, M is the number of end-systems, N is the number of
provider nodes and k is the number of connections to be established.
Heuristic 1: Randomized Approach
Step 1: Intialize CRH = ∞ and DRH = 0.
Step 2: Repeat steps 3 to 13 W times (the parameter W is set by the user to
determine the number of times the random process is repeated).
Step 3: Set R = {(s1 , t1 ), (s2 , t2 ), . . . , (sk , tk )}.
Step 4: Randomly choose a pair (si , ti ) from R, and remove it.
Step 5: Compute the shortest path from si to ti . Suppose in the computed
shortest path, si is connected to provider node Pj , and ti is connected
to Pk . Call these provider nodes gateways for si and ti , and denote
them G(si ) and G(ti ) respectively.
Step 6: Set DRH = DRH + {weight of the shortest path computed in step 5}.
Step 7: Set the weights of all the links on the computed shortest path zero.
Step 8: Repeat steps 9-12 till R is empty.
Step 9: Randomly choose a pair (si , ti ) from R, and remove it.
Step 10: If G(si ) and G(ti ) are known, compute the shortest path between
G(si ) and G(ti ); else if G(si ) is known while G(ti ) is not known, compute the shortest path between G(si ) and ti ; else if G(si ) is not known

On Topological Design of Service Overlay Networks

Step
Step
Step
Step

65

while G(ti ) is known, compute the shortest path between si and G(ti );
else if neither G(si ) nor G(ti ) is known, compute the shortest path
between si and ti .
11: Set DRH = DRH + {weight of the shortest path computed in step 10}.
12: Set weights of all the links on the computed shortest path zero.
13: Set CRH = min(CRH , DRH ).
14: Output CRH . This is the cost of the solution.

Computational Complexity: The computational complexity of the Randomized Heuristic is O(kW (M + N )log(M + N )), where W is the number of times
the random process is repeated.
Heuristic 2: Gain Based Approach
Step
Step
Step
Step

Step
Step
Step
Step
Step
Step
Step
Step
Step

1:
2:
3:
4:

Initailize CGBH = 0.
Set R = {(s1 , t1 ), (s2 , t2 ), . . . , (sk , tk )}.
Compute shortest paths for all pairs of end-systems in R.
Identify the source-destination pair (si , ti ) from R that has the longest path
length. Remove this pair from R. Suppose in the computed shortest path, si is
connected to provider node Pj , and ti is connected to Pk . Call these provider
nodes gateways for si and ti , and denote them G(si ) and G(ti ) respectively.
5: Set CGBH = CGBH + {weight of the path chosen in step 4}.
6: Set the weights of all the links on the path chosen in step 4 zero.
7: Repeat steps 8-12 till R is empty.
8: Compute shortest paths for all the pairs of end-systems in R. If either G(si )
or G(ti ) is identiﬁed in one of the earlier iterations, in the shortest path
computation, G(si ) and G(ti ) should replace si and ti respectively.
9: Note the gain, i.e. the change in path length, for all the pairs in the set R.
10: Identify the end-system pair (si , ti ) with largest gain. Remove it from R.
11: Set CGBH = CGBH + {weight of the path chosen in step 10}.
12: Set the weights of all the links on the path chosen in step 10 zero.
13: Output CGBH . This is the cost of the solution.

Computational Complexity: The computational complexity of the Gainbased Heuristic is O(k(M + N )3 ). A diﬀerent implementation can realize this in
O(k 2 (M + N )2 ). The implementation should be chosen based on the values of
M, N and k.
Heuristic 3: Spanning Tree Based Approach
Step 1: Initialize CST H = 0.
Step 2: Set R = {(s1 , t1 ), (s2 , t2 ), . . . , (sk , tk )}.
Step 3: Compute the minimum spanning tree M STP N of the subgraph induced by
the Provider Nodes. Set CST H = Cost of M STP N .
Step 4: Connect each end-system to its nearest provider node. Update CST H with
the additional cost of connecting all the end-systems.
Step 5: Remove those provider nodes from M STP N that are not used to connect any
end-systems pair, and also remove the cost used to connecting them from
CST H .
Step 6: Output CST H . This is the cost of the solution.

66

A. Sen et al.

Computational Complexity: The computational complexity of the Spanning
Tree based Heuristic is O((M + N )2 log(M + N )).
The approximate algorithms for the multi-homed version are similar to the
ones for the single-homed version, except that end-system is no longer required
to connect to only one provider node. So the shortest path for each end-system
pair should be computed directly, and the gateway information is not needed.

5

Experimental Results

In this section, we compare the performance of our three heuristics for the
SONDP-SHE/UN/FC problem against the optimal solution obtained by solving
ILP. Simulation experiments are carried out using randomly generated input
sets. We develop a random graph generator, which takes as input the number
of nodes and average degree, and generates connected undirected graphs. It also
randomly generates the weights on the links from a uniform distribution over a
speciﬁed range (we use the ranges of 3 to 8, and 3 to 80 for our experiments).
The graphs produced by the generator are used as the network for the provider
nodes. The random weights on the edges are the transport cost among provider
nodes. Once the network for provider nodes is generated, a speciﬁed number of
end-systems are connected to the provider nodes in the following way:
Step 1: The degree of an end-system is randomly generated from a uniform distribution over the range of 1 to 10.
Step 2: The provider node neighbors of an end-system are randomly generated with
uniform distribution.
Step 3: The access cost from the end-system to the provider node is randomly generated with a uniform distribution over the range of 3 to 8 (small access cost
variation) or 3 to 80 (large access cost variation).
Step 4: Communication requests between end-systems are also randomly generated.

In our simulation experiments, we compute the optimal cost of SON design
and the costs obtained by three diﬀerent heuristics. These results are presented in
Table 4. The time taken by the optimal solution as well as the heuristic solutions
are also presented. In Table 4, M and N represent the number of end-systems
and provider nodes respectively. There could potentially be M (M −1)/2 possible
requests between M end-systems. The term Req% represents the percentage of
M (M − 1)/2 possible requests that is considered for the instance. The term
Cost-variation ratio is deﬁned to be the ratio of the cost diﬀerence between the
heuristic solution(s) and the optimal solution to the cost of the optimal solution.
The values of cost-variation for three diﬀerent heuristics are presented. It may
be observed that ILP fails to ﬁnd a solution within a reasonable amount of time
when the problem instance increases in size. The heuristics however are able to
produce solutions for these instances. As noted earlier, the link cost distribution
is taken to be 3 to 8 for some of the instances and 3 to 80 for the rest. We did
not notice any perceptible impact of the variation of the link weights on results.
From the experiment results, it may be concluded that Heuristic 2 produces the
best solution for most of the instances, whereas Heuristic 3 produces the solution

On Topological Design of Service Overlay Networks

67

Table 4. Simulation Results for SONDP-SHE/UN/FC Problem
Instance
Cost
Cost-var. Ratio (%) Running Time (s)
# M N Req% Opt H1 H2 H3
H1
H2
H3
Opt H1 H2 H3
1* 10 10 44
124 134 134 142 8.1+ 8.1+
14.5
1 < 1< 1 < 1
2* 12 10 36
93 131 128 141 40.9 37.6+ 51.6
< 1 < 1< 1 < 1
3* 15 10 29
103 141 165 145 36.9+ 60.2
40.8
<1 1 <1 <1
4
20 15 53
94 130 124 132 38.3 31.9+ 40.4
3 < 1< 1 < 1
5
25 15 42
102 150 140 144 47.1 37.3+ 41.2
2
1 1
<1
6
30 25 34
143 186 191 202 30.1+ 33.6
41.3
40 1 1
<1
7
35 25 29
129 195 185 205 51.2 43.4+ 58.9
14 3 1
<1
8* 40 30 26
704 1083 1247 1073 53.8
77.1 52.4+
43 3 1
<1
9
45 35 23
169 301 285 309 78.1 68.6+ 82.8
11 3 2
<1
10* 50 40 20
904 1646 1475 1441 82.1
63.2 59.4+
68 5 3
<1
11 55 45 19
237 393 364 371 65.8 53.6+ 56.5
693 6 4
<1
12* 60 50 17 1285 2245 2208 2004 74.7
71.8 56.0+
72 6 6
<1
13 65 55 16
242 408 392 445 68.6 62.0+ 83.9
325 8 8
<1
14* 70 60 14 1464 2038 1962 2248 39.2 34.0+ 53.6
578 9 8
<1
15 75 65 14
295 496 482 548 68.1 63.4+ 85.8
422 11 11 < 1
16 85 75 12
313 566 552 594 80.8 76.4+ 89.8
176 14 17 < 1
17 95 85 11 N/A 608 592 638 N/A N/A N/A N/A 19 25 < 1
18* 80 70 13 N/A 2721 2851 2726 N/A N/A N/A N/A 13 14 < 1
19 105 95 10 N/A 673 674 729 N/A N/A N/A N/A 23 34 < 1
+: Result with best cost variation ratio
*: Link cost is between 3 and 80; otherwise, it is between 3 and 8.
N/A: ILP failed to ﬁnd an optimal solution.

in the least amount of time. Clearly, a tradeoﬀ between quality of solution and
the time taken to ﬁnd it exists in these two heuristics. It may be noted that
all three heuristics produce a reasonable quality solution in a fraction of time
needed to ﬁnd the optimal solution.

References
1. D. G. Anderson, H. Balakrishnan, M.F. Kaashoek and R. Morris, “Resilient Overlay Network”, Proc. 18th ACM SOSP 2001, Banﬀ, October 2001.
2. A. Keromytis, V. Mishra and D. Rubenstein, “Secure Overlay Networks”, Proc. of
SIGCOMM’02, pp. 61-72, 2002.
3. Z. Duan, Z. L. Zhang and Y.T. Hou, “Service Overlay Networks:SLAs, QoS and
Bandwidth Provisioning”, Proc. 10th IEEE International Conference on Network
Protocols”, Paris, France, November 2002. Also in IEEE/ACM Trans. on Networking, vol. 11, no. 6, pp. 870-883, 2003.
4. M. Garey and D. Johnson, “Computers and Intractability: A guide to the theory
of NP-COmpleteness”, W.H. Freeman, 1979.

68

A. Sen et al.

5. X. Gu, K. Nahrstedt, R. H. Chang and C. Ward, “QoS-Assured Service Composition in Managed Service Overlay Networks”, Proc. of 23rd IEEE International
Conference on Distributed Computing Systems, Providence, May 2003.
6. Internap Network Services Corporation, http://www.internap.com
7. L. Subramanian, I. Stoica, H. Balakrishnan and R. H. Katz, “OverQoS: Oﬀereing
Internet QoS using Overlays”, Proc. HotNET-I Workshop, October 2002.
8. S. L. Vieira and J. Liebeherr, “An algorithmic approach to topological design of
Service Overlay Networks”, Proc. of IWQoS’04, 2004, Montreal, Canada.
9. J. Walrand and P. Varaiya, “High Performance Communication Networks”, Morgan Kaufman Publishers, 2000.
10. S. Savage, T. Anderson, and et al. “Detour: a case for informed internet routing
and transport.” IEEE Micro, 19(1):5059, January 1999.
11. Ion Stoica, Daniel Adkins, Shelley Zhuang, Scott Shenker and Sonesh Surana,
“Internet Indirection Infrastructure,” Proc. of ACM SIGCOMM, August, 2002
12. A. Akella, J. Pang, A. Shaikh, B. Maggs and S. Seshan, “ A Comparison of Overlay
Routing and Multihoming Route Control,” In Proc. of ACM SIGCOMM, 2004

On Delay Tolerant Airborne Network Design
Shahrzad Shirazipourazad, Arun Das(B) , and Arunabha Sen
Computer Science and Engineering Program, School of Computing, Informatics and
Decision System Engineering, Arizona State University, Tempe, USA
{sshiraz1,arun.das,asen}@asu.edu

Abstract. Mobility pattern of nodes in a mobile network has signiﬁcant impact on the connectivity properties of the network. Due to its
importance in civil and military environments, and due to the several
complex issues present in this domain, one such mobile network that has
drawn attention of researchers in the past few years is Airborne Networks
(AN). Since the nodes in an airborne network (AN) are heterogeneous
and mobile, the design of a reliable and robust AN is highly complex and
challenging. This paper considers a persistent backbone based architecture for an AN where a set of Airborne Networking Platforms (ANPs)
such as aircrafts, UAVs and satellites, form the backbone of the AN. As
ANPs may be unable to have end-to-end paths at all times due to the
limited transmission ranges of the ANPs, the AN should be delay tolerant and be able to transmit data among ANPs within a bounded time.
In this paper we propose techniques to compute the minimum transmission range required by the ANPs in such delay tolerant airborne networks.
Keywords: Airborne Networks · Airborne Networking Platform · Delay
tolerant networks

1

Introduction

An Airborne Network (AN) is a mobile ad-hoc network that utilizes a heterogeneous set of physical links (RF, Optical/Laser and SATCOM) to interconnect a
set of terrestrial, space and highly mobile Airborne Networking Platforms (ANPs)
such as satellites, aircrafts and Unmanned Aerial Vehicles (UAVs). Airborne networks can beneﬁt many civilian applications such as air-traﬃc control, border
patrol, and search and rescue missions. The design, development, deployment and
management of a network with mobile nodes is considerably more complex and
challenging than a network of static nodes. This is evident by the elusive promise of
the Mobile Ad-Hoc Network (MANET) technology where despite intense research
activity over the past years, mature solutions are yet to emerge [1,2]. One major
challenge in the MANET environment is the unpredictable movement pattern of
the mobile nodes and its impact on the network structure. In case of an AN, there
exists considerable control over the movement pattern of the mobile platforms.
For instance, to realize the functional goals of an AN, Air Force personnel can
c ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2017

Y. Zhou and T. Kunz (Eds.): ADHOCNETS 2016, LNICST 184, pp. 357–368, 2017.
DOI: 10.1007/978-3-319-51204-4 29

358

S. Shirazipourazad et al.

specify the controlling parameters of the network, such as the location, ﬂight path
and speed of the ANPs that form the backbone of the AN. Such control provides
designers an opportunity to develop a topologically stable network even when the
network nodes are highly mobile.
It is increasingly being recognized in the networking research community that
the level of reliability needed for continuous operation of an AN may be diﬃcult
to achieve through a completely mobile, infrastructure-less network [3]. In order
to enhance reliability and scalability of an AN, Milner et al. in [3] suggested the
formation of a backbone network with ANPs. In order to deal with the reliability
and scalability issues of an AN, we consider an architecture for an AN where
a set of ANPs form the backbone of the AN. This set of ANPs may be viewed
as mobile base stations with predictable and well-structured ﬂight paths and the
combat aircrafts on a mission as mobile clients.
It is desirable that such a backbone network remain connected at all times
even though the topology of the network may change with the movement of the
ANPs. Such continuous network connectivity can be achieved if the transmission
range of the ANPs is suﬃciently large. However, a large transmission range also
implies high energy usage. Accordingly, one would like to know the smallest
transmission range for the ANPs which ensures connectivity at all times. In
[4], the authors precisely address this problem and propose techniques to ﬁnd
the smallest transmission range to ensure that the backbone network remains
connected at all times. The authors deﬁne the critical transmission range (CTR)
as the minimum transmission range of the ANPs to ensure that the dynamic
network formed by the movement of the ANPs remains connected at all times,
and present algorithms to compute the CTR when the ﬂight paths are known.
Due to the critical nature of ANs, the ANPs may be subject to adversarial attacks such as Electromagnetic Pulse attacks or network jamming. Such
attacks can impact speciﬁc geographic regions at speciﬁc times and if an ANP is
within the fault region during the time of attack, it will be rendered inoperable.
In [5], the authors consider the scenario where some of the AN nodes fail due
to a region fault, i.e., the failed nodes are conﬁned to a geographic region.
The authors deﬁne a critical transmission range in faulty scenario (CTR f ) as
the smallest transmission range necessary to ensure that the surviving nodes of
the AN remain connected irrespective of the location of the fault and the time
of the fault. The authors study this problem in [5] and propose techniques to
compute the CTRf .
It may be noted that in previous problems studied in [4,5] the backbone
network is required to be connected at all times. Accordingly, techniques were
proposed to compute CTR and CTRf in [4,5]. However, it may not be possible
to equip the transmitters of the ANPs with transmission ranges at least as large
as the CTR or CTRf . In such a scenario, the backbone network may be forced to
operate in a disconnected mode for some amount of time. It is also conceivable
that the data to be transmitted through the ANPs may be tolerant to some
amount of delay. Hence, ANPs may not need to have end-to-end paths at all
times but should be able to transmit data to each other within a bounded time.

On Delay Tolerant Airborne Network Design

359

These requirements lead us to study the problem of computation of critical
transmission range in delay tolerant airborne networks. More speciﬁcally, the
critical transmission range in delay tolerant network (CTRD ) is deﬁned as the
minimum transmission range necessary to ensure that every pair of nodes in the
backbone network can transmit at least one bit of data with each other within
a bounded time. In this paper we formulate the problem for computing CTRD
and propose techniques to compute CTRD . To the best of our knowledge this
problem has not been studied before.
The rest of the paper is organized as follows: In Sect. 2 we present the related
works, in Sect. 3 the AN architecture considered in this study is detailed, in
Sect. 4 the connectivity problem in delay tolerant ANs is formulated and solution
techniques proposed, ﬁnally in Sect. 5 we present our experiments.

2

Related Works

Due to the Joint Aerial Layer Networking (JALN) activities of the U.S. Air
Force, design of a robust and resilient ANs has received considerable attention
in the networking research community in recent years. It has been noted that
purely mobile ad-hoc networks (i.e., networks without infrastructure) have limitations with respect to reliability, data transmission, communication distance
and scalability [3,6]. Accordingly, the authors of [3,6] have suggested the introduction of a mobile wireless backbone network where the nodes serve as mobile
base stations (analogous to cellular telephony or the Internet backbone), in which
topology of the dynamic backbone network can be managed through the control
of movement patterns and transmission ranges of the backbone nodes.
Although there have been several studies on various aspects of mobile ad-hoc
networks, most of these studies consider infrastructure-less networks, whereas
the focus of this study is on ANs with a backbone infrastructure. Noted among
the studies on infrastructure-less mobile ad-hoc networks is topology control in
MANETs [6–9]. The goal of these studies is to assign power values to the nodes
to keep the network connected while reducing energy usage. The authors of
[7,8] have proposed distributed heuristics for power minimization in mobile adhoc networks, but have oﬀered no guarantees on their worst case performance.
Santi in [9] studied the minimum transmission range required to ensure network
connectivity in mobile ad-hoc networks.
He proved that the critical transmission


n
range for connectivity (CTR) is c ln
πn for some constant c where the mobility
model is obstacle free and nodes are allowed to move only within a bounded
area. In these studies the mobility patterns are not known unlike the problem
studied in this paper where it is assumed that the ﬂight paths of the ANPs are
known. Also, this paper studies the computation of the minimum transmission
range in a delay tolerant setting that has not been studied in previous studies.
As there may be times that the networks may have to operate in a disconnected mode, the last few years have seen considerable interest in the networking
research community in delay tolerant network (DTN) design [10]. The authors of
[11] survey challenges in enhancing the survivability of mobile wireless networks.

360

S. Shirazipourazad et al.

It is mentioned in [11] that one of the aspects that can signiﬁcantly enhance network survivability is the design of end-to-end communication in environments
where the path from source to destination is not wholly available at any given
instant of time. In this design, adjusting the transmit power of the nodes plays
an important role. Existing DTN research mainly focuses on routing problem
in DTNs [12,13]. The paper [14] provides a survey on routing algorithms for
DTN. For such algorithms to be eﬀective, every pair of nodes should be able to
communicate with each other within a bounded period of time. Papers such as
[15,16] have studied the problem of topology control in DTNs. In these papers,
the time evolving network is modeled by a space-time graph and it is assumed
that the this graph is initially connected and the problem is to ﬁnd the minimum
cost connected subgraph of the original graph. To the best of our knowledge, no
studies exist that study the computation of the minimum transmission range of
nodes in DTNs such that the time evolving network is connected over time.

3

System Model and Architecture

As mentioned previously, the level of reliability needed for continuous operation
of an AN may be diﬃcult to achieve through a completely mobile, infrastructureless network, and if possible, a backbone network with ANPs should be formed
to enhance reliability. It may be noted that in [5] the authors present the system
model and architecture of the AN considered in this paper. We summarize that
description in this section to preserve the completeness of our presentation.
In order to achieve the goal of reliability, an architecture of an AN is proposed
where a set of ANPs form a backbone network and provide reliable communication services to combat aircraft on a mission. In this architecture, the nodes
of the backbone networks (ANPs) may be viewed as mobile base stations with
predictable and well-structured ﬂight paths and the combat aircrafts on a mission
as mobile clients. A schematic diagram of this architecture is shown in Fig. 1.
In the diagram, the black aircrafts are the ANPs forming the infrastructure of
the AN (although in Fig. 1, only aircrafts are shown as ANPs, UAVs/satellites
can also be considered as ANPs). It is assumed that the ANPs follow a circular ﬂight path. The circular ﬂight paths of the ANPs and their coverage area
(shaded spheres with ANPs at the center) are also shown in Fig. 1. Thick dashed
lines indicate the communication links between the ANPs. The ﬁgure also shows
three ﬁghter aircrafts on a mission passing through a space known as an air corridor, where network coverage is provided by ANPs 1 through 5. As the ﬁghter
aircrafts move along their trajectories, they pass through the coverage area of
multiple ANPs and there is a smooth hand-oﬀ from one ANP to another when
the ﬁghter aircrafts move from the coverage area of one ANP to that of another.
At points P1, P2, P3, P4, P5 and P6 of Fig. 1, the ﬁghter aircrafts are connected
to the ANPs (4), (2, 4), (2, 3, 4), (3), (1, 3) and (1), respectively.
In this paper, it is assumed that two ANPs can communicate with each
other whenever the distance between them does not exceed the speciﬁed threshold (transmission range of the on board transmitter). We are aware that in an

On Delay Tolerant Airborne Network Design

361

Fig. 1. A schematic view of an Airborne Network

actual airborne network deployment, successful communication between two airborne platforms does not depend only on the distance, but also on various other
factors such as (i) the line of sight between the platforms [17], (ii) changes in
the atmospheric channel conditions due to turbulence, clouds and scattering,
(iii) the banking angle, the wing obstruction and the dead zone produced by the
wake vortex of the aircraft [18] and (iv) Doppler eﬀect. Moreover, the transmission range of a link is not a constant and is impacted by various factors, such as
transmission power, receiver sensitivity, scattering loss over altitude and range,
path loss over propagation range, loss due to turbulence and the transmission
aperture size [18]. However, the distance between the ANPs remains an important parameter in determining whether communication between the ANPs can
take place, and as the goal of this study is to understand the basic and fundamental issues of designing an AN with twin invariant properties of coverage and
connectivity, such simplifying assumptions are necessary and justiﬁed. Once the
fundamental issues of the problem are well understood, factors (i)–(iv) can be
incorporated into the model to obtain more accurate solutions.
For simplicity of the analysis, two more assumptions are made. We assume
that (i) all ANPs are ﬂying at the same altitude thus restricting the problem
to a two-dimensional plane, and (ii) they follow a circular ﬂight path. Although
these assumptions are made for this study to simplify our analysis, our techniques
remain valid even when the ANP altitudes are diﬀerent and the ﬂight paths are
irregular. The techniques proposed in this paper are equally applicable to any
scenario as long as the ﬂight paths are periodic and are a priori known.

362

4

S. Shirazipourazad et al.

Computation of Critical Transmission Range in Delay
Tolerant Airborne Networks CTRD

In [4] the critical transmission range (CTR) was deﬁned as the minimum transmission range of the ANPs required to ensure that the dynamic network formed
by the movement of the ANPs remains connected at all times. This deﬁnition
of CTR implies that although the network topology is dynamic and may change
with time, if the ANPs have their transmission ranges set to CTR, the network
will remain connected at all times. In [4] algorithms were proposed to compute
the CTR when ﬂight paths of the ANPs are known.
As ANPs may be susceptible to faults, speciﬁcally region based or geographically correlated faults that may render ANPs in a particular geographic region
to fail, the authors of [5] introduce the notion of critical transmission range in
faulty scenario (CTRf ). For a given fault region radius R, the authors of [5]
deﬁne CTRf as the smallest transmission range necessary to ensure network
connectivity at all times in the presence of at most one region fault of radius R
anywhere in the network. In this study it is assumed that all nodes within the
fault radius R become inoperable after such a fault. The authors of [5], given
region fault radius R, propose techniques to compute the CTRf that allows
the network to remain connected at all times after a region failure of radius R,
irrespective of the location of the fault and the time of failure.
As noted above, the CTR and CTRf computations of [4,5] respectively
ensure that the network always remains connected in a non faulty, and faulty
scenario. However, due to resource constraints of the AN, it may not be possible
to equip the ANPs with radios that have coverage of radius CTR or CTRf .
Therefore, in this situation the backbone network cannot remain connected at
all times. On the other hand, based on the type of data that should be transmitted between ANPs, data transmissions may be tolerant to some amount of
delay. Hence, ANPs may not require to have end-to-end paths between all ANPs
at all times, but instead they should be able to transmit data to each other
within some limited time through intermediate nodes across diﬀerent network
topologies over time. In this section we investigate the problem of computation
of minimum transmission range in such delay tolerant airborne networks.
We consider that the trajectories and the distance function sij (t) of the network nodes are periodic over time. As a consequence, the network topologies
are periodically repeated. However, periodicity is not an underlying assumption
and our results can be utilized in non-periodic scenario as well as long as the
node trajectories for the whole operational duration of a network are given.
In [5] the authors present how to compute the link lifetime timeline and accordingly the network topologies caused by ANPs mobility in a time period when
all inputs are given. We represent the set of topologies in a periodic cycle starting from time t0 (current time) by the set G = {G1 , G2 , . . . , Gl }. Each network
topology Gi exists for a time duration of Ti .
In Fig. 2, an example of a dynamic graph with two topologies G1 and G2 , one
periodic cycle is shown. G1 and G2 last for T1 and T2 time units respectively.
It can be observed that there is no end-to-end path from A to C in either G1

On Delay Tolerant Airborne Network Design

363

or G2 . However, A can transmit data to B in G1 , and B can forward it to C
in G2 . In this situation we say that A can reach C through a temporal path
with delay equal to the lifetime of G1 , i.e. T1 ; and that the temporal path is
completed in G2 . We deﬁne a temporal path from node s to d to be a set of
tuples {(t1 , (v1 , v2 )), (t2 , (v2 , v3 )), . . . , (tk , (vk−1 , vk ))} such that v1 = s, vk = d,
vi ∈ V , and for every tuple (ti , (vi , vi+1 )), edge (vi , vi+1 ) is active at time ti ,
and ti ≥ ti−1 for all 1 ≤ i ≤ k. Moreover, without loss of generality, we assume
that ti corresponds to the starting time of a topology in G. Then, the path delay
is deﬁned to be tk − t0 where t0 is the starting time of G1 in the ﬁrst periodic
cycle. We note that all path delays are computed with respect to starting point
t0 but we later show that we can modify the starting point to any time.

A

B

A

B

C

C

G1

G2

Fig. 2. A dynamic graph with two topologies G1 and G2

We note that the existence of a path from node i to j with some delay does
not guarantee the existence of a path from j to i with the same delay. For
example, in Fig. 2 the path from C to A has a delay of T1 + T2 while the path
delay from A to C is equal to T1 . We say that a dynamic graph G(t) is connected
with delay D if there exists a temporal path from every node i ∈ V to every node
j ∈ V \ {i} with delay smaller than D. It may be noted that, if the transmission
range T r is too small, ANPs may not be able to communicate with each other
at all, i.e. there may be no temporal path of ﬁnite delay between the ANPs.
We deﬁne critical transmission range in delay tolerant network (CTRD ) to be
the minimum transmission range necessary to ensure that the dynamic graph
is connected with delay D. We deﬁne the connectivity problem in delay tolerant
networks as the problem of computation of CTRD given the delay threshold D,
and the following input parameters:
1. a set of points {c1 , c2 , . . . , cn } on a two dimensional plane (representing the
centers of circular ﬂight paths),
2. a set of radii {r1 , r2 , . . . , rn } representing the radii of circular ﬂight paths,
3. a set of points {p1 , p2 , . . . , pn } representing the initial locations of the platforms, and
4. a set of velocities {v1 , v2 , . . . , vn } representing the speeds of the platforms
In order to ﬁnd the value of CTRD , ﬁrst we explain a technique to check
whether a transmission range T r is adequate for having a connected dynamic network with delay D. First, we determine the set of events (L(tr)) when the state

364

S. Shirazipourazad et al.

of a link changes from active to inactive (and vice-versa), when the transmission
range is set to T r. The technique to build the set L(tr) was proposed in [5] and we
restate it here for the sake of completeness. From the speciﬁed input parameters
(1) through (4) we ﬁrst determine the lifetime (active/inactive intervals) of every
link between every pair of nodes i and j by comparing sij (t) with T r and ﬁnding the time points that the state of a link changes. Let L(T r) = {e1 , e2 , . . . , el }
denote the set of events, or ei ’s, when the state of a link changes when transmission range is T r; let L(tr) be sorted in increasing order of the time of the events.
Hence, between two consecutive events ei and ei+1 that occur at times ti and
ti+1 the set of active links is unchanged. Algorithm 1 summarizes this technique
of computing L(T r).
Algorithm 1. Link Lifetime Computation
Input: (i) a set of points {c1 , c2 , . . . , cn } representing the centers of circular ﬂight paths,
(ii) a set of radii {r1 , r2 , . . . , rn } representing the radii of circular ﬂight paths, (iii) a
set of points {p1 , p2 , . . . , pn } representing the initial locations of the platforms, (iv) a
set of velocities {v1 , v2 , . . . , vn } representing the speeds of the platforms.
Output: L(T r): an ordered set of events that the state of a link changes from active to
inactive or inactive to active.
1: L(T r) ← ∅
2: for all pairs i, j do
3:
Compute l to be the set of time points t such that sij (t) = T r (using equation
(5) of [5]) over a period of time, to ﬁnd the instances of times t where the state of
the link (i, j) changes. If sij (t) = T r and is sij (t) increasing at t, it implies that
the link dies at t, and if sij (t) decreasing at t, it implies that the link becomes
active at t.
4:
for all lk ∈ l do
5:
Find the position of lk in L(T r) using binary search and Add the event into
L(T r). (L(T r) is sorted in increasing order)
6:
end for
7: end for

It has been shown in [5] that the time complexity of Algorithm 1 is O(n4 ), and
|L(T r)| = O(n2 ). Before describing the rest of the technique to check whether
a transmission range T r is adequate for having a connected dynamic network
with delay D, we propose the following observation:
Observation 1. For a given transmission range T r, there is a temporal path
from every
lnode u to every node v with ﬁnite delay iﬀ the superimposed graph
Gc = {V, i=1 Ei }, where Ei is the set of edges in Gi , is connected.
Although a transmission range T r may be enough to result in a connected superimposed graph Gc , it may not be suﬃcient for the existence of a temporal path
between every
pair of nodes with delay smaller than a threshold D even if D is
l−1
as large as i=1 Ti . Figure 3 depicts an AN with three topologies in one period.

On Delay Tolerant Airborne Network Design

A

B

A

D

C

D

G1
(a)

G2
(b)

B

A

C

D

365

B
C
G3
(c)

Fig. 3. A dynamic graph with three topologies G1 , G2 and G3

It can be observed that A cannot have a temporal path from A to D in the
ﬁrst period. Actually the fastest path includes edges (A, B) in G3 in ﬁrst period,
(B, C) in G2 in the second period and (C, D) in G1 in the third period. Therefore, the path delay is 2(T1 +T2 +T3 ). Generally, in the worst case in every period
just a subpath (a set of consecutive edges) in one topology is used and therefore
l
the maximum delay of a temporal path will be Dmax = (l − 1) i=1 Ti . Hence,
if D ≥ Dmax , examining the connectivity of Gc is enough to decide whether
for a transmission range there exists a temporal path of delay smaller than D
between every pair of nodes in the dynamic network.
We now present an algorithm that checks for a given value of transmission
range T r, whether a network is connected with delay D where D < Dmax . Let
N (u) denote the set of nodes that are reachable from u ∈ V with delay smaller
than D. Initially N (u) = {u}. The algorithm starts by computing the connected
components in every topology Gi . Let Ci = {Ci,1 , Ci,2 , . . . Ci,qi } represent the
set of connected components in Gi where Ci,j is the set of nodes in the jth
component of Gi , and qi = |Ci |. Let g and h be the quotient and remainder of
l D
respectively, and t0 + h is the time where the network topology is Gp for
i=1 Ti
a p, 1 ≤ p ≤ l. Therefore, the topologies in time duration t0 to t0 +D includes G1
to Gl for g number of cycles and G1 to Gp in the last periodic cycle. Starting from
the ﬁrst topology G1 in the ﬁrst period, in each topology Gi , if a node v is in the
same connected component with a node w ∈ N (u), then v can be reachable from
u through atemporal path which is completed in Gi ; hence, N (u) is updated
to N (u) ∪ ( k:N (u)∩Ci,k =∅ Ci,k ). In this step the algorithm goes through all the
topologies from t0 to t0 + D. In the end, if N (u) = V for all u ∈ V , then the
transmission range T r is suﬃcient for having a connected network with delay D.
In Algorithm 2 the steps of checking the connectivity of a dynamic graph with
delay D is presented.
As shown in [5] the number of topologies, l in a single period of the dynamic
topology is O(n2 ). Thus, the computation of the connected components of a graph
Gi = (V, Ei ) using either breadth-ﬁrst search or depth-ﬁrst search requires a time
complexity of O(|V |+|Ei |) = O(n2 ). Hence, Step 2–4 of Algorithm 2 takes O(n4 ).
l
It may be noted that this algorithm is used for the case when D < (l − 1) i=1 Ti .
Therefore, the number of periods g < l − 1, and g = O(n2 ). Computation of

366

S. Shirazipourazad et al.

Algorithm 2. Checking Connectivity of Airborne Network with delay D
Input: G(t) = {G1 , G2 , . . . , Gl } and delay threshold D
Output: true if dynamic graph G(t) is connected with delay D; otherwise false.
1: Initialize N (u) = {u} for every u ∈ V
2: for all topologies Gi , 1 ≤ i ≤ l
3:
Compute Ci = {Ci,1 , Ci,2 , . . . Ci,qi }, the set of connected components of Gi
4: for all periods 1 to g
5:
for all topologies Gi , 1 ≤ i ≤ l
6:
for all node u ∈ V 
7:
N (u) ← N (u) ∪ ( k:N (u)∩Ci,k =∅ Ci,k )
8: for all topologies Gi , 1 ≤ i ≤ p (the topologies in the last period)
9:
for all node u ∈ V

Ci,k )
10:
N (u) ← N (u) ∪ (
k:N (u)∩Ci,k =∅

11: for all node u ∈ V
12:
if N (u) = V , return f alse
13: return true

Step 5 is also O(n2 ) since |N (u)| and the total size of all components in Gi is O(n).
Overall, it can be concluded that the time complexity of Algorithm 2 is O(n7 ).
Additionally, as all the delays are computed with respect to t0 , we can easily
extend this technique to any ti , by repeating Algorithm 2 for every ti , 1 ≤ i ≤ l
where ti is the starting time of topology Gi . Thus, this extension increases the
complexity by a factor of l = O(n2 ).
Finally, similar to the computation of CTR and CTRf in [4,5], in order to
compute CTRD a binary search can be carried out within the range 0−T rmax to
determine the smallest transmission range that will ensure the AN is connected
with delay D during the entire operational time. The binary search adds a factor
of log T rmax to the complexity of Algorithm 2 to compute CTRD .

5

Simulations

The goal of our simulation is to compare critical transmission ranges in diﬀerent
scenarios of non faulty (CTR), faulty (CTRf ) and delay tolerant (CTRD ) to
investigate the impact of various parameters, such as the number of ANPs, the
region radius and delay, on the critical transmission range. In our simulation
environment, the deployment area considered was a 1000 × 1000 square mile
area. The centers of the orbits of the ANPs were chosen randomly in such a way
that the orbits did not intersect with each other. In our simulation, we assumed
that all the ANPs move at the same angular speed of ω = 20 rad/h. Hence a
period length is 0.1π h. One interesting point to note is that, in this environment
where all the ANPs are moving at the same angular speed on circular paths, the
value of CTR is independent of the speed of movement of the ANPs. This is
true because changing the angular speed ω eﬀects just the time at which the
events take place, such as when a link becomes active or inactive. If we view the

On Delay Tolerant Airborne Network Design
300
CTR_f (R=60)
CTR_f (R=20)
CTR
CTR_D (D=0.15)
CTR_D (D=0.6)

550

Transmission Range

500
450

400
350
300

n=30
n=50
n=70

280
260

CTRD

600

367

240

220
200

250
200

180

150

160

100

0

20

40
60
80
Number of Nodes (n)

(a)

100

120

0

0.2

0.4
Delay (D)

0.6

0.8

(b)

Fig. 4. (a) Transmission Range vs. Number of Nodes; (b) Transmission Range (CTRD )
vs. Delay

dynamic topology of the backbone network over one time period as a collection
of topologies G = {G1 , G2 , . . . , Gl }, where Gi morphs into Gi+1 , 1 ≤ i ≤ l at
some time, by increasing or decreasing the angular speed of all ANPs, we just
make the transitions from Gi to Gi+1 faster or slower, without changing the
topology set G. Similarly, the set of ANPs that fail due to failure of a region at
a certain time remains unchanged.
In our ﬁrst set of experiments we compute CTR, CTRf when R = 20, 60, and
CTRD when D = 0.5 period, 2 period for diﬀerent values of number of nodes, n.
Figure 4(a) depicts the result of these experiments. In these experiments, for each
value of n we conducted 30 experiments and the results were averaged over the
30 diﬀerent random initial setups. We set orbit radius = 10. We observed that
as expected, an increase in the number of nodes results in a decrease in CTR,
CTRf and CTRD . Moreover, CTRD ≤ CTR ≤ CTRf for all instances. In all of
the experiments, we computed CTRD with respect to all times (corresponding
to beginning of a new topology) and not just t0 .
In the second set of experiments, we conducted experiments to investigate
the impact of delay D on the value of CTRD . Figure 4(b) depicts the results.
We observe that when value of delay D is zero the value of CTRD is equal to
CTR and by increasing delay, CTRD decreases and the interesting observation
is that when delay becomes greater than 2 period the decrease in the value of
CTRD is unnoticeable or even zero.

References
1. Burbank, J.L., Chimento, P.H., Haberman, B.K., Kasch, W.T.: Key challenges of
military tactical networking and the elusive promise of MANET technology. IEEE
Commun. Mag. 44, 39–45 (2006)
2. Conti, M., Giardano, S.: Multihop ad-hoc networking: the reality. IEEE Commun.
Mag. 45, 88–95 (2007)

368

S. Shirazipourazad et al.

3. Milner, S.D., Thakkar, S., Chandrashekar, K., Chen, W.L.: Performance and scalability of mobile wireless base-station-oriented networks. ACM SIGMOBILE Mob.
Comput. Commun. Rev. 7(2), 69–79 (2003)
4. Shirazipourazad, S., Ghosh, P., Sen, A.: On connectivity of airborne networks with
unpredictable ﬂight path of aircrafts. In: Proceedings of the First ACM MobiHoc
Workshop on Airborne Networks and Communications, pp. 1–6. ACM (2012)
5. Shirazipourazad, S., Ghosh, P., Sen, A.: On connectivity of airborne networks in
presence of region-based faults. In: 2011-MILCOM 2011 Military Communications
Conference, pp. 1997–2002. IEEE (2011)
6. Milner, S., Llorca, J., Davis, C.: Autonomous reconﬁguration and control in directional mobile ad hoc networks. IEEE Circ. Syst. Mag. 9(2), 10–26 (2009)
7. Ramanathan, R., Rosales-Hain, R.: Topology control of multihop wireless networks
using transmit power adjustment. In: INFOCOM 2000 of Nineteenth Annual Joint
Conference of the IEEE Computer and Communications Societies Proceedings,
vol. 2, pp. 404–413. IEEE (2000)
8. Cabrera, J., Ramanathan, R., Gutierrez, C., Mehra, R.: Stable topology control
for mobile ad-hoc networks. IEEE Commun. Lett. 11(7), 574–576 (2007)
9. Santi, P.: The critical transmitting range for connectivity in mobile ad hoc networks. IEEE Trans. Mob. Comput. 4, 310–317 (2005)
10. Fall, K.: A delay-tolerant network architecture for challenged internets. In: Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and
Protocols for Computer Communications, SIGCOMM 2003, pp. 27–34. ACM,
New York (2003)
11. Sterbenz, J.P.G., Krishnan, R., Hain, R.R., Jackson, A.W., Levin, D.,
Ramanathan, R., Zao, J.: Survivable mobile wireless networks: issues, challenges,
and research directions. In: Proceedings of the 1st ACM Workshop on Wireless
Security, WiSE 2002, pp. 31–40. ACM, New York (2002)
12. Jain, S., Fall, K., Patra, R.: Routing in a delay tolerant network. In: Proceedings of
the 2004 Conference on Applications, Technologies, Architectures, and Protocols
for Computer Communications, SIGCOMM 2004, pp. 145–158. ACM, New York
(2004)
13. Alonso, J., Fall, K.: A linear programming formulation of ﬂows over time with
piecewise constant capacity and transit times. Intel Research Technical report IRBTR-03-007 (2003)
14. Cao, Y., Sun, Z.: Routing in delay/disruption tolerant networks: a taxonomy, survey and challenges. IEEE Commun. Surv. Tutorials 15(2), 654–677 (2013)
15. Huang, M., Chen, S., Zhu, Y., Xu, B., Wang, Y.: Topology control for time-evolving
and predictable delay-tolerant networks. In: 2011 IEEE 8th International Conference on Mobile Adhoc and Sensor Systems (MASS), pp. 82–91 (2011)
16. Huang, M., Chen, S., Zhu, Y., Wang, Y.: Cost-eﬃcient topology design problem in
time-evolving delay-tolerant networks. In: Global Telecommunications Conference
(GLOBECOM 2010), pp. 1–5. IEEE (2010)
17. Tiwari, A., Ganguli, A., Sampath, A.: Towards a mission planning toolbox for
airborne networks: optimizing ground coverage under connectivity constraints. In:
IEEE Aerospace Conference, pp. 1–9, March 2008
18. Epstein, B., Mehta, V.: Free space optical communications routing performance in
highly dynamic airspace environments. In: IEEE Aerospace Conference Proceedings (2004)

859

On Shortest Path Problems
with “Non-Markovian” Link Contribution
to Path Lengths
Arunabha Sen1 , K. Selçuk Candan1 , Afonso Ferreira2 , and Bruno Beauquier2 ,
and Stephane Perennes2
1

Department of Computer Science and Engineering
Arizona State University
Tempe 85287, AZ, USA
{asen, candan}@asu.edu
2
SLOOP, CNRS - INRIA - UNSA
2004 Rote des Lucioles, BP 93
F-06902 Sophia-Antipollis, FRANCE
{Afonso.Ferreira, Beaquier, Stephane.Perennes}@sophia.inria.fr

Abstract. In this paper we introduce a new class of shortest path problems, where the contribution of a link to the path length computation
depends not only on the weight of that link but also on the weights of the
links already traversed. This class of problems may be viewed as “nonMarkovian”. We consider a specific problem that belong to this class,
which is encountered in the multimedia data transmission domain. We
consider this problem under different conditions and develop algorithms.
The shortest path problem in multimedia data transmission environment
can be solved in O(n2 ) or O(n3 ) computational time.

1

Introduction

Path problems have been extensively studied by many researchers of Computer
Science and Operations Research because of its applications in many problems
in these domains. In most of these problems, one or more weights are associated
with a link representing, among other things, the cost, delay or the reliability of
that link. The objective most often is to find a least weighted path between a
specified source-destination pair. In almost all the path problems studied so far
(and discussed in the literature), the contribution of a link to the path length
computation depends only on the weight of that link and is independent of the
weights of the links already traversed. This condition is similar to a Markov chain
where the next state is dependent only on the current state and is independent of
the past states. In this paper, we introduce a new variant of the path problem. In
this variant, the contribution of a link to the path length computation depends
not only on the weight of that link but also on the weights of the links already
traversed. This class of problems may be viewed as “non-Markovian” as the
contribution of a link towards the path length depends on the current link as well
as the links traversed in the past on the path from the source to the destination.
G. Pujolle et al. (Eds.): NETWORKING 2000, LNCS 1815, pp. 859–870, 2000.
c Springer-Verlag Berlin Heidelberg 2000


860

A. Sen et al.

As an example, we consider a specific problem that belongs to this class. This
problem is encountered in the multimedia data transmission domain. We consider
this problems under different conditions and develop appropriate algorithms.
The shortest path problem in multimedia data transmission environment can
be solved in O(n2 ) or O(n3 ) computational time. We also provide mathematical
programming solutions for this problem.

2

Prior Work

Shortest path problems are among the most widely studied problems in Computer Science and Operations Research. Because of its wide applications in many
diverse domains, these problems have been studied for at least the last forty years
[1,4,5]. In the earliest version of the shortest path problem [1,4,5], a weight is
associated with each link of the network and the path length is computed by
summing up the weights of the links belonging to the path. In a generalization
of this version of the problem, multiple weights are associated with each link of
the network. If there are m different weights associated with each link of the
network, there are m different path lengths associated with each path. The i-th
path length, (1 ≤ i ≤ m), is obtained by summing up the i-th weights of the
links belonging to the path. This version of the shortest path problem is known
as multicriteria shortest path problem or constrained shortest path problem and is
fairly extensively studied in [6,8,12]. In view of the attention that the Quality of
Service issues have received in the networking community in recent years, study
of this version of the shortest path problem has become increasingly important
[14].
Another version of the shortest path problem that has received considerable
attention is the one where the weights associated with the links of the network
are allowed to change with time. Both centralized as well as distributed algorithms for the shortest path in this scenario have been developed under various
waiting constraints in [9,10]. In yet another version of the problem, each link,
e, of the network has two weights, transit time b(e, u) and cost c(e, u), where
u is the departure time at the starting node of the link. In this version, the
problem is to find the least cost path such that the total traversal time is below
some prespecified threshold value T . A dynamic programming algorithm for the
shortest path problem with time windows and additional linear costs on node
service start times is presented in [7]. In [11] the authors consider a version of the
problem termed as the quickest path problem, where the objective is to transfer
a specified amount of data from the source to the destination with minimum
transmission time. The transmission time in this problem is dependent on both
the capacities and the traversal times of the links in the network. The shortest
path problem in multimedia data transmission environment is discussed in [3].

Path Problems with “Non-Markovian” Link Contribution to Path Lengths

3

861

Problem Formulation

In the classical path problem, each edge ei ∈ E of the graph G = (V, E) has a
weight wi associated with it and if there is a path P from the node v0 to vk
w

w

w

w

v0 →1 v1 →2 v2 →3 . . . →k vk
then the path length or the distance between the nodes v0 and vk is given by
P L(v0 , vk ) = w1 + w2 + · · · + wk
This model is valid as long as the weights on the links represents the cost or the
delay associated with the link. However, if the weight represents the reliability
or the bandwidth associated with the link, then addition of the link weights on
the path is not meaningful. In case, the weights represent the reliability, the
calculation once again becomes meaningful if the addition operator is replaced
by a multiplication operator. In case, the weight represents the bandwidth, the
calculation becomes meaningful if a minimum operator replaces the addition
operator. Thus a generalization of the path length will be
P L(v0 , vk ) = w1 ⊕ w2 ⊕ w3 ⊕ · · · ⊕ wk
where ⊕ is a suitable operator for the particular application. In [14], the authors
consider three diferent types of operators and call them additive, multiplicative,
and concave metrics respectively.
At the next level of generalization, the path length computation is based on
not the link weight itself but a function of the link weight. In this case the path
length is given by
P L(v0 , vk ) = f (w1 ) ⊕ f (w2 ) ⊕ f (w3 ) ⊕ · · · ⊕ f (wk )
where f (wi ) can be any function of the link weight wi , appropriate for the
particular application.
At the next higher level of generalization each link has multiple weights associated with it. This model realistically captures the data transmission environment where the Quality of Service (QoS) issues are of paramount importance.
The various weights associated with a link may represent among other things,
the delay, the cost, the jitter, the cell loss rate etc. In this case the path length
computation is carried out in one of the following two ways:
Case I: In this case each path has multiple path lengths associated with it. If
(wi,1 , wi,2 , . . . , wi,m ) are m different link weights associated with the link ei , then
there are m different path lengths, [P L1 (v0 , vk ), . . . , P Lm (v0 , vk )], associated
with a path between a given source node v0 and a given destination node vk ,
where
P Li (v0 , vk ) = f (w1,i ) ⊕ f (w2,i ) ⊕ · · · ⊕ f (wk,i )

862

A. Sen et al.

This class of problems is known as the multicriteria optimization problems and
is studied in [6,8,12,14].
Case II: In this case each path has a single path length associated with it:
P L(v0 , vk ) = f (w1,1 , . . . , w1,m ) ⊕ f (w2,1 , . . . , w2,m ) ⊕ · · · ⊕ f (wk,1 , . . . , wk,m )
It may be noted that this formulation gives rise to a single criterion optimization
problem as opposed to the multiple criteria optimization problem in Case I.
Both Case I and Case II of the previous level can be further generalized at
the next higher level. As in the previous case, each link has multiple weights
associated with them. In this level of generalization, the contribution of a link in
the path length computation depends not only on the weights associated with
that link but also on the weights of the links already traversed. In this case the
path length, [P L1 (v0 , vk ), . . . , P Lm (v0 , vk )], for Case I is such that
P Li (v0 , vk ) = f (w1,i ) ⊕ · · · ⊕ f (w1,i , · · · , wk,i ).
At this level of generalization, the path length for Case II is
P L(v0 , vk ) = f (w1,1 , . . . , w1,m ) ⊕ f (w1,1 , . . . , w1,m , w2,1 , . . . , w2,m ) ⊕ · · · ⊕
f (w1,1 , . . . , w1,m , . . . , wk,1 , . . . , wk,m ).

We say that the edges in this category have “non-Markovian” link contributions.

4

Path Problem in Multimedia Data Transmission

The example path problem discussed in this paper belongs to this last category.
This problem is based on a multimedia data transmission model we recently presented in [3]. The model allows an active network to perform certain operations
to the data at the network nodes. These operations, such as format conversions
for distributed multimedia collaborations and lossy/lossless compressions may
change the sizes and qualities of multimedia object being transmitted. In this
paper, we use a subset of this model, where the quality is not taken into account
for path selection.
In the variant of the path problem for the multimedia data transmission,
each edge ei has two weights, δi and si associated with it, δi ≥ 0 and si ≥ 0.
These two weights are referred to as (i) the per unit delay factor and (ii) the size
factor respectively. If P is a path from the node v0 to vk ,
δ1 ,s1

δ2 ,s2

δ3 ,s3

δk ,sk

v0 → v1 → v2 → . . . → vk .
then the path length or the total delay between the nodes v0 and vk denoted
P L(v0 , vk ) is given by
P L(v0 , vk ) = δ1 + s1 δ2 + s1 s2 δ3 + . . . + s1 . . . sk−1 δk
=

k
X
i=1

δi

i−1
Y
j=1

sj with

0
Y
j=1

sj = 1.

Path Problems with “Non-Markovian” Link Contribution to Path Lengths

863

It is clear that the path length in this case fits into the most general case
discussed in the previous paragraph with m = 2, wi,1 = δi , wi,2 = si and
f (w1,1 , w1,2 , w2,1 , w2,2 , . . . , wi,1 , wi,2 ) = s1 s2 . . . si−1 δi , for all i, 1 ≤ i ≤ k, and
s0 = 1.
The physical significance of the parameters δi and si are as follows: The
transmission delay is clearly proportional to the size of the multimedia data file
being transmitted. Therefore we consider the per unit delay factor δi and to compute the total delay, we multiply δi with the size of the file being transmitted. As
a multimedia data file travels through different nodes in a network on its journey from the source to the destination, it passes through some transformation
algorithms. As a result, the size of the multimedia data file may change. The
size factor si captures this aspect of multimedia data transmission. We remark,
however, that the total delay remains proportional to the amount of data transmitted along the path. Therefore, the expression for the path length (or total
delay) stated above is given for transmitting one unit of data from the source to
the destination.
4.1

Why Is This Problem Different?

The length of a path P (v0 , vk ) : v0 →v1 →v2 → . . . →vk , in the multimedia data
transmission problem, is given by P L(v0 , vk ) = δ1 + s1 δ2 + s1 s2 δ3 + . . . +
s1 . . . sk−1 δk . The traditional shortest path algorithms such as the Dijkstra’s
algorithm make the observation that “subpaths of shortest paths are shortest
paths” and exploits it to develop the shortest path algorithm.
In other words, to get to the destination from the source using the shortest
path, the intermediate nodes must be visited using the shortest path from the
source to the intermediate nodes. This is true because, the path length in this case
is computed as the sum of weights on the links that make up the path. In case
of multimedia data transmission problem, where the path length is not computed
as the sum of the links weights, this is no longer true. This is demonstrated with
an example shown in Figure 1. The δi and si values associated with the links of
this graph is also given in Figure 1.
With this data set the length of the path, S → C → X → D → T , is
δS,C + sS,C δC,X + sS,C sC,X δX,D + sS,C sC,X sX,D δD,T = 1 + 3.1 + 3.1.1 + 3.1.1.1 =
1 + 3 + 3 + 3 = 10 whereas the length of the path S → A → B → X → D → T is
δS,A + sS,A δA,B + sS,A sA,B δB,X + sS,A sA,B sB,X δX,D + sS,A sA,B sB,X sX,D δD,T =
1 + 1.1 + 1.1.1 + 1.1.4.1 + 1.1.4.1.1 = 1 + 1 + 1 + 4 + 4 = 11. Thus the path
S → C → X → D → T is shorter than the path S → A → B → X → D → T
in the example. However, in this example the length of the path S → C → X is
1 +3.1 = 4, which is greater than the length of the path S → A → B → X, 1 +
1.1 + 1.1.1 = 3.
On the other hand, the path length function in the multimedia data transmission problem has an interesting property and this property is utilized to establish
the following lemma.
Lemma 1. Given a weighted directed graph G = (V, E) with weight functions
(δi , si ), associated with each link ei , (ei ∈ E, 1 ≤ i ≤ |E|), and the length of a

864

A. Sen et al.
A

B

T

S

X

D

C

(S,A) (S,C) (A,B) (B,X) (C,X) (X,D) (D,T)
Delay factor (δ) 1
1
1
1
1
1
1
Size factor (s)
1
3
1
4
1
1
1
Fig. 1. An Example Graph for MMD Transmission and the Associated Delay and Size
Factors

path P (v0 , vk ) : v0 →v1 → . . . →vk is computed as P L(v0 , vk ) = δ1 +s1 δ2 +s1 s2 δ3 +
. . .+s1 . . . sk−1 δk . Let P (v0 , vk ) : v0 →v1 → . . . →vk be a shortest path from vertex
v0 to vertex vk and for any i, 1 ≤ i ≤ k − 1, let P (vi , vk ) : vi →vi+1 → . . . →vk
be a subpath of P from vertex vi to vertex vk . Then P (vi , vk ) is a shortest path
from vi to vk , 1 ≤ i ≤ k.
The proof of the lemmas and theorems in this paper are omitted for space
considerations. The proofs may be found in [13].
4.2

Path Problem in Multimedia Data Transmission
with No Reduction in Size

In this subsection, we consider a special case where the size factor, si , associated
with a link ei is greater than or equal to unity for all the links. This implies
that the data size will never reduce from its original size while passing through
a link. The more general case where the size factor, si , does not have any such
restriction (i.e., si is allowed to be less than unity) will be considered in the next
subsection.
Beacuse of lemma 3 and the fact si ≥ 1, δi ≥ 0, we can apply a modified
version of Dijkstra’s algorithm to solve the shortest path problem in the multimedia data transmission environment. The traditional version of the algorithm
starts from the source node and computes the shortest path to other nodes until
it finds the shortest path to the destination. In this modified version, we start
from the destination node and compute the shortest path from other nodes to
the destination nodes until it finds the shortest path from the source to the
destination node. The algorithm is given in Figure 2.
Theorem 1. If ∀i, j the delay factor δ(i, j) ≥ 0 and the size factor s(i, j) ≥ 1
then the above algorithm correctly computes the shortest path from any node
i, 1 ≤ i ≤ n − 1 to the destination node n.
Theorem 2. The complexity of the algorithm is O(n2 ).

Path Problems with “Non-Markovian” Link Contribution to Path Lengths

865

Shortest Path Algorithm for Multimedia Data Transmission Environment
Input: The directed graph G = (V, E), (V = {1, 2, . . . , n}), two n × n matrices
δ and s, the (i, j)-th entry of the matrices stores the delay factor δ and the size
factor s of the link from the node i to node j. If there is no link from the node i to
j, both δi,j and si,j is taken to be ∞. Without any loss of generality, we assume
that the node 1 is the source node and node n is the destination node.
Output: Array D(1, . . . , n), that stores the shortest path length from node i to
the destination node n for all i, 1 ≤ i ≤ n.
Comments: The algorithm starts from the destination node and in each iteration
finds the shortest path from a node i in the graph to the destination node n,
1≤i≤n−1 .
begin
C := {1, 2, . . . , n − 1};
for i := n − 1 downto 1 do
D[i] := δ[i, n]
repeat n − 2 times
begin
v := i ∈ C such that D[i] has the minimum value;
C := C \ {v};
for each w ∈ C do
D[w] := min(D[w], δ[w, v] + s[w, v]D[v]);
end
end

Fig. 2. Shortest Path Algorithm for Multimedia Data Transmission Environment

An example graph and the corrsonding result of the execution of the algorithm on the graph is shown in Figure 3 (the source node is 1 and the destination
is 6). The shortest path length from the node 1 to node 6 is 5 and the path is
v1 → v3 → v4 → v6 .
It is well known that if the path length is measured as the sum of the weights
on the links, Dijkstra’s algorithm fails to compute the shortest path betwen
the source-destination nodes, in case some of the link weights are negative. For
exactly the same reason, our modified version of the Dijkstra’s algorithm fails to
compute the shortest path if si,j < 1. An example of the case where the above
algorithm fails to compute the shortest path is shown in Figure 4. The δi and si
values associated with the links of this graph is also given in Figure 4 (a). The
result of the execution of the modified Dijkstra algorithm on this graph is shown
in Figure 4.
At the termination of the algorithm, the shortest path length between the
source node S and the destination node T is given as 6 and the path is S →
A → X → D → T (δS,A + sS,A δA,X + sS,A sA,X δX,D + sS,A sA,X sX,D δD,T = 1 +
1.2 + 1.1.2 + 1.1.1.1 = 6). However, this result is incorrect because the length
of the path S → A → X → B → C → T is δS,A + sS,A δA,X + sS,A sA,X δX,B +

866

A. Sen et al.
2

4

(2, 1)

Iteration

(2, 2)

(1, 3)

(1, 2)
1

(2, 2)

(1, 2)

(1, 1)

6

(3, 2)

(3, 1)
3

1
2
3
4
5

(2, 3)

Nodes of the Graph
1
∞
∞
∞
7
5∗

2
∞
3
3∗
3
3

3
∞
4
4
4∗
4

4
1∗
1
1
1
1

5
3
3∗
3
3
3

6
0
0
0
0
0

5

(b)

(a)

Fig. 3. (a) An Example Graph for MMD transmission and (b) the Corresponding
Shortest Path Computation
B

S

C

Iteration
T

A

X
D

(S,A) (A,X) (X,B) (X,D) (B,X) (C,T) (D,T)
δ
1
2
1
2
2
2
1
s 1
1
0.25
1
1
1
1
(a)

1
2
3
4
5
6

Nodes of the Graph
S
∞
∞
∞
∞
∞
6∗

A
∞
∞
∞
5
5∗
5

X
∞
3
3∗
3
3
3

B
∞
∞
4
4∗

C
2
2∗
2
2
2
4 2

D
1∗
1
1
1
1
1

T
0
0
0
0
0
o

(b)

Fig. 4. (a) An Example Graph for MMD Transmission and (b) the Corresponding
Shortest Path Computation

sS,A sA,X sX,B δB,C + sS,A sA,X sX,B sB,C δC,T = 1 + 1.2 + 1.1.1 + 1.1.(0.25).2 +
1.1.(0.25).1.2 = 5. In this case, the algorithm computes the shortest path length
incorrectly, because one of the size factors, sX,B < 1 (sX,B = 0.25).
4.3

Path Problem in Multimedia Data Transmission
with Reduction in Size

It was mentioned in the previous section that in this path problem if some size
factor si < 1, it has the same effect as a negative weighted link in a traditional
shortest path problem. The example given earlier, shows that our version of the
Dijktra’s algorithm fails to correctly compute the shortest path from the source
to the destination in this situation. In the traditional shortest path problem,
where the path length is computed as the sum of the weights on the links of
a path, there is a notion of a negative weighted cycle. A cycle is referred to as
a negative weighted cycle if the sum of the weights on the links making up the
cycle is a negative number. The multimedia data transmission problem that is

Path Problems with “Non-Markovian” Link Contribution to Path Lengths
2
1

3
4

867

Delay Factor δ Size Factor s
δ1,2 = d1
s1,2 = 1
δ2,3 = 
s2,3 = 1/p
δ3,2 = 
s3,2 = 1/p
δ3,4 = d2
s3,4 = 1

Fig. 5. An Example Graph with Negative Weighted Cycle and the Associated Delay
and Size Factors

presently under consideration, both the weights (the delay factor δi and size
factor si ) associated with a link ei , are non-negative. However, in this problem
path length is computed in a different way. In this problem also we have a notion
of a negative weighted cycle. The implication of such a negative weighted cycle
is that the data size decreases every time it goes around such a cycle in such a
way that the total delay is also reduced every time the data goes around such a
cycle. An example of such a phenomenon is given next.
Consider the graph in Figure 5 with the nodes 1 and 4 being the source
and the destination respectively. The nodes 2 and 3 form a loop, as shown in the
figure. Now, consider the path 1 → 2 → 3 → 4. This is a no loop path between the
nodes 1 and 4. The path length of this path is d1 +1.+1.(1/p).d2 = d1 ++d2 /p.
The length of the path from 1 to 4, if it passes through the loop 1, 2, . . . , k times
is
d1 + (1 + 1/p + . . . + 1/p2k ) + d2 /p2k+1 .
Thus the path length increases by ((p + 1)/p2k+2 ) and decreases by ((p2 −
1)/p2k+3 )d2 if the number of times the path goes through the loop increases from
k to k + 1. If d2 is much larger than , then the total decrease is much larger
than the total increase and as a result if the path goes through the loop one
more time, the path length decreases. The situation is similar to the negative
weighted cycle problem in the traditional shortest path length.
In the path problem for multimedia data transmission environment, we can
compute the shortest path between a specified source-destination pair, even with
“negative” weights (i.e., with size factor si < 1) on the links, as long as there
is no negative weighted cycles in the graph. We use a modified version of the
Bellman-Ford algorithm for this purpose.
Just like the traditional Bellman-Ford algorithm, we find the shortest path
lengths subject to the constraint that paths contain at most one link, then relax
the condition on the length of the path and find the shortest path length subject
to the constraint that paths contain at most two links and so on. Using the same
terminology and notations as in [2] we call the shortest path that uses at most
h links as the shortest (≤ h) path.
Suppose that Dh i denotes the shortest (≤ h) path length from node i to the
destination node n, (1 ≤ i ≤ n − 1). Dh n = 0 for all h. The algorithm is given
in Figure 6.

868

A. Sen et al.

Shortest Path Algorithm for Multimedia Data Transmission Environment
Input: The directed graph G = (V, E), (V = {1, 2, . . . , n}), two n × n matrices
δ and s, the (i, j)-th entry of the matices stores the delay factor δ and the size
factor s of the link from the node i to node j. If there is no link from the node i to
j, both δi,j and si,j is taken to be ∞. Without any loss of generality, we assume
that the node 1 is the source node and node n is the destination node.
Output: The shortest path length from every node in the graph to the destination
node n.
Comments: The algorithm starts from the destination node and in each iteration
finds the shortest (≤ h) path from a node i in the graph to the destination node
n, 1 ≤ i, h ≤ n − 1 .
begin
for i := 1 to n-1 do
D0 i := ∞;
for i := 1 to n-1 do
D1 i := δ(i, n);
for h := 1 to n-2 do
for i := 1 to n-1 do
Dh+1 i := min1≤j≤n−1 [s(i, j)Dh j + δ(i, j)];
end

Fig. 6. Shortest Path Algorithm for Multimedia Data Transmission Environment

Theorem 3. If the graph G = (V, E) does not contain any negative weighted
cycle, then the above algorithm correctly computes the shortest path length from
any node i, 1 ≤ i ≤ n − 1 to the destination node n, even when some of the size
factors si associated with a link ei is less than 1.
Theorem 4. The complexity of the algorithm is O(n3 ).
An example of the result of the execution of the algorithm on the graph in
Figure 4 is shown in Table 1.
4.4

Mathematical Programming Solution to the Path Problem in
Multimedia Data Transmission

In this subsection we show that the shortest path problem for the multimedia
data transmission problem can also be solved using mathematical programming
techniques.
Given a graph G = (V, E) with weights δi and si associated with each link
ei ∈ E and two specified vertices s and t, the problem is to find a shortest (or
the least weighted) path from s to t.
In the mathematical programming formulation of the problem, we associate a
binary indicator variable xi,j with each link i, j of the directed graph G = (V, E).

Path Problems with “Non-Markovian” Link Contribution to Path Lengths

869

Table 1. Shortest Path Computation for the Graph in Figure 4 using modified
Bellman-Ford Algorithm
Iteration Number
1
2
3
4
5
6

Nodes of the Graph
S
∞
∞
∞
∞
5
5

A
∞
∞
∞
4
4
4

X
∞
3
2
2
2
2

B
∞
4
4
4
4
4

C
2
2
2
2
2
2

D
1
1
1
1
1
1

T
0
0
0
0
0
0

By assigning a zero or a one to the variable xi,j the solution indicates whether or
not the link i, j is a part of the shortest path from the source to the destination.
We also introduce two other variables yi,j and zi,j , (1 ≤ i, j ≤ |V |). By assigning
a value to the variable yi,j (resp. zi,j ), the solution indicates how much data is
entering (resp.
P leaving) the link (i, j).
Minimize (i,j)∈E δi,j yi,j subject to the following constraints:
P
P
1.
{j|(i,j)∈E} xi,j −
{j|(j,i)∈E} xj,i = ri , ∀i ∈ V ri = 1 if i is the source node,
ri = −1 if i is the destination node and ri = 0 if i is any other node.
2. yP
s,j = xs,j ∀j ∈ V −
P{s} and (s, j) ∈ E, (s is the source node)
3.
{j|(i,j)∈E} yi,j −
{j|(j,i)∈E} zj,i = 0, ∀i ∈ V − {s, t}
4. zi,j = si,j yi,j , ∀(i, j) ∈ E
5. zi,j ≤ Kxi,j , ∀(i, j) ∈ E where K is a large constant.
The first constraint establishes a path from the source to the destination.
As data passes through a link (i, j), its size changes by a factor si,j . This is
ensured by the constraint (iv). The delay keeps accumulating as the data file
passes through various links on its journey from the source to the destination.
This aspect is captured by the constraint (iii). The purpose of constraint (v) is
to ensure that the variable zi,j does not have a non-zero value when xi,j = 0,
i.e., when the the link (i, j) is not a part of the path from the source to the
destination. The contribution of the delay factor δi,j associated with the link
(i, j) is taken into account in the objective function of the formulation.

5

Conclusion

In this paper we introduced a new class of the shortest path problems. In this
class of path problems, the contribution of a link towards the path length depends
not only on the weight of the link itself but also on the weights of all the links
traversed before traversing the link under consideration. We considered a specific
path problem that belong to this new class. This problem is encountered in
multimedia data transmission domain. Exploiting an interesting property of the
multimedia transmission path problem, we could develop low order polynomial

870

A. Sen et al.

time algorithms for this problem. Additionally, we could solve this problem using
mathematical programming techniques. Other path problems that belong to this
class are currently under investigation.

References
1. R. Bellman,“On a Routing Problem”, Quarterly of Applied Mathematics, vol. 16,
no. 1, pp. 87-90, 1958.
2. D. Bertsekas and R. Gallager, Data Networks, Prentice Hall, 1987.
3. K.S. Candan and Y.Yang,“Least-Cost High-Quality Object Retrieval for Distributed Multimedia Collaborations”, IEEE Multimedia Computing and Systems
Conference, pp. 649-654, Florence, Italy, June 1999.
4. E.W. Dijkstra, “A Note on Two Problems in Connection with Graphs”, Numerische Mathematik, vol. 1, pp. 269- 271, 1959.
5. L.E. Ford and S.M. Johnson, “A Tournament Problem”, The American Mathematical Monthly, vol. 66, pp. 387-389, 1959.
6. G.Y. Handler and I. Zang, “ A Dual Algorithm for the Constrained Shortest Path
Problem”,Networks,vol. 10, pp. 293-310, 1980.
7. I. Ioachim et. al., “ A Dynamic Programming Algorithm for the Shortest Path
Problem with Time Windows and Linear Node Costs”, Networks,vol. 31, pp. 193204, 1998.
8. J.M. Jaffe, “Algorithms for Finding Paths with Multiple Constraints”, Networks,vol. 14, pp. 95-116, 1984.
9. A. Orda and R. Rom, “Shortest-Path and Minimum-Delay Algorithms in Networks
with Time-Dependent Edge-Lengths”, Journal of the Association for Computing
Machinery, vol. 37, no. 3, pp. 605-625, 1990.
10. A. Orda and R. Rom, “Minimum Weight Paths with Time-Dependent Networks”,
Networks, vol. 21, pp. 295-319, 1991.
11. J.B. Rosen, S. Z. Sun and G.L. Xue, “Algorithms for the Quickest Path Problem and the Enumeration of Quickest Paths”, Computers and Operation Research,
vol. 18, no. 6, pp. 579-584, 1991.
12. H.F. Salama, D.S. Reeves and Y. Viniotis, “A Distributed Algorithm for DelayConstrained Unicast Routing”, IEEE INFOCOM’97, pp. 1c.2.1-1c.2.8, 1997.
13. A. Sen, K. S. Candan, A. Ferreira, B. Beauquier, S. Perennes, “On Shortest
Path Problems with “non-Markovian” Link Contribution to Path Lengths”, Tech.
Report-00-002, Dept. of Computer Science and Engineering, Arizona State University.
14. Z. Wang and J. Crowcroft, “ Quality-of-Service Routing for Supporting Multimedia
Applications”, IEEE Journal on Selected Areas of Communications, vol. 14, no. 7,
pp. 1228-1234, 1996.

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

Impact of Region-Based Faults on the Connectivity
of Wireless Networks in Log-normal Shadow
Fading Model
Sujogya Banerjee and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85281
Email: {sujogya, asen }@asu.edu

Abstract—The traditional studies on fault-tolerance in networks assume that the faults are random in nature, i.e., the
probability of a node failing is independent of its location in the
deployment area. However, this assumption is no longer valid if
the faults are spatially correlated. In this paper we focus on the
study of the impact of region-based faults on wireless networks.
Most of the studies on connectivity of wireless networks assume
a unit disk graph model, i.e., links exist between two nodes if
they are within a circular transmission range of one another.
However, the unit disk graph model does not capture wireless
communication environment accurately. The log-normal shadow
fading model for communication was introduced to overcome
the limitations of the unit disk graph model. In this paper
we investigate connectivity issues of wireless networks in a lognormal shadow fading environment where the faults are spatially
correlated. If dmin (G) denotes the minimum node degree of the
network, we provide the analytical expression and method for
computing P (dmin (G) ≥ 1) in a region-based fault scenario,
where P (dmin (G) ≥ 1) denotes the probability of the minimum
node degree being at least 1. Through extensive simulation, we
find P (κ(G) ≥ 1), where κ(G) represents the connectivity of the
graph G formed by the distribution of nodes on a 2D plane
and examine the relationship between P (dmin (G) ≥ 1) and
P (κ(G) ≥ 1).

I. I NTRODUCTION
The connectivity of a graph is defined as the fewest number
of nodes/links whose deletion disconnects the graph. Conectivity is traditionally considered as the primary metric for evaluation of the fault-tolerance capability of both wired and wireless
networks. Most of the traditional studies on fault-tolerance in
networks [1]–[7] assume that the faults are random in nature,
i.e., the probability of a node or link failing is independent of
its location in the deployment area. However, the assumption
of random node failure is not valid in many scenarios. In fact in
some networks, e.g. network in military environment or sensor
network, faults may be spatially-correlated or confined in a
region. Connectivity analysis in such localized fault scenario is
considerably different than random fault or fault-free scenario.
In this paper we study the impact of region-based faults on the
This research is supported in part by a grant from the U.S. Defense Threat
Reduction Agency under grant number HDTRA1-09-1-0032 and by a grant
from the U.S. Air Force Office of Scientific Research under grant number
FA9550-09-1-0120.

the connectivity of wireless networks in log-normal shadow
fading model.
Most of the studies [1]–[7] in geometric wireless networks
use unit disk graph model as communication model with the
assumption that transmission range of the nodes in the network
is uniform. But the unit disk graph model does not capture
wireless communication environment accurately. This model
assumes that two nodes located at points p and q on a 2D
plane forms a link if distance between them does not exceed
a threshold value rt (transmission range).
However, measurement studies of wireless radio signal show
that signal strength not only depends on the distance between
the transmitter and the receiver, but also on various other
factors like communication environments. The log-normal
shadow fading model for communication discussed in [8]
overcomes the limitations of the unit disk graph model. In
this model [8], signal strength varies log-normally around the
mean transmission power. This phenomenon of variation of
signal strength is called shadowing effect. These variations
are unavoidable in wireless communication and are caused
by obstruction and irregularities in surrounding environment.
The implication of shadowing effect is that all nodes within
the transmission range of a transmitter are not guaranteed to
have a communication link with the transmitter. On the other
hand some nodes outside the transmission range may form a
communication link with transmitter (see Fig 1). Log-normal
Shadow Fading Model of radio propagation [8] captures this
variation. Connectivity study [9], [10] of wireless networks
considering shadow fading model as network communication
model is a more appropriate approach in this regard.
Also most of the studies on connectivity of the wireless
networks either considers fault-free scenario [1]–[6], [11] or
assuming random faults [7]. In a wireless network not only
the nodes can fail, the failure may not be random. In fact,
there may be considerable amount of correlation between
the failed nodes, particularly spatial. The spatially correlated
faults is specially relevant in military networks, where an
enemy bomb may destroy a large number of nodes confined
in a particular area (region) or an enemy jammer jams a
part of the network disabling the nodes in that part only.

U.S. Government work not protected by U.S. copyright

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

Fig. 1.

Links between nodes with and without shadowing effect

Spatially correlated failures may be encountered in sensor
networks as well where nodes of a region may get destroyed
due to fire or increase in temperature. A wireless network
formed by the nodes and a fault region is shown in Fig. 2.
Recently, there has been considerable interest in study of
the impact of spatially-correlated or region-based faults on
networks, both wired and wireless [12]–[15]. The authors in
[12] introduced the notion of region based faults and region
based connectivity and showed how this new metric (regionbased connectivity) can be utilized to design networks with
the same level of robustness as the metric connectivity, but
with significantly lesser amount of networking resources (e.g.,
transmission power of the nodes). The idea of region-based
faults was further extended in [16], where faults were confined
to be in multiple regions instead of just one. The results
presented in this paper are based on the region-based fault
models considered in [12], [16].

domain is presented in Table II.
Penrose in [1] proved that in wireless network with uniformly distributed nodes and with uniform transmission range
of all the nodes, if node density is very high, then probability
of a random geometric network becoming k-connected is equal
to the probability of each node in the graph having minimum
degree of k. In [6] authors verified that statement through
simulation and analytical studies in circular deployment area.
The implication of the above result is that in highly dense
random geometric network with uniform transmission range
for all nodes, the network will become connected as soon as
the transmission range is large enough to achieve a network
with no isolated nodes. So in this scenario probability of having no isolated nodes (i.e. P (dmin (G) ≥ 1) where dmin (G)
denotes the minimum node degree of the network) is a good
estimation of probability of connectivity (i.e. P (κ(G) ≥ 1)
where κ(G) represents the connectivity of the graph G).
But all these results are valid in fault-free network scenario
and with unit-disk communication model. In this paper we
investigate whether probability of having no isolated nodes
(i.e. P (dmin (G) ≥ 1) is still a good estimate of connectivity
of wireless network with region-based fault scenario and lognormal shadow fading communication model.
The contributions of the paper are as follows:
1) We provide an analytical expression for computing
P (dmin (G) ≥ 1) where node distribution is uniform and
faults are region-based,
2) Through extensive simulation we find P (κ(G) ≥ 1) and
compare it with P (dmin (G) ≥ 1) and
3) Study the impact of region based faults on the connectivity of the wireless network under unit disk graph and
log-normal shadow fading model.
The rest of the paper is organized as follows: in section II
we describe the node distribution, wireless channel model and
region-based fault model used in this paper, in section III we
give the analytical expression for computing P (dmin (G) ≥ 1),
in section IV we describe the simulation setup and discuss the
results and conclude the paper in section V.

Fault Region

II. S YSTEM M ODELS
Fig. 2.

Network with a Fault Region

We provide a taxonomy of networks, faults and communication models and then identify the class where we focus our
attention. To the best of our knowledge, this is the first study
that presents connectivity related results in wireless networks
in log-normal shadow fading model and with localized fault
scenario. In our taxonomy, we have four parameters - network
description, network state, fault model and communication
model. By network description, we imply a topological description of the network, or a geometric description. By
network state, we imply a fault-free or a faulty network. By
fault model we imply a random fault or a spatially correlated
fault and finally, by communication model we imply a unit disk
graph or a log-normal shadow fading model. The taxonomy
is presented in Table I. A summary of related work in this

In this section we discuss about the node distribution model,
wireless channel model and fault model considered in this
paper.
A. Node distribution
We assume that nodes are distributed randomly and uniformly on an infinitely large two dimensional plane with a
constant node density ρ. We consider a circular subarea A of
radius rd containing n nodes from this infinite plane such that
ρ = n/A. In limiting case for large n, the uniform distribution
of n nodes in an area A is well approximated by Poisson point
process.
B. Signal propagation model
In wireless radio communication, received signal power
decreases logarithimically as the distance between transmitter

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

TABLE I
TAXONOMY TABLE

Most Studies
Our Study

Network Description
Topological
Geometric

Network State
Network with fault-free nodes
Network with faulty nodes

Fault Model
Random (if faults are considered)
Spatially correlated

Communication Model
Unit Disk Graph Model
Log-normal Shadow Fading Model

TABLE II
R ELATED W ORKS TABLE

Network Description
Topological
Topological
Geometric
Geometric
Geometric
Geometric
Geometric

Network Characteristics
Network State
Fault Model
Fault-free
NA
Faulty
Random
Fault-free
NA
Faulty
Random
Faulty
Spatially Correlated
Fault-free
NA
Faulty
Spatially Correlated

and receiver increases. This phenomenon is called path loss.
If transmitter u transmits a signal at power Pt (u) and receiver
v receives the signal at power Pr (v), then
1
Pr (v)
∝ α
Pt (u)
d
where d is the separation distance between transmitter and
receiver and α is the path loss exponent. α depends on the
propagation environment and terrain structure and can vary
from 2 in free space to 6 in dense urban environment [8].
A wireless link is established between u and v if the
received power Pr (v) at v is greater than a threshold power
Pth (v). The most commonly used signal propagation model
in wireless ad-hoc networks literature assumes that maximum
transmission range of a transmitter is equal to the distance R0
for which Pr (v) = Pth (v) and the received signal power is
uniform at any receiver v at a distance r, 0 ≤ r ≤ R0 [6]. So
the probability of having a link between two nodes u and v
at a distance r is a normalized function of distance r̂ = r/R0
and is given by p(r̂) = 1 if r̂ ≤ 1 and p(r̂) = 0 otherwise.
The graph formed by this model is called unit-disk graph.
Wireless network graph construction based on the above
model may be highly inaccurate as it does not consider the
variation of the signal strength due to presence of different
obstructions in its path. Measurement studies of wireless signal
power show that the mean power of signal at different distances from the transmitter varies log-normally (with standard
deviation σ) around the area mean power [8]. The value of σ
varies from 0 to as large as 12dB [8]. This model is known as
log-normal shadowing model and is very close to real radio
propagation model. The link probability between two nodes at
distance r (as shown in [9] [10]) is given by :


10α
r
1 1
dB .
(1)
p(r) = − erf √ log10
2 2
R0
2σ
where the normalization factor R0 is the maximum distance
at which a link can be formed in absence of shadowing effect
, i.e. at σ = 0 and erf(.) denotes the error function. The
implication of this link probability is that some nodes at a

Studied In
Communication Model
NA
NA
Unit Disk
Unit Disk
Unit Disk
Log Normal Shadow Fading
Log Normal Shadow Fading

[11]
[17]
[1]–[6], [18], [19]
[7]
[12], [16], [20]
[9], [10]
This paper

normalized distance r̂ < 1 from the transmitter may not have
a link while some nodes at a normalized distance r̂ > 1
may form a link with transmitter. Here we assume that all
transmitters transmit at equal power, i.e., R0 is the same
for all transmitters and all links formed in the network are
undirected. Also we assume that, given a specific value of α
and σ, a node will have no neighbors beyond distance R if
p(r > R) ≈ 0. Then the area πR2 around a node is called
neighbor-hood area of the node.
C. Fault Model
In this paper we consider that faults are localized and
confined in a single region. We consider fault region to be
circular of radius of rf . It is assumed that all the nodes in this
fault region are faulty and do not participate in the network
connectivity. Since we test the connectivity of the nodes only
within a circular subarea A of radius rd , any region fault
center occurring within a distance of (rd + rf + R) from the
center of A, will either remove some of the nodes in area A or
from the neighbor-hood of the nodes in area A . We call this
Potential Fault Location area (PFL). Fault centers occurring
outside this area will have no affect on the nodes of A or on
their neighbors.
III. P ROBABILITY O F H AVING N O I SOLATED N ODES I N
T HE G RAPH
Let X and Y be the random variables denoting the location
of a node in the area A, and the location of the center of a
Fault region (FR), respectively. In the node distribution model
we considered nodes are uniformly distributed over A with
area A = πrd2 . For these uniformly distributed nodes over
finite area A, the probability density function (pdf) is given
by:
 1
, if x ∈ A
A
(2)
fX (x) =
0
, otherwise
The node density of the area A is ρ = n/A. The distribution
of Y is uniform over PFL with area Af = π(rd + rf + R)2

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

and probability density function (pdf):
 1
, if y ∈ P F L,
Af
hY (y) =
0
, otherwise

E(D | x) = μ(x). For Poisson point process the probability
of a node having degree d is
(3)

In presence of a random fault region (FR), a node within a
circle of πrf2 from the fault center will be considered nonoperational or dead. Any node outside FR will be considered
operational or live. Only live nodes will be considered to be in
the network system. Now we define another random variable Z
which denotes location of a live node in the system after fault.
Note that random variable Z can only take a subset of values
that random variable X can take. So the probability density
function of Z over A is a sub-probability density function [21]
and consequently the probability of Z over A will not sum
up to 1. Given a fault at location y, region of overlap of FR
with A is denoted by A (y). The conditional sub-probability
density function of Z | Y is given by:
 1
, if z ∈ A \ A (y)
A
(4)
gZ|Y (z | y) =
0
, otherwise
Then sub-probability density function of Z is given by
 
gZ (z) =
gZY (z, y)dy
Af
 
=
gZ|Y (z | y)h(y)dy
Af
 
1
=
y ≤ rd + rf + R dy
AAf
z − y > rf
2
(Af − πrf )
=
= p̄ , if z ∈ A
AAf
= 0
, otherwise

μ(x)d −μ(x)
e
d!
Then the probability that the given node is isolated is
P (D = d|x) ≈

P (node iso|x) = P (D = 0|x) ≈ e−μ(x)

(6)

(7)

Thus the probability of having any isolated node is given by
 
P (node iso|x)gZ (x)dx
(8)
P (node iso) =
A

Probability that none of the n nodes in subarea A are isolated
is
(9)
P (no node iso) = (1 − P (node iso))n
Again applying Poisson approximation,
 


e−μ(x) gZ (x)dx
P (no node iso) ≈ exp − n

(10)

A

In section III-B, we show the analytical expression for calculating of the value of expected degree at any possible node
location x ∈ A.
B. Expected degree of a node in A due to any random region
fault
In absence of a fault the expected value of D of a node X
located at x can be computed by integrating ρp(r) over the
entire system plane and is given as [9]:
 2π  ∞
p(r)r dr dθ
(11)
E1 = ρ
0

(5)

where . denotes the Euclidean distance of a location from
the center of A.
Here we are considering a circular subarea A out of infinite
plane. The nodes in area A are said to be connected if there
exists at least one path between each pair of nodes in A. It
should be noted that we are not considering border affect of
this finite area A. So expected degree of any node in the border
region of A is same as any node in the central region. The
nodes outside the region A can act as relay node and help in
connecting two nodes inside A. But due to any region fault
in the area Af the expected degree of the nodes in A will
decrease more than the fault free scenario. As stated earlier,
P (no isolated node) ≥ P (connectivity) for any graph. The
goal of this paper is to investigate how tight is this upper
bound of P (connectivity) in region-based fault scenario with
log-normal shadow model.
A. Probability of having no isolated nodes in the graph
Let us consider a node at location x. The probability of node
at x having a link with another node at a distance r is given in
equation(1). Degree D of a node is the number of neighbors
of that node. Let the expected degree of the node at x is

0

In presence of a fault with center Y located at y, the expected
degree of node X will decrease. Let d(x, y) be the distance
between X and fault center Y . Then the loss of neighbors by
node X at a distance d(x, y) ≥ rf from fault center Y can
be approximately given as a function of distance d(x, y)
 2π  d(x,y)+rf
rf
p(r)rdrdθ (12)
E2 (d(x, y)) = ρ
4d(x, y) 0
d(x,y)−rf
We get the above expression by considering that the effect
of fault will be symmetrical around the node in the annular
area of width 2rf at a distance between d(x, y) − rf and
d(x, y) + rf from the node X (see Fig(3)). So the net loss
of neighbors due to a single fault at distance d(x, y) will be
proportional to the value of the net loss of neighbors in this
rf
is the
whole annular area. The multiplicative factor 4d(x,y)
ratio of area of a single fault to the area of the whole annular
region. Then expected degree of a node given the node location
and fault location can be stated as

E1 − E2 (d(x, y)) , if d(x, y) ≥ rf
(13)
E(D|x, y) =
0
, if d(x, y) < rf
Expected degree of a node for any location of fault center
in PFL can be given as
 
E(D|x) =
E(D|X, Y )h(y)dy
(14)
Af

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

Fig. 3.

Effect of fault is symmetrical around the node at X

Using (14) in expression (10) we get the probability of no
isolated node in the graph.
X and Y being random variables and independent of
each other we can employ Monte-Carlo integration [22] to
numerically evaluate P (no node iso) in equation (10). We
generate N random points for variable X uniformly in area A
and for each X we generate N random fault center for variable
Y uniformly within the fault region Af . Then according to
Monte-Carlo integration method if N is very large :
E(D|xi ) =

N
1 	
E(D|xi , yi )
N i

and approximate value of P (no node iso) is given by:
N


πr2 	
P (no node iso) = exp − np̄ d
exp(−E(D|xi ))
N i

where xi , yi are N points i = 1, . . . , N such that xi ∈ A and
yi ∈ Af . We start the evaluation with N = 1000 and increase
N thereafter until equation (10) converges.
IV. S IMULATION R ESULTS AND D ISCUSSION
We perform extensive simulations in order to study the
relation between the probability of the graph being connected
and the probability of having no isolated node in presence of
a region based faults. We also study the impact of rf (fault
radius) on the probability of the graph being connected for
different values of node density and different link formation
models. We consider a circular simulation area B containing
the circular observation subarea A in the middle. We test the
probability of the graph being connected and the probability
of no isolated nodes, only for the nodes inside area A. Let
radius of area A is rd . In order to minimize border effect of
area A we consider radius of B twice as the radius of A.
First, nB = ρB nodes are uniformly and randomly distributed on the deployment area B to ensure the density of
area A remains ρ. Then a circular fault area of radius rf is
generated with its center placed randomly over the circular
area within distance (rd + rf + R) from the center of the
deployment area A. All the nodes (say n ), which lie in fault
region are removed and graph G(V, E) is formed with the
remaining (nB − n ) nodes. The edges are formed between

nodes following the link probability given in equation (1).
Then we check whether area A has any isolated nodes or there
exists a path between any two pairs of nodes in A. We used
the java library Jgrapht to construct the graphs by this process.
In all these experiments area A is taken as 2.5 × 105 m2 . We
study the impact of two parameters on the probability of teh
graph being connected: (i) the density of nodes n in A and
(ii) the fault region radius rf keeping other factors α, σ and
R0 (see equation (1)) as constant. For a particular rf and ρ, the
same experiment is repeated for k1 number of different node
locations and k2 number of times for different fault locations,
and finally, the percentage of connected topologies and the
percentage of topologies with no isolated nodes are computed.
If k1 k2 is large enough, this experiment gives us fairly good
estimate of probability of the graph being connected P (conn)
and probability of no isolated nodes P (no iso node) in area
A. In our experiments, the value of rf is varied from 0 to
80 at steps of 20 and the value of ρ is varied from 0.0001
to 0.0008 at steps of 0.00004 with k1 k2 taken as 10, 000.
In all the experiments we consider value of α = 3, σ = 4,
R0 = 43.34 m and R = 120 m.
In the first set of experiments, we compared the probability
of no isolated nodes in the graph with probability of the graph
being connected due to a region based fault (in a random
location) in the graph for different values of ρ. We varied
ρ from 0.0001 to 0.0008 for rf = 0 and 80. Probability of
the graph being connected decreases when fault radius is 80
(rf = 80) as opposed to rf = 0. Results are shown in Fig. 4.
It may be noted that for the same rf probability of the graph
being connected is always less than probability of no isolated
nodes in the graph for low value of ρ, but when ρ is relatively
high both these probabilities merges and eventually becomes
1. This means that even with region based faults, if nodes are
distributed uniformly, probability of having no isolated nodes
in the graph is a good estimate of probability of connectivity
of the graph.
In the second set of experiments, we studied the impact
of increase of fault radius on the probability of the graph
being connected. The value of rf is varied from 0 to 80 for
ρ = 0.00074 and ρ = 0.0005. At node density ρ = 0.00074
the the graph is connected with probability 1 in fault free
scenario ( i.e. rf = 0). But with increase in fault radius
the probability of connectivity decreases. Results for change
of graph connectivity with fault radius is shown in Fig. 5.
An interesting observation is that decrease in probability of
connectivity with the increase in fault radius rf is insignificant
when node density is high.
In third set of experiments, we studied the effect of increase
in fault radius on probability of the graph being connected
(P(conn)) in two different communication link models: i) Unit
Disk graph (UDG) model and ii) Log-normal Shadow fading
model. In order to make these two models comparable, we
kept the expected degree of a node equal in no fault scenario
for both these models. The transmission range (rt ) of a node in
UDG model is required to be set to 64m to achieve the same.
The value of rf is varied from 0 to 80 keeping the node-density

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings

Simulation results show us that even in case of region-fault at
high node density P (dmin (G) ≥ 1), is a good estimate of
P (κ(G) ≥ 1), where dmin (G) represents the minimum node
degree and κ(G) represents the connectivity of the graph G.
Further study on this work can be done by relaxing the strict
requirement of all nodes to be connected and focusing on
the impact of region-based faults on the size and number of
connected components in a wireless network disconnected due
to fault.

P(conn) or P(no isolated node)

1
0.8
P(conn) rf =0
0.6

P(no iso) r =0
f

P(conn) r =80 m
f

0.4

P(no iso) r =80 m
f

0.2
0
1

2

3

4
5
6
2
Node Density (nodes/m )

7

8
x 10

−4

Fig. 4. Probability that the graph has no isolated node P (no iso node) in
comparison with Probability of Connectivity P (conn), when rf = 0 and
rf = 80

P(connectivity)

1
P(conn) ρ =0.00074
P(conn) ρ =0.0005

0.95
0.9
0.85
0.8
0

20

40
Fault radius(r )

60

80

f

Fig. 5. Effect of Fault Radius on the Probability of Connectivity P (conn),
when ρ = 0.00074 and ρ = 0.0005 in Shadow fading model

1
P(conn) in Unit Disk Graph Model
P(conn) in Log−normal Shadow Model

P(connectivity)

0.99
0.98
0.97
0.96
0.95
0.94
0

20

40
Fault radius (r )

60

80

f

Fig. 6. Effect of Fault Radius on the Probability of Connectivity P (conn),
when ρ = 0.00074 for UDG and Shadow fading model

is ρ = 0.00074. Results for change of graph connectivity with
fault radius is shown in Fig. 6. As evident from the result,
in both cases P(conn) decreases with increase in rf , but for
Log-normal Shadow model this decrease is significantly lower.
V. C ONCLUSION
In this paper, we examined the impact of region-based faults
(defined as spatially correlated faults) on the connectivity of
wireless networks using log-normal shadow fading model for
signal propagation. Through analytical study and simulation,
we investigated the the probability of a network being connected with varying values of node density and the fault radius.

R EFERENCES
[1] M. D. Penrose, “On k-connectivity for a geometric random graph,”
Random Structures and Algorithms, Wiley, vol. 15, no. 2, pp. 145–164,
1999.
[2] ——, “the longest edge of the random minimal spanning tree,” The
Annals of Applied Probability, vol. 7, no. 2, pp. 340–361, 1997.
[3] F. Xue and P. R. Kumar, “The number of neighbors for connectivity
of wireless networks,” Wireless Networks, vol. 10, no. 2, pp. 169–181,
March 2004.
[4] M. J. B. Appel and R. P. Russo, “The connectivity of a graph on uniform
points on [0, 1]d ,” Statistics & Probability Letters, vol. 60, pp. 351–357,
2002.
[5] P. Gupta and P. R. Kumar, “Critical power for asymptotic connectivity
in wireless networks,” in Stochastic Analysis, Control, Optimization and
Applications, 1998, pp. 547–566.
[6] C. Bettstetter, “On the connectivity of ad hoc networks,” The Computer
Journal, vol. 47, no. 4, p. 432, 2004.
[7] X.-Y. Li, P.-J. Wan, Y. Wang, and C.-W. Yi, “Fault tolerant deployment and topology control in wireless networks,” in MobiHoc ’03:
Proceedings of the 4th ACM international symposium on Mobile ad
hoc networking & computing, 2003, pp. 117–128.
[8] T. Rappaport, Wireless communications: principles and practice. Prentice Hall PTR Upper Saddle River, NJ, USA, 2001.
[9] C. Bettstetter and C. Hartmann, “Connectivity of wireless multihop
networks in a shadow fading environment,” Wireless Networks, vol. 11,
no. 5, pp. 571–579, 2005.
[10] R. Hekmat and P. Van Mieghem, “Connectivity in wireless ad-hoc
networks with a log-normal radio model,” Mobile Networks and Applications, vol. 11, no. 3, pp. 351–360, 2006.
[11] B. Bollobás, Random graphs. Cambridge Univ Pr, 2001.
[12] A. Sen, B. H. Shen, L. Zhou, and B. Hao, “Fault-tolerance in sensor
networks: A new evaluation metric,” in Proceedings of Infocom, 2006.
[13] S. Neumayer, G. Zussman, R. Cohen, and E. Modiano, “Assessing the
impact of geographically correlated network failures,” in IEEE Military
Communications Conference, 2008. MILCOM 2008, 2008, pp. 1–6.
[14] ——, “Assessing the vulnerability of the fiber infrastructure to disasters,”
in Proceeding of INFOCOM, 2008.
[15] S. Neumayer and E. Modiano, “Network Reliability With Geographically Correlated Failures,” in Proceeding of INFOCOM, 2010.
[16] A. Sen, S. Murthy, and S. Banerjee, “Region-based connectivity - a new
paradigm for design of fault-tolerant networks,” in Proceedings of High
Performance Switching and Routing, 2009.
[17] S. Nikoletseas, K. Palem, P. Spirakis, and M. Yung, “Connectivity
properties in random regular graphs with edge faults,” International
Journal of Foundations of Computer Science, vol. 11, no. 2, pp. 247–
262, 2000.
[18] C. Bettstetter, “On the connectivity of wireless multihop networks with
homogeneous and inhomogeneous range assignment,” in Proceedings of
Vehicular Technology Conference, vol. 13, Vancouver, Canada, September 2002, pp. 1706–1710.
[19] ——, “On the minimum node degree and connectivity of a wireless
multihop network,” in MOBIHOC, 2002.
[20] A. Sen, S. Banerjee, P. Ghosh, and S. Shirazipourazad, “Impact of
region-based faults on the connectivity of wireless networks,” in Communication, Control, and Computing, 2009. Allerton 2009. 47th Annual
Allerton Conference on, 2009.
[21] K. Kobayashi, Mathematics of information and coding. Amer Mathematical Society, 2002.
[22] S. Weinzierl, “Introduction to monte carlo methods.” [Online].
Available: http://arxiv.org/abs/hep-ph/0006269v1

arXiv:1404.6890v2 [cs.DS] 25 Jan 2017

d-Hop Dominating Set for Directed Graph with
in-degree Bounded by One
Joydeep Banerjee, Arun Das, and Arunabha Sen
School of Computing, Informatics and Decision System Engineering
Arizona State University, Tempe, Arizona 85287
Email: {joydeep.banerjee, arun.das, asen}@asu.edu

Abstract
Efficient communication between nodes in ad-hoc networks can be established through repeated cluster formations with designated clusterheads. In this context minimum d-hop dominating set problem was introduced for cluster formation in ad-hoc networks and is proved to be
NP-complete. Hence, an exact solution to this problem for certain subclass of graphs (representing an ad-hoc network) can be beneficial. In this
short paper we perform computational complexity analysis of minimum
d-hop dominating set problem for directed graphs with in-degree bounded
by 1. The optimum solution of the problem can be found polynomially
by exploiting certain properties of the graph under consideration. For a
digraph GD = (VD , ED ) an O(|VD |2 ) solution is provided to the problem.

1

Introduction

An Ad-hoc network is characterized by a set of dynamic nodes communicating
through wireless links. Certain nodes in an ad-hoc network (namely clusterheads and gateways) are elected to form a backbone [1] which support efficient
inter-communication in between nodes. All the messages generated by nodes
inside a cluster is routed via a cluster-head. Gateway nodes function as an entity
for inter-cluster communication. Pertaining to the dynamics of the nodes in the
network, certain nodes in a cluster might go out of reach from its cluster-head.
This necessitates continuous re-election of cluster-heads.
In defining a cluster, the ad-hoc networks needs to be represented in form
of a digraph GD = (VD , ED ). The vertex set VD consists of all nodes in the
network. The edge set ED consist of ordered pairs. An edge (v1 , v2 ) exists if a
node v2 is in the wireless range of a node v1 . A node v2 is d-hops away from
node v1 if there exists a path v1 → vp1 → ... → vp(d−1) → v2 in the graph
GD with d being the smallest such integer. In [2], a cluster formation problem
is proposed in which a node belonging to a cluster is either a cluster-head or
is at most d-hops away from the cluster. This is referred to as the minimum

1

d-hop dominating set problem . The problem is proved to be NP complete. In
[2] only bidirectional links between two nodes (i.e. a network represented by
an undirected graph) is considered. In this paper the problem is redefined for
digraphs to achieve the scope of having both unidirectional and bidirectional
links. For a bidirectional link between nodes v1 and v2 a pair of directed edges
(v1 , v2 ) and (v2 , v1 ) is included in the edge set ED of the graph GD . With these
definitions the decision version of minimum d-hop dominating set problem for
digraphs is stated as follows:
Instance: A directed graph GD = (VD , ED ), two positive integers d and K.
0

0

Question: Is there a subset |VD | ⊆ VD with |VD | ≤ K such that every vertex
0
0
v∈
/ VD − VD is at most d hops away from at least one vertex in VD .
In a directed graph GD = (VD , ED ) a node u dominates a node v if the
edge (u, v) ∈ E. For directed graphs, dominating set is proved to NP complete
[3]. But for directed graphs with in-degree of at most 1, the problem can be
solved polynomially [3]. With a similar context, in this short paper we analyze
the minimum d-hop dominating set problem with a restricted sample space.
The restriction is imposed on the graph GD which has its in-degree bounded
by 1. We show the existence of a polynomial time algorithm of the restricted
version for the problem by exploiting certain properties of the graph under
consideration. This is beneficial for attaining exact solution to all cluster-head
re-election phases where the graph has the given property. The solution also
has application to approach a subclass of particular problem in interdependent
network [5]. To best of our knowledge, all subsequent works which define a
cluster as in [2] or any other analysis of d-hop dominating set problem doesn’t
approach this restricted version.

2

Analysis of Restricted Version of the Problem

Certain properties and definitions of a directed graph with in-degree bounded
by one (denoted as the graph GD = (VD , ED )) are stated before the analysis of
the problem.
Property 2.1 A weakly connected subgraph GSD of the directed graph GD has
the following properties — 1) GSD can’t have more than one cycle (shown in
Figure 1(a)). If there exists more than one cycle then at least one node has
an in-degree greater than 1. 2) Owing to (1) no cycle of GSD can have a
subgraph whose vertices and edges form another cycle (shown in Figure 1(b)).
3) Additionally all nodes of a cycle has no directed incoming edge from another
node not in the cycle (shown in Figure 1(c)).
Hence based on Property 2.1, the subgraph GSD is either a Directed Acyclic
Graph (i.e. a DAG with in-degree of all nodes bounded by 1) or has one cylce
with all directed edges going out from nodes in the cycle.

2

Figure 1: Figures showing weakly connected graphs (a) with two cycles, (b)
with a cycle and a subgraph of the cycle which also forming a cycle, (c) cycle
having a node which has an incoming edge. All the graphs have at least one
node with in-degree greater than one (marked as white)
Property 2.2 The number of directed edges of the graph GD is upper bounded
by |VD | as all nodes have in-degree bounded by 1.
Property 2.3 If the subgraph GSD (as in Property 2.1) is a DAG then there
exists exactly one node which has an in-degree of zero (otherwise there would be
a cycle). Such a node is referred as the root node.
Definition 2.1 A leaf node of a weakly connected component is defined
as the node with an incoming edge and no outgoing edge.
Definition 2.2 The distance of a vertex v in a weakly connected component is
defined as the number of hops by which v is away from the root node if the graph
is a DAG. If the weakly connected component has a cycle then the distance of
vertex v is defined as the number of hops by which v is away from the closest
node in the cycle. All vertices in the cycle has a distance value of 0.
Definition 2.3 An isolated strongly connected component is defined as
the component which is not a subgraph of any weakly connected component of
the graph GD
Property 2.4 Each isolated strongly connected subgraph of the directed graph
GD is just a single cycle(follows directly from Property 2.1).
Property 2.5 The graph GD consists of weakly connected components and isolated strongly connected components as given by Property 2.1 and 2.4 respectively.
3

Exploiting the properties of the graph GD an algorithm (Algorithm 1) is
designed to solve the restricted minimum d-hop dominating set problem for
directed graphs. The proof of optimality and time complexity analysis of the
algorithm are provided in Theorem 2.1 and Theorem 2.2 respectively.

Algorithm 1: Algorithm for finding minimum d-hop dominating set of
graph GD

1
2
3
4
5
6
7

8
9
10
11

12
13

14

15
16

Data: A directed graph GD = (VD , ED ) with in-degree bounded by one and a
positive integer d
Result: minimum d-hop dominating set D for the graph GD
begin
Set D = ∅;
Compute all weakly connected components and isolated strongly connected
components of the graph GD ;
for (Each weakly connected component GW = (VW , EW ) of graph GD ) do
Set of covered vertices S = ∅ ;
while (S 6= VW ) do
Pick the node u ∈ VW /S in GW having the highest distance. If
there exists more than one node then pick arbitrarily;
if If the distance value of node u is 0 and VW /S 6= ∅ then
Form individual graphs G0 = (V 0 , E 0 ) with V 0 inVW /S
composed of each connected component ;
Merge two connected components in vertices in VW can be used
to merge them and the merging uses < d vertices ;
Select vertices from the merged components and add them to D
such that all vertices in VW /S are d-hop dominated and the
number of selected vertices is minimized ;
Break ;
Include node u in set D such that the number of hops from u to v
is maximum but is less than or equal to d;
Update set S by including vertex u along with all other vertices
that are d hop dominated by v;
for (Each isolated strongly connected component GS = (VS , ES ) of graph
GD ) do
S|
Include d |V
e nodes in set D with number of nodes between each
d+1
vertex picked being ≥ 1 and ≤ d ;

Theorem 2.1 Algorithm 1 gives the optimum solution of d-hop dominating set
problem for the graph GD .
Proof For an isolated strongly connected component GS = (VS , ES ) at least
|VS |
S|
d |V
d+1 e nodes has to be included in the solution. Algorithm 1 selects d d+1 e
nodes for each strongly connected component with number of nodes between
each vertex picked being ≥ 1 and ≤ d. Thus it includes the optimum number
4

nodes in the solution for all strongly connected components. For a weakly
connected component all the vertices included in the solution with respect to
the node having highest distance value > 0 is optimal. This can be proved
by contradiction. If a node is included in the solution that does not d − hop
dominate the node having current highest distance value at any iteration then
another vertex needs to be included in the solution to dominate it. Hence
the cardinality of the solution set would increase. After a certain number of
iterations in the while loop (line 6 − 13) the nodes that are not d-hop dominated
(if exist) would essentially be the nodes in the cycle. So they would have a
distance value of 0 and the algorithm would enter the computation steps as in
line 8 − 11. Each graph formed in line 9 would essentially be a path graph. Two
graphs G01 and G02 are merged using vertices inside the cycle and iff the number
of vertices to include is < d. This ensures when a vertex in G0 is selected in
solution it would take into account the vertices it can dominate in G00 . Selecting
the minimum number of vertices that finds the d-hop dominating set of these
merged components is straightforward and is easily seen to be optimal. Hence
Algorithm 1 returns an optimal solution to the d-hop dominating set problem
with in-degree bounded by 1.
Theorem 2.2 Algorithm 1 solves d-hop dominating set problem for the graph
GD polynomially with time complexity of order O(|VD |2 )
Proof All strongly connected components of the graph GD can be found using
Tarjan’s algorithm [4] in O(|VD | + |ED |) = O(|VD |) (as |ED | ≤ |VD |). The
isolated strongly connected components can be separated by checking the outdegree of all nodes in a strongly connected components which should be exactly
equal to 1. This is done in O(|VD |). All other components form the weakly connected components. Hence step 3 takes O(|VD |). The first for loop computes
the minimum d-hop dominating set for all weakly connected components. In
worst case all the vertices of the graph GD belongs to some weakly connected
components. Consider that there are m weakly connected components (with
m ≤ |VD |) and the graphs G1 = (V1 , E1 ), ..., Gm = (Vm , Em ) represent the
components. Accordingly, the for loop in step 4 iterates for m times. In each
iteration of the while loop in step 6, at least 1 nodes is included in S. Hence, in
ith iteration of the for loop, the while loop iterates for at most |Vi | times. For a
given weakly connected graph Gx = (Vx , Ex ) in the xth iteration of the for, the
distance of the nodes (in step 6) can be found in O(|Vx | + |Ex |) = O(|Vx |) (considering the fact |Ex | ≤ |Vx | and the distance values for nodes in any component
has to be computed exactly once). The computations in step 13 − 14 can then
be done in O(|Vx |) . Similarly the computations in the branch of lines 9−11 can
be done in O(|Vx |). So the running time of the while loop is O(|Vx |2 ) considering the while loop iterates for the maximum number
Pmof times. In overall the
running time of the for loop is bounded by maxm (d i=1 |VDm |2 e) = O(|VD |2 )
where VDm are vertices in the weakly connected component at mth iteration.
The second for loop starting in step 15 computes the minimum d-hop dominating set for all isolated strongly connected component. Again, in the worst

5

case all vertices of the graph GD belongs to some isolated strongly connected
component. Similarly consider that there are m components (with m ≤ |V2D | )
and the graphs G1 = (V1 , E1 ), ..., Gm = (Vm , Em ) represent the components.
For an iteration i of the for loop the computation time of step 19 is bounded
from above by |Vi |. Hence with m iteration of the for loop the computation
m
P
complexity in steps 18 − 19 is upper bounded by
|Vi | = O(|VD |). So the total
i=1

time complexity for finding minimum d-hop dominating set for isolated strongly
connected components as in step 18 − 19 is O(|VD |). Hence Algorithm 1 takes
in total an O(|VD |2 ) time to compute the optimum solution of the problem.

3

Conclusion

In this short paper we analyze the minimum d-hop dominating set problem
for directed graph with in-degree bounded by one. It is found that exploiting
certain properties of the graph under consideration an algorithm can solve the
problem in polynomial time, with run time complexity bounded by two times
the number of vertices in the graph. This result can be used in any application
(as in [5]), apart from cluster-head election in wireless ad-hoc networks, where
1) the problem can be formulated as d-hop dominating set 2) the underlying
graph has in-degree of all nodes bounded by one.

References
[1] Baker, Dennis J., and Anthony Ephremides. ”The architectural organization
of a mobile radio network via a distributed algorithm.” Communications,
IEEE Transactions on 29.11 (1981): 1694-1701.
[2] Amis, Alan D., et al. ”Max-min d-cluster formation in wireless ad hoc networks.” INFOCOM 2000. Nineteenth Annual Joint Conference of the IEEE
Computer and Communications Societies. Proceedings. IEEE. Vol. 1. IEEE,
2000.
[3] Chlebk, Miroslav, and Janka Chlebkova. ”Approximation hardness of dominating set problems.” AlgorithmsESA 2004. Springer Berlin Heidelberg,
2004. 192-203..
[4] Tarjan, R. E. ”Depth-first search and linear graph algorithms”, SIAM Journal on Computing 1 (2): 146160, 1972.
[5] Das, Arun, Joydeep Banerjee, and Arunabha Sen. ”Root cause analysis of
failures in interdependent power-communication networks.” Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014.

6

2016 International Workshop on Wireless Sensor, Actuator and Robot Networks - ICNC Workshop

On Mobile Sensor Data Collection Using Data Mules
Arun Das, Anisha Mazumder, Arunabha Sen

Nathalie Mitton

School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287, USA
Email: {arun.das, amazumde, asen}@asu.edu

Inria
40 Avenue Halley
59650 Villeneuve D’ASCQ, France
Email: nathalie.mitton@inria.fr

Abstract—The sensor data collection problem using data
mules has been studied fairly extensively in the literature.
However, in most of these studies, while the mule is mobile, all
sensors are stationary. The objective of most of these studies is
to minimize the time needed by the mule to collect data from all
the sensors and return to the data collection point from where it
embarked on its data collection journey. The problem studied in
this paper has two major differences with these earlier studies.
First, in this study we assume that both the mule as well as
the sensors are mobile. Second, we do not attempt to minimize
the data collection time. Instead, we minimize the number of
mules that will be needed to collect data from all the sensors,
subject to the constraint that the data collection process has to be
completed within some pre-specified time. We show that the mule
minimization problem is NP-Complete and analyze the problem
in two settings. We provide solutions to the problem in both
settings by first transforming the problem to a generalized version
of the minimum flow problem in a network, and then solving it
optimally using Integer Linear Programming. Finally, we evaluate
our algorithms through experiments and present our results.

I.

I NTRODUCTION

In the prevalent literature, “data mules” are referred to
mobile devices that travel to, and collect data from sensors
located at sparsely dispersed points in a deployment area. The
mules then subsequently bring back the collected data to a
central collection point [1], [2]. From an energy saving perspective, data mules offer an attractive alternative to the sensor
data collection process carried out by multi-hop forwarding
techniques. Data mules travel to the vicinity of sensors in the
deployment area and once within the communication range of
the sensors, start collecting data from these sensors. Since the
amount of data stored in different sensors may vary, the data
collection time required by the mule for each sensor may also
vary. Although data collection using data mules may result
in energy savings, it might also result in increased delay (or
latency) for data collection. Accordingly, a number of studies
have also been undertaken to find intelligent paths for the
mules with the objective of minimizing the delay [3].
Although sensor data collection problems using data mules
have been studied fairly extensively in the literature, in most of
these studies, while the mule is mobile, the sensors are assumed
to be stationary. The objective of a majority of these studies
is to minimize the time needed by the mule to collect data
from all the sensors and return to the central collection point.
The problem studied in this paper has two major differences
with earlier studies. First, in this study we assume that both
mules and sensors are mobile. Second, we do not attempt to
minimize the data collection time, instead, we minimize the
number of mules required to collect data from all sensors,

978-1-4673-8579-4/16/$31.00 ©2016 IEEE

subject to the constraint that the entire data collection process
has to be completed within some pre-specified time. We term
this problem as the Mule Minimization Problem (MMP). It may
be noted that stationary sensors can be viewed as a special case
of mobile sensors, hence a solution technique for the MMP is
equally applicable to both mobile and stationary sensors. We
now outline some of the specifics of the MMP.
In the MMP, we assume that a central controller has the
knowledge of: (i) the number of sensors in the deployment
area, (ii) the trajectories of their movement, (iii) their location
at every instance of time during the data collection period
T , (iv) their speed, and (v) the amount of data available on
each sensor. From this information, a centralized controller
computes (i) the minimum number of mules required to read
data from all sensors within the pre-specified time T , and
(ii) the trajectories that the mules should follow in order
to accomplish this task. We illustrate the problem with the
help of an example. Figure 1 shows the trajectories of six
mobile sensors on a two dimensional deployment area and
their locations at various instances of time. We assume that the
speed of the sensors are uniformly 1 unit/sec, (where the unit
of measure can be feet, meter, etc.). The location of the sensors
moving at this speed at various instances of time between the
time interval [0-46] are also shown in Figure 1, however, to
retain clarity, not all locations of each sensor are shown.
- Sensor Location
- Sensor Trajectory

𝑆4 at t=32
20
𝑆3 at t=40
𝑆3

𝑆4 at t=46

15

𝑆4
𝑆5 at t=28

𝑆5

𝑆2

10

𝑆2 at t=26
𝑆6 at t=16

5

0

5

𝑆1 at t=2
𝑆1

𝑆6 at t=2

𝑆2 at t=12
𝑆3 at t=26

0

𝑆5 at t=14

𝑆6

𝑆1 at t=16
10

15

20

Fig. 1: Locations and trajectories of six sensors S1 , . . . , S6 between
the time interval [0-46]

The goal of the MMP is to find the minimum number of
mules needed to collect data from all sensors within a given
pre-specified time T , where T denotes the maximum allowable
time step within which the data collection process must end.
It may be noted that T does not include travel time from
the central collection center to the start of the data collection

process, (the location at the first instance of time when data is
transferred between a sensor and mule), and also excludes the
travel time to the collection center from the end of the data
collection process, (the location at the instance of time when
data from all sensors have been read by one or more mules).
For the sake of simplicity, for the example of Figure 1
we assume that (i) a mule can collect data from a sensor
only when the distance between the mule and the sensor is
at most one unit, (ii) the speed of the mule is equal to that of
the sensors, i.e. 1 unit/sec, and (iii) the rate of data transfer
between the sensor and the mule is 1 unit/sec (where the unit
of measure can be megabytes, kilobytes, etc.). The solution to
the problem of Figure 1 is shown in Figure 2, where only one
mule is sufficient to collect all data from six sensors within
the pre-specified time of T = 46. In Figure 2, the trajectory
of the mobile data mule is shown with a thick red line, and
the locations where the mule collects data from the sensors are
shown with hatched rectangles. Specifically, the mule collects
3 units of data from S6 during time interval [10-13], 2 units of
data from S1 during time interval [13-15], 2 units of data from
S2 during time interval [17-19], 3 units of data from S5 during
time interval [23-26], 4 units of data from S3 during time
interval [33-37], and finally 6 units of data from S4 during time
interval [38-44] at a uniform data transfer rate of 1 unit/sec.
- Sensor Location
- Sensor Trajectory
- Mule Location
- Mule Trajectory
- Sensor Data Read

𝑆4 at t=32
20
𝑆3 at t=40 𝑀1 at t=38
𝑆3 read at [33,37]
𝑆3
𝑆4 read at [38,44] 𝑆4 at t=46
𝑆4 𝑀1 at t=44
𝑀1 at t=23
𝑆5 at t=28
𝑆5 read at [23,26] 𝑆

15

𝑀1 at t=33

10

𝑆5 at t=14

2

𝑀1 at t=17
5

𝑆2 at t=12
𝑆3 at t=26

0

𝑆5

0

5

𝑆1 at t=2
𝑆6 at t=16 𝑆2 at t=26
𝑆
𝑆2 read at [17,19]
1
𝑆1 read at [13,15]
𝑆
at
t=2
𝑆6 read at [10,13]
6
𝑀1 at t=10
𝑆6
𝑆1 at t=16
10

15

20

Fig. 2: A single mule is sufficient to read data from all six sensors
of Figure 1 within time step T = 46. The trajectory of the mule is
shown with a thick red line and the locations where the mule collects
data from the sensors are shown with hatched rectangles.

As shown in Figure 1, our model allows different sensors
to have different amounts of data to transfer. This implies that
each sensor may require different amounts of time to transfer
such data to a mule. This raises an important question with
respect to the available data collection infrastructure of the
mobile data mules: Whether fragmented data collection from
sensors is allowed or not? In other words, whether a single
mule should collect all available data from a sensor, or can
multiple mules collect fragments of the data from a sensor that
can then be put together by a central system? If multiple mules
are allowed to pick up fragments of data from a sensor, then
there has to be synchronization between the mules to determine
which mule picks up which part of the data. Additionally, the
mules must also possess a level of intelligence to facilitate such
synchronization by reading parts of the sensor data at specific

intervals of time. In this paper we consider both versions of the
problem, one in which fragmented data collection is allowed,
i.e. mules have sufficient intelligence to allow synchronization
and are capable of reading specific parts of the sensor data, and
the other, when fragmented data collection is disallowed, i.e.
mules do not possess such intelligence and is necessary that a
single mule read a sensor’s data in its entirety. As discussed
in Section III, the complexity of the solution for the second
version is considerably higher than the first.
The rest of the paper is organized as follows: in Section
II we outline the related work on this topic, in Section III
we formally state the MMP, show that it is NP-Complete, and
provide solution techniques for the two settings of the problem.
Section IV discusses the experiments conducted using our
techniques, and in Section V we conclude the paper.
II.

R ELATED W ORK

As indicated earlier, to the best of our knowledge, the
mule minimization problem with a constraint on the data
collection time has not been studied. Most of the previous work
either consider different problems and assumptions, or focus
on similar issues but with different goals and objectives. In [4],
[5] the authors focus on the problem of choosing the path of
a data mule that traverses through a sensor field with sensors
generating data at a given rate. To this purpose, the authors of
[4] designed heuristic algorithms to find a path that minimizes
the buffer overflow at each sensor node, that they later extended
to multiple data mules and viewed it as a vehicle routing
problem (VRP) [6]. In these works, however, it was assumed
that data mules need to travel to the sensor nodes’ exact
location to collect data (i.e., excluding remote communication).
This assumption facilitates TSP-type formulations for their
problem and makes the data mule path selection problem
similar to a packet routing problem, such as the one studied in
[7]. However, these formulations under-utilize communication
capabilities, as data mules can use wireless communication to
collect data from nodes without visiting their exact locations.
Zhao and Ammar [8] studied the problem of optimally
controlling the motion of a data mule in mobile ad-hoc
networks. A data mule, called a message ferry, mediates
communications between sparsely deployed stationary nodes.
They considered remote communication, but path selection
was done based on a TSP-like formulation. They extended
their work to multiple data mules in [9] and presented heuristic
algorithms. In [3], the authors proposed to adapt the motion
of the mule to minimize the full delay of data gathering. Ma
and Yang [10] discussed the path selection problem under
different assumptions. Their objective was to maximize the
network lifetime, which is defined as the time until the first
node dies (i.e. minimum of the lifetime of all nodes). They
considered remote wireless communication and also multi-hop
communication among nodes. When the path of a mule is
given, they showed the problem of maximizing the network
lifetime is formulated as a flow maximization problem that
has a polynomial time algorithm. Choosing the mule’s path is
done by their heuristic algorithm that uses a divide and conquer
approach to find a near optimal path for each part of the path.
Other approaches like the one in [11] are also inspired from
vehicle networks to transfer data. They are called carry-andforward techniques and offer more opportunistic data delivery.

They can thus neither guarantee any QoS, nor limit the number
of mules. Finally, as mentioned, all these works do not consider
mobile sensors. To the best of our knowledge, the only study to
consider mobile sensors is [12], but this work assumes a given
number of mules and does not try to minimize this number, but
instead proposes the best coverage possible at a given time.
III.

M ULE M INIMIZATION P ROBLEM

Our technique for solving the Mule Minimization Problem
(MMP) is to first transform the problem into a network flow
problem, and then utilize Integer Linear Programming (ILP) to
solve the network flow problem. As indicated in Section I, in
this paper we consider two variants of the problem: (i) when
the mules have sufficient intelligence that allow fragmented
data collection, i.e. the task of reading data from a single sensor
can be distributed to different mules, and (ii) when the mules
do not possess such intelligence, and the entirety of a single
sensor’s data must be read by a single mule. In Section III-A
we present our solution for the MMP where mules have such
intelligence, and in Section III-B, we extend the technique
from Section III-A to address the scenario where the mules do
not possess such intelligence.
A. Fragmented Data Collection - Mules with Intelligence
Although the MMP is a continuous time domain problem
(as the mobile sensors and mules can be anywhere in the
deployment area at a given time), our approach to solving the
MMP is to discretize both time and space. In our technique,
time is discretized into equal intervals of length δ, and space
into equal intervals of length ε. The discretization of time and
space allows the possibility of degradation of the quality of the
solution, i.e. our approach may not be able to find the absolute
minimum number of mules needed to collect data from all
sensors. However, the advantages of such discretization is
lower computation time, as the computational complexity of
our solution is inversely proportional to the magnitude of
the variables δ and ε. Thus our technique provides a direct
mechanism to manage the trade-off between the quality of the
solution (measured in terms of accuracy) and the cost of the
solution (measured in terms of computation time).
We formally setup the problem as follows: We consider
a set of n mobile sensors A = {a1 , . . . , an } moving on
a one dimensional plane (i.e., a line)1 over time instances
0, . . . , T . It may be noted that although the movement of
sensors is restricted to one dimension, there is no restriction
on the direction of their movement, i.e. they can move either
left and/or right, and can change directions arbitrarily. Let
p(ai , t) = x(ai , t) be the location of sensor ai at time instance
t where x(ai , t) denotes the x-coordinate of ai at time t. We
assume that data from a sensor ai can be collected by a mule
Mj only if the distance between ai and Mj is less than the
communication range of the sensor and mule, denoted by r.
Theorem 1. Mule Minimization Problem is NP-complete.
Proof: The problem instance of MMP where the sensors are
stationary (a special case of mobile sensors), is equivalent to
1 We present the formulation in one dimension for clarity of explanation
and brevity. Once the underlying principle of our solution to the problem
is understood, extension to higher dimensions is straightforward as the same
principles apply.

the Geometric Disk Cover Problem which is known to be NPcomplete [13].
To find the solution for the MMP, we first transform it
to a generalized version of the minimum flow problem on a
directed graph G = (V, E). In this formulation, individual
flows correspond to a path from the source to the destination
node in G = (V, E). The number of flows provides the number
of mules required, and each path corresponds to the required
trajectory of a mule as it moves through the deployment area
collecting data from the sensors. Since we transform the MMP
into a generalized version of the Minimum Flow Problem
(MFP) [14], we first outline the MFP and then its generalized
version, GMFP.
Minimum Flow Problem (MFP): Given a capacitated network
G = (V, E) with a non-negative capacity c(i, j) and with a
non-negative lower bound l(i, j) associated with each edge
(i, j) and two special nodes, a source node S and a sink node
D, a flow is defined to be a function f : E → R+ satisfying
the following conditions:
(
F,
i=S
P
P
0,
i
6= S, D
f
(i,
j)
−
f
(j,
i)
=
j∈V
j∈V
−F, i = D
l(i, j) ≤ f (i, j) ≤ c(i, j)
for some F ≥ 0 where F is the value of the flow f . The MFP
finds a flow f for which F is minimized.
Generalized Minimum Flow Problem (GMFP): The generalized version of the MFP is similar to the MFP, except that
the lower bound on the flow requirement l(i, j) is no longer
associated with an edge (i, j), but associated with a set of
edges Ek ⊆ E of the graph G = (V, E), and is denoted by
lk . Formally, the problem can be stated as follows:
Given a capacitated network G = (V, E) with a non-negative
capacity c(i, j) associated with each edge (i, j), a set of subsets
E 0 of the edge set E (i.e. E 0 = {E1 , . . . , Ep }, where Ek ⊆
E, ∀k, 1 ≤ k ≤ p), a lower bound on the flow requirement lk
associated with each Ek , 1 ≤ k ≤ p, and two special nodes, a
source node S and a sink node D. A flow is defined to be a
function f : E → R+ satisfying the following conditions:
(
F,
i=S
P
P
0,
i 6= S, D
f
(i,
j)
−
f
(j,
i)
=
j∈V
j∈V
−F, i = D
∀Ek , 1 ≤ k ≤ p, ∃lk , a lower bound of flow in Ek , implying
that
P the total flow on the set of edges in Ek is such that
(i,j)∈Ek f (i, j) ≥ lk , and f (i, j) ≤ c(i, j), ∀(i, j) ∈ Ek
for some F ≥ 0, where F is the value of the flow f . The
GMFP finds a flow f for which F is minimized.
It may be noted that when |Ek | = 1, ∀k, 1 ≤ k ≤ p, and
p = |E|, the GMFP reduces to MFP.
MMP Graph Construction
We now outline the MMP graph construction process through
an example where the movements of the sensors are restricted
to a one dimensional space, i.e. the sensors are allowed to
move either left or right on a straight line and are allowed to
change directions arbitrarily. This restriction is imposed only
to explain the graph construction process in a lucid way. Once
the construction process is understood, the same principles can

𝑡0

𝑎1

Unavailable
Zone

Time

𝑡1

𝑟
𝑎1

𝑎2 𝑎3
𝑎2 𝑎3

𝑥0 𝑥1 𝑥2 𝑥3 𝑥4 𝑥5 𝑥6 𝑥7 𝑥8
Space

Fig. 3: Locations of three sensors on a one dimensional space (line)
at two different instances of time

be followed for constructing the MMP graph when the sensors
move in a two or three dimensional space. It may be recalled
that data from each sensor can be collected by one or more
mules, within a pre-specified data collection time T , and the
goal of the MMP is to collect data from all sensors with as
few mules as possible within time T .
Figure 3 illustrates our example, our example has three
sensors and a pre-specified data collection time T = 2, each
sensor has one unit of data that must be read by a mule and the
rate of data transfer available to all mules is one unit of data
per time step. As seen in Figure 3, sensor a1 is at location x1
at time t0 and in location x0 at time t1 . Similarly, the sensor
a2 is at location x5 at time t0 and in location x4 at time t1 ,
and sensor a3 is at location x6 at time t0 and at location x5
at time t1 . Although in this example, all sensors are moving
left at the same speed, the sensors are free to move in either
direction and at different speeds. A mule can collect data from
a sensor only if the sensor is within the communication range
r. If we assume that r = ε, as shown in Figure 3, then for
this example, in order to collect one unit of data from sensor
a1 , there must be a mule at (x0 , t0 ) or (x1 , t0 ) or (x2 , t0 ) or
(x0 , t1 ) or (x1 , t1 ), where (xi , tj ) indicates location xi at time
tj . Using a similar reasoning we can conclude that in order to
collect data from sensor a2 , there must be a mule at (x4 , t0 ) or
(x5 , t0 ) or (x6 , t0 ) or (x3 , t1 ) or (x4 , t1 ) or (x5 , t1 ). Also, in
order to collect data from sensor a3 , there must be a mule at
(x5 , t0 ) or (x6 , t0 ) or (x7 , t0 ) or (x4 , t1 ) or (x5 , t1 ) or (x6 , t1 ).
The MMP graph G = (V, E) is a directed graph and is
constructed in the following way: It may be noted that for
a mule Mj to collect data from a sensor ai , 1 ≤ i ≤ n the
distance between the mule and the sensor cannot exceed the
communication range r. For this reason, in the above example,
to collect data from sensor a1 , there must be a mule at (x0 , t0 )
or (x1 , t0 ) or (x2 , t0 ) or (x0 , t1 ) or (x1 , t1 ). Corresponding to
each sensor ak , 1 ≤ k ≤ n, there exists a set of potential
LTk = (location, time) pairs of the form (Xk,i , Tk,j ), and a
mule must be in at least one of these locations at the specific
time to be able to collect one unit of data from the sensor ak .
Also, the time taken to collect one unit of data from a
sensor is inversely proportional to the rate of data transfer
available to the mule. If the rate of data transfer is assumed
to be uniform for all mules at µ units of data per δ unit
of time, then if dk units of data have to be collected from
sensor ak , then at least d0k = d dµk e elements of the set LTk
must be chosen in order to satisfy the requirement that dk
units of data are collected from sensor ak . In the example
of Figure 3, with δ = 1, µ = 1, and d1 = d2 = d3 = 1 we
will have LT1 = {(X1,1 , T1,1 ), (X1,2 , T1,2 ), . . . , (X1,5 , Tk,5 )},
LT2 = {(X2,1 , T2,1 ), (X2,2 , T2,2 ), . . . , (X2,6 , Tk,6 )}, LT3 =
{(X3,1 , T3,1 ), (X3,2 , T3,2 ), . . . , (X3,6 , Tk,6 )}.
For each LTk = {(Xk,1 , Tk,1 ), . . . , (Xk,pk , Tk,pk )}, 1 ≤

k ≤ n, in graph G = (V, E) we introduce (i) Xk,i type nodes,
(ii) Tk,i type nodes, and (iii) a directed edge from node Xk,i to
node Tk,i , ∀i, 1 ≤ i ≤ pk . It may be noted that the (Xi,j , Ti,j )
pair need not be unique and that the pairs (Xi,j , Ti,j ) and
(Xk,l , Tk,l ) may represent the same (location, time) pair. In
case of non-unique (Xi,j , Ti,j ) pairs, only one pair of nodes
are created in the graph G = (V, E). In our example the pairs
(X2,2 , T2,2 ) = (X3,1 , T3,1 ) = (x5 , t0 ) thus only one pair of
nodes corresponding to location x5 at time t0 will be created
in G. This construction is shown in Figure 4. The Xi,j type
nodes are referred to as location nodes, and Ti,j type nodes
are referred to as time nodes. In addition to these nodes, we
also add one source node S and one sink node D. In addition
to the directed edges of type Xk,i → Tk,i , we also include
three additional types of edges in G:
1) Mobility edges: If a mule located at xa at time tb , can
move to a location xc at time td , then in the graph G =
(V, E), we add a directed edge from the node tb to xc .
It may be noted that whether the mule can move from
location xa at time tb to a location xc at time td , depends
on (i) distance between locations xa and xc , (ii) time
interval between tb and td , and (iii) speed of the mule.
2) Source edges: There is a directed edge from the source
node S to all location nodes.
3) Sink edges: There is a directed edge from all time nodes
to the sink node D.
The capacity c(i, j) is set to 1 for all edges in G = (V, E).
As discussed earlier, an instance of the GMFP has a set
of subsets E 0 of the edge set E (i.e. E 0 = {E1 , . . . , Ep },
where Ek ⊆ E, ∀k, 1 ≤ k ≤ p), with a lower bound on the
flow requirement lk associated with each Ek . If lk is the lower
bound of flow in Ek , the GMFP requires
Pthat there must exist
at least lk edges (i, j) ∈ Ek such that (i,j)∈Ek f (i, j) ≥ lk
and f (i, j) ≤ c(i, j). In the graph G = (V, E), we set
Ek = LTk , ∀k, 1 ≤ k ≤ p. In our example, since LT1
is {(x0 , t0 ), (x1 , t0 ), (x2 , t0 ), (x0 , t1 ), (x1 , t1 )}, we set E1 =
{(x0 → t0 ), (x1 → t0 ), (x2 → t0 ), (x0 → t1 ), (x1 → t1 )}.
We set the lower bound of the flow requirement in Ek , 1 ≤
k ≤ p to be d0k , i.e., lk = d0k , where d0k is the number of
time units required to collect dk data units from sensor ak , at
µ units of data per δ unit of time. In this example if δ = 1,
µ = 1 and d1 = 3, then at least three edges in the edge set
{(x0 → t0 ), (x1 → t0 ), (x2 → t0 ), (x0 → t1 ), (x1 → t1 )}
must have a flow of one unit. The GMFP graph G = (V, E)
constructed for the problem instance with three sensors in
Figure 3 is shown in Figure 4. The directed edge set E1 , E2 and
E3 , corresponding to three sensors, a1 , a2 and a3 are shown
enclosed in three rectangular boxes, colored red, yellow and
blue respectively.
Solution of MMP
We solve the MMP problem by solving the GMFP using
Integer Liner Programming. The ILP formulation is as follows:
minimize F
subject to,
(
X
j∈V

f (i, j) −

X
j∈V

f (j, i) =

F,
0,
−F,

i=S
i 6= S, D
i=D

𝑺

𝒙𝟎

𝒕𝟎

𝒙𝟎

𝒕𝟏

𝒙𝟏

𝒕𝟎

𝒙𝟏

𝒕𝟏

𝒙𝟐

𝒕𝟎

𝒙𝟑

𝒕𝟏

𝒙𝟒

𝒕𝟎

𝒙𝟒

𝒕𝟏

𝒙𝟓

𝒕𝟎

𝒙𝟓

𝒕𝟏

𝒙𝟔

𝒕𝟎

𝒙𝟔

𝒕𝟏

𝒙𝟕

𝒕𝟎

B. Unfragmented Data Collection - Mules without Intelligence

𝑎1

𝑎2

𝑫

𝑎3

Fig. 4: MMP graph constructed from the instance of the problem
shown in Figure 3

∀Ek , 1 ≤ k ≤ p, if edge (i, j) ∈ Ek ,

X

f (i, j) ≥ d0k

∀(i, j) ∈ E, f (i, j) ≤ c(i, j)
∀f (i, j) = 0/1
We prove that the minimum number of mules required to
collect data from all the sensors is equal to the solution of
the GMFP in graph G = (V, E), and the trajectories of the
mules can be constructed from the solution of the GMFP.
Theorem 2. Any valid flow of the GMFP provides the minimum number of mules needed to collect data from all the
mobile sensors within the specified data collection time T .
The solution also provides the trajectory that the mules need
to follow in order to collect data from the sensors.
Proof: If the minimum number of mules needed to collect
data from all the sensors is m, there exists m flows (paths)
from the source to the destination node in G. Suppose that
the location-time pair of mule Mi , 1 ≤ i ≤ m is given by,
(li,1 , ti,1 ), (li,2 , ti,2 ), . . . , (li,qi , ti,qi ). Being at these locations
at these times, enables the mules to collect µ units of data from
the sensors. A flow of one unit (or a path) for the source node S
to the destination node D can be constructed in the following
way: A path from S to D will be S → li,1 →, . . . , li,qi → D.
Since such a path can be constructed from S to D for every
mule Mi , there will be m unit flows from S to D.
If the solution to the GMFP is m unit flows from S to D,
then m mules are sufficient to collect data from all the sensors
in the deployment area. Because of the way the constraints are
set up, each unit flow corresponds to a path from S to D where
the intermediate nodes are of the type Xk,i and Tk,i and edges
are of the form location → time. Suppose that there is a flow
from S to D of the form S → la → tb → lc → td → le →
tf → D. From this flow, we can construct a trajectory of a
mule, where it moves from location la at time tb to location
lc at time td to location le at time tf , collecting at least a part
of the data to be collected from the sensors. Since m flows
are sufficient to satisfy the lower bound constraints imposed
on the graph by each sensor (which is equal to the amount of
data to be collected from each sensor), we can conclude that
m mules are sufficient to collect all data from all sensors.

As discussed earlier, if multiple mules are allowed to
pick up fragments of data from a sensor, then a central
synchronization mechanism must exist and mules must be
capable of reading parts of a sensor’s data. The previous
section (Section III-A) addressed this scenario, and in this
section we address the scenario where such synchronization
capabilities are unavailable and a single mule is responsible to
collect data from a given sensor in its entirety. First, we note
that if the amount of data to be collected from a sensor is more
than µ units then the solution proposed in Section III-A may
not be able to guarantee that the entire data from a sensor will
be collected by a single mule. We explain this with the help
of the following example.
Consider a scenario where data has to be collected from
two sensors S1 and S2 . Sensor S1 has 2µ units of data to
provide, and the sensor S2 has µ units of data to provide.
Suppose that due to the locations of the sensors, their speeds of
movements, and the data collection threshold time T , there are
only two (location, time) pairs (l1 , t1 ), (l2 , t2 ) where data collection from S1 is feasible. Similarly, there are two (location,
time) pairs (l2 , t2 ) and (l3 , t3 ) where data collection from S2
is feasible. Suppose also, that due to the speed of movement of
the mules, it is possible for a mule to travel from location l1 to
l2 within time interval [t1 -t2 ] and also to travel from location
l2 to l3 within time interval [t2 -t3 ]. In addition, suppose that
T is at least as large as the time interval between [t1 -t2 ] and
[t2 -t3 ], but is less than the time interval [t1 -t3 ]. To illustrate
our example further, suppose that t1 = 1, t2 = 2, t3 = 1 and
T = 2. In this case there can be two optimal solutions:
1) Solution 1: Mule M1 collects 2µ units from S1 from l1 at
time t1 , and l2 at time t2 , and mule M2 collects µ units
from S2 from l3 at time t1 .
2) Solution 2: Mule M1 collects µ units from S1 from l1
at time t1 . Mule M2 collects µ units from S2 from l3 at
time t1 and µ units from S1 from l2 at time t2 .
Clearly, in Solution 1, only one mule collects the entire data
(2µ units) from S1 , but in Solution 2, one mule collects only
half the data (µ unit) from S1 and the other mule collects
the rest. However, there is no way for the optimal solution for
the GMFP on graph G = (V, E) (Section III-A), to distinguish
between these two solutions as the minimum flow in both these
cases will be two. However, we show that the version of the
MMP where a single mule is required to collect the entire
data from a single sensor, can be solved by constructing a
new graph G = (V, E) and solving the GMFP on this new
graph. We describe the construction process of graph G next.
As discussed earlier, in the graph shown in Figure 4,
corresponding to each sensor ak , 1 ≤ k ≤ n, there exists a set
of edges Ek = (location, time) pairs of the form (Xk,i , Tk,j ),
such that a mule is able read µ units of data from sensor
ak if it is present at location Xk,i at time Tk,j . In Figure 4,
all such (location, time) pairs are enclosed within a black
rectangle, which we now refer to as a layer. To accommodate
the requirement that a single mule collects all the data from
a given sensor, we construct graph G = (V, E) by replicating
the sole layer of G, n times in G. We keep the structure of
the edges and nodes in each layer as is, but distinguish the
nodes and edges of different layers by associating them with

the layers they belong to. For example, location and time nodes
of the form Xk,i and Tk,j in V , is respectively represented as
Xk,i,m and Tk,j,m for each layer m, 1 ≤ m ≤ n in V. Similarly
(location, time) edges of the form (Xk,i , Tk,j ) in E, is
represented as (Xk,i,m , Tk,j,m ) for each layer m, 1 ≤ m ≤ n
in E. The set of edges of the form (Xk,i,m , Tk,j,m ) that appear
in layer m, 1 ≤ m ≤ n is referred to as Edges of Layer m and
is denoted by ELm .
Corresponding to each sensor ak , in each layer m, 1 ≤
k, m ≤ n, there exists a set of edges Ek,m = (location, time)
pairs of the form (Xk,i,m , Tk,j,m ) signifying a location Xk,i,m
where a mule can be present at time Tk,j,m to read µ units of
data from sensor ak . We define Edges Across Layers for Sensor
k as EALS k , to be the set of all edges across all layers that
signify this (location, time) pair where a mule can be present
to collect µ units of data from sensor ak . That is:
EALS k =

n
[

Ek,m ,

∀k = 1, . . . , n

m=1

In addition to introducing the nodes and edges discussed above,
for each layer m, 1 ≤ m ≤ n we introduce additional nodes
um , vm , and an edge um → vm , as shown in Figure 5. A
source node S is also introduced in G and is connected to
all um , 1 ≤ m ≤ n nodes. For each layer m, 1 ≤ m ≤ n
we connect node vm to all location nodes in layer m, that is
vm → Xk,i,m , ∀Xk,i,m ∈ ELm . Lastly, we introduce a sink
node D and connect all time nodes of the form Tk,j,m ∈ E to
D as shown in Figure 5. Note that for clarity, all nodes and
edges are not shown in Figure 5.

there should be at least lk units of flow through the edges of at
least one set of edges Ek,m , 1 ≤ m ≤ n for all k, 1 ≤ k ≤ n.
Because of the structure of the graph G, this lower bound
requirement, together with the constraint that the upper bound
of capacity of each edge set to one, the solution of the NGMFP
on G results in the solution of the MMP for mules without
intelligence when we set lk = d0k , where d0k is the number of
time units required to collect dk data units from sensor ak , at
µ units of data per δ unit of time. The NGMFP can be solved
by using Integer Liner Programming (ILP), and is formulated
using the following inputs:
Given (i) a set of sensors A = {a1 , . . . , an }, and a weight d0k ,
representing the time units needed to collect dk data units from
sensor ak at µ units of data per δ unit of time, (ii) a directed
graph G = (V, E), with subsets of edges associated with each
layer ELm , 1 ≤ m ≤ n, and a subset of edges associated with
each sensor EALS k , 1 ≤ k ≤ n, and (iii) capacity of all edges
set to one. We first outline the variable used for the ILP:
For each sensor ak , ELm , and directed edge (i, j):
(
P
0
1, if
(i,j)∈(EALS k ∩ELm ) f (i, j) ≥ dk
yk,m =
0, otherwise
The objective of the ILP is as follows:
minimize F
subject to,
(
X

f (i, j) −

j∈V

𝑢1

𝑣1

𝑢2

𝑣2
𝑀2

j∈V

f (j, i) =

F,
i=S
0,
i=
6 S, D
−F, i = D

∀k, m, 1 ≤ k, m ≤ n, if the edge (i, j) ∈ (EALS k ∩ ELm )
X
f (i, j) ≥ d0k × yk,m

𝑀1

𝑆

X

𝐷

n
X

yk,m ≥ 1, ∀k = 1, . . . , n

m=1

𝑢3

𝑣3

∀(i, j) ∈ E, f (i, j) ≤ c(i, j)

𝑀3

∀f (i, j) = 0/1
∀yk,m = 0/1
Fig. 5: MMP graph constructed from the problem instance of Fig. 3

The MMP for mules without intelligence can be solved by
solving a generalized version of the MFP, although it may be
noted that this generalization is different from the version of
the MFP for mules with intelligence (Section III-A). It may
be recalled that in the solution to the GMFP discussed in
Section III-A, the lower bound on the flow requirement lk
was associated with a set of edges Ek ⊆ E of the graph
G = (V, E). In this version of GMFP titled New Generalized
Minimum Flow Problem (NGMFP), the lower bound on the
flow requirement lk is no longer associated with a set of edges,
but instead with a set of set of edges EALS k ⊆ E.
The lower bound requirement of the NGMFP states that

Using a similar reasoning from Theorem 2, it can be shown
that a valid flow for the NGMFP provides the minimum number of mules (and their trajectories), required to collect data
from all mobile sensors within the specified data collection
time T , such that data collected from a single sensor is not
fragmented across multiple mules. Due to a lack of space we
leave out the formal proof.
Extension to higher dimensions: We solved the MMP problem by constructing a graph G = (V, E) from an instance
of the MMP problem and solving the GMFP (NGMFP) on it.
We provided the explanation for the graph construction process
through an example, where the movements of sensors were restricted to one dimension. However, our solution technique for
the MMP is not restricted to only one dimensional movement

5

2 𝑢𝑛𝑖𝑡𝑠
15

3 𝑢𝑛𝑖𝑡𝑠

3 𝑢𝑛𝑖𝑡𝑠
10

3 𝑢𝑛𝑖𝑡𝑠

5

4 𝑢𝑛𝑖𝑡𝑠

Number of Data Mules

20

4

0

5

10

15

(a) Sensor Trajectories

20

2
1

1

1.5

2

2.5

3

Rate of Data Transfer

(b) MMP Results

Fig. 6: (a) Trajectories and available data units of 5 sensors in the
time interval [0-10], (b) Number of mules vs. rate of data transfer,
varying mule speeds in time interval [0-8], ε = 1.0, δ = 1.0, r = 1

of the sensors. A critical component of the graph is the directed
edges of the form location → time node pairs. If the locations
of the sensors are restricted to one dimension, location →
time node pair takes the form (x) → t, where x is the location
and t is the time. If the locations of the sensors are expanded
to two or three dimensions the location → time node pairs
will take the form (x, y) → t or (x, y, z)→ t, respectively
to capture the two or three dimensional coordinates. However,
such a representation will not in any way affect the generalized
minimum flow based approach to the solution of the MMP.
IV.

closer to the optimal, smaller values of ε and δ increases the
cost of computation considerably.
V.

3

0

0

𝑆𝑝𝑒𝑒𝑑 𝑜𝑓 𝑀𝑢𝑙𝑒 = 1
𝑆𝑝𝑒𝑒𝑑 𝑜𝑓 𝑀𝑢𝑙𝑒 = 2
𝑆𝑝𝑒𝑒𝑑 𝑜𝑓 𝑀𝑢𝑙𝑒 = 4

E XPERIMENTAL R ESULTS

We present the experimental results of the solution techniques for the MMP in this section. For our experiments we
considered 5 mobile sensors in a 2-dimensional deployment
area over the time interval [0-10]. The sensor trajectories and
their available data units considered for our experiments are
shown in Figure 6(a). The sensor trajectories were specified by
unique parametric equations and thus not all speeds considered
were uniform and constant.
We used IBM CPLEX Optimizer 12.5 to solve ILPs to
compute solutions for the MMP of Figure 6(a) under both
intelligent and unintelligent mule settings. To discretize the
deployment area, we set δ = 1 and ε = 1. We then investigated
the impact of different sensor and mule parameters, namely,
the data transfer rate µ per δ unit of time, and the speed of
the mule, on the total number of mules needed to gather data
from all sensors in T = 8 time. Figure 6(b) shows the required
number of mules at a given speed, as the available rate of data
transfer (µ) is varied. For this specific problem instance and
the given mule parameters, the number of mules required to
read all sensor data within T = 8 time does not vary under
the intelligent and unintelligent mule settings. However, the
computed mule trajectories were observed to be different in
these two settings. Our experiments confirmed that for a given
mule speed, increasing the data transfer rate lowers the number
of mules required.
In our experiments, we also varied the variables ε and δ,
that are used to discretize time and space. Our observations
indicate that smaller values of ε and δ allow our solution
technique to be closer to the optimal solution in a continuous
setting (when space and time are not discretized). Smaller
values of ε and δ increases the granularity of the solution
space by increasing the number of (location, time) pairs
considered by the ILP. Though this may result in solutions

C ONCLUSION

In this paper we studied the Mule Minimization Problem
(MMP) where both the sensors and mules were mobile, and
the objective was to minimize the number of mules required
to collect data from all sensors within a pre-specified time
T . We showed that the MMP is NP-Complete and analyzed
the problem in two settings. We provided solutions for the
MMP by transforming the problem to a generalized version
of the minimum flow problem in a network, and then solving
it optimally using Integer Linear Programming. We evaluated
our approach through experiments and presented our results.
R EFERENCES
[1] I. Vasilescu, K. Kotay, D. Rus, M. Dunbabin, and P. Corke, “Data
collection, storage, and retrieval with an underwater sensor network,” in
Proceedings of the 3rd international conference on Embedded networked
sensor systems. ACM, 2005, pp. 154–165.
[2] D. Mascareñas, E. Flynn, C. Farrar, G. Park, and M. Todd, “A mobile
host approach for wireless powering and interrogation of structural health
monitoring sensor networks,” Sensors Journal, IEEE, vol. 9, no. 12, pp.
1719–1726, 2009.
[3] R. Sugihara and R. K. Gupta, “Path planning of data mules in sensor
networks,” ACM Trans. Sen. Netw., vol. 8, no. 1, pp. 1:1–1:27, Aug.
2011. [Online]. Available: http://doi.acm.org/10.1145/1993042.1993043
[4] A. Somasundara, A. Ramamoorthy, M. B. Srivastava et al., “Mobile
element scheduling for efficient data collection in wireless sensor networks with dynamic deadlines,” in Real-Time Systems Symposium, 2004.
Proceedings. 25th IEEE International. IEEE, 2004, pp. 296–305.
[5] Y. Gu, D. Bozdağ, R. W. Brewer, and E. Ekici, “Data harvesting with
mobile elements in wireless sensor networks,” Computer Networks,
vol. 50, no. 17, pp. 3449 – 3465, 2006. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S138912860600020X
[6] A. A. Somasundara, A. Ramamoorthy, and M. B. Srivastava, “Mobile
element scheduling with dynamic deadlines,” IEEE Trans. Mob.
Comput., vol. 6, no. 4, pp. 395–410, 2007. [Online]. Available:
http://doi.ieeecomputersociety.org/10.1109/TMC.2007.57
[7] A. Meliou, D. Chu, J. Hellerstein, C. Guestrin, and W. Hong, “Data
gathering tours in sensor networks,” in Proceedings of the 5th international conference on Information processing in sensor networks. ACM,
2006, pp. 43–50.
[8] W. Zhao, M. Ammar, and E. Zegura, “A message ferrying approach for
data delivery in sparse mobile ad hoc networks,” in Proceedings of the
5th ACM international symposium on Mobile ad hoc networking and
computing. ACM, 2004, pp. 187–198.
[9] M. M. Bin Tariq, M. Ammar, and E. Zegura, “Message ferry
route design for sparse ad hoc networks with mobile nodes,” in
Proceedings of the 7th ACM International Symposium on Mobile
Ad Hoc Networking and Computing, ser. MobiHoc ’06. New
York, NY, USA: ACM, 2006, pp. 37–48. [Online]. Available:
http://doi.acm.org/10.1145/1132905.1132910
[10] M. Ma and Y. Yang, “Sencar: An energy-efficient data gathering mechanism for large-scale multihop sensor networks,” IEEE Transactions on
Parallel and Distributed Systems, vol. 18, no. 10, pp. 1476–1488, 2007.
[11] G. Fenu and M. Nitti, “Strategies to carry and forward packets in
VANET,” in Digital Information and Communication Technology and
Its Applications - International Conference, DICTAP 2011, Dijon,
France, June 21-23, 2011. Proceedings, Part I, 2011, pp. 662–674.
[Online]. Available: http://dx.doi.org/10.1007/978-3-642-21984-9 54
[12] T. Razafindralambo, N. Mitton, A. C. Viana, M. Dias De Amorim,
and K. Obraczka, “Adaptive Deployment for Pervasive Data Gathering
in Connectivity-Challenged Environments,” in Eighth Annual IEEE
International Conference on Pervasive Computing and Communications
(PERCOM), France, Mar. 2010, p. 000. [Online]. Available: https:
//hal.archives-ouvertes.fr/hal-00472656
[13] R. J. Fowler, M. S. Paterson, and S. L. Tanimoto, “Optimal packing and
covering in the plane are np-complete,” Information processing letters,
vol. 12, no. 3, pp. 133–137, 1981.
[14] S. Even, Graph algorithms. Cambridge University Press, 2011.

Computer Communications 24 (2001) 868±876

www.elsevier.com/locate/comcom

On new architectures for lightwave networks
A. Sen a,*, S. Bandyopadhyay b, B.P. Sinha c
a

Department of Computer Science & Engineering, Arizona State University, Tempe, AZ 85287, USA
b
School of Computer Science, University of Windsor, Windsor, Ont., Canada N9B 3P4
c
Advanced Computing and Microelectronics Unit, Indian Statistical Institute, Calcutta 700035, India
Received 6 November 2000; accepted 6 November 2000

Abstract
In this paper, we propose two different architectures, the multi-mesh (MM) and the incomplete multi-mesh (IMM), for use in wavelength
division multiplexed optical networks. These architectures can be deployed as a physical topology for single hop or a logical topology for
multihop networks. In terms of simplicity of interconnection and routing, these architectures are comparable to the regular mesh and the torus
networks. However, the new architectures exhibit signi®cantly superior topological properties than the mesh and the torus. For example,
whereas a two-dimensional torus with N nodes has a diameter of Q (N 0.5), an MM network with the same number of nodes and links has a
diameter of Q (N 0.25). The IMM can be constructed for any number of nodes and is incrementally expandable. We provide results of our
simulation studies for the call blocking probabilities for the MM network. The advantage of the IMM over the de Bruijn network lies in the
fact that unlike the de Bruijn network, this network can be constructed for any number of nodes and is incrementally expandable. q 2001
Elsevier Science B.V. All rights reserved.
Keywords: Optical networks; Mesh; Torus; de Bruijn graph; Multi-mesh; Multihop networks

1. Introduction
Optical networks use interconnection of high-speed
broadband ®bers to transmit information. Typically, in
such a network, the stations (also referred to as the endnodes) are interconnected by optical ®bers and routers.
Wavelength division multiplexing (WDM) enables multiple
communication at different wavelengths over a single ®ber.
Each wavelength on a given ®ber is called a channel. These
networks support lightpaths, which are end-to-end communication paths passing through one or more ®bers, using one
WDM channel per ®ber. Optical networks can be divided
into two classes Ð single hop and multihop networks. An
important objective in a single hop network design is to
minimize the total number of carrier wavelengths needed
for communication among various end stations. An important requirement for high throughput in a multihop optical
network is to ensure that the average delay due to buffering
at the intermediate nodes is small. Simple interconnection
topologies, such as the ring and the mesh, and more complex
ones, such as the shuf¯enet and the de Bruijn graph have
been proposed for single and multihop networks [1,6,10,12].
Ideally, a topology should be incrementally scalable, should
* Corresponding author. Tel.: 11-480-965-6153; fax: 11-480-965-2751.
E-mail address: asen@asu.edu (A. Sen).

have a simple routing strategy and should be able to
communicate even in presence of node (router) and link
failures. In addition, the requirement for carrier wavelengths
(channels) should be low in a single hop network and the
average delay should be low in a multihop network. None of
the proposed topologies satisfy all these orthogonal, if not
con¯icting, requirements [8,10].
In a single hop network, a lightpath is a path from a
source S to a destination D that uses the same wavelength
for communication over the entire path. It may be noted that
this path may pass through several routers on its journey
from the source to the destination. If a lightpath from S to D
passes through k intermediate routers on its journey from the
source to the destination, then it has to reserve k 1 1 channels, one for each segment (®ber) in the path from S to D.
Since two different lightpaths cannot be assigned the same
WDM channel if they share the same ®ber [8], the demand
for the number of channels per ®ber will be higher if many
lightpaths have to share the same ®ber. As the length of the
source±destination path is bounded by the diameter
(distance between two nodes of the network that are furthest
from each other), one might think that the demand for the
number of channels per ®ber will be low, if the network
topology has a small diameter. However, this is not necessarily true. The reason for this is, even though the topology
may have a small diameter, because of the interconnection

0140-3664/01/$ - see front matter q 2001 Elsevier Science B.V. All rights reserved.
PII: S 0140-366 4(00)00359-5

A. Sen et al. / Computer Communications 24 (2001) 868±876

869

Fig. 1. MM network with 81 nodes (not all interblock links are shown for clarity).

pattern, a large number of lightpaths may be forced to travel
through a small number of ®bers, thereby increasing the
demand for the number of channels in those ®bers.
In a multihop network, the communication delay may be
proportional to the number of intermediate routers the packets have to pass through on their journey from the source to
the destination. As a consequence, in such an environment a
topology with a small diameter is quite attractive.
In this paper, we have proposed the use of two new architectures Ð multi-mesh (MM) and incomplete multi-mesh
(IMM) for lightwave application. The MM architecture
was originally proposed for parallel processing systems
[2±5]. The MM structure is de®ned for n 4 nodes for any
integer n. Since in a LAN/MAN/WAN environment, it is
dif®cult to satisfy such a stringent requirement on the
number of nodes, we have generalized this architecture so
that it can be de®ned for any number of nodes N, 1 # N #
n 4 for some integer n. The new architecture, IMM, is also
incrementally expandable, i.e. an N node network can be
extended to an N 1 1 node network without major changes
to the existing structure. It may be noted that although the
authors in Ref. [13] refer to the network discussed in their
paper as the MM network, in the research community this
network is known as the Manhattan Street Network [7]. The
MM network in our paper refers to the network proposed in
Refs. [2±5].
The architecture proposed in this paper can be used both
as a physical as well as a logical topology for optical
networks. In terms of simplicity of interconnection and routing, the architecture is comparable to the regular mesh and
the torus. However, it exhibits signi®cantly superior

topological properties than the mesh and the torus. For
example, whereas a two-dimensional torus with N nodes
has a diameter of Q (N 0.5), an MM network with the same
number of nodes and links has a diameter of Q (N 0.25).
The advantage of an IMM over a de Bruijn network lies in
the fact that unlike the de Bruijn network, this network can
be constructed for any number of nodes and is incrementally
expandable.
The paper is organized as follows. Section 2 describes the
MM and IMM architectures. Sections 3 and 4 establish the
connectivity and diameter of the architectures, respectively.
Section 5 describes the simulation environment and simulation result, and Section 6 concludes the paper.
2. The multi-mesh and incomplete multi-mesh
architectures
The MM interconnection network was originally
proposed for parallel processing in Ref. [2]. Since then
researchers have developed techniques for ef®ciently carrying out several different computational tasks on this network
[3,4]. The interconnection pattern in this topology is an
extension of the simple mesh connection. In an n £ n
mesh, the processors are arranged in n rows and n columns.
The MM uses this as a building block for the construction of
the network. The idea is to use n 2 such blocks arranged
again in n rows and n columns. Thus an MM network has
exactly n 4 nodes. Each of these n 4 nodes are identi®ed with a
four tuple label (a , b , x, y). The ®rst two, (a , b ), identify
the block and the last two (x, y) identify the node within a

870

A. Sen et al. / Computer Communications 24 (2001) 868±876

Fig. 2. IMM network with 40 nodes.

block. Each of these co-ordinates can take a value between 1
and n (both inclusive).
The n £ n nodes within each block are connected as a
regular two-dimensional mesh. The interblock connections
are made using the following rules:
(1) ;b; 1 # b # n; the node (a , b , 1, y) is connected to
the node (y, b , n, a ) where 1 # y; a # n:
(2) ;a; 1 # a # n; the node (a , b , x, 1) is connected to
the node (a , x, b , n) where 1 # x; b # n:
It may be observed that the interconnection rules given
above generates a regular topology where each node is
connected to exactly four edges. A partially completed
MM network with 81 nodes is shown in Fig. 1.
The number of nodes in an N node MM network is always
n 4 for some integer n. This restriction may be acceptable in a
parallel processing environment where the number of nodes
do not change very often, but may not be acceptable in its
use in a local, metropolitan or wide area network environment where the number of nodes change quite frequently.
We propose a new variation of the MM interconnection
topology where we relax the MM topology requirement
that the number of nodes always be n 4. This variation of
MM, called incomplete multi-mesh (IMM) can be
constructed for any number of nodes. The construction
mechanism is simple. One needs to specify the total number

of nodes N and the block size n. Then the IMM will have
m  dN=n 2 e blocks. The blocks will be arranged in rows and
columns as in MM. There will be dm/ne rows, with each row
having at the most n blocks. The last row of blocks may be
partially complete with mod(m, n) blocks. A complete block
will have n £ n  n2 nodes. A partially complete block with
q nodes will have bq/nc complete rows with each row having
exactly n nodes. The last row within a block may be
partially complete with mod(q, n) nodes. An IMM with 40
nodes is shown in Fig. 2. The main advantages of the IMM
is that not only does it allow the construction of such a
network for any number of nodes, it is also incrementally
expandable in the sense that an N node network can easily
be augmented to an N 1 1 node network. This incremental
expansion possibility of IMM makes it very attractive for
use in a LAN/MAN/WAN environment where the number
of nodes undergo frequent changes.
3. The connectivity of multi-mesh and incomplete multimesh architectures
An MM network is regular graph with node degree four.
The node connectivity between a pair of nodes in a graph is
the number of node disjoint paths between these nodes. The
node connectivity of a graph is the minimum of the node
connectivity over all pairs of nodes. Since the degree of each

A. Sen et al. / Computer Communications 24 (2001) 868±876

871

S(a1,b1,x1,y1)

B(a1,b1)

B(a1,b3)

B(a3,b1)

B(a3,b2)

D(a2,b2,x2,y2)
B(a2,b3)

B(a2,b2)

Fig. 3. Disjoint paths between the nodes (a 1, b 1, x1, y1) and (a 2, b 2, x2, y2).

node in an MM is four, it is clear that the upper bound of
connectivity of MM is four. In this section, we show that in
an MM network this upper bound is indeed realized, i.e. the
connectivity of MM is four.
Theorem 1.

The connectivity of MM network is four.

Proof. We show that there exists four node disjoint paths
between the nodes (a 1, b 1, x1, y1) and (a 2, b 2, x2, y2) for any
value of (a 1, b 1, x1, y1) and (a 2, b 2, x2, y2). We consider four
cases:
Case
Case
Case
Case

1: a1
2: a1
3: a1
4: a1

± a2 and b1 ± b2 ;
 a2 and b1 ± b2 ;
± a2 and b1  b2 ;
 a2 and b1  b2 .

Case 1: (a1 ± a2 and b1 ± b2 : The four disjoint paths
between the nodes (a 1, b 1, x1, y1) and (a 2, b 2, x2, y2) are

(x1, y1)

(x2, y2)

Fig. 4. Disjoint paths between the nodes (ai ; bi ; x1 ; y1 ) and (ai ; bi ; x2 ; y2 ).

shown in Fig. 3 (;i, ai  ai and bi  bi in the ®gure). The
four node disjoint paths are as follows:
Path 1: (a 1, b 1, x1, y1) ! (a 1, b 1, x1, n) !
(a 1, b 1, b 3, n) ! (a 1, b 3, b 1, 1) !
(a 1, b 3, n, 1) ! (a 1, b 3, n, a 2) !
(a 2, b 3, 1, a 1) ! (a 2, b 3, 1, n) !
(a 2, b 3, b 2, n) ! (a 2, b 2, b 3, 1) !
(a 2, b 2, x2, 1) ! (a 2, b 2, x2, y2).
Path 2: (a 1, b 1, x1, y1) ! (a 1, b 1, x1, 1) !
(a 1, b 1, b 3, 1) ! (a 1, b 3, b 1, n) !
(a 1, b 3, 1, n) ! (a 1, b 3, 1, a 2) !
(a 2, b 3, n, a 1) ! (a 2, b 3, n, 1) !
(a 2, b 3, b 2, 1) ! (a 2, b 2, b 3, n) !
(a 2, b 2, x2, n) ! (a 2, b 2, x2, y2).
Path 3: (a 1, b 1, x1, y1) ! (a 1, b 1, n, y1) !
(a 1, b 1, n, a 3) ! (a 3, b 1, 1, a 1) !
(a 3, b 1, 1, n) ! (a 3, b 1, b 2, n) !
(a 3, b 2, b 1, 1) ! (a 3, b 2, n, 1) !
(a 3, b 2, n, a 2) ! (a 2, b 2, 1, a 3) !
(a 2, b 2, 1, y2) ! (a 2, b 2, x2, y2).
Path 4: (a 1, b 1, x1, y1) ! (a 1, b 1, 1, y1) !
(a 1, b 1, 1, a 3) ! (a 3, b 1, n, a 1) !
(a 3, b 1, n, 1) ! (a 3, b 1, b 2, 1) !
(a 3, b 2, b 1, n) ! (a 3, b 2, 1, n) !
(a 3, b 2, 1, a 2) ! (a 2, b 2, n, a 3) !
(a 2, b 2, n, y2) ! (a 2, b 2, x2, y2).
It can be seen from Fig. 3 that these paths are node disjoint.
Case 2: (a1  a2 and b1 ± b2 : This case is very similar

872

A. Sen et al. / Computer Communications 24 (2001) 868±876

Row
1

Col
1

Col
2

Col
q

Col
q+1

Col
n

Blocks with

Row
2

n X n nodes

Row
p-1

Partially full Block

Only the last Block may
be partially full
Fig. 5. Blocks in a generalized MM network.

to case 1. The two paths originating from the node (x1, y1) in
the block (a 1, b 1) ®rst reach their left and right boundary of
the block. Since a1  a2 ; the two blocks have a cycle
between them. The two paths leave the left and right boundary of the (a 1, b 1) block, through the links that form the
cycle, to enter the right and left boundary of the block (a 2,
b 2), respectively. From these boundary nodes, they reach
the destination node (x2, y2).
Two other paths from the source node (x1, y1) in the block
(a 1, b 1) ®rst reach their top and bottom boundary of the
block. The block (a 1, b 1) forms a vertical cycle with the
block (a 3, b 1), a1 ± a3 : The two paths leave the top and
bottom boundary of the (a 1, b 1) block, through the links that
form the cycle, to enter the bottom and top boundary of the
block (a 3, b 1), respectively. The block (a 3, b 1) forms a
horizontal cycle with the block (a 3, b 2). The two paths
leave the left and right boundary of the (a 3, b 1) block,
through the links that form the cycle, to enter the right
and left boundary of the block (a 3, b 2), respectively. The
block (a 3, b 2) forms a vertical cycle with the block (a 2, b 2).
The two paths leave the top and bottom boundary of the (a 3,
b 2) block, through the links that form the cycle, to enter the
bottom and top boundary of the block (a 2, b 2), respectively.
From these entry points in the block (a 2, b 2) they reach the
destination node (x2, y2). The four paths established this way
between the nodes (a 1, b 1, x1, y1) to (a 2, b 2, x2, y2), a1 
a2  are disjoint.
Case 3: (a 1 ± a 2 and b1  b2 : Since the MM network is
symmetric with respect to horizontal and vertical direction,
this case is identical to case 2 with a ninety degree rotation
of the network. Same arguments as in case 2 apply in this case.
Case 4: a1  a2 and b1  b2 : In this case, we need to
establish four disjoint paths between the nodes (a i, b i, x1, y1)

and (a i, b i, x2, y2) for 1 # i # n as shown in Fig. 4. Since
both the source and the destination nodes belong to the same
block, we will omit identifying the block. The four disjoint
paths are as follows:
Path
Path
Path
Path

1:
2:
3:
4:

(x1,
(x1,
(x1,
(x1,

y1) ! (x1, y2) ! (x2, y2);
y1) ! (x2, y1) ! (x2, y2);
y1) ! (x1, 1) ! (n, 1) ! (n, y2) ! (x2, y2);
y1) ! (1, y1) ! (1, n) ! (x2, n) ! (x2, y2).

In all the cases there exists four disjoint paths between
every pair of nodes. This proves the theorem. A
The connectivity of an IMM can be as low as one. In the
40 node IMM network shown in Fig. 2, it can be seen that
the node (2, 2, 2, 1) is attached to only one node (2, 2, 1, 1).
Since the degree of this node is one, the connectivity of this
graph cannot be any more than one.
4. The diameter of multi-mesh and incomplete multimesh architectures
The diameter of a complete MM network with N nodes,
where N  n4 nodes was shown to be 2n  2N 1=4 in Ref.
[2]. This property is very attractive because a comparable
network Ð a two-dimensional torus, with exactly same
number of nodes and edges will have a diameter N 1/2. In
the following we show that when the MM is incomplete, the
diameter increases by at most 2n 2 1. In other words, the
diameter of an IMM network with N nodes (1 # N # n 4)
and blocks of size n £ n is at most 4n.
At the risk of being repetitive, we state once again that, if

A. Sen et al. / Computer Communications 24 (2001) 868±876
Table 1
Comparison table of various network attributes
Network type

Node degree

Connectivity

Diameter

Torus
Multi-Mesh
de Bruijn

4
4
D,D21

4
4
D22

Q (N 0.5)
Q (N 0.25)
Q (log N)

the total number of nodes in an IMM is N and the block size
is n  dN 0:25 e; then the structure will have m  dN=n2 e
blocks. The blocks will be arranged in a two-dimensional
array of blocks as shown in Fig. 5. There will be p  dm=ne
rows, with each row having at most n blocks. The last row of
blocks may be partially complete with q  modm; n
blocks. A complete block will have n £ n  n2 nodes. A
partially complete block with r nodes will have br=nc
complete rows with each row having exactly n nodes. The
last row within a block may be partially complete with
mod(r, n) nodes. The nodes within a block are connected
as a regular two-dimensional mesh.
Theorem 2.
4n 2 1.

The diameter of an IMM network is at most

Proof. Consider any source node S  a1; b1; x1; y1 and
any destination node D  a2; b2; x2; y2: Without loss of
generality, we assume a1 # a2. The IMM topology is
constructed in such a way that only the last row can have
less than n blocks and the last block in the last row can have
less than n 2 nodes. We will refer to the block in which the
source (destination) node is present as the source (destination) block.
We consider the following two cases:
Case 1: Both the source and the destination block is
complete. The diameter of any complete block is n. The
routing from the block (a1, b1) to the block (a2, b2) can
be done in the following way: (a1, b1) ! (a1, b2) ! (a2,
b2). Since the (a1, b2) block can be reached from the (a1,
b1) block in one hop and the (a2, b2) block can be reached
from the (a1, b2) block in one hop, the length of a path from
any node in the block (a1, b1) to the block (a2, b2) is at most
n 1 1 1 n 1 1 1 n  3n 1 2:
Case 2: The destination block is incomplete. If both the
source and the destination is in the incomplete block, maximum distance between them can be at most 2n 2 3: This can
happen only when both the horizontal and the vertical loop
present in a complete block is missing. If the source and the
destination nodes are in different blocks, the routing from
the source block (a1, b1) to the destination block (a2, b2)
can be done in the following way: (a1, b1) ! (a1, b2)
! (a2, b2). Since the (a1, b2) block can be reached from
the (a1, b1) block in one hop and the (a2, b2) block can be
reached from the (a1, b2) block in one hop, the length of a
path from any node in the block (a1, b1) to the block (a2,
b2) is at most n 1 1 1 n 1 1 1 2n 2 3  4n 2 1: A

873

Note: In our extensive experimentation to determine the
diameter of an IMM network, we never found the diameter
to exceed 3.5n. We conjecture that the diameter of an IMM
is indeed 3.5n.
In Table 1, we present a comparison between various
parameters of torus, MM and de Bruijn networks.
5. Simulation environment and results
We have studied the route and wavelength assignment
(RWA) problem in a wavelength routed single hop network
with a physical topology based on the MM architecture. The
RWA problem is a complex and intractable problem and as
is done by other researchers [10] we have used an extensive
set of simulation experiments to study the RWA problem.
5.1. Simulation objectives
Given that each node of the network has a speci®ed
number of optical transmitters and receivers, m, the objective of the simulation is to ®nd the number of wavelengths
necessary to ensure that the call blocking probability p will
be below an acceptable threshold value. We use MonteCarlo simulation to determine the number of wavelengths
necessary to ensure a small call blocking probability. In this
simulation we have used a ®xed route to give us an upper
bound on the number of wavelengths needed for RWA.
5.2. Assumptions in simulation experiments
The following assumptions are made in our study:
(a) The traf®c between all source±destination pairs are
equally likely.
(b) Each source node has the capability of tuning its
transmitter to any one of the N wavelengths used in the
network.
(c) The statistical traf®c model given in Ref. [10] is
applicable to our network.
(d) The wavelength continuity constraint is satis®ed, so
that a light path traversing a number of ®bers always uses
the same wavelength on every ®ber in the path.
5.3. Performance metric
In our experiment, we are determining the number of
wavelengths necessary to ensure that the call blocking
probability p will be below an acceptable threshold
value. Our performance metric is the number of wavelengths we would need in a network of a given size. This
is reasonable because a network with a given number of
nodes that require fewer wavelengths is less expensive
compared to a network requiring more wavelengths,
which, in general, require more ®bers and hence more
optical hardware (e.g. ampli®ers and routers).

874

A. Sen et al. / Computer Communications 24 (2001) 868±876

Multimesh with 81 nodes
1
0

Blocking probability

-1
-2
-3
-4
-5
-6
-7
-8
-9
12

14

16

18

20

22

24

26

28

30

34

42

Number of wavelengths
Fig. 6. Call blocking probability versus number of wavelengths in 81 node MM.

Step 1: The simulation is initialized by setting the number
of blocked calls to 0 and the total number of attempted
connections to 0.
Step 2: The steps 3±8 were repeated one hundred times.
Step 3: The network is initialized by ensuring that no
edge is carrying any lightpath and all m transmitters

5.4. Simulation algorithm
The following steps were taken to observe the variation of
the call blocking probability p with the number of wavelengths used in the network N, when each node of the
network has exactly m transmitters and receivers.

Multimesh with 256 nodes
1
0

Blocking probability

-1
-2
-3
-4
-5
-6
-7
-8
-9
18

20

22

24

26

28

30

32

34

36

38

40

42

44

46

48

50

52

Number of wavelengths
Fig. 7. Call blocking probability versus number of wavelengths in 256 node MM.

54

56

58

60

A. Sen et al. / Computer Communications 24 (2001) 868±876

875

Multimesh with 625 nodes
1
0

Blocking probability

-1
-2
-3
-4
-5
-6
-7
-8
-9
-10
28

32

36

40

44

48

52

56

60

64

68

72

76

80

84

Number of wavelengths
Fig. 8. Call blocking probability versus number of wavelengths in 625 node MM.

and receivers of each node in the network are available
for use.
Step 4: While at least one node S has a transmitter free
and one node D has a receiver free (S ± D), repeat steps
5±8.
Step 5: Select at random, a pair (S, D) of nodes such that
(a) the node S has at least one of its m transmitters free
and
(b) the node D has at least one of its m receivers free
and
(c) S is distinct from D and
(d) either the node pair (S,D) was never selected earlier
or the last attempt for a connection from S to D was
successful.
Step 6: Examine the shortest path P from S to D (or one of
the shortest paths if there were more than one) to ®nd out
if there exists a wavelength li ; 0 # i , N such that none
of the edges in the path P currently has a lightpath with a
wavelength l i.
Step 7: If no such wavelength is available, we conclude
that the call is blocked. In this case we
(a) increase the number of blocked calls by 1,
(b) insert the pair (S, D) in a list of source and destination pairs of nodes that were blocked in previous
attempts and
(c) return to step 4.
Step 8: If a wavelength l i is found, we
(a) allocate the wavelength l i on each ®ber of the path
P for the communication from S to D,
(b) reduce the number of transmitters of the source
node S and the number of receivers of the destination
node D by 1 and
(c) return to step 4.

5.5. Simulation experiments, results and discussion
We carried out the experiment for three different values
of m, 5, 10 and 15, (m is the number of transmitters and
receivers in each node). In our Monte-Carlo simulation of
the MM network, we varied the number of wavelengths N
used in the network to observe the variation of the call
blocking probability p. At the completion of the simulation,
we calculate the relative frequencies of the blocked connections to obtain an estimate of the blocking probability when
the network has N available wavelengths for connection
establishment. This simulation had to be repeated many
times with increasing values of N so that the call blocking
probability p falls below an acceptable threshold value.
In Figs. 6±8, we plotted the blocking probability versus
the number of wavelengths using a logarithmic scale. In the
graphs, the squares, diamonds and crosses represent the
cases with m  5, 10, and 15, respectively. These graphs
may be used by a network designer to determine an appropriate value of N for a network with a speci®ed value of m
and acceptable limit of the call blocking probability p.
The graphs show a steep decline in the values of p as N is
increased. This implies that it is possible reduce the call
blocking probability to an arbitrarily small value by a
modest increase in the number of wavelengths. It is also
interesting to see that the number of wavelengths needed
is comparable to that used in more complex topologies such
as the de Bruijn or the Kautz graph. These graphs have been
studied for optical networks and are well known for their
low diameter and simple routing schemes. The results
related to the analytic and simulation studies of these
networks can be found in Refs. [9,12]. The call blocking
probability is the metric that is used by the researchers in

876

A. Sen et al. / Computer Communications 24 (2001) 868±876

this area to evaluate the ef®cacy of any particular topology
[9,12]. For this reason we also use the same metric to evaluate the effectiveness of the MM topology.
6. Conclusions
In this paper, we have proposed two new architectures for
wavelength division multiplexed optical networks. We
provide analytical results related to the topological properties
of the network. We prove that the MM network is optimally
fault-tolerant, in the sense that its connectivity is equal to its
node degree, which is four. We also prove that the diameter of
an IMM network is 4n 2 1: The diameter of a MM network
with n 4 nodes was shown to be 2n in Ref. [2]. Thus, in case of
an IMM, diameter increases at most by 2n 2 1. In our extensive experimentation, we never found an instance where the
diameter was greater than 3.5n. In addition to the analytical
results, we provide the results of our simulation studies. Simulation results indicate that these new topologies are comparable to the Torus, de Bruijn and Kautz graphs proposed for
WDM lightwave networks [9,12] and can be used as an alternative to these networks. In this paper we have studied the
situation when no wavelength conversion is allowed. The
ef®cacy of this architecture when wavelength conversion
is allowed, is currently under study.
Acknowledgements
The research of S. Bandyopadhyay was supported by a
research grant from the National Sciences and Engineering
Research Council of Canada.

References
[1] S. Chatterjee, S. Pawlowski, All optical networks, Communications
of the ACM 42 (6) (1999) 75±83.
[2] D. Das, B.P. Sinha, Multi-mesh Ð an ef®cient topology for parallel
processing, Proceedings of the Ninth International Parallel Processing
Symposium, Santa Barbara, CA, April 1995, pp. 17±21.
[3] M. De, B. Kundu, B.P. Sinha, Wormhole routing for complete
exchange in Multi-Mesh, Proceedings of the Fourth International
Conference on High Performance Computing, Bangalore, India,
December 1997.
[4] M. De, D. Das, M. Ghosh, B.P. Sinha, An ef®cient sorting algorithm
on the Multi-Mesh network, IEEE Transactions on Computers C46
(10) (1997) 1132±1137.
[5] D. Das, M. De, B.P. Sinha, A new network topology with multiple
meshes, IEEE Transactions on Computers C48 (5) (1999) 536±551.
[6] M.A. Marsan, A. Bianco, E. Leonardi, F. Neri, Topologies for wavelength-routing all-optical networks, IEEE/ACM Transactions on
Networking 1 (1993) 534±546.
[7] M.F. Maxemchuk, Comparison of de¯ection and store-and-forward
techniques in Manhattan Street and Shuf¯e-Exchange networks,
Proceedings of IEEE INFOCOM'89, 1989, pp. 800±809.
[8] B. Mukherjee, Optical Communication Networks, McGraw-Hill,
New York, 1997.
[9] G. Panchapakesan, A. Sengupta, On multihop optical network topology using Kautz digraph, Proceedings of IEEE INFOCOM'95, April
1995, pp. 675±682.
[10] R. Ramaswami, K. Sivarajan, Optical Networks: A Practical Perspective, Morgan Kaufmann, Los Altos, CA, 1998.
[12] K. Sivarajan, R. Ramaswami, Lightwave networks based on de Bruijn
graphs, IEEE/ACM Transactions on Networking 2 (1) (1994) 70±79.
[13] T.D. Todd, E.L. Hahne, Local and metropolitan Multi-Mesh
networks, Proceedings of IEEE International Communication Conference (ICC), 1992, pp. 900±904.

Computer Communications 29 (2006) 490–501
www.elsevier.com/locate/comcom

Relay node placement in large scale wireless sensor networks
Jian Tang*, Bin Hao, Arunabha Sen
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287, USA
Available online 19 February 2005

Abstract
Scalability and extended lifetime are two critical design goals of any large scale wireless sensor network. A two-tiered network model has
been proposed recently for this purpose. This is a cluster-based network model composed of relay and sensor nodes. Relay nodes are placed
in the playing field to serve as cluster heads and to form a connected network topology for information dissemination at the higher tier. The
relay nodes are capable of aggregating data packets from the sensor nodes in their clusters and transmitting them to the sink node via wireless
multi-hop paths. In this paper, we study the relay node placement problem in large scale wireless sensor networks. Our objective is to place
the fewest number of relay nodes in the playing field of a sensor network such that (1) each sensor node can communicate with at least one
relay node and (2) the network of relay nodes is connected.
However, placement strategies realizing goals (1) and (2) do not provide any fault-tolerance as the network may lose functionality after
failure of some of the relay nodes. In order to incorporate fault-tolerance in such a network, we ensure that every sensor node is able to
communicate with at least two relay nodes and the induced network topology is 2-connected. This strategy will ensure survivability of the
network in the event of single fault, in lieu of higher relay node placement cost. We formulate the relay node placement in wireless sensor
networks as two optimization problems: (i) Connected Relay Node Single Cover (CRNSC) problem and (ii) 2-Connected Relay Node Double
Cover (2CRNDC) problem. We present two polynomial time approximation algorithms to solve the CRNSC problem. We prove that the ratio
of the number of relay nodes needed by the approximation algorithm to the number of relay nodes needed by the optimal algorithm is
bounded by 8 for the first algorithm and 4.5 for the second. In addition, for the 2CRNDC problem we provide two approximation algorithms
with performance bounds 6 and 4.5, respectively.
q 2005 Elsevier B.V. All rights reserved.
Keywords: Relay node placement; Wireless sensor network; Fault-tolerance

1. Introduction
A wireless sensor network is composed of hundreds or
even thousands of sensor nodes which use wireless links to
perform distributed sensing tasks. Each sensor node
includes a sensing module, a computing module, memory
and a wireless communication module with a very limited
communication range. Wireless sensor network has
received intensive research attentions due to its enormous
application potential in battlefield surveillance, environmental monitoring, biomedical observation and other fields
[1]. The three basic requirements for designing efficient

* Corresponding author. Tel.: C1 480 9664572; fax: C1 4809652751.
E-mail addresses: jian.tang@asu.edu (J. Tang), binhao@asu.edu
(B. Hao), asen@asu.edu (A. Sen).
0140-3664/$ - see front matter q 2005 Elsevier B.V. All rights reserved.
doi:10.1016/j.comcom.2004.12.032

wireless sensor networks are scalability, fault-tolerance and
energy efficiency. A sensor network, comprising of a
number of sensor nodes, is usually required to cover a
large geographic area. New sensor nodes may be added to
the network and existing sensor nodes may become
inoperative at any time. This large scale and frequently
changing network requires scalable protocols and algorithms. Factors, such as energy depletion, harsh environmental conditions, and/or malicious attacks may result in
node failures in a wireless sensor network. Therefore,
survivability of sensor networks is a critical design goal.
Moreover, energy is one of the most precious resource in
wireless sensor networks. Sensor nodes are normally
powered by batteries and can only last for a fairly short
period of time if operated at high transmission power levels.
As a consequence, energy efficient design is needed for
prolonging network lifetime.

J. Tang et al. / Computer Communications 29 (2006) 490–501

Relay Node
Sensor Node

Fig. 1. A two-tiered wireless sensor network.

Information dissemination, i.e. sending out queries from
sinks to sensor nodes and gathering results from sensor
nodes back to sinks, is a basic function of wireless sensor
networks. Direct transmission from data sources to sinks is
usually not practical because sinks are generally far away
from data sources and sensor nodes have very limited
communication ranges. Hence, wireless multi-hop paths
need to be established to route data. However, sensor nodes
participating in data routing may consume a large volume of
energy due to transmission. In order to relieve this burden
from sensor nodes and to provide a scalable framework
for sensor networks, a two-tiered network model is proposed
[8,9,16]. Fig. 1 shows a two-tiered wireless sensor network,
where the radii of dotted circles (centered at relay nodes)
indicate the communication ranges of sensor nodes. From
the figure, we can see that sensor nodes are grouped in
clusters and relay nodes serve as cluster heads. It is well
known that cluster-based architectures are scalable. In the
two-tied network, the low tier consists of a large number of
sensor nodes whose task is to sense the vicinity, to generate
corresponding data packets and to transmit them directly to
their cluster heads. Relay nodes are on the high tier. They
are also called Gateway Nodes in [8] or Application Nodes
(AN) in [16]. A relay node is more powerful than a sensor
node in terms of energy storage, computing and communication capability. It can extract useful information and
remove redundancy in data packets gathered from sensor
nodes in its cluster. It can then generate outgoing packets
with much smaller total size and send them to sinks via
multi-hop paths composed of other relay nodes [16]. This
in-network data fusion can dramatically decrease the traffic
load thereby extend the network lifetime.
In this paper, we assume that sensor nodes are
uniformly deployed in a playing field. In order to make
the two-tiered sensor network work properly, a basic
requirement is to guarantee that each sensor node can reach
at least one relay node and that the relay nodes form a
connected network topology. A sensor node is covered by a
relay node if the relay node is within the communication
range of the sensor node. We first study the problem of
placing the minimum number of relay nodes in the playing
field such that all sensor nodes are covered and relay nodes

491

form a connected network topology. Relay nodes, just like
sensor nodes, may fail due to various reasons. Failures of
relay nodes may disconnect the network and prevent the
collected data from reaching the sink node. In order to
provide some measure of fault-tolerance in the network, it
is preferable that each sensor node is covered by at least
two relay nodes and there exist two node-disjoint paths
between each pair of relay nodes in the network. This will
ensure that in the event of a single relay node failure, the
sensor data can still be delivered to the sink node.
Moreover, this will also facilitate load balancing among
the relay nodes, thereby extend network lifetime. The idea
of covering each sensor node by multiple relay nodes was
first considered by [9] and it provides an opportunity for
load-balanced clustering. Our scheme provides faulttolerance for the sensor network in lieu of higher cost for
relay node placement.
We formulate the relay node placement in wireless
sensor networks as two optimization problems: (i) Connected Relay Node Single Cover (CRNSC) problem and (ii)
2-Connected Relay Node Double Cover (2CRNDC) problem. We present two polynomial time approximation
algorithms to solve the CRNSC problem. We prove that the
ratio of the number of relay nodes needed by the
approximation algorithm to the number of relay nodes
needed by the optimal algorithm is bounded by 8 for the first
algorithm and 4.5 for the second. In addition, for the
2CRNDC problem we provide two approximation algorithms with performance bounds 6 and 4.5, respectively. To
the best of our knowledge, this is the first paper to provide
constant bound approximation algorithms for the relay node
placement problems in the two-tiered large scale wireless
sensor networks. Once the positions of the relay nodes are
computed using the algorithms presented in this paper, the
clustering schemes described in [8,9] and the data gathering
protocols like LEACH [10] can be used.
The rest of the paper is organized as follows. We discuss
related work in Section 2. We formally define problems and
notations in Section 3. We describe our approximation
algorithms, their correctness proofs and approximation
bounds in Sections 4 and 5. We present and analyze our
simulation results in Section 6. We conclude the paper in
Section 7.

2. Related work
In recent times, several protocols and algorithms have
been developed for two-tiered wireless sensor networks.
The authors of [16] consider a two-tiered wireless sensor
network consisting of sensor clusters deployed around
strategic locations and Base Station (BS) whose locations
are relatively flexible. They propose approaches to maximize the network lifetime by arranging the location of BSs
and inter-Application Nodes (same as relay node in this
paper). It may be noticed that their work does not address

492

J. Tang et al. / Computer Communications 29 (2006) 490–501

the issue of covering sensor nodes by relay nodes, which is
one of the major goals of this research work. A fault-tolerant
clustering scheme is proposed in [8]. The objective of this
scheme is to detect failure and to recover sensor nodes from
the failed gateway node. In addition, authors of [9] propose
a clustering scheme whose objective is to balance the traffic
load among all gateway nodes. The optimization problems
considered in [8] are substantially different from the ones
considered here as they do not address the placement
problem of gateway/relay nodes.
The research that is closest to the results presented in this
paper is due to [3] by Cheng et al. They assume that the
sensor nodes are not connected initially and they study the
placement problem of relay nodes to make the induced
network topology globally connected. They formulate it as
an optimization problem called Steiner minimum tree with
minimum number of Steiner points and bounded edge
length. This problem was originally proposed by Lin and
Xue in [14]. They give two constant bound polynomial time
approximation algorithms to solve this problem. Those
approximation algorithms have performance ratios of 3 and
4, respectively. Their work is different from ours in two
respects. First, their research is not based on the two-tiered
network model. Second, the network survivability is not
addressed by their work.
There may exist some redundancy, in case sensor nodes
are densely packed in a geographic region (playing field). In
some of the schemes, the redundant sensor nodes are put to
sleep mode to save energy. Some recent work [6] analyzes
sensing redundancy among neighboring sensor nodes. A
coverage-based off-duty eligibility rule and node scheduling
scheme is proposed in [20]. This scheme guarantees that the
original sensing coverage is maintained after turning off
redundant nodes. The authors of [7] propose an approximation algorithm to compute a connected sensor cover
which includes minimum number of sensor nodes forming a
connected topology. Information dissemination is also a
fundamental issue in wireless sensor networks, and has been
extensively studied in the literature. The authors of [10]
present an energy-efficient cluster-based protocol, LEACH,
for gathering data packets from all sensor nodes and
delivering them to the Base Station (BS). In LEACH, only a
fraction of nodes become head nodes in every round. Data
reports from non-head sensor nodes are aggregated at the
head nodes and sent directly to BS. In [13], authors present
an improved scheme, called PEGASIS (Power-Efficient
Gathering in Sensor Information Systems), which constructs
a chain for data gathering. Nodes on the chain take turn to
transmit aggregated packets to the BS. Directed diffusion, a
flooding-based scheme, is presented in [12] for routing
queries from sinks to all sensor nodes and for gathering
result packets along the opposite direction. The sensor
network considered in [12] is single-tiered and comprises of
sensor nodes only (no relay nodes). The in-network data
aggregation and packet relaying are carried out by the
sensor nodes.

Since nodes in wireless sensor networks are vulnerable to
failure, fault-tolerant design of sensor networks is important. The traditional way for tolerating node/link failures is to
establish disjoint paths from the source node to the sink.
Suurballe in [19] proposes an optimal algorithm to compute
link disjoint paths with the minimum total cost in the
network. The broadcasting feature of radio makes the faulttolerant routing in the wireless network different from that
in the traditional networks. Srinivas and Modiano in [18]
propose elegant optimal algorithms for finding both nodedisjoint and link-disjoint paths with minimum total energy
in wireless networks.
The well known Minimum Geometric Disk Coverf
(MGDC) problem [5] is similar to problems under study
in this paper. The MGDC problem was proved to be NPhard in [5]. A polynomial time approximation scheme
capable of solving different kinds of geometric covering
problem is given in [11]. It may be noted that, optimization
problems studied in this paper are substantially different
from the ones in [5]. This paper considers the issues of (1)
connectivity and (2) double-cover, which are absent in [5].

3. Problem statements
In this section, we formally define the two problems
addressed in this paper, i.e. Connected Relay Node Single
Cover (CRNSC) problem, and 2-Connected Relay Node
Double Cover (2CRNDC) problem.
Consider a sensor network formed by a set of sensor
nodes which are uniformly distributed in a rectangular
region. We assume that all sensor nodes have the same
communication range r. The network model is two-tiered,
comprising of sensor and relay nodes. Each relay node has a
communication range, R. Normally, R is much larger than r.
In this paper, we assume that RR4r. This assumption is
consistent with the current communication ranges of sensor
nodes and 802.11-based wireless ad hoc nodes. A relay node
can communicate with any sensor node within a distance of
r, the communication range of the sensor nodes.
To accomplish the task of data gathering, an intuitive
objective may be to place the fewest number of relay nodes
so that each sensor node is covered by at least one relay
node. Since gathered data has to be transmitted to the sink
node, which may be far away from the data sources, we also
require that all relay nodes be able to communicate with
each other through a multi-hop path.
We call a set of relay nodes connected, if any pair of
them can communicate with each other through a multi-hop
path of relay nodes. Now, we are ready to define the
Connected Relay Node Single Cover problem.
Definition 1 [Connected Relay Node Single Cover
(CRNSC) Problem]. Given a set of locations of uniformly
distributed sensor nodes S, the communication range of
sensor nodes r, and the communication range of relay

J. Tang et al. / Computer Communications 29 (2006) 490–501

nodes R, find the minimum number of relay nodes and their
corresponding locations, so that each sensor node is covered
by at least one relay node, and that the set of relay nodes is
connected.
If the network of relay nodes are not required to be
connected, then the problem becomes the Relay Node
Single Cover (RNSC) problem.
We also study a fault-tolerant version of the above
problem. In particular, we study a relay node placement
problem which is resilient to single (relay) node failure. For
this purpose, we require that each sensor node to be covered
by at least two relay nodes and that the network induced by
relay nodes to be 2-connected [2]. The network of relay
nodes is 2-connected, if there exists at least two node
disjoint paths between every pair of relay nodes.
Definition 2 [2-Connected Relay Node Double Cover
(2CRNDC) Problem]. Given a set of locations of uniformly
distributed sensor nodes S, the communication range of
sensor nodes r, and the communication range of relay nodes
R, find the minimum number of relay nodes and their
corresponding locations so that each sensor node is covered
by at least two relay nodes, and that the network of relay
nodes is 2-connected.
Remark. We make the assumption that no two relay nodes
are placed at the same location. This is due to the fact that
failure of a relay node is often caused by some event in a
specific location. The same event may jeopardize two relays
if they are placed at the same location.
If we ignore connectivity of the network of relay nodes,
then 2CRNDC problem becomes the Relay Node Double
Cover (RNDC) problem.
3.1. Complexity of CRNSC and 2CRNDC problems
If we consider a special case of the Connected Relay
Node Single Cover (CRNSC) problem, where the relay node
communication range R is large enough, then the set of relay
nodes located at any points within a bounded region will
automatically be connected. In this case, the CRNSC
problem reduces to the RNSC Problem. In RNSC problem,
relay nodes must be placed in locations such that each
sensor node is able to reach at least one relay node. In other
words, if the communication range of the sensor node is r,
relay nodes must be placed in those locations such that if
disks of radius r are placed in those locations, they should
cover all the sensor nodes. The problem where one tries to
find the minimum number of disks of radius r, needed to
cover all the sensor nodes (or the points where they are
located), is known as the Minimum Geometric Disk Cover
problem [5]. It has been shown in [5], that the Minimum
Geometric Disk Cover problem is NP-complete. Since the
CRNSC problem is a special case of the RNSC problem, we
conclude that it is also NP-Complete. Unfortunately,
complexity result of the 2CRNDC problem is unknown at

493

this time. Since it appears to be a harder problem in
comparison with the CRNSC problem, we conjecture that
the 2CRNDC problem is also NP-Complete. In Sections 4
and 5, we present polynomial time approximation algorithms with provable performance bounds for the CRNSC
and the 2CRNDC problem, respectively.

4. Approximation algorithms for the CRNSC problem
In this section, we present two polynomial time
approximation algorithms for the CRNSC problem. We
first present an algorithm which is easy to understand. The
performance guarantee of this algorithm is 8. We then refine
the algorithm to get another polynomial time algorithm with
a performance guarantee of 4.5.
Before describing our algorithms, we give some
definitions and notations. We call a position p a Possibleposition denoted by P-position for relay nodes, if there exist
two sensor nodes s and s 0 such that distance(s,p)Z
distance(s 0 ,p)Zr, where r is the communication range of
sensor nodes. If a sensor node s is separated from other
sensor nodes by a distance of more than 2r, we take any two
distinct points on the circle of center s and radius r as
P-positions. One can easily see that corresponding to any
pair of sensor nodes with distance less than 2r, there are two
P-positions for the relay nodes. Similarly, corresponding to
any pair of sensor nodes with distance exactly 2r, there is
exactly one P-position for the relay node. For any
P-position, p, of relay nodes, denote by C(p) the set of
sensor nodes which can be covered by a relay node located
at position P. For a set H of P-positions, denote by C(H) the
set of sensor nodes which can be covered by some relay
node located at a position in H. Ignoring connectivity, we
claim that deploying relay nodes at all P-positions is
sufficient to cover the set of sensor nodes. This can be
verified as follows. Let H be any set of relay nodes, which
covers the set of sensor nodes. Suppose that some of the
relay nodes in H are not located at P-positions. For each
relay node q in H which is not located at a P-position, we
can find a P-position from where it can cover at least those
sensor nodes, which can be covered from its original
position. This is illustrated in Fig. 2. In the figure on the left
side, the relay node is not located at a P-position. In the
figure on the right side, the relay node has been moved to a
P-position, from where it is able to cover all those sensor
nodes it was covering before.
The main ideas of our algorithms are as follows:
(i) divide the region, within which the sensor nodes are
distributed, into small square boxes called cells; (ii) without
considering the connectivity, find the optimal solution to
cover (or double cover) the sensor nodes within each cell;
(iii) make the network of relay nodes connected (or 2connected), by adding in extra relay nodes, if necessary.
One might think that the running time of the algorithms will
be exponential, because in order to find the optimal solution

494

J. Tang et al. / Computer Communications 29 (2006) 490–501

C

C

B

B
A

A

(a)

Relay Node
Sensor Node

(b)

Fig. 2. (a) A relay node not located at a P-position covering sensor nodes A,
B and C; (b) a relay node located at a P-position covering A, B and C.

P-position outside cell

for each cell, one has to exhaustively search all the subsets
of P-positions. Fortunately, that is not the case, when the
size of a cell is small. For example, suppose the size of a cell
is 2r!2r, where r is the communication range of sensor
nodes. Then, one cell can always be covered by four disks of
radius r, which implies that we only need to search the
subsets of size less than or equal to 4. As a consequence, the
algorithms run in polynomial time.
The idea of dividing the region into cells, and finding
optimal solution for each cell was first introduced by
Hochbaum and Maass [11]. Let B be a rectangular region in
which a given set of points is placed. In order to cover those
points by disks of radius r, divide B into strips of width l$2r,
where l is an integer. Let A be an approximation algorithm
for any strip, whose performance ratio is rA. Here, the
Performance Ratio of A is defined as the ratio of the size of
the solution provided by A divided by the size of optimal
solution. Let SA be the algorithm for the whole region B,
which combines the solutions of A on every strip.
Lemma 1. Shifting Lemma [11]


1
rS A % rA 1 C
l
where rSA is the performance ratio of algorithm SA.
Clearly, inside each strip, we can divide it into square
boxes (cells) of side length l$2r, and apply Shifting Lemma
again. Let A 0 be an approximation algorithm for one cell,
with performance ratio rA 0 . Let SA 0 be the algorithm for the
whole region B, which combines the solutions of A 0 on
every cell. By applying the Shifting Lemma twice, we can
have the following corollary [11].
Corollary 1.


1 2
rS A 0 % rA 0 1 C
l
where rSA0 is the performance ratio of algorithm SA 0 .
For the rest of the paper, we call l the partition factor of
an algorithm.

P-position inside cell
P-position after shrink
Fig. 3. Shrink operation.

As one may notice, given a cell Bi, some of the Ppositions for sensor nodes inside Bi may not be in Bi. In
[11], the authors do not consider any connectivity issue, so
there is no need to require all P-positions to be inside the
cell. But, in our case in order to make relay nodes
connected (or 2-connected), we want the P-positions to be
inside the cell. Hence, we define a Shrink Operation as
follows (See Fig. 3): given a cell Bi and the set P of Ppositions of relay nodes for the sensor nodes inside Bi. For
all p2P, if p is outside Bi, replace p by a point q on the
border of Bi, such that q is the closest point from Bi to p. It
is not hard to see that by applying the shrink operation, the
new set of P-positions can cover at least the same set of
sensor nodes inside Bi.
Now, we are ready to present the algorithms. We first
give an approximation algorithm for CRNSC. We use K to
represent the number of times a sensor node needs to be
covered, e.g. KZ1 means single covering, and KZ2 means
double covering.
Algorithm 1.1. (CRNSC, lZ1)
Step 0. Divide the region into cells with side length DZ
2r, i.e. the partition factor lZ1. For each cell, find all Ppositions for relay nodes. Apply a shrink operation.
Step 1. Inside each cell, exhaustively search all 1,2,3,4subsets of the P-positions inside (or on) the cell, to find a
subset with smallest order which can cover all the sensor
nodes in the cell.
Step 2. For each cell Bi, let Hi be the set of relay nodes
found in Step 1 for Bi.
For all Hi, suppose HiC1 is the set to the right of Hi, and
HiCx is the set directly under Hi.
If either Hi and HiC1 are not connected or Hi and HiCx
are not connected, we add a relay node at the right bottom
corner of Bi. We add a relay node at the right top corner of
Bi, if Bi is in the bottom row (see Fig. 4).

J. Tang et al. / Computer Communications 29 (2006) 490–501

2r

2r

495

other. And by Step 2, relay nodes in different cells are
connected to one another.
Now, we show that the performance ratio of Algorithm
1.1 is bounded by 8. Let H 0 be the set of relay nodes found in
Step 1, i.e. H 0 ZgHi. Let OPT be the optimal solution of the
corresponding RNSC problem. By Corollary 1, we have
jH 0 j
% ð1 C 1=lÞ2 Z 4
jOPTj
Let H* be the solution provided by Algorithm 1.1. By our
assumption that each cell contains at least one sensor node,
H 0 has at least one relay node for each cell.
By Step 2, at most one extra relay node is added for each
cell. Hence,

Fig. 4. Possible locations to add extra relay nodes for Step 2 of Algorithm 1.1.

Repeat Step 2, until all Hi’s are connected.
Theorem 1. Algorithm 1.1 can always give a solution for
the CRNSC problem. And its performance ratio is bounded
by 8.

jH * j
%2
jH 0 j
Therefore,
jH * j
%8
jOPTj
,

Proof. In order to prove the correctness of Algorithm 1.1,
we need to prove the following:
I. All sensor nodes are covered by at least one relay node.
II. All relay nodes are connected to one another.
I. Every sensor node will be covered by at least one relay
node, if Step 1 of the algorithm ends properly.
If any 1, 2, 3, or 4-subset of the set of P-positions can
cover all the sensor nodes in a cell, then the subset will be
found by Step 1.
Suppose no subset with size no more than 4 is found by
Step 1 for some cell. We are going to derive a contradiction.
We know that each cell of side length 2r can be covered by
four disks of radius r, which means these four disks can
cover all the sensor nodes inside the cell. If all the four disks
are centered at P-positions, then the set of their centers is a
4-subset of the set of P-positions, and should be found by
Step 1, a contradiction. So, we assume some of the centers
are not at P-positions. For each disk C whose center is not at
a P-position, we can move the disk towards the sensor nodes
it covers, till there are two sensor nodes on the edge of the
disk. Now, the distances between the center of the disk and
the two sensor nodes are both r, i.e. the center of the disk is
at a P-position. So, by moving the disks around, we get a
4-subset of the set of P-positions which covers all sensor
nodes in the cell, a contradiction. Thus, we proved that after
Step 1, every sensor node is covered by at least one relay
node.
II. Clearly, since the side length of each cell is 2r, and
RR4r, any two relay nodes in a cell are connected to each

As one may notice that the partition factor l plays an
important role in the performance ratio of the algorithm. If
everything else is fixed, then the bigger l gets, the smaller
the performance ratio will be. So, next, we try lZ2.
Algorithm 1.2. (CRNSC, lZ2)
Step 0. Divide the region into cells with side length DZ
4r, i.e. lZ2. For each cell, find all P-positions for relay
nodes. Apply a shrink operation.
Step 1. Inside each cell, exhaustively search all 1 through
9-subsets of the P-positions inside (or on) the cell, to find a
subset with smallest order which can cover all the sensor
nodes in the cell.
Step 2. Try to connect the relay nodes found in Step 1.
Step 2.1. First connect the relay nodes inside each cell as
follows.
cHi, if it is not connected, add a relay node, q, at the
center of Bi. Set HiZHig{q}.
Step 2.2. Connect the Hi’s.
Let HiC1 be the set of relay nodes to the right of Hi, and
HiCx be the set directly under Hi.
If Hi and HiC1 are not connected, try to add a relay node,
qi, at the center of Bi, or a relay node, qiC1, at the center of
BiC1, or both, to make the new Hi and HiC1 connected.
Similarly, if Hi and HiCx are not connected, try to add a
relay node, qi, at the center of Bi, or a relay node, qiCx, at the
center of BiCx, or both, to make the new Hi and HiCx
connected.
Theorem 2. Algorithm 1.2 can always give a solution for
CRNSC problem. And its performance ratio is bounded
by 4.5.

496

J. Tang et al. / Computer Communications 29 (2006) 490–501

Proof. Similar to Algorithm 1.1, in order to show the
correctness we need to show the following:
I. All sensor nodes are covered by at least one relay node.
II. All relay nodes are connected to one another.

2r

2r

I. Based on the fact that each cell of side length 4r can be
covered by 9 disks of radius r [15], apply the same argument
for the correctness of Algorithm 1.1, we can have that every
sensor node is covered by at least one relay node.
II. By Step 2, all relay nodes are connected to one
another.
Now, we show the performance ratio of Algorithm 1.2 is
bounded by 9/2. Let H 0 be the set of relay nodes found in
Step 1, i.e. H 0 ZgHi. By Corollary 1, we have
jH 0 j
9
% ð1 C 1=lÞ2 Z
jOPTj
4
Let H* be the solution provided by Algorithm 1.2. By our
assumption that each cell contains at least one sensor node,
H 0 has at least one relay node for each cell. By Step 2, at
most one extra relay node is added for each cell. Hence,
jH * j
%2
jH 0 j
Therefore,
jH * j
9
%
jOPTj 2
,
Be aware of the fact that the optimal solution of CRNSC
problem is also a solution for the corresponding RNSC
problem (relay nodes are not necessarily connected). Hence,
the performance ratio of our approximation solution against
the optimal solution of the corresponding RNSC problem is
an upper bound of the actual performance ratio.

5. Approximation algorithms for the 2CRNDC problem
Now, we consider the 2-Connected Relay Node Double
Cover problem.
Remarks. As one may notice, both Shifting Lemma and
Corollary 1 are for single cover scenario. But it is not hard to
see, from the proof of the Shifting Lemma, that it does not
depend on the number of times the points being covered.
Hence, the Shifting Lemma can still be applied to the double
cover scenario, and so can Corollary 1.
Algorithm 2.1. (2CRNDC, lZ1)
Step 0. Divide the region into cells of side length DZ2r,
i.e. lZ1. For each cell, find all P-positions for relay nodes.
Apply a shrink operation.
Step 1. Inside each cell, exhaustively search all 1 through
8-subsets of the P-positions inside (or on) the cell, to find

Fig. 5. Possible locations to add extra relay nodes for Step 2 and 3 of
Algorithm 2.1.

a subset with smallest order, which can cover all sensor
nodes in the cell at least twice. For each cell Bi, let Hi be the
set of relay nodes found for Bi (note that each Hi has at least
two relay nodes).
Step 2. Clearly, since the side length is 2r, and RR4r, any
Hi containing more than two relay nodes will be 2connected. For all Hi’s containing exactly two relay
nodes, we put a relay node, q, at the right top corner of Bi,
and set HiZHig{qi}. Add q at the left top corner of Bi, if
there are no cells to the right of Bi (see Fig. 5).
Step 3. Now, we make the set of the chosen relay nodes to
be 2-connected (see Fig. 5).
Step 3.1. Connect each row of the Hi’s.
Let HiC1 be the set of relay nodes to the right of Hi. If Hi
and HiC1 are not connected, add a relay node, qi, at the right
top corner of Bi, and set HiZHig{qi}. Add qi at the right
bottom corner of Bi, if Bi and BiC1 are in the bottom row.
(Notice that we only make them to be connected here,
instead of 2-connected.)
Repeat step 3.1, until every row of Hi’s is connected.
Step 3.2. Connect each column of the Hi’s.
For each Hi, denote LEFTBi the set of relay nodes in Hi,
which are connected to some relay nodes in the set to the left
of Hi, i.e. LEFTBi Z fq 2Hi jq is connected to a node in the
set to the left of Hi}; denote RIGHTBi the set of relay nodes
in Hi, which are connected to some relay nodes in the set to
the right of Hi, i.e. RIGHTBi Z fq 2Hi jq is connected to a
node in the set to the left of Hi}.
If jLEFTBi g RIGHTBi jO 1, then H 0 iZHi. Otherwise,
0
Hi Z Hi  LEFTBi g RIGHTBi .
Let HiCx be the set of relay nodes directly under Hi. If H 0 i
and H 0 iCx are not connected, then add a relay node, q, at the
right bottom corner of Bi, and set HiZHig{q}.
Repeat Step 3.1, until every column of Hi’s is connected.
Theorem 3. Algorithm 2.1 can always give a solution for
the 2CRNDC problem. And its performance ratio is
bounded by 6.

J. Tang et al. / Computer Communications 29 (2006) 490–501

Proof. In order to show the correctness, we need to prove
the following:

497

q

I. All sensor nodes are covered by at least two relay nodes.
II. All relay nodes are 2-connected. Let u, v be two distinct
relay nodes.
I. Similar to the previous algorithms, this is based on the
fact that a cell of side length 2r can be covered twice by 8
disks of radius r. As shown in Fig. 6, all eight disks are
centered on the diagonals of the cell. Four of the centers are
r units away from the left top, right top, left bottom and right
bottom
pﬃﬃﬃcorners of the cell, respectively. And the other four
are ð 2=2Þr units away from the left top, right top, left
bottom and right bottom corners of the cell, respectively.
II. Let q, q 0 be two relay nodes. We need to show that
there are two node-disjoint q–q 0 paths. Consider the
following cases.
Case 1. q, q’are in the same cell. By Step 2, they are 2connected.
Case 2. q, q’are in different cells. Say q is in cell B, and q 0
is in cell B 0 . By the choice of the sets LEFT and RIGHT in
Step 3.2, and by the way the cells are vertically connected,
one can verify that no matter how B and B 0 are located, there
are always two node-disjoint paths from q to q 0 .
To save space, we only discuss one subcase to illustrate
how to connect q and q 0 . Suppose B is in ith row and jth
column, and B 0 is in i 0 th row and j 0 th column; and i!i 0 , j!
j 0 , i.e. B 0 is to the right of B and is blow B. Also suppose B
use the same relay node to connect to its right cell and to the
cell underneath. Then, by Step 3.2, we know B must have a
different relay node connect to its left cell (for otherwise,
jLEFTBgRIGHTBjZ1). So, we can have one path going
from q to the right cell of B, going horizontally then going
down to q 0 , and the other path going to the left cell of B, then
turning down to the i 0 -row, and then going horizontally to q 0 .
See Fig. 7.
Therefore, the set of relay nodes is 2-connected.
Now, we show the performance ratio of Algorithm 2.1 is
6. Let H 0 be the set of relay nodes found in Step 1, i.e. H 0 Z
gHi be the optimal solution of the corresponding RNDC

Fig. 6. Cover 2r!2r cell by eight disks.

q'

Fig. 7. Two node-disjoint paths between two relay nodes in different cells.

problem. By Corollary 1, we have
jH 0 j
% ð1 C 1=lÞ2 Z 4
jOPTj
Let H* solution provided by Algorithm 2.1. By our
assumption that each cell contains at least one sensor node,
H 0 has at least two relay nodes for each cell. Suppose we
have x rows and y columns of cells, then jH 0 jR2xy By Step
2, number of extra relay nodes added is at most (xC1)(yK
1). WLOG, we assume xRy, for otherwise, we can switch
rows and columns in the algorithm. Hence,
jH * j 3
%
jH 0 j 2
Therefore,
jH  j
%6
jOPTj
,
Algorithm 2.2. (2CRNDC, lZ2)
Step 0. Divide the region into cells of side length DZ4r,
i.e. lZ2. For each cell, find all P-positions for relay nodes.
Apply a shrink operation.
Step 1. Inside each cell, exhaustively search all 1 through
21-subsets of the P-positions inside (or on) the cell, to find a
subset with smallest order, which can cover all the sensor
nodes in the cell at least twice.
For each cell Bi, let Hi be the set of relay nodes found for
Bi (note that each Hi has at least two relay nodes).
Step 2. Inside each cell Bi, if the relay nodes in Hi are
connected but not 2-connected, add a relay node on the
horizontal mid-line of Bi, 4r units away from the left-top
corner of Bi. If they are not connected, then add one more
relay node on the vertical mid-line of Bi, 4r units away from
the left-top corner of Bi (see Fig. 8).
Step 3. Now, we make the set of the chosen relay nodes to
be 2-connected (see Fig. 8).
Step 3.1. Connect each row of the Hi’s.
Let HiC1 be the set of relay nodes to the right of Hi.
If Hi and HiC1 are not connected, add a relay node, qi,
on the horizontal mid-line of Bi, 4r units away from the
left-top corner of Bi, and set HiZHig{qi}. If they are
still not connected, add a relay node, qiC1, on the

498

J. Tang et al. / Computer Communications 29 (2006) 490–501

4r

4r
4r

4r

nine disks. We can get another nine disks by shifting the
centers of these nine original disks to the right for a small
amount, say r/100. Then, most part of the cell can
be covered twice by these 18 disks, except a strip of width
r/100 at the left end of the cell.
We use another three disks to cover this strip. Thus, the
21 disks cover the cell twice.
II. Similar to the argument for the correctness of
Algorithm 2.1, the set of relay nodes is 2-connected.
Now, we show the performance ratio of Algorithm 2.2 is
bounded 4.5. Let H 0 be the set of relay nodes found in Step
1, i.e. H 0 ZgHi. By Corollary 1, we have
jH 0 j
9
% ð1 C 1=lÞ2 Z
jOPTj
4

Fig. 8. Possible locations to add extra relay nodes for Steps 2 and 3 of
Algorithm 2.2.

horizontal mid-line of BiC1, 4r units away from the lefttop corner of BiC1, and set HiC1ZHiC1g{qiC1} (notice
that we only make them to be connected here, instead of
2-connected).
Repeat Step 3.1, until every row of Hi’s is connected.
Step 3.2. Connect each column of the Hi’s.
For each Hi, denote LEFTBi the set of relay nodes in Hi,
which are connected to some relay nodes in the set to the left
of Hi, i.e. LEFTBi Z fq 2Hi jq is connected to a node in the
set to the left of Hi}; denote RIGHTBi the set of relay nodes
in Hi, which are connected to some relay nodes in the set to
the right of Hi, i.e. RIGHTBi Z fq 2Hi jq is connected to a
node in the set to the left of Hi}.
If jLEFTBi g RIGHTBi jO 1, then H 0 iZHi. Otherwise,
0
Hi Z Hi K LEFTBi g RIGHTBi .
Let HiCx be the set of relay nodes directly under Hi. If H 0 i
and H 0 iCx are not connected, then add a relay node, q, on the
vertical mid-line of Bi, 4r units away from the left-top
corner of Bi, and set HiZHig{q}. If they are still not
connected, add a relay node, q 0 , on the vertical mid-line of
BiCx, 4r units away from the left-top corner of BiCx, and set
HiCxZHiCxg{q 0 }.
Repeat Step 3.1, until every column of Hi’s is connected.
Theorem 4. Algorithm 2.2 can always give a solution for
the 2CRNDC problem. And its performance ratio is
bounded by 4.5.

Let H* be the solution provided by Algorithm 2.2. By our
assumption that each cell contains at least one sensor node,
H 0 has at least two relay nodes for each cell. And by the
algorithm, we add at most two extra relay nodes for each
cell. So,
jH * j
%2
jH 0 j
Therefore,
jH * j
9
%
jOPTj 2
,
Similarly, the optimal solution of 2CRNDC problem is
also a solution for the corresponding RNDC problem.
Therefore, the performance ratio of our approximation
solution against the optimal solution of the corresponding
RNDC problem is an upper bound of the actual performance
ratio.
Remarks. As mentioned above, the partition factor l plays
an important role in the performance ratio of the algorithm.
One may think to make l as big as possible to improve the
performance ratio. We would like to point out two major
tradeoffs for large values of l. First of all, each algorithm
needs to perform an exhaustive search for every cell of
sidelength l$2r. The greater l is, the more sensor nodes there
will be inside each cell. And this means much longer
execution time for the exhaustive search. Another tradeoff
of larger l is that when the size of cells getting larger, more
extra relay nodes need to be put in to guarantee the
connectivity (or 2-connectivity), which will increase the
performance ratio.

Proof. The following needs to be proved for the correctness:
I. All sensor nodes are covered by at least two relay nodes.
II. All relay nodes are 2-connected.

6. Performance evaluations

I. Similar to the previous cases, we only need to show
that a cell with side-length 4r can be covered twice by 21
disks of radius r. By [15], the cell can be covered once by

In this section, we evaluate the performance of our
algorithm via simulations. In all simulation scenarios, we
use the relay cover size as the performance metric because

J. Tang et al. / Computer Communications 29 (2006) 490–501

K=1 l=1
K=1 l=2
K=2 l=1
K=2 l=2

140
130

Cover Size

120
110
100
90
80
70
60
50
600

800

1000

1200

1400

N
Fig. 9. Relay node cover size computed by different algorithms.

our objective is to minimize the network cost, i.e. the
number of relay nodes in the cover, under the constraints
that the network can work properly. We consider networks
in which sensor nodes are uniformly distributed in a squareshaped playing field with the size of 480!480 m2. In
addition, we assume that all relay nodes’ communication
ranges are 200 m, because that it is possible that the relay
node adopts the same 802.11-based wireless communication
system as what is used by the general wireless ad hoc node
and the typical communication range for such kind of node
is between 200 and 250 m.
In the first simulation, we fix the communication range of
each sensor node to be 40 meters since practically this value
for the sensor node is between 20 and 40 [12,17].We adjust
the network density by changing the number of nodes in the
network. Simulations are run on networks with 600, 800,
1200 and 1400 sensor nodes, respectively. Fig. 9 shows the
results.
In the figure, l is the partition factor and K stands for the
times that each sensor node is covered. It is easy to
understand that usually the relay node cover size increases
with the increase of the number of sensor nodes. But, we can
see that if the network is dense enough, this kind of increase
becomes not very substantial. We can imagine that currently
we have 1200 sensor nodes in the network and then we add
200 more sensor nodes into the network. It is not necessary
to add too many new relay nodes for covering new added
sensor nodes since most of them will be close enough to old
sensor nodes and can be covered by the existing relay nodes.
The most important observation about this figure is that
when l is changed from 1 to 2, the cover size decreases
dramatically. For example, for the double cover case of
network with 1000 nodes, only 109 relay nodes are needed
while the cover size is 140 if lZ1, which involves a 22%
improvement. Actually it has been shown that in the worst
case, the performance ratio of our approximation algorithm
for 2CRNDC problem with lZ2 is 4.5, while the one with

lZ1 is 6. Simulation results show that in the average case,
the approximation algorithm with lZ2 is also much better
than the one with lZ1. Section 4 explains the corresponding
reasons. However, increasing the partition factor will
increase the time complexity of the algorithm since
exhaustive search in one cell will become much longer.
During the simulation, we find out that in the densely
distributed large scale network, it will take a very long time
to get the optimal solution for a cell if l is greater than 2. So
we only consider cases with l no more than 2. In addition,
we can see that the size for the double cover is always about
twice as large as that for the single cover. So the gain for
bringing survivability to the network is comparable to the
extra cost needed to be paid.
In the second scenario, we still run the simulation on
networks consisting of 600, 800, 1000, 1200 and 1400
sensor nodes. But the sensor’s communication range is set to
be different values, that is, 24, 30 and 40 m, respectively.
Since through experiments in scenario one, we find out that
the efficiency can be achieved by choosing bigger partition
factor l. Considering the tradeoff between time complexity
and performance ratio, lZ2 should be the best choice. So in
the following cases, we only consider algorithms with lZ2.
Actually, both the number and the communication range of
sensor nodes are important factors influencing the size of
relay node cover since the number of sensor nodes covered
by a specific relay node will become less if the sensor nodes’
communication range become smaller. According to
Fig. 10, for the network having 1000 nodes, comparing to
the double cover size of 109 with the communication range
equal to 40 m, the double cover size is 252 if the
communication range is 24 m, which is more than twice
of the former one. Moreover, if the network has 600 sensor
nodes and their communication ranges are 40 m, only 96
relay nodes are needed to cover each sensor node at least
twice. However, we need 268 relay nodes to double-cover
300

K=1 SR=24
K=1 SR=30
K=1 SR=40
K=2 SR=24
K=2 SR=30
K=2 SR=40

250

Cover Size

150

499

200

150

100

50
600

800

1000

1200

1400

N
Fig. 10. The influence of sensor communication range (SR) on relay node
cover size.

500

J. Tang et al. / Computer Communications 29 (2006) 490–501

Table 1
Performance ratios of our algorithms
Cases
KZ1

KZ2

Our Alg
Optimal
Ratio
Our Alg
Optimal
Ratio

(500, 24)

(400, 30)

(400, 40)

105
84
1.250
209
166
1.259

67
57
1.75
132
113
1.168

48
39
1.231
94
75
1.253

1200 sensor nodes if their communication ranges are 24 m.
So, we can conclude that the influence of the communication range to the cover size is more substantial than the
change of the number of sensor nodes. Another interesting
observation is that for a specific network, no matter what the
communication range of the sensor node is, the double cover
size is also always one time larger than that of the single
cover.
Comparing proposed algorithms against optimal solutions should be the best way for evaluating their
efficiency. But it may be noticed that it is not even
possible to give an ILP formulation for our CRNSC and
2CRNDC problems, because it is impossible to identify
finite number of possible positions for relay nodes.
However, by removing connectivity constrains, we can
formulate RNSC and RNDC problems by Integer Linear
Programming (ILP). In the simulations, we use CPLEX to
compute the optimal solutions for those formulations and
employ them as lower bounds on the optimal solutions of
the CRNSC and 2CRNDC problems since the size of
optimal solutions for RNSC and RNDC problems must be
less than those for the corresponding CRNSC and
2CRNDC problems. When the network is dense enough,
it takes extremely long time for CPLEX to figure out the
optimal solution. Hence, we can only work on relatively
small cases in this scenario. The following table shows the
performance ratios of our approximation algorithms
(K has the same meaning as that in two previous
scenarios). We consider three different instances. Firstly,
we generate a network with 500 uniformly distributed
nodes whose communication range is set to be 24 m. Then
we uniformly generate two networks, both of which have
400 nodes. But, communication ranges are 30 and 40,
respectively. We apply our approximation algorithms with
partition factor equal to 2 to all of instances and then
observe how close the sizes of our solutions could be to
the optimal ones (Table 1).
Based on our theoretic analysis about performance
ratios in the last section, our approximation algorithms
with partition factor lZ2 for both problems achieve a
performance ratio of 4.5 in the worst cases. From the
table, we can see that on the average cases, performance
ratios of our algorithms are much better than the bound,
4.5, no matter for the single or double covering. They are
only 1.26 or even less under different network instances.

So, the sizes of solutions given by our approximation
algorithms are fairly close to that of optimal solutions in
average cases.

7. Conclusions
In this paper, we have formulated two optimization
problems for relay node placement in large scale sensor
networks, one is called Connected Relay Node Single Cover
(CRNSC) problem and another is 2-Connected Relay Node
Double Cover (2CRNDC) problem. Two polynomial time
approximation algorithms are presented to solve the
CRNSC problem. We show that the size of the CRNSC
given by the first approximation algorithm is bounded by
eight times that of the optimal solution and the second one
achieves a performance ratio of 4.5. Moreover, we propose
two approximation algorithms to solve the 2CRNDC
problem and we also prove that performance ratios
associated with them are 6 and 4.5, respectively. In
simulations, we find out the approximation algorithms
with partition factor l equal to 2 tradeoff the time complexity
and efficiency well. The cover sizes given by them are much
smaller than algorithms with lZ1 on any specific instance.
We also study the influence of the network size and the
communication range of the sensor node to the cover size
given by our algorithms. In addition, our simulation results
show that sizes of solutions provided by our approximation
algorithms are very close to that of optimal solutions under
different instances.
In the future, We will extend our work to the more
general K-Connected Relay Node K Cover (KCRNKC)
problem, i.e. covering each sensor node at least K
times and the network composed of relay nodes forms a
K-connected topology, where K can be any positive
integer.

Acknowledgements
Jian Tang’s research was supported in part by ARO grant
W911NF-04-1-0385 and NSF grant CCF-0431167.

References
[1] I.F. Akyildiz, W. Su, Y. Sankarasubramaniam, E. Cayirci, Wireless
sensor networks: a survey, Computer Networks Journal 38 (2002)
393–422.
[2] J.A. Bondy, U.S.R. Murthy, Graph Theory with Applications, North
Holland, New York, 1976.
[3] X. Cheng, D.Z. Du, L. Wang, B. Xu, Relay sensor placement in wireless
sensor networks, ACM Wireless networks submitted for publication.
Also available at http://citeseer.nj.nec.com/cheng01relay.html
[5] R.J. Fowler, M.S. Paterson, S.L. Tanimoto, Optimal packing and
covering in the plane are NP-complete, Information Processing Letter
12 (1981) 133–137.

J. Tang et al. / Computer Communications 29 (2006) 490–501
[6] Y. Gao, K. Wu, F. Li, Analysis on the redundancy of wireless sensor
networks, Proceedings of ACM WSNA’2003.
[7] H. Gupta, S.R. Das, Q. Gu, Connected sensor cover: self-organization
of sensor networks for efficient query execution, Proceedings of ACM
MOBIHOC’2003, pp. 189–200.
[8] G. Gupta, M. Younis, Fault-tolerant clustering of wireless sensor
networks, Proceedings of IEEE WCNC’2003, pp. 1579–1584.
[9] G. Gupta, M. Younis, Load-balanced clustering of wireless sensor
networks, Proceedings of IEEE ICC’2003, pp. 1848–1852.
[10] W. Heinzelman, A. Chandrakasan, H. Balakrishnan, Energy efficient
communication protocols for wireless microsensor networks,
Proceedings of HICSS’2000, pp. 3005–3014.
[11] D.S. Hochbaum, W. Maass, Approximation schemes for covering and
packing problems in image processing and VLSI, Journal of ACM 32
(1985) 130–136.
[12] C. Intanagonwiwat, R. Govindan, D. Estrin, Directed diffusion: a
scalable and robust communication paradigm for sensor networks,
Proceedings of ACM MOBICOM’2000, pp. 56–67.
[13] S. Lindsey, C. Raghavendra, K.M. Sivalingam, Data gathering
algorithm in sensor networks using energy metrics, IEEE Transactions
on Parallel and Distributed System 13 (2002) 924–935.

501

[14] G. Lin, G. Xue, Steiner tree problem with minimum number of steiner
points and bounded edge-length, Information Processing Letters 69
(1999) 53–57.
[15] K.J. Nurmela, P.R.J. Ostergard, Covering a square with up to 30 equal
circles, Research Report A62, Helsinki University of Technology,
2000. Also available at http://www.tcs.hut.fi/Publications/info/bibdb.
HUT-TCS-A62.shtml
[16] J. Pan, Y.T. Hou, L. Cai, Y. Shi, S.X. Shen, Topology control for
wireless sensor networks, Proceedings of ACM MOBICOM’2003, pp.
286–299.
[17] C. Schurgers, V. Tsiatsis, S. Ganeriwal, M. Srivastava, Topology
management for sensor networks: exploiting latency and density,
Proceedings of ACM MOBIHOC’2002, pp. 135–145.
[18] A. Srinivas, E. Modiano, Minimum energy disjoint path routing in
wireless ad-hoc network, Proceedings of ACM MOBICOM’2003, pp.
122–133.
[19] J.W. Suurballe, Disjoint paths in a network, Networks 4 (1974)
125–145.
[20] D. Tian, N.D. Georganas, A coverage-preserving node scheduling
scheme for large wireless sensor networks, Proceedings of ACM
WSNA’2002, pp. 32–41.

Graph Clustering Using Distance-k Cliques
Software Demonstration
Jubin Edachery1 , Arunabha Sen1? , and Franz J. Brandenburg2
1

Department of Computer Science
Arizona State University
Tempe 85287, AZ, USA
{jubin,Arunabha.Sen}@asu.edu
2
Lehrstuhl für Informatik
Universität Passau
94030 Passau, Germany
brandenb@informatik.uni-passau.de

Abstract. Identifying the natural clusters of nodes in a graph and treating them as supernodes or metanodes for a higher level graph (or an
abstract graph) is a technique used for the reduction of visual complexity of graphs with a large number of nodes. In this paper we report
on the implementation of a clustering algorithm based on the idea of
distance-k cliques, a generalization of the idea of the cliques in graphs.
The performance of the clustering algorithm on some large graphs obtained from the archives of Bell Laboratories is presented.

1

Introduction

Visualization tools can be of tremendous help to network planners and administrators for management and control of large networks. For this purpose, a
hierarchical view of the network is most appropriate. This provides the network
administrators at different levels the ability to view their realm of the network
at an appropriate level of detail without being overwhelmed by unnecessary and
unimportant minutiae. This hierarchical view of the networks can be provided
in graphs by grouping the nodes into some supernodes or metanodes [3]. As a result, the higher level graph will have much fewer metanodes and thus the visual
complexity will be significantly reduced.
Identifying the natural clusters of nodes in a graph and treating them as
supernodes or metanodes for a higher level graph is a technique that can used
for the reduction of visual complexity of graphs with a large number of nodes.
An abstract graph is constructed in such a way that a node in the abstract
graph represents a set of nodes of the original graph and the edges represent the
relationship between these sets of nodes. Thus an abstract graph construction
?

Corresponding author, Telephone: 480-965-6153, Fax: 480-965-2751; This research in
part was supported by a grant from the NATO Scientific and Environmental Affairs
Division and a grant from Tom Sawyer Software under NIST Advanced Technology
Program.

J. Kratochvı́l (Ed.): GD’99, LNCS 1731, pp. 98–106, 1999.
c Springer-Verlag Berlin Heidelberg 1999


Graph Clustering Using Distance-k Cliques

99

problem reduces to the problem of partitioning the node set of the original graph
G = (V, E), into a subset of nodes V1 , V2 , ..., Vk , such that ∪ki=1 Vi = V and
Vi ∩ Vj = Ø for i 6= j.
The subsets Vi s (1 ≤ i ≤ k) are known as the clusters of the graph G = (V, E).
One problem with this approach is that there is no consensus among the researchers as to what constitutes a natural cluster. There is some intuitive understanding of what constitutes a cluster but there is no universally accepted formal
definition of a natural cluster. In case the nodes and edges of the graph have
some semantic information associated with them (in the form of labels), such
information can be used for the purpose of clustering (or grouping) the nodes.
An example of such information could be the IP addresses associated with the
nodes in a telecommunication network. In case the graph has no such information, then the structural properties of the graph have to be utilized for the
purpose of generating the clusters. Several candidates for the structures to be
used as clusters have been proposed in the literature. These include biconnected
components, paths and triangles, circles of cliques and others [1, 2, 5, 6].

2

Clustering Using Distance-k Cliques

In spite of the differences of opinion as to what constitutes a cluster, one idea
is universally accepted: the nodes belonging to a cluster must have a strong
relationship between them in comparison with the nodes outside the cluster.
Now, we are confronted with another question and that is how to measure the
strength of a relationship ? In this paper we measure the strength of a relationship
between two nodes in a graph in terms of the distance between the nodes. The
distance between two nodes in a graph is the length of the shortest path length
between the two nodes. If dist(u, v) gives the distance between the nodes u and
v, and if dist(p, q) > dist(r, s), then we say that the strength of the relationship
between p and q is weaker than the strength of the relationship between r and
s. In other words, the strength of a relationship between two nodes is inversely
proportional to the distance between them.
A subset V 0 of the node set V of a graph G = (V, E) is defined to be a
Distance-k Clique if every pair of nodes in V 0 is connected in G by a path of
length at most k. The standard graph theoretic term, clique, is a special case
of a distance-k clique with k = 1. It may be noted that a distance-k clique of a
graph G = (V, E) is a subgraph of G with diameter k.
In our clustering problem, we would like to partition the node set V of the
graph G = (V, E) into fewest number of distance-k cliques. The decision problem
version of our clustering problem can be stated as follows:
Partition into Distance-k Cliques Problem
INSTANCE: Given a graph G=(V, E) and positive integers j and k, 1 ≤ j, k ≤
|V |.
QUESTION: Can the vertices of G be partitioned into i ≤ j disjoint sets
V1 , V2 , . . . , Vi such that, for 1 ≤ m ≤ i,the subgraph induced by Vm is a distancek clique ?

100

J. Edachery, A. Sen, and F.J. Brandenburg

It is not difficult to realize that the Partition into Distance-k Cliques problem is NP-Complete because if k = 1, it reduces to the Partition into Cliques
problem, which is known to be NP-Complete [4].
As Partition into Distance-k Clique problem turns out to be an NP-Complete
problem, we look for heuristic solutions to the problem that will produce a
good solution rather than an optimal solution. In the following paragraph we
describe a class of such algorithms. All these algorithms are variations of one
main algorithm which is referred to as the algorithm A1.
2.1

Description of the Algorithms

The clusters are are referred to as C1 , C2 , . . . , Cp for some integer p. Every node
v of the graph G = (V, E) belongs to one cluster Ci , 1 ≤ i ≤ p. This information
is stored in an array called C[1, .., n]. If for a node v ∈ V , C[v] = j, it implies
that the node v is an element of the cluster Cj .
A node v of the graph G = (V, E) is known as an bridge node if v ∈ Ci , (v, u) ∈
E and u 6∈ Ci , where Ci represent the cluster i.
The neighbors of a node v ∈ V is defined as the nodes that are adjacent to v
in the graph G = (V, E)), i.e., N (v) = {u|(v, u) ∈ E}.
Every bridge node v ∈ V will have a cluster-list associated with it. The
cluster-list associated with node v, is the set of all the clusters where the neighbors of v, not in C[v], belong, i.e., CL(v) = {Ci |N (v) ∩ Ci 6= ∅ and i 6= C[v]}.
The estimated diameters of the clusters is an upper bound of the diameter of
the clusters. They are stored in an array called EstDia. EstDia(Ci ) gives the
estimated diameter of the cluster Ci .
We first describe the Algorithm A1 and then its variations, algorithms A2
through A5. In all the algorithms, the user specifies the value of k for the algorithms to create distance-k cliques.
Algorithm A1
As a first step, A1 constructs an initial clustering of the nodes. The algorithm
InitialCluster1 is used for this purpose. InitialCluster1 chooses high degree nodes
of the graph and forms clusters around those nodes. The set of initial clusters
called ClusSet and it contains the clusters C1 , C2 , . . . , Cp for some integer p.
This procedure also computes the estimated diameter of each cluster in ClusSet
and stores in array EstDia . The InitialCluster1 procedure also identifies the
bridge nodes at this stage of clustering and for each node v ∈ V determines the
cluster in which this node belongs. This information is stored in array C array,
A list of clusters is associated with each bridge node. The bridge node that
has the largest number of nodes in its cluster list is used to (tentatively) form a
larger cluster (ComClus), comprising of the cluster in which the bridge node belongs and all the clusters in its cluster list. If the estimated diameter of the new
cluster is less than or equal to k, then the new cluster is made permanent and
the cluster list of all the affected bridge nodes are updated. In case the diameter
of the new cluster is greater than k, then such a cluster cannot be formed. In this

Graph Clustering Using Distance-k Cliques

101

case, one cluster, (RemClus), is removed from the tentative cluster (ComClus)
so as to reduce the overall diameter and the cluster list of all the affected bridge
nodes are updated. The whole process is repeated till the cluster list associated
with each bridge node (CL(i)) becomes empty.
Algorithm InitialCluster1 (V, E)
begin
V 0 = V ; P = 0;
while(V 0 6= ∅)
begin
P = P + 1;
m = the highest degree node in V 0 ;
Cluster(P ) = {m};
V 0 = V 0 − {m}
∀(m, n) ∈ E do
begin
if n ∈ V 0 then;
begin
Cluster(P ) = Cluster(P ) ∪ {n}
V 0 = V 0 − {n};
end
end
end
end
Algorithm 1: Generation of Clusters using Distance-k Cliques
begin
(S1:) Form IntialClusters; (Suppose at this stage there are
q bridge nodes, u1 , u2 , . . . , uq .)
for i = 1 to q ;
begin
CL(i) = {C
Pj |N (ui ) ∈ Cj and j 6= C[ui ]};
|CL(i)| = Cj ∈CL(i) |Cj |;
(|Cj | gives the number of nodes in the cluster Cj )
end
while(true);
begin
If ∀i, 1 ≤ i ≤ q, CL(i) = ∅ then EXIT;
(S2:) Find um such that the |CL(m)| is the largest among
all CL(i), 1 ≤ i ≤ q;
Clist = CL(m) ∪ {C(um )};
Create a new cluster ComClus
and set ComClus = {v|v ∈ Ci and Ci ∈ Clist};
Find the clusters with the largest and second largest estimated
diameter in the Clist and call them LC1 and LC2 ;

102

J. Edachery, A. Sen, and F.J. Brandenburg

end

(S3:) EstDia(ComClus) = EstDia(LC1 ) + EstDia(LC2 ) + d,
where d = 1
if LC1 = C(um ) or LC2 = C(um ), otherwise d = 2;
if EstDia(ComClus) ≤ K then
begin
ClusSet = ClusSet ∪ {ComClus} − {Ci |Ci ∈ Clist};
∀v ∈ ComClus set C[v] = ComClus;
for i = 1 to q;
begin
∀Cx ∈ Clist if Cx ∈ CL(i)
then remove Cx from CL(i);
If at least one Cx is removed from CL(i) and
if EstDia(ComClus) < K
then CL(i) = CL(i) ∪ ComClus;
end
if EstDia(ComClus) = K then
∀i such that ui ∈ ComClus set CL(i) = ∅ ;
end
else
begin
if (LC1 6= C(um )then set RemClus = LC1
else set RemClus = LC2 ;
if LC1 = C(um ) or LC2 = C(um )then
begin
∀ui ∈ ComClus
set CL(i) = CL(i) − RemClus;
∀ui ∈ RemClus
set CL(i) = CL(i) − ComClus;
end
else
CL(m) = CL(m) − RemClus;
end
end

Algorithm A2
This is essentially same as algorithm A1, except that the step marked S2 in
A1, should be replaced by the following step:
(S2:) Find um such that the |CL(m)| is the smallest among all CL(i), 1 ≤
i ≤ q;
This step allows smaller clusters to grow in parallel, as opposed to the algorithm A1 where one cluster grows to its maximum size before another cluster
gets an opportunity for growth in size.

Graph Clustering Using Distance-k Cliques

103

Algorithm A3
This again is essentially same as algorithm A2, except that in step marked
S1, the procedure InitialCluster2 should be used for initial clustering instead of
the procedure InitialCluster1. In InitialCluster2, each node forms its own cluster
and that is taken as the starting point of the algorithm A3. It may be noted
that in this case every node v ∈ V is a bridge node if degree(v) > 0.
Algorithm InitialCluster2 (V, E), (V = {v1 , . . . , vn })
begin
for i = 1 to n do
Cluster(i) = {vi };
end
Algorithm A4
This again is essentially same as algorithm A2, except that in step marked
S3, instead of estimated diameter of ComClus, the exact diameter of ComClus
is computed. This modification gives a higher quality solution at an increased
computational cost.
Algorithm A5
This again is essentially same as algorithm A3, except as in algorithm A4,
instead of estimated diameter of ComClus, the exact diameter of ComClus is
computed. Just like in A4, this modification gives a higher quality solution at
an increased computational cost.
2.2

Analysis of Algorithms

We analyze the algorithm A1. The procedure InitialCluster1 can be carried out
in O(n2 ) time where n is the number of nodes in the graph. The procedure
InitialCluster2 can be carried out in O(n) time. The first for-loop can be carried
out in O(n2 ) time. The while-loop continues till the cluster list associated with
each bridge node becomes empty. A bridge node may become a non-bridge node
during execution of the algorithm. However, a non-bridge node will never become
a bridge node. The number of bridge nodes can be at most n. Inside the whileloop all steps upto and including step marked S3 can be carried out in O(n). The
If-statement following S3 is either true or false. If it is true, the statements inside
the begin-end block can be carried out in O(n∆) time where ∆ is the maximum
node degree of the graph. If it is false, the statements within the begin-end
block can be carried out in O(n). If the begin-end block associated with the true
part is executed once, it reduces the number of bridge nodes by at least one.
If the begin-end block associated with the false part is executed ∆ times, it is
guaranteed that the number of bridge nodes will reduce by at least one. Suppose
during a particular run, the true part is executed y times and the false part is
executed z times. As a result of y time execution of the true part, at least y
bridge nodes will cease to be bridge nodes. The remaining q − y bridge nodes
must become non-bridge nodes before the while-loop exits. This might require at

104

J. Edachery, A. Sen, and F.J. Brandenburg
Table 1. Performance comparison between five algorithms.
Number of clusters generated by different algorithms
Graph
No. of Nodes Diameter k A1 A2 A3 A4 A5
4 14 14 13 11 5
Bell Lab 1487
48
9
5 14 11 7 5 3
6 13 10 8 4 2
5 14 12 15 9 7
Bell Lab 1572
65
13
7 12 9 12 7 2
9 14 7 9 4 2

most (q −y)∆ executions of the false part of the If-statement. Therefore the total
computation in the while-loop will be O(n) + O(yn∆ + n(q − y)∆) = O(nq∆).
Since the nuber of bridge nodes q and the maximum node degree ∆ can be as
high as O(n), the worst case complexity of the algorithm will be O(n3 ).

3

Experimental Results and Discussions

We tested our implementation of the distance-k clique based clustering algorithm
with a wide range of graphs - small, medium and large. We extensively used
the Bell Laboratory graph library for testing purpose. This library has a large
collection of graphs of wide range of variation in terms of number of nodes,
edges and node degrees. The results of the output of our clustering algorithm
on Bell Lab graphs 1487 and 1572 with different values of k is presented in
table 1. Visualization of the clustering algorithm results is presented in figures
1 through 6. One way to measure the quality of our heuristic algorithm is to
find the proximity of the solution produced by it to the optimal solution. We
use the following strategy to compute the quality of our solutions: Suppose that
the number of clusters generated by heuristic algorithm Ai , 1 ≤ i ≤ 5 for a
given graph G = (V, E)) and a specified value of k, is N Ci,k . Suppose for the
same graph and same specified k, the optimal number of clusters is OCk . If
we know OCk , we can easily find its proximity to N Ci,k . However, OCk in
general is not known. However, we can make the following observation about
OCk : OCk ≥ diameter(G)/k. We evaluated the performance of our heuristic
algorithms using this criteria. The performance of A5 is much better than the
performance of A1 at the expense of higher computational time. The performance
of the algorithms A1, A2 and A3 is poor in comparison with A4 and A5 because
the error in estimated diameter keeps accumulating, resulting in a larger number
of clusters. It may be noted that as the objective of the distance-k clustering
algorithm is to cluster the nodes that have strong relationship between them,
unlike many other clustering algorithms, it does not necessarily minimize the
number of intercluster edges.

Graph Clustering Using Distance-k Cliques

105

Fig. 1. Bell Lab graph 1572 before
clustering

Fig. 2. Bell Lab graph 1572 A5k7

Fig. 3. Bell Lab graph 1572 A4k7

Fig. 4. Bell Lab graph 1487 before
clustering

Fig. 5. Bell Lab graph 1487 A5k6

Fig. 6. Bell Lab graph 1487 A4k6

106

J. Edachery, A. Sen, and F.J. Brandenburg

References
[1] F. J. Brandenburg, “Graph Clustering: Circles of Cliques,” Proceedings of Graph
Drawing Symposium, GD’97 Rome, September 1997. Lecture Notes in Computer
Science, Berlin: Springer-Verlag, 1353, pp. 158-168, 1998.
[2] J.S. Deogun, D. Kratsch and G. Steiner, “An approximation algorithm for clustering graphs with dominating diametral paths,” Information Processing Letters,
61, pp. 121-127, 1997.
[3] P. Eades, “Multilevel Visualization of Clustered Graphs,” Proceedings of Graph
Drawing’96, Berkeley, California, September,1996.
[4] M.R. Garey and D.S. Johnson, Computers and Intractability: A Guide to the
Theory of NP-Completeness. W.H. Freeman Publishers, San Francisco, 1978.
[5] D.W. Matula and L.L. Beck, “Smallest-last ordering and clustering and graph
coloring algorithms,” Journal of the Association of Computing Machinery 30 pp.
417-427, 1983.
[6] T. Roxborough and A. Sen, “Graph clustering using multiway ratio cut,” Proceedings of Graph Drawing Symposium, GD’97 Rome, September 1997. Lecture
Notes in Computer Science, Berlin: Springer-Verlag, 1353, pp. 291-296, 1998.

Approximation Algorithm for Avoiding Hotspot
Formation of Sensor Networks for Temperature
Sensitive Environments
Nibedita Das, Pavel Ghosh and Arunabha Sen
Computer Science and Engineering Department
Arizona State University
Tempe, AZ, USA, 85287
{nmaulik, pavel.ghosh, asen}@asu.edu
Abstract—Sensing and transmission phenomena of an implanted sensor dissipates energy which results in rise in temperature of its surroundings. Simultaneous operation of such
multiple active sensors increases the temperature of the surrounding environment causing hotspots. Such hotspots are highly
undesirable as they may cause damage to the environment as
well as to the sensor network, posing a challenge for deployment
of sensors. The problem is further enhanced for a temperature
sensitive environment, as the allowable threshold temperature
for such environments is less. Here we investigate the formation
of hotspots in such temperature sensitive environments due to
the heat dissipation of multiple active sensors and try to achieve
a maximum coverage of such networks avoiding hotspots. We
formulate this as a variation of the maximum independent set
problem for hypergraphs. We devise an Integer Linear Program
to achieve the optimal solution for the problem. We also provide
a greedy heuristic solution for the problem. For a special case
of this problem, where the hotspots are formed due to pairs of
sensors only, we prove a 5-approximation bound for the greedy
solution. Experimental results show that our algorithm achieves
near-optimal solutions in almost all the test cases.

I. I NTRODUCTION
Current day sensor networks are not only formed by randomly deploying a collection of sensor nodes over a given region of interest (as in military domain), but also by embedding
a given set of sensors at predetermined locations within the
sensing region (as in body sensor networks)[1]. The embedded
sensors sense and gather information while being implanted in
the sensing environment and in the process dissipates energy
which results in temperature rise of its surrounding. Since the
sensor node circuitry is being placed within the medium of
interest, the key challenges faced in these types of sensor
networks is to obtain desired information without causing any
damage to the sensing environment.
Such sensor networks are being used in modern day medical
applications involving remote monitoring and sensing of patients. Here the sensors are implanted within the human/animal
body or a specific organ of the body in order to monitor
their functioning and record conditions. However, the heat
dissipation due to the operation of such sensors and the
consequent increase in temperature is even more undesirable
for such applications [2], [3]. The human/animal body can
tolerate increase in temperature only upto a certain threshold

value. If the rise in temperature of the surrounding tissues
of an implanted sensor goes beyond that maximum allowable
threshold value, it may cause irreparable injuries to the surrounding medium.
This problem is not only pertinent to the medical applications, but also for any embedded sensor network that is
implanted in a medium sensitive to increase in temperature.
Hence special attention needs to be given for such networks
to ensure that the continuous operation of the sensors does
not cause the surrounding temperature to increase beyond
the threshold of the medium. It is also worth mentioning
that temperature sensitive environments like the human/animal
body poses a further restriction from the fact that the intricate
nature of the embedding medium restricts the positioning of
the sensors only to certain potential locations. Hence one needs
to select a subset of a given set of probable sensor locations
such that due to continuous operation of sensors at those
locations, the temperature is within the maximum allowable
threshold value at all place.
II. R ELATED W ORK AND M OTIVATION
The coverage problem in sensor networks is to select the
minimum number of sensors such that all the points in the
region of interest can be sensed by the selected sensors. In
the sensor network literature, extensive study have already
been performed by researchers regarding coverage issues in
various domains [4], [5], [6], [7], [8], [9], [10]. Fowler et.
al. [11] proved that sensor coverage problem is NP-complete.
Hochbaum [12] developed approximation schemes for the
coverage related NP-complete problems. Coverage and connectivity problem was studied in an integrated fashion in [6],
[8], [9]. The authors in [8] presented a coverage-configuration
protocol that provides different degrees of coverage depending
on the needs of the applications and explored the relationship
between coverage and connectivity. Also the authors in [13]
derive necessary and sufficient conditions for the coverage
and connectivity of an unreliable sensor network with high
probability. Recently there has been an increased interest
and importance of the emerging field of sensor networks
for healthcare and clinical applications. In such missioncritical applications, biomedical sensors are implanted in the

978-1-4244-4148-8/09/$25.00 ©2009
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2009 proceedings.

human or animal body to monitor and transmit biological
information such as retinal pressure [3], oxygen level on
the surface of exteriorized tissues [14], etc. Among many
other applications of biomedical sensor networks, the one that
pertains to artificial retina [15] deserves special attention. The
authors in [16] note that organs that are especially sensitive
to any temperature increase due to a lack of blood flow to
them are prone to thermal damage (e.g., lens cataracts). The
authors in [2] also emphasize on the importance of considering
possible health hazards for individuals exposed to EM field and
identify the EM field values that is safe for the human body.
The authors in [3] note that heat build-up from the sensor
electronics can jeopardize the implantation of the sensor, as
elevated temperature may cause infection, especially when the
implanted sensor becomes a haven for bacteria.
In one of our previous work we put emphasis on the
coverage and connectivity issue of sensor network embedded
in a temperature sensitive environment like the human/animal
body. From our previous finding [17] we concluded that
Corresponding to the sensor power dissipation Pdiss , there
exists a critical inter-sensor distance, dcr , such that if the
distance between any two deployed sensors is less than dcr ,
then the temperature in the vicinity of the sensors exceeds
the maximum allowable temperature Tthreshold . Therefore,
attention must be paid during sensor deployment to ensure
that the distance between any two sensors is at least as
large as dcr . However further research on this aspect reveals
- maintaining a critical inter-sensor distance between every
pair of deployed sensors is necessary but not sufficient to
ensure that the temperature in the entire region of interest
remains below the threshold value of the region. After placing
every pair of sensors a minimum separation distance apart,
the cumulative effect of heat generated by a set of them
could be such that the temperature at some other region/point
goes beyond the threshold. We call these regions hotspots.
Motivated by this phenomenon of hotspot formation of sensors
deployed in a temperature sensitive environment, we introduce
a new version of the sensor coverage problem. Here our goal
is to select sensor locations for placement in a temperature
sensitive region such that no hotspots are formed within the
region of interest and maximum number of points are covered.
Our contributions in this paper are as follows
•

•
•
•

•

We introduce a newer version of the sensor coverage
problem avoiding any hotspot formation in the region
of interest (SenCovhotspot ) for a temperature sensitive
environment.
We formulate the sensor coverage problem as the maximum independent set of hypergraphs.
We provide optimal solution for this problem using
Integer Linear Programming.
We introduce a greedy solution for the SenCovhotspot
problem and a 5-approximation algorithm to solve a
special case of the the SenCovhotspot problem.
Through extensive experiments, we compare the efficacy
of our proposed solutions.

III. P ROBLEM F ORMULATION
In the sensor placement and coverage problem discussed in
this paper, we are given:
1) a set of locations (or points) {a1 , . . . , an } to be sensed.
2) a set of m potential locations for placements of sensors.
Sensors placed at each such location can sense a set of
points depending on their sensing radius rsen .
3) a temperature threshold value Tthresh .
Using the given temperature threshold value of the surrounding medium and heat transfer analysis, we can easily
obtain the collection of all incompatible subsets of sensor
locations. Simultaneous activation of all the sensors in each
such incompatible subset will increase the ambient temperature beyond Tthresh causing hotspots.
The goal is to activate sensors from potential locations such
that maximum number of points ai are sensed and none of
the incompatible subset of sensors are selected entirely in the
solution. Like most of the previous studies in this area [5],
[7], [9], we assume that each sensor is capable of sensing a
circular area (disk) of radius rsen with the location of the
sensor being the center of the circle. We assume that the
sensing radius of all the sensors are identical. An example of
the coverage problem is shown in Figure 1, where the circular
nodes {1, . . . , 10} represent the points to be sensed, the
square nodes {a, . . . , h} represent the potential locations of the
sensors and the circles represent the area sensed by the sensor
located at the center of the circle. Each of the shaded area
represents the set of incompatible points. For example, here
the subset of locations {a, b, c}, {g, h}, {c, e, f, g} represent
the incompatible subsets. This means that none of these set of
locations can be selected as a whole in the final solution.

Fig. 1.

Potential sensor locations, sensing points and sensing disk

A solution to the problem of Figure 1 is presented in
Figure 2. Here only the locations b, c, d, f and g are selected
for sensor deployment. These locations cover the maximum
number (in this case points 2 through 10) of points to be
sensed. It is worthwhile to note that none of the incompatible

978-1-4244-4148-8/09/$25.00 ©2009
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2009 proceedings.

subset of locations are selected entirely in the final solution,
hence no hotspots are formed. The point 1 could not be
covered as it can only be sensed by sensor a. Inclusion of
sensor location a in the solution would imply including an
incompatible subset {a, b, c} and hence causing hotspot over
the region of interest.

Each hyperedge in H represents an incompatible subset
of sensor locations Si .
• Each vertex is associated with a list of points. A point
ai ∈ S will be in the list associated with a node v ∈ V
if and only if the Euclidean distance between ai and Si
is less than the sensing radius rsen .
Here the objective is to find an independent set V  ⊆ V of
the hypergraph G(V, H) such that the number of points sensed
by the vertices in V  is maximized.
•

IV. I NTEGER L INEAR P ROGRAM FOR O PTIMAL S OLUTION
In this section we provide the Integer Linear Programming
formulation for solving the SenCovHotspot problem as defined in the section above. We consider the following two
0 − 1 matrices as part of our input instance: P = [pij ], 1 ≤
i ≤ m, 1 ≤ j ≤ n, where:

1, if aj ∈ Si
pij =
0, otherwise
and Q = [qij ], 1 ≤ i ≤ l, 1 ≤ j ≤ m, where

1, if Sj ∈ Si
qij =
0, otherwise
Fig. 2.

Solution to the sensor coverage problem avoiding hotspots

A. Optimization Version of the Sensor Coverage Problem
avoiding Hotspots
Formally this problem can be stated as an optimization
problem as follows:
Problem 1: Sensor Coverage avoiding Hotspots formation
(SenCovHotspot )
INSTANCE: Given
(i) a set of points S = {a1 . . . an } to be sensed
(ii) a set of potential placement locations of the sensors de
noted by S  = {S1 , . . . , Sm
} where each Si ⊆ S (representing
the set of points it can sense)
(iii) a collection of incompatible subsets of S  denoted by
S  = {S1 , S2 , . . . , Sl }, where each Si ⊆ S  .
OBJECTIVE:
To find a subset S  ⊆ S  , such that

(i) | S  ∈S  Si | is maximized, and
i
(ii)¬∃j, 1 ≤ j ≤ l, such that , Sj ⊆ S 
B. SenCovHotspot as a variation of the Maximum Independent Set Problem for Hypergraphs
A hypergraph G(V, H) is a set of vertices V =
{v1 , . . . , vm } and a set of hyperedges H = {h1 , . . . , hl },
where each hyperedge hi represent a subset of vertices of
V . An independent set of a hypergraph is a subset V  ⊆ V ,
such that no hyperedge hi is entirely contained in V  , i.e.,
hi  V  , 1 ≤ i ≤ l.
The SenCovHotspot can be transformed to a variation of
the maximum independent set problem for hypergraphs. From
an instance of the SenCovHotspot problem we construct a
hypergraph G(V, H) as follows:

• Each node in V represents a probable sensor location Si .

Decision Variables:
We use the decision variable xi and yj in the formulation
defined as follows:

1, if Si ∈ S  is selected
xi =
0, otherwise

yj =

1,
0,

if aj is covered by at least one Si ∈ S 
otherwise

Objective Function:
Since the objective is to maximize the number of covered
elements by the subset S  ⊆ S  , the objective function can
be defined as:
n

obj : maximize
yj
j=1

Constraints:
1) From the definition of the decision variables xi , yj , and
the 0 − 1 input matrix P , it can be readily seen that :

m
1, if i=1 pij xi ≥ 1
yj =
0, otherwise
This condition can be easily transformed into the following constraint:
m

∀j, 1 ≤ j ≤ n :
pij xi ≥ yj
i=1

2) Since there should not be any Si , such that all of its
elements are part of S  , this constraint can be written
as follows:
m

∀i, 1 ≤ i ≤ l :
qij xj <| Si |
j=1

978-1-4244-4148-8/09/$25.00 ©2009
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2009 proceedings.

With the given matrix Q, the cardinality of each Si can
be expressed as:
∀i, 1 ≤ i ≤ l :| Si |=

m


qij

j=1

Therefore the above constraint can be re-formulated as:
m
m


∀i, 1 ≤ i ≤ l :
qij xj <
qij
j=1

j=1

V. G REEDY S OLUTION FOR THE S ENSOR C OVERAGE
P ROBLEM
In this section we develop a greedy heuristic for solving the
SenCovHotspot problem. We utilize the hypergraph formulation of the problem in our approach.
Algorithm 1 Algorithm for SenCovHotspot
Input : Graph G (V, H), where V = {v1 , v2 , . . . , vm } is
the set of vertices and H = {h1 , h2 , . . . , hl } is the set
of hyperedges. Each vertex is associated with a list of
elements that it covers.
Output : Independent set V  ⊆ V of the hypergraph covering the maximum number of elements.
1: Define the compatible vertex set as V1 = V
2: Initialize independent set V  = φ
3: while compatible vertex set NOT empty and NOT all
elements have been covered do
4:
Choose the vertex v ∈ V1 that covers maximum number
of uncovered elements
5:
V1 = V1 \{v}
6:
V  = V  {v}
7:
for all u ∈ V1 do
8:
if ∃h ∈ H such that h \ {u} ⊆ V  then
9:
V1 = V1 \ {u}
10:
end if
11:
end for
12: end while
13: RETURN V 
In line 1 and 2 of the algorithm, we initialize the compatible
vertex set to be the set of all vertices V and the independent set
to be empty. In each iteration of the while-loop in line 3 − 12,
we choose a vertex, from the compatible vertex set, that covers
the maximum number of yet uncovered elements. This vertex
v is then removed from the compatible vertex set V1 (line
5) and added to the independent set V  (line 6). After this
modification, some other vertices may become incompatible
since all the other vertices in some hyperedge h ∈ H may
now be part of the current independent set. We check this
condition (line 8) for all the vertices in V1 in the for-loop
(line 7 − 11). All such incompatible vertices are also removed
from V1 in line 9. The outer while-loop continues until all
elements have been covered or the list of compatible vertex
set to cover any new element is empty. At termination, the
algorithm outputs an independent set V  of the hypergraph
G(V, H). The Algorithm 1 runs in O(nml) time, where n is

the number of points to be sensed, m is the size of the vertex
set V , and l is the size of the set of hyperedges H.
Now let us consider a special case of the SenCovHotspot
problem where hotspots are formed of two sensors only. For
such an instance of the problem, it is evident that the hypergraph transformation reduces to a general graph Gsp = (V, E)
where we have a set of vertices V = {v1 , . . . , vm } and a set
of edges E = {e1 , . . . , el }, where each edge ei represents
an incompatible pair of vertices of V . For such a special
case, the SenCovHotspot problem can viewed as a variation
of the Maximum Independent Set Problem for Graphs. Here
the objective is to find an independent set V  ⊆ V of the
graph G(V, E) such that the number of points sensed by the
vertices in V  is maximized. In order to solve this, we apply
the above greedy procedure to the underlying graph Gsp (with
each edge ei being treated as an hyperedge consisting of a pair
of nodes). Next we prove that the above Algorithm 1 has an
approximation bound of 5 for this special case.
A. Approximation Bound
In this paper we have considered uniform sensing radius
rsen for each of the sensors. Therefore, each sensor is capable
of sensing a circular area (disk) of radius rsen with the location
of the sensor being the center of the circle. For this special
case, we consider a graph with the sensor location as the
vertices and there exists an edge between two vertices if and
only if the sensing disks of these two vertices overlap. This
underlying graph can be viewed as a Unit Disk Graph [18].
The maximum independent set problem for unit disk graphs
is widely known to be an NP-complete problem [19]. The
authors in [18] have already proved (lemma 3.1) that if C is
a circle of radius r and S is a set of circles of radius r, such
that every circle in S intersects C and no two circles in S
intersect each other, then | S |≤ 5. We utilize this result to
prove the performance bound of our approximation algorithm.

Fig. 3. Example showing approximation bound of sensor coverage problem
where hotspots are formed by pairs of sensors only

978-1-4244-4148-8/09/$25.00 ©2009
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2009 proceedings.

VI. E XPERIMENTAL R ESULTS
We conducted extensive simulations to compare the performance of the greedy heuristic with that of the optimal
solution. For experimental purpose, we generated 5 sets of
test instances. The number of points to be sensed in these
instances are 50, 70, 100, 150, and 200, respectively.
Within each of these test cases, we vary the number of
potential sensor locations (denoted by m in the graphs) in
the range from 7 to 35. The points sensed by each of these
potential sensor locations are selected randomly. In all cases,

50

Number of Points Covered

Optimal

Heuristic

40

30

20

10

0

m=7

m=10
m=15
Optimal vs. Heuristic

m=25

Fig. 4. Number of points covered by the sensor points - comparison of
the optimal solution achieved by solving the ILP and the solution from the
greedy heuristic - instances with 50 sensor points and number of potential
sensor locations (m) as 7, 10, 15 and 25

70
Optimal

Heuristic

Number of Points Covered

60
50
40
30
20
10
0

m=10

m=15
m=25
Optimal vs. Heuristic

m=35

Fig. 5. Number of points covered by the sensor points - comparison of
the optimal solution achieved by solving the ILP and the solution from the
greedy heuristic - instances with 70 sensor points and number of potential
sensor locations (m) as 10, 15, 25 and 35

100
Optimal
Number of Points Covered

Before getting into the formal proof of the performance
bound , let us start with a simple example. Figure 3 shows one
center vertex a and five peripheral non-intersecting vertices
b, c, d, e, f of an unit disk graph. Let the number of points
covered by each of these vertices be 5. Our greedy algorithm
may choose the central vertex a, thus not being able to select
any of the remaining five vertices (due to the incompatibility
subset constraint). The algorithm thus covers 5 points in total.
On the other hand, the optimal algorithm, in this case, will
select all the peripheral vertices, covering 5 × 5 = 25 points
in total. Therefore, the greedy solution performs 5 times worse
than the optimal for this example. Also, from the above
discussion on unit disk graphs, there cannot be more than 5
such peripheral vertices, and therefore the performance bound
cannot be any worse.
Theorem 1: Algorithm 1 is an 5-approximation algorithm
for the special case where each hyperedge consists of two
vertices only, i.e., if number of points covered by Algorithm
1 is denoted by AP P (for this special case) and the optimal
number of points covered for the same instance is denoted by
OP T
OP T , then AP
P ≤ 5.
Proof: Let us consider the list of vertices selected by
Algorithm 1 for the special case as vp[1] , . . . , vp[k] and the list
of vertices selected by the optimal solution as vo[1] , . . . , vo[q] .
Without loss of generality, let us also consider that these lists
are ordered in descending number of new elements covered.
The proposed Algorithm 1 follows a greedy strategy by
selecting the vertex that covers the maximum number of new
elements, and continues the same till it terminates without violating the incompatibility subset constraint. Therefore, clearly
the number of elements covered by vp[1] is at least equal to
the number of elements covered by vo[1] . Since the underlying
graph of sensor nodes have the property of unit disk graphs,
by the selection of vp[1] , the proposed algorithm can prevent
itself from selecting at most 5 vertices, which are part of the
optimal solution. Also, each such vertex, which is part of the
optimal, can cover at most the number of points equal to that
covered by vp[1] (due to the greedy nature). Following the same
argument, we can continue with selecting each such vertex
from the proposed solution, which may prevent itself from
selecting 5 other equally beneficial vertices which are part of
the optimal. Therefore, the total number of points covered by
the proposed algorithm AP P is at least 15 times the number
OP T
of points covered by optimal solution OP T , i.e., AP
P ≤ 5.
Also, the above example proves that this bound is tight.

Heuristic

80

60

40

20

0

m=15

m=25
Optimal vs. Heuristic

m=35

Fig. 6. Number of points covered by the sensor points - comparison of the
optimal solution achieved by solving the ILP and the solution from the greedy
heuristic - instances with 100 sensor points and number of potential sensor
locations (m) as 15, 25 and 35

the number of probable sensor locations have been chosen to
be significantly lower than that of the points to be sensed. The
incompatible subsets of sensors are also generated randomly,
and the number of such subsets is varied in the range from
4 to 17. It should be noted here that with actual information
on the temperature threshold and the incompatibility of the
sensor locations available, with a lower value of the temperature threshold the size of collection of incompatible subsets
(denoted as S  in the problem formulation) increases, and vice
versa. The optimal solution is obtained for all the test case
data by running the Integer Linear Programming formulation
using ILOG CPLEX version 10.1 Concert Technology on a

978-1-4244-4148-8/09/$25.00 ©2009
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2009 proceedings.

VII. C ONCLUSION

140

Number of Points Covered

Optimal

Heuristic

120
100
80
60
40
20
0

m=15

m=25
Optimal vs. Heuristic

m=35

Fig. 7. Number of points covered by the sensor points - comparison of the
optimal solution achieved by solving the ILP and the solution from the greedy
heuristic - instances with 150 sensor points and number of potential sensor
locations (m) as 15, 25 and 35
180
Optimal

Heuristic

In this paper we have introduced a newer version of
the sensor placement and coverage problem for temperature
sensitive domains. The notion of hotspot formation has been
introduced for such domains. We have formulated this problem
of sensor coverage avoiding hotspot formation as a variation of
the independent set problem for hypergraphs with an objective
of maximizing the number of points covered. We developed an
Integer Linear Program to obtain the optimal solution for the
the problem. We also proposed a greedy heuristic to solve the
problem. For a special case of the problem instances, where
hotspots are formed due to pairs of sensor locations only, the
algorithm is proved to have an approximation factor of 5.
Experimental results show that our greedy algorithm achieves
near optimal solutions for almost all the test cases.

Number of Points Covered

160

R EFERENCES

140
120
100
80
60
40
20
0

m=15

m=25
Optimal vs. Heuristic

m=35

Fig. 8. Number of points covered by the sensor points - comparison of the
optimal solution achieved by solving the ILP and the solution from the greedy
heuristic - instances with 200 sensor points and number of potential sensor
locations (m) as 15, 25 and 35

Pentium IV 3.2 GHz processor with 1 GB RAM. The greedy
heuristic is implemented in C++ and executed on the same
machine. The results from our experiments are presented in
Figures 4, 5, 6, 7, and 8. In all the test cases, the results
obtained from the heuristic solution are found to be equal
to the optimal solution or very close to that. For our test
cases, the solution returned by the greedy heuristic differs
from the optimal solution by at most 2%. Though analytical
results have shown that the approximation algorithm produces
solution bounded by a factor of 5 for a special case of the
problem, the experimental results obtained for all the test cases
are much better than that. From the experimental results, it
can also be observed that, as the number of potential sensor
locations (m) are increased within each of the test cases, the
number of points sensed/covered increases for both the optimal
and the heuristic solutions. The number of constraints in the
ILP formulation increases exponentially with the increase in
the size of the problem instance, whereas the running time of
the greedy heuristic is polynomial in the size of the problem
instance. Although, for our test cases, both the optimal and
heuristic solutions were found within a few seconds, but for
larger problem instances, the execution time for the CPLEX
optimizer can be much higher than that of the greedy heuristic.

[1] M. Y. Guang-Zhong Yang, Body Sensor Network. Birkhuser, 2006.
[2] P. Bernardi, M. Cavagnaro, S. Pisa, and E. Piuzzi, “Sar distribution and
temperature increase in an anatomical model ofthe human eye exposed
to the field radiated by the user antenna in a wireless lan,” IEEE Trans.
on Microwave Theory and Techniques, vol. 46, no. 12, 1998.
[3] L. Schwiebert, S. K. Gupta, P. Siy, G. Auner, and et al, “A biomedical
smart sensor for the visually impaired,” IEEE Sensors, 2002.
[4] Z. Abrams, A. Goel, and S. Plotkin, “Set k-cover algorithms for energy
efficient monitoring in wireless sensor networks,” in IPSN, 2004.
[5] S. Funke, A. Kesselman, F. Kuhn, and Z. Lotker, “Improved approximation algorithms for connected sensor cover,” in International Conf.
on AD-HOC Networks and Wireless, 2004.
[6] H. Gupta, S. R. Das, and Q. Gu, “Connected sensor cover: selforganization of sensor networks for efficient query execution,” in MobiHoc, 2003.
[7] K. Kar and S. Banerjee, “Node placement for connected coverage in
sensor networks,” in WiOpt, 2003.
[8] X. Wang, G. Xing, Y. Zhang, C. Lu, R. Pless, and C. Gill, “Integrated
coverage and connectivity configuration in wireless sensor networks,” in
SenSys, 2003.
[9] H. Zhang and J. Hou, “Maintaining sensing coverage and connectivity
in large sensor networks,” UIUC, Tech. Rep. UIUCDCS-R-2003-2351,
2003.
[10] I. F. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci, “Wireless
sensor networks: a survey,” Computer Networks, vol. 38, no. 4, 2002.
[11] R. J. Fowler, M. S. Paterson, and S. L. Tanimoto, “Optimal packing and
covering in the plane are np-complete,” Information Processing Letters,
vol. 12, no. 3, 1981.
[12] D. Hochbaum, Approximation Algorithms. PWS Publishing, 1997.
[13] S. Shakkottai, R. Srikant, and N. Shroff, “Unreliable sensor grids,
coverage connectivity and diameter,” in INFOCOM, 2003.
[14] B. J. Sargent and D. A. Gough, “Design and validation of the transparent
oxygen sensor array,” IEEE Trans. on Biomedical Engineering, vol. 38,
no. 5, 1991.
[15] L. Schwiebert, S. K. Gupta, and J. Weinmann, “Research challenges
in wireless networks of biomedical sensors,” in ACM/IEEE Conf. on
Mobile Computing and Networking, 2001.
[16] A. Hirata, G. Ushio, and T. Shiozawa, “Calculation of temperature rises
in the human eye for exposure to em waves in the ism frequency bands,”
IEICE Trans. on Communications, vol. E83-B, no. 3, 2000.
[17] A. Sen, N. Das, L. Zhou, B. Shen, S. Murthy, and P. Bhattacharya,
“Coverage problem for sensors embedded in temperature sensitive environments,” in IEEE Conf. on Sensor Mesh and Ad-hoc Communications
and Networks, SECON 2007, 2007.
[18] M. V. Marathe, H. Breu, H. B. H. III, S. S. Ravi, and D. J. Rosenkrantz,
“Simple heuristics for unit disk graphs,” Networks, vol. 25, pp. 59–68,
1995.
[19] M. Garey and D. Johnson, Computers and Intractability: A Guide to the
Theory of NP-Completeness. Freeman Press, 1979.

978-1-4244-4148-8/09/$25.00 ©2009
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2009 proceedings.

Adaptilve Data Collectilon Scheme for Trackilng Mobille Target 'in W'ireless Sensor
Networks

Ling Zhou and Arunabha Sen
Department of Computer Science and Engineering
Arizona State University, Tempe, AZ 85287-5406
{ ling.zhou,asen}lasu.edu
Abstract
Trac king mobile targets is an important application of
wireless sensor networks. However, mobility of the target brings new challenges to designing energy-efficient and
scalable data collection schemes. A novel Dynamic Gridbased Tracking (DGT) scheme for tracking mobile target is
proposed in this paper This scheme is distributed in nature,
and can be adaptive to the mobility of the target. The underlying ideac of embedding a virtual grid structure and restricting mobility-related choices to grid nodes can be generally applied to other wireless sensor networks applications, where mobility is an important consideration during
protocol design.

1. Introduction
Wireless sensor networks [1] are envisioned to consist of
a large number of sensor nodes, which are deployed over
a vast field and collaborating to obtain high-precision sensing data. The specific features of wireless sensor networks,
such as the large quantity of nodes, limited power supply
and computational capability, highly correlated traffic and
many-to-one traffic pattern, bring many new challenges to
the protocols design. Distributed nature, energy efficiency,
scalability, and adaptability to changing environment are all
important factors to be considered.
This paper studies an important application of wireless
sensor networks: tracking mobile target (refer to Figure
1), where the deployed sensor nodes will track a mobile target in a collaborative way. Those sensor nodes, which have
detected the mobile target (they are called source nodes)
are expected to report their sensing data to a sink; node
at a specified rate Since the sensing data from different
source nodes surrounding the target will be correlated, it
is highly desirable to select an aggregation node near the
source nodes, so that it can locally collect the data and re-

1-4244-0499-1/06/$20.00 ©2006 IEEE

move any redundancy before sending them to the sink. Introducing an aggregation node and the corresponding innetwork processing is expected to reduce the energy consumption, when compared with the case where each source
node sends its data independently to the sink. Then in this
scenario, the selection of the aggregation node and its adaptive adjustment with the movement of the target become the
challenging issues.
This paper proposes a novel Dynamic Grid-based Tracking (DGT) scheme with adaptive aggregation node selection
for tracking a mobile target. In our scheme, (1) for each
reporting interval, an aggregation nod.e will be selected, by
each source node after communication with only one-hop
neighbors; (2) although each source node selects its own aggregation node, the majority of the source nodes will agree
on the same aggregation node, (3) the selected aggregation
node(s) will be close to the target to reduce data collection
overhead, and be adaptively adjusted as the target moves.
The research in this paper is done within the context of
some existing and exciting work [2.[3.. In [4], the importance and challenges of scalable coordination in wireless
sensor networks have been addressed. Directed Diffusion
and some related work [2][5][6] studied the model of datacentric routing and, the benefit of aggregation during collecting data from source nodes to the sink node. But they
did not address the issues about adjusting the aggregation
tree under changing environment, such as in tracking a mobile target. Papers [7][8][9] studied the problem of how to
predict the present and future positions of moving objects.
To the best of our knowlLedge, Dynamic Convoy Tree-based
Collaboration (DCTC)[1O][1 1 ] is the first paper, which proposed a concrete scheme for collaborative tracking a mobile
target.
In the following we highlight the major differences and
improvement of our research compared] with DCTC.

* The convoy tree (same as the aggregation tree in DGT)
in DCTC incrementally evolves as the target moves
and selects the source node closest to the target as

the aggregation node after each reconfiguration; while
DGT use a virtual grid structure to restrict the choices
of aggregation node only to grid nodes and discretizes
the construction of aggregation trees at each reporting
interval, potentially saving the expansion or reconfiguration overhead between successive reporting intervals
needed in DCTC.
*

*

The increase of the target's velocity will make reconfiguration more frequent in DCTC and, thus incurring
more overhead, while DGT is immune to that effect.
The numerical comparison shows that DGT consumes
less energy than DCTC, and the energy savings will
increase with the increase of the target's velocity.
The DGT framework has more generality. The underlying idea of embedding a virtual grid structure and
restricting mobility-related choices to grid, nodes can
be generally applied to other wireless sensor networks
applications, where adaptability to the changing environment is an important consideration.

In this paper, we make the following contributions: (1)
Formulate the mobile target tracking problem as the problem of finding a sequence of optimal aggregation trees to
minimize the total energy consumption. (2) Present centralized optimal solutions for different scenarios. The centralized optimal solution can be used as a benchmark to evaluate the proposed, distributed schemes. (3) Propose a novel
distributed Grid-based framework to facilitate the adaptive
aggregation node selection as the target moves. (4) Propose
two aggregation node selection algorithms under the general framework. (5) Evaluate the proposed, framework and,
algorithms using both analytical and simulation techniques.
O0
°

0°0 0 00°

o o°000 00

0

a

ao
00°

0 o
o0
4 o04 0°0
° 00
° °o
0o
00°o
00

0

00

00

c000000

g

_0

00
'I>

C

rd

0

°

0 0

,0 lo

00

0

Oc

)<

0

0

0

00

0

4

00

0

m

°

''a°0°
0

_

°

o0

0

0

0

° °

~~~~a0

°

0

0

0

0°

00

o

000

000

0

13~ 0000
0

node

* target

0

000

°

o seisor

13

'-

El 0sink.3 llode

0 00

0

0

0

0

0

a

00

00

:

0

Figure 1. Tracking Mobile Target
2.

Distributed Gridl based Tracking Scheme

2.1. System Model

The mobile target tracking problem can be formulated as
finding a series of optimal aggregation trees as follows.
Instance: At any time t, given the sensor network G(V; E),
the set of source nodes St and, a sink node s (note that the
movement of the target has been reflected on the different
set of source nodes St at different time t).
Question: How to choose a sensor node jt as aggregation
node, and how to establish transmission paths from source
nodes to node it and from node it to the sink s, such that
the sum of the energy consumed by all the source nodes St
sending their data to node it and the energy consumed by
node j sending the aggregated data to the sink s is minimized.

~~~~0,00000o
oa

00

:

o

4Z:

10.1

000000
00

00000

00

0

0

0

0

=>

0

°

°0

0

000000000000000
0
0
0
00°
0 0
00~~~%

0

on 4s

°

0

0

0

0

o

a° >

4

field. Each sensor node is static with transmission range
Rt and sensing range Rs The formed sensor network can
be modeled, as a graph G(V, E), where vertices represent
sensor nodes, and there is an edge between node i and j
if they are within each other's transmission range Rt. Each
sensor node i is aware of its own location (xi, y), and will
maintain its neighbors' locations after communication with
them. A mobile target will enter and, leave the sensing field,
at time t, and t, respectively, and move with unknown velocity. As the target moves, it can be detected by those sensor nodes within R, distance from itself. Let St denote
the set of source nodes at time t. In this paper, we just
assume that each source node can tell the signal strength
it has detected from the sensed data (the concrete form is
application-specific). The closer the sensor node is to the
target, the stronger signal strength it will detect. The sensor
node itself cannot precisely predict the location ofthe target
or the distance between the target and itself. Collaborative
processing is implemented in our scheme. For simplicity,
we assume that the sink node s is static and its position
(.o, Yo) will be known by all the sensor nodes either by
pre-configuration or by any broadcast-nature procedure initiated by the sink. We will mention later that our scheme
can also be extended to the scenarios with multiple mobile
sinks.

&

Problem Formulation

Let T(t, it) denote the optimal aggregation tree rooted
at node it at time t, which contains all the paths used by
source nodes in St sending their data to node it and, the path
used by node it sending the aggregated data to the sink s.

Let E(T(t, it)) denote the energy consumed by sending all
the sensing data to the sink along T(t, t) Then the mobile
target tracking problem during time (t- t,) aims to find
a sequence of aggregation trees S
{T(t, itJ) T(t, +
jt +,) .... T(te t )}, such that the energy optimization
at time t t <
(minimization) is achieved by each T(t
1

We consider

network where N sen
sor nodes are randomly and densely deployed in a rectangle
a

wireless

sensor

t

<

t, (Time is assumed to be discrete for simplicity).

S: Source node

A:Aggregation node

Sink

s6

slI

Algorithm 1:
1. For each node it in V
2. Set L(T(t,j))
3. For each node i in St U {s}
4.
Compute the shortest path L(i, jt)
5.
L(T(t, it))+ = L(i, jt)
6. Find the node itin

Figure 2. Aggregation Tree Rooted at Node A
2.2. Centralilzed Optimal Solution
Centralized optimal solution assumes that the global
knowledge is available. Although unachievable in real applications, centralized optimal solution still can be used as a
benchmark to evaluate the goodness of distributed schemes.
Centralized optimal solution will be different with different assumptions about whether the aggregation only occurs
at aggregation node or data can be aggregated, at intermediate nodes before reaching the aggregation node. Let's illustrate the difference by taking the aggregation tree shown in
Figure 2 as an example.
For the first case (aggregation can only occur at aggregation node), any inteirmediate node only forwards the packets it receives from others. Therefore the aggregation tree
shown in Figure 2 will require 16 transmissions to collect
the data from all the source nodes and send the aggregated
data to the sink. This kind of scenario corresponds to those
applications, where it is hard to determine which part of the
data is redundant until enough datum have been collected,
(at the aggregation node). For this case, the optimal solution
for T(t,it) is a Shortest Path Tree rooted at node which
minimized E(T(t, jt) given by the following formula:

E(T(t, t)=

E
iEStu{ ST

edLi jt)

L(i t) is the length of the shortest path between node i
and it in terms of hops (node s is the sink). e is the energy
consumed by transmitting one unit of data for one hop, and
d is the size of the data. Assuming e and d are constant,
the optimal solution (also node it) can be obtained, by Algorithm 1 with time complexity 0(kV(VlgV + E)), where
kI= Stl.
For the second case, the intermediate node can even aggregate the halfway data from others with its own data, before forwarding them to the aggregation node. Therefore
the aggregation tree shown in Figure 2 will require 11L transmissions to collect the data from all the source nodes and
then send the aggregated data to the sink. The minimum
number of transmissions needed is exactly the same as the
number of edges in the aggregation tree. This kind of sce-

V, which minimizes

L(T(t,j)

nario corresponds to those applications, where even the intermediate node can determine which part of the data can
be eliminated safely, for example when only the maximum
(or the minimum) value of the data is required. For this
case, the optimal solution for T(t t) is actually a Minimum
Steiner Tree, T(V5t, E,t), in the graph G(V, E), which connects all the source nodes in St and sink node s (each edge
corresponding to a single hop is assumed to have the same
weight). After obtaining T(Vt, E£t), the node it can be determined by reversely tracing the path starting at the sink s
untill the first node where path divergence occurs. For this
case, E(T(t, it)) is computed as
E(T(t, jt))

=

e

d

E,t

where and, d have the same meaning as in the first case,
and 1EstI is the number of edges in the Minimum Steiner
Tree. In general, finding Minimum Steiner Tree is a NPComplete problem [12]. However in the context of tracking mobile target, if the subgraph G' induced by all the
source nodes St is connected (which is normally true due
to the close distances among all the source nodes), the Minimum Steiner Tree can be found in polynomial time by Algorithm2. The correctness proof of Algorithm 2 is similar
as the one in [6]. The time complexity of Algorithm 2 is
0(E + VlgV) + 0(k(E + VlgV)) = 0(k(E + VlgV)), where
kI= Stj [13].
c

Algorithm2:
1. Find, the Minimum Spanning Tree in the subgraph G'
induced by all the source nodes St
2. Find the shortest path from sink node s to the closest
node

it

in

St

In the following text, we only focus on the first scenario,
where aggregation can only occur at the aggregation node.
However, the DGT scheme proposed in next section is applicable for both scenarios.
2.3. Framework of

DGT

Scheme

For each source node if global information including the

trace of the target is available (as in the centralized optimal

solution) it can accurately compute the optimal aggrega
tion tree for each reporting interval. Then it knows which

node is the aggregation node, and to which neighbor/parent
the data destined to the aggregation node should be sent.
For different intervals, it just switches between different
neighbors. However in reality, the assumption of knowing global information at any node (even only at the sink
node) is not realistic. Therefore, a distributed algorithm is
needed, where each node only has local information and,
some limited, pre-configured information available. Then
the question is: How can all the source nodes agree on the
same aggregation node in a distributed manner with only
local information, for example only information from onehop neighbors available? Even if it is hard, to agree on the
unique aggregation node, how to make the majority of the
source nodes agree on the same aggregation node, while the
total number of different aggregation nodes selected is as
few as possible?
Before presenting the general framework of DGT
scheme, let's make clear the goals of our design. (1) The
imajority of the source nodes should select the sadme aggregation node, even if each source node makes its decision
distributedly. (2) The selected aggregation node(s) is close
to the target, and will be adaptively changed as the target
moves.
Our general framework is as follows: (1) Use a virtual
Grid structure to restrictlreduce the choices of aggregation
nodes only to those grid nodes. At any time t, we would
like to select the grid node closest to the target as the aggregation node. By choosing the size of grid, cell carefully,
we will make sure thatfrom the point of view of each source
node i, the grid node closest to the target must be among the
4 grid nodes of the local cell where the source node i itself
is located. Therefore, each source node can further restrict
the choice of aggregation node to the four grid nodes of its
local cell, which is extremely helpful to fonrm a distributed
algorithm. (2) By fully exploring the available informnation,
each source node independently predicts the current location of the target and chooses the local grid node closest to
the predict location as the aggregation node. In the following, we explain the DGT scheme in more details.
For simplicity, the sensing field is assumed to span a twodimensional plane. In DGT, after all the sensor nodes have
been deployed, the sink node will initiate a grid construction
phase (similar as the one in [3]) to divide the whole sensing
field into a grid structure, with each cell a 2R, x 2R, square.
The sensor nodes closest to the grid points (locations) will
become the corresponding grid, nodes, and, maintain the infornnation about which grid point(s) they are representing.
The reason we choose 2R as the length of the grid cell is
that, at any time t the circle where all the sources nodes
are located will cover at most 4 adjacent grid cells (refer to
Figure 3). From the figure, it can be easily seen that for
each source node the grid node from its local cell which
is closest to the target is also the grid node which is glob-

ally closest to the target. Therefore, if somehow the source
node can know or predict the current location ofthe target, it
only needs to select the local grid, node closest to the target
as the aggregation node. We will mention that with some
basic information, each source node can easily compute the
locations represented by the four local grid nodes. From
Figure 3, we can also see that for the third, and, fourth cases,
all the source nodes could select the same aggregation node.
For the first and second cases, which should occur less frequently than the last two, each source node can choose the
aggregation node using the sink node's location to break the
tie.
I

00
0 0a

,

0

0

-

cla,;g0

g\

00

Q

.o0
o a,SOa 0

1 1:~~

0

00a

0

0

00

0

0

0
C)

~

0

0

0.,

j ~~..

0 00a

o

D

0

D0

0 0

0

0

~
0

oD 0

a

0

0
*o

E
o°o
o,LJ J.

00

0

0

0
0

0
0

Figure 3. St Covers at most 4 Grid Cells
The second step in our general framework is for each
source node to choose the aggregation node independently.
The question is that how each source node should. explore
the local information to predict the target's location, such
that from its own point of view, the local grid node closest
to the predicted location is same as the grid node selected by
others? In DGT scheme, we assume that the length of the
grid cell, 2R,, is a pre-configured information. Each sensor
node can also know the location of the sink node, either
by pre-configuration or by some broadcast-nature procedure
initiated by the sink.
Our strategy is that once detecting the target, each source
node will share its signal strength with its neighbors for
each subsequent reporting interval, until it can no longer
detect the target. For each reporting interval, a decision algorithm in each source node will take the current information as input, and select the aggregation node according to
some rules. Since the movement of the target has been totally reflected by the different set of source nodes and their
detected signal strengths at different time, different aggregation nodes will be chosen for different reporting intervals
as the target moves.
In the following, we list all the input information each
source node i will have before selecting the aggregation
node: (1) location information: its own location (xi, yi),
sink node's location (xo, yo), the length of grid cell 2R,
and location ('j,y) for each neighbor j (2) signa,lI
strength information: its own detected signal strength, and
its neighbors' reported signal strength. Please note that
given the above location information, each sensor node
can compute the locations represented, by the four local grid,
nodes as follows (refer to figure 4)

P:(x +2R..(xi- xo)2R

yo +2R.(yyo).2R,])

2Rs
(xo + 2Rs
[ L(xi
Pi2
R)/2R,,] ,yo
( (yi _ yo)/2R,] + 1))
Pi3 : (xo + 2Rs (Mxi - o)/2R,] - 1),yo + 2R1
( L(yi _ yo)/2R,] + 1))
(L(xi - Xo)/2R,] - 1),yo + 211
Pi4 : (xo + 2R1
- YO)/2Rj])
L (yi-

Sensor Node

Grid Node
* Mobile Target
*

Base Station

Figure 4. Determination of Four Local Grid
Nodes' Location
In the next section, we propose two decision algorithms,
which take the above information as input and select the
aggregation node.

2.4.

Algorithms to Choose Aggregation Node

Decision

The main ideas of the two decision algorithms are similar: each source node uses the available information to predict the current location of the target, and then select the
local grid node closest to the predicted location as the aggregation node.
The first decision algorithm uses the idea of Voronoi Diagram to predict the area where the target is potentially located, and then chooses the local grid node closest to the
predicted area as the aggregation node. The closeness can
be measured, for example, by comparing the intersection
area between the predicted area and the four quadrants (each
corresponds to one grid nod.e) of the local cell respectively.
The algorithm with respect to node i is formalized, below.
Figure 5 illustrates the steps used to predict the area where
the target is located.

Voronoi Diagram-based Decision Algorithm:
1. Assume the area where the target is potentially located,
is the whole local cell.
2. For each pair of its neighbors, node j and k, do the following

steps:

2.1. Draw the perpendicular bisector between node j and
k, and reduce the predicted area (where the target is potentially located), based, on the comparison of signal strength
of nodej and k.
3. Compute the intersection area between the predicted area
and the four quadrants respectively.
4. Choose the local grid node, which is located in the quadrant having the most intersection with the predicted. area, as
the aggregation node. Use sink node's location to break any
tie.

The second decision algorithm is simpler than the first
from the aspect of implementation. It predicts the
current location of the target, based on the average signal
strength from each quadrant with respect to the source node,
and then chooses the local grid node closest to the predicted
location as the aggregation node. The algorithm with respect to node i is formalized below.
one

Quadrant-based Decision Algorithm:
1. Divide all the neighbors into four quadrants based, on
their relative locations to its own location (Xti, Yi)
2. Compute the average signal strength from each quadrant:
S1, S2, S3 and S4
3. Compute A = rnax(sl, s 2, 3, S4) - Tmin(Si,S2, 53, S4)
3.1. If A is greater than threshold tho, randomly generate
a number D within the range (0, RP] Use D and its own
location (xi, yi) to compute the location L, which is D distance away from (xi, yi) along the diagonal in the quadrant
with maximum average signal strength. Treat L as the potential location of the target.
3.2. Else set L equal to (Xi, Yi), that is, treating its own
location (xi, yi) as the potential location of the target.
4. If L is more than R,la (a > 6) distance from the center of the local cell, choose from the four grid nodes the
one closest to L as the aggregation node; else choose from
the four grid nodes the one closest to the sink node as the
aggregation node.

0 0

In Step 3, if A exceeds some threshold tho (a tunable
application-specific parameter), node i can roughly deter-

0
0

0

mine which quadrant (with respect

Figure 5. Predict the Area where Target is Potentially

Located

to

itself) the target might

fall into Since the distance between node i and the target

must be less than

,a random number

D

is used to predict

the target's location. If A is less than the given threshold,
which might indicate that the target is very close to node i,

its own location (xi, y ) will be treated as the target's location. After predicting the target's location in either way, the
algorithm aims to choose the grid node closest to the predicted location as the aggregation node. In case where the
predicted location is very close to the center ofthe local cell
(here we use R, /a, a > 6 as boundary), which might correspond to the first case of Figure 3, the sink node's location
is used to break the tie.
Both decision algorithms work without knowing the exact propagation model of the signal strength. Furthermore,
under the general framework of DGT, the decision algorithms can be enhanced by any better prediction techniques.
The more accurate the prediction is, the higher the possibility that all the source nodes choose the same aggregation
node will be, and the less energy will be consumed in DGT.

2.5. Routing-related Issues & Grid Functionality
Since in DGT scheme, the major traffic pattern is either
from source nodes to some aggregation node(s) or from aggregation node(s) to sink node, there are two routing issues
we need to address. The first one is to establish routes for
sending packet from source nodes to the selected aggregation node(s), and the second one is to establish routes for
sending packet from aggregation node(s) to the sink node.
For simplicity, we can use totally proactive routing protocols to establish routes for packet transmission. For example, the sink node can initiate a global level tree construction to establish the shortest path between itself and
any other sensor node (especially the grid nodes). Before
the construction, all the sensor nodes have level oo, and the
sink node has level 0. The sink node will start the level tree
construction by sending its neighbors a level packet with its
own level 0 included. Whenever a sensor node receives a
level-packet, if the level included in the packet plus one is
less than its current level, it will update its level and set the
sender of the packet as its parent in the level tree; otherwise
the packet will be dropped. Once its level has been updated, the sensor node will send a similar level packet to its
own neighbors. Such process repeats until each node knows
the fewest hops it is away from the sink, and, the neighbor to which the packet destined to the sink node should be
sent. The level tree constructed is called global level tree,
since all the sensor nodes will participate in the construction. Similarly, each grid, node (the potential aggregation
node) can initiate a local level tree construction to establish
the shortest paths between itself and the surrounding sensor
nodes. The level tree constructed is called local level tree,
since the propagation of the level packets should be constrained to local cells. For each source node, it only needs
to save 4 parent nodes information for the local level trees
initiated by the 4 local grid nodes respectively. Although

the global/local tree constructions both have broadcast nature, the energy consumption for such construction is not
excessive, especially in the case where the traffic pattern
is fixed and, the sensor nodes are static. The cost involved,
in level tree constructions will be amortized by the benefit of subsequent packet transmissions along the established
shortest paths. Other routing schemes, such as GPSR[ 14]
and, SHARP [15] can also be applied in the framework of
DGT We leave out those discussion due to page limitation.
It can be seen that the major functionality of the established grid structure is to make the aggregation node selection scalable and adaptive to the movement of the target.
Actually the grid, structure can also be used to route/direct
the aggregated data from the aggregation node to the sink,
especially when the sink is also mobile. Paper [3] discussed
how to use a grid structure to disseminate data to mobile
sinks. Our DGT scheme can be combined with that, and
thus extended, to the scenarios with multiple mobile sinks.

3. Simulation and Performance Evaluation
3.1. Comparison with Centralized Opt'imal
Solution
We implement the DGT scheme with the Quadrantbased Decision algorithm (due to its simplicity) in NS-2
[16], and, compare it with the centralized, optimal solution
obtained, by Algorithm 1. We compare two aspects. One
is the aggregation node selection and the other is the total
number of transmissions involved in transmitting packets
from source nodes to aggregation node, and then from aggregation node to the sink.
Regarding the aggregation node selection, the centralized opti;al solution will return a unique aggregation node.
For DGT, we are interested in (1) how many different aggregation nodes are selected, at each reporting interval, and,
what is its distribution over all the intervals; (2) what is the
distribution of source nodes on different aggregation nodes
selected for each interval, and whether the majority of the
source nodes agree on the same aggregation node. Figure
6 shows the result of one experiment we have conducted.
The configuration is 1000 sensor nodes uniformly distributed in a 4000 x 4000Tn2 field. Each has transmission
range Rt = 2 0rn (the default value in NS-2) and sensing range R, = 400m. From Figure 6, we can see that
at the first reporting interval, two aggregation nodes are selected with around 93% of the source nodes choosing the
same one; while at the third interval all the source nodes
choose the same aggregation node. For all the intervals,
the majority of the source nodes (exceeding 50) choose
the same aggregation node. Table I shows all the experiment results regarding the aggregation node selection The
first part of the table is the configuration of each experi-

ment, and the second part is the statistic results. For each

experiment, 38 reporting intervals

examined. NV represents the percentage of those intervals where one unique
aggregation node is selected. Similarly, V2 represents the
percentage of those intervals where two aggregation node
are selected, and etc. Al represents the average percentage
of those source nodes, which choose the same aggregation
node chosen by the majority of the source nodes. For example, in the first experiment, our records indicate that on the
average around 80% of the source nodes agree on the same
aggregation node over all the intervals. From Table 1, we
can see that (1) at any interval, the maximum number of different aggregation nodes selected by DGT scheme does not
exceed 4, although the total number of possible aggregation
nodes, which can be selected by the four cases of Figure 3
are 4, 6, 8 and 9 respectively. Furthermore, in most of the
time, only one or two different aggregation nodes will be selected (2) On the average, the majority of the source nodes
(around 80 ) will agree on the same aggregation node.
are

that at the first interval, the ratio ofthe total number of transmissions needed by DGT scheme to the optimal number of
transmissions computed by Algorithm 1 is 1.32, while at
the seventh interval the ratio is 1.96. The average ratio over
all the intervals is 1.52. Table 2 shows all the comparison
results. Min, Max and Aver represent the minimal, maximal and, average ratios respectively. From Table 2, we can
see that the average ratio of DGT scheme to the centralized,
optimal solution is around 2.
All the above results are restricted by the light-weight
prediction techniques used by the Quadrant-based Decision
Algorithm. We believe that under the general framework of
DGT, with the aid, of some improved, prediction techniques,
both the number of selected aggregation nodes and the total
number of transmissions involved will be reduced.
140

.0
-)

c

E

100%

cn

90%

s:

80%
-0

70%

a)

50%
40%

0D

30%

{n

_
_
__ _
_
. __
_
_ _
_ _ _

on1
80nu

.

.

..

0

.

0

DU

iLiliniflu

40n
-+r

BUD

l 4h choice
j * 3rd choice
2nd

20%

1l

100

IH =~ I 9.I.,

60%

.2_

G
I=
IUIen p

1

120

__
lIL

choice|

l
I

1

3

5

7

9

11

13 15 17 19 21 23 25 27 29 31

33 35 37

reporting intervals

l

10%

1

3

5

7

Figure 6. N

and Rs

=

9

=

400

Figure 7. N = 1000, F
and Rs = 400

11 1 3 1 5 1 7 1 9 2 1 23 25 27 29 3 1 33 3w5 37
reporting intervals

1000, F

=

4000

x

4000, Rt

=

=

4000 x 4000, Rt

=

250

250

Table 2. Statistics about total transmissions
involved
N
Field Min Max Aver
Rt R,
Table 1. Statistics about Aggregation node
selection with Field = 4000 x 4000
N
Ni
N2
N= N4M
Rt R
340 37% 80
250 400 1500 210
80%
250 400 1200] 340
470 - lb
80
82%
3Cc
250 350
800] 130c 74Cc lb
79%
26%
13%
8%
250 400 1000
53%
83G
33 79C
250 350 bO j 30
91% 30

250

250
250
250
250

250

3.2.

Regarding the total number of transmissions involved in
each reporting interval, we are most interested in the
parison between the optimal number computed by Algo
rithm 1 and the number (of transmissions) involved in the
DGT scheme Figure 7 shows the comparison result for the

400

400

350
350
400
350

1500
1200
800
700
1000

bOO0

Comparison

4000

1

4000
3000
4000
4000

1.06
1
1.03
1.26

4000

DCTC[10] [11]

1

with

3.34
2.1)
4.23
3.04
3.03
4.38

Related

1 .61
1.46
1.96
1.6
1.52
2.07

Work

com-

same

experiment as shown in Figure 6.

Our records

indicate

The convoy tree (same as aggregation tree) in DCTC
evolves in the following way as the target moves:
Initially, a convoy tree rooted at the source node clos
est to the target will be constructed. As the target moves,

1. Construct the Initial Convoy Tree
2. Do the following steps as target moves, until it leaves:
2. 1. Tree Expansion and Pruning
2.2. Tree Reconfiguration

the convoy tree will be expanded by adding/pruning some
nodes. During the expansion, the original root (same as aggregation node) will not be changed, although it is no longer
closest to the target. When the distance between the current
root and the predicted location of the target exceeds some
threshold dr, a new root will be selected and the convoy
tree will be reconfigured. Expansion and Reconfiguration
will alternate until the target leaves. Given the convoy tree
formed incrementally as above, for each reporting interval,
all the source nodes will pass their data along the convoy
tree to the root, which will aggregate them and generate a
final report for the sink.
The framework of DDCTC is intuitive and elegant: The
root of the convoy tree is expected to be closest to the target
to reduce data collection overhead, however it will consume
a significant amount of energy to reselect the source node
closest to the target as the root and, reconfigure the convoy
tree frequ.ently as the target moves. Clearly, a tradeoffbetween the expensive reconfiguration overhead and the overhead of data collection along the incrementally expanded
convoy tree exists. DCTC deals with this tradeoff by carefully determining the distance threshold d,.
However, adjustinig the convoy tree ini such aniicremental manner might incur more transmission overhead than
required, especially when the target's velocity is fast. As
indicated in [1 1], when the velocity of the target exceeds
1 6m/s, the interval of reconfiguration k(v) has become 1
second, (same as the reporting interval of sensors nodes they
assumed), which means that the convoy tree needs reconfiguration for each reporting interval. Things become worse
if the velocity of the target makes the reconfiguration rate
exceed the reporting rate, where most reconfigurations will
occur between successive reporting intervals and, those reconfigured trees will not be used by source nodes to pass on
data. Thus the reconfiguration overhead will be wasted.
To overcome the impact ofthe target's velocity, the DGT
scheme discretizes the construction of aggregation trees by
taking the following design choices: (1) Use a virtual grid
structure to reduce the choices of aggregation node only to
those grid, nodes. By setting the length of grid cell as 2Rs,
the choice of aggregation node for each source node has
been further restricted to the four local grid nodes (2) For
every reporting interval, each source node selects the local
grid node closest to the (predicted location of) target as aggregation node. The construction of aggregation tree at each
reporting interval is further eased by the local level tree ini

tiated by those limited number of grid nodes proactively.

Therefore, when a source node has selected the aggregation

node, it will know to which neighbor the packet destined to

the aggregation node should be sent. However such advantage of proactive routing cannot be taken by DCTC, since
the root of the convoy tree can be any sensor node. It is
expensive to let each sensor node establish local level trees
proactively.
In the following, we numerically estimate the energy
consumed by DCTC and DGT, and make a comparison
between them. Formulas (I)-(5) are for DCTC, and formulas (6)-(9) are for DGT. The used notations are listed
in Table 3. We give a brief explanation for these formulas (Reader should refer [lLO][l 1] for more details about
DCTC). The two terms in Einit correspond, to the two
phases of initial convoy tree construction of DCTC, where
(d2 d 2) is the number of root candidates generated in the
first phase. r )1 (Eep + Ed(i v)) is the energy consumed for convoy tree expansion and data collection between successive root replacements. Et (k(v) v) + Ed (O)
is the energy for tree reconfiguration and, data collection
when root replacement occurs. The first term of E6p is the
energy consumed by the grid heads (of those grids including the new source nodes) to propagate the join message
during expansion, while the second term is the energy consumed by new source nodes attaching to the expanded convoy tree. Please note that DCTC also divides the sensing
field into grid cells, within each cell a grid head will help
propagate control messages. The side length of their cell is
dc/2. In the comparison, we omit the overhead of grid constructions in both schemes. Similar as E p, the first term in
Et(k(v) v) is the energy consumed by the grid heads (of
those grids including the current source nodes) to propagate
the reconfig message during reconfiguration, while the second tern is the energy consulmed by source nodes detaching
from the old convoy tree and attaching to the new one. The
formulas for DGT have more concise structure. In Epro,
the first term corresponds to the global level tree construction initiated by sink, while the second term corresponds to
local level tree constructions initiated by (a/2d, + 1)2 grid
nodes. We establish an upper bound for the overhead of
data collection in DGT scheme. Since for any source node,
the aggregation node it selects must be one of the four local
grid nodes, the distance between the source node and the
aggregation node it selects will never exceed 2 2d, (the
length of the diagonal of grid, cell). Therefore the number
of transmissions between the source node and the selected
aggregation node will not exceed 22dsldd. This upper
bound is used for the comparison computation Figure 8
shows the ratio of the energy consumed by DCTC to the
energy consumed, by DGT for the configurations froml
d d 30, p 6000 (400
400), a 400,
d,=20- =
W
20 0
50%, and =d s The numerical results have
validated the analysis in the beginning of this setion. The
=

increase of the target's velocity will deteriorate the performance of DCTC, while DGT is immune to that.
+
(1)
EDCTC
Einit
k(v)

(Zk_tl)-l (Fep + Ed(i. v))

(2) Einit
(3) Eep

=

Et(k(v) v) + Ed(O))

_d

V(X+ U)2 +dydw

J

(5) Et(k(v) v) = 7d'/(dc/2)2
(6) EDGT

=

(7) Epr,o = p
(8) Ea,gg

=

Epro +
a

L

e-

S 0+ 2pwd
2P

e SC 0

(Eagg + Ed)

(a/2d, + 1)

sc +

p

(4ds)

Sc

p rds2 e SC

(9) Ed (u) = 2esd
d*
< p

Eep

Spredict/(dC/2) e SC + P* Spredict *e SC
= d2 T(7 + sin
lo), o = 2 arccos(v 2d,)

(4) E(u)

7d2

(x+u)2 y edydS
rd fV/°e dUd2

jd

d

£E/inhito ad to construct initial convoy tre
overhead to expand convoy tree

Ed(u) data collection overhead when the coordinates of

C + (d2ldc)p d eS
rTd2 es

p

Spredict

+

Table 3. Notations

DCTC

'

.(2-\ 2d,d,) C'Sd

target and root are (0, 0) and (=u, 0) respectively
Et (u tree reconfiguration overhead, when distance between old root and new root is u
distance threshold for reconfiguration
dr
v
expected velocity of target
k(v) k(v) d Iv time interval for reconfiguration
e
energy consumed by transmitting a unit of data
for one hop
Spr d the predicted area where those new source nodes
are located during convoy tree expansion (Figure
5(b)in [10])
In Interception-based reconfiguration scheme
0
[I 1], assume only 0 percent of the source nodes
need, to be reconfigured,
T total time for track
p node density

sensing range

d
Sd

d

size of sensnng data

sc

ing

transmission range
size of control message

DGT

Epro overhead to proactively establish global/local

level trees
Eagg overhead to select aggregation node(s) for each
reporting interval
E (u) overhead of data collection in aggregation tree
a
suppose the sensing field is an a a square

Ratio of E(DCTC) to E(DGT)

uFiur

8.

15

16

Iso

between

[v: k(v) ]

Figure 8. INumerical Comparison between
DCTC and DGT

4.

Conclusion and Future Work
This

paper proposes

a

novel data collection scheme for

tracking a mobile target in wireless sensor networks. The
general framework of embedding a virtual grid structure
and restricting the aggregation node choices to grid nodes,
makes the proposed DGT scheme scalable to the mobility
scenario. Under the general frarmework, each source node

is able to select the aggregation node independently with local information, while at the same time agrees to the choice
made by the majority. The effectiveness of DGT scheme
has been validated both theoretically and experimentally.
By discretizing the construction of aggregation trees at
the specified reporting intervals, the DGT scheme consumes
less energy than DCTC, and is immune to the target's velocity. Such nice property could be further enhanced, if the reporting interval of sensor nodes can also intelligently adapt
to the changing environment, such as the target's velocity.
This challenging problem is currently under our study.
References

[11

.F.Akyildiz, W.Su, Y.Sankarasubramaniam, and E.Cayirci,
"Wireless sensor networks survey" Computer Networks
Vol. 38, No. 4, pp. 393-422, Mar.2002
a

[2] C.Intanagonwiwat, R.Govindan, and D.Estrin, "Directed Diffusion: A Scalable and Robust Communication". ACM Mobicom'00, Aug. 2000
[3] F.Ye, H.Luo, J.Cheng, S.Lu and L.Zhang, "A Two-Tier Data
Dissemination Model for Large-scale Wireless Sensor Networks", ACM Mobicom, pp. 148-159, Sep. 2002
[4] D.Estrin, R.Govindan, J.Heidemann, and S.Kumar, "Next
Century Challenges: Scalable Coordination in Sensor Networks", ACM Mobicom'99, pp. 263-270, Aug. 1999
[5] C.Intanagonwiwat, D.Estrin, R.Govindan, J.Heidemanm,
"Impact of Network Density on Data Aggregation in Wireless Sensor Networks", Proceeding of the 22nd International
Conference on Distributed Computing Systems(ICDCS'02)
[6] B.Krishnamachari, D.Estrin, and S. Wicker, "Modelling DataCentric Routing in Wireless Sensor Networks", IEEE INFOCOM'02, June, 2002
[7] A.Civilis, C.S.Jensen, J.Nenortaite, and S.Pakalnis, "Efficient
tracking of moving objects with precision guarantees", IEEE
MobiQuitous'04, pp. 164-173, Aug. 2004
[8] Y.Xu, J.Winter, and W.Lee, "Dual Prediction-based Reporting for Oblect Tracking Sensor Networks", IEEE MobiQuitous '04, pp. 154- 163, Aug. 2004
[9] P.De, K.Basu, and S.K.Das, "An ubiquitous architectural
framework and protocol for object tracking using RFID tags",
IEEE MobiQuitous'04, pp. 174- 182, Aug. 2004
[10] W.Zhang and G.Cao, "DCTC: Dynamic Convoy Tree-Based
Collaboration for Target Tracking in Sensor Networks". IEEE
Transactions on Wireless Communication, Vol. 3, No. 5,
pp. 1689- 1701, Sept. 2004
[1 1] W.Zhang and G.Cao, 'Optimizing Tree Reconfiguration for
Mobile Target Tracking in Sensor Networks", IEEE Infocom,
Vol. 4, pp. 2434-2445, Mar. 2004
[12] M.Garey and D.Johnson, "Computers and Intractability: A
guide to the theory of NP-Completeness", W.H. Freeman,
1979
[13] T.H.Cormen, C.E.Leiserson, R.L.Rivest and C.Stein, "Introduction to Algorithms", The MIT Press, 2001
[14] B.Karp and H.T.Kung, "Greedy Perimeter Stateless Routing
for Wireless Networks", ACM/IEEE MobiCom'00, pp. 243254, August, 2000
[15] V.Ramasubramanian, Z.J.Haas and E.G.Sirer, "SHARP: a
hybrid adaptive routing protocol for mobile ad hoc networks",
ACM Mobihoc, pp. 303-314, June 2003
[L 6] The
Network
Simulator
NS-2,
http://www.isi.edu/nsnam/ns/

On Multipath Routing with Transit Hubs
A. Sen1 , B. Hao1 , B.H. Shen1 , S. Murthy1 , and S. Ganguly2
1

Dept. of Computer Science and Engineering,
Arizona State University, Tempe, AZ 85287-8809, USA
{asen, binhao, bao, sudhi}@asu.edu
2
Dept. of Broadband and Mobile Networks,
NEC Laboratories, USA
samrat@nec-lab.com

Abstract. Empirical studies report frequent occurrences of path failure
in the Internet. In providing resilience to such failures, we propose the
computation of alternate backup end-to-end path that is disjoint to the
default IP path. This disjoint path is created using transit hubs that can
be located at diverse points in the Internet. Transit hubs provide better
utilization of network resources. Assuming an IP layer routing between
any two nodes, we show that the problem of computing such a disjoint
path is NP-complete. We present an exact and a heuristic solution for
the problem. Using routing data obtained from PlanetLab, we evaluate
the eﬃcacy of our heuristic solution.
Keywords: transit hub, disjoint path, multipath routing, network resilience.

1

Introduction

Current Internet routers select only a single path between a source and destination node. The choice of this default IP path is not left to the end hosts,
instead it is left to the Administrative (AS) domain operators or on the BGP
level policies. Often it is desirable for the end hosts to have better control over
the route selected in context of traﬃc engineering and QoS controlled applications. A solution framework that has been gaining immense interest recently in
the research community is using transit hubs. Transit hub routing allows routing
between end hosts via a set of dedicated transit nodes that are placed at diverse
locations on the Internet.
The beneﬁts of transit hub routing are multidimensional. Transit hubs can
forward packets, compute an end-to-end alternate path by chaining a set of
default IP paths and thus facilitate better utilization of network resources. It
provides better control over the load distribution in a network and can route
packets over less congested areas. An interesting application area is bulk data
transfers. Transit caches were deployed on the Internet in experiments [7], which
resulted in transmission speeds of up to 47.6 Mbps during transfers of 3 TB
of data between SanDiego and Urbana-Champaign. Bulk data transfer is not
R. Boutaba et al. (Eds.): NETWORKING 2005, LNCS 3462, pp. 1043–1055, 2005.
c IFIP International Federation for Information Processing 2005


1044

A. Sen et al.

readily supported by current Internet routers and would have been improbable
without transit hubs. Another ongoing research eﬀort in this direction is the
Logistical Networking project [2] in which the transit hubs (called depots) oﬀer
middleware-like storage services that can be used in many applications.
Relying just on the single default IP path may lead to various end-to-end
performance bottlenecks. Experimental studies conducted by authors of detour
indicate that in many cases, alternate paths have better latency and throughput
characteristics than direct default IP paths. Albeit the strong case for alternate
paths, they cannot be directly constructed using existing routers as they do
not provide any rerouting ﬂexibilities. Transit hubs provide an elegant solution
framework (using techniques like IP-in-IP encapsulation [4], overlay networks
[1] or ﬂexible extension headers of IPv6 datagrams) for creating alternate paths
between end hosts and works seamlessly with the existing routers.
There is immense literature on traditional multipath routing problem[3, 10, 8]
(others not provided due to space constraints). The advantages of multipath
routing can be exploited to its fullest extent if the paths are link (or node)
disjoint. Suurballe [10] presented polynomial time algorithms for computation
of a pair of disjoint paths such that the sum of the path lengths is minimum. The
idea of using intermediate nodes to provide a level of indirection in creating an
alternate path was proposed in [1, 5, 12, 2]. These intermediate nodes have been
referred with various nomenclature: as overlay nodes [1, 5], rendezvous points
[12], depots [2], hubs [4] and transit hubs in this paper.
Our ﬁrst contribution is to establish the NP-completeness of the K-transit
hub routing problem. Secondly, we present an exact algorithm for solving the KTransit hub routing problem. Our third contribution is a heuristic based solution
that is eﬀective for large networks. We evaluate the eﬃcacy of our heuristic
through extensive experimentation on PlanetLab’s Abilene network and various
randomly generated topologies.

2

Disjoint Path Routing Using Transit Hubs

The objective of the K-Transit hub routing problem is to ﬁnd out if it is possible
to construct a path from s to d by concatenating at most K + 1 paths (from
the set of n ∗ (n − 1) paths) so that (i) each of these paths is edge-disjoint with
the original s to d path and (ii) the paths are mutually edge-disjoint. In other
words, can we ﬁnd a set of paths {Ps,v1 , Pv1 ,v2 , Pv2 ,v3 , Pv3 ,v4 , . . . , Pvk−1 ,vk , Pvk ,d },
1 ≤ k ≤ K + 1 such that the concatenation of these paths will produce a path
from s to d and condition (i), (ii) above are satisﬁed.
The idea is illustrated with the help of an example overlay network (Fig.
1) obtained from the PlanetLab. The overlay network has ﬁve nodes 1 through
5. The nodes a through j represent routers through which the overlay nodes
establish paths between each other. In this example, the primary path for data
transfer from overlay node 1 to node 4 is through the link 1-4 (path P3 ). If the
following question is asked: “Is it possible to construct an alternate path from
node 1 to 4, disjoint from the default path, by concatenating at most two mutually

On Multipath Routing with Transit Hubs
1

1
0
0
1

1045

2

11
00
00
11
00
11
a
b 11
c 11
d
e
0
1
0
1
00
00
0
1
00
11
00
11
j
0 1
1
011
0011
00 1
0
005
11
00
11
0 1
1
011
0011
00 1
0
00
11
04 h
1
00
11
3 11
g
f
0 1
1
0
1
0
0
1
00
00
0
0
1
0 11
1
00
11
i1
0
1
0
1
0
1
00
11
00
0Nodes in Overlay Network11
1
00 Nodes in Physical Network
11
0
1
Fig. 1. Overlay Network from the PlanetLab (See legend)

Legend: Nodes in Fig 1
1: PlanetLab 1, University of Arizona
2: PlanetLab 2, Carnegie Mellon University
3: PlanetLab 2, Duke University
4: PlanetLab 2, University of Washington
5: PlanetLab 2, Princeton University
a: kscyng-dnvrng.abilene.ucaid.edu
b: iplsng-kscyng.abilene.ucaid.edu
c: chinng-iplsng.abilene.ucaid.edu
d: nycmng-chinng.abilene.ucaid.edu
e: washng-nycmng.abilene.ucaid.edu
f: nycmng-washng.abilene.ucaid.edu
g: chinng-nycmng.abilene.ucaid.edu
h: iplsng-chinng.abilene.ucaid.edu
i: kscyng-iplsng.abilene.ucaid.edu
j: dnvrng-kscyng.abilene.ucaid.edu

Paths connecting overlay nodes
P1: 1 - a - b - c - d - e - 2;
P2: 1 - a - b - c - d - e - 3;
P3: 1 - 4; P4: 1 - a - b - c - d - e - 5;
P5: 2 - f - g - h - i - j - 1; P6: 2 - 3;
P7: 2 - f - g - h - i - 4; P8: 2 - 5;
P9: 3 - f - g - h - i - j - 1;
P10: 3 - 2;
P11: 3 - f - g - h - i - 4; P12: 3 - 5;
P13: 4 - 1;
P14: 4 - a - b - c - d - e - 2;
P15: 4 - a - b - c - d - e - 3;
P16: 4 - a - b - c - d - e - 5;
P17: 5 - f - g - h - i - j - 1;
P18: 5 - 2; P19: 5 - 3;
P20: 5 - f - g - h - i - 4;

disjoint paths?”, the answer to the question is “yes” because such a path can be
constructed by concatenating paths P4 and P20 .

3

Problem Formulation and Complexity Analysis

As indicated earlier, the input to the K-Transit hub routing problem is (i) an
undirected network graph G = (V, E), (ii) a set of n ∗ (n − 1) paths (|V | = n)
between every source-destination node pair (the path from node i to j may notbe
same as the path from j to i) and (iii) speciﬁed source and destination nodes
s and d respectively. The objective of the K-Transit hub routing problem is to
ﬁnd out if it is possible to construct a path from s to d by concatenating at most
K + 1 paths (from the set of n ∗ (n − 1) paths) so that (i) each of these paths
is edge-disjoint with the original s to d path and (ii) the paths are mutually
edge-disjoint.
In order to ﬁnd an answer to this question, we ﬁrst remove all the edges used
by the path from s to d from the graph G = (V, E). Let P be the set of all
n ∗ (n − 1) paths given as the input. After removal of the edges belonging to
the s to d path, many of the paths in P may become disconnected. We refer

1046

A. Sen et al.

to such paths as “unavailable” (Punav ). The other subset of paths in P are the
“available” paths (Pav ).
Note that here we make no attempt to consider future connection requests
and the bandwidth required to satisfy them. It could very well happen that
the alternate path computed by our algorithms may not be able to satisfy a
connection request due to other network traﬃc at that time. In our approach,
we merely try to ascertain whether such an alternate path exists in the network.
The rationale behind this decision is that the K-Transit hub problem by itself
without considering any of these parameters is NP-complete. In order to keep
the problem tenable, we focus on just the computation of an alternate path
by concatenating at most K + 1 paths. In this regard, we do not consider the
capacity of the links in our model.
3.1

Definitions and Notations

Definition 1. Intersection set of paths: The intersection set of two paths Pi and
Pj is the set of edges common between the paths and is denoted by Pi ∩ Pj .
Definition 2. Compatible Paths: Two paths Pi and Pj are said to be compatible
if their intersection set is empty.
Definition 3. Concatenation of Paths: If Pi is a path from si to di and Pj is
a path from sj to dj , they can be concatenated if di = sj and the result of the
concatenation operation is a path from si to dj .
Definition 4. K-Transit Hub Routing Problem
Instance: Given an undirected graph G = (V, E), a set of triples (si , di , Pi ), 1 ≤
i ≤ r, where si is a source node, di is destination node and Pi is a path from si
to di and r is the number of such triples, speciﬁed source, destination nodes s
and d respectively and an integer K.

⊆ Pav such that:
Question: Suppose Pav = {P1 , . . . , Pr }. Is there a subset Pav

(i) | Pav
| ≤K +1


(ii) The paths in Pav
are mutually compatible, i.e., if Pi , Pj ∈ Pav
, then Pi ∩
Pj = ∅, ∀i = j and

(iii) A path from s to d can be constructed by concatenating the paths in Pav
.

3.2

Complexity Analysis

Theorem. The K-Transit Hub Routing Problem is NP-Complete.
Proof. It is not diﬃcult to verify that the K-Transit hub routing problem is in
NP. We show that the K-Transit Hub Routing Problem is NP-complete by a
polynomial transformation from the 3SAT problem. From a given instance of
the 3SAT problem, speciﬁed by a set of variables X = {x1 , . . . , xn } and a set

On Multipath Routing with Transit Hubs

1047

of clauses C = {C1 , . . . , Cm }, we construct an instance of the K-Transit Hub
Routing Problem in the following way. First, we classify each edge in the graph
G = (V, E) as a path-edge or a non-path-edge.
Definition 5. An edge (u, v) ∈ E is called a path-edge, if ∃Pi = (u, v) where
Pi ∈ Pav . Otherwise, the edge is known as a non-path-edge.
It may be noted that a path between a source-destination node pair may
comprise of both of these types of edges. The instance (G, Pav , s, d, K) of the
K-Transit hub routing problem can be generated from the instance of the 3-SAT
problem in three steps: (i) We construct a subgraph G1 of G. (ii) Paths in Pav
consisting of more than one edge are speciﬁed. (iii) We augment G1 with additional nodes and edges to construct G. It may be noted that all paths in Pav
consisting of exactly one edge are speciﬁed in (i) and (iii), and all paths with
more than one edge are speciﬁed in (ii).
Step 1. ∀xi ∈ X and ∀Cj ∈ C, construct a 4-node subgraph with node set


} and edge set {(ui,j , ui,j ), (vi,j , vi,j
)}, where both edges are
{ui,j , ui,j , vi,j , vi,j
path-edges. For each ﬁxed xi , ∀j = 1, . . . , m − 1, we connect the subgraph
for xi , Cj and the one for xi , Cj+1 with two non-path-edges (ui,j , ui,j+1 ) and

, vi,j+1 ). Then for each xi , we add six more vertices: ai , bi , ci , ai , bi and ci .
(vi,j
We connect ai , bi , ci with path-edges (ai , bi ) and (ai , ci ) and connect ai , bi , ci with
path-edges (ai , bi ) and (ai , ci ). For each xi , we add four more non-path-edges:

, ci ). In addition, ∀i = 1, . . . , n − 1, we add
(bi , ui,1 ), (ci , vi,1 ), (ui,m , bi ) and (vi,m

a path-edge (ai , ai+1 ) to connect the subgraphs corresponding to xi and xi+1 .
If the instance of the 3SAT problem is given by φ = (x̄1 ∨ x̄2 ∨ x̄3 ) ∧ (x̄1 ∨ x2 ∨
x3 ) ∧ (x1 ∨ x2 ∨ x3 ) ∧ (x̄1 ∨ x2 ∨ x̄3 ), then the graph G1 corresponding to φ is
shown in Fig. 2.

x1
C4
C3
C2
C1

a'1

x1

b'1

x2

a'2

x2

a'3

u'1,4

v'1,4

u'2,4

v'2,4

u'3,4

v'3,4

u1,4

v'1,4

u2,4

v'2,4

u3,4

v'3,4

u'1,3

v'1,3

u'2,3

v'2,3

u'3,3

v'3,3

u1,3

v'1,3

u2,3

v'2,3

u3,3

v'3,3

u'1,2

v'1,2

u'2,2

v'2,2

u'3,2

v'3,2

u1,2

v'1,2

u2,2

v'2,2

u3,2

v'3,2

u'1,1

v'1,1

u'2,1

v'2,1

u'3,1

v'3,1

u1,1

v 1,1

u2,1

v 2,1

u3,1

v 3,1

b1

c1

b2

c2

b3

c3

a2

x1

x3

b'2

a1

c'2

x3

c'1

b'3

c'3

a3

Path-Edges
Nonpath-Edges

Fig. 2. Subgraph G1 of G

C4
C3
C2
C1

a'1

x1

b'1

x2

a'2

x2

a'3

x3

b'2

u'1,4

v'1,4

u'2,4

v'2,4

u'3,4

v'3,4

u1,4

v'1,4

u2,4

v'2,4

u3,4

v'3,4

u'1,3

v'1,3

u'2,3

v'2,3

u'3,3

v'3,3

u1,3

v'1,3

u2,3

v'2,3

u3,3

v'3,3

u'1,2

v'1,2

u'2,2

v'2,2

u'3,2

v'3,2

u1,2

v'1,2

u2,2

v'2,2

u3,2

v'3,2

u'1,1

v'1,1

u'2,1

v'2,1

u'3,1

v'3,1

u1,1

v 1,1

u2,1

v 2,1

u3,1

v 3,1

b1

c1

b2

c2

b3

c3

a1

c'2

x3

c'1

b'3

a2
Path-Edges
Nonpath-Edges

c'3

a3
Long Path

Fig. 3. Long paths consisting of more
than one edge

1048

A. Sen et al.

Step 2. In this step, we specify all the paths in Pav consisting of more than one
edge. ∀i = 1, . . . , n, a path between bi and bi : Pbi = bi − ui,1 − ui,1 . . . − ui,m − bi


and a path between ci and ci : Pci = ci − vi,1 − vi,1
. . . − vi,m
− ci are added
into Pav . For the example used in Step 1, the paths speciﬁed in this step are
highlighted in Fig. 3.
Step 3. This step has two parts. First, a set of nodes {s, w0 , w1 , . . . , wm , d} is
added to the graph G1 (Recall that m is the number of clauses in the 3SAT
instance). Second, a set of path-edges is added as follows: (i) connect s and
w0 by a path-edge (s, w0 ), connect an and d by a path-edge (an , d) (ii) ∀i =
1, . . . , n, ∀j = 1, . . . , m, if xi ∈ Cj , then add (wj−1 , ui,j ), (ui,j , wj ) and if x̄i ∈ Cj ,

, wj ) (iii) connect wm to a1 by a path-edge (wm , a1 ).
then add (wj−1 , vi,j ), (vi,j
This completes the construction procedure of G with all of paths in Pav . The
resulting graph G is shown in Fig. 4.
d

x1
C4
C3
C2
C1

a'1

x1

b'1

x2

c'1

b'2

u'1,4

v'1,4

u'2,4

u1,4

v'1,4

u2,4

u'1,3

v'1,3

u'2,3

u1,3

v'1,3

u2,3

u'1,2

v'1,2

u'2,2

u1,2

v'1,2

u2,2

u'1,1

v'1,1

u'2,1

u1,1

v 1,1

u2,1

b1

c1

b2

a'2
w4

w3

w2

w1

w0

a1

x2

c'2

x3

d

a'3

x3

b'3

c'3

v'2,4

u'3,4

v'3,4

v'2,4

u3,4

v'3,4

v'2,3

u'3,3

v'3,3

v'2,3

u3,3

v'3,3

v'2,2

u'3,2

v'3,2

v'2,2

u3,2

v'3,2

v'2,1

u'3,1

v'3,1

v 2,1

u3,1

v 3,1

c2

b3

c3

a2
s

x1

a3

C4
C3
C2
C1

a'1

x1

b'1

x2

c'1

b'2

u'1,4

v'1,4

u'2,4

u1,4

v'1,4

u2,4

u'1,3

v'1,3

u'2,3

u1,3

v'1,3

u2,3

u'1,2

v'1,2

u'2,2

u1,2

v'1,2

u2,2

u'1,1

v'1,1

u'2,1

u1,1

v 1,1

u2,1

b1

c1

b2

a'2
w4

w3

w2

w1

w0

a1

x2

c'2

x3

a'3

x3

b'3

c'3

v'2,4

u'3,4

v'3,4

v'2,4

u3,4

v'3,4

v'2,3

u'3,3

v'3,3

v'2,3

u3,3

v'3,3

v'2,2

u'3,2

v'3,2

v'2,2

u3,2

v'3,2

v'2,1

u'3,1

v'3,1

v 2,1

u3,1

v 3,1

c2

b3

c3

a2

a3

s
Path-Edges
Nonpath-Edges

Fig. 4. Graph G

Path-Edges
Nonpath-Edges

Fig. 5. The transit hub path corresponding to the truth assignment of φ

Set s, d to be the source node and destination node respectively in graph
G = (V, E).Set K=|E|. Construction of the instance of the K-Transit hub problem is now complete.
Claim: There exists a truth assignment satisfying the instance of the 3SAT problem, if and only if a path from s to d can be constructed in the generated instance
of the K-Transit hub routing problem by concatenating at most K + 1 mutually
compatible paths.
Proof of the claim: Suppose that there is a truth assignment satisfying the instance of the 3SAT problem. We can construct a path from s to d by concatenating a subset of paths in the following way: (i)Go from s to w0 following the

On Multipath Routing with Transit Hubs

1049

path-edge between them. (ii)Each Cj , j = 1, . . . , m, has at least one literal, z that
has been assigned “true” by the truth assignment. This implies that we can go
from wj−1 to wj using the corresponding path-edges (i.e., wj−1 − ui,j − ui,j − wj

or wj−1 − vi,j − vi,j
− wj ). (iii)Go from wm to a1 using the path-edge between
them. (iv)If x1 = “true”, then no edge on the path from c1 to c1 has been used
so far; otherwise, if x1 = “false”, then no edge on the path from b1 to b1 has
been used yet. Hence, we can go from a1 to a1 using one of the following two
sequences of paths: if the path from b1 to b1 is unused, then take (a1 , b1 ), path
from b1 to b1 , and then (b1 , a1 ); if the path from c1 to c1 is unused, then take
(a1 , c1 ), path from c1 to c1 , and then (c1 , a1 ). (v)∀i, 1 ≤ i ≤ n − 1, go from ai
to ai+1 , using the path-edge between them. (vi)∀i, 2 ≤ i ≤ n go from ai to ai
following the same process as in step (iv). (vii)Go from an to d following the
path-edge between them. Thus, we ﬁnd a s − d path, which is a concatenation
of a sequence of mutually compatible paths.
To prove the converse, suppose that we can go from s to d by concatenating
a sequence of mutually compatible paths. It is not hard see that we must ﬁrst go
from s to wm by following the path-edges incident to wj ’s. Then, from wm , we
have to go through each subgraph corresponding to xi ’s, from ai to ai , by using
the long paths from bi to bi , or the ones from ci to ci . ∀i = 1, . . . , n, if the path
from bi to bi is used, then assign xi to be “false” and if the path from ci to ci is
used, then assign xi to be “true”. It is not hard to check this assignment satisﬁes the corresponding 3SAT problem. This completes the proof of the theorem.2
In the sample 3SAT instance φ considered in Steps 1, 2, and 3, the truth
assignment f (x1 ) = F ALSE, f (x2 ) = T RU E, f (x3 ) = T RU E satisﬁes φ. The
corresponding s − d path is shown in Fig. 5

4

Exact Solution for the K-Transit Hub Routing
Problem

In this section, we provide an exact algorithm for the solution of the K-Transit
hub routing problem. As a ﬁrst step in that direction, we ﬁrst construct a Path
Intersection Graph (PIG).
Definition 6. A Path Intersection Graph is the intersection graph of paths in
the set Pav . This is a graph Gpig = (Vpig , Epig ), where each node represents a
path in the set Pav and two nodes have an edge between them, if the corresponding
paths have any common edge.
Definition 7. An independent set (or a stable set) in a graph G = (V, E) is a
subset V  ⊆ V , such that no two nodes in V  are adjacent to each other in the
graph G = (V, E).
Definition 8. An independent set in a graph G = (V, E) is called a maximal
independent set if it is not a proper subset of any other independent set in the
graph.

1050

A. Sen et al.

As a second step towards construction of the alternate s to d path, we compute all the maximal independent sets of the path intersection graph. The maximal independent sets of the path intersection graph will correspond to the sets
of maximal compatible paths in Pav . Let {M IS1 , M IS2 , . . .} represent the set
of maximal independent sets of the path intersection graph.
As a third step in the process to construct an alternate s to d path, we construct a Path Construction Graph corresponding to each maximal independent
set M IS1 , M IS2 , . . . , M ISt computed in the previous step.
Definition 9. Each node in a Path Construction Graph corresponding to a
M ISi , 1 ≤ i ≤ t, Gpcg (i) = (Vpcg (i), Epcg (i)), corresponds to a path in M ISi and
two nodes have an edge between them if the corresponding paths have a common
terminating point, i.e., if the terminating points of a path are vi , vj and the
terminating points of another path are vk , vj , then the nodes corresponding to
these two paths will have an edge between them in the graph Gpcg (i).
Let Vpcg (i, s) = {vs,1 , vs,2 , . . . , vs,p } denote the set of nodes that correspond to
paths whose one terminating point is the designated source node s. Similarly, let
Vpcg (i, d) = {vd,1 , vd,2 , . . . , vd,q } denote the set of nodes that correspond to paths
whose one terminating point is the designated destination node d. Now in the
graph Gpcg (i), we compute the shortest path between the nodes vs,j , 1 ≤ j ≤ p
and vd,k , 1 ≤ k ≤ q. If any of these paths have length at most K + 1, then it
is possible to construct an alternate path from s to d, disjoint from the original
path Ps,d in the graph G = (V, E), by concatenating compatible paths in the
set Pav . This process of building a path construction graph Gpcg (i) from M ISi
followed by the computation of shortest path needs to be repeated ∀i, 1 ≤ i ≤ t,
where t is the number of maximal independent sets. If a shortest path of length
at most K + 1 cannot be found in any one of these graphs Gpcg (i), 1 ≤ i ≤ t,
then it is impossible to construct an alternate path from s to d, disjoint from the
original path Ps,d in the graph G = (V, E) by concatenating compatible paths
in the set Pav .
4.1

Algorithm Analysis

The algorithm ﬁrst computes the Path Intersection Graph of the set of available
paths Pav and then computes all maximal independent sets of this graph. The
maximal independent sets give the set of compatible paths that can be concatenated for constructing the path from the source s to destination d. In step 5 of
the algorithm the Path Construction Graph is constructed and in step 7, the
shortest path between a vi,s and vi,d is computed. Since the process is repeated
for all maximal independent sets that contains a vi,s and vi,d and for all vi,s
and vi,d , if a path between s to d can be obtained by concatenating at most
K + 1 compatible paths in the set Pav , this process will ﬁnd it. This ensures the
correctness of the algorithm.
For generating all maximal independent sets of a graph, algorithms such as
the ones presented in [11] and [6] can be used. Both the algorithms produce
the maximal independent sets one after another in such a way that the delay

On Multipath Routing with Transit Hubs

1051

Algorithm 1 . K-Transit Hub Routing Exact Solution(G, Pav , s, d, K)
step 1 Compute Path Intersection Graph, Gpig = (Vpig , Epig ) for the paths in Pav .
=
step 2 Compute all Maximal Independent Sets of Gpig , MIS
{M IS1 , M IS2 , . . . , M ISt }.
step 3 Compute a subset MIS  ⊆ MIS, such that all elements of MIS  , contain
at least one path whose terminating point is s and another path whose terminating point is d.
step 4 Repeat steps 5-7 for each elements M ISi of MIS  ,
step 5 Compute the Path Construction Graph Gpcg (i) corresponding to M ISi
step 6 Let Vi,s be the set of nodes in Gpcg (i) that corresponds to those paths whose
one terminating point is s and Vi,d be the set of nodes in Gpcg (i) that corresponds to those paths whose one terminating point is s. Repeat step 5 for
each element vi,s ∈ Vi,s and for each element vi,d ∈ Vi,d
step 7 Compute the shortest path from vi,s to vi,d . If the shortest path length is at
most K + 1, then an alternate path from s to d using compatible paths from
the set Pav exists. EXIT from the loop.
step 8 If no path of length at most K + 1 can be found in any of the combinations of
vi,s and vi,d , then an alternate path from s to d using compatible paths from
the set Pav does not exist.
step 9 EXIT

between generation of two consecutive maximal independent sets is bounded
by a polynomial function of the input size. The computation complexity of the
algorithm in [11] is O(n ∗ m ∗ α) and the algorithm in [6] is O(n3 ∗ α) where n, m
and α represents the number of nodes, edges and the maximal independent sets
of the graph respectively. We use the algorithm in [6] for generating all maximal
independent sets in step 2 of the K-Transit hub routing algorithm.
Let α, β represent the number maximal independent sets of the path intersection graph and the paths (i.e. |Pav |) respectively. The worst case computational
complexity of step 1 of the algorithm is O(β 2 ), step 2 is O(β 3 ∗ α) and step 3 is
O(β 2 ∗ α). Thus, the overall complexity of the algorithm is O(α ∗ β 4 ).

5

Heuristic Solution for the K-Transit Hub Routing
Problem

The main overhead involved in the exact algorithm is in the computation of all
the maximal independent sets of the path intersection graph. In this section, we
present a heuristic solution using randomization technique for the K-Transit Hub
Routing problem that produces a solution with high probability. The complexity
of the solution is bounded by a polynomial function of the number of nodes in
the overlay network.
5.1

Complexity Analysis

As in the exact algorithm, the heuristic solution starts by determining the Path
Intersection Graph of the available paths Pav . However, instead of ﬁnding all

1052

A. Sen et al.

Algorithm 2 . Heuristic for K-Transit Hub Routing Problem (G, Pav , s, d, K)
step 1 Compute Path Intersection Graph Gpig = (Vpig , Epig ) for paths in Pav .
step 2 Compute set of nodes, Vs ∈ Vpig that correspond to the paths whose one
terminating point is s and Vd ∈ Vpig as nodes that correspond to the paths
whose one termination point is d.
step 3 Repeat steps 4 through 7 for every node-pair (vs , vd ) ∈ Vs × Vd .
step 4 Construct a maximal independent set with two nodes vs and vd , MIS =
{vs , vd }
step 5 Let N N S(S),
 the “Non-Neighborhood Set” of S be deﬁned as N N S(S) =
Vpig \(N (S) S), where N (S) represents the neighborhood set of S. Select
with equal probability anode v ∈ N N S(S). Augment the maximal independent set, MIS = MIS {v}.
step 6 If MIS is not a maximal independent set, go back to step 5. Otherwise, form
the Path Construction Graph Gpcg . Compute Vi,s , Vi,d ∈ V (Gpcg ) as the set of
nodes corresponding to paths having one terminating point in s, d respectively.
step 7 Compute the shortest path between every pair vi,s ∈ Vi,s and vi,d ∈ Vi,d . If
there exists a shortest path of length at most K + 1 between any vi,s ∈ Vi,s
and vi,d ∈ Vi,d , then an alternate path from s to d using compatible paths
from the set P exists. EXIT from the loop.
step 8 If none of the combinations of vs and vd report a path of length at most K + 1,
then there is no alternate path from s to d using compatible paths from the
set Pav .
step 9 EXIT

maximal independent sets involving the two nodes(paths) vs and vd that terminate in s and d respectively, the algorithm randomly generates a maximal
independent set for each pair-wise combination of vs and vd . The random generation procedure ﬁrst includes the two nodes vs and vd into a working set
MIS of independent nodes. It then randomly selects a node from all the remaining non-neighboring nodes of MIS in the Path Intersection Graph and
includes it into MIS. This process is continued until MIS is maximally independent.
Let β is the number of nodes in the Path Intersection Graph. Step 1 has
worst case computational complexity of O(β 2 ). Steps 5 and 6 perform O(β 2 )
operations in the worst case to compute a Maximal Independent Set. Step 7 of
the algorithm performs O(β 2 ) operations to check if there exist compatible paths
in the Path Construction Graph between the source node and the destination
node. Thus, the overall complexity of the algorithm is O(β 4 ).
5.2

Performance of the Heuristic Solution

To evaluate the performance of our proposed exact and heuristic solutions, we
conducted experiments for the K-transit hub routing problem on randomly generated topologies and the Abilene network.

On Multipath Routing with Transit Hubs

1053

The problem instances were generated in 3 steps:
Step 1. Georgia-Tech Internet Topology Model topology generator was used to
generate the random physical layer topologies having 30 nodes and average node
degree varying between 2 and 6.
Step 2. A subset of these nodes were randomly chosen with uniform distribution
as the set of overlay nodes.
Step 3. Shortest Paths using Dijkstra’s algorithm between every pair of the overlay nodes were computed. These act as the primary paths in our experiments.
One of the metrics used for the evaluation of the performance of the heuristic
is the success ratio. Success ratio is the ratio of number of source-destination
pairs for which a path was found by the algorithm to total number of sourcedestination pairs.
Three sets of experiments were conducted to study the performance of the
heuristic solutions. In the ﬁrst set (6(a)), the number of overlay nodes were
varied from 3 to 7 and success ratio of both the exact and the heuristic solutions
were measured for all source-destination pairs. The value of K was chosen to be
greater than the number of overlay nodes. In the second set of experiments 6(b),
diﬀerent physical topologies consisting of 30 nodes were chosen with varying
average node degrees. In each case, 6 nodes were chosen to be overlay nodes and
the success ratio of the exact and heuristic algorithms were measured for all the
30 source-destination pairs. The aim of this experiment was to study the impact
of the average node degree on the performance of the algorithm. The third set
of experiments (7) were conducted with two data sets. For various values of K,
the success ratio of both the algorithms were recorded. The physical topology
had 30 nodes with an average node-degree of 4 and the overlay structure had 7
nodes.
100%

Success
Ratio

100%

Exact Sol.

80%

Heuristic Sol.

60%

Success
Ratio

40%

20%

20%
3

4

5

6

Number of Overlay Nodes

(a)

7

Heuristic Sol.

60%

40%

0%

Exact Sol.

80%

0%

2

3
4
5
Average Node Degree

6

(b)

Fig. 6. Performance of the Heuristic Solution, (a) Success Ratio vs. Number of overlay
nodes; (b) Success ratio vs. Average node degree

In most of the cases, the success ratio of the heuristic was close to the exact
algorithm. Increasing average node-degree in the physical topology (6(b)) has a
positive eﬀect on ﬁnding alternate paths in the overlay. The success ratio for both
the heuristic and exact algorithms increase with increased average node-degree.
The results (7(a), (b)) indicate that the performance of the heuristic solution
is not signiﬁcantly dependent on the value of K, the number of paths that are

1054

A. Sen et al.
100%

Success
Ratio

Heuristic Sol.

Success
Ratio

60%
40%

Exact Sol.

80%

Heuristic Sol.

60%
40%

20%
0%

100%

Exact Sol.

80%

20%
3

4

5

Value of K

(a)

6

0%

3

4

5

6

7

Value of K

(b)

Fig. 7. Performance of the Heuristic Solution, (a) Success ratio vs. Value of K for
instance 1; (b) Success ratio vs. Value of K for instance 2

allowed to be concatenated to construct the source to destination path. In all
these experiments, the execution times of the heuristic and exact solution were
noted. In many instances, the execution time of the exact solution was almost
1000 times more than that of the heuristic. We thus conclude that our heuristic
technique almost always produces a very high quality solution in a fraction of
time needed to ﬁnd the exact solution.

6

Conclusion

In this paper, we consider the problem of computing an alternate path that is
disjoint to the default IP path. Such an alternate path can be computed by
exploiting transit hubs placed at opportunistic locations on the Internet. We
show that the problem of ﬁnding such a path with constraint on the number
of transit hubs is NP-complete. We provide an exact and approximate solution
for the problem. Our experimentations demonstrate that our heuristic produces
near optimal solution for most of the instances in a fraction of time needed to
ﬁnd the optimal solution.

References
1. D. Anderson, H. Balakrishnan, M. Kaashoek and R. Morris, “Resilient Overlay
Networks,” In Proc. 18th ACM SOSP, Canada, October 2001.
2. M. Beck, J. Dongarra, J. Plank and R. Wolski, Logistical Network Project,
http://loci.cs.utk.edu/scidac.
3. I. Cidon, R. Rom and Y. Shavitt, Analysis of Multi-path Routing, IEEE/ACM
Trans. on Networking, vol. 7, no. 6, pp. 885-896, 1999.
4. R. Cohen and G. Nakibli, On the computational complexity and eﬀectiveness of
“N-hub shortest path routing”, Proc. of IEEE Infocom, 2004.
5. N. Feamster, D. Anderson, H. Balakrishnan and M. Kaashoek, “Measuring the
Eﬀects of Internet Path Faults on Reactive Routing,” In Proc. of ACM SIGMETRICS, San Diego, CA, June 2003.
6. D. S. Johnson, M. Yannakakis and C. H. Papadimitriou, On Generating All Maximal Independent Sets, Information Processing Letters, vol 27, pp. 119-123, 1988.

On Multipath Routing with Transit Hubs

1055

7. T. Kosar, G. Kola and M. Livny, A Framework for Self-optimizing, Fault-tolerant,
High Performance Bulk Data Transfers in a Heterogenous Grid Environments,
Proc. ISPDC 2003.
8. S. Lee and M. Gerla, Split Multipath Routing with Maximally Disjoint Paths in
Ad-hoc Networks, Proc. of IEEE ICC 2001.
9. C. Perkins, “IP encapsulation within IP,” IETF RFC 2003, October 1996.
10. S. Suurballe and R. Tarjan, A Quick Method for Finding Shortest Pair of Disjoint
Paths, Networks, vol. 14, pp. 325–336, 1984.
11. S. Tsukiyama, M. Ide, H. Ariyoshi and I. Shirakawa, A New Algorithm for Generating All Maximal Independent Sets, SIAM Journal of Computing, vol. 6, pp.
505-517, 1977.
12. S. Z. S. S. I. Stoica, D. Adkins and S. Surana, “Internet indirection infrastructure,”
in Proceedings of ACM SIGCOMM 2002, August 2002.

Fault-Tolerant Design of Wireless Sensor
Networks with Directional Antennas
Shahrzad Shirazipourazad1, Arunabha Sen1 , and Subir Bandyopadhyay2
1

School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85281, USA
{sshiraz1,asen}@asu.edu
2
School of Computer Science
University of Windsor
Windsor, ON N9B 3P4, Canada
subir@uwindsor.ca

Abstract. A tree structure is often used in wireless sensor networks to
deliver collected sensor data to a sink node. Such a tree can be built
using directional antennas as they oﬀer considerable advantage over the
omni-directional ones. A tree is adequate for data gathering from all
sensor nodes as long as no node in the tree fails. Since the connectivity
of the tree is one, failure of any one node disconnects the tree and may
disable the sink node from collecting data from some of the sensor nodes.
In this paper we study the problem of enhancing the fault tolerance capability of a data gathering tree by adding a few additional links so that
the failure of any one sensor would not disconnect the tree. Assuming
that the addition of each link to the tree involves some cost, we study
the problem of least-cost augmentation of the tree, so that even after
failure of a single node, all the surviving nodes will remain connected
to the sink node. We prove that the least-cost tree augmentation problem is NP-complete. Moreover, we provide an approximation algorithm
with performance bound of two. The experimental evaluations of the algorithm demonstrate that the approximation algorithm performs even
better in practice and almost always produces near-optimal solution.

1

Introduction

The primary goal of a wireless sensor network is to deliver the data collected
by the sensor nodes to the sink node, possibly after some data aggregation.
Generally, each sensor node has two components: the sensing component and the
communication component. The sensing component gathers information from
the surrounding area and the communication component transmits it to some
other node for further processing. Most often, the data collection operation from
all the sensor nodes is carried out by creating a tree topology that spans all the
sensor nodes, with the sink node as the root [1, 2].
Directional antennas oﬀer substantial advantages over their omni-directional
counterparts, as they can focus their transmission energy in a speciﬁc direction,
D. Frey et al. (Eds.): ICDCN 2013, LNCS 7730, pp. 133–147, 2013.
c Springer-Verlag Berlin Heidelberg 2013


134

S. Shirazipourazad, A. Sen, and S. Bandyopadhyay

using a narrow beam of width α. A directional antenna can be mounted on a
swivel and can be oriented towards a target or alternately each sensor can be
equipped with multiple antennas, each occupying a sector with beam width α.
The transmitted signal disperses in any unguided wireless media and as a consequence, the signal strength diminishes with distance. Although attenuation is in
general a complex function of the distance and the makeup of the environment
through which the signal propagates, a signiﬁcant cause of signal degradation
is free space loss. Free space loss for an ideal isotropic antenna is measured as
2
)
the ratio of the transmitted power to the received power and is given by (4πd
λ2 .
where λ is the carrier wavelength, and d is the propagation distance between
transmission and reception antennas. In particular, the energy required by an
antenna to reach all nodes within its transmission radius is proportional to the
area covered. Thus, an omni-directional antenna with a transmission radius r
will consume power proportional to πr2 (the area of a circle with radius r) while
a directional antenna with beam width α radians will consume power proportional to α2 r2 . The expression is valid under the assumption that the signal is
transmitted over the primary lobe and the power consumed by the remaining
lobes is negligible [3]. The expressions show that with a directional antenna with
α
. Moreover, in
beam width α, power consumption can be reduced by a factor of 2π
comparison with omnidirectional antennas, the directional antennas have significantly less interference and fading [4, 5]. For additional information on antenna
theory, we refer the reader to [6].
Due to such attractive features, sensors with directional antennas are being
increasingly used for wireless sensor networks. Some examples include camera
networks for vision-based sensing, radar networks for weather monitoring and
sonar network for underwater object detection [5]. With rapid advances in the
miniaturization of directional antenna technology, it is likely that the use of
directional antennas in sensor platforms will proliferate. This trend is demonstrated by the increasing interest in the use of directional antennas for performance improvement in wireless networks in general and wireless sensor networks
in particular [4].
Just as the directional antennas oﬀer a number of advantages, it also introduces a few problems. When sensor nodes use omni-directional antennas, the
network topology typically is a mesh and not a tree. A tree that spans over this
mesh topology is utilized for the purpose of data gathering. Although it may
appear that the use of omni-directional antenna for the purpose of data gathering is wasteful, as many of the network links created by such antennas are never
utilized, this is not completely true. The unused links essentially introduce a
certain level of redundancy that can be utilized when one or more of the sensor
nodes fail and the data gathering spanning tree becomes disconnected.
One advantage of the directional antennas in the sensor application is that
it can build the data gathering tree directly, instead of ﬁrst creating a mesh
network and then constructing a data gathering spanning tree for it (as is done
with omni-directional antennas). The tree constructed with directional antennas
is more eﬃcient because it does not have the redundant links that are created but

Fault-Tolerant Design of Wireless Sensor Networks

135

not used by the omni-directional antennas. However, the negative aspect of this
lack of redundancy is that it can no longer deal with a fault scenario, where one
or more sensor nodes fail. Without any built-in redundancy, when some nodes
fail, the data gathering tree is disconnected and the sink node fails to receive
any data from some of the sensors.
The primary motivation of our work is to retain the advantage of both types
of antennas by combining the eﬃciency of directional antennas with the redundancy of omni-directional ones. Speciﬁcally, our objective is to ensure that the
failure of any one sensor node would not prohibit the surviving nodes from communicating with the sink node. We consider that the sensor nodes are equipped
with directional antennas and nodes p and q can communicate with each other if
antennas of p and q direct their beams to each other. In this case a bidirectional
link is used between the nodes p and q. Fig. 1 shows a data collection tree with
two sensor nodes u and v and a sink r. The sectors show the communication
ranges and the lines show the wireless links. The addition of an edge between two
nodes p and q in the sensor network topology corresponds to the deployment of
two new directional antennas at the nodes p and q directed towards each other.

r

u

v

Fig. 1. A data collection tree constructed by directional antennas

Most of the prior studies on fault tolerant sensor network design [7–9] focus
their attention to sensor nodes with omni-directional antennas. The primary goal
of these studies is to compute transmission power to the nodes, so that the power
consumption is minimized, subject to the constraint that the resulting network
is k-connected. In a k-connected (i.e., k − 1 fault-tolerant) network, there are
k disjoint paths between every pair of nodes. However, for data collection in
sensor networks, it is not necessary that the network be k connected. As long
as every sensor node has k disjoint paths to the sink node, failure of k − 1
sensors nodes can be tolerated. It may be noted that we assume that the sink
node does not fail, i.e., it is more reliable than other sensor nodes. In [10, 11]
the authors studied the problems of all-to-one and one-to-all k-fault-tolerant
topology control problems. In these study also the sensor nodes are assumed to
have omni-directional antennas.
In this paper we study the case of single node failure of sensor networks,
where each node is equipped with directional antenna(s). Even for this restricted
scenario, the problem turns out to be computationally hard. Assuming that the

136

S. Shirazipourazad, A. Sen, and S. Bandyopadhyay

addition of each link to the tree involves a cost, our objective is to solve the
problem of least-cost augmentation of the tree, so that, even after the failure of a
single node, all the surviving nodes will remain connected to the sink node. We
will call this the Tree Connectivity Augmentation (T CA) problem. We prove that
the T CA problem is NP-complete and we provide an approximation algorithm,
with a performance bound of two. Experimental evaluation of the algorithm
demonstrates that it performs even better in practice and almost always produces
near-optimal solution.
In the theoretical computer science community, problems of this vein are
known as the graph augmentation problems. Two important problems in this
class are the bi-connectivity augmentation (BICA) and the bridge-connectivity
augmentation (BRCA) (a bridge is deﬁned to be an edge whose removal disconnects the graph) [12]. Although at a ﬁrst glance, it may appear that T CA is the
same as BICA or BRCA, we demonstrate through the example shown in Fig.
2 that T CA is distinctly diﬀerent from both BICA and BRCA. The solid lines
in Fig. 2 are the existing edges in the input graph. A few of the edges that may
be added to the graph are shown in dashed lines and cost of each of these edges
is 1. The cost of the edges that can be added to the graph, but not shown in
dashed lines, is 10. In Fig. 2(a), the solution of the T CA is the addition of edges
{(a, b), (c, d)} with a total cost of 2. However, for BICA, more edges are needed
and as such the total cost will be at least 12. The Fig. 2(b) shows an example in
which T CA has a solution with cost 3, (addition of edges {(g, h), (h, i), (i, j)})
but BRCA requires the addition of edges {(a, e), (g, h), (i, j), (d, f )} with a total
cost of 4.

r

r

a

b
e

a

b c
(a)

d

g

d

c
f

h i

j

(b)

Fig. 2. Comparison of TCA in single fault model with (a) BICA (b) BRCA

Our presentation of the rest of the paper follows the following structure. In
Section 2 we formally deﬁne the T CA problem and prove it to be NP-complete. In
Section 3 we present an approximation algorithm for T CA with a performance
bound of two. In Section 4 we report the results of our experimental evaluation of the approximation algorithm. We conclude by making a few remarks in
Section 5.

Fault-Tolerant Design of Wireless Sensor Networks

2

137

Computational Complexity

In Section 1 we have indicated that our goal is to ensure that the data gathering
tree in a sensor network with directional antennas do not get disconnected due
to the failure of any one sensor node. This objective can be realized by ensuring
that every node (except the root and its adjacent nodes) has an alternate path
to the root (diﬀerent from the path that exists in the data gathering tree). We
formally state the problem below.
Definition: u−fault tolerant graph: A graph G = (V, E) with a speciﬁed
vertex u ∈ V is said to be u−fault tolerant if after the failure of any one node
v ∈ V − {u}, any residual node w ∈ V − {v} remains connected to node u.
Tree Connectivity Augmentation Problem (T CA)
Instance: Complete undirected graph G = (V, E), weight function c(e) ∈ Z+ ,
∀e ∈ E, a spanning tree T1 = (V, E1 ) of G rooted at some node r ∈ V , and a
cost budget B.
aug
⊆ E − E1 , such that the graph (V, E1 ∪ E aug ) is
Question: Is there a set
E
r−fault tolerant and e∈E aug c(e) ≤ B?
We show that the T CA problem is NP-complete, by a transformation from
the 3-dimensional matching, which is known to be NP-complete [13].
3-Dimensional Matching (3DM)
Instance: A set M ⊆ W × X × Y , where W , X and Y are disjoint sets having
the same number q of elements.
Question: Does M contain a matching, that is, a subset M  ⊆ M such that
|M  | = q and no two elements of M  agree in any coordinate?
Theorem 1. T CA is NP-complete.
Proof. Let M ⊆ W × X × Y be an instance of 3DM, with |M | = p and W =
{wi |i = 1, 2, . . . , q}, X = {xi |i = 1, 2, . . . , q} and Y = {yi |i = 1, 2, . . . , q}. We
start by creating a set of nodes having labels as follows:
- r, where r will be the root of the spanning tree T1 ,
- wi (xi , yi ) for all wi ∈ W (respectively xi ∈ X, and yi ∈ Y ),
- for each wi ∈ W (xi ∈ X, yi ∈ Y ), one additional node with label wi (respectively xi and yi ),
- for each triple (wi , xj , yk ) ∈ M , three additional nodes with labels
aijk , aijk , āijk .
We now create an instance of T CA as follows:
V = {r} ∪ {wi , wi , xi , xi , yi , yi |i = 1, 2, . . . , q} ∪ {aijk , aijk , āijk |(wi , xj , yk ) ∈ M }

E = {(u, v)|u, v ∈ V and u = v}
E1 = {(r, wi ), (wi , wi )|i = 1, 2, . . . , q} ∪
{(r, xi ), (xi , xi ), |i = 1, 2, . . . , q} ∪
{(r, yi ), (yi , yi )|i = 1, 2, . . . , q} ∪
{(r, aijk ), (aijk , aijk )|(wi , xj , yk ) ∈ M } ∪
{(wi , āijk )|(wi , xj , yk ) ∈ M }

138

S. Shirazipourazad, A. Sen, and S. Bandyopadhyay

B =p+q
c(xj , āijk ) = c(yk , aijk ) = c(āijk , aijk ) = 1, for all
(wi , xj , yk ) ∈ M . All other edges in E have weight 2.
We claim that M contains a matching M  iﬀ there is a set E aug of cost no more
than B, such that the graph (V, E1 ∪ E aug ) is a r−fault tolerant graph.
To prove the only if part, let M contain a matching M  . We form E aug by
following the procedure given below:
Step i) For each triple (wi , xj , yk ) ∈ M  , we add edges (xj , āijk ) and (yk , aijk ),
Step ii) For each triple (wi , xj , yk ) ∈ M − M  , we add edge (aijk , āijk ).
Since |M  | = q and |M − M  | = p − q, and the cost of each edge added in Steps i
and ii is 1, the total cost of the added edges in steps i and ii is 2q + p − q. Thus,
the total cost of the edges in E aug is p + q. E1 ∪ E aug includes the following
cycles that pass through r:
– r, wi , wi , āijk , xj , xj , ∀i, j, k : (wi , xj , yk ) ∈ M  ,
– r, yk , yk , aijk , aijk , ∀i, j, k : (wi , xj , yk ) ∈ M  ,
– r, aijk , aijk , āijk , wi , wi , ∀i, j, k : (wi , xj , yk ) ∈ M − M  .
It can be readily veriﬁed that all the nodes in V − {r} appear in at least one of
the above cycles. Therefore, there are two disjoint paths from r to each vertex.
To prove the if part, let there be a set of edges E aug ⊆ (E − E1 ), with a
cost of at most p + q, so that in the graph (V, E1 ∪ E aug ) every non-adjacent
vertex of root r has two node disjoint paths to r. There are exactly 2p + 2q leaf
nodes in T1 = (V, E1 ) and they are not adjacent to r. Among these leaf nodes,
there exists p nodes having labels of the form aijk and āijk and q nodes having
labels of the form yk and xj . To ensure that 2p + 2q leaf nodes have two node
disjoint paths to r, at least p + q edges must be added to T1 = (V, E1 ). It may
be noted that there are only three types of edges i) (xj , āijk ), ii) (yk , aijk ) and
iii) (āijk , aijk ) that have cost 1 and every other edge in E have a cost 2. Since
the cost of the edges in E aug is at most p + q and |Eaug | ≥ p + q, it implies that
|E aug | = p + q and the cost of each edge in E aug is 1.
In order to have two node disjoint paths from 2p + 2q leaf nodes to r, each
node of the from yk and xj must be connected to a node of the form aijk or āijk .
The total cost of this set of edges will be 2q. Since the total cost E aug is p + q,
the cost of the edges to connect the remaining leaves of the type aijk or āijk ,
(i.e., the ones that were not connected to either yk and xj ), must be p − q and
the number of such leaves must be 2(p − q). To connect 2(p − q) leaves, at least
p − q edges will be necessary. Since the total cost of these edges is p − q and at
least p − q edges will be necessary, the cost of these edges must be 1. However,
this will only be possible if 2(p − q) leaves can be grouped into p − q pairs of
nodes (aijk , āi j  k ), such that i = i , j = j  , k = k  . This implies that exactly q
nodes, each of the form aijk or āijk , must be connected to q nodes, each of the
form yk and xj , and these 2q nodes (aijk or āijk ) can be grouped into q pairs
of nodes (aijk , āi j  k ), such that i = i , j = j  , k = k  . Since these q pairs of ijk
indices connects to all the yk and xj nodes, the corresponding subset M  ⊆ M
must be a matching for the instance of the 3DM problem.

Fault-Tolerant Design of Wireless Sensor Networks

3

139

Approximation Algorithm for the TCA

In this section we propose an approximation algorithm with a guaranteed performance bound for T CA. The input to the algorithm is a complete undirected
graph G1 = (V, E) with cost function c : E → Z+ deﬁned on the edges, and
T1 = (V, E1 ), a spanning tree of G1 with a speciﬁed vertex r ∈ V as the root. The
output is a set of edges E aug ⊆ E − E1 , such that, in the graph (V, E1 ∪ E aug ),
there are two node disjoint paths from every node v (v ∈ V − r) to the node
r. Since the tree T1 = (V, E1 ) is given as part of the input, we assume that
the cost of the edges in E1 is zero, i.e., we do not have to pay for these edges,
c(e) = 0, ∀e ∈ E1 . We compute the edge set E aug using a sequence of steps where,
in each step, we construct a new graph/tree (undirected/directed). The sequence
of construction of graphs is as follows: [T1 = (V, E1 )] ⇒ [T2 = (V2 , E2 )] ⇒ [G2 =
(V2 , E2 )] ⇒ [T2d = (V2 , A2 )] ⇒ [Gd2 = (V2 , A2 )], where T2 is a tree constructed
from T1 , G2 is a complete graph deﬁned with the vertex set of T2 , T2d is a directed
tree deﬁned on T2 , and Gd2 is a completely connected directed graph deﬁned with
the vertex set of T2d . From Gd2 we identify a set of arcs (directed edges) Aaug
2 ,
so that the directed graph (V2 , A2 ∪ Aaug
2 ) is strongly connected [14]. Finally, we
construct E aug from Aaug
2 . We now describe, in detail, the construction rules for
these graphs/trees.
[A] Construction of T2 : Let Vp ⊂ V be the set of leaves in T1 and let
Vq = V − (Vp ∪ {r}) be the set of all internal (non-leaf) nodes except the root.
We deﬁne a new tree T2 = (V2 , E2 ) using the following rules:
– V2 = V ∪ {vij |i, j ∈ V − {r} and (i, j) ∈ E1 }.
– For each edge (i, j) ∈ E1 , we include in E2 ,
• edge (i, j), if i = r or j = r,
• edges (i, vij ) and (vij , j), otherwise.
[B] Construction of G2 :
Let G2 = (V2 , E2 ) be the complete graph deﬁned on V2 . We deﬁne the cost
function c : E2 → Z+ ∪ {∞} as follows. For every edge (x, y) ∈ E2 , if x, y ∈ V ,
c (x, y) = c(x, y); otherwise, c (x, y) = ∞ (i.e., we set the initial cost of the edges
between a node u ∈ V2 − V and every other node in V2 to inﬁnity).
Next we deﬁne two functions d(u, v) (distance function) and p(u, v) (pointer
function) for every pair of nodes u and v in G2 . We deﬁne both the functions in
terms of c and T2 .
Definition: d(u, v) = min{c (x, y)|u and v are on the path from x to y in T2 }.
Definition: p(u, v) is a pointer to the edge (s, t), such that d(u, v) = c (s, t),
where u and v are on the path from s to t in T2 .
Example: The Fig. 3(a) shows a spanning tree T1 = (V, E1 ) of a complete graph
G1 = (V, E) (for the sake of clarity, only three edges from the set E − E1 ,
(a − b), (c − d) and (d − e) are shown in Fig. 3(a)). The solid lines indicate the
edges in E1 and the dashed lines show a subset of the edges in E − E1 . The cost

140

S. Shirazipourazad, A. Sen, and S. Bandyopadhyay

of each edge in E1 = 0. The weights associated with the dashed lines indicate
the cost of these edges. All other edges in E − E1 , (not shown in Fig. 3(a)), have
a cost of 10. Fig. 3(b) shows the tree T2 = (V2 , E2 ) constructed from T1 in Fig.
3(a). From T2 , we can construct the complete graph G2 = (V2 , E2 ) following
the construction rules described earlier. The solid lines in Fig. 3(b) indicate the
edges in E2 and the dashed lines show a subset of the edges in E2 − E2 (only
ﬁve edges with associated weights are shown). In this example, d(vac , vad ) =
c (c, d) = c(c, d) = 1, d(a, b) = c (d, e) = c(d, e) = 1 and p(vac , vad ) = (c, d),
p(a, b) = (d, e).
We now discuss the rationale for deﬁnitions of the d(u, v) and p(u, v) given
above. In order to have another path from a to b in Fig. 3(a), (diﬀerent from the
one in the tree T2 , a − r − b), the edge (d, e) with cost 1 or the edge (a, b) (in G2 )
with cost 4 can be added to the tree. The addition of the link (d, e) will result in
a cheaper path from a to b with cost 1. The goal of the distance function d(u, v)
is to identify this edge. The function p(u, v) is deﬁned to be a pointer to the
edge selected by the function d(u, v).
Computation of d(u, v): It has been shown in [12] that d(u, v) for all pairs
of nodes can be computed in O(|V |2 ). For ease of reference, we summarize the
algorithm for computing the function d(u, v) presented in [12]. Initially, for every
pair of nodes u, v ∈ V2 , d(u, v) = c (u, v) and p(u, v) = (u, v). Let l(u, v) be
the number of edges on the path from u to v in T2 and s(u, v) be the node
adjacent to v on this path. The edges (u, v) ∈ E2 − E2 are sorted in nondecreasing order, based on l(u, v). For each edge (u, v) ∈ E2 −E2 , we compute the
distance function d(u, v) as follows. If d(u, v) < d(u, s(u, v)) then d(u, s(u, v)) =
d(u, v) and p(u, s(u, v)) = (u, v). If d(u, v) < d(s(v, u), v), then d(s(v, u), v) =
d(u, v) and p(s(v, u), v) = (u, v).
[C] Construction of T2d = (V2 , A2 ): We construct T2d from T2 = (V2 , E2 ) by
directing all edges of T2 towards the root node r. We will use A2 to represent
the set of arcs (directed edges) corresponding to the undirected edges in E2 .
[D] Construction of Gd2 = (V2 , A2 ): Gd2 is a completely connected directed
graph, with associated cost c (u, v) with each arc u → v ∈ A2 as follows:
⎧
if v = r,
⎨ ∞,
if u ∈ Vq and v ∈ subtree(u)
c (u, v) = ∞,
⎩
d(u,v), otherwise.
Here we deﬁne subtree(u) to be the set of nodes in the subtree rooted at node
u in tree T2 .
We note that each arc u → v ∈ A2 where v = r has a zero cost. The rationale
for assigning the arc costs in this speciﬁc way is as follows:
(a) By assigning a cost of ∞ to the edges where v = r, we ensure that the
minimum cost arborescence on Gd2 is rooted at r,
(b) By assigning a cost of ∞ to the edges (u, v) where u ∈ Vq and v ∈ subtree(u)
in T2 , we ensure that, in Gd2 , no node u ∈ Vq will have a directed path from u to
the nodes in subtree(u), unless it ﬁrst goes through some nodes not in subtree(u).

Fault-Tolerant Design of Wireless Sensor Networks

r
a

r

1

d

a

b

4

c

141

vac

1

(a)

e

c

b

4
∞
∞
1

vad
d

vbe

1

e

(b)

Fig. 3. (a) An example of T1 ; E1 includes the edges shown with solid lines. (b) The
tree with solid lines is T2 corresponding to T1 in (a) and dashed lines are some of the
edges in E2 − E2 .

When constructing Gd2 = (V2 , A2 ), our goal is to identify a set of arcs Aaug
2 ,
⊆ A2 ), so that the graph (V2 , A2 ∪ Aaug
(Aaug
2
2 ) is strongly connected, i.e., there
exists a directed path between every pair of nodes in V2 . We obtain the arc
set Aaug
by computing the least-cost arborescence [15] in Gd2 = (V2 , A2 ), which
2
aug
from Aarb
by
we denote by T2arb = (V2 , Aarb
2 ). We obtain the set of arcs A2
2
aug
arb
arb 
excluding those arcs with cost zero, i.e., A2 = A2 − {a ∈ A2 |c (a) = 0}.
Finally, we construct the set of edges E aug that we have to add to the input
tree T1 = (V, E1 ), to obtain the r−fault tolerant graph (V, E1 ∪ E aug ) as follows:
E aug = {p(u, v)|u → v ∈ Aaug
2 }.

Algorithm 1. T CA Algorithm
Input: G1 = (V, E), a complete graph with cost c(e) for every edge e ∈ E; T1 = (V, E1 ),
a spanning tree of G1 with root r.
Output: A set of edges E aug ⊆ E − E1 , such that (V, E1 ∪ E aug ) is r−fault tolerant.
1: Construct T2 = (V2 , E2 ), a complete graph G2 = (V2 , E2 ) and the cost function
c (.) from T1 and G1 using the technique described in [A].
2: Compute d(u, v) and p(u, v) for each pair of nodes u, v ∈ V2 using the technique
described in [B].
3: Compute a directed tree T2d = (V2 , A2 ) by directing all edges in E2 toward root r
using technique described in [C].
4: Compute a completely connected directed graph Gd2 = (V2 , A2 ) with cost c deﬁned
on the arcs set A2 using technique described in [D].
d

5: Compute a minimum cost arborescence T2d = (V2 , Aarb
2 ) of the graph G2 = (V2 , A2 ).
aug
arb
arb 
6: Set A2 = A2 − {a ∈ A2 |c (a) = 0}.
}.
7: Set E aug = {p(u, v)|u → v ∈ Aaug
2
8: Return E aug .

We note that the time complexity of the T CA algorithm (Algorithm 1) is
O(|V |2 ). Line 4 has O(|V 2 |) time complexity. Finding minimum cost arborescence also needs O(|V |2 ) time [15].

142

S. Shirazipourazad, A. Sen, and S. Bandyopadhyay

Theorem 2. Algorithm T CA ﬁnds a set of edges E aug such that (V, E1 ∪ E aug )
is r−fault tolerant.
Proof. In order to prove that (V, E1 ∪ E aug ) is r−fault tolerant, we need to show
that there is no node in Vq whose removal disconnects the graph (Vq is the set
of all internal nodes in the tree T1 = (V, E1 ) except the root r). Since the graph
d
(V2 , A2 ∪ Aaug
2 ) is constructed by augmenting T2 = (V2 , A2 ) with the arcs of the
arb
T2 (excluding the arcs that are already in A2 ), it must be strongly connected.
Accordingly, there must be a directed path from any node v ∈ Vq to the nodes
in subtree(v). Let w = v be a node in subtree(v). Since there is no directed edge
∈ A2 going out of v to the nodes in subtree(v) (because the cost of these edges
is inﬁnity), the ﬁrst arc in a path from v to any other node in subtree(v) should
go through a node s where s is not in subtree(v). Since the graph (V2 , A2 ∪Aaug
2 )
is strongly connected, there must be a path from s to w in (V2 , A2 ∪ Aaug
2 ) not
including v. Suppose that c → d ∈ Aaug
is on the path from s to w which
2
does not include v. If (c, d) is replaced by p(c, d) = (e, f ), (e ∈ subtree(c) and
f ∈ subtree(d)), the new path also will not include v, because v cannot be on the
path from c to e or f to d in T2 . Hence, even if a node v ∈ Vq is removed from the
graph (V, E1 ∪ E aug ), the graph remains connected. Therefore, (V, E1 ∪ E aug ) is
r−fault tolerant.
Theorem 3. Algorithm T CA ﬁnds a set of edges E aug with a total cost C aug ,
such that C aug ≤ 2C opt , where C opt is the cost of the optimal solution.
Proof: Our proof strategy is as follows. Let C opt be the optimal cost of edges
E opt necessary to add to the input tree T1 = (V, E1 ), so that the resulting graph
becomes r−fault-tolerant and C aug is the cost of edges E aug selected by the
T CA Algorithm. We show that there exists a subset of arcs A in the graph
Gd2 = (V2 , A2 ) with three useful
If C  is the cost of the arcs in
properties.





A , A ⊆ A2 − A2 , (i.e., C = u→v∈A c (u, v)), then (i) C opt ≥ C  /2, (ii)
C aug ≤ C  , and (iii) the graph (V2 , A2 ∪ A ) is strongly connected. From (i) and
(ii) it follows that C aug ≤ 2C opt .
We can compute the set of arcs A ⊆ A2 − A2 from the optimal solution
opt
E ⊆ E − E1 following the procedure described below.
Let Q be the set of nodes that are strongly connected in (V2 , A2 ∪A ). Initially,
we set A = ∅, Q = {r} and mark all the edges in E opt as unused. We update
Q, A and the marking of the edges in E opt using the following procedure:
While Q = V2 repeat the following steps:
– Select an unused edge (u, v) from E opt , such that there is a node t ∈ Q − Vq
on the weakly directed path from u to v in T2d = (V2 , A2 ). (The weakly
directed path from u to v in T2d is the path from u to v in T2d in which the
direction of the arcs is ignored.)
– If t = u, add t → u to A and if t = v add t → v to A .
– Add all the nodes on the weakly directed path from u to v in T2d to Q.
Since t has been selected from set Q, it is already accessible from root r in
(V2 , A2 ∪ A ). Therefore, by adding the new arcs to A in step (ii) all the

Fault-Tolerant Design of Wireless Sensor Networks

143

nodes on the weakly directed path from u to v in T2d are now accessible from
root r in current augmented directed graph (V2 , A2 ∪ A ).
– Change the marking of the edge (u, v) from unused to used.
We need to show that, during the execution of the iterative process, an unused
edge (u, v) ∈ E opt and a suitable vertex t ∈ Q − Vq exist. While there are some
edges in E opt which have not been used previously, there is still some node
w in Vq whose deletion disconnects some node a ∈ subtree(w) from the root
in (V2 , A2 ∪ A ). So, there is no directed path from r to a in (V2 , A2 ∪ A )
during that iteration. Therefore, there should be an edge (u, v) in E opt which
creates a path from r to a ∈ subtree(w) such that the path does not include
w in (V, E1 ∪ E opt ). Also, the path from u to v in T1 will contain more nodes
from Q than just one vertex from Vq ; otherwise, the removal of that vertex would
disconnect the graph (V, E1 ∪E opt ) which contradicts the fact that (V, E1 ∪E opt )
is r−fault tolerant. Since there is no directed edge, in T2d , between the nodes in
Vq , the weakly directed path from u to v in T2d should
 include a node t ∈ Q − Vq .
Let C  be the cost of the arcs in A ; C  =
c (u, v). For every
u→v∈A
t
edge (u, v) ∈ E opt , we have to add at most two arcs t → u and t → v to A .
Because c (t, u) ≤ d(u, v) and c (t, v) ≤ d(u, v), C  ≤ 2C opt . Also we know
that the graph (V2 , A2 ∪ A ) is strongly connected. We can, therefore, construct
an arborescence on (V2 , A2 ∪ A ) rooted at r using c as the cost of the edges.
Since the T CA algorithm gives us the minimum cost arborescence on Gd2 and
A2 ∪ A ⊆ A2 , C aug ≤ C  ≤ 2C opt .

4

Experimental Results

In this section we present the experimental results of the proposed approximation
algorithm for the T CA problem. In the experiments we compare the results of
the approximation algorithm against the optimal solution. Moreover, we examine
energy consumption by directional and omni-directional antennas deployed in a
sensor network.
For every instance of our experiment, we generate the locations of the sensor
nodes randomly, using a uniform distribution on a square deployment area of size
100 × 100 units. We take the cost of edge between the nodes u and v in the sensor
network, as an indicator of the transmit power needed by the nodes to reach each
other. Accordingly, we construct a complete graph G = (V, E) by setting the cost
of each edge c(i, j) proportional to d2 (i, j) where d(i, j) is the Euclidean distance
between nodes i and j. We assume that an omni-directional antenna will consume
power proportional to r2 where r is the radius of the coverage circle. A directional
antenna with same transmit range r but with transmit beam width α degrees will
α 2
r [3]. In our model, an edge represents two
consume power proportional to 360
directional antennas transmitting signals to each other. Therefore, if the beam
2α 2
d (i, j). For each problem
width is α we assume that the cost of the edge (i, j) is 360
instance we compute the minimum spanning tree to be the initial tree T1 and select
a node randomly as the root of the tree.

144

S. Shirazipourazad, A. Sen, and S. Bandyopadhyay

In our ﬁrst set of experiments our objective was to compare the results of
the approximation algorithm with the optimal solution, obtained by solving an
integer linear programming (ILP). We have denoted the ILP used to ﬁnd the
optimal solution of T CA by ILP . We used the CPLEX package to solve the ILP.
Since ILP takes considerable amount of time in these experiments, we vary the
number of nodes, n, from 5 to 25 in steps of 5. For each value of n, we generate
10 random instances of network layouts in a 2-dimensional plane. We set the
beam width to 30 degrees. We present in Figure 4 the comparisons between the
optimal augmentation cost and the augmentation cost computed by the T CA
algorithm. For each n, we compute the average cost over 10 instances. We note
that in these simulations, the ratio of the average cost of augmentation computed
by the T CA algorithm to the average cost of optimal solution is smaller than
1.46. This indicates that in practice the performance of the TCA algorithm is
signiﬁcantly better than it’s guaranteed worst case bound (2).
In our second set of experiments, we compare the power consumption of
directional antennas versus omni-directional antennas. We vary the number
of nodes, n, from 10 to 50 in steps of 10. For each value of n, we generate
50 random instances of network layouts in a 2-dimensional plane. We compute the average cost of augmentation by taking the average of costs incurred
in these 50 instances. We perform the experiments for two values of beam
width, 20 and 40 degrees. For each instance we execute the T CA algorithm.
We assume that the transmit range of the omni-directional antenna is large
enough to cover furthest neighbor in the augmented graph. Therefore, the total
power
consumption in network with omni-directional antennas is proportional to

2
1≤i≤n max{j|(i,j)∈E1 ∪E aug } d (i, j). In the case of directional antennas, every
edge in the augmented graph corresponds to two antennas, where each antenna
α 2
d (i, j) amount of power
Hence, the total power
needs 360
 to reach the2αother.
d2 (i, j). The Fig. 5 illustrates
consumption is proportional to (i,j)∈E1 ∪E aug 360
the comparison of power consumption in these two cases. We observe that the
total power consumption in the network is signiﬁcantly smaller when directional
antennas are used instead of omni-directional ones. More speciﬁcally, when the
beam width is 20 degrees, the power consumption with directional antennas is
less than 9 percent of omni-directional antennas and when the beam width is 40
degrees it is less than 18 percent. We note that in order to make the network
r−fault tolerant, we need to install additional directional antennas, which has a
ﬁxed cost associated with it. When nodes have omni-directional antenna, each
node requires only one antenna. When we use directional antennas, the total
number of antennas deployed in every sensor node is equal to the node degree
(including both the initial set of antennas in the tree and the antennas installed
to augment the tree). In Fig. 6 we illustrate the average number of antennas that
is needed in the network for each value of n. The ﬁrst diagram shows the total
number of directional antennas needed in the network. We observe that the ratio
of the total number of directional antennas to the number of omni-directional
antennas in these experiments is less than 2.7. However, in the T CA problem
we consider that the initial set of edges in the tree has cost zero (it is not part

Fault-Tolerant Design of Wireless Sensor Networks

145

of augmentation cost). Therefore, only the cost of the new (augmenting) edges
(i.e., the cost of the corresponding additional antennas) that are added during
augmentation phase needs to be considered. The third diagram in Fig. 6 depicts
the average number of directional antennas that are added to the network during the augmentation process. This number is smaller than 75 percent of the
number of omni-directional antennas. More accurately, augmenting directional
antennas that are needed to make the network r−fault tolerant are fewer than
75 percent of number of omni-directional antennas. Therefore, the savings in
power consumption with directional antennas outweighs the cost of additional
directional antennas needed, particularly when the width of the antenna beam
is narrow.

800

Cost

600
400

TCA
ILP

200
0
5

10

15

20

25

Number of Nodes
Fig. 4. Comparison of augmentation cost of T CA algorithm and ILP .

Power

20000

16000

Directional (beam width=20)

12000

Directional (beam width=40)

8000

Omnidirectional

4000
0
0

20
40
Number of nodes

60

Fig. 5. Comparison between power consumption of directional antennas and omnidirectional antennas

Number of antennas

146

S. Shirazipourazad, A. Sen, and S. Bandyopadhyay

160
#Directional antennas

120

#Omnidirectional antennas

80

#Augmenting directional
antennas

40

0
10

20
30
40
Number of nodes

50

Fig. 6. Comparison between number of directional antennas and omni-directional
antennas

5

Conclusions

This paper was motivated by the importance of both data collection and fault
tolerance in wireless sensor networks. We have studied the problem of enhancing
the fault tolerance capability of a sensor network where the sensor nodes are
equipped with directional antennas using techniques for graph augmentation.
We proved that the least cost tree augmentation problem to achieve resilience
with respect to single faults is NP-complete. We have proposed an approximation algorithm, with a performance bound of two. Experimental evaluation of
the approximation algorithm shows that it performs even better in practice. In
future we plan to study the tree augmentation problem under more general topological fault models, where a fault is deﬁned as any subgraph with diameter not
exceeding d or when a fault is deﬁned, based on the network geometry.
Acknowledgments. The research was supported in part by the DTRA grant
HDTRA1-09-1-0032 and the AFOSR grant FA9550-09-1-0120.

References
[1] Incel, O., Krishnamachari, B.: Enhancing the data collection rate of tree-based
aggregation in wireless sensor networks. In: Secon (2008)
[2] Li, X.Y., Wang, Y., Wang, Y.: Complexity of data collection, aggregation, and
selection for wireless sensor networks. IEEE Transactions on Computers 60,
386–399 (2011)
[3] Kranakis, E., Krizanc, D., Williams, E.: Directional Versus Omnidirectional Antennas for Energy Consumption and k-Connectivity of Networks of Sensors. In:
Higashino, T. (ed.) OPODIS 2004. LNCS, vol. 3544, pp. 357–368. Springer, Heidelberg (2005)

Fault-Tolerant Design of Wireless Sensor Networks

147

[4] Yu, Z., Teng, J., Bai, X., Xuan, D., Jia, W.: Connected coverage in wireless networks with directional antennas. In: INFOCOM (2011)
[5] Wang, Y., Cao, G.: Minimizing service delay in directional sensor networks. In:
INFOCOM (2011)
[6] Balanis, C.A.: Antenna Theory: Analysis and Design, 2nd edn. Wiley (1997)
[7] Bredin, J.L., Demaine, E.D., Hajiaghay, M., Rus, D.: Deploying sensor networks
with guaranteed capacity and fault tolerance. In: MobiHoc (2005)
[8] Hajiaghayi, M., Immorlica, N., Mirrokni, V.: Power optimization in fault-tolerant
topology control algorithms for wireless multi-hop networks. IEEE/ACM Transactions on Networking 15, 1345–1358 (2007)
[9] Pishro-Nik, H., Chan, K., Fekri, F.: Connectivity properties of large-scale sensor
networks. Wireless Networks 15(7), 945–964 (2009)
[10] Wang, F., Thai, M., Li, Y., Cheng, X., Du, D.Z.: Fault-tolerant topology control for
all-to-one and one-to-all communication in wireles networks. IEEE Transactions
on Mobile Computing 7, 322–331 (2008)
[11] Cardei, M., Yang, S., Wu, J.: Algorithms for fault-tolerant topology in heterogeneous wireless sensor networks. IEEE Trans. Parallel Distrib. Syst. 19(4), 545–558
(2008)
[12] Frederickson, G.N., Ja’Ja’, J.: Approximation algorithms for several graph augmentation problems. SIAM J. on Computing 10, 270–283 (1981)
[13] Garey, M., Johnson, D.: Computers and intractability. A guide to the theory of
NP-completeness. Freeman (1979)
[14] West, D.B.: Introduction to Graph Theory, 2nd edn. Prentice Hall (2001)
[15] Tarjan, R.E.: Finding optimum branchings. Networks 7, 25–35 (1977)

Influence Propagation in Adversarial Setting: How to
Defeat Competition with Least Amount of Investment
Shahrzad Shirazipourazad, Brian Bogard, Harsh Vachhani, Arunabha Sen
School of Computing, Informatics and Decision Systems Engineering
Arizona State University
Tempe, AZ 85287

{sshiraz1, bbogard, hvachhan, asen}@asu.edu
Paul Horn
Department of Mathematics
Harvard University
Cambridge, MA 09322

phorn@math.harvard.edu
ABSTRACT

provides better performance if the second player utilizes it
instead of the greedy algorithm to maximize its influence.

It has been observed that individuals’ decisions to adopt a
product or innovation are often influenced by the recommendations of their friends and acquaintances. Motivated
by this observation, the last few years have seen a number
of studies on influence maximization in social networks. The
primary goal of these studies is identification of k most influential nodes in a network. A major limitation of these
studies is that they focus on a non-adversarial environment,
where only one player is engaged in influencing the nodes.
However, in a realistic scenario multiple players attempt to
influence the nodes in a competitive fashion. The proposed
model considers a competitive environment where a node
that has not yet adopted an innovation, can adopt only one
of the several competing innovations and once it adopts an
innovation, it does not switch. The paper studies the scenario where the first player has already chosen a set of k
nodes and the second player, with the knowledge of the
choice of the first, attempts to identify a smallest set of
nodes (excluding the ones already chosen by the first) so
that when the influence propagation process ends, the number of nodes influenced by the second player is larger than
the number of nodes influenced by the first.
The paper studies two propagation models and shows that
in both the models, the identification of the smallest set of
nodes to defeat the adversary is NP-Hard. It provides an
approximation algorithm and proves that the performance
bound is tight. It also presents the results of extensive experimentation using the collaboration network data. Experimental results show that the second player can easily
defeat the first with this algorithm, if the first utilizes the
node degree or closeness centrality based algorithms for the
selection of influential nodes. The proposed algorithm also

Categories and Subject Descriptors
F.2.2 [Analysis of Algorithms and Problem Complexity]: [Non-numerical Algorithms and Problems]

General Terms
Algorithms, Experimentation, Performance

Keywords
Social Networks, Influence Maximization, Adversarial Environment

1.

INTRODUCTION

It has been widely observed in various studies in social sciences and economics that an individuals’ decision to adopt
a product, behavior or innovation is often influenced by the
recommendations of their friends and acquaintances. Motivated by this observation, the last few years have seen a
number of studies on influence maximization problem in social networks [2, 3, 4, 6, 7, 11, 13]. One major goal of several
of these studies is identification of k most influential nodes
in a network. A product manufacturer may want to identify
the k most influential nodes in the network, as she may want
to incentivize these nodes to buy the new product by providing free samples to them, on the expectation that once
these nodes are convinced about the quality of the product, they will recommend it to their friends on the social
network and encourage them to buy the product. This set
of k nodes, being the most influential on the network, will
have the largest impact on convincing the rest of the nodes
about the quality of the product. Since the manufacturer
has a fixed budget for advertising, she can provide free samples only to a limited number of nodes in the network. The
size of the advertising budget determines the value of the
parameter k.
It may be noted that most of the studies on influence
propagation are geared toward a non-adversarial environment, where only one manufacturer (player) is attempting

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CIKM’12, October 29–November 2, 2012, Maui, HI, USA.
Copyright 2012 ACM 978-1-4503-1156-4/12/10 ...$15.00.

585

to influence the nodes of a social network to buy her product. However, in a realistic market scenario, most often
there exists multiple players, each attempting to sell their
competing products or innovations. For example, just as
Coke attempts to convince customers in an emerging market about the quality of their beverage, its main competitor,
Pepsi, also does the same. Both the competitors have only a
finite advertisement budget and both of them want to derive
the greatest benefit out of their advertising campaign. The
goal of both the players often is to capture a share of this
emerging market that is larger than its competition.
The non-adversarial influence propagation models consider scenarios where a user (a node u in a social network
graph G = (V, E)) adopts (or does not adopt) an innovation based on how her acquaintances have adopted the innovation. In these models each node u in the social network
graph is in one of the following two states: (i) u has adopted
innovation A, and (ii) u has not adopted innovation A but u
is open to the idea of adoption. One can visualize such a scenario by coloring the nodes of the social network graph with
red if they have adopted the innovation A and with white if
they have not adopted A yet, but are open to the idea of
adopting A in the future. As the diffusion process progresses
with time, by observing changing color of the nodes of the
graph one can infer if innovation A is being adopted by the
members of the social network. Although, this paper focus
on influence propagation in social networks, conceptually,
the scenario is identical for spread of any contagion through
a network - be it spread of diseases through a human contact
network or spread of worms through the Internet.
The influence (contagion) propagation models can be divided into three distinct classes:

to maximize the coverage subject to a budget constraint but
without any constraint on the number of time steps.
The Class I problem considered in [11] can be stated in
the following way: “Which k white nodes should be colored
red initially, so that the largest number of white nodes turn
to red at the end of propagation process?”. The Class II
problems can be stated in the following way: “Given that
some nodes are already colored red, which k white nodes
should be colored blue, so that this set of nodes will have the
largest impact in preventing the white nodes from turning
red.
In Class I, there is no notion of an adversary. The red
nodes are trying to convert all the white nodes into red
nodes and there is no agent that is actively trying to prevent this conversion. The Class II, although it has a notion
of an adversary (i.e., the blue nodes) which is trying to slow
down (or stop) white-to-red conversion, at best this agent
can be viewed as a passive adversary, because its goal is
to prevent white-to-red conversion, and it is not engaged in
white-to-blue conversion. This gives rise to Class III, a truly
adversarial scenario, where the red agent is trying to convert
all the white nodes into red, while the blue agent is trying
to convert all the white nodes into blue. In this case, the
blue agent can be viewed as an active adversary of the red
agent.
The Class III models the scenario where a node u is being
actively encouraged by an adversary not only not to adopt
the innovation but also to adopt a competing innovation. In
this case, each node u in the social network graph can be
in one of following three states: (i) u has adopted innovation A , (ii) u has adopted innovation B, and (iii) u has not
adopted any innovation A or B but is open to the idea of
adopting either one of them. This adversarial scenario can
be viewed as a classic case of a strategic conflict game between the proponent(s) and the opponent(s) of adoption of
an innovation and a game is won by the proponent(s) if u
decides to adopt the innovation A.
This paper studies a Class III scenario where two vendors
(players) are trying to sell their competing products by influencing the nodes of a social network. The goal of both
the players is to have a market share that is larger than its
competition. It considers the scenario where the first player
(P1 ) has already chosen the k nodes to have a large influence (coverage) on the social network. The second player is
aware of the first player’s choice and the goal of the second
player (P2 ) is to identify a smallest set of nodes (excluding
the ones already chosen by the first player) so that the number of nodes influenced by the second player will be larger
than the number of nodes influenced by the first player within
D time steps. In other words, the objective of the problem
is to minimze the cost subject to the constraint that the coverage of the second player is larger than the coverage of the
first player within D time steps. Since the goal of the second
player is to win the “game” (i.e., to have a larger coverage or
market share), with influencing (incentivizing) as few nodes
as possible, the problem under study in this paper is referred
to as the “Winning with Minimum Investment” (WMI) problem. In [3], the authors study a similar problem belonging
to class III. However, the objective of the problem studied in
[3] is different from the one being studied in this paper. The
goal of the second player in the problem studied in [3] is not
to defeat the first player with least amount of investment,
but to maximize its own influence.

• Class I: Non-adversarial
• Class II: Adversarial with passive adversary
• Class III Adversarial with active adversary
The problems in classes I and II can be stated as follows:
• Class I: How to identify a set of k initial (seed) nodes,
so that once they are influenced/infected, they will infect the largest number of uninfected nodes in the network?
• Class II: Given that a subset of the nodes is already influenced/infected, how to identify a set of k uninfected
nodes, so that when they are immunized, they will have
the largest impact in preventing the uninfected nodes
from being infected.
In most of the influence propagation models, influence
propagates in a step-by-step fashion and as such there is
a notion of time step (or propagation step) involved. The
expected number of nodes influenced at the end of time step
D is at most the expected number of nodes influenced at the
end of time step D + 1. In other words, expected number
of nodes influenced at the end of time step D is a nondecreasing function of D.
The Class I influence propagation problem considered in
[11] may be viewed to have three dimensions, (i) the number
of seed nodes activated at the beginning (budget or cost of
influence), (ii) the expected number of activated nodes at
the end of propagation (impact or coverage of initial seed
nodes), and (iii) time steps for propagation. The objective
of the influence maximization problem considered in [11], is

586

Using the same two influence propagation models introduced in [3], the contributions of the paper may be listed as
follows:

influence maximization problem, they showed that computation on the sparsified model provided significant improvements in terms of speedup without compromising accuracy.
Wang et. al. in [17] considered the influential node identification problem in a mobile social network and presented a
two step process, where in the first step, communities in the
social network are detected and in the second step a subset
of communities is selected to identify the influential nodes.
Experimental results with data from large real world mobile
social network showed that their algorithm performed an
order of magnitude faster than the state-of-the-art greedy
algorithm for finding the top-k influential nodes. A simulated annealing (SA) based algorithm for finding the top-k
influential nodes was presented in [10]. It has been reported
in [10], that using data from four real networks, the SA based
algorithm performed 2-3 orders of magnitude faster than the
state-of-the-art greedy algorithm.
In addition to attempts to address the scalability issue of
the greedy algorithm in [11], efforts on variations of the original problem formulation and also the computation model is
underway in the research community. In [7] two new problem formulations are provided. In the first formulation, the
goal is to minimize the cost, subject to the constraint that
coverage exceeds a minimum threshold ν without any constraint on the number of time steps. The goal of the second
formulation is to minimize the number of time steps, subject to a budget constraint k and a coverage constraint ν.
For the first version of the problem, the authors provide a
simple greedy algorithm and show that it provides a bicriteria approximation. For the second version, they show that
even bicriteria or tricriteria approximations are hard under
several conditions. In [1], the authors argue that a user (a
node in the social network) may be influenced by positive
recommendations from a group of friends (neighbors in the
network) but that does not necessarily imply that she will
adopt the product herself. However, she may pass on her
positive impression about the product to another group of
friends. Clearly, such a model departs from the model considered in [11]. The authors in [1] consider an “adoption
maximization” problem instead of “influence maximization”
problem and present both analytical and experimental results for the new problem. The authors in [12] argue that a
limitation of the traditional influence analysis technique is
that they only consider positive relations (agreement, trust)
and ignore the negative relations (distrust, disagreement).
Moreover, the traditional techniques also ignore conformity
of people, i.e., an individual’s inclination to be influenced.
The paper studies the interplay between influence and conformity of each individual and computes the influence and
conformity indices of individuals. The authors in [5] suggest
an alternate way of measuring the influencing capability of
an individual on her peers, through the individuals reach
within the social network for certain actions.
All the references discussed in the last three paragraphs
pertain to the class I (non-adversarial) problems as defined
in the previous section. Results on study of class II problems
(adversarial with passive adversary) is presented in [8]. It
focuses on identification of blockers, the nodes that are most
effective in blocking the spread of a dynamic process through
a social network, and reports that simple local measures such
as the degree of a node are good indicators of its effectiveness
as a blocker. The blocker identification problem has been
extensively studied in the public health community, where

• Introduction of a new influence propagation problem
in an adversarial setting where the goal of the second
player is to defeat the first within D time steps and
least amount of cost (i.e., number of seed nodes)
• NP-Hardness proof for the problem under both the
influence propagation models
• Approximation algorithm for the problem with a tight
performance bound.
• Experimental evaluation of the Approximation algorithm with collaboration network data
Experimental results show that utilizing the proposed algorithm, the second player can easily defeat the first, if the first
player utilizes the node degree or closeness centrality based
algorithms for the selection of the initial (seed) nodes. The
proposed algorithm also provides better performance for the
second player if she utilizes it instead of the algorithm to
maximize influence proposed in [3], in the sense that it requires selection of a fewer number of seed nodes to defeat
the first player.
The rest of the paper is organized as follows. The section II summarizes related work on influence propagation.
The section III describes the propagation models used in
the paper in detail. The sections IV, V and VI discuss the
problem statement, computational complexity and approximation algorithm results respectively. The results of experimental evaluation is presented in section VII and section
VIII concludes the paper.

2.

BACKGROUND AND RELATED WORK

The studies on identification of influential nodes in a social
network were triggered by a paper authored by Domingos
and Richardson [6]. They introduced the notion of “network
value” of a node in a social network and using a Markov
random field model where a joint distribution over all node
behavior is specified, computed the network value of the
nodes. Kempe, Kleinberg and Tardos followed up the work
in [6] by providing new models derived from mathematical
sociology and interacting particle systems [11]. They made
a number of important contributions by providing approximation algorithms for maximizing the spread of influence
in these models by utilizing the submodularity property of
the objective functions. In addition to providing algorithms
with provable performance guarantee, they also presented
experimental results on large collaboration networks. Their
experimental results showed that their greedy approximation algorithm significantly out-performed the node selection
heuristics based on degree centrality and distance centrality
[18].
The approximation algorithm proposed in [11] is computeintensive. Accordingly, several researchers approached the
issue of scalability from different directions. Chen et. al. in
[4] provided improvement of the original greedy algorithm of
[11] and proposed a degree discount heuristic to improve influence spread. Mathioudakis in [13] introduced the notion
of sparsification of influence networks and presented an algorithm, SPINE, to compute the “backbone” of the influence
network. Utilizing SPINE as a pre-processing step for the

587

the goal is to stop or slow down progress of an infectious
disease by immunizing a small set of key individuals in the
community.
As indicated in the previous section, the WMI problem
studied in this paper belongs to Class III (adversarial with
active adversary). Unfortunately, there exists only a handful of studies on problems belonging to Class III. Bharathi
et. al. were one of the earliest to study a Class III problem
[2]. They proposed a mathematical model for diffusion of
multiple innovations in a network, an approximation algorithm with a (1 − 1/e) performance guarantee for computing the best response to an opponent’s strategy. In addition
they prove that the “price of competition” of the game is
at most 2. While game theoretic framework was utilized
for deriving the results in [2], Carnes et al. used an algorithmic framework to study a Class III problem [3]. Their
research primarily extends the problem studied in [11] from
the Class I domain to the Class III domain. They study
the follower’s perspective (i.e., the player who entered the
market after the first player) and investigate how a follower
can maximize her influence in the network with a limited
budget, given that the first player has already entered the
market and influenced a certain number of key individuals
(nodes in the network). They prove that the influence maximization problem for the second player is NP-complete and
provide an approximation algorithm that is guaranteed to
produce a solution within 63% of the optimal. Adversarial models in evolutionary game dynamics was studied by
Istrate em et al. in [9].
In all the problems discussed in [2, 3] once a node adopts
an innovation (i.e., changes its color from white to red or
white to blue), it is not allowed to change its color, i.e., the
model precludes the possibility of an individual changing her
mind. However, the model considered by Nowak et al. in
[16] there are only red and blue nodes (no white nodes) and
the model allows a node to change its color from red to blue
and vice-versa. Although this model was developed to capture a biological phenomenon involving viruses and cells, this
model can be equally effective in capturing the phenomenon
of the spread of ideas and behaviors in human population.
Using evolutionary game theoretic and evolutionary graph
theoretic techniques, the authors establish fundamental laws
that govern choices of competing players regarding strategies.

3.

spective. Since this paper studies the problem with only
two competing players, the models proposed in [3] are more
relevant for this study than the one proposed in [2]. Accordingly, the influence propagation models of [3] are used here.
Since these models, Distance-based Model (DBM) and Wavepropagation Model (WPM), are generalization of the ICM,
the paper first discusses ICM and then DBM and WPM.

3.1

Independent Cascade Model

The social network is modeled as a graph G = (V, E),
where each node represents an individual. Each individual
may either be active (i.e., has adopted innovation) or inactive. A node can switch from an inactive state to an active
state but cannot switch back in the other direction. The
propagation process from the perspective of an inactivate
node v ∈ V can be described in the following way: With
passage of time, more and more of v’s neighbors become active and this may cause v to become active at some time step.
The activation of v in turn may trigger activation of some
of v’s inactive neighbors. In the ICM model there exists a
set of nodes V 0 ⊂ V that are active (seed nodes) initially
and the rest of the nodes are inactive. Influence propagation unfolds in discrete steps following a randomized process.
When a node v first becomes active in time step d, it has
a single chance to activate each of its inactive neighbors w
with probability pv,w at time step d + 1. If v succeeds, w
become active at d + 1. However, if v fails, it doesn’t get
another chance to turn w active. The process of conversion
of nodes from the inactive to the active state continues, till
no further activation is possible. Since v influences w with
probability pv,w , the v − w edge is considered active with
probability pv,w . The set of active edges is denoted by Ea .

3.2

Generalized ICM for Adversarial Scenario

The ICM can be adapted to handle adversarial scenario by
allowing the nodes to be in one of the following three states
- (i) active by adopting innovation A, (ii) active by adopting
innovation B, and (iii) inactive. We use the notation IA and
IB to indicate the initial adopters (seed nodes) of technologies A and B respectively. The nodes in the set V −(IA ∪IB )
are the nodes that are inactive initially. The sets IA and IB
are disjoint, i.e., IA ∩ IB = ∅. Just as in ICM, an active
node v may influence each one of its inactive neighbors w
with probability pv,w . However, in an adversarial scenario,
an inactive node w, may be in a situation where one of its
active neighbor v attempts to influence w with innovation A,
whereas another active neighbor u attempts to influence w
with innovation B. In order to deal with this situation, the
authors in [3] proposed two new models - (i) Distance-based
Model, and (ii) Wave-propagation Model. The models specify the probability with which the node w will be influenced,
when its active neighbors attempt to influence w with two
competing technologies. The GICM operates on a random
subgraph of the social network graph G = (V, E), where each
edge is included independently with probability pv,w . The
details of these two models are described in the following
two subsections.

INFLUENCE PROPAGATION MODELS

A number of influence propagation models for the
non-adversarial scenario have been proposed in the literature [11]. Among these, the Linear Threshold Model (LTM)
and the Independent Cascade Model (ICM) have drawn most
attention in the research community. As indicated earlier,
the literature on influence propagation in adversarial scenario with active adversaries is very sparse [2, 3]. Bharati
et al. in [2] and Carnes et al. in [3] have studied influence
propagation in adversarial scenario with active adversaries,
and have proposed two different models for it. Both of these
two models are generalizations of the Independent Cascade
Model. The model proposed in [2] is suitable for a multiplayer scenario, whereas the model proposed in [3] is for two
competing players. Bharati et al. in [2] study the problem from a game-theoretic perspective and focus on finding
best response strategies for the players. Carnes et al. on
the other hand study the problem from an algorithmic per-

3.3

Distance-based Model

Suppose that du (I, Ea ) denotes the shortest path distance
from the node u to the node set I where I = IA ∪IB along the
active edges in the edge set Ea . If u is not connected to any
node of I using only the active edges Ea , then du (I, Ea ) =

588

L1

Ln

...

e3

...

...

y1

en

s2

s3

y2

xn
...

e2

s1

a

sm

x1

ynr

Figure 1: Graph G = (V, E) of WMI instance in set
cover reduction

5.1

Distance-based Model

Decision version of WMI: Is there a set IB where |IB | ≤ M
and σ2 (IA , IB , D) > σ1 (IA , IB , D)?
Theorem 1. WMI is NP-hard for the distance-based model.
Proof: In order to prove that WMI is NP-hard when diffusion is based on distance based model, we reduce the NPcompete Set Cover problem to W M I. The decision version
of the Set Cover problem is defined in the following way: A
ground set of elements S = {e1 , e2 , . . . , en }, a collection of
sets C = {s1 , s2 , . . . , sm } such that si ⊆ S and a positive
integer K ≤ |C| are given. The question is whether there
exists a collection Q ⊆ C that covers all the elements in S
and |Q| ≤ K.
Given an instance of set cover problem we construct an
instance of W M I. We compute G = (V, E) in the following
way. For every element ei ∈ S we add a node ei and for
every set sj ∈ C we add a node sj to V . We add an edge
(ei , sj ) to E for every ei and sj if ei ∈ sj . Also, we add a
node a and nodes x1 , . . . , xn to V . Then, for every ei we
add edges (a, xi ) and (xi , ei ) to E. Moreover, for every ei
we add a set of r nodes, Li = {li,j |1 ≤ j ≤ r} to V and
we connect them directly to ei . We identify the value of r
later in the proof. Finally, we add n × r additional nodes,
y1 , . . . , yn×r , to V and edges (yt , a), 1 ≤ t ≤ n × r (Fig. 1).
We consider that all edges are active; i.e., pu,v = 1 for all
edges in E. We assign D = 4 equal to the diameter of the
graph G, M = K and IA = {a}.
Now, we show that the set cover problem has a solution if
and only if there is a set IB ⊆ V −IA such that |IB | ≤ M and
σ2 (IA , IB , D) > σ1 (IA , IB , D). First we consider that there
is a collection Q ⊆ C that covers S and |Q| ≤ K. Then IB
includes all nodes sj corresponding to the sets in Q. In this
case, all ei will be at distance one from IB and two from IA .
So, all ei and the nodes in Li will adopt IB with probability
one. Moreover, the nodes sj ∈
/ IB are two hops away from
IB while 3 hops away from IA . Hence, all nodes sj will adopt
IB . Therefore, we have σ2 (IA , IB , D) = m + n(1 + r); so,
σ2 (IA , IB , D) > σ1 (IA , IB , D).
Next, we show that if there is no collection Q of size K
that covers all elements then there is no set IB ⊆ V − IA
of size M where σ2 (IA , IB , D) > σ1 (IA , IB , D). Considering that set cover does not have a solution, there should be
at least one ei whose distance from IB cannot be one; so,
there is an ei and consequently nodes in Li that choose A
1
and the probability that
with the probability at least K+1
K
they choose B is at most K+1 . Also, at most K nodes from
x1 , . . . , xn can be at distance less than or equal to 1 from
IB . Hence n − K of them will adopt A with probability one.
Therefore, we have

Wave-propagation Model

In this model, in step d < D all nodes that are at distance
d − 1 from some node in I have adopted technology A or B
and all nodes that are farther than d − 1 from I have not
adopted any technology yet(where the distance is measured
with respect to active edges). Every node at distance d
from I chooses one of its neighbors at distance d − 1 from
I independently at random and adopt the same technology
as its neighbor. For every node u, S denotes the set of
neighbors of u that are closer to I than u; i.e., their distance
from I is du (I, Ea ) − 1. In this model Pi (u|IA , IB , Ea , D),
the probability that node u adopts innovation i ∈ {A, B} in
at most D steps, is computed as follows:
If du (I, Ea ) ≤ D,
P
P (v|I ,I ,E ,D)
Pi (u|IA , IB , Ea , D) = v∈S i |S|A B a ;
otherwise, it is zero.
In this model the expected number of nodes which adopt
i ∈ {A, B} will be computed in the following way:
"
#
X
σj (IA , IB , D) = E
Pi (u|IA , IB , Ea , D)
u∈V

where j = 1 if i = A; else j = 2 and the expectation is over
the set of active edges.

PROBLEM STATEMENT

The WMI problem can be stated informally as follows:
Given a diffusion model and the information that a subset of network nodes IA have already adopted innovation A
marketed by player P1 , what is the fewest number of nodes
should player P2 (marketing innovation B) target so that
by the end of D time steps, the number of nodes that adopt
innovation B will exceed the number of nodes that adopt
innovation A? If σ1 (IA , IB , D) and σ2 (IA , IB , D) denote the
expected number of nodes that adopt innovations A and B
respectively within D time steps, the objective of the WMI
problem is to
minimize | IB |
subject to
σ2 (IA , IB , D) > σ1 (IA , IB , D)

5.

e1

L3

x2

where j = 1 if i = A; else j = 2 and the expectation is over
the set of active edges.

4.

...

...

u∈V

3.4

L2

...

...

∞. Let νu (IA , du (I, Ea )) and νu (IB , du (I, Ea )) be the number of nodes in IA and IB respectively, at distance du (I, Ea )
from u along edges in Ea . The probability that node u
adopts innovation i ∈ {A, B} when maximum number of
propagation steps is D is denoted by Pi (u|IA , IB , Ea , D) and
is computed in the following way:
if du (I, Ea ) ≤ D,
νu (Ii ,du (I,Ea ))
;
Pi (u|IA , IB , Ea , D) = νu (IA ,du (I,E
a ))+νu (IB ,du (I,Ea ))
otherwise, it is zero.
In this model the expected number of nodes which adopt
i ∈ {A, B} will be computed in the following way:
#
"
X
σj (IA , IB , D) = E
Pi (u|IA , IB , Ea , D)

COMPUTATIONAL COMPLEXITY

In this section, we prove that W M I problem is NP-hard
for both propagation models.

589

K
σ2 (IA , IB , D) ≤ m + (n − 1)(1 + r) + K+1
(r + 1) + K and
1
σ1 (IA , IB , D) ≥ 1 + nr + n − K + K+1 (r + 1). We choose r in

Algorithm 1 GWMI
Input: G = (V, E), IA , D
Output: IB
1: while ω(IA , IB , D) ≤ 0 do
2: for every node i ∈ V − (IA ∪ IB ) do
3:
Compute Fi
4: end for
5: Select node j with maximum Fj
6: IB = IB ∪ {j}
7: end while
8: return IB

(m+2K−2)(K+1)+K−1
.
2

our instance large enough such that r >
1
Then we have 1 + nr + n − K + K+1
(r + 1) > m + (n − 1)(1 +
K
r) + K+1 (r + 1) + K; so σ2 (IA , IB , D) < σ1 (IA , IB , D).

5.2

Wave Propagation Model

Theorem 2. WMI is NP-hard for the wave propagation
model.

In [11], it is mentioned that computing the exact value of
σ1 (IA , ∅, D) efficiently is an open question. Similarly, there
is no known way to compute σ1 (IA , IB , D), σ2 (IA , IB , D)
in both propagation models efficiently. However, by sampling the active sets we can get a close approximation with
high probability. Given IA , IB and a set of active edges Ea ,
computation of σ1 and σ2 in both propagation models has
O(n3 ) time complexity since it needs computation of single all-pairs shortest paths. Given IA , IB and input graph
G, using sampling, we can then approximate σ1 and σ2 to
within (1+γ) for any γ > 0 where the running time depends
on 1/γ [3].

Proof: Similar to Theorem 1, we reduce decision version of
Set Cover problem to decision version of W M I when wave
propagation model is used for diffusion. We construct an
instance of W M I in the same way as in Theorem 1. The
only change that should be made to this instance is the value
of r which will be computed later.
We need to show that the set cover problem has a solution
if and only if there is a set IB ⊆ V − IA such that |IB | ≤ M
and σ2 (IA , IB , D) > σ1 (IA , IB , D). First we consider that
there is a collection Q ⊆ C that covers S and |Q| ≤ K.
Then IB includes all nodes sj corresponding to the sets in Q.
Similar to the proof of Theorem 1 we have σ2 (IA , IB , D) =
m + n(1 + r); so, σ2 (IA , IB , D) > σ1 (IA , IB , D).
Next, we show that if there is no collection Q of size K
that covers all elements then there is no set IB ⊆ V − IA
of size M where σ2 (IA , IB , D) > σ1 (IA , IB , D). Considering the construction of G and the fact that set cover does
not have a solution , there should be at least one ei whose
distance from IB cannot be one or smaller. Since the node
xi connected to this ei will have probability 1 to accept A
and the maximum number of nodes in first hop neighborhood of ei that are at distance one from IA ∪ IB is m + 1,
there is an ei and consequently nodes in Li that choose A
1
and the probability that
with the probability at least m+1
m
they choose B is at most m+1 . Also, at most K nodes from
x1 , . . . , xn or y1 , . . . , yn×r can be at distance less than or
equal to 1 from IB . Hence n(r + 1) − K of them will adopt
A with probability one. Therefore, we have
m
(r + 1) + K and
σ2 (IA , IB , D) ≤ m + (n − 1)(1 + r) + m+1
1
σ1 (IA , IB , D) ≥ 1+n(r+1)−K + m+1 (r+1). We choose r in

6.1

Theorem 3. GWMI has a log n approximation ratio.
t
be the set of B’s initial adopters selected by
Proof. Let IB
0
, D) =
GW M I at step t. Initially, IB is empty and ω(IA , IB
−σ1 (IA , ∅, D). In every iteration t, the nodes in the optiopt
t−1
∪
mal set of B’s initial adopters, IB
, will make ω(IA , IB
opt
opt
IB , D) positive. We denote the size of IB by OP T and
the size of the solution of GW M I by H. Therefore, There
t−1
will be at least one node in V − {IA ∪ IB
} that increases
t−1
, D) at least by
ω(IA , IB

t−1
|ω(IA ,IB
,d)|
.
OP T

Let, vt be the node

selected by GW M I at iteration t. Then, Fvt ≥
Therefore, for t < H we have
t
t−1
, D)| −
|ω(IA , IB
, D)| ≤ |ω(IA , IB

0
≤ |ω(IA , IB
, D)|(1 −

2

our instance large enough such that r > m2 + K(m + 1) − 23 .
1
Then we have 1 + n(r + 1) − K + m+1
(r + 1) > m + (n −
m
1)(1+r)+ m+1 (r +1)+K; so σ2 (IA , IB , D) < σ1 (IA , IB , D).

6.

Upper Bound Computation

t−1
|ω(IA ,IB
,D)|
.
OP T

t−1
|ω(IA , IB
, D)|
OP T

1 t
)
OP T

0
, D)| = σ1 (IA , ∅, D) ≤ n. Hence
Also, we know that |ω(IA , IB
we have
−t
1 t
t
|ω(IA , IB
, D)| ≤ n(1 −
) ≤ ne OP T .
OP T
Since adding a node to IB will increase ω(IA , IB , D) at least
t
by one, we need to find the smallest t that |ω(IA , IB
, D)| < 1.
Then adding at most one more node will make ω(IA , IB , D)
positive. Therefore, H ≤ 1 + OP T ln n. We note that this
proof holds for both propagation models.

APPROXIMATION ALGORITHM

Since we proved that finding the optimal solution for W M I
is hard, in this section we propose a greedy algorithm called
GW M I. In this algorithm either of the two propagation
models discussed before can be used as the diffusion process.
Let ω(IA , IB , D) be (σ2 (IA , IB , D) − σ1 (IA , IB , D)). We
define Fi to denote the amount of increase in the value of ω
when node i is added to IB ; i.e., Fi = ω(IA , IB ∪ {i}, D) −
ω(IA , IB , D). Initially IB is empty. Hence, ω(IA , IB , D) ≤
0. The algorithm executes through iterations and in each
iteration node i ∈ V − IA with the maximum Fi is selected.
The steps of the algorithm GW M I has been shown in Algorithm 1.

6.2

Lower Bound Computation

We now give a construction giving the lower bound for
GWMI when distance-based propagation model is used. Let
X and Y be disjoint sets of n2 vertices and G(n, 3/4) be the
Erdős-Renyi random graph on X ∪ Y with p = 3/4.
We take two new vertices u and v, connect u to all vertices
of X and v to all vertices of Y . Now, we add a disjoint star
S with n + 2 leaves and connect the center of the star to u
and v. This yields our graph G (Fig. 2).

590

larly v) is chosen, then increase is at most
G(n, 3/4)
X

Y

n/2

n/2

u

1
1
|X 0 | +
|Y 0 |
(1)
k+1
(k + 1)(k + 2)
1
n
1
n
=
(1/4)k +
(1/4)k + O(n3/4 ).
k+1
2
(k + 1)(k + 2)
2

1+

v

On the other hand, if a vertex x in X 0 ∪ Y 0 is chosen, the
increase is at least
1
1
|∂(x) ∩ X 0 | +
|∂(x) ∩ Y 0 |
(2)
k+1
k+1
1
|X 0 ∪ Y 0 \ ∂(X)|
+
(k + 1)(k + 2)
1
3
n
1
n
=2·
· (1/4)k +
(1/4)k+1 + O(n3/4 );
k+1 4
2
(k + 1)(k + 2)
2

Red set

Figure 2: Construction of G.

We consider that the center of the star is the only initial
adopter of A (red node), and pu,v is uniform and it is 1 for
all the edges of G and D = 3. An optimal set of initial
adopters of B (initial blue nodes) includes u, v and any of
the leaves of S. We claim that the greedy algorithm GW M I
will select Ω(log n) vertices with high probability, assuming
n is large enough.
In order to prove this we first state a technical lemma
giving a condition that G satisfies with high probability. Let
S ⊆ X ∪ Y . We say S is fair if

therefore, (2) - (1) is positive and hence the vertex in X 0 ∪
Y 0 will be chosen as desired. We note that this construction
is for sufficiently large n and (1/4)k n >> n3/4 .
1
Proof of Lemma 4. Let S ⊂ X∪Y , with |S| < 100
ln n.
Then
n
E[|X \ ∂S|] = (1/4)|S| (|X| − |X ∩ S|) = (1/4)|S| + O(ln n).
2

Let XS = |X \ ∂S|. Chernoff bounds imply that
1. |X \ ∂(S)| = (1/4)|S| n2 + O(n3/4 ) and |Y \ ∂(S)| =
(1/4)|S| n2 + O(n3/4 ).

P(|XS −E[XS ]| > n3/4 ) ≤ exp(−Ω(

n3/2
)) ≤ exp(−Ω(n1/2 )).
E[XS ]

Bounds for |Y \∂S| follow similarly. On the other hand there
are at most
1 ln n
!
100
X
n
1
≤
ln(n) · nln n ,
i
100
i=1

where ∂(S) is the set of one hop neighbors of vertices in
S.
We claim the following lemma, whose proof we defer:
Lemma 4. With probability 1 − o(1) every set S ⊂ X ∪ Y
1
ln(n) is fair. Furthermore, the induced graph
with |S| < 100
on X ∪ Y has diameter 2, every vertex in Y is at distance at
most 2 from u and every vertex in X is at distance at most
2 from v.

sets S. Thus union bounds imply every set is fair with probability 1 − exp(−Ω(n1/2 )).
Note that the expected number of common neighbors between x and y in X 0 ∪ Y 0 is 9n
, and Chernoff bounds plus
16
union bounds imply every pair x and y is of distance 2 (and
in fact has (1 − o(1)) 9n
common neighbors). Likewise, u
16
expected neighbors and Cherand a vertex in Y have 3n
8
noff bounds imply that every pair has (1 + o(1)) 3n
common
8
neighbors. Likewise, for v and vertices in X. A union bound
over all events completes the proof.

Assuming Lemma 4 we prove the lower bound. In particular we prove the following: The greedy algorithm selects at
1
least 100
ln n vertices from X ∪ Y . We proceed by induction.
At the first step, the greedy algorithm has to choose between
a vertex in X ∪ Y , one of u or v, or one of the vertices in the
star. Selecting a vertex in the star will cause the number of
blue vertices to increase by one and red vertices to decrease,
a net change of two. Selecting u (or resp. v) will increase
blue (and decrease red) by a total of 1 + n2 + n4 ; since every
vertex in X will be at distance 1 from a blue vertex and
every vertex in Y will be at distance 2 from both u and the
red vertex if u is selected. On the other hand, by fairness, if
a vertex x in X ∪Y is selected; the increase in blue is at least
3n
+ n8 + O(n3/4 ); since 3n
+ O(n3/4 ) vertices are at distance
4
4
n
1 from x and the other 4 + O(n3/4 ) are at distance 2 from
both x and the red vertex. Therefore the greedy algorithm
will select from X ∪ Y at the first time.
Now suppose that the greedy algorithm has selected from
1
X ∪Y a total of k < 100
ln n times. Let B denote the selected
0
set, and X = X \ ∂(B) and Y 0 = Y \ ∂(B). Every vertex in
X 0 ∪ Y 0 is at distance two from all k blue vertices, and hence
k
they are currently blue with probability k+1
. Furthermore
0
0
by fairness X and Y are both of size (1/4)k n2 + O(n3/4 ).
Again, the greedy algorithm must choose: If u (or simi-

7.

SIMULATION

In this section we evaluate the performance of our approximation algorithm, GW M I, on a real network data set.
It has been suggested in [15] that the co-authorship graphs
are representative of typical social networks. As such, we
use the real collaboration network data set of the scientists posting preprints on the high-energy theory archive
at www.arxiv.org, 1995-1999 [14]. This network has 8361
nodes (authors) and 15751 edges. The largest connected
component has 5835 number of nodes (authors) and maximum distance between the nodes in a connected component
is 19.
Our experiments were conducted on a high performance
computer which is a 5K processor Dell Linux Cluster. The
program is parallelized with OpenMP, optimized with Intel
compiler and was executed on an 8 core compute node. The
cores in the node have equal access to a common pool of
shared memory. Each node is comprised of 2.66/2.83 GHz

591

processors, 8MB cache, 16GB memory and 8 cores. Since
our experiments required execution of the algorithm on a
large number of instantiation of a social network (the graphs
were different as their set of active edges were different), we
used OpenMP for parallelization of the graph instances for
the simulation with one data set.
In the first set of experiments we evaluate the performance of GWMI algorithm against the results obtained from
the heuristics based on node degree and closeness centrality.
These heuristics are most often used in social networks to
identify most influential nodes [11]. We also compare performance of GWMI with the greedy algorithm proposed in
[3] for selection of seed nodes for the second player P2 . In
our model the first player P1 is trying to market product
A and the second player P2 is trying to market product B.
Since WMI problem is NP-hard and the input data set is
large, computation of the optimal solution within a reasonable amount of time is unlikely. It may be noted that there is
no known way of computing the exact value of σ1 (IA , IB , D)
and σ2 (IA , IB , D) efficiently [11]. Accordingly, we use sampling of the active edge sets to obtain close approximation
of σ1 (IA , IB , D), σ2 (IA , IB , D) with high probability. As in
the experiments reported in papers [11, 3], we assign the
edge probabilities to be 0.1. In all the experiments we use
WPM as the diffusion model.
The node degree based heuristic selects the nodes in the
decreasing order of their degrees and the closeness centrality
based heuristic selects the nodes in the increasing order of
their average distance to other nodes. The distance between
two nodes that are not in the same connected component is
taken to be n, where n is number of nodes in the network.
In the greedy algorithm proposed in [3], in every iteration
the node that increases σ2 (IA , IB , D) the most is selected.
We refer to this algorithm as Second Player Influence Maximization (SPIM) algorithm. In these experiments, maximum number of propagation steps is taken to be 10, i.e.,
D = 10. In the experiments, the player P1 used node degree
based heuristic to select its k initial adopters. In our experiments, the size of initial adopters of A is varied from 20 to
100. The results of this set of experiments using the WPM
is shown in Fig. 3. The Fig. 3 shows that all five sizes of
the initial adopters of A (20, 40, 60, 80, 100), the GWMI
algorithm required the fewest number of initial adopters of
B necessary to defeat A’s influence at the end of time step
10. The legend Degree-Degree in Fig. 3 denotes that both
the players are using the node degree based heuristics to select the initial adopters. Similarly,the legend Degree-GWMI
denotes that while P1 is using the node degree based heuristics to select the initial adopters, P2 is using the GWMI
algorithm to do the same.
The Figs. 4 and 5 show the coverage (i.e., the number of
nodes influenced at the end of 10 time steps) for players P1
and P2 respectively. Although the GWMI algorithm does
not make an effort to minimize the coverage of P1 , it may
be observed from the Fig. 4, the coverage of P1 is less if
P2 uses GWMI instead of SPIM. Thus P2 is better off using
GWMI instead of SPIM, if in addition to be able to defeat
P1 with least investment (i.e., initial adopters), P2 wants
to have a smaller market share for P1 . The Fig. 5 shows
the coverage of P2 at the end of ten time steps. It may be
observed from the Fig. 5, that at all five data points the coverage for P2 is highest when she uses the SPIM algorithm.
This is not surprising as the stated goal of SPIM is to maxi-

Number of Initial Adopters of B

200
160

120
80

Degree-Closeness
Degree-Degree
Degree-SPIM
Degree-GWMI

40

0
0

20

40
60
80
100
Number of Initial Adopters of A

120

Figure 3: Number of initial adopters of B for different values of |IA |

350

Coverage of A

300
250

Degree-Degree
Degree-Closeness
Degree-GWMI
Degree-SPIM

200
150
100
20

40
60
Number of Initial Adopters of A

80

Figure 4: Expected number of nodes adopting A
after 10 propagation steps

mize P2 ’s coverage (influence). However, this figure may be
somewhat misleading because it does not provide the information pertaining to the number of initial adopters required
by the SPIM algorithm to achieve the higher coverage. By
its stated objective, the number of initial adopters required
by GWMI to defeat P1 cannot be higher than the the number of initial adopters required by SPIM. Once this is factored in, and we compute the coverage per initial adopter,
we find that the coverage per initial adopter of the SPIM algorithm is very close to that of the GWMI algorithm. This
is shown in Fig. 6.
From Fig. 3 it is clear that the node degree and centrality
based heuristics and the SPIM algorithm require a larger
number of initial adopters of B to beat A than is needed
by the GW M I algorithm. While this is a negative aspect
of SPIM (cost), it also has a positive aspect in the sense
that at the end of ten time steps, it also secures a larger
coverage for B (benefit). We compute the additional benefit
provided by the additional initial adopters. Let IB(X) be
the smallest set of initial adopters of B that is required by
algorithm X to defeat A and σ2(X) be the expected number
of nodes that adopt B after D propagation steps. Here X
can be node-degree or centrality based heuristic or the SPIM
algorithm. In the case, (σ2(X) − σ2(GW M I) ) indicates the
additional benefit and (|IB(X) | − |IB(GW M I) |) indicates the
additional cost. In this case, (σ2(X) − σ2(GW M I) )/(|IB(X) | −
|IB(GW M I) |) indicates the average market share gain of B
with each additional initial adopter when using algorithm

592

0.4

350
Extended Benefit of B per
Additional Initial Adopter

Coverage of B

300

Degree-Degree
Degree-Closeness
Degree-GWMI
Degree-SPIM

250

200
150

Degree-Degree
Degree-Closeness
Degree-SPIM

0.3
0.2

0.1
0
20

100
20

40
60
Number of Initial Adopters of A

Figure 8: Extended benefit that B can capture per
additional initial adopter with respect to GW M I

Coverage of B per Initial Adopter of B

but also (σ1(GW M I) − σ1(X) ). It introduces a notion of extended benefit by combining these two factors in the following
way: (σ2(X) − σ2(GW M I) ) − (σ1(GW M I) − σ1(X) ). With this
notion of extended benefit,

9
Degree-Degree
Degree-Closeness
Degree-GWMI
Degree-SPIM

7
6

((σ2(X) − σ2(GW M I) ) + (σ1(GW M I) − σ1(X) ))
|IB(X) | − |IB(GW M I) |

5
4
3

indicates the average market share gain of B with each additional initial adopter when using algorithm X. The Fig.
8 depicts the results for the heuristics and SPIM. It may be
observed from Fig. 7 that when extended benefit is considered, the average market share gain of B with each additional initial adopter diminishes even more drastically with
increase of the number of initial adopters of A, when it uses
the SPIM algorithm. Moreover, the gain of each additional
initial adopter is smaller than 1 and implies that the additional adopter is not worth its cost.
In the second set of experiments we investigate different
strategies for selection of initial adopters of A when P2 uses
GW M I. The strategies that we consider for selection of
initial adopters of A includes the greedy algorithm proposed
in [11] and heuristics based on node degree and closeness
centrality. In these experiments WPM is used as diffusion
model and D = 10.
Fig. 9 depicts the results of these experiments. We observe that the closeness-centrality based heuristic performs
poorly in comparison to other two algorithms. This is true
because the number of initial adopters of B that it needs
to defeat A’s overall influence (coverage) is much smaller
than the size of initial adopters of A. More specifically, for
closeness-centrality based heuristic, for |IA | values greater
than 60, the number of initial adopters of B is less than
50% of |IA |. This set of results show that if the influence
maximization algorithm (IM) proposed in [11] is used for
the selection of IA , it forces P2 to select a large set for IB in
order to be able to defeat P1 within D time steps.

2
1
0
20

40
60
Number of Initial Adopters of A

80

Figure 6: Expected number of nodes adopting B per
initial adopter of B after 10 propagation steps

Average Increase in Market Share
of B per additional initial adopter

X. The Fig. 7 depicts the results for the heuristics and
SPIM. The negative gains are not shown. It may be observed
from Fig. 7 that the average market share gain of B with
each additional initial adopter diminishes with increase of
the number of initial adopters of A, when it uses the SPIM
algorithm.
While the stated objective of P2 is to have a larger market
share than P1 with the fewest number of initial adopters, it
may also have two other unstated objectives - (i) to have a
large σ2(X) and (ii) a small σ1(X) for all X (σ1(X) be the
expected number of nodes that adopt A after D time steps).
Therefore while considering the benefit of the additional initial adopters, we can consider not only (σ2(X) − σ2(GW M I) )

12
Degree-Degree
10

Degree-Closeness
Degree-SPIM

8

6

8.

4

0
40
60
Number of Initial Adopters of A

CONCLUSION

In this paper we have introduced a new influence propagation problem in an adversarial setting where the goal
of the second player is to defeat the first within D time
steps and least cost, measured in terms of the number of
seed nodes. Considering two different influence propagation
models, we provided the NP-Hardness proof for the problem
and an approximation algorithm with a tight performance
bound. In addition, we evaluated the performance of the
approximation algorithm with collaboration network data.

2
20

80

80

Figure 5: Expected number of nodes adopting B
after 10 propagation steps

8

40
60
Number of Initial Adopters of A

80

Figure 7: Average market share increase that innovation B can capture per additional initial adopter
with respect to GW M I

593

Number of Initial Adopters of B

120

Degree-GWMI
IM-GWMI
Closeness-GWMI

100
80
60
40
20
0
0

20

40
60
80
Number of Initial adopters of A

100

120

Figure 9: Size of initial adopters of B for different
values of |IA |
We can envisage at least two new directions of research with
this problem. In the first direction, P2 is not aware of P1 ’s
choice. In the second direction, back and forth transition of
the nodes between two competing products is allowed.

9.

ACKNOWLEDGMENTS

The research was supported in part by a grant to the Center for the Study of Religion and Conflict at Arizona State
University (N00014-09-1-0815). The award was funded through
the Office of the Secretary of Defense Minerva program, and
managed out of the Office of Naval Research. The content
is solely the responsibility of the authors and does not necessarily represent the views of the Office of Naval Research.
In addition, it was also supported in part by the DTRA
grant HDTRA1-09-1-0032 and the AFOSR grant FA955009-1-0120.

10.

REFERENCES

[1] S. Bhagat, A. Goyal, and L. V. Lakshmanan.
Maximizing product adoption in social networks. In
Proceedings of the fifth ACM international conference
on Web search and data mining, WSDM ’12, pages
603–612, 2012.
[2] S. Bharathi, D. Kempe, and M. Salek. Competitive
influence maximization in social networks. In
Proceedings of the 3rd international conference on
Internet and network economics, WINE’07, pages
306–311, 2007.
[3] T. Carnes, C. Nagarajan, S. M. Wild, and A. van
Zuylen. Maximizing influence in a competitive social
network: a follower’s perspective. In Proceedings of the
ninth international conference on Electronic
commerce, ICEC ’07, pages 351–360, 2007.
[4] W. Chen, Y. Wang, and S. Yang. Efficient influence
maximization in social networks. In Proceedings of the
15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’09,
pages 199–208, 2009.
[5] K. Dave, R. Bhatt, and V. Varma. Modelling action
cascades in social networks. 2011.

594

[6] P. Domingos and M. Richardson. Mining the network
value of customers. In Proceedings of the seventh ACM
SIGKDD international conference on Knowledge
discovery and data mining, KDD ’01, pages 57–66,
2001.
[7] A. Goyal, F. Bonchi, L. V. S. Lakshmanan, and
S. Venkatasubramanian. Approximation analysis of
influence spread in social networks.
arXiv:1008.2005v4, 2011.
[8] H. Habiba, Y. Yu, T. Y. Berger-Wolf, and J. Saia.
Finding spread blockers in dynamic networks. In
Proceedings of the Second international conference on
Advances in social network mining and analysis,
SNAKDD’08, pages 55–76, 2010.
[9] G. Istrate, M. V. Marathe, and S. S. Ravi. Adversarial
models in evolutionary game dynamics. In Proceedings
of the twelfth annual ACM-SIAM symposium on
Discrete algorithms, SODA ’01, pages 719–720, 2001.
[10] Q. Jiang, G. Song, C. Gao, Y. Wang, W. Si, and
K. Xie. Simulated annealing based influence
maximization in social networks. 2011.
[11] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing
the spread of influence through a social network. In
Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ’03, pages 137–146, 2003.
[12] H. Li, S. S. Bhowmick, and A. Sun. Casino: towards
conformity-aware social influence analysis in online
social networks. In Proceedings of the 20th ACM
international conference on Information and
knowledge management, CIKM ’11, pages 1007–1012,
2011.
[13] M. Mathioudakis, F. Bonchi, C. Castillo, A. Gionis,
and A. Ukkonen. Sparsification of influence networks.
In Proceedings of the 17th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ’11, pages 529–537, 2011.
[14] M. Newman.
http://networkdata.ics.uci.edu/data/hep-th/.
[15] M. E. J. Newman. The structure of scientific
collaboration networks. Proceedings of the National
Academy of Sciences of the United States of America,
98(2):404–409, 2001.
[16] M. A. Nowak, C. E. Tarnita, and T. Antal.
Evolutionary dynamics in structured populations.
Philosophical Transactions of the Royal Society B:
Biological Sciences, 365(1537):19–30, 2010.
[17] Y. Wang, G. Cong, G. Song, and K. Xie.
Community-based greedy algorithm for mining top-k
influential nodes in mobile social networks. In
Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ’10, pages 1039–1048, 2010.
[18] S. Wasserman and K. Faust. Social Network Analysis:
Methods and Applications. Number 8 in Structural
analysis in the social sciences. Cambridge University
Press, 1 edition, 1994.

Fair Queuing with Round Robin: A New Packet Scheduling Algorithm for
Routers
Arunabha Sen and Ibraz Mohammed
Dept. of Computer Science
Arizona State University
USA
fasen, ibrazg@asu.edu

Ravikanth Samprathi
R & D Division
CISCO Systems
USA
rsamprat@cisco.com

Abstract
In the last few years several queuing policies have been
proposed to ensure fairness between competing requests at
a service point. Fair Queuing (FQ) algorithm due to Demers, Keshav and Shenkar is a queuing technique that attains near perfect fairness, where perfect fairness is considered to be the one attained by a fluid flow model. In data
network, head of the line processor sharing (PS) is considered to be the most fair algorithm. It has been shown
that the difference in throughput at any time, in any queue,
for any arrival pattern between the FQ and the PS discipline will never exceed MAX, where MAX is the maximum
packet size. This difference in throughput is taken as a metric for fairness measure of a queuing algorithm. The drawback of the FQ algorithm is its high packet processing overhead (O(log N )), where N is the number of active flows.
To alleviate this problem of high computational complexity,
Shreedhar and Varghese proposed a fair queuing algorithm
based on the idea of deficit round robin (DRR). Although
DRR reduces the packet processing overhead to O(1), its
fairness measure is considerably worse (3MAX) than that
of FQ (MAX). In this paper, we present a new round robin
based fair queuing algorithm (FQRR) whose packet processing overhead is O(1) and fairness measure is 2MAX.

1 Introduction
The packet scheduler in a router plays an important role
in determining how effectively the QoS service requirements are met by the packets belonging to a flow. Two simple packet scheduling algorithms are First Come First Serve
(FCFS) and Round Robin (RR) [5]. These two algorithms
however, cannot guarantee fair allocation of network resources in cases of contention between competing resource

Subir Bandyopadhyay
School of Computer Science
University of Windsor
CANADA
subir@cs.uwindsor.ca

requests by the flows. To address the issue of fairness,
several policies, such as Fair Queuing (FQ) and Weighted
Fair Queuing (WFQ) [3], Deficit Round Robin (DRR) [6],
Worst-case-Fair Weighted Fair Queuing (W F 2 Q) [1], have
been proposed in the last few years.
The FQ and WFQ algorithms proposed in [3] received
considerable attention in the research community due to
their near perfect fairness (established in [4]) and delay
bound. Since the algorithm has to maintain a priority queue
to determine the packet to be scheduled next, its complexity
is O(log N ) where N is the number of active flows through
the router. To alleviate the problem of high packet processing cost of FQ, Shreedhar and Varghese in [6] proposed the
low overhead Deficit Round Robin algorithm. Although
DRR reduces the packet processing cost to O(1), its fairness
measure (difference in throughput between the idealized PS
scheme and DRR) is considerably worse (3MAX) than that
of FQ (MAX), where MAX is the maximum packet size.
Our goal in undertaking this research was to attempt to
develop a fair queuing algorithm that would have the excellent fairness measure of the FQ algorithm (MAX) and low
packet processing overhead of DRR (O(1)). Although the
Fair Queuing with Round Robin (FQRR) algorithm being
proposed in this paper does not quite realize this objective,
we believe it makes an important step in that direction. We
show that FQRR has the low overhead of DRR (O(1)), and
its fairness measure is 2MAX, which is not as good as FQ
(MAX) but is better than that of DRR (3MAX). Considering
the fact that DRR is an important fair queuing algorithm that
can be used in a high speed router (as evidenced by the use
of DRR in CISCO’s Gigabit Switch Router 12016), we believe that FQRR may be used in applications where DRR is
used. We show that not only FQRR has a better worst-case
fairness measure (2MAX as compared to 3MAX), it consistently outperforms DRR in a simulation environment. Our
simulation results show that the average fairness measure
for FQRR varies between 0.81MAX to 1.64MAX, whereas

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE

for DRR it varies between 2.76MAX to 2.89MAX.

2 Prior Work
The idea of using round-robin scheduling for fair allocation of bandwidth was originally proposed by Nagle [5].
In his scheme the gateways maintained separate queues for
packets from individual flows. The queues are serviced in
a round-robin fashion. The goal was to prevent a flow from
arbitrarily increasing its share of the outgoing link bandwidth or the delay of other flows. Although at a first glance,
the scheme appears to have provided a satisfactory solution
to the fairness issue but on closer examination it reveals a
serious flaw. This is due to the fact that in a large heterogeneous network packet sizes are not required to be identical.
As round-robin servicing of the queues can be done in constant time [6], the complexity of this algorithm is O(1).
Demers, Keshav and Shenkar in [3] proposed a clever
technique to overcome the problem associated with Nagle’s
algorithm. They introduced a notion of virtual time such
that corresponding to any real time t there is a virtual time
R(t) associated with it. According to their scheme, the rate
of change of the virtual time is inversely related to the number of active queues at that time. Specifically,

d
1
R(t) =
dt
maxf1; Nact(t)g

(1)

where Nact (t) is the number of active queues at time t.
The virtual start time and virtual finish time of each
packet are computed on its arrival at the queue. Suppose
that the (real) arrival time of the i-th packet of -th queue is
i , R(i ) is the virtual time corresponding to real time i
and Pi is the (normalized) length of the packet. Then the
virtual start and finish times of the i-th packet of -th queue
(Si and Fi respectively) are computed by the following
recurrence relations, with S0 = F0 = 0.

Si
Fi

=
=

maxfFi 1 ; R(i )g
Si + Pi

(2)
(3)

There are two different versions of the FQ algorithm: (i) FQ
with earliest start time (FQS) and (ii) FQ with earliest finish time (FQF) [4]. In FQS, when the output link becomes
free after servicing a packet, it selects the packet with the
earliest virtual start time from the set of packets waiting for
service. In the case of FQF, it selects the packet with the
earliest finish time. Since the packet with the earliest start
time or the earliest finish time has to be selected from the
set of packets at the heads of various queues, the computational complexity of this algorithm is O(log N ), where N
is the number of queues. Greenberg and Madras in [4] have

Algorithm
RR [NAG87]
FQ [DEM89]
DRR[SHR96]
FQRR

Fairness Measure

Complexity

MAX
3MAX
2MAX

O(1)
O(log N )
O(1)
O(1)

1

Table 1.

established the following result for both the versions of the
FQ algorithm.
Theorem 1 For all possible realizations of the arrival
process, for every time t and for every queue :
jB (; P S; 0; t) B (; F Q; 0; t)j  MAX , where
B (; P S; 0; t) and B (; F Q; 0; t) are the throughputs between times 0 and t for the service disciplines PS and FQ
respectively.
Although the fairness measure of the FQ algorithm
is excellent, due to its high packet processing overhead
(O(log N )), this scheme may be not be very suitable for
high speed networks. To reduce the overhead of the FQ
algorithm, Shreedhar and Varghese in [6] proposed a variation of Nagle’s round-robin algorithm. Instead of processing exactly one packet from each queue as in Nagle’s algorithm, they associate a quanta of service for each queue
during each round-robin cycle. In addition, they also introduce a notion of deficit. Suppose that the size of the
quanta is Q bits, and during a round-robin cycle only S bits
(S < Q) from a queue  are serviced, even though packets
were waiting for service in this queue. In this case, during
the next cycle, the size of the quanta for this queue is incremented by an amount Q S , known as the deficit. The
complexity of this algorithm (O(1)), is significantly lower
than that of the FQ algorithm (O(log N )). However, this
benefit of reduced complexity is off-set by the reduction in
fairness measure. The fairness measure of this algorithm is
3MAX, whereas for the FQ algorithm it is MAX.
In spite of the reduced fairness measure of the DRR algorithm (3MAX compared to MAX of FQ), because of its low
packet processing overhead, the algorithm is used in one of
the fastest routers available in the market today (CISCO’s
Gigabit Switch Router 12016). The FQRR algorithm being
proposed here, has the same low overhead of DRR (O(1)),
but it is more fair than DRR. The computational complexity
and the fairness measures of a few scheduling algorithms
are given in table 1.

3 Fair Queuing with Round Robin
Fair Queuing with Round Robin (FQRR) is a roundrobin algorithm which utilizes some of the concepts

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE

developed in connection with the Fair Queuing algorithm
by Demers et. al in [3]. The notion of virtual time
developed in [3] is used in FQRR. As soon as the i-th
packet in queue  arrives, the FQRR algorithm computes
its virtual start time, Si and virtual finish time, Fi . A
virtual time quantum, Q is allocated to each queue for
each cycle of the round-robin process. The size of this
quantum is set equal to (real) time needed to process the
largest sized packet. If a server speed is normalized and
it is assumed that it services at 1 bit per second and the
largest sized packet has MAX bits, Q is taken as MAX. The
queues are served in a round robin fashion. During the k -th
round of the round-robin cycle, all the packets in queue
; (1    N ), whose virtual finishing time is at most
k  Q = k  MAX are served. To discriminate active
queues from the inactive ones, the algorithm maintains a
data structure called the ActiveList like the DRR algorithm
in [6]. All active queues are linked to the ActiveList and
removed from the ActiveList when the queue becomes
inactive. This prevents the router from having to check the
head of up to N packet queues before finding an active
queue that can actually be served. We present the FQRR
algorithm next and explain the operation of the algorithm
with the help of an example.

Real arrival time i
Transmission time Pi
Virtual start time Si
Virtual finish time Fi

Queue 
Pkt 1 Pkt 2
0
2
3
1
0
3
3
4

Queue 
Pkt 1 Pkt 2
1
2
1
4
1
2
2
6

Queue 
Pkt 1
3
3
2
5

Table 2.
head of queue m)
K = K + 1;
Q = K  MAX ;
for every queue i in the ActiveList
begin
while (Fji[i]  Q)
begin
dequeue j [i]-th packet at the head of the
queue i;
j [i] = j [i] + 1;
end
If queue i is empty then
Remove queue i from the ActiveList;
end
end
end

Initialization:
begin
for (i := 0; i < N ;
begin
S0i = 0;
F0i = 0;
end
K = 0;
Q = MAX;
end

i = i + 1)

Enqueue Module: (On arrival of i-th packet in queue )
begin
Enqueue the i-th packet in queue ;
R(i ) = virtual time corresponding to real time
of arrival i ;
Si = maximum [Fi 1 ; R(i )];
Fi = Si + Pi ; (*Pi is the length of the packet*)
If queue  does not exist in the ActiveList then
Insert queue  into ActiveList;
end
Dequeue Module:
begin
while (TRUE)
begin

j [m] = 1;

1

 m  N ; (j [m] is the packet at the

Example: We use the same example as in [4]. Suppose that
the arrival times, packet sizes and the queues are as given
in table II. The virtual start times and virtual finish times
of all the packets in all the queues are computed and are
shown in the table. Suppose that the maximum packet size
is 4 bits. Assuming a server that serves at 1 bit per unit of
time, Q = MAX = 4. According to the FQRR algorithm,
during cycle 1, all the packets in all the queues whose virtual
finish times are at most 4 will be serviced. Therefore, in this
example the following packets are serviced during the first
cycle:



Queue : The first and the second packets of queue 
are served because F1 = 3 and F2 = 4.



Queue  : The first packet of queue  is served because
F  = 2.
1



Queue 
 : No packet is served as there is no packet with
virtual finish time at most 4.

The second cycle begins at the completion of the first cycle
and during this cycle all the packets in all the queues whose
virtual finish times are at most 2*4 = 8 are served. Therefore, in this example the following packets are serviced during the second cycle:



Queue : No packet is served as the queue is empty.

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE



Queue  : The second packet of queue  is served because F2 = 6.

Proof: The proof of this lemma has not been included here
for the sake of brevity. The proof is given in [7].



Queue 
 : The first packet of queue 
 is served because
F 
 = 5.

Lemma 4 For every  and t,

1

The progress of packets through the server for PS, FQS,
FQF, DRR and FQRR is shown in figure 1. In the figure, t
represents real time, R(t) represents virtual time.

4 Analytical Results
The analytical results presented in this paper follow
closely the techniques used in [4]. Accordingly, we follow
the definitions and notations used in [4].
Definitions and Notations:
Maximum number of queues (Queues are numbered
from 1 through N )
Nact(t): The number of active queues at time t
i : The arrival time of the i-th job in queue 
Pi : The length of the i-th packet in queue 
MAX : Maximum packet size
R(t): The virtual time corresponding to real time t
R 1(t): The real time corresponding to virtual time t
Si : Virtual start time of the i-th job in queue 
Fi : Virtual finish time of the i-th job in queue 
PS: Head of the line Processor Sharing queuing discipline
FQRR: Fair Queuing with Round Robin queuing discipline
B (; D; s; t): The number of bits serviced from queue 
during the real time interval [s; t) using queuing discipline

D
Q:

Virtual time quantum allocated for each round of the
round-robin process
i : Start time (real) of the i-th job in queue  in FQRR
discipline
i : Finish time (real) of the i-th job in queue  in FQRR
discipline
nF QRR (t): The number of packets serviced in queue  by
time t in FQRR
Lemma 2 The PS system is busy at real time t if and only
if the FQRR system is busy at time t. More specifically, for
every time t,

X B !; P S; ; t X B !; F QRR; ; t
N

!=1

N

0

)=

Proof: The proof of this lemma is similar to that of lemma
3, but it has been not included here for the sake of brevity.
The proof is shown in [7].
Theorem 5 For all possible realizations of the arrival
process, for every time t and for every queue :

jB (; P S; 0; t) B (; F QRR; 0; t)j  2MAX
Proof: Follows from lemmas 3 and 4.

5 Computational Complexity

N:

(

B (; P S; 0; t) B (; F QRR; 0; t)  2MAX

!=1

(

0

)

Proof: The proof of this lemma has not been included here
for the sake of brevity. The proof is given in [7].
Lemma 3 For every  and t,

B (; P S; 0; t) B (; F QRR; 0; t) 

MAX .

2

In this section we present the packet processing overhead
associated with the FQRR algorithm and compare it with
the DRR algorithm.
(i) When a packet arrives at the input link of the
router, FQRR checks to see if this flow is already in the
ActiveList. This requires one basic operation (a comparison).
(ii) If the queue is not part of the ActiveList then FQRR
adds this queue to the ActiveList. One elementary operation is needed for this purpose. But this overhead is not associated with every single packet, but only with the packet
that turns an inactive queue to an active one. Therefore,
the overhead of this operation may be amortized over many
packets and consequently, the overhead associated with an
individual packet may be negligible in that situation. However, in a worst-case scenario, only one packet may have
this overhead associated with it. Accordingly, the overhead
associated with each packet can vary from 0 to 1 basic operation.
(iii) In the FQRR algorithm the virtual start time and the
virtual finish time of each packet are computed on its arrival
at the queue using equations 2 and 3 given in section II.
The (real) arrival time of the i-th packet of -th queue,
i is known. If both Fi 1 and R(i ) are known, then
Si can be computed with only one comparison operation.
Since Fi 1 is computed at the time of arrival of the (i 1)th packet in queue , it is already known at the time Si
is being computed. Next we find the number of operations
needed to compute R(i ).
In the router only two types of events take place - packet
arrival and packet departure. As a consequence of these
events, the number of active queues may or may not change.
One variable can be used to keep track of the number of
active queues and updated whenever necessary. If the virtual time is plotted against the real time the gradient of this

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE

t

R(t)

Queue α
Packet 1

0

0

1

1

2
3

2

4
5
6

3

7
8
9

4

10
11

5

12

6

PS

:

FQS

:

FQF

:

Packet 2

Queue β
Packet 1

Queue γ

Packet 2

(0,3,3)
111
000
0
1
0
1
111
000
0 11
1
0
1
00
00
11
00
11
0
1
00
11
00
11
00
11
0
1
00
11
00
11
(1,1,2)
00
11
0
1
00
11
00
00
11
000
111
00
11
0
1
00 11
11
00
11
00
11
000
111
00
11
0
1
00
11
00
11
00
11
0
1
00
11
(2,4,6)
00
11
00
11
0
1
00
11
00
00 11
11
00
11
0
1
00
11
00
11
00
11
00
11
00
11
0
1
00
11
11
00
11
00
00
11
00
11
00
11
0
1
00
11
00
11
0
1
00
11
00
11
(3,1,4)
00
11
0
1
00
11
00
11
11 11
00
00
000
111
00
11
00
11
00
11
00
11
00
11
00
11
000
111
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00 00
11
00
11
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00 00
11
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
0
1
00
11
00
11
00
11
00
11
0
1
00
11
00
11
0
1
00
11
00
11
0
1
00
11
00
11
0
1
00
11
00
11
00
11
0
1
00
11
0
1
00
11
0
1
00
11
00
11
0
1
0
00
11
001
11
00
11
0
1
0
1
00
11
0
1
0
1
00
11
00
11
0
1
0
1
00
11
00
11
0
1
0
1
00
11
0
00 1
11
0
1
00
11
0
00 1
11
0
1
00
11
0
1
00
11
0
1
0
1
00
11
0
1
0
1
00
11
0
1
0
1
00
11
0
1
0
1
00
11
0
1
0
1
00
11
0
1
0
00 1
11

111111111
000000000
000000000
111111111
0
1
0
00
11
00
11
0
00
11
00
11
00
11
01
1
0
1
00
11
00 1
11
0
1
00
11
00
11
00
11

DRR

:

FQRR

:

Packet 1

(2,3,5)
00
11
00
11

11
00
00
11
00
11
00
11
00
11
00
11
00
11
00
11
00
11
0
1
00
11
0
1
00
11
0
1
00
11
0
1
00
11
0
1
00
11
0
1
00
11
0
1
00
11
0
1
00
11
0
1
00
11
0
1
00
11
0
1
00
11
0
1
00
11
11
00
00
11
00
11
00
11
00
11
00 1
11
0
00
11
00
11
0
1
00
11
00
11
00
11
0
00 1
11
00
11
00
11
0
1
00
11
00
11
0
00
11
00 1
11
00
11
0
1
00
11
00
11
0
00 1
11
0
1
0
1
0
1
0
1

111111111111111111111
000000000000000000000
000000000000000000000
111111111111111111111
000
111
000
111
0001
111
011
00 1
0
000
111
000
111
0001
111
011
00 1
0

Figure 1. Packet processing in PS, FQS, FQF, DRR and FQRR. The boxes mark packet arrival times.
The (S, P, F) triplets next to the boxes indicate the virtual start times, packet lengths and virtual finish
times of the packets.

line will change only at times when the number of active
queues change. The virtual time R(i ) corresponding to
real time i can be computed as follows: Suppose that
 is the latest time prior to i when the number of active queues changed and Nact() is the number of active
queues between the instances of time  and i . In this case
R(i ) = R() + (i )=maxf1; Nact() g.
Therefore, the computation of R(i ) will require 4 elementary operations (an addition, a subtraction, a division
and a comparison). Once R(i ) is computed, Si can be
computed with one additional comparison operation. Finally, Fi can be computed with one more addition operation. Thus Si can be computed with five operations and Fi
can be computed with one additional operation.
(iv) When a particular round of service is initiated, the
round number (K ) is incremented and the virtual time quantum (Q) is updated. This constitutes two elementary operations (an addition and a multiplication). But this compu-

tation is done once for each round and not for each packet.
Therefore, the overhead is distributed among the packets
serviced from all the queues active during the current round.
Accordingly, the overhead associated with each packet can
vary from 0 to 2 basic operations.
(v) When a particular flow  has to be serviced, the algorithm checks to see if the virtual finish time Fi of the packet
at the head of the queue  is less than or equal to quantum
Q for this round. This constitutes one elementary operation
(a comparison). If the condition is satisfied then the packet
is removed and is sent to the output link for transmission.
(vi) The algorithm then checks to see if queue i has become empty. This requires one basic operation (comparison). If the queue has become empty then the queue is
removed from the ActiveList. This may require one additional operation (an assignment). Although two elementary
operations are needed if the queue is empty, they are not
associated with every single packet, but associated with all

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE

the packets that are serviced during a round. Accordingly,
the overhead associated with each packet can vary from 0
to 2 basic operations.
Thus the total overhead associated with processing of a
packet in the FQRR algorithm (measured in terms of the
number of basic operations) varies from 8 to 13.
A similar analysis shows that the total overhead associated with processing of a packet in the DRR algorithm
(measured in terms of the number of basic operations)
varies from 6 to 12.
The number of basic operations for the FQRR algorithm
is 8 to 13 and for the DRR algorithm it is 6 to 12. In the
worst case scenario FQRR requires one more operation than
DRR; and in the best case scenario, two more operations
are required. In view of the fact that the fairness measure of
FQRR is better (2MAX) compared to DRR (3MAX), this
additional overhead seems to be adequately justified.

6 Simulation Results
The objective in undertaking the simulation experiments
was to check if the worst case bounds of the fairness measure for the FQRR (2MAX) and the DRR (3MAX) are attained in practice. Although our analytical results showed
that the maximum difference in throughput of PS and FQRR
service disciplines is 2MAX, and in the case of DRR, it is
3MAX, it is conceivable that in practice, the difference in
throughput between the PS and the FQRR is A.MAX and
between the PS and the DRR is B.MAX, where B is less
than A. In other words, we wanted to check if in practice
the performance of these two disciplines comes anywhere
near their worst case behavior.
Our simulation is for a single router. The maximum
number of queues considered is 20. The arrival process is
Poisson. The packet sizes are drawn from a uniform distribution between 0 and MAX, where MAX is the maximum packet size. We consider two different scenarios, in
the first scenario there are no ill-behaved sources and in the
second scenario there are two ill-behaved sources. The simulation is conducted for fifty different values for the arrival
rate . The simulation is carried for 200 units of time, and
the result of the difference in throughput between PS and
FQRR (DRR) is noted at time t = 10; 20; : : : ; 200 for 20
different queues. MaxDiff is the maximum difference in
throughput between either PS and FQRR or PS and DRR
taken over all twenty queues and over all 20 instances of
time t = 10; 20; : : : ; 200. The experiments are conducted
for 10 different values for MAX and as such we have 10
such tables, which are not presented here for brevity. The
results are available in [7].
We have computed the mean and the standard deviation of the maximum difference in throughput between PS
and FQRR, and PS and DRR. For the results presented in

the table in [7], the mean of the maximum difference in
throughput between PS and FQRR is 1.63678MAX and between PS and DRR is 2.855349MAX, when there is no illbehaving source. In the case where ill-behaving sources are
present, the values are 1.64863MAX and 2.847862MAX
respectivly. In absence of ill-behaving sources, for different packet sizes, the results varied from 0.806968MAX
to 1.63678MAX for PS-FQRR and 2.790318MAX to
2.89866MAX for PS-DRR . In presence of ill-behaving
sources the differences varied from 0.806825MAX to
1.648630MAX and 2.766610MAX to 2.893551MAX respectively.
As predicted by our analytical results the difference in
throughput between PS and FQRR never exceeded 2MAX
in our 160,000 experiments. Similarly, as predicted in
[6] the difference in throughput between PS and DRR
never exceeded 3MAX. The fact that the simulation results
are completely compatible with the analytical results enhances the confidence of the analysis. From these simulation results, we also learn that the maximum difference
in throughput between PS and FQRR is considerably lower
(1.63678MAX) than that of its worst case bound (2MAX),
whereas the difference in case of PS and DRR can actually reach close (2.89866MAX) to its worst case bound of
3MAX. From these perspectives, it appears that the FQRR
scheme is a much fairer service discipline than DRR.

References
[1] J. C. R. Bennet and H. Zhang,“W F 2 Q: Worst-case fair
weighted queueing”, Proc. of IEEE INFOCOM’96, pp. 120128, San Francisco, March 1996.
[2] J. C. R. Bennet and H. Zhang,“Why WFQ is not good
enough for Integrated Services Networks”, Proc. of NOSSDAV, Shonan Village International Conference Center, Zushi,
Japan, April. 1996.
[3] A. Demers, S. Keshav and S. Shenkar,“Analysis and simulation of a fair queuing algorithm”, Journal of Internetworking
Research and Experience, pp. 3-26, October 1990. Also in
Proc. of SIGCOMM’89, pp. 3-12.
[4] A. G. Greenberg and N. Madras, “How fair is fair queuing”,
Journal of the Association of Computing Machinery, vol. 39,
no. 3, pp. 568-598, July 1992.
[5] J. B. Nagle, “On packet switches with infinite storage”, IEEE
Transactions on Communications, COM 35, no. 4, pp. 435438, 1987.
[6] M. Shreedhar and G. Varghese, “Efficient fair queuing using
deficit round robin”, IEEE/ACM Transactions on Networking, vol. 4, no. 3, pp. 375-385, June 1996. Also in Proc. of
SIGCOMM’95, pp. 231-243, Boston, MA, September 1995.
[7] A. Sen, R. Samprathi, I. Mohammed and S. Bandopadhyay
“Fair Queuing with Round Robin: A New Packet Scheduling
Algorithm for Routers”, Technical Report, Dept. of Computer Science & Engg., ASU, 2001.

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE

IEEE Workshop on Inter-Dependent Networks 2015

On the Entity Hardening Problem in Multi-layered
Interdependent Networks
Joydeep Banerjee, Arun Das, Chenyang Zhou, Anisha Mazumder and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {joydeep.banerjee, arun.das, czhou24, anisha.mazumder, asen}@asu.edu
Abstract—The power grid and the communication network
are highly interdependent on each other for their well being.
In recent times the research community has shown significant
interest in modeling such interdependent networks and studying
the impact of failures on these networks. Although a number
of models have been proposed, many of them are simplistic in
nature and fail to capture the complex interdependencies that
exist between the entities of these networks. To overcome the
limitations, recently an Implicative Interdependency Model that
utilizes Boolean Logic, was proposed and a number of problems
were studied. In this paper we study the “entity hardening”
problem, where by “entity hardening” we imply the ability of the
network operator to ensure that an adversary (be it Nature or
human) cannot take a network entity from operative to inoperative
state. Given that the network operator with a limited budget
can only harden k entities, the goal of the entity hardening
problem is to identify the set of k entities whose hardening
will ensure maximum benefit for the operator, i.e. maximally
reduce the ability of the adversary to degrade the network. We
classify the problem into four cases and show that the problem is
solvable in polynomial time for the first case, whereas for others
it is NP-complete. We provide an inapproximability result for the
second case, an approximation algorithm for the third case, and
a heuristic for the fourth (general) case. We evaluate the efficacy
of our heuristic using power and communication network data
of Maricopa County, Arizona. The experiments show that our
heuristic almost always produces near optimal results.

I.

I NTRODUCTION

The critical infrastructures of a nation form a complex symbiotic ecosystem where individual infrastructures are heavily
interdependent on each other for being fully functional. Two
such critical systems that rely heavily on each other for their
well being are the power and communication network infrastructures. For instance, power grid entities such as SCADA
systems, that are used to remotely operate power generation
units, receive their control commands over the communication
network infrastructure, while communication network entities
such as routers and base stations are inoperable without electric
power. Thus, failure introduced in the system either by Nature
(hurricanes), or man (terrorist attacks), can trigger further
failures in the system due to interdependencies between the
entities of the two infrastructures.
Although a number of models have been proposed for
This research was supported in part by the DTRA grant HDTRA1-09-10032, the AFOSR grant FA9550-09-1-0120, and the NSF grant 1441214. The
data for the Maricopa county communication network used in this research
was provided by GeoTel communications (www.geo-tel.com).

978-1-4673-7131-5/15/$31.00 ©2015 IEEE

648

modeling and analysis of interdependent multi-layered networks [1], [2], [3], [4], [5], [6], [7], [8], many of these
models are simplistic in nature and fail to capture the complex
interdependencies that exists between the entities of these
networks. As noted in [9], these models fail to model complex
interdependencies that may exist between network entities,
such as when entity ai is operational, if entities (i) bj and bk
and bl are operational, or (ii) bm and bn are operational, or (iii)
bp is operational. Graph based dependency models proposed in
the literature such as [3], [4], [5], [10], [6], [7] including [1],
[2] cannot capture such complex interdependency involving
both conjunctive and disjunctive terms between entities of
multi-layer networks. To overcome these limitations, an Implicative Interdependency Model that utilizes Boolean Logic,
was recently proposed in [9], and a number of problems
including computation of K most vulnerable nodes [9], root
cause of failure analysis [11], and progressive recovery from
failures [12], were studied using this model.
In this paper we study the “entity hardening” problem in
the interdependent power-communication network using the
Implicative Interdependency Model (IIM). By “entity hardening”, we imply the ability of the network operator to ensure
that an adversary (be it Nature or human), cannot take a network entity from an operative (operational) to an inoperative
(failed) state. We assume that the adversary is clever and
is capable of identifying the most vulnerable entities in the
network that causes maximum damage to the interdependent
system. However, the adversary does not have an unlimited
budget and has the resources to destroy at most K entities
of the interdependent network. The network operator is also
aware of adversary’s target entities for destruction. Since we
assume that once an entity is “hardened” by the network
operator it cannot be destroyed by the adversary, if all K
targets of the adversary are hardened by the network operator,
then the adversary cannot induce any failure in the network.
However, if due to resource limitations the network operator is
able to strengthen only k entities, where k < K, these k entities
have to be carefully chosen. The goal of the entity hardening
problem is to identify the set of k entities whose hardening
will ensure maximum benefit for the operator, i.e. maximally
reduce the adversary’s ability to degrade the network.
We classify the entity hardening problem into four different
cases depending on the nature of the interdependency relationships. We show that the first case can be solved in polynomial
time, and all other cases are shown to be NP-complete. We
provide an inapproximability result for the second case, an

IEEE Workshop on Inter-Dependent Networks 2015
2

Entities

approximation algorithm for the third case, and a heuristic
for the fourth (general) case. We evaluate the efficacy of our
heuristic using power and communication network data of
Maricopa County, Arizona. The experiments show that our
heuristic almost always produces near optimal results.
The paper is organized as follows, the IIM model is
presented in Section II, in Sections III and IV we formally
state the entity hardening problem and analyze its computational complexity, Section V outlines the optimal and heuristic
solutions to the problem, and finally Section VI shows the
experimental results.
II.

I NTERDEPENDENCY M ODEL

We now present an overview of the underlying IIM interdependency model [9]. IIM uses Boolean Logic to model
the interdependencies between network entities, these interdependent relationships are termed as Implicative Interdependency Relations (IDRs). We represent this interdependent network setting as I(A, B, F(A, B)), where sets A
and B are the power and communication network entities
respectively, and F(A, B) is the set of dependency relations,
or IDRs. Table I represents a sample interdependent network I(A, B, F(A, B)), where A = {a1 , a2 , a3 , a4 }, B =
{b1 , b2 , b3 } and F(A, B) is the set of IDRs (dependency
relations) between the entities of A and B. In this example,
the IDR b1 ← a1 a3 + a2 implies that entity b1 is operational
when both the entities a1 and a3 are operational, or entity a2
is operational. The conjunction of entities, such as a1 a3 , is
also referred to as a minterm.
It may be noted that although in the IDRs of Table I, A (B)
type entities appear either on the left hand side or on the right
hand side of an IDR, the IIM does not require that the A (B)
type entities appear only on one side of an IDR. In other words,
an IDR can be of the form ai ← aq bj bk bl + ar bm bn + bp + as ,
or ai ← aq ar + as , or ai ← bj bk + bp .
Power Network
a1 ← b1 b2
a2 ← b1 + b2
a3 ← b1 + b2 + b3
a4 ← b1 + b3

Comm. Network
b1 ← a1 a3 + a2
b2 ← a1 a2 a3
b3 ← a1 + a2 + a3
−−

TABLE I: Implicative Interdependency Relations of a sample network

Given a set of inoperable (failed) entities, a time stepped
failure cascade can be derived from the dependency relationships outlined in the IDR set. For example, for the interdependent network outlined in Table I, Table II shows the failure
propagation when entities {a2 , b3 } fail at the initial time step
(t = 0). It may be noted that the model assumes that dependent
entities fail immediately in the next time step, for example,
when {a2 , b3 } fail at t = 0, b2 fails at t = 1 as b2 is dependent
on a2 for its survival. The system reaches a steady state when
the failure propagation process stops. In this example, when
{a2 , b3 } fail at t = 0, the steady state is reached at time step
t = 4.
A primary consideration for using this model is the accurate
formulation of the IDRs that is representative of the underlying
physical power and communication network infrastructures.
This can either be done by careful analysis as done in [8], or
by consultation with experts of these infrastructures. We utilize
IIM to model the interdependency between the two networks
and analyze the entity hardening problem in this setting.

649

a1
a2
a3
a4
b1
b2
b3

0

1

0
1
0
0
0
0
1

0
1
0
0
0
1
1

Time Steps (t)
2
3
4
1
1
0
0
0
1
1

1
1
0
0
1
1
1

1
1
1
1
1
1
1

5

6

1
1
1
1
1
1
1

1
1
1
1
1
1
1

TABLE II: Failure cascade propagation when entities {a2 , b3 } fail at
time step t = 0. A value of 1 denotes entity failure, and 0 otherwise

III.

P ROBLEM F ORMULATION

Before we make a formal statement of the entity hardening
problem in the IIM setting, we explain it with the help of an
example. Consider an interdependent system as outlined in the
IDR set shown in Table I. It may be easily checked that when
the adversary budget is K= 2, the most vulnerable entities
of this system are {a2 , b3 }. If the network operator doesn’t
harden any one of the entities a2 or b3 , then in this example
all the network entities eventually fail, as seen from the fault
propagation in Table II. When the network operator chooses
to harden both a2 and b3 then none of the entities in the
network fail if the adversary restricts the attack only to the two
most vulnerable entities of the network, which in this example
happens to be {a2 , b3 }. If the network operator has resources
to harden only one entity and the operator chooses to harden
a2 , the destruction of b3 by the adversary will eventually lead
to the failure of no other entities of the network, as shown in
Table III(a). If on the other hand, the network operator chooses
to harden b3 , destruction by the adversary of a2 will eventually
lead to the failure of the entities {a2 , b2 , a1 , b1 } as shown in
Table III(b). Clearly in this scenario the operator should harden
a2 instead of b3 .
Definition: Kill Set of a set of Entities(S): The kill set of a
set of entities S, is the set of all entities that will eventually
fail due to failure of S and the interdependencies between the
entities of the network as given by the set of IDR’s. The kill
set of a set of entities S is denoted by KillSet(S).
It may be noted that the search for k entities to be hardened
is restricted to the KillSet(S), where S is the set of K
most vulnerable entities in the network, because hardening any
entity not in KillSet(S) does not provide any benefit to the
network operator. In this study we also assume that the set of
K most vulnerable entities in the network is unique.
Entities
0
a1
a2
a3
a4
b1
b2
b3

0
∗
0
0
0
0
1

Time Steps (t)
1
2
3
0
∗
0
0
0
0
1

0
∗
0
0
0
0
1

0
∗
0
0
0
0
1

(a) Entity a2 is hardened

Entities
4
0
∗
0
0
0
0
0

0
a1
a2
a3
a4
b1
b2
b3

0
1
0
0
0
0
∗

Time Steps (t)
1
2
3
0
1
0
0
0
1
∗

1
1
0
0
0
1
∗

1
1
0
0
1
1
∗

4
1
1
0
0
1
1
∗

(b) Entity b3 is hardened

TABLE III: Failure cascade propagation with entity hardening. Entities {a2 , b3 } are attacked at time step t = 0. A value of 1 denotes
entity failure, 0 otherwise. A hardened entity is denoted by ∗.

We now proceed to formulate the entity hardening
problem formally. Given an interdependent network system
I(A, B, F(A, B)), and the set of K most vulnerable entities
of the system A0 ∪ B 0 , where A0 ⊆ A and B 0 ⊆ B:

IEEE Workshop on Inter-Dependent Networks 2015
3

The Entity Hardening (ENH) problem
INSTANCE: Given:
(i) An interdependent network system I(A, B, F(A, B)),
where the sets A and B represent the entities of the two
networks, and F(A, B) is the set of IDRs.
(ii) The set of K most vulnerable entities of the system
A0 ∪ B 0 , where A0 ⊆ A and B 0 ⊆ B
(iii) Two positive integers k, k < K and EF .
QUESTION:Is there a set of entities H = A00 ∪ B 00 , A00 ⊆
A, B 00 ⊆ B, |H| ≤ k, such that hardening H entities results
in no more than EF entities to fail after entities A0 ∪ B 0 fail
at time step t = 0.
We note some of the assumptions for the ENH problem:
First, we assume that once an entity is hardened, it is always
operational and does not fail at any time step of the observation, even when the entity is part of the K most vulnerable
entities. Second, we assume that k < K, as otherwise the
selection of K entities for hardening ensures that no entities
fail at all. Finally, as noted earlier, we assume that the set of
K most vulnerable entities in the network is unique. We now
proceed to analyze the computational complexity of the ENH
problem.
IV.

C OMPUTATIONAL C OMPLEXITY A NALYSIS

For an interdependent network I(A, B, F(A, B)) the IDRs
can be represented in four different forms. We analyze the
computational complexity of the ENH problem for each of
these cases separately.
A. Case I: Problem Instance with One Minterm of Size One
The IDRs of Case I have a single minterm of size 1. This
can be represented as xi ← yj , where xi and yj are entities of
network A(B) and B(A) respectively. Algorithm 1 solves the
ENH problem for Case I optimally in polynomial time. The
proof of this claim is left out due to lack of space, the proof
can be found in [13].
Algorithm 1: Entity Hardening Algorithm for systems
with Case I type interdependencies

1
2
3
4
5
6
7

8
9
10

B. Case II: Problem Instance with One Minterm of Arbitrary
Size
The IDRs of Case II have a single
Qpminterm of arbitrary
size. This can be represented as xi ← j=1 yj , where xi and
yj are entities of network A(B) and B(A) respectively and the
size of the minterm is p. The Entity Hardening problem with
respect to Case II is NP-complete and is proved in Theorem
1. An inapproximability proof for this case of the problem is
given in Theorem 2
Theorem 1. The Entity Hardening problem for Case II is NP
Complete
Proof: The Entity Hardening problem for case II is proved
to be NP complete by giving a reduction from the Densest pSubhypergraph problem [14], a known NP-complete problem.
An instance of the Densest p-Subhypergraph problem includes
a hypergraph G = (V, E), a parameter p and a parameter M .
The problem asks the question whether there exists a set of
vertices |V 0 | ⊆ V and |V 0 | ≤ p such that the subgraph induced
with this set of vertices has at least M hyperedges. From an
instance of the Densest p-Subhypergraph problem we create
an instance of the ENH problem in the following way. For
each vertex vi and each hyperedge ej an entity bi and aj are
added to the set B and A respectively. For each hyperedge ej
with ej = {vm , vn , vq } (say) an IDR of form aj ← bm bn bq is
created. It is assumed that the value of K is set of |V |. The
values of k and EF are set to p and |V | + |E| − p − M (where
|A| = |V | and |B| = |E|) respectively.
In the constructed instance only entities of set A are
dependent on entities of set B. Additionally the dependency
for an entity ai consists of conjunction of entities in set B.
Hence for an entity ai ∈ A to fail, either it itself has to fail
initially or all entities to which ai is dependent on has to fail.
It is to be noted that the entities in set B has no induced failure
i.e., there is no cascade. Following from this assertion, with
K = p, the solution A0 = ∅ and B 0 = B would fail all entities
in set A ∪ B. Moreover this is the single unique solution to
the problem instance. This is because by including one entity
ai in the initial failure set would result in not failing at least
one entity bj for a given budget K = p. Hence it won’t fail
the entire set of entities in A ∪ B.
If an entity in set A is hardened then it would have no effect
in failure prevention of any other entities. Whereas hardening
an entity bm ∈ B might result in failure prevention of an entity
ai ∈ A with IDR aj ← bm bn bq provided that entities bn , bq
are also defended. With k = p (and K ≤ |V | = |B|) it can be
ensured that entities to be defended are from set B 0 .

Data: An interdependent network I(A, B, F(A, B)), set of
K most vulnerable entities A0 ∪ B 0 , A0 ⊆ A, B 0 ⊆ B,
hardening budget k and a set H = ∅.
Result: Set of hardened entities H.
begin
For each entity xi ∈ (A0 ∪ B 0 ) compute the set of kill sets
C = {Cx1 , Cx2 , ..., CxK }, where Cxi = KillSet(xi ) ;
Create a copy D = {Dx1 , Dx2 , ..., DxK } of set C ;
for (i=1; i ≤ K; i++) do
for (j=1, j 6= i; j ≤ K; j++) do
if Cxj ⊂ Cxi then
Dxi ← Dxi \ Dxj ;

To prove the theorem consider that there is a solution to the
Densest p-Subhypergraph problem. Then there exist p vertices
which induces a subgraph which has at least M hyperedges.
Hardening the entities bi ∈ B 0 for each vertex vi in the solution
of the Densest p-Subhypergraph problem would then ensure
that at least M entities in set A are protected from failure.
This is because the entities in set A for which the failure
is prevented corresponds to the hyperedges in the induced
subgraph. Thus the number of entities that fail after hardening
p entities is at most |V | + |E| − p − M , solving the ENH
problem. Now consider that there is a solution to the ENH
problem. As previously stated, the entities to be hardened will
always be from set B 0 . So defending p entities from set B 0

Choose the top k sets from D with highest cardinality ;
For each of the Dxi ⊆ D sets chosen in Step 8,
H ← H ∪ xi ;
return H

650

IEEE Workshop on Inter-Dependent Networks 2015
4

Theorem 2. For an interdependent network I(A, B, F(A, B))
with n = |A ∪ B| and F(A, B) having IDRs of form Case II,
it is hard to approximate the ENH problem within a factor of
1
for some λ > 0.
log(n)λ
2

ai ∈ A there exist at least one entity (in set B) that is hardened.
Hence the number of entities that fails after hardening is m−M
which is equal to EF , thus solving the ENH problem. Now,
consider that there is a solution to the ENH problem. As
discussed above the entities to be hardened should be from
set B 0 . To achieve EF = m − M with k = M , no entities
in the set A must fail. Hence for each entity ai ∈ A at least
one entity in set B that appears in its IDR has to be hardened.
Thus, it directly follows that the union of subsets in set S
corresponding to the entities hardened is equal to the set S,
solving the Set Cover Problem.

Proof: From Theorem 1, Densest p-Subhypergraph problem has been shown to be a special case of the ENH problem
with IDRs of form Case II. Densest p-Subhypergraph problem
1
is proved to be inapproximable within a factor of log(n)
λ
2
(λ > 0) in [14]. Hence the theorem follows.

1) Approximation Scheme for Case 3: In this subsection we
provide an approximation algorithm for Case 3 of the problem.
For an interdependent network I(A, B, F(A, B)) with the
initial failed set of entities as A0 ∪ B 0 we define Protection
Set of each entity as follows.

C. Case III: Problem Instance with an Arbitrary Number of
Minterm of Size One

Definition: For an entity xi ∈ A ∪ B the Protection Set is
defined as the entities that would be prevented from failure
by hardening the entity xi when all entities in A0 ∪ B 0 fails
initially. This is represented as P (xi |A0 ∪ B 0 ).

would result in failure prevention of at least M entities in set
A such that EF ≤ |V | + |E| − p − M . Hence, the vertex
induced subgraph would have at least M hyperedges when
vertices corresponding to the entities hardened are included
in the solution of the Densest p-Subhypergraph problem, thus
solving it.

The IDRs of Case III have arbitrary number
Pp of minterm of
size 1. This can be represented as xi ← q=1 yq , where xi
and yq are entities of network A(B) and B(A) respectively
and the number of minterms are p. The ENH problem with
respect to Case III is NP-complete and is proved in Theorem
3.
Theorem 3. The ENH problem for Case III is NP Complete
Proof: The ENH problem for case III is proved to be NP
complete by giving a reduction from the Set Cover Problem,
a well known NP-complete problem. An instance of the Set
Cover problem includes a set S = {x1 , x2 , ..., xn }, a set S =
{S1 , S2 , ..., Sm } where Si ⊆ S and a positive integer M . The
problem asks the question whether there exists at most M
subsets from set S whose union would result in the set S. From
an instance of the set cover problem we create an instance of
the ENH problem in the following way. For each element xi
in set S we add an entity ai in set A. For each subset Si in
set S we add an entity bi in set B. For all subsets in S, say
Sp , Sm , Sn , which has the element xi there is an IDR of form
ai ← bm + bn + bl . The values of positive integers k and EF
are set to M and m − M respectively. It is assumed that the
value of K = m.
With similar reasoning as that of Case II it can be shown
that for K = m the maximum number of node failures (i.e.
failure of all entities in A ∪ B) would occur if A0 = ∅ and
B 0 = B. This is also the single unique solution to the problem
instance.
The constructed instance also ensures that the entities to
be hardened are from set B 0 (A0 not considered as it is equal
to ∅). This is because protecting an entity ai ∈ A would only
result in prevention of its own failure whereas protecting an
entity bj ∈ B would result in failure prevention of its own and
all other entities in set A for which it appears in its IDR.
To begin with the proof, consider that there is a solution
to the Set Cover problem. Then there exist M subsets (or
elements in set S) whose union results in the set S. Hardening
the entities in set B corresponding to the subsets selected
would ensure that all entities in set A are prevented from
failure. This is because for the dependency of each entity

651

The Protection Set of each entity can be computed in O((n +
m)2 ) where n and m are the number of entities and number of minterms respectively in an interdependent network
I(A, B, F(A, B)) .
Theorem 4. For two entities xi , xj ∈ A ∪ B, P (xi |A0 ∪ B 0 ) ∪
P (xj |A0 ∪ B 0 ) = P (xi , xj |A0 ∪ B 0 ) when IDRs are of form
Case III.
Proof: Assume that defending two entities xi and xj
would result in preventing failure of P (xi , xj |A0 ∪ B 0 ) entities
with |P (xi |A0 ∪ B 0 ) ∪ P (xj |A0 ∪ B 0 )| < |P (xi , xj |A0 ∪ B 0 )|.
Then there exist at least one entity xp ∈
/ P (xi |A0 ∪ B 0 ) ∪
P (xj |A0 ∪ B 0 ) such that it’s failure is prevented only if xi and
xj is protected together. So two entities xm and xn (with xm ∈
P (xi |A0 ∪B 0 ) and xn ∈ P (xj |A0 ∪B 0 ) or vice versa) have to be
present in the IDR of xp . As the IDRs are of form Case III so if
any one of xm or xn is protected then xp is protected, hence
a contradiction. On the other way round P (xi , xj |A0 ∪ B 0 )
contains all entities which would be prevented from failure
if xi or xj is defended alone. So it directly follows that
|P (xi |A0 ∪ B 0 ) ∪ P (xj |A0 ∪ B 0 )| > |P (xi , xj |A0 ∪ B 0 )| is
not possible. Hence the theorem holds.
Theorem 5. There exists an 1 − 1e approximation algorithm
that approximates the ENH problem for Case III.
Proof: The approximation algorithm is constructed by
modeling the problem as Maximum Coverage problem. An
instance of the maximum coverage problem consists of a
set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where
Si ⊆ S and a positive integer M . The objective of the problem
is to find a set S 0 ⊆ S and |S 0 | ≤ M such that ∪Si ∈S Si
is maximized. For a given initial failure set A0 ∪ B 0 with
|A0 |+|B 0 | ≤ K, let P (xi |A0 ∪B 0 ) denote the protection set for
each entity xi ∈ A ∪ B. We construct a set S = A ∪ B and for
each entity xi a set Sxi ⊆ S such that Sxi = P (xi |A0 ∪ B 0 ).
Each set Sxi is added as an element of a set S. The conversion
of the problem to Maximum Coverage problem can be done
in polynomial time. By Theorem 4 defending a set of entities
X ⊆ S would result in failure prevention of ∪xi ∈X Sxi entities.
Hence, with the constructed sets S and S and a positive integer

IEEE Workshop on Inter-Dependent Networks 2015
5

M (with M = k) finding the Maximum Coverage would
ensure the failure protection of maximum number of entities in
A ∪ B. This is same as the ENH problem of Case III. As there
exists an 1 − 1e approximation algorithm for the Maximum
Coverage problem hence the theorem holds.

the effect of hardening an entity in the current iteration on
entities hardened in the subsequent iterations.
Algorithm 2: Heuristic Solution to the ENH Problem

D. Case IV: Problem Instance with an Arbitrary Number of
Minterms of Arbitrary Size
The IDRs of Case IV have arbitrary number of minterm
of
Pp arbitrary
Qqj1 size. This can be represented as xi ←
j2 =1 yj2 , where xi and yj2 are entities of network
j1 =1
A(B) and B(A) respectively and there are p minterms each
of size qj1 .

1
2
3
4

Theorem 6. The Entity Hardening problem for Case IV is NP
Complete
5

Proof: Case II and Case III are special cases of Case
IV. Hence following from Theorem 1 and Theorem 3 the
computational complexity of the Entity Hardening problem is
NP-complete in Case IV.
V.

6
7
8
9

S OLUTIONS TO THE E NTITY H ARDENING P ROBLEM

A. Optimal Solution using Integer Linear Programming

10

Due to lack of space we omit the ILP formulation that finds
the optimal solution to the ENH problem. This formulation can
be found in [13].

11
12

16

Update S ← S \ P (xd |S 0 );
Update F(A, B) by removing (i) IDRs corresponding
to all entities in P (xd |S 0 ), and (ii) occurrence of
these entities in IDRs of entities not in P (xd |S 0 );
if (xd ∈ S 0 ) then
Update S 0 ← S 0 \ {xd };

17

Update H ← H ∪ xd ;

13

B. Heuristic Solution

14

In this subsection we provide a greedy heuristic solution
to the Entity Hardening problem. For a given interdependent
network I(A, B, F(A, B)) with the initial failed set of entities
as A0 ∪ B 0 , apart from the Protection Set of each entity as
defined earlier, we now define the Minterm Coverage Number
of each entity in A ∪ B as follows:
Definition: For an entity xi ∈ A ∪ B the Minterm Coverage
Number is defined as the number of minterms that can be
removed from F(A, B) without affecting the cascading process
by hardening the entity xi when all entities in A0 ∪ B 0 fails
initially. This is represented as M (xi |A0 ∪ B 0 ).
It may be noted here that the Minterm Coverage Number of
each entity can be computed in O((n+m)2 ). We now present a
heurisitc for the ENH problem in Algorithm 2. The algorithm
takes as input an interdependent network I(A, B, F(A, B))
with S = A ∪ B. In steps 4-5 the set Q is constructed to
include the set of entities that do not fail when the K most
vulnerable entities fail. These Q entities are removed from the
search space as they have no impact on the hardening process.
In each iteration of the while loop in steps 6-17, an entity xd
is greedily selected such that when xd is hardened, it prevents
failure of the maximum number of entities. This ensures that
at each step the number of entities that may fail is minimized.
In case of a tie, among all entities involved in the tie, the entity
having the highest Minterm Coverage Number is included in
the solution. This tie breaking technique gives more priority
to the entity which when hardened has more impact on failure
minimization in subsequent iterations of the while loop. The
interdependent network I(A, B, F(A, B)) is updated in steps
13-16 of the algorithm. This update is done to take into account

652

Data: An interdependent network I(A, B, F(A, B)) (with
S = A ∪ B), set of entities A0 ∪ B 0 failed initially
causing maximum failure in the interdependent network
with |A0 | + |B 0 | = K and hardening budget k.
Result: Set of hardened entities H.
begin
Initialize S 0 ← A0 ∪ B 0 ;
Initialize H ← ∅;
Update F(A, B) as follows — (a) let Q be the set of
entities that does not fail on failing K entities, (b) remove
IDRs corresponding to entities in set Q, (c) remove from
minterm of entities not in set Q all entities which are in
set Q ;
Update S ← S \ Q ;
while (k entities are not hardened) do
For each entity xi ∈ S compute the Protection Set
P (xi |S 0 );
Choose the entity xd with highest cardinality of the
set |P (xd |S 0 )|;
if (more than one entity has the same highest
cardinality value) then
For each such entity xj compute the Minterm
Coverage Number M (xj |S 0 ) ;
Choose the entity xd with highest Minterm
Coverage Number. ;
In case of a tie choose arbitrarily;

15

18

return H ;

Run Time Analysis of Algorithm 2: For the purpose of this
analysis we consider n to be the total number of entities, and
m to be the total number of minterms. Updates in step 4 can be
done in O(m) and step 5 in O(n). The while loop iterates for
k times. In each iteration of the while loop step 7 and step 8
takes at most O((n + m)2 ) and O(nlog(n)) time respectively.
On branching in step 9, step 10 and step 11 takes O((n+m)2 )
and O(nlog(n)) time respectively. Updates in step 13 takes
O(n) time and in step 14 takes O(n + m) time. Step 12,
16 and 17 runs in constant time. Hence Algorithm 2 runs in
O(k(n + m)2 ) time.
VI.

E XPERIMENTAL R ESULTS

In this section we present the experimental results of the
Entity Hardening problem by comparing the optimal solution
computed using an ILP, and the proposed heuristic algorithm.
The experiments were conducted on real world power grid data
obtained from Platts (www.platts.com), and communication
network data obtained from GeoTel (www.geo-tel.com) for
Maricopa County, Arizona. The data consisted of 70 power
plants and 470 transmission lines in the power network, and
2, 690 cell towers, 7, 100 fiber-lit buildings and 42, 723 fiber

IEEE Workshop on Inter-Dependent Networks 2015

13

ILP solution
Heuristic

12

10
8
6

7
6
4

4

3
2

2
0

1

1
3
5
7
Number of entities hardened (k)

14 13

13

ILP solution
Heuristic

12
10
8

7

Total number of failed entities

7

6
4

3

0

1

1

1
3
5
7
Number of entities hardened (k)

14
12

13

ILP solution
Heuristic

12

10
8

8
6

6

5

4

3

2
0

11

11

ILP solution
Heuristic

8
6

6

5
4

4

3
2

2

1

1
3
5
7
Number of entities hardened (k)

(d) Region 4

3
1

1
3
5
7
Number of entities hardened (k)

(b) Region 2

10

0

3

2

(a) Region 1
12

Total number of failed entities

12

Total number of failed entities

14

Total number of failed entities

Total number of failed entities

6

(c) Region 3

8
7

ILP solution
Heuristic

7

6
5

5

4
3

3

2
1

0

1

1
3
5
7
Number of entities hardened (k)

(e) Region 5

Fig. 1: Comparison of the optimal (ILP), and heuristic approaches on the total number of entities that fail when the K most vulnerable entities
fail and k entities are hardened. In our experiments K was set to 8. For the Regions 1-5, when no entities were hardened (k = 0), the failure
of the K = 8 most vulnerable entities resulted in the total failure of 28, 23, 28, 28 and 27 entities respectively.

links in the communication network. We identified five nonintersecting geographical regions from the data set and labeled
them from regions 1 through 5. For each of the regions, the
entities of the power and communication network that were
located within the geographic region formed the set A and B
respectively. Each region was represented by an interdependent
network I(A, B, F(A, B)). We use the IDR construction rules
as defined in [9] to generate F(A, B).
In all of our simulations IBM CPLEX Optimizer 12.5 to
solve ILPs and Python 3 for heuristic is used. To analyze the
Entity Hardening problem the value of K was set to 8. The
ILP in [9] was used to compute the K most vulnerable nodes
in the network, and the set of failed entities due to the failure
of the K entities was also computed. For the five regions,
when no entities were hardened, the failure of the K = 8 most
vulnerable entities resulted in the failure of 28, 23, 28, 28 and
27 entities respectively. With the K most vulnerable nodes and
final set of failed nodes as input, the ILP and heuristic of the
Entity Hardening problem are compared with k = 1, 3, 5, 7.
The results of these simulations are shown in Figure 1. It is
observed that the heuristic solution differs more from optimal
at higher values of k (factor of 0.5 and 0.67 for Regions 1
and 3 respectively with k = 7). This is primarily because of
the greedy nature of Algorithm 2. However on an average the
heuristic solution differs by a factor of 0.13 from the optimal.
R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
“Catastrophic cascade of failures in interdependent networks,” Nature,
vol. 464, no. 7291, pp. 1025–1028, 2010.
[2] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, “Networks formed
from interdependent networks,” Nature Physics, vol. 8, no. 1, pp. 40–48,
2011.

653

[3] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, “Cascade of failures
in coupled network systems with multiple support-dependence relations,”
Physical Review E, vol. 83, no. 3, p. 036116, 2011.
[4] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, “Modelling interdependent infrastructures using interacting
dynamical models,” International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63–79, 2008.
[5] P. Zhang, S. Peeta, and T. Friesz, “Dynamic game theoretic model of
multi-layer infrastructure networks,” Networks and Spatial Economics,
vol. 5, no. 2, pp. 147–178, 2005.
[6] M. Parandehgheibi and E. Modiano, “Robustness of interdependent
networks: The case of communication networks and the power grid,”
arXiv preprint arXiv:1304.0356, 2013.
[7] D. T. Nguyen, Y. Shen, and M. T. Thai, “Detecting critical nodes in
interdependent power networks for vulnerability assessment,” 2013.
[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
“Power grid vulnerability to geographically correlated failures-analysis
and control implications,” arXiv preprint arXiv:1206.1099, 2012.
[9] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, “Identification of k most vulnerable nodes in multi-layered network using a new
model of interdependency,” in Computer Communications Workshops
(INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014, pp.
831–836.
[10] J.-F. Castet and J. H. Saleh, “Interdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,” PloS one, vol. 8, no. 4, p. e60402, 2013.
[11] A. Das, J. Banerjee, and A. Sen, “Root cause analysis of failures in
interdependent power-communication networks,” in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910–915.
[12] A. Mazumder, C. Zhou, A. Das, and A. Sen, “Progressive recovery from
failure in multi-layered interdependent network using a new model of
interdependency,” in Conference on Critical Information Infrastructures
Security (CRITIS), 2014. Springer, 2014.
[13] J. Banerjee, A. Das, C. Zhou, A. Mazumder, and A. Sen, “On the entity
hardening problem in multi-layered interdependent networks,” arXiv
preprint arXiv:1412.6686, 2014.
[14] M. Hajiaghayi, K. Jain, K. Konwar, L. Lau, I. Mandoiu, A. Russell,
A. Shvartsman, and V. Vazirani, “The minimum k-colored subgraph
problem in haplotyping and dna primer selection,” in Proceedings of the
International Workshop on Bioinformatics Research and Applications
(IWBRA). Citeseer, 2006.

Topology Design of Service Overlay Network with
A Generalized Cost Model
Ling Zhou and Arunabha Sen
Department of Computer Science and Engineering
School of Computing and Informatics
Arizona State University
Tempe, AZ 85287, USA
Email: {ling.zhou, asen}@asu.edu
Abstract—Service Overlay Network (SON) was proposed to
alleviate the difficulties encountered in providing end-to-end
Quality of Service (QoS) guarantees. SON is able to provide
QoS guarantees by purchasing bandwidth from individual network domains and building a logical end-to-end data delivery
infrastructure on top of the existing Internet. We focus on SON
topology design problems under a generalized cost model. Earlier
research in this topic considered two distinct cost models fixed (leased) cost model and variable (usage-based) cost model.
However in most applications, the costs of both nodes and links
have a fixed component as well as a variable component that
often depends on usage. Our generalized cost model takes this
fact into account and our topology design algorithm uses this
cost model to find the optimal topology. Since the SON topology design problem is NP-complete, we provide approximation
algorithm with guaranteed performance bound. We validate the
effectiveness of our algorithm through extensive simulation.

I. I NTRODUCTION
Service Overlay Network (SON) was proposed to facilitate
services such as VoIP and Video-on-Demand that require endto-end Quality-of-Service(QoS) guarantees [2]. SON is able
to provide QoS guarantees by purchasing bandwidth from
individual network domains and building a logical end-to-end
data delivery infrastructure on top of the existing Internet. In
SON, there exist well-defined business relationships among
the Overlay Service Providers (OSP), the underlying network
domains or the Internet Service Providers (ISP), and the customers (end users) of SON. OSP deploys across multiple ISP
domains a number of dedicated servers (overlay nodes), which
perform service-specific functions, such as routing and data
processing at the application layer. By entering into a Service
Level Agreement (SLA) with ISPs regarding guaranteed QoS,
OSP establishes a virtual network comprising of overlay nodes
and links between them at the application layer. The users
access the value-added services provided by SON through one
or more overlay nodes, and pay OSP through some pricing
mechanism. Fig.1 shows the structure of a SON.
In this paper we focus on the SON topology design problem
from the OSP’s perspective. The goal is to find the least
expensive design that can satisfy all the traffic demands from
users. The cost model used by the OSP plays a crucial role
in the design of the optimal topology. Earlier research in this
topic primarily considered two distinct cost models - fixed
(leased) cost model and variable (usage-based) cost model.

Customer 2

Customer 1

ON 2
ON 1
ISP 1

ISP 2

ON 3

Customer 3

ON 4
ISP 5

ISP 4

ISP 3

ON 5

Overlay Node
(ON)
Customer

Customer 5
Customer 4

Fig. 1.

SON (Physical Network)

TABLE I
C OST M ODELS FOR SON T OPOLOGY D ESIGN
Reference
Vieira et. al.[9]
Sen et. al.[7]
Fan et. al.[3]
Tran et. al.[8]
This paper

Node Cost
not considered
not considered
node degree based
not considered
mixed-cost

Edge Cost
usage-based
fixed & usage-based
usage-based
mixed-cost
mixed-cost

However in many applications, the costs of overlay nodes and
links have a fixed component as well as a variable component.
The variable component usually depends on the usage, but
as shown in [3] it can depend on other design parameters as
well. A summary of different cost models considered by earlier
researchers is given in Table I.
In this paper we present a generalized cost model that
subsumes all the cost models considered earlier. In this model,
cost can be associated with both overlay nodes and links.
Moreover the costs have two components - fixed component
and variable component. Accordingly, we refer to this cost
model as mixed-cost model. The rationale for such a model is
as follows:
• Fixed cost of overlay node corresponds to the cost of
buying and deploying an overlay node (dedicated server
or server cluster) among multiple network domains [2]. It
can also correspond to the latency caused by an overlay
node in data processing [5].
• Variable cost of overlay node corresponds to the resource

75
1930-529X/07/$25.00 © 2007 IEEE
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE GLOBECOM 2007 proceedings.

requirement that increases when large volume of traffic
transits through the overlay node [5]. The cost may also
be related to the degree of an overlay node [3], since
OSP will incur a large control overhead/cost (e.g.,link
condition probing overhead) for an overlay node with
larger degree.
• Fixed cost of overlay link corresponds to the maintenance
fee charged by the ISPs for each overlay link [3].
• Variable cost of overlay link corresponds to the fee
paid by the OSP to the underlying ISPs for consuming
bandwidth to carry traffic on the overlay link [3].
It may be noted that none of the earlier papers on this topic
dealt with the SON topology design problem under such a
generalized cost model. The results presented in this paper
are equally applicable to any design scenario, where one or
more of the fixed and/or variable costs are negligible due to
special circumstances. The key contributions of this paper are
as follows:
• We propose a generalized cost model that captures any
combination of fixed and variable costs associated with
nodes and links in SON topology design.
• We formulate two SON topology design problems where
the variable costs associated with the overlay nodes are
dependent on (i) traffic carried by the nodes, and (ii)
degree of the nodes in the resulting topology. We show
that the second problem can be reduced to the first
problem in polynomial time.
• We show that both the problems are NP-complete, and
hard to approximate to within a factor less than logarithm
unless P = N P .
• We provide an integer linear programming formulation
for the SON topology design problem.
• We present an approximation algorithm for the SON
topology design problem with guaranteed performance
bound.
• We establish the efficacy of our solution through extensive simulation.

We assume that the communication requirements of all users
of SON have been aggregated and are represented as the traffic
demands between overlay nodes. As pointed by [7], the access
model of SON users can be either single-homed (connected
to one overlay node) or multi-homed (connected to multiple
overlay nodes). Since we take an aggregated traffic view at
overlay nodes, we do not address the issues related to access
model (whether single-homed or multi-homed) in this paper.
In the cost model of the TDNEW problem, there are two
separate node-cost functions c1 : V → + and l1 : V → + .
For each node v ∈ V , c1 (v) is a traffic-independent (fixed) cost
of v, while l1 (v) is a traffic-dependent cost. l1 (v) is involved
whenever there is one unit of data transited through node v.
Thus, if k1 > 0 units of data transited through node v, the total
cost involved will be k1 ·l1 (v)+c1 (v). Similarly, there are two
separate edge-cost functions c2 : E → + and l2 : E → + .
For each edge e ∈ E, c2 (e) is a traffic-independent (fixed) cost
of e, while l2 (e) is a traffic-dependent cost. l2 (e) is involved
whenever there is one unit of data transited through edge e.
Thus, if k2 > 0 units of data transited through edge e, the
total cost involved will be k2 · l2 (e) + c2 (e).
Now consider the SON topology design problem to support
all the traffic demands from the overlay node pairs defined
by the traffic demand set D. Given the graph G(V, E), the
objective is to select a subgraph G (V  , E  ), such that all the
traffic demands can be supported with minimum overall cost.
Suppose the traffic from si to ti is routed through path pi in
G (V  , E  ), we can now formalize the SON topology design
problem as finding a subgraph G (V  , E  ) of G(V, E), such
that the following cost metric is minimized:


k





c1 (v) +
c2 (e) +
ri ·
l1 (v) +
l2 (e)
v∈V 

e∈E 

i=1

v∈pi

e∈pi

(1)
The first two terms in the total cost are the fixed costs
associated with those selected nodes and edges respectively.
The last term is the traffic-dependent costs added up for each
traffic demand pair.

II. P ROBLEM F ORMULATION
We define the SON topology design problem with the
generalized cost model in this section. In the generalized cost
model, both overlay nodes and links have two different cost
functions associated with them. We refer to the problem as
Topology Design with Node-weighted and Edge-Weighted cost
model (TDNEW).
A. TDNEW Problem
The input to the SON topology design problem is a weighted
undirected graph G(V, E), where v ∈ V represents overlay
node and e ∈ E represents the potential overlay link that
connects overlay nodes. The weights on the nodes and edges
represent the costs associated with them. In addition, a traffic
demand set D = {(s1 , t1 , r1 ), (s2 , t2 , r2 ), ..., (sk , tk , rk )} is
given, where each pair (si , ti ) represents two overlay nodes
and ri represents the amount of traffic from si to ti .

B. D-TDNEW Problem
In this section, we introduce a variant of the TDNEW problem, which is referred to as the Degree-dependent TDNEW (DTDNEW) problem. The only difference between TDNEW and
D-TDNEW lies in the way the node-cost functions are defined.
In the cost model of the D-TDNEW problem, there are two
separate node-cost functions c1 : V → + and g1 : V → + .
As in the TDNEW problem, c1 (v) represents the fixed cost of
node v. The node cost g1 (v), however, is degree-dependent. If
G (V  , E  ) is the resulting topology, v ∈ V  and node degree
of v is d, then the total cost associated with node v will be
d · g1 (v) + c1 (v). It may be noted that the notion of degreedependent node cost was first considered in [3].
In the D-TDNEW problem, given an undirected graph
G(V, E), a traffic demand set D, node-cost functions c1 and
g1 , and edge-cost functions c2 and l2 , the objective is to find

76
1930-529X/07/$25.00 © 2007 IEEE
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE GLOBECOM 2007 proceedings.

a subgraph G (V  , E  ) such that the following cost metric is
minimized:


k




(dv · g1 (v) + c1 (v)) +
c2 (e) +
ri ·
l2 (e)
v∈V 

e∈E 

e∈pi

i=1

(2)

where dv denotes the degree of node v in G (V  , E  ).
C. Reduction of D-TDNEW Problem to TDNEW Problem

Theorem 1 The D-TDNEW problem can be reduced to the
TDNEW problem in polynomial time.
Proof: Given any instance of the D-TDNEW problem, where
each edge e = (u, v) is associated with variable (trafficdependent) cost l2 (e) and fixed cost c2 (e), and each node
v is associated with variable (degree-dependent) cost g1 (v)
and fixed cost c1 (v), we transform the instance into an
instance of the TDNEW problem in the following way. For the
instance of the TDNEW problem, we need parameters c1 (v)
and l1 (v) for each node v and parameters c2 (e) and l2 (e) for
each edge e. These parameters can be computed as follows:
(i) c1 (v) (T DN EW ) = c1 (v) (D − T DN EW )
(ii) l1 (v) (T DN EW ) = 0
(iii) c2 (e) (T DN EW ) = c2 (e) + g1 (u) + g1 (v) (D −
T DN EW ), where e = (u, v)
(iv) l2 (e) (T DN EW ) = l2 (e) (D − T DN EW )
Any feasible solution (a subgraph G (V  , E  )) for the instance
of the D-TDNEW problem is also a feasible solution for the
constructed instance of the TDNEW problem, and vice versa.
We now show that the same feasible solution - a subgraph
G (V  , E  ) has the same cost in both instances. Therefore, the
optimal solutions in both instances also have the same cost
and one-to-one correspondence by the transformation.
The feasible solution G (V  , E  ) for the constructed instance of the TDNEW problem has the following overall cost:

v∈V 

c1 (v)+

u,v∈e


(c2 (e) + g1 (u) + g1 (v))+

k


ri ·

i=1

e∈E 



l2 (e)

e∈pi

(3)
The solution G (V  , E  ) for the D-TDNEW problem has the
same overall cost as the one given in the expression (2).
Expressions (2) and (3) have the same value since:

v∈V 

dv · g1 (v) =

u,v∈e


(g1 (u) + g1 (v))

(4)

e∈E 

The correctness of equation (4) follows from the fact that
whenever the edge e is selected in E  , the degrees of its
incident nodes u and v are increased by 1 respectively.
III. C OMPUTATIONAL C OMPLEXITY OF SON T OPOLOGY
D ESIGN
In this section, we show that both the TDNEW problem and
the D-TDNEW problem are NP-complete.
Consider a special case of the TDNEW problem where ∀v ∈
V , c1 (v) = 0, l1 (v) = 0 and ∀e ∈ E, l2 (e) = 0. Similarly,
consider a special case of the D-TDNEW problem where ∀v ∈

V , c1 (v) = 0, g1 (v) = 0 and ∀e ∈ E, l2 (e) = 0. In both cases,
the only non-zero costs are the fixed costs associated with
edges of the graph. With such cost constraints, the TDNEW
and D-TDNEW problems reduce to the problem of finding
the minimum edge-weighted Steiner Forest in G(V, E), whose
goal is to connect each traffic demand pair si and ti . Since the
Steiner Forest problem is a well-known NP-complete problem
[4], we can conclude that both the TDNEW and the D-TDNEW
problems are NP-complete. The cost model of these special
cases is referred to as the fixed cost model in [9] and [7].
Now consider a different special case of the TDNEW
problem where ∀v ∈ V , c1 (v) = 0, l1 (v) = 0 and
∀e ∈ E, c2 (e) = 0. Similarly, consider a special case of the
D-TDNEW problem where ∀v ∈ V , c1 (v) = 0, g1 (v) = 0
and ∀e ∈ E, c2 (e) = 0. In both cases, the only non-zero
costs are the variable (traffic-dependent) costs associated with
edges of the graph. With such cost constraints, the TDNEW
and D-TDNEW problems reduce to the problem of finding the
least-cost (shortest) paths to connect each traffic demand pair
si and ti , with non-zero edge cost function l2 associated with
each edge. These special cases can be solved in polynomial
time. The cost model of these special cases is referred to as
the usage-based cost model in [3], [7] and [9].
Since the general versions of both the TDNEW and DTDNEW problems are NP-complete, we now explore the
possibility of designing efficient approximation algorithms for
the problems with guaranteed performance bound. For that
purpose, we consider a special case of both the problems. In
the special case that we consider for the TDNEW problem, we
have ∀v ∈ V , l1 (v) = 0 and ∀e ∈ E, c2 (e) = 0, l2 (e) = 0.
Similarly, for the special case of the D-TDNEW problem, we
have ∀v ∈ V , g1 (v) = 0 and ∀e ∈ E, c2 (e) = 0, l2 (e) = 0.
In both cases, the only non-zero costs are the fixed costs
associated with nodes of the graph. These special cases of
the TDNEW and the D-TDNEW problems reduce to the
problem of finding the minimum node-weighted Steiner Forest
to connect each traffic demand pair (si , ti ), 1 ≤ i ≤ k .
Since the node-weighted Steiner Forest problem is hard to
approximate to within a factor of o(log k) unless P = N P
[1], therefore both the TDNEW problem and the D-TDNEW
problem are hard to approximate to within a factor of o(log k)
unless P = N P .
IV. O PTIMAL S OLUTION FOR SON T OPOLOGY D ESIGN
In this section we provide an Integer Linear Programming
(ILP) formulation of the SON topology design problem. Since
the D-TDNEW problem can be transformed into the TDNEW
problem, we only provide the ILP formulation of the TDNEW
problem. The ILP finds the optimal solution of the TDNEW
problem.
The formulation uses four types of indicator variables.
•
•

x1 (v) = 1 if node v is selected in G (V  , E  ), which
means v ∈ V  , otherwise x1 (v) = 0.
x2 (e) = 1 if edge e is selected in G (V  , E  ), which
means e ∈ E  , otherwise x2 (e) = 0.

77
1930-529X/07/$25.00 © 2007 IEEE
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE GLOBECOM 2007 proceedings.

•
•

y1i (v) = 1 if node v is on the path from si to ti in G ,
otherwise y1i (v) = 0.
y2i (e) = 1 if edge e is on the path from si to ti in G ,
otherwise y2i (e) = 0.

Minimize



x1 (v) · c1 (v) +

v∈V

+

k



ri ·

i=1



x2 (e) · c2 (e)

e∈E



v∈V

y1i (v)

· l1 (v) +




y2i (e)

· l2 (e)

e∈E

(5)
Subject to
x2 (e) ≤ x1 (v)

si ∈e



ti ∈e


v∈e

y2i (e)
y2i (e)

∀e ∈ E, ∀v ∈ e

(6)

y1i (v)

∀1 ≤ i ≤ k, ∀e ∈ E, ∀v ∈ e

(7)

≤ x2 (e)

∀1 ≤ i ≤ k, ∀e ∈ E

(8)

y2i (e) = 1

∀1 ≤ i ≤ k

(9)

y2i (e) = 1

∀1 ≤ i ≤ k (10)

≤

y2i (e) = 2 · y1i (v)
x1 (v) = 0/1
x2 (e) = 0/1
y1i (v)
y2i (e)

∀1 ≤ i ≤ k, ∀v, v = si , v = ti (11)
∀v ∈ V (12)
∀e ∈ E (13)

= 0/1

∀1 ≤ i ≤ k, ∀v ∈ V (14)

= 0/1

∀1 ≤ i ≤ k, ∀e ∈ E (15)

The formulation uses four types of constraints. The constraints of type A, comprising of constraints 9,10 and 11, make
sure that there is a path connecting each traffic demand pair
si and ti . The constraints of type B, comprising of constraints
6, make sure that whenever edge e is selected, its endpoints
are also selected. The constraints of type C, comprising of
constraints 7, make sure that whenever edge e appears on the
path from si to ti for any i, 1 ≤ i ≤ k, its endpoints are also
on the same path. The constraints of type D, comprising of
constraints 8, make sure that whenever edge e appears on the
path from si to ti for any i, 1 ≤ i ≤ k, edge e is selected
in G (V  , E  ). The objective (5) is to minimize the overall
cost, which has the same structure as the cost metric given in
expression (1).

Topology Design Algorithm:
1. Initialize G (V  , E  ) as empty
2. While D = ∅
3.
Select an arbitrary pair (si , ti ) from D
4.
Make the following changes to node/edge costs in G(V, E)
5.
∀v ∈ V  , define its integrated cost as ri · l1 (v)
6.
∀v ∈ V \ V  , define its integrated cost as ri · l1 (v) + c1 (v)
7.
∀e ∈ E  , define its integrated cost as ri · l2 (e)
8.
∀e ∈ E \ E  , define its integrated cost as ri · l2 (e) + c2 (e)
9.
Find a least-cost path (including both node and edge costs)
from si to ti in G(V, E) using Dijkstra’s algorithm

10. Add the selected path to G (V  , E  )
11. D = D − {(si , ti )}
12. Output G (V  , E  )

Fig.2
illustrates
the
execution
of
Topology Design Algorithm on a sample graph G(V, E) with
traffic demands D = {(0, 4, 2), (0, 3, 1)}. Fig.2(a) shows
G(V, E) with each node v ∈ V labeled with cost (c1 (v), l1 (v))
and each edge e ∈ E labeled with cost [c2 (e), l2 (e)]. Fig.2(b)
shows the iteration for the first traffic demand pair, where
node 0 needs to send 2 units of data to node 4. All the nodes
and edges in Fig.2(b) are labeled with the costs assigned
by Steps 5-8 in the algorithm. Then a least-cost path from
node 0 to node 4 with total cost 38 is selected and added
into G (V  , E  ). Fig.2(c) shows the iteration for the second
traffic demand pair. Please note that since nodes 0, 1, 4
and edges (0, 1), (1, 4) have been selected in the previous
iteration, their costs in Fig.2(c) do not include the fixed
cost. A least-cost path from node 0 to node 3 with total cost
19 is selected in this iteration. Fig.2(d) shows the resulting
topology G (V  , E  ) with total cost 57 to support all the
traffic.
(2,3)
0

3
(3,4)

[2,3]

1

[2,3]

2

6
(1,4)

7

4
(2,4)

3
11

(a)

7
7

3
7

4

10

(b)

5

2 5
4

4
4

(c)

Fig. 2.

[1,2]

[2,2]

3

2

7

(2,3)
0

2

5

2 9

4

3
0

2 1

5
8

1

8

[1,3]

[3,2]
[1,3]

V. A PPROXIMATE S OLUTION WITH G UARANTEED
P ERFORMANCE B OUND
In this section we present an approximation algorithm for
the TDNEW problem. We prove that the solution found by this
approximation algorithm has a cost, which is at most k times
the cost of the optimal solution, where k is the number of
traffic-demand pair (si , ti ) specified in the TDNEW problem.
In each iteration of the algorithm, it finds a least-cost path to
connect the node pair (si , ti ). After such a path P is computed,
the fixed costs associated with the nodes and edges that are
part of P are set equal to zero, so that they are not counted
by multiple times.

[1,2]

[2,2]
(3,2)

8
0

(3,2)
[2,3]
3
(3,4)

1

[2,3]

(1,4)

[1,3]

[3,2]
[1,3]
(d)

2

4
(2,4)

Illustration of Algorithm 1

78
1930-529X/07/$25.00 © 2007 IEEE
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE GLOBECOM 2007 proceedings.

Theorem 2 The Topology Design Algorithm finds a topology
whose cost is at most k times the cost of the optimal topology,
where k is the number of traffic-demand pairs in the demand
set D.

In earlier research, different heuristics have been proposed
for SON topology design under different cost models [3], [7],
[8], [9]. However guaranteed performance bounds have not
been reported for them.

Proof: Define PGi as the cost of the least-cost path from si
to ti in the graph G(V, E), where the integrated cost for each
node v ∈ V is ri ·l1 (v)+c1 (v), and the integrated cost for each
edge e ∈ E is ri · l2 (e) + c2 (e). Let G be the topology found
by the Topology Design Algorithm, and G∗ be the optimal
topology. Let C(G ) and C(G∗ ) denote the total cost of G
and G∗ , respectively.
We have the following inequalities:
k
PGi
(16)
C(G ) ≤

VI. P ERFORMANCE E VALUATION

i=1

PGi

≤

PGi ∗

(17)

PGi ∗ ≤ C(G∗ )

(18)

Equation (16) is due to the fact that in Topology Design Algorithm, when we find the least-cost path
for each pair (si , ti ), 1 ≤ i ≤ k in G(V, E), the fixed
costs of those selected nodes or edges have been omitted.
Equation (17) is due to the fact that G∗ ⊆ G. Equation (18)
is due to the fact that k ≥ 1 and C(G∗ ) may contain the cost
to route traffic for other pairs. Therefore, we have:
k
k
k
PGi ≤
PGi ∗ ≤
C(G∗ ) = k·C(G∗ )
C(G ) ≤
i=1

or

i=1

i=1

In this section, we evaluate the performance of Topology Design Algorithm by comparing its solution with the
optimal solution obtained by solving the ILP given in section
IV. In section V, we have proved that the approximate solution cannot be worse than k (the number of traffic demand
pairs) times the optimal solution. However, in practice its
performance can be significantly better than its worst case
bound. The goal of our evaluation is to find how close the
approximate solution gets to the optimal one. The comparison
results demonstrate that our approximate algorithm always
produces near-optimal solution in a fraction of time needed
to find the optimal one.
All our experiments were carried out on the 30-node and
77-edge graph as shown in Fig.3. This graph was generated
using Waxman random network topology generator [10], [6].
10

9

8

7

6

5

4

C(G )
≤k
C(G∗

3

2

1

Recently, [1] proposed a O(log4 k)-approximation algorithm, which can be used to solve special instances of the
TDNEW problem, where fixed and variable costs are associated with nodes only, in other words c2 (e) = l2 (e) = 0 for
all edges. An instance of the TDNEW problem with nonezero edge costs can be easily transformed to an instance with
zero edge costs, by introducing a new node corresponding to
each edge and moving the edge costs to the corresponding
nodes. Once such a transformation is carried on an instance,
the algorithm presented in [1] can be utilized to find a
solution. The performance bound (the ratio of the approximate
solution to the optimal solution) of the algorithm presented
in [1] is O(log4 k), whereas the algorithm presented in this
paper is O(k). Theoretically, the performance of the algorithm
presented in [1] is significantly better than the one presented
in this paper. However since k ≤ log4 k when k ≤ 65536, the
performance of the algorithm presented in this paper is better
than the one given in [1] as long as k ≤ 65536, which means
the number of traffic demand pairs to be connected is less
than 65536 (true for most application scenarios). It may also
be noted that the algorithm presented in this paper is simple
and may be extended into the dynamic scenario with timevarying traffic, whereas the one presented in [1] is significantly
complex.

0

0

Fig. 3.

1

2

3

4

5

6

7

8

9

10

A Random Graph with 30 nodes and 77 edges

In addition to the graph G(V, E), the SON topology design
is governed by six design parameters: (i) fixed cost c1 (v) of
each node v ∈ V , (ii) traffic-dependent cost l1 (v) of each node
v ∈ V , (iii) fixed cost c2 (e) of each edge e ∈ E, (iv) trafficdependent cost l2 (e) of each edge e ∈ E, (v) k, the number
of traffic demand pair (si , ti ) in the demand set D, and (vi)
traffic demand ri from si to ti , 1 ≤ i ≤ k.
The experiments were conducted to evaluate the performance of our algorithm by varying (i) traffic-dependent cost
l1 (v), (ii) traffic-dependent cost l2 (e), (iii) k, the number of
traffic demand pairs and (iv) traffic demand ri . In all the
experiments, the node fixed cost c1 (v) and the edge fixed
cost c2 (e) was drawn randomly from the fixed range [80, 100],
and k traffic demand pairs are randomly selected from all the
overlay nodes.
For the first set of experiments, l1 (v) is continuously
increased, while l2 (e) is randomly selected from the range
[15, 20], k is set equal to 25 and ri is randomly selected from
the range [2, 4]. The results are shown in Fig.4.
For the second set of experiments, l2 (e) is continuously
increased, while l1 (v) is randomly selected from the range

79
1930-529X/07/$25.00 © 2007 IEEE
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE GLOBECOM 2007 proceedings.

Overall Cost of SON

20000

Overall Cost of SON

Approximate Solution
16000
12000
8000
4000
0
5

10

15

20

25

30

35

40

45

50

Number of Traffic Demand Pairs

Fig. 6.

Overall Cost vs. Number of Traffic Demand Pairs

32000

Optimal Solution
Approximate Solution

28000
24000
20000
16000
12000
8000
4000

Approximate Solution

12000

4

5

6

7

8

9

10

11

2

3

4

5

6

7

8

9

Traffic Demand Range

8000
4000

Fig. 7.

0
5

10

15

20

25

30

35

40

45

50

1

5

10

15

20

25

30

35

40

45

Node Traffic-dependent Cost Range

Fig. 4.

R EFERENCES

Optimal Solution
16000
Approximate Solution
12000
8000
4000
0
5

10

15

20

25

30

35

40

45

50

1

5

10

15

20

25

30

35

40

45

Edge Traffic-dependent Cost Range

Fig. 5.

Overall Cost vs. Traffic Demand

The SON topology design problem under dynamic scenario,
where traffic demands between nodes change over time, are
currently under our study.

Overall Cost vs. Node Traffic-dependent Cost

20000

Overall Cost of SON

Optimal Solution

20000

0

Optimal Solution
16000

24000

Overall Cost of SON

[15, 20], k is set equal to 25 and ri is randomly selected from
the range [2, 4]. The results are shown in Fig.5.
For the third set of experiments, k is continuously increased,
while both l1 (v) and l2 (e) are randomly selected from the
range [15, 20], and ri is randomly selected from the range
[2, 4]. The results are shown in Fig.6.
For the fourth set of experiments, ri is continuously increased, while both l1 (v) and l2 (e) are randomly selected from
the range [15, 20], and k is set equal to 25. The results are
shown in Fig.7.
It can be seen from the Figures 4-7, that in all the
experiments, our topology design algorithm produced nearoptimal solutions. Since these experiments were conducted
using widely varying design parameters, we expect that our
algorithm can also produce near-optimal solutions in other
parameter settings. The computational time for our algorithm
however was significantly less than the time taken to find the
optimal solution using ILP (seconds vs. hours on average).

Overall Cost vs. Edge Traffic-dependent Cost

VII. C ONCLUSIONS
In this paper we have proposed a generalized cost model for
the SON topology design problem. Based on this cost model,
we have formulated two SON topology design problems.
We establish the hardness of the problems and provide both
optimal and approximate solutions. Extensive experimental
results demonstrate the effectiveness of our solution.

[1] C. Chekuri, M. T. Hajiaghayi, G. Kortsarz, and M. Salavatipour. Approximation algorithms for node-weighted buy-at-bulk network design.
In ACM SODA’07, pages 1265–1274, New Orleans, USA, 2007.
[2] Z. Duan, Z.-L. Zhang, and Y. T. Hou. Service overlay networks: Slas,
qos, and bandwidth provisioning. IEEE/ACM Trans. Netw., 11(6):870–
883, 2003.
[3] J. Fan and M. H. Ammar. Dynamic topology configuration in service
overlay networks: A study of reconfiguration policies. In IEEE Infocom’06, 2006.
[4] M. R. Garey and D. S. Johnson. Computers and Intractability; A Guide
to the Theory of NP-Completeness. W. H. Freeman & Co., New York,
NY, USA, 1990.
[5] O. Papaemmanouil, Y. Ahmad, U. Cetintemel, and J. Jannotti.
Application-aware overlay networks for data dissemination. In ICDEW
’06: Proceedings of the 22nd International Conference on Data Engineering Workshops, Washington, DC, USA, 2006.
[6] W.
random
network
topology
generator.
http://www.math.uu.se/research/telecom/software/stgraphs.html.
[7] A. Sen, L. Zhou, B. Hao, B. H. Shen, and S. Ganguly. On topological
design of service overlay networks. In IWQoS’05: Proceedings of the
13th International Workshop on Quality of Service, pages 54–68, 2005.
[8] H. T. Tran and T. Ziegler. A design framework towards the profitable
operation of service overlay networks. Computer Networks, 51(1):94–
113, 2007.
[9] S. L. Vieira and J. Liebeherr. Topology design for service overlay
networks with bandwidth guarantees. In IWQoS’04: Proceedings of
the 12th International Workshop on Quality of Service, pages 211–220,
2004.
[10] B. M. Waxman. Routing of multipoint connections. IEEE Journal on
Selected Areas of Communication, 6(9):1617–1622, 1988.

80
1930-529X/07/$25.00 © 2007 IEEE
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE GLOBECOM 2007 proceedings.

SocialCom/PASSAT/BigData/EconCom/BioMedCom 2013

Spatio-Temporal Signal Recovery from Political
Tweets in Indonesia
Anisha Mazumder, Arun Das, Nyunsu Kim, Sedat Gokalp, Arunabha Sen, Hasan Davulcu
School of Computing, Informatics and Decision Systems Engineering
Arizona State University
Tempe, Arizona - 85287
Email: {Anisha.Mazumder, adas22, nkim30, Sedat.Gokalp, asen, hdavulcu}@asu.edu
time. For this analysis a region corresponds to a province of
Indonesia. Finally, from the Radicalization Index and Location
Index of individuals, Heat Index of a region , which is a
composite measure of the number of radical tweeters of that
region and their ‘degree of radicalism’, is computed.
In our model we have a set of tweeters (or users), U =
{U1 , U2 , . . . , Un }. Each user Ui , 1 ≤ i ≤ n creates a set of
tweets Ti = {Ti,1 , Ti,2 , . . . , Ti,t
in}. The set of all tweets by
all users is denoted by T = i=1 Ti . The geographic area
from where the tweets originate is divided into a set of regions
R = {R1 , R2 , . . . , Rm }. In our study m is equal to thirty four,
the number of provinces and special administrative regions of
Indonesia. Each user Ui , 1 ≤ i ≤ n has a home location
HLi , 1 ≤ i ≤ n associated with her, which may or may not
be declared. Each tweet Ti,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti has a
geo-location GLi,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti associated with
it. However, GLi,k for some tweets Ti,k may not be known
as the user Ui might turn her GPS off. Accordingly, we can
divide the set of users in four different classes:
(i) Class 1: user Ui whose home location is declared and
geo-location of at least one tweet is known,
(ii) Class 2: Ui whose home location is not declared and
geo-location of at least one tweet is known,
(iii) Class 3: Ui whose home location is declared and geolocation of none of the tweets are known, and
(iv) Class 4 : Ui whose home location is not declared and
geo-location of none of the tweets are known.
From the input data set (U, T, R ), we compute, (i) Location
Index, Li of each user Ui , 1 ≤ i ≤ n, (ii) Radicalization Index,
RDi of each user Ui , 1 ≤ i ≤ n, and ﬁnally, combining Li
and RDi , we compute (iii) Heat Index, Hj of each region
Rj , 1 ≤ j ≤ m. It may be noted that whereas RDi , 1 ≤ i ≤ n
is a scalar value, Li is a vector of size m, (Li,1 , . . . , Li,m ),
where Li,j indicates the probability of user Ui being located
in region Rj i.e. Li,j indicates the probability of the Actual
home location of Ui being Rj . Finally, the HeatIndex Hj of
n
region Rj , 1 ≤ j ≤ m is computed as Hj = i=1 RDi ×
Li,j , ∀j, 1 ≤ j ≤ m. We thus provide a generic technique for
generating time-varying political Heat Maps of a geographical
region based on the Twitter data analysis. Throughout this
paper we have used ‘region ’ and ‘location’ interchangeably
to mean an ‘Indonesian Province’. It is to be noted that for
our calculations, we have considered all Indonesian provinces

Abstract—Online social network community now provides an
enormous volume of data for analyzing human sentiment about
people, places, events and political activities. It is becoming
increasingly clear that analysis of such data can provide great
insights on the social, political and cultural aspects of the participants of these networks. As part of the Minerva project, currently
underway at Arizona State University, we have analyzed a large
volume of Twitter data to understand radical political activity in
the provinces of Indonesia. Based on analysis of radical/counter
radical sentiments expressed in tweets by Twitter users, we
create a Heat Map of Indonesia which visually demonstrates the
degree of radical activities in various provinces of Indonesia.
We create the Heat Map of Indonesia by computing (i) the
Radicalization Index and (ii) the Location Index of each Twitter
user from Indonesia, who has expressed some radical sentiment
in her tweets. The conclusions derived from our analysis matches
signiﬁcantly with the analysis of Wahid Institute, a leading
political think tank of Indonesia, thus validating our results.
Index Terms—radical, tweet, Radicalization Index, Location
Index, Heat Map

I. I NTRODUCTION
The sheer popularity of online social media nowadays is
reﬂected by the immense amount of data being fed every
second by people from all over the world. It is becoming
increasingly evident that analysis of this huge online dataset
can provide great insights on the social, political and cultural
aspects of the Twitter users and possibly the non-Twitter
users as well. In this study, we have developed a tool for
recovering spatio-temporal signals from tweets generated in
Indonesia. Our interest in analyzing tweets from Indonesia
developed in the context of the Minerva1 project, currently
underway at Arizona State University. The goal of this project
is to increase the understanding of movements within Muslim
communities towards radicalism or counter radicalism. Based
on the support and opposition of certain beliefs and practices
of an individual (as expressed in her tweet), we can assign a
Radicalization Index to that individual. In addition, from the
self declared home location of a Twitter user and the locations
of her tweets, we can compute a distribution of Location
Index for that user. The map of Indonesia is divided up into
a set of regions and the Location Index of a user provides the
probability of the user to be in a speciﬁc region at a speciﬁc
1 This research was supported in part by US DOD Minerva Research
Initiative grant N00014-09-1-0815.

978-0-7695-5137-1/13 $26.00 © 2013 IEEE
DOI 10.1109/SocialCom.2013.46

280

including special administrative regions such as Yogyakarta
and special capital region such as Jakarta and we have ignored
Class 4 users.

Fig. 1: The ﬂow diagram of our Heat Map computation technique. The Web data mentioned here refers to the documents
generated by crawling the web pages of radical and counter
radical organizations of Indonesia.

II. R ELATED W ORK
Identiﬁcation of the location of users using Twitter data has
been quite a focus of recent research ([11], [12]). [4], [5]
combine location information and text from social-network
data history to infer user preferences and provide recommendations. However, we do not rely on any ‘checking in’
information for our computations. ‘Geo-coding’ (the use of
gazetteers) is applicable to our problem since we employ the
notion of regions. Following [3], we too argue that location
estimates are multi-modal probability distributions, rather than
particular points or regions. However, it may be noted that in
contrast to [3], our estimate of the location of the user must
be the probability of each Indonesian province as the Actual
home location of the user under consideration, rather than the
probability of the user being located anywhere on the surface
of the earth. In this study, we have developed a simple yet
effective means of computing the geo-location of the user as
compared to other more complex methodologies such as Topic
Detection Techniques [16], [17].
Human mobility is modeled as a stochastic process in [6].
In [1], the authors study the manner in which the movements
of human beings are related to time of the day, geography
as well as social ties. Similar problems have been studied
by [7], [8]. However, in our problem, there is no notion of
prediction of location of users involved. Besides, we consider
categorical distribution but we apply the concept of mixture
of distributions in the lines of [1]. Another line of research
which focuses on location estimation by content-analysis of
the tweets of a user has been studied by [9], [10]. However, in
this current work we rely on the geo-location containing tweets
of users and their declared home location to obtain the location
distribution of the users. In [13], the authors analyze tweets
generated during the United Kingdom 2010 General Election
to infer the political afﬁliation of a user based on her tweets.
We also study a similar problem, however our goal is not to
identify the political afﬁliations of users, rather we compute
the ‘degree of radicalism’ of the user. Besides, unlike them,
we apply a very simple yet effective term-frequency analysis
of tweets and leverage heavily on our team of domain experts.
The work in [30] which is followed by [14] is very relevant
to our technique of Radicalization Index assignment to users.
These works speciﬁcally focus on presenting a framework for
combining entity matching techniques for detecting extremist
behavior on discussion boards. Identiﬁcation and analysis of
such weak signals of radicalism by the ‘lone wolf terrorists’
through the use of topic-ﬁltered web harvesting as well as
application of natural language processing techniques, thereby
fusing aliases for identifying the person form the basis of the
works of [30]. Their work is fundamentally different from ours
because we deal speciﬁcally with the users’ publicly available
tweets only - this eliminates the availability of the vital background information such as characteristic ( ‘radical internet

forum’, ‘capability internet forum’ ) annotation of particular
discussion boards that is leveraged in [30] .Furthermore, [30]
and [14] do not deal with location proﬁling of users which is
one of the two major goals of our work.
III. M OTIVATION AND D ISTINGUISHING F EATURES OF
OUR WORK

The goal of our research is to create a visual description
of the spatio-temporal distribution of the radical population
of Indonesia by recovering political signals from Twitter data.
A ﬂow diagram of our methodology is provided in Figure
1. In [2], the authors retrieve road-kill signals from Twitter
data using human beings as sensors. Like [2], we too use
human beings as sensors to the extent that we use tweets
of Indonesian people to infer radicalism Heat Indices of the
provinces of Indonesia. However, unlike [2], ﬁrst we intend to
ﬁnd the distribution of (radical) individuals, so we should not
factor in any ‘human population bias’ i.e variation of densities
of people across the different provinces of Indonesia. Second,
our problem is much more complex because we not only need
to know from which location have the radical tweets come
in greater number, but also the ‘degree of radicalism’ of the
tweets - so we need to comprehend the sentiment of the tweets.
So, questions of interest for us are(Qs1) the ‘degree of radicalism’ of tweet tw
(Qs2) the originating location of tweet tw
Thus, Heat Index of a region factors in both the count of
the radical tweets from the region as well as the ‘degree of
radicalism’ of the tweets. However, there are certain challenges
in answering these questions. As for Qs1, a tweet can at most
be 140 characters long. This is indeed too little information
to ascertain the ‘degree of radicalism’ of tweets on individual
basis. Thus, we go one level up the hierarchy and consider
individual users instead of individual tweets. We collect all
the tweets from individual users and assign the ‘degree of
radicalism’ to the user based on her tweets. Now, Qs2 would
have been easy to answer with respect to individual tweets if
all the tweets had geo-co-ordinate information because Twitter
API2 provides geo-location information of tweets if the user
had chosen to reveal her location at the time of tweeting.
2 https://dev.twitter.com/docs/streaming-apis
and
ter.com/docs/platform-objects/tweets have been used

281

https://dev.twit-

Home Location matrix gCHL, from the entire dataset barring
the timespan (month in our case) for which the Heat Map is
being generated. The created matrix gCHL is an m×m matrix
where the entry gCHLa,b , 1 ≤ a ≤ m, 1 ≤ b ≤ m, is the
conditional probability of the Actual home location of a user
being region Rb , when her Declared Home Location is region
Ra , as learnt from the dataset. The gCHL matrix is computed
using the following three steps provided in Algorithm 1. Thus,
gCHLa,b is given by:

Algorithm 1 Counting Algorithm for computation of the
general Computed Home Location gCHL
•
•

•

Step 1: Initialize gCHLa,b = 0, 1 ≤ a ≤ m, 1 ≤ b ≤ m
Step 2: For each tweet tw in T , increment gCHLa,b if
Declared home location of the author of tw and the geolocation of tw are Ra and Rb respectively
Step 3: Make each row gCHLa of gCHL matrix row
stochastic, 1 ≤ a ≤ m

gCHLa,b = X
Y
However, there are certain problems with this approach - ﬁrst,
the tweets containing geo-location information are very scarce
(such tweets constitute less than 1% of our dataset). Second,
when we consider individual users, it is unjustiﬁed to assume
that all her tweets containing geo-location information will
point to a single region, even if all her tweets contained geolocation information. Thus, the best estimate of the location
of the user is the probability distribution of the user’s location
over the Indonesian provinces.
We consider categorical distribution of the users into the
thirty four provinces of Indonesia. The motivation behind
employing categorical distribution, instead of say the more
popular Gaussian distribution over the entire landscape of
Indonesia, is that we wish to obtain a political Heat Map
of Indonesia with the granularity level of a province. Our
technique of Location Index computation is discussed in
further details in the following section.
Sentiment Analysis using social media data has been attempted by works such as [26] which tries to exploit patterns
in online social media communication and also by [27] which
uses background lexical information and reﬁning of the same
for speciﬁc domains by supervised learning techniques. However, we have computed Radicalization Indices using simpler
text regression techniques similar to [28], [29]. Our technique
of Radicalization Index computation, which is veriﬁed to be
quite accurate is discussed in further details in Section V.
In summary, individual Twitter users are our chosen level of
granularity. We characterize a user not only on the radicalization scale but we also obtain a location distribution of the user
over the regions of Indonesia. Hence, there is no prediction
of the location of the user involved as in [1]. It is to be noted
that we consider only the users classiﬁed as radical by our
Radicalization Index computation method.

where,
X = The number of tweets in T such that the author of the
tweet has Declared Home Location as Ra and geo-location of
the tweet is Rb
Y = The number of tweets in T such that the author of the
tweet has Declared Home Location as Ra
Let the ath row of the gCHL matrix be denoted by gCHLa .
Now Computed Home Location vector for the user Ui denoted
by CHLi is assigned the value of gCHLa if DHLi is region
Ra . It is to be noted that the gCHL matrix is general (and not
user speciﬁc) and is computed using the entire Twitter data set
comprising all users.
From those tweets Ti,k , 1 ≤ k ≤ ti of user Ui , that contain the
geo-location information GLi,k , we compute the Computed
Geo Location vector CGLi of length m, where CGLi,j , 1 ≤
j ≤ m, is the probability of the Actual home location of
user Ui being region Rj , as learnt from the tweets of Ui . The
CGLi,j is computed in the following way:
A
CGLi,j = B

where,
A = The number of tweets in Ti whose geo-location is
Rj and
B = The number of tweets in Ti whose geo-location is
known
We thus obtain two pieces of information about the Actual
home location of Ui in the form of two distributions: CHLi
and CGLi , where CGLi is completely user-speciﬁc. However,
CHLi is partially user-speciﬁc - it does depend on Ui because
CHLi is based on her Declared Home Location, but it
also depends on the general distribution which depends on
the entire population mass. It is evident that both CHLi
and CGLi are categorical distributions over the thirty four
Indonesian provinces. Now, a mixture of discrete distributions
over any ﬁnite number of categories is just another distribution
over those categories. In order to combine CGLi and CHLi
we obtain a convex combination of the two to obtain Li,j in
the following way:

IV. L OCATION I NDEX C OMPUTATION
As discussed earlier, each user Ui , 1 ≤ i ≤ n has a home
location HLi , 1 ≤ i ≤ n associated with her, which may or
may not be declared. Each tweet Ti,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti
has a geo-location GLi,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti associated
with it. However, GLi,k for some tweets Ti,k may not be
known as the user Ui might turn her GPS off. Even when
user Ui has a Declared Home Location DHLi , it may not be
accurate. User Ui might intentionally or inadvertently misstate
her location. Accordingly, we do not accept the DHLi at its
face value as the Actual home location of Ui . Instead, we
compute a matrix, which we term as the general Computed

Li,j = (1 − ωi ) ∗ CHLi,j + ωi ∗ CGLi,j

(1)

Now, the mixture weights ωi for Ui is learnt from the data
itself and is calculated as ωi = |Ti |/|Ti |.
Li,j essentially is given by




Li,j = |Ti | ∗ CHLi,j + |Ti | ∗ CGLi,j

282

(2)



label each organization as radical or counter radical based on
these organizations beliefs and practices. Using web crawling
tools, we download a large number of documents from the web
sites of these organizations. We use the term ‘vocabulary’ to
mean the set of all unique terms that appear in all documents
from all organizations. All the documents of an organization
are assigned the same Radicalization Index as that assigned
to the organization by the domain experts in our team. This
set of documents together with their Radicalization Indices
form the training dataset for our model. After that we use the
model to assign a Radicalization Index to the document Di
created from the tweets of user Ui . This Radicalization Index
of document Di is taken to be RDi of user Ui .

which gives equation (1) when normalized by |Ti | = |Ti | +

|Ti | i.e the total number of tweets posted by the user Ui ,
where
Ti = set of tweets produced by user Ui
Ti = subset of Ti and represents the set of tweets by Ui
that contains geo-location information
Ti = subset of Ti and represents the set of tweets by Ui
that do not contain geo-location information
The motivation behind this deﬁnition of the mixture weight

is that for the Ti tweets which contain geo-location information, we consider the user-speciﬁc location distribution
information inferred from the particular user’s geo-location

containing tweets. However, for the tweets of Ti , we have no
location information except for the general information that
given a Declared Home Location for any user Uv in our dataset
as Ra , CHLv for Uv is gCHLa . Thus, if DHLi of Ui is
given to be Ra , we consider CHLi = gCHLa . This simple
formulation of Li,j also captures the fact that we rely more
on CGLi than on CHLi when the number of tweets with
geo-location information, generated by Ui is high - however
if that count is low ( or even absent), instead of discarding
Ui ’s information, we obtain the location distribution of Ui
from her DHLi . We experimented by using only geo-location
containing tweets and we saw that the results are far more
accurate if we included users of Type 3 - this is intuitively
correct because the geo-location containing tweets form less
than 1% of the entire dataset. We compute Li,j for users
belonging to Classes 1-3 (deﬁned previously) using equation
(1). For the users belonging to Class 3, we obtain ωi to be
zero, as we do not have any geo-location data from the tweets
to compute ωi .

A. Problem Formulation:
We formulate the problem in a general sparse learning
framework and solve the following optimization problem (3)
using the techniques from [22] . This is indeed a sparse learning problem because the vocabulary is very large compared to
the number of words used in a document.
1
ρ
2
2
min Ax − y2 + x2 + λ x1
x 2
2

(3)

where A ∈ Rs×p , y ∈ Rs×1 , and x ∈ Rp×1
In our application, we have
a) A is Document × Term matrix which is constructed as
follows: The set of terms (t1 , . . . tp ) includes all the terms from
all the documents by all the organizations, barring the stop
words. The size of the vocabulary in this case is p. If data
is collected by crawling web sites of different organization
(O1 , . . . , Oq ) and documents (di,1 , . . . , d1,ri ) are collected
from the web site of organization Oi , 1 ≤ i ≤ q, the total
number of rows of the matrix A is s = Σqi=1 ri . Thus, Aij =
term frequency of the j th term in the ith document such that
Aij ≥ 0, 1 ≤ i ≤ s, 1 ≤ j ≤ p.
b) yi ∈ {+1, −1} is the class of each document Di , 1 ≤
i ≤ s. The Radicalization Index of a document is the same
the Radicalization Index of the organization that created that
document. Thus, when an organization is labeled as radical
(or counter radical) by the domain experts, all the documents
pertaining to that organization is marked as +1 (or −1). Thus
yi = +1 (or −1) if Di , 1 ≤ i ≤ s belongs to an organization
marked as radical (or counter radical) by the experts.
c) xj is the weight for each term tj , 1 ≤ j ≤ p. This is the
parameter estimated by optimizing the objective function (3).
The xj ’s thus form the predictor variables of the model.
Let us further clarify the three terms involved in the convex
optimization problem:
2
a) 12 Ax − y2 - this ﬁrst term is related to the sum of
the squared errors to ﬁt a straight line to a set of data points.
The objective function (3) thus is the optimization problem of
minimizing this sum of squared-errors.
2
b) ρ2 x2 - this term deals with the ridge regression, which
is an extra level of shrinkage. We set ρ = 0 as we were mainly
driven by sparsity.

V. R ADICALIZATION I NDEX C OMPUTATION
We intend to assign a Radicalization Index RDi to Ui based
on the content of her tweets. We collect tweets from users
over a period of time (a month-in our case) and for each
user Ui we create a document Di that contains all the tweets
of that user, during that period of time. As there exists a
one-to-one correspondence between Ui and Di , by assigning
a Radicalization Index to Di , we essentially assign RDi
to Ui . Classical predictive model Multiple Linear regression
[18], [19], [20] ﬁts our application, since it is a dichotomous
classiﬁcation problem with multiple predictor variables, where
the predictor variables are the terms of our ‘vocabulary’.
Classical classiﬁcation methods such as Logistic Regression
which has applications in a wide variety of domains can
also be used for document classiﬁcation [21]. Thus, Logistic
Regression can also be applied for our problem. However,
Linear Regression was selected instead of Logistic Regression
because it out-performed the Logistic one through 10-fold
cross validation. Linear Regression showed around 98%
accuracy, but Logistic Regression showed 83-85% of accuracy.
The implementation of our approach proceeds in the following
way: First, we identify a set of Indonesian political organizations. Next, social scientists in our Minerva team, who are
domain experts for Indonesia, hypothesize a classiﬁcation to

283

TABLE I: The table provides the top 5 province or special region names based on their computed Heat Index values (also
mentioned alongwith) for October 10 - November 10, November 11 - December 10, December 11- January 10
Province Name
Jakarta
East Java
West Java
Yogyakarta
Central Java

Heat Index
5.48
2.95
2.68
1.74
1.68

Province Name
Jakarta
East Java
Yogyakarta
Central Java
West Java

c) λ x1 - this term involving the L1 norm deals with the
sparsity of the solution vector x. For different values of λ
we obtain a solution vector x which represents the weights
associated with each term tj , 1 ≤ j ≤ p ( the same terms
which are considered in the A matrix). Some of these weights
are positive, some negative (values can be very close to 0). The
terms with positive (or negative) weights are the radical (or
counter radical) words. The top (ones with weights having high
magnitude) radical and counter radical words are presented to
the experts for validation. We experiment with several λ values
resulting in x vectors of various sparsity until the list of top
radical and counter radical words are approved by the ﬁeld
experts.
We use the Matlab implementation of the SLEP package
[23] that utilizes gradient descent approach to solve the
optimization problem (3). This package can handle matrices
of 20M entries within a couple of seconds on a machine with
standard conﬁguration. The input to the SLEP package are the
values of A, λ, and y. The SLEP model outputs the weight
vector x.

Heat Index
16.16
12.33
4.53
3.7
3.39

Province Name
Jakarta
Yogyakarta
West Java
East Java
Central Java

Heat Index
4.71
1.82
1.25
1.20
0.69

VI. H EAT I NDEX C OMPUTATION
Once we have obtained the Location Indices Li , 1 ≤ i ≤ n
and Radicalization Indices RDi , 1 ≤ i ≤ n, for all the users
Hj of region Rj , 1 ≤ j ≤ m
Ui , 1 ≤ i ≤ n , the Heat
Index
n
RD
is computed as Hj =
i × Li,j , ∀j, 1 ≤ j ≤ m.
i=1
The Heat Index Hj for a region Rj indicates the degree of
prevalence of radical ideologies among the people of Rj by
taking into account both the number of radical tweeters living
in Rj and also their ‘degree of radicalism’.
VII. DATA C OLLECTION
Since our model requires the computation of both RDi as well
as Li for each user Ui , we followed a two step data collection
procedure described as follows:
i) For the purpose of collecting the training data set for computing the Radicalization Index, we crawled the websites of
36 well-known Indonesian organizations which are classiﬁed
as radical or counter radical by our ﬁeld experts. A few of the
organizations are mentioned in Table II. We crawled the websites of all these different organizations and collected a total
of 78,135 documents which after pre-processing and ﬁltering
resulted into 49,250 documents. The reason for this reduction
in numbers is that many of the crawled documents did not have
any relevant information (for example documents having only
advertisements). Each of the documents on a average contained
280 words i.e on an average 2880 characters. All documents
pertaining to an organization were labeled as radical or counter
radical depending on the outlook professed by the organization
itself. These were then used for ﬁtting our Radicalization Index
computation model.
ii) For our study on recovery of political signals pertaining to
trend of radical activities in Indonesia, we chose Twitter as
the data collection platform as Indonesia accounts for 19.0%
to 20.8% of Twitter’s total reach by country (Dec 2010)3 .
No other publicly available portal offers access to opinions
posted online by the Indonesian populace on a similar scale
as does Twitter. For gathering tweets, we use Twitter’s Stream
API to access Twitter’s global stream of publicly available
tweet data. Since our goal is to recover ‘political signals’, we
setup a keyword ﬁlter on the Stream API to gather tweets that
relate to radical and counter radical ideologies. The keywords
used for this ﬁltration have been identiﬁed by the social
scientists in our Minerva project team and are considered to

B. Assignment of Radicalization Index:
For each time period (in our case one month), each user
Ui will be assigned an RDi based on their tweets within that
period. This is done as follows:
a) As mentioned earlier, from the tweets of each user Ui we
form a User Document Di . It is to be noted here that many
users choose to tweet quite infrequently, hence even if we
collect tweets for one month, a user might have tweeted only
once or twice during the entire one month which defeats the
purpose of collecting tweets for a month. Hence, we further
apply the constraint that we consider only those users who
have tweeted at least seven times in a month. The value of this
threshold has been arrived at empirically after experimentation
with various values of the threshold.
b) With the help of the model that has been ﬁtted using the
organization documents, we classify the Di ’s. Let each Di
which is a term frequency row be denoted by the row vector
tc of count of terms from our ‘vocabulary’.
c) Each user Ui receives
ap ‘score’ which we refer to as RDi
given by RDi = tc .x = j=1 tcj xj .
This provides us a time-series of RDi values for the users.
This makes it possible to analyze the transition dynamics of
each user. Evidently, a high positive RDi indicates that Ui is
highly radical whereas a high negative RDi indicates that Ui
is highly counter radical.

3 http://www.billhartzer.com/pages/comscore-twitter-latin-america-usage/

http://www.comscoredatamine.com/2011/02/the-netherlands-leads-globalmarkets-in-twitter-reach/

284

TABLE II: Table showing some of the well-known radical and
counter radical organizations of Indonesia
Radical Organizations
AbuJibriel
PKS
Arrahmah
EraMuslim
HizbutTahrir

Fig. 2: Figure showing the number of tweets collected over
our observation period

Counter radical Organizations
NU
Interﬁdei
IslamLiberal
PPIM
LKIS

TABLE III: Keyword markers used for ﬁltering Twitter Stream
API
Keyword
‘penegakan syariah’
‘jihad majelis’
‘mati syahid’
‘ajaran islam’
‘pendidikan agama di sekolah’
‘demokrasi yang’

Interpretation
enforcement of Sharia
jihad assemblies
martyrdom
the teaching of Islam
religious education in schools
democracy

VIII. E XPERIMENTAL R ESULTS
We created Heat Maps of Indonesia on a monthly basis. We
computed the RDi of each user U i for each month from
October 10 to January 10, as long as Ui tweeted at least 7
times in that month Again, for each user Ui we computed
the Location Index Li by considering all her tweets over
the period of the month. For that we computed the general
Computed Home Location gCHL matrix. The gCHL matrix
provides interesting insights on the Indonesian population. We
computed the gCHL matrix on all possible doublets among
the three months of observation period. i.e for each month for
calculating the Location Indices Li of users, we have generated
the gCHL matrix using the other two months of data. Thus,
in each case, we had training data of two months and test
data of one month. We observed that people with Declared
Home Locations in various different provinces from all around
Indonesia such as Bangka Belitung, Banten, Maluku, West
Nusa Tenggara, East Nusa Tenggara and Papua have a very
strong tendency to have high probability of having Actual
home location in Jakarta (as observed from our results over
three months). This is very intuitive because Jakarta being the
Capital Region must have attracted people from different parts
of Indonesia for prospective settlement. We further made an
observation that people with Declared Home Location of East
Kalimantan have considerable geo-location containing tweets
from Central Kalimantan.

be signiﬁcant markers of radical and counter radical ideologies
in the Indonesian context. A few such markers are listed in
Table III.
We collected tweet data for a three-month interval and
gathered a total of 12,152,874 tweets from October 10, 2012
to January 10, 2013 ( Figure 2) that matched the keyword
ﬁltration criteria. In this research, we are interested in the
probability distribution Location Index Li of user Ui over
the thirty four provinces of Indonesia, thus we focus only on
users from Indonesia. The keywords used are in Indonesian
language and narrows down the tweets we obtained from the
Twitter API. Thus, the geo-code in majority of cases indicated
a location in Indonesia. However, not all geo-codes are from
Indonesia. We ignore those tweets in the current work. Thus,
out of these 12 million tweets, 110,063 tweets contained
geo-locations that mapped to regions within Indonesia. To
apply this reverse geo-coding, we used the OpenStreetMap
API4 . A user repository was constructed by including only
those users whose Declared Home Locations matched with an
identiﬁable Indonesian city or province. We found that many
users have put texts such as ’Dark side of the moon’ or ‘Here’
or ‘infront of my laptop’ as home location and hence, there is
a need for pre-processing of the text. Also, the users provided
location information to varied degrees of granularity ranging
from continents to towns. However we are interested in the
ﬁxed granularity level of Indonesian provinces and the special
regions such as Jakarta and Yogyakarta. Hence we manually
created a database of towns and cities of all of the Indonesian
provinces. Each of the provinces were annotated with 42 cities/
towns on an average with Papua being the highest which
was annotated with 70 cities/towns. Using this database we
then assigned a legitimate Declared Home Location to as
many users as possible. The ﬁnal user repository consisted
of 959,911 unique users.

The Heat Indices values for the thirty four Indonesian
provinces are computed using our approach for three months
of our observation period - namely October 10 - November 10,
November 11 - December 10, December 11 -January 10. We
found a drastic change in the heat indices during the interval
of November 10 – December 10. But we could not discern any
particular event which could have triggered the same. Among
all Indonesian provinces the top ﬁve provinces and special
regions along with their Heat Index values are presented in
Table I for the three months. Color maps of Indonesia with
Heat Indices is shown in Figure 3, where darker colors indicate
a higher level of radical tweeting, and lighter colors indicate a
lower level of radical tweeting. It may be seen from Figure 3
that the area around Jakarta and the Java provinces are highly
active in radical tweet creation. According to our Twitter data
analysis, the provinces Jakarta, East Java, Yogyakarta and
Central Java, along with West Java are the top provinces that

4 The relevant information about the API could be found at http://wiki.openstreetmap.org/wiki/Nominatim

285

(a) Heat Map for October 10 to November 10

(b) Heat Map for November 11 to December 10

(c) Heat Map for December 11 to January 10

(d) Radicalism
Scale

Index

Fig. 3: Heat Maps of Indonesia
generate a high level of radical activities.
may be mentioned here that ﬁeld studies7 in January 2012
by Setara Institute8 , a well-known Indonesian NGO, showed
that the strong radicalism of the young muslim population
in Yogyakarta and Central Java are making them hot targets
to be recruited as Jihadists. In May 2012, there was a mob
attack by Indonesian Mujahidin Council in Yogyakarta and in
September 2012, there has been arrests of potential terrorists
from Yogyakarta9 . Because, Wahid Institute has mentioned
about Indonesian provinces only, it might be expected that
Jakarta and Yogyakarta, being special administrative regions,
are missing from their list - however, we do not have access to
their full report. The high radicalism of the Java provinces are
also corroborated by reports of the Setara Institute. The only
radically active province that shows up in the Wahid Institute
report but does not appear at the top of our list is Aceh, located
at the north west corner of Indonesia. It is worth mentioning
here that Aceh was completely devastated by the 2004 Indian
Ocean Tsunami and is still recovering from its effects. Aceh
is also one of the least economically developed provinces
of Indonesia. We believe that due to the lack of economic
advancement in Aceh, the level of Internet penetration in Aceh
is fairly small and not many people from Aceh are active
tweeters. This may explain the reason for Aceh not showing
up among our list of top radically active provinces.

IX. VALIDATION
For the purpose of validation of the Radicalization Index (not
the ‘degree of radicalism’), we computed the Radicalization
Indices of some well-known counter radical leaders of Indonesia for the months that they had tweeted for more than
7 times which we consider as our threshold. Our classiﬁer
gave perfect accuracy. By accuracy of the classiﬁcation we
mean the percentage of time the leaders who are thus known
to be counter radical were classiﬁed as counter radical by our
classiﬁer. We did not validate the Location Index computation
technique because of the lack of the ground truth of the
Actual home location of users. However, our results of Heat
Index are validated by the ﬁndings of the Indonesia-based
Wahid Institute5 . Wahid Institute promotes a moderate version
of Islam through dialogue events, publications, and public
advocacies. According to the Wahid Institute’s Annual Report
of 20126 , the top four provinces of Indonesia where radical
activities are most observable are West Java, Aceh, East Java,
and Central Java. It may be noted here, that three out of the
four most radical provinces identiﬁed by the Wahid Institute,
also appear at the very top of our list. Also, our ﬁeld experts
have conﬁrmed Jakarta to be a center of radical activities. It
5 http://berkleycenter.georgetown.edu/resources/organizations/wahidinstitute
6 Released on December 28, 2012

7 http://www.setara-institute.org/en/content/study-shows-how-youngradical-indonesian-muslims-become-terrorists
8 http://www.setara-institute.org/
9 http://www.washingtontimes.com/multimedia/image/indonesia-terrorjpg/

286

X. C ONCLUSION
We have developed a generic technique for recovering
signals pertaining to a geographical area such as a country
using Twitter Data. We have applied our technique to our
Indonesian dataset and have observed high accuracy. The
goal of our work is the generation of a political Heat Map
of Indonesia which highlights the Indonesian provinces with
prominent radical narrative. Thus, we have analyzed tweets
made by a user Ui in a month to assign a Radicalization
Index RDi to Ui where RDi indicates how radical is Ui
in her political outlook. Also, by mining the tweets in our
database we assign a Location Index Li to Ui . For computation
of the Location Index, we use not only the geo-location
tagging as provided by Twitter API but also the user declared
home location information from her proﬁle (after considerable
amount of pre-processing and cleansing). We have combined
these two sources of information for inferring the probability
distribution of the location of the users among the provinces
of Indonesia. Thus, by considering the RDi ’s in conjunction
with the Li ’s for all the users over a period of one month
we generate the political Heat Maps of the likes of Figure 3.
Such Heat Maps can prove to be very useful in studying the
spatio-temporal dynamics of the people of Indonesia so far as
their political outlook is concerned

[12] Cheng, Z., Caverlee, J., & Lee, K. (2010, October). You are where
you tweet: a content-based approach to geo-locating twitter users. In
Proceedings of the 19th ACM international conference on Information
and knowledge management (pp. 759-768). ACM.
[13] Boutet, A., Kim, H., & Yoneki, E. (2012, August). What’s in Twitter: I
Know What Parties are Popular and Who You are Supporting Now!. In
Advances in Social Networks Analysis and Mining (ASONAM), 2012
IEEE/ACM International Conference on (pp. 132-139). IEEE.
[14] Dahlin, J., Johansson, F., Kaati, L., Martenson, C., & Svenson, P.
(2012, August). Combining Entity Matching Techniques for Detecting
Extremist Behavior on Discussion Boards. In Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International
Conference on (pp. 850-857). IEEE.
[15] Eisenstein, J., Ahmed, A., & Xing, E. P. (2011, June). Sparse additive
generative models of text. In International Conference on Machine
Learning (ICML).
[16] Hong, L., Ahmed, A., Gurumurthy, S., Smola, A. J., & Tsioutsiouliklis,
K. (2012, April). Discovering geographical topics in the twitter stream.
In Proceedings of the 21st international conference on World Wide Web
(pp. 769-778). ACM.
[17] Yin, Z., Cao, L., Han, J., Zhai, C., & Huang, T. (2011, March).
Geographical topic discovery and comparison. In Proceedings of the
20th international conference on World wide web (pp. 247-256). ACM.
[18] Zhang, T. (2009). Some sharp performance bounds for least squares
regression with L1 regularization. The Annals of Statistics, 37(5A),
2109-2144.
[19] S.Kim, K.Koh, M.Lustig, S.Boyd and D. Gorinevsky, An Interior-Point
Method for Large-Scale l1 - Regularized Least Squares Journal of
seleccted topics in Signal Processing, Vol. 1, No. 4, pages 606 - 617,
Dec 2007
[20] Kolter, J. Z., & Ng, A. Y. (2009, June). Regularization and feature
selection in least-squares temporal difference learning. In Proceedings
of the 26th Annual International Conference on Machine Learning (pp.
521-528). ACM.
[21] Brzezinski, J. R., & Knaﬂ, G. J. (1999). Logistic regression modeling for context-based classiﬁcation. In Database and Expert Systems
Applications, 1999. Proceedings. Tenth International Workshop on (pp.
755-759). IEEE.
[22] Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for
generalized linear models via coordinate descent. Journal of statistical
software, 33(1), 1.
[23] J. Liu, S. Ji and Jieping Ye. SLEP: Sparse Learning with Efﬁcient
Projections, Arizona State University (2009)
[24] Lee, S. I., Lee, H., Abbeel, P., & Ng, A. Y. (2006, July). Efﬁcient
L˜ 1 Regularized Logistic Regression. In Proceedings of the National
Conference on Artiﬁcial Intelligence (Vol. 21, No. 1, p. 401). Menlo
Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.
[25] T. P. Minka, A comparison of numerical optimizaers for logistic regression, Technical report, 2007
[26] Thelwall, M., Buckley, K., Paltoglou, G., Skowron, M., Garcia, D.,
Gobron, S., ... & Holyst, J. A. (2013). Damping Sentiment Analysis
in Online Communication: Discussions, Monologs and Dialogs. In
Computational Linguistics and Intelligent Text Processing (pp. 1-12).
Springer Berlin Heidelberg.
[27] Melville, P., Gryc, W., & Lawrence, R. D. (2009, June). Sentiment analysis of blogs by combining lexical knowledge with text classiﬁcation.
In Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining (pp. 1275-1284). ACM.
[28] Joshi, M., Das, D., Gimpel, K., & Smith, N. A. (2010, June). Movie
reviews and revenues: An experiment in text regression. In Human
Language Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational Linguistics (pp.
293-296). Association for Computational Linguistics.
[29] Kogan, S., Levin, D., Routledge, B. R., Sagi, J. S., & Smith, N. A.
(2009, May). Predicting risk from ﬁnancial reports with regression.
In Proceedings of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Association for
Computational Linguistics (pp. 272-280). Association for Computational
Linguistics
[30] Brynielsson, J., Horndahl, A., Johansson, F., Kaati, L., Martenson, C.,
& Svenson, P. (2012, August). Analysis of weak signals for detecting
lone wolf terrorists. In Intelligence and Security Informatics Conference
(EISIC), 2012 European (pp. 197-204). IEEE.

R EFERENCES
[1] Cho, E., Myers, S. A., & Leskovec, J. (2011, August). Friendship
and mobility: user movement in location-based social networks. In
Proceedings of the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining (pp. 1082-1090). ACM.
[2] Xu, J. M., Bhargava, A., Nowak, R., & Zhu, X. (2012). Socioscope:
spatio-temporal signal recovery from social media. In Machine Learning
and Knowledge Discovery in Databases (pp. 644-659). Springer Berlin
Heidelberg.
[3] Priedhorsky, R., Culotta, A., & Del Valle, S. Y. (2013). Inferring the
Origin Locations of Tweets with Quantitative Conﬁdence. arXiv preprint
arXiv:1305.3932.
[4] Yang, D., Zhang, D., Yu, Z., & Wang, Z. (2013, May). A sentimentenhanced personalized location recommendation system. In Proceedings
of the 24th ACM Conference on Hypertext and Social Media (pp.
119-128). ACM.
[5] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. (2012, December).
Dissecting foursquare venue popularity via random region sampling.
In Proceedings of the 2012 ACM conference on CoNEXT student
workshop (pp. 21-22). ACM.
[6] Gonzalez, M. C., Hidalgo, C. A., & Barabasi, A. L. (2008). Understanding individual human mobility patterns. Nature, 453(7196), 779-782.
[7] Sadilek, A., Kautz, H., & Bigham, J. P. (2012, February). Finding your
friends and following them to where you are. In Proceedings of the
ﬁfth ACM international conference on Web search and data mining (pp.
723-732). ACM.
[8] Noulas, A., Scellato, S., Lambiotte, R., Pontil, M., & Mascolo, C.
(2012). A tale of many cities: universal patterns in human urban mobility.
PloS one, 7(5), e37027.
[9] Chang, H. W., Lee, D., Eltaher, M., & Lee, J. (2012, August). @
Phillies Tweeting from Philly? Predicting Twitter User Locations with
Spatial Word Usage. In Advances in Social Networks Analysis and
Mining (ASONAM), 2012 IEEE/ACM International Conference on (pp.
111-118). IEEE.
[10] Mahmud, J., Nichols, J., & Drews, C. (2012). Where is this tweet from?
inferring home locations of twitter users. Proc AAAI ICWSM, 12.
[11] Eisenstein, J., O’Connor, B., Smith, N. A., & Xing, E. P. (2010,
October). A latent variable model for geographic lexical variation. In
Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing (pp. 1277-1287). Association for Computational
Linguistics.

287

Optimal Tracking of Multiple Targets
Using UAVs
David Hay1 , Shahrzad Shirazipourazad2(B) , and Arunabha Sen2
1

School of Computer Science and Engineering, Hebrew University, Jerusalem, Israel
dhay@cs.huji.ac.il
2
School of Computing, Informatics and Decision System Engineering,
Arizona State University, Tempe, AZ 85281, USA
{sshiraz1,asen}@asu.edu

Abstract. Target tracking problems have been studied fairly extensively by researchers in the last few years. However, the problem of
continuous tracking of all mobile targets using the fewest number of
mobile trackers, even when the trajectories of all the targets are known
in advance, has received very little attention. In this paper we study this
problem, where the goal is to ﬁnd the fewest number of trackers needed
to track all the targets for the entire period of observation. Speciﬁcally,
given a set of n targets moving in n diﬀerent (known) trajectories in
a two (or three) dimensional space, our objective is to ﬁnd the fewest
number of velocity-bounded UAVs (mobile sensors, trackers) and their
trajectories, so that all the targets are tracked during the entire period
of observation. We also study two other versions of the problem where
not only the number of trackers but also the time during which the
trackers are active is also taken into account. We formulate these problems as network ﬂow problems and propose algorithms for their solution.
We evaluate the performance of our algorithms through simulation and
study the impact of parameters such as the speed and sensing range of
the trackers.

1

Introduction

Motivated by the importance of target tracking in military and civilian environments and widespread use of UAVs in target tracking, considerable research has
been conducted on target tracking problems using UAVs and mobile sensors
[1–6]. There also exists a large body of research on target tracking problems using
sensor networks. However, in most of these studies the sensor nodes (trackers)
are static and as such the issue of path planning of trackers does not exist. The
authors in [7] provide a survey of these studies. Typically, the target tracking problem using mobile trackers has two components: (i) estimation of target positions
using sensor data, and (ii) mobility management of trackers (sensors). Most of
the studies on target tracking using mobile sensors focus on the quality of detection of the mobile targets with a given set of UAVs (mobile sensors). In these
studies one or more mobile trackers are used to track a single or multiple targets.
c Springer International Publishing Switzerland 2014

Z. Zhang et al. (Eds.): COCOA 2014, LNCS 8881, pp. 750–763, 2014.
DOI: 10.1007/978-3-319-12691-3 55

Optimal Tracking of Multiple Targets Using UAVs

751

In [6] the authors study distributed mobility management of a given set of mobile
sensors for better tracking of a single target. Enyang et al. in [5] study a scenario
when there is one mobile sensor, a set of static sensors and a mobile sensor controller. The controller receives the data from the sensors and estimates the location
of the target and direct mobility of sensors. The authors in [8] study multi-target
tracking using multiple UAVs and develop a decentralized approach for target
location estimation and UAV mobility management. In spite of extensive studies
on target tracking problem, there exists only a handful of studies on the problem
of continuous time tracking of mobile targets with an optimal number of mobile
trackers.
In this paper we focus on the scenario where there are multiple mobile targets
whose trajectories are known in advance. Accordingly, estimation and prediction
of target location is not an issue here and the focus of this target tracking problem is on ﬁnding the minimum number of UAVs with bounded velocity and their
trajectories so that every target is tracked (covered) by at least one UAV during
the entire period of observation. Although it may appear that the assumption
regarding complete knowledge of the trajectories of the targets makes the problem very simple (if not trivial), we show that even with this assumption, the
problem of computation of minimum number of trackers and their trajectories
for continuous coverage of the mobile targets remains hard, i.e., NP-complete.
In [1] the authors study energy eﬃciency issues related to mobile target
tracking and provide a power eﬃcient target tracking solution by adjusting the
UAVs position (altitude). In [9] authors study a similar problem in which their
goal is to ﬁnd the smallest set of mobile backbone nodes such that mobile regular
nodes are always under coverage of at least one backbone node. In these papers
[1,9] it is assumed that trackers have the capability of moving very fast, i.e.,
inﬁnite speed. The authors in [10] study the problem of ﬁnding the optimal set
of UAVs, assuming that the UAVs have only a ﬁnite speed. They propose an
algorithm in which it ﬁrst computes the solution of coverage problem for each
time instance (static disk coverage problem) using a greedy algorithm. Then
they propose a motion assignment algorithm determining the movement of UAVs
from their old positions to new positions. The greedy algorithm proposed by the
authors does not guarantee optimality of the solution.
In this paper we study three diﬀerent versions of the target tracking problem (TTP). The inputs to all three versions of the problem are the same and
it comprises of (i) n targets with their trajectories in two or three dimensional space, (ii) speed of the trackers, and (iii) sensing range of the trackers,
(iv) period of observation. We assume that a tracker can only track (sense) a
target if the target is within the sensing range of the tracker. A tracker can either
be in an active or and inactive state. A tracker is said to be in an active state if
either (i) it is sensing a target, or (ii) moving towards a target to sense it in a
future time, before the end of the period of observation. The diﬀerent versions
of the Target Tracking Problem studied in this paper are stated next.
TTP1 : The objective of TTP1 is to minimize the total number of trackers needed
to track all the targets during the entire period of observation.

752

D. Hay et al.

Fig. 1. Trackers and their trajectories for tracking all the three targets.

TTP2 : The objective of TTP2 is to minimize the total active time of the trackers
within the period of observation.
TTP2A : The objective of TTP2A is to minimize the total active times of the
trackers within the period of observation, subject to the constraint that the
number of trackers do not exceed a speciﬁed number P .
TTP3 : The objective of TTP3 is to minimize the total active times of the
trackers within the period of observation, subject to the constraint that the
number of trackers do not exceed the absolute minimum number of trackers
necessary to track all the targets. We note that the answer to the TTP1 problem
provides the absolute minimum number of trackers necessary to track all the
targets. Also, we note that TTP3 can be solved ﬁrst by computing the solution
of TTP1 and then computing the solution of TTP2A when P is set to the solution
of TTP1 . However, in this paper we model TTP3 independently.
Figure 1 depicts an example with three targets. The locations of three targets
(red squares) on their trajectories (red curves) are shown at time instances 0 to
5. Blue dashed curves show the trajectories of trackers and the disks centered
at the locations of trackers depict the sensing area of trackers at diﬀerent time
instances.
In this paper we model the target tracking problems using network ﬂows
and we propose techniques using integer linear programming to solve the three
versions optimally.
The rest of the paper is organized as follows. In Sect. 2 we formulate target
tracking problems. Section 3 presents our technique to solve these problems. In
Sect. 4 we evaluate our technique via simulations. Section 5 concludes the paper.

2

Problems Formulation and Hardness Results

In order to formulate the three problems, we ﬁrst discretize time into equal
time intervals of length δ. We consider a set of n targets A = {a0 , . . . , an−1 }

Optimal Tracking of Multiple Targets Using UAVs

753

moving on two dimensional space over time instances 0, . . . , T 1 . Let p(ai , t) =
(x(ai , t), y(ai , t)) be the location of target ai at time instance t where x(ai , t)
and y(ai , t) denote the x-coordinate and y-coordinate of ai at time t. We assume
that a target is covered by a tracker bj if the distance between them is less than
sensing radius r. Let |p1 ∼ p2 | denote the distance between two points p1 and
p2 on the two dimensional space.
In TTP1 our goal is to ﬁnd a smallest set of trackers B, such that:
1. Coverage - For any target ai and any time instance t, there is a tracker bj ∈ B
whose location in time t, denoted by p(bj , t) is in distance r from p(ai , t); i.e.,
|p(ai , t) ∼ p(bj , t)| ≤ r.
2. Mobility - For any tracker bj with speciﬁed velocity d and any time slot t ≥ 1,
|p(bj , t − 1) ∼ p(bj , t)| ≤ d.
Let B(t) ⊆ B denote the set of trackers being used at time instance t. Also,
let Tj denote the number of time instances that target bj is active. In TTP2 the
objective is to ﬁnd a set of trackers Bwhere the total
T active time of the trackers
within the period of observation, i.e. bj ∈B Tj = t=0 |B(t)| is minimized while
both the Coverage and Mobility constraints of TTP1 are satisﬁed. TTP2A adds
an additional constraint to TTP2 : the size of B should not exceed P . Considering
of TTP1 and TTP2 , in TTP3 the objective is to minimize
T
 both objectives
T
=
|B(t)|
subject to the constraint that the number of trackers
j
bj ∈B
t=0
do not exceed the absolute minimum number of trackers necessary to track all
the targets. It may be noted that the answer to the TTP-1 problem provides the
absolute minimum number of trackers necessary to track all the targets.
Theorem 1. All three versions of TTP are NP-complete.
Proof. Considering a special case of TTP where targets should be covered at
just one time instance t = T = 0, all the three versions of TTP problem will
be equivalent to the NP-complete Geometric Disk Cover Problem [11]; that is,
given points in the plane, to identify a minimally sized set of disks (of prescribed
radius) covering all points.

3

Solution

In this section we propose our techniques to compute the solution of the three
versions of target tracking problem, TTP1, TTP2 and TTP3 . In order to compute the solutions of these problems, ﬁrst we model them by a ﬂow network, by
discretizing also the space. Next, We ﬁnd the solution of the problems by computing (a modiﬁed version of) minimum ﬂow on a directed graph G = V, E.
The ﬂow is then splitted into paths, where each path represents movement of a
tracker. Before explaining our graph construction ﬁrst we give the deﬁnition of
the classical minimum flow problem [12].
1

We present the formulation in two dimensions for brevity. Extensions to higher
dimensions is straightforward and is discussed in Sect. 3.

754

D. Hay et al.

Minimum Flow Problem: Given a capacitated network G = V, E with a
nonnegative capacity c(i, j) and with a nonnegative lower bound l(i, j) associated
with each edge (i, j) and two special nodes, a source node S and a sink node D,
a ﬂow is deﬁned to be a function f : E → R+ satisfying the following conditions:
⎧
⎨ F, i = S


0, i 
= S, D
f
(i,
j)
−
f
(j,
i)
=
j∈V
j∈V
⎩
−F, i = D
l(i, j) ≤ f (i, j) ≤ c(i, j)
for some F ≥ 0 where F is the value of the ﬂow f . The minimum ﬂow problem
is to determine a ﬂow f for which F is minimized.
3.1

TTP Graph Construction

We construct a directed graph G = V, E representing the positions of the
targets and the possible trackers’ movements. This construction involves space
discretization (in addition to the time discretization described in Sect. 2). Specifically, we consider a grid over the two- (or three-) dimensional space and restrict
our trackers to move between points on that grid. The granularity of our grid
is denoted by ε, which represents a tradeoﬀ between the accuracy of our solution and its running time. Let N be the set of all points in the grid; namely,
for the two-dimensional space, N = {i · ε, j · ε | i, j ∈ Z}. We note that based
on the coverage constraint, for a target ai to be covered by a tracker at a time
instance t, there should be at least one tracker in the disk of radius r centered
at p(ai , t). Let D(p(ai , t), r) denotes that disk. Thus, a tracker should be located
in one of the points in N ∩ D(p(ai , t), r). For every such potential location,
at any given time, we add a vertex graph, which is represented by the triplet
location, target id, timeslot, where location corresponds to the coordinates of
the points on the grid (namely, the same location can be added multiple times
for many targets and/or timeslots). Fig. 2(a) depicts a disk D(p(ai , t), r). Blue
circles show N ∩ D(p(ai , t), r) and the red square represents the center of disk.
In addition, we add one supersource vertex S and one supersink vertex D. In
other words, the set of vertices will be
V = {S, D} ∪

T n−1



N ∩ D(p(ai , t), r), ai , t.

t=0 i=0

We will have four types of edges:
1. Intra-target edges: For every target ai and time-slot t we construct a directed
ring connecting all the vertices with target id ai and time instance t (Fig. 2(b)).
We note that the order of the nodes in the ring is arbitrary.
2. Mobility edges: {(p, ai , t, p , ai , t )|i 
= i , |p ∼ p| ≤ d|t − t|}. Note that if
t = t then p = p. These edges have capacity 1 and demand 0. The direction
of these edges goes from the node with lower timeslot to the node with higher
timeslot, where ties are broken by nodes’ target id. A mobility edge represents
that a tracker can move from a location p to p during time interval |t − t|.

Optimal Tracking of Multiple Targets Using UAVs

755

Fig. 2. (a) A disk centered at the location of target ai at time t (b) The ring corresponding to the discrete points in disk shown in (a).

3. Supersource edges: All vertices are connected to the supersource S with edges
of capacity 1 and demand 0. These edges originate in the supersource.
4. Supersink edges: All vertices are connected to the supersink D with edges of
capacity 1 and demand 0. These edges terminate in the supersink.
We note that space discretization comes with a price, as it rules out solutions
in which the trackers are not at a grid point in each time-slot. As ε 
 d (namely,
the granularity of the grid is much ﬁner than the maximum velocity of the
tracker), these diﬀerences are negligible.
3.2

Modified Minimum Flow Problem

In Fig. 3 we illustrate an example on a line (one dimensional) with 3 targets
and we show their locations on the X coordinate at two timeslots. In this example r = 2,  = 1, and d = 1. Figure 4 depicts graph G = V, E. For the
sake of clarity, not all edges in E are shown. The number on each node shows
the location on X dimension. In this example we note that the minimum ﬂow
value is zero while value of f of intra-target edges is 1 and all the other edges
have ﬂow zero. More speciﬁcally, all the lower bound constraints on the intratarget edges are satisﬁed while there is no ﬂow starting from S. Hence, we have
modiﬁed the minimum ﬂow problem such that the lower bound constraints on
the intra-target edges in graph G = V, E are satisﬁed if the ﬂow starts from
source node S. In this regard, we add additional constraints to make sure there is
a path from S to every node in V \{S, D} such that the ﬂow f on the edges of the
path is 1. Let gu be a ﬂow function deﬁned with respect to node u ∈ V \{S, D}.
A closer look at our construction shows that if there is a path from S to one
node in a ring, there is a path to all nodes in that ring. Hence, for every ring in
G, corresponding to a target ai at a time instance t, we select an arbitrary node
u (which is of form p, ai , t) and add the following constraints to minimum ﬂow
problem:
⎧
⎨ 1, i = S


0, i 
= S, D
g
(i,
j)
−
g
(j,
i)
=
u
u
j∈V
j∈V
⎩
−1, i = D
gu (i, j) ≤ f (i, j)

756

D. Hay et al.

Fig. 3. An example of target tracking problem.

3.3

Solution of TTP1

Considering that a tracker covers a target only if it is placed at a discrete point
in the disk centered at the location of target at each time instance, we will prove
that the minimum number of trackers required to cover the targets at all time
instances is equal to the value of modiﬁed minimum ﬂow in graph G = V, E,
and the trajectories of trackers can be computed from the solution of modiﬁed
minimum ﬂow problem.
In Fig. 4, the blue edges show the edges with ﬂow 1 in the modiﬁed minimum
ﬂow solution. We can see that the value of modiﬁed minimum ﬂow is 2 meaning
that 2 trackers are required to cover the three targets in two time slots. The
ﬁrst tracker at time 0 is at location X = 3 (x(a0 , 0) +  = 3) covering target a0
and moves to X = 4 at time 1 covering both a0 and a1 (x(a0 , 1) +  = 4 and
x(a1 , 1) − 2 = 4). The second tracker at time 0 is at location X = 9 covering
targets a1 and a2 where x(a1 , 0) + 2 = 9 and x(a2 , 0) −  = 9 and moves to
X = 10, (x(a2 , t1 ) −  = 10) covering a2 .
Lemma 1. Any valid flow with value F on the constructed graph can be decomposed to a valid assignment of F trackers.
The decomposition works by ﬁrst assigning a tracker to a supersource edge with
ﬂow 1. Then, we get to the node p, ai , t to which this supersource edge is
connected. We assign the tracker b a location p at timeslot t. If the intra-edges
of target ai still have ﬂow of size 1, then we traverse these intra-edges, and say
that b covers ai at timeslot t. After the traversal (or in case no traversal was
needed), we pick one outgoing edge from p, ai , t with ﬂow 1. Such edge exists
from ﬂow conservation. We look at the edge’s destination, p , ai , t . If t = t
then p = p, and then we try to take the ring of ai , add it to the coverage of
b at time t and continue recursively with an outgoing edge from p , ai , t . If
t > t (that the only other case), then we move b to position p at timeslot t .
This move is legal since only mobility edges with |p ∼ p| ≤ d|t − t| are added to
the graph. We continue this process until we hit D. Then, we have the trajectory
of b (as well as which targets it covers at each timeslot). We reduce the ﬂow of
b from the ﬂow we got and continue the decomposition on the remaining ﬂow
(assigning more trackers). In the end of the process we will have the trajectories
of all trackers, obeying their mobility restriction (movement of at most distance
d at each timeslot). In addition, we ensure that every target is covered at all time
instances, because the ﬂow was valid which implies that all intra-edges had ﬂow

Optimal Tracking of Multiple Targets Using UAVs

757

Fig. 4. The modiﬁed minimum ﬂow solution of the example in Fig. 3

1 (meeting their demand); and therefore every target is traversed by a tracker
in the decomposition of F . Since we assign one tracker to each supersource edge
of ﬂow 1, our tracker assignment will have exactly F trackers.
Lemma 2. Any valid assignment
B of F trackers with maximum velocity d −
√
√
2ε and sensing radius r − 22 ε in the two-dimensional space can be represented
as a valid flow with value F on this graph.2
Every tracker bj starts its trajectory at some timeslot ti,j , at location pi,j , covering a set of targets Ai,j . Let pi,j be the closest grid point to pi,j (in the
two-dimensional plane, pi,j is at most of distance
√
2
2 ε

√
2
2 ε

of pi,j ). Thus, as all tar-

of pi,j , they are within distance r of
gets in Ai,j are within distance r −
pi,j . Next, tracker bj moves to a new location pi ,j to cover a set of targets Ai ,j
at timeslot ti ,j > ti,j . Let pi ,j be the closest grid point to pi ,j . Tracker bj will
be active till it is not assigned to cover any target anymore and its trajectory
ﬁnishes. We can represent the trajectory of bj by a ﬂow of value one in G in the
following way. Corresponding to the tracker bj , we add a ﬂow of value one to G
leaving S to the node pi,j , ax , ti,j  where ax ∈ Ai,j and ax has the lowest target
id among other targets in Ai,j . pi,j , ax , ti,j  ∈ V since ax is within distance
r of pi,j and pi,j ∈ N . The ﬂow traverses the ring corresponding to ax at ti,j
satisfying the demand on the intra-target edges and it leaves the ring from the
same node pi,j , ax , ti,j . Next, the ﬂow enters the node pi,j , ay , ti,j  in the ring
corresponding to the next target ay ∈ Ai,j where y > x. It traverses the ring and
this process continues till all the rings corresponding to the targets in Ai,j at ti,j
are traversed by this ﬂow (all the edges in these rings have a ﬂow value of one).
We note that if a target is covered by more than one tracker at a time instance,
2

This result can be easily extended to spaces with higher dimensions. The diﬀerence
√
√
between velocity d and velocity d − 2ε, or between sensing radius r and r − 22 ε,
is negligible for all practical purposes.

758

D. Hay et al.

just one of them is considered to be in charge of covering the target at that time
instance. In other words, just a ﬂow of value one is selected to traverse every ring.
Hence, the ﬂow value on the intra-target edges does not exceed the capacity. We
also note that for every two targets ax , ay ∈ Ai,j at time ti,j , there is a mobility edge from pi,j , ax , ti,j  to pi,j , ay , ti,j  (based on the graph construction),
where the ﬂow goes from one ring to next ring through this edge. Next, through
a mobility edge, this ﬂow enters the node pi ,j , az , ti ,j  where az ∈ Ai ,j . Since,
the tracker is able to move from location
√ pi,j to pi ,j between timeslots ti,j and
is
bounded
by
d
−
2ε, the distance between pi,j and pi ,j is
ti ,j and its velocity
√
at most (d − 2ε)(tj − tj ) and hence the distance between pi,j to pi ,j is at most
√
√
√
2
2ε)(tj − tj ) + 22 ε ≤ d(tj − tj ), implying there is a mobility edge
2 ε + (d −
(pi,j , ay , ti,j , pi ,j , az , ti ,j ) in G. Then, the ﬂow continues recursively based
on the trajectory of tracker, and at the time that the tracker becomes inactive
the ﬂow goes to the supersink node D. It can be seen that the ﬂow constraints
on all the edges are satisﬁed. Also, since we add a ﬂow of value one to the graph
from S to D for every tracker, total ﬂow will be exactly F .
These lemmas imply that the minimum ﬂow yields the minimum assignment
and we are done.
Extension to higher dimensions: In 2 dimensional space we use O((r/ε)2 )
nodes connected in a ring to represent the disk around a target. In order to
extend this solution to 3 dimensions it is enough to discretize space in all 3
dimensions and use O((r/ε)3 ) nodes to represent the ball around a target and
connect the nodes as a ring in anyway. Similarly, we can extend the model to
higher dimensions.
3.4

Solution of TTP2

Even though in the TTP1 problem the objective is to minimize number of trackers in use during the tracking time, it may not minimize the total time that
trackers are active. Fig. 5 illustrates an example in one dimensional line for two
time steps. It is assumed that r = ε = 1. In this example minimum number
of trackers needed is two. However, there can be diﬀerent tracking solutions in
which the two trackers are used: (1) the two trackers are active in both time
steps where p(bi , t) = p(ai , t), ∀i ∈ {0, 1} for t = 0 and t = 1. (2) at t = 0,
x(b0 , 0) = x(a0 , 0) + 1 and x(b1 , 0) = x(a1 , 0); at t = 1 just one tracker b0
is active and x(b0 , 1) = x(a0 , 1) + 1. We observe that total time that trackers
are active in ﬁrst solution is 4 while in second solution is 3. Hence, the second
solution is more eﬃcient in using the trackers comparing to the ﬁrst solution.
In TTP2 , the objective is to minimize the total time that trackers are active in
the observation period.
In order to solve TTP2 we use similar ﬂow network model G = V, E and
ILP as in TTP1 . As we explained in TTP1 a ﬂow in the solution of TTP1
corresponds to a tracker. In order to compute the time duration that a tracker
is in use we add following costs (timestamps) to edges in E:

Optimal Tracking of Multiple Targets Using UAVs

759

Fig. 5. An example of target tracking problem.

– Cost of supersource edges: every edge connecting the supersource to a node
p, i, t ∈ V where p is location, i is target id and t is the time corresponding to
this node, has time-stamp t. If a ﬂow value on this edge is one, the time-stamp
actually shows the time that the tracker corresponding to this ﬂow begins to
track.
– Cost of supersink edges: every edge connecting a node p, i, t ∈ V to the
supersink node has time-stamp t + 1. If a ﬂow value on this edge is one, the
time-stamp actually shows the time that the tracker corresponding to this
ﬂow will no longer be in use.
– The rest of the edges have cost zero.
Let c(u, v) denote the cost (time-stamp) of an edge (u, v) ∈ E. The total
time 
that the trackers are active
over observation period can be computed as

S = j∈V c(j, D)f (j, D) − j∈V c(S, j)f (S, j). Hence, the objective of TTP2
will be minimizing S and the constraints will be the same as TTP1 .
In TTP2A version, there is one more constraint; the number of trackers should
not exceed a number P . In this case we need to add one more constraint to our
ﬂow model, i.e., F ≤ P .
3.5

Solution of TTP3

We note that solution of TTP2 may not minimize total number of distinct trackers over time. For the example depicted in Fig. 5 (and explained in previous part),
The objective value of TTP2 is 3. A solution of TTP2 with objective value 3 can
include three distinct trackers where two of them are used at ﬁrst time step
and third tracker is located at x(a0 , 1) + 1 at second time step to cover both
targets. The total time that trackers are active is still 3 while total number
of distinct trackers is not minimized (minimum number of distinct trackers is
2). In TTP3 both objectives of TTP1 and TTP2 are considered together. In
TTP3 we would like to ﬁnd a tracking solution while it minimizes total time
of trackers being active, the total number of distinct trackers does not exceed
the solution of TTP1 . We note that TTP3 can be solved ﬁrst by computing
the solution of TTP1 and then computing the solution of TTP2A when P is set
to the solution of TTP1 . However, we model TTP3 independently as one ﬂow
network problem. In this regard, using the same ﬂow network model in TTP2 ,
the objective of TTP3 is to minimize M.F + S/T where F is the value of ﬂow

760

D. Hay et al.

(number of trackers), S is the total time that trackers are active, and M is a
suﬃcient large number.

4

Simulation Results

In this section, we investigate the eﬀect of diﬀerent parameters such as coverage
radius, r, and speed of trackers, d, on the number of trackers needed to cover
all the targets. We also perform experiments to examine the eﬀects of values of
ε and δ on the accuracy of our technique. We use CPLEX to solve the ILP for
TTP1 . In our simulations we considered 5 targets are moving on 2 dimensional
area. Their trajectories over time interval [0, 10], depicted in Fig. 6, are given by
parametric equations.

Fig. 6. Trajectories of 5 targets in time interval [0, 10]

In the ﬁrst set of experiments, we compute the minimum number of trackers
to cover the targets (Fig. 6) for diﬀerent values of the tracker speed, d. In these
experiments r = 2, ε = 1, δ = 2 and total observation period is [0, 10]. Figure 7
illustrates the results of these experiments. It can be seen that when d is smaller
than the speed of targets, number of trackers needed to cover all targets is more
than number of targets and by increasing the value of d number of trackers
decreases drastically initially and when d ≥ 2 which is close or greater than
speed of targets, three trackers can cover the targets in all time steps. We note
that the speed of targets in these experiments is not constant and is not uniform
since the parametric equations of their trajectories are diﬀerent.
In the second set of experiments, we examine the eﬀect of coverage radius
of trackers on the solution of TTP1 . In these experiments ε = 0.5, δ = 2 and
total observation period is [0, 10]. Figure 8 shows the results of these experiments
for two diﬀerent values of d. We can see when speed of trackers is large enough
(in this example d = 2), number of trackers needed to cover the targets will

Optimal Tracking of Multiple Targets Using UAVs

761

Fig. 7. Number of trackers vs. trackers speed in time interval [0, 10], r = 2, ε = 1,
δ = 2.

Fig. 8. Number of trackers vs. trackers coverage radius in time interval [0, 10], d = 1,
ε = 0.5, δ = 2.

not exceed number of targets, even if tracker radius is very small. When tracker
radius increases then number of trackers for both values of d is getting closer
together. We note that when value of r is large enough then at least one tracker
is needed to cover all the targets.
In the experiments, we examined diﬀerent values for δ and ε. Generally, the
smaller are ε and δ, the closer is the solution of our technique to the optimal
solution of continuous scenario. We note that smaller value of δ results in lower
chance that a target may not be covered by a tracker; also, more tracker may
be needed to cover the targets in all time instances comparing to larger values
of δ (considering that larger value of δ is an integer multiple of smaller value).
On the other hand, smaller values of ε increases the possible locations that are
considered for trackers and it results in smaller number of trackers in comparison
to larger value of ε. While smaller values of ε and δ increase accuracy of the
solution of TTP, they will increase the cost of computation. For the instance
depicted in Fig. 6, we examined diﬀerent values of δ = 1, 2, 4, 8 and  = 0.5, 1
where r = 2, d = 2 in time interval [0, 8]. For this set of trajectories of targets,

762

D. Hay et al.

change of δ and  does not eﬀect the solution of TTP. In all these experiments
the solution of TTP1 is 3. However, if the trajectories change then for diﬀerent
values of δ and , the solution of TTP1 may change. Especially, if the targets get
very close and then very far in short time intervals. For example, Fig. 9 depicts
trajectories of two targets in time interval [0, 4]. Let ε = 0.5, r = 1, and d = 2.
We computed the solution of TTP1 for diﬀerent values of δ ∈ {0.5, 1, 2, 4}. When
δ = 0.5 the solution of TTP1 is 2 while for the rest of values the solution of TTP1
is 1.

Fig. 9. Trajectories of 2 targets in time interval [0, 4]

5

Conclusion

In this paper, we studied the problem of continuous coverage of multiple mobile
targets using mobile trackers. We considered the case that trajectories of targets
are known in advance and we studied three diﬀerent problems of (1) minimizing number of trackers (2) minimizing total time that trackers are active and
(3) minimize the total active times of the trackers within the period of observation, subject to the constraint that the number of trackers do not exceed the
absolute minimum number of trackers necessary to track all the targets. We
proposed a model using ﬂow networks to formulate these three problems. Using
simulations we investigated the eﬀect of diﬀerent parameters such as coverage
radius on the minimum number of trackers needed for complete coverage. In the
future, we plan to present an approximation algorithm for these problems where
the solution is continuous and provides all time coverage of all targets. Also,
we would like to study the on-line scenario where trajectories of targets are not
known in advance.
Acknowledgments. This research is supported in part by grants from the U.S.
Defense Threat Reduction Agency under grant number HDTRA1-09-1-0032, the U.S.
Air Force Oﬃce of Scientiﬁc Research under grant number FA9550-09-1-0120, and the
Israeli Centers of Research Excellence (I-CORE) program (Center No. 4/11).

Optimal Tracking of Multiple Targets Using UAVs

763

References
1. Zorbas, D., Razaﬁndralambo, T., Luigi, D.P.P., Guerriero, F.: Energy eﬃcient
mobile target tracking using ﬂying drones. Procedia Comput. Sci. 19, 80–87 (2013)
2. Zhan, P., Casbeer, D., Swindlehurst, A.: A centralized control algorithm for target
tracking with uavs. In: Conference Record of the Thirty-Ninth Asilomar Conference
on Signals, Systems and Computers, pp. 1148–1152, October 2005
3. Wheeler, M., Schrick, B., Whitacre, W., Campbell, M., Rysdyk, R., Wise, R.:
Cooperative tracking of moving targets by a team of autonomous uavs. In:
IEEE/AIAA 25th Digital Avionics Systems Conference, pp. 1–9, October 2006
4. Nitinawarat, S., Atia, G., Veeravalli, V.: Eﬃcient target tracking using mobile sensors. In: 4th IEEE International Workshop on Computational Advances in MultiSensor Adaptive Processing (CAMSAP), pp. 405–408, December 2011
5. Xu, E., Ding, Z., Dasgupta, S.: Target tracking and mobile sensor navigation in
wireless sensor networks. IEEE Trans. Mob. Comput. 12(1), 177–186 (2013)
6. Zou, Y., Chakrabarty, K.: Distributed mobility management for target tracking in
mobile sensor networks. IEEE Trans. Mobile Comput. 6(8), 872–887 (2007)
7. Naderan, M., Dehghan, M., Pedram, H.: Mobile object tracking techniques in wireless sensor networks. In: International Conference on Ultra Modern Telecommunications Workshops, ICUMT ’09, pp. 1–8, October 2009
8. Adamey, E., Ozguner, U.: A decentralized approach for multi-UAV multitarget
tracking and surveillance. In: Society of Photo-Optical Instrumentation Engineers
(SPIE) Conference Series, vol. 8389. May 2012
9. Srinivas, A., Zussman, G., Modiano, E.: Construction and maintenance of wireless
mobile backbone networks. IEEE/ACM Trans. Networking 17(1), 239–252 (2009)
10. Radhakrishnan, G., Saripalli, S.: Target tracking with communication constraints:
An aerial perspective. In: IEEE International Workshop on Robotic and Sensors
Environments (ROSE), pp. 1–6, October 2010
11. Fowler, R.J., Paterson, M.S., Tanimoto, S.L.: Optimal packing and covering in the
plane are np-complete. Inf. Process. Lett. 12(3), 133–137 (1981)
12. Even, S.: Graph Algorithms. W. H. Freeman & Co., New York (1979)

Graph Clustering Using Multiway Ratio Cut
(Software Demonstration)
Tom Roxborough and Arunabha Sen*
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287
USA

Abstract. Identifying the natural clusters of nodes in a graph and treating them as supernodes or metanodes for a higher level graph (or an abstract graph) is a technique used for the reduction of visual complexity
of graphs with a large number of nodes. In this paper we report on the
implementation of a clustering algorithm based on the idea of ratio cut,
a well known technique used for circuit partitioning in the VLSI domain.
The algorithm is implemented in WINDOWS95/NT environment. The
performance of the clustering algorithm on some large graphs obtained
from the archives of Bell Laboratories is presented.

1

Introduction

Graphs are frequently used to model problems from various diverse domains
such as telecommunication networks, VLSI circuit design, databases and computational chemistry. In these domains the nodes are used to represent certain
entities of that domain and the edges represent the relationships between them.
The relationship between the entities can be very effectively conveyed visually by
a nice layout of the graph. However, in most of the realistic problem instances,
the number of nodes and edges is far too many for a nice layout and also for
comprehension of the information the graph was supposed to convey. In such
situations, an abstract graph is constructed where each node represents a set
of nodes of the original graph and the edges represent the relationship between
these sets of nodes. Thus an abstract graph construction problem reduces to the
problem of partitioning the node set of the original graph G -- (V, E), into a
k Ei = V and Vi n ~ = 0 for i ¢ j.
subset of nodes V1, V2, ..., V~, such that Ui=l
The subsets Vis (1 < i < k) are known as the clusters of the graph G = (V, E).
One problem with this approach is that there is no consensus among the researchers as to what constitutes a cluster. There is some intuitive understanding
of what constitutes a cluster but there is no universally accepted formal definition of a cluster. In case the nodes and edges of the graph have some semantic
information associated with them (in the form of labels), such information can
* Corresponding author, Telephone: 602-965-6153, Fax: 602-965-2751, e-mail: arunabha.sen~asu.edu. This work was partially supported by a grant from Tom Sawyer
Software under NIST Advanced Technology Program.

292
be used for the purpose of clustering (or grouping) the nodes. An example of such
information could be the IP addresses associated with the nodes in a telecommunication network. In case the graph has no such information, then the structural
properties of the graph have to be utilized for the purpose of generating the
clusters. We will refer to such graphs as the fiat graphs. Several candidates for
the structures have been proposed in the literature. These include biconnected
components [2], paths and triangles [1], circles of cliques [7]. The reader is referred to [5] for discussions of some other possible structures for clustering. In
spite of the differences of opinion as to what constitutes a cluster, one idea is
universally accepted: the nodes belonging to a cluster must have a strong relationship between them in comparison with the nodes outside the cluster. In case
of a fiat graph this translates to finding a partition that minimizes the number
of inter-cluster edges (or maximizes the intra-cluster edges).

2

Ratio Cut Technique

The Ratio Cut technique was proposed in [4] for the purpose of identifying the
natural clusters of a graph. The technique was proposed in the VLSI domain for
the circuit partitioning problem. Both in the case of circuit partitioning as well
as graph clustering, minimization of the cut edges is a very important objective.
In case the node set needs to be partitioned into only two subsets 1/1 and V2, the
minimum cut partition can easily be computed using the max-flow techniques.
However, the technique does not have any control on the size of subsets V1
and 1/2. In the VLSI domain each of the subsets has to fit into an integrated
circuit chip and as such the size of each subset has to conform to some prespecified maximum limit. Therefore, the max-flow technique is not very useful
in the circuit partitioning problem. The Kernighan-Lin technique, a well known
heuristic for circuit partitioning, requires that the size of the two partitions 1/1
and V2 of the node set V be equal. This technique heuristically tries to find a
partition with a small cut value, all the while keeping the size of the two subsets
V1 and V2 equal.

_

m

_

{

.

{o

-

lo

-

;o

!

t"

{o

lo

io

io

I
io

I

1o

Io

|o

I

i

Fig. 1. Partitions produced by Kernighan-Lin Algorithm and Ratio Cut Algorithm
respecively

293

As shown in [4], such a strict size requirement often forces a partition with
a high cut size. The K-L technique produces the partition shown in figure 1,
whereas a partition into natural clusters with a much better cut size is shown
in figure 2. To attain the twin objectives of (i) minimizing the cut value and (ii)
minimizing the difference in the size of the subsets, the authors of [4] proposed
a new metric called the ratio cut to measure the quality of a partition. The ratio
cut is defined as follows:
Consider a graph G = (V, E). Suppose cid is the capacity (or the weight) of
the edge connecting the nodes i and j. Suppose V1, V2 is a partition of the node
set V. (V1 : V~) denotes a cut that separates the nodes of V1 from the nodes of
V2 = V - 111. The capacity of this cut is equal to
=

i~v~ j~v2

The corresponding ratio value is given by

R(V1, V2) C'(V1,V2) l l V, I x IV2 I
=

The ratio cut is defined to be the cut that has the minimum ratio among all
possible cuts of the graph, i.e., a cut (V1, V2) will be known as a ratio cut if
R(VI, V2) =

3

min

XCV;Y:V\X;X,Y¢O

R(X, Y)

Implementation

As seen in the discussion in the previous section, the ratio cut technique proposed
in [4] is applicable for a two-way partition of the graph. The authors dealt
with the multi-way partition problem by repeated application of the two-way
partition. For the graph clustering problem, we adapted the two-way ratio cut
principle to a multi-way partition problem. In case of a k-way partition we
compute the ratio as follows:

=
where

I V, I x IV, I x...x I

I

k

C(V1, V2,..., Vk) = 1/2 ~ ~ ~ c,j
p=1 iev~ j~vp
The ratio cut is defined exactly the same way as before, that is the cut that has
the minimum ratio among all possible cuts of the graph.
In our implementation, we did not put a limit on the maximum number of
nodes in a cluster, as we felt that such a restriction is artificial. However, our
algorithm requires the user to specify the number of clusters in which the node
set should be partitioned. The rational for this requirement is the following: The
reason for clustering the nodes of the graph is to reduce the visual complexity
and as such it should be left to the user to determine what level of complexity
is acceptable for his application.

294

4

E x p e r i m e n t a l Results and Discussions

We tested our implementation of the multiway ratio cut based clustering algorithm with a wide range of graphs - small, medium and large. We created some
example graphs for testing purposes and extensively used the Bell Laboratories graph library. This library has a large collection of graphs of wide range
of variation in terms of number of nodes, edges and node degrees. Some representative examples of the output of our clustering algorithm is attached. The
clustering algorithm runs on both the UNIX and the PC environment. We used
Tom Sawyer Software's Graph Layout Toolkit for the layout of the graphs. The
clustering algorithm computed the clusters in less than a few seconds in almost
all of the Bell Laboratories graphs. Only in a small number of cases it required
a few minutes for clustering.

5

D e m o Environment

The clustering algorithm was implemented on a WINDOWS95/NT environment
using Microsoft Visual C + + version 4.0. All graph layouts were produced by the
Graph Layout Toolkit version 2.3 of Tom Sawyer Software Corporation. A PC
with a 486 or Pentium processor along with 16MB of RAM is the only hardware
requirement for the demo. The demo currently runs under either WINDOWS95
or NT.

6

Acknowledgements

The authors wish to acknowledge Jay Noh for his work on the initial development
of the clustering technique.

References
1. R. Sablowski and A. Frick, "Automatic Graph Clustering," Proceedings o/Graph
Drawing'96, Berkeley, California, September,1996.
2. U. Dogrusoz, B. Madden and P. Madden, "Circular Layout in the Graph Layout
Toolkit," Proceedings of Graph Drawing'96, Berkeley, California, September,1996.
3. P. Eades, "Multilevel Visualization of Clustered Graphs," Proceedings of Graph
Drawing'96, Berkeley, California, September,1996.
4. Y.C. Wei and C. K. Cheng, "Ratio Cut Partitioning for Hierarchical Designs,"
IEEE Transactions on Computer Aided Design, vol. 10, no. 7, pp.911-921, July
1991.
5. C. J. Alpert and A. B. Kahng, "Recent Directions in Netlist Partitioning: A Survey," INTEGRATION: the VLSIjournal, vol. 19, pp.1-81,1995.
6. D. Kimmelman, B. Leban, T. Roth, D. Zernik "Dynamic Graph Abstraction for
Effective Software Visualization," The Australian Computer Journal, vol. 27, no. 4,
pp.129-137, Nov 1995.
7. F. J. Brandenburg, M. Himsolt and K. Skodinis, "Graph Clustering: Circles of
Cliques," submitted to Graph Drawing'9Z

295

Fig. 2. Bell Lab graph 1572 before clustering

Fig. 3. Bell Lab graph 1572 with 4 clusters

Fig. 4. Bell Lab graph 1572 with 5 clusters

296

Fig. 5. Bell Lab graph 1487 before clustering

Fig. 6. Bell Lab graph 1487 with 5 clusters

Fig. 7. Bell Lab graph 1487 with 6 clusters

Brief Announcement: On Regenerator Placement
Problems in Optical Networks
∗

Arunabha Sen, Sujogya Banerjee, Pavel Ghosh and Sudheendra Murthy
Computer Science and Engineering Program
School of Computing, Informatics and Decision Systems Engineering
Arizona State University

{asen, sujogya, pavel.ghosh, sudhi}@asu.edu
Hung Ngo
Department of Computer Science and Engineering
University of Buffalo (SUNY)

hung.ngo@ubuffalo.edu

ABSTRACT

The RPP has been studied by a number of researchers
[4, 5, 2]. However most of the published methods of locating regenerators operate by iteratively improving previously computed routes until they become feasible. These
methods usually generate a path between each pair of nodes
and then place regenerators, as needed, along those paths to
make them feasible. However, this approach usually results
in placing a significantly higher number of regenerators than
are needed to ensure that a path can be established between
every source-destination node pair.
Although most of the studies indicated earlier focused on
the technological aspects of regenerator placement in optical networks, the theoretical computer science community
also has investigated these problems [2]. In a recent paper in SPAA [2], the authors claim that their study is the
first that presents a theoretical framework to study the RPP
and related problems. They present polynomial time algorithms, NP-complete proofs, approximation algorithms and
inapproximability results for four different versions of the
RPP problem.
The contribution of this paper is the following:
• We show that the RPP can be effectively solved using an approximation algorithm for the minimum connected dominated set problem.
• We point out several serious flaws of the algorithm
presented for the solution of RPP in [2].

Optical reach is defined as the distance optical signal can traverse before its quality degrades to a level that necessitates
regeneration. It typically ranges from 500 to 2000 miles,
and as a consequence, regeneration of optical signal becomes
essential in order to establish a lightpath between a sourcedestination node pair whose distance exceeds the limit. In a
translucent optical network, the optical signal is regenerated
at selected nodes of the network before the signal quality
degrades below the acceptable threshold. Given the optical
reach of the signal, to minimize the overall network design
cost, the goal of the regenerator placement problem is to
find the minimum number of regenerators necessary in the
network, so that every pair of nodes is able to establish a
lightpath between them. In this paper, we study the regenerator placement problem and present complexity result for
that.
ACM Categories: Computer Applications
General Terms: Algorithms, Theory
Keywords: optical networks, regenerator placement

1.

INTRODUCTION

In a translucent optical network, the optical signal is regenerated at the regeneration points (typically a subset of
the network nodes with the regeneration capability) to carry
the signal over long distances. Optical reach (the distance
an optical signal can travel before its quality degrades to a
level that necessitates regeneration) usually ranges from 500
to 2000 miles [4]. To transmit an optical signal beyond this
distance, it is essential to re-amplify, reshape and re-time (a
process often called 3R regeneration) it. The Regenerator
Placement Problem (RPP) problem is to find i) the minimum number of regenerators and ii) their locations, so that
a communication path can be established between every pair
of source-destination nodes in the network.

2. REGENERATOR PLACEMENT PROBLEM
In this section, we first discuss the approach taken in [2]
for the solution of the RPP. After pointing out a few limitations of their approach, we present our technique in subsection 2.2.

2.1 Flammini et al. approach to RPP
An optical network is modeled as an unweighted undirected graph G = (V, E) in [2]. The length of a path is
measured in terms of the number of edges that constitute
the path and the notion of optical reach is incorporated by
putting a bound (d) on the number of edges a lightpath
can traverse before requiring regeneration. A connection
between a source (s) and a destination (t) comprises of a
sequence of lightpaths from s to a regenerator node, or from
one regenerator node to another, or from one regenerator

∗
This research is supported in part by the U. S. Army Research Office, the Air Force Office of Scientific Research and
Defense Threat Reduction Agency.

Copyright is held by the author/owner(s).
SPAA’10, June 13–15, 2010, Thira, Santorini, Greece.
ACM 978-1-4503-0079-7/10/06.

178

node to t (of length at most d) whose concatenation form a
path from s to t. A solution to the RPP consists of identification of the smallest subset of nodes U ⊆ V to place the
regenerators, such that paths can be established between
source-destination node pairs.
We make a few comments on the network model and solution technique proposed in [2].

share any edge that is traversed in the same direction are
acceptable, respectively, then X ≥ Z ≥ Y .
H
2000
A

Comment 1: Given that in reality the distance between
nodes are not identical and deterioration of signal strength
is proportional to the distance traversed by an optical signal,
we feel that an edge-weighted graph would have been a more
appropriate model for the RPP, instead of the un-weighted
graph used in [2]. In our model described in section 2.2, we
use an edge-weighted graph model, where the weights on the
edges represent the distance between the nodes.

750

B

1000

C

1500

100

D

3000

E

100
50

G

F

Figure 1:
Example illustrates need for edgedisjointness among directed path segments
Comment 3: In proof of the Theorem 2.8 in [2], the authors describe an algorithm to find a solution to a version of RPP denoted by RPP/∞/req. RPP/∞/req problem
states that given route requests between different sourcedestination pairs and an optical reach d > 0, find locations for the smallest number of regenerators. The algorithm starts by transforming the instance of RPP/∞/req
problem to an instance of the set-cover problem. Unfortunately, the algorithm at times may fail to find a solution
of the RPP/∞/req problem. The solution computed by this
algorithm when executed on the graph G = (V, E) is shown
in Figure 2. For this graph the solution corresponding to the
set-cover instance is the solution for the RPP/∞/req problem with d = 2. But it is not a solution for the RPP/∞/req
problem for the graph G with d = 2 as the path length
between the nodes v2 and v3 is greater than 2. Also the solution of the set-cover instance of RPP/∞/req problem does
not always guarantee that a simple path can be found between every source-destination node pairs, once regenerators
are placed at the locations identified by the algorithm. The
Figure 3 shows another example where the solution returned
by this algorithm for the RPP/∞/req problem with d = 3 is
the solution corresponding to the set-cover instance of graph
G = (V, E). In Figure 3 the algorithm returns node r as the
final solution for placement of a regenerator. However, if a
regenerator is placed at only this node, the path between the
leaf nodes v1 and v2 will be non-simple. In order to have
at least one simple path between every pairs of nodes the
correct solution has to place regenerators at nodes a and b.

Comment 2: The authors in [2] make a distinction between simple path case and non-simple path case. They
correctly note that if only simple paths are acceptable for
lightpath establishment, then the number of regenerators
needed to establish connection between the nodes can be
significantly higher than the number of regenerators needed
when non-simple paths are also acceptable. However, we
would like to point out that the distinction should not be
drawn between simple and non-simple path cases as nonsimple path cases are acceptable under some conditions and
unacceptable under some others. We elaborate our observation with an example shown in Figure 1. In the figure,
the weights assigned on the edges represent the distance
between the corresponding nodes in miles. If the optical
reach is 3250 miles, we can establish a non-simple path
P1 : A − B − C − D − F − G − B − C − H from node
A to H, with signal regeneration at node D. The two
path segments that make up the path from A to H are
(P1,1 : A − B − C − D) and (P1,2 : D − F − G − B − C − H).
However, this may not be an acceptable solution when we
consider free wavelengths available on each of the fiber links.
Let λ1 be the only wavelength available on the fiber links
AB and CH and let wavelengths {λ1 , λ2 , . . .} be available
on the remaining links. In this scenario both the path segments P1,1 and P1,2 must use wavelength λ1 to set up the
lightpath. This will not be an acceptable solution since the
link BC appears in both path segments. Thus in this case,
the non-simple path P1 from A to H will be unacceptable.
However, the non-simple path P2 from A to H given by
A − B − G − F − D − F − G − B − C − H will be perfectly acceptable, as this path will be composed of two path segments
P2,1 : A − B − G − F − D and P2,2 : D − F − G − B − C − H.
The difference between the two cases is that in the first case
the two path segments P1,1 and P1,2 share an edge, BC, that
is traversed in the same direction and in the second case the
two path segments P2,1 and P2,2 do not share any edge that
is traversed in the same direction. Since in an optical network, traffic in opposite directions are normally carried by
two different optical fibers, the non-simple path P1 is unacceptable, whereas the non-simple path P2 is acceptable.
The authors in [2] do not make a distinction between these
two types of non-simple paths. If X, Y, Z represent the number of regenerators that will be needed for a RPP problem
instance for the cases where (i) only simple paths are acceptable, (ii) all simple and non-simple paths are acceptable
and (ii) all simple and only non-simple paths that do not

2.2 Our approach to solution of the RPP
r
a

v1

v2

v3

v4

v1

Figure 2: Solution returned by the algorithm
for RPP/∞/req problem
instance of graph G,
where d = 2

179

b

v2

v3

v4

Figure 3: Solution returned by the algorithm
for RPP/∞/req problem
instance of graph G,
where d = 3. There is no
simple path between v1
and v2 in the solution

In our model G = (V, E) is a connected edge-weighted
graph with edge-weights representing distances between the
corresponding nodes.
Path Segment w.r.t. node set V ′ : Given a source node s, a
destination node t and a subset V ′ ⊆ V , a subpath P S of a
path from s to t (henceforth called a s−t path) is referred to
as a path segment, if the end-points of P S are in V ′ ∪ {s, t}
and no intermediate node is in V ′ .
Optical reach constraint: Given a path in a network between
a source and a destination, the optical reach constraint ensures that the distance of any path segment between two
regenerator nodes, or the distance from the source to a regenerator or the distance from a regenerator to the destination on the s − t path does not exceed fiber’s optical reach.
Regenerator Placement Problem (RPP): Given G = (V, E),
the problem is to find the smallest V ′ ⊆ V such that there
exists a path between every pair of nodes {s, t} ∈ V where
(i) no path segment of the s − t path has a length greater
than R and (ii) no two path segments of the s − t path share
an edge that is traversed in the same direction.
Reachability Graph: Given a network graph G = (V, E) with
edge weights representing the distances between the nodes,
and an optical reach distance R, the reachability graph G′ =
(V ′ , E ′ ) corresponding to G is constructed as follows: V ′ =
V and two nodes vi and vj in V ′ will have an edge between
them if the shortest path length between those two nodes in
V is at most R.

used in the segment si ; si+1 and then again in segment
sj ; sj+1 . Without loss of generality, assume i < j. Now,
on the sj ; sj+1 segment, the length of the sj ; u part has
to be at least the length of the u ; si+1 part. Otherwise,
from si we could have gone to u, and then take the u ; sj
path to sj ; this “shortcut” would contradict that fact that
the s′ , t′ -path we chose was a shortest path.
Thus, the v ; si+1 part is strictly shorter than the sj ; u
path. Consequently, from si+1 we can go to v and then
take the v ; sj+1 branch; this would be shorter than the
current sj ; sj+1 path, again contradicting the shortest
path choice. This proves that a feasible solution for the
MCDS instance is a feasible solution for the RPP instance.
Lemma 2: If there’s an α-approximation algorithm for
the RPP, then there’s an α-approximation algorithm for the
MCDS problem.
Proof: Consider an instance G of the MCDS problem. Construct an instance G′ of the RPP by setting G′ = G, and
R = 1. Set the weight of each edge to be 1. It’s easy to see
that if a solution is feasible for RPP on G′ then it is feasible
for MCDS problem on G.
However, both lemma 1 and 2 holds except the trivial case
when reachability graph G′ is clique. If G′ is a clique then
solution of RPP will return 0 node while MCDS of G′ will
give exactly 1 node.
From recent [3], we know that minimum connected dominating set (MCDS) can be approximated to within about
ln n + O(1). We also know that MCDS cannot be approximated (unless P = N P ) to within ln n − Θ(ln ln n) [1].

Theorem 1: The Minimum Connected Dominating Set
(MCDS) of the reachability graph G′ = (V ′ , E ′ ) of the network graph G = (V, E) represents the solution of Regenerator Placement Problem (RPP), in non-trivial cases in which
G′ is not a clique.

Corollary 3: There’s a O(ln n)-approximation algorithm
for the RPP.
Comment 4: In [2] the authors present an algorithm for the
RPP/∞/req with approximation ratio of 23 log m + 1, where
the demand matrix is all-to-all. If the demand matrix is
all-to-all m is O(n2 ), where n is the number of nodes in
the network. In this case the approximation ratio becomes
3log n + 1. In our MCDS based approach for the solution of
RPP, we can provide better performance, as MCDS can be
computed with approximation ratio ln δ + 2 where δ is the
maximum degree in the input graph [3].

Proof: The proof follows from the following lemmas.
Lemma 1: If there’s an α-approximation algorithm for the
MCDS problem, then there’s an α-approximation algorithm
for the RPP.
Proof: Given an instance [G = (V, E), w : E → R+ , R] of
the RPP problem, construct an instance G′ = (V ′ , E ′ ) of
the MCDS problem in the following way: Set V ′ = V , and
let (u, v) ∈ E ′ iff dG (u, v) ≤ R. Here dG (u, v) denotes the
shortest distance between the nodes u and v in G in terms of
the distance function w. For convenience, define a distance
function w′ : E ′ → R+ by assigning w′ (uv) = dG (u, v).
Let ∅ =
6 S ⊆ V = V ′ be a MCDS of the graph G′ . Consider any two vertices s and t in V . Without loss of generality, assume st ∈
/ E ′ . (Otherwise, there’s a path from s to t of
length ≤ R and we’re done.) We need to show that there’s
a walk from s to t in G such that no two path segments in
the walk share an edge that is traversed in the same direction twice. First, let s′ be the vertex in S for which w′ (ss′ )
is smallest, and t′ be the vertex in S for which w′ (tt′ ) is
smallest. If s ∈ S then s = s′ . If t ∈ S, then t = t′ .
Let s′ = s1 , s2 , . . . , sm = t′ be the shortest path in G′ [S]
between s′ and t′ . Since S is a connected dominating set,
such a path exists. Now, for each edge si si+1 ∈ E ′ on
this path, there’s a corresponding (si , si+1 )-path in G with
length ≤ R. Hence, putting everything together, we can find
a walk from s to t in G consisting of segments s = s0 ; s1 ,
s1 ; s2 , ..., sm−1 ; sm , sm ; t = sm+1 . Each segment
has length ≤ R.
Now, suppose there’s some edge uv ∈ E which is traversed
twice in the same direction (from u to v). Suppose uv was

3. REFERENCES
[1] M. Chlebı́k and J. Chlebı́ková. Approximation hardness
of dominating set problems in bounded degree graphs.
Information and Computation, 206(11):1264–1275,
2008.
[2] M. Flammini, M. Spaccamela, et al. On the complexity
of the regenerator placement problem in optical
networks. In Proceedings of the 21st SPAA, pages
154–162. ACM, 2009.
[3] L. Ruan, H. Du, X. Jia, W. Wu, Y. Li, and K. Ko. A
greedy approximation for minimum connected
dominating sets. Theoretical Computer Science,
329(1-3):325–330, 2004.
[4] J. Simmons, M. Archit, and N. Holmdel. Network
design in realistic¿ all-optical¿ backbone networks.
IEEE Communications Magazine, 44(11):88–94, 2006.
[5] X. Yang and B. Ramamurthy. Sparse regeneration in
translucent wavelength-routed optical networks:
architecture, network design and wavelength routing.
Photonic network communications, 10(1):39–53, 2005.

180

Interference-Aware Multicasting in Wireless
Mesh Networks
Sudheendra Murthy1 , Abhishek Goswami2 , and Arunabha Sen1
1

School of Computing and Informatics, Arizona State University, Tempe, AZ 85287
sudhi@asu.edu,asen@asu.edu
2
Mobile Devices Software, Motorola
abhishek.goswami@motorola.com

Abstract. Multicasting is one of the most important applications in
Wireless Ad hoc Networks and the currently emerging Wireless Mesh
Networks. In such networks, interference due to the shared wireless
medium is a prime factor in determining the data rate achievable by a
multicast application. In this research work, we present an interferenceaware multicast routing algorithm that takes into account the eﬀects of
interference to determine the maximum bandwidth multicast structure.
We characterize the problem of computing maximum bandwidth multicast structure as a graph problem of ﬁnding minimum degree weakly
induced subgraph in a graph subject to the connectivity and interference
constraints. We establish the intractability of the problem and provide
eﬃcient heuristic that performs close to the optimal in most of the cases.
We also present the design of a more practical distributed algorithm. The
simulation results demonstrate the beneﬁts of our heuristic over Shortest
Path Tree and Minimum Steiner Tree approximation algorithms.
Keywords: Wireless Mesh Network, Minimum Interference Multicast,
Weakly Induced Connected Subgraph, NP-Complete.

1

Introduction

Research in Wireless Mesh Networks (WMNs) has gained tremendous momentum recently as a result of its commercial deployment in many US cities including
Seattle, Philadelphia, Tempe [1]. WMNs are increasingly being used to provide
cost eﬀective and reliable Internet connectivity to residents and businesses in
these cities. These WMNs consist of a set of wireless routers (access points) to
which the end users connect, a set of wireless routers that act as forwarding
nodes and a set of gateway routers that provide connectivity to the Internet.
Data from the end users is routed in the WMN towards the gateway routers and
to the Internet.


This research was supported in part by ARO grant W911NF-06-1-0354. The information reported here does not reﬂect the position or the policy of the federal
government.

I.F. Akyildiz et al. (Eds.): NETWORKING 2007, LNCS 4479, pp. 299–310, 2007.
c IFIP International Federation for Information Processing 2007


300

S. Murthy, A. Goswami, and A. Sen

The widespread deployment of WMNs has fueled research work in providing better support to multimedia applications like real-time video transport and
Voice over the Internet (VoIP) services. Common to many of these applications
is the need for a multicast framework that facilitates eﬃcient distribution of
datagrams to a cohort of hosts. Multicasting results in bandwidth savings as
compared to multiple unicast sessions. Early eﬀorts in providing multicast support in WMNs ignored to consider the interference eﬀects of the shared wireless
medium in which the mesh routers operate. Interference is an important factor that dictates the bandwidth available for the multicast transmission. The
mesh routers are usually equipped with IEEE 802.11 a/b/g interfaces. Interference in the context of this paper refers to the bandwidth sharing eﬀect between
nodes operating in close range caused due to the CSMA/CA nature of 802.11. In
this paper, we present a novel multicast framework that considers the eﬀects of
interference and constructs a multicast structure that provides maximum bandwidth to the applications. Our objective is to identify multicast forwarding group
nodes whose transmission induces least amount of interference. To the best of
our knowledge, this is the ﬁrst paper that proposes such a framework. The main
contributions of this work are as follows.
1. We provide a novel formulation of the interference-aware multicasting problem as a graph problem of ﬁnding weakly induced subgraph in a graph
representing the mesh routers.
2. We prove the NP-completeness of the problem, thereby establishing its intractable nature.
3. We provide an Integer Linear Program technique of optimally solving this
problem.
4. We provide a centralized heuristic algorithm that performs close to the optimal and describe a distributed implementation of the algorithm. We compare
the performance of the proposed heuristic with the optimal, shortest path
solution and minimum Steiner tree solution.
The road map for rest of the paper is as follows. In section 2, we introduce
the required notation and give the formal problem deﬁnition. The complexity
analysis of the problem is provided in section 3. The Integer Linear Program formulation to obtain the optimal solution is provided in section 4. The centralized
heuristic and the design of its distributed extension are provided in section 5.
Section 6 presents the evaluation of our heuristics. Section 7 reviews the related
work in this area, while section 8 concludes the paper.

2
2.1

Problem Formulation
Wireless Transmission and Interference Model

We assume uniform transmission range RT and interference range RI for all the
routers in the WMN. We represent a WMN by a directed potential communication graph G(V, E) in which the vertices represent the mesh routers. Denote

Interference-Aware Multicasting in Wireless Mesh Networks

7

RT
2

7

2
1

RI
2

7

2
3
4

6

301

2

1

6

1

6

3
5

4

5

3
4

Fig. 1. A Potential Communication Graph G(V, E)
with transmission radius
RT

Fig. 2. Potential Interference Graph H(V, F )
with interference radius
RI =1.5RT

5

Fig. 3. SH , the subgraph
weakly induced by the vertex set S = {3, 6, 7} on the
graph of ﬁgure 2

the Euclidean distance between routers u and v by dist(u, v). Two directed
edges {(u, v), (v, u)} ∈ E if dist(u, v) ≤ RT and implies that mesh router u
can communicate directly with mesh router v. We assume that each router is
equipped with one radio and all routers operate on the same channel. Any of
the mesh routers can be the source node in the multicast tree. However in many
applications, the multicast source comes from the wired network and hence, the
multicast source is a gateway mesh router. An access point router becomes a
receiver of the multicast structure if there is at least one end-user connected to
the access point who wants to receive this multicast stream. We assume a 802.11
CSMA/CA medium access control scheme1 . Thus, a transmission between two
nodes may prevent all nodes within the transmission range of the sender from
transmitting due to carrier sensing.
We assume that the interference range RI is q × RT with q ≥ 1. We model the
co-channel interference of the WMN with the help of a directed potential interference graph. A directed potential interference graph is represented by H(V, F )
in which the vertex set corresponds to the mesh routers. Two directed edges
{(u, v), (v, u)} ∈ F if dist(u, v) ≤ RI . A directed edge (u, v) in the potential interference graph implies that the transmission of router u can cause interference
at router v. Figure 1 shows a sample potential communication graph with 7 mesh
routers. The circles around the routers are drawn with radius RT /2 and hence
two intersecting circles implies that the two routers are within the transmission
range of each other. Figure 2 shows the corresponding potential interference
graph with RI =1.5RT . Since RT ≤ RI , the potential communication graph is
always a subgraph of the potential interference graph.
This interference model is diﬀerent from the frequently used conﬂict graph
representation of interference in a WMN. In the conﬂict graph representation,
a vertex vij is in the vertex set of the conﬂict graph if (i, j) is an edge in the
potential communication graph. There exists an edge between two vertices vij
and vkl in the conﬂict graph if min (dist(i, k), dist(i, l), dist(j, k), dist(j, l))
1

Although we consider the standard 802.11 protocol without RTS/CTS, our techniques can be applied to RTS/CTS 802.11 environments with some minor changes.

302

S. Murthy, A. Goswami, and A. Sen

≤ RI . In the context of the problem explored in this paper, our interference
model oﬀers some beneﬁts over the conﬂict graph model. Firstly, our interference
model is more intuitive in the sense that it captures the idea of co-channel
interference occurring at the receiving nodes in a WMN. Secondly, it can easily
be seen that the conﬂict graph and the potential interference graph can be
derived from each other when the potential communication graph is known.
Finally, the potential interference graph modeling makes our problem deﬁnition
more elegant.
2.2

Graph Deﬁnitions and Notations

All graphs deﬁned in this paper are directed graphs. We drop the preﬁx directed
and henceforth, a graph implies a directed graph. In this section, we introduce
the required graph terminology.
– In-degree of a vertex v in a graph G is the number of arcs coming into v and
the out-degree of v is the number of arcs going out of v. The in-degree and
−
+
out-degree of a vertex are represented by δG
(v) and δG
(v) respectively.
– The maximum in-degree of a graph G represented by Δ− (G) is the maxi−
mum of the in-degrees of its vertices. That is, Δ− (G) = maxv∈V (G) δG
(v).
+
Similarly, the maximum out-degree of a graph G represented by Δ (G) is
the maximum of the out-degrees of its vertices. That is, Δ+ (G) = maxv∈V (G)
+
δG
(v).
– A vertex u is a neighbor of vertex v if there is a directed edge from v to u.
The (closed) neighborhood of vertex v in graph G(V, E) denoted by N [v] is
the set that includes v and the neighbors of v. The (closed) neighborhood of
a subset S ⊆ V of graph G(V, E) denoted by 
N [S] is the set that includes S
and the neighbors of S. That is, N [S] = S ∪ v∈S N [v].
– The subgraph weakly induced by vertex set S ⊆ V in graph G(V, E) is deﬁned
as the graph with the vertex set N [S] and edge set E ∩ (S × N [S]). In other
words, the edge set of the subgraph weakly induced by S consists of all the
edges induced by the vertices of S along with the directed edges from set
S to its neighbors in the graph. The weakly induced subgraph of graph G
on the vertex set S is denoted by S	G . An example of the subgraph weakly
induced by the set of vertices S = {3, 6, 7} on the potential interference
graph of ﬁgure 2 is shown in ﬁgure 3.
Throughout this paper, we use the notation G(V, E) to represent the potential
communication graph and H(V, F ) to represent the potential interference graph.
2.3

Problem Deﬁnition

The maximum data rate that can be achieved in a multicast structure is limited
by the data rate of the bottleneck link. The data rate of the bottleneck link is
determined by the amount of interference experienced by the receiver of the link.
For instance, a mesh node present in the communication range of 5 transmissions

Interference-Aware Multicasting in Wireless Mesh Networks

303

experiences more interference and thus provides less throughput compared to a
mesh node present in the communication range of 4 transmissions. In an eﬀort
towards maximizing the data rate of the multicast structure, we try to select
the forwarding group nodes that induce least amount of interference. That is,
given the locations of wireless mesh routers, a multicast source node and a set
of multicast receivers, our goal is to ﬁnd the group of forwarding nodes that
induce the least interference on the forwarding nodes and the receivers. In terms
of the potential communication graph and the potential interference graph, the
problem is formally stated as follows.
Given a potential communication graph G(V, E), its corresponding interference graph H(V, F ), a multicast source vertex s ∈ V and a set of receiver vertices
R ⊆ V \ s, ﬁnd vertex set S ⊂ V such that
1. the subgraph of G weakly induced by S, i.e., S	G has directed paths connecting s to each ri ∈ R and
2. the maximum in-degree of the vertices of the subgraph of H weakly induced
by S, i.e., Δ− (S	H ) is minimized.
We term this problem as the Minimum-Degree Weakly Induced Connected Subgraph (MDWICS) problem. The optimal subset S of the MDWICS problem contains the source vertex and the forwarding group vertices that result in the least
interference. In the next section, we prove the hardness of the MDWICS problem.

3

Computational Complexity

To prove the NP-completeness of the MDWICS problem, we provide a polynomial time transformation from Exact Cover by 3 Sets (X3C) problem [2].
Consider the scenario in which the interference and transmission ranges of the
transmitters are equal. In this case, the potential communication graph and the
potential interference graph have the same set of vertices, edges and thus, can be
represented by a single graph, say G(V, E). The decision version of the MDWICS
problem is then stated as follows.
INSTANCE: Directed graph G = (V, E), vertex s ∈ V designated as source,
vertices R ⊆ V \ s designated as the set of receivers and a positive integer K.
QUESTION: Is there a subset S ⊂ V such that the subgraph weakly induced
by S in G denoted by S	G has paths from s to each ri ∈ R and Δ− (S	G ) is at
most K.
Theorem 1. MDWICS problem is NP-complete.
Proof. Clearly MDWICS problem is in NP since a nondeterministic algorithm
need only guess the vertex set S and check in polynomial time whether the
subgraph weakly induced by S in G has paths from s to each ri ∈ R and the
maximum in-degree of the subgraph is at most K.
Suppose a ﬁnite set X = {x1 , x2 , . . . , x3q } and a collection C = {C1 , C2 ,
. . . , Cm } of 3-element subsets of X make up the instance of X3C. From this

304

S. Murthy, A. Goswami, and A. Sen

instance, we will construct an instance of the MDWICS problem using local
replacement technique. Corresponding to every element xi ∈ X, 1 ≤ i ≤ 3q and
subset Cj ∈ C, 1 ≤ j ≤ m, introduce vertices xi and ci respectively in graph
G(V, E). These vertices together with an additional vertex s make up the vertex
set of G. The edge set of G consists of two types of edges. The ﬁrst set includes
directed edges (cl , xi ), (cl , xj ) and (cl , xk ) for every subset Cl = {xi , xj , xk }.
The second set includes directed edges from s to each ci . The graph constructed
using this mechanism has m+3q+1 vertices and 4m edges. Designate vertex s
as source and vertices R = ∪3q
i=1 xi as the set of receivers. This completes the
construction procedure of the proof. Suppose that K = 1. We claim that there
exists a S ⊂ V such that S	G has paths from s to each xi , 1 ≤ i ≤ 3q and
Δ− (S	G ) ≤ 1 if and only if the corresponding X3C instance contains an exact
cover for X.
s
c1

x1

c2

x2

c3

x3

c5

c4

x4

x5

x6

Fig. 4. Local replacement for subsets C1 ={x1 , x2 , x3 }, C2 ={x2 , x4 , x6 }, C3 ={x3 , x5 ,
x6 }, C4 ={x1 , x3 , x5 }, C5 ={x4 , x5 , x6 }

It is easy to verify that if X3C has an exact cover, then the vertices corresponding to the subsets in the X3C solution along with vertex s is a solution
to the MDWICS problem. Conversely, suppose S ∈ V such that S	G has paths
from source s to each of the receivers x,i s and maximum in-degree of S	G is at
most 1. Consider the set S  = S ∩ {c1 , . . . , cm }. Note that each vertex ci ∈ V ,
1 ≤ i ≤ m has exactly 3 outgoing edges. |S  | = q, since otherwise there would
be either no path (|S  | < q) or multiple paths (|S  | > q) to some xi thereby
violating maximum in-degree of 1. It follows that the subsets corresponding to
the vertices in S  form an exact cover to the X3C problem.

4

Optimal Solution

In this section, we provide an Integer Linear Program (ILP) formulation [3] to
solve the MDWICS problem optimally. Given a potential communication graph
G(V, E), potential interference graph H(V, F ), source s ∈ V and receivers R ⊆
V \s, the problem is to ﬁnd subset S ∈ V that minimizes the maximum in-degree
of S	H subject to the constraint that S	G has paths from s to each ri ∈ R.
The ILP ﬁnds the weakly induced subgraph that achieves the above mentioned
goal. From this subgraph, we can extract the desired node set S by removing
leaf nodes, that is, nodes with zero out-degree.

Interference-Aware Multicasting in Wireless Mesh Networks

305

The indicator variables are deﬁned as follows. xi,j = 1, if edge (i, j) ∈ E is
p
in the optimal solution, 0 otherwise. Deﬁne fi,j
=1, if there is a ﬂow from s to
p
receiver p through link (i, j) ∈ E in the optimal solution, 0 otherwise. fi,j
is
used to ensure connectivity from s to receiver r. Deﬁne yi,j =1 for edge (i, j) ∈ F
if node i is the transmitter in the optimal solution, 0 otherwise. yi,j is used to
capture the interference caused by i’s transmission on node j. The objective
is to minimize D, the maximum in-degree of all the nodes in the interference
subgraph. The following set of constraints deﬁne the problem accurately.
– The degree constraint speciﬁes that the maximum in-degree of the nodes in
the
optimal solution interference subgraph should be no larger than D. That
is, (i,v)∈F yi,v ≤ D, ∀v ∈ V .
– The broadcast constraint captures the broadcast characteristics of the wireless medium. When a node transmits, its transmission aﬀects all nodes in its
transmission range. This is represented by x(v,i) = x(v,j) , ∀v ∈ V, (v, i) and
(v, j) ∈ E, i = j.
– The interference constraint models the interference characteristic of the network. For every node in the potential communication graph, if there is a
directed communication edge going out, then there is a directed interfering
edge going out from the corresponding node in the potential interference
graph. This is represented by yv,i ≥ xv,j , ∀(v, i) ∈ F, ∀(v, j) ∈ E.
Connectivity from the source node to each receiver is ensured by the
following ﬂow conservation constraints. These constraints are similar to the
multi-commodity ﬂow constraints [3].
– The total incoming ﬂow into an intermediate
node is 
equal to the total

p
p
= (v,j)∈E fv,j
, ∀p ∈
outgoing ﬂow from that node. That is, (i,v)∈E fi,v
[1, k], ∀v ∈ {V \ {s ∪ R}}.
– For each receiver and for each ﬂow, the incoming ﬂow
to 
outgoing
 is equal
p
ﬂow except for the ﬂow destined for the receiver. (i,r)∈E fi,r
= (r,j)∈E
p
fr,j
, ∀p ∈ {R \ r}, ∀r ∈ {R}.
– There is zero incoming ﬂow and unit outgoing ﬂow to the source node. That

|R| 
p
p
= 0 and (s,i)∈E fs,i
= 1, ∀p ∈ R
is, p=1 (i,s)∈E f(i,s)
– For each receiver, the outgoing
ﬂow
from
that
receiver should be zero, if the

r
ﬂow is for the same receiver. (r,j)∈E fr,j
= 0, ∀r ∈ R
p
– The dependence between fi,j
and xi,j for edge (i, j) ∈ E is represented by
|R| p
|R| × xi,j ≥ p=1 fi,j , ∀(i, j) ∈ E

5

Proposed Algorithms

In this section, we ﬁrst present a centralized heuristic for the MDWICS problem
and then describe the design of a distributed version.
5.1

Centralized Heuristic

The greedy heuristic takes as input the potential communication graph G(V, E),
the potential interference graph H(V, F ), multicast source vertex s and multicast

306

S. Murthy, A. Goswami, and A. Sen

receiver set R. The output of the heuristic is the set of multicast forwarding group
nodes. In the algorithm, S	G and S	H are the weakly induced subgraphs of G
and H respectively on vertices S as deﬁned before. The algorithm maintains a
feasible solution set S of vertices that contain paths from the source vertex to
all the receivers and a set W of vertices that contains the set of visited vertices.
Initially, all vertices in the graph are included in set S and set W contains the
source vertex (Step 1). The algorithm then selects in each iteration a vertex
in S \ s that has the maximum in-degree in the subgraph weakly induced by
S in graph H. If there are multiple such vertices, any one of them is selected
arbitrarily. This vertex will be a potential candidate for removal since it has the
highest in-degree in S	H . Step 5 checks if removal of this vertex from the graph
G disconnects any of the receivers from s. If not, this vertex and all its incident
edges are removed from both G and H. This vertex is added to the set of vertices
W visited so far. The algorithm terminates when all vertices in the graph have
been visited.
Algorithm 1. MDWICS Heuristic
Input: potential communication graph G(V, E), potential interference graph H(V, F ),
source s ∈ V , receiver set R ⊆ V \ s
Output: set S of multicast forwarding group vertices
1: S ← V , W ← {s}
2: while V \ W = ∅ do
−
(v)|v ∈ S \ s}
3:
v ← arg max{δS
H

4:
S ←S\v
5:
if ∃ directed paths from s to each ri ∈ R in the graph S  G then
6:
S ← S
7:
Remove vertex v and all its incident edges from both graph G and H
8:
end if
9:
W ← W ∪ {v}
10: end while
11: return S \ s

5.2

Distributed Protocol

The design of the distributed interference-aware multicast protocol is based on
the Optimized Link State Routing (OLSR) protocol [4]. OLSR could be used in
mesh networks to maintain the state and quality of the links. The distributed
protocol takes full beneﬁt of the topology knowledge obtained by the OLSR
protocol with its Topology Control (TC) messages. The TC messages are eﬃciently dispersed in the network through multipoint relays (MPRs). Setting the
parameter TC REDUNDANCY=2 at each node in the OLSR protocol ensures
that each node gets information about every other node, link in the network
(Section 15.1 [4]). Each node in the network is uniquely identiﬁed by its IP address. The source initiating the multicast session generates a unique ID for the
session based on its IP address and a sequence number for the multicast group.

Interference-Aware Multicasting in Wireless Mesh Networks

307

The source with its topology information independently computes the set of
multicast forwarding group nodes. It then disseminates through the MPRs, a
MC FG message consisting of the IP addresses of the forwarding group nodes
and a MC JOIN message to the multicast group members. A node upon receiving a MC FG message checks if its IP address is listed in the message. If so, it
records the session ID in a forwarding group (FG) table. Only the ﬁrst MC FG
message received by a node is broadcasted with subsequent MC FG messages being discarded. The MC JOIN message is to inform the multicast group members
about the initiation of the session. The session ID is included in every packet
forwarded by the multicast source. If a node receiving this packet has the session
ID listed in its FG table, it forwards the packet. Evaluation of this distributed
version is the focus of current ongoing work.

6

Simulation Environment, Results and Discussion

We conducted extensive experiments to evaluate our centralized heuristic. We
compared the performance of our heuristic with the Shortest Path Tree (SPT)
algorithm, Minimum Steiner Tree (MST) approximation algorithm [5] and the
optimal solution obtained by solving the ILP formulation given in section 4.
The Shortest Path Tree algorithm ﬁnds the set of edges connecting s to each
receiver such that the length of the shortest path (measured as the number of
hops) from s to each receiver is minimized. The MST algorithm presented in [5]
is a O(log 2 k)-approximation algorithm (k is the number of receivers) for the
Minimum Steiner Tree problem. We used Cplex 10 to solve the ILPs for the
optimal solution.
In all our experiments, the number of mesh routers in the network was ﬁxed
to be 70. The area for the deployment of the mesh nodes was a square area
whose sides were computed based on the required node-density. For instance,
for a required node-density of 100 nodes/km2 , the locations of the 70 nodes
were randomly generated in a square of area of 0.7 km2 . The locations of the
70 nodes were generated randomly. The transmission radius of the mesh routers
was ﬁxed at 100m. The multicast source and the multicast receivers were selected randomly. The simulation scenarios were designed to measure the impact
of three parameters namely, the number of receivers, the density of the network (nodes per unit-area) and the ratio of interference radius to transmission
radius (RI /RT ) on the interference-degree of the multicasting structure produced by the four approaches. The interference-degree is the maximum number
of forwarding group mesh nodes that aﬀect any node in the multicast structure.
As discussed before, this determines the maximum achievable data rate for the
multicast structure.
In the ﬁrst set of experiments (ﬁgure 5), the number of receivers was ﬁxed
at 28 and the node-density at 100 nodes/km2 . RI was increased from 100m to
250m in steps of 50m resulting in RI /RT ratio to vary between 1 and 2.5. It is
to be noted from ﬁgure 5 that as the RI /RT ratio, in turn the interference range
of the mesh routers increase, the interference-degree increases rapidly. This is

308

S. Murthy, A. Goswami, and A. Sen

intuitive since an increase in the interference range introduces more interference
on the mesh nodes. For all values of RI /RT ratio, our heuristic performs better
than the SPT and MST algorithms.

Fig. 5. Interference-degree vs RI /RT
ratio with # Receivers=28 and nodedensity=100 nodes/km2

Fig. 6. Interference-degree vs # receivers with RI /RT =2 and nodedensity=100 nodes/km2

In the second set of experiments (ﬁgures 6, 7), we ﬁxed the RI /RT ratio at 2.
For node densities 100 nodes/km2 and 200 nodes/km2 , we plotted the variation
of the interference-degree with the number of receivers. It can be observed that
as the number of receivers increases, the interference-degree for each algorithm
increases. However, the interference-degree of the heuristic stays closer to the
optimal compared with the SPT and MST algorithms.

Fig. 7. Interference-degree vs # re- Fig. 8. Interference-degree vs nodeceivers with RI /RT =2 and node- density with RI /RT =1 and #
density=200 nodes/km2
receivers=28

In the next experiments (ﬁgure 8), we studied the eﬀect of varying density on
the interference-degree. The number of receivers was set at 28 and the RI /RT

Interference-Aware Multicasting in Wireless Mesh Networks

309

ratio at 1.5. The results may seem to be counter-intuitive at ﬁrst glance, since
increased density must lead to increased interference-degree and the ﬁrst two
points in the graph do not conﬁrm this observation. However for each value of
node-density, the area in which the nodes are deployed changes and thus, the
locations of the nodes are recomputed for each node-density resulting in diﬀerent
communication and interference topologies.
In all the experiments, the performance of our heuristic matches closely with
that of the optimal. The performance SPT and MST algorithms in most of the
cases are far from optimal. This is natural since these algorithms prefer shorter
paths from source to the receivers and do not pay attention to the interference
caused by these paths on the network nodes.

7

Related Work

Most of the related work in this area focusses on eﬃcient network layer multicast
and broadcast in multihop wireless networks and MANETs. Multicast schemes
can be classiﬁed as either source-based or mesh-based. Source-based protocols
construct shortest paths from source to the receivers. AMRIS, MAODV and
MOLSR [6] are the well known source-based protocols. The mesh-based protocols consider multiple paths from source to the receivers. Examples of meshbased protocols are ODMRP, CAMP and FGMP [7]. In the presence of mobility,
mesh-based protocols are advantageous as they maintain alternate paths for each
receiver. Another scheme [8] based on on-demand routing and genetic algorithm
executes faster than the conventional multicast schemes. Moreover, 802.11 QoS
issues studied in [9] reveal that the design framework of multicast protocols
should share ﬂow characteristics across multiple layers and cooperate to meet
the application’s requirement. Study done in [10] states that in multicasting there
is no one size ﬁts all protocol that can optimally serve the need of all types of
application. Recently, a joint optimization approach in [11] emphasizes network
coding technique for multicast routing and game theory approach for interference
management. The closest work to ours is [12] in which the problem of computing multicast trees with minimal bandwidth consumption in mesh networks is
considered. The authors show that this NP-complete problem is equivalent to
minimizing the number of multicast transmissions rather than the edge cost or
the total number of edges of the multicast tree. On the contrary, a tree with
minimum number of transmissions may not provide minimum interference or
maximum bandwidth.

8

Conclusion and Future work

In this paper, we categorized the interference-aware multicasting problem as a
graph problem of ﬁnding a weakly induced subgraph of minimum degree. For this
purpose, we introduced a new model of interference called the potential interference graph. We analyzed the intractable nature of this problem and presented
an eﬃcient greedy-based heuristic algorithm. We also presented a distributed

310

S. Murthy, A. Goswami, and A. Sen

extension of our heuristic. The simulation results provide substantial evidence
of the superior performance of our heuristic compared to the Shortest Path Tree
and Minimum Steiner Tree approximation algorithms. Future work in this area
lies in analyzing the performance of the distributed algorithm and providing
approximation bounds to the centralized algorithm.

References
1. Online: City-wide wi-ﬁ projects. (http://www.seattlewireless.net, http://
www.wirelessphiladelphia.org, http://www.waztempe.com)
2. Garey, M., Johnson, D.: Computers and Intractability: A Guide to the Theory of
NP-Completeness. Freeman Press (1979)
3. Bazaraa, M., Jarvis, J., Sherali, H.: Linear Programming and Networks Flows.
John Wiley & Sons (2004)
4. Online: Rfc 3626 - optimized link state routing protocol (olsr). (http://www.
ietf.org/rfc/rfc3626.txt)
5. Charikar, M., Chekuri, C., yat Cheung, T., Dai, Z., Goel, A., Guha, S.: Approximation algorithms for directed steiner problems. In: ACM-SIAM Symposium on
Discrete Algorithms (SODA). (1998)
6. Kunz, T., Cheng, E.: Multicasting in ad-hoc networks: Comparing maodv and
odmrp. In: International Conference on Distributed Computing Systems (ICDCS).
(2002)
7. Madruga, E., Garcia-Luna-Aceves, J.: Multicasting along meshes in ad-hoc networks. In: International Conference on Communications. (1999)
8. Banerjee, N., Das, S.: Modern: Multicast on-demand qos-based routing in wireless
networks. In: Vehicular Technology Conference. (2001)
9. Zhu, H., Li, M., Chlamtac, I., Prabhakaran, B.: A survey of quality of service in
ieee 802.11 networks. IEEE Wireless Communications 11 (2004)
10. Gossain, H., Cordeiro, C., Agrawal, D.: Multicast: wired to wireless. IEEE Communications Magazine 40 (2002) 116–123
11. Yuan, J., Li, Z., Yu, W., Li, B.: Cross-layer optimization framework for multihop
multicast in wireless mesh networks. IEEE Journal on Selected Areas in Communication 24 (2006)
12. Ruiz, P., Gomez-Skarmeta, A.: Heuristic algorithms for minimum bandwith consumption multicast routing in wireless mesh networks. In: ADHOC-NOW. (2005)

A Performance-Driven 1 / 0 Pin Routing Algorithm
Dongsheng Wang
Ping Zhang
Chung-Kuan Cheng
Arunabha Sen
Department of Computer Sciences and Engineering
University of California at San Diego, La Jolla, CA 92093, USA

Abstract

in [4]. Their algorithm guaranteed that the difference
of the number of wires between the adjacent pins on
the same ring is less than or equal to 1. However,
since their physical routing was based on the rubberband approach, the wire distribution between the adjacent pins was still non-uniform, i.e., all the wires
crowded to one of the pins. Recently, Yu, Darnauer
and Dai proposed a more general pin routing approach
in [5]. They used a min-cost max-flow algorithm to
solve the interchangeable pin routing problem. However, wire uniformity problem was not addressed in
this approach.
In this paper, we present a performance-driven 1 / 0
pin routing algorithm with special consideration of
wire uniformity. The algorithm contains three phases.
First, a topological routing based on min-cost maxflow algorithm tries to achieve a globally uniform wire
distribution on the whole routing area. Then, a constructive physical routing phase is proposed with special consideration on wire uniformity of the fanout
area nearby the periphery of chip pads. Finally, each
wire is adjusted by a balanced position based wire
polishing phase to further improve the local wire uniformity by polishing each wire from broken line into
smooth curve. In addition, around the periphery area
of each pin, the algorithm guarantees the specified
wire-pin pitch requirements to be satisfied while the
wire goes around the pin.

This paper presents a performance-driven 1/0
pin routing algorithm with special consideration of
wire uniformity. First, a topological routing based
on min-cost max-flow algorithm is proposed. I n this
phase, an exponential weight function is used t o guide
the flow distribution which is very helpful i n distributing wires, globally and uniformly, on the whole routing
area. Then a physical routing phase is applied to implement one-to-one connection between chip pads and
1/0 pins, which focuses on the wire uniformity of the
fanout area nearby the periphery of chip pads. Finally,
a balanced position based wire polishing approach is
proposed t o further improve the local wire uniformity
which tries t o modify each wire into a smooth curve
instead of broken line while satisfying the specified design rules such as wire-wire pitch and wire-pin pitch.
A routing cost function is adequately deJined t o guide
the whole routing process, which leads t o a good tradeoff between wire uniformity and wire length. The algorithm has been implemented and tested on up to
10-ring 600-pin P G A and the experimental results are
very promising.

1

Introduction

Due to the tremendous increase in the complexity of IC designs, the number of 1/0 pins on a IC
chip becomes larger and lar er. It may be reaching up
to 2000 in the near future 711. In addition, designers
are trying to extend the excellent package technologies
to increasingly higher signal speeds. So a high performance automatic 1/0 pin routing tool is required. The
routing on the typical package structures like Pin Grid
Array (PGA) and Ball Grid Array (BGA) have been
studied well. However, few researchers have studied
the wire uniforniity problem, which is very important
for high performance PGA or BGA routing since it
directly affect the electrical characteristics of the interconnections. Uniform configurations of all the wires
provide predictable characteristics and allow accurate
RLC modeling of smaller subsections. They also facilitate differential signal routing which is very important
for packages that are expected to have higher simultaneous switching noise in the future.
There exist some PGA or BGA routers [2]-[4].
These routers took advantage of the special geometrics
and symmetries of their respective problems and the
freedom of interchangeable pins. [2] and [3] was the
earlier work in this field. The wire u n i f o r d t y problem for BGA routing was discussed by Yu and Dai

0-7803-5012-X /99/$10.00 01999 IEEE.

2

Problem Formulation

We define pins to be connectors on the package that are arranged in a grid array. Pads are via
pads that are arranged on the periphery of IC chip
inside the pin array, as shown in Fig. 1. Assume
a PGA package has P pins Pin = { p 1 , p z , . . . , P P }
and P pads Pad = { q l , q z , ...,qp}.. All the pins
are arranged in R rings and each ring contains P'
pins Ring' = { p i , p ; , . . . , p;,}. Ring' and RingR are
called the inner ring and outer ring respectively. We
assume each pad is connected to one and only one pin.
So P wires W i r e = ( w 1 , w2,. . . , w p } are required to
complete the one-to-one routing. Wire wi connects a
pad qi and a pin p , i.e., wi = ( q i , p ) . But we do not
care which pad is connected to which pin. In expression wi = ( q i , p ) , we do not indicate the subscript of p
because we do not know pad qi is connected to which
pin before the wire is routed. Once a wire is routed, a
subscript will be assigned to pin p . We will use single
layer for routing. The routing area is the minimum
bounding box including all the pins but excluding the

129

U, or a super node. A super node is either a source
node v, into which all the pads are grouped or a target node vt which collects all the flows injected into
all the pin nodes. Each directed edge e ( i , j ) connects
two nodes from vi to v j , associated with a capacity

chip area, i.e., the area between chip periphery and
outer ring. Each square area surrounded by four adjacent pins is called the routing cell, as shown in Fig.
1.
Pins

rouling
cell
cut

or v i = up or vj = U,.
Because of the symmetrics of PGA structure of Fig. 1,
we only need to consider one-eighth of the whole flow
network of PGA which is shown in Fig. 2. According
to these denotations, the topological routing 'for PGA
can be formulated as the min-cost max-flow problem.

oyter
ring

inner
ring

input:
output:
minimize:

Fig. 1. PGA package
We assume the routing topology is monotonic. A
monotonic topological routing is such a topological
routing that wire wi = ( q i , p ) connecting pad qi and
~ ) some ,E
pin p intersects exactly one cut ( p ; , ~ ; + for
in each ring 1 5 s 5 R - 1.
We allow a wire to be a curve instead of broke:n
line. Each wire is approximated by a set of wire s e g
ments, i.e. wi = { s e g l , seg2, . . . , SegK,}. Let Li,j be
the length of segment segj of wire w i , D L i , j and D R i , j
be the distances between segment segj of wire wi and
its left and right adjacent wire or pin, respectively.
Thus, the cost function of the PGA routing can be
defined as:

PGA routing problem
all the flows f ( i , j ) > 0
Cv,,v,EV

vi,CV,,zV(f(i,j ) - f ( j i

Where, g ( i , j ) is exponential weight function associated with flow f ( i , j ) , which is defined as:

P is a constant determined by experiments, and bi
is defined as:

Parameter a is a positive constant which takes a tradeoff between wire length and wire uniformity. The goal
of the algorithm is to complete the routing within
given area with minimum cost function.
We specify a minimum wire-wire pitch pitch,,, and
a minimum wire-pin pitch pitch,,,.
With the above
assumptions and denotations, the performance-driven
PGA routing problem is formulated as follows.

output :
minimize:
subject to:

3

a given PGA routing area
net list wi = ( q i , p ) , a = 1 , 2 , . . . , P
pitch specification patch,,, and pitch,
wires Wire = { w l , w2, . . . , w p }
cost in equation (1)
V i ,j , pitch,,,,,
2 pitch,,, and
v i ,j , pitch,,,,,
1 pitch,,,

if vi = U , , (source node)
if vi = u t , (target node)
others

(3)

super node

outer ring

inner ring
,P

source
super node

Fig. 2. Flow network of P G A
The min-cost max-flow problem defined above can
be easily solved using the lower bound algorithm in
6 . Our experiments show that the weight function
2 does help to improve the wire uniformity.

Algorithm Description

tS

The routing algorithm is composed of three
phases: topological routing, physical routing and wire
polishing based on balanced position method.

3.1

i)) = bi

vi,j10 I f(i,j ) 5 c ( i , j )

P
-P
0

input:

S ( i , j >. f ( i , j )

subject to:

3.2

Topological Routing

Physical Routing

Physical routing in the one-eighth routing area
is divided into two regions, A and B, as shown in Fig.
1. The routings in region A and B are separately processed.

We define a flow network G(V,E) where V is a
vertex set and E is a edge set. Each vertex vi E I/
presents either a pin node v, or a routing cell node

130

3.2.1

Routing In Region A

these tangential points, as shown in Fig. 3(b). The
connection between each pad and each pin or passing
point is composed of two parts. One is the tangent between the pin or passing point and its corresponding
tangential point. T h e other is the arc between the tangential point and the corresponding pad. The smooth
arc nearby the periphery of chip improves the wire uniformity and the signal continuity, and the fanout area
denote fanout area
is dramatically reduced. Let Sorsq
of the region surrounded by curve orsq and the bottom
line oq, Sop denote fanout area of the right triangle
opq, and a Le length of the bottom line og. The following theorem gives the difference between area So,.$,
and Sop,.

Region A contains the most congested area. The
goals of the routing in region A are t o reduce the most
congested area into a smaller area compared with the
area used by other approaches and to generate smooth
curves instead of broken lines to make the wire distribution in the most congested area more uniform.
Such smooth curves could potentially result in reduced
crosstalk between wires.
top line
(inner ring)

Theorem 1 The difference between area Sorsg
of the
fanout area orsq and area Sop, of the right triangle opq
bottom line
(pads)

0

as:

: passing point

e
(a) previous approaches

Because of the space limitation, the proof is omitted. It should be noted that in actual layout implementation, the arcs within area orsq might have to be
approximated by straight line segments depending on
tool limitations.
3.2.2

Routing In Region B

When the routing in region A is finished, routing
in region B can be easily carried out. For each edge
e ( i , j with non-zero flow in region B, do the division
i c / f ) i , j ) . Thus all the turning point on all the edges
are obtained. Then according the the flow network
of region B, connect the corresponding turning points
‘.’.
_
_
_
_
,
.-:--___-.
,
~.
along with the flow directions. Finally, each pad is
-*
---____--,
connected to only one pin, which is guaranteed by the
(b) our approach
flow network.
Fig. 3. Routing in region A
3.3 Wire Polishing
Previous approaches used broken lines to completed
The basic idea of balanced position based wire
the one-to-one connection. Each line connects one pad
polishing approach is to split each wire into many
to a pin or a passing point using one broken point.
short segments, each of which has such short length
Passing points on inner ring will pass the connections
that the wire can be approximated as a curve by linkto the pins outside inner ring. All the broken points
ing all the segments together. For each wire segment,
lies on a straight line which keeps a fixed angle Q with
the distances between it and its two adjacent obstathe bottom line. Thus, the fanout area is right tricles are measured. Then, we try to move the segment
angle opq, as shown in Fig. 3(a), where the number
to a “middle” position, called the balanced position,
of passing points between the adjacent two pins is debetween the two adjacent obstacles. The operation
termined by the flow distribution generated in phase
of such a wire movement is called the wire polishing.
1. This approach has two disadvantages. One is the
The “balanced position” is defined as a physical pofanout area, the most congested area, is too big. The
sition with minimum cost function that is defined by
other is that the broken points may lead t o increased
equation (1) in section 2. Other performance critecapacitive discontinuity and crosstalk.
ria could be used such as reduced trace capacitance or
crosstalk.
Our approach uses curves instead of broken lines
This idea can be implemented by the “string balls”
to complete routing in region A . The approach is outtechnique. Imagine that each wire bunches a string of
lined as follows. Taking point o (the position of the
balls, say, m balls { b l , b 2 , . . . , bm}. The radius of each
most right pad) as center, the distance between pads
ball and the distance between two adjacent balls are
and the center as radius, for each pad, draw a circle
initially set to be equaling the wire width. In Fig. 4,
passing it with the same center. For each circle, draw
the thick solid lines indicate the routed wires (obstaa tangent passing the corresponding pin or passing
cles) wk and wI respectively. The thin solid line indipoint. Thus, a tangential point is obtained for each
cates wire wi to be adjusted. For the sake of clearness,
circle. For example, points T , s, and q are three of
_I

131

the balls in Fig. 4 are drawn with radius larger than
wire width. All the balls on wire wi are iteratively
processed one by one. For each iteration, a ball is
pumped to increase its radius. If the ball touches one
of its two adjacent obstacles, it is moved by a small distance toward the other adjacent obstacles. The movement is accepted and the ball is pumped again if i.t
has the routing cost function (1) reduced. The ball
pumping and movement continues until the cost function can not be further reduced. At this time, the
ball has reached the balanced position. The iteration
terminates when there is no ball left to be moved to
reduce the cost function. Finally, the whole wire uii
has reached the balanced position, which is indicated
by the thick dash line in Fig. 4.

[#I

1I 6 5
1 - 1 224
I I 314
r 7
8
9

w1

wk

Fig. 4. String balls

4

Experiments

The performance-driven 1 / 0 pin routing algorithm been implemented in C programming language,
and all experiments are performed on a Sun Sparc20
workstation.
Table 1. PGA SDecifications
#rings
#pins

pitch,,,

1 4
I 144

1

I
I

5 1 6 1 7 I 8 1 9
200 I 264 I 336 416 I 504
120
40

I

I

I

routing cost ( I )
I I I1 I I11

600

Eight PGAs have been tested. Parameter a in
equation (1) is selected as 0.00005. Table 1 lists the
main characteristics of the tested PGAs. The result,s
are shown in Table 2, where # indicates the number
of rings of each PGA. In order to show the effect of exponential weight function (a), we report two medium
results of physical routing with ,B = 0 and ,B > 0,
which are indicated as case I and case 11, respectively.
The final results of wire polishing phase is indicated
as case 111.
As shown, according,to case I and case I1 tested,
by using weight function (2) in min-cost max-flow
algorithm, the routing cost (1) can be reduced by up
to 10.35% (6-ring PGA), and there is some reduction
in the total wire length for most PGAs. By comparing
case I1 and case 111, we can see that wire polishing can
reduce the routing cost (1) by up to 30.35% (IO-ring
PGA) with wire length increase by at most 1.14% (6ring PGA). If case I and case I11 are directly compared,
the routing cost (1) can be reduced by up to 36.58%
(10-ring PGA) with wire length increase by at most
0.73% (10-ring PGA). These results show that the wire

132

-153
213
309

143
208
288

I

I

I

672148
857972
1051849

wire length
I1
I

I

1

CPU

I11

668192
669660
858202
860329
1052390 1 1 0 5 5 8 7 7

I

(s)

I
I
I

54
110
109

IEEE ICC 2015 - Mobile and Wireless Networking Symposium

Upper and Lower Bounds of Choice Number for
Successful Channel Assignment in Cellular Networks
Ran Wang† , Chenyang Zhou∗ , Anisha Mazumder∗ , Arun Das∗ , H. A. Kierstead† , Arunabha Sen∗
∗ School of Computing, Informatics and Decision System Engineering
† School of Mathematical Sciences and Statistics

Arizona State University, Tempe, Arizona 85287
Email: {rwang31, czhou24, anisha.mazumder, arun.das, asen, kierstead}@asu.edu
Abstract—A cellular network is often modeled as a graph
and the channel assignment problem is formulated as a coloring problem of the graph. Cellular graphs are used to model
hexagonal cell structure of a cellular network. Assuming a 2band buffering system where the interference does not extend
beyond two cells away from the call originating cell, we study
a version of the channel assignment problem in cellular graphs
that been studied only minimally. In this version, each node has
a fixed set of frequency channels where only a subset of which
may be available at a given time for communication (as other
channels may be busy). Assuming that only a subset of frequency
channels are available for communication at each node, we try
to determine the size of the smallest set of free channels in a
node that will guarantee that each node of the cellular graph
can be assigned a channel (from its own set of free channels)
that will be interference free in a two band buffering system.
The mathematical abstraction of this problem is known as the
Choice Number computation problem and is closely related to
the List Coloring problem in Graph Theory. In this paper we
establish a lower and an upper bound of the distance-2 Choice
Number of cellular graphs. In addition we also conduct extensive
experimentation to study the impact of the availability of the
number of free channels in a node to the percentage of the
total number of nodes in the network that can be assigned an
interference free channel in a two band buffering system.

I. I NTRODUCTION
The service area of a mobile cellular network is usually
conceived to be a tessellation of hexagonal cells as shown in
Figure 1. Each cell is allocated a set of frequency channels
to meet the traffic demands in that cell. The same frequency
channel can be used in two different cells as long as there is no
perceptible interference. Although the geographical distance
between the cells is not the only factor in determining the
interference between the cells, it is a major factor. If the
distance between two different cells in the service area is
sufficiently large, then there will be no interference even when
the same frequency channel is used for communication in
these two cells. Such cells are known as co-channel cells.
A cellular network can be modeled as a graph where each
node of the graph represents a hexagonal cell and two nodes
have an edge between them if the corresponding cells share
a common boundary. The cellular graph corresponding to the
cellular structure shown in Figure 1, is shown in Figure 2. The
cellular network is said to have a k-band buffering system if it
is assumed that the interference does not extend beyond k cells
away from the call originating cell. An example of a buffering
system with k = 2 is shown in Figure 3. In the figure, each Zi
represents the cell i and the letter A indicates that the same
frequency A can be used in all these cells without interference.

978-1-4673-6432-4/15/$31.00 ©2015 IEEE

3370

There have been a number of studies on channel assignment problems in cellular networks where the goal is to
minimize the total number of channels used [1], [2], [3],
[4], [5]. In their abstraction, most of them turn out to be
equivalent to computation of the chromatic number [6] of the
corresponding cellular (interference) graph. Similar problems
in cognitive radio networks have been studied in [7], [8],
[9]. Although the problem being studied in this paper deals
with channel assignment in cellular networks, it is distinctly
different from most of the other channel assignment problems
studied. To the best of our knowledge the only paper that
reported some results related to the problem under study in
this paper is [10] and even then the results reported in [10] are
significantly different from the ones presented in this paper.
As indicated earlier each cell is allocated a set of frequency
channels to meet the traffic demands in that cell. This situation
is somewhat similar to 802.11-based Wireless Local Area Networks (WLAN), where each WLAN standard (802.11/a/b/g)
defines a fixed number of channels for use by access points
and mobile users. For example, the 802.11b standard defines a
total of 14 frequency channels of which only 1 through 11 are
permitted in the US [5]. In our model of the cellular network,
we assume that each cell has a fixed number of frequency
channels, say 15, (indexed from 1 through 15), but only a
subset of the channels are free for assignment at any given
time, as the other channels have already been assigned and
are busy. For example, the set of free channels in cells 1, 2
and 3 may be (2, 5, 7, 9, 11), (3, 11, 12, 13) and (2, 3, 7, 9, 11,
12) respectively. The problem that we study in this paper is
the following: What is smallest size of the set of free channels
associated with the cells (nodes of the cellular graph) that
can guarantee interference free channel assignment to all the
nodes? The problem of allocating channels to the nodes from
the list of free channels can be formalized as a list coloring
problem [6]. In the list coloring problem, every node of the
graph is associated with a list of colors (free channels), and the
requirement is to find a proper coloring (i.e., no two adjacent
nodes are assigned the same color) such that each node is
colored with one of the colors in its list. The goal of the
optimization version of the list coloring problem is to minimize
the size of the list that is necessary for proper coloring of the
graph. The smallest list size that guarantees a proper coloring
of the graph is known as the Choice Number of the graph [6].
In this paper we focus on the cellular graphs and a 2band buffering system for interference avoidance. A 2-band
buffering system implies that two cells (nodes) that are at
most distance two apart in the cellular graph, are not allowed
to have the same channel. With this restriction, our problem

IEEE ICC 2015 - Mobile and Wireless Networking Symposium

Fig. 1: Hexagonal cell structure of cellular
network

Fig. 2: Cellular graph corresponding to the
hexagonal cell structure

corresponds to the computation of distance-2 Choice Number
of cellular graphs. In this paper, we present an upper and
a lower bound of the distance-2 Choice Number of cellular
graphs. To the best of our knowledge, no such results are
presently available in the literature. We establish a lower bound
of 8, and an upper bound of 10 for distance-2 Choice Number
of cellular graphs. The implication of the upper bound result
is that if each node of the cellular graph has a list of at least 10
free channels, then each node can be assigned a channel from
among its own list that will be interference-free under a two
band buffering system. We will refer to such an assignment as
a successful channel assignment. The implication of the lower
bound result is that if at least one node of the cellular graph has
a list of fewer than 8 free channels, then successful channel
assignment is impossible for at least one problem instance.
This however does not rule out the possibility of having a
successful channel assignment for some problem instances
where some of the nodes have less than 8 free channels. In
order to explore the impact of the availability of the number
of free channels in a node on successful channel assignment
(when all nodes are assigned a channel) we conduct extensive experimentation. When successful channel assignment is
impossible, our experiments determine the percentage of the
total number of nodes in the network that can be assigned an
interference free channel in a 2-band buffering system.
II. P ROBLEM F ORMULATION
First we provide a few definitions from [11].
Definition: A proper k-coloring of a graph G = (V, E) is
a function c : V (G) → {1, . . . , k} such that c(u) 6= c(v)
whenever uv ∈ E. A graph is k-colorable if it has a proper
k-coloring. The minimum k such that G is k-colorable is its
chromatic number, denoted by χ(G).
Definition: A proper distance-2 k-coloring of a graph G =
(V, E) is a function c : V (G) → {1, . . . , k} such that
c(u) 6= c(v) whenever the distance between the nodes u
and v, denoted by d(u, v), in G is at most two. A graph is
distance-2 k-colorable if it has a proper distance-2 k-coloring.
The minimum k such that G is distance-2 k-colorable is its
distance-2 chromatic number, denoted by χ2 (G).
Definition: A list assignment L on G is a function that assigns
to every vertex v a list L(v) of colors available to be used
on v (the colors may be represented by positive integers).
An L-coloring is a proper coloring of G such that the color
on v is chosen from L(v), for each v ∈ V (G). A graph
is k-choosable (or list k-colorable) if G has an L-coloring
whenever L is a list assignment such that |L(v)| ≥ k for
all v ∈ V (G). The minimum k such that G is k-choosable is

3371

Fig. 3: Two band buffering

the list chromatic number (or the choosability or the choice
number) of G, denoted by χlist (G). The definition of choice
number can be extended to distance-2 choice number just
the way the definition of chromatic number was extended to
the definition of distance-2 chromatic number. The distance-2
choice number of G is denoted by χlist
2 (G).
Problem Statement: Compute the distance-2 choice number
χlist
2 (G), where G is an infinite (i.e. an infinite number of
rows and columns) hexagonal grid graph as shown in Figure
2, corresponding to a cellular network structure as shown in
Figure 1.
Although the goal of this paper was to find χlist
2 (G) where
G is an infinite hexagonal grid graph, we are only able to
establish an upper and a lower bound of χlist
2 (G) that lies
within a small interval. In the following section we establish
that 8 and 10 are lower and upper bounds of χlist
2 (G) of
a hexagonal grid graph respectively. Also, in the following
sections “cellular graphs” and “hexagonal grid graphs” are
used interchangeably.
III. U PPER AND L OWER B OUNDS OF C HOICE N UMBER
A. Lower Bound of Choice Number
In [2] it was proven that for an infinite cellular graph
χ2 (G) = 7. Since for any graph G, χlist
2 (G) ≥ χ2 (G), for
an infinite cellular graph χlist
2 (G) ≥ 7. In this subsection, we
prove that χlist
2 (G) is at least 8.
Definition: Consider a node (i, j) in a cellular graph G as
shown in figure 4(a). We will refer to a seven node subgraph
comprised of the nodes (i, j + 1), (i, j + 2), (i − 1, j), (i −
1, j + 1), (i − 1, j + 2), (i − 2, j), (i − 2, j + 1) as a 7-wheel.
Note that in this coordinate system, the rows correspond to the
horizontal lines indexed from top to bottom and the columns
correspond to 60o angled lines with the rows, indexed from left
to right. At times, we refer to rows and columns as “straight
lines” in this paper.
We claim the following facts for the coloring function c(.). We
note that since χ2 (G) = 7 [2], a proper distance-2 k-coloring
“c(.)” of an infinite cellular graph G with k = 7 exists.
Claim 1. In any valid distance-2 k coloring of an infinite
cellular graph with k = 7, either c(i, j) = c(i − 2, j + 1) or
c(i, j) = c(i − 1, j + 2) where (i, j), (i − 2, j + 1), (i − 1, j + 2)
are the nodes as shown in figure 4(a).
Proof: Let the distance-1 neighborhood of node (i − 1, j + 1)
be noted as N 1 [i − 1, j + 1]. The 7-wheel formed by N 1 [i −
1, j + 1] ∪ {(i − 1, j + 1)} is a distance-2 clique (a subgraph
where every node is within distance-2 of every other node) of
the cellar graph, as the distance between every pair of nodes

IEEE ICC 2015 - Mobile and Wireless Networking Symposium

(a) Figure for proving Claim 1

(b) Figure for proving Claim 2

(d) Case 1

(c) The counterexample that χlist
2 (G) > 7
with list assignment L

(e) Case 2

Fig. 4: Figures for proving claims 1 and 2, and distribution of c(i, j) in the proper distance-2 coloring of G

of the 7-wheel is at most two. Accordingly, all seven colors
must be used to color N 1 [i − 1, j + 1] ∪ {(i − 1, j + 1)}
when distance-2 coloring is considered. Thus, there must exist
some node v ∈ N 1 [i − 1, j + 1] ∪ {(i − 1, j + 1)} such that
c(v) = c(i, j). However, for any v ∈ {N 1 [i − 1, j + 1] ∪ {(i −
1, j + 1)}}\{(i − 2, j + 1), (i − 1, j + 2)}, d(v, (i, j)) ≤ 2.
Therefore, the only possibilities are c(i, j) = c(i − 2, j + 1) or
c(i, j) = c(i − 1, j + 2).
Claim 2. In any valid distance-2 k coloring of an infinite
cellular graph with k = 7, if the nodes (i, j) and (i − 2, j + 1)
are colored with the same color α, then node (i − 3, j − 2)
must also be colored with the same color α, where the nodes
(i, j), (i − 2, j + 1), (i − 3, j − 2) are as shown in figure 4(b).
Proof: Suppose that the nodes (i, j) and (i − 2, j + 1) are colored with the same color α as shown in figure 4(b). Consider
the 7-wheel formed by the nodes (i − 1, j − 1), (i − 1, j), (i −
2, j − 2), (i − 2, j − 1), (i − 2, j), (i − 3, j − 2), (i − 3, j − 1).
Some nodes of this 7-wheel must have the color α, as these
nodes form a distance-2 clique and hence seven distinct colors
must be used to color these nodes. Given that the node
(i, j) is assigned the color (i, j), only two nodes in the 7wheel, (i − 3, j − 2), (i − 3, j − 1), can have the color α to
satisfy the distance-2 proper coloring requirement. Similarly,
if (i − 2, j + 1) is assigned color α, only three nodes in the
7-wheel, (i − 1, j − 1), (i − 2, j − 2), (i − 3, j − 2) can have the
color α to satisfy the distance-2 proper coloring requirement.
Since the node (i − 3, j − 2) is the only common node in these
sets, in order to satisfy the coloring constraints imposed by the
nodes (i, j) and (i − 2, j + 1), the node (i − 3, j − 2) must
have the same color α.
Lemma 3. Any proper distance-2 k coloring “c(.)” of an
infinite cellular graph G, where k = 7 satisfies the following
property: For any straight line l ⊂ G , if v, w ∈ l with
c(v) = c(w) and v, w are not endpoints, then d(v, w) ≥ 7.
Proof: We prove the lemma by using Claims 1 and 2. Claim
1 states that in any valid distance-2 k-coloring of an infinite
cellular graph with k = 7, either c(i, j) = c(i − 2, j + 1) or
c(i, j) = c(i − 1, j + 2):
Case I: Suppose that c(i, j) = c(i−1, j +2) = α. Consider the
7-wheel comprising of the nodes (i − 1, j), (i − 1, j + 1), (i −

3372

2, j −1), (i−2, j), (i−2, j +1), (i−3, j −1), (i−3, j) in figure
4(d). Because of the arguments similar to the one provided in
Claim 2, the constraint c(i, j) = c(i − 1, j + 2) = α, forces the
node (i − 3, j − 1) of the 7-wheel to have the color α. Now
consider the 7-wheel formed by the nodes (i − 2, j + 1), (i −
2, j + 2), (i − 3, j), (i − 3, j + 1), (i − 3, j + 2), (i − 4, j), (i −
4, j + 1). Given that c(i − 1, j + 2) = c(i − 3, j − 1) =
α the node (i − 4, j + 1) of the 7-wheel is forced to have
the color α. Now consider the 7-wheel formed by the nodes
(i − 1, j + 3), (i − 1, j + 4), (i − 2, j + 2), (i − 2, j + 3), (i −
2, j +4), (i−3, j +2), (i−3, j +3). Given that c(i−1, j +2) =
c(i−4, j+1) = α the node (i−2, j+4) of the 7-wheel is forced
to have the color α. Given that the node c(i − 1, j + 2) = α,
it precludes the possibility of using the color α in the nodes
(i, j+1), (i, j+2), (i, j+3), (i, j+4), as these nodes are within
distance-2 of the node c(i−1, j+2) (see figure 4(d)). Similarly,
Given that the node c(i − 2, j + 4) = α, it precludes the
possibility of using the color α in the nodes (i, j +5), (i, j +6),
as these nodes are within distance-2 of the node c(i − 2, j + 4)
(see figure 4(d)). Accordingly, the nearest node on the i-th row
from the node (i, j) where the same color α can be used is
the node (i, j + 7). The same conclusion (i.e., the nearest node
on the i-th row to the node (i, j), where the color α can be
used used the node (i, j + 7).
Case II: Suppose that c(i, j) = c(i − 2, j + 1) = α. The same
conclusion as in Case I (i.e., the nearest node on the i-th row
to the node (i, j), where the color α can be used is the node
(i, j + 7)), can be drawn in this case also by using the same
type of arguments provided in Case I (see figure 4(e)). For
brevity, the arguments are not provided here.
Notice from the lemma above, we can easily conclude that
seven distinct colors have to be used in any seven consecutive
nodes in a straight line when proper distance-2 coloring of G
is considered.
Theorem 4. Let G be an infinite hexagonal grid (or cellular)
graph. Then χlist
2 (G) ≥ 8.
Proof: Suppose that G is a cellular graph and χlist
2 (G) = 7,
if possible. We will provide a counter-example by creating
a problem instance (i.e., an infinite cellular graph, and a list
assignment L on each node v of G, such that the size of the list

IEEE ICC 2015 - Mobile and Wireless Networking Symposium

on each node is 7) and demonstrating that a proper distance-2
coloring is impossible for this problem instance. Consider the
infinite cellular graph G shown in figure 4(c). We construct a
counterexample by creating a list assignment L on G such that
each node v of a cellular graph G receives lists of size seven
such that no proper distance-2 L-coloring of G exists. We can
view the graph G to be made up of three parts: (i) a subgraph
made up of the nodes belonging to a single row (say, i-th row,
shown as the straight line T in figure 4(c)), (ii) a subgraph
made up of the nodes belonging to rows (i − 1) and lower
(shown as subgraph Ga in figure 4(c)), and (iii) a subgraph
made up of the nodes belonging to rows (i + 1) and higher
(shown as subgraph Gb in figure 4(c)). For each v ∈ Ga ,
we assign L(v) = {1, 2, 3, 4, 5, 6, 7} and for each v ∈ Gb ,
we assign L(v) = {8, 9, 10, 11, 12, 13, 14}. Consider
a set of
 
nodes U = {u1 , u2 , . . . , ur }, where r = 74 73 = 1225, on
the straight line T (i.e., i-th row nodes) such that the distance
between the nodes ui and ui+1 is 7 for all i, 1 ≤ i ≤ r −1. We
create a set of lists L of size r, i.e., L = {L1 , L2 , . . . , Lr }. The
size of each Li , 1 ≤ i ≤ r is seven. The first four elements of
Li , 1 ≤ i ≤ r are all possible combinations of four numbers
from the set {1, 2, 3, 4, 5, 6, 7} and the last three elements of
Li , 1 ≤ i ≤ r are all possible combinations of three numbers
from the set {8, 9, 10, 11, 12, 13, 14}. Each Li , 1 ≤ i ≤ r is
unique. The list Li , 1 ≤ i ≤ r is assigned to the node ui , 1 ≤
i ≤ r. This completes the creation of the problem instance.
Next we argue that proper distance-2 coloring is impossible in
this problem instance.
Consider a node up , 1 ≤ p ≤ r on the line T . Suppose that
it is the j-th node on the i-th row of the cellular graph. The
node up has four nodes on the (i − 1)-th row and four nodes
on the (i + 1)-th row that are within distance two of the node
up . For proper distance-2 coloring of the nodes, the colors
assigned to these eight nodes cannot be assigned to the node
up (in other words, these eight colors are blocked for up ). Since
the list of each node on the (i−1)-th row is {1, 2, 3, 4, 5, 6, 7},
each node must have received one of these seven colors under
proper distance-2 coloring. Similarly, each node on the (i+1)th row must have received one of these {8, 9, 10, 11, 12, 13, 14}
seven colors under proper distance-2 coloring. Moreover, from
the Lemma 3, we know that for proper distance-2 k-coloring
with k = 7, seven different colors have to be used in any
seven consecutive nodes in a straight line. In other words, a
permutation P of the numbers {1, 2, 3, 4, 5, 6, 7} must have
been repeated on the (i − 1)-th row and a permutation Q of
the numbers {8, 9, 10, 11, 12, 13, 14} must have been repeated
on the (i + 1)-th row. Since the distance between the nodes
ui and ui+1 is 7 for all i, 1 ≤ i ≤ r − 1, the four distance2 neighbors of ui on the (i − 1)-th row must have received
the same four colors as the four distance-2 neighbors of ui+1
on the (i − 1)-th row. For exactly the same reason, the four
distance-2 neighbors of ui on the (i + 1)-th row must have
received the same four colors as the four distance-2 neighbors
of ui+1 on the (i + 1)-th row. This implies that each ui , 1 ≤
i ≤ r has exactly the same set of eight blocked colors, four
drawn from the set {1, 2, 3, 4, 5, 6, 7} and four drawn from
the set {8, 9, 10, 11, 12, 13, 14}. Let the set of blocked colors
for each ui , 1 ≤ i ≤ r be {b1 , . . . , b8 }, where {b1 , . . . , b4 } are
four elements of the set {1, 2, 3, 4, 5, 6, 7} and {b5 , . . . , b8 } are
four elements of the set {8, 9, 10, 11, 12, 13, 14}. Since each
ui , 1 ≤ i ≤ r has a unique list of seven colors, where four
colors chosen from the set {1, 2, 3, 4, 5, 6, 7} and the remaining

3373

three from the {8, 9, 10, 11, 12, 13, 14}, at least one ui , 1 ≤ i ≤
r must have a list associated with it that contains the elements
{b1 , . . . , b7 }. As such no color can be assigned to ui without
violating distance-2 coloring constraint. This proves that the
distance-2 Choice Number of an infinite Cellular graph is at
least eight, i.e., χlist
2 (G) ≥ 8.
B. Upper Bound of Choice Number
In this section, we prove that the distance-2 Choice Number
of an infinite Cellular graph is at most ten, i.e., χlist
2 (G) ≤ 10.
Our proof is constructive in the sense that it also provides
a polynomial time algorithm which always finds a proper
distance-2 coloring for an infinite cellular graph where each
node has a list of size ten associated with it.
Definition: Given a cellular graph G = (V, E), with |V | = n
and |E| = m, we define an ordering to be an ordered sequence
of the vertices of V and denote it as as U .
Lemma 5. Given a cellular graph G = (V, E), with
|V | = n and |E| = m, if there exists an ordering U =
{u1 , u2 , . . . , , un } of the node set V , such that ∀ui , 1 ≤
i ≤ n, there are at most k distance-2 neighbors of ui (i.e.,
nodes whose distance to ui is at most two) in the node set
{u1 , . . . , ui−1 }. Then the distance-2 Choice Number of G is
at most k + 1, i.e., χlist
2 (G) ≤ k + 1.
Proof: We prove this by induction on n. Base case, n = 1,
this is a trivial case and is obviously true.
Induction step, n > 1. Suppose we have an ordering of the
nodes of G = (V, E), U = {u1 , u2 , . . . , , un } such that
∀ui , 1 ≤ i ≤ n, there are at most k distance-2 neighbors
of ui in the node set {u1 , . . . , ui−1 }. Let G0 be an induced
subgraph of G consisting of the nodes u1 , u2 , . . . , un−1 . By
0
0
induction hypothesis, χlist
2 (G ) ≤ k + 1 and G has a proper
distance-2 coloring function c(.). Now we consider un . By
assumption, un has at most k distance-2 neighbors in G0 .
If un has a list with size no smaller than k + 1, then it is
guaranteed that un has a color which if assigned to un will not
violate the proper distance-2 coloring requirement. This color
assignment, together with the color assignment c(.), constitutes
a proper distance-2 coloring for G. As the addition of the node
un with additional edges to the graph induced by the nodeset
{u1 , u2 , . . . , un−1 } leaves open the possibility that the distance
between a pair of nodes connected to un in the graph might
change as they now have a new path through un . Accordingly,
after addition of the node un , the color assignment c(.) may no
longer satisfy the distance-2 coloring requirement. However,
due to the structure of the cellular graph and the way nodes are
added in the induction process, this is impossible. When a node
(i, j) is added to the cellular graph induced by the node set
(1, 1), (1, 2), . . . , (2, 1), (2, 2), . . . , (i, 1), (i, 2), . . . , (i, j − 1),
(see figure 4(d)), the new node (i, j) is linked to at most
three other nodes, (i, j − 1), (i − 1, j), (i − 1, j + 1). However
addition of the node (i, j) and the edges, do not change the
shortest path distance between any pair of nodes in the set
(i, j − 1), (i − 1, j), (i − 1, j + 1). As such the color assignment
c(.) continues to satisfy the distance-2 coloring requirement
even after the addition of the node (i, j).
Therefore in order to prove the lemma, it suffices to show
for any cellular graph G = (V, E), we can always find an
ordering U = {u1 , u2 , . . . , un } such that ∀ui , 1 ≤ i ≤ n,
there are at most nine distance-2 neighbors of ui in the node
set {u1 , . . . , ui−1 }.

IEEE ICC 2015 - Mobile and Wireless Networking Symposium

Consider an ordering of the nodes of a cellular graph,
where the nodes are ordered row by row and column by column, i.e., (i−1, j), (i−1, j +1), (i−1, j +2), . . . , (i, j), (i, j +
1), (i, j + 2), . . . , (i + 1, j), (i + 1, j + 1), (i + 1, j + 2), . . .,
as shown in figures 4(a), 4(b), 4(d), 4(e). We will refer to this
ordering as the row-column ordering.
Claim 6. In a cellular graph with row-column ordering of the
nodes, there can be at most nine nodes in the set v1 , . . . , vi−1
whose distance to the node vi (∀vi , 1 ≤ i ≤ n) is at most two.
Proof: Consider a node (i, j) as shown in figure 4(a). The
number of distance-2 neighbors of the node (i, j) in the node
set {(1, 1), . . . , (i, j − 1)} is nine. They are on the (i) i-th
row: (i, j − 1), (i, j − 2), (ii) (i − 1)-th row: (i − 1, j − 2), (i −
1, j − 1), (i − 1, j)(i − 1, j + 1), and (iii) (i − 2)-th row: (i −
2, j − 2), (i − 2, j − 1), (i − 2, j). No other nodes in the set
{(1, 1), . . . , (i, j − 1)} are within distance-2 of node (i, j).
Theorem 7. The distance-2 Choice Number of an infinite
cellular graph is at most ten, i.e., χlist
2 (G) ≤ 10.
Proof: The proof follows from lemma 5 and claim 6.
IV. C HANNEL A SSIGNMENT A LGORITHMS
As discussed earlier, in the context of hexagonal cellular
networks a “successful” channel assignment refers to the
assignment of a channel to a cell of the network such that
the cell is interference free, i.e. no other cell in its distance-2
neighborhood are assigned the same channel. The objective in
such cellular networks would thus be to make all cells in the
network interference free. However, in real world scenarios,
due to physical constraints of the network it may not be
possible to make all cells interference free, in this situation the
best strategy may be to maximize the number of interference
free cells to maximize network throughput.
We refer to this problem as the “max successful” channel
assignment problem and define it in a formal setting as follows:
Given graph G = (V, E) and the list of available colors for
each of the nodes in V , allocate the maximum number of
nodes a color from its available color list such that no two
nodes within a two hop distance from each other are assigned
the same color. In the following sections we present optimal
and heuristic approaches for solving this problem.
A. Optimal Solution using Integer Liner Programming
The inputs to the Integer Linear Program (ILP) to derive an
optimal solution for the “max successful” channel assignment
problem are as follows:
1) Graph G = (V, E) where V = {v1 , v2 , ....vn }
2) Colors Available C = {c1 , c2 , ..., cm }.
3) Color List CLi for node vi , CLi ⊆ C, ∀vi ∈ V
The variable used by the ILP is defined as follows – for each
node vi ∈ V and color cj ∈ C:

1
if node vi is assigned color cj
xij =
0
otherwise
The objective of ILP can now be written as:
max

n X
m
X

xij

(1)

i=1 j=1

3374

Such that:

m
X

xij ≤ 1,

∀i = 1, ..., n

(2)

j=1

xij ≤ 0,

∀cj ∈ C \ CLi , i = 1, ..., n

xij + xpj ≤ 1,

∀j = 1, ..., m

(3)
(4)

such that {vi , vp } ∈ V are within 2 hops away from each other
xij = {0, 1},

∀i = 1, ..., n, j = 1, ...m

(5)

For each node vi ∈ V and color cj ∈ C, the variable xij is
set to 1 iff the node vi is assigned color cj . Constraint (2)
ensures that a node cannot be assigned more than one color,
and constraint (3) ensures that a node cannot be assigned a
color that is not an element of its available color list. Constraint
(4) ensures that a color cj is not assigned to two nodes vi and
vp when these nodes are within 2 hops away from each other
in graph G. Finally, constraint (5) sets
the integer constraint
Pnup P
m
for variable xij . Thus, maximizing i=1 j=1 xij maximizes
the total number of nodes that can be successfully assigned a
color, this objective is outlined in (1).
B. Heuristic Solution
We now present an heuristic approach for the “max successful” channel assignment problem as outlined in Algorithm
1. We elucidate the working of the proposed heuristic with the
help of an example. Consider a section of an infinite cellular
graph G = (V, E) as shown in Figure 6.
Algorithm 1: Coloring by Ordering Algorithm
1 Given a cellular graph G = (V, E), consider a
row-column ordering order(G) of G
2 Consider nodes in V row-by-row, column-by-column
according to order(G)
3 foreach node v do
4
compute neighbor(v) = {u | u is within 2 hops of
v and u is ahead of v in order(G)}
5
check each u ∈ neighbor(v) and remove color of u
from v 0 s list if it appears in the same
6
if v 0 s list is not empty then
7
Assign the least index color c(v) from v 0 s list
to v
8
else
9
Skip coloring node v.
10

Return coloring of G

For this example let the lists associated with nodes (1, 1),
(1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2) and (3, 3) be
{c1 , c2 , c3 }, {c2 , c3 , c4 }, {c1 , c2 , c3 }, {c1 , c2 , c4 }, {c1 , c2 , c3 },
{c2 , c5 , c6 }, {c5 , c6 , c7 }, {c6 , c7 , c8 } and {c7 , c8 , c9 } respectively. Consider that order(G) is as follows: (1, 1), (1, 2),
(1, 3), . . . , (2, 1), (2, 2), (2, 3), . . . , (3, 1), (3, 2), (3, 3), . . . .
Since, node (1, 1) is the first node in order(G), none of its
distance-2 neighbors have been colored previously. Hence,
(1, 1) is assigned its least index color i.e., c1 (even though c2
and c3 are available in its list). Similarly, (1, 2) is colored with
least index color c2 (even though c3 and c4 are available). (1, 3)
and (2, 1) are assigned colors c3 and c4 respectively because
the other colors in their lists – c1 and c2 , have already been
used to assign colors to one of their distance-2 neighbors that
have appeared previously in order(G). Now, when node (2, 2)

IEEE ICC 2015 - Mobile and Wireless Networking Symposium

Label Cells # Free Channels
A
50
2
B
40
2
10
3
C
30
2
20
3
D
20
2
30
3
E
10
2
40
3
F
50
3
G
40
3
10
4
H
30
3
20
4
Fig. 5: Comparison of optimal vs heuristic approaches.
Refer to Table I for the legend of the x-axis labels.

is considered, all the colors in its list have already been used
to assign colors to some of its distance-2 neighbors that have
appeared previous to (2, 2) in order(G). In this situation the
heuristic skips coloring (2, 2) and moves on to the next node
in order(G) and repeats this approach of using the least index
color from the node’s list that has not already been used in the
node’s distance-2 neighborhood.

Fig. 6: Section of an infinite Cellular Graph

V. E XPERIMENTAL R ESULTS
In this section we present experimental results for the “max
successful” channel assignment problem. We conducted our
experiments on a hexagonal cellular network consisting of 50
cells. The cells were arranged in a grid layout with each of the
10 rows containing 5 cells. The set of free channels assigned
to the nodes was chosen from a set of 10 distinct channels.
The size of each set of free channels assigned to the nodes
was varied from 2–5. Table I shows the size of each set of
free channels considered in our experiments, and also acts as
the legend corresponding to the x-axis of figure 5.
Next, for each cell, the free channels associated with that
cell was randomly chosen from the set of 10 distinct channels
creating an instance of the max successful channel assignment
problem. 2000 such problem instances with a specified number
of free channels associated with the nodes were created, and
subsequently optimal and heuristic solutions for these instances
were computed. Then, the average number of successful assignments for these 2000 instances were computed. In our
experimentation we considered 16 different scenarios with
different number of free channels associated with the nodes.
The optimal solutions were obtained by solving ILPs using
the IBM CPLEX Optimizer 12.5. The results of the optimal
solution and the heuristic solution are presented as red and
blue lines respectively in figure 5. As seen in the figure, in
most cases the heuristic algorithm gives a solution close to
the optimal. In all our experiments the heuristic produces a

3375

Label Cells # Free Channels
I
20
3
30
4
J
10
3
40
4
K
50
4
L
40
4
10
5
M
30
4
20
5
N
20
4
30
5
O
10
4
40
5
P
50
5

TABLE I: Set of free channels used in experiments

solution that is within 12% of the optimal.
VI. C ONCLUSION
In this paper we introduced the distance-2 choice number problem for an infinite hexagonal cellular graph. We
established analytically the lower and upper bounds for this
problem and showed that the bounds lie within a small interval.
We then considered the “max successful” channel assignment
problem for hexagonal cellular graphs that aims to maximize
the number of cells that are assigned channels such that no
other cell within their distance-2 neighborhood are assigned
the same channel. We presented an optimal solution using
ILP, and a heuristic for this problem. We conducted extensive
experiments varying the number of free channels for a set of
nodes, and observed that the heuristic produced a solution that
was at most 12% of the optimal.
R EFERENCES
[1] G. K. Audhya, K. Sinha, K. Mandal, R. Dattagupta, S. C. Ghosh, and
B. P. Sinha, “A new approach to fast near-optimal channel assignment
in cellular mobile networks,” IEEE Transactions on Mobile Computing,
vol. 12, no. 9, pp. 1814–1827, 2013.
[2] A. Sen, T. Roxborough, and B. P. Sinha, “On an optimal algorithm for
channel assignment in cellular networks,” in International Conference
on Communications, 1999, vol. 2. IEEE, 1999, pp. 1147–1151.
[3] A. Sen, T. Roxborough, and S. Medidi, “Upper and lower bounds of a
class of channel assignment problems in cellular networks,” in INFOCOM’98. Seventeenth Annual Joint Conference of the IEEE Computer
and Communications Societies. Proceedings. IEEE, vol. 3. IEEE, 1998,
pp. 1284–1291.
[4] S. C. Ghosh, B. P. Sinha, and N. Das, “Coalesced cap: an improved technique for frequency assignment in cellular networks,” IEEE Transactions
on Vehicular Technology, vol. 55, no. 2, pp. 640–653, 2006.
[5] A. Mishra, S. Banerjee, and W. Arbaugh, “Weighted coloring based
channel assignment for wlans,” ACM SIGMOBILE Mobile Computing
and Communications Review, vol. 9, no. 3, pp. 19–31, 2005.
[6] R. Diestel, “Graph theory. 2005,” Grad. Texts in Math, 2005.
[7] J. Tigang, W. Honggang, and L. Supeng, “Channel allocation and
reallocation for cognitive radio networks,” Wireless Communications and
Mobile Computing, vol. 13, no. 12, pp. 1073–1081, 2013.
[8] T. Jiang, H. Wang, and A. V. Vasilakos, “Qoe-driven channel allocation
schemes for multimedia transmission of priority-based secondary users
over cognitive radio networks,” IEEE Journal on Selected Areas in
Communications, vol. 30, no. 7, pp. 1215–1224, 2012.
[9] T. Jiang, H. Wang, and Y. Zhang, “Modeling channel allocation for multimedia transmission over infrastructure based cognitive radio networks,”
IEEE Systems Journal, vol. 5, no. 3, pp. 417–426, 2011.
[10] N. Garg, M. Papatriantafilou, and P. Tsigas, “Distributed long-lived
list colouring: How to dynamically allocate frequencies in cellular
networks,” Wireless Networks, vol. 8, no. 1, pp. 49–60, 2002.
[11] O. V. Borodin, “Colorings of plane graphs: A survey,” Discrete Mathematics, vol. 313, no. 4, pp. 517–539, 2013.

Pervasive and Mobile Computing 13 (2014) 258–271

Contents lists available at ScienceDirect

Pervasive and Mobile Computing
journal homepage: www.elsevier.com/locate/pmc

Fast track article

Fault-tolerant design of wireless sensor networks with
directional antennas
Shahrzad Shirazipourazad a,∗ , Arunabha Sen a , Subir Bandyopadhyay b
a

School of Computing, Informatics and Decision System Engineering, Arizona State University, Tempe, AZ 85281, USA

b

School of Computer Science, University of Windsor, Windsor, ON N9B 3P4, Canada

article

info

Article history:
Available online 29 March 2014
Keywords:
Tree connectivity augmentation problem
Directional antenna
Region based fault

abstract
A tree structure is often used in wireless sensor networks to deliver sensor data to a
sink node. Such a tree can be built using directional antennas as they offer considerable
advantage over the omni-directional ones. A tree is adequate for data gathering from all
sensor nodes if no node fails. We study the problem of enhancing the fault tolerance of a
data gathering tree by adding additional links so that failure of a sensor or a pair of adjacent
sensors would not disconnect the tree. We prove that the least-cost tree augmentation
problem is NP-complete and provide approximation algorithms one for single node failure
and the other for a pair of adjacent node failure, with performance bounds of two and four
respectively.
© 2014 Elsevier B.V. All rights reserved.

1. Introduction
The primary goal of a wireless sensor network is to deliver the collected sensor data to the sink node, either in the raw
form or after in-network aggregation. Generally, each sensor comprises of two components: the sensing component and the
communication component. The sensing component gathers information from the surrounding area and the communication
component is responsible for communicating the gathered information to the appropriate node for processing. Most often,
the data collection operation from all the sensor nodes is carried out by creating a tree topology that spans all the sensor
nodes, with the sink node as the root [1,2].
Directional antennas offer substantial advantages over their omni-directional counterparts, as they can focus their transmission energy in a specific direction, using a narrow beam of width α . A directional antenna can be mounted on a swivel
and can be oriented towards a target or alternately each sensor can be equipped with multiple antennas, each occupying a
sector with beam width α . The transmitted signal disperses in any unguided wireless media and as a consequence, the signal
strength diminishes with distance. Although attenuation is in general a complex function of the distance and the makeup of
the environment through which the signal propagates, a significant cause of signal degradation is free space loss. Free space
loss for an ideal isotropic antenna is measured as the ratio of the transmitted power to the received power and is given by
(4π d2 )
,
λ2

where λ is the carrier wavelength, and d is the propagation distance between transmission and reception antennas.
In particular, the energy required by an antenna to reach all nodes within its transmission radius is proportional to the area
covered. Thus, an omni-directional antenna with a transmission radius r will consume power proportional to π r 2 (the area
of a circle with radius r) while a directional antenna with beam width α radians will consume power proportional to α2 r 2 . The
expression is valid under the assumption that the signal is transmitted over the primary lobe and the power consumed by the

∗

Corresponding author.
E-mail addresses: shahrzad.azad@asu.edu (S. Shirazipourazad), asen@asu.edu (A. Sen), subir@uwindsor.ca (S. Bandyopadhyay).

http://dx.doi.org/10.1016/j.pmcj.2014.03.004
1574-1192/© 2014 Elsevier B.V. All rights reserved.

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

259

Fig. 1. A data collection tree constructed by directional antennas.

remaining lobes is negligible [3]. The expressions show that with a directional antenna with beam width α , power consumption can be reduced by a factor of 2απ . Moreover, in comparison with omni-directional antennas, the directional antennas
have significantly less interference and fading [4,5]. For additional information on antenna theory, we refer the reader to [6].
Due to such attractive features, sensors with directional antennas are being increasingly used for wireless sensor
networks. Some examples include camera networks for vision-based sensing, radar networks for weather monitoring and
sonar network for underwater object detection [5]. With rapid advances in the miniaturization of directional antenna
technology, it is likely that the use of directional antennas in sensor platforms will proliferate. This trend is demonstrated
by the increasing interest in the use of directional antennas for performance improvement in wireless networks in general
and wireless sensor networks in particular [4].
Just as the directional antennas offer a number of advantages, it also introduces a few problems. When sensor nodes
use omni-directional antennas, the network topology typically is a mesh and not a tree. A tree that spans over this mesh
topology is utilized for the purpose of data gathering. Although it may appear that the use of omni-directional antenna for
the purpose of data gathering is wasteful, as many of the network links created by such antennas are never utilized, this is
not completely true. The unused links essentially introduce a certain level of redundancy that can be utilized when one or
more of the sensor nodes fail and the data gathering spanning tree becomes disconnected. However, the negative aspect of
this lack of redundancy is that it can no longer deal with a fault scenario, where one or more sensor nodes fail. Without any
built-in redundancy, when some nodes fail, the data gathering tree is disconnected and the sink node fails to receive any
data from some of the sensors.
The primary motivation of our work is to retain the advantage of both types of antennas by combining the efficiency of
directional antennas with the redundancy of omni-directional ones. In this paper we study the problem of enhancing the
fault tolerance capability of a data gathering tree by adding a few additional links so that the failure of any one sensor or
a pair of adjacent sensors would not disconnect the tree. We consider that the sensor nodes are equipped with directional
antennas and nodes p and q can communicate with each other if antennas of p and q direct their beams to each other. In this
case a bidirectional link is used between the nodes p and q. Fig. 1 shows a data collection tree with two sensor nodes u and
v and a sink r. The sectors show the communication ranges and the lines show the wireless links. The addition of an edge
between two nodes p and q in the sensor network topology corresponds to the deployment of two new directional antennas
at the nodes p and q directed towards each other.
Most of the previous studies on the fault tolerant design of wireless sensor networks [7,8] consider that the sensor nodes
are equipped with omni-directional antennas and the objective is to assign transmission power to the nodes such that the
network is k-connected while power consumption is minimized. In a k-connected (k-fault-tolerant) network there are k
disjoint paths between every two nodes. However, for data collection in sensor networks it is necessary and sufficient that
every sensor node has k disjoint paths to the sink node and k disjoint paths between every two nodes may not be needed.
In [9] the authors studied the problems of all-to-one and one-to-all k-fault-tolerant topology control problems. In these
problems also the sensor nodes are considered to have omni-directional antennas and the network graph is directed. In [10],
authors introduced a new fault tolerance metric called region-based connectivity. In [10], it is assumed that the nodes that
may fail are confined to a region. A region may be defined in several ways based either on the geometric layout of the sensor
nodes in the deployment area or on the network topology. In this paper, we are confining our attention to the fault models
based on the network topology of the sensor network and will study two failure models (i) single node failure and (ii) the
simultaneous failure of two adjacent nodes. Even for these two specific models the problems turn out to be computationally
hard. Assuming that addition of each link to the tree involves a cost, we study the problem of least cost augmentation of the
tree so that even after failure of a single node or a pair of adjacent nodes, all the surviving nodes will remain connected to
the sink node. We prove that the least cost tree augmentation problem is NP-complete under both types of fault scenarios.
Moreover, we provide two approximation algorithms, one for single node failure and the other for a pair of adjacent node
failure, with performance bounds of two and four respectively. The experimental evaluations of the algorithms demonstrate
that they perform even better in practice and almost always produce near optimal solution.
In the theoretical computer science community, problems of this vein are known as the graph augmentation problems.
Two important problems in this class are the bi-connectivity augmentation (BICA) and the bridge-connectivity augmentation
(BRCA) (a bridge is defined to be an edge whose removal disconnects the graph) [11]. Although at a first glance, it may appear
that tree connectivity augmentation under single node fault model, TCA1 , is the same as BICA or BRCA, we demonstrate through
the examples shown in Fig. 2(a) and (b) that TCA1 is distinctly different from both BICA and BRCA. The solid lines in Fig. 2(a)
and (b) are the existing edges in the input graph. A few of the edges that may be added to the graph are shown in dashed
lines and cost of each of these edges is 1. The cost of the edges that can be added to the graph, but not shown in dashed
lines, is 10. In Fig. 2(a), the solution of the TCA1 is the addition of edges {(a, b), (c , d)} with a total cost of 2. However, for

260

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

a

b

c

Fig. 2. (a) Comparison of TCA1 with BICA. (b) Comparison of TCA1 with BRCA. (c) An instance of TCA1 corresponding to a 3DM instance.

BICA, one more edge such as (b, c ) is also needed and as such the total cost will be at least 12. Fig. 2(b) shows an example
in which TCA1 has a solution with cost 3, (addition of edges {(g , h), (h, i), (i, j)}) but BRCA requires the addition of edges
{(a, e), (g , h), (i, j), (d, f )} with a total cost of 4.
In [12], the authors have studied the problem of least cost augmentation of a graph such that there exist at least k + 1
disjoint paths from a root node r to each node v ̸= r. They propose an approximation algorithm with approximation ratio 2
where its time complexity is O(n4 ). Even though the TCA1 problem is a special case of the problem studied in [12], we prove
that TCA1 is still NP-complete. We propose an approximation algorithm for TCA1 with the same approximation ratio of 2 but
O(n2 ) time complexity. To the best of our knowledge the graph augmentation problems under (topological) region based
failures has not been studied before. Hence, towards this direction we study the problem of tree augmentation problem
under two adjacent node failure.
The rest of the paper is organized as follows. In Section 2 we formally define the problems we have studied. In Section 3,
we show that both problems are NP-complete. In Sections 4 and 5 we outline approximation algorithms for the singlenode and adjacent node failure problems, respectively. In Section 6 we report the experimental results and conclude in
Section 7.
2. Problem formulation

Definition: u ⊙ 1 fault tolerant graph—A graph G = (V , E ) with a specified vertex u ∈ V is u ⊙ 1 fault tolerant
if after the failure of any one node v ∈ V − {u}, any residual node w ∈ V − {v} remains connected to the node u.
Definition: u ⊙ 2 fault tolerant graph—A graph G = (V , E ) with a specified vertex u ∈ V is u ⊙ 2 fault tolerant
if it has both the following properties:
(1) G is u ⊙ 1 fault tolerant.
(2) After the failure of any pair of adjacent nodes v, w ∈ V − {u}, any residual node x ∈ V − {v, w} remains connected to
the node u.

Tree Connectivity Augmentation Problem—TSingle Fault (TCA1 ).
Instance: Complete undirected graph G = (V , E ), weight function c (e) ∈ Z+ , ∀e ∈ E, a spanning tree T1 = (V , E1 ) of G
rooted at some node r ∈ V , and a cost budget B.

Question: Is there a set E aug ⊆ E − E1 , such that the graph (V , E1 ∪ E aug ) is r ⊙ 1 fault tolerant and e∈E aug c (e) ≤ B?
Tree Connectivity Augmentation Problem—TAdjacent Double Fault (TCA2 ).
Instance: The same as the instance in TCA1 problem.

Question: Is there a set E aug ⊆ E − E1 , such that the graph (V , E1 ∪ E aug ) is r ⊙ 2 fault tolerant and e∈E aug c (e) ≤ B?
We note that the input graph G is considered to be complete. However, G is weighted and cost (weight) of an edge can
show how costly it can be to select that edge for augmentation. The edges in tree T1 are considered to be originally deployed
and they have cost zero. If it is not possible to add an edge in a network then the edge may have a very large cost.
3. Computational complexity
In this section we show that both the tree connectivity augmentation problems, TCA1 and TCA2 are NP-complete, by a
transformation from the 3-dimensional matching, which is known to be NP-complete [13].
3-Dimensional Matching (3DM).
Instance: A set M ⊆ W × X × Y , where W , X and Y are disjoint sets having the same number q of elements.
Question: Does M contain a matching, that is, a subset M ′ ⊆ M such that |M ′ | = q and no two elements of M ′ agree in any
coordinate?
Theorem 1. TCA1 is NP-complete.
Proof. Let M ⊆ W × X × Y be an instance of 3DM, with |M | = p and W = {wi |i = 1, 2, . . . , q}, X = {xi |i = 1, 2, . . . , q}
and Y = {yi |i = 1, 2, . . . , q}. We start by creating a set of nodes having labels as follows:
– r, where r will be the root of the spanning tree T1 ,
– wi (and similarly xi , yi ) for all wi ∈ W (respectively xi ∈ X , and yi ∈ Y ),

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

261

– for each wi ∈ W (and xi ∈ X , yi ∈ X ), one additional node with label wi′ (respectively x′i and y′i ),
– for each wi ∈ W , one additional node with label wi′′
– for each triple (wi , xj , yk ) ∈ M, two nodes with labels aijk , āijk .
We now create an instance of TCA1 as follows:
V = {r } ∪ {wi , wi′ , wi′′ , xi , x′i , yi , y′i |i = 1, 2, . . . , q} ∪ {aijk , āijk |(wi , xj , yk ) ∈ M }
E = {(u, v)|u, v ∈ V and u ̸= v}
E1 = {(r , wi ), (wi , wi′′ ), (wi′′ , wi′ )|i = 1, 2, . . . , q} ∪ {(r , xi ), (xi , x′i ), |i = 1, 2, . . . , q}
∪{(r , yi ), (yi , y′i )|i = 1, 2, . . . , q} ∪ {(wi , aijk )|(wi , xj , yk ) ∈ M } ∪ {(wi′ , āijk )|(wi , xj , yk ) ∈ M }
B=p+q
c (x′j , āijk ) = c (y′k , aijk ) = c (āijk , aijk ) = 1,

for all (wi , xj , yk ) ∈ M .

All other edges in E have weight 2.
Fig. 2(c) depicts an instance of TCA1 corresponding to the 3DM instance where q = 2 and M = {(w1 , x1 , y1 ),
(w1 , x2 , y2 ), (w2 , x2 , y2 )}. The solid lines show edges in E1 and the dashed lines show the edges in E − E1 with cost 1.
We claim that M contains a matching M ′ iff there is a set E aug of cost no more than B, such that the graph (V , E1 ∪ E aug )
is r ⊙ 1 fault tolerant.
To prove the only if part, let M contain a matching M ′ . We form E aug by following the procedure given below:
Step (i) For each triple (wi , xj , yk ) ∈ M ′ , we add edges (x′j , āijk ) and (y′k , aijk ),
Step (ii) For each triple (wi , xj , yk ) ∈ M − M ′ , we add edge (aijk , āijk ).
Since |M ′ | = q and |M − M ′ | = p − q, and the cost of edges added in Step i and Step ii is 2 and 1 respectively, the total cost
of the added edges in Step i and Step ii is 2q + p − q. Thus, the total cost of the edges in E aug is p + q. E1 ∪ E aug includes the
following cycles that pass through r:

• r , wi , wi′′ , wi′ , āijk , x′j , xj , ∀i, j, k : (wi , xj , yk ) ∈ M ′ ,
• r , wi , aijk , y′k , yk , ∀i, j, k : (wi , xj , yk ) ∈ M ′ ,
• Considering that each wi is in a triple (wi , xj′ , yk′ ) ∈ M ′ and is in previous cycles, there is a cycle r , wi , aijk , āijk , wi′ , āij′ k′ ,
x′j′ , xj′ , ∀i, j, k : (wi , xj , yk ) ∈ M − M ′ .
It can be readily verified that all the nodes in V − {r } appear in at least one of the above cycles. Therefore, there are two
disjoint paths from r to each vertex.
To prove the if part, let there be a set of edges E aug ⊆ (E − E1 ), with cost at most p + q, so that in the graph (V , E1 ∪ E aug )
every non-adjacent vertex of root r has two node disjoint paths to r. There are exactly 2p + 2q leaf nodes in T1 = (V , E1 )
and they are not adjacent to r. Among these leaf nodes, there exists p nodes having labels of the form aijk and āijk and q
nodes having labels of the form y′k and x′j . To ensure that 2p + 2q leaf nodes have two node disjoint paths to r, at least p + q
edges must be added to T1 = (V , E1 ). Since the cost of the edges in E aug is at most p + q and |E aug | ≥ p + q, it implies that
|E aug | = p + q and the cost of each edge in E aug is 1. It may be noted that there are only three types of edges (i) (x′j , āijk ), (ii)
(y′k , aijk ) and (iii) (āijk , aijk ) that have cost 1. In order to have two node disjoint paths from 2p + 2q leaf nodes to r, each node
of the from y′k and x′j must be connected to a node of the form aijk and āijk respectively. The total cost of these set of edges will
be 2q. Since the total cost of E aug is p + q, the cost of the edges to connect the remaining leaves of the type aijk or āijk , (i.e., the
ones that were not connected to either y′k and x′j ), must be p − q and the number of such leaves must be 2(p − q). This will only
be possible, if 2(p − q) leaves can be grouped into p − q pairs of nodes (aijk , āi′ j′ k′ ), such that i = i′ , j = j′ , k = k′ . This implies
that exactly q nodes, each of the form aijk and āijk , must be connected to q nodes, each of the form y′k and x′j , respectively,
and these 2q nodes (aijk and āijk together) can be grouped into q pairs of nodes (aijk , āi′ j′ k′ ), such that i = i′ , j = j′ , k = k′ .
Moreover, since there are two disjoint paths from every node to r, nodes wi , wi′ and wi′′ are in a cycle with just one x′j , xj and
r. Otherwise, there would be a wi′ which is not in any cycle with r and its failure disconnects the graph. Since these q pairs
of ijk indices connects to all the y′k and x′j nodes and wi can be in exactly one cycle with x′j , the corresponding subset M ′ ⊆ M
must be a matching for the instance of the 3DM problem.
Theorem 2. TCA2 is NP-complete.
Proof. The proof is identical to that of Theorem 1 and hence omitted.
4. Tree connectivity augmentation—single fault scenario
In this section we propose an approximation algorithm with a guaranteed performance bound for TCA1 . The input to
the algorithm is a complete undirected graph G1 = (V , E ) with cost function c : E → Z+ defined on the edges, and
T1 = (V , E1 ), a spanning tree of G1 with a specified vertex r ∈ V as the root. The output is a set of edges E aug ⊆ E − E1 with
minimum cost, such that, in the graph (V , E1 ∪ E aug ), there are two node disjoint paths from every node v (v ∈ V − r )
to the node r. Since it is considered that the edges of T1 = (V , E1 ) have already been deployed, we assume that the
cost of the edges in E1 is zero, i.e., c (e) = 0, ∀e ∈ E1 . We compute the edge set E aug using a sequence of steps where,

262

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

a

b

Fig. 3. (a) An example of T1 ; E1 includes the edges shown with solid lines. (b) The tree with solid lines is T2 corresponding to T1 in (a) and dashed lines are
some of the edges in E2′ − E2 .

in each step, we construct a new graph/tree (undirected/directed). The sequence of construction of graphs is as follows:
[T1 = (V , E1 )] ⇒ [T2 = (V2 , E2 )] ⇒ [G2 = (V2 , E2′ )] ⇒ [T2d = (V2 , A2 )] ⇒ [Gd2 = (V2 , A′2 )], where T2 is a tree constructed
from T1 , G2 is a complete graph defined with the vertex set of T2 ; T2d is a directed tree defined on T2 , and Gd2 is a completely
aug
connected directed graph defined with the vertex set of T2d . From Gd2 we identify a set of arcs (directed edges) A2 , so that
aug
aug
aug
the directed graph (V2 , A2 ∪ A2 ) is strongly connected [14]. Finally, we construct E
from A2 . We now describe, in detail,
the construction rules for these graphs/trees.

[A] Construction of T2 : Let Vp ⊂ V be the set of leaves in T1 and let Vq = V − (Vp ∪ {r }) be the set of all internal
(non-leaf) nodes except the root. We define a new tree T2 = (V2 , E2 ) using the following rules:
• V2 = V ∪ {vij |i, j ∈ V − {r } and (i, j) ∈ E1 }.
• For each edge (i, j) ∈ E1 , we include in E2 ,
– edge (i, j), if i = r or j = r,
– edges (i, vij ) and (vij , j), otherwise.
[B] Construction of G2 : Let G2 = (V2 , E2′ ) be the complete graph defined on V2 . We define the cost function
c ′ : E2′ → Z+ ∪ {∞} as follows. For every edge (x, y) ∈ E2′ , if x, y ∈ V , c ′ (x, y) = c (x, y); otherwise, c ′ (x, y) = ∞
(i.e., we set the initial cost of the edges between a node u ∈ V2 − V and every other node in V2 to infinity).
Next we define two functions d(u, v) (distance function) and p(u, v) (pointer function) for every pair of nodes u and v in G2 .
We define both the functions in terms of c ′ and T2 .

Definition: d(u, v) = min{c ′ (x, y)|u and v are on the path from x to y in T2 }.
Definition: p(u, v) is a pointer to an edge (s, t ), such that d(u, v) = c ′ (s, t ), where u and v are on the path from s to t in
T2 . If there are more than one such edge, any one of them can be selected as the value of p(u, v).
Example: Fig. 3(a) shows a spanning tree T1 = (V , E1 ) of a complete graph G1 = (V , E ) (for the sake of clarity, only
three edges from the set E − E1 , (a, b), (c , d) and (d, e) are shown in Fig. 3(a)). The solid lines indicate the edges in E1
and the dashed lines show a subset of the edges in E − E1 . The cost of each edge in E1 = 0. The weights associated with
the dashed lines indicate the cost of these edges. All other edges in E − E1 , (not shown in Fig. 3(a)), have a cost of 10.
Fig. 3(b) shows the tree T2 = (V2 , E2 ) constructed from T1 in Fig. 3(a). From T2 , we can construct the complete graph
G2 = (V2 , E2′ ) following the construction rules described earlier. The solid lines in Fig. 3(b) indicate the edges in E2 and
the dashed lines show a subset of the edges in E2′ − E2 (only five edges with associated weights are shown). In this example,
d(vac , vad ) = c ′ (c , d) = c (c , d) = 1, d(a, b) = c ′ (d, e) = c (d, e) = 1 and p(vac , vad ) = (c , d), p(a, b) = (d, e).
We now discuss the rationale for definitions of the d(u, v) and p(u, v) given above. In order to have another path from a to
b in Fig. 3(a), (different from the one in the tree T2 , a − r − b), the edge (d, e) with cost 1 or the edge (a, b) (in G2 ) with cost
4 can be added to the tree. The addition of the link (d, e) will result in a cheaper path from a to b with cost 1. The goal of the
distance function d(u, v) is to identify this edge. The function p(u, v) is defined to be a pointer to the edge selected by the
function d(u, v). We note that since the cost of the edges
Computation of d(u, v): It has been shown in [11] that d(u, v) for all pairs of nodes can be computed in O(|V |2 ).
For ease of reference, we summarize the algorithm for computing the function d(u, v) presented in [11]. Initially, for
every pair of nodes u, v ∈ V2 , d(u, v) = c ′ (u, v) and p(u, v) = (u, v). Let l(u, v) be the number of edges on the path
from u to v in T2 and s(u, v) be the node adjacent to v on this path. The edges (u, v) ∈ E2′ − E2 are sorted in nondecreasing order, based on l(u, v). For each edge (u, v) in the sorted order, we compute the distance function d(u, v) as
follows. If d(u, v) < d(u, s(u, v)) then d(u, s(u, v)) = d(u, v) and p(u, s(u, v)) = (u, v). If d(u, v) < d(s(v, u), v), then
d(s(v, u), v) = d(u, v) and p(s(v, u), v) = (u, v).
[C] Construction of T2d = (V2 , A2 ): We construct T2d from T2 = (V2 , E2 ) by directing all edges of T2 towards the root
node r. We will use A2 to represent the set of arcs (directed edges) corresponding to the undirected edges in E2 .

[D] Construction of Gd2 = (V2 , A′2 ): Gd2 is a completely connected directed graph, with associated cost c ′′ (u, v) with
each arc u → v ∈ A′2 as follows:

∞,
if v = r ,
′′
if u ∈ Vq and v ∈ subtree(u)
c (u, v) = ∞,
d(u, v), otherwise.
Here we define subtree(u) to be the set of nodes in the subtree rooted at node u in tree T2 .

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

263

We note that each arc u → v ∈ A2 where v ̸= r has a zero cost. The rationale for assigning the arc costs in this specific way
is as follows:
(a) By assigning a cost of ∞ to the edges where v = r, we ensure that the minimum cost arborescence on Gd2 is rooted at r.
(b) By assigning a cost of ∞ to the edges (u, v) where u ∈ Vq and v ∈ subtree(u) in T2 , we ensure that, in Gd2 , no node u ∈ Vq
will have a directed path from u to the nodes in subtree(u), unless it first goes through some nodes not in subtree(u).
aug

aug

aug

When constructing Gd2 = (V2 , A′2 ), our goal is to identify a set of arcs A2 , (A2 ⊆ A′2 ), so that the graph (V2 , A2 ∪ A2 ) is
aug
strongly connected, i.e., there exists a directed path between every pair of nodes in V2 . We obtain the arc set A2 by computing
aug
d
arb
arb
′
the least-cost arborescence [15] in G2 = (V2 , A2 ), which we denote by T2 = (V2 , A2 ). We obtain the set of arcs A2 from
aug
arb
arb
arb ′′
A2 by excluding those arcs with cost zero, i.e., A2 = A2 − {a ∈ A2 |c (a) = 0}. Finally, we construct the set of edges
E aug that we have to add to the input tree T1 = (V , E1 ), to obtain the r ⊙ 1 fault tolerant graph (V , E1 ∪ E aug ) as follows:
aug
aug
E aug = {p(u, v)|u → v ∈ A2 }. We note that if A2 has any arc whose endpoint is in V2 − V , using the function p(u, v) it
will be replaced by an edge in E. The reason is that c ′ (u, v) = ∞ if u or v is in V2 − V .
Algorithm 1 TCA1 Algorithm
Input: G1 = (V , E ), a complete graph with cost c (e) for every edge e ∈ E; T1 = (V , E1 ), a spanning tree of G1 with root r.
Output: A set of edges E aug ⊆ E − E1 , such that (V , E1 ∪ E aug ) is r ⊙ 1 fault tolerant.
′
′
1: Construct T2 = (V2 , E2 ), a complete graph G2 = (V2 , E2 ) and the cost function c (.) from T1 and G1 using the technique
described in [A].
2: Compute d(u, v) and p(u, v) for each pair of nodes u, v ∈ V2 using the technique described in [B].
d
3: Compute a directed tree T2 = (V2 , A2 ) by directing all edges in E2 towards root r using technique described in [C].
d
′
′
′′
4: Compute a completely connected directed graph G2 = (V2 , A2 ) with cost c defined on the arcs set A2 using technique
described in [D].
arb
arb
d
′
5: Compute a minimum cost arborescence T2 = (V2 , A2 ) of the graph G2 = (V2 , A2 ).
aug
arb
arb ′′
6: Set A2 = A2 − {a ∈ A2 |c (a) = 0}.
aug
7: Set E
= {p(u, v)|u → v ∈ Aaug
2 }.
aug
8: Return E
.
We note that the time complexity of the TCA1 algorithm (Algorithm 1) is O(|V |2 ). Line 4 has O(|V |2 ) time complexity.
Finding minimum cost arborescence also needs O(|V |2 ) time [15].
Theorem 3. Algorithm TCA1 finds a set of edges E aug such that (V , E1 ∪ E aug ) is r ⊙ 1 fault tolerant.
Proof. In order to prove that (V , E1 ∪ E aug ) is r ⊙ 1 fault tolerant, we need to show that there is no node in Vq whose
removal disconnects the graph (Vq is the set of all internal nodes in the tree T1 = (V , E1 ) except the root r). Since the graph
d
arb
(V2 , A2 ∪ Aaug
(excluding the arcs that are already
2 ) is constructed by augmenting T2 = (V2 , A2 ) with the arcs of the T2
in A2 ), it must be strongly connected. Accordingly, there must be a directed path from any node v ∈ Vq to the nodes in
subtree(v). Let w ̸= v be a node in subtree(v). Since there is no directed edge ∈ A′2 going out of v to the nodes in subtree(v)
(because the cost of these edges is infinity), the first arc in a path from v to any other node in subtree(v) should go through
aug
a node s where s is not in subtree(v). Since the graph (V2 , A2 ∪ A2 ) is strongly connected, there must be a path from s to w
aug
aug
in (V2 , A2 ∪ A2 ) not including v . Suppose that c → d ∈ A2 is on the path from s to w which does not include v . If (c , d)
is replaced by p(c , d) = (e, f ), (there is a path going through e, c , d, f in order in T2 ), the new path also will not include
v , because v cannot be on the path from c to e or f to d in T2 . Hence, even if a node v ∈ Vq is removed from the graph
(V , E1 ∪ E aug ), the graph remains connected. Therefore, (V , E1 ∪ E aug ) is r ⊙ 1 fault tolerant.
Theorem 4. Algorithm TCA1 finds a set of edges E aug with a total cost C aug , such that C aug ≤ 2C opt , where C opt is the cost of the
optimal solution.
Proof. Our proof strategy is as follows. Let C opt be the optimal cost of edges E opt necessary to add to the input tree
T1 = (V , E1 ), so that the resulting graph becomes r ⊙ 1 fault tolerant and C aug is the cost of edges E aug selected by the
TCA1 Algorithm. We show that there exists a subset of 
arcs A′′ in the graph Gd2 = (V2 , A′2 ) with three useful properties. If C ′′
′′
opt
is the cost of the arcs in A′′ , A′′ ⊆ A′2 − A2 , (i.e., C ′′ =
≥ C ′′ /2, (ii) C aug ≤ C ′′ , and (iii) the
u→v∈A′′ c (u, v)), then (i) C
′′
aug
opt
graph (V2 , A2 ∪ A ) is strongly connected. From (i) and (ii) it follows that C
≤ 2C .
We can compute the set of arcs A′′ ⊆ A′2 − A2 from the optimal solution E opt ⊆ E − E1 following the procedure described
below.
Let Q be the set of nodes that are strongly connected in (V2 , A2 ∪ A′′ ). Initially, we set A′′ = ∅, Q = {r } and mark all the
edges in E opt as unused. We update Q , A′′ and the marking of the edges in E opt using the following procedure:
While Q ̸= V2 repeat the following steps:

• Select an unused edge (u, v) from E opt , such that there is a node t ∈ Q − Vq on the weakly directed path from u to v in
T2d = (V2 , A2 ). (The weakly directed path from u to v in T2d is the path from u to v in T2d in which the direction of the arcs
is ignored.)

264

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

• If t ̸= u, add t → u to A′′ and if t ̸= v add t → v to A′′ .
• Add all the nodes on the weakly directed path from u to v in T2d to Q . Since t has been selected from set Q , it is already
accessible from root r in (V2 , A2 ∪ A′′ ). Therefore, by adding the new arcs to A′′ in step (ii) all the nodes on the weakly
directed path from u to v in T2d are now accessible from root r in current augmented directed graph (V2 , A2 ∪ A′′ ).
• Change the marking of the edge (u, v) from unused to used.
We need to show that, during the execution of the iterative process, an unused edge (u, v) ∈ E opt and a suitable vertex
t ∈ Q − Vq exist. While there are some edges in E opt which have not been used previously, there is still some node w in Vq
whose deletion disconnects some node a ∈ subtree(w) from the root in (V2 , A2 ∪ A′′ ). So, there is no directed path from r
to a in (V2 , A2 ∪ A′′ ) during that iteration. Therefore, there should be an edge (u, v) in E opt which creates a path from r to
a ∈ subtree(w) such that the path does not include w in (V , E1 ∪ E opt ). Also, the path from u to v in T1 will contain more
nodes from Q than just one vertex from Vq ; otherwise, the removal of that vertex would disconnect the graph (V , E1 ∪ E opt )
which contradicts the fact that (V , E1 ∪ E opt ) is r ⊙ 1 fault tolerant. Since there is no directed edge, in T2d , between the nodes
in Vq , the weakly directed path from u to v in T2d 
should include a node t ∈ Q − Vq .
′′
opt
Let C ′′ be the cost of the arcs in A′′ ; C ′′ =
, we have to add at most
u→v∈A′′ c (u, v). For every edge (u, v) ∈ E
t

two arcs t → u and t → v to A′′ . Because c ′′ (t , u) ≤ d(u, v) and c ′′ (t , v) ≤ d(u, v), C ′′ ≤ 2C opt . Also we know
that the graph (V2 , A2 ∪ A′′ ) is strongly connected. We can, therefore, construct an arborescence on (V2 , A2 ∪ A′′ ) rooted
at r using c ′′ as the cost of the edges. Since the TCA1 algorithm gives us the minimum cost arborescence on Gd2 and
A2 ∪ A′′ ⊆ A′2 , C aug ≤ C ′′ ≤ 2C opt .
5. Tree connectivity augmentation—adjacent double fault scenario
In this section, we propose an approximation algorithm for the TCA2 problem. The input to the algorithm is a complete
undirected graph, G1 = (V , E ), with cost function c : E → Z+ and a spanning tree T1 = (V , E1 ) rooted at a specific node
r ∈ V . The output is a set of edges E aug ⊆ E − E1 with minimum cost, such that the graph (V , E1 ∪ E aug ) is r ⊙ 2 fault tolerant.
The cost of the edges in E1 is considered to be zero. Since in our model we assume that r does not fail, we exclude the
possibility of two adjacent node failures when one of them is r. We define st (u, T ) to denote the set of nodes in the subtree
rooted at a node u in a tree T . If T is directed we just ignore the direction of edges. With respect to the tree T1 , we define
par (u) and s(u) to denote u’s parent and the set of u’s siblings, respectively. Also, we use [u, v] to denote two adjacent nodes
u and v in T1 , where u = par (v) and u ̸= r. When [u, v] fails, the surviving nodes in st (u, T1 ) get disconnected from r. Hence,
we need to augment the tree T1 such that in the augmented graph there is a path from every node w ∈ st (u, T1 ), w ̸= u, v
to r and the path includes neither u nor v . In the tree augmentation algorithm TCA2 , the tree T1 is augmented in two phases.
aug
aug
In phase I, we find a set of edges E1 ⊆ E − E1 such that the graph (V , E1 ∪ E1 ) has the following properties; (1) after the
failure of any two adjacent nodes [u, v] the remaining nodes in st (v, T1 ) will have a path to r or at least to one of the nodes in
s(v), (2) all the nodes not directly connected to the root have two node disjoint paths to r. We note that completing phase I
aug
may not ensure the connectivity of nodes in s(v) to r after the failure of [u, v] and hence (V , E1 ∪ E1 ) may not be r ⊙ 2 fault
aug
aug
aug
tolerant. In phase II, we add another set of augmenting edges E2 ⊆ E − (E1 ∪ E1 ) to the graph (V , E1 ∪ E1 ), such that
aug
aug
the failure of any two adjacent nodes [u, v] does not disconnect the nodes in s(v) from r. Thus, in graph (V , E1 ∪ E1 ∪ E2 ),
after the failure of a [u, v] the remaining nodes will be connected to the root r.
In phases I and II, we only consider the failures of nodes that are adjacent in T1 . However, the edges that we added during
phase I and phase II of TCA2 introduce the possibility of additional failures, as two nodes i and j, that were previously not
adjacent, will become adjacent after the augmentation process. We will explain later how we deal with this possibility and
ensure that the augmented graph is r ⊙ 2 fault tolerant.
5.1. TCA2 Phase I
aug

aug

In this phase we propose an algorithm to find a set of edges E1 ⊆ E − E1 such that in graph (V , E1 ∪ E1 ) for every [u, v]
and node w ∈ st (v, T1 ), w has a path to the root or to a node in s(v) that does not include either u or v . We use an approach
similar to that in Section 4 for the TCA1 algorithm where we created a sequence of graphs to find the augmenting edges. In
this case, since we need to deal with the failures of two adjacent nodes, the algorithm and the proofs are more complicated.
First, we compute d(u, v) and p(u, v) functions defined in Section 4 [B] for every pair of nodes u and v ∈ V using T1 and
c as inputs, so that d(u, v) = min{c (x, y)|u and v are in the path from x to y in T1 } and p(u, v) is the pointer to the edge
selected by the function d(u, v). Next, we construct a new directed tree T2d and directed graph Gd2 as follows.

[A] Construction of T2d = (V2 , A2 ): We construct T2d from T1 by directing all edges of T1 towards the root node r. Then
for every three consecutive nodes k → j → i where i ̸= r we add a new node vijk between j and k and replace the directed
edge k → j with k → vijk and vijk → j; i.e., V2 = V ∪ {(vijk )|i, j, k ∈ V and j = par (k) and i = par (j) and i ̸= r } and
A2 = {j → i|i = par (j) and (i = r or par (i) = r )} ∪ {k → vijk , vijk → j|j = par (k) and i = par (j) and i ̸= r }. In Fig. 4(a) the
tree formed by solid lines is an example of T1 and the tree with solid arrows in Fig. 4(b) is the corresponding T2d .

[B] Construction of Gd2 = (V2 , A′2 ): Gd2 is a directed graph on V2 . We use another pointer function p′ : (A′2 − A2 ) → E,
where p′ (i, j) is an undirected edge in E corresponding to the arc i → j ∈ (A′2 − A2 ). We compute A′2 , p′ and the cost of the

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

a

b

265

c

Fig. 4. (a) An example of T1 ; solid lines show the edges in E1 and dashed ones show a subset of edges in E − E1 . (b) T2d corresponding to T1 in part (a); solid
arrows show the arcs in T2d and dashed ones show a subset of arcs in A′2 − A2 . (c) T2arb , minimum cost arborescence computed on Gd2 corresponding to part (b).

a

b

c

aug

Fig. 5. (a) Solution of TCA2 Phase I, i.e., (V , E1 ∪ E1 ). (b) Solid arrows form the tree T3d corresponding to the example shown in Fig. 4. (c) T3arb .

arcs in A′2 , c ′ in the following way:
(i) For every edge i → j ∈ A2 :
• If j ̸= r, add i → j to A′2 , c ′ (i, j) = 0.
• Else add j → i to A′2 , c ′ (j, i) = 0, p′ (j, i) = (i, j).
(ii) For every edge (i, j) ∈ E − E1 :
• If j ∈ st (i, T1 ) and i ̸= r, add vikl → j to A′2 where there is a directed path from j to vikl in T2d . In this case,
p′ (vikl , j) = (i, j), c ′ (vikl , j) = d(i, j).
• Else if j ̸= r, add i → j to A′2 , p′ (i, j) = (i, j), c ′ (i, j) = d(i, j).
(iii) Step (ii) is repeated by interchanging i and j.
In Fig. 4(a) dashed lines show a subset of edges in E − E1 and the value on every edge (i, j) shows its cost c (i, j). Other
edges in E − E1 that are not shown in the figure are supposed to have cost 10. In Fig. 4(b) a subset of arcs in A′2 − A2 are
shown in dashed arrows. We note that the dashed arrow with cross on it means that the arc a → g is not in A′2 and instead
vacg → g is in A′2 and p′ (vacg , g ) = (a, g ).
aug
Similar to the algorithm TCA1 we augment the directed tree T2d with some additional arcs A2
⊆ A′2 such that the
aug
aug
augmented graph (V2 , A2 ∪ A2 ) is strongly connected. In order to compute A2 , we compute the minimum cost arborescence
arb
rooted at r on graph Gd2 = (V2 , A′2 ) using c ′ and denote it by T2arb = (V2 , Aarb
corresponding
2 ). Fig. 4(c) depicts T2
aug
arb
to the example depicted in 4(a). The set of arcs A2 is obtained by excluding from A2 those arcs with cost zero, i.e.,
aug
aug
aug
′
is obtained by including in the set E1 the edge
= Aarb
− {a ∈ Aarb
A2
2
2 |c (a) = 0}. Finally, the set of edges E1
aug
aug
aug
′
′
= {p(p (u, v))|u → v ∈ A2 }. In the example shown in Fig. 4, we get
p(p (u, v)) for each u → v ∈ A2 , i.e., E1
aug
aug
E1 = {(r , c ), (c , d), (d, i), (g , i), (h, i), (r , j), (e, f ), (j, k)}. The augmented graph (V , E1 ∪ E1 ) is shown in Fig. 5(a). The
steps of TCA2 Phase I are summarized in Algorithm 2 Phase I.
aug

aug

Theorem 5. Algorithm TCA2 Phase I finds a set of edges E1 such that in the graph (V , E1 ∪ E1 ) if any two adjacent nodes
[i, j] ∈ T1 fail, the remaining nodes in st (j, T1 ) have a path to r or to a node in s(j). The graph is also r ⊙ 1 fault tolerant.
aug

Proof. Let Gtmp = (V , E1 ∪ Etmp ) be an undirected graph on V where Etmp = {p′ (u, v)|u → v ∈ A2 }. Consider the two
adjacent nodes [i, j] in T1 such that their removal disconnects st (k, T1 ) from r where k is one of the j’s children. We know
aug
aug
that (V2 , A2 ∪ A2 ) is strongly connected. So, there should be a directed path from j to k in (V2 , A2 ∪ A2 ). Since there is no
d
′
arc in A2 from j to the nodes in st (j, T2 ), there should be a directed path, P, from j to k such that the node following j on P,
called f is not in st (j, T2d ) and the subpath from f to k does not include j. There are three possibilities for f : (i) f can be vhij .
The node after vhij on P can be either i or a node a ∈ st (j, T2d ). If it is i, as there is no directed edge from i to its subtree in
A′2 the next node on P is some node ̸∈ st (i, T2d ). So, in this case there is a directed path from some node ̸∈ st (i, T2d ) to k that
includes neither i nor j. In second case there will be a directed path from vhij to k which does not include i and j. Because of
the way p′ is computed the edge from vhij to a is replaced by h (i’s parent) to a in Gtmp . Hence, in both cases, in Gtmp there will
be a path from r to k that includes neither i nor j. (ii) f can be a node ̸∈ st (i, T2d ). Then there is a path from f to k that does not

266

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

include either i or j. (iii) f can be a node in a subtree of a j’s sibling ∈ s(j). We denote this j’s sibling by b. Then there will be
a subpath from f to k that the path includes neither i nor j. In this case in Gtmp after removal of [i, j], nodes ∈ st (k, T1 ) have
a path to b. So, in all cases, when [i, j] fails, st (j, T1 ) is connected to r or to a node ∈ s(j) in Gtmp .
Now it remains to show that after replacing (w, x) in Etmp with p(w, x), k still remains connected to r or a node ∈ s(j).
Let p(w, x) = (y, z ); If (w, x) is on a path from k to r or k to j’s sibling that eliminates i and j as two adjacent node failure in
Gtmp , then definitely i and j cannot be on the path from w to y and z to x in T1 . Therefore, after replacing (w, x) in Etmp with
p(w, x), k has a path to root or a j’s sibling not including i and j.
aug

Theorem 6. Let C1
for TCA2 problem.

aug

aug

be total cost of the edges in E1 . We claim that C1

≤ 2C opt where C opt is the cost of the optimal solution

Proof. In order to prove the theorem, we find a set of arcs A′′1 ⊆ A′2 using the edges in the 
optimal solution E opt such that
′
graph (V2 , A2 ∪ A′′1 ) is strongly connected and total cost of the arcs in A′′1 , C1′′ ≤ 2C opt (C1′′ =
u→v∈A′′ c (u, v)).
1

opt

Let E1 = E opt −{(i, l)|i = par (par (l)) and i ̸= r } and Q1 be the set of nodes which are strongly connected in (V2 , A2 ∪ A′′1 ).
opt
Initially, Q1 = {r }, A′′1 = ∅ and all the edges in E1 are marked unused. We use the following procedure to compute A′′1
and Q1 .
While Q1 ̸= V2 repeat the following steps:

• Select an unused edge (i, j) from E1opt such that one of the following conditions holds and update A′′1 and Q1 accordingly:
– The lowest common ancestor of nodes i and j, LCA(i, j), in T1 is r (LCA(i, j) in a tree is the shared ancestor of i and j that
is located farthest from the root. A node is allowed to be an ancestor of itself). If r ̸= i add the directed edge r → i and
if r ̸= j add r → j to A′′1 . Also, add all the nodes on the weakly directed path from i to j in T2d to Q1 since these nodes are
opt
strongly connected with root r in current (V2 , A2 ∪ A′′1 ). At least one of the edges in E1 has this condition. Otherwise,
in (V , E1 ∪ Eopt ) none of the nodes could have two disjoint paths to r which is a contradiction. We note that because
of the way we compute cost of the arcs in A′ , c ′ (r , i) ≤ c (i, j) and c ′ (r , j) ≤ c (i, j).
– i ∈ Q1 , j ̸∈ st (i, T1 ) and j ̸= r. Add i → j to A′′1 and add all the nodes on the weakly directed path from j to i in T2d to Q1 .
(This condition should be checked for the case that i and j are interchanged.)
– LCA(i, j) = k, k ∈ Q1 and k ̸∈ {r , i, j}. There is a node v ∈ Q1 ∩ V such that par (v) = k and i or j is in st (v, T1 ). Let
i ∈ st (v, T1 ). Add arc v → j and if v ̸= i, add j → i to A′′1 . Since v is on the path from i to j in T1 , c ′ (v, j) ≤ c (i, j). Then,
add the nodes on the weakly directed path from i to j in T2d to Q1 .
– LCA(i, j) = k, k ∈ Q1 , k ̸= r and k ∈ {i, j}. There is a node vklm ∈ V2 − V such that i or j is in st (vklm , T2d ) and vklm ∈ Q1 .
Let i ∈ st (vklm , T2d ). Add arc vklm → i to A′′1 . Obviously c ′ (vklm , i) = d(k, i) ≤ c (i, j). Then, add the nodes on the weakly
directed path from i to j in T2d to Q1 .
• Change the marking of the edge (i, j) to used.
Now we need to show that while Q1 ̸= V2 there is some unused edge (i, j) such that one of the conditions in the procedure
opt
holds for it. Let Eused ⊆ E1 be the set of edges marked used during the procedure. Assume that Q1 ̸= V2 ; so, there exists
some node x ∈ V2 ∩ V that is not reachable from r. Definitely, the nodes ∈ st (x, T2d ) are not reachable from r either. If x has
some sibling y in T1 , that y ∈ Q1 then there is no directed path from y to x in (V2 , A2 ∪ A′′1 ); otherwise, x would be reachable
from r and it would be ∈ Q1 . Also, the directed path from x to y in (V2 , A2 ∪ A′′1 ) can only go through par (x), w , since during
the procedure only directed edges from nodes ∈ Q1 to other nodes can be added. In this case if [par (w), w] fails, in the graph
(V , E1 ∪ Eused ), x gets disconnected both from r and y. So, there should be some unused edge (i, j) ∈ E1opt that x is on the path
from i to j in T1 and one of the above conditions holds for it. Otherwise, it contradicts with E opt to be the optimal solution.
If y ̸∈ Q1 then similarly it can be concluded that in the graph (V , E1 ∪ Eused ) failure of [par (w), w] disconnects both x and y
opt
from r and from each other. So, there should be some unused edges ∈ E1 that one of the above conditions holds and make
x and y connected to the root or one of their siblings.
opt
Based on this procedure, for every edge (i, j) in E1 at most two arcs from A′2 are added to A′′1 such that each one has
′′
cost ≤c (i, j). Therefore, total cost of the arcs in A1 , C1′′ ≤ 2C opt . We know that (V2 , A2 ∪ A′′1 ) is strongly connected. So, on
(V2 , A2 ∪ A′′1 ) we can construct an arborescence tree using c ′ as the cost of the edges. Since (A2 ∪ A′′1 − {i → r |i ∈ V }) ⊆ A′2
aug
and in TCA2 Phase I we compute minimum arborescence tree on (V2 , A′2 ), C1 ≤ C1′′ ≤ 2C opt .
5.2. TCA2 Phase II
aug

After adding the edges E1 obtained in TCA2 Phase I to E1 it may still happen that the failure of two adjacent nodes [i, j]
disconnects some nodes in s(j) from root r. It may also disconnect the nodes in st (j, T1 ) from r, but based on Theorem 5
aug
these nodes are connected to at least a node in s(j) in graph (V , E1 ∪ E1 ). Hence, we need to find a new set of edges
aug
aug
aug
aug
E2 ⊆ E − (E1 ∪ E1 ) such that in graph (V , E1 ∪ E1 ∪ E2 ) after the failure of any two adjacent nodes [i, j], nodes in s(j)
remain connected to r.
aug
We define a cost function c ′′ for (i, j) ∈ E in the following way. If (i, j) ∈ E1 then c ′′ (i, j) = 0, else c ′′ (i, j) = c (i, j).
Again we need to compute functions d(i, j) and p(i, j) defined in Section 4 [B] for every pair of nodes u and v ∈ V but using

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

267

T1 and c ′′ as input; i.e., d(u, v) = min{c ′′ (x, y)|u and v are in the path from x to y in T1 } and p(u, v) is the pointer to the edge
selected by the function d(u, v).
aug
Let S = {[i, j]|i ̸= r , i = par (j) in T1 and failure of [i, j] disconnects the graph (V , E1 ∪ E1 )}. If S is empty it means that
aug
aug
there is no [i, j] whose removal disconnects (V , E1 ∪ E1 ) and E2 will be ∅. Otherwise, for every [i, j] ∈ S we define a set
aug
Dj to be the set of connected components that do not include r in graph (V , E1 ∪ E1 ) caused by failure of [i, j]. Let Cjk be
the kth component in Dj . We note that each component Cjk ∈ Dj includes at least a node from s(j). Now, we construct a new
directed tree T3d and directed graph Gd3 as follows.

[A] Construction of T3d = (V3 , A3 ): We construct T3d from T1 by directing all edges of T1 towards the root node r. Then
for every two consecutive nodes j → i where i ̸= r we add a new node uij and replace the directed edge j → i with j → uij
and uij → i; For every [i, j] ∈ S and every component Cjk ∈ Dj we add two new nodes xjk and yjk to V3 and add the arcs
xjk → yjk and yjk → j to A3 .
The solid arrows in Fig. 5(b) shows the directed tree T3d corresponding to the example in Fig. 4. It can be seen that there
aug
are two nodes xi1 and xi2 associated with node i. The reason is that after failure of [c , i] in (V , E1 ∪ E1 ) nodes g and h get
disconnected from r and they are in two separate components.

[B] Construction of Gd3 = (V3 , A′3 ): Let p′′ : (A′3 − A3 ) → E be a pointer function where p′′ (i, j) is an undirected edge in
E pointing to an arc i → j ∈ (A′3 − A3 ) and c ′′′ (i, j) be the cost of the arcs in A′3 . We compute A′3 and c ′′′ through the following
procedure:
(i) For every edge i → j ∈ A3 :
• If j ̸= r, add i → j to A′3 , c ′′′ (i, j) = 0.
• Else add j → i to A′3 , c ′′′ (j, i) = 0, p′′ (j, i) = (i, j).
(ii) For every edge (i, j) ∈ E − E1 :
• If j ∈ st (i, T1 ) and i ̸= r add one of the following arcs to A′3 : Let k be the i’s child where j ∈ st (k, T1 ) in T1 .
aug
– If failure of [i, k] disconnects j from r in (V , E1 ∪ E1 ), i.e., j is in a component Ckl in Dk , add ykl → j to A′3 ;
′′
′′′
p (ykl , j) = (i, j); c (ykl , j) = d(i, j);
– else, add uik → j to A′3 ; p′′ (uik , j) = (i, j) and c ′′′ (uik , j) = d(i, j).
• Else if par (j) ̸= r , j ̸= r and failure of [par (j), j] disconnects i from r in (V , E1 ∪ E1aug ), add i → xjz to A′3 where i ∈ Cjz
for a component number z. Also, p′′ (i, xjz ) = (i, j) and c ′′′ (i, xjz ) = d(i, j).
• Else if par (i), i ̸= r and removal of [par (i), i] from (V , E1 ∪ E1aug ) disconnects j from r, add xiw → j to A′3 where j ∈ Ciw
for a component number w ; Also, p′′ (xiw , j) = (i, j) and c ′′′ (xiw , j) = d(i, j).
• Else if j ̸= r add i → j to A′3 and p′′ (i, j) = (i, j) and c ′′′ (i, j) = d(i, j).
(iii) Repeat step (ii) by interchanging i and j.
aug

In Fig. 5(b) the dashed lines without a cross show a subset of the arcs in A′3 − A3 . In graph (V , E1 ∪ E1 ) the failure of
[a, c ] disconnects c’s children from r but they remain connected to d. Therefore, corresponding to the edge (a, g ) ∈ E − E1
the arc yc1 → g is added to A′3 and there is no arc in A′3 from the node uac to its subtree. However, after the failure of [b, e], j
and k remain connected to r. So, the edge ube → j and ube → k are in A′3 corresponding to the edges (b, j) and (b, k) in E − E1
respectively.
aug
aug
aug
We augment T3d with a set of additional edges A3 such that (V3 , A3 ∪ A3 ) is strongly connected. A3 is obtained
arb
arb
d
arb
by computing minimum cost arborescence tree T3
= (V3 , A3 ) on G3 (Fig. 5(c)). T3 is rooted at r as in A′3 there is
aug
the arcs with cost zero; i.e.,
no edge from the nodes in V3 to r. The set of arcs A3 is obtained by excluding from Aarb
3
aug
aug
aug
′′′
is obtained by including in the set E2 the value
− {a ∈ Aarb
A3
= Aarb
3 |c (a) = 0}. Finally, the set of edges E2
3
aug
aug
aug
= {p(p′′ (u, v))|u → v ∈ A3 }. In the example depicted in Fig. 5,
of p(p′′ (u, v)) for each u → v ∈ A3 , i.e., E2
aug
aug
A3 = {j → i, ube → k, k → f } and using functions p′′ and p, we have E2 = {(i, j), (k, f )}.
aug

aug

Theorem 7. Algorithm TCA2 Phase II finds a set of edges E2 such that in the graph (V , E1 ∪ E1
two adjacent nodes [i, j] the remaining nodes still have path to r.
aug

∪ E2aug ) after removal of any

Proof. Based on Theorem 5, in graph (V , E1 ∪ E1 ) if [i, j] fails, the nodes in st (j, T1 ) remain connected to r or at least to
aug
aug
one of the nodes in s(j). Now, we prove that in (V , E1 ∪ E1 ∪ E2 ) after failure of [i, j], nodes in s(j) still have a path to r.
aug
aug
Let Gtmp = (V , E1 ∪ E1 ∪ Etmp ) be an undirected graph where Etmp = {p′′ (u, v)|u → v ∈ A3 }. Now, consider [i, j] whose
aug
failure disconnects j’s sibling, k, from r. We know that (V3 , A3 ∪ A3 ) is strongly connected. Hence, there should be a directed
aug
path from j to k in (V3 , A3 ∪ A3 ). Because of the way we defined the arcs in A′3 we know that there is no arc in A′3 from j to
aug
d
any node in st (k, T3 ) and no edge from j to any other node that in (V , E1 ∪ E1 ) gets disconnected by failure of [i, j]. Also,
d
′
we know there is no edge in A3 from j to the nodes in st (j, T3 ). So the node following j, f on the directed path from j to k is
either some node out of st (i, T3d ) or f is uij . In first case, in Gtmp there will be a path from some node w out of st (i, T1 ) to k
that the path includes neither i nor j. Since w ̸∈ st (i, T1 ) after removal of [i, j], it remains connected to r in Gtmp . Therefore,
k remains connected to r in Gtmp . In second case, the node after uij is some node t ∈ st (j, T1 ) such that the subpath from t to
k includes neither i nor j. Because of the way we defined A′3 , there is an edge in A′3 from uij to t if t remains connected to the

268

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271
aug

root in (V , E1 ∪ E1 ) when [i, j] fails. So, there should be a path from t to r in Gtmp where it includes neither i nor j. So, in Gtmp
there is a path from k to r which eliminates i and j. Similar to the proof of Theorem 5, it can be shown if edge (a, b) ∈ Etmp
is on a path from k to r in Gtmp such that the path eliminates [i, j] (k ∈ st (i, T1 )), after replacing it by p(a, b) = (w, z ), there
will be a new path that includes neither i nor j. The reason is that i and j cannot be on the path from a to w and z to b in T1 .
Algorithm 2 TCA2 Algorithm
Input: The same as input in Algorithm 1.
Output: A set E aug ⊆ E − E1 such that (V , E1 ∪ E aug ) is r ⊙ 2 fault tolerant.
Phase I
1: Compute d(u, v) and p(u, v) for each pair of nodes u, v ∈ V using T1 and c as the inputs.
d
2: Compute a directed tree T2 = (V2 , A2 ) using the technique described in [A] in Section 5.1
d
′
′
′
3: Compute a directed graph G2 = (V2 , A2 ), functions c (.) and p (.) using the technique described in [B] in Section 5.1.
arb
arb
d
′
4: Compute a minimum cost arborescence T2 = (V2 , A2 ) of the graph G2 = (V2 , A2 ).
aug
arb
arb ′
5: Set A2 = A2 − {a ∈ A2 |c (a) = 0}.
aug
6: Set E1
= {p(p′ (u, v))|u → v ∈ Aaug
2 }.
Phase II
aug
′′
′′
′′
1: Compute the cost function c : if ((u, v) ∈ E1 ) c (u, v) = 0 ; else, c (u, v) = c (u, v)
′′
2: Compute d(u, v) and p(u, v) for each pair of nodes u, v ∈ V using T1 and cost function c .
aug
aug
3: Compute the set S = {[i, j]|(i, j) ∈ E1 , i ̸= r and failure of [i, j] disconnects the graph (V , E1 ∪ E1 )} and Dj ∀[i, j] ∈ E1
4:
5:
6:
7:
8:
9:
10:

Compute the directed tree T3d = (V3 , A3 ) using the technique described in [A] in Section 5.2
Compute the directed graph Gd3 = (V3 , A′3 ), functions c ′′′ (.) and p′′ (.) using the technique described in [B] in Section 5.2.
d
′
Compute the minimum cost arborescence T3arb = (V3 , Aarb
3 ) of the graph G3 = (V3 , A3 ).
aug
arb
arb ′′′
Set A3 = A3 − {A3 |c (a) = 0}.
aug
aug
Set E2 = {p(p′′ (u, v))|u → v ∈ A3 }.
aug
aug
aug
aug
Remove edges (u, v) from E1 ∪ E2 if failure of u and v disconnects (V , E1 ∪ E1 ∪ E2 )
aug
aug
Return E1 ∪ E2

aug

Theorem 8. Let C2
solution.

aug

aug

be the total cost of the edges in E2 . We claim that C2

≤ 2C opt where C opt is the cost of the optimal

aug

Proof. We know that in (V , E1 ∪ E1 ) the nodes i ∈ Cjk , kth component in Dj , get disconnected from r after failure of
[par (j), j]. Moreover, in (V , E1 ∪ E1aug ) there are two disjoint paths from i to r. Hence, one of the paths from i to r goes
through j that does not include par (j). This subpath from i to j includes only nodes in Cjk ∪ {j}. Also, we know that in TCA2
aug
Phase II cost of the edges in E1 ∪ E1 is zero. Corresponding to this undirected subpath from i to j there is a directed path in
d
G3 from i to xjk with cost zero. We denote this zero cost directed path from i to j in Gd3 by P0ij .
In order to prove the theorem we find a set
edges A′′3 ⊆ A′3 using the edges in the optimal solution E opt such
 of directed
′′
′′′
that (V3 , A3 ∪ A3 ) is strongly connected and u→v∈A′′ c (u, v) ≤ 2C opt .
3

Let Q2 show the set of nodes which are strongly connected in (V3 , A3 ∪ A′′3 ). Initially, Q2 = {r }, A′′3 = ∅ and all the edges
in E opt are marked unused. We use the following procedure to compute A′′3 and Q2 .
While Q2 ̸= V3 repeat the following steps:
1. Select an unused edge (i, j) from E opt such that one of the following conditions holds:
(i) LCA(i, j) = r. In this case if r ̸= i add the arc r → i and if r ̸= j add r → j to A′′3 . It should be noted that c ′′′ (r , i) ≤ c (i, j)
and c ′′′ (r , j) ≤ c (i, j). Then go to step 2.
(ii) LCA(i, j) = k, k ∈ Q2 , k ̸= r and k ∈ {i, j}. Let q ∈ {i, j} where q ̸= k. Also, one of the following conditions holds.
• There is a node ukl ∈ V3 ∩ Q2 such that q ∈ st (ukl , T3d ). Also, ukl → q ∈ A′3 . Add ukl → q to A′′3 . Obviously
c ′′′ (ukl , q) = d(k, q) ≤ c (i, j).
• There is a node ylm ∈ Q2 where l ∈ V , par (l) = k, q ∈ st (l, T1 ) and q ∈ Clm . Add ylm → q to A′′3 .
(iii) LCA(i, j) = k, k ∈ Q2 , k ̸∈ {r , i, j}. There is a node v ∈ Q2 ∩ V such that par (v) = k and i or j is in st (v, T1 ). Let
i ∈ st (v, T1 ). If j is in a component Cv m ∈ Dv and xv m ∈ Q2 , add xv m → j. Else if v is in a component Cjm′ ∈ Dj add
v → xjm′ . Else, if v is not in any component in Dj add v → j to A′′3 . Similarly, if v ̸= i add j → i or xjp → i or j → xip′ . More
precisely, if j is not in any components of Di and i is not in any component of Dj , add j → i. Else if i is in a component
Cjp ∈ Dj and xjp ∈ Q2 add xjp → i. Else if j is in a component Cip′ ∈ Di add j → xip′ .
In all cases at most two arcs with cost ≤c (i, j) are added to A′′3 . Then go to step 2.
2. For every new arc a → b added to A′′3 in previous step add every node k on the weakly directed path from a to b in T3d to
Q2 . Also, if k ∈ V and k is in some component Clm ∈ Dl for every node l ∈ s(k), add the arcs on the path P0kl to A′′3 ; and add

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

269

every node t on P0kl and every node on the path from t to r in T3d to Q2 . We note that all the arcs on path P0kl are have cost
zero.
3. Change the marking of the edge (i, j) to used.
We need to show that while Q2 ̸= V3 there is some unused edge (i, j) ∈ E opt such that one of the conditions in the
procedure holds for it. In each iteration Q2 shows the nodes from V3 that are strongly connected with r in (V3 , A3 ∪ A′′3 ).
Assume that Q2 ̸= V3 ; so, there should be some node k ∈ V3 ∩ V where it is not accessible from r (Note that if all the nodes
in V3 ∩ V are in Q2 then all the other nodes ∈ V3 − V can definitely get reached through zero cost arcs and can be added to
Q2 ). Hence, the nodes in st (k, T3d ) are not accessible from r and there is no directed path from the nodes in Q2 to the nodes
in st (k, T3d ) in (V3 , A3 ∪ A′′3 ). Also, the directed path from k to the nodes in Q2 can only go through k’s parent, t. In this case
if t fails then k gets disconnected from r. Hence (V , E1 ∩ Eused ) will get disconnected after failure of t. So, Eused ̸= E opt and
there should be some unused edge from a node in st (k, T1 ) to some node in st (j, T1 ) where j is a node in Q2 . Therefore, there
should be some unused edge ∈ E opt that one of the above conditions holds for it. Otherwise, it contradicts with E opt to be
the optimal solution.
Based on this procedure, for every edge (i, j) in E opt at most two nonzero arcs from A′3 are added to A′′3 such that
each one has cost ≤c (i, j). Therefore, total cost of the arcs in A′′3 , C2′′ ≤ 2C opt . We know that (V3 , A3 ∪ A′′3 ) is strongly
connected. So, on (V3 , A3 ∪ A′′3 ) we can construct an arborescence tree rooted at r using c ′′′ as the cost of the edges. Since
aug
A3 ∪ A′′3 − {i → r |i ∈ V } ⊆ A′3 and in TCA2 Phase II we compute minimum arborescence tree on (V3 , A′3 ), C2 ≤ C2′′ ≤ 2C opt .
As we mentioned before, the edges that are added during TCA2 Phase I and Phase II to T1 introduces possibility of additional
two adjacent node failures since two nodes i and j that were not adjacent before augmentation will be adjacent in graph
(V , E1 ∪ E1aug ∪ E2aug ) if (i, j) ∈ E1aug ∪ E2aug . In the following theorem we show that if any new two adjacent node failure {i, j}
aug
aug
aug
aug
results in disconnection in (V , E1 ∪ E1 ∪ E2 ) the edge (i, j) can be removed from E1 ∪ E2 and the remaining graph will
aug
be r ⊙ 2 fault tolerant. Therefore, the final set of augmenting edges, E
is constructed in the following way: For every edge
(i, j) ∈ E1aug ∪ E2aug where i ̸= r and j ̸= r, if removal of both nodes i and j does not disconnect the graph (V , E1 ∪ E1aug ∪ E2aug ),
edge (i, j) is added to E aug . Algorithm 2 shows the steps in algorithm TCA2 . Similar to Algorithm TCA1 , the time complexity
aug
of TCA2 Phase I is O(|V 2 |). In the third step in Phase II, S = O(|V |) and since the edges in E1 is computed based on T2arb ,
its cardinality is O(|V |). If we use depth first search to find the components for a two adjacent node failure [i, j] it takes
aug
O(|V | + |E1 ∪ E1 |) = O(|V |). So, time complexity of this step is O(|V |2 ). Similarly, step 9 takes O(|V |2 ). Therefore, time
complexity of Algorithm 2 is O(|V |2 ).
Theorem 9. The augmented graph (V , E1 ∪ E aug ) is r ⊙ 2 fault tolerant.
aug

aug

aug

aug

Proof. Consider that (i, j) ∈ E1 ∪ E2 and failure of i ̸= r and j ̸= r disconnects the graph (V , E1 ∪ E1 ∪ E2 ). Let u ̸= i, j
be one of the nodes that gets disconnected from root when i and j fail. u should be in st (i, T1 ) or st (j, T1 ). We know that in
(V , E1 ∪ E1aug ∪ E2aug ) there are two node disjoint paths from node u to r. So one path should go through i and not include j
and the other path should go through j and not include i. So there will be a cycle containing u, j, r , i, u in order. Edge (i, j)
is a chord in this cycle which divides the cycle into two subpaths, one from i to j which includes r and the other is from i
to j and it includes u. There cannot exist an edge from the nodes on first subpath to the nodes on the second subpath of the
aug
aug
cycle; otherwise, u would not get disconnected in graph (V , E1 ∪ E1 ∪ E2 ) after removal of i and j. So, these two subpaths
aug
aug
can replace the edge (i, j) and after removal of edge (i, j), in graph (V , E1 ∪ E1 ∪ E2 − {(i, j)}) if two adjacent node [u, v]
fails where (u, v) ∈ E1 the graph still remains connected. Consequently, the graph (V , E1 ∪ E aug ) is r ⊙ 2 fault tolerant.
Theorem 10. Algorithm TCA2 finds a set of edges E aug with total cost C aug ≤ 4C opt where C opt is the cost of the optimal solution.
aug

Proof. Since E aug ⊆ E1

∪ E2aug based on Theorems 6 and 8, C aug ≤ 4C opt .

6. Experimental results
In this section we present the experimental results of the approximation algorithms proposed for TCA1 and TCA2
problems. In the experiments we compare the results of the approximation algorithms against the optimal solution.
Moreover, we examine energy consumption by directional and omni-directional antennas deployed in a sensor network.
For every instance of our experiment, we generate the locations of the sensor nodes randomly, using a uniform
distribution on a square deployment area of size 100 × 100 units. We take the cost of edge between the nodes u and v
in the sensor network, as an indicator of the transmit power needed by the nodes to reach each other. Accordingly, we
construct a complete graph G = (V , E ) by setting the cost of each edge c (i, j) proportional to d2 (i, j) where d(i, j) is the
Euclidean distance between nodes i and j. We assume that an omni-directional antenna will consume power proportional
to r 2 where r is the radius of the coverage circle. A directional antenna with same transmit range r but with transmit beam
width α degrees will consume power proportional to 2απ r 2 [3]. In our model, an edge represents two directional antennas
transmitting signals to each other. Therefore, if the beam width is α we assume that the cost of the edge (i, j) is πα d2 (i, j).
For each problem instance we compute the minimum spanning tree to be the initial tree T1 and select a node randomly as
the root of the tree.
In our first set of experiments our objective is to compare the results of the approximation algorithms with the optimal
solution, obtained by solving an integer linear programming (ILP). We denote the ILP used to find the optimal solution

270

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

a

b

Fig. 6. (a) Comparison of augmentation cost of TCA1 algorithm and ILP 1 . (b) Comparison of augmentation cost of TCA2 algorithm and ILP 2 .

a

b

Fig. 7. Comparison between (a) power consumption of directional antennas and omni-directional antennas, (b) number of directional antennas and omnidirectional antennas.

of TCA1 and TCA2 by ILP 1 and ILP 2 , respectively. We used the CPLEX package to solve the ILP formulations. Since ILPs takes
considerable amount of time in these experiments, we vary the number of nodes, n, from 5 to 25 in steps of 5. For each value of
n, we generate 10 random instances of network layouts in a 2-dimensional plane. We set the beam width to 30°. We present
in Fig. 6(a) and (b) the comparisons between the optimal augmentation costs and the augmentation costs computed by the
TCA1 and TCA2 algorithms, respectively. For each n, we compute the average cost over 10 instances. We note that in these
simulations, the ratio of the average cost of augmentation computed by the TCA1 algorithm to the average cost of optimal
solution is smaller than 1.46. Also, the ratio of the average augmentation cost obtained by TCA2 algorithm to the average
optimal cost for each value of n is smaller than 1.62. These ratios are significantly better than the approximation factors
for TCA1 and TCA2 algorithms that are 2 and 4, respectively. In all of these experiments TCA1 and TCA2 algorithms execute
in a small fraction of the time required to achieve the optimal solution. For larger values of n, ILPs take so much time that
we were unable to get the optimal solution in a reasonable amount of time while TCA1 and TCA2 can find an augmentation
solutions with costs close to those of optimal solutions within a reasonable time.
In our second set of experiments, we compare the power consumption of directional antennas versus omni-directional
antennas. We vary the number of nodes, n, from 10 to 50 in steps of 10. For each value of n, we generate 50 random instances
of network layouts in a 2-dimensional plane. We compute the average cost of augmentation by taking the average of costs
incurred in these 50 instances. We perform the experiments for two values of beam width, 20 and 40°. For each instance we
execute the TCA1 algorithm. We assume that the transmit range of the omni-directional antenna is large enough to cover
furthest neighbor in the augmented
graph. Therefore, the total power consumption in network with omni-directional an
tennas is proportional to 1≤i≤n max{j|(i,j)∈E1 ∪E aug } d2 (i, j). In the case of directional antennas, every edge in the augmented
graph corresponds to two antennas, where each antenna needs 2απ d2 (i, j) amount of power to reach the other. Hence, the to
tal power consumption is proportional to (i,j)∈E1 ∪E aug πα d2 (i, j). Fig. 7(a) illustrates the comparison of power consumption
in these two cases. We observe that the total power consumption in the network is significantly smaller when directional
antennas are used instead of omni-directional ones. More specifically, when the beam width is 20°, the power consumption
with directional antennas is less than 9% of omni-directional antennas and when the beam width is 40° it is less than 18%.
We note that in order to make the network r ⊙ 1 fault tolerant, we need to install additional directional antennas, which has
a fixed cost associated with it. When nodes have omni-directional antenna, each node requires only one antenna. When we
use directional antennas, the total number of antennas deployed in every sensor node is equal to the node degree (including
both the initial set of antennas in the tree and the antennas installed to augment the tree). In Fig. 7(b) we illustrate the
average number of antennas that is needed in the network for each value of n. The first diagram shows the total number of
directional antennas needed in the network. We observe that the ratio of the total number of directional antennas to the
number of omni-directional antennas in these experiments is less than 2.7. However, in the TCA1 problem we consider that
the initial set of edges in the tree has cost zero (it is not part of augmentation cost). Therefore, only the cost of the new (augmenting) edges (i.e., the cost of the corresponding additional antennas) that are added during augmentation phase needs to

S. Shirazipourazad et al. / Pervasive and Mobile Computing 13 (2014) 258–271

271

be considered. The third diagram in Fig. 7(b) depicts the average number of directional antennas that are added to the network during the augmentation process. This number is smaller than 75% of the number of omni-directional antennas. More
accurately, augmenting directional antennas that are needed to make the network r ⊙ 1 fault tolerant are fewer than 75% of
number of omni-directional antennas. Therefore, the savings in power consumption with directional antennas outweighs
the cost of additional directional antennas needed, particularly when the width of the antenna beam is narrow.
7. Conclusion
Motivated by the importance of both data collection and fault tolerance in wireless sensor networks, we studied the
problem of enhancing the fault tolerance capability of a data gathering tree by adding a few additional links. We considered
two fault models: (1) single node failure and (2) two adjacent node failure. We proved that the least cost tree augmentation
problem is NP-complete under both types of fault scenarios. Moreover, we proposed two approximation algorithms, one for
single node failure and the other for a pair of adjacent node failure, with performance bounds of two and four respectively.
Experimental evaluation of the approximation algorithms shows that they perform even better in practice. In the future we
plan to study the tree augmentation problem under more general topological fault models like when a fault is defined as a
subgraph with diameter d.
References
[1] O. Incel, B. Krishnamachari, Enhancing the data collection rate of tree-based aggregation in wireless sensor networks, in: SECON, 2008.
[2] X.-Y. Li, Y. Wang, Y. Wang, Complexity of data collection, aggregation, and selection for wireless sensor networks, IEEE Trans. Comput. 60 (2011)
386–399.
[3] E. Kranakis, D. Krizanc, E. Williams, Directional versus omnidirectional antennas for energy consumption and k-connectivity of networks of sensors,
in: Proc. of 8th International Conference on Principles of Distributed Systems, 2004, pp. 357–368.
[4] Z. Yu, J. Teng, X. Bai, D. Xuan, W. Jia, Connected coverage in wireless networks with directional antennas, in: INFOCOM, 2011.
[5] Y. Wang, G. Cao, Minimizing service delay in directional sensor networks, in: INFOCOM, 2011.
[6] C.A. Balanis, Antenna Theory: Analysis and Design, second ed., Wiley, 1997.
[7] J.L. Bredin, E.D. Demaine, M. Hajiaghay, D. Rus, Deploying sensor networks with guaranteed capacity and fault tolerance, in: MobiHoc, 2005.
[8] M. Hajiaghayi, N. Immorlica, V. Mirrokni, Power optimization in fault-tolerant topology control algorithms for wireless multi-hop networks, IEEE/ACM
Trans. Netw. 15 (2007) 1345–1358.
[9] F. Wang, M. Thai, Y. Li, X. Cheng, D.-Z. Du, Fault-tolerant topology control for all-to-one and one-to-all communication in wireles networks, IEEE Trans.
Mob. Comput. 7 (2008) 322–331.
[10] A. Sen, B.H. Shen, L. Zhou, B. Hao, Fault-tolerance in sensor networks: a new evaluation metric, in: INFOCOM, 2006.
[11] G.N. Frederickson, J. Ja’Ja’, Approximation algorithms for several graph augmentation problems, SIAM J. Comput. 10 (1981) 270–283.
[12] S. Khuller, B. Raghavachari, Improved approximation algorithms for uniform connectivity problems, Algorithms 21 (1996) 434–450.
[13] M. Garey, D. Johnson, Computers and Intractability. A Guide to the Theory of NP-Completeness, Freeman, 1979.
[14] D.B. West, Introduction to Graph Theory, second ed., Prentice Hall, 2001.
[15] R.E. Tarjan, Finding optimum branchings, Networks 7 (1977) 25–35.

Round Robin with Look Ahead: A New Scheduling Algorithm for Bluetooth
Daqing Yang, Gouri Nair, Balaji Sivaramakrishnan, Harishkumar Jayakumar and Arunabha Sen
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287, USA
fdyang, ggn, sbv, harishkumar, aseng@asu.edu
Abstract
In this paper we propose two new Media Access Control
(MAC) scheduling algorithms for Bluetooth whose objective
is to achieve high channel utilization (throughput). Conventional scheduling policies such as Round Robin (RR) in
Bluetooth environment results in wastage of slots and hence
poor utilization of the network resources. As Bluetooth devices are designed to carry both voice and data, scheduling
becomes a complex task, as slots are reserved for voice traffic at periodic intervals and the data packets are allowed to
have variable size. In this paper we view the MAC scheduling problem in Bluetooth as an online Bin Packing problem.
The two scheduling policies being proposed in the paper,
Look Ahead (LA) and Look Ahead Round Robin (LARR)
can be viewed as online bin packing with lookahead. In
this paper, we first analytically demonstrate that an optimal scheduling policy can have about 66% improvement in
throughput over the Round Robin policy. Our extensive simulation shows that both LA and LARR achieves nearly 10%
improvement in throughput over RR. As the computational
complexity of LARR is lower than that of LA and also LARR
avoids the possibility of starvation, we suggest the use of
this algorithm over RR.

1 Introduction
The development of the Bluetooth protocol in late 1990s
is the result of an effort to standardize a simple and inexpensive network protocol for personal ad-hoc networks,
comprising of devices such as notebook computers, personal digital assistants, cellular phones, digital still and
video cameras. Bluetooth uses an ad-hoc piconet structure that uses short range (10 meters) frequency hopping
(1600hops/sec) to improve robustness of the link in the
2.4GHz ISM band. A piconet is an ad-hoc network formed
by at most eight Bluetooth enabled devices, one of which
acts as the master and the remaining ones as slaves. Blue-

tooth uses Time Division Duplex (TDD) scheme, where
communication from master-to-slave and slave-to-master
strictly alternates. The master of the piconet is responsible
for slot scheduling for communication with the slaves. A
frame comprises of a set of time-division duplex slots, with
each Bluetooth packet occupying 1, 3 or 5 slots. Bluetooth
supports both voice and data traffic [3, 8]. The members
of a piconet can use either Synchronous Connection Oriented (SCO) links or Asynchronous Connectionless Links
(ACL) for communication between them. SCO links are
used for voice communication, whereas ACLs are used for
data communication. In figure 1, the interleaving of SCO
and ACL slots are shown for a piconet with four members.
Since SCO slots carry voice traffic, they occupy fixed slots
assigned apriori by the Master. In figure 1, between every
SCO slot pair (one from master-to-slave and the other for
slave-to-master communication) there are four slots that are
used by the ACLs for data communication. The frame size
is four in this scenario.
It may be observed that as the frame size is fixed, unless
the slots in a frame are properly utilized for data communication using ACLs, the slots are wasted, thereby reducing
the throughput of the network. Scheduling policy used for
filling up the slots of a frame by the ACLs determine to
a large extent the utilization of the slots. Recognizing the
important role of the scheduling policy in determining network throughput, several researchers in recent years have
focussed their attention on development of more efficient
scheduling algorithms [2, 5]. In this paper we propose a new
scheduling policy that performs significantly better than the
Round Robin algorithm, widely used scheduling algorithm
in Bluetooth today.

2 Bluetooth Scheduling and Online Bin
Packing
Consider a piconet comprising of a master and two
slaves. The master has two queues MQ1 and MQ2 where

Proceedings of the International Conference on Parallel Processing Workshops (ICPPW’02)
1530-2016/02 $17.00 © 2002 IEEE

Figure 1. Interleaved SCO and ACL slots for
communication between the Master and the
Slaves

the packets to be delivered to two slaves are stored. Similarly the slaves S1 and S2 , maintains two separate queues
SQ1 and SQ2 where the packets to be delivered to the master are stored. As indicated earlier, the packets can have
only three sizes 1, 3 and 5. Suppose the content of MQ1
is (5, 5, 5, 5, 5) and the content of MQ2 is (1, 1, 1, 1,
1). Similarly, the content of SQ1 is (5, 5, 5, 5, 5) and the
content of SQ2 is (1, 1, 1, 1, 1). Since master-slave communication takes place in pairs, we can think of two combined
master-slave queues CMSQ1 and CMSQ2 whose content
will be the sum of the contents of MQ1 , SQ1 and MQ2 ,
SQ2 respectively. Thus the content of the hypothetical (or
distributed) queues CMSQ1 and CMSQ2 will be (10, 10,
10, 10, 10) and (2, 2, 2, 2, 2) respectively. Suppose in our
scenario the number of slots between every SCO slot pair
(one from master-to-slave and the other for slave-to-master
communication) is 10. These 10 slots can be used by the
ACLs for data communication. It can easily be verified that
if Round Robin scheduling is used, it will take 10 frames to
transfer all data between the master and the slaves. However, using a different scheduling policy all data could have
been transferred using only 6 frames.
Bin Packing Problem: In a bin packing problem we
have a set of objects L = fa1 ; : : : ; an g with a size s(ai )
associated with each object ai . We also have a set of bins
with a fixed capacity B . The objective is to pack the objects
ai ; 1  i  n, into as few bins as possible, subject to the
constraint that sum of the sizes of the objects placed in a bin
does not exceed the bin capacity B .
Since the number of slots between every SCO slot pair
is fixed (say 10), we can view this as a bin with capacity
10. The packets in the combined master-slave queues can
be viewed a objects that we need to place within the bins
(frames). A good scheduling algorithm would attempt to
put the packets in as few frames as possible, because it will
increase the network throughput.

In the off-line version of the bin packing problem, the
packing algorithm has the complete knowledge of all the
elements fa1 ; : : : ; an g and their sizes, and this knowledge
can be used by the packing algorithm to efficiently pack
the objects in the bins. However, in the online [7] version of the bin packing problem, the packing algorithm
does not have the complete knowledge of all the elements
fa1; : : : ; an g and their sizes. Instead, objects arrive in a sequence a1 ; a2 ; : : : ; an , and the scheduling algorithm has to
place the i th object ai in the bin as soon as it arrives.
At this time the scheduling algorithm has already placed
the objects a1 ; : : : ; ai 1 in some bins, which cannot be rearranged, and has no knowledge of the sizes of the objects
ai+1 ; : : : ; an that are yet to arrive. The scheduling algorithm for Bluetooth can be viewed as online bin packing
problem with a limited amount of future knowledge. This
is so, beacuse if there are x slaves present in the piconet, the
scheduling algorithm can examine the packets at the front of
the queues CMSQ1 ; : : : ; CMSx (head-of-the-line (HOL)
packets), and decide which packet to place in the current
frame.

Bin packing is known as an NP-complete problem [6].
There are a number of approximation algorithms for bin
packing, of which the First-Fit (FF), the Best-Fit (BF) and
the Next-Fit (NF) are best known [1, 4]. The FF rule places
each successive object into the first bin in the sequence
B1 ; B2 ; : : :, where it will fit. The NF rule fills the bins in
sequence, i.e., if an object ai is placed in a bin Bj no object
ak ; k  i can be placed in any bin Bl with l  j . The NF
scenario describes the Bluetooth scheduling appropriately,
because the frames leave the master or the slave device and
is not available for further packet placement at a later time.

The quality of the approximate solutions like FF, BF and
NF, is usually measured by their worst case performance
bound of the form A(L)  OP T (L), where A(L) and
OP T (L) are the number of bins required to pack the set of
objects L by algorithm A. The bound  for FF and BF is
1.7 and for NF it is 2. It is well known that if the objects in
the set are arranged into a nonincreasing order of the object
size, the performance bound can be further improved. The
corresponding versions of the FF, BF and NF techniques are
known as FFD, BFD and NFD respectively. The bound 
for FFD and BFD is 1.22 and for NF it is 1.691 [1].

In the Bluetooth scheduling environment, we can deploy
a version of the NFD, where only the HOL packets are arranged in nonincreasing order before they are scheduled in
frames. This will essentially be an online version of the NF
algorithm with a limited amount of lookahead.

Proceedings of the International Conference on Parallel Processing Workshops (ICPPW’02)
1530-2016/02 $17.00 © 2002 IEEE

3 Potential for Improvement over Round
Robin

Step 1. Examine

the

CMSQ1; : : : ; CMSQx

(HOL)
packets
for all the active slaves

in the piconet.

For any scheduling algorithm A in Bluetooth, let list
a ; a2 ;    ; an ) denote the packets being scheduled
by algorithm A, let A(L) denote the number of bins used by
packing list L, and let OP T (L) denote the minimum number of bins required to pack list L, the asymptotic worstcase performance ratio is defined as

L

= ( 1

RA = nlim
!1

A(L)
:
OPT (L) = n OP T (L)
max

We have the following theorem:
Theorem 1 For the Round Robin scheduling algorithm A
in Bluetooth, RA = 53 .
Proof: Notice the Bluetooth scheduling algorithm has
bin sizes 10, and item sizes can only be 2, 4, 6, 8, 10.
Therefore, the Round Robin algorithm A guaranteed that
any neighboring pair of bins packed item sizes at least 12.
If OP T (L) = n, the packets need to be packed have size at
n e. This shows RA = 5 .
most 10n, therefore, A(L)  d 20
12
3
Roughly speaking, this theorem shows that the Round
Robin algorithm A compared with any scheduling algorithm B in Bluetooth, A is at most 53 worse than B.
The following example shows that Round Robin algorithm compared with Round Robin with 1-look-ahead can
be as bad as 53 .
Example 3.1
Queue 1: L1
Queue 2: L2

; ; ; ; ;   )
; ; ; ; ;   )

= (10 10 10 10 10
= (2 2 2 2 2

In this example, if the number of items in L1 and L2 is
n, then the Round Robin algorithm needs 10n bins, while
Round Robin with 1-look-ahead needs only 6n bins.

5

4 Scheduling Algorithms
In this section, two new algorithms proposed in this work
are presented.

4.1 Look Ahead
In this scheme, the master examines the HOL
packets in the combined master-slave queues,
CMSQ1; : : : ; CMSQx (assuming that there are x
slaves) and selects the largest packet that will fit into the
current frame. This policy attempts to mimic the NFD
policy for bin packing discussed in the previous section.
This algorithm schedules packet as follows:

Step 2. Prioritize slaves in order of non-increasing size of
their HOL packets in CMSQ (the slave with the largest
HOL packet has the highest priority).
Step 3. If possible, schedule the highest priority slave in
the current frame.
Step 4. If HOL packet of the highest priority does not fit,
search through HOL priority list for highest priority
that will fit the current frame.
Step 5. If none fits, wait for the start of a new frame and
put the HOL packet in the new frame. Repeat steps 1
through 5.

4.2 Look Ahead Round Robin
There are two problems associated with the Look Ahead
algorithm. First, every time a packet is scheduled, the HOL
packet with the next largest size has to be identified. If their
are N slaves, using a priority queue data structure, it can
be computed in O(log N ) amount of computation. However, in Bluetooth environment, this amount of computational overhead may be unacceptably high. Second, as the
algorithm always attempts to schedule the packet with the
largest size, it may unduly favor some slaves at the expense
of others. In the extreme case, this may lead to starvation
for some of the slaves.
The Look Ahead Round Robin (LARR) was developed
to overcome the drawbacks of the Look Ahead algorithm.
It combines the simplicity of the Round Robin algorithm
with the look-ahead feature of the Look Ahead algorithm.
To avoid the possibility of starvation and also to reduce the
computational complexity, it no longer attempts to find the
largest HOL packet. Instead the slaves are serviced in a
round robin fashion and if the HOL packet of the combined
master-slave queue (CMSQ) currently being examined fits
in the currently open frame, it is scheduled. The only difference from RR occurs when the current packet does not
fit. In this situation, in order to ensure maximum slot usage,
the algorithm looks ahead and attempts to schedule a packet
from next slave in line to receive service in the round robin
fashion.
The slaves are all serviced in the order in which they
enter the active piconet. This algorithm schedules packet
slots in the following way:
Step 1. The algorithm will fill the current frame with the
HOL packets in a Round Robin fashion, if the HOL
packet can fit in the frame.

Proceedings of the International Conference on Parallel Processing Workshops (ICPPW’02)
1530-2016/02 $17.00 © 2002 IEEE

Step 2. If the HOL packet of the queue being serviced is
too large to fit in the current frame, following Round
Robin fashion, find the queue whose HOL packet will
fit in the current frame. If no such queue exists, wait
for the start of a new frame and schedule the HOL
packet in the new frame. Repeat steps 1 and 2.
It may be noted that in order to schedule a packet from
CMSQi; 1  i  x, the Master needs to know the size
of the HOL packet in the slaves. This information may be
conveyed to the master by slaves using the unused fields in
the header of the immediately preceding packet.

5 Advantages of Look Ahead Round Robin
The Look Ahead Round Robin algorithm is based on the
conventional Round Robin scheduling with a look-ahead
feature.
In this algorithm if the HOL packet of the queue being
serviced is too large to fit in the current frame, the algorithm
will look ahead to see if their are some queues (which will
be serviced in future) with HOL packets that will fit in the
current frame. In k-look-ahead scheme the algorithm will
examine the HOL packets of the next k queues, and fill the
current frame with the first HOL packet that can fit in the
frame.
Clearly, the new algorithm maintains the computational
simplicity Round Robin algorithm. It runs in linear time and
uses constant space (with respect to the constant of k as in
the k-look-ahead). Since the algorithm is based on Round
Robin, it will avoid starvation.
In this section, we examine some interesting scheduling
cases that shows Round-Robin with k-look-ahead may significantly reduce the number of frames necessary to transmit all the packets. In all the following examples, we suppose the number of packets in each queue is n.
Example 5.1
Queue 1: L1
Queue 2: L2
Queue 3: L3
Queue 4: L4

; ; ; ; ;   )
; ; ; ; ;   )
= (4; 4; 4; 4; 4;   )
= (2; 2; 2; 2; 2;   )
= (6 6 6 6 6
= (8 8 8 8 8

In example 5.1, the Round Robin algorithm needs 3n
frames, and the Round Robin with 1-look-ahead algorithm
needs 2n frames, with performance comparison ratio 32 .
More generally,
Example 5.2
Queue 1:
Queue 2:
......
Queue 2k-1:
Queue 2k:

L1 = (6; 6; 6; 6; 6;   )
L2 = (6; 6; 6; 6; 6;   )
L2k 1 = (6; 6; 6; 6; 6;   )
L2k = (8; 8; 8; 8; 8;   )

Queue 2k+1:
Queue 2k+2:
......
Queue 4k-1:
Queue 4k:

L2k+1 = (4; 4; 4; 4; 4;   )
L2k+2 = (4; 4; 4; 4; 4;   )
L4k 1 = (4; 4; 4; 4; 4;   )
L4k = (2; 2; 2; 2; 2;   )

In example 5.2, the Round Robin algorithm needs 3kn
frames, and the Round Robin with (2k 1)-look-ahead algorithm needs 2kn frames, with performance comparison
ratio 32 .
Example 5.3
Queue 1:
L1 = (6; 6; 6; 6; 6;   )
Queue 2:
L2 = (6; 6; 6; 6; 6;   )
......
Queue 2k:
L2k = (6; 6; 6; 6; 6;   )
Queue 2k+1: L2k+1 = (8; 8; 8; 8; 8;   )
Queue 2k+2: L2k+2 = (4; 4; 4; 4; 4;   )
Queue 2k+3: L2k+3 = (4; 4; 4; 4; 4;   )
......
Queue 4k+1: L4k+1 = (4; 4; 4; 4; 4;   )
Queue 4k+2: L4k+1 = (2; 2; 2; 2; 2;   )
In example 5.3, the Round Robin algorithm needs (3k +
n frames, and the Round Robin with 2k-look-ahead needs
3k+1
(2k +1)n frames, with performance comparison ratio 2k+1 .

1)

The above examples demonstrate that with Round Robin
with k-look-ahead can reduce the number of frames by 50%
over just Round Robin.

6 Simulation Environment
In the previous sections we analytically showed that the
Round Robin with Look Ahead algorithm can significantly
reduce the number of frames necessary to carry out data
transfer between the master and the slaves in a Bluetooth
environment. However, this was established by choosing
some specific situations, where the master and slave queues
had packets of specific size. In general, the master and slave
queues may not have packets with those specific sizes. We
wanted to find out the performance of our new algorithm
in a more general setting. We turned to simulation for this
purpose.
Our simulation environment was developed in Java, using threads to create pseudo-parallel packet generation and
scheduling. We assumed that packet arrival at the master
and the slaves followed a Poisson process. The size of the
packets were restricted to 1, 3, or 5 slots and was drawn
from a uniform distribution. Packet generation was carried
out in pairs, one for the master and other for the slave. Once
generated, the packets are placed in the appropriate queues.
The exact same set of packets were used for the evaluation
of three different algorithms: (i) Round Robin, (ii) Look

Proceedings of the International Conference on Parallel Processing Workshops (ICPPW’02)
1530-2016/02 $17.00 © 2002 IEEE

7 Simulation Results
The results of some of our simulation experiments are
presented in figures 2 to 4. In figure 2, average frame savings by the Look Ahead and the Look Ahead Round Robin
over Round Robin is plotted. It can be seen that LARR initially out performs LA, but LA eventually has greater savings, when the number of slaves is greater than 5. This is
quite expected because the LA algorithm in this situation
has a larger number of queues to examine to fill the current
frame.
In figures 3, 4 and 5 average savings of the LA and
LARR is plotted against the RR algorithm. It may be observed that average savings ranged from 5.5% (LA with 3
slaves) to 12% (LA with 7 slaves). The average savings for
LA was less than that of the LARR when the number of
slaves was small (3). When the number of slaves were increased LA performed at least as well as LARR and with
seven slaves, it outperformed LARR. However, the average
gain of the LARR algorithm over the RR algorithm was not
significantly less than that of the LA algorithm. With the
number of slaves greater than 5 and arrival rate greater than
0.5 both LA and LARR algorithm had approximately 10%
savings over the RR algorithm. We believe that this savings

is quite substantial for a device like Bluetooth.
10

9

8

Average % Savings from RR

7

6

5

4

3

2

1
LA
LARR
0
1

2

3

4
Number of Slaves

5

6

7

Figure 2. Variation of performance with number of slaves

8

7.5

Average % Savings from RR

Ahead and (iii) Look Ahead Round Robin. One packet generator thread was initialized for every slave included in the
simulation. The algorithms were implemented as separate
threads to promote pseudo-parallel processing. This more
accurately captures the packet scheduling process with dynamically changing queue sizes.
The variables in our experiment were (i) the number of
slaves in the piconet, and (ii) packet arrival rate. In the first
set of simulation experiments, the packet arrival rate was
fixed and the number of slaves in the piconet were varied.
In the second set of experiments, the number of slaves in a
piconet were fixed and the arrival rate varied. The arrival
rate parameter  was varied from 0.1 to 1.0 in increments
of 0.1. The simulation was carried out for 1000 units of
time. The number of frames needed to transfer the entire
data set generated within this time period was computed
for the three algorithms: Round Robin, Look Ahead and
Look Ahead Round Robin. The number of frames needed
to transfer data was largest for the Round Robin algorithm.
Using the number of frames need by the Look Ahead and
Look Ahead Round Robin algorithms, the savings in the
number of frames by these two algorithms over the Round
Robin algorithm was computed. With the number of slaves
and the packet arrival rate fixed, we computed the savings
of the LA and LARR algorithm over RR. Each experiment
was repeated 500 times to compute average frame savings.
We assumed that the number of slaves did not change during the execution of an experiment.

7
LA
LARR

6.5

6

5.5
0.1

0.2

0.3

0.4

0.5
0.6
Arrival rates (Lambda)

0.7

0.8

0.9

Figure 3. Variation of percentage savings with
arrival rates, 3 slaves in piconet

8 Conclusion
In this paper we have presented two scheduling algorithms for Bluetooth. Through analysis and simulation we
have shown that these algorithms perform significantly better than the Round Robin algorithm used for scheduling in

Proceedings of the International Conference on Parallel Processing Workshops (ICPPW’02)
1530-2016/02 $17.00 © 2002 IEEE

1

Bluetooth today. Since the computational complexity of the
Look Ahead Round Robin is smaller than that of the Look
Ahead algorithm, we propose its use in Bluetooth. In addition, the LARR algorithm avoids starvation. Currently we
are examining the efficacy of these two algorithms from the
power consumption point of view.

10.2

10

9.8

References

Average % Savings from RR

9.6

9.4

[1] B.S. Baker and E.G. Coffman, A tight asymptotic bound for
Next-Fit-Decreasing Bin Packing, in SIAM Journal on Alg.
Disc. Meth, vol. 2, no. 2, June 1981 (pg. 147-152).

9.2

9

[2] D. Bansal, M. Kalia, R. Shorey, MAC Scheduling and SAR
policies for Bluetooth: A Master Driven TDD Pico-Cellular
Wireless System. in IEEE Mobile Multimedia Conference (
Momuc’99) (pg. 384-388), San Diego, California, November
99.

8.8

8.6

8.4
LA
LARR
8.2
0.1

0.2

0.3

0.4

0.5
0.6
Arrival rates (Lambda)

0.7

0.8

0.9

1

[3] ”Bluetooth Special Interest Group”,Specification of the
Bluetooth System”,”http:
www.bluetooth.com”, 2001.
[4] E. G. Coffman,Jr, M. R. Garey, D.S.Johnson, Approximation
Algorithms For Bin Packing: A Survey. In Approximation
Algorithms for NP-hard Problems, D. Hochbuam, Ed. PWS
Publishing Company, 1997, Ch. 2.

Figure 4. Variation of percentage savings with
arrival rates, 5 slaves in piconet

[5] A. Das, A. Ghose, A. Razdan, H. Saran and R. Shorey, Enhancing performance of asynchronous data traffic over the
Bluetooth Wireless ad-hoc network. in Proceedings of IEEE
INFOCOM’01, (pg. 591-600), 2001.
[6] M. R. Garey, D.S.Johnson, Computers and Intractability: A
Guide to the Theory of NP-Completeness. W. H. Freeman,
San Francisco, 1979.

12.5

[7] E. F. Grove, Online bin packing with lookahead, In Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 430-436, San Francisco, California,
22-24 January 1995.

12

Average % Savings from RR

11.5

[8] B. A. Miller and C. Bisdikian, ”Bluetooth Revealed”, Prentice Hall, 2000.
11

10.5

10

9.5
0.1

LA
LARR

0.2

0.3

0.4

0.5
0.6
Arrival rates (Lambda)

0.7

0.8

0.9

1

Figure 5. Variation of percentage savings with
arrival rates, 7 slaves in piconet

Proceedings of the International Conference on Parallel Processing Workshops (ICPPW’02)
1530-2016/02 $17.00 © 2002 IEEE

2014 IEEE 17th International Conference on Computational Science and Engineering

	









	


"#( 

,#-./


	
 !


"#
$%&



'

") *+
!
%$")

'
##

%*	
	$%


#'#
(
'#









	






	

 	
 
   
 
 
 
 









 
 	
	 
   
 

	
  


 
	
  
 

  	
	 
  
  



	

	

 

 




	

 
 	
 
 



  

 
 
  



 

 
 

 
 
 



 
 

  

 		   


	
 

 	
  
  

 

 

 

 
	

 
 
 
	  



 
 
  


 
 
 	
 
 
  
 









	
	




 

 
	 ! " 
 	
 
#$%%& '(() 
*
  	 

 

 

 



	







	
	
 

 
  
 
    
 	
   

	
  
	
 
 
 
   

 	
 
 
  

	 
 
 
 
 

 
 
 
   
 	
	










 
	



 

 
  

 





 
 



0 0%	*%0
%
#++(!!
(
#

+!#!



 %  # 
 ! 
 
$ #$ !
$  
# 
%   &# !
 
    # 
  #
 
 !
( 
# (!    +!   


 (! 
 
 " 1  

 12 
 ( ( % +!  
 
      

#
 + 
#  )  (
 # 
#

3#
$
#

)
0
#
#
$



(#




(!##(
)

978-1-4799-7981-3/14 $31.00 © 2014 IEEE
DOI 10.1109/CSE.2014.242

1256

%
 
      /
 +!
4/5

#067899:;2<%

&;8<(!
7#
6#7
0
+(
 &# #(  1
 
 (  

$

##(


 + 
    
$  ( 
1
 (
  

 ;=< 
 
(#+
#
  # #
  (
  

!$   
     
!$ 

  !     
# 

%  ;>< #   +
&11#($

;:<+
!
#
;><
  
 (   1#  
&1
   $  
 + ( #
  

  
  
! 
 +! 
(

  

 !  

 

$ #
   !  #! 
$ 
+!
$
1##

;?<$;@<$;29<1;28<
%  #
A   
(  
#!  
(
+!
(



%
 
#! 
(
  /   

  # 


#(
%
 
 
 (! #
+   & 

   
  % 
  +!  


+$

!
#
+&


%
 
#
   (
  
  #  




%

+
#  
#(

   
$  
 
  
    
    
(!
A

0


1$
(1;@<
,+$  
   +    
 ( 1

%+!&,
+  
 

  
 
 ( ! 1
1
%
 
#! 
    (
 +!

#

#
&#

)


# 1 +!%

#
(!


IJ5,  #(  
#


+(!425
	
	
	

	425


%  
 ) 
 
 " 
 
 

 
 #

 (!  
#   
(
+
%
# 

#


    
 1   
)
#
  #
  + #(#  #
 
   
  &
  
   


6!$

+
#




(!&
+
8֩֪   !     #
(#A
#

%    
 

 
!
  (
$   
 
 #!  B(



# 
$#
#
 
#
$  &
   
 

00 %B70C
7
    !  
 #
  
 
(#!
&(;D<"

#


#!
"
E4F$B5
EGFG
$4$35
B$
 
         3 % 
4
! #5  
 #(

4
! # 5  0  #
  
 #
  
  3# 
"  3

3#


3%


3+&3"
 
 
!   !   (  
!     % +!    
   
+  ! 12 
 
 
   

!$+
 



#
    (  
!  %
+! 
 (+#
! (# (!  #  
!0+

+!$

   !   % 
 (


#(


(
 %     
  
 +#  

(!

"    #
 HA     HA

   
   
 HA  HA  
 
$ 
    
 # + 

  

 #  
#   


(+
6 2 

      E 8   
%

$(

+%(#

%
#(
#

3#2I842I8

1257


62 #
8$ 

"#
+&#$#



+
#

( 
 %  (
 +!  
  /$ 
 #  ! /12 
  (
+ #     

!#+/K
$+
/ #
(   !  (
   6 
#
#(
"       E 4F$ B5 
 45 E 4F2$ B25

#F2EBB2EL4$(5GE4#2$#85

B(E4#8$#?5
BM
"B&B45;@<$ # 
E4F$B5     +! $ 
 45  

$N
#(
B45E
OI$
#B45

$+!$
4B455P45I8
000 "
, %*%0
 
% +      



#



+!

;@<0


#1	$

#

# 1
0



$
A


4	I85
";@<
!


(#
1$@#(


812%# 1

#

!


!1#
 812

  @ B&

  #( HA  (
 HA 


  485$  
    ( 
  B45 
+#
(

!4!!!I4125"I4185"I41?5"J"I9485


B 
 #
  +#
 #( 

 (!   
  #(  N % 
  
 
@$B45

4185

 
   (!  
 8   ,
A

(#(+ (!@I84185
E8%

(
 #  



(##
&
68+


(


2$
! 8#

#(
(!8
9
4Æ($


(5


+#
(

812$


(
812 
%
#  

$ (#  
#   ! 1
 #    
 
 ( 
(
+#

#!;@<%&


    
 N 
  
+!  $ (# 
 2     

%#
B45
8%
+#


$ (#&

+&
#(


#

6? "

9R
/
4?
5
9Æ2$8
2Æ8$9 8Æ9$2

2RB4512
454?O8E:
5
28Æ89$82
92Æ28$29 98Æ89$82
82Æ28$29
29Æ92$98 89Æ92$98




68 	#
#
+B45


  !
"!$

  (!    
   
# 

6&
 (

QC$QC(!
3#
 
$   
#   
    
#(QCQ


C




QC%

!







6?

 &
  
     #(  
  
   3#   #(  
    
&

#$ #(


45

#(

"
&;@<
##




(E2@E8
2@(
8E2229
,(

22(
8
$
B45 2#
#(
(!8

B454S
542455
92Æ&
&Æ28
82Æ&
&Æ29
92Æ28$29 98Æ89$82
28Æ89$82 29Æ92$98
89Æ92$98 82Æ28$29

8RB4518

454SO8E2@
5
92&Æ&28$&29
&28Æ2889$2882
82&Æ&28$&29
&29Æ2992$2998
9229Æ2992$2998 9889Æ8992$8998
9882Æ8228$82& 2889Æ8992$8998
2882Æ8228$82& 2992Æ92&$9229
2998Æ9889$9882 8992Æ92&$9229
8998Æ9889$9882 8228Æ2889$2882



#$#




(
$
!92298228




9229ÆTÆ8228

%
 
  #
+!   B45 +
  
4 25+2982




9229!+(Æ
Æ,Æ
Æ'+"8228

"  # # #  $  +    
98 


1258



9229429!(Æ
Æ,Æ
Æ'"8258228

9Æ8&


$+((
#
+!#
 3#
 (#
+!#







9229Æ429Æ98Æ8258228





9229Æ2998Æ9882Æ8228

0F B0B%0F0%
%C"	"	0* 

09882
(#!$+
9882+
#9882#
!
,#9Æ2Æ8
+
#
R




922942949Æ2Æ858258228

%#
$ 9229Æ2992Æ9228Æ2882Æ8228 +
  #!

 "#$%$#
%

#
H$#$$$#A

   
     0   A
 

#&



   
   
  #   
A



%"/B0
60B	606*BB"	 ,%B %
%
"%, 

	








	















	
	















	
		

	









$#!$$!$!!
!+(#

     "
$ 

  
  
 1
$!
&

ÆJÆ

# 

(
1$


1







	

		
	


	










+ 

4
5+
#,
++
#
  $ (#  $  
  
   

+
$(!
#
+

0 
       #
    ( 

&!
$

!
12

(
&!

U  +   

  
 

(



&




!$

+#&


HA2



%
  ( + (!  (! 

#  
#&



 



8


B45
 
 
  6 @ #

+!  (
 

  
 #    
   # 
#


(&!




6@ 
 (!    #
# &

   # 
2




(
%((20


+
#((((
 
    
  (! 
##

+ B45


 "  H!A ( H&A   # 

  &Æ! &
   B45 +$   #



#B45+#
#!
(



#HA




 HA
3#
( #


4
!5
 
  
   ( ÆÆÆÆ
Æ$   #

#!#$


  #     HA  
 $

#
($$
    
     $
$$!$!! 
 #!$
     ÆJÆ   
  +
 

1259

"

#$#!
#$
! +  
# 
   +


0 + 
 +
+
 
 ( &  !$
(
(






&!
+
+!

++
H&A


#(
3#B45
R

Æ(ÆÆ&ÆÆÆÆ
4
5R 
H&A#
2

(Æ(Æ&Æ&ÆÆÆ
4B4512R
H&A#
8


((Æ(&Æ&&Æ&ÆÆ
4B4518R
H&A#
?


0      
     B45
$  # &

 & # (  I2 
 0





   I2 # ( # (!   &  

  $   +  + 
   #

VW8X
(+$

#+&


0



1$
+12
VW8X#
$


!  %
 #      
 1
(

#
VW8X
6     
    !4!!! I 4125"I 41
 #(  
  B45



E֩֪18$


( 
  
  0 
$  # 
#
#(B45


E ֩֪ 18$     12 
 (

$   
 +  
  #
   E
V44֩֪W85 1 25X % #(  
    


 
 + (! 48;44I25 125W4125< Y 25$   E
V4֩֪W8512X
85"I41?5"J"I9

0
#
+
+!#
+  
   
   
 1
 0  &   (
 +!
+$ 
 
 
  1 (
 

#12#
+
844I25125W4125Y2
%#

+$

#!DDDD
290#
+((!(#
#!
"


" #
- +   #
 # 
#
 
#((#
D
82
$%!


" #
D
$


$
(
!$

# 
 (  %
 #

   
(
#

(#


$   
     
 

(#

+


;8< " $ 7#!$ /$\1/
+!Y"

  	
  6#1 
]$ %((( 	)
%#$*!+$ ,-!.!$
899D
;?< 
 	 
$  ,#$ \  /
 6
 # 
#  , %
  
 6#
]$ %(((
	/ %# $  *! +$  ,-! 
.!0892@
;@< 
	
$ ,#$\7#	

+!   	
]$ ,1 $ %# #
' ,%'
2' $892@
;=<  %+
$  6 " .#
$ 
 F 7$ " 0$  $
\ 
  1
 
   ]$ %"%+
-3!$$892?
;:< C .(!
$  .
#$ \7&1 1#   


#
#]$%"&&'892@
;S<  $ , +$ 	 7))$ \
 	
 

 
 
]$ %# $   #
%##!'# #899S
;><  #!$ " B$ B 7$ \ 7&16  71
#*#	
6#7]$%"&&'8928
;D<  /$   *1!$ "
$ % 
R
1,$2DS?
;29< 7"6$0"$"C($\
4
5(
$]+!$	)%#
,1    $ $ $2D>?
;22< 7 " 6$ "  $   F$ \	
  (
  
45($] 4$+8=$29=1288$2D>>


F * 0"	6*%*BU.
0 #
$  + ! +   

 


(!&

8֩֪$  1   1 $ 

V4֩֪W8512X#
%#(


12

(#(!4125O4844I25125W41
25Y25
84I25112%
#


#
#
#!
(#



6##   
     
$ !     
 

  !
    
  

"
#!#(#!


#

++

B6BBB 


;2< " $  Z#$   / ,$ [6#1  

 
R "
+#[%"&&'$899:

1260

Poster Abstract: A New Metric for Fault-Tolerance
in Sensor Networks
Bin Hao, Arunabha Sen and Bao Hong Shen
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287-5406

Email: {binhao, asen, bao}@asu.edu
ABSTRACT
Faults in some sensor networks are likely to be localized. The
conventional metric of fault-tolerance – connectivity of the
network graph – fails to capture any notion of locality. This
research introduces a new metric – region-based connectivity –
that incorporates the notion of locality.

In some applications of sensor networks, faults are likely to be
localized. This is particularly true in military applications, where
an enemy bomb may destroy a large number of sensor nodes
confined in a particular region. Such an attack may inflict
significant damage to a part of the network, while leaving the
remaining parts unaffected. Thus, massive but localized faults
become two signature characteristics of failure in sensor networks.

Categories and Subject Descriptors
C.2.1 [Network Architecture and Design]

General Terms
Algorithms, Design, Reliability.
(a)

Keywords
Sensor network,
connectivity.

fault-tolerance,

connectivity, region-based

(b)

Figure 1
Figure 1 (a) and (b) show a sensor network and a fault affected
region in the network, respectively. As one may notice, the
conventional metric connectivity of the network graph fails to
capture any notion of locality. Hence, in this research, we
introduce a new metric – region-based connectivity – to
incorporate the notion of locality.

1. INTRODUCTION
A sensor network is composed of a large number of sensor nodes
distributed in a geographical region. The communication network
formed by the sensor nodes is referred to as a sensor network.
Sensor networks have been extensively studied by researchers for
the last few years due to its wide application potential – military,
environmental, health, home, etc. [1]. Among various factors,
such as scalability, fault-tolerance, production cost and others
that impact the design of sensor networks, fault-tolerance plays a
very important role. Fault-tolerance in sensor networks is the
ability of the operational nodes of the network to remain
connected after failure of some of the sensor nodes due to the
men-made or natural causes. The metric used to measure the faulttolerance capability of a sensor network is the connectivity of the
network topology (graph)[2][3]. In graph theoretical terms, the
connectivity (node/link) of a graph is the minimum number of
nodes (links) that has to be removed before the graph is
disconnected. A sensor network will remain connected after
failure of any k – 1 node or link failures, if it is k-connected. Such
a network is said to be able to tolerate up to k – 1 failures.

2. Region-Based Connectivity
Region-based connectivity can be informally defined as the
minimum number of nodes that has to fail within any region of
the network before it is disconnected. A region may be defined
either with reference to the network graph or with reference to the
network geometry – the layout of the network. With reference to
the network graph, a region may be defined as a sub-graph with
diameter d (the diameter of a graph is the maximum of the
shortest path distance between a pair of nodes, taken over all
source-destination node pairs. With reference to the network
geometry, a region may be defined as a collection of nodes and
links of the network graph covered by a circular area in the
network layout. It may be noted that although the number of such
circular areas is infinite, as the number of nodes and links are
finite, the number of regions will also be finite.
Formally, the region-based connectivity, κd(G) may be defined as
follows: let R1,…, Rj be the set of all possible regions of the graph
G. For each i = 1,…, j, let T[i] indicate the number of nodes in
region i whose failure will disconnect G. Set T[i] equal to infinity,
if G remains connected after the failure of any subset of nodes in
region i. The region-based connectivity of the layout of graph G,
with a circular region of diameter d is κ d (G ) = min T [i ] .

Copyright is held by the author/owner(s).
SenSys’04, November 3–5, 2004, Baltimore, Maryland, USA.
ACM 1-58113-879-2/04/0011.

1≤i ≤ j

289

distance is exactly d), whose distance from each of the two nodes
is exactly d/2. These points are the centers of all possible regions

The connectivity of a graph G and its region-based connectivity
may be widely diverse. Consider the network graph shown in
figure 2(a). If the region is defined to be a sub-graph of diameter
2, then the region-based connectivity of this graph is m + 1,
although the connectivity is 2. Thus, the difference between the
region-based connectivity and connectivity may be arbitrarily
large.

Step 1. In the network graph, G, formed by sensor nodes, for each
set of nodes, Ri, do the following:
Step 1.1 Let C1,…, Ck be the connected components of G/Ri.
Shrink each Cj to a single node cj, while keeping all the links
between Cj and Ri

If the sensor network is to tolerate up to k node failures, and it is
known the fault will be restricted to a region (say, a circular area
of diameter d), then it is cost effective to design a network
topology whose region-based connectivity is k + 1, instead of a
topology whose connectivity is k + 1. This is illustrated with the
help of an example in figure 2(b). The positions of the sensor
nodes and the distances between them are shown in figure 2(b). If
the fault is restricted to a circular region of diameter 5 meters and
the transmission range is 10 meters, then the resulting graph will
have a connectivity of 2 and a region-based connectivity of 3 (as
shown in figure 2(b)). In order to realize a topology with
connectivity 3, the transmission range has to increase to 10 3
meters, that is more than 71% increase in transmission range.
Since power consumption is proportional to the square of the
transmission range, there has to be 250% increase in power. This
is unnecessary, as it is known that faults will be restricted to a
circular area of diameter 5 meters.

Step 1.2 For each cj obtained in Step1.1, replace cj by a clique of
size (|Ri| + 1), and connect each node in the clique to all neighbors
of cj.
Step 1.3 Computer the connectivity, κ, of the graph obtained in
Step1.2. If κ ≤ |Ri|, set κ(Ri) = κ; otherwise, set κ(Ri) = ∞
Step 2. Set the region-based connectivity, κd(G) = min{κ(Ri)}

3.2 Design Sensor Network with Specified
Region-Based Connectivity
In the following we give a polynomial algorithm to find the
minimum transmission range of the sensors needed to form a
network topology with region-based connectivity K. The input to
the algorithm is (i) the position of the sensors in the plane, (ii)
diameter of the circular fault region, and (ii) the desired regionbased connectivity K. The output of the algorithm is the
transmission range of the sensor nodes

u

a1

a2

am

x1

bm

y1

x2

xm

Alg. MinTranRange

Km,m

Km,m
b1

b2

y2

Step 0. Compute all the distances between any two sensor nodes,
and restore them in a list, L, in an increasing order

ym

v

Step 1. Execute a binary search on L: For each visited value in L,
use it as transmission range, and call Alg. BConn to compute the
region-based connectivity κd of the corresponding network graph.

(a)

If κd ≥ K, return “<”, so binary search will visit a smaller value.
If κd < K, return “>”, so binary search will visit a larger value.
The value returned by the binary search is the minimum
transmission range needed.

4. REFERENCES

(b)

[1] I.F. Akyildiz, W. Su, Y. Sankarasubramaniam and E.
Cayirci, “Wireless Sensor Networks: a Survey”, Computer
Networks, vol. 38, pp. 393-422, 2002.Ding, W., and
Marchionini, G. A Study on Video Browsing Strategies.
Technical Report UMIACS-TR-97-40, University of
Maryland, College Park, MD, 1997.

Figure 2

3. Algorithms for Region-Based Connectivity
3.1 Region-Based Connectivity Computation
In the following, we give a polynomial time algorithm to compute
the region-based connectivity of a sensor network G = (V, E) with
its geometric layout, and a region diameter d.

[2] C. Bettstetter, “On the minimum node degree and
connectivity of a wireless multihop network”, Proceedings of
ACM MOBIHOC, 2002.Lamport, L. LaTeX User’s Guide
and Document Reference Manual. Addison-Wesley,
Reading, MA, 1986.

Alg. RBConn
Step 0. In the network geometry, identify R1,…, Rj – the sets of
nodes covered by all possible regions.

[3] C. Bettstetter and C. Hartmann, “Connectivity of wireless
multihop networks in a shadow fading environment”,
Proceedings of ACM MSWiM'03, pp. 28-32, 2003.

The centers of possible regions are found as follows:
For each pair of sensor nodes with distance in between no more
than d (region diameter), find the two points (one point if the

290

2011 European Intelligence and Security Informatics Conference

A System for Ranking Organizations using Social
Scale Analysis
Inayah Rochmaniyah, Ali Amin

Sukru Tikves, Sujogya Banerjee, Hamy Temkit, Sedat Gokalp,
Hasan Davulcu, Arunaba Sen, Steven Corman, Mark Woodward

Center for Religious and Cross Cultural Studies
Gadjah Mada University, Yogyakarta, Indonesia

Arizona State University, P.O. Box 87-8809, Tempe, AZ, 85281 USA

{rochmaniyah, aleejtr77}@yahoo.com

{stikves, sujogya, mtemkit, sgokalp, hdavulcu, asen, scorman, mataram}@asu.edu
Tel:(602) 206-8641, Fax: (480) 965-2751

Abstract—In this paper we utilize feature extraction and model
fitting techniques to process the rhetoric found in the web sites
of 23 Indonesian religious organizations – comprising a total of
37,000 articles dating from 2005 to 2011 – to profile their ideology
and activity patterns along a hypothesized radical/counter-radical
scale. We rank these organizations by assigning them to probable
positions on the scale. We show that the developed Rasch model
fits the data using Andersen’s LR-test. We create a gold standard
of the ranking of these organizations through an expertise
elicitation tool. We compute expert-to-expert agreements, and
we present experimental results comparing the performance of
three different baseline methods to show that the Rasch model
not only outperforms our baseline methods, but it is also the only
system that performs at expert-level accuracy.

radical organizations into pure clusters. Pure radical clusters
were easily identified due to high similarity among their
support for violent practices. Pure counter-radical clusters
were identified due to their strong reactionary opposition to
violent practices through protests and rhetoric. But the rest of
the groupings were mixed. We realized that binary labeling
as counter-radical or radical does not capture the overlap,
movement and interactivity among these organizations. In this
paper we hypothesize that both counter-radical and radical
movements in Muslim societies exhibit distinct combinations
of discrete states comprising various social, political, and
religious beliefs, attitudes and practices, that can be mapped
to a latent linear continuum or a scale. Using such a scale, an
analyst can determine where exactly along the spectrum any
particular group lies, and also potentially where it is heading
with its rhetoric and activity.

I. I NTRODUCTION
Being able to asses information on radical and moderate actors in a geographic area is an important research topic for our
national security. Radicalism is the ideological conviction that
it is acceptable and in some cases obligatory to use violence to
effect profound political, cultural and religious transformations
and change the existing social order fundamentally. Muslim
radical movements have complex origins and depend on diverse factors that enable translation of their radical ideology
into social, political and religious movements. In [1] Crelinsten
states that “both violence and terrorism possess a logic and
grammar that must be understood if we are to prevent or control them”. Therefore, analysis of Muslim radical and counterradical movements requires attention to the global, national
and local social, economic and political contexts in which
they are located. Similarly, in the Islamic context, counterradical discourse takes various different forms; discursive and
narrative refutations of extremist claims, symbolic action such
as ritual and other religious and cultural practices, and Islamic
arguments for pluralism, peaceful relations with non-Muslims,
democracy, etc. The most effective counter-radicals are likely
to be religiously conservative Muslims. Effective containment
and defeat of radicalism depends on our ability to recognize
various levels of radicalization, and detection of counterradical voices.
In our previous work [2], we attempted a clustering approach to obtain “natural groupings” of a number of local
non-government religious social movements and organizations
in Indonesia. Social scientists on our team observed that
clustering was not fully able to separate all counter-radical or
978-0-7695-4406-9/11 $26.00 © 2011 IEEE
DOI 10.1109/EISIC.2011.37

Given the complex nature of the task, such as regional
differences in local cultures, beliefs and practices, and in
the absence of readily available high accuracy parsers, highly
structured religio-social ontologies, and information extraction
systems; we decided to devise a multi-lingual non-linguistic
text processing pipeline that relies on only statistical modeling
of keyword frequency and co-occurrence information.
We worked with social scientists on our team to come
up with an orthogonal model comprising of two primary dimensions. Both dimensions, (i) radical/counter-radical and (ii)
violent/non-violent, are characterized as latent, partial orders
of discrete beliefs and practices based on a generalization of
item order in Guttman scaling [3] using a Rasch model [4].
A true Guttman scale is a deterministic process, i.e. if a
social movement subscribes to a certain belief or practice,
than it must also agree with all lower order practices and
beliefs on the scale. Of course, such perfect order is rare in
the social world. The Rasch model provides a probabilistic
framework for Guttman scales to accommodate for incomplete
observations and measurement errors.
In this paper we present feature extraction and model fitting
techniques to process the rhetoric found in the web sites of
23 religious Indonesian organizations – comprising a total of
37,000 articles dating from 2005 to 2011. We aim to profile
their ideology and activity patterns along the hypothesized
radical/counter-radical scale, and rank them by assigning them
308

is expected to respond positively to all the items of lesser
difficulty. For example, in order to find out how extreme a
subject’s view is on Guttman scale, the subject is presented
with the following series of items in question form: (1) Are
you willing to permit immigrants to live in your country?
(2) Are you willing to permit immigrants to live in your
community? (3) Are you willing to permit immigrants to
live in your neighborhood? (4) Are you willing to permit
immigrants to live to your next door? and (5) Are you willing
to permit your child to marry an immigrant? If the items form
a Guttman scale, any subject agreeing with any item in this
series will also agree with other items of lower rank-order
in this series. Guttman scale is a deterministic process and
the score of a subject depends on the number of affirmative
responses he has made on the items. So, a score of 2 for
a subject in the above Guttman scale not only means he
has given affirmative response to two of the questions or
items, but also indicates that he agrees with two particular
questions, namely the first and second. Scores in Guttman
scale can also be interpreted as the “ability” of a subject in
answering questions sorted in increasing order of “difficulty”.
These scores when presented on an underlying scale, give us
an ordering of the subjects based on their “ability” too.
The objective of our paper is to order the Indonesian
Islamic organizations based on their views on religio-social
keywords which have an inherent ordering. For example, two
such keywords are “Quran” and “Sharia”. An organization
supporting “Sharia” will also likely to “believe in Quran”. So
it makes sense to use Guttman scaling procedure to rank the
organizations and their beliefs and practices. One drawback of
Guttman scale is that it is deterministic and assumes a strict
ordering of the items. In real world, it is difficult to order all
the items in such a strict level of increasing difficulty, therefore
perfect scales are not often observed in practice. Furthermore,
many times, the order of the items are not known since they are
not straightforwardly comparable. Also measurement errors
might lead to responses that do not strictly fit the ordering.
As a result we can no longer conclude deterministically that
if a subject answers a question affirmative, whether she will
be able to give affirmative answers to other questions of lower
order in the same questionnaire. We use Rasch model to
overcome this drawback by taking into account measurement
error.

to probable positions on this scale [5]. We used the eRm1
package to fit the Rasch model on this data set, and identify
organizations’ positions based on maximum likelihood estimation [6]. The automated ordering of organizations is formed
by ranking the organizations according to their estimates on
the latent scale. We show that the model fits the data using the
Andersen’s LR-test[7]. We also created a gold standard of the
ranking of these organizations through an expert opinion elicitation tool, and through the opinions of three ethnographers
on our team who collectively possess 35 years of scholarly
expertise on Indonesia and Islam. We computed expert-to-gold
standard agreements, as well as compared the performance of
three different baseline computational methods to show that
the Rasch model presented here not only performs the best
among the baseline methods, but that it also performs at an
expert level of accuracy.
A. Organization of the paper
Next section provides an introduction to the theory of
Guttman Scaling and Rash Models. Section III defines the
problem, presents the system architecture, and the methods
used to solve the problem. Section IV describes the Indonesian
corpus, expert opinion elicitation tool, baseline computational
methods, and experimental evaluations. Section V concludes
the paper.

II. I NTRODUCTION OF G UTTMAN S CALING AND R ASCH
M ODEL
In social science scaling is a process of measuring and
ordering entities called subjects based on their qualitative
attributes called items. In general, subjects are requested to respond to surveys conducted by means of structured interviews
or questionnaires. Items are presented to the subjects in form
of questions. Statistical analysis of the response of the subjects
on the questions about items are used in scaling the subjects.
Some of the widely followed scaling procedure in social
science surveys are Likert scale [8], Thurnstone scale [9], and
Guttman scale [10]. Guttman scaling procedure orders both
the subjects and the items simultaneously with respect to some
underlying cumulative continuum. In this paper we follow the
Guttman scaling process to rank the organizations based on
their response on the radical and counter-radical keywords.

B. Rasch Model

A. Guttman Scaling

Rasch model [4] provides a probabilistic framework for
Guttman scales. In Rasch model, the probability of a specified
binary response (e.g. a subject agreeing or disagreeing to an
item) is modeled as a function of subject’s and item’s parameters. Specifically, in the simple Rasch model, the probability
of a positive response (yes) is modeled as a logistic function
of the difference between the subject and item’s parameters.
Item parameters pertain to the difficulty of items while subject
parameters pertain to the ability of subjects who are assessed.
A subject of higher ability relative to the difficulty of an item,
has higher probability to respond to a question affirmatively. In

A Guttman [3] scale presents a number of items to which
each subject is requested to provide a dichotomous response,
e.g. agree/disagree, yes/no, or 1/0. This scaling procedure is
based on the premise that the items have strict orders (i.e., the
items are presented to the subjects ranked according to the
level of the item’s difficulty). An item “A” is said to be “more
difficult” than an item “B” if any subject answering “yes”
on item “A” implies that the subject will also answer “yes”
on item “B”. A subject who responds to an item positively
1 http://r-forge.r-project.org/projects/erm/

309

this paper Rasch models are used to assess the organizations
degree of being radical or counter-radical based on the religiosocial keywords (items) appearing in their rhetoric.
Rasch model also maps the responses of the subjects to
the items in binary or dichotomous format , i.e., 1 or 0. Let
Bernoulli variable Xvi denotes the response of a subject v to
the item i, variable θv denotes the parameter of “ability” of
the subject v and βi denotes the parameter of “difficulty” of
an item i. According to simple Rasch model the probability
that subject v responds 1 for item i is given by
P (Xvi = 1|θv , βi ) =

and counter-radical keywords. These keywords represent the
radical and counter-radical beliefs and practices of the organizations. An organization responding “yes” to a feature means
the organization exhibits that feature while an organization
responding “no” to a feature indicates that the organization
does not exhibit such a feature. Difficulty of an item translates
to strength of the corresponding attitude in defining radical or
counter-radical ideology of any organization. Similarly ability
of a subject in this case means degree of radicalism or counterradicalism exhibited by an organization’s rhetoric. Details of
keyword selection is presented in the next section.

exp(θv − βi )
1 + exp(θv − βi )

III. M ETHOD
A. Problem Definition

Rasch model assumes that the data under analysis have the
following properties
1) Unidimensionality: P (xvi = 1|θv , βi , α) = P (xvi =
1|θv , βi ), i.e., the response probability does not depend
on other variable
2) Sufficiency: sum of responses contains all information
on ability of a subject, regardless which item it has
responded
3) Conditional independence: for a fixed subject there is
no correlation between any two items
4) Monotonicity: response probability increases with higher
values of θ, i.e., subject’s ability
Pn
Items with si = v xvi value of 0 or n, and subjects with
Pk
rv = i xvi value of 0 or k are removed prior to estimation,
where n is the total number of subjects and k is the total
number of items. Running Rasch model on the data gives us
an Item parameter estimate or a score for each item. Generally
the estimation of βi or score for a item i is calculated through
Conditional Maximum Likelihood (CML) estimation [11]. The
conditional likelihood function for measuring item parameter
estimate is defined as
Y
exp(−βi si )
Lc =
P (xvi |rv ) = Q P
r
x|r exp(−βi xvi )
v

The goal of this study is to build a semi-automated method
to rank religious organizations from a certain geographical
region on a scale of radicalism vs. counter-radicalism using
their web sites. The efficacy of the generated model is evaluated by comparing it against baseline methods and expert level
performance.
B. System Architecture

where r represents the sum over all combinations of r items.
Similarly maximum likelihood is used to calculate subject
parameter estimation θv or score for each subject.
In order to evaluate the quality of these measurements we
run Anderson Likelihood Ratio test (LR-test) [7] on the set of
data. The test gives us a goodness of fit of the data in Rasch
model, i.e., it tells us whether the data follows the assumptions
of Rasch model. A p-value, returned by the test, indicates the
goodness of fit and a p-value2 higher than 0.05 indicates no
presence of lack of fit.

Fig. 1.

A model of the system architecture.

The system architecture is shown in Figure 1. Here the flow
of the processes and data can be seen as interactions between
experts and automated modules. The system works as follows:
• Initially, social scientists use their technical and area
expertise to identify a set of organizations, and hypothesize any number of unipolar or bipolar scales that could
explain the variance among the beliefs and practices of
the organizations. In this paper, we primarily focus on
the construction and validation of the bipolar radical vs.
counter-radical (R/CR) scale, however the techniques can

C. Implementing in Text Mining Domain
In this paper, we use Guttman scaling and Rasch model to
find a ranking of some political organizations based on how
extreme their views are on radicalism and counter-radicalism.
In our project, model subjects are a group of religious organizations and items are a set of socio-religious radical
2 http://en.wikipedia.org/wiki/P-value

310

•

•

•

be readily applied to the construction and validation of
any other relevant scale.
Next, we crawl and download the web sites of the
organizations, and the system automatically extracts the
top-k candidate keywords for consideration in the hypothesized scale. Social scientists screen the list of extracted
keywords, and select the relevant ones for inclusion in
further analysis.
The system builds response tables; a pair of tables for
a bipolar scale (such as R/CR), or a single table for a
unipolar scale, by thresholding the occurrence frequencies of the selected keywords in the organizations’ web
corpus. See Fig. 3 and Fig. 4 for the response tables for
the R/CR scale. The response tables are fed as input to the
Rasch Model building algorithm. The algorithm produces
a metric to validate the fitness of the model, and rankings
of the organizations and keywords.
In parallel to this, two types of other information are collected for evaluation purposes. First, expert rankings of
the organizations, using a graphical drag-and-drop expert
opinion elicitation tool shown in Fig. 2). Expert rankings
are merged into a consensus gold standard of rankings.
Next, two other computational baseline methods; one
based on simple sorting, and another based on principal
component analysis [12], are used to generate alternative
computational rankings shown in Fig. 5.

This task was performed in a simple three step procedure;
initially the occurrence frequencies of particular keywords
were counted within each organization’s corpus, then a threshold matrix was calculated from the initial values, and finally
a binary response matrix was generated by applying these
thresholds to the initial values.
The frequency metric is shown in formula (1), where k is
the keyword, o is the organization, and Do is the document
set pertaining to that particular organization.
fo,k =

|{d | k ∈ d, d ∈ Do }|
|Do |

(1)

A threshold value for each keyword is calculated by taking
the median of the values in the related column. Median was
preferred over mean as a threshold, since the distribution of the
values did not fit Gaussian distribution, yet median empirically
proved to be a better measure.
Finally, each element was converted into a binary value by
comparing it to the column’s threshold. English translations
of the keywords is presented for clarity in Fig. 3 and Fig.
4, additionally names of the corresponding organizations are
anonymized consistently in all figures and tables.
E. Model Fitting
We fit the Rasch model on two datasets - (1) radical
organizations with radical keywords and (2) counter-radical
organizations with counter-radical keywords. We used the eRm
package in R, an open source statistical software package3 , to
fit a Rasch model to the dataset, and obtain the organizations’
scores on the latent scale, which are the the subject parameter
estimates (θv ) discussed in previous section. The eRm package4 fits Rasch models and provide subjects or organizations
parameter estimates based on maximum likelihood estimation.
The automated scale of the organizations is formed by
ranking the organizations according to their estimates on
the latent scale. Not only we can provide the organization
estimates but we can also assess whether the model fits the
data by looking at several goodness of fit indices, such as the
Andersen’s LR-test.

C. Keyword Selection
In order to identify candidate keywords, one option was
to translate the documents into English and apply readily
available keyword extraction methods [13]. However it was
preferable to preserve the original expression of the phrases in
the original language. Therefore, we utilized a non-linguistic
technique that relies only on statistical occurrence and frequency information.
Within each document, the words were separated by whitespace or punctuation marks. We considered each keyword to
be an n-gram of one to three words. We treated each organization as one document and calculated the term frequency
- inverse document frequency (TF-IDF) [14] values for every
single n-gram mentioned by these organizations. The n-grams
with highest TF-IDF value gave us the topics that each of
the organization discusses most. Then the top 100 n-grams
from each organization were made into a list of candidate
keywords. Finally, belief and practice keywords that belong to
one of the following categories {social, politics, economics,
religion} were manually identified by the experts as relevant
for inclusion. This process assessed a total of 790 candidate
keywords; of which 29 and 26 were selected by experts for
inclusion in the radical and counter-radical scales respectively.

IV. E XPERIMENTAL E VALUATION
A. Indonesian Corpus
The corpus domain is the online articles published by the
web sites of the 23 religious organizations identified in Indonesia, in the Indonesian language. These sources are the web
sites or blogs of the identified think tanks and organizations.
As discussed in the introduction, each source was classified
as either radical or counter-radical by the area experts. We
downloaded a total of 37,000 Indonesian articles published in
these 23 web sites, dating from 2005 to 2011. For each web
site, a specific REGEX filter was used to strip off the headers,
footers, advertising sections and to extract the plain text from
the HTML code.

D. Feature Extraction
After identifying the keywords for the analysis, we needed
to search the web site corpus of the organizations for the
matching items. This yielded a term-document matrix.

3 http://cran.r-project.org/
4 http://r-forge.r-project.org/projects/erm/

311

Fig. 3. Radical subset of organizations and keywords, sorted according to
aggregate row values.

Fig. 4.
Counter-Radical subset of organizations and keywords, sorted
according to aggregate row values.

Fig. 2. The visual interface of the expert opinion collector for manually
placing the organizations on the two dimensional scale

For two exactly matching rankings, the error(G, R) will be
zero, whereas for two inversely sorted rankings it is expected
to be 0.5 (when the size of O is even). Also a random ranking
is expected to have a error of 0.375.

B. Expert Opinion and Gold Standard of Rankings
We collaborated with three area experts, who collectively
possess 35 years of scholarly expertise on Indonesia and
Islam. In order to build a gold standard of orderings of
the organizations, we built a graphical drag-and-drop user
interface tool to collect the opinions of each of the area experts.
A screenshot of the tool is shown in Fig. 2.
Each expert separately evaluated and ranked the organizations in the dataset according to a two dimensional scale of
radical/counter-radical (R/CR) and violent/non-violent (V/NV)
axis. The consensus among the experts was high; since per
item standard deviations among the experts’ scores along the
R/CR axis over a range of [−10, 10], across all organizations
were 2.75. Also, 90% of the items have less than 22.6%
difference in their rankings. The individual scores for each
organization were combined and averaged to obtain the consensus gold standard rankings along the hypothesized R/CR
scale.

D. Expert-to-Gold Standard Error
We calculated the error between each expert’s ranking and
their consensus gold standard of rankings. The first expert’s
error measure is 0.06, and the second and third expert’s errors
are 0.12 and 0.14 correspondingly as shown in the last row of
the table in Fig. 5. The average error of our experts against
their gold standard ranking is 0.11.
E. Baseline - Sorting with Aggregate Score
The first baseline we used was constructed by sorting the
organizations according to the number of different keywords
observed in their corpus. While this provided a pattern similar to a Guttman Scale, and orderings of the organizations
matched to a certain degree with the gold standard as shown
in Fig. 5, the error for this baseline was 0.19, which is higher
than the average expert’s performance.

C. Computationally Generated Scale
The ranking discovered by the Rasch model fitting the
corpus has been evaluated against the gold standard rankings
of the organizations provided by the experts. The difference
between two separate rankings have been calculated by using
the following misplacement error measure in Equation 2.
P
|G(o)−R(o)|
error(G, R) =

o∈O

|O|

|O|

F. Baseline - Principal Component Analysis
A stronger baseline was built by employing principal component analysis [12], and sorting the organizations according
to their projections in the first principal component of the
term-document matrix. Since experts selected the R/CR scale
relevant keywords only, it was expected that the first principal
component would reflect the corresponding scale. PCA proved
to be performing better than the aggregate score sorting, with
an error measure of 0.18. However, this error rate is still
higher than the error rate of each expert.

(2)

Here, O is the set of organizations, G and R are one to one
mapping functions of rankings from set O to range [1, |O|].

312

G. Performance of the Rasch Model Ranking System
The p-values from the Anderson LR goodness of fit test
from model (1) and model (2) (mentioned in section III-E)
are 0.85 and 0.669 respectively, suggesting no evidence of
lack of fit. The Rasch models allow us to get a natural order of
the organizations, according to their “abilities”, i.e.: radicalism
and counter-radicalism in this case. This system had an error
measure of 0.10, which actually provided a higher ranking
performance than the average performance of our experts’ –
performing better than the majority of our area experts.
H. Evaluations
Our experiments showed that the hypothesized compatibility
of the R/CR scale for the Indonesian corpus is valid. Not only
the Rasch model was statistically fitting the response matrix,
but also the generated ranking performance was better than
the average expert performance. Among our computational
baseline methods, the Rasch Model was the only method
producing expert-level performance as shown in Fig. 5. This
preliminary analysis with the R/CR scale shows that when
experts assist the system with keyword selection, the web
corpus of organizations provides rich enough information
and patterns to enable a computational method to rank them
accurately.

Fig. 5.

Computational and expert rankings

R EFERENCES
[1] R. Crelinsten, “Analysing terrorism and counter-terrorism: A communication model,” Terrorism and Political Violence, vol. 14, pp. 77–122,
2002.
[2] H. Davulcu, S. T. Ahmed, S. Gokalp, M. H. Temkit, T. Taylor,
M. Woodward, and A. Amin, “Analyzing sentiment markers describing
radical and counter-radical elements in online news,” in Proceedings of
the 2010 IEEE Second International Conference on Social Computing,
ser. SOCIALCOM ’10. IEEE Computer Society, 2010, pp. 335–340.
[3] L. Guttman, “The basis for scalogram analysis,” Measurement and
prediction, vol. 4, pp. 60–90, 1950.
[4] D. Andrich, Rasch models for measurement. Sage, 1988, no. 68.
[5] R. D. McPhee and S. Corman, “An activity-based theory of communication networks in organizations, applied to the case of a local church,”
Communication Monographs, vol. 62, pp. 1–20, 1995.
[6] L. Le Cam, “Maximum likelihood an introduction,” ISI Review, vol. 58,
no. 2, pp. 153–171, 1990.
[7] D. Hessen, “Likelihood ratio tests for special rasch models,” Journal of
Educational and Behavioral Statistics, vol. 35, no. 6, p. 611, 2010.
[8] R. Likert, “A technique for the measurement of attitudes,” Archives of
Psychology, vol. 140, pp. 1–55, 1932.
[9] L. L. Thurstone, “Attitudes can be measured,” American Journal of
Sociology, vol. 33, pp. 529–554, 1928.
[10] J. McIver and E. Carmines, Unidimensional Scaling. Sage Publications,
Inc, 1981, vol. 24.
[11] Y. Pawitan, In all likelihood: statistical modelling and inference using
likelihood. Oxford University Press, USA, 2001.
[12] I. Jolliffe, Principal Component Analysis. Springer Series in Statistics,
2002.
[13] W. Michael and J. Kogan, Text Mining: Applications and Theory. Wiley,
2010.
[14] G. Salton and C. Buckley, “Term-weighting approaches in automatic
text retrieval,” in Information Processing and Management, 1988, pp.
513–523.
[15] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—but
is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the Conference on Empirical Methods in
Natural Language Processing, ser. EMNLP ’08. Stroudsburg, PA, USA:
Association for Computational Linguistics, 2008, pp. 254–263. [Online].
Available: http://portal.acm.org/citation.cfm?id=1613715.1613751
[16] A. W. James and L. M. John, “Algebraic representations of beliefs
and attitudes: Partial order models for item responses,” Sociological
Methodology, vol. 29, pp. 113–146, 2002.

V. C ONCLUSIONS AND F UTURE W ORK
In our experiments, not only did the data show fitness with
the Rasch Model for the R/CR scale, but also the Rasch
rankings of the organizations are better than the output of the
other baseline computational methods, and they are at expert
level performance when compared with the consensus gold
standard rankings.
Rasch Model also provided us with another output, namely
the ranking of selected keywords (items) on the R/CR scale.
Although preliminary observations indicates that this can be
a valuable asset by itself, we plan to further investigate the
quality and utility of this ranking as future work.
While the model has been demonstrated to fit on the R/CR
scale, two major expansion points can be investigated in
the future work, namely the violent/non-violent scale, and
enhancement of feature selection. Although our experts have
identified a second dimension, evaluating its correlation to
R/CR axis, or existence of other significant ones could be
beneficial. Additionally, the features can be enhanced by
experimenting with the significance of the radical keywords
in the counter-radical organization corpuses, and vice-versa.
Other interesting work includes making our expert opinion
elicitation tool available online to a wider and more geographically distributed audience to crowdsource [15] the needed
expertise for making lists of local organizations, identifying
their web sources, and overcome the complex task of construction and validation of significant and fitting scales. Another
interesting dimension is to look at synthesis and analysis of
scales that do have a strict hierarchy of keywords, but adhere
to more flexible partial order models [16].

313

170

IEEE TRANSACTIONS ON

CLi A-C K

OULl

I

C-La
tA s

o

<
0

<

L

0CK

node

lAcS
1,

IScan,

T

J-L--J-L---FL SCK Moce

I-1.
a

X

Norrnqc

-

Fig. 9. Ternary clock-signal evels and timing.

COMPUTERS, VOL.

c-35, NO. 2,

FEBRUARY

1986

works assume the system to be modeled as in [4]. In this correspondence,
we consider a more general model of the system and study the diagnosability criteria in presence of three-valued test outcomes. In this model,
each unit is tested jointly by a number of other units of the system as
opposed to each test being carried out by a single unit of the system as in
[4]. Necessary and sufficient conditions for the diagnosability of a system
under this general model have been presented in this correspondence.
Throughout the correspondence, diagnosability without repair has
been considered.

Index Terms -Connection assignment, diagnosability, fault tolerance, graph models, multiple-valued test outcome, self-diagnosable system, syndrome.
I. INTRODUCTION

Fig. 10. A ternary slave-clock generator circuit.

of complexity. However for the binary-with-decoder scheme, twoclock level-sensitive operation is used in the normal mode only. For
the scan mode, single-clock operation is employed. In contrast, for
the ternary scheme proposed in this correspondence, the preferred
two-clock level-sensitive mode is used in both normal and scan
modes. This implies that the ternary-scan scheme can be made
inherently more reliable than can the binary-with-decoder scheme.
REFERENCES

[1] E. B. Eichelberger and T. W. Williams, "A logic design structure for LSI
testability," in Proc. 14th Design Automat. Conf., 1977, pp. 462-468.
[2] M. R. Mercer and V. D. Agrawal, "A novel clocking technique for
VLSI circuit testability," IEEE J. Solid-State Circuits., vol. SC-19,
pp. 207-212, Apr. 1984.
[3] M. J. Y. Williams and J. B. Angell, "Enhancing testability of LSI circuits
via test points and additional logic," IEEE Trans. Comput., vol. C-22,
pp. 46-60, Jan. 1973.
[4] M. Hu and K. C. Smith, "On the use of CMOS ternary gates to realize a
self-checking binary logic system," in Proc 11th Int. Symp. MultipleValued Logic, May 1981, pp. 212-217.
[5] T. Higuchi and M. Kameyama,."Ternary logic system based on T gate,"
in Proc. 5th Int. Symp. Multiple-Valued Logic, May 1975, pp. 290-304.
[6] K. C. Smith, "The prospects for multivalued logic: A technology
and applications view," IEEE Trans. Comput., vol. C-30, pp. 619-634,
Sept. 1981.
On the Diagnosability of a General Model of
System with Three-Valued Test Outcomes

A. SENGUPTA AM A. SEN
Abstract -The problem of diagnosability of a system with three-valued
test outcomes was considered in earlier works [11-[3]. However all these

Manuscript received May 28, 1985; revised September 13, 1985.
The authors are with the Department of Computer Science, University of
South Carolina, Columbia, SC 29208.
IEEE Log Number 8406764.

The need of reliable computing systems has motivated the researchers to investigate about diagnosable systems where the different units of the system test each other. Preparata et al. [4] first
introduced the concept of diagnosable systems and diagnosability of
systems where, in presence of failures, the faulty units can be
uniquely identified. Since then, considerable research has been reported in the literature on diagnosable systems in connection with
their diagnosability [1]-[13]. Most of these (except [5], [7], and
[11]) are concemed with the model of the system introduced in [4],
assuming only binary test outcomes to be present (except [1]-[3]).
Butler [1] presented some properties of a system in connection with
its diagnosability in case of three-valued test outcomes. The necessary and sufficient conditions for the diagnosability of a system with
three-valued test outcomes were presented in [3]. However, all these
works [1]-[3] involving three-valued test outcomes assume the system being modeled as in [4]. In this correspondence, we consider a
more general model of the system proposed in [14] and study its
diagnosability in presence of three-valued test outcomes. We will
first describe the model briefly and compare it to the other existing
models.
The model of the system can be represented as follows. A system
is assumed to be composed of a number of units and to check
whether the units are functioning properly, each unit is tested by a
number of tests. Each of these tests is carried out (or contributed) by
a number of other units of the system (the model [4] effectively
assumes that each test is contributed only by a single unit of the
system). Depending on the status (i.e., faulty or fault-free) of the
units contributing to a test, the test might be a valid or an invalid
test. The outcome of each such test is three-valued. We will consider
two cases with respect to the test outcomes: a) incomplete test
outcomes and b) incorrect test outcomes. In case a, the outcome is
0 if the tested unit passes the test, 1 if it fails the test and 2 if the
result of testing is somehow missing possibly because of faulty
transmission. Whether an outcome of a test represents the true status
of the tested unit depends on whether the test is valid. We will
assume that the "finding" of a valid test is always correct so long as
the outcome is not missing, whereas the outcome of a test does not
convey any information about the status of the tested unit if the
outcome is missing or the test is invalid. Thus, if the outcome of a
valid test is 0, then the tested unit is fault-free and if it is 1, the tested
unit is faulty. The case b arises in case of intermittent failures. If a
unit is intermittently faulty, then a valid test might discover it to be
fault-free possibly because the unit was working in a fault-free manner when the testing was carried out. 4ccordingly, a three-valued
test outcome 0, 1, O' was considered in t2] where the test outcomes
0 and 1 represent the "finding" of the tested unit exactly as before,
whereas the outcome o' represents the incorrect pass of an intermittently faulty from a valid test. For diagnosis purpose, of course, O'
behaves just like a 0. In both cases a and b, the diagnosability
problem arises because of the fact that two distinct sets of faulty
units may produce the same set of test outcomes. In case a, we will
say a system is t/x diagnosable if all the faulty units can be uniquely
identified from the set of test outcomes whenever there are no more
than t faulty units and no more than x missing test outcomes. In case

0018-9340/86/0200-0170$01.00 © 1986 IEEE

171

IEEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 2, FEBRUARY 1986

b, we will say a system to be t[x] diagnosable if all the faulty units
can be uniquely identified from the set of test outcomes whenever no
more than t units are faulty and no more than x test outcomes are incorrect pass.
The model may be represented by (G, B, F) where G is a bipartite
digraph, B is a set of Boolean symbols and F is a set of Boolean
functions. The digraph G is represented by (C U T, E) where
C U T is the set of nodes of G where each element of C represents
a unit of the system and each element of T represents a test applicable to at least one unit of the system. E is the set of edges of G and
can be represented as
E C (C x T) U (T x C)

such that an edge from a node representing a unit (henceforth referred as a unit node) to a node representing a test (henceforth
referred as a test node) means the unit contributes to the test and an
edge from a test node to a unit node means that the unit is tested by
that test. If the system has N units, the set B is a set of N Boolean
symbols and the ith symbol reflects the status of the ith unit. The ith
element bi of B will have a value 1 if the ith unit is faulty and a value
O if it is fault free. If there are M tests in the system, the set F is a
set of M Boolean functions of bl, b2, * * *, bN. The jth element fj of
F gives in Boolean functional form, under what condition thejth test
turns out to be invalid, depending on the status of the units contributing to the jth test. Depending on the values of the b's, if the
value of any fj turns out to be 1, then the jth test tj is invalid;
otherwise, the test remains valid. Naturally, as the number of faulty
units increases, the number of invalid tests may increase or remain
the same, but can never decrease, and as such, each of the functions
fj is a positive function. This model, therefore, reduces to the model
in [4], if eachfj in F, 1 ' j ' M, contains a single term having only
one literal and reduces to the model in [7], [11] if each f, 1 j <
M, in its sum-of-product form, has each term containing a single
literal. This model reduces to the model in [5] if each test node has
only one outgoing edge. For obvious reasons, we will assume that
no unit contributes to a test which tests itself, i.e., if for some i,j,
(ce, tj) E E then (tj, ci) t E. The finding of each test about the unit
it is testing may be represented in the digraph G by labeling the
corresponding test node to unit node edge, i.e., the outcome of the
test tj applied to the unit ci is used as a label of the edge (tj, c'). A
complete set of test outcomes will be referred as a syndrome.
Because an invalid test may produce a test outcome 0 or 1 irrespective of the nature of the tested unit, and some of the test outcomes may be missing also, whenever some of the units are faulty
causing at least one of the tests to be invalid, the syndrome that is
possible is not unique. Consider a fault situation, where an S C C
represents the set of faulty units of the system. Any of the syndromes which may be produced under this fault situation with no
more than x missing test outcomes will be referred as S consistent
in connection with incomplete test outcomes. Similarly, in presence
of intermittent failures, because there might be incorrect test outcomes O' representing incorrect pass, a produced syndrome under
this fault situation (having no more than x O' outcomes) after replacing each O' by 0 will be called S consistent in connection with
incorrect pass test outcomes. It might be noted that a O' outcome can
occur only with a valid test on an intermittently faulty unit. If there
exist distinct Si, S2 C C, such that there exists at least one syndrome
which is Si consistent as well as S2 consistent, then the faulty
components cannot be identified uniquely from the syndrome and
as such the system will be referred as a nondiagnosable system.
Conversely, a system is diagnosable if and only if the faulty units
can be uniquely identified from the syndrome, i.e., if a syndrome
is S consistent, then for no S2 distinct from S1, the syndrome is
52 consistent.
In Section III of this correspondence, we present necessary and
sufficient conditions a system, represented as above, should satisfy
in order to be tlx diagnosable. In Section IV, we present similar
results for the t[x] diagnosability of a system. Section II includes
some definitions and notations for discussions in Sections III
and IV.

II. DEFINITIONS AND NOTATIONS

Throughout this correspondence we will assume that no more
than n units can be become faulty simultaneously and no more than
x test outcomes can be missing or incorrect pass. Suppose the
system has N units and M tests. Let bi and fj, respectively, denote
the ith and the jth element of B and F. For some given S C C and
U C T, let NO_EDGES(U, S) denote the cardinality of the follow-

ing set:

{(ti, cj) ti E

U and

cj E S and (ti, cj) E E}.

Let CONTR(tj) denote the set of units contributing to the test
tj E T. If CONTR(tj) = {cj,, cj2,*.*, Cjk}, thenfj is a Boolean function in bj,, bj,
bj,k. This function f shows under what condition
the test tj may be invalid. Given a set of units CO C C, VALID(Co)

will represent the set of tests which remain valid even when all the
units of CO are faulty. Because each of thefj 's is positive, whenever
SI C S2 C C, VALID(S2) C VALID(S1). Given some S, C C, a
test ti will be called invalidated by S1, if t4 0 VALID(S1). Given
S1,52 C C, we will represent (SI - S2) U (S2 - SI) by SI S S2.
Consider a fault situation where S C C represents the set of
faulty units of the system and IS I< t. We will refer to this fault
situation by fault pattern S. Let S, and S2 be two fault patterns and
there be at least one syndrome which is S, consistent as well as S2
consistent. Then, SI, S2 will be referred as an indistinguishable pair;
otherwise it will be referred to as a distinguishable pair.
With the above definitions and notations, we now proceed to
derive the necessary and sufficient conditions for t/x diagnosability
and t[x] diagnosability of a system. Throughout our discussion, we
will use the unit (test) nodes of the digraph and the units (tests) of
the system interchangeably.
III. TESTING FOR t/x DIAGNOSABILITY

From the above discussions, it is obvious that if no more than
x test outcomes are missing, then a system is t/x diagnosable if
and only if for every SI, S2 C C, such that jSl|,KS21 C t, SI, S2 are
distinguishable.
Lemma 1: If no more than x outcomes are missing then two sets
of units SI, S2 are distinguishable if and only if

NOQEDGES(VALID(SI) n VALID(S2), SI D S2) > x.
Proof: Necessity: Let us assume that the condition is not satisfied. Then, NOQEDGES(VALID(S1) n VALID(S2), S1 D S2) =
m < x. Consider a syndrome as follows.
1) An edge (ti, cj) has a label 2 if ti E VALID(S1) n VALID(S2)
and cj E 51 (D S2
2) An edge (ti, cj) has a label 1 if ti E VALID(S1) n VALID(S2)

and cj E

S, n S2.

3) An edge (ti, cj) has a label 1[0] if ti E VALID(Sk) [VALID
(Ss-k)] and cj E 5k excepting the edges considered in a above,
k E {1,2}.
4) An edge (ti, cj) has a label 0 if ti E VALID(S1) n VALID(S2)
and cj 4 S, U S2.
5) The remaining tests to unit edges have any arbitrary labeling.
The above syndrome has no more than x outcomes missing and
may be observed to be S, consistent as well as S2 consistent. Thus,
S, and S2 is an indistinguishable pair.
Sufficiency: If the condition is satisfied, then there exists at least
one unit ci and one test tj such that tj is not invalidated by S, or S2
and ci E S, - S2 or ci E S2 - S, and tj tests ci. If ci E S, -2,
then the labeling of (tj, ci) is 0[1] in every S2[S1] consistent syndrome. If ci E S2 - SI, then the labeling of (tj, ci) is 0[1] in every
S1[S2] consistent syndrome. Thus no syndrome is S, consistent as
well as S2 consistent. Hence, S, and S2 is a distinguishable pair.
Using Lemma 1, we can specify the necessary and sufficient
condition for the t/x diagnosability of a system. Since none of the
fault patterns can have more than t faulty units, we have the following theorem.
Theorem 1: A system is t/x diagnosable if and only if, for every
S1, S2 C C, such that |SI I, IS21 S t,

IEEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 2, FEBRUARY 1986

172

NO&EDGES(VALID(S1)n VALID(S2),SI($S2) > .X
However, given any arbitrary system, in order to test for its t/x
diagnosability, we need not consider every pairSI, S2 C C, such
' t. We will show that the consideration of every pair
that lSlI, $S21
SI,S2 C C, such that 1S11 = 1S21 = t is sufficient. Before we prove
this result, we will present the following lemmas, which we will
need in our proof. Lemma 2 below gives a necessary condition for
a t/x diagnosable system.
Lemma 2: If a system is t/x diagnosable then the condition A,
given below, holds.
A) For every Si C C, such that IS, = t, each unit of SI must be
tested by more than x tests of VALID(S1).
Proof: Assume the contrary. Suppose there exists an SI C C,
such that IS,f = t and some unit ci E Si is tested by no more than x
tests of VALID(S1). Form a setS2 = SI -{ci}. Then SI $52 ={ci}
and sinceS2 C Sl, VALID(S1) C VALID(S2). Now, 1S11, 1S21 t
and VALID(S1)n VALID(S2) = VALID(S1). Since ci is tested by
no more than x tests of VALID(S1), the system is not t/x diagnosable by Theorem 1. This is a contradiction.
Lemma 3: If a system satisfies the necessary condition A of
Lemma 2, then for every SI,S2 C C, such that SI CS2 and
t, Si, S2 is a distinguishable pair.
|S21 Proof:
Let Sa C C - S2 be such that IS2 U Sal = t. Let S2a =
S2 U Sa and Sia = SI U Sa. Now, VALID(S2a) C VALID(S2) C
VALID(S1). Consider any unit c, E S2, - S,a. This ci is also an
elementofS2 - S1. Since the condition A of Lemma 2 is satisfied,
ci is tested by more than x tests of VALID(S2a) and obviously by
more than x tests of VALID(S2). But VALID(S2) = VALID(S1) n
VALID(S2). Thus, there is one unit ci E S2 - SI, such that ci is
tested by more than x tests of VALID(S1) n VALID(S2). HenceS5
and S2 is distinguishable by Lemma 1.
Lemma 4: If a system satisfies the condition A of Lemma 2 then,
for every Sx, S, C C such that, ISA, IS, < t, (Sx,S5) is a distinguish= t, (SI, S2) is a
able pair if for allSl, S2 C C such that|S11 =
distinguishable pair.
Proof: The result is already proved in Lemma 3 for Sx C S,.
We will consider the case when Sx ¢ S,. Let s JSI < t or
|S,Q < ISl ' t. Suppose (Sx, S,) is an indistinguishable pair. Then
by Lemma 1,
NO-EDGES(VALID(Sx) n VALID(Sy), S, ®D Sy) x.
-

is,

|S21

lSxl

-

Let Sa C Sy-Sx be such thatjSyI = Sx U SaI. Form Sa= Sx U
-SI
C
be such that S U Sb = [SY U SbI = t.
Sa. Let S CC
Form Sx1 = S5D: U Si and Sy1 = Sy U S5. It is quite obvious that Sa
and Sb always exist. Then, because each fj is a positive function,

VALID(SX1) n VALID(Syl) C VALID(Sx) n VALID(Sy)
and also we observe that Sx n sy c sx, n Sy,. Thus, if

NO-EDGES(VALID(SX) n

VALID(Sy), S, e Sy) ' x

holds good then,

NOAEDGES(VALID(S.1) n VALID(Syi),

Sx.

+

Sy,) < x

will hold too. But this implies that (Sx,, Sy,) is an indistinguishable pair with ISxll = |Syll = t, which is a contradiction and hence
the lemma.
Taking into account of the results of Lemma 1 and Lemma 4 and
realizing that a system will be t/x diagnosable if and only if every
SI, S2 is a distinguishable pair whenever Sl, S2 C C and ISl,
S,l it, the criteria for t/x diagnosability of a system can be specified as follows.
Theorem 2: A system is t/x diagnosable iff it satisfies the condition A of Lemma 2 and for every pair (S1, S2), such that S1, S2 C C,
and |S11 = S21 = t, the following condition holds:

NOAEDGES(VALID(S1) n VALID(S2), S1 & S2) > x .
Given a system (G, B, F), finding whether the system is t/x diag-

nosable for some given t and x, needs the checking of the validity

of the condition of Theorem 2, for all possible pairs of subsets of
C, such that each subset contains t units. Given any S, C C,
VALID(S1) can be readily computed using the functionf's.

IV. TESTING FORt [x] DIAGNOSABILITY
If some of the units may fail intermittently, the diagnosis of the
fault units becomes more complicated because an intermittently
faulty unit might pass a valid test. The diagnosability of a system in
presence of intermittent failures was studied in [12]-[13] assuming
an upper bound on the number of faulty units and the syndromes to
be "permanent fault compatible" (i.e., the same syndrome can be
produced by some permanent fault situation). However, all these
studies assume the system to be modeled as in [4]. The problem of
testing the diagnosability of a system when no more than t units are
faulty and no more than x test outcomes are incorrect pass will be
addressed in this section. According to our previous discussions, a
system is
diagnosable if and only if there exists no indistinguishable pair (Si,S2) with Si, S2 C C and IS1 S21 St. The following lemma gives the necessary and sufficient conditions for the
distinguishability of a pair of subsets of C.
Lemma 5: For some SI,S2 C C, (SI, S2) is a distinguishable pair
if and only if either of the following conditions are satisfied.

t[x]

i)
ii)

|,

NoQEDGES(VALID(SI) n
NOQEDGES(VALID(S1) n

VALID(S2),S5 - S2) >x.
VALID(S2), S2 - SI) > X-

Proof:

Necessity: Suppose neither the conditions is satisfied.
Let Sa be the set of all the units of SI - S2 tested by VALID(SI) n
VALID(S2) and let be the set of all the unitsof S2 - S1 tested by
Consider a syndrome as follows.
An
has
a
label
O' ifti E VALID(S1) n VALID(S2)
1) edge (ti, cj)
andcj E Sa.
(ti, cj) has a label 1 if ti E VALID(S1) n VALID(S2)
2) AnSedge
n s2.
and cj ES
3) An edge Cj) has a label 1 if ti E8 VALID(S1) and cj E S1
excepting the edges considered in a above.
4) An edge (ti,Cj) has a label 0 if ti E VALID(S1) n VALID(S2)
and cj 0 S1 U S2.
5) The remaining tests to unit edges have any arbitrary labeling.
Consider another syndrome constructed exactly in a similar manner as in steps 1-4 above, using Sb in place ofS5, interchanging S5
and S2 and having the arbitrary labeling given by 5 to be identical.
If neither of the conditions i and ii is satisfied, both the two syndromes will have no more thanx O' outcomes. These two syndromes
are identical when each O' is replaced by a 0. These two syndromes
are, respectively, SI consistent and S2 consistent. Thus, (S1,S2) is an
indistinguishable pair.
Sufficiency: If condition i is satisfied, then there exists at least
one unit ci E8 S - S2, such that the outcome of the test of ci by some
test of VALID(S1) n VALID(S2) is not 0'. This outcome must be 1
in all consistent syndromes while it must be 0 in all S2 consistent
syndromes. Thus, SI, S2 is a distinguishable pair if conditionofi is
satisfied. Using a similar argument and interchanging the roles S1
and S2, it can be shown that whenever condition ii is satisfied, Sl, S2
is a distinguishable pair. This completes the proof.
Using the Lemma 5, we can formulate the necessary and sufficient conditions for the t[x] diagnosability of a system as follows.
Theorem 3: A system is
diagnosable iff for every S1, S2,
such that Sl, S2 C C and KS21 < t, either of the following conditions is satisfied.

Sb
VALID(S1) n VALID(S2).
(ti,

S1

t[x]
|SI|,
i) NOTEDGES(VALID(S1) n VALID(S2), S- S2) > x.
ii) NO-EDGES(VALID(SI) n VALID(S2), S2- S) > x.

Corollary: If a system is t[x] diagnosable, it is also t/x
diagnosable.
However, given any system, in order to check for its t x ] diagnosof theorem 3 need not be
ability, the validity of the conditions
checked for all possible S1, S2 such that |S,|, IS21 ' t. As in the case
of t/x diagnosability, we will show that the testing of the validity of

173

IEEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 2, FEBRUARY 1986

the conditions of Theorem 3 for all possible Si, S2, such that iS, =
S21 = t, is sufficient.
Lemma 6: The condition A of Lemma 2 is a necessary condition
for a t[x] diagnosable system.
Proof: The proof is very similar to that of Lemma 2. Referring
to the proof of Lemma 2, it may be observed that for SI and S2 as
defined there, SI - S2 = {ciI and S2 - SI = 0. Arguing exactly as
in Lemma 2, Lemma 6 follows.
Lemma 7: If a system satisfies the necessary condition referred
in lemma 6, then for every SI, S2 C C, such that SI C S2 and $S,
S21 c t, Si, S2 is a distinguishable pair.
Proof: The proof is very similar to that of Lemma 3. Using
the same argument as in Lemma 3, any unit ci E S2 - SI is tested
by more than x tests of VALID(S1) n VALID(S2) whenever the
condition referred in Lemma 6 is satisfied. This implies that the
condition ii) of Lemma 5 is satisfied for Si and S2. Hence, they are
distinguishable.
Lemma 8: If a system satisfies the condition referred in Lemma 6
then, for every S, S, C C such that, Sj, S) . t, (S,, S,) is a distinguishable pair if for all 51,52 C C such that ISI| = S2 =
t, (SI, S2) is a distinguishable pair.
Proof: The result is partially proved in Lemma 7 for the case
S, C S,. We will now prove the result for the case when S, ¢ S,. Let
JSj ISyl < t or ISAl < ISyl t. Suppose (S, Sy) is an indistinguishable pair. Then by Lemma 5,

NO-EDGES(VALID(sX) n VALID(Sy), S, - Sy) x and
NOQEDGES(VALID(SX) n VALID(Sy), Sy - S) .x .
If S& and Sy, are constructed exactly the same way as was referred

in the proof of Lemma 4, then it might be observed that

VALID(sX1) n VALID(Sy,) q VALID(S,) n VALID(Sy) and
C Sy-Sx.
Sx1-Sy, = Sx-Sy and SyI-S1q
Then obviously, we will have

NO_EDGES(VALID(SX1) n VALID(SY1), SIC
No_EDGES(VALID(S.1) n VALID(Sy1), SyI

-

SYO)
S.)

Sx

and

xX.

This implies, by Lemma 5, S,,, Sy, is an indistinguishable pair. But
= ISyll = t and this is a contradiction. Hence the lemma.
IS,11Taking
into account of the results of Lemma 5 and Lemma 8 and
realizing that a system will be t[x] diagnosable if and only if every
Si, S2 is a distinguishable pair whenever SI, S2 C C and IlS,IlsI|
t, we can specify the criteria for t/x diagnosability of a system
as follows.
Theorem 4: A system is t/x diagnosable iff it satisfies the condition in Lemma 6 and for every pair (SI, S2), such that Si, S2 C C,
and IS,| = 1S21 = t, at least one of the following conditions hold:

a) NO-EDGES(VALID(S) n VALID(S2), SI - S2) > X.

b) NOJEDGES(VALID(S) n VALID(S2), S2 - SI) > x.
IV. CONCLUSION

A new representation of general diagnosable system has been
considered in this correspondence and a general solution of diagnosability problem without repair has been discussed in presence of
three-valued test outcomes. Two possible cases of three-valued test
outcomes have been considered: a) outcomes representing pass,
incorrect pass, and fail, and b) pass, missing, and fail. Earlier work
in connection with diagnosability without repair either refer to a less
general model as in [4] or consider binary test outcomes [5] with a
similar but less general model. In this correspondence, a characterization of the system is shown where all the faulty units can be
uniquely identified provided some upper bound on the number of
faulty units as well as that on the number of incorrect pass and
missing outcomes respectively for the cases a and b are known. In

a recent paper [ 15], systems with four-valued test outcomes were
considered assuming that missing and incorrect pass outcomes
might be simultaneously present. However, the model considered
there was the model in [4]. It is worth investigating the characterization of the diagnosability of the system with four-valued test
outcomes as considered in [15] when it is modeled in a more general
way as has been considered in this correspondence.

REFERENCES
[1] J. T. Butler, "Properties of three-valued system diagnosis," in Proc. 11th
Int. Symp. Multiple-Valued Logic, May 1981, pp. 85-89.
[2]
, "Relations among system diagnosis models with three-valued test
outcomes," in Proc. 13th Int. Symp. Multiple-Valued Logic, May 1983,
pp. 350-355.
[3] A. Sengupta and A. Sen, "On system diagnosis with multivalued test
outcomes," in Proc. 13th Int. Symp. Multiple-Valued Logic, May 1983,
pp. 356-360.
[4] F. P. Preparata, G. Metze, and R. T. Chien, "On the connection assignment problem of diagnosable systems," IEEE Trans. Comput.,
vol. C-16, pp. 448-454, Dec. 1967.
[5] J. D. Russell and C. R. Kime, "System fault diagnosis: Masking, exposure and diagnosability without repair," IEEE Trans. Comput.,
vol. C-24, pp. 1156-1161, Dec. 1975.
[6] S. L. Hakimi and A. T. Amin, "Characterization of the connection assignment of diagnosable systems," IEEE Trans. Comput., vol. C-23,
pp. 86-88, Jan. 1974.
[7] S. N. Maheshwari and S. L. Hakirmi, "On models of diagnosable systems
and probabilistic fault diagnosis," IEEE Trans. Comput., vol. C-25,
pp. 228-236, Mar. 1976.
[8] F. J. Allan, T. Kameda, and S. Toida, "An approach to the diagnosability
analysis of a system," IEEE Trans. Comput., vol. C-24, pp. 1040-1042,
Oct. 1975.
[9] F. Barsi, E Grandoni, and P. Maestrini, "A theory of diagnosabilityof digital systems," IEEE Trans. Comput., vol. C-25, pp. 585-593,
June 1976.
[101 S. Karunanithi and A. D. Friedman, "Analysis of digital systems using a
new measure of system diagnosis," IEEE Trans. Comput., vol. C-28,
pp. 121-133, Feb. 1979.
[11] C. S. Holt and J. E. Smith, "Diagnosis of systems with asymmetric invalidation," IEEE Trans. Comput., vol. C-30, pp. 679-690, Sept. 1981.
[12] A. Sengupta, A. Sen, and S. Bandyopadhyay, "On system diagnosability
in presence of hybrid faults," IEEE Trans. Comput., vol. C-35,
pp. 90-93, Jan. 1986.
[13] S. Mallela and G. M. Masson, "Diagnosis without repair for hybrid fault
situations," IEEE Trans. Comput., vol. C-29, pp. 461-470, June 1980.
[14] A. Sengupta and A. Sen, "On the diagnosability problem for a general
model of diagnosable systems," Commun. Informat. Sci.
[15] R. A. Leonetti and J. T. Butler, "A characterization of diagnosability in
systems with four-valued test results," in Proc. 15th Int. Symp. MultipleValued Logic, May 1985.

lteration Properties of Multivalued Switching Functions
CORINA REISCHER AND DAN A. SIMOVICI
Abstract -The purpose of this correspondence is to survey the literature concerning the iterative properties of multivalued switching functions. These properties are important for the synthesis of switching circuits
Manuscript received June 20, 1985; revised September 12, 1985. This work
was supported by the Canada Natural Science and Engineering Research

Council under Grant NSERC A4063.
C. Reischer is with the Department of Mathematics and Computer Science,
University of Massachusetts, Boston, MA 02125, on leave from the
Departement de Mathematiques et d'Informatique, Universit6 du Quebec,
Trois-Riviers, P.Q., G9A 5H7, Canada.
D. A. Simovici is with the Department of Mathematics and Computer Science, University of Massachusetts, Boston, MA 02125.
IEEE Log Number 8406763.

0018-9340/86/0200-0173$01.00 C) 1986 IEEE

The Optimal Cost Chromatic Partition
Problem for Trees and Interval Graphs

Leo G. K r o o n
Erasm~as University R o t t e r d a m
P.O. Box 1738, NL-3000 DR Rotterdam
The Netherlands
A r u n a b h a Sen
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287
USA
Haiyong Deng
Computer Aided Design Group
Intel Corporation
Santa Clara, CA 95052
USA
Asim Roy
Department of Decision and Information Sciences
Arizona State University
Tempe, AZ 85287
USA

A b s t r a c t . In this paper we study the Optimal Cost Chromatic Partition (OCCP) problem for trees and interval graphs. The OCCP problem
is the problem of coloring the nodes of a graph in such a way that adjacent nodes obtain different colors and that the total coloring costs are
minimum.
In this paper we first give a linear time algorithm for the OCCP problem
for trees. The OCCP problem for interval graphs is equivalent to the
Fixed Interval Scheduling Problem with machine-dependent processing
costs. We show that the OCCP problem for interval graphs can be solved
in polynomial time if there are only two different values for the coloring
costs. However, if there are at least four different values for the coloring costs, then the OCCP problem for interval graphs is shown to be
NP-hard. We also give a formulation of the latter problem as an integer
linear program, and prove that the corresponding coefficient matrix is
perfect if and only if the associated intersection graph does not contain
an odd hole of size 7 or more as a node-induced subgraph. Thereby we
prove that the Strong Perfect Graph Conjecture holds for graphs of the
form K x G, where K is a clique and G is an interval graph.

280
1

Introduction

In this paper we study the Optimal Cost Chromatic Partition (OCCP) problem
for trees and interval graphs. The general OCCP problem can be described as
follows: given a graph G = (V, E) with n nodes and a sequence of coloring
costs ( k l , . . . , k n ) , find a proper coloring C(v) E { 1 , . . . , n } of each node v C V
such that the total coloring costs ~ = 1 kc(v) are minimum. Here a coloring is
proper if adjacent nodes have different colors. Thus an alternative formulation
of the OCCP problem is the following: Given a graph G = (V, E) with n nodes
and a sequence of coloring costs ( k l , . . . , k~), partition the vertex set V into
independent sets V1,..., V~ such that ~ = 1 k~lVcl is minimum. Here all nodes in
the independent set Vc are colored by the same color c. Without loss of generality
we assume kc < kc, whenever c < c/ throughout this paper.
The OCCP problem was introduced by Supowit [22] in the VLSI context. He
referred to the problem as the Weighted Coloring Problem of a graph. However,
the term Weighted Coloring Problem is used by Gr5tschel et al. [12] to describe a
different problem. In order to avoid further confusion, we refer to the problem as
the Optimal Cost Chromatic Partition problem. This problem is a generalization
of the Chromatic Sum problem introduced by Kubicka & Schenk [17].
It is not difficult to show that the OCCP problem is NP-hard for arbitrary
graphs. Supowit [22] deals with the OCCP problem for circle graphs. It follows
from the results of Garey et al. [10] that this problem is NP-hard. Furthermore,
Sen et al. [21] consider the OCCP problem for permutation graphs.
In this paper we study the OCCP problem for trees and interval graphs. The
OCCP problem for interval graphs is equivalent to the Fixed Interval Scheduling Problem (FISP) with machine-dependent processing costs. In this scheduling
problem each job j to be carried out requires processing during a fixed time interval (sj, fj). A sufficient number of machines is available, and all jobs may be carried out by all machines. However, the processing costs are machine-dependent.
That is, if job j is carried out by machine m, then the associated processing
costs are kin. The objective is to find a feasible non-preemptive schedule for all
jobs against minimum total processing costs.
The outline of this paper is as follows: In Section 2 we describe an integer
linear program that can be used to solve the general OCCP problem. Next, in
Section 3 we show that the OCCP problem for trees can be solved in linear
time. In Section 4 we study the OCCP problem for interval graphs. We show
that this problem can be solved in polynomial time if there are only two different
values for the coloring costs. However, if there are at least four different values
for the coloring costs, then the problem is shown to be NP-hard. We improve the
given integer linear program by describing all clique inequalities, and we prove
that the corresponding coefficient matrix is perfect if and only if the associated
intersection graph does not contain an odd hole of size 7 or more as a nodeinduced subgraph. Thereby we show that the Strong Perfect Graph Conjecture
holds for graphs of the form K x G, where K is a clique and G is an interval
graph.

281

2

Model formulation

In this section we give a formulation of the OCCP problem as an integer linear
program. To that end, suppose we have an instance 1 of the OCCP problem
containing a graph G = (1/",E) with n nodes and a sequence of coloring costs
(kl, . . . , kn). Then the integer program to solve this instance of the OCCP problem uses the binary decision variables x~,~ indicating whether or not node v is
colored by color c (v, c = 1 , . . . , n ) . The objective and the constraints of the
model can be described as follows:
/~

min E

r/,

kcxv,~

E

(0)

v ~ l c=l

subject to
n

1

v=t,.

(1)

xv,~ + x~,,c _< 1

(v,v') E E; c = 1 , . . . , n

(2)

xv,c e { 0 , 1 }

v =

(3)

czl

c =

The objective function (0) specifies that we are interested in minimizing the total coloring costs. Constraints (1) require each node to be colored exactly once.
Constraints (2) guarantee that two nodes v and v' that are connected by an edge
(v, v') E E are colored by different colors. Note that these constraints could be
tightened by replacing them by the corresponding clique constraints. Unfortunately, for arbitrary graphs the number of cliques and clique constraints may
be extremely large, and it may require quite some time to find them. However,
for trees and interval graphs the number of cliques is linear in the number of
nodes. In fact, the only cliques of a tree are the edges. Furthermore, in Section
4.2 we will show how for an interval graph the constraints (2) can be replaced
by the corresponding clique constraints. Finally, the constraints (3) declare the
variables x~,c as binary variables.
The coefficient matrix corresponding to the restrictions (1) to (3) associated
with an instance I of the OCCP problem is called M(I). Note that M(I) is a
zero/one matrix. A zero/one matrix M is said to be perfect if the polyhedron
P ( M ) = { X I M . X < 1 and x > 0}

has only integral extremal points. Here the vector 11 is a vector containing all l's.
It follows that the problem (0) to (3) can be solved by applying a linear programming algorithm if the matrix M(I) is perfect. Padberg [18] gives a complete
characterisation of perfect matrices in terms of forbidden submatrices.

282
3

The

OCCP

problem

for Trees

Sen et al. [21] show that the coefficient matrix M(I) associated with the integer
linear program (1) to (3) is perfect if the graph G is a tree. Thus the OCCP
problem for trees can be solved in polynomial time by any polynomial linear
programming algorithm (GrStsehel et al. [12]).
However, in this section we describe a linear time algorithm for solving the
OCCP problem for trees. To that end, let T be a tree with n nodes, and let
( k l , . . . , k~) be the corresponding sequence of coloring costs. In this section we
denote the degree of node v, which is the number of different neighbours of node
v, by Dr. The maximum node degree over all nodes is denoted by D.
Next, we will show that for any graph G the number of colors used by a
proper coloring of minimum costs may be bounded by D + 1. This upper bound
is sharp, as is demonstrated by the complete graphs (cliques).
L e m m a 1. For a graph G there exists a proper coloring C of minimum costs

such that C(v) < Dv + 1 for all nodes v.
P r o o f . Let C be a proper coloring of minimum costs, and suppose there is a
node v with C(v) = e > Dv + 1. Then there exists a color d with d < Dv + 1 such
that e t r C(v r) for all neighbour v' of v. Now we define the following alternative
coloring C r of the graph G.
C' (v) = ~"c'

[

if v = n,

C(v) if v r n.

Clearly, C I is a proper coloring of G. Furthermore, kc, _< he, since c' < c. Thus
C ~ is also a coloring of minimum costs. By repeating this argument as often as
necessary, we obtain a proper coloring of the graph of minimum costs satisfying
the condition of the lemma.
[]
The algorithm for solving the OCCP problem for trees uses the result of Lemma
1 Recall that the chromatic number of a tree is 2 (or 1). On the other hand, it is
not difficult to create an instance of the OCCP problem for trees for which the
number of colors used in an optimal solution is arbitrarily large.
The algorithm for solving the OCCP problem for trees is based on the idea
of dynamic programming. To that end, we first add a direction to the edges of T
by choosing an arbitrary node as the root r, and by directing an edge (v, v t) E E
from v to v ~ if the unique path in T from root r to node v does not visit node
v ~. In the obtained directed tree each node v is the root of the subtree rooted at
node v.
The algorithm to solve the OCCP problem for trees keeps track of the following information for each node v:

(i) Kl(v), representing the minimum costs of coloring the subtree rooted at
node v (this is called the primary coloring of the subtree rooted at node v),

283

(ii) C(v), representing the color used for node v in the primary coloring of the
subtree rooted at node v,

(iii) K2(v), representing the minimum costs of coloring the subtree rooted at
node v with node v colored differently from C(v) (this is called the secondary
coloring of the subtree rooted at node v).
Obviously, if v is a leaf node, then Kl(v) = cl, C(v) = 1, and K2(v) = c2.
If v is a non-leaf node with d children, vl,..., Vd, then first the values K1 (vi),
C(vi), and K2(vi) are determined recursively for i = 1 , . . . , d. Next, the coloring
costs K(v,c) are determined for c = 1 , . . . , d + 1 by equation (4) below. The
coloring costs K(v, c) denote the costs of coloring the subtree rooted at node v,
when node v is colored with color c and all subtrees rooted at the children of
node v are colored as cheap as possible. Thus,

K(v,c):=kc+

E
Kl(vi)+ E
K2(vi) f o r c = l , . . . , d + l
i:C(vi)#c
i:C(v~)=c

(4)

Note that (4) can be calculated in an amount of time that is linear in the number
of subtrees d by the following steps:
d

g := Z Kl(v )
i=1

Forc:=ltod+ldoA(c):=K
F o r i := 1 t o d d o
If

C(vi) < d + 1 t h e n A(C(vi)) := A(C(vi)) + K:(v~) - Kl(vi)

F o r c := 1 t o d + 1 d o

K(v,c) := kc + A(c)

Given the coloring costs K(v, c) for c = 1 , . . . , d + 1, the values
K2 (v) are computed using the following formulas.
Kl(v) :=min{K(v,c)

C(v)

[c=l,...,d+l},

:=argmin{K(v,c)

[c=l,...,d+l

K2(v) : = m i n { K ( v , c ) ] c = l , . . . , d + l ;

K1 (v), C(v), and
(5)

},

e # C ( v ) }.

(6)
(7)

In (5) the minimum costs of coloring the subtree rooted at node v are determined,
and in (6) the corresponding color is determined. Next, in (7) the costs of a
secondary coloring of the subtree rooted at node v are determined.
Obviously, the complexity of the above algorithm for coloring node v is O(d),
where d is the number of children of node v. Therefore the overall complexity
of the algorithm for computing the minimum costs of coloring the entire tree
is proportional to the number of edges, which, in a tree, gives rise to an (9(n)
algorithm.

284
4

The

OCCP

problem

for interval

graphs

An interval graph G = (If, E) is a graph where each node corresponds to a time
interval (sj, fj), and where two nodes are connected by an edge if and only if the
corresponding intervals are overlapping. Thus the OCCP problem for an interval
graph G can be considered as the problem of coloring the corresponding intervals
(s j, fj) in such a way that overlapping intervals obtain different colors, and such
that the total coloring costs are minimum. The costs of coloring an interval with
a certain color only depend on the color to be used.
As was mentioned already in the introduction of this paper, the OCCP problem for interval graphs is equivalent to the Fixed Interval Scheduling Problem
(FISP) with machine-dependent processing costs. In this scheduling problem
each job j to be carried out requires processing during a fixed time interval
(sj, fj). A sufficient number of machines is available, and all jobs may be carried out by all machines. However, the processing costs are machine-dependent.
That is, if job j is carried out by machine m, then the associated processing
costs are kin. The objective is to find a feasible non-preemptive schedule for al!
jobs against minimum total processing costs. It follows that the intervals and the
colors of the OCCP problem for interval graphs correspond with the jobs and
the machines of FISP with machine-dependent processing costs, respectively.
Several other variants of FISP have been considered in the literature (of.
Arkin & Silverberg [1], Dondeti & Emmons [4, 5], Fischetti, Martello & Toth
[6, 7, 8], and Kolen & Kroon [13, 14, 15, 16]). The computational complexity of
these variants of FISP has been studied extensively.
4.1

Computational complexity

In this section we prove that the OCCP problem for interval graphs is NP-hard
if there are at least four different values for the coloring costs. This result is
somewhat surprising, because almost every other combinatorial problem related
to interval graphs, such as the computation of the chromatic number, the maximum independent set, the maximum clique, and the dominating set can be
accomplished in polynomial time.
On the other hand, if the first s coloring costs are equal and the last n - s
coloring costs are equal as well, i.e., kl = k2 . . . . .
k8 = A1, and ks+l =
ks+2 . . . . .
kn = A2 (where it may be assumed, without loss of generality,
that AI < A2), then the optimal solution is obtained if the largest s-colorable
subgraph is colored with the colors 1 , . . . , s, and the other nodes are colored with
the remaining colors. For interval graphs, the problem of finding the largest scolorable subgraph can be solved in polynomial time by the greedy algorithm
of Yanakakis & Gavril [25]. Note that, if there are only two different values for
the coloring costs, then the OCCP problem for any graph is equivalent to the
problem of finding the largest s-colorable subgraph of the graph.
Next we show that the OCCP problem for interval graphs is NP-hard if
there are at least four different values for the coloring costs. The proof uses a

285
reduction from the problem Numerical Three Dimensional Matching (N3DM),
which is defined in following way:
of N3DM:
- A positive integer t and 3t rational numbers ai, bi and ei satisfying
~ti= l(a~ + bi + c~) = t and 0 < a~,b~,ci < 1 for i = 1 , . . . , t .

Instance

Question:

- Do there exist permutations p and a of { 1 , . . . , t} such t h a t ai + bp(i) §
c~(i) = 1 for i = 1 , . . . , t ?
It is well-known that N3DM is NP-eomplete in the strong sense (Garey and
Johnson [9]). Therefore any problem in NP that is more general than N3DM
is NP-complete as well. The proof of Theorem 2 is illustrated with the following instance of N3DM with t = 3: ( a l , a s , a 3 ) = (1/8, 1/4,3/8), (bl, bs, ha) =
(1/8, 1/4, 1/2) and @1, cs, c3) = (1/4, 3/8, 3/4). This instance of N3DM is a yesinstance, since al + bl + c3 = as + b3 q- Cl = a3 -b bs + es = 1.
T h e o r e m 2. The OCCP problem for interval graphs is NP-hard if there are at

least four different values for the coloring costs.
The idea behind the proof is that as m a n y as possible intervals should be colored by the cheaper colors and as few as possible should be colored by the more
expensive ones.
P r o o f . The theorem is proved by a reduction from N3DM. Hence let /i be an
instance of N3DM containing the integer t and the rational numbers ai, bi and
ci for i = 1 , . . . , t . Next, for i , j = 1 , . . . , t the rational numbers Ai, Bj and
Xi,j are chosen in such a way that all these numbers are different and that
4<Ai<5<Bj
<6and7<Xi,j
<9fori,j=l,...,t.
Based on these data, an instance/2 of the O C C P problem for interval graphs
is constructed. The intervals that have to be colored in I2 are the following.
t times (0, 1),
t times (1, 2),
t times (13, 14),
t 2 - t times (0, 3),
t 2 - t times (12, 14),
(O, Bj)
forj = 1,...,t,
(2, Ai)
for i = 1 , . . . , t ,

(11 - c k , 13)
t - 1 times (0, Ai)
t - 1 times (3, Bj)

(Ai, Xi,j)

x ,j)
(Xi,j, 14)
(Xi,j, 10 + ai + by)

for k = 1 , . . . , t ,
for/=l,...,t,
for j = 1 , . . . , t ,
for i , j = 1 , . . . , t ,
for i , j = 1 , . . . , t ,
for i , j = 1 , . . . , t ,
for i , j = 1, . . . ,t.

Furthermore, for coloring these intervals there are t different colors with costs
0, there are t 2 - t different colors with costs 1, there are t 2 different colors with
costs 2, and all other colors have costs 12t 2. Note t h a t the construction o f / 2
can be carried out in a polynomial amount of time. The instance/2 constructed
from an i n s t a n c e / 1 of N3DM defined above is shown in Figure 1.
Now we will prove the following statement: /1 is a yes-instance of N3DM if
and only if the minimum total coloring costs for Is don't exceed 11t ~ - 5t.

9

h~D

i..a

0

c~

i.,,t

oq

with

costs

1

with

costs

2

iiii[i[i~iii]i]]]i]i]i]iiiiii]iil]~i[iii]i]iiiiiiiiiiiiiii[]~!!iil]t

I

1

I

0

2

I

3

I

4

I

5

I

6

I

7

I

8

I

9

I

10

I

11

I

i!!]~]]]]::]]]][]]][i] ]][[[]ii]i~:::~i]i:~]]][i]]]]][[]i]]]]]:]~] :]::::]~]::~]::~]]iiiii]]~i]]i]i]]i]]~]]]i][!:.i~]]~]!i~]]i]i]][]]]~::i[]]]]]~]Ii]~]]]]ii]i::iii]iii]]]] ii][i~ii]]]i]~]]i~]]]]~]~ii]]]]]]]]]~]::!~]~]]]]]~]]][[[i][[]~]]i]i~i]i]i[iii[[ [i ~[]i [[[ ]]: i

12

I

I

14

::i:,iii~!{i|

13
time

I

]]ii~][i ]]]i][ i]]]iiii]iiii

l::i::i]i[~]i::i::ii[i-::i]i
ii-]i::i~i[iii]i]i
]i]iZ~i]iii]i::i::i::i::i::i::i::i::i]i]!::i~!::i
:/:~i::i]i]i~i]i::i::i::i::i::i[i:~i]i~]ii::~]i~!i!::!]i]~::i::~::i]i]i::i::i::~]i::
i~i[i::i]{::i]i::i
]i~i::i::i::i::i::ii
~]~]~i]]ii!]i::i]~i]~]~i::i]~]i::i::i::Z]~::~]~]i~i!::i]i]i::i::i~i~{::~::ii!::ii~[i[i::~::i]i]i[i::i::i::~]i::i::i::i]i]i]i]i::i~
i]i::i::i]i]iil
~iii::i::i::i::
|]i::~i::i:.!::ii:::::i::i::i::i]i::i]i::i[i]i~i]i]i]~{!i!!ii::i]i::i]i::i::i::ii]i::i::i
~]i~ii~]iiiiiii::i
i iiiiiii[ii~::ii::i::~;
i]iiiiiil illiiiiii::iiiiiiii~;iiiiii~iiiiii]i!]iiiii ii~!:.i:-i]i:.i::i]i]i::i[ili
i{ !!~!ii|
l i::i::i::i~i::i::i::i[i]i]i]i::i]i]i[i
]i]i]i::i::i]iiii]!il]iili::i::i::i~::i::i::Z:i]i::i::i]i~i]i
~i::]i:::::i]~ii]i::i::i]i::i]i::i:i]iii::i::!]ii~i::i::i~i::i::i::i~i::i::i::i::i]i~i::i::i::i]i]i]i
]i]iiii]i]ili::
~i~i]i::i|]i]i~~i~i::i]iii::i[i[i]i~!i]i]i~ii::i::~]i::i::i[Z:~]i~i::iiii~ii[~i::i]i::i.
~]i~iii::i::i!i~ii::i!i::i]i[i[i::i::i::i::i]i::
~i::iii::i::i::i::i::i]i::
[:i::i~]i:i::::~i::i[i::i::i::i::i~i~i]!iiiii!iiiiil]i::i::i::i]i~i]i::
i~-i]]]i::i::~ii
[i]i::i::i::ii[i::i::i]i]i~i::i~]i!i::i]iii::ili::i]::::iil]
~~iiii::il]i::i~i-'i]i-::i::i::i::i::i~]ii]~i]i::
!i~]i]i]::]::ii[i::i[i::i]i]
::]::::ii::::~!::iiit

~iiii~ii~.~iiiiii~iiiiiiii~ii~::i::!~!~iii~iiiiiiiiiiiiiiiiii]~ii~i~!:`i~::i:`~ii~ii::iiiiiiiiiiiiiiiiiiii~]ii~::~;:ii~::i~:~i::i~:i::i`~iiiii~]iiiii~iiiiiiiiii~iiiiiiiiiiiiiii~i~iii~!i~!~ii~::i::i~i::i::i::i:~i::i::i~::~:

L~
O0

li]i::]~]i't-~]i]];.i]i]i]i]ii]i]i]!:!~i]!]i[i[i]i]i]i]~i]ili|

i ::ii ]~:ii~i::ii ::i~i ~i ::!~:i!i;~:~ii~i:::~::ii ::i~i~~ii::~:~::~:/
iiii:/:::::::::i::ili::i::i::i::i::i::i::iiiiiii::i!iii::iiiiiiiiii
:~::~:f:::~::ii::i! ::::iii::i]i::i::i::i-::i::i::i::i::i::iii::i::iii]i::i~i:~ii!it
]i]~i::i::iiiiii::ii i ~iii:::::::::i i ::i :::-iiiiiii!iii::ii]iiiiiiiiiiiii::iiii
::::i i ::ii::iii::i::iii::{]iiiii!l':i:,ii~i
::::::::::::::::::::::::::::::::::::::::::::
ii::iiiiiiii::ii::::::iiiiiiiii:~iiii ::i::i::i::i::i::ii~:i:i~~ii!:i:ii i::i~i::i::i::::ii ~i i~:~ii i i ~i~i~i i i i::ii ::i::i::i:~i!i ~i~i::iiiiiii::i::i::i::iiiiii!
i ::ii i i ::i~i i ii~iii
::::~ii il

t 2 colors

l~i i i ~i::i::i::i::i]i::i::i~ii~i~]i~i:i~/::i~i~i i~iiiiiiiii
i iiiiiiii~iiiiiiiiiiiiiiiiiii~iiiiiiiiiiiiiiii~iiiiiiiiil~i ii:,ili~il
i::i::!::i::tiiiii!~ii-ii~i~i
::i]i
iiii!~]i~ii~i~iiiiiiiiii]ii]i~i~i::i::l::i::~il
iiiiiiiiiiiiiiiiiiii:~i~i]iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
ii~i~i[]ii[~'~i~]i] I

l iii~]il] :-::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
[i-]::::::iiiii!ilil]i~::::::::]~]]]]]~:]]:]!::i::i]i::i]i;i]i]i]i]
::::i::::::~[[]i::i::i]i]i]l::i]i::~G-'::.]i]i]
i]i]ii::i~]::]::::::i]i~i[i]i::i::i[i[i::
]::::::]]:::::::: ::!::i::i]i]i::i]i]ii!ii~!~ii~]~]::ii]i::i::i::i]i~i::i::i::i~i::i::i:::]~h:i]i]~
iI
I::i::i::i
::Oi::i::i~~:i!:.i
i~ ~i::i::i::i
::i::i~i::i::i::~::ii!i~!
::i::iii::i]iiii
:~: i::i~i::iii~
i::i::i::i]i]i~i~i::i]i~]i~
i::i::i:i::i::i::i::i~i~iliiii!~::i::i::i::i::i]i::]
:i~
~i::i~i:::~
:i::i:::.~:.::ii::i~i~ii
~i::i i~i::i~i::i~:i::i~i::::~Z:i
i:: ::i::i::i::i::i[ii::i|i]i~::i]i-~]]~]iiiii:.i]i]i]i]
i::i::i::i]i::i]i
[i]i]i::i::i::i::i::itlil
ii~i~i ]i::i::i~i]i::i!iiiiiii::i[i::i]ilil
i[ii::i]i::i::i]i]i]i]i]i
]i::i::i]i]iii!i::i
~i[i]
::::::::::::::::::::::::::::::::::::::::::::::::
::]::::~]~::~]~]~:~[[]~P:]::~]]]]::~]~::~::]]::]]::]][::::::::::~]]~::~]~::P:::]]::::P:]::::]]~
]];] ]:~::~::~::::::::]]::]~]]~::~]~]~]~]~::~]~::~::~J~]]]]]]]]~]~]]]~]]]~]~i~]~::~]~]]
]][[[[::::::::P::~::~i::i::~iiiiii]~ii~i::i::i::i::i::i::i~]::i::i::i]i::i]i]]::ii]~]iii!~i
i

t ~-t colors

t colors with costs 0
l ii::i::i~i::i::i!i~i::i~ii::i::i::iii~i::i::
t::i::i::i~i::i~i~i~i~i::i~i~i::i::i::i::i~iiii
iliiii~i::i::~i::i~i::i::i::i::ii~:i~i::i::i~!::i~i~:~::i~i~i~i::i~i::i::i::i~ii::i::i~i~i::ii~iiii~iiii::i~:i]::i::i:~N::~::i::i::i::i::i::i::i~i::i~iii:~iiiiii~i!i
~i::iii~i::i~i:=i',i]i::i~iii~i::i::iii::iii::i:,
i::i:,i~iii::i::i~i~i::i~i::
i::Z.i|::iii
~i~iN ~iii::i~i::iii~i~iii::i::i::i~i::i::i~i::i::i::i::i
!~ii~i!i::i]i::i~i::iiiii~i~i~i::ii~N::i
~ ~!i~i~|::ii::i::~i~Nil]i~ ~i~i~i::i::i::iii::i::i::iZ~i~i~i~i::i~i]i~i]i::i!i~iiiii!!i~i~ii~i~i
~::i::i::i~tii!ii!!iii::i]i~i~i~i::i::~
~i::i::i::i

287
Suppose the minimum total costs of coloring all intervals of I2 don't exceed
11t 2 - 5 t . Then only the 2t 2 cheapest colors are used, because otherwise the total
costs would be at least 12t 2. Furthermore, the overlap of the intervals during the
time interval (0,10) and during the time interval (12,14) equals 2t 2, as can be
verified easily. Therefore at least 2t 2 colors are required to color all intervals. It
can be concluded that all intervals are colored by t colors with costs 0, by t 2 - t
colors with costs 1, and by t 2 colors with costs 2.
Since all intervals (Ai,Xi,j) and (Bj,Xi,j) are overlapping, and since the
total number of these intervals (= 2t 2) equals the total number of used colors,
each color is used for coloring exactly one of these intervals. Furthermore, the
overlap of the intervals in the time interval (0,10) and in the time interval (12,14)
equals the total number of used colors. Therefore, if an interval finishes either
in the time interval (0,10) or in the interval (12,14), then the next interval that
is colored by the same color fits seamlessly to its predecessor.
Thus an interval (Ai, Xi,j) will be colored by the same color as a sequence of
intervals of the form (0, 1), (1, 2), (2, Ai), or as one of the intervals (0, Ai). An
interval (By, Xi,j) will be colored by the same color as a sequence of intervals
of the form (0,3), (3, Bj), or as an interval (0, Bj). Furthermore, an interval
(Ai, Xi,j) or (Bj, Xi,j) will be colored by the same color as a sequence of intervals
of the form (Xi,j, 10 + ai + bj), (11 - Ck, 13), (13, 14), as a sequence of intervals
of the form (Xi,j, 10 + ai + bj), (12, 14), or as one of the intervals (Xi,j, 14).
From this list of sequences, it can be concluded that each color colors at least
3 intervals and at most 7. If each color with costs 0 colors 7 intervals, each color
with costs 1 colors 5 intervals, and each color with costs 2 colors 3 intervals, then
the total costs e q u a l t x 7 x 0 + ( t
2-t) x5xl+t
2 x3x2=llt
2-5t.Also,
any other assignment of colors to sequences of intervals will lead to higher total
coloring costs. Thus if the total costs don't exceed 11t 2 - 5 t , then each color with
costs 0 colors a sequence of intervals of the form (0, 1), (1, 2), (2, Ai), (Ai, Xi,j),
(Xi,j, 10 + ai + bj), (11 - Ck, 13), (13, 14), where each i and each k occur exactly
once. Further, each color with costs 1 colors a sequence of intervals of the form
(0, 3), (3, Bj), (Bj, Xi,j), (Xi,j, 1 0 + a i + b y ) , (12, 14), where each j occurs exactly
t - 1 times. Hence among the intervals (Xi,j, 10 + ai + bj) that are colored by
the colors with costs 0 each j also occurs exactly once.
The fact y'~ti=1 (ai + bi + ci) = t implies that, if two consecutive intervals are
colored by the same color with costs 0, then they fit seamlessly after another. It
can be concluded that, if an interval (Xi,j, 10 + ai + bj) is colored by the same
color with costs 0 as an interval (11 - ck, 13), then 10 + ai + bj = 11 - ck. This
means that ai + bj + ck = 1. So if we define p(i) = j and a(i) = k whenever
interval (Xi,j, 10 + ai + by) is colored by the same color with costs 0 as interval
(11 - Ck, 13), then p and (r are the required permutations for I1. It can be
concluded t h a t / 1 is a yes-instance of N3DM.
Conversely, given a feasible solution f o r / 1 , the construction can be reversed
to find a feasible coloring for all intervals o f / 2 with total costs 11t 2 - 5t. As
N3DM is NP-complete, the OCCP problem for interval graphs is NP-hard. []

288
In the above proof it is assumed that the four different values for the coloring
costs are 0, 1, 2, or 12t 2. However, the statement "I1 is a yes-instance of N3DM
if and only if the minimum total costs of all intervals of/2 don't exceed l l t 2 - 5t"
also holds if the coloring costs are 0, 1, 2, and 3. However, in that case it takes
somewhat more effort to see that in a coloring with minimum total costs no
intervals are colored by colors with costs 3.
As was noted already, if there are only two different values for the coloring
costs, then the OCCP problem for interval graphs can be solved in polynomial
time. Hence an open question still to be answered asks for the computational
complexity of the OCCP problem for interval graphs, if there are exactly three
different values for the coloring costs. We conjecture this problem to be NP-hard
as well. This is a subject for further research.
4.2

Improved model formulation

In this section we improve the integer linear program (0) to (3) by replacing
constraints (2) by the corresponding clique constraints. To that end, let I be an
instance of the OCCP problem for interval graphs containing n intervals (s j, fj)
and a sequence of coloring costs (l~1, k2,..., kn). Furthermore, suppose the set
{t~lr = 0 , . . . , H} contains the start times of the intervals in chronological order.
That is, {t~rr = O,...,R} = { s j t j = 1 , . . . , n } and t~-i < t~ for r = 1 , . . . , R .
Then constraints (2) may be replaced by the following constraints (2~):
~{jl~_<t~<ijIXj,c <_ 1

c = 1,...,n; r = 0,...,t~.

(2')

Constraints (2 ~) state that at most one of the intervals overlapping the start time
~r of a certain interval can be colored with color e. Note that, if two intervals are
overlapping, then either the first interval overlaps the start time of the second
interval, or vice versa. Thus constraints (21) guarantee that overlapping intervals
are colored by different colors. Note that the number of constraints (2 r) does not
exceed the number of intervals n.
In the following section necessary and sufficient conditions for the coefficient
matrix M ' ( I ) of the integer program (0), (1), (2'), (3) to be perfect are expressed
in terms of the associated intersection graph G(I). The intersection graph of a
zero/one matrix M is a graph G containing exactly one node for each column of
the matrix Mo Two nodes x and y of G are connected if and only if Mr,x.M~,y = 1
for some row r of the matrix M.
Note that the nodeset of G(I) is the set {Vj,clj, c = 1 , . . . , n}. Two different
nodes Vj,c and vj,,c, are connected if j = j~ and c r d, or if c = c a and (sj, fj) ;7
(sj,, fj,) ~ ~. An edge connecting two nodes vj,c and vj,,~, with j = j ' and c ~ c /
is called a bridge edge. An edge connecting the nodes vj,c and vj,,c, with c = d
and (sj, f j ) C / ( s y , fj,.) r ~ is called an overlap edge.
Note that for c = 1 , . . . ,n the subgraph Go(I) induced by the set of nodes
{vj,~]j = 1 , . . . , n } is equivalent to the interval graph G corresponding to the
intervals (sj, fj). The subgraphs Gc (I) are coupled by the bridge edges. It follows
that G(I) has the form K x G, where K is a clique and G is an interval graph.

289
L e m m a 3. If I is an instance of the OCCP problem for interval graphs, then

M'(I) is a clique matrix of G(I).
P r o o f . It follows from the definition of G(I) that each clique of G(I) contains
either bridge edges or overlap edges, but not edges of both types.
If K is a maximum clique of G(I) containing only bridge edges, then there
exists an interval j such that the nodeset of K is the set {vj,c]c = 1 , . . . , n } .
Thus K is represented in MI([) by the row corresponding to the constraint

Ec=I X J , c

:

1.

Next, let K be a maximum clique of G(I) containing only overlap edges.
Suppose K is a subgraph of Go(I) corresponding to color c, and let the nodeset
of K be denoted by {vjxlj C S}. Then the intervals p and q are defined as
follows: Sp = max{sj]j C S} and fq = min{fj]j E S}. Now, by definition,
(Sp, fp) n (sq, fq) 7s 9, which implies sp < fq. Note that this is trivially true
if p = q. The definition of p and q implies that the time interval (Sp, fq) is
a subset of all intervals in the set S. It follows that S = {j]sj <_ Sp < fj }.
Thus K is represented in MI(I) by the row corresponding to the constraint
~{jlsj<sp<~j} xj,c _< 1 in (21). This completes the proof of Lemma 3.
[]

4.3

Strong Perfect Graph Conjecture

In this section necessary and sufficient conditions for the matrix MI(I) to be
perfect are expressed in terms of the associated intersection graph G(I). To that
end, an undirected graph G = (V, E) is said to be perfect if for each subset
V / C V the node-induced subgraph G / = ( V I , E ~) has the property
~ ( a 1) x co(a 1) ___ fY/I,

(8)

where a ( G I) denotes the size of a maximum independent set in G / and where

co(GI) denotes the size of a maximum clique in G/. If a graph G is not perfect,
then it contains a node-induced p-critical subgraph G/. A p-critical graph is not
perfect, but all of its node-induced subgraphs are perfect.
It is well known that interval graphs are perfect. Typical examples of nonperfect graphs are the odd holes and the odd antiholes of size 5 or more. A hole
is a graph with nodes 1 , . . . , n and edges (v, v + 1) for v = 1 , . . . , n - 1 as well as
the edge (n, 1). An antihole is the complement of a hole.
I f G is an odd hole of size 5 or more, then a(G) = (Iv{- 1)/2 and co(G) = 2.
Similarly, if G is an odd antihole of size 5 or more, then a(G) = 2 and co(G) =
(IVI - 1)/2. Thus if G is an odd hole or an odd antihole of size 5 or more, then
inequality (8) is not satisfied for G itself. It follows that any graph that contains
an odd hole or an odd antihole of size 5 or more as a node-induced subgraph is
not perfect. The converse of this statement is called the Strong Perfect Graph
Conjecture (Berge [2]).

Strong Perfect Graph Conjecture. If a graph G is not perfect, then it contains an odd hole or an odd antihole of size 5 or more as a node-induced subgraph.

290
Another formulation of the Strong Perfect Graph Conjecture states that the odd
holes and the odd antiholes of size 5 or more are the only p-critical graphs.
For several classes of graphs the Strong Perfect Graph Conjecture has been
validated. For example, it holds for toroidal graphs (Grinstead [11]), for claw-flee
graphs (Parthasarathy & Ravindra [19]), for planar graphs (Tucker [23]), and
for K4-free graphs (Tucker [24]). However, a general proof of the Strong Perfect
Graph Conjecture is still awaited at this moment.
In Theorem 4 we will show that the Strong Perfect Graph Conjecture also
holds for graphs G(I) where I is an instance of the OCCP problem for interval
graphs. In other words, the Strong Perfect Graph Conjecture holds for graphs
of the form K x G, where K is a clique and G is an interval graph.

If I is an instance of the OCCP problem for interval graphs, then
G(I) is perfect if and only if G(I) does not contain an odd hole of size 5 or more
as a node-induced subgraph.
Theorem4.

P r o o f . Let I be an instance of the OCCP problem for interval graphs, and let

G(I) be the associated intersection graph. Suppose G(I) is not perfect. Then
G(I) contains a node-induced p-critical subgraph G r. It is well known that each
node of a p-critical subgraph G ~ is contained in exactly w(G ~) maximal cliques
of G', if w(G') denotes the size of a maximum clique of G' (Padberg [18]).
As for c = 1 , . . . ,n the interval graph Go(I) is perfect, G ~ contains at least
one bridge edge of G(I), say between GI(I) and G2(I). Thus the subgraphs
G ~n G1 (I) and G r N G2 (I) are not empty. Now we consider the interval p defined
by fp = min{fjlnj,1 E G'}. According to the definition of interval p, all intervals
j with vj,1 E G t that are overlapping interval p are also overlapping each other.
It follows that all nodes in the set {vj,1 e G'I(sj, fj) f3 (Sp, fp) r ~} make up a
maximum clique of G ~. This clique contains only overlap edges. Note that Vp,1
is an element of this clique. Node vp,1 is contained in at most 1 other maximum
clique, namely a clique containing only bridge edges. Thus node Vp,1 is contained
in at most 2 maximum cliques. As G ~ is p-critical, each node is contained in at
least 2 maximum cliques. It follows that vp,1 is contained in exactly 2 maximum
cliques. As a consequence, w(G ~) = 2. The latter implies that G ~ is an odd hole
of size 5 or more.
[]
The result of Theorem 4 can be improved slightly. Indeed, if the node-induced
subgraph G j of G(I) would be an odd hole of size 5, then G' would contain
exactly 2 bridge edges and exactly 3 overlap edges. (Indeed, recall that G' can
not be a subgraph of one of the graphs Gc(I), since the latter are perfect). Let
the corresponding intervals be called 1, 2 and 3, and let, the corresponding colors
be called 1 and 2. Then the nodes of G ~ are indicated by v1~1, v2,1, vl,2, v2,2 and
v3,2. Since there is an edge between the nodes v1,1 and v2,1, the intervals 1 and 2
are overlapping. But then there is also an edge between the nodes vl,2 and v~,2.
This implies that G ~ contains a chord and, as a consequence, it is not an odd
hole. Thus the following corollaries have been obtained.

291

Corollary 5. If I is an instance of the OCCP problem for interval graphs, then
G(I) is perfect if and only if it does not contain an odd hole of size 7 or more
as a node-induced subgraph.
Corollary 6. If I is an instance of the OCCP problem/or interval graphs, then
M~(I) is perfect if and only if G(I) does not contain an odd hole of size 7 or
more as a node-induced subgraph.
P r o o f . This result is a consequence of the following well-known result: if the
zero/one matrix M is a clique matrix of its associated intersection graph G,
then M is perfect if and only if G is perfect (Chvatal [3]).
[]

5

Final R e m a r k s

In this paper we have studied the Optimal Cost Chromatic Partition problem
for trees and interval graphs. First, we have given an algorithm based on the
idea of dynamic programming for solving the OCCP problem for trees which
runs in linear time.
Furthermore, we have shown that the OCCP problem for chordal graphs can
be solved in polynomial time if there are only two different values for the coloring
costs. This result is also valid for interval graphs, since each interval graph is a
chordal graph. However, if there are at least four different values for the coloring
costs, then the OCCP problem for interval graphs is NP-hard.
We have presented a formulation of the problem as an integer program with
coefficient matrix M ' ( I ) . The matrix M ' ( I ) is shown to be a clique matrix of its
associated intersection graph G(I). This implies that perfectness of the matrix
M ' ( I ) is equivalent to perfectness of the graph G(I). Thus, if G(I) is perfect,
then the problem can be solved by linear programming. We have shown that
G(I) is perfect if and only if it does not contain an odd hole of size 7 or more
as a node induced subgraph. Thereby we have proved that the Strong Perfect
Graph Conjecture is valid for graphs of the form K x G, where K is a clique
and G is an interval graph.

References
1. E.M. Arkin, and E.L. Silverberg. Scheduling jobs with fixed start and finish times.
Discrete Applied Mathematics, 18 (1987), 1-8.
2. C. Berge. Fs
von Graphen deren sgmtliche bzw. deren ungerade Kreise starr
sin& Wiss. Z. Martin-Luther Univ. Halle-Wittenberg. Math.-Natur. Reihe, 114
(1961).
3. V. Chvatal. On certain polytopes associated with graphs. J. Comb. Theory, B-18
(1975) 138-154.
4. V.R. Dondeti, and H. Emmons. Job scheduling with processors of two types. Operations Research, 40 (1992) $76-$85.
5. V.R. Dondeti, and H. Emmons. Algorithms for preemptive scheduling of different
classes of processors to do jobs with fixed times. European Journal of Operational
Research, 70 (1993) 316-326.

292
6. M. Fischetti, S. Martello, and P. Toth. The Fixed Job Schedule Problem with
spread time constraints. Operations Research, 6 (1987) 849 858.
7. M. Fischetti~ S. Mm'tello, and P. Toth. The Fixed Job Schedule Problem with
working time constraints. Operations Research, 3 (1989) 395-~403.
8. M. Fischetti, S. Martello and P. Toth. Approximation algorithms for Fixed Job
Schedule Problems. Operations Research, 40 (1992) $96-$108.
9. M.R. Garey, and D.S. Johnson. Computers and intractability: A guide to the theory
of NP-Completeness. Freeman, San Fransisco, 1979.
10. M.R. Garey~ D.S. Johnson, G.L. Miller and C.H. Papadimitriou. The complexity
of coloring circular arcs and chords. SIAM Journal of Alg. Disc. Meth, 1 (1980)
216-227.
11. C. Grinstead. The perfect graph conjecture for toroidal graphs. Annals of Discrete
Mathematics, 21 (1984) 97-101.
12. M. GrStschel, L. Lovasz, and A. Schrijver. Geometric algorithms and combinatorial
optimization, (1988). Springer Verlag.
13. A.W.J. Kolen, and L.G. Kroon. On the computational complexity of (maximum)
class scheduling. European Journal of Operational Research, 54 (1991), 23-38.
14. A.W.J. Kolen, and L.G. Kroon. Licence Class Design: complexity and algorithms.
European Journal of Operational Research, 63 (1992), 432-444.
15. A.W.J. Kolen, and L.G. Kroon. On the computational complexity of (maximum)
shift class scheduling. European Journal of Operational Research, 64 (1993), 138151.
16. A.W.J. Kolen, and L.G. Kroon. Analysis of shift class design problems. European
Journal of Operational Research, 79 (1994)~ 417-430.
17. E. Kubicka, and A. Schwenk. Introduction to chromatic sums. Proc. ACM Computer Science Conference, (1989).
18. M.W. Padberg. Perfect Zero-One matrices. Mathematical Programming, 6 (1974)
180-196.
19. K.R. Parthasarathy, and G. Ravindra. The Strong Perfect Graph Conjecture is
true for K1,3-free graphs. J. Comb. Theory, B-21 (1976) 212-223.
20. A. Sen, H. Deng, and S. Guha. On a graph partition problem with an application
to VLSI layout. Information Processing Letters, 43 (1991) 87-94.
21. A. Sen, H. Deng, and A. l~oy. On a graph partition problem with application
to multiprocessor scheduling. TR-92-020, Department of Computer Science and
Engineering, Arizona State University.
22. K.J. Supowit. Finding a maximum planar subset of a set of nets in a channel.
IEEE Trans. on Computer Aided Design, CAD 6, 1 (1987) 93-94.
23. A. Tucker. The Strong Perfect Graph Conjecture for planar graphs. Canad. Y.
Math, 25 (1973) 103-114.
24. A. Tucker. The validity of the perfect graph conjecture for K4-free graphs. Annals
of Discrete Mathematics, 21 (1984) 149-157.
25. M. Yanakakis, and F. Gavril. Tile maximum k-colorable subgraph problem for
chordal graphs. Information Processing Letters, 24 (1987)~ 133-137.

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

An Interference-Aware Channel Assignment
Scheme for Wireless Mesh Networks
Arunabha Sen and Sudheendra Murthy

Samrat Ganguly and Sudeept Bhatnagar

Dept. of Computer Science & Engineering
Arizona State University
Tempe, Arizona 85281
Email: {asen, sudhi}@asu.edu

Broadband Computing Division
NEC Laboratories America, Inc.
Princeton, NJ 08540, USA
Email: {samrat, sudeept}@nec-labs.com

Abstract—Multichannel communication in a Wireless Mesh
Network with routers having multiple radio interfaces significantly enhances the network capacity. Efficient channel assignment and routing is critical for realization of optimal
throughput in such networks. In this paper, we investigate the
problem of finding the largest number of links that can be
activated simultaneously in a Wireless Mesh Network subject
to interference, radio and connectivity constraints. Our goal
is to activate all such links and we present an interference
aware channel assignment algorithm that realizes this goal. We
show that the Link Interference Graph created by utilizing a
frequently used interference model gives rise to a special class
of graphs, known as Overlapping Double-Disk (ODD) graphs. We
prove that the Maximum Independent Set computation problem
is NP-complete for this special class of graphs. We provide a
Polynomial Time Approximation Scheme (PTAS) for computation
of the Maximum Independent Set of an ODD graph. We use
this PTAS to develop a channel assignment algorithm for a
multiradio multichannel Wireless Mesh Network. We evaluate the
performance of our channel assignment algorithm by comparing
it with the optimal solution obtained by solving an integer linear
program. Experimental results demonstrate that our channel
assignment algorithm produces near optimal solution in almost
all instances of the problem.

I. I NTRODUCTION
Multichannel communication in a wireless mesh environment with routers having multiple radio interfaces significantly
enhances the network capacity [10]. Efficient channel assignment and routing schemes are crucial for realization of optimal
throughput in such networks. It is well known that commonly
used 802.11a/b/g protocols support multiple channels. In the
Wireless Mesh Network (WMN) considered in this paper,
each node is equipped with multiple radios (Network Interface
Cards) and each radio can be tuned to a channel. A pair on
nodes (a, b) can communicate with each other if they are
within the transmission range of one another and at least one
radio in a shares a common channel with a radio in b. Clearly,
assignment of channels on the links of the WMN plays a
critical role in determining the achievable throughput. We investigate the following question: Given the (i) locations of the
wireless mesh routers, (ii) transmission and interference ranges
of the transmitters, (iii) the number of channels available on
each link and (iv) the number radio interfaces available at each
Supported in part by ARO grant W911NF-06-1-0354

router, what is the largest number of links that can be activated
simultaneously subject to interference and radio constraints so
that the resulting network is connected? Our goal is to activate
all such links and we present an interference-aware channel
assignment algorithm that realizes this goal.
Our channel assignment scheme is traffic unaware in the
sense that the channels are assigned without taking into
account traffic pattern or the paths to be taken for establishing
connections between source-destination node pairs. Changes
and adjustments in the assignments are made only when new
routers are added to the network or some existing routers
are disabled due to failure. Such traffic unaware channel
assignment schemes have been used by other researchers.
Raniwala et. al on the other hand have proposed channel
assignment algorithms that require prior knowledge about
traffic patterns and communication paths [10]. In this paper,
we show that the Link Interference Graph constructed with the
widely used interference model gives rise to a special class of
graphs known as Overlapping Double-Disk (ODD) graphs. We
prove that the Maximum Independent Set (MIS) computation
problem is NP-complete, even for this special class of graphs.
The contributions of this paper are summarized below.
•
•

•

•

Novel characterization of the Link Interference Graphs as
Overlapping Double-Disk graphs.
Development of a Polynomial Time Approximation
Scheme (PTAS) for computation of MIS of an ODD
graph.
Development of a channel assignment algorithm with an
objective of activating the largest number of links subject
to interference and radio constraints.
Comprehensive performance evaluation of the heuristic
solution in comparison with the optimal solution obtained
by solving an integer linear program.

The rest of the paper is organized as follows. Section II
provides a summary of related work in this area. Section
III provides the problem formulation. Section IV discusses
the interference model. Section V presents the PTAS and
the performance guarantee proofs. Section VI presents the
optimal and heuristic solutions and describes the simulation
environment, results. Section VII wraps up the paper with
conclusion.

1-4244-0353-7/07/$25.00 ©2007 IEEE
3471

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

II. R ELATED W ORK
The advantages of deploying multiple channels and multiple
radios in wireless mesh networks is obvious and several
researchers in the last few years have been studying various
aspects of such deployment [1], [5]–[11]. Most of the work
in this area can be classified into four different groups, (i)
network capacity and related issues, (ii) channel assignment,
(iii) routing and (iv) joint routing and channel assignment.
The issues related to capacity of wireless networks and impact of multiple radios and interfaces have been studied by
several researchers [5]–[8]. Modeling the capacity of multiple
channels and interfaces and understanding the benefits was
studied by authors in [6], [7]. The authors in [6] provide
capacity model for multiple channels, using which feasibility
of a rate matrix can be verified. The issues related to channel
assignment have been studied in [9]. The authors in [9] extend
the notion of Conflict (or Link Interference) Graph to multiple
radio environment and use this multiradio conflict graph to
model interference between the routers. Several researchers
have investigated the routing and channel assignment problem
simultaneously [1], [10], [11]. In [10], the authors have proposed a framework for centralized channel assignment scheme
for maximizing the throughput.
Although various aspects of channel assignment in a multichannel multiradio wireless mesh networks have been investigated, to the best of our knowledge prior efforts did not
characterize the nature of the Link Interference (or Conflict)
Graph and exploit such characterization for the development
of efficient channel assignment algorithm.
III. P ROBLEM D EFINITION
In this section, we formally describe our system model and
notations. The system model described here is consistent with
the models considered by other researchers [1]. Most of the
researchers model a WMN as an undirected graph. Since the
wireless routers are distributed over a geometric region (often
a two dimensional plane), we consider the network geometry
in our model and exploit it to design efficient algorithms. To
the best of our knowledge, prior research in the area did not
use any geometric information in their proposed solutions.
At the first level of abstraction, we view the routers of a
WMN as some points (pi , . . . , pn ) (specified by their x, y coordinates) on a two dimensional plane. Associated with each
point pj , 1 ≤ j ≤ n is a transmission range, RT and an
interference range, RI , RI > RT (we assume that all routers
have identical transmission and interference ranges). At the
second level of abstraction, we construct a graph G = (V, E),
in which each node represents a point on the plane (router)
and there is an edge from node vi to node vj if the Euclidean
distance between the corresponding points pi to pj is less
than or equal to the transmission range RT . This assumption
implies that each router has a circular coverage area with the
center of the circle at the location of the router. The circular
coverage area associated with point pi (and node vi ) will
be referred to as the disk associated with the point pi (and
node vi ). The graph G = (V, E) will be referred to as the

Potential Communication Graph (PCG). A link between any
two nodes in this graph indicates that this pair of nodes can
communicate with each other if their transmitters and receivers
are assigned the same channel. It may be noted that even
though these nodes are within the communication range of
each other, they may not be able to communicate with each
other unless the same channel is assigned to both of them. A
Potential Communication Graph is shown in figure 1.
The unit disk graphs are a special class of graphs, where
each node represents a point in the plane and two nodes
have an edge between them if the unit radius disks associated
with the points intersect. It may be noted that the Potential
Communication Graph is a unit disk graph. It was noted earlier
that our channel assignment algorithm does not consider the
traffic pattern in the network. In the absence of the load
information between source-destination pairs in the network,
a good channel assignment strategy would be to do channel
assignment in such a way that the resulting communication
graph can support as many simultaneous active links. The
problem considered in this paper is as follows: Given
• L, the location of the wireless routers
• RT , the transmission range
• RI , the interference range
• N , the number of available channels and
• K, the number of available radios at each of the routers,
the problem is to assign channels such that the number of
links that can be activated simultaneously is maximized subject
to radio, interference constraints and the resulting graph is
connected.
IV. I NTERFERENCE M ODEL : OVERLAPPING D OUBLE -D ISK
G RAPHS
In this section, we focus our attention on the interference
graph between the links of the PCG. We follow the interference model used in [1]. Simultaneous transmission on a
common channel on two distinct edges e1 and e2 of PCG
connecting nodes (u1 , v1 ) and (u2 , v2 ) respectively are said
to interfere with each other if minimum {d(u1 , u2 ), d(u1 , v2 ),
d(u2 , v1 ), d(v1 , v2 )} ≤ RI , where d(ui , vj ) indicates the
Euclidean distance between the nodes ui and vj and RI
indicates the interference range. The Link Interference Graph
LIG is constructed as follows: Corresponding to every link in
PCG, there is a node in LIG and two nodes in LIG have an
edge between them only if the corresponding links interfere
with each other. The notion of LIG is not new and has been
used in [1]. However, in all previous studies LIGs were treated
as any arbitrary graphs. We show that LIGs are not arbitrary
graphs and belong to a special class, which we refer to as
Overlapping Double-Disk (ODD) graphs.
Given the locations (p1 , . . . , pn ) of the routers on a two
dimensional plane, we draw a line connecting points pa and
pb to indicate the link la,b between the routers, if the distance
between pa and pb less than or equal to RT . Similarly, we
draw a line connecting points pc and pd to indicate the link
lc,d between the routers if the distance between pc and pd less
than or equal to RT . In order to determine if the links la,b and

3472

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

f

RT
2

b
a

f

b
e
a

c
d

Fig. 1. Potential Communication Graph of 6 mesh
routers

RI
2

e
c

vce
vbc

d
Fig. 2.
Overlapping
Double-Disks (ODD) of
links (a, b), (c, d) and
(e, f ). ODDs of links (b, c)
and (c, e) not shown for
clarity.

v5
vcd

vab
Fig. 3.
Graph

Link Interference

lc,d interfere with each other, we do the following: We draw a
circle with centers at the points pa , pb , pc , pd with radius RI /2.
Since d(pa , pb ) ≤ RT and RI ≥ RT , the circles with centers
at pa and pb will overlap, and create a figure of the form
shown in 2. The same thing will happen for the circles with
centers at pc and pd . We refer to this figure as Overlapping
Double-Disks (ODD). The mid-point of the line joining the
centers of the two disks will be referred to as the center of the
double disks. The links la,b and lc,d will interfere with each
other if and only if the corresponding ODDs intersect. Since
the LIG is the intersection graph of ODDs, we will refer to
the LIG as an ODD graph shown in figure 3.
Next, we show that the class of ODD graphs is a proper
subset of the set of all graphs. Consider the star graph with
12 nodes shown in figure 4. We claim that this is not an ODD
graph. The ODD corresponding to node v0 intersects with the
ODDs associated with nodes v1 through v11 . However, the
nodes v1 through v11 are not adjacent to each other and as
such the ODDs associated with them do not intersect with
each other. Suppose that in figure 5 the ODD in the center
corresponds to the node v0 . Since v1 is adjacent v0 , the ODD
corresponding to v1 will intersect the ODD corresponding
to the node v0 . Similarly, since v2 is adjacent v0 , the ODD
corresponding to v2 will intersect the ODD corresponding to
the node v0 . However, since v2 is not adjacent v1 , the ODD
corresponding to v2 will not intersect the ODD corresponding
to the node v1 . Continuing such argument, we need to have
the ODDs corresponding to the nodes v1 through v11 intersect
the ODD at the center (i.e., the ODD corresponding to node
v0 ) but not intersect each other. However, from figure 5 it is
clear that existence of 11 such ODDs that intersect with the
central ODD but do not intersect with each other is impossible.
This is true because after 7 ODDs in figure 5 (11 in the worst
case), there is no room to place another ODD that intersects
with the central ODD, but does not intersect with any other.
Accordingly, a star graph with 12 nodes cannot be an ODD
graph. The geometric proof establishing 11 to be the maximum
number of ODDs is not provided here as it is outside the
context of this paper.
V. PTAS FOR THE MIS P ROBLEM IN ODD G RAPHS
Theorem 1: The MIS problem for LIGs are NP-complete.

vef

v6
v7
v8
Fig. 4.
nodes

v4 v3
v0

v2
v1

v11
v9 v10

Star graph with 12

Fig. 5. Not all graphs are
ODD graphs

Proof: We have already established that the LIGs are
ODD graphs. Consider a special case of the ODD graph, where
the two disks overlap completely (i.e., we see only a single
disk). In this special case, the ODD graphs reduces to unit
disk graphs. Since the MIS Problem for the unit disk graphs
is NP-complete and unit disk graphs are restricted version of
the the ODD graphs, we can claim that the MIS problem for
the ODD graphs is also NP-complete.
Since the MIS problem for the ODD graphs is NP-complete,
we need an approximation algorithm to solve the problem.
An approximation algorithm for a maximization problem Π
is said to provide a performance guarantee σ (σ ≤ 1), if for
all instances I of Π, ratio between the approximate solution
produced by the algorithm to the optimal solution is at least
as large as σ. A Polynomial Time Approximation Scheme for a
maximization problem Π is a polynomial time approximation
algorithm that given any instance I of Π and a specified 
returns a solution that is at least as large as (1 − ) times the
optimal solution. In the following, we present a PTAS to solve
the channel assignment problem. We use the shifting strategy
proposed in [3] and used by Hunt et. al [4] to solve the MIS
problem of unit disk graphs. Erlebach et. al. [2] also used a
similar strategy to develop PTAS for intersection graphs of
disks with different diameters. We first describe the basic idea
of the PTAS for MIS of the ODD graphs. Given a set of n
ODDs distributed in a two dimensional plane A, we first divide
the area into strips of certain width. All the intervals are top
closed and bottom open. Given an  > 0, we calculate the
smallest integer k such that k/(k + 1) ≥ 1 − . We partition
the ODDs into r disjoint sets by removing the ODDs in the
horizontal strips congruent to i mod(k + 1), 0 ≤ i ≤ k. It may
be noted that different values of i will give rise to different
partitions. Two such partitions are shown in figures 6 and 7.
The approximate algorithm for computation of a MIS of a
ODD graph is given next.
A. Computation of MIS(Gi,j )
Consider the ODDs whose centers lie in a rectangular slice
RS of height 4k and width 4 as shown in figure 8. Since Gi,j
is an ODD graph, RS can contain no more than 6k+3 mutually
non-intersecting ODDs. This gives a bound on the size of the
MIS of ODDs whose centers lie in RS. Furthermore, removal

3473

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

Algorithm 1 PTAS: MIS of ODDGraphs(L, A, )
Input: Locations L of n ODDs distributed in a two dimensional
plane A,  < 0.5
Output: An Independent Set that is at least as large (1 − ) times
the MIS.
1: Find the smallest integer k/(k + 1) ≥ 1 − 
2: Divide the plane A into horizontal strips of width 4.
3: for i = 0 to k do
4:
Partition the set of ODDs into r disjoint sets
Gi,1 , Gi,2 , . . . , Gi,r by removing all the ODDs in every
horizontal
 strip congruent to i mod(k + 1)
5:
Gi = 1≤j≤r Gi,j
6:
for j = 0 to r do
7:
Compute the MIS in Gi,j
8:
end for

9:
M IS(Gi ) = 1≤j≤r M IS(Gi,j )
10: end for
11: Independent Set IS(G) = max0≤i≤k M IS(Gi )

Fig. 6.

Grouping 1 of the strips

Grouping 2 of the strips

Fig. 7.

of ODDs in RS divides the set of ODDs into disjoint sets L
and R. Suppose that the ρ is the number of ODDs in RS. By
examining each combination of at most 6k+3 ODDs (or nodes
in Gi,j ), we get all the independent sets in RS. This can be
done in O(ρ6k+3 )
L

Fig. 8.

RS

R

Rectangular slice RS divides the area into L and R

)|OP T (G)|. The proof of this claim follows along the same
lines as the proof for the unit disk graphs given in [4].
From the description of the algorithm, it is clear that
during each iteration (for different values of i), the algorithm
excludes some ODDs from independent set computation. First,
we establish the fact that the number of ODDs excluded
from independent set computation is a small fraction of the
MIS, at least during one iteration of the algorithm. During
each iteration of i, we exclude the ODDs (and hence the
vertices in the ODD graph) in the strips j1 , j2 , . . . , jp where
jl = i mod(k + 1), 1 ≤ l ≤ p. Suppose that Si denotes the
set of ODDs excluded during the ith iteration and ISopt (Si )
denotes the vertices (corresponding to the ODDs) in the set
Si that are part of the optimal (maximum) independent set
OP T (G).
k
|OP T (G)|
Lemma 2: max0≤i≤k |OP T (Gi )| ≥ k+1
Proof: Noting that different strips are considered during
different iterations, we make the following
t=k observations: 0 ≤
i, j ≤ k, i = j, Si ∩ Sj = ∅ and t=0 Si = V (G). From
this, we get |ISopt (S0 )| + |ISopt (S1 )| + . . . + |ISopt (Sk )| =
T (G)|
|OP T (G)|. This leads to min0≤t≤k |ISopt (St )| ≤ |OPk+1
.
k
=⇒ max0≤i≤k |OP T (Gi )| ≥ k+1 |OP T (G)|
Theorem 3: |IS(G)| ≥ (1 − )|OP T (G)|
j=r
Proof: We note that |OP T (Gi )| = j=1 |OP T (Gi,j )|
From the above equation and lemma 2, we make the
following 
derivation: |IS(G)| ≥ max
0≤i≤k |IS(Gi )| =
j=r
j=r
max0≤i≤k j=1 |IS(Gi,j )| = max0≤i≤k j=1 |OP T (Gi,j )|
k
= max0≤i≤k |OP T (Gi )| = k+1 |OP T (G)|
=⇒ |IS(G)| ≥ (1 − )|OP T (G)|
VI. C HANNEL A SSIGNMENT IN M ULTICHANNEL
M ULTIRADIO W IRELESS M ESH N ETWORK
In this section, we present a technique of solving the
channel assignment problem optimally using Integer Linear
Programming (ILP). We then present a heuristic that applies
the PTAS described in the previous section repeatedly to
compute the channel assignment efficiently.
A. Optimal solution using ILP

Suppose that the disks associated with the independent
sets in RS are {ISRS1 , ISRS2 , . . . , ISRSt }. The following
principle of optimality holds for L, R and RS.
M IS(L ∪ RS ∪ R) = max1≤i≤t (M IS(L − L ∩ ISRSi )+
|ISRSi | + M IS(R − R ∩ ISRSi ))
The computational time for the procedure will be given by
T (L ∪ RS ∪ R) ≤ T (L) + O(ρ6k+3 ) + T (R)
Overall execution time complexity of the algorithm will be
O(nO(k) ) where n is the total number of ODDs in the two
dimensional plane A.
B. Performance Guarantee of Approximate Algorithm
In this section, we prove that the size of the independent
set computed by the approximate algorithm is close to the
size of the MIS. Formally, we will show that |IS(G)| ≥ (1 −

Let G(V, E) and G (V  , E  ) be PCG and LIG respectively.
Let N be number of channels in the network. Let k be number
of radios per node. The binary variables are defined as follows.
∀v ∈ V, 1 ≤ i ≤ N , Xiv = 1, if channel i is used on
node v, 0 otherwise. ∀e ∈ V  , 1 ≤ i ≤ N, Yie = 1, if
channel i is used on both end points of e, 0 otherwise. The
objective is to maximize the number
links
 that can be activated

simultaneously, that is, max ∀e∈V  1≤i≤N Yie
The set of constraints are as follows.
• Radio constraints: The number of channels assigned to
a node should not exceed
the number of radios on the
N
v
node. That is, ∀v ∈ V,
i=1 Xi ≤ k
• Link-Channel Usage Constraints: Channel i is used on
link e if and only if channel i is set on both the end
points of e. ∀e = (u, v) ∈ E, Xiu + Xiv ≥ 2Yie and
Xiu + Xiv − 1 ≤ Yie .

3474

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

•

•

Capacity Constraints: The set of links in G corresponding
to a maximal clique in G defines the set of mutually
interfering links. In this set, at most one link can be
active.
maximal clique C in G , ∀1 ≤ i ≤
For every
e
N,
e∈C Yi ≤ 1
Connectivity Constraints: To ensure connectivity, we try
to route unit flows from an arbitrary source node s to
all other nodes in the graph. Define for convenience, a
t
=1
t-flow as the flow destined towards node t. Let F(u,v)
if there is a t-flow on link (u, v), 0 otherwise. The flow
conservation states that the total incoming flow should be
equal to the total outgoing flow at the intermediate
nodes.

t
=
That is, ∀t ∈ V \ s, ∀v ∈ V \ {s, t}, (u,v)∈E F(u,v)

t
F
.
There
is
a
unit
flow
going
out
of
source
(u,v)∈E (v,u)

t
and no flow into the source: ∀t ∈ V , (s,v)∈E F(s,v)
=

t
1 and
(v,s)∈E F(v,s) = 0. Note that there can be a
t-flow through link (u, v) only when u and v have at
least one channel in common. This can be represented
by the following
constraints: ∀t 
∈ V \ s, ∀e = (u, v),
N
N
t
t
≤ i=1 Yie and F(v,u)
≤ i=1 Yie .
F(u,v)

Algorithm 3 MIS channel assignment(location of the
routers, RT , RI , N)
1: construct the potential communication graph G from location of

the routers and RT .

2: c ← 1
3: while |E(G)| > 0 do
4:
construct an isomorphic copy H of graph G with V (H) =
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

15:
16:

B. Heuristic
The heuristic for the channel assignment takes as input
the location of the routers, transmission radius, interference
radius, number of channels, number of radios on each node
and outputs the channels assigned to the radios of each router.
The heuristic starts by reserving one radio on each node for
ensuring connectivity later. The channel assignment heuristic
invokes two functions namely, MIS channel assignment
and ensure connectivity.
Algorithm 2 channel assignment(L, RT , RI , N, K)
Input: Location of the routers L, Transmission range RT , Interference Range RI , Number of channels N, Number of radios K
Output: Channels assigned to each router
1: Reserve one radio on each node for ensuring connectivity
2: G ← MIS channel assignment(location of the routers, RT ,
RI , N)
3: Free the reserved radio on each node
4: ensure connectivity(G, N)

The objective of MIS channel assignment is to assign
channels such that the number of links that can be activated
simultaneously is maximized. The algorithm computes largest
independent sets of the nodes of LIG repeatedly and assigns
a channel to the nodes in each independent set. Recall that
the nodes of the LIG are the links in the PCG. Thus, this
algorithm in each iteration tries assigning the same channel
to as many non-interfering links (more specifically to the
end routers of the link) as possible in the PCG. The algorithm applies the PTAS MIS of ODDGraphs to compute the
(largest) independent set in the LIG. The nodes in this set are
temporarily removed from the LIG and another independent
set is computed. This process is repeated until all nodes in
PCG have been assigned some channel. At this stage, the LIG
is constructed again and the process is repeated. Throughout,

17:
18:
19:
20:

V (G) and E(H) = E(G).
construct conflict graph H  of graph H.
while |E(H)| > 0 do
M IS ← MIS ODD graph(H  ). {Note that M IS ⊆

E(H) since
 V (H ) = E(H)}
let VH = u, v such that (u, v) = e ∈ M IS. Let VG be
the corresponding set of vertices in graph G.
if channel c is already assigned to all nodes in VG then
find the next channel c that is not assigned to at least
one node in VG .
end if
∀v ∈ VG , assign channel c to a free radio in node v.
for all v ∈ VG such that v has zero radios left do
remove vertex v and all edges incident on v from graph
G and graph H. Also, remove the corresponding vertices
and the edges incident on these vertices from H  .
end for
E(H) ← E(H)\M IS. Remove the corresponding vertices
and the incident edges from the conflict graph H  .
c ← (c + 1) mod N
end while
end while
return G

Algorithm 4 ensure connectivity(G, N)
Input: Potential Communication Graph G with partial channel assignments, number of channels N
Output: Potential Communication Graph with all the channel assignments
1: Compute all the connected components C1 , C2 , ..., Cp in G.
2: S ← C1 {S is the connected subgraph of G}
3: while ∃Ci ∈
/ S do
4:
next path ← φ, next channel ← 0
5:
smallest int degree ← ∞
6:
for all path P such that P connects S with a component not
in S do
7:
channel(P ) ← channel assignment that results in least
interference on path P .
8:
interf erence degree(P ) ← interference degree on P by
assigning channel(P ) on P .
9:
if (interf erence degree(P ) <
smallest int degree) then
10:
smallest int degree ←
interf erence degree(P )
11:
next path ← P .
12:
next channel ← channel(P ).
13:
end if
14:
end for
15:
Assign channel next channel on all links of path
next path.
16:
Let next
 path connect S with component Ci .
S ← S Ci .
17: end while

the number of free radios on each node in PCG is kept track
of and whenever all radios in a node are used up, the node

3475

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings.

Fig. 9. Maximum number links that can be active
simultaneously for different number of radios and
for 8 channels

Fig. 10. Maximum number links that can be active
simultaneously for different number of radios and
for 10 channels

and all its edges are removed from the PCG and LIG.
At the end of the MIS channel assignment algorithm,
the topology resulting from the channel assignment may have
several connected components. The ensure connectivity algorithm uses the single radio that was reserved earlier to
connect all the components. The algorithm maintains a set S
consisting of a single connected component. At each iteration,
all paths that connect S with some component not in S are
examined. The interference degree of a path is the largest
number of edges interfered by an edge in the path. The path
and the channel to be assigned on all nodes of this path that
lead to the least interference is computed. Channel assignments
are done on this path and the component Ci connected by this
path is included into S. This procedure is repeated until all
components are merged into S.
C. Simulation Environment, Results and Discussion
We conducted experiments to evaluate the efficiency of our
heuristic. In all experiments, there were 30 routers each having
a transmission radius of 150m and an interference radius of
300m. The locations for the routers were randomly generated
in an area of width 1500m and height 1500m. In the first set of
experiments (figures 9,10), we studied the effect of increasing
the number of radios in each router on the total number of links
that can be activated simultaneously. We varied the number of
radios from 2 to 8. The experiment was conducted for 8 and
10 channels. In the second set of experiments (figures 11,12),
we studied the effect of increasing the number of channels
on the number of links that can be active simultaneously. We
varied the number of channels from 5 to 12. The experiment
was conducted for 4 and 5 radios on each router. For the PTAS
for computing MIS, we chose  = 0.1. Additional experiments
were conducted to measure the effect of transmission radius
and the interference radius on the number of links that can
be activated simultaneously. These results are not included
here due to the paucity of space. The experimental results
demonstrate that our channel assignment algorithm produces
near optimal solution in almost all instances of the problem.
VII. C ONCLUSION
In this paper, we have provided a heuristic for the channel
assignment problem in Wireless Mesh Networks. In the process, we characterize the LIG as ODD graphs and provide a

Fig. 11. Maximum number links that can be active
simultaneously for different number of channels
and each router having 4 radios

Fig. 12.
Maximum number links that can be active simultaneously for
different number of channels and each router having 5 radios

PTAS to compute MIS for ODD graphs. Our results demonstrate the effectiveness of the heuristic.
R EFERENCES
[1] M. Alicherry, R. Bhatia, and L. Li, “Joint channel assignment and routing
for throughput optimization in multi-radio wireless mesh networks,” In
proc of ACM MOBICOM 2005.
[2] T. Erlebach, K. Jansen, and E. Seidel, “Polynomial-time approximation
schemes for geometric intersection graphs”, SIAM Journal of Computing,
vol.34, no.6, pp.1302-1323, 2005.
[3] D. S. Hochbaum, and W. Maass, “Approximation schemes for covering
and packing problems in image processing and VLSI”, Journal of the
Association for Computing Machinery, vol.32, no.1, pp.130-136, January,
2005.
[4] H. B. Hunt, et. al., “NC-approximation schemes for NP- and PSPACEhard problems for geometric graphs”, Journal of Algorithms, vol.26,
pp.238-274, 1998.
[5] P. Gupta, and P. R. Kumar, “The capacity of wireless networks”, IEEE
Transactions on Information Theory, 46[2]: 388-404, 2000.
[6] M. Kodialam, and T. Nandagopal, “Characterizing the capacity region
in multi-radio multi-channel wireless mesh networks”, In proc of ACM
Mobicom, 2005.
[7] P. Kyasanur, and N. Vaidya, “Capacity of Multi-Channel Wireless Networks: Impact of Number of Channels and Interfaces,” In Proc. of ACM
Mobicom, 2005.
[8] J. Li, et. al., “Capacity of Ad Hoc Wireles Networks, Proc. of ACM
Mobicom 2001.
[9] K. N. Ramachandran, E. M. Belding, K. C. Almeroth, and M. M. Buddhikot, “Interference-aware channel assignment in multi-radio wireless
mesh networks”, Proc. of IEEE Infocom 2006.
[10] A. Raniwala, K. Gopalan, and T. Chiueh, “Centralized channel assignment and routing algorithms for multi-channel wireless mesh networks”,
Mobile Computing and Communications Review, vol.8, no.2, 2005.
[11] J. So, and N. Vaidya, “Routing and channel assignment in multi-channel
multi-hop wireless networks with single network interface,” In Proc of
QShine, 2005.

3476

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

Dynamic Lightpath Allocation in Translucent WDM
Optical Networks†
Subir Bandyopadhyay1 , Quazi Rahman1 , Sujogya Banerjee2 , Sudheendra Murthy2 and Arunabha Sen2
1

School of Computer Science, University of Windsor
Windsor, Canada N9B 3P4 Email: {subir, rahmanq}@uwindsor.ca
2
Department of Computer Science and Engineering, Arizona State University
Tempe, Arizona 85281 Email: {sujogya.banerjee, sudhi, asen}@asu.edu

Abstract—The optical reach (the distance an optical signal
can travel before the signal quality degrades to a level that
necessitates regeneration) ranges from 500 to 2000 miles. To
establish a lightpath of length greater than the optical reach, it is
necessary to regenerate optical signals. In a translucent optical network, there are regeneration points, where the signal undergoes
Optical-Electronic-Optical (O-E-O) conversion. In this paper we
have proposed routing algorithms for translucent networks in a
dynamic lightpath allocation environment in which requests for
communication arrive continuously. In response to each request
for communication, the objective is to establish, if possible, a
path, from the source to the destination of the request for
communication, so that a lightpath may be established, using the
path that requires the fewest stages of regeneration. In practical
transparent networks, a lightpath must satisfy the wavelength
continuity constraint. However, in a translucent network, this
constraint can be relaxed at the regeneration points. We have
proposed an Integer Linear Program, to give the optimum results
for small networks, as well as an efficient heuristic for this
problem that works for larger networks. We have evaluated
the heuristic through extensive simulations to establish that the
heuristic produces close-to-optimal solutions in a fraction of the
time needed for the optimal solutions. Our extensive evaluations
demonstrate the relative impact of a set of network resources,
such as (i) the number of regenerators, (ii) the optical reach
of the regenerators and (iii) the number of wavelengths, on the
network performance, measured in terms of the call blocking
probability. To the best of our knowledge this is the first study
that undertakes such an evaluation for translucent networks.

I. I NTRODUCTION
Communication in all-optical or transparent networks is
carried out purely in the optical domain without any OpticalElectronic-Optical (O-E-O) conversion. However, factors such
as optical noise, chromatic dispersion, nonlinear effects, polarization mode dispersion (PMD) and cross-talk cause the
optical signal quality to degrade as it propagates through a
fiber [1]–[4]. The distance r an optical signal can propagate
before its quality degrades to a level that necessitates regeneration, is called the optical reach, which typically ranges
from 500 to 2000 miles [2]. To establish any communication
path greater than r, it is necessary to reamplify, reshape and
retime the optical signal (often called 3R regeneration) [3].
In translucent networks, since 3R regeneration is expensive,
† This is based upon work supported by, or in part by, the U. S. Army
Research Laboratory and the U. S. Army Research Office under contract/grant
number W911NF-06-1-0354. S. Bandyopadhyay was supported by a grant
from the Natural Sciences and Engineering Research Council of Canada.

only a subset of the nodes are capable of performing the 3R
regeneration. Henceforth we will call the lightpath from a
source to a destination that involves one or more regenerators
as a translucent lightpath. A translucent lightpath consists of
two or more transparent (i.e., all-optical) lightpaths where one
transparent lightpath is from the source of the communication
to a regenerator, one from a regenerator to the destination for
the communication and the remaining transparent lightpath(s),
if any, is (are) from one regenerator to another. The total
length of the fibers in each transparent lightpath must not
exceed the optical reach, r. Two problems in translucent network design have received attention recently. The Regenerator
Placement Problem (RPP) [1]–[9] is to find i) the minimum
number of regenerators and ii) their locations, so that a
communication path can be established between every pair of
source-destination nodes in the network. In the Routing with
Regenerators Problem (RRP), the locations of the regenerators
are known and the objective is to compute a path between
a source-destination node pair using as few regenerators as
possible [10], [11]. In this paper we study the RRP problem.
We will call each transparent component of a translucent
lightpath as a segment. It is important to carry out regeneration
as sparingly as possible, since regeneration results in both
increased delays and Bit Error Rates (BER) as a result of
O-E-O conversion [10]. An important objective in dynamic
translucent lightpath allocation in translucent networks is,
therefore, to compute a source-destination path that requires
a minimum number of regenerators. Since O-E-O conversion
takes place at regenerators, wavelength conversion is available for free at the regenerators. We assume that all-optical
wavelength converters are not available, so that the wavelength
continuity constraint [12] must be satisfied for each segment.
An example of a long haul network with distances between
the nodes in miles is shown in Figure 1. If the optical reach is
r = 2000 miles, an optical signal from node A cannot reach
node H without regeneration. For communication between A
and H, if there is a regenerator at D, a translucent lightpath
(P = A → B → C → D → F → G → B → C → H)
with two segments (S1 = A → B → C → D) and
(S2 = D → F → G → B → C → H) can be established.
Property 1: If a translucent lightpath involves two segments, Sa and Sb that have one or more common fiber (s),
the same wavelength cannot be used for both segments Sa

978-1-4244-3435-0/09/$25.00 ©2009 IEEE

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

and Sb [13].
For instance, in Figure 1, the segments S1 and S2 of the
translucent lightpath P have the common fiber B → C. If
channel c1 is available on all the fibers in the network, when
processing a request for communication from A to H, channel
c1 cannot be used for both segments S1 and S2 .
H
1200
A

500

B

500
C

700

1500

D

E

50

50

G

F
100

Fig. 1. Long haul optical network with distances between the nodes in miles

To the best of our knowledge, this property has not been
taken into account in translucent network design [1]–[11].
In other words, existing algorithms for routing in translucent
networks may give an invalid route and wavelength if segments
have common fiber(s). An important contribution of this paper
is to show how this restriction may be taken into account
when solving the RRP problem. In this paper, we have studied
the problem of dynamic lightpath allocation to establish a
lightpath from some node S to some other node D in the
presence of other lightpaths (translucent or transparent), set
up earlier in response to previous requests for communication.
We have used Property 1 to
• propose an Integer Linear Program (ILP) which gives an
optimal path for a translucent lightpath using a minimum
number of regenerators1 .
• propose an efficient heuristic to compute the path with a
minimum number of regenerators.
• establish, through extensive simulations with dynamic
call arrivals, the effectiveness of the heuristic by measuring call blocking probability.
• demonstrate, through extensive simulations, the relative
impact of a set of network resources, such as (i) the
number of regenerators, (ii) the optical reach of the
regenerators and (iii) the number of wavelengths, on
the network performance, measured in terms of the call
blocking probability.
The rest of the paper is organized as follows. Section II is a
review of related work in this area. Section III proposes an ILP
and a heuristic to solve the problem of establishing translucent
lightpaths. Section IV describes the simulation results and
section V concludes the paper.
II. R ELATED W ORK
Recently the Minimum Regenerators Path problem has received some attention [1], [10], [11]. In [14], the authors have
proposed two dynamic routing algorithms based on the MPLS
hierarchy that considers regeneration resources available at
each regenerator and the link states. The proposed algorithms
1 The rationale for using regenerators as sparingly as possible has been
mentioned earlier.

try to minimize the total number of regeneration hops and
the total Bit Error Rate (BER) on the computed route. In
[11], the authors provide an intra-domain routing algorithm
considering a number of optical-layer constraints in a dynamic
call arrival setting. However, it may be difficult to obtain
BER, optical dispersion and other physical impairment factors
in real-time. Recent research [1] indicates that the optical
reach, r, can be used a rough approximation for all these
factors. Simmons [2] suggests some techniques for finding the
minimum regeneration route between a node pair, based on
enumerating a large number of paths between the source and
the destination. However, these techniques may not always
ensure the existence of a solution. In addition, the existing
routing techniques do not take into account the property mentioned above about segments that share fibers. As a result, the
source to destination path computed by the existing techniques
may not be a simple path (that is, edges may be repeated).
Such a path cannot be used in case the overlapping segments
have same channel number available for path establishment.
There are a number of routing algorithms (e. g., [15]) that
consider capacity constraints, physical impairments and the
wavelength continuity constraint in minimizing the cost of the
desired route. But, most of them use static computation in
which the routes are computed before the requests arrive. The
work in [11] is the closest to our work. However, since the
authors in [11] consider different set of parameters than our
problem, our solution techniques cannot be compared with
their work.
III. F ORMULATIONS FOR DETERMINING DYNAMIC
TRANSLUCENT LIGHTPATHS

In the heuristic described in Section III-C, we will use
the term path intersection graph to denote a graph GP =
(VP , EP ), where each vertex in VP represents a path through
the optical network that may be used to set up a transparent
lightpath. If p and q are two vertices in VP , there will be an
edge, in EP , between p and q, iff the paths through the optical
network, corresponding to p and q, share one or more edge(s).
A. Notation used in the formulations
r:
n:
s:

the optical reach.
the total number of nodes in the network.
a constant denoting the maximum possible number
of segments in a translucent lightpath.
dij : a constant denoting the distance of the edge i → j.
for node i, 1 ≤ i ≤ n, defined as follows:
δi : a constant

1 if the ith node is a 3R regenerator,
δi =
0 otherwise.
E : the set of all pairs (i, j) of nodes such that i → j is
an edge in the physical topology.
nch : the number of channels available on a fiber.
p
: A constant
wij
⎧ defined as follows
⎨ 1 if an existing lightpath uses channel p
p
on edge i → j,
wij
=
⎩
0 otherwise.

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

Wpk : A binary variable denoting whether channel p is
allocated
⎧ to segment kth such that
⎨ 1 if the k segment of the new
translucent lightpath uses channel p,
Wpk =
⎩
0 otherwise.
k
: a binary variable for each edge i → j in the physical
Xij
topology
⎧ and for eachthsegment k, such that
⎨ 1 if the k segment of the new
k
lightpath uses the edge i → j
=
Xij
⎩
0 otherwise.
m : the number of paths, for transparent lightpaths, that
will be pre-computed between every node pairs.
Pxy : the set of pre-computed paths from node x to y.
S : a set of states that need to be explored.
N : a set of new states.
C:
a set of paths from the current node.
R : the set of regenerator nodes in the optical network.
x:
a node in the optical network.
P : the set of paths, each with a total length ≤ r, that
may be used to reach x from source S.
cij : the set of channels still available on edge i → j.
Wv : the set of colors to color vertex v of GP .
W : the set of the set of colors {Wv : v ∈ VP }.
p:
A transparent path.
v:
A vertex of a path intersection graph.
B. An ILP formulation for the problem
The ILP formulation below determines the minimum number of regenerators to ensure that a translucent lightpath
may be set up from any source to any destination. This
formulation does not take into account existing lightpaths or
the pattern of requests for connections in a dynamic lightpath
allocation scenario. In our experiments we have shown that an
increase in the number of regenerators in a translucent network
decrease the blocking probability and the number determined
by the ILP represent the minimum number of regenerators
required in a translucent network. One important feature of the
translucent lightpath is that there may be unavoidable cycles
in the path used. For instance, in Figure 1, there is a cycle
C → D → F → G → B → C. The ILP below allows such
cycles in a translucent lightpath by setting up segments which
satisfy standard flow balance equations. Clearly, the path for a
segment (over which a transparent lightpath may be defined)
need not contain a cycle. Here it is necessary to specify an
upper limit on the value of s, the number of segments in the
path. This will be determined by the acceptable limits on the
delay and the Bit Error Rates (BER) [10].
Objective function:
minimize

s



k
δi · Xij

(1)

k=1 (i,j)∈E

subject to:
1) Satisfy the flow balance equations.

j:(S,j)∈E

1
XSj

= 1;

s




k=1 j:(j,S)∈E

k
XjS
=0

(2)

s




s


k
XjD
= 1;

k=1 j:(j,D)∈E



k
Xij
−

j:(i,j)∈E



k
XDj
= 0 (3)

k=1 j:(D,j)∈E



k
Xji
= 0 : δi = 0,
(4)
j:(j,i)∈E
∀k, 1 ≤ k ≤ s,
∀i, 1 ≤ i ≤ n

k+1
Xij
−

j:(i,j)∈E





k
Xji
= 0 : δi = 1,
(5)
j:(j,i)∈E
∀k, 1 ≤ k ≤ s,
∀i, 1 ≤ i ≤ n

2) The length of a segment cannot exceed the reach r.


k
Xij
· dij ≤ r,

∀k, 1 ≤ k ≤ s

(6)

j:(i,j)∈E

3) Each segment of the translucent lightpath must have
exactly one channel number assigned to it.
nch


Wpk = 1,

∀k, 1 ≤ k ≤ s

(7)

p=1

4) The channel number assigned to a segment must be
unused on each fiber used in the lightpath.
p
k
· Xij
+ Wpk ≤ 1,
wij

∀(i, j) ∈ E, ∀k, 1 ≤ k ≤ s (8)

5) If two segments share a fiber, they must be assigned
distinct channel numbers.
k1
k2
+Xij
+Wpk1 +Wpk2 ≤ 3, ∀(i, j) ∈ E,
Xij
∀k1, k2, 1 ≤ k1, k2 ≤ s,

∀p, 1 ≤ p ≤ nch

(9)

C. A heuristic for the problem
The objective of the heuristic outlined below is to establish,
if possible, a translucent lightpath, using a minimum number
of regenerators. The heuristic is not for use for designing
a network with a minimum number of regenerators but for
use once the network has been deployed with regenerators in
place. This heuristic has used a simple scheme of considering
a fixed number of routes when establishing a transparent
lightpath [12], [15] and has not considered how to avoid using
critical resources such as the last regenerator in a regenerating
node. Our primary objective was to show how to handle
the problem of overlapping segments discussed in Section
I. This heuristic is based on the “central agent” approach
[12] where an end-node of the network is designated as the
site where the heuristic will be executed. A request for a
communication from S to D has to be communicated from
S to the site where the heuristic will be executed and, if the
heuristic succeeds in establishing a route for the translucent
lightpath, messages have to be communicated to the routers
and the 3R regenerators in the path, followed by a time

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

lag sufficient to set up the routers and the 3R regenerators
before communication can start. Another approach which
could be used is to have a distributed heuristic which uses local
information to determine, if possible, the path for a translucent
lightpath. The performance of the heuristic below is based on
global information and therefore gives a lower bound on the
performance which is useful for calibrating any distributed
heuristic based on a similar approach. The site where the
heuristic is executed has access to a database containing
1) the set Pxy of m (or fewer, if all m paths cannot be
found) paths, for all node pairs (x, y) where the total
length of each path ≤ r.
2) the set of channels, cpq , currently available on each fiber
p → q in the network.
3) the distance dij between all pairs of end-nodes.
4) a list of all nodes capable of 3R regeneration.
Each path in Pxy is a potential candidate for setting up a
transparent lightpath from x to y. Given a path of length ≤ r,
say u → v → · · · → y → z, the values of cuv , · · · , cyz allow
us to determine the set of channel numbers that may be used
to set up a new transparent lightpath from u to z.
The heuristic uses A*, a well-known best-first search [16].
In the heuristic, each state in the state-space for the problem
consists of the triple (x, P , GP ) where x is a node in the
network, P is the set of transparent paths used by the search
to reach node x starting from node S and GP is the path
intersection graph corresponding to P . Here the cost of a
translucent lightpath is the number of regenerators needed in
the path used by the lightpath. Given a state (x, P , GP ), the
cost to reach node x from node S is |P | − 1. The heuristic
estimate of the cost to reach D from x is dxD /r. This is
clearly an admissible heuristic [16].
A state (x, P , GP ) is valid if it may be used to set up
a translucent lightpath from S to x. Let a vertex v ∈ GP
correspond to the path p = u → v → · · · → y → z.
The set of channel numbers that may be used to set up
a transparent lightpath from u to z, using path p, may be
viewed as the set of colors Wv to color vertex v of the path
intersection graph GP = (VP , EP ). If two nodes u, v ∈ VP
are adjacent in GP , it means that the paths corresponding
to u and v share one or more fiber (s). In this situation, to
satisfy the property given in Section I, transparent lightpaths
using the paths through the optical network, corresponding
to u and v, cannot be assigned the same channel number.
We have used a list coloring algorithm [17], with Wv as the
list of colors for vertex v to color graph GP . If the coloring
algorithm succeeds, every path in P may be used to set up a
transparent lightpath, using the channel number obtained using
the coloring algorithm, and the state (x, P , GP ) is valid. Only
essential points of the heuristic are included in Algorithm 1.
In Algorithm 2, we have described the function
createN ewStates(C, P, GP ). The remaining functions
in Algorithms 1 and 2 are informally described below.
removeBest(S): This function takes a set of states S and
returns the state X = (x, P, GP ), X ∈ S with the lowest
estimated value of the number of regenerators needed to reach

Algorithm 1 Dynamic lightpath allocation from S to D
1: S ← (S, {}, ({}, {}))
2: while S 	= {} do
3:
(x, P, GP ) ← removeBest(S)
4:
if x = D then
5:
return (P , GP )
6:
else
7:
if dxD ≤ r then

8:
C ← PxD pathsT oRegenerators(x)
9:
else
10:
C ← pathsT oRegenerators(x)
11:
end if
12:
N ← createN
ewStates(C, P, GP )

13:
S←S N
14:
end if
15: end while
Algorithm 2 createN ewStates(C, P, GP )
1: N ← {}
2: for each path p ∈ C do
3:
Gnew
← augmentGraphByP
ath(p, GP )

P
4:
P new ← P {p}
5:
for each node v ∈ VPnew do
6:
Wv ← assignListColors(v)
7:
end for
new
8:
if listColor(G
 P , W) then new new
9:
N ← N (lastN ode(p), P
, GP )
10:
end if
11: end for
12: return N

node D from node S. The function removes the state X from
S as well.
pathsT oRegenerators(x): This function takes a node x of
the network and returns the set of pre-computed paths from x
to all regenerators that are within the optical reach of x.
augmentGraphByP ath(p, GP ): This function takes a
path p in the network from some node x and the path
intersection graph GP = (VP , EP ) corresponding to the paths
used to go from S to x. The function returns a graph Gnew
P
by adding, to GP , a new vertex v corresponding to path p and
requisite new edges between v and the nodes in VP .
assignListColors(v): This function takes vertex v of the
path intersection graph GP = (VP , EP ) (which corresponds
to some path in the network with a total length ≤ r) and
returns the set of channel numbers that are not used on any of
the edges in the path. This set is used to define Wv that can
be used to color vertex v.
listColor(GP , W): This function takes a path intersection
graph GP = (VP , EP ) and, for each vertex v ∈ VP , a set of
colors Wv . The function returns true if it can successfully use
a list coloring algorithm to assign a color from Wv to vertex
v, ∀v ∈ VP .
lastN ode(p): This function takes a transparent path p
through the optical network and returns the last node in p.
IV. S IMULATION R ESULTS AND A NALYSIS
We conducted two sets of experiments to study the efficacy
of our heuristics on many realistic networks of varying sizes,

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

Call Blocking Probability

ARPANET
LATA’X’
USANET

0.25
0.2
0.15
0.1

Fig. 2.

2

5

7

10

Number of Paths Between Regenerators

0.5
0.4

Effect of varying the number of paths considered

namely, ARPANET(20 nodes, 32 links), LATA ‘X’(28, 47) and
USANET(53,68) [18]. The link distances in these networks
were uniformly randomly chosen between 1 and 1000 miles. In
the first set of experiments, the design parameters considered
were (i) number of precomputed paths between regenerators
(ii) varying traffic load (iii) number of regenerators in the
network and (iv) optical reach distance. We studied the effect
of these design parameters on the call blocking probability
for a dynamic call scenario. We assumed 10,000 calls to
arrive dynamically in the system. Each call i was a tuple
(si , ti , ai , di ) where si , ti are the source and destination of the
call respectively, ai is the arrival time and di is the duration
of the call. The call arrival process was assumed to follow
the Poisson distribution and exponential holding time was
assumed for the call durations. The traffic load in the network
was defined to be the ratio of the call arrival rate to the service
rate. The traffic load was varied by changing the arrival and the
service rates. In all these experiments, nodes with regeneration
capabilities were uniformly randomly selected.
Figure 2 shows the effect of the number of precomputed
paths maintained by the heuristic on the call blocking probability. In this experiment, the number of wavelengths available
on each link and traffic load were taken to be 8 and 50
erlangs respectively. For the different networks, it can be
observed from the graph that maximum improvement in terms
of call blocking probability can be achieved by maintaining 5
precomputed paths between every pair of regenerators. The
improvement in call blocking probability is not significant beyond 5 precomputed paths. Correspondingly, in the subsequent
experiments, we selected the number of precomputed paths to
be 5. the results indicate the tradeoff between the time taken
by the heuristic and the quality of the solution produced by the
heuristic. For all the networks, the heuristic completed within
≈5 seconds when number of paths was ≤5 and took several
(≤2) minutes when more than 5 paths were computed between
every pair of regenerators.
Figures 3 and 4 shows the effect of varying traffic load on
the call blocking probability for different number of wavelengths per link in the LATA ‘X’ and ARPANET network
respectively. Here the traffic load was varied until 50 erlangs
in steps of 5 erlangs. The call blocking probabilities were
measured for 4, 8, 12 and 16 wavelengths per link in the
networks. The value of r was set to be 1000 miles and
≈ 33% of nodes were randomly selected as regenerators. It

λ4
λ8
λ 12
λ 16

0.3
0.2
0.1
0
0

15

Fig. 3.

Call Blocking Probability

0.05
1

10

20

30

40

Traffic Load (in Erlangs)

50

Blocking probability v/s traffic load for LATA ‘X’

0.6
0.5
0.4

λ4
λ8
λ 12
λ 16

0.3
0.2
0.1
0
0

Fig. 4.

10

20

30

40

Traffic Load (in Erlangs)

50

Blocking probability v/s traffic load for ARPANET

can be observed that at higher loads, increasing the number of
wavelengths significantly reduces the call blocking probability.
For instance, it can be observed that at load 50, the call
blocking probability improves by 13-fold when number of
wavelengths are increased from 4 to 16 in ARPANET.
Figure 5 shows the effect of the number of regenerator
capable nodes in the ARPANET network on the call blocking
probability for traffic load = 50 erlangs and optical reach
distance = 1000 miles. An interesting observation that can
be made from this plot is that the increasing the number of
wavelengths per link in the network has a higher impact on
the call blocking probability than increasing the number of
regenerators in the network. For instance, doubling the number
of regenerators from 6 to 12 marginally reduces the call
blocking probability from 0.515 to 0.458, whereas doubling
the number of wavelengths from 4 to 8 reduces the call
blocking probability from 0.515 to 0.286 in ARPANET.
Figure 6 shows the effect of varying optical reach distances
on the call blocking probability for the ARPANET network
with traffic load = 50 erlangs and 6 regenerators. From the
figures, it can be observed that increasing the optical reach

Call Blocking Probability

Call Blocking Probability

0.3

0.5

0.3
0.2
0.1
0
4

Fig. 5.

λ=4
λ=8
λ = 12
λ = 16

0.4

6

8

Number of Regenerators

10

12

Blocking probability v/s number of regenerator for ARPANET

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

0.6

Call Blocking Probability

0.5
λ =4
λ =8
λ = 12
λ = 16

0.4
0.3
0.2
0.1
0
1000

1500

Optical Reach (in Miles)

1750

2000

Blocking probability v/s optical reach distance for ARPANET

Average # Regenerators

Fig. 6.

1250

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

3.9
3568.0

4.7

344.5

3.7 20.2

COST-small

ARPANET

Heurisc

LATA 'X'

Opmal

Fig. 7.
Average number of regenerators computed by the heuristic and
optimal solutions for different networks

has marginal benefit in terms of reducing the call blocking
probability. It is to be noted that the tradeoffs between the
different design parameters presented in this paper can be
effectively used by a network designer to select the right set
of additional resources to improve the network performance.
In the second set of experiments, the goal was to compare
the number of regenerators on the path produced by the
heuristic with that of the optimal solution found by solving
the Integer Linear Program (ILP). In these experiments, a
single common wavelength was assumed to be available on
each link of the network. The heuristic was used to compute
the end-to-end path for each node-pair in the network. The
experiments were conducted on 3 different networks - COSTSMALL2 , ARPANET and LATA ‘X’. The ILP was solved
using CPLEX-10 optimizer. The results of these experiments
are shown in Figure 7. The number on the bars indicates
the total execution time in seconds taken by the heuristic
and CPLEX to compute the end-to-end paths for all node
pairs. The number of regenerators for only those node pairs
are considered in the average computation for which the
heuristics found a solution. It is evident from the results that
the heuristics produce near-optimal solutions in a fraction of
time needed to find the optimal solutions, even for mediumsize networks. Also, for LATA ‘X’ network, the heuristic failed
to find a path even when such a path existed (as shown in the
optimal results) for a total 11 source-destination pairs out of
338 source-destination paths.
2 COST-SMALL network consists of 11 nodes and 22 edges. USANET
results are omitted since the ILP did not produce optimal solution even after
a significant amount of time.

V. C ONCLUSIONS
Existing algorithms for dynamic lightpath allocation have
not taken into account an important constraint when lightpath
segments overlap. We have given an example where this can
happen and shown that there may be cycles in the path
from a source to a destination, where the cycle includes a
regenerator. There are two repercussions of this - normal
network flow techniques do not allow cycles and results
using existing dynamic lightpath allocation may give invalid
solutions. We have shown how to develop an ILP formulation
by adapting single commodity network flow techniques where
this constraint has been taken into account. We have also
shown that an A* algorithm using an admissible heuristic can
be developed to handle medium or large size networks. The
constraint mentioned above can be handled satisfactorily in our
A* algorithm using list coloring algorithms. It is possible that
existing algorithms may adopt the same approach to take care
of the constraint mentioned here. The simulation experiments
conducted with several realistic network graphs demonstrate
the low call blocking probability even in the presence of high
traffic load in the network. In addition, the comparison with
the optimal solution obtained by solving the Integer Linear
Program showed that the heuristic performs well compared to
the ILP and that the heuristic runs in a fraction of the time
required to solve the ILP.
R EFERENCES
[1] J. Simmons, “On determining the optimal optical reach for a long-haul
network,” Journal of Lightwave Technology, vol. 23, March 2005.
[2] ——, “Network design in realistic all-optical backbone networks,” IEEE
Communications Magazine, vol. 44, 2006.
[3] G. Shen and R. S. Tucker, “Translucent optical networks: the way
forward,” IEEE Communications Magazine, vol. 45, 2007.
[4] G. Shen, W. D. Grover, T. H. Cheng, and S. K. Bose, “Sparse placement
of electronic switchoing nodes for low blocking in translucent optical
networks,” OSA Journal of Optical Networks, vol. 1, 2002.
[5] T. Carpentar, D. Shallcross, J. Gannet, J. Jackal, and A. V. Lehman,
“Method and system for design and routing in transparent optical
networks,” U. S. Patent Number 7286480 B2, October 2007.
[6] X. Yang and B. Ramamurthy, “Sparse regeneration in translucent
wavelength routed optical networks: architecture, network design and
wavelength routing,” Photonic network communications, vol. 10, 2005.
[7] N. Shinomiya, T. Hoshida, Y. Akiyama, H. Nakashima, and T. Terahara, “Hybrid link/path-based design for translucent photonic network
dimensioning,” Journal of Lightwave Technology, vol. 5, no. 10, pp.
2931–2941, October 2007.
[8] B. Zhou, S. R. Pramod, and H. T. Mouftah, “Adaptive ber-assured routing in translucent optical networks,” in Workshop on High Performance
Switching and Routing (HPSR 2004), 2004, pp. 209–213.
[9] B. Chatelain, S. Mannor, F. Gagnon, and D. V. Plant, “Non-cooperative
design of translucent networks,” in Global Telecommunications Conference (GLOBECOM ’07), 2007.
[10] X. Yang and B. Ramamurthy, “Dynamic routing in translucent wdm
optical networks: the interdomanin case,” Journal of Lightwave Technology, vol. 23, March 2005.
[11] ——, “Dynamic routing in translucent wdm optical networks: The
intradomain case,” Journal of Lightwave Technology, vol. 23, no. 3,
pp. 955–971, March 2005.
[12] S. Bandyopadhyay, Dissemination of Information in Optical Networks.
Springer, 2008.
[13] A. Sen, S. Murthy, and S. Bandyopadhyay, “On sparse placement of
regenerator nodes in translucent optical networks,” in Globecom, 2008.
[14] X. Yang and B. Ramamurthy, “Interdomain dynamic wavelength routing
in the next-generation translucent optical internet,” OSA Journal of
Optical Networking, 2004.
[15] H. Zang, J. P. Jue, and B. Mukherjee, “A review of routing and
wavelength assignment approaches for wavelength-routed optical WDM
networks,” SPIE Opt. Net. Mag., vol. 1, no. 1, January 2000.
[16] S. Russell and P. Norvig, Artificial Intelligence: A Modern Approach.
Prentice Hall, 1995.
[17] T. R. Jensen and B. Toft, Graph Coloring Problems. Wiley, 1995.
[18] S. Kim, X. Zhang, and S. Lumetta, “Rapid and efficient protection for
wdm mesh networks,” IEEE Journal of Selected Areas in Communication, 2007.

2016 International Conference on Computing, Networking and Communications, Social Computing and Semantic Data Mining

On Social Network Firewall Selection
Anisha Mazumder and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {anisha.mazumder,asen }@asu.edu

Abstract—It is common knowledge that the decision of an
individual regarding adoption of a product or technology is,
more often than not, heavily influenced by their friends and
acquaintances. In real world, there are different competing
products and innovations that try to garner as many loyal
followers as possible. Over the past few years, there has been
a significant interest in the research community to study social
network problems with a flavor of competition. Such problems
often focus on identification of a set of people in a given social
network by the competing players in order to achieve some goal.
In this paper, we introduce the weighted Segregating Vertex Set
(wSVS) problem, in which we are given a weighted undirected
graph with a subset of nodes identified as the seedset of the first
player and the goal for the second player is to identify a subset
of nodes (firewall) of minimum cumulative weight, such that the
total weight of the nodes reachable by the first player is strictly
less than the total weight of the nodes not reachable by the first
player. Thus, the second player tries to contain the reach of the
first player within the social network community. This problem
is also relevant for containment of disease in epidemiology,
containment of forest fire and several other domains. We prove
that this problem is NP-complete and provide an optimal solution
through the use of Mixed Integer Linear Programming. We also
provide a heuristic solution for the wSVS problem and show its
efficacy through detailed experimentation. Our heuristic solution
delivers near optimal solution in lesser time compared to that
needed to find the optimal solution.

I. I NTRODUCTION
Over the past few years, particularly with the boom of the
Online Social Networks (OSNs), there has been a considerable
amount of research interest in the study of problems pertaining to OSNs. These include - problems related to Influence
Maximization [1], [2], Influence Blocking Maximization[3],
[4], Community Detection [5] and others.
In particular, such problems in a two-player setting has
gained considerable interest in the recent past. The motivation
for such interest is two fold. First, one’s decision to adopt
a new technology or a product is often influenced by one’s
friends and family. Second, in the real world, there are always
more than one competing technology or product to choose
from and the product manufacturers are often promoting their
product so that the general population adopt their product
instead of their competitor’s. Each competing manufacturer
(or player) in the market would like to win over as many
loyal followers as possible but at the same time, she would
also like to try to “contain” the spread of the other player’s
product. Besides, each individual, represented as a node in
the social network, can be of different utility or value to the

978-1-4673-8579-4/16/$31.00 ©2016 IEEE

player and as such the player may assign a different weightage
to each individual, taking into account an individual’s age,
social status, educational status, economic status and various
other factors. Also, in order to win over a population, a
company needs to incentivize individuals to subscribe to their
products. Certainly, a company would like to spend as little of
its resources in incentivization that will achieve its objective,
which may be to capture a certain percentage of the market
share or other similar goals. We can also think about the
weightage of an individual from a company’s perspective as
the amount of incentivization needed by the individual to
become a loyal customer of the company.
Motivated by such considerations, in this research, we consider the problem of weighted Segregating Vertex Set problem
(wSVS). In this problem, we are given a weighted undirected
social network graph and we consider that there are two
players - A and B who are competing against each other.
Let us consider that player A has already selected a subset
of the population, which we refer to as the seedset of player
A. This means that player A has already won over the loyalty
of these people and as a result these people are unavailable to
the second player i.e., player B. Now, player B would like to
contain the possible spread of player A’s influence by selecting
a firewall of nodes from the network. This firewall will ensure
that the spread of player A’s influence is limited to only a
part of the network such that the total weight of the nodes
beyond the reach of player A is more that the total weight
of the nodes within the reach of player A. This will in turn
mean that even if player A is able to influence all the people
within the reach of her seedset, she would never be able to
conquer half or more than half the social network - this will
ensure that player B a reasonable chance to have a majority of
the market share in this social network. However, from player
B’s perspective, she would like to construct this firewall with
the least amount of investment for incentivization. Thus, the
problem has a definite flavor of the vertex separator problem
[6] that has been studied extensively in the Computer Science
literature . The wSVS problem is formally defined later.
It may be noted that the wSVS problem is similar to an
extent to the Influence Blocking Maximization (IBM) problem
[3]. But, wSVS is significantly different from the IBM problem
because in the IBM problem, it is not required that the
influence of the first player be contained within less than half
of the entire network. We can conceive many scenarios where
the wSVS problem formulation can be used as a model of a

2

real life situation. Consider two competing players, A and B,
such as two house builders or two companies manufacturing
some heavy products such as cars or expensive electronics. It is
clear that an individual having made an expensive investments
in such items at a certain point in time is extremely unlikely
to invest again in recent future. If player A starts marketing
while player B realizes that its product can only be ready in
about half a year or so, player B would like to stop customers
from buying from player A in the meantime. Such scenarios
are exactly captured by the wSVS problem formulation.
Even though the primary setting of the wSVS problem
is in the domain of social networks, it may be noted that
in abstraction, the problem can be easily transported to the
domains of damage control or epidemic control. Consider, for
example, that a part of a population has become infected by
a contagious disease. A primary prevention method by health
service officials could be to try to vaccinate individuals in
such a way that the vaccinated individuals form a firewall that
ensures that at least a majority of the population would be
safe-guarded against the epidemic. Similarly, in a distributed
data storage network, if a part of the network becomes
compromised, it may be prudent to build such a firewall so that
more than half of the network is protected. Although it might
appear that these scenarios are completely different from the
social network scenarios described earlier, in abstraction the
underlying problem is the same.
The rest of the paper is organized as follows - in section II,
we discuss related works for the wSVS problem; in section III,
we formally define the wSVS problem and provide a formal
proof of hardness of the wSVS problem; in section IV, we
provide an optimal solution to the wSVS problem using Mixed
Integer Linear Program formulation and a heuristic solution
to solve the wSVS problem in polynomial time. In section V,
we demonstrate the efficacy of our heuristic solution through
experimentation on three families of network namely BarabasiAlbert graph, Erdos-Renyi graph and Watts-Strogatz graph,
besides a real world Facebook dataset. Our heuristic computes
near optimal solution in lesser time compared to that needed by
the optimal solution. Finally, section VI concludes our paper.
II. R ELATED W ORKS
The research community in recent times has seen a heightened level of interest in social computing or social network
problems. In the Influence Maximization (IM) problem [1],
[7], given a network, a player wants to select the minimum
number of nodes that she must incentivize so as to obtain
the maximum number of loyalists in the network. The natural
generalization of this problem is to extend the setting to
a multi-player scenario [2], where there might be multiple
players trying to capture a given market. A number of different
models of propagation of influence through a social network
has been proposed - these include probabilistic influence
propagation [1] as well as deterministic influence propagation
[8]. [9] is a variation of the IM problem in a two player
setting where the first player has already selected a seedset
and the goal of the second player is to select a subset (of

minimum cardinality) of nodes from the remaining population
such that after the influence from the seedset of both the
players propagate, the expected number of nodes influenced
by second player is strictly greater than that by the first player.
Influence Blocking Maximization (IBM) is the problem
where the second player attempts to stall the influence propagation of the first player (under a budget constraint) to as
high an extent as possible through strategic selection of a
seedset that could initiate influence propagation of its own.
[3] proposes a solution technique for the IBM problem under
competitive linear threshold (CLT) model. [10], [11] are works
on variations of the IBM problem from a game theoretic
perspective.
Another line of research involves the propagation of negative influence, contagious diseases and so on. [12] studies the
problem of minimizing the influence of negative information.
In [13], given a graph where a node has been marked to be
the ‘source’, the goal is to obtain a cut minimizing the number
of nodes on the partition containing the source, such that the
capacity of the cut does not exceed a pre-determined budget.
Although all these studies delve into different aspects of
social network problems and there are numerous studies on the
Set Partition Problem [14],[15], to the best of our knowledge
none of them focus on problems related to the wSVS problem
where there is a strict constraint that the weighted sum of the
nodes reachable from the first player is to be restricted to less
than half of the total weighted sum of all the nodes in the
network. Here, by reachability, we generalize to any model
of influence propagation. This means that irrespective of the
model of propagation considered, the total weighted influence
of the first player must be restricted to less than half of the
total weight of the entire network.
III. P ROBLEM F ORMULATION
In this section, we provide a formal statement of the
weighted Segregating Vertex Set (wSVS) problem. The wSVS
problem is defined as follows:
Given a weighted undirected graph G and a subset R ⊂ V (G),
find a least weighted vertex separator C (C ⊂ V (G) \ R) such
that C divides the graph G into two components (say, P and
Q), where (i) R ⊆ P and (ii) weight(P ) < weight(Q) +
weight(C).
It may be noted that R in this formulation represents the
seedset of the first player (referred to as seedsetA ) in discussion earlier. P
For a subset of nodes V 0 ⊂ V (G), we define
0
weight(V ) = v∈V 0 wv , where wv is the weight of the node
v. For the decision version of the wSVS problem, the question
is as follows: Is there a vertex separator C of weight at most
B, such that
weight(P ) < weight(Q) + weight(C)....... (i)
P
P
We assume that v∈seedsetA wv < v∈V (G)\seedsetA wv i.e.,
if the second player selects all the remaining nodes after
selection of the nodes by the first player, constraint (i) will
certainly be satisfied.
Theorem III.1. The wSVS problem is NP-complete.

3

For each node v ∈ V (G):
Xv = 1, if node v belongs to P and 0 otherwise,
Yv = 1, if node v belongs to Q and 0 otherwise.
Let V (G) and E(G) denote the vertex set and edge set of G
respectively. The MILP can now be written as:
X
min
wv × (1 − Xv − Yv )
v∈V

Fig. 1.

Construction for hardness proof of wSVS problem

Proof: Given an instance of the wSVS problem and a
solution to the problem (i.e., the vertex separator C), it is
easy to verify in polynomial time whether C indeed provides
a feasible solution. Accordingly, wSVS is in NP. We prove
that the wSVS problem is NP-complete by reducing the Set
Partition Problem, a well-known NP complete problem, to it.
Set Partition Problem: An instance of the Set Partition problem
is made up of a set of integer numbers S, and it asks the
following question: Is there a partition of the set S into two
subsets A and Ā (Ā = S \A), such that the
P sum of the
Pintegers
in the two sets A and Ā are equal (i.e., x∈A x = x∈Ā x).
Given an instance of the set partition problem, we construct
an instance of the wSVS problem as shown in Fig. 1:
Let us enumerate the numbers in S as s1 , s2 , . . . , sn . For
each number si ∈ S, we create a node vi of weight
wvi = value of si . For e.g., if the enumeration of S is as
follows 5, 3, 6, . . ., then wv1 = 5, wv2 = 3, wv3 = 6, . . .. For
notational purpose, let us denote V = {v1 , v2 , . . . , vn }. We
add a single red node r (forming the seedset of the first player)
and add undirected edges (r, vi ), 1 ≤ i ≤ n. Also, we add two
disjoint
P nodes u1 and u2 , where wr = wu1 = wu2 = 1. Let,
T = i∈S i i.e., sum of all the elements of S. Let B = T2 .
We next provide both directions of the NP-hardness proof.
If case: If there is a partition of S into A and Ā, then C = A.
So, weight(C) = B = T2 and P = {r}∪ Ā and Q = {u1 , u2 }
and thus condition (i) is satisfied.
Only if case: If there is a yes answer for the wSVS
problem, let C be the corresponding vertex separator where
weight(C) ≤ T2 and condition (i) is satisfied.
Now, if weight(C) < T2 , then weight(V \ C) > T2 . Since,
all the weights are integers, we can say that
weight(V \ C) − weight(C) ≥ 1 = weight({u1 , u2 }) − wr
=⇒ weight(V \ C) + wr ≥ weight(C) + weight({u1 , u2 })
which violates condition (i). This implies that weight(C) =
T
2 = weight(V \ C) and a partition of S exists.
IV. S OLUTIONS FOR THE W SVS PROBLEM
In this section, we provide optimal and heuristic solutions
for the wSVS problem.
A. Optimal solution
We provide an optimal solution for the wSVS problem using
Mixed Integer Linear Program (MILP) formulation. Given a
graph G, we define the following variables:

Xu + Yv ≤ 1 ∀(u, v) ∈ E(G)

(1)

Xv + Yv ≤ 1 ∀v ∈ V (G)

(2)

Xv = 1 ∀v ∈ R
(3)
X
X
X
(wv × Xv ) <
(wv × Yv ) +
(wv × (1 − Xv − Yv ))
v

v

v

(4)

Xv ∈ {0, 1}; Yv ≥ 0;
The objective function implies that we want to minimize the
total weight of the nodes in the separator C (or equivalently
maximize the total weight of the nodes which are as assigned
to components P and Q). Constraint (1) implies that there
can be no edge between P and Q. Constraint (2) implies
that P ∩ Q = ∅. Constraint (3) implies that R ⊆ P or
equivalently
seedset
Finally, constraint (4) implies
P
P A ⊆ P. P
that v∈P wv < v∈Q wv + v∈C wv .
B. Heuristic Solution
Since, solving MILP can be NP hard, we provide a heuristic
solution by solving the Linear Program (LP) with relaxed integrality constraints of the MILP formulation given in section
IV-A and then using the output of the LP in order to obtain
the final firewall.
Algorithm 1: Heuristic algorithm for solving wSVS problem
1: Solve the relaxed linear program formulation for the
wSVS problem
2: Initialize C as the empty-set
3: while total weight of nodes reachable from seedsetA is
greater than or equal to total weight of nodes
unreachable from seedsetA do
4:
Add node v to C where v has the highest value of
1 − Xv − Yv among all v 0 ∈ V (G) \ C;
breaking ties randomly
5: end while
6: Return C
1) Description: The output of the relaxed LP formulation
for the MILP formulation given in section IV-A gives fractional values (referred to as lp values by us) to the nodes of
the input social network graph G. The MILP formulation very
evidently has no properties such as half integrality or totalunimodularity. So, our heuristic solution performs rounding
of the lp values for the nodes. Since, the heuristic will select
some nodes from V (G) \ seedsetA for the final output set

4

C, constraints 1 − 3 of the MILP formulation are automatically satisfied. Besides, we are explicitly checking whether
constraint (4) is satisfied in the condition of the while loop
of line 3 and so constraint (4) is also always satisfied. The
heuristic selects node in non-increasing order of the values
(1 − Xv − Yv ), ∀v ∈ V (G) assigned by the solution of the
relaxed LP formulation, breaking ties randomly. The intuition
behind this is that higher the value (1−Xv −Yv ) for a node v,
the greater fraction of the node v has been used by the linear
program solution in its final solution. Since, we can not use
a fraction of a node as the final solution of wSVS problem,
the heuristic includes the entire node in its solution set C. The
efficacy of this simple algorithm is proven empirically through
our experimentation provided in section V.
2) Time Complexity: Step 1 of solving the relaxed LP
formulation of the MILP formulation given in section IV-A
takes polynomial time. The condition for the while loop
of steps 3 − 5 can be computed through a graph traversal
algorithm such as depth-first search or breadth-first search
which takes O(|V (G)| + |E(G)|) time. We can sort and store
the 1 − Xv − Yv values as a pre-computation step for efficient
computation of step 4. A standard sorting algorithm on O(n)
nodes takes O(n log n) time. The while loop can be executed
for a maximum of V (G) − |seedsetA | which is O(n). Hence,
Algorithm 1 runs in polynomial time in |V (G)|.
V. E XPERIMENTAL R ESULTS AND D ISCUSSIONS
In this section we present results of our experimentations to
prove the effectiveness of our simple heuristic algorithm. For
this, on one hand, we consider three families of graphs namely
- Barabasi-Albert graph [16], Erdos-Renyi graph [17] and
Watts-Strogatz graph [18] on 100 nodes and different parameters relevant for the particular type of graph. And on the other
hand, we conisder an ego-Facebook real world dataset (freely
available for download from https://snap.stanford.edu/data/)
consisting of 4039 nodes and 88, 234 edges. For generating
data for the three graph families, we have used the NetworkX
python library and because these are random graphs, we have
experimented with 500 instances for each set of parameters.
Barabasi-Albert graphs are random scale-free networks generated using a preferential attachment algorithm. This means
that a graph of n nodes is constructed through the process of
attaching new nodes each with a specified number of edges
that are preferentially attached to existing nodes with high
degree. The reason we have chosen Barabasi-Albert graph
as one of the families of graphs for our experiment is that
such scale-free networks are frequently observed in different
social networks. For Barabasi-Albert graphs, in the context
of wSVS problem, the parameters that we have experimented
with are - (i) m i.e., the number of edges to attach from a
new node to existing nodes and it has been varied from 10
to 50 in steps of 10 as well as (ii) the size of seedset of
player A and it has been varied from 5 to 25 in steps of 5.
Erdos-Renyi graphs are a family of random graphs. We have
considered Erdos-Renyi graphs as baseline graph family. The
parameters of Erdos-Renyi graphs, in the context of wSVS

problem, that we have experimented with are - (i) p i.e., the
probability that each edge exists and it has been given values
of 0.1, 0.25, 0.5, 0.75 as well as (ii) the size of the seedset of
player A and it has been varied from 5 to 25 in steps of 5.
Finally, Watts-Strogatz graphs is a family of graphs with smallworld properties, which include high clustering properties and
short average path lengths. Such networks are also seen in
social networks. For the Watts-Strogatz graphs, the parameters
that we have experimented with in the context of the wSVS
problem are - (i) k i.e., each node is connected to k nearest
neighbors in ring topology and it has been given values of
4, 10, 16, (ii) p i.e., the probability of rewiring each edge and
it has been given values of 0.25, 0.5, 0.75, and (iii) size of the
seedset of player A and it has been varied from 5 to 25 in
steps of 5. For the Facebook dataset, we have used the data
as is and have considered the entire network graph with all
the nodes and all the edges from all the egonets combined.
For all the datasets, the degree of each node in the graph
is assigned as its weight. With these datasets in hand, we
have computed the optimal solution for the wSVS problem
by solving the MILP formulation as provided in section IV-A
by using CPLEX Optimization Studio 12.5. The heuristic
solution is implemented in Java and run for these datasets on a
Windows 7 Intel core i7 laptop. The results are plotted in Fig.
2, where for each graph, we plot along x-axis the percentage
of total number of nodes selected by the first player as her
seedset - for e.g., at x = 15, we plot the results when the
first player has selected 15% of the total number of nodes as
her seedset. And along y-axis, we plot the ratio of the weight
of the seedset selected by the heuristic solution to the weight
of the seedset selected by the optimal solution. In all of our
experiments, the heuristic has obtained a solution value within
a factor of 2 of the optimal solution value but in lesser time
compared to that required to compute the optimal solution.
VI. C ONCLUSION
We have introduced a new problem involving a two-player
game where, given a social network represented as a weighted
undirected graph, the goal is to select a subset of nodes
of minimum weight for the second player from the graph
excluding a subset of nodes marked as seedset of the first
player, such that the total weight of the nodes reachable by
the first player’s seedset is less than half the total weight of the
nodes of the graph. We prove that the problem is NP-complete
and provide optimal and heuristic solutions for the same.
R EFERENCES
[1] D. Kempe, J. Kleinberg, and É. Tardos, “Maximizing
the spread of influence through a social network,” in
Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining.
ACM, 2003, pp. 137–146.
[2] T. Carnes, C. Nagarajan, S. M. Wild, and A. Van Zuylen,
“Maximizing influence in a competitive social network:
a follower’s perspective,” in Proceedings of the ninth

5

(a) Results for Barabasi-Albert Network

(b) Results for Erdos-Renyi Network

(d) Results for Watts-Strogatz graph for k = 10 (e) Results for Watts-Strogatz graph for k = 16
Fig. 2.

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

(c) Results for Watts-Strogatz graph for k = 4

(f) Results for Facebook data

Experimental results for Barabasi-Albert Network, Erdos-Renyi Network, Watts-Strogatz Network and Facebook graph for different parameters

international conference on Electronic commerce. ACM,
2007, pp. 351–360.
X. He, G. Song, W. Chen, and Q. Jiang, “Influence
blocking maximization in social networks under the
competitive linear threshold model.” in SDM. SIAM,
2012, pp. 463–474.
G. Tuli, C. J. Kuhlman, M. V. Marathe, S. Ravi, and
D. J. Rosenkrantz, “Blocking complex contagions using
community structure,” 2012.
J. Leskovec, K. J. Lang, and M. Mahoney, “Empirical
comparison of algorithms for network community detection,” in Proceedings of the 19th international conference
on World wide web. ACM, 2010, pp. 631–640.
M. D. Biha and M.-J. Meurs, “An exact algorithm for
solving the vertex separator problem,” Journal of Global
Optimization, vol. 49, no. 3, pp. 425–434, 2011.
M. Kimura, K. Saito, and R. Nakano, “Extracting influential nodes for information diffusion on a social network,”
in AAAI, vol. 7, 2007, pp. 1371–1376.
P. Shakarian and D. Paulo, “Large social networks can
be targeted for viral marketing with small seed sets,”
in Proceedings of the 2012 International Conference
on Advances in Social Networks Analysis and Mining
(ASONAM 2012). IEEE Computer Society, 2012, pp.
1–8.
S. Shirazipourazad, B. Bogard, H. Vachhani, A. Sen, and
P. Horn, “Influence propagation in adversarial setting:
how to defeat competition with least amount of investment,” in Proceedings of the 21st ACM international
conference on Information and knowledge management.
ACM, 2012, pp. 585–594.
J. Tsai, T. H. Nguyen, and M. Tambe, “Security games
for controlling contagion.” in AAAI, 2012.

[11] M. Jain, D. Korzhyk, O. Vaněk, V. Conitzer,
M. Pěchouček, and M. Tambe, “A double oracle
algorithm for zero-sum security games on graphs,”
in The 10th International Conference on Autonomous
Agents and Multiagent Systems-Volume 1. International
Foundation for Autonomous Agents and Multiagent
Systems, 2011, pp. 327–334.
[12] S. Wang, X. Zhao, Y. Chen, Z. Li, K. Zhang, and J. Xia,
“Negative influence minimizing by blocking nodes in
social networks,” in Workshops at the Twenty-Seventh
AAAI Conference on Artificial Intelligence, 2013.
[13] A. Hayrapetyan, D. Kempe, M. Pál, and Z. Svitkina, “Unbalanced graph cuts,” in Algorithms–ESA 2005.
Springer, 2005, pp. 191–202.
[14] M. Conforti, M. Di Summa, F. Eisenbrand, and L. A.
Wolsey, “Network formulations of mixed-integer programs,” Mathematics of Operations Research, vol. 34,
no. 1, pp. 194–209, 2009.
[15] M. Padberg and G. Rinaldi, “A branch-and-cut algorithm
for the resolution of large-scale symmetric traveling
salesman problems,” SIAM review, vol. 33, no. 1, pp.
60–100, 1991.
[16] A.-L. Barabási and R. Albert, “Emergence of scaling in
random networks,” science, vol. 286, no. 5439, pp. 509–
512, 1999.
[17] P. Erd6s and A. Rényi, “On the evolution of random
graphs,” Publ. Math. Inst. Hungar. Acad. Sci, vol. 5, pp.
17–61, 1960.
[18] D. J. Watts and S. H. Strogatz, “Collective dynamics
of small-worldnetworks,” nature, vol. 393, no. 6684, pp.
440–442, 1998.

Energy Efficient Application Mapping to NoC Processing Elements
Operating at Multiple Voltage Levels
Pavel Ghosh and Arunabha Sen
Department of Computer Science and Engineering
Arizona State University, Tempe, AZ
Email: {pavel.ghosh, asen}@asu.edu

Alexander Hall
Department of EECS
UC Berkeley, CA, USA
Email: alex.hall@gmail.com

Abstract

mization subject to performance constraint has become one
of the most important objectives.

An efficient technique for mapping application tasks to
heterogeneous processing elements (PEs) on a Network-onChip (NoC) platform, operating at multiple voltage levels,
is presented in this paper. The goal of the mapping is to
minimize energy consumption subject to the performance
constraints. Such a mapping involves solving several subproblems. Most of the research effort in this area often address these subproblems in a sequential fashion or a subset
of them. We take a unified approach to the problem without
compromising the solution time and provide techniques for
optimal and heuristic solutions. We prove that the voltage
assignment component of the problem itself is NP-hard and
is inapproximable within any constant factor. Our optimal
solution utilizes a Mixed Integer Linear Program (MILP)
formulation of the problem. The heuristic utilizes MILP relaxation and randomized rounding. Experimental results
based on E3S benchmark applications and a few real applications show that our heuristic produces near-optimal solution in a fraction of time needed to find the optimal.

The problem of minimizing energy consumption during
application execution while satisfying the performance constraints can be divided into four main subproblems: (i) mapping of the application tasks to the PEs, (ii) mapping of the
PEs to the routers of the NoC architecture, (iii) assigning
operating voltages to the PEs (in case they can operate at
multiple voltages) and (iv) routing of data paths, i.e., traffic
movement on the NoC architecture. As consideration of all
four subproblems simultaneously increases the complexity
of the problem, most of the research effort in this domain
[4, 6, 8, 9] either solve problems (i), (ii), (iii) and (iv) in a
sequential fashion, or solve only a subset of them.

1

To find an energy efficient application mapping, all four
problems (i)-(iv) have to be solved. There are two options
available - solve them sequentially or solve them in a unified
way. The sequential approach has manifold disadvantages.
Firstly, decision taken at an early phase may turn out to be
expensive later. Secondly, because of some earlier decisions
may lead to violation of constraints at some later phase, and
thus resulting in re-execution of all the steps multiple times
involving an enormous amount of computation.

Introduction

We show with a motivating example here and later with
extensive experimental results that the sequential approach
may lead to sub-optimal solution. To the best of our knowledge, our proposed technique is the first that unifies all the
four subproblems under a single problem formulation and
develops optimal and heuristic solutions for the problem.
Although scaling down voltage levels of PEs is favorable
for reduction of energy consumption, excessive number of
voltage islands may be detrimental from the perspective of
physical design as it creates voltage island fragmentation of
the chip and increases the complexity of the power delivery network. Therefore, the number of voltage islands on
the chip should follow an upper bound. In literature [7, 8],
the constraint on the maximum number of voltage either has
not been captured properly or involves a huge computation

In recent years System-on-Chip (SoC) design has become extremely challenging due to the increasing complexities in processor and semiconductor technologies. Multicore SoC based embedded systems may contain either all
homogeneous generic processing cores, or a varying number of heterogeneous PEs. These heterogeneous PEs may
represent programmable general purpose cores, task specific co-processors or hardware accelerators, etc. Networkon-Chip (NoC) architectures provide an alternative to the
bus-based communication mechanism that can meet the
challenging requirements of performance, scalability and
flexibility [1, 2]. As the number of PEs on a SoC and the
data traffic between them continues to grow, energy mini1

978-1-4244-4143-3/09/$25.00 ©2009 IEEE

(a) Application Task Graph

(b) Mapping using 2-step approach

(c) Mapping using 1-step approach

Figure 1. Task Graph and Mappings
time for the solution. We incorporate this constraint by an
efficient formulation of the upper bound on the number of
voltage islands created.
The motivation behind our unified approach comes from
the following example. Fig. 1(a) shows an example application task graph consisting of four tasks T 0, T 1, T 2 and T 3.
The edges represent the task dependencies and the labels on
the edges represent the inter-task communication volume in
number of bytes. Tables 1 and 2 show the execution time
and power consumption of these four tasks on four available PEs P 0, P 1, P 2 and P 3, respectively.
Following a sequential approach, the resultant mapping
is as shown in Fig. 1(b), with computation energy consumption 3.8124µJ and communication energy consumption 0.952µJ. Thus, the overall energy consumption for
the application is (3.8124 + 0.952) = 4.7644µJ. With
such a mapping, the execution finish-time of the application is 86.104µs, well within the specified deadline of
122µs. For our proposed unified mapping, as shown in
Fig. 1(c), the total energy consumption is 3.8734µJ =
3.8575µJ + 0.0159µJ, leading to 18.7% save of energy as
compared with the sequential approach. With this mapping,
the execution finish-time of the application is 84.852µs, still
within deadline of 122µs.
Energy consumption can be further reduced if we take
advantage of operating the PEs of the NoC at multiple voltage levels. When the PEs are allowed to operate at different
voltages, a certain amount of energy will be consumed by
Table 1. Execution time (in seconds) for taskprocessor pair
Tasks

P0

P1

P2

P3

T0
T1
T2
T3

7.7e-06
4.1e-06
7.2e-06
5.47e-05

7e-06
5.2e-06
4.2e-06
4.15e-05

6.9e-06
4.9e-06
5.95e-06
5.95e-05

5.8e-06
4e-06
4.4e-06
7.2e-05

Table 2. Task Power (in Watts) for taskprocessor pair
Tasks

P0

P1

P2

P3

T0
T1
T2
T3

0.16
0.07
0.102
0.048

0.14
0.045
0.25
0.084

0.12
0.065
0.131
0.041

0.15
0.077
0.21
0.028

the generation of additional clock signals, voltage level converters and mixed clock-mixed voltage FIFOs used by the
level shifters connecting adjacent PEs in NoC architecture.
From the example above, it is clear that a unified approach to application mapping to NoC PEs, operating at
multiple voltages leads to better energy utilization than a
sequential approach. The energy consumption model considered in this paper has three components - (i) energy consumption due to computation, (ii) energy consumption due
to communication and (iii) voltage transition energy between adjacent PEs operating at different voltages.
In this paper, we have assumed a regular mesh architecture as the communication infrastructure, where each router
has 5 ports. One of the ports is used for connecting it to
a PE and the other four are for connection to the neighboring routers. The algorithms proposed in this paper can
be used with any other NoC topologies as well. Communication power consumption parameter values used for the
evaluation of our techniques are taken from [9].

2

Problem Formulation

In this section, we provide formal definition of the application mapping problem. The input instance to this problem is explained in Table 3. The output of the problem is as
shown in Table 4. The objective is to minimize the overall
energy consumption, such that:
1. All the application tasks finish their execution before
deadline D.
2. Dependencies among the tasks are maintained.
3. Bandwidth constraint on each router link is satisfied.
4. The total number of voltage islands created does not exceed κ.
5. Total energy consumption (computation + communication + voltage transition) is minimized.

3

Computational Complexity

In this section we define the voltage assignment problem, subproblem (iii) of the application mapping problem
and prove it to be NP-complete and inapproximable within
any constant factor. If M1 (tj ) = pi , then for each pi
we can find out an allowable set of voltage levels Li =

Table 3. Input of the Problem
Symbols

Explanation

G(VT , ET )
wij
D
κ
P
Pt ⊆ P
Vp

Set of tasks VT = {t1 , t2 , . . . , tm }; directed edge eij = (ti , tj ) ∈ ET representing the dependency of task tj on task ti
For edge eij ∈ ET , the data communication volume (in Mbps) between tasks ti and tj
Application deadline, by which all the tasks need to be completed
Maximum number of voltage islands created
A set of processing elements {p1 , p2 , . . . , pk }
For each task t ∈ VT , a subset of PEs, potential to execute
the task t o
n
For each PE p ∈ P , an allowable set of voltage levels v1 , v2 , . . . , vnp in which this PE can operate, where np denotes number of distinct

τ (t, p, vp )
 (t, p, vp )

voltage levels it can operate
Execution time of task t ∈ VT on PE p ∈ Pt , when p is operating at voltage vp ∈ Vp
Computation energy consumption of task t ∈ VT on PE p ∈ Pt , when p is operating at voltage vp ∈ Vp

β
λ
Lij

An undirected n × n mesh architecture graph, where each node r ∈ VR denotes a router in the NoC architecture, whereas each edge
eij = (ri , rj ) represents a router link in the NoC architecture between those two router nodes
Capacity of each link eij ∈ ER
Data communication latency on each link eij ∈ ER
Link length of each link eij ∈ ER

ψi
ψo
ψl
αvk vl

Communication energy consumption rate at the router input port = 328nW/M bps[9]
Communication energy consumption rate at the router output port = 65.5nW/M bps[9]
Communication energy consumption rate at the router links = 79.6nW/M bps/mm[9]
Power consumption by the level shifter when two adjacent PEs are operating at voltages vk and vl , respectively

GR (VR , ER )

Table 4. Output of the Problem
Output

Explanation

Task to PE Mapping function M1 :

M1 (ti ) = pj , which means that task ti has been mapped to PE pj ; ti ∈ VT , and pj ∈ Pti ⊆ P

PE to Router Mapping Function M2 :
PE to Voltage Assignment Function M3 :

M2 (pi ) = rj , which means that PE pi has been mapped to NoC router rj ; pi ∈ P and rj ∈ VR
M3 (pi ) = vj , which means that PE pi has been assigned to voltage vj ; pi ∈ P and vj ∈ Vpi

Task Edge to Path in Mesh Mapping Function
M4 :

If eix ∈ ET , M1 (ti ) = pj , M2 (pj ) = rk and˘M1 (tx ) =
)=
y , M2 (py¯
i , tx ) = rk →
¯ p˘
˘ rz , then M4 (t¯
rπ(1) → rπ(2) → . . . → rπ(l) → rz , where r1 , rπ(1) , rπ(l) , rz , rπ(q) , rπ(q+1) ∈ ER , ∀q ∈
[1, l − 1] and π(q) 6= k, z, ∀q ∈ [1, l − 1]

{vi1 , vi2 , . . . , vini }, such that the task tj can meet its deadline dj (obtained using deadline D and communication latencies) whenever pi is operating at the voltage levels in Li .
We consider two components for energy consumption:
(1) Energy consumption by PE pi when operating at voltage
v ∈ Li . We denote this component by ηpvi .
(2) Energy consumption by the level shifter connecting two
adjacent PEs pi and pj , where pi is operating at voltage x
and pj is operating at voltage y, denoted by αxy .
Voltage Assignment Problem: Given an n × n = N grid,
where each node represents a router in the NoC architecture
with a PE attached to it and a list Li of allowable operating
voltages associated with each PE pi , the problem is to assign a voltage vj to PE pi , where vj ∈ Li for all the PEs,
such that the following energy consumption expression is
minimized:
X
X
ηpvi +
αxy
i=1 to N,v=M3 (pi )

x=M3 (pi ),y=M3 (pj ),eij ∈ER

The voltage assignment problem has one-to-one correspondence with the minimum weight grid coloring problem, defined as follows:
Minimum Weight Grid Coloring Problem: Given a grid
graph G = (V, E) of dimension a × b = M . Each node
u ∈ V has an associated set of colors Cu = {1, . . . , Ku }
each with a certain color-cost c (k) ∈ <+ , k ∈ Cu . Let
u, w ∈ V be neighbors in the grid-graph, i.e., {u, w} ∈ E.
We are also given combination-costs c (k, l) ∈ <+ for each

color combination k ∈ Cu and l ∈ Cw . The goal of the
minimum weight grid coloring problem is to find a coloring,
i.e., for each node u ∈ V a color k ∈ Cu , which minimizes
the following objective function:
X
X
c (ku ) +
c (ku , kw )
v∈V

{u,w}∈E

We now prove the decision version of this problem,
called MINCOL, to be NP-hard by reducing an instance of
the known NP-hard problem GRAPH 3-COLORABILITY
with no vertex degree exceeding 4 [5]-(page 85) to an
instance of MINCOL.
Theorem 1. Min weight grid coloring is NP-hard and even
inapproximable within any factor.
Proof. Let G0 = (V 0 , E 0 ) denote the given planar graph.
From this graph we construct an instance of the min weight
grid coloring problem as follows.
We embed G0 in a grid graph G = (V, E) of dimension
O (n0 ) × O (n0 ), where n0 is the number of nodes in G0 ,
using a polynomial algorithm for computing an orthogonal
representation. Let W ⊆ V denote the subset of nodes
in G which corresponds to the embedded node set V 0 of
the given planar graph. For each edge e0 = {u0 , w0 } ∈
E 0 we denote by pe0 = u1 , e1 , u2 , . . . , ej−1 , uS
j the path
corresponding to the embedding of e0 . P = e0 ∈E 0 pe0
denotes the set of all such paths. Fig. 2 gives an example of
how such an embedding could look like for a simple graph

Table 5. Varibales used in MILP Formulation
Variable

(a)
Example
planar graph

(b) Embedding into a grid
graph of dimension 6 × 6

Figure 2. Planar graph and its embedding
with four nodes. With each node u ∈ V of G we associate
3 colors Cu = {1, 2, 3}. We set all color-costs c (k) = 0
for k ∈ Cu and u ∈ V . Similarly, all combination-costs
of edges {u, w} ∈ E contained in none of the paths, i.e.,
{u, w} ∈
/ P , are set to zero as well: c (k, l) = 0 for k ∈ Cu
and l ∈ Cw . For each path p ∈ P we set the combinationcosts as follows:
i) For e1 = {u1 , u2 } we set c (k, l) = 0 for k ∈ Cu1 , l ∈
Cu2 and k 6= l. For remaining combinations with k = l we
set c (k, l) = 0. In other words, if u1 and u2 are assigned
different colors, we have cost 0, otherwise 1.
ii) For ei with i ∈ {2, . . . , j − 1} we set c (k, l) = 1 for k ∈
Cui , l ∈ Cui+1 and k 6= l. For the remaining combinations
with k = l we set c (k, l) = 0. In other words, if ui and
ui+1 are assigned the same color, we have cost 0, otherwise
cost 1.
If we aim for a total cost of 0, the path p will propagate
the color chosen for uj all the way to u2 . For the cost to remain at zero, u1 and u2 (and therefore uj ) must be colored
with different colors. Therefore, the given planar graph G0
is 3-colorable, if and only if there is a solution to the constructed min weight grid coloring problem of total cost zero.
This proves the problem to be NP-hard. The inapproximability within any factor follows, since any approximation
algorithm with multiplicative approximation ratio ρ and additive factor ρ0 could be used to decide whether G0 is 3colorable as well: simply multiply all combination costs by
ρ0 + 1. If G0 is 3-colorable, the optimal solution has cost 0
and therefore the approximation algorithm must find a solution with cost ≤ ρ · 0 + ρ0 = ρ0 . Otherwise, if G0 is not
3-colorable, any solution has cost ≥ ρ0 + 1. Hence, the approximation algorithm distinguish between two cases.

4

Optimal Solution for Application Mapping

In this section we use the mathematical programming
techniques to solve the application mapping problem. We
formulate the problem as a Mixed Integer Linear Program
(MILP). In Table 5 we define a few of the variables used
in the MILP formulation and also declare their types. The
parameters used for calculation of energy consumption and
execution time are as follows:
τtpv = execution time of task t ∈ VT on PE p ∈ Pt at voltage level v ∈ Vp (in sec)
tpv = computation energy consumption for task t ∈ VT on

Type

Definition

v
δtp

Binary

ζrv

Binary

1, if t ∈ VT , p ∈ Pt ⊆ P, v ∈ Vp , and
M1 (t) = p and M3 (p) = v; 0, otherwise
1, if r ∈ VR operating at voltage v, i.e.,
M2 (p) = r and M3 (p) = v for some p ∈ P ; 0,
otherwise

ij
fxy

Binary

1, if eij ∈ ET , exy ∈ ER , and exy ∈ M4 (eij );
0, otherwise

PE p ∈ Pt at voltage level v ∈ Vp (in Joules)
αv1 v2 = voltage transition energy consumption parameter
for two adjacent nodes operating at voltage levels v1 and
v2 , respectively (in Joules)
The objective of the application mapping problem is to
minimize the energy consumption of the system subject to
the application deadline constraint, mesh interconnection
link bandwidth constraint and maximum allowed number
of voltage islands constraint, i.e.,
Obj: minimize E = Ec + Er + El + Evt
where Ec is the computation energy consumption, Er and
El are the communication energy consumed at the router
ports and links, respectively and Evt is the voltage transition
energy consumption, and calculated as:
X X X

v
Ec =
δtp
tpv
t∈VT p∈Pt v∈Vp

X

Er =

X


ij
fxy
wij (ψi + ψo )

∀exy ∈ER ∀eij ∈ET

El =

X

X



ij
fxy
wij Lxy ψl

∀exy ∈ER ∀eij ∈ET

Evt =

X X

X

(ζxv1 ζyv2 ) αv1 v2

v1 ∈V v2 ∈V ∀exy ∈ER

In order to eliminate the quadratic term in the expression of
Evt , we define the following decision variable:
v1 v2
θxy
= 1, if ζxv1 = 1 and ζyv2 = 1, otherwise 0
X X X
v1 v2
Hence, Evt =
θxy
αv1 v2
v1 ∈V v2 ∈V ∀exy ∈ER

Constraints:
Due to the limitations in space, here we omit the mathematical details of the formulations of the constraints.
(1) Each task executes on exactly one PE, at exactly one
voltage, connected to exactly one router node.
(2) Each router operates at exactly one voltage level and is
attached with at most one processor.
(3) Each task needs to finish within the specified application
deadline, and need to maintain task dependencies.
(4) Bandwidth constraint on each router link: the total flow
passing through a link does not exceed its bandwidth.
(5) The number of voltage islands is less than or equal to
the specified maximum allowed value κ.

5

!5

Heuristic for Application Mapping

3

4(3.

0.08

Algorithm 1 Randomized Rounding based Heuristic
1: Relax all the integer constraints in the MILP (/* relaxation */)
2: Solve the relaxed LP using CPLEX
3: while (Solution NOT Integral) do
4:
Set the integral variables in LP solution as constants and leave them unaltered during further iterations (/* variable fixing */)
for (each task t ∈ VT ) (/* rounding */) do
repeat
v
Round δtp
to 1 with probability as its value
v
until (δtp
= 1 for some p and v)
repeat
Round γtr to 1 with probability as its value
until (γtr = 1 for some r)
Round ζrv variables following co-relation constraints
Round σrp variables following co-relation constraints
end for
Solve modified LP
if (NO Constraint Violation) then
if (Integer Solution) then
Solution Found - break out of outer while loop
end if
else
v
Eliminate constraint violations by modifying the rounding of δtp
and
γtr variables for the corresponding t ∈ T
22:
end if
23: end while
24: return Solution in terms of functions M1 , M2 , M3 and M4

5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

6

Experimental Results

In this section we present the experimental results to
evaluate our proposed approach. All our experiments can
be classified into the following three categories considering
the evaluation goal of the experiment:
i) The optimal solution for variable voltage setup is compared with the optimal solution for fixed voltage setup.
ii) The optimal solution of our proposed unified approach is
compared with that of the sequential approach.
iii) The quality of the heuristic solution is evaluated by comparing it with the optimal solution.
The experiments are performed using applications (autoindustry, consumer, networking and office-automation)
from the E3S benchmark suite [3] and three real applications MPEG4, MWD (Multi-Window Display) and OPD
(Object Plane Decoder). The following six voltage setups
were used, the first one as variable voltage (VV) setup and
the other five as fixed voltage (FV) setups: a) Case I : Voltage level varies in the range from V0 to V4 , b) Case II : Voltage level is fixed at V0 = 1.9V , c) Case III : Voltage level
is fixed at V1 = 2.3V , d) Case IV : Voltage level is fixed at
V2 = 2.5V , e) Case V : Voltage level is fixed at V3 = 3.3V ,
f) Case VI : Voltage level is fixed at V4 = 3.6V .

0.06

Joules

./1
./,

0.04

0.02

./0
.

!"#$%&'())

0

!"#(*)(+&#(),-

(a) Auto-Industry

Optimal VV

Opt FV (at V4)

(b) Consumer

!.

2

3(4/

0.012

1

Joules

0.008

567'89

In this section we describe our proposed MILP relaxation and randomized rounding based heuristic. The algorithm takes as input the parameters specified in the Section 2
and the corresponding MILP formulation. Output of the algorithm consists of the mappings M1 , M2 , M3 and M4 ,
defined in Table 4. The heuristic is explained below.

678'9:

./2

.

0.004
0

/

!"#$%&'())

!"#(*)(+&#(),-

!"#(*)(+&#().-

(c) Networking

0

Optimal VV

Opt FV (at V3)

Opt FV (at V4)

(d) Office-Automation

Figure 3. Optimal with variable voltage levels
vs. Optimal with fixed voltage Energy Consumption Comparison for E3S Benchmark
For low voltage setups, no feasible solution was found.
On the other hand, for high voltage setups feasible solutions
were found with higher energy consumption values. The
heuristic is implemented in C++. The MILP for achieving
the optimal solution and the corresponding relaxed LP as
part of the heuristic were executed on the same machine
using ILOG CPLEX 10.0 Concert technology.
Experiments with E3S Benchmark Applications: Mesh
dimension 4 × 4 was used for the auto-industry application
and 3 × 3 for all the other three applications. The value
for the maximum number of voltage islands κ was set to
4 and 3, respectively, for these two different sizes of mesh
topologies. Fig. 3 compares the optimal solution while using the variable voltage (VV) levels with the optimal solution while setting a fixed voltage (FV) level, i.e., optimal at
Case I with the optimals for all other cases. On average for
these four applications, the variable voltage setup can save
18% energy consumption over the fixed voltage setup.We
compare the solution quality of our proposed unified approach with the sequential approach in Fig. 4. For these
four applications, on average, we are able to save 16% energy consumption. Fig. 5 shows the quality of our proposed
heuristic as compared with the optimal solution. i.e., the
optimal and heuristic solution for Case I. In all the cases,
the heuristic is able to find near-optimal solutions. Fig. 6
shows that the time taken to obtain the heuristic solution is
negligible compared to that required to solve the MILP for
the optimal solution.
Experiments with Real Applications: Mesh dimension
3×3 for MPEG4 and MWD applications and 4×4 was used
for the OPD application. The value of maximum number of
voltage islands κ was set to 3 and 4, respectively, for these
two different sizes of mesh. Fig. 7 compares the optimal
solution while using the variable voltage (VV) levels with
the optimal solution while setting a fixed voltage (FV) level,
i.e., optimal at Case I with the optimals for all other cases.

!6

!/

3 453-

1 234-

−3

1

0.012

.

0.004

0.04

x 10

0.8

-.0
-./

0.04

Joules

0.008

0.06

Joules

/

0.06

Joules

-.1

Joules

0
56$)"7

78$)"9

-.2

0.02

0.6
0.4

0.02
0.2

-

!"#$"%&'()

-

*%'+'",

(a) Auto-Industry

!"#$"%&'()

0

*%'+'",

(b) Networking

Sequential

0

Unified

(c) Office-Automation

Figure 4. Comparison of the Energy Consumption values using Our Proposed Unified
Approach and the Sequential Approach
!8

6

7(60

0.08

0.06

014

Joules

9:,'+.

015

013

0.04

0.02

012
0

!"#$%&'())

0

*+,-$.#$/())

(a) Auto-Industry

Optimal VV

Heuristic VV

(b) Consumer

5(60

0.012

Heuristic VV

(a) MPEG4

0

Optimal VV

Heuristic VV

(b) MWD

0

Optimal VV

Heuristic VV

(c) OPD

Figure 8. Variable Voltage Optimal vs. Heuristic Energy Comparison for Real Applications
shows that the optimal solution with flexible voltage levels is more energy-efficient than the optimal at some fixed
voltage level. Moreover, it shows that optimal solutions can
be achieved using the unified approach as opposed to the
sub-optimal solution obtained by the sequential approach.

7

!2

4

Optimal VV

Conclusion

3

Joules

78,'+.

0.008

2

0.004

1

0

!"#$%&'())

0

*+,-$.#$/())

(c) Networking

Optimal VV

Heuristic VV

(d) Office-Automation

Figure 5. Variable Voltage Optimal vs. Heuristic Energy Consumption Comparison for E3S
Benchmark Applications

In this paper, we have proposed a unified approach to
solve the application mapping problem on a heterogeneous
NoC platform for energy minimization. The voltage assignment problem is proven to be NP-hard. Our solution techniques are evaluated using benchmark suite E3S [3] and
three real applications. Experimental results demonstrate
effectiveness of our heuristic, superiority of the unified approach over sequential approach and advantage of operating
the PEs at multiple voltage levels for energy minimization.

milli seconds (in log scale)

10

10

Optimal
Heuristic

References

5

10

0

10

auto_industry

consumer

networking office_automation

0.06

0.04

−3

1

x 10

0.8

Joules

0.08

0.06

Joules

Joules

Figure 6. Exec. Time Comparison (log scale)
0.08

0.04

0.6
0.4

0.02

0.02

0.2
0

Optimal VV

Opt FV(at V3)

Opt FV(at V4)

(a) MPEG4

0

Optimal VV

Opt FV(at V3)

(b) MWD

Opt FV(at V4)

0

Optimal VV

Opt FV(at V3)

Opt FV(at V4)

(c) OPD

Figure 7. Optimal with variable voltage levels
vs. Optimal with fixed voltage Energy Consumption Comparison for Real Applications
On average for these three applications, the variable voltage
setup can save 11% energy consumption over the fixed voltage setup. Fig. 8 shows the quality of our proposed heuristic
as compared with the optimal solution. i.e., the optimal and
heuristic solution for Case I. In all the cases, the heuristic is
able to find near-optimal solutions within seconds.
Thus, the experimental results support our claim of
achieving near-optimal solutions from the heuristic. It also

[1] W. Dally and B. Towles. Route Packets, Not Wires: On-Chip
Interconnection Networks. In Proc. Design Automation Conf.,
pages 684–689, Las Vegas, Nevada, USA, June 2001.
[2] G. De-Micheli and L. Benini. Networks On Chips. Morgan
Kaufmann, 2006.
[3] R. Dick.
Embedded System Synthesis Benchmarks
Suite(E3S).
[4] M. A. A. Faruque, R. Krist, and J. Henkel. ADAM: Runtime Agent-based Distributed Application Mapping for onchip Communication. In Proceedings of DAC, Anaheim, California, USA, June 8-13 2008.
[5] M. R. Garey and D. S. Johnson. Computers and Intractability:
A Guide to the Theory of NP-Completeness. W. H. Freeman,
1979.
[6] A. Hansson, K. Goossens, and A. Rǎdulescu. A Unified
Approach to Mapping and Routing on Network-on-Chip for
Both Best-Effort and Guaranteed Service Traffic. Networks,
2007, 2007.
[7] W. K. Mak and J. W. Chen. Voltage Island Generation under Performance Requirement for SoC Designs. In ASPDAC,
pages 798–803, 2007.
[8] U. Y. Ogras, R. Marculescu, P. Choudhary, and D. Marculescu. Voltage-Frequency Island Partitioning for GALSbased Networks-on-Chip. In Proc. Design Automation Conf.,
San Diego, California, USA, June 2007.
[9] K. Srinivasan, K. S. Chatha, and G. Konjevod. LinearProgramming-Based Techniques for Synthesis of Networkon-Chip Architectures. IEEE Trans. on VLSI Systems,
14(4):407–420, April 2006.

Optical Switching and Networking 23 (2017) 85–96

Contents lists available at ScienceDirect

Optical Switching and Networking
journal homepage: www.elsevier.com/locate/osn

Software-deﬁned adaptive survivability for elastic optical networks
Michał Aibin a,n, Krzysztof Walkowiak a, Arunabha Sen b
a
b

Department of Systems and Computer Networks, Faculty of Electronics, Wroclaw University of Science and Technology, Wroclaw, Poland
School of Computing, Informatics and Decision Systems Engineering, Arizona State University, USA

art ic l e i nf o

a b s t r a c t

Article history:
Received 31 January 2016
Received in revised form
19 May 2016
Accepted 21 June 2016
Available online 21 June 2016

Cloud computing is dominating Internet services and would continue to expand in the foreseeable future. It is very challenging for network operators to evolve their infrastructures to be more intelligent and
agile in resource orchestration. Nowadays, the optical networks term denotes high-capacity telecommunications networks based on optical technologies and components that can provide capacity,
provisioning, routing, grooming, or restoration at the wavelength level. The proposed Elastic Optical
Networks (EONs) technology is expected to mitigate this problem by adaptively allocating spectral resources according to client trafﬁc demands. In this paper, we focus on survivability problems in dynamic
routing in EONs. We propose Adaptive Survivability (AS) approach to achieve the best trade-off between
the efﬁciency of path protection and cost of routing. Moreover, we propose entirely new Routing,
Spectrum and Modulation Assignment (RMSA) algorithm to optimize both anycast and unicast trafﬁc
ﬂows. Finally, we evaluate the performance of RMSA algorithms and assess the effectiveness of AS approach under various network scenarios. The main conclusion is that using AS approach results in signiﬁcant improvement of network performance.
& 2016 Elsevier B.V. All rights reserved.

Keywords:
Elastic optical networks
Survivability
Dynamic routing
Anycasting
Path protection

1. Introduction
Over the past few years, the Internet consolidated itself as a
very powerful platform. It has changed our means of communication and has given a “globalized” dimension to the world.
We may observe the impact of the progression of social media in
our life. Web services such as Facebook, Google þ, etc., break
records in the number of users. For example, on August 27, 2015,
a billion users of Facebook were online, which represents 1/7 of
the world's population. Shortly after, What's App announced
that it has 900 million active users. The ﬁrst widely available
measurement from December 1995, revealed 16 million registered Internet users (0.4% of world population). Since then, the
number of people connected to a global network has multiplied
more than 200 times.
Communication networks have to accommodate the inﬂation
of clients. Elastic Optical Networks (EONs), based on the optical
orthogonal frequency-division multiplexing (O-OFDM) technology
[1], have attracted intensive research interests as it may signiﬁcantly improve the spectral efﬁciency of the optical layer with
ﬂexible bandwidth allocation [2,3]. According to a deﬁnition of
EONs included in ITU-T recommendation (G.694.1) [4], the
n

Corresponding author.
E-mail addresses: michal.aibin@pwr.edu.pl (M. Aibin),
arunabha.sen@asu.edu (A. Sen).
http://dx.doi.org/10.1016/j.osn.2016.06.008
1573-4277/& 2016 Elsevier B.V. All rights reserved.

spectrum is divided into narrow frequency segments (called slices). A lightpath required to serve a network request is deﬁned by
a routing path and an optical channel. These consist of a ﬂexibly
assigned subset of slices around a nominal central frequency.
Failure of an optical network element (e.g., a ﬁber cut) may cause
huge data loss, resulting in the failure of several lightpaths. This
problem becomes additionally challenging, when lightpaths are
upgraded to high bit rates (e.g. 100 Gbps, 400 Gbps and more).
Subsequently, survivability in optical networks is a basic issue
[5,6].
In this paper, we focus on the Routing, Modulation and Spectrum Allocation (RMSA) problem with survivability constraints in
the context of dynamic routing of anycast and unicast trafﬁc.
Anycasting – deﬁned as one-to-one-of-many transmission – is a
very effective way to serve network services provisioned in data
centers (DCs) including popular cloud computing and contentoriented services. The anycast request is deﬁned by a client node,
where the request is issued and two demands (connections) are
required to serve the request, namely, an upstream demand (from
the client node to the DC) and a downstream demand (from the
DC to the client node). These two demands that realize the same
anycast request are called associated. For more details on modeling
of anycast trafﬁc refer to [7,8].
By breaking the ﬁxed-grid spectrum allocation limit of conventional Wavelength Division Multiplexing (WDM) networks,
EONs increase the ﬂexibility in the connection provisioning. As a
matter of fact, the available Routing and Wavelength Assignment

86

M. Aibin et al. / Optical Switching and Networking 23 (2017) 85–96

(RWA) proposals for WDM networks are no longer directly applicable in EONs. In EONs, routing concerns establishment of
lightpaths for individual connection requests. The process is accompanied by solving the problem of Routing and Spectrum
Allocation (RSA), which concerns ﬁnding a routing path and a
contiguous segment of spectrum subject to the constraint of no
frequency overlapping in network links [9]. The RMSA problem is
to determine three elements for a lightpath: a routing path, a
contiguous segment of spectrum subject to the constraint of no
frequency overlapping in network links, and a modulation format
(MF) [10,11].
Focusing on the survivability in EONs, the following methods should be mentioned [5,6]. The ﬁrst technique is called the
Dedicated Path Protection (DPP). It implies that each demand is
served by a working (primary) path and a backup path, which
is link-disjoint with the working path. The DDP method requires a large amount of extra capacity for protection purposes,
keeping protection resources idle when there is no failure. The
second way of obtaining survivability in optical networks is
Shared Backup Path Protection (SBPP). It allows different
backup paths to share spectrum resources on the overlapping
portion if the corresponding working paths are link-disjoint.
SBPP utilizes capacity more efﬁciently than DPP, but in some
cases may not provide 100% protection. For e.g., there can be
multiple-link failures in the network, which concurrently affect several demands that share the same resources on the
backup paths. Another technique of ensuring network survivability in EONs is Squeezed Path Protection (SPP) [12]. The key
idea of SPP is the use of bandwidth squeezing after a link
failure, i.e., only a part (e.g. 25%) of the trafﬁc on the working
path is to be protected (or restored). Therefore, the backup
path requires much less spectrum resources compared to the
working path. Moreover, similar to the SBPP technique, the SPP
method allows different backup paths to share spectrum on the
overlapping portion if the corresponding working paths are
link-disjoint. The concept of SPP guarantees a Service Level
Agreement (SLA) while simultaneously the network does not
need to reserve so much bandwidth as in conventional dedicated protection. The backup path bandwidth is reduced to the
required minimum amount, which enables cost-effective restoration in terms of spectral resource utilization.
So far, there have been several solutions proposed for survivable EONs in the literature. Survivable RSA algorithms under
single-link failure for DDP have been studied in [13–15]. Concurrently, ofﬂine problems for SBPP have been proposed in [16,17].
Eventually, dynamic survivable EON scenarios have been studied
in [18]. All these solutions assure network survivability under
single link failures. Algorithm for multi-link failure has been proposed in [19].
This paper is an extended version of our conference paper [20].
The extension concerns mainly: a new algorithm for network
survivability problems under multi-link failures in EONs; enhanced comparison of path protection techniques in various scenarios of link failures that occur in the network rendering to a
parameter ∂ described in the Section 3. According to the best of
our knowledge, this extended paper is the ﬁrst one that addresses
the issues of multi-link survivability problems with trafﬁc classes
aware in dynamic routing in EONs with the ability to change MF
between nodes. Note that in this paper, we continue our research
on dynamic routing in EONs, as started in our previous papers
[10,20,21].
The rest of the paper is divided into ﬁve sections. In Section 2,
survivability and RMSA problems are deﬁned. Furthermore, we
introduce trafﬁc classes' support in our algorithm. In Section 3, we
proceed to describe simulation setup. In Section 4 results are
presented. Finally, Section 5 contains conclusions.

2. Dynamic routing modulation and spectrum assignment
problem with network survivability
In this section, we formulate the problem of a dynamic RMSA in
survivable EONs with both unicast and anycast requests. The objective is to minimize Bandwidth Blocking Probability (BBP), deﬁned as the volume of rejected trafﬁc divided by the volume of all
trafﬁc offered to the network, while enabling network survivability. In addition, in Paragraph C, we introduce the concept of
trafﬁc classes in EONs.
2.1. Notation
We use similar notations as in [10]. The physical network is
modeled as graph G(V, E, B, L) where V denotes a set of nodes, E is a
set of ﬁber links, each ﬁber link may accommodate B frequency
slices (slots) at most, and L ¼ [l(1),l(2),…,l(|E|)] represents link
lengths for each e∈E. We assume that R data centers are already
located at some nodes of the network. In addition, we assume that
data centers (DCs) are equally connected to network nodes, to which
are connected to, which means that we do not take the physical
connection between the server and the backbone network node.
Furthermore, we assume that each anycast request may be assigned
to each DC, because DCs provide the same service of content.
Each request d may be of two types: unicast (one-to-one) or and
anycast (one-to-one of many). The unicast request is described by
the following: source node s(d), destination node t(d), capacity
(bitrate) c(d). The anycast request described by: source (client)
node s(d), downstream capacity c_down(d), upstream capacity
c_up(d).
Let l(p) ¼ Σe∈p l(e) denote length of path p calculated as the
sum of link lengths included in the path. Let P(s, t) denote a set of
k-shortest paths for node pair (s, t). Notice that in the case of a
unicast request, the set of candidate paths include exactly k paths.
Concerning anycast requests, the situation is different, i.e., for each
downstream (upstream) request the set of candidate paths contains of k|R| paths, since each DC node r∈R is considered. For instance, in the case of a downstream request d the set of candidate
paths P_down(d) will include all paths from sets P(r, t) for each r∈R.
For the sake of simplicity, we assume that paths included in each
set P(s, t), P_down(d), and P_up(d) are sorted according to increasing values of the path length.
We assume that various MFs may be used in the EON. Let M
denote a set of available MFs. According to the considered physical
model, for each MF m∈M and bit-rate c there is a constant dist(m,
c) that denotes the maximum distance that a particular modulation may support for bit-rate c. Additionally, let nn(c(d), p, m) represent the number of slices required to serve request d with bitrate c(d) using path p and modulation m for primary path and
analogously let nn'(c(d), p, m) denote the number of slices required
for backup path. Without a loss of generality, we assume that the
greater the MF is, then a higher spectral efﬁciency is achieved.
Since regenerators are costly, we assume that the selection of a MF
is made in order to minimize the number of regenerators placed in
the network. Finally, we do not allow grooming of the regenerators, therefore one regenerator serves one request at a time.
2.2. Problem description
Considering the physical topology discussed above, our goal is
to ﬁnd a route, assign a MF and allocate spectrum for each connection request such that the average BBP is minimized. The solution to the problem is subject to the following constraints:

 Spectrum contiguity, spectrum continuity and slice opacity: in
EONs, continuous spectrum resources to the speciﬁc demand

M. Aibin et al. / Optical Switching and Networking 23 (2017) 85–96

87

Fig. 1. RMSA example. (For interpretation of the references to color in this ﬁgure, the reader is referred to the web version of this article.)



must be assigned. In other words, once chosen set of slices have
to be used over entire end-to-end optical path. Moreover, if one
frequency slice has been allocated to an existing request, this
slice cannot be assigned to another request.
Modulation format constraint: In EONs, different MFs may be
used. However, there exists a trade-off between reachability and
spectrum minimization. In brief, if we use a high level MF to
reduce the spectrum cost, the reachability of the signal will
diminish.

Let's consider the following example (Fig. 1). We want to establish a connection between node A and node G with bit-rate
400 Gb/s. In the Table 1, we present example that estimates the
transmission reach of an optical signal in a function of the selected
modulation format and transported bit-rate for 400 Gb/s request
according to the physical model from [22]. More details on the
transmission model are introduced in the next sections.
The procedures of dynamic RMSA with path protection are as
follows. First, we want to ﬁnd the best route to establish a primary
connection (light blue line). Searching for the best routing path,
we must remember that different modulations have different
transmission ranges and spectral efﬁciency. Therefore, the designed RMSA algorithm must select a path and MF together, in
order to reduce blocking probability. In our example, we choose
path A–D–E–G. Total distance between the source and the destination node is 1850 km. For the case without regenerators, we
could use only BPSK MF, because of transmission reach limitations.
On the other hand, there are available regenerators in nodes C, D
and E, which we may use to regenerate the signal. Therefore, the
path from A to G is divided into the following segments: A–D, D–E
and E–G, with distances of 450 km, 600 km and 800 km, respectively. Using regenerators allows to use MFs with higher spectral
efﬁciency, for example 8-QAM or 16-QAM.
Additionally, in our work we assume establishment of backup
path (yellow line). For more details on distance adaptive routing
refer to [23]. When the routing path and MF is chosen, the designed RMSA algorithm allocates the spectrum by choosing the
slices in the links. This process is performed for each incoming
request and it is described in Section 3.
Table 1
MFs transmission reach for 400 Gb/s request.
MF

BPSK

QPSK

8-QAM

16-QAM

32-QAM

64-QAM

SE [b/s/Hz]
# slices
Range [km]

1
34
1912

2
18
1200

3
14
989

4
10
779

5
10
569

6
8
359

2.3. Trafﬁc classes
Three trafﬁc classes according to trafﬁc priority and the different ways how the trafﬁc is protected against network failures
are deﬁned. Let's consider a different network example - blue line
is used to indicate primary paths and red line is used for backup
paths:

 Class A includes requests of the highest priority that cannot be
interrupted at the time of a network failure. Therefore, requests
of class A are always equipped with a backup path using the
DPP approach. Even if a current primary/backup path is broken
due to link failure, a new primary/backup path is established
immediately to assure that the request of class A is always
protected by a dedicated backup path. Moreover, as it is shown
on Fig. 2, primary and backup paths are a link-disjoint and they
are using the same spectrum width.

 Class B consists of requests that cannot be interrupted at the
time of a network failure, however, the trafﬁc restored after the
failure, using the backup, may be squeezed. This means that a
backup path needs less spectrum resources, than the primary
path. This approach follows from the fact that many network
services may accept a temporary decrease of available bit-rate.
For instance, it is possible to decrease the quality of a video
transmission provided by services like YouTube. Another example is related to the fact that a lot of applications are used to
make calls over the Internet, such as Skype, Facebook, WhatsApp, etc. In current technology, the moment of deterioration of
a network connection (caused by a decrease in speed or a
weaker signal), the application used to make calls switches to
the inferior audio codec, this reduces a delay in the transmission, or prevents a connection failure. In our case, we perform
bandwidth squeezing by looking at a request bitrate, setting
squeeze ratio to 1:4. It means that for a demand of 400 Gbps,
protection path that may handle bit rate of 100 Gbps, is established. It results in lower transmission bitrates and a lower
number of available slices (less available spectrum). Therefore,
this enables the use of backup spectrum more efﬁciently than in
class A. The same as in Class A, primary and backup paths are
link-disjoint, but Class B allows different backup paths to share
spectrum on the overlapping portion if the corresponding
working paths are link-disjoint. (Fig. 3)

 Class C embraces requests of the lowest priority that are not
guaranteed with a dedicated backup path. In the case of network failure, request of class C is attempted to be restored. More

88

M. Aibin et al. / Optical Switching and Networking 23 (2017) 85–96

Fig. 2. Class A path protection scheme. (For interpretation of the references to color in this ﬁgure, the reader is referred to the web version of this article.)

Fig. 3. Class B path protection scheme. (For interpretation of the references to color in this ﬁgure, the reader is referred to the web version of this article.)

Fig. 4. Class C path protection scheme. (For interpretation of the references to color in this ﬁgure, the reader is referred to the web version of this article.)

precisely, there are at most three trials to restore the request. If
this operation fails using all three trials, the request is marked
as rejected and volume of this demand is added to the lost
trafﬁc (Fig. 4).

The main reason behind trafﬁc classes is to show the impact of
the effective handling of requests in order to improve network
performance in terms of blocking probability of new requests as
well as in terms of the amount of network trafﬁc lost (unrestored)

M. Aibin et al. / Optical Switching and Networking 23 (2017) 85–96

89

3. Path protection algorithms

Fig. 5. Path protection types.

due to a network failure. For instance, one trafﬁc class may need a
100% path protection scheme with a bandwidth guarantee,
whereas for another class, a less available spectrum with a lower
cost of path protection may be sufﬁcient. This solution is often
used at the IP layer for ensuring QoS. The authors of this paper
want to demonstrate the advantages of this technique, if it is introduced to the optical layer. Incorporating advantages to the optical layer could be accomplished using network orchestration.
Requests from the respective classes could be addressed at the
appropriate transponders in the network by SDN controllers. Cooperation between SDN controllers in OpenFlow standard with
EON was presented in [24–26]. In our case, it is used for assigning
the network trafﬁc to appropriate class (Fig. 5).
As it is shown in Fig. 5, the SDN controller is responsible for
above-mentioned operation. Then, the OpenFlow devices switch
the appropriate classes of trafﬁc to speciﬁc optical transponders.
Enabling trafﬁc control on SDN controller is performed to avoid
delays in the optical layer, which is responsible only for simple
decisions at the time of establishing a lightpath using fast algorithms FF and LF.

2.4. Multiple link failure model
Multiple link failures in a network may occur due to different
situations [27]. We may expect failures due to natural disasters, a
virus/worm attack or device failures. Natural disasters occur in
speciﬁc geographical locations and disrupt speciﬁc parts of the
network. Therefore, the geography of the network determines the
failure effect on the network's connectivity and capacity. A virus/
worm attack may happen anywhere in the network, the same as
device failures. In this work we do not consider geography of the
network. We consider only virus/warm attacks or device failures.
During the simulation, we randomly choose links and mark them
as broken. Then, depending on the trafﬁc class, we either switch to
a backup path or re-establish a connection. The number of links
that we choose for failure simulation varies from 2 to 4. The frequency of link failures is introduced in Section 4.

The main innovation of this work is the use of a novel approach
named Adaptive Survivability (AS) that combines various protection
methods according to different survivability requirements deﬁned for
various trafﬁc classes. In our paper, traditional DDP and SBPP techniques of path protection are compared to our new approach. We use
AS approach for two different RMSA algorithms. Firstly, we use AS
with the Adaptive Modulation and Regenerator Aware (AMRA) algorithm [21]. In addition, we introduce a novel Auxiliary, Composite and
Expanded Graph Construction (ACEGC) algorithm, to test three abovementioned path protection techniques.
Since Routing, Modulation and Spectrum Allocation (RMSA)
scenarios are assumed in this paper, regeneration may be used to
provision the lightpath. Therefore, routing paths may include
several path segments (sub connections). Path segment is deﬁned
as portion of a path that falls between two regeneration points or
between an end point and a regeneration point. In addition, we
assume that MF may be changed in the nodes with the regenerators [28].
Algorithm 1 shows the pseudo-code of the path requirements calculation with the AS approach. In line 1, we update the
available spectrum on each link according to the current state of
the network. In the next step (line 2), we calculate all candidate
paths. For each candidate paths, we perform a ﬁrst loop (lines 3–
6). Initially, we check the available MFs between nodes that
have unassigned regenerators (line 4). This step is necessary to
continue the algorithm, when we have to decide which MF to
choose by looking at the modulation maximum transmission
distances. If there are no available MFs on current path, it is
removed from candidate paths set (line 5). Finally, the algorithm
checks whether there are any elements of the collection P (line
7). In the case when there are no candidate paths, algorithms
return INF, because the allocation of the demand is not possible
in current network state.
Following the calculation of path metrics, AS algorithm process
the incoming request according to the particular trafﬁc class (lines
9–13) – it is calculating the number of required slices nn for the
primary (nnp) and the backup (nnp) for each MF. As a remainder
from the previous section, demands from class A use the Shared
Backup Path Protection method, whereas demands from class B
use the Squeezed Backup Path Protection and there is no backup
path for demand from the class C. In the last step, the RMSA algorithm is called, to allocate demands with different approaches.
In Algorithms 2–5 we present the processing of a unicast and
anycast requests by two different RMSA algorithms. The steps from
Algorithm 1 are performed for each incoming demand into the
network, before the execution of RMSA algorithms.
Algorithm 1. Adaptive survivability approach.
1 Update available spectrum on each link.
2 Calculate candidate paths for node pair (s(d), t(d))
3 for each p∈P(s(d), t(d)) do
4 Check available m∈M between nodes with regenerators
5 if (m ∈ ∅) then remove p from P(s(d), t(d))
6 end for
7 if (P ¼ ¼ ∅) return INF
8 Sort available candidate paths and choose n from them
9 switch (d):
10 case d ∈ class A: nnp ¼ ∑nnps (c(d), pps, mps), nnb ¼ ∑nnbs
(c(d), pbs, mbs), break
11 case d ∈ class B: nnp ¼ ∑nnps (c(d), pps, mps), nnb ¼ ∑nnbs (c
(d)/4, pbs, mbs), break
12 case d ∈ class C: nnp ¼ ∑nnps (c(d), pps, mps), null, break
13 end switch
14 Use RMSA algorithm to allocate d

90

M. Aibin et al. / Optical Switching and Networking 23 (2017) 85–96

First, we describe the AMRA algorithm for unicast requests. After
searching for candidate paths (line 1), for each of the path segments
of candidate path p included in P, we try to allocate a request requiring nns slices on each of the new path segments (lines 2 and 3).
We choose adaptively the best MF in line 4. This process is described
in more details in [21]. Next, the First Fit (FF) method is used to ﬁnd
the lowest available slice possible for allocation of the demand (line
5). If a request is allocated (line 7), we perform a second loop, using
Last Fit (LF) method to calculate the backup path. Important to notice
is that we could use only candidate paths P′∩P ¼ ∅. This is caused by
primary and backup paths, which must be link-disjointed to ensure
network survivability. If the process of establishing primary and
backup paths is successful, we return true (line 13). Otherwise, we
mark a rejection of the request as a BBP (line 17) returning false.
Algorithm 2. AS_AMRA_Unicast.
1 Calculate path metrics using Algorithm 1
2 for each p ∈ P(s(d), t(d)) do
3 for each ps ∈ p do
4
Pick adaptively the best MF
5
FF(ps, nns)
6 end for
7 if nno INF then //established primary path
8
for each p′ ∈ P'(s(d), t(d)) where P′ ∩ P ¼ ¼ ∅ do
9
for each p′s ∈ p' do
10
Pick adaptively the best MF
11
LF(ps, nns)
12
end for
13
if nn′ oINF then return true //established primary and
backup paths
14
end for
15 end if
16 end for
17 return false //blocked
The anycast version of the AS algorithm is in general similar to the
unicast one, with the main difference following from the fact that two
associated anycast requests (downstream and upstream) must be
processed together to assure that both associated anycast requests use
the same DC node (shown in Algorithm 3). To this end, ﬁrst the larger
request (in terms of bit-rate) of the associated requests is allocated
considering all possible DCs (in our example downstream request,
lines 2–11). After establishment of primary (line 7) and backup (line
13) paths, the corresponding smaller associated request is served as a
unicast, since the DC node is already selected (line 14). For the backup
path, we consider case when it may be connected to a different DC
node. If both of the associated requests have primary and backup
paths, we consider allocation as successful.
Algorithm 3. AS_AMRA_Anycast.
1 Calculate path metrics using Algorithm 1
2 for each p ∈ P_down(s(d)) do
3 for each ps ∈ p do
4
Pick adaptively the best MF
5
FF(ps, nns)
6 end for
7 if nno INF then //established primary path
8
for each p′ ∈ P_down'(s(d)) where P_down' ∩ P_down
¼ ¼ ∅ do
9
for each p′s ∈ p' do
10
Pick adaptively the best MF
11
LF(ps, nns)
12
end for
13
if nn'o INF then //established primary and backup paths
for downstream request

14
p_down’p, r_down’r(p), p_down'’p', r_down'’r
(p)′
15
AMRA_Unicast (s(d),r_down, c_up(d))
16
end if
17
end for
18 end if
19 end for
20 return false //blocked
Another algorithm that we introduce is: ACEGC. In the following we
analyze and solve a routing problem in EONs that support multiple
MFs, and where the path length is deﬁned as a combination of number
of applied regenerators and spectrum consumption. Different MFs are
available and each MF m∈M is characterized with constant bm that
denotes the maximum supported distance range and constant nm that
denotes the number of required slices. Due to the use of regeneration,
path p is divided into segments. A single path segment is deﬁned as
portion of a path that falls between two regeneration points or between
an end point and a regeneration point. On each segment, a different MF
may be used. Let Rp denote the number of regenerators used in path p.
Moreover, let Sp deﬁne the overall number of spectrum slices used in all
links included in p according to the selected MFs on particular path
segments. For instance, let assume that path p contains of 5 links. If on
the whole path, a MF requiring 6 slices is used, then Sp ¼5  6¼30. In
turn, if on the ﬁrst segment with two link 4 slices are required, and next
on the second segment (with 3 links) 8 slices are needed, then
Sp ¼ 2  4þ3  8¼32. The length of the path p is deﬁned as a function
Rp and Sp and is given as: αRp þ(1 α)Sp, for a weighting parameter α.
We provide an algorithm for computing the shortest path between a
source-destination node pair in an EONs that supports multiple MFs
and the length of a path is computed in the way speciﬁed in the previous line. The input and out of the algorithm is provided next.
Input: Graph G ¼ (V, E, B, L), weight w(e) associated with each
edge e∈E (w(e) represents the length of the edge e), location of a
set of regenerators R1,…, Rr. modulation techniques M1,…, Mq,
optical reach of modulation technique Mi denoted by Oi and
data carrying capacity of modulation technique Mi denoted by
Ci, source node s(d) and destination node t(d).
Output: A shortest path between s(d) and t(d), using the metric
that takes into account the number of regenerators and the
number of spectrum slices used (αRp þ(1–α)Sp).
Since the algorithm is somewhat complex, we ﬁrst explain the
idea behind the algorithm. The vertex set V of the input graph G¼(V,
E, B, L), may be split into two groups VR and VN, indicating the nodes of
the graphs where regenerators are located and the nodes where regenerators are not located. As a ﬁrst step of the algorithm, we compute the shortest path between every pair of nodes in the node set {s
(d)}∪{t(d)}∪VR and store the path (i.e., the set of edges that make up
the path) and the shortest path distance between the nodes in a table
called Distance Table (DisTab). We then compute q auxiliary graphs,
H1,…, Hq, corresponding to each modulation technique M1,…, Mq in
the following way. Each auxiliary graph Hk 1 r k r q has rþ2 nodes
where r is the number of regenerator nodes and two additional nodes
correspond to the source and destination nodes s(d) and t(d). In the
auxiliary graph Hk 1 r k r q, there is an edge between the nodes i
and node j if the shortest distance between i and j as stored in DisTab
is at most the optical reach Ok for Mk 1 r k r q. The length (or
weight) of this edge is x*B/Ck, where x is the number of edges of the
graph G ¼ (V, E, B, L) that makes up the shortest path from i to j, Ck is
the capacity of the modulation technique Mk and B is bandwidth
requested by the connection from s(d) and t(d). In the seventh step of
the process, we construct a composite graph Gc from the set of

M. Aibin et al. / Optical Switching and Networking 23 (2017) 85–96

auxiliary graphs H1,…, Hq, in the following way. The composite graph
Gc has exactly the same set of nodes as Hk graphs, 1 r k r q. The
weight of the edge between nodes i and j in Gc is equal to the smallest
weight between the nodes i and j in Hk taken over all 1 r k r q. In
the eighth step, we create an expanded graph, Ge by replacing each
node of Gc (except s and t) by a clique of size equal to the degree of the
node. The weight of the edges in the clique is the value of the parameter Alpha (α) and the weight of the remaining edges of Ge is equal
to (1–α) times the weight of the corresponding edge in Gc. Finally, we
compute the shortest path from the s to the t using any shortest path
algorithm such as the one due to Dijkstra.
Algorithm 4. ACEGC Unicast Algorithm.
1 for each vi, vj ∈ {s(d)}∪{t(d)}∪VR do compute shortest path
between vi, vj and store it in DisTab(i, j).
2 for k ¼ 1 to q do
3 Construct auxiliary graph Hk with node set {s(d)}∪{t(d)}∪VR.
4 If Ok Z DisTab(i, j) then have an edge between in vi, vj in
Hk, for each pair of nodes vi, vj ∈ {s(d)}∪{t(d)}∪VR

91

5

Assign a weight of x*B/Ck on this edge, where x is the
number of edges of the graph G ¼ (V, E, B, L) that makes up
the shortest path from vi and vj, Ck is the capacity of the
modulation technique Mk and B is bandwidth requested by
the connection from s(d) to t(d).
6 end for
7 Construct composite graph Gc with node set {s(d)}∪{t(d)}∪VR.
Assign weight wc(i j) on the edge (i j) in Gc, in the following
way: wc(i j) ¼ min of wk(i j), over all k, 1 r k r q, where wk(i
j), represent the weight of the edge (i j) in Hk
8 Construct expanded graph Ge by replacing each node of Gc
(except s and t) by a clique of size equal to the degree of the
node. Assign a weight of Alpha (α) to the edges in the clique.

To the remaining edges of Ge, assign a weight of (1 – α) times
the weight of the corresponding edge in Gc.
9 Compute the shortest path from s(d) to the t(d) in the graph
Ge using any shortest path algorithm, such as the one due to
Dijkstra.

Fig. 6. Execution of the ACEGC algorithm.

92

M. Aibin et al. / Optical Switching and Networking 23 (2017) 85–96

For the sake of elaboration, we explain the execution of the algorithm with the help of an example. In the example shown in Fig. 6,
we would like to compute the shortest path between the source node
v1 to the destination node v17 and the distance between the nodes are
as shown in Fig. 6(a). The shortest path between every pair of nodes
the node set {s(d)}∪{t(d)}∪VR is computed and stored in distance table
DisTab (i, j). The shortest path between the regenerator nodes v3 and
v7 is 500 and comprises of two edges e4 and e9 and is stored in DisTab
(v3, v7). The other input parameters for the problem, such as the requested bandwidth B for the connection from the source to the
destination node, available MFs Mi (3 in the example), their optical
reach Oi and data carrying capacity Ci is shown in Fig. 6(c). Since the
optical reach for M1 is 500, there will be an edge between v3 and v7 in
the auxiliary graph H1, shown in Fig. 6(d). Since the requested
bandwidth for the request is 100 and capacity C1 of M1 is 10, 100/
10¼10 slices will be needed to be carried over two edges e4 and e9. As
such the length of this edge will be 2*100/10¼20. The constructed
auxiliary graphs corresponding to modulation techniques M1, M2 and
M3 are shown Fig. 6(d), (e) and (f) respectively. As shown in Fig. 6(d),
(e) and (f), the distance between the nodes v8 and v9 in H1, H2 and H3
are 10, 5 and 3.33 respectively. We take the smallest (minimum) of
these set of values (3.33) and assign it to be the weight of edge between the nodes v8 and v9 in the composite graph, Gc shown in Fig. 6
(g). The degrees of nodes v8 and v9 in the composite graph Gc is 3 and
4 respectively. Accordingly, in the expanded graph Ge, v8 is replaced
by a clique of size 3 and v9 is replaced with a clique of size 4, as
shown in Fig. 6(h). The weight of the edges in the clique is the value
of the parameter Alpha (α), which in our simulation is 0.7. Finally, the
shortest path between the v3 and v7 is computed and shown in Fig. 6
(h).
Moreover, in the following we present a shortest path algorithm
for anycast. Anycasting – deﬁned as one-to-one-of-many transmission, is speciﬁed by the quadruple: (client node, set of admissible DC
nodes upstream bandwidth requirement, downstream bandwidth requirement). Since the upstream and downstream bandwidth requirement may be different, if the path length is measured using the
metric, (αRp þ (1 – α)Sp), the upstream path length from the client
node to a DC node may be different from the downstream path length
between the same pair of nodes. For this reason, we deﬁne the “total
path length” between the client and the DC node to be equal to the
sum of the upstream and downstream path lengths.
Algorithm 5. ACEGC Anycast Algorithm.
1 for each DC node s, compute the path from client c to server
s that has the shortest total path length using the metric,
(αRp þ(1  α)Sp), and store the shortest path length in SP(c, s).

2 ﬁnd the server s* such that SP(c, s*) ¼ min (SP(c s)) over all s
∈ S, where S is the set of all DC nodes.
3 Use s* as the server to fetch data request from client c.
To address the issues related to survivability, often times a pair
of paths, primary and backup, is computed between the source–
destination node pair, so that in case of failure of one path, the
other one may be utilized for data transmission. In order to tolerate node failures, the primary and backup paths must be node
disjoint, as otherwise failure of a single node may disable both the
paths. Similarly, in order to tolerate link failures, the primary and
backup paths must be link disjoint. An algorithm due to Suurballe
[29] computes two node/link disjoint paths between a sourcedestination node pair in a nonnegatively-weighted directed graph
such that the sum of the path lengths is minimum over all such
path pairs. Even when the path length between the source-destination path pair is computed using the metric, (αRp þ (1–α)Sp),
Suurballe's algorithm still may be utilized to compute node/link
disjoint path pairs with minimum path length, just the way Dijkstra's algorithm was utilized in computing the shortest path between a source-destination node pair in Algorithm 4 for unicast
communication.

4. Simulation setup
In the simulations, two different network topologies have been
evaluated (Fig. 7): US26 network (26 nodes, 84 unidirectional links
and 7 DCs) and Euro28 network (28 nodes, 82 unidirectional links
and 7 DCs). In all these scenarios, the entire band of 2 THz spectrum divided into 6.25 GHz frequency slices has been made
available, resulting into 320 slices. The networks have three interconnection points to the other networks that are used to carry
international trafﬁc. The data provided by [30] determines the
location of DCs and interconnection points (e.g., locations for
submarine cable landing stations). We take into consideration the
physical impairment of links (ﬁber attenuation, component insertion loss) and use regenerators to amplify the signal in the links
that require higher MFs. The location of the regenerators is set on
the initiation of the simulation; 100 regenerators are assigned to
each node.
We use a similar trafﬁc model to [31]. The model is created
under the forecast in “Cisco Visual Networking Index” and “Cisco
Global Cloud Index” reports; it shares the trafﬁc forecasts from
year 2016. We assume that requests have some lifetime after

Fig. 7. Optical networks: a) Euro 28, b) US26.

M. Aibin et al. / Optical Switching and Networking 23 (2017) 85–96

which they are torn down. The requests arrive one by one, following a Poisson process with an average arrival rate of λ requests
per time-unit like in other papers on dynamic routing in optical
networks. The lifetime of each request follows a negative exponential distribution with an average of 1/μ. Consequently, the
trafﬁc load is λ/μ Erlangs (ER). The number of requests in the
dynamic scenario is 105,000. We remove the ﬁrst 5000 requests
from the evaluation, since the network is not in a steady state.
There are four types of generated requests according to data presented in abovementioned Cisco reports:

 City – City (CC) to carry all non-data center trafﬁc (7.1% of all
trafﬁc, 10–100 Gbps of requested bit-rate).

 City – Data Center (CD) represents all data center to user trafﬁc

93

respectively (i.e., the PM capability doubles their initial spectral
efﬁciency). We use a transmission model from [22].
To examine the inﬂuence of network failures on performance of
the analyzed protection methods in more detail, parameter ∂ is
introduced. In particular, parameter ∂ denotes the frequency of
link failures in the network. For instance, ∂¼ 0 shows that there are
no failures, while ∂ ¼ 1 means that the failure frequency is equal
to the average lifetime of the request. As it is stated in previous
Sections, this paper is an extended version of [20]. For our experiments we evaluated different values of ∂: 0.2 and 0.4. Moreover, it is assumed that after a failure, the broken link needs to be
repaired and the repair time is ten times of the average lifetime of
the request.

(51.8% of all trafﬁc, 10–200 Gbps).

 Data Center – Data Center (DD) trafﬁc (21.1% of all trafﬁc, 40–


400 Gbps).
International (IN) trafﬁc – all trafﬁc leaving/entering the network (20% of all trafﬁc, 10–100 Gbps)

The main performance metric is BBP deﬁned as the volume of
rejected trafﬁc divided by the volume of all trafﬁc offered to the
network. Moreover, we introduce Lost Trafﬁc Ratio (LTR) metric to
evaluate the quality of protection in the network. More speciﬁcally, when a failure occurs and a request cannot be restored due
to the lack of spare spectrum and/or regenerators, the trafﬁc of this
request that was not delivered is classiﬁed as lost trafﬁc. The LTR
metric is a percentage ratio of lost trafﬁc comparing to overall
trafﬁc in the network. For example, if the trafﬁc incoming into the
network is 200 GB and 2 GB of the trafﬁc is lost, the value of LTR is
1%. It should be noted that if the failure occurs when some of the
request has been successfully transmitted, only a proportional part
of the trafﬁc associated with that request is counted as a lost
trafﬁc. Due to dynamic nature of the problem, all routing decisions
are to be made in a matter of milliseconds time. The execution
times for both algorithms are similar, less than 20 ms per request.
In our experiments, we assume that the number of k-shortest
paths being calculated for each node pair is 10. We have to limit
the number of candidate paths to 10, as the dynamic routing requires fast decisions based on the current network state. The
lightpath requests, end nodes and bit-rate, are generated at random using a Gaussian distribution to provide the percentage share
of each trafﬁc type. Trafﬁc types such as CC, CD and IN are served
by unicast requests, while DD trafﬁc is served by both anycast and
unicast requests. We use a physical model of EONs as in paper [30].
In detail, concerning the BV-T equipped at EONs nodes, we assume
that they may operate either at 100, 200, 300 or 400 Gb/s. Moreover, they may employ any of the following MFs: Polarization
Mode (PM)-BPSK, PM-QPSK, PM-8-QAM, PM-16-QAM, PM-32QAM and PM-64-QAM with spectral efﬁciencies of 1,…,6 bits/s/Hz,

5. Results
In this section, we present simulation results. The main objective is to show the improvement of transmission efﬁciency with
the use of three distinct classes of trafﬁc, resulting in a decrease of
the BBP parameter over traditional path protection methods: DPP
and SBPP, as well as decreasing LTR parameter in comparison to
the SBPP method. The work does not focus on the implementation
of hardware solutions, showing only the advantages of this approach in terms of optimization of usage of network resources
during dynamic routing. In addition, authors assume that acceptable BBP level is up to 1%. Higher level for the most operators is an
unacceptable situation in order to guarantee SLA.
Figs. 8 and 9 show the DDP approach with different values of ∂
in Euro28 and US26 networks. Comparing AMRA algorithm with
the ACEGC, we may see that for the lower values of Erlangs (ER),
BBP is acceptable for AMRA, exceeding acceptable limit slightly
with ACEGC. With ER higher than 300 ER, both algorithms exceed
the acceptable value of BBP. Therefore, we may conclude that with
two above-mentioned algorithms, the acceptable parameters of
routing with DPP are achieved for the trafﬁc less than 300 ER. It
should be noted that in the context of the DPP approach, we do not
report the LTR metric, since – as described above – our DPP
method always provides 100% restoration for a link failure. The
trend of the results for the US26 network is similar to results
obtained for the network Euro28, except that the BBP is slightly
higher. The main reason that causes this behavior are larger link
lengths between nodes in the network, and thus the usage of the
less spectral efﬁcient MFs and more frequent signal regeneration
need in the network.
Another approach of path protection that we want to test is
SBPP. In Figs. 10 and 11, we observe two metrics - BBP on the
principal axis (left side) and LTR on the minor axis (right side). The
LTR metric is reported, since some of the requests (with shared
backup capacity) affected by the multiple link failures cannot be

Bandwidth Blocking Probability

1.00E+00

1.00E-01

1.00E-02

1.00E-03

BBP AMRA (∂ = 0.2)
BBP ACEGC (∂ = 0.2)

1.00E-04

BBP AMRA (∂ = 0.4)
BBP ACEGC (∂ = 0.4)

1.00E-05
100

150

200

250

300

350

Erlangs
Fig. 8. DDP approach in Euro28 network.

400

450

500

94

M. Aibin et al. / Optical Switching and Networking 23 (2017) 85–96

Bandwidth Blocking Probability

1.00E+00

1.00E-01

BBP AMRA (∂ = 0.2)

1.00E-02

BBP ACEGC (∂ = 0.2)

1.00E-03

BBP AMRA (∂ = 0.4)
BBP ACEGC (∂ = 0.4)

1.00E-04
100

150

200

250

300

350

400

450

500

Erlangs
Fig. 9. DDP approach in US26 network.

restored on the backup path. We want to underline that with this
approach, we may observe not only the blocking probability encountered when new requests arrive, but also network performance when failures occur (shown by the LTR parameter).
The ﬁrst observation is that after the introduction of the SBPP,
BBP generally has been understated for all values of the network
trafﬁc in comparison to DPP. AMRA algorithm improved more than
ACEGC. Both algorithms provide acceptable values of BBP for the
trafﬁc lower than 350 ER in Euro28 network. LTR is similar for both
algorithms for trafﬁc below 300 ER, equals to about 0.5%. Signiﬁcant change in LTR for AMRA at 500 ER, whereas for ACEGC we
may observe gradual increase of this value up to the vicinity of 3%
at 500 ER. With BBP of about 10%, it means that 30% of trafﬁc is
rejected because of lack of backup links at the time of the multiple
link failures. For US26 network, the values have not been signiﬁcantly improved in comparison to the DPP approach. BBP
LTR AMRA (∂ = 0.2)

LTR ACEGC (∂ = 0.2)

LTR AMRA (∂ = 0.4)

LTR ACEGC (∂ = 0.4)

BBP AMRA (∂ = 0.2)

BBP ACEGC (∂ = 0.2)

BBP AMRA (∂ = 0.4)

BBP ACEGC (∂ = 0.4)

4.00%
3.50%

1.00E-01
3.00%
2.50%

1.00E-02

2.00%
1.00E-03

1.50%

Lost Traffic Ratio

Bandwidth Blocking Probability

1.00E+00

values are acceptable for the trafﬁc of less than 250 ER. This is due
to the similar fact as in DPP - the length of the network paths is
longer, so the establishment of backup paths, even with shared
resources, puts substantial load on the available spectrum, and
thus, in the event of multiple link failures, it causes higher BBP. In
general, we may say that with the usage of SBPP, network is not
protected in 100%, but maintains a much lower level of BBP. Thus,
in the ﬁnal analysis, SBPP compares favorably to the DDP, allowing
the transmission of much larger amounts of data without
rejections.
After comparing traditional methods of path protection methods (DDP and SBPP), we show beneﬁts from the use of the AS
methods aware of trafﬁc classes. Figs. 12 and 13 shows BBP and
LTR (as in Figs. 10 and 11). The ﬁrst advantage of using three
classes of trafﬁc that may be observed is a signiﬁcant reduction of
BBP. For the Euro 28 network, BBP level is acceptable up to 400 ER,

1.00%
1.00E-04
0.50%
1.00E-05

0.00%
100

150

200

250

300

350

400

450

500

Erlangs
Fig. 10. SBPP approach in Euro28 network.

4.00%
LTR AMRA (∂ = 0.2)

LTR ACEGC (∂ = 0.2)

LTR AMRA (∂ = 0.4)

LTR ACEGC (∂ = 0.4)

BBP AMRA (∂ = 0.2)

BBP ACEGC (∂ = 0.2)

BBP AMRA (∂ = 0.4)

BBP ACEGC (∂ = 0.4)

3.50%

1.00E-01
3.00%
2.50%

1.00E-02

2.00%
1.00E-03

1.50%
1.00%

1.00E-04
0.50%
0.00%

1.00E-05
100

150

200

250

300

350

400

Erlangs
Fig. 11. SBPP approach in US26 network.

450

500

Lost Traffic Ratio

Bandwidth Blocking Probability

1.00E+00

M. Aibin et al. / Optical Switching and Networking 23 (2017) 85–96

95

4.00%
LTR AMRA (∂ = 0.2)

LTR ACEGC (∂ = 0.2)

LTR AMRA (∂ = 0.4)

LTR ACEGC (∂ = 0.4)

BBP AMRA (∂ = 0.2)

BBP ACEGC (∂ = 0.2)

BBP AMRA (∂ = 0.4)

BBP ACEGC (∂ = 0.4)

3.50%

1.00E-01
3.00%
2.50%

1.00E-02

2.00%
1.00E-03

1.50%

Lost Traffic Ratio

Bandwidth Blocking Probability

1.00E+00

1.00%
1.00E-04
0.50%
1.00E-05

0.00%
100

150

200

250

300

350

400

450

500

Erlangs
Fig. 12. AS approach in Euro28 network.
4.00%
LTR AMRA (∂ = 0.2)

LTR ACEGC (∂ = 0.2)

LTR AMRA (∂ = 0.4)

LTR ACEGC (∂ = 0.4)

BBP AMRA (∂ = 0.2)

BBP ACEGC (∂ = 0.2)

BBP AMRA (∂ = 0.4)

BBP ACEGC (∂ = 0.4)

3.50%
1.00E-01

3.00%
2.50%

1.00E-02

2.00%
1.00E-03

1.50%

Lost Traffic Ratio

Bandwidth Blocking Probability

1.00E+00

1.00%
1.00E-04
0.50%
1.00E-05

0.00%
100

150

200

250

300

350

400

450

500

Erlangs
Fig. 13. AS approach in US26 network.

whereas for US26 up to 300 ER. Gain with usage of AS methods is
slightly higher for AMRA than for the ACEGC algorithm. In addition, with the much greater spectral efﬁciency, it guarantees much
higher protection level than SBPP. In the case of SBPP, we lowered
the BBP, but a larger number of not restored link failures may be
observed. With the usage of AS algorithm, LTR for the values of ∂ is
not exceeding 0.5% for Euro28 and 1% for US26. This metric will be
particularly useful for operators in the evaluation of this algorithm
to obtain satisfactory SLA.

6. Conclusion
In this paper, we focused on survivability problems in EON. We
described a new path protection method, called Adaptive Survivability. In addition, we proposed a new RMSA routing algorithm for
network survivability problems that included an option to change the
MF in the network nodes on the lightpath. The key novelty of this
paper is the possibility to serve separate trafﬁc classes in a different
way according to survivability requirements. The conclusion, which
stems from a comparison of DDP against SBPP, is as that SBPP allows
for a better use of the spectrum with a small number of rejected calls
due to restoration errors, and thus reducing the cost of network
maintenance. On the other hand, with large amounts of free spectrum, the decision to use DDP is justiﬁed because it guarantees 100%
network survivability. Moreover, the use of trafﬁc aware dynamic
routing adaptive algorithm will result in a signiﬁcant improvement in
the routing parameters in EON, guaranteeing high network survivability level with lower cost of the network maintenance than in the
traditional DDP and SBPP methods.
For future implications, we plan to work on new algorithms for
dynamic routing in survivable EON for multicast trafﬁc. In

addition, we plan to compare performance of ﬂexible grid EONs
against baseline solution provided by traditional ﬁxed grid WDM
networks.

Acknowledgment
This work was supported in part by the Polish National Science
Centre (NCN) under Grant DEC-2012/07/B/ST7/01215 and by the
European Commission under the 7th Framework Programme, Coordination and Support Action, Grant agreement number 316097,
ENGINE - European research centre of Network intelliGence for INnovation Enhancement (http://engine.pwr.wroc.pl/).

References
[1] J. Armstrong, OFDM for optical communications, J. Lightw. Technol. 27 (3)
(2009) 189–204.
[2] O. Gerstel, M. Jinno, A. Lord, B. Yoo, Elastic optical networking: a new dawn for
the optical layer? IEEE Commun. Mag. 50 (2) (2012) S12–S20.
[3] M. Jinno, et al., Distance-adaptive spectrum resource allocation in spectrumsliced elastic optical path network, IEEE Commun. Mag. 48 (8) (2010) 138–145.
[4] ITU-T Recommendation G.694.1 (ed. 2.0), “Spectral grids for WDM applications: DWDM frequency grid”, Feb. 2012.
[5] R. Goścień, K. Walkowiak, M. Klinkowski, J. Rak, Protection in elastic optical
networks, IEEE Netw. 29 (6) (2015) 88–96.
[6] Shen Gangxiang, Guo Hong, Bose SanjayK, Survivable elastic optical networks:
survey and perspective, Photonic Netw. Commun. (2015) 1–17.
[7] K. Walkowiak, Anycasting in connection-oriented computer networks: models,
algorithms and results, Int. J. Appl. Math. Comput. Sci. 20 (1) (2010) 207–220.
[8] K. Walkowiak, M. Klinkowski, Joint anycast and unicast routing for elastic optical
networks: modeling and optimization, in: Proceedings of IEEE International
Conference on Communications ICC2013, 2013.
[9] M. Klinkowski, K. Walkowiak, Routing and spectrum assignment in spectrum
sliced elastic optical path network, IEEE Commun. Lett. 15 (8) (2011) 884–886.

96

M. Aibin et al. / Optical Switching and Networking 23 (2017) 85–96

[10] M. Aibin, K. Walkowiak, Dynamic routing of anycast and unicast trafﬁc in
elastic optical networks with various modulation formats - trade-off between
blocking probability and network cost, in: Proceedings of IEEE HPSR, Vancouver, Canada, July 2014.
[11] K. Christodoulopoulos, et al., Elastic bandwidth allocation in ﬂexible
OFDM based optical networks, IEEE J. Lightware Technol. 29 (9) (2011)
1354–1366.
[12] K.D.R. Assis, R.C. Almeida, H. Waldman, MILP formulation for squeezed protection in spectrum-sliced elastic optical path networks, in: Proceedings of
SPECTS 2012, Genoa, Italy, Jul. 2012.
[13] M. Klinkowski, K. Walkowiak, Ofﬂine RSA algorithms for elastic optical networks with dedicated path protection consideration, in: Proceedings of
RNDM, St. Petersburg, Russia, Oct. 2012.
[14] M. Klinkowski, A genetic algorithm for solving RSA problem in elastic optical
networks with dedicated path protection, Adv. Intell. Syst. Comput. Series 189
(2013).
[15] A.N. Patel et al., Survivable transparent ﬂexible optical WDM (FWDM) networks, in: Proceedings of OFC, Los Angeles, USA, Mar. 2011.
[16] A. Eira et al., Optimized design of shared restoration in ﬂexible-grid transparent optical networks, in: Proceedings of OFC, Los Angeles, USA, 2012.
[17] A. Castro et al., Path-based recovery in ﬂexgrid optical networks, in: Proceedings of ICTON, Coventry, England, Jul. 2012.
[18] X. Shao, Y.-K. Yeo, Z. Xu, X. Cheng, L. Zhou, Shared-path protection in OFDMbased optical networks with elastic bandwidth allocation, in: Proceedings of
OFC, Los Angeles, USA, 2012.
[19] B. Chen, J. Zhang, Y. Zhao, C. Lv, W. Zhang, Y. Gu, S. Huang, W. Gu, A novel
recovery algorithm for multi-link failures in spectrum-elastic optical path
networks, IEEE ACP (2011).

[20] M. Aibin, K. Walkowiak, Adaptive survivability algorithm for path protection
with various trafﬁc classes in elastic optical networks, in: International
Workshop on Reliable Networks Design and Modeling (RNDM), Monachium,
Germany, 2015.
[21] M. Aibin, K. Walkowiak, Adaptive Modulation and Regenerator-Aware Dynamic
Routing Algorithm in Elastic Optical Networks, ICC, London, UK, June 2015.
[22] C. Politi et al., Dynamic operation of Flexi-Grid OFDM-based networks, in:
Proceedings of OFC, 2012.
[23] R. Goscien, K. Walkowiak, M. Klinkowski, Distance-adaptive transmission in
cloud-ready elastic optical networks, IEEE/OSA J. Opt. Commun. Net. 6 (10)
(2014) 816–828.
[24] Y. Yoshida et al., First international SDN-based Network Orchestration of
Variable-capacity OPS over Programmable Flexi-grid EON, IEEE OFC, San
Francisco, USA, 2014.
[25] T. Tanaka, Flexible and robust optical network technologies for SDN and
network virtualization, in: Proceedings of the 2014 12th International Conference on Optical Internet 2014 (COIN), 2014, pp. 1–2.
[26] Z. Zhu, C. Chen, X. Chen, S. Ma, L. Liu, X. Feng, S.J. Ben Yoo, Demonstration of
cooperative resource allocation in an openﬂow-controlled multi-domain and
multinational SD-EON testbed, J. Lightw. Technol. 33 (8) (2015) 1508–1514.
[27] J.-P. Vasseur, M. Pickavet, P. Demeester, Network recovery. The Morgan Kaufmann Series in Networking, 2004.
[28] J.M. Simmons, Optical Network Design and Planning, Optical Networks Series,
2nd ed., Springer, 2014.
[29] J.W. Suurballe, Disjoint paths in a network, Networks 4 (2) (1974) 125–145.
[30] DataCenterMap. [Online]. Available: 〈http://www.datacentermap.com〉.
[31] M. Klinkowski, K. Walkowiak, On advantages of elastic optical networks for
provisioning of cloud computing trafﬁc, IEEE Netw. 27 (6) (2013) 44–51.

Wireless Networks 3 (1997) 71–82

71

A new model for scheduling packet radio networks
Arunabha Sen and Mark L. Huson
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287, USA

Packet radio networks are modeled as arbitrary graphs by most researchers. In this paper we show that an arbitrary graph is an
inaccurate model of the radio networks. This is true because there exists a large class of graphs which will not model the radio networks.
Radio networks can be modeled accurately by a restricted class of graphs called the planar point graphs. Since the radio networks
can accurately be modeled only by a restricted class of graphs, the NP-completeness results for scheduling using an arbitrary graph as
the model, do not correctly reflect the complexity of the problem. In this paper we study the broadcast scheduling problem using the
restricted class as the model. We show that the problem remains NP-complete even in this restricted domain. We give an O(n log n)
algorithm when all the transceivers are located on a line.

1. Introduction
A packet radio network is a collection of radio transmitters and receivers (transceivers) located in a geographical
region. Associated with each transceiver is a transmission
range, which depends on its transmission power. A transceiver Tj located within the transmission range of another
transceiver Ti can receive the messages from Ti . The transceivers of the network share the communication medium
for message transfer. This gives rise to conflict situations
where two or more of the transceivers may want to use
the same shared medium simultaneously. Such conflicts
result in garbling and eventual loss of messages. Communication protocols are used to avoid such events. Such
protocols may be divided into two broad classes, static and
dynamic. In case of a static protocol an a priori assignment
of the communication resources, such as time, frequency or
code, is made among the transceivers to avoid contention.
Dynamic protocols have no a priori resource allocation, instead resource requests are met on demand. Time Division,
Frequency Division and Code Division multiple access (referred to as TDMA, FDMA and CDMA respectively) are
examples of the a priori assignment of resources. ALOHA
and CSMA are examples of dynamic protocols. Static allocation is well suited for a steady stream of traffic while
dynamic allocation is well suited for a bursty traffic.
In this paper we restrict our attention to static allocation
in general and TDMA in particular. However, it may be
noted that the results presented in this paper are also valid
for other static allocation schemes. In a packet radio network operating under a TDMA scheme, some transceivers
are allowed to transmit simultaneously. However, the geographic locations and the transmitting powers of these transceivers should be such that not only do they not cause interference to each other but also do not cause interference
at any other transceiver which may be within their transmission ranges. A schedule is a sequence of fixed length
time slots where a slot is assigned to a set of non-interfering
transceivers for transmission of messages. Schedule length
 J.C. Baltzer AG, Science Publishers

is measured by the number of time slots in a schedule.
A schedule is said to be optimal if it uses the minimum
number of slots.
The problems associated with the construction of an optimal schedule have been studied extensively by researchers
[1,7,8,12,18–20]. Most of these studies dealt with the construction of two different types of schedules, broadcast
schedules and link schedules, under two different types of
interferences, primary interference and secondary interference [18,19]. In a broadcast schedule each transceiver is
scheduled to ensure collision free transmission of messages
to all transceivers within its range. In link scheduling, the
transmission of a station is intended for only one specific
transceiver within its range and the schedule needs to ensure
that there are no collisions. Primary interference is said to
occur when a transceiver is expected to perform more than
one operation at the same time, such as receiving from two
different transmitters at the same time or transmitting and
receiving at the same time. Secondary interference occurs
when a transmission from a neighboring transmitter unwittingly interferes at the receiving end of a communication
between a transmitter and a receiver. Primary interference
can be of two types. In figure 1(a) the transceivers a and
b are within the transmission range of one another. In this
case if a and b start transmission at the same time, then
both the transceivers will be expected to transmit and receive simultaneously. We refer to this as type 1 primary
interference. In figure 1(b) the transceivers a and b are not
within the transmission range of one another, but there is a
third transceiver c which is within the transmission range
ra

rb
a

b

(a)

ra

rb
a

c

(b)

Figure 1. Type 1 and type 2 interferences.

b

72

A. Sen, M.L. Huson / Packet radio networks

of both a and b. In this case if a and b start simultaneous
transmissions then c will be expected to receive from both
a and b at the same time. We refer to this as type 2 primary
interference.

2. Prior work
For the purpose of constructing an optimal schedule all
prior research [1,7,8,12,18–20] modeled a packet radio network as a graph, where a node in the graph represents a
transceiver and there is a directed edge from the node vi to
the node vj if the transceiver j is within the transmission
range of the transceiver i. It may be noted that the resulting
graph is a directed graph in which the existence of an edge
from vi to vj does not necessarily imply the existence of
an edge from vj to vi . However, in a network where the
transmission range of all the transceivers are equal, existence of an edge from vi to vj will imply the existence of
an edge from vj to vi . We will refer to such a network as
an uniform range packet radio network and model it using
an undirected graph.
Even et al. [8] were among the earliest researchers to
model a network as an arbitrary undirected graph to study
the complexity of network testing under various conditions.
Although their interest was primarily in network testing,
under some conditions their testing problems were very
close to the optimal schedule construction problem. Using
a slightly different definition of interference they showed
that the optimal schedule construction problems were NPhard. Chlamtacet et al. [4,5] also studied the scheduling
problem of multi-hop radio networks using the same model.
Hajek and Sasaki [12] using the same network model
presented two polynomial time algorithms for link scheduling in a spread spectrum radio network. The ability of
spread spectrum modulation to allow multiple conversations to occur at the same time and place enabled the development of the polynomial algorithm, whereas in a single
frequency narrow band system, the problem turns out to be
an NP-hard scheduling problem [1]. Ogier in [17] presented more efficient algorithms for link scheduling than
the algorithms proposed by Hajek and Sasaki.
Arikan [1] used a directed graph to model packet radio
networks and showed that the decision problem regarding
the membership of a node in the capacity region (the set of
all origin-to-destination message rates that are achievable
via any arbitrary protocol) of the network is NP-hard.
Ramaswami and Parhi [20] using an arbitrary undirected
graph model of the network proved that the construction of
a minimum length schedule is an NP-complete problem.
The decision problem they proved to be NP-complete is
stated as follows: Given an undirected graph G = (V , E)
and an integer constant k, can this graph be colored using
at most k colors such that the node pairs that are connected by a path of length at most two have different colors? Ephremides and Truong in [7] essentially proved the
same NP-completeness results as Ramaswami and Parhi.

Although Bar-Yehudaet et al. in [2] commented that
most packet radio networks can be modeled as trees, Ramanathan and Lloyd [18,19] were the first to suggest the
use of a restricted class of graphs as network models. Since
it is not too difficult to see that not all packet radio networks
can be modeled as trees, they suggested the use of another
class of graphs – the planar graphs, as network models.
They showed that if the network can be modeled as a tree,
then the optimum schedule can be constructed in polynomial time. However, the problem remains NP-complete
even when the radio network is modeled as a planar graph
instead of an arbitrary graph.

3. Network model
The discussion in the previous section clearly shows
that, with the sole exception of [18,19], almost all prior research in this area used an arbitrary graph (either directed
or undirected) as the model of the radio network. Many
scheduling problems were shown by the researchers to be
NP-complete using this model. As indicated in the previous section in connection with Ramaswami and Parhi’s
research, most of the scheduling problems are formulated as
graph coloring problems. Broadcast scheduling problems
are generally set up as node coloring problems and the link
scheduling problem is set up as an edge coloring problem
[18,19]. These problems were shown to be NP-complete in
[1,7,8,18–20].
It is very important to realize that if a problem is NPcomplete in a certain domain, it does not imply that the
problem will continue to be NP-complete when the domain is substantially restricted. More specifically, NPcompleteness of a graph problem does not necessarily imply
the NP-completeness of the problem for a restricted class of
graphs. A classic example is the chromatic number problem, where we are required to find the minimum number of
colors necessary to color the nodes of the graph such that no
two adjacent nodes have the same color. It is well known
that this problem is NP-complete [9]. However, if we restrict the class of graphs to be connected and cycle-free,
then the problem becomes trivial. The chromatic number
of all non-trivial (a graph with more than one node) connected, cycle-free graphs, (i.e., trees) is two.
Models are designed to encapsulate the essential features
of the system under study. The use of an arbitrary graph
(directed or undirected) as the model of a radio network
fails to capture some essential features of the network.
At the first level of abstraction of the radio networks, we
think of the transceivers as some points (specified by their
(x, y)-coordinates) on a two dimensional plane. Associated
with each point is a range value, representing the transmission range of the transceiver. At the second level of abstraction, we construct a graph where each node represents a
point on the plane (transceiver) and there is a directed edge
from node vi to node vj , if the range ri , associated with
point pi exceeds or equals the Euclidean distance between

A. Sen, M.L. Huson / Packet radio networks

73

4

point graphs
5

3

space graphs

1
6

2
7

unit disk graphs
(a)

(b)

Figure 2. Relations between point graphs, space graphs and unit disk
graphs.

(c)

(e)

(d)

(f)

(g)

Figure 3. Forbidden subgraphs of planar point graphs.

the points pi and pj on the two dimensional plane. The
graph constructed in this manner captures all the pertinent
features of the radio network. This mechanism of constructing the graph model for a radio network was used by many
other researchers, notably [1,18–20]. Although, this graph
construction mechanism was used, the earlier researchers
assumed that any arbitrary graph could be constructed using this mechanism and based their NP-completeness proofs
of the scheduling problems on this assumption.
In this paper we show that the assumption is incorrect.
The class of graphs that will be generated by the above
construction mechanism forms a proper subset of all graphs.
Ramanathan and Lloyd [18,19] were the first to recognize
this possibility and, as a result, they restricted their attention
to planar graphs and trees. However, in this paper we
show that the graphs generated by the above construction
mechanism are not arbitrary graphs or planar graphs or
trees.
Point graphs are introduced in this paper for the purpose
of modeling packet radio networks. Given a set of points
P = (p1 , . . . , pn ) in an m-dimensional space, each specified
by an m vector (x1 , . . . , xm ) and a range ri associated with
each pi , the point graph constructed from this set of points
is a graph G = (V , E), where each node vi represent a
point pi and there is a directed edge from node vi to vj
if d(pi , pj ) 6 ri , where d(pi , pj ) is the Euclidean distance
between the points pi and pj . Planar point graphs are point
graphs constructed from the points on a two dimensional
plane (m = 2). Linear point graphs are point graphs of
points on a line (m = 1). Symmetric point graphs are point
graphs where for all nodes vi , vj , there is a directed edge
from vi to vj , if and only if there is a directed edge from vj
to vi . Otherwise, the graph is an asymmetric point graph.
Symmetric point graphs are drawn as undirected graphs by
replacing two oppositely directed edges by an undirected
edge.
It may be noted that the point graphs are generalizations
of space graphs and unit disk graphs introduced in [15]
and [6] respectively. If the range value ri associated with
a point pi is equal for all points then a point graph reduces
to a space graph. A space graph of dimension two is a unit
disk graph. The relationship between point graphs, space
graphs and unit disk graphs is shown in figure 2.

T3
T1

T1

T1
T2

T2

(b)

(a)

(c)

T4

T4
T3
T1

T4
T3

T5

T3
T1

T1
T2

T2

(d)

T5

(e)

T2

T
6

(f)

Figure 4. Impossibility of figure 3(a) being a planar point graph.

The location-range representation of an n node point
graph is a set of n tuples (li , ri ), 1 6 i 6 n, where li
represents the location of point pi and ri represents the
range associated with the point. If pi is a point in an mdimensional space, its location li is specified by an m vector
(x1 , . . . , xm ).
It is clear from the above definitions that planar point
graphs capture the pertinent features of a radio network.
We show that the modeling of a radio network using an arbitrary graph is inappropriate because planar point graphs
are a proper subset of the set of all graphs. We demonstrate
this by providing several graphs (figure 3) which are not
planar point graphs. These graphs are some of the forbidden subgraphs of planar point graphs, i.e., any graph that
contains a graph from figure 3 as node induced subgraph
cannot be a planar point graph. Next we explain why the
graph in figure 3(a) is not a planar point graph.
We assume that all transmitters are omnidirectional. We
draw a circle with the transmitter position at the center and
radius equal to the transmission range of the transmitter.
This circle represents the area covered by the transmitter.
We refer to this circle as the transmission circle.
Since the nodes of the graph correspond to the transceivers, transceiver 1 is located in a plane with its transmission circle as shown in figure 4(a). Since transceiver 2

74

A. Sen, M.L. Huson / Packet radio networks

(a)

(b)

Figure 5. Planar point graph and planar graph.

Similar explanations can be provided for all the graphs of
figure 3.
Ramanathan and Lloyd realized the inappropriateness of
modeling radio networks with arbitrary graphs and proposed the use of planar graphs and trees as the models.
However, we show that planar point graphs are distinctly
different from planar graphs and trees. Figure 5(a) shows a
planar point graph that is not a planar graph and figure 5(b)
shows a planar graph but not a planar point graph. Figure 6(a) shows a planar point graph that is not a tree and
figure 6(b) shows a tree but not a planar point graph. The
relations between arbitrary graphs, planar graphs, planar
point graphs and trees are shown in figure 7.

4. Complexity of broadcast scheduling

(a)

(b)

Figure 6. Planar point graph and tree.

Graphs
Planar Graphs

Planar Point Graphs

Trees
asymmetric

symmetric

Figure 7. Relations between planar point graphs, planar graphs and trees.

receives messages from transceiver 1, it must be within 1’s
transmission range. Moreover, as 2’s transmission reaches
1, 1 must be within 2’s transmission range. Figure 4(b)
shows 2’s transmission circle overlapped on the transmission circle of 1. Transceivers 1 and 3 are within one another’s transmission range but 2 and 3 are not within range.
A possible position of transmitter 3 and it transmission circle is shown in figure 4(c). Proceeding in this way, we show
the position of transceivers 1, 2, 3, and 4 in figure 4(d), the
position of transceivers 1, 2, 3, 4, and 5 in figure 4(e) and
the position of transceivers 1, 2, 3, 4, 5, and 6 in figure 3(f).
At this time it is clear that there is no room for the placement of transmitter 7, without violating the interconnection
pattern displayed in the graph shown in figure 3(a). We
thus conclude that figure 3(a) is not a planar point graph.

In the previous section we demonstrated the appropriateness of using planar point graphs as models of packet
radio networks. The optimal broadcast schedule construction (OBSC) problem can now be stated in terms of coloring
the nodes of a planar point graph. As discussed earlier, any
broadcast schedule must be free of primary interference of
type 1 and 2. In terms of the model, the problem can be
stated as follows:
Optimal Broadcast Schedule Construction (OBSC) problem.
INSTANCE: Given a planar point graph G = (V , E)
and an integer constant k.
QUESTION: Is there a partition V = V1 ∪ · · · ∪ Vk such
that if u and v are distinct nodes of some set Vi in the
partition then (i) there is no edge from u to v, (ii) there
is no edge from v to u and (iii) there is no node w in the
graph G such that there is an edge from u to w and from
v to w?
We will also refer to the OBSC problem as either distance-2 k-colorability of planar point graphs or
distance-2 coloring of planar point graphs.
A similar problem was considered by McCormick [16].
The distance-d chromatic number χd , of a graph G =
(V , E) is the smallest integer k for which the node set
V can be partitioned into V1 , . . . , Vk such that if u and v
are distinct nodes in some set Vi in the partition, then there
is no path from u to v in G of length d or less. He proved
that distance-d chromatic number problem for an arbitrary
graph is an NP-complete problem [14,16].
In this paper we show that the OBSC problem is NPcomplete. It may be noted that the NP-completeness of
the OBSC problem does not immediately follow from
McCormick’s results because his results are true for arbitrary graphs whereas the OBSC problem relates to one
specific class of graphs – the planar point graphs.
A geometric version of the distance-2 k-colorability of
planar point graphs can be stated as follows:

A. Sen, M.L. Huson / Packet radio networks

Optimal Broadcast Schedule Construction (OBSC) problem
(geometric version).
INSTANCE: Given (i) a set of points P = (p1 , . . . , pn )
in a 2-dimensional plane, each specified by their (x, y)coordinates, (ii) a range ri associated with each pi and (iii)
an integer constant k.
QUESTION: Is there a partition P = P1 ∪ · · · ∪ Pk
such that if u and v are distinct points in some set Pi of
the partition, then (i) d(u, v) > max(ru , rv ), where d(u, v)
represents the Euclidean distance between the points u and
v and (ii) there is no point w in the set P such that d(u, w) 6
ru and d(v, w) 6 rv )?
It is easy to see that the two versions of the OBSC problem are equivalent. Next we show that the OBSC problem
is NP complete.
We prove the distance-2 k-colorability of planar point
graphs NP-complete by proving the distance-2 3-colorability
of planar point graphs NP-complete.
Distance-2 3-colorability of planar point graph problem.
INSTANCE: Given a planar point graph G = (V , E).
QUESTION: Is there a partition V = V1 ∪ V2 ∪ V3 such
that if u and v are distinct nodes of some set Vi in the
partition then (i) there is no edge from u to v, (ii) there
is no edge from v to u and (iii) there is no node w in the
graph G such that there is an edge from u to w and from
v to w?
Theorem 1. The distance-2 3-colorability of planar point
graphs is NP-complete.
Proof. We prove the distance-2 3-colorability of planar
point graphs NP-complete by giving a polynomial time
transformation from the planar graph 3-colorability with
node degree at most 4 problem which was shown to be
NP-complete by Garey, Johnson and Stockmeyer in [10].
It is easy to see that the distance-2 3-colorability of planar point graphs is in NP, since a non-deterministic algorithm can be used to assign colors to the nodes of the graph
and check in polynomial time if nodes u and v are assigned
the same color then (i) there is no edge from u to v, (ii)
there is no edge from v to u and (iii) there is no node w in
the graph G such that there is an edge from u to w and v
to w.
We use the following result due to Valiant [6,21] in our
proof.
A planar graph with maximum degree 4 can be embedded in the plane using O(|V |) area in such a way that its
vertices are at integer coordinates and its edges are drawn
so that they are made up of line segments of the form x = i
or y = j, for integers i and j.
We also use a result due to Biedl and Kant [3] where
they present a linear time algorithm which produces an
embedding of graph G = (V , E) on a grid such that each
vertex is at a grid point and each edge is composed of
vertical and horizontal segments with at most two bends.

75

To prove the NP-completeness of the distance-2 3colorability of planar point graphs we need to generate an
instance of the problem which is a planar point graph. To
ensure that the graph is indeed a planar point graph we
go back one step further and generate from an instance of
the planar graph 3-colorability with node degree at most 4
problem, a set of points on the plane (specified by their
x, y coordinates) and a range value associated with each
point. The graph constructed from this set of points and
their associated range values will indeed be a planar point
graph.
We start with an arbitrary instance of a planar graph
with node degree at most 4, and use Biedl’s algorithm to
produce an embedding in the grid. We scale the embedding
by a factor of three to ensure that all pairs of points in the
embedding are separated by a distance of at least three units.
There will be two categories of points which will make
up the instance of the distance-2 3-colorability of planar
point graph problem. Corresponding to each embedded vertex of the planar graph on the grid, there will be a vertex
replacement point whose (x, y) coordinates will be specified by its position in the grid. Corresponding to each edge
of the planar graph (which is embedded in the grid by a
sequence of horizontal and vertical segments) there will be
a set of edge replacement points. The (x, y) coordinates of
the edge replacement points will be specified in the next
paragraph. There will be two different types of edge replacement points: A and B. The range value associated
with each vertex replacement point is 1 unit. Each type A
edge replacement point has a range of 0.25 units and each
type B edge replacement point has a range of 0.45 units.
The (x, y) coordinates of the edge replacement are computed as follows: Suppose u and v are two adjacent nodes
of the planar graph that makes up an instance of the planar
graph 3-colorability with node degree at most 4 problem.
Also suppose that in the embedding of the graph on a grid
the node u is located at the grid point (0,0) and the point
v at the point (0,1). Since we scale the embedding by a
factor of 3, after scaling, we may assume that u is located
at point (0,0) and v at point (0,3).
Type A edge replacement points corresponding to the
edge (u, v) of the planar graph will be located at coordinates
(0.8, −0.4) and (1, −0.3) with a type B point at (1.1, 0).
There will be two more type A points at (1.4, 0) and (1.6, 0)
followed by a type B point located at (2, 0). In case the
embedding of the edge between u and v takes more than
one segment of the grid then the pattern is repeated for each
additional segment with two more type A points located at
(x.4, 0) and (x.6, 0) and a type B point located at (x +
1, 0), where x is an integer number. For example if in
the embedding, u is located at the grid point (0, 0) and
the point v at the point (2, 0) then after scaling u will be
located at point (0, 0) and v at point (6, 0). There will
be vertex replacement points corresponding to the nodes u
and v at locations (0, 0) and (6, 0). There will be type A
edge replacement points at locations (0.8, −0.4), (1, −0.3),
(1.4, 0), (1.6, 0), (2.4, 0), (2.6, 0), (3.4, 0), (3.6, 0), (4.4, 0)

76

A. Sen, M.L. Huson / Packet radio networks

p1

v
(0,0)

u1

(1.6,0)

(1.1,0)

(1.4,0)

v1

(2,0)

(1,-.3)

Figure 11. Interconnection pattern in the planar point graph generated
from an edge of the planar graph when the adjacent nodes are connected
by 3 segments in the scaled grid embedding.

Figure 8. Edge replacement point positions relative to the start vertex v
and the first segment of the edge being replaced.
p1

u1

v1

w1
q1
u2 v2 w2

Figure 9. Complete edge replacement between vertex replacement points
v and w, 3 units apart in the scaled embedding.

(0, 0)

(1, -2)

(3, -6)

(a)

u 3 v3 w3

u s-1 vs-1 ws-1

Figure 12. Interconnection pattern in the planar point graph generated
from an edge of the planar graph when the adjacent nodes are connected
by s segments in the scaled grid embedding.

w

(0, 0)

q1
u2 v2 w2

(.8,-.4)

v

w1

(b)

Figure 10. Embedding of an edge in a grid before and after scaling.

and (4.6, 0). There will be type B edge replacement points
at locations (1.1, 0), (2, 0), (3, 0), (4, 0) and (5, 0).
Figure 9 shows the edge replacement for an edge between two vertex replacement points 3 units apart in the
scaled embedding.
In another example, assume the adjacent nodes u
and v of the planar graph in their grid embedding are
located at positions (0, 0) and (1, −2) (figure 10) and
the edge is embedded by three segments ((0, 0), (1, 0)),
((1, 0), (1, −1)) and ((1, −1), (1, −2)), after scaling u will
be in (0.0) and v will be in (3, −6) and the edge will
be embedded by nine segments as shown in figure 10.
The vertex replacement points will have coordinates (0, 0)
and (3, −6). Type A edge replacement points will be in
locations (0.8. − 0.4), (1, −0.3), (1.4, 0), (1.6, 0), (2.4, 0),
(2.6, 0), (3, −0.4), (3, −0.6), (3, −1.4), (3, −1.6), (3, −2.4),
(3, −2.6), (3, −3.4), (3, −3.6), (3, −4.4) and (3, −4.6).
Type B edge replacement points will be in locations (1.1, 0),

(2, 0), (3, 0), (3, −1), (3, −2), (3, −3), (3, −4) and (3, −5).
The edge replacement points are generated in such a
way that in the resulting planar point graph they create a
specific directed edge pattern. If p and q are two adjacent
nodes in the planar graph and in the grid embedding the
edge (p, q) is realized by one grid segment (before scaling),
then the edge replacement points are created in such a way
that the resulting planar point graph has the directed edge
pattern as shown in figure 11. The nodes p1 and q1 in the
planar point graph correspond to the vertex replacement
points corresponding to the nodes p and q of the planar
graph. The ui and vi nodes correspond to type A edge
replacement points for the edge (p, q) of the planar graph
and the wi nodes correspond to the type B edge replacement
points. If the edge (p, q) is realized by s grid segments in
the scaled embedding, then the edge replacement points
are created in such a way that in the resulting planar point
graph the directed edge pattern in one box of figure 11 is
repeated s − 1 times as shown in figure 12.
It may now be observed that in any distance-2 3-coloring
of the generated instance of the planar point graph, the
colors at nodes u1 and v1 should be different from each
other and also different from the node p1 . This forces the
node w1 to have the same color as the node p1 . In fact
if the scaled embedding of the edge (p, q) of the planar
graph is realized by s grid segments then all the wi nodes
(1 6 i 6 s − 1), are forced to have the same color as
the node p1 in the distance-2 3-coloring of the planar point
graph. This is shown in figure 13.
As figures 9 and 8 show, the combination of positions
and ranges of type A and type B edge replacement points
requires a scaling factor of 3.
To show that this is a viable replacement process two
properties must be maintained. First it must be possible to
make a “bend” in the edge without changing the arc pattern
shown in figure 11 (recall Biedl and Kant showed each edge
replacement will have at most two bends). A “bend” in the
embedded edge is a 90◦ turn, a change from vertical to
horizontal or horizontal to vertical trace.

A. Sen, M.L. Huson / Packet radio networks

77

v'

v

v'

v

v'

Figure 13. 3-coloring of vertex replacement point, v, and an edge replacement starting at that point. The type-1 and type-3 points are the
same color.

v'

v
Figure 15. The construction for a vertex, v, of degree 4 from the original
planar graph G. Each vertex, v0 has the same color as vertex v in a valid
distance-2 3-coloring of the constructed graph, G0 .

Figure 14. An edge construct with a 90 degree bend, showing that such
bends are possible without causing interference.

The second property is that the edge constructs must
allow each node of the planar graph to have four noninterfering edge replacements. Figure 15 shows one such
node.
For each edge of the planar graph in its scaled embedding in the grid with embedded segments of length s, there
will be 3(s − 1) points in the instance of the distance-2
3-colorability problem. If m is the maximum length of
any edge embedding of the planar graph, there are then
O(|V | + 3m|E|) nodes in the instance of planar point graph
produced by the transformation. The total time complexity
to produce the instance of distance-2 3-colorability problem
is a polynomial function of the size of the input.
Let G be an instance of the planar graph 3-colorability
with node degree 4 problem and G0 be the instance of
the planar point graph constructed from G using the above
construction rules. We need to show that G is 3-colorable
if and only if G0 is 3-colorable.
Suppose G is 3-colorable. Then we can assign colors to
the nodes of G0 in the following way: the nodes of G0 that
correspond to the vertex replacement points (corresponding
to the nodes of G) are assigned the same color as the corresponding node of G. If p and q are two adjacent nodes in
the planar graph and p1 and q1 are the corresponding nodes
in the planar point graph, then p1 has same color as p and
q1 has same color as q. In the planar point graph there
are a number of nodes between p1 and q1 corresponding to

the edge replacement points. If, ignoring the orientations
of the edges for a moment, we consider a path between
the nodes p1 and q1 , only one of these nodes will have
a neighboring node on this path which corresponds to a
type A edge replacement point. The other node will have a
neighboring node on this path which corresponds to a type
B edge replacement point. Suppose the node p1 has a type
A neighbor. The undirected path from p1 to q1 will then
be p1 followed by a type A neighbor, followed by a second
type A neighbor, followed by a type B neighbor and so on.
Without loss of generality, if we assume that the node p1
is assigned color 1, then its first type A neighbor may be
assigned color 2 and the second type A neighbor may be
assigned color 3. The next node in the path – the type B
neighbor may again be assigned color 1 without violating
the requirements of the distance-2 coloring. Proceeding this
way the last type B node in the path (neighbor of node q1 )
will be assigned color 1. Since the node q1 has the same
color as node q of graph G, the color of node q1 must be
different from color 1. (This is true because the color of p1
is the same as the color of p and the color of q1 is the same
as the color of q, p and q are adjacent nodes in G and G is
3-colorable.) It is clear that this coloring scheme will not
violate any distance-2 coloring requirement. Therefore, G0
is distance-2 3-colorable.
Now suppose that G0 is distance-2 3-colorable. Consider
the set of nodes of G0 that correspond to the vertex replacement points (note, there is a one-to-one correspondence between the nodes of G and the vertex replacement points).
Assign to the nodes of G the same colors as are assigned
to the corresponding nodes of G0 . Suppose, if possible, in
this assignment two adjacent nodes p and q of G are colored using the same color – a clear violation of the coloring
requirement. This implies that the corresponding nodes p1

78

A. Sen, M.L. Huson / Packet radio networks
1

2

2

1
4

3

3

(a)
1

2
3

4

(b)

4

(c)

- Vertex replacement point
- Type A edge replacement point
- Type B edge replacement point

Figure 16. Planar graph, its embedding in grid and the locations of vertex
and edge replacement points.

and q1 in G0 must have the same color. Due to the nature of
the construction of G0 , the only way this can happen without violating the requirements of a distance-2 coloring, is
if 4 different colors are used to color the nodes corresponding to the edge replacement points that appear between the
nodes p1 and q1 . Since G0 is distance-2 3-colorable, this is
impossible. Therefore G is 3-colorable.

An example of a planar graph, its embedding in a grid
and the locations of the A and B type edge replacement
points are shown in figure 16.
5. Optimal schedule for transceivers on a line
In this section we present an algorithm for the optimal
broadcast schedule construction problem when the transceivers are placed on a line. In the last section we showed
that the more general problem of optimal schedule construction when the transceivers are located in a two dimensional
plane is NP-complete. In this section we show that in this
restricted case where the transceivers are located on a line,
the problem can be solved using a polynomial time algorithm. The model of the radio network with transceivers
located on a line is a linear point graph introduced in section 3. The optimal schedule construction problem now reduces to distance-2 coloring problem of linear point graph.
A variation of McCormick’s [16] definition of distance-d
chromatic number of a graph is used to define distance-2
chromatic number of a directed graph. The distance-2 chromatic number of a directed graph is the smallest integer k
for which the node set V can be partitioned into V1 , . . . , Vk
such that if u and v are distinct nodes in some set Vi in the
partition, then (i) there is no directed edge from u to v, (ii)
there is no directed edge from v to u and (iii) there is no

node w in the graph G such that there is a directed edge
from u to w and v to w. The following algorithm computes
the distance-2 chromatic number of a linear point graph.
The location-range representation of the linear point
graph is provided as the input. Since in this case the location is specified by a scalar, the location-range representation is a set of n ordered pairs, where the first number
provides the location of the point (transceiver) and the second number provides the range.
Our algorithm is based on the interval graph coloring
algorithm used for the VLSI channel assignment by Gupta,
Lee and Leung in [11].
First, from the location and the range of a point a transmission interval associated with the point is computed. If
a point i is located at pi and has a range ri , then its transmission interval spans from pi − ri to pi + ri . Thus corresponding to n nodes of the graph we have 3n points, of
which n are left end points of the transmission intervals, n
are right end points, and n are positions of the points on the
line. The points are numbered from 1 through n. Without
loss of generality we assume that pi < pj if i < j. The
left and right end points of the point i will be denoted by
Li and Ri respectively.
The algorithm scans the 3n points from left to right.
Suppose Π is a permutation of the numbers 1, . . . , n such
that LΠ(1) 6 LΠ(2) 6 · · · 6 LΠ(n) . The nodes of the graph
are colored in the following order: Π(1), Π(2), . . . , Π(n).
As soon as the left endpoint of the transmission interval for
point i is encountered during the left to right scan of the
3n points, the point i is put on a list of points waiting to
be colored. All the points in this waiting list are assigned
a color one after another as soon as the position of a point
is encountered during the scan. The nodes Π(s), Π(s + 1),
. . . , Π(t) are assigned colors when the algorithm while
scanning the points from left to right reaches the first point
q at position pq such that pq−1 < LΠ(s) 6 LΠ(s+1) 6 · · · 6
LΠ(t) 6 pq . At this time the nodes Π(s), Π(s+1), . . . , Π(t)
are all assigned different colors. When the right endpoint
of an interval is encountered, the color used to color the
corresponding interval is freed up and is made available to
color the next node.
Algorithm for distance-2 coloring of linear point graphs
Step 1: Sort the 3N endpoints so zi 6 zi+1 6 · · · 6 z3N .1
Step 2: COU N T ER ← 0, M AXCOLOR ← 0
COLOR ← 1, N U M T OP ROC ← 0
Step 3: for i = 1 to N do:
N EXT (i) ← i + 1
Step 4: for j = 1 to 3N do:
if zj is a left endpoint xk
N U M T OP ROC ← N U M T OP ROC + 1
W AIT IN G(N U M T OP ROC) ← k
else if zj is a right endpoint yk
COU N T ER ← COU N T ER − 1
T EM P ← ASGN (k)
N EXT (T EM P ) ← COLOR
COLOR ← T EM P

A. Sen, M.L. Huson / Packet radio networks

else zj is a position point pk
for i = 1 to N U M T OP ROC do:
COU N T ER ← COU N T ER + 1
M AXCOLOR ← max{COU N T ER,
M AXCOLOR}
ASGN (W AIT IN G(i)) ← COLOR
COLOR ← N EXT (COLOR)
N U M T OP ROC ← 0
Theorem 2. The algorithm for distance-2 coloring of linear
point graphs optimally colors the nodes of the graph.
Proof. From the definition of distance-2 chromatic number of a graph, two distinct nodes u and v should not be
assigned the same color if any one of the following conditions holds: (i) there is a directed edge from u to v, (ii)
there is a directed edge from v to u (iii) there is a node w
in the graph G such that there is a directed edge from u to
w and a directed edge from v to w.
First, we show that the algorithm colors the nodes of the
graph in such a way that none of the conditions (i), (ii) or
(iii) is violated.
Case 1: Consider two nodes i and j of the graph with
i < j (from our assumption this implies pi < pj ). Suppose
that there is a directed edge from i to j. This implies
Ri > pj . Suppose a color has been assigned to the node i
and a color is about to be assigned to the node j. The
algorithm in its scan of points from left to right at this
time has encountered the points Li and Lj but has not
encountered Ri . In this situation the algorithm will assign a
color to j which is different from the one that was assigned
to i.
Case 2: Consider nodes i and j of the graph with i < j
and suppose that there is a directed edge from j to i. This
implies Lj < pi . Suppose a color has been assigned to the
node i and a color is about to be assigned to the node j.
The algorithm in its scan of points from left to right at
this time has encountered the points Li and Lj but has not
encountered Ri . In this situation the algorithm will assign a
color to j which is different from the one that was assigned
to i.
Case 3: Consider nodes i and j of the graph with i < j
and suppose that there is no directed edge from i to j or
from j to i but there is a node k in the graph such that there
is a directed edge from i to k and a directed edge from j
to k. This implies Ri > pk and Lj < pk . Suppose a color
has been assigned to the node i and a color is about to be
assigned to the node j. The algorithm in its scan of points
from left to right at this time has encountered the points Li
and Lj but has not encountered Ri . In this situation the
algorithm will assign a color to j which is different from
the one assigned to i.
Therefore the algorithm colors the nodes of the linear
1

If zi = zi+1 = · · · = zj , the left endpoints precede position points
which precede right endpoints.

79

point graph without violating the distance-2 coloring requirements (i), (ii) and (iii).
If χopt is the optimum number of colors necessary to
distance-2 color the nodes of the linear point graph then
χopt > max(din (v)) over all nodes of the graph, where din (v)
is the number of incoming edges to node v. This must hold
to satisfy requirement (iii) of the distance-2 coloring of the
graph.
We define the crossing number of a point i, c(i) to be the
cardinality of the set of transmission intervals that span over
the point i (i.e., c(i) = |S(i)|, where S(i) = {j | Lj 6 pi 6
Rj }). If χ is the number of colors used by the algorithm
to distance-2 color the nodes of the linear point graph, then
χ = max(c(i)), 1 6 i 6 n, where n is the number of nodes
in the graph. If χ = c(k) for some k, 1 6 k 6 n, the
indegree of the node k of the graph is c(k) − 1.
Since this algorithm distance-2 colors using χ colors and
there exists a node in the graph whose indegree is χ − 1, no
algorithm can distance-2 color this graph with fewer than χ
colors. This implies χopt = χ and this algorithm optimally
distance-2 colors the nodes of the linear point graph. 
The complexity of the algorithm is O(n log n) which is
determined by the sorting time of the 3n points.

6. Approximate schedules for transceivers on a plane
In the previous sections we showed that the optimal
schedule construction is possible in polynomial time if the
transceivers are located on a line, while the problem is NPcomplete if the transceivers are on a plane. We develop the
idea of an approximation algorithm for the NP-complete
problem from this observation. The idea is to project the
points from a 2-dimensional plane to 1-dimension (i.e., a
line) and then use the optimal algorithm presented in section 5 to construct the schedule. If u and v are two points on
the plane and u0 and v 0 the corresponding projected points
on a line, then d(u0 , v 0 ) 6 d(u, v), where d(u, v) is the
Euclidean distance between the points u and v. For this reason, it may be possible that although d(u, v) > max(ru , rv ),
d(u0 , v 0 ) 6 max(ru , rv ), where ru is the range associated
with point u. This implies that the schedule constructed
from the projected points may have more slots than the optimum schedule. So this method will only produce an approximate schedule. The difference in the number of slots
used in the approximate schedule to the optimum schedule
is the error introduced in the approximation process. Obviously, we would like to make the error as small as possible.
Consider a set of three uniform range transceivers located
at points (1, 1), (2.1, 1) and (1.5, 4). The range of the transceivers is 1. The optimum schedule for these transceivers
will have only one slot. If the points are projected on the
line X-Y as shown in figure 17 the schedule will have
three slots, but if they are projected on the line S-T the
schedule will have two slots. Thus the error will clearly
depend on the choice of the line on which the points are

80

A. Sen, M.L. Huson / Packet radio networks
T

A

(1.5, 4)

A2

A1
A3

a

A4

j

i

c

b

D

C
d
(1, 1)

(2.1, 1)

S
X

Y

B
Figure 17. Projection of the points on line X-Y and S-T .

being projected. The projection line that will introduce
the least amount of error will be called the best projection
line.
6.1. Computation of best projection line
In this section we describe a technique for finding the
best projection line from a given set of n points on a two
dimensional plane specified by their (x, y) coordinates and
the range values associated with them.
Consider a circle enclosing the n points as shown in
figure 18 and two points i and j inside the circle. If the
Euclidean distance between i and j, d(i, j) 6 min(ri , rj )
then in the planar point graph constructed from these set
of points, there is a directed edge from the node vi to
node vj and a directed edge from vj to vi . If i0 and j 0
are the projected points on a line corresponding to i and
j, then d(i0 , j 0 ) 6 d(i, j). As a result in the linear point
graph constructed from the set of projected points there is
a directed edge from the node vi0 to node vj0 and a directed
edge from vj0 to vi0 . In this situation the projection line
has no impact on the connection pattern between the nodes
of the graph. For this reason those pairs of points (i, j)
with d(i, j) 6 min(ri , rj ) need not be considered during
the computation of the best projection line.
Consider a pair of points (i, j) such that d(i, j) >
min(ri , rj ). Draw a line passing through the points i and j
as shown in figure 18. If the points i, j are projected on
any line parallel to the line passing through i and j then
the distance between the projected points will be exactly the
same as the distance between the original points. Therefore
this projection line is the best projection line for this pair of
points. If the points i, j are projected on any line perpendicular to the line passing through i and j then the distance
between the projected points will be zero. Therefore this
projection line will be the worst projection line for this pair

Figure 18. Valid projection range for the pair of points i, j.

of points. Draw four tangents to the circle, two parallel
and two perpendicular to the line through i, j as shown in
figure 18. Suppose the parallel tangents touch the circle at
points A and B and the perpendicular tangents touch the
circle at points C and D. A projection line which is tangent
at point A is the best possible projection line for the points
i, j and a projection line which is tangent at point C is the
worst possible projection line for the points i, j. However,
a projection line which is a tangent (or a line parallel to it)
at point A1 or A2 may still be considered a good projection
line if the distance between the projected points i0 and j 0
is such that d(i0 , j 0 ) > min(ri , rj ). Moving away from the
point A on the circumference of the circle, let A3 and A4
be the furthest points from A on the circumference such
that projected points i0 , j 0 on the tangents drawn at these
points will still satisfy the condition d(i0 , j 0 ) > min(ri , rj ).
This part of the circumference of the circle from the point
A3 to A4 will be referred to as the valid projection range
for the points i, j.
For every pair of points (i, j) such that d(i, j) >
min(ri , rj ) we can construct a valid projection range as
shown figure 19. In figure 19 the arc i0 -j 0 represents the
valid projection range for the pair of points i and j. From
these set of arcs, we find that part of the circumference of
the circle, where there is the largest number of overlap of
valid projection ranges. In figure 19 the part of the circumference between the points A and B is this part. This part of
the circumference will be referred to as the best projection
range. If we draw a tangent at a point lying within the best
projection range and project the n points on this line, then
it will introduce the fewest occurrences of the condition
that d(i0 , j 0 ) 6 min(ri , rj ) although d(i, j) > min(ri , rj ).
In graph terms, if the points are projected on such a line,
there will be fewest occurrences of the situation that the
linear point graph has an edge between the nodes i and j,
although there is no such edge in the corresponding planar
point graph. A tangent to the circle at a point within the

A. Sen, M.L. Huson / Packet radio networks

Table 1
Algorithm comparison showing average percent difference from the best
coloring algorithm for a series of 150 trials. Note: the results reflect the
average of trials at various ranges, therefore the “best” algorithm may not
have a value of 0% as it’s entry.

2'
6'

3'

7'

B

4'

A

Algorithm
2

1
3

100

Number of transceivers
200 300 400 500

600

7

1'

5'

81

6

4
8

5
8'

Figure 19. Valid projection ranges for pairs of points and the best projection range.

best projection range will be a best projection line for the
given set of n points.
6.2. Experimental evaluation
We have done extensive experimentation to evaluate the
performance of the approximate algorithm suggested in the
previous section. We refer to it as the linear projection algorithm. The performance of this algorithm was compared
with some other approximation algorithms using techniques
such as (i) depth-first search, (ii) breadth-first search, (iii)
random coloring, (iv) static maximum degree first [18] and
others. The main difference between these algorithms with
the linear projection algorithm is that these algorithms use
topological information for coloring whereas the projection algorithm uses geometric information. However, the
projection algorithm has one disadvantage. As the points
are projected from the two dimensional plane to one dimension there is a certain amount of information lost. As
a result two nodes which are not adjacent in the original
graph may become adjacent in the graph constructed from
the projected points. The number of such false adjacencies
between the nodes turned out to be substantial in some of
the experiments. In such cases, the projection algorithm
performed poorly in comparison with other algorithms.
Table 1 gives a rough comparison of the performance
of various algorithms. These results reflect the average
performance of each algorithm compared to the algorithm
which had the best average performance for the same sets
of test data. Each entry in the table represents 150 test
cases, where each test case represents uniformly distributed
transceivers of varying ranges in a 200-by-200 grid. The
uniform distribution leads to a relatively poor performance
of the linear projection algorithm due to the large number
of false adjacencies introdued by the projection.
The projection algorithm performed better than or at
least as well as the other algorithms when the number of
false adjacencies were low. Limitations on the size of this

Linear projection
183% 207% 222% 231% 236% 244%
Linear projection ordering
7% 12% 15% 16% 18% 19%
Linear projection ordering 2
4%
5%
5%
5%
6%
6%
Topological algorithm #1
4%
8% 10% 11% 13% 14%
Topological algorithm #2
13% 19% 23% 24% 25% 28%
Topological algorithm #3
6% 11% 10% 12% 12% 13%
Topological algorithm #4
1%
2%
2%
3%
2%
3%
Greedy matrix (random)
6%
9% 11% 12% 12% 13%
Breadth-first
0%
1%
0%
1%
0%
1%
Depth-first
2%
4%
5%
6%
6%
7%
Static maximum degree-first
1%
3%
4%
5%
5%
7%
Random
6% 10% 10% 12% 12% 14%
Maximum degree-first
2%
5%
5%
6%
6%
8%
Minimum degree-last
7%
3%
2%
2%
1%
1%
Distance-2 min degree-last
8%
5%
2%
1%
1%
0%

paper prevents us from discussing the experimental results
in greater detail. For further details the reader is referred
to [13].
7. Conclusions
In this paper we have shown that an arbitrary graph is
not an accurate model of a packet radio network. This is
true because there exists a large class of graphs which will
never be generated from radio networks. We have defined
a new class of graphs called point graphs. A subset of point
graphs called planar point graphs, captures the essential features of a packet radio network and, as such, is most appropriate for modeling the network. As the NP-completeness
of a problem in a certain domain, does not necessarily imply the NP-completeness of the problem in a substantially
restricted domain, the NP-completeness results for scheduling a radio network using an arbitrary graph as a model,
do not correctly reflect the complexity of the problem. In
this paper we have studied the broadcast scheduling problem using the planar point graphs as the model. We have
shown that the problem remains NP-complete even in this
restricted domain. We have presented an O(n log n) algorithm for the case where all the transceivers are located on
a line. We have also presented the idea of an approximate
algorithm for the NP-complete problem and discussed the
results of our experimental evaluation of the algorithm.
References
[1] E. Arikan, Some complexity results about packet radio networks,
IEEE Trans. Inform. Theory 30 (July 1984) 681–685.
[2] R. Bar-Yehuda, A. Israeli and A. Itai, Multiple communication in
multi-hop radio networks, in: Proc. 8th Ann. ACM Sympos. Princ.
Distrib. Comput. (ACM, 1989) pp. 329–338.
[3] T. Biedl and G. Kant, A better heuristic for orthogonal graph drawing, in: Proc. 2nd Ann. European Sympos. Algorithms (1994).

82

A. Sen, M.L. Huson / Packet radio networks

[4] I. Chlamtac and S. Kutten, A spatial reuse tdma/fdma for mobile multihop radio networks, in: INFOCOM (IEEE, March 1985)
pp. 389–394.
[5] I. Chlamtac and A. Lerner, A link allocation protocol for mobile
multi-hop radio networks, in: GLOBECOM (IEEE, 1985) pp. 238–
242.
[6] B. N. Clark, C. J. Colbourn and D. S. Johnson, Unit disk graphs,
Discrete Math. 86 (1990) 165–177.
[7] A. Ephremides and T. V. Truong, Scheduling broadcasts in multihop
radio networks, IEEE Trans. Communications 38 (April 1990) 456–
461.
[8] S. Even, O. Goldreich, S. Moran and P. Tong, On the NPcompleteness of certain network testing problems, Networks 14
(1984) 1–24.
[9] M. R. Garey and D. S. Johnson, Computers and Intractability:
A Guide to the Theory of NP-Completeness (Freeman, 1979).
[10] M. R. Garey, D. S. Johnson and L. J. Stockmeyer, Some simplified
NP-complete graph problems, Theoret. Comput. Sci. 1 (1976) 237–
267.
[11] U. I. Gupta, D. T. Lee and J. Y.-T. Leung, An optimal solution for the
channel–assignment problem, IEEE Trans. Computers 28 (November
1979) 807–810.
[12] B. Hajek and G. Sasaki, Link scheduling in polynomial time, IEEE
Trans. Inform. Theory 34 (September 1988) 910–917.
[13] M. L. Huson, A new model for scheduling radio networks, Ph.D.
thesis, Arizona State University, Tempe, AZ (August 1995).
[14] D. S. Johnson, The NP-completeness column: An ongoing guide,
J. Algorithms 3 (1982) 182–195.
[15] H. Maehara, Space graphs and sphericity, Discrete Appl. Math. 7
(1984) 55–64.
[16] S.T. McCormick, Optimal approximation of sparse Hessians and its
equivalence to a graph coloring problem, Technical Report SOL 8122, Stanford University, Department of Operations Research (1981).
[17] R. Ogier, A decomposition method for optimal link scheduling, in:
Proc. 24th Allerton Conf. (October 1986) pp. 822–823.
[18] S. Ramanathan and E. L. Lloyd, Scheduling algorithms for multi-hop
radio networks, in: SIGCOM (ACM, 1992) pp. 211–222.

[19] S. Ramanathan and E. L. Lloyd, Scheduling algorithms for multihop radio networks, IEEE/ACM Trans. Networking 1 (April 1993)
166–172.
[20] R. Ramaswami and K. K. Parhi, Distributed scheduling of broadcasts
in a radio network, in: INFOCOM (IEEE, 1989) pp. 497–504.
[21] L. G. Valiant, Universality considerations in VLSI circuits, IEEE
Trans. Computers 30 (February 1981) 135–140.

Arunabha Sen received his bachelor’s degree
in electronics and tele-communication engineering from Jadavpur University, Calcutta, India, and
a Ph.D. in computer science from University of
South Carolina, Columbia, SC, in 1987. He is an
Associate Professor of Computer Science and Engineering at Arizona State University since 1993.
His research interests are in the area of graph theoretic optimization problems in networking and
VLSI domains.
E-mail: arunabha.sen@asu.edu
Mark L. Huson received a B.S. in computer science from the University of Tulsa in 1985, a M.S.
in systems management from the University of
Southern California in 1988, a M.S. in computer
science from the Air Force Institute of Technology
in 1989, and a Ph.D. in computer science from
Arizona State University in 1995. He is an Assistant Professor of Computer Science at the United
States Air Force Academy. A member of ACM
since 1988, he is also a member of IEEE, IEEE
Communications Society, and AFCEA. He has been active in military
command control, communications and computing for the last 14 years.
His research interests include computing theory, algorithms, information
warfare and electronic warfare.
E-mail: mhuson@cs.usafa.af.mil

Energy Minimization using a Greedy Randomized Heuristic for the
Voltage Assignment Problem in NoC
Pavel Ghosh and Arunabha Sen
Department of Computer Science and Engineering
Arizona State University, Tempe, AZ 85281
Email: {pavel.ghosh, asen}@asu.edu

Abstract— Scaling down the voltage levels of the
processing elements (PEs) in a Network-on-Chip
(NoC) can significantly reduce the computation energy
consumption with an overhead of the level shifters
between two adjacent PEs operating at two different
voltage levels. The objective of the voltage assignment problem in the NoC domain is to assign a voltage
level to each PE within its allowable voltage range
such that the overall energy consumption, due to
the processing of the PEs and the level shifters, is
minimized subject to the task deadline constraints.
In this paper, we formulate the voltage assignment
problem as a Quadratic Programming (QP) and reduce it to an Integer Linear Program (ILP). A greedy
randomized heuristic is then proposed for solving
the voltage assignment problem. Experimental results
based on the E3S benchmark suite show the quality
of our proposed heuristic as for all the benchmark
applications it finds solutions close to the optimal
ones in negligible amount of time. The effectiveness
of the approach of using multiple voltage levels for
the PEs in energy minimization is also reflected from
the experimental results.

I. I NTRODUCTION
In recent years multi-core System-on-Chip(SoC)
based embedded systems design has become extermely challenging due to the increasing complexities in processor and semiconductor technologies.
Due to increasing requirements in performance,
scalability and flexibility, a shared bus communication infrastructure for SoC is no longer adequate.
Network-on-Chip(NoC) architectures provide an alternative to the bus-based communication mechanism that can meet the challenging requirements of
performance, scalability and flexibility. As the number
of PEs on a SoC and the data traffic between them
continue to grow, energy minimization subject to
performance constraint has become one of the most
important objectives. The flexibility of operating the
PEs at multiple voltage levels and the corresponding
level shifters between adjacent PEs make the energy
minimization problem even more complicated.

978-1-4244-2596-9/08/$25.00 ©2008 IEEE

79

Several factors contribute to performance and energy consumption during application execution: (i)
mapping of the application tasks to the PEs, (ii)
mapping of the PEs to the routers of the NoC architecture, (iii) traffic movement on the NoC architecture
- routing of data paths and (iv) choice of operating
voltages of the PEs. The solution to the problems
addressing factors (i), (ii) and (iii) can be obtained
using several available techniques in the literature
[1]–[3]. There is still a degree of freedom available for energy minimization in terms of selecting
appropriate voltage levels for the PEs. Given the
application deadline and the communication latency
due to inter-task data transfer, the deadlines for the
individual tasks can be obtained. Operating a PE
at a lower voltage level slows down the execution
of the corresponding task on the PE. At the same
time, it helps reducing a significant amount of energy consumption provided it does not inhibit the
corresponding task to finish within its deadline. Thus,
from the task deadlines we can calculate a list of
allowable voltage levels for each PE, such that the
corresponding task can be executed within the specified deadline on each of these voltage levels. When
the PEs are allowed to operate at different voltages,
a certain amount of energy will be consumed by the
generation of additional clock signals, voltage level
converters and mixed clock-mixed voltage FIFOs
used by the level shifters connecting adjacent PEs
in the NoC architecture. The NoC mesh architecture
with level shifters between adjacent routers is shown
in Fig. 1.
In this paper we focus on the problem of assigning
the operating voltages of the PEs, such that the (a)
energy consumption due to the processing of the
PEs and (b) the energy consumption at the level
shifters between adjacent PEs operating at different
voltage levels, is minimized. We make the assumption that the application tasks have already been

Fig. 1.

Level Shifters between adjacent PEs

mapped to the available PEs using the task mapping
algorithm described in [1] and the PEs have been
mapped and the traffic have been routed on the NoC
nodes and links, respectively, using the algorithm
in [2] considering regular mesh architecture. The
authors in [3], have considered the operation of PEs
of an NoC at multiple voltage levels, but in their case
the voltage levels of individual PEs have already
been determined. So, they do not consider the scope
of energy optimization by changing the voltage levels
of PEs subject to task deadline constraints. In [4],
the authors have considered the problem of voltagefrequency islanding of NoC. Each iteration of their
technique involves solving a non-linear program formulation and thus the computation time of the overall
execution of the algorithm can grow substantially
with the increasing size of the input instance. The
contributions of this paper are as follows:
•
•
•
•

Modeling of the voltage assignment problem as
a Quadratic Programming (QP) problem.
Elimination of the quadratic terms to reduce it
to an Integer Linear Program (ILP).
Proposing an efficient greedy randomized
heuristic to perform the voltage assignment.
Evaluation of the heuristic solution using realistic benchmark data (E3S benchmark suite [5]).

In this paper, we have assumed a regular mesh
NoC architecture. Each router has 5 ports, one used
for connecting it to a PE and the other four being
available for connection to the neighboring routers.
The techniques proposed in this paper can be used
with any other NoC architectures as well. The task
deadline calculation from the application deadline is
performed as pre-processing of the input instance.
In order to handle the increasing complexities in
the SoC designs, the necessity for NoC has been

80

discussed by several researchers [6], [7]. Energyaware mapping of PEs to the router nodes on an
NoC satisfying the performance requirements was
proposed in [8]. Techniques for energy minimization by mapping of PEs to NoC router nodes and
mapping of data paths on the NoC links have been
considered in [2], [9], [10]. In this paper, we focus
on the problem of voltage assignment to the PEs
with an objective of energy minimization. The voltage
assignment problem is solved at the design phase
of the system. We do not consider the run-time dynamic voltage scaling in this paper as task deadlines
and other parameters specified as part of the input
do not change during run-time.
The rest of the paper is organized as follows.
We formulate the voltage assignment problem as an
energy minimization problem subject to performance
constraints in Section II. Quadratic Program (QP)
and Integer Linear Program (ILP) is developed for
optimal solution in Section III. An efficient heuristic based on greedy randomization is developed in
Section IV. In Section V, we use E3S benchmark
suite [5] for experimental evaluation of our heuristic
solution. Section VI concludes the paper.
II. P ROBLEM F ORMULATION
In this section, we provide a formal definition of
the voltage assingment problem. An input instance
of the problem consists of:
•

•

•

•
•

•
•

An undirected N × N mesh architecture graph
GR (VR , ER ) where number of nodes n = N 2
and number of edges m = 2N (N − 1).
Set of processing elements (PEs) P =
{p1 , p2 , . . . , pn } attached with each of the mesh
router nodes as shown in Fig. 1.
Set of tasks T = {t1 , t2 , . . . , tn }, where ti is
executing on PE pi for i = 1 . . . n. Some of these
may be idle tasks as the number of tasks in the
application may be less than the number of PEs
available on the NoC.
Task deadlines {d1 , d2 , . . . , dn }, where di is
deadline for task ti ∈ T for i = 1 . . . n.
	
List of voltage levels Li = vi1 , vi2 , . . . , vini
associated with each PE pi , such that execution
time of ti on pi at voltage level v is ≤ di , ∀v ∈ Li .
ηiv = energy consumption of PE pi at voltage
level v ∈ Li for i = 1 . . . n.
kl
αij
= energy consumption of the level shifter,
where k ∈ Li and l ∈ Lj , for two adjacent PEs
pi and pj , i.e., {ri , rj } ∈ ER .

Output of the problem produces a voltage assignment function M (pi ) = vj , where vj ∈ Li , ∀i =
{1, . . . , n}, such that the following energy consumption function is minimized:
n
X
X
M (p )M (pj )
M (p )
ηi i +
αij i
i=1

yijkl defined as follows:

1, if xik = 1 and xjl = 1
yijkl =
0, otherwise
This condition can be ensured by the following
two sets of constraints:

{ri ,rj }∈ER

where the first and second expression corresponds
to the energy consumption of the PEs, and the energy consumption of the level shifters, respectively.
The list of allowable voltage levels Li associated
with each PE pi is calculated using the task deadlines {d1 , d2 , . . . , dn }. The task deadline values will
not be further required in either the optimal solution
or the heuristic for the voltage assignment. This
is because the allowable voltage levels associated
with a PE is defined to be voltage levels operating
at either of them the PE will be able to finish the
corresponding task within its specified deadline.
III. O PTIMAL S OLUTION FOR VOLTAGE
A SSIGNMENT P ROBLEM
In this section, first we formulate the voltage
assignment problem as a Quadratic Program (QP),
and then we eliminate the quadratic terms to convert
it to an Integer Linear Program (ILP). The following
decision variable is used in the QP formulation.

1, if M (pi ) = k, where k ∈ Li
xik =
0, otherwise
The objective of the voltage assignment problem
is to minimize the energy consumption, i.e.,
Obj: minimize (E1 + E2 )
where E1 is the computation energy consumption
on the PEs, and E2 is the energy consumption on
the level shifters.
X
E1 =
ηik xik
i=1...n,∀k∈Li

E2 =

X

kl
αij
xik xjl

∀eij ∈ER ,∀k∈Li ,∀l∈Lj

subject to the constraint that exactly one voltage
level has to be assigned for each PE, i.e.,
X
xik = 1
∀pi ∈ P :
k∈Li

Now we convert this QP to a corresponding ILP
by eliminating the quadratic term in the expression
of E2 . For this we introduce a new decision variable

81

∀eij ∈ ER , ∀k ∈ Li , ∀l ∈ Lj :
xik + xjl ≥ 2yijkl ,and
xik + xjl − 1 ≤ yijkl
Thus, the objective function in the corresponding
ILP can be stated as:

E=

X

i=1...n,∀k∈Li

Obj: minimize E
X
ηik xik +

kl
αij
yijkl

∀eij ∈ER ,∀k∈Li ,∀l∈Lj

This energy expression is used to calculate the
cost the solution achieved by solving the ILP or from
the greedy heuristic in section IV and V. This is
similar to the formulation of the quadratic assignment
problem in [11]. Computationally, it is one of the
most difficult combinatorial optimization problems
and has been listed to be one of the known NP-hard
problems in [12], page 218.
IV. H EURISTIC S OLUTION FOR VOLTAGE
A SSIGNMENT P ROBLEM
In this section, we describe our proposed greedy
randomized heuristic for the voltage assignment
problem. The greedy characteristic of the heuristic
is associated with a greedy function based on the
kl
values, for ri , rj ∈ VR , {ri , rj } ∈ ER , k ∈
ηik and αij
Li , l ∈ Lj , whereas the randomized characteristic of
the heuristic is associated with a selection parameter
β, whenever selection of a value from a list sorted by
the greedy function is required. Idea of the heuristic
is based upon a greedy randomized adaptive search
procedure proposed for the quadratic assignment
problem in [13]. The algorithm runs in an iterative
fashion. After each iteration, the current solution
found in this iteration is compared with the best
solution found so far and updated accordingly. The
details of the algorithm are as follows:
After initialization of the best solution found so far
(indicating no solution has yet been found) in step 1,
in step 2 of the algorithm,
the list F consisting of the

kl
kl
, for all edges eij of the mesh
= ηik + ηjl + αij
fij
graph GR and all voltage levels k ∈ Li and l ∈ Lj
is calculated. The cardinality of the set F is denoted

Algorithm 1 Greedy Randomized Heuristic
Input: N × N mesh GR (VR , ER ) with n = N 2 nodes and m =
2N (N −1) edges; Set of PEs P = {p1 , . . . , pn } with pi being
attached with mesh 
router node	ri ∈ VR ; Allowable list of
n
voltage levels Li = vi1 , . . . vi i , for each pi ∈ P ; ηik , for
pi ∈ P, k ∈ Li ; αkl
,
for
all
e
= {ri , rj } ∈ ER , where k ∈
ij
ij
Li , l ∈ Lj ; Selection parameter β and Number of iterations
numiter.
Output: M (pi ) = k ∀pi ∈ P , where k ∈ Li
1: Initialize cost(bestSolution) = ∞
kl =
2: ∀eij ∈ ER , ∀k ∈ Li , ∀l ∈ Lj , calculate list F of fij



k
l
kl
2
ηi + ηj + αij values; a = |F | = O n
3: Sort list F
4: for (iter = 1 to numiter) do
5: Γ = N U LL /*Set of PEs already assigned*/
kl from the smallest ⌊βa⌋ values of F
6:
Select randomly fij
7:
Γ = Γ ∪ {i, j}
8:
Assign M (pi ) = k and M (pj ) = l
9:
for (assignment = 3 to n) do
10:
unassigned = 0
/ Γ and y ∈ Γ, ∀z ∈ Lw ) do
11:
for (ewy ∈ ER , w ∈
z = η z + αzM (py ) values
12:
Calculate list G of gw
wy
w
13:
unassigned = unassigned + 1
14:
end for
15:
sort list G
z from the smallest ⌊β × unassigned⌋
16:
Select randomly gw
values of G
17:
Γ = Γ ∪ {w}
18:
Assign M (pw ) = z
19: end for
20: if (cost(currentSolution) < cost(bestSolution)) then
21:
bestSolution = currentSolution
22: end if
23: end for
24: Return bestSolution


by a, which is of the order of O n2 . Step 3 of the
algorithm sorts F in ascending order. Based on the
value of the selection parameter β, a member of F
is randomly selected from the smallest ⌊βa⌋ values
of sorted F . The set of assigned PEs Γ is updated
and the PEs pi and pj are assigned to voltage levels
k and l, respectively(step 7 and 8). In the following
f or−loop (step 9-19), the rest of the unassigned PEs
are asigned to some allowable voltage level one at
a time. The f or − loop inside (step 11-14) calculates
the list G of collective cost of assigning an unassigned PE pw to one of its allowable voltage level z
for all possible combinations, where its neighboring
PE py has already been assigned to some voltage
level. The list G is sorted in step 15. Again one
z
from the list G is randomly selected from
member gw
the smallest ⌊β × |G|⌋ members of the sorted list G.
The set of assigned PEs is updated (step 17) and
the PE pw is assigned to voltage level z. The cost of
the current solution is compared with the cost of the
best solution found so far and updated accordingly

82

(step 20-21). This entire procedure is repeated over
a number of iterations (step 4-23), which is an input
parameter and best solution among all is returned
as the solution to the voltage assignment problem
(step 24).
The algorithm 1 uses the greedy function for selecting voltage assignments leading energy efficiencies, whereas the randomized selection is used for
avoiding local optimals in the search space of the
solution. Increasing the number of iterations of the
algorithm increases the probability of achieving a
better solution, whereas it increases the execution
time of the algorithm. This trade-off has to be chosen
according to the required quality of the solution.
V. E XPERIMENTAL R ESULTS
In this section we evaluate the efficacy of our
greedy randomized heuristic and the effectiveness
of operating the PEs at multiple voltage levels for
energy consumption minimization purpose. Experimental results are gathered using benchmark applications (office-automation, networking, consumer
and auto-industry) from the E3S benchmark suite
[5]. Numbers of task nodes and communication
edges in the task graphs of these applications and
the dimension of the square mesh used is as shown
in Table I.
We used five discrete voltage levels for the PEs
as V0 = 1.9V , V1 = 2.3V , V2 = 2.5V , V3 = 3.3V ,
and V4 = 3.6V . Allowable voltage levels for each
of the PEs is calculated from the deadline information for each of the application tasks mapped to a
corresponding PE. Energy consumption and execution time variations of the tasks with voltage level
scaling were found from several processor vendors’
datasheets mentioned in the E3S benchmark suite.
All the experiments were executed on a Pentium4 3.2 GHz processor with 1 GB RAM. The ILP for
achieving the optimal solution were executed on the
same machine using ILOG CPLEX 10.0 Concert
technology. The value of the selection parameter β
in the greedy randomized heuristic was chosen as
TABLE I
B ENCHMARK A PPLICATIONS AND M ESH D IMENSION
Application
Office-Automation
Networking
Consumer
Auto-Industry

Task Nodes

Task Edges

Mesh Size

5
13
12
24

5
9
12
21

3×3
4×4
4×4
5×5

0

6

7

y

3

4

.

w

|

}

5

1
0

z

{

2



D



C
0

.

8

0

.

6

)

)

:


2

s

y
0

.

1

5

t

x

s



B

e

e


A

l

l

u
u

?

J



@



o
J

o


>

/

0

(

.

y

1
(



=

y
y


<

g
g

0

.

4



;

r

r



:

e

e

w

t

x

1

0

.

0

5

~
8
E



9

n

E

n

0

.

2

w

0

0

0

g

!

O

p

t

i

m

a

l

H

e

u

r

i

s

t

i

c

O

p

t

a

t

V

"

#

$

%

&

'

(

)

*

+

$

,

#

$

-

!

"

#

&

#

.

O

O

f

ﬁ

c

e



A

u

t

o

m

a

t

i

o

h

i

j

k

l

m

n

o

p

q

j

r

i

j

s

g

h

i

t

l

i

u

v

/

4

n

N

e

t

w

o

r

k

i

n

g

p

t

i

m

a

l

C

H

o

n

e

s

u

u

r

m

i

s

t

e

i

r

c

O

p

t

a

t

V

4

A

u

t

o



I

n

d

u

s

t

r

y

Fig. 2. Energy Consumption Comparison for Optimal at variable voltage levels, Heuristic at variable voltage levels and Optimal at fixed
voltage level for E3S Benchmark Applications

0.3, while running 100 iterations and selecting the
best solution among them.
Fig. 2 shows the quality of our heuristic as in all
the cases it is able to find solutions close to the
optimal ones. The time required to get the heuristic
solution is a small fraction of that required to get the
optimal one. The energy consumption while setting
the voltage at the highest voltage level V4 can also
be seen in the figures. It is clear that we save a
significant amount of energy by allowaing the PEs
to operate at multiple voltage levels. The possibility
of operating all the PEs at some fixed lower voltage
level is ruled out since tasks miss their deadlines
due to the slow execution at lower voltage levels.
VI. C ONCLUSION
In this paper, we defined the voltage assignment problem in the NoC domain as a quadratic
assignment problem. Quadratic Programming and
corresponding ILP formulation is used for optimal
solution of the problem. Then, we proposed a greedy
randomized heuristic. Experimental results based on
E3S benchmark suite [5] demonstrate effectiveness
of our heuristic and advantage of operating the PEs
at multiple voltage levels for energy minimization.
R EFERENCES
[1] L. Yang, T. Gohad, P. Ghosh, D. Sinha, A. Sen, and A. Richa,
“Resource Mapping and Scheduling for Heterogeneous Network Processor Architectures,” in Proceedings of ACM/IEEE
Symposium of Architecture of Network and Communication Systems(ANCS), Princeton, New Jersey, USA, October
2005, pp. 19–27.

83

[2] K. Srinivasan, K. S. Chatha, and G. Konjevod, “LinearProgramming-Based Techniques for Synthesis of Networkon-Chip Architectures,” IEEE Trans. on VLSI Systems,
vol. 14, no. 4, pp. 407–420, April 2006.
[3] C. L. Chou and R. Marculescu, “Incremental Run-time Application Mapping for Homogeneous NoCs with Multiple Voltage Levels,” in Proc. CODES+ISSS Conf., Salzburg, Austria,
Sept. 30- Oct. 5 2007.
[4] U. Y. Ogras, R. Marculescu, P. Choudhary, and D. Marculescu, “Voltage-Frequency Island Partitioning for GALSbased Networks-on-Chip,” in Proc. Design Automation Conf.,
San Diego, California, USA, June 2007.
[5] R. Dick, “Embedded System Synthesis Benchmarks Suite(E3S).” [Online]. Available: http://www.ece.
northwestern.edu/∼dickrp/e3s/
[6] G. De-Micheli and L. Benini, Networks On Chips. Morgan
Kaufmann, 2006.
[7] W. Dally and B. Towles, “Route Packets, Not Wires: OnChip Interconnection Networks,” in Proc. Design Automation
Conf., Las Vegas, Nevada, USA, June 2001, pp. 684–689.
[8] J. Hu and R. Marculescu, “Energy-Aware Mapping for Tilebased NoC Architectures Under Performance Constraints,”
in Proceedings of ASPDAC Conf., Kitakyushu, Japan, January 2003.
[9] K. Srinivasan, K. S. Chatha, and G. Konjevod, “An Automated Technique for Topology and Route Generation of
Application Specific On-Chip Interconnect Networks,” in Proceedings of ICCAD, 2005.
[10] U. Y. Ogras and R. Marculescu, “It’s a Small World After
All: NoC Performance Optimization Via Long-Range Link
Insertion,” IEEE Transactions on VLSI Systems, vol. 14,
no. 7, pp. 693–706, July 2006.
[11] E. Lawler, “The quadratic assignment problem,” Management Science, vol. 9, pp. 163–184, 1963.
[12] M. R. Garey and D. S. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness. W. H. Freeman, 1979.
[13] Y. Li, P. M. Pardalos, and M. G. C.Resende, “A Greedy
Randomized Adaptive Search Procedure For the Quadratic
Assignment Problem,” DIMACS Series in Discrete Mathematics and Theoretical Computer Science, vol. 16, pp. 237–
261, 1993.

84

Distributed Computing Track Chair’s Message
Arunabha Sen
Arizona State University, USA

Abstract. The Distributed Computing track of ICDCIT 2005 received
181 papers. Based on the review by the members of the Program Committee, 16 full and 9 short papers were selected for inclusion in the proceedings of the conference. The accepted papers cover a wide range of
topics in Distributed Computing. Design of MAC protocol, network architecture and routing protocol for Wireless Ad-Hoc Networks seem to
attract the attention of many researchers. 5 of the 16 accepted full papers fall in this area. The other popular areas include, Network Security,
Sensor Networks, Fault Detection and Recovery and Grid Computing.
Each of these areas will have at least 3 papers in the proceedings. The
Distributed Computing track will also have papers in the areas of Cellular Networks, Peer-to-Peer Networks, Optical Networks, Information
Retrieval, QoS and Mobile IP. We have put together a program that
covers many important areas of Distributed Computing. We hope you
will ﬁnd the papers in this track informative, interesting and useful.

G. Chakraborty (Ed.): ICDCIT 2005, LNCS 3816, p. 2, 2005.
c Springer-Verlag Berlin Heidelberg 2005


Sixth IEEE International Workshop on Network Science for Communication Networks (NetSciCom 2014)

Identiﬁcation of K Most Vulnerable Nodes in
Multi-layered Network Using a New Model of
Interdependency
Arunabha Sen, Anisha Mazumder, Joydeep Banerjee, Arun Das and Randy Compton
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {asen, amazumde, Joydeep.Banerjee, adas, Randy.Compton}@asu.edu

Abstract—The critical infrastructures of a nation including
the power grid and the communication network are highly
interdependent. Recognizing the need for a deeper understanding
of the interdependency in a multi-layered network, signiﬁcant
efforts have been made by the research community in the last
few years to achieve this goal. Accordingly a number of models
have been proposed and analyzed. Unfortunately, most of the
models are over simpliﬁed and, as such, they fail to capture the
complex interdependency that exists between entities of the power
grid and the communication networks involving a combination of
conjunctive and disjunctive relations. To overcome the limitations
of existing models, we propose a new model that is able to capture
such complex interdependency relations. Utilizing this model, we
provide techniques to identify the K most “vulnerable” nodes
of an interdependent network. We show that the problem can
be solved in polynomial time in some special cases, whereas
for some others, the problem is NP-complete. We establish that
this problem is equivalent to computation of a ﬁxed point of a
multilayered network system and we provide a technique for its
computation utilizing Integer Linear Programming. Finally, we
evaluate the efﬁcacy of our technique using real data collected
from the power grid and the communication network that span
the Maricopa County of Arizona.

I. I NTRODUCTION
In the last few years there has been an increasing awareness
in the research community that the critical infrastructures of
the nation are closely coupled in the sense that the well being
of one infrastructure depends heavily on the well being of another. A case in point is the interdependency between the electric power grid and the communication network. The power
grid entities, such as the SCADA systems that control power
stations and sub-stations, receive their commands through
communication networks, while the entities of communication
network, such as routers and base stations, cannot operate
without electric power. Cascading failures in the power grid,
are even more complex now because of the coupling between
power grid and communication network. Due to this coupling,
not only entities in power networks, such as generators and
Acknowledgment: This research was supported in part by the DTRA grant
HDTRA1-09-1-0032 and the AFOSR grant FA9550-09-1-0120. The Maricopa
county communication network data used in this research was provided by
GeoTel communications (www.geo-tel.com).

978-1-4799-3088-3/14/$31.00 ©2014 IEEE

transmission lines, can trigger power failure, communication
network entities, such as routers and optical ﬁber lines, can
also trigger failure in power grid. Thus it is essential that
the interdependency between different types of networks be
understood well, so that preventive measures can be taken to
avoid cascading catastrophic failures in multi-layered network
environments.
Recognizing the need for a deeper understanding of the
interdependency in a multi-layered network, signiﬁcant efforts
have been made in the research community in the last few
years to achieve this goal [1], [2], [3], [4], [5], [6], [7], [8].
Accordingly a number of models have been proposed and
analyzed. Unfortunately, many of the proposed models are
overly simplistic in nature and as such they fail to capture
the complex interdependency that exists between power grid
and communication networks. In a highly cited paper [1], the
authors assume that every node in one network depends on one
and only one node of the other network. However, in a follow
up paper [2], the same authors argue that this assumption may
not be valid in the real world and a single node in one network
may depend on more than one node in the other network. A
node in one network may be functional (“alive”) as long as
one supporting node on the other network is functional.
Although this generalization can account for disjunctive
dependency of a node in the A network (say ai ) on more than
one node in the B network (say, bj and bk ), implying that ai
may be “alive” as long as either bi or bj is alive, it cannot
account for conjunctive dependency of the form when both bj
and bk has to be alive in order for ai to be alive. In a real
network the dependency is likely to be even more complex
involving both disjunctive and conjunctive components. For
example, ai may be alive if (i) bj and bk and bl are alive, or
(ii) bm and bn are alive, or (iii) bp is alive. The graph based
interdependency models proposed in the literature [3], [4], [5],
[9], [6], [7] including [1], [2] cannot capture such complex
interdependency between entities of multilayer networks. In
order to capture such complex interdependency, we propose
Implicative Interdependency Model (IIM) using Boolean logic.
Utilizing this comprehensive model, we provide techniques to
identify the K most “vulnerable” nodes of an interdependent

831

Sixth IEEE International Workshop on Network Science for Communication Networks (NetSciCom 2014)
2

multilayered network system. We show that the this problem
can be solved in polynomial time for some special cases,
whereas for some others, the problem is NP-complete. We also
show that this problem is equivalent to computation of a ﬁxed
point [10] and we provide a technique utilizing Integer Linear
Programming to compute that ﬁxed point. Finally, we evaluate
the efﬁcacy of our technique using real data collected from
power grid and communication networks that span Maricopa
County of Arizona.
II. I MPLICATIVE I NTERDEPENDENCY M ODEL
We describe the model for an interdependent network with
two layers. However, the concept can easily be generalized
to deal with networks with more layers. Suppose that the
network entities in layer 1 are referred to as the A type
entities, A = {a1 , . . . , an } and entities in layer 2 are referred
to as the B type entities, B = {b1 , . . . , bm }. If the layer 1
entity ai is operational if (i) the layer 2 entities bj , bk , bl
are operational, or (ii) bm , bn are operational, or (iii) bp
is operational, we express it in terms of live equations of
the form ai ← bj bk bl + bm bn + bp . The live equation for
a B type entity br can be expressed in a similar fashion
in terms of A type entities. If br is operational if (i) the
layer 1 entities as , at , au , av are operational, or (ii) aw , az
are operational, we express it in terms of live equations of
the form br ← as at au av + aw az . It may be noted that the
live equations only provide a necessary condition for entities
such as ai or br to be operational. In other words, ai or br
may fail independently and may be not operational even when
the conditions given by the corresponding live equations are
satisﬁed. A 
live equation
in general will have the following
T i tj
form: xi ← j=1
k=1 yj,k where xi and yj,k are elements
of the set A (B) and B (A) respectively, Ti represents the
number of minterms in the live equation and tj refers to the
size of the j-th minterm (the size of a minterm is equal to the
number of A or B elements in that minterm). In the example
ai ← bj bk bl + bm bn + bp , Ti = 3, t1 = 3, t2 = 2, t3 = 1,
xi = ai , y2,1 = bm , y2,2 = bn .
We refer to the live equations of the form ai ← bj bk bl +
bm bn + bp also as First Order Implicative Dependency Relations (IDRs), because these relations express direct dependency of the A type entities on B type entities and vice-versa.
It may be noted however that as A type entities are dependent
on B type entities, which in turn depends on A type entities,
the failure of some A type entities can trigger the failure of
other A type entities, though indirectly, through some B type
entities. Such interdependency creates a cascade of failures
in multilayered networks when only a few entities of either A
type or B type (or a combination) fails. We illustrate this with
the help of an example. The live equations for this example
is shown in table I.
As shown in table II, the failure of only one entity a1 at
time step t0 triggered a chain of failures that resulted in the
failure of all the entities of the network after by timestep t4 .
A table entry of 1 indicates that the entity is “dead”. In this
example, the failure of a1 at t0 triggered the failure of b3 at

t1 , which in turn triggered the failure of a3 at t2 . The failure
of b3 at t1 was due to the IDR b3 ← a1 a2 and the failure of a3
at t2 was due to the IDR a3 ← b1 b2 b3 . The cascading failure
process initiated by failure (or death) of a subset of A type
entities at timestep t0 , A0d and a subset of B type entities Bd0
till it reaches its ﬁnal steady state is shown diagrammatically in
ﬁgure 1. Accordingly, a multilayered network can be viewed
as a “closed loop” control system. Finding the steady state
after an initial failure in this case is equivalent of computing
the ﬁxed point of a function F(.) such that F(Apd ∪ Bdp ) =
Apd ∪ Bdp , where p represents the number of steps when the
system reaches the steady state.
We deﬁne a set of K entities in a multi-layered network
as most vulnerable, if failure of these K entities triggers the
failure of the largest number of other entities. The goal of
the K most vulnerable nodes problem is to identify this set of
nodes. This is equivalent to identifying A0d ⊆ A, Bd0 ⊆ B, that
maximizes |Apd ∪ Bdp |, subject to the constraint |A0d ∪ Bd0 | ≤ K.
The IDRs (live equations) can be formed either by careful
analysis of the multilayer network along the lines carried out
in [8], or by consultation with the engineers of the local
utility and internet service providers. It may be noted that
it is possible that A (B) type entities may depend on A (B)
type entities themselves. The IIM model can deal with such a
scenario by not distinguishing between A and B type entities
and treating them as a third type entity C. The techniques
described in this paper are equally applicable in this case also.
Power Network
a1 ← b1 + b2
a2 ← b1 b 3 + b 2
a3 ← b1 b 2 b 3
a4 ← b1 + b2 + b 3

Communication Network
b 1 ← a1 + a2 a3
b 2 ← a1 + a3
b 3 ← a1 a2
−−

TABLE I: Live equations for a Multilayer Network
Entities
a1
a2
a3
a4
b1
b2
b3

t0
1
0
0
0
0
0
0

t1
1
0
0
0
0
0
1

Time Steps
t2
t3
t4
1
1
1
0
0
1
1
1
1
0
0
1
0
1
1
0
1
1
1
1
1

t5
1
1
1
1
1
1
1

t6
1
1
1
1
1
1
1

TABLE II: Time Stepped Cascade Effect for a Multilayer Network

Fig. 1: Cascading failures reach steady state after p time steps

III. C OMPUTATIONAL C OMPLEXITY AND A LGORITHMS
Based on the number and the size of the minterms in the
IDRs, we divide them into four different cases as shown in
Table III. The algorithms for ﬁnding the K most vulnerable
nodes in the multilayer networks and computation complexity
for each of the cases are discussed in the following four
subsections.

832

Sixth IEEE International Workshop on Network Science for Communication Networks (NetSciCom 2014)
3

Case
Case I
Case II
Case III
Case IV

No. of Minterms
1
1
Arbitrary
Arbitrary

Size of Minterms
1
Arbitrary
1
Arbitrary

or Cxi ∩ Cxj = Cxi or Cxi ∩ Cxj = Cxj , where xi 
= xj .

TABLE III: Types of Implicative Dependency Relations (IDRs)

A. Case I: Problem Instance with One Minterm of Size One
In this case, a live equation in general will have the following form: xi ← yj where xi and yj are elements of the set A
(B) and B (A) respectively. In the example ai ← bj , xi = ai ,
y1 = bj . It may be noted that a conjunctive implication of
the form ai ← bj bk can also be written as two separate
implications ai ← bj and ai ← bk . However, such cases are
considered in Case II and is excluded from consideration in
Case I. The exclusion of such implications implies that the
entities that appear on the LHS of an implication in Case I
are unique. This property enables us to develop a polynomial
time algorithm for the solution of the K most vulnerable node
problem for this case. We present the algorithm next.
Algorithm 1
Input: (i) A set S of implications of the form of y ← x,
where x, y ∈ A ∪ B, (ii) An integer K.
Output: A set V  where |V  | = K and V  ⊂ A ∪ B such
that failure of entities in V  at time step t0 results in failure
of the largest number of entities in A ∪ B when the steady
state is reached.
Step 1. We construct a directed graph G = (V, E), where
V = A ∪ B. For each implication y ← x in S, where x, y ∈
A ∪ B, we introduce a directed edge (x, y) ∈ E.
Step 2. For each node xi ∈ V , we construct a transitive
closure set Cxi as follows: If there is a path from xi to some
node yi ∈ V in G, then we include yi in Cxi . It may be
recalled that |A| + |B| = n + m. So, we get n + m transitive
closure sets Cxi , 1 ≤ i ≤ (n + m). We call each xi to be the
seed entity for the transitive closure set Cxi .
Step 3. We remove all the transitive closure sets which are
proper subsets of some other transitive closure set.
Step 4. Sort the remaining transitive closure sets Cxi ,
where the rank of the closure sets is determined by the
cardinality of the sets. The sets with a larger number of entities
are ranked higher than the sets with a fewer number of entities.
Step 5. Construct the set V  by selecting the seed entities
of the top K transitive closure sets. If the number of remaining
transitive closure sets is less than K (say, K ), arbitrarily select
the remaining entities.
Time complexity of Algorithm 1: Step 1 takes O(n + m + |S|)
time. Step 2 can be executed in O((n+m)3 ) time. Step 3 takes
at most O((n + m)2 ) time. Step 4 sorts at most |S| entries, a
standard sorting algorithm takes O(|S| log |S|) time. Selecting
K entities in step 5 takes O(K) time. Since |S| ≤ n+m, hence
the overall time complexity is O((n + m)3 )
Theorem 1. For each pair of transitive closure sets Cxi and
Cxj produced in step 2 of algorithm 1, either Cxi ∩ Cxj = ∅

Proof: Consider, if possible, that there is a pair of transitive
closure sets Cxi and Cxj produced in step 2 of algorithm 1,
such that Cxi ∩Cxj 
= ∅ and Cxi ∩Cxj 
= Cxi and Cxi ∩Cxj 
=
Cxj . Let xk ∈ Cxi ∩ Cxj . This implies that there is a path
from xi to xk (path1 ) as well as there is a path from xj to xk ,
(path2 ). Since, xi 
= xj and Cxi ∩Cxj 
= Cxi and Cxi ∩Cxj =
Cxj , there is some xl in the path1 such that xl also belongs
to path2 . W.l.o.g, let us consider that xl be the ﬁrst node in
path1 such that xl also belongs to path2 . This implies that
xl has in-degree greater than 1. This in turn implies that there
are two implications in the set of implications S such that xl
appears in the LHS of both. This is a contradiction because this
violates a characteristic of the implications in Case I. Hence,
our initial assumption was wrong and the theorem is proven.
Theorem 2. Algorithm 1 gives an optimal solution for the
problem of selecting K most vulnerable entities in a multilayer network for case I dependencies.
Proof: Consider that the set V  returned by the algorithm is
not optimal and the optimal solution is VOP T . Let us consider
there is a entity xi ∈ A ∪ B such that xi ∈ VOP T \ V  .
Evidently, (i) Cxi was either deleted in step 3 or (ii) |Cxi | is
less than the cardinalities of all the transitive closure sets with
seed entities xj ∈ V  , because our algorithm did not select
xi . Hence, in both cases, replacing any entity xj ∈ V  by xi
reduces the total number of entities killed. Thus, the number
of dead entities by the failure of entities in VOP T is lesser than
that caused by the failure of the entities in V  , contradicting
the optimality of VOP T . Hence, the algorithm does in fact
return the optimal solution.
B. Case II: Problem Instance with One Minterm of Arbitrary
Size
In this case, a live equation in general will have the
q
following form: xi ← k=1 yj where xi and yj are elements
of the set A (B) and B (A) respectively, q represents the size
of minterm. In the example ai ← bj bk bl , q = 3, xi = ai ,
y 1 = b j , y 2 = bk , y 3 = b k .
1) Computational Complexity: We show that computation
of K most vulnerable nodes (K-MVN) in a multilayer network
is NP-complete in Case II. We formally state the problem next.
Instance: Given a set of IDRs between A and B type
q
entities in the form of live equations xi ← k=1 yj , integers
K and L.
Question: Is there a subset of A and B type entities of
size at most K whose “death” (failure) at time t0 , triggers a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached?
Theorem 3. The K-MVN problem is NP-complete.
Proof: We prove that the K-MVN problem is NP-complete
by giving a transformation for the vertex cover (VC) problem.

833

Sixth IEEE International Workshop on Network Science for Communication Networks (NetSciCom 2014)
4

An instance of the vertex cover problem is speciﬁed by an
undirected graph G = (V, E) and an integer R. We want to
know if there is a subset of nodes S ⊆ V of size at most
R, so that every edge has at least one end point in S. From
an instance of the VC problem, we create an instance of the
K-MVN problem in the following way. First, from the graph
G = (V, E), we create a directed graph G = (V, E  ) by
replacing each edge e ∈ E by two oppositely directed edges
e1 and e2 in E  (the end vertices of e1 and e2 are same as
the end vertices of e). Corresponding to a node vi in G that
has incoming edges from other nodes (say) vj , vk and vl , we
create an IDR (live equation) vi ← vj vk vl . We set K = R
and L = |V |. The corresponding death equation is of the form
v¯i ← v¯j + v¯k + v¯l (obtained by taking negation of the live
equation). We set K = R and L = |V |. It can now easily be
veriﬁed that if the graph G = (V, E) has a vertex cover of
size R iff in the created instance of K-MVN problem death
(failure) of at most K entities at time t0 , will trigger a cascade
of failures resulting in failures of at least L entities, when the
steady state is reached.
2) Optimal Solution with Integer Linear Programming:
In this case, we can ﬁnd and optimal solution to the KMVN problem using Integer Linear Programming (ILP). We
associate binary indicator variables xi (yi ) to capture the state
of the entities ai (bi ). xi (yi ) is 1 when ai (bi ) is dead and
0 otherwise. Since we want ﬁnd the set of K entities whose
failure at time step t0 triggers cascading failure resulting in the
failure of the largest number of entities, the
the
of
nobjective
m
ILP can be written as follows maximize
i=1 xi +
i=1 yi
It may be noted that the variables in the objective function
do not have any notion of time. However, cascading failure
takes place in time steps, ai triggers failure of bj at time
step t1 , which in turn triggers failure of ak in time step t2
and so on. Accordingly, in order to capture the cascading
failure process, we need to introduce the notion of time into
the variables of the ILP. If the numbers of A and B type
entities are n and m respectively, the steady state must be
reached by time step n + m − 1 (cascading process starts at
time step 0, t0 ). Accordingly, we introduce n + m versions
of the variables xi and yi , i.e., xi [0], . . . , xi [n + m − 1] and
yi [0], . . . , yi [n+m−1]. To indicate the state of entities ai and
bi at times t0 , . . . , tn+m−1 . The objective of the ILP is now
changed to
n
m


xi [n + m − 1] +
yi [n + m − 1]
maximize
i=1

i=1

Subject to the constraint that no more than K entities can
fail at time t0 .
m
n
Constraint 1: i=1 xi [0] + i=1 yi [0] ≤ K In order to
ensure that the cascading failure process conforms to the IDRs
between type A and B entities, additional constraints must be
imposed.
Constraint 2: If an entity fails at time step p, (i.e., tp )
it should continue to be in the failed state at all time steps
t > p. That is xi (t) ≥ xi (t − 1), ∀t, 1 ≤ t ≤ n + m − 1. Same
constraint applies to yi (t).

Constraint 3: The death equation a¯i ← b¯j + b¯k + b¯l can be
translated into a linear constraint in the following way xi (t) ≤
yj (t − 1) + yk (t − 1) + yl (t − 1), ∀t, 1 ≤ t ≤ n + m − 1.
The optimal solution to K-MVN problem for Case II can be
found by solving the above ILP.
C. Case III: Problem Instance with an Arbitrary Number of
Minterms of Size One
A live equation
q in this special case will have the following
form: xi ←
j=1 yj where xi and yj are elements of the
set A (B) and B (A) respectively, q represents the number of
minterms in the live equation. In the example ai ← bj +bk +bl ,
q = 3, xi = ai , y1 = bj , y2 = bk , y3 = bl .
1) Computational Complexity: We show that a special
case of the problem instances with an arbitrary number
of minterms of size one is same as the Subset Cover
problem (deﬁned below), which is proven to be NPcomplete. We deﬁne Implication Set(A) to be the
Ti
set of all implications of the form ai ←
j=1 bj and
to
be
the
set
of
all
implications
of
Implication Set(B)
Ti
the form bi ←
a
.
Now
consider
a
subset
of
the
set
j
j=1
of problem instances with an arbitrary number of minterms
of size one where either Implication Set(A) = ∅
=
∅. Let A
=
or Implication Set(B)
{ai |ai is the element on the LHS of an implication}
in the Implication Set(A). The set B  is deﬁned
accordingly. If Implication Set(B) = ∅ then B  = ∅. In
this case, failure of any ai , 1 ≤ i ≤ n type entities will not
cause failure of any bj , 1 ≤ j ≤ m type entities. Since an
adversary can cause failure of only K entities, the adversary
would like to choose only those K entities that will cause
failure of the largest number of entities. In this scenario, there
is no reason for the adversary to attack any ai , 1 ≤ i ≤ n type
entities as they will not cause failure of any bj , 1 ≤ j ≤ m
type entities. On the other hand, if the adversary attacks
K bj type entities, not only those K bj type entities will
be destroyed, some ai type entities will also be destroyed
due to the implications in the Implication Set(A). As
such the goal of the adversary will be to carefully choose
K bj , 1 ≤ j ≤ m type entities that will destroy the largest
number of ai type entities. In its abstract form, the problem
can be viewed as the Subset Cover problem.
Subset Cover Problem
Instance: A set S = {s1 , . . . , sm }, a set S of r subsets of S,
i.e., S = {S1 , . . . , Sr }, where Si ⊆ S, ∀i, 1 ≤ i ≤ r, integers
p and q.
Question: Is there a p element subset S  of S (p < m) that
completely covers at least q elements of the set S? (A set S 
is said to be completely covering an element Si , ∀i, 1 ≤ i ≤ r
of the set S, if S  ∩ Si = Si , ∀i, 1 ≤ i ≤ r.)
The set S in the subset cover problem corresponds to the set
B = {b1 , . . . , bm }, and each set Si , 1 ≤ i ≤ r corresponds to
an implication in the Implication Set(A) and comprises of
the bj ’s that appear on the RHS of the implication. The goal
of the problem is to select a subset B  of B that maximizes
the number of Si ’s completely covered by B  .

834

Sixth IEEE International Workshop on Network Science for Communication Networks (NetSciCom 2014)
5

Theorem 4. The Subset Cover problem is NP-complete.
Proof: We prove that the Subset Cover problem is NPcomplete by giving a transformation from the well known
Clique problem. It may be recalled that an instance of the
Clique problem is speciﬁed by a graph G = (V, E) and an
integer K. The decision question is whether or not a clique of
size at least K exists in the graph G = (V, E). We show that
a clique of size K exists in graph G = (V, E) iff the Subset
Cover problem instance has a p element subset S  of S that
completely covers at least q elements of the set S.
From an instance of the Clique problem, we create an
instance of the Subset Cover problem in the following way.
Corresponding to every vertex vi , 1 ≤ i ≤ n of the graph
G = (V, E) (V = {v1 , . . . , vn }), we create an element
in the set S = {s1 , . . . , sn }. Corresponding to every edge
ei , 1 ≤ i ≤ m, we create m subsets of S, i.e., S =
{S1 , . . . , Sm }, where Si corresponds to a two element subset
of nodes, corresponding to the end vertices of the edge ei . We
set the parameters p = K and q = K(K − 1)/2. Next we
show that in the instance of the subset cover problem created
by the above construction process, a p element subset S  of
S exists that completely covers at least q elements of the set
S, iff the graph G = (V, E) has a clique of size at least K.
Suppose that the graph G = (V, E) has a clique of size
K. It is clear that in the created instance of the subset cover
problem, we will have K(K − 1)/2 elements in the set S,
that will be completely covered by a K element subset of
the set S. The K element subset of S corresponds to the set
of K nodes that make up the clique in G = (V, E) and the
K(K − 1)/2 elements in the set S corresponds to the edges
of the graph G = (V, E) that corresponds to the edges of
the clique. Conversely, suppose that the instance of the Subset
Cover problem has K element subset of S that completely
covers K(K − 1)/2 elements of the set S. Since the elements
of S corresponds to the edges in G, in order to completely
cover K(K − 1)/2 edges, at least K nodes (elements of the
set S) will be necessary. As such, this set of K nodes will
constitute a clique in the graph G = (V, E).
2) Optimal Solution with Integer Linear
Programming: If
q
the live equation is in the form xi ← k=1 yj then the “death
equation” (obtained by taking negation
of the live equation)
q
will be in the product form x̄i ← j=1 ȳj . If the live equation
is given as ai ← bj + bk , then the death equation will be given
as a¯i ← b¯j b¯k .
By associating binary indicator variables xi and yi to
capture the state of the entities ai and bi , we can follow almost
identical procedure as in Case II, with only one exception.
It may be recalled that in Case II, the death equations such
as ai ← bj + bk was translated into a linear constraint
xi (t) ≤ yj (t − 1) + yk (t − 1), ∀t, 1 ≤ t ≤ n + m − 1.
However a similar translation in Case III, with death equations
such as ai ← bj1 bj2 . . . bjg , will result in a non-linear
constraint of the form xi (t) ≤ yj1 (t − 1)yj2 (t − 1) . . . yjg (t −
1), ∀t, 1 ≤ t ≤ n + m − 1. Fortunately, a non-linear constraint
of this form can be replaced a linear constraint such as

g
g × xi (t) ≤ h=1 yjh (t − 1), ∀t, 1 ≤ t ≤ n + m − 1. After
this transformation, we can compute the optimal solution using
integer linear programming.
D. Case IV: Problem Instance with an Arbitrary Number of
Minterms of Arbitrary Size
1) Computational Complexity: Since both Case II and Case
III are special cases of Case IV, the computational complexity
of ﬁnding the K most vulnerable nodes in the multilayer
network in NP-complete in Case IV also.
2) Optimal Solution with Integer Linear Programming:
The optimal solution to this version of the problem can be
computed by combining the techniques developed for the
solution of the versions of the problems considered in Cases
II and III. For e.g., a live equation a1 ← bj bk bl + bm bn + bp
can be written as a1 ← d1 + d2 + bp and treated as in Case III,
where d1 ← bj bk bl , d2 ← bm bn . Each of these IDRs belong
to Case II.
IV. E XPERIMENTAL RESULTS
We applied our model to study multilayer vulnerability
issues in Maricopa County, the most densely populated county
of Arizona with approximately 60% of Arizonas population
residing in it. Speciﬁcally, we wanted to ﬁnd out if some
regions of Maricopa County were more vulnerable to failure
than some other regions. The data for our multi-layered
network were obtained from different sources. We obtained
the data for the power network (network A) from Platts
(http://www.platts.com/). Our power network dataset consists
of 70 power plants and 470 transmission lines. Our communication network (network B) data were obtained from GeoTel
(http://www.geo-tel.com/). Our communication network data
consists of 2, 690 cell towers and 7, 100 ﬁber-lit buildings as
well as 42, 723 ﬁber links. Snapshots of our power network
data and communication network data are shown in ﬁgure 2. In
the power network snapshot of sub-ﬁgure(a), the orange markers show locations of powerplants while the yellow continuous
lines represent the transmission lines. In the communication
network snapshot of sub-ﬁgure (b) the pink markers show the
location of ﬁber-lit buildings, the orange markers show the
location of cell towers and the green continuous lines represent
the ﬁber links. In our dataset, ‘load’ in the Power Network is
divided into Cell towers and Fiber-lit buildings. Although there
exists various other physical entities which also draw electric
power and hence can be viewed as load to the power network,
as they are not relevant to our study on interdependency
between power and communication networks, we ignore such
entities. Thus in network A, we have the three types of Power
Network Entities (PNE’s) - Generators, Load (consisting of
Cell towers and Fiber-lit buildings) and Transmission lines
(denoted by a1 , a2 , a3 respectively). For the Communication
Network, we have the following Communication Network
Entities (CNE’s) - Cell Towers, Fiber-lit buildings and Fiber
links (denoted by b1 , b2 , b3 respectively). We consider the
Fiber-lit buildings as a communication network entities as they
house routers which deﬁnitely are communication network

835

Sixth IEEE International Workshop on Network Science for Communication Networks (NetSciCom 2014)
6

(a) Snapshot of Power Network in Maricopa County

(b) Snapshot of Communication Network in Maricopa County

Fig. 2: Snapshots of power network and communication network in Maricopa County

entities. From the raw data we construct Implication Set(A)
and Implication Set(B), by following the rules stated below:
Rules: We consider that a PNE is dependent on a set of
CNEs for being in the active state (‘alive’) or being in the
inactive state (‘dead’). Similarly, a CNE is dependent on a set
of PNEs for being active or inactive state. For simplicity we
consider the live equations with at most two minterms. For
the same reason we consider the size of each minterm is at
most two.

as the transmission lines and the ﬁberlinks are not dependent
on any other entities. We notice that all the entities of the
two networks can be destroyed with a budget of about 60%
of the number of entities of the two networks A and B. Most
importantly, we ﬁnd that the degree of vulnerability of all
the ﬁve regions considered in our study are close and no one
region stands out as being extremely vulnerable.

Generators (a1,i , 1 ≤ i ≤ p, where p is the total number
of generators): We consider that each generator (a1.i ) is
dependent on the nearest Cell Tower (b1,j ) or the nearest
Fiber-lit building (b2,k ) and the corresponding Fiber link (b3,l )
connecting b2,k and a1,i . Thus, we have
a1,i ← b1,j + b2,k × b3,l
Load (a2,i , 1 ≤ i ≤ q, where q is the total number of loads):
We consider that the loads in the power network do not depend
on any CNE.
Transmission Lines (a3,i , 1 ≤ i ≤ r, where r is the total number of transmission lines): We consider that the transmission
lines do not depend on any CNE.
Cell Towers (b1,i , 1 ≤ i ≤ s, where s is the total number
of cell towers): We consider the cell towers depend on the
nearest pair of generators and the corresponding transmission
line connecting the generator to the cell tower. Thus, we have
b1,i ← a1,j × a3,k + a1,j  × a3,k
Fiber-lit Buildings (b2,i , 1 ≤ i ≤ t, where t is the total number
of ﬁber-lit buildings): We consider that the ﬁber-lit buildings
depend on the nearest pair of generators and the corresponding
transmission lines connecting the generators to the cell tower.
Thus, we have b2,i ← a1,j × a3,k + a1,j  × a3,k
Fiber Links (b3,i , 1 ≤ i ≤ u, where u is the total number of
ﬁber links)): We consider that the ﬁber links do not depend
on any PNE.
Because of experimental resource limitation, we have considered 5 regions of Maricopa County for our experiments.
We used IBM CPLEX Optimizer 12.5 to run the formulated
ILP’s on the experimental dataset. We show our results in
the ﬁgure 3. We observe that in each of the regions there
is a speciﬁc budget threshold beyond which each additional
increment in budget results in the death of only one entity. The
reason for this behavior is our assumption that entities such

Fig. 3: Experimental results of failure vulnerability across ﬁve regions
of Maricopa county

R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
“Catastrophic cascade of failures in interdependent networks,” Nature,
vol. 464, no. 7291, pp. 1025–1028, 2010.
[2] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, “Networks formed
from interdependent networks,” Nature Physics, vol. 8, no. 1, pp. 40–48,
2011.
[3] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, “Cascade of
failures in coupled network systems with multiple support-dependence
relations,” Physical Review E, vol. 83, no. 3, p. 036116, 2011.
[4] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, “Modelling interdependent infrastructures using interacting
dynamical models,” International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63–79, 2008.
[5] P. Zhang, S. Peeta, and T. Friesz, “Dynamic game theoretic model of
multi-layer infrastructure networks,” Networks and Spatial Economics,
vol. 5, no. 2, pp. 147–178, 2005.
[6] M. Parandehgheibi and E. Modiano, “Robustness of interdependent
networks: The case of communication networks and the power grid,”
arXiv preprint arXiv:1304.0356, 2013.
[7] D. T. Nguyen, Y. Shen, and M. T. Thai, “Detecting critical nodes in
interdependent power networks for vulnerability assessment,” 2013.
[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
“Power grid vulnerability to geographically correlated failures-analysis
and control implications,” arXiv preprint arXiv:1206.1099, 2012.
[9] J.-F. Castet and J. H. Saleh, “Interdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,” PloS one, vol. 8, no. 4, p. e60402, 2013.
[10] A. Fudenberg and J. Tirole, Game Theory. Ane Books, 2010.

836

A Peer-to-Peer Network Based on Multi-Mesh
Architecture
Sudheendra Murthy and Arunabha Sen
{asen, sudhi}@asu.edu
Department of Computer Science and Engineering
Arizona State University
Tempe, Arizona 85281

Abstract— This paper presents the design and evaluation of
a highly scalable, decentralized and self-organizing peer-topeer network architecture based on the multi-mesh topology.
Our network automatically adapts to dynamic node arrivals,
departures and failures. Each node maintains a fixed set of
neighbor connections, regardless of the size of the network.
This demonstrates the scalability of the network. Our network
is close in spirit to the Content-Addressable Network. While
the Content-Addressable Network uses torus as the underlying
network topology, our network uses multi-mesh. Multi-mesh
has some unique advantages over torus and this is reflected in
the evaluations of our network against the Content-Addressable
Network.

to both of them. For example, if N is the total number of nodes
in the network, a two dimensional torus has a diameter of
Θ(N 1/2 ), where as a multi-mesh has a diameter of Θ(N 1/4 ).
In both the graphs however, the degree of each node is four.
The rest of this paper is organized into three parts. In
the first part, we introduce some necessary background and
related work in P2P systems and multi-mesh network. In the
second part, we present the design of our P2P system based
on the multi-mesh architecture and the simulation results. The
third part discusses some future extensions of the multi-mesh
architecture to three dimensions.

I. I NTRODUCTION

II. BACKGROUND AND R ELATED W ORK

Peer-to-peer(P2P) Overlay Networks have gained a lot of
popularity over the last few years for their applicability in
highly scalable, decentralized distributed applications. Among
the most recent P2P systems are Pastry [1], Tapestry [2], Chord
[3] and Content-Addressable Network (CAN) [4]. These systems are based on the distributed resource discovery idea
presented in [5] and fall into the category of distributed
hash table systems. The issue of resource discovery in a P2P
environment is to locate the distributed objects in an efficient
way. Principal to the resource discovery scheme is a scalable
distributed hash table, in which the objects are mapped on to
different nodes in the system. The entire hash table itself is
distributed among all the participants of the system.
In a P2P system, a lookup request for a particular object
travels from node to node making application-level jumps
and reaches the destination node containing the object. These
application level hops are expensive and a P2P system should
try to minimize the number of hops involved. Closely related
is the issue of state space and routing efficiency tradeoff. State
space is the space required to store the routing tables in each
node to keep track of its neighbors. One of the main objectives
in a P2P network is to achieve better routing efficiency, while
maintaining a fixed state space. In this paper, we present a
new P2P network that displays good routing efficiency, still
maintaining a very small and fixed number of neighbors. Our
network is based on the Multi-Mesh architecture [6]. A multimesh network consists of nodes with uniform degree four. The
multi-mesh network is similar to a mesh and a torus network
in terms of its simplicity of interconnections and routing.
However, it possesses better topological properties compared

Each node in a P2P network has a node identifier and
stores all those objects whose key closely matches with the
node identifier. This node is referred to as the home node
for those objects. The most basic operation in a P2P system
is lookup(key), which locates the node that stores the
object with the key. When some node in the network issues
a lookup request for an object, the request is routed in the
overlay network towards the home node of that object. The
existing distributed hash table P2P systems namely, Pastry,
Tapestry, Chord and CAN form a different overlay network
structure and has a different routing algorithm.
In Pastry, the node identifiers are chosen from a 128-bit
circular key space. The routing table has log b N rows (b is
some integer), with each row having nodes whose identifiers
match one prefix more than those of the previous row. Routing
is done by matching the identifier in the local routing table for
the longest shared prefix with the key. Pastry routes a message
within O(log b N ) hops.
Tapestry is a variant of the resource discovery idea presented
in [5]. The modifications support dynamic node insertions
and departures that were not supported earlier. As in Pastry,
Tapestry nodes maintain links to a set of neighbors that share
common prefixes with its identifier. Tapestry can route a
message in O(log b N ) hops.
In Chord, the home node of an object is the node whose
identifier most closely succeeds the key of that object. The
routing table in each node has a list of k nodes that immediately follow it in the key space. The graph formed by Chord
can be visualized as a circle with a number of chords. Each
node maintains O(log 2 N ) chords spaced exponentially around

GLOBECOM 2003

- 3840 -

0-7803-7974-8/03/$17.00 © 2003 IEEE

the key space and these chords improve the efficiency of the
routing. Chord can route a message within O(log 2 N ) hops.
CAN places nodes into a virtual, multi-dimensional key
space. The entire key space is divided among all the existing
live nodes. The borders of this virtual space are wrapped
around to form a d-dimensional torus topology. Each node
owns a region of this space called the zone and its neighbors
are the nodes that own the contiguous zones. Routing queries
are passed along the axes in this virtual space until they reach
the destination. CAN nodes maintain O(d) neighbors and the
path-lengths are O(dn1/d ) hops.
Though Pastry, Tapestry and Chord have shorter pathlengths than CAN, it comes at the expense of having to
maintain more state information in the routing tables. The
routing state information in Pastry, Tapestry and Chord grows
with the number of nodes in the network. This overhead
involves not just the extra storage required to store the routing
tables but also the number of updates that have to be done
when nodes join or leave the network. We would like to
achieve shorter path-lengths, while maintaining a fixed number
of neighbors and this is achieved by our Multi-Mesh based P2P
system.
These systems are often evaluated with respect to three
parameters.
1) Overlay path-length - This is the diameter of the P2P
overlay network measured in terms of number of hops.
2) Neighbor state - This is the size of the routing table in
each participating node in the P2P overlay network.
3) Cost of node insertions - An insertion of a new node
into the overlay network incurs some overhead in terms of the
messages exchanged in order to affect the routing tables in the
neighboring nodes.
Table I shows a comparison of Pastry, Tapestry, CAN, Chord
with respect to these parameters.

Networks [7]. A multi-mesh network is a uniform graph, in
which the degree of each node is four. A Complete MultiMesh (CMM) network consists of exactly n4 nodes, where
n is some integer (n ≥ 3). The basic unit of a multi-mesh
network is a block, which is a complete mesh consisting of
n2 nodes arranged in n rows and n columns. n2 such blocks
arranged in n rows and n columns make up a CMM. Each node
in a multi-mesh is identified by a four tuple (α, β, x, y), where
1 ≤ α, β, x, y ≤ n. The first two literals (α, β) identify the
block and the last two (x, y) identify the node in that block.
The n2 nodes in a block are connected as a regular twodimensional mesh. The boundary nodes in a two- dimensional
mesh have degree less than four. In a multi-mesh, these nodes
are connected to other blocks using the following rules.
Rule 1) ∀β, 1 ≤ β ≤ n, and node (α, β, 1, y) is connected
to the node (y, β, n, α) where 1 ≤ y, α ≤ n,
Rule 2) ∀α, 1 ≤ α ≤ n, node (α, β, x, 1) is connected to
the node (α, x, β, n) where 1 ≤ y, α ≤ n

TABLE I
M ETRICS C OMPARISON

One limitation of a CMM is its requirement for exactly
n4 nodes. This is a serious restriction on some networks
that have less than n4 nodes or dynamically changing set
of nodes, like the P2P systems. In [7], the authors have
presented a generalization of the multi-mesh interconnection
topology, which relaxes this requirement. This modification
termed as Generalized Multi-Mesh (GM) supports any number
of nodes. In an GM comprising of N nodes with a block
size of n, 1 ≤ N ≤ n4 , there are m = N/n2  number
of blocks, which are arranged in rows and columns. There
are p = m/n rows, in which only the last row is partially
complete with q = mod(m, n) blocks. All the other rows have
n complete blocks, each block having n2 nodes. Only the last
block in the last row may be incomplete. Four functions are
used to describe the inter-block connections:
1) top(α, β, i): If the ith column in the block has at least
one node, then top(α, β, i)is the node in row 1, column i.
Otherwise, it is not defined.
2) bottom(α, β, i): If the ith column in the block has at
least one node, then bottom(α, β, i)is the ith column node in
the highest indexed row that has at least i nodes. Otherwise,

Parameter
Overlay
Path-Length
NeighborState
Messages
to insert

Pastry
O(log b N )

Tapestry
O(log b N )

Chord
O(log 2 N )

CAN
O(N 1/d )

O(b log b N )

O(b log b N )

O(log 2 N )

O(d)

O(log b N )

O(log 22 N )

O(log 22 N )

O(dN 1/d )

CAN is based on torus network and the above measures
of diameter and path-lengths are directly derived from the
properties of torus. The multi-mesh network has the same fixed
neighbor state as that of two-dimensional torus but has a better
path-length than torus, which makes it an ideal candidate for
adaptation to P2P networks.
III. T HE M ULTI -M ESH N ETWORK
The Multi-Mesh Network [6] was originally intended for
use in parallel processor environments. Since its inception, it
has also been proposed as an architecture for use in Optical

GLOBECOM 2003

Fig. 1. A partially completed CMM with 81 nodes, n=3 (Not all links are
shown for clarity)

- 3841 -

0-7803-7974-8/03/$17.00 © 2003 IEEE

it is not defined.
3) lef t(α, β, i): If the ith row in the block has at least
one node, then lef t(α, β, i)is the node in row i, column 1.
Otherwise, it is not defined.
4) right(α, β, i): If the ith column in the block has at least
one node, then right(α, β, i)is the highest indexed column
node in row i. Otherwise, it is not defined.
The inter-block connections are given by the following
rules.
Rule 1.1) If the node top(x, β, α) exists, then
bottom(α, β, x) is connected to the node top(x, β, α).
Rule 1.2) If the node top(x, β, α) does not exist and the
node top(α+1, β, x) exists, then bottom(α, β, x) is connected
to the node top(α + 1, β, x)
Rule 1.3) If the node top(x, β, α) as well as the node
top(α+1, β, x) do not exist, then bottom(α, β, x)is connected
to the node top(i, β, x), where i is the lowest indexed row for
which top(i, β, x) is not connected using Rule 1.1).
Rule 2.1) If the node lef t(α, x, β) exists, then
right(α, β, x) is connected to the node lef t(α, x, β).
Rule 2.2) If the node lef t(α, x, β) does not exist and the
node lef t(α, β + 1, x) exists, then right(α, β, x) is connected
to the node lef t(α, β + 1, x)
Rule 2.3) If lef t(α, x, β) as well as lef t(α, β + 1, x)
do not exist, then right(α, β, x) is connected to the node
lef t(α, i, x), where i is the lowest indexed column for which
lef t(α, i, x)is not connected using Rule 2.1).

Fig. 2.

A GM Network with 40 nodes, n=3

The process of routing in a multi-mesh in described in [6].
In a multi-mesh, a block has connections to all the blocks in
the same row or same column. Routing to a destination node
in a block that is in the same row or same column as the
source block (block containing the source node) is trivial. If
the destination block is not in the same row or same column as
that of the source block then, there is one intermediate block
in the path between the source block and the destination block.
We will scrutinize in some more detail how routing is done
in this case.
To route a message from a source node (αs , βs , xs , ys ) to
a destination node (αd , βd , xd , yd ), first imaginary lines are
drawn in the source block connecting the nodes (αs , βs , 1, αd )
& (αs , βs , n, αd ) and (αs , βs , βd , 1) & (αs , βs , βd , n). Similar
imaginary lines are drawn in the destination block connecting
the nodes (αd , βd , 1, αs ) & (αd , βd , n, αs ) and (αd , βd , βs , 1)
& (αd , βd , βs , n). These lines divide the source block and the
destination block into four quadrants each. The four boundary
nodes through which these imaginary lines are drawn in the
source block are called the source block exit points and the
four boundary nodes in the destination block are called the
destination block entry points. Now, the quadrants into which
the source node and the destination node fall in their respective
blocks are located. There are two exit points in the source
block closest to the source node and two entry points in
the destination block closest to the destination node. The
two possible paths passing through these two pairs of entry
points and exit points are enumerated and the path with the
lower number of hops is selected. The intermediate block
has an entry point and an exit point. A list consisting of the
source block exit node, intermediate block exit node and the
destination node is made and it is included in the routing
message for routing purposes. Each node that receives this
message tries to route the message to the node at the top of
the list. Further, when the message reaches the exit nodes, the
nodes remove their entry at the top of the list, so that the
message travels towards the next item in the list and so on,
until it reaches the destination. The P2P network construction
is described in the next section.
B. P2P Network Construction

4

A special case of GM is when N = n , in which case, it
reduces to a CMM. Note that in this case, only rules 1.1) and
2.1) apply as top(x, β, α) and lef t(α, x, β) always exist.
Both GM and CMM have diameter of Θ(N 1/4 ), whilst the
per-node degree is four. This feature of the multi-mesh makes
it very attractive for use in P2P systems.
IV. P2P N ETWORK BASED ON M ULTI -M ESH
Our P2P Network design is centered around the generalized
multi-mesh. Each node in the peer-to-peer network at a given
instant of time assumes a particular position in this generalized
multi-mesh and the network grows incrementally. Initially, the
system designer sets a suitable value for the block size (n =
N 1/4 ).

GLOBECOM 2003

A. Routing in a Multi-Mesh Network

Four main algorithms are principal to the construction of all
P2P systems in general. The first two algorithms describe the
manner in which new nodes join and leave the P2P network.
The next two describe the procedures for inserting a new object
into the system and retrieving an object.
1) Join algorithm: The key in our case is the four tuple
(α, β, x, y), 1 ≤ α, β, x, y ≤ n, where (α, β) represents the
block position and (x, y) represents the node position in the
block. Each node in the overlay network is associated with a
pair of keys. The first one is called the Real Identifier (RId),
which is the position in the multi-mesh network that this node
occupied when it initially joined the network. Recall that the
GM grows incrementally. So, nodes joining successively in the
network have consecutive RId s. The second identifier termed

- 3842 -

0-7803-7974-8/03/$17.00 © 2003 IEEE

as the Virtual Identifier (V Id) is actually the key of the objects
for which this node acts as the home node. Both RId and
V Id are of the form (α, β, x, y) as mentioned earlier. Note
that RId of a node depends on the state of network when the
node joined the network. But, V Id is constant for a particular
node, obtained by hashing its unique identifier (for example,
the machine IP address). Each node maintains a routing table,
which has the neighbor RId s and their corresponding IP
addresses. In addition, every node maintains a small ’keytable’, which has the IP of the home node that has its V Id
same as the RId of this node. This key-table can be regarded
as a pointer to the home node.
It is assumed that a new node wishing to join the P2P
network knows at least one of the existing nodes in the
network. This existing node in the network acts on behalf of
the new node and issues the join request in the network. The
join operation is presented in the following algorithm.
Step 1) The new node sends a join(V Id, IP ) request to
one of the existing nodes in the network.
Step 2) The existing node finds the position of the new
node in the network, which is the new node’s RId. Recall
that the multi-mesh network grows incrementally row-by-row
in a block and, then block-by-block. If the current last block
in the network is an incomplete block, then this position is
the next position after the last node’s position in this block,
otherwise, it is the first node’s position in a new block.
Step 3) Next, a message is routed towards the node in the
network that has its RId same as the V Id of the new node,
informing it about the position of the new node. This step is
required since all requests for objects published at the new
node go to this node.
Step 4) Lastly, the routing table of the new node and its
neighbors are updated.
2) Leave algorithm: When a node leaves the network, it
leaves behind a void in the space it occupied in the multi-mesh
network. This void is filled up by the current last node (with
the highest RId) in the network. The departing node inserts
a special leave(RId, key-table) message into the network and
exits. This special message is routed towards the final node
in the network. The last node upon receiving this message,
updates its state, informs existing and new neighbors about
the changes and changes its RId to the RId in the message.
Node failures are handled in a similar way, except for the
fact that the node failures have to recognized by some node
that tried routing a message to the failed node. This node then
takes care of informing the last node in the network to fill up
the void left behind by the failed node.
3) Insert algorithm: The operation of publishing an object
in the system is the operation of inserting the object into
its home node. To insert an object, an insert(key, object)
message is routed towards the node that has its RId equal to
key. The destination node upon receiving this message checks
its key-table. If the key-table has an RId entry then, the object
is sent to the node in the key-table, otherwise, the object is
published in the destination node itself.

GLOBECOM 2003

4) Retrieve algorithm: The node that wants to retrieve an
object with a key, routes a retrieve(key) message towards the
node whose RId is equal to the key. The destination node upon
receiving this request checks its key-table to see if the home
node for this object exists. If the home node exists, then the
request is forwarded to the home node. If the home node does
not exist, then the destination node checks its local repository
for the object. If the object is found, the object is sent back to
the requester, otherwise, an error message informing that the
object does not exist in the system is sent to the requester.
C. Simulation Environment and Results
Our P2P network is comparable to 2-dimensional CAN,
since both the networks maintain a fixed per-node degree of
four and given N nodes, both have the same number of overlay
edges. While the CAN is based on the torus, our network
is based on multi-mesh. In all our simulation experiments,
we compare the performance of our network with CAN. Our
simulation environment consists of a physical network of 5500
nodes modelled as a Transit-Stub topology using the GTITM topology generator [8]. Transit-Stub topologies have 2level hierarchy of routing domains with transit domains that
interconnect lower level stub domains.
1) Performance Metrics: Our evaluations focuss on two
main performance aspects.
• Average stretch
• Average hop-length
Stretch is defined as the ratio of the path-length on the
overlay network to the path-length on the physical network.
Stretch gives a notion of how close the topology of the overlay
network is to the physical topology. This can be improved by
a variety of techniques including landmark ordering of nodes
[4]. In landmark ordering of nodes, every node is ordered in
the overlay network according to its relative distance measured
to a set of fixed nodes in the network called landmark nodes.
Landmark ordering technique can be applied to a number of
P2P systems including all those that we have discussed in this
paper. We currently have not implemented landmark ordering
in our P2P network, hence, all our measurements with CAN
are without the landmark ordering technique.
2) Simulation Procedure: Given the physical network and
the number of nodes in the overlay network (N ), we first
construct two P2P networks, one based on multi-mesh and
the other based on CAN . The same set of physical nodes
participate in both the P2P networks. Next k number of
random sender-receiver pairs are selected and for each pair,
a message is routed from the source to the destination in both
the networks. The stretch and the number of hops for each
sender-receiver pair are measured in both the networks. The
experiment is repeated for different values of N .
3) Simulation Results and Discussion: In all our experiments, we vary the number of overlay nodes from 81 to 4096
nodes. While it is possible to have an generalized multi-mesh
of any number of nodes, we had a complete multi-mesh in
all our experiments. CMMs are easier to construct than GMs
and further, both have approximately the same performance

- 3843 -

0-7803-7974-8/03/$17.00 © 2003 IEEE

metrics. All the results that we obtained with CMMs would
hold true even for GMs.
The first graph shows the average stretch as a function of
the number of overlay nodes in the network.

Fig. 3.

Average Stretch

The graph of Fig. 4. shows the number of routing hops as
a function of the size of the overlay network.

overlay topology of our P2P network is in a sense more
strongly connected than in CAN. In large P2P networks
typically consisting of several tens of thousands of nodes,
multi-mesh P2P network would provide far better performance
statistics than CAN.
V. F UTURE W ORK
Currently, the multi-mesh architecture exists only in two dimensions. However, it is possible to extend it to 3 dimensions
and in general to d dimensions. A two-dimensional multi-mesh
is superior to a two-dimensional torus in many aspects and a
d-dimensional multi-mesh would be superior compared with
its counterpart. A d-dimensional multi-mesh has 2d number
of neighbors. This gives us a clue that increasing the per-node
state space, i. e., going to higher dimensions, would result in
better routing performances. We have at present, extended the
multi-mesh to 3-dimensions. Due to the space limitation in
this paper, we will not be discussing the details of it here.
The effect of concurrent node failures in a multi-mesh P2P
network is yet to be determined. The number of simultaneous
node failures that would seriously impact the performance of
the network would help us understand the resilience of the
multi-mesh P2P network.
VI. C ONCLUSION
We have outlined the design and implementation of a
new P2P network based on the multi-mesh architecture. The
system supports dynamic node arrivals and departures. The
evaluations demonstrate the advantages of this network over
CAN. Analysis shows us that while still maintaining the same
number of nodes and edges as CAN, this network provides
better connectivity. This is indeed a unique property.

Fig. 4.

ACKNOWLEDGMENT
The authors would like to thank Dr. Sylvia Ratnasamy for
providing us with the source code for the CAN simulations.

Average Number of hops

The graph of Fig. 5. shows the variation of the maximum
number of hops for the selected source-destination pairs in
both CAN and multi-mesh P2P network.

Fig. 5.

Maximum Number of hops

It is evident from the results that the multi-mesh P2P
networks provide some advantages over CAN networks. The

GLOBECOM 2003

R EFERENCES
[1] A. Rowstron and P. Druschel, “Pastry: Scalable, Decentralized Object
Location and Routing for Large-scale Peer-to-Peer Systems”, Proceedings
of IFIP/ACM International Conference on Distributed Systems Platforms
(Middleware 2001), Heidelberg, Germany, November, 2001.
[2] B. Y. Zhao and J. Kubiatowicz, “Tapestry: An infrastructure for faulttolerant wide-area location and routing”, Technical Report, UCB CSD
01-1141, University of California at Berkeley, Computer Science Department, 2001.
[3] I. Stoica, R. Morris, D. Karger, F. Kashoek and H. Balakrishnan,
“Chord: A scalable Peer-To-Peer lookup service for internet applications”,
Proceedings of ACM SIGCOMM ’01, San Diego, USA, August, 2001.
[4] S. Ratnasamy, P. Francis, M. Handley, R. Karp and S. Shenker, “A Scalable Content-Addressable Network”, Proceedings of ACM SIGCOMM
’01, San Diego, CA, August, 2001.
[5] G. Plaxton, R. Rajaram and A. Richa, “Accessing Nearby Copies of
Replicated Objects in a Distributed Environment”, Proceedings of ACM
SPAA, June, 1997.
[6] D. Das, M. De and B. P. Sinha, “A New Network Topology with Multiple
Meshes”, IEEE transacations on Computers, vol. 48, no. 5, May 1999.
[7] A. Sen, S. Bandyopadhyay and B. P. Sinha, “A New Architecture and a
New Metric for Lightwave Networks”, Journal of Lightwave Technology,
vol. 19, no. 7, July 2001.
[8] E. Zegura, K. Calvert and S. Bhattacharjee, “How to Model an Internetwork”, Proceedings of IEEE Infocom ’96, San Francisco, CA, May,
1996.

- 3844 -

0-7803-7974-8/03/$17.00 © 2003 IEEE

Survivable Routing in WDM Networks - Logical
Ring in Arbitrary Physical Topology
Arunabha Sen, Bin Hao, Bao Hong Shen and Guohui Lin

Abstract—In this paper we consider the problem of routing the
lightpaths of a logical topology of a WDM network on an arbitrary physical topology, such that the logical topology remains connected even after the failure of a physical link. We focus our attention on the ring interconnection as the logical topology because
it is widely used in many protection schemes. We ﬁrst establish
the necessary and sufﬁcient condition for a ring logical topology to
withstand failure of a single physical link. Next we show that the
testing of this necessary and sufﬁcient condition is an NP-complete
problem. Finally, we give an algorithm for testing the necessary
and sufﬁcient condition and demonstrate the execution of the algorithm with the help of an example.

I. I NTRODUCTION
Survivabilty of high bandwidth optical networks has become
important area of research in recent times due to its tremendous importance as a national and international infrastructure
for moving large volumes of data from one part of the globe
to another. A signiﬁcant number of papers addressing various
aspects of survivability have appeared in the literature [2, 4-10,
12]. In a recent paper [7], Modiano and Narula-Tam introduced
the notion of survivable routing, where the objective is to route
the lightpaths (corresponding to the links of a logical topology)
in the physical topology in such a way, that failure of any single physical (optical ﬁber) link cannot disconnect the logical
network. As deﬁned in [7], a routing is known as survivable
if the failure of any physical link leaves the (logical) network
connected. A somewhat similar problem was studied in [2],
[10] where the objective was to support IP networks over WDM
networks.
We illustrate the concept of survivable routing with the help
of an example given in [7]. Suppose that the logical topology is a ring (ﬁgure 1a) and the physical topology is the network shown in ﬁgure 1b. The logical topology may be embedded in the physical topology in many different ways. For
the embedding, the nodes of the logical topology ﬁrst have to
be mapped onto the nodes of the physical topology and then
the links of the logical topology have to be mapped onto the
A. Sen is with the Dept. of Computer Science and Engineering, Arizona State
University, Tempe, AZ 85287, e-mail: asen@asu.edu.
Bin Hao is with the Dept. of Computer Science and Engineering, Arizona
State University, e-mail: binhao@asu.edu.
Bao Hong Shen is with the Dept. of Computer Science and Engineering,
Arizona State University, e-mail: bao@asu.edu.
Guohui Lin is with the Dept. of Computing Science, University of Alberta,
Edmonton, Canada, T6G 2E8, e-mail: ghlin@cs.ualberta.ca.

1
A

4

E

B

D

C

(a)

1

4

5

3

2

5

2
3

(b)

(c)

Fig. 1. Logical and physical topologies of WDM networks

paths in the physical topology. Consider the following embedding of the logical topology into the physical topology shown
in ﬁgures 1a and 1b respectively. The nodes are mapped as
A ! 1; B ! 2; C ! 3; D ! 4; E ! 5, and the edges are
mapped as (A B ) ! (1 2); (B C ) ! (2 3); (C D) !
(3 4); (D E ) ! (4 5) and (E
A) ! (5
4 1). If the
lightpaths are established using this routing, it is clear that the
failure of the ﬁber link between the nodes 4 and 5 will disconnect the logical topology as the node E will lose its connection
to both the nodes A and D. However, this situation can be
avoided by carrying out the edge mapping (routing of the lightpaths) in a slightly different way: (A B ) ! (1 2); (B
C ) ! (2
3); (C
D ) ! (3
4); (D
E ) ! (4
5) and
(E
A) ! (5
3 1). If this mapping (routing) is used, then
failure of any single physical link cannot disconnect the logical
topology (ring). Thus it is clear that the way the lightpaths are
routed has a tremendous impact on the survivability of the logical network. In this paper we examine the issues related to the
existence of a survivable routing of a logical ring in a physical
network of arbitrary topology.
In the previous example, it was possible to ﬁnd survivable
routing just by changing the edge mapping and without changing the node mapping. However, there exist instances where
an “incorrect” node mapping creates an environment where
no survivable routes can be found. The following example
illustrates the point. We consider mapping the same logical
topology (ﬁgure 1a) onto the same physical topology (ﬁgure
1b), but this time around the nodes are mapped as follows:
A ! 5; B ! 2; C ! 4; D ! 3, and E ! 1. It can be veriﬁed that no survivable routes between the nodes can be found
in this case. These two examples show that survivable routes
may be found if the nodes of the logical topology are mapped
“correctly” onto the nodes of the physical topology and may
not be found in case they are mapped “incorrectly”. However,
for certain physical topologies there may not be any “correct”

0-7803-7400-2/02/$17.00 © 2002 IEEE

2771

node mapping and as such survivable routes for a logical ring
cannot be found in such physical topologies. Survivable routes
for the logical ring in ﬁgure 1a cannot be found if the physical
topology is as shown in ﬁgure 1c.
It is clear from the previous example that survivable routes
for a logical ring can be found for some physical topologies
and cannot be found for some other topologies. In this paper
we investigate the necessary and sufﬁcient condition for the existence of survivable routes for a logical ring in any arbitrary
physical topology.
II. P ROBLEM F ORMULATION
The physical topology of the network is represented by an
undirected graph Gp = (Vp ; Ep ), where Vp is the set of nodes
and Ep is the set of physical links. Similarly, the logical topology is represented by another undirected graph Gl = (Vl ; El ),
where Vl is the set of nodes and El is the set of logical links. We
assume that jVp j = jVl j. The objective of the survivable routing problem is to ﬁnd a way to route (map) the logical topology
on the physical topology such that the logical topology remains
connected inspite of the failure of any one single physical link.
In order to establish a logical link between the nodes s and
t of the logical network, a lightpath needs to be established between the nodes f (s) and f (t) in the physical network, where
f (s) and f (t), are the images (or mappings) of the nodes s and
t in the physical network. Such a lightpath may use a set of
physical links and some wavelengths on these links. Since the
objective of this paper is to focus on the issues of a survivable
design, as in [7], we assume that either a sufﬁcient number of
wavelengths or a sufﬁcient number of wavelength converters
are available, so that the issues related to wavelength continuity
can be ignored.
We use the standard graph theoretical terminologies from [1].

i ) \ F (ej ) (i.e., e 2 F (ei ) \ F (ej ); ei 6= ej , then failure
of e would disconnect the logical links ei and ej . Since Gl is a
ring network, failure of the links ei and ej would disconnect Gl ,
contradicting the assumption that (f; F ) is a survivable routing
for Gl in Gp . Thus all F (ei )’s are pair-wise edge-disjoint. It is
not difﬁcult to check that F (e1 )F (e2 ) : : : F (en ) forms a closed
trail visiting each node of Gl at least once.
(= Suppose that Gp has a closed trail, w =
u(0)u(2):::u(m
1), visiting each node at least once. Here,
each u(i) is a node of Gp . It may be noted that for some i and
j , u(i) may be equal to u(j ) even though i 6= j . Suppose that
P (u(i); u(j )) denotes the path between the nodes u(i) and u(j )
on w.
Now we construct a mapping (f; F ).
F (e

begin
for (i := 0;

i < m; i

mark (u(i))
f (v0 )

:=

:=

i

+ 1)

:= 0;

u(0)

mark (u(0))

:= 1;
:= 1;
j := 1;
while (j < n) do fn is the number of nodes in Gl g
begin
while (mark (u(index)) = 1) do
index := index + 1;
f (vj ) := u(index)
mark (u(vj )) := 1;
index := index + 1;
j := j + 1 ;
end
for (i := 1; i < n; i := i + 1)
F (ei ) := P (f (vi 1 ); f (vi ));
F (en ) := P (f (vn 1 ); f (v0 ));
index

end
III. S URVIVABLE ROUTING OF R ING IN A RBITRARY
P HYSICAL T OPOLOGY

We claim that (f; F ) is a survivable routing. First, since
visits each node of Gp at least once, f () is a mapping
from Vl to Vp . Since w is a trail, the way the function F () is
constructed, F (ei ) \ F (ej ) = ; for all i and j when i 6= j .
Therefore, the failure of one link in Gp will disconnect at most
one edge in Gl , leaving the logical network (ring) connected.
Thus (f; F ) is survivable.
w

Let (f; F ) denote a mapping between two graphs, where f is
the node mapping, F is the edge-to-path mapping.
Theorem 1: Let Gp = (Vp ; Ep ) and Gl = (Vl ; El ) represent
the physical and the logical topology of the network respectively. If Gl is a ring network, then there exists a survivable
routing for Gl in Gp if and only if Gp contains a closed trail
visiting each node at least once.
Proof: Suppose that the logical ring network is the cycle with
n nodes represented as v0 e1 v1 e2 v2 : : : en 1 vn 1 en v0 such that
for 1  i  n 1, the ends of ei are vi 1 and vi and the end
of en is vn and v0 . Let (f; F ) denote a mapping of Gl in Gp ,
where f (vi ) is the mapping of the nodes of Gl onto the nodes of
Gp and F (ei ) is the mapping of the edges of Gl onto the paths
of Gp (paths may be of length one).
=) Suppose that there is a survivable routing for Gl in Gp
given by (f; F ). Then for all ei 6= ej , F (ei ) \ F (ej ) = ;. This
is true because, if there was an edge e of the physical graph in

As noted earlier an Euler tour is a tour which traverses each
edge exactly once. Since such a tour traverses each edge exactly once, it must be traversing each node at least once. The
necessary and sufﬁcient condition for a graph to have an Euler
tour is given by the following theorem [1].
Theorem 2: A nonempty connected graph is eulerian if and
only if it has no nodes of odd degree.
As stated in theorem 1, it is possible to ﬁnd a survivable routing for a logical ring network in a physical network of arbitrary
topology, if and only if the physical network Gp = (Vp ; Ep )
contains a closed trail visiting each node at least once. Suppose

2772

that G0p = (Vp0 ; Ep0 ) is a subgraph of Gp such that (i) Vp = Vp0
and Ep0  Ep and G0p is eulerian.
It is not difﬁcult to verify that Gp = (Vp ; Ep ) contains a
closed trail visiting each node at least once, if and only if such
a subgraph G0p of Gp exists. From theorem 2 it is known that G0p
will be eulerian if and only if G0p is connected and has no node
with odd degree. It may be noted that the physical network Gp
may or may not have such a subgraph G0p . Thus a survivable
routing for a logical ring network in a physical network, Gp ,
of arbitrary topology exists if and only if Gp contains such a
subgraph G0p .
Survivable Routing of Ring Problem (SRRP)
Instance: A connected undirected graph G = (V ; E ).
Question: Does G have a subgraph G0 = (V 0 ; E 0 ), such that
0 and E 0  E , such that G0 is connected and has no
V = V
node with an odd degree.
We prove that SRRP is NP-complete by restricting it to cubic
graphs (it may be recalled that a graph is called cubic if all the
nodes in the graph are of degree 3)
Survivable Routing of Ring Problem in Cubic Graph (SRRPC)
Instance: A connected undirected cubic graph G = (V ; E ).
Question: Does G have a subgraph G0 = (V 0 ; E 0 ), such that
0 and E 0  E , such that G0 is connected and has no
V = V
node with an odd degree.
Hamiltonian Cycle Problem in Planar, Cubic and Triply connected Graph (HCPPCT)
Instance: A connected undirected graph G = (V ; E ), which is
(i) planar, (ii) cubic and (iii) triply connected (i.e., deletion of
any two nodes leaves the graph connected).
Question: Does G contain a Hamiltonian Cycle?
It has been proven in [3] that HCPPCT is NP-complete.
Hamiltonian Cycle Problem in Cubic Graph (HCPC)
Instance: A connected undirected cubic graph G = (V ; E ).
Question: Does G contain a Hamiltonian Cycle?
HCPC is NP-complete because HCPPCT, a restricted version
of HCPC is NP-complete.
Theorem 3: Survivable Routing of Ring Problem in Cubic
Graph is NP-complete.
Proof: Clearly, SRRPC is in NP as it is fairly simple to check if
0
G is connected and has no nodes with odd degree.
We will give a transformation from the known NP-complete
problem HCPC. We take the instance of SRRPC to be the same
as the instance of HCPC. Suppose that this instance is G =
(V ; E ). We claim that G has a Hamiltonian cycle if and only if
there is subgraph G0 = (V 0 ; E 0 ) in G that is connected and has
no node with odd degree.
Suppose that G = (V ; E ) contains a Hamiltonian cycle.
Construct a subgraph G0 = (V 0 ; E 0 ) as follows: V 0 = V
and E 0 is the set of edges that make up the Hamiltonian cycle. Clearly, G0 is connected and has no node with odd degree
(all nodes of G0 have degree 2).

Conversely, suppose that G = (V ; E ) has a subgraph G0 =
(V 0 ; E 0 ) such that V 0 = V , E 0  E and G0 has no node of odd
degree. Since G is cubic graph all the nodes of G are of degree
3. Since G0 does not have any node with odd degree, all nodes
of G0 must be of degree 2. Since all nodes of G0 are of even
degree (2), G0 has a Euler tour. Since all nodes of G0 are of
degree 2, this Euler tour is also a Hamiltonian cycle of G0 and
hence G. This proves the theorem.
Theorem 4: Survivable Routing of Ring Problem is NPcomplete.
Proof: SRRP is NP-complete because SRRPC, a restricted version of SRRP is NP-complete.
IV. A LGORITHM FOR S URVIVABLE ROUTING
In this section, we describe an algorithm for ﬁnding survivable routes, if they exist, in the physical network Gp . To this
end, we ﬁrst prove a theorem.
Theorem 5: Suppose that Gp = (Vp ; Ep ) is the physical
topology. Suppose that S , (S  Ep ) is an edge-minimal subset
of Ep such that G0p = (Vp ; Ep S ) is Eulerian. Let GS denote
the graph formed by the set of edges S together with the corresponding nodes. For all such S  Ep , GS can be decomposed
into paths with both endpoints being odd degree nodes in Gp .
Proof: In Gs , pair (arbitrarily) up odd degree vertices and
connect each pair by an artiﬁcial edge, get the Euler tour of the
resultant graph, and delete the artiﬁcial edges from the tour to
get a set of paths each of which connect two odd degree vertices
of Gs . Please note that each odd degree vertex in Gs must have
an odd degree in Gp . Therefore, each of these paths connects
two odd degree vertices in Gp .
The above theorem can be utilized to develop an algorithm
for determining if a survivable route for the logical ring can be
found in the physical topology Gp . From the earlier theorems,
we know that if Gp has only nodes of even degree, then survivable routes exist and they can be found fairly easily. If Gp
has nodes of both even and odd degree, then existence of surviable routes will depend on the existence of a subset of edges
S  Ep , whose removal from Gp , would make the remaining
graph eulerian. We now discuss a method to test the existence
of such a set S . The algorithm has two phases.
A. Phase I
It is known that in any graph the number of nodes with odd
degree is even [1]. Suppose Gp contains 2k nodes of odd degree. From the above theorem, we know that if an edge minimal subset S of Ep exists, whose removal makes the remaining
graph eulerian, then this set S , together with the corresponding
nodes makes a forest. It is also known that this forest can be
decomposed into k paths with the endpoints of the paths being
the nodes with odd degree in Gp .
To illustrate the execution of our algorithm, we choose a nonplanar version of ARPANET with 20 nodes and 32 links [11]
shown in ﬁgure 2. In this example, the following set of nodes

2773

have odd degree, f1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 15, 16, 17, 18,
20g. We refer to these nodes as problem nodes. The “problem”
associated with these nodes (i.e., their odd degree) can be resolved by removal of an edge incident on these nodes. Suppose
that v is a node of odd degree in Gp and it is adjacent to t other
nodes u1 ; u2 ; : : : ; ut . If one of these nodes ui ; (1  i  t)
is also of odd degree, then removal of the edges (v ui ) will
resolve the “problem” associated with both the nodes v and ui .
However, if the node ui is of even degree, then removal of the
edge (v ui ) will resolve the problem associated with node v ,
but will introduce a new problem at node ui . In a sense, the
removal of this edge merely shifts the problem from node v to
node ui . In this case, we should examine all the adjacent nodes
of ui (except v ) and remove one such edge to “ﬁx” the problem
at node ui . The process continues till we ﬁnd an edge whose
removal does not introduce a new “problem” at another node.

In the network of ﬁgure 2, node 3 is a “problem” node
because it is a node of odd degree. We can “ﬁx” the problem
at node 3 by removing any one of the edges e2 ; e9 or e10 . The
removal of the edge e9 ﬁxes node 3’s problem and does not
introduce any new problem. In fact, in addition to solving node
3’s problem it also “solves” node 7’s problem. Therefore, the
removal of the edge e9 is a “complete solution” for the nodes
3 and 7. However, if instead of e9 , e10 is removed then it
solves the problem at node 3 but introduces a problem at node
8. Thus removal of the edge e10 is only a “partial solution” to
the problem at node 3. To obtain a complete solution from this
partial solution and to ﬁx the problem at node 8 introduced by
the deletion of the edge e10 , we need to delete one additional
edge at node 8. If e13 is deleted, then it solves the problem
at node 8 as well as at node 9 and does not introduce any
new problem. Thus the removal of the set of edges fe10 ; e13 g
is a complete solution for the nodes 3 and 9. Similarly, the
removal of the set of edges fe10 ; e16 g is a complete solution
for the nodes 3 and 12. If e15 is removed, after the removal of
e10 , the problem propagates to node 10. A complete solution
can be derived by extending the partial solution obtained by
the path formed by e10 and e15 . The set of complete solutions obtained by extending the partial solution fe10 ; e15 g are

ff 10 15 11 (7)g f 10 15
f 10 15 17 31 (17)g f 10
e

e

;e

;e

;e

;e

;

;e

e

;e

;

e

g f 10

; e14 (9) ;

e

g
gg. The number

; e15 ; e17 ; e26 (20) ;

; e15 ; e17 ; e27 (16)

within () indicates the “other” node (besides node 3), whose
problem is “solved” by the removal of the corresponding set of
edges. We refer to this node as the terminal node. As a ﬁrst
step towards ﬁnding the survivable route, we ﬁrst construct
all such “complete solutions”. For the graph of ﬁgure 2, all
such complete solutions are shown in table I. The complete
solutions can be obtained by performing depth ﬁrst search on
the graph Gp . Please note that to avoid redundancy, we only
list the complete solutions, where the index of the problem
node is less than the index of the terminal node.

Problem node
1
1
1
2
2
3
3
3
3
3
3
3
3
4
4
5
6
7
7
7
7
7
7
9
9
9
9
9
11
12
12
12
12
13
13
15
16
16
16
17
17

Solution Edges
e1
e7
e2
e3
e4
e9
e10 ; e13
e10 ; e16
e10 ; e15 ; e11
e10 ; e15 ; e14
e10 ; e15 ; e17 ; e26
e10 ; e15 ; e17 ; e31
e10 ; e15 ; e17 ; e27
e5
e12
e6
e8
e11 ; e14
e11 ; e15 ; e13
e11 ; e15 ; e16
e11 ; e17 ; e31
e11 ; e17 ; e26
e11 ; e17 ; e27
e30
e14 ; e17 ; e31
e14 ; e17 ; e26
e14 ; e17 ; e27
e14 ; e15 ; e16
e18
e20
e19 ; e21
e19 ; e29
e19 ; e32
e23
e24
e28
e22
e27 ; e26
e27 ; e31
e31
e25

Terminal Node
2
6
3
4
5
7
9
12
7
9
20
17
16
5
11
6
7
9
9
12
17
20
16
11
17
20
16
12
13
18
18
15
20
15
17
16
18
20
17
19
20

TABLE I

B. Phase II
Once the set of “complete solutions” associated with a pair of
nodes is obtained, we try to combine them to obtain complete
solution to all the problem nodes. If such a solution can be
found (the solution is a set of edges S indicated earlier), then
survivable routes for the ring in the given physical topology can
be found. Otherwise, survivable routing for the ring in Gp is
impossible. To demonstrate the combination process, we again
use the example of ﬁgure 2. As noted earlier, the problem nodes
in this example are f1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 15, 16,
17, 18, 20g. From the set of complete solutions in table I, we
choose one solution after another till all the problem nodes are
“ﬁxed”. The process constructs the set S (if it exists) and works

2774

20
17

e31

e24
4
e12

e3

11

e5

2

e23

e30
9
e7

1
e2

6

e14
e8

e11

7

e9
3

Fig. 2. Euler Tour in G0p = (Vp ; Ep

e22
e29

e16

e10

16

e17

10
e15

e13

e 27

e28

15

e6

e1

e26
19

e32

13
e18

5

e4

e25

e19

14

8

e21

e20

We continue with the backtracking process, till we ﬁnd
a feasible solution or conclude that no feasible solution exists. In the example of ﬁgure 2, during the sixth attempt of
the backtracking process we ﬁnd the following feasible solution: fe7 ; e4 ; e9 ; e12 ; (e14 ; e17 ; e27 ); e20 ; e23 ; e25 g. If this set of
edges is removed, all the nodes will be of even degree and as
such an Euler tour can be constructed in the new graph. The
Euler tour is shown in ﬁgure 2. The tour is: (2 ! 1 ! 3 !

! 12 ! 14 ! 18 ! 16 ! 15 ! 14 ! 20 ! 19 ! 17 !
! 11 ! 9 ! 8 ! 10 ! 7 ! 6 ! 5 ! 4 ! 2).

13

Suppose that the nodes of a 20 node logical ring are labeled
from A to T , with the logical edges going from A B , B C ,
: : :, S
T, T
A. Then the following mapping of the nodes of
the logical graph to the nodes of the physical graph will create
surviable routes. A ! 2; B ! 1; C ! 3; D ! 8; E !

18

12

8
S)

as follows. The set S is empty initially.

! 14 ! 18 ! 16 ! 15
! 17 ! 13 ! 11 ! 9
! 6 ! 5 ! 4.

12; F

;G

19; L

;M

7; R

;S

;H

;N

;I

;O

;J

;P

! 20
! 10

;K
;Q

!
!

;T

Attempt 1: Include the edge e1 in S . This ﬁxes the problem
for nodes 1 and 2. Mark these nodes as ﬁxed. Since the next
higher indexed node that needs ﬁxing is node 3, we include
the edge e9 in S . Inclusion of this node ﬁxes the problem for
nodes 3 and 7. Mark these nodes as ﬁxed. To ﬁx the next higher
indexed problem node (4) we choose e5 for inclusion in S . This
ﬁxes the nodes 4 and 5. Mark these nodes as ﬁxed. Now to ﬁx
the next higher indexed problem node (6) we need to choose
e8 which would ﬁx the problem of node 7. But the problem
of node 7 was already ﬁxed when we included the edge e9 in
S . As such we cannot choose e8 now, and these set of edges
fe1; e9 ; e5 g cannot lead to a complete solution for all the nodes.
Because of this failure to ﬁnd a solution, we backtrack and
make a second attempt.

V. C ONCLUSION
In this paper we have studied the issues related to survivable
routing of a logical ring in a physical network of arbitrary topology. There are three important contributions of paper on this
problem. First, we have given a necessary and sufﬁcient condition for the existence of survivable routes. Second, we have
shown the problem of determining whether or not the condition
is satisﬁed by an arbitrary physical topology is NP-complete.
Third, we have presented an algorithm for ﬁnding the survivable routes, if they exist in the physical network. We are currently investigating the issues related to the existence of survivable routes when the logical topology also has an arbitrary
structure.

Attempt 2: This time around, after choosing e1 and e9 , instead of e5 , we choose e12 , which ﬁxes the problem at node 11.
Next, to ﬁx the problem at node 5, we choose e6 which also
ﬁxes the problem at node 6. Now, the unﬁxed problem node
with the smallest index is 9 . We cannot choose the edge e30 to
ﬁx node 9’s problem because it would also have solved problem
at node 11. However, the problem at node 11 is already solved
when we chose the edge e12 . Therefore, instead of choosing
e30 , we choose the set fe14 ; e17 ; e31 g, which ﬁxes node 17. To
ﬁx the problem at node 12, we choose e20 which also ﬁxes the
problem at node 18. To ﬁx the problem at node 13, we choose
e23 which also ﬁxes the problem at node 15. To ﬁx the problem at node 16, we cannot choose e22 because the problem with
its terminal node (18) was already ﬁxed by e20 . Therefore, we
choose the edge set fe27 ; e26 g, which ﬁxes the node 20. If S
= fe1 ; e9 ; e12 ; e6 ; (e14 ; e17 ; e31 ); e20 ; e23 ; (e27 ; e26 )g, the set of
edges constructed by this process is removed from the graph
Gp , no nodes will have an odd degree in the remaining graph.
However, if this set of edges are removed, the remaining graph
will also be disconnected, because node 19 will have all its incident edges removed. Accordingly, this S is also not a feasible
solution.

R EFERENCES
[1] J. A. Bondy and U. S. R. Murthy, Graph Theory with Applications, North
Holland, 1976.
[2] O. Crochat and J. Y. Le Boudec, “Design Protection for WDM Optical
Networks”, IEEE JSAC, Vol. 16, No. 7, Sept. 1998.
[3] M.R. Garey, D.S. Johnson and R. E. Tarjan, “The planar Hamiltonian
circuit problem is NP-complete”, SIAM Journal onf Computing, vol. 5,
no. 4, pp. 704-714, December 1976.
[4] O. Gerstel and R. Ramaswami, “Optical layer survivability: A services
perspective”, IEEE Communications Magazine, vol. 38, no. 3, pp. 104113, March 2000.
[5] N. Ghani, S. Dixit and T. S. Wang,“On IP-over-WDM Integration”, IEEE
Communications Magazine, vol. 38, no. 3, pp. 72-84, March 2000.
[6] M. Medard, S. Finn and R. A. Barry, “WDM Loop-back Recovery in
Mesh Networks”, Infocom ’99, New York, March, 1999.
[7] E. Modiano and A. Narula-Tam, “Survivable routing of logical topologies
in WDM networks”, Proc. IEEE INFOCOM’01, 2001.
[8] S. Ramamurthy and B. Mukherjee, “Survivable WDM Mesh Netwoks”,
Proc. of IEEE INFOCOM’99, pp. 744-751, March, 1999.
[9] S. Ramamurthy and B. Mukherjee, “Survivable WDM Mesh Netwoks:
Part II -Restoration,”, ICC ’99, Vancouver, CA, June, 1999.
[10] G. H. Sasaki, C. F. Su and D. Blight,“Simple layout algorithms to maintain network connectivity under faults”, Proc. of 2000 Annual Allerton
Conference, Champaign Il, October 2000.
[11] T. E. Stearn and Krishna Bala, Multiwavelength Optical Networks - A
Layered Approach, Addison Wesley, 1999.
[12] H. Zang, J. P. Jue and B. Mukherjee, “A review of routing and wavelength
assignment approaches for wavelength-routed optical WDM networks,”
Optical Networks Magazine, January 2000.

2775

Survivable Routing in WDM Networks
Arunabha Sen, Bin Hao and Bao Hong Shen
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287, USA
fasen, binhao, baog@asu.edu
Abstract
In this paper we consider the problem of routing the
lightpaths of a logical topology of a WDM network on an
arbitrary physical topology, such that the logical topology
remains connected even after the failure of a physical link.
In a recent paper, Modiano et. al. introduced the notion
of survivable routing and established a necessary and sufficient condition for the existence survivable routes of a logical topology in a physical topology. In this paper we show
that problem of determining whether survivable routing is
possible for a logical topology in a given physical topology is an NP-Complete problem. Moreover, we show that
the problem remains NP-complete, even when the logical
topology is restricted to be a ring with a specific ordering
of the nodes.

1 Introduction
Survivabilty of high bandwidth optical networks has become an important area of research in recent times due to
its tremendous importance as a national and international
infrastructure for moving large volumes of data from one
part of the globe to another. A significant number of papers
addressing various aspects of survivability have appeared in
the literature [2, 3, 5, 6, 7]. Many of these studies focus on
restoration mechanisms to restore all lightpaths in the event
of physical link failure. In a recent paper [6], Modiano
and Narula-Tam introduced the notion of survivable routing, where the objective is to route the lightpaths (corresponding to the links of a logical topology) in the physical
topology in such a way, that failure of any single physical
(optical fiber) link cannot disconnect the logical network.
As defined in [6], a routing is known as survivable if the
failure of any physical link leaves the (logical) network connected. A somewhat similar problem was studied in [2, 7]
where the objective was to support IP networks over WDM
networks. They studied the problem of layout of IP network

topology over the WDM network so that the IP network remains connected under single fiber-link failure.
We illustrate the concept of survivable routing with the
help of an example given in [6]. Suppose that the logical topology is a ring (figure 1a) and the physical topology is the network shown in figure 1b. The logical topology may be embedded in the physical topology in many
different ways. For the embedding, links of the logical
topology have to be mapped onto the paths in the physical topology. If the logical link (u1 -u2 ) is mapped onto
the physical path (v1 v2 : : : vk ), we denote it by
L(u1 u2 ) ! P (v1 v2 : : : vk ). Consider the following embedding of the logical topology (figure 1a) into the
physical topology shown in figure 1b.
Two possible embedding of the logical topology in the
physical topology are as follows:
Embedding 1: L(1 2) ! P (1 2), L(2 3) ! P (2 3),
L(3 4) ! P (3 4), L(4 5) ! P (4 5), L(5 1) !
P (5 4 1).
Embedding 2: L(1 2) ! P (1 2), L(2 3) ! P (2 3),
L(3 4) ! P (3 4), L(4 5) ! P (4 5), L(5 1) !
P (5 3 1).
It can be observed that in embedding 1, failure of the
fiber link between the nodes 4 and 5 will disconnect the logical topology, whereas no single link failure will disconnect
the logical topology in embedding 2. Thus it is clear that
the way the lightpaths are routed has a tremendous impact
on the survivability of the logical network. In the example,
survivable routes could be found if the logical links were
mapped on the physical paths in the “correct” way and could
not be found if they were not. However, for certain physical topologies there may not be any “correct” link mapping
and as such survivable routes for a logical topology cannot
be found in such physical topologies. Survivable routes for
the logical ring in figure 1a cannot be found if the physical
topology is as shown in figure 1c. In this paper we examine
the issues related to the existence of a survivable routing of
a logical topology in a physical network of arbitrary topology.

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE

1

1

4

2

5

3

4

(a)

1

4

5

3

2

(b)

2

5

3

(c)

Figure 1. Logical and physical topologies of WDM networks
It may be noted that as in [6], we assume in this paper
that both the logical and physical topologies of the network
are given and we try to find if survivable routes for the given
logical topology exist in the given physical topology. A
number of researchers have examined the issues of protection and restoration in WDM network and have suggested
the use of protection cycles [3] and other techniques [5].
However, as noted in [6], in many cases providing protection at the optical as well as electronic layer may be somewhat redundant and a less stringent requirement that the network remain connected in spite of the failure of any single
fiber link may be sufficient. This paper addresses the issues
related to such cases.

2 Problem Formulation
The physical topology of the network is represented by
an undirected graph Gp = (Vp ; Ep ), where Vp is the set
of nodes and Ep is the set of physical links. Similarly, the
logical topology is represented by another undirected graph
Gl = (Vl ; El ), where Vl is the set of nodes and El is the
set of logical links. We assume that Vp = Vl . The objective
of the survivable routing problem is to find a way to route
(map) the logical topology on the physical topology such
that the logical topology remains connected inspite of the
failure of any one single physical link.
In order to establish a logical link between the nodes s
and t of the logical network, a lightpath needs to be established between the nodes f (s) and f (t) in the physical network, where f (s) and f (t), are the images (or mappings)
of the nodes s and t in the physical network. Such a lightpath may use a set of physical links and some wavelengths
on these links. Since the objective of this paper is to focus
on the issues of a survivable design, as in [6], we assume
that either a sufficient number of wavelengths or a sufficient
number of wavelength converters are available, so that the
issues related to wavelength continuity can be ignored. It
may be noted that it is possible that f (s) = s and f (t) = t.

Survivable Routing Problem (SRP)
Instance: Two connected undirected graphs Gp = (Vp ; Ep )
and Gl = (Vl ; El ) representing the physical and the logical
topology of the network respectively, (Vp = Vl ).
Question: Is it possible to map the edges of Gl into paths
of Gp such that removal of any one link from Gp would not
disconnect Gl ?
We will prove that SRP is NP-complete, by considering a
restricted version of the problem when the logical topology
is a ring. We will consider two different versions, (i) node i
of the logical topology is mapped to node i of the physical
topology for all i, and (ii) node i of the logical topology is
not necessarily mapped to node i of the physical topology.
In our notation the first situation is described by f (s) = s.
We use standard graph theoretical terminologies from
[1]. For the convenience of our proof, we consider closed
trails [1] with different origins to be different. For example,
1 ! 2 ! 3 ! 1 ! 4 ! 5 ! 1 is considered to be a
different closed trail from 2 ! 3 ! 1 ! 4 ! 5 ! 1 ! 2.
A graph is called cubic if all the nodes in the graph are of
degree 3.

3 Survivable Routing of a Logical Ring
Let (f; F ) denote a mapping between two graphs, where
f is the node mapping, F is the edge-to-path mapping.
Theorem 1 Let Gp = (Vp ; Ep ) and Gl = (Vl ; El ) represent the physical and the logical topology of the network
respectively. If Gl is a ring network, then there exists a survivable routing for Gl in Gp if and only if Gp contains a
closed trail visiting each node at least once.
Proof: Suppose that the logical ring network is the cycle
with n nodes represented as v0 e1 v1 e2 v2 : : : en 1 vn 1 en v0
such that for 1  i  n 1, the ends of ei are vi 1 and
vi and the ends of en are vn and v0 . Let (f; F ) denote a
mapping of Gl in Gp , where f (vi ) is the mapping of the
nodes of Gl onto the nodes of Gp and F (ei ) is the mapping
of the edges of Gl onto the paths of Gp (paths may be of
length one).

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE

=) Suppose that there is a survivable routing for Gl in
Gp given by (f; F ). Then for all ei 6= ej , F (ei ) \ F (ej ) =
;. This is true because, if there was an edge e of the physical graph in F (ei ) \ F (ej ) (i.e., e 2 F (ei ) \ F (ej ); ei 6= ej ,
then failure of e would disconnect the logical links ei and
ej . Since Gl is a ring network, failure of the links ei
and ej would disconnect Gl , contradicting the assumption
that (f; F ) is a survivable routing for Gl in Gp . Thus all
F (ei )’s are pair-wise edge-disjoint. It is not difficult to
check that F (e1 )F (e2 ) : : : F (en ) forms a closed trail visiting each node of Gl at least once.
(= Suppose that Gp has a closed trail, w =
u(0)u(2):::u(m 1), visiting each node at least once. Here,
each u(i) is a node of Gp . It may be noted that for some i
and j , u(i) may be equal to u(j ) even though i 6= j . Suppose that P (u(i); u(j )) denotes the path between the nodes
u(i) and u(j ) on w.
Now we construct a mapping (f; F ).

begin
for (i := 0; i < m; i := i + 1)
mark (u(i)) := 0;

f (v0 ) := u(0)
mark (u(0)) := 1;
index := 1;
j := 1;
while (j < n) do
fn is the number of nodes in Gl g
begin
while (mark (u(index)) = 1) do
index := index + 1;

f (vj ) := u(index)
mark (u(vj )) := 1;
index := index + 1;
j := j + 1 ;

end
for (i := 1; i < n; i := i + 1)

end

F (ei ) := P (f (vi 1 ); f (vi ));
/*P (f (vi 1 ); f (vi )) denotes the
path from the node f (vi 1 ) to the node
f (vi ) on the closed trail,
w = u(0)u(2):::u(m 1)
in the physical network Gp */
F (en ) := P (f (vn 1 ); f (v0 ));

We claim that (f; F ) is a survivable routing. First,
since w visits each node of Gp at least once, f () is a
mapping from Vl to Vp . Since w is a trail, the way the
function F () is constructed, F (ei ) \ F (ej ) = ; for all i
and j when i 6= j . Therefore, the failure of one link in
Gp will disconnect at most one edge in Gl , leaving the
logical network (ring) connected. Thus (f; F ) is survivable.

As noted earlier an Euler tour is a tour which traverses
each edge exactly once. Since such a tour traverses each
edge exactly once, it must be traversing each node at least
once. The necessary and sufficient condition for a graph to
have an Euler tour is given by the following theorem [1].
Theorem 2 A nonempty connected graph is eulerian if and
only if it has no nodes of odd degree.
As stated in theorem 1, it is possible to find a survivable
routing for a logical ring network in a physical network of
arbitrary topology, if and only if the physical network Gp =
(Vp ; Ep ) contains a closed trail visiting each node at least
once. Suppose that G0p = (Vp0 ; Ep0 ) is a subgraph of Gp
such that (i) Vp = Vp0 and Ep0  Ep and G0p is eulerian.
It is not difficult to verify that Gp = (Vp ; Ep ) contains
a closed trail visiting each node at least once, if and only
if such a subgraph G0p of Gp exists. From theorem 2 it is
known that G0p will be eulerian if and only if G0p is connected and has no node with an odd degree. It may be noted
that the physical network Gp may or may not have such a
subgraph G0p . Thus a survivable routing for a logical ring
network in a physical network, Gp , of arbitrary topology
exists if and only if Gp contains such a subgraph G0p .
Survivable Routing of Ring Problem (SRRP)
Instance: A connected undirected graph G = (V; E ).
Question: Does G have a subgraph G0 = (V 0 ; E 0 ), such
that V = V 0 and E 0  E , G0 is connected and has no node
with an odd degree.
We prove that SRRP is NP-complete by restricting it to cubic graphs (it may be recalled that a graph is called cubic if
all the nodes in the graph are of degree 3)
Survivable Routing of Ring Problem in Cubic Graph
(SRRPG)
Instance: A connected undirected cubic graph G = (V; E ).
Question: Does G have a subgraph G0 = (V 0 ; E 0 ), such
that V = V 0 and E 0  E , such that G0 is connected and
has no node with an odd degree ?
Hamiltonian Cycle Problem (HCP)
Instance: A connected undirected graph G = (V; E ).
Question: Does G contain a Hamiltonian Cycle ?
Hamiltonian Cycle Problem in Planar, Cubic and Triply
connected Graph (HCPPCT)
Instance: A connected undirected graph G = (V; E ), which
is (i) planar, (ii) cubic and (iii) triply connected (i.e., deletion of any two nodes leaves the graph connected).
Question: Does G contain a Hamiltonian Cycle ?
It has been proven in [4] that HCPPCT is NP-complete.
Hamiltonian Cycle Problem in Cubic Graph (HCPC)
Instance: A connected undirected cubic graph G = (V; E ).

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE

Question: Does G contain a Hamiltonian Cycle ?
HCPC is NP-complete because HCPPCT, a restricted version of HCPC is NP-complete.
Theorem 3 Survivable Routing of Ring Problem in Cubic
Graph (SRRPCG) is NP-complete.
Proof: Clearly, SRRPCG is in NP as it is fairly simple to
check if G0 is connected and has no nodes with odd degree.
We will give a transformation from the known NPcomplete problem HCPC. We take the instance of SRRPCG
to be the same as the instance of HCPC. Suppose that this
instance is G = (V; E ). We claim that G has a Hamiltonian
cycle if and only if there is subgraph G0 = (V 0 ; E 0 ) in G
that is connected and has no node with an odd degree.
Suppose that G = (V; E ) contains a Hamiltonian cycle.
Construct a subgraph G0 = (V 0 ; E 0 ) as follows: V 0 = V
and E 0 is the set of edges that make up the Hamiltonian
cycle. Clearly, G0 is connected and has no node with an odd
degree (all nodes of G0 have degree 2).
Conversely, suppose that G = (V; E ) has a subgraph
G0 = (V 0 ; E 0 ) such that V 0 = V , E 0  E and G0 has no
node of odd degree. Since G is a cubic graph all the nodes
of G are of degree 3. Since G0 does not have any node with
an odd degree, all nodes of G0 must be of degree 2. Since
all nodes of G0 are of even degree (2), G0 has a Euler tour.
Since all nodes of G0 are of degree 2, this Euler tour is also
a Hamiltonian cycle of G0 and hence G. This proves the
theorem.
Theorem 4 Survivable Routing of Ring Problem is NPcomplete.
Proof: SRRP is NP-complete because SRRPCG, a restricted version of SRRP is NP-complete.
In the next section, we consider the situation f (s) = s,
i.e., the nodes of the logical topology is labeled exactly the
same way as the nodes of the physical topology.

4 Ordered Traversal of Closed Trail (OTCT)
Problem
In this section, we consider an “ordered” version of the
survivable routing of rings. In the previous discussion of
this paper, the node-mapping, from Vl onto Vp is not given.
Now, suppose Gl is still a ring, and the node-mapping, f ,
is given, we try to see if there exists an edge-mapping, F ,
from El to Ep , such that (f; F ) forms a survivable routing
for Gl in Gp . Suppose jVl j = jVp j = n, and nodes in Vl
are labeled as 1; 2; :::; n, where i is adjacent to i + 1, for all
i 2 f1; :::; n 1g, and n is adjacent to 1. Without loss of
generality, we can also let Vp = f1; 2; :::ng. Then, a nodemapping is nothing but a permutation (an order),  , of Vp

(or say the set f1; 2; :::; ng); and to find an edge-mapping is
the same as to find a set of paths, Pi , such that Pi connects
 (i) and  (i + 1), for all i 2 f1; :::; n 1g, and Pn connects
 (n) and  (1).
By the proof of Theorem 4, the Pi ’s must be
pairwise edge-disjoint. Hence,  (1)P1  (2)P2 ::: (n
1)Pn 1  (n)Pn  (1) is a closed trail, which contains  =
 (1) (2)::: (n) as a subsequence.
Therefore, we have the following corollary of Theorem
4.
Corollary 5 : Let Gp = (Vp ; Ep ) and Gl = (Vl ; El ) represent the physical and the logical topology of the network
respectively. Suppose that the node mapping is such that
f (v ) = v , for all nodes. Suppose that Gl is a ring network and  is the permutation of the nodes of Vl of that
defines the interconnection of Gl . In this case, there exists
an edge-mapping, F , from El to Ep such that (f; F ) forms
a survivable routing for Gl in Gp if and only if Gp contains
a closed trail, which contains  as a subsequence.
Ordered Traversal of Closed Trail Problem
Instance: A connected undirected graph G = (V; E ), a permutation  of the nodes of V .
Question: Is there a closed trail CT in G, such that the
nodes of G appear in the order specified by  , i.e.  is a
subsequence of CT ? (Note: the appearance in a sequence
does not necessarily imply the first appearance.)
We prove OTCT is NP-complete by giving a transformation from Edge Disjoint Paths Problem .
Edge Disjoint Paths Problem (EDP)
Instance: An undirected graph G = (V; E ), collection of
disjoint vertex pairs, (s1 ; t1 ); (s2 ; t2 ); : : : ; (sk ; tk ).
Question: Does G contain k mutually edge-disjoint paths,
one connecting si and ti for each i, 1  i  k ?
It is known that EDP is NP-complete.
Theorem 6 Ordered Traversal of Closed Trail Problem
(OTCT) is NP-complete.
Proof: Clearly, the problem is in NP, as it is fairly simple
to check if the nodes of G appear in the closed trail in the
order specified by  , i.e., if  is a subsequence of the closed
trail.
We give the transformation from the Edge Disjoint Paths
problem which is known to be NP-complete.
Given a graph G = (V; E ), suppose S and T are two disjoint subsets of the vertex set V and S = fs1 ; s2 ; : : : ; sk g
and T = ft1 ; t2 ; : : : ; tk g, S \ T = ;. We create an instance of the OTCT problem from an instance of the EDP
problem. From the graph G = (V; E ), which is part
of the instance of the EDP problem, we create the graph
G+ = fV [ V 0 ; E [ E 0 g, which is a part of the instance of

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE

the OTCT problem in the following way:
8i 2 f1; : : : ; k 1g, we introduce a new node ui 2 V 0
and two new edges (ti ; ui ) 2 E 0 and (ui ; si+1 ) 2 E 0 . The
new node and edges establish a path from the node ti to the
node si+1 through the node ui . We refer to this path as Qi .
S T , i.e., the set of nodes of G
Suppose that A = V
that are neither in S nor in T .
Case A: A = ;. Introduce a new node uk 2 V 0 and connect
tk and s1 by a path Qk : tk uk s1 .
Case B: A 6= ;. Suppose that A = fv1 ; v2 ; : : : ; va g. Introduce new nodes w0 ; w1 ; : : : ; wa 1 ; wa and a new path
Qk : tk w0 v1 w1 v2 : : : wa 1 va wa s1 .
The construction of G+ is now complete. Now we
give a permutation  of the node set V [ V 0 . If A = ;,
 = s1 t1 u1 s2 t2 u2 ; : : : ; tk 1 uk 1 sk tk uk ,
If
A
6=
;,

=

s1 t1 u1 s2 t2 u2 ; : : : ; tk 1 uk 1 sk tk w0 v1 w1 v2 w2 ; : : : ; wa 1 va wa .
Clearly, the construction of G+ and the permutation 
can be carried out in polynomial time.
Claim: 9 a closed trail CT in G+ such that  is a subsequence of CT , if and only if 9 a set of edge-disjoint paths
in G connecting the nodes (si ; ti ) for 1  i  k .
( Suppose that there exists paths P1 ; : : : ; Pk connecting
the node pairs (si ; ti ) for 1  i  k respectively. Then
clearly, P1 Q1 P2    Qk 1 Pk Qk is a closed trail in G+ preserving the order of  .
) Suppose 9 such a closed trail CT in G+ .
8i 2 f1; : : : ; kg, let Wi be the walk along CT , connecting the node si with the node ti . We obtain Pi from
Wi by removing all cycles contained in Wi . Obviously,
Pi connects si with ti 8i; 1  i  k . Furthermore, Pi0 s
are edge disjoint. Now, the only thing we need to show is
Pi  G 8i; 1  i  k , i.e., the path Pi ; 1  i  k , do not
use the nodes ui ; 1  i  k , and wi ; 1  i  a. Suppose
9i such that Pi 6 G, which implies that Pi visits some
of the new nodes of the form ui or wi . But the degree of
these nodes is two, which means that these nodes can be
visited exactly once in CT . Recall that the permutation is
either  = s1 t1 u1 s2 t2 u2 ; : : : ; tk 1 uk 1 sk tk uk , or  =

5 Mathematical Programming Formulation
A mathematical programming [8] solution to the optimization version of the survivable routing problem is given
next. This formulation seeks to find a set of k mutually
edge disjoint paths between a set of k source-destination
node pairs (s1 ; t1 ); (s2 ; t2 ); : : : ; (sk ; tk ).
The integer linear programming formulation for establishing edge disjoint paths between k source-destination
node pairs (s1 ; t1 ); (s2 ; t2 ); : : : ; (sk ; tk ) is given below.
The variable r is used to identify a path, 1  r  k .
A binary indicator variable xri;j is associated with each link
(i; j ) of the graph G = (V; E ). If the variable xri;j = 1, it
indicates that the link (i; j ) is a part of the r-th path from
the source to the destination and if xri;j = 0 then it is not a
part of such a path.

Minimize

n
n
k
X
X
X

=1 =1 =1

i

j

xri;j

r

Subject to the following constraints:

(i)

8j 2 V;

X

fj2 g
i i

xri;j

V

(ii)
(iii)

xri;j

fj2 g
i i

k
X

r

X

=1

xri;j

= 0=1;

xrj;i =

V



1;

8
<
:

1
1
0

if j is the source
if j is the destination
if j is any other node

8 (i; j ) 2 E

8 r; 1  r  k; 8(i; j ) 2 E

If the above ILP produces a solution, it implies that survivable routes exist for the logical topology in the given
physical topology. However, if the ILP fails to produce a
solution, we in general, cannot infer that such routes do not
exist. However, if logical topology is a ring, we can draw
such an inference. The main problem with the above fors1 t1 u1 s2 t2 u2 ; : : : ; tk 1 uk 1 sk tk w0 v1 w1 v2 w2 ; : : : ; wa 1 va wa . mulation is that the variables xri;j are required to take only
According to the permutation  , the ui and wi nodes have
1/0 values, indicating whether the link is a part of the path
to be visited once outside Wi in CT . If these nodes
or not. A fractional value xri;j , (e.g. xri;j = 0:37) does not
are traversed once within Wi , it implies that these nodes
have a physical meaning. The computational complexity of
were traversed twice in CT , contradicting our assumption
the ILP problems is exponential, whereas LP problems can
of CT being a trail. Therefore, fP1 ; : : : ; Pk g is a set of
be solved in polynomial time by algorithms using interior
edge-disjoint paths in G, connecting si with ti .
point methods [8]. If the constraint (iii) could be relaxed in
the following way,
The above theorem shows that it is NP-complete to deterr
mine if survivable routes exists for a logical topology Gl , in
(iv ) xi;j  0; 8 r; 1  r  k ; 8(i; j ) 2 E
an arbitrary physical topology Gp , even when Gl is a ring.
Since this restricted problem (Gl restricted to be a ring) is
the ILP problem reduces to an LP problem, which can be
NP-complete, the more general Survivable Routing Probsolved in polynomial time. However, relaxing the constraint
lem (section 2), where topology of Gl can also be arbitrary,
(iii) and replacing it with (iv), leaves open the possibility
is NP-Complete.
of finding a solution where xri;j has a fractional value (e.g.

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE

FCM1

0
FCM2

0
FCMk

I

I

I

I

Figure 2. Structure of the constraint matrix of
the disjoint path problem; F CMi : Flow conservation matrix for (si ; ti ); I : Identity matrix.

xri;j

= 0:37). However, such a solution will never be obtained in case the constraint matrix of the LP formulation
has a specific structure.
Definition: Totally Unimodular Matrix: A matrix A is
totally unimodular if each square submatrix of A has determinant 0, +1 or -1 [8].
It is well known that if the constraint matrix of a LP problem is totally unimodular, then the polytope defined by the
constraint matrix has integral extreme points only [8], and
as such integral solutions to the problem can be obtained
without having explicit integrality constraints, such as constraint (iii) in the above formulation.
The set of constraints given in (i) in the above formulation for each r; 1  r  k , is known as the flow conservation constraints. These constraints essentially set up a path
from sr to tr for each r; 1  r  k . We refer to the constraint matrix formed by the constraints for a specific r as
the flow control matrix F CMr ; 1  r  k . It may be observed that F CMr ’s are nothing but a node-arc incidence
[8] matrix of the physical topology of the WDM network.
The structure of the constraint matrix with constraints (i)
and (ii) is as shown in figure 2, where F CMr ; 1  r  k ,
represent the flow conservation matrices associated with
each path from sr to tr for 1  r  k and I represents an
identity matrix. As the F CMr ’s are node-arc incidence matrix, from the following theorem, we conclude that F CMr ’s
are totally unimodular matrices.
From [8] (pp. 280) it is known that total unimodularity is
preserved under the 1-sum composition, defined as follows:

A 1 B =



A
0

0



B

i.e., if A and B are totally unimodular matrix then so is
the matrix obtained by performing 1-sum operation. By ex-

amining the structure of the constraint matrix shown in 2,
it is clear that matrix formed by F CMi ; 1  i  k form a
totally unimodular matrix (as each F CMi ; 1  i  k , is individually totally unimodular). Because of the disjointness
constraint among the paths, the lower part of the constraint
matrix of figure 2 comprises of k identity matrices. Because of the presence of these identity matrices, the complete matrix of figure 2 is no longer guaranteed to be totally unimodular. However, because for the most part the
matrix of figure 2 maintains totally unimodular property,
we conjecture that most of the extreme points of the polytope defined by this matrix are integral points. This implies
that if our conjecture is correct, we should obtain integral
solutions for the disjoint path problem, most of the time,
even when we execute the problem by relaxing the integrality constraint, (i.e. xri;j  0 instead of xri;j = 0=1,
8 r; 1  r  k; 8(i; j ) 2 E ).
We conducted a series of experiments to test our conjecture. We generated a series of random graphs, with a varying number of nodes, average degree of nodes and number
of edge disjoint paths. Using the mathematical programming formulation given earlier, we executed it on a SUN
Ultra workstation using CPLEX 6.5 mathematical programming package. We executed each data set, once with the
constraint xri;j = 0=1 and once with xri;j  0. The results
of these experiments showed that almost always optimal solutions were obtained with relaxed condition.

References
[1] J. A. Bondy and U. S. R. Murthy, Graph Theory with Applications, North Holland, 1976.
[2] O. Crochat and J. Y. Le Boudec, “Design Protection for
WDM Optical Networks”, IEEE JSAC, Vol. 16, No. 7, Sept.
1998.
[3] G. Ellinas, A. G. Hailemariam and T. E. Stern, “Protection
Cycles in Mesh WDM Networks”, IEEE Journal on Selected
Areas in Communications, vol. 18, no.10, October 2000.
[4] M.R. Garey, D.S. Johnson and R. E. Tarjan, “The planar
Hamiltonian circuit problem is NP-complete”, SIAM Journal
onf Computing, vol. 5, no. 4, pp. 704-714, December 1976.
[5] Y. Liu, D. Tipper and P. Siripongwutikorn,“Approximating
Optimal Spare Capacity Allocation by Successive Survivable
Routing”, Proceedings of IEEE Infocom’01, 2001.
[6] E. Modiano and A. Narula-Tam, “Survivable routing of
logical topologies in WDM networks”, Proc. IEEE INFOCOM’01, 2001.
[7] G. H. Sasaki, C. F. Su and D. Blight,“Simple layout algorithms to maintain network connectivity under faults”, Proc.
of 2000 Annual Allerton Conference, Champaign Il, October
2000.
[8] A. Schrijver, Theory of Linear and Integer Programming,
John Wiley and Sons, 1986.

Proceedings of the Seventh International Symposium on Computers and Communications (ISCC’02)
1530-1346/02 $17.00 © 2002 IEEE

Problem for Sensors Embedded
Temperature Sensitive Environments

Coverage

in

Arunabha Sen*, Nibedita Das*, Ling Zhou*, Bao Hong Shen*, Sudheendra Murthy* and Prajesh
Bhattacharyat *Department of Computer Science and Engineering
Arizona State University, Arizona 85281
Email: {asen, nibedita.maulik, ling.zhou, bao, sudhi}@asu.edu
tG.W. Woodruff School of Mechanical Engineering
Georgia Institute of Technology, Georgia 30332
Email: prajesh.bhattacharya@me.gatech.edu

Abstract- The coverage and connectivity problem in
sensor networks has received significant attention of the
research community in the recent years. In this paper, we
study this problem for sensors deployed in temperature
sensitive environments. This paper is motivated by the
issues encountered during deployment of bio-sensors in
a human/animal body. Radio transmitters during operation dissipate energy and raise the temperature of its
surroundings. A temperature sensitive environment like
the human body can tolerate such increase in temperature
only up to a certain threshold value, beyond which serious
injury may occur. To avoid such injuries, the sensor
placement must be carried out in a way that ensures the
surrounding temperature to remain within the threshold.
Using a thermal model for heat distribution from multiple
heat sources (radio transmitters), we observed that if the
sensor nodes are placed sufficiently apart from each other,
then the temperature of the surrounding area does not
exceed the threshold. This minimum separation distance
constraint gives rise to a new version of the sensor coverage
problem that has not been studied earlier. We prove that
both the optimization version and the feasibility version
of the new problem are NP-complete. We further show
that an c-approximation algorithm for the problem cannot
exist unless P = NP. We provide two heuristic solutions for
the problem and evaluate the efficacy of these solutions
by comparing their performances against the optimal
solution. The simulation results show that our heuristic
solutions almost always find near optimal solution in a
fraction of the time needed to find the optimal solution.
Finally, an algorithm for forming a connected sensor
network with minimum transmission power in such a

etc. Recently, there has been an immense interest in
new sensing, monitoring, wearable wireless devices and
sensor networks for healthcare and clinical applications.
This increased interest and importance of the emerging
field is demonstrated by the successful conclusion of
the third IEEE International Workshop on Wearable and
Implantable Body Sensor Networks [3]. Among many
other applications of biomedical sensor networks, the

one that pertains to artificial retina [4] deserves special
attention. The authors in [5] note that "organs that are
especially sensitive to any temperature increase due to a

lack of blood flow to them are prone to thermal damage
(e.g., lens cataracts)". The authors in [6] also emphasize
on the importance of considering possible health hazards
for individuals exposed to EM field and identify the EM
field values that is safe for human body. The authors
in [1] note that heat build-up from the sensor electronics
can jeopardize the implantation of the sensor, as elevated
temperature may cause infection, especially when the
implanted sensor becomes a haven for bacteria.
An example of a bio-medical sensor network currently
used in clinical situations [7] is shown in figure 1.
As seen in the figure, the geodesic Sensornet has a
large number of wires connecting the sensors to the
controller. This situation is clearly unwieldy. Efforts are
currently underway to replace the wired sensor network
by a wireless one in many universities and research
laboratories. The formation of a network with implanted
sensors on human or animal body poses a number of
scenario is provided.
challenges. Firstly, the biomedical sensors cannot be
implanted in any arbitrary location of the body. The
placement of the sensors has to be confined within a
I. INTRODUCTION
set of potential locations. Secondly, the placement must
A biomedical sensor is a device, which is implanted be done in such a way that the increase in temperature
in a human or animal body to monitor and transmit due to the operation of the sensors (radio transmitters) is
biological information such as retinal pressure [1], oxy- within an acceptable limit. Although such implantation
gen level on the surface of exteriorized tissues [2] poses many challenges, the researchers in our bioengi1-4244-1268-4/07/$25.00 t2007 IEEE
Thiis full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts forpublication in the IEEE SECON 2007 proceedings.
520

neering department have already implanted such sensors
in monkey brain. It is anticipated that such implantations
will become fairly regular in the next few years.
Although the coverage and connectivity problems in
sensor networks have received considerable attention
from the research community in recent years [8]-[13],
to the best of our knowledge, sensor placement and
coverage problems for a temperature sensitive environment have not been studied earlier. Clearly, the area
surrounding the location of a sensor will observe an
increase in temperature due to the operation of the
sensor and its radio transmitter. However, increase in
temperature in such sensitive areas cannot be allowed
to exceed a specified threshold. The introduction of this
thermal constraint makes the coverage problem alone
considerably more complex than similar problems studied in [8], [10], [12].
In the sensor network literature, there exists two different versions of the coverage problem [11]. In the regioncoverage version, the problem is to find the optimal
placement of the sensors so that the given region(s) of
interest can be sensed. On the other hand in the pointcoverage version, the problem is to find the optimal
placement of the sensors so that a set of pre-specified
points can be sensed. The version of the coverage
problem discussed in this paper is different from either
the region-coverage or the point-coverage problem. New
constraints are imposed due to the fact that (i) the sensors
cannot be placed in any arbitrary location if the sensing
region happens to be the human/animal body and (ii) the
sensors cannot be placed very close to each other as the
increase in temperature due to their joint operation may
exceed the acceptable threshold temperature and cause
thermal impairment.
Fowler et. al. in [14] showed that sensor coverage
problems are NP-complete. Hochbaum in [15] developed
approximation schemes for these NP-complete problems.
Coverage and connectivity problem was studied in an
integrated fashion in [10], [12], [13]. The authors in
[12] presented a Coverage Configuration Protocol that
provides different degrees of coverage depending on the
needs of the applications and explored the relationship
between coverage and connectivity. Abrams et. al. in [8]
developed a strategy for energy efficient monitoring of
wireless sensor networks.
In this paper, we present our findings on the placement
and coverage problem for medical bio-sensors implanted
in human/animal body. We thoroughly analyzed the
heat distribution phenomenon in a temperature sensitive
environment like human body. Our analysis indicates that
even if the temperature increase due to the operation of
an isolated sensor remains below the allowable thresh-

Fig. 1.

Geodesic Sensor Network for measurement of EEG

old, the surrounding temperature can still exceed the
threshold if multiple sensors operate at close proximity
of one another. However, if the sensors are placed
sufficiently apart from each other (beyond a critical
separation distance), then the temperature increase will
remain below the threshold. Due to the page limitations
and the focus of this conference, detailed discussion
on the heat transfer process in the human body is not
included in this paper. Interested readers are referred to
our report [16]. The main conclusion of our heat transfer
analysis is the following: Corresponding to the sensor
power dissipation Pdi,s, there exists a critical intersensor distance, dcr, such that if the distance between
any two deployed sensors is less than dcr, then the
temperature in the vicinity of the sensors exceeds the
maximum allowable temperature Tthreshold. Therefore,
attention must be paid during sensor deployment to
ensure that the distance between any two sensors is at
least as large as dcr,
This conclusion from our thermal analysis leads to
the formulation of a new version of the sensor coverage
problem. In this version, one would like to find out
the fewest number of sensors that have to be placed
in the region of interest, such that (i) the entire region
of interest is sensed and (ii) the distance between any
two sensors is at least as large as the critical separation
distance. The introduction of the requirement (ii) adds a
new dimension to the sensor coverage problem resulting
in additional complexity.
The contributions of this paper are as follows:
1) We introduce a new version of the placement and
coverage problem for sensors in a temperature
sensitive environment.
2) We prove that both the optimization version and
the feasibility version of the new problem are NPcomplete.
3) We show that an E-approximation algorithm for the
problem cannot exist unless P = NP.
4) We provide an integer linear programming formulation for the optimal solution of the problem.
5) We provide two heuristic solutions for the problem
and evaluate the efficacy of these solutions by
comparing their performances against the optimal

521

solution. The simulation results show that our
heuristic solutions almost always find near optimal
solution in a fraction of the time needed to find the
optimal solution.
6) We also propose an algorithm to maintain a connected network with the required coverage in such
temperature sensitive environments.
Before proceeding further into the details of our work,
we would like to draw the reader's attention on three
points regarding the results presented in this paper.
Firstly, in our simulation experiments, we have assumed
a circular sensing region associated with each sensor.
However, the heuristic solutions proposed in this paper
do not assume the sensing region to be circular and
can be used with any irregular shaped sensing region.
Secondly, although the problem studied in this paper
was motivated by an application of sensors in a temperature sensitive environment, our results are equally
applicable in other domains that have a requirement for
a minimum separation distance between any sensor node
pair. Finally, we point out that due to the minimum
separation distance constraint, performance bound of the
form (O(log N)) given in [10] for the connected sensor
cover problem cannot be obtained for this problem unless
P = NP.
II. PROBLEM FORMULATION FOR SENSOR
COVERAGE
In the sensor placement and coverage problem discussed in this paper, we have (i) a set of locations (or
points pi) to be sensed, (ii) a set of potential locations (or
points qi) for placements of sensors and (iii) a minimum
separation distance (dcr) between each pair of sensors.
The goal is to deploy as few sensors as possible in
potential placement locations such that all points pi are
sensed and no two sensors are closer than dcr Like most
of the previous studies in this area [9], [11], [13], we
assume that each sensor is capable of sensing a circular
area (disk) of radius rsen} with the location of the sensor
being the center of the circle. We assume that the sensing
radius of all the sensors are identical. An example of
the coverage problem is shown in figure 2, where the
circular nodes represent the points to be sensed (blue
points), the square nodes represent the potential locations
of the sensors (green points) and circles represent the
area sensed by the sensor located at the center of the
circle. A line connecting two square nodes indicates that

the distance between them is less than the minimum
separation distance dcr and a sensor can be deployed
in at most one of these two locations. A solution to the
problem consists of selecting the locations b, c, d, f, g

for sensor deployment whose coverage is shown by the
shaded region in the figure. It may be noted that no two
nodes in this set have a line between them, indicating
that the distance between them is at least as large as dcr

*

I Point to be se

a Potential senlso

h

lcation

\

/

D Senlsor covei-age
_
=., Node pair withili imiiliLu-n separat:ionl distalice

Fig. 2. Potential sensor locations, sensing points and sensing disk

Formally, the decision version of our sensor coverage
problem can be stated as follows:
Problem 1: Sensor Coverage Problem (SenCov)
INSTANCE: Given (i) a set of points P = {Pl, . ,Pm}
to be sensed (we will refer to these points as blue points)
(ii) a set of points Q = {ql, . .., qn }, the potential
placement locations of the sensors (we will refer to these
points as green points)
(iii) sensing radius rsen of the sensors
(iv) critical separation distance dcr
(v) an integer K
QUESTION: Is there a subset Q' C Q, such that

(i) Q'I < K,

(ii) Vpi c P, 3qj c Q' such that dist(pi, qj) < rsen,
(iii) Vqi, qj C Q', dist(qi, qj) > dcr
where dist(x, y) represents the Euclidean distance between the points x and y.
In the optimization version of the problem, the input
does not have the parameter K and the objective is to
find the smallest K such that the conditions (ii) and (iii)
are satisfied.
A. Sensor Coverage as Generalized Set Cover Problem
It may be noted that the sensor coverage problem
described above can be viewed as a generalized version
of the set cover problem as well as the independent set
problem [17]. These two problems have been studied
extensively. However, to the best of our knowledge, the
generalized version of the problems as presented in this
paper have not been studied before.
The sensor coverage problem can be transformed to
the generalized set coverage in the following way. Each
blue point pi in the instance of the sensor coverage
problem can be viewed as the element si to be covered
in the instance of the set cover problem. Corresponding

522

to green point qj in the instance of the sensor coverage
problem, we can construct a subset Sj in the instance of
the set cover problem. The set Sj will comprise of the
elements si, if the dist(pi, qj) < rsen. The generalized
version of the set cover introduced in this paper also has
a notion of a a collection of incompatible subsets. Two
subsets Si and Sj are said to be incompatible, if the
Euclidean distance between the corresponding points qi
and qj is less than the critical separation distance dcr
Otherwise, the subsets Si and Sj are compatible. The
generalized set cover problem is specified as follows:
Problem 2: Generalized Set Cover Problem (GSC)
INSTANCE: (i) a set of elements S = {sI, , smm},
(ii) a collection of subsets Si C S, 3S {SI .... , S
(iii) a collection of tuples of incompatible subsets
INC = {(Stl jSl)7 (Si27 Sj2) **(Sit, Sjt)}
(iv) an integer K
QUESTION: Is there a subset S' C S, such that
(i) 3S'l < K
(ii) VsiCS,
SSj SI,' such that si C Sj,
(iii) if Si, Sj c S' then (Si, Sj) , INC.

B. Sensor Coverage as Generalized Independent Set
Problem
The Sensor Coverage problem can also be viewed as
a generalization of the Independent Set problem [17].
The sensor coverage problem can be transformed to
the generalized independent set problem in the following
way. From an instance of the sensor coverage problem,
we can construct a graph G = (V, E) where each node in
V represents a green point qi and two nodes vi, vj C V
have an edge between them if the distance between the
corresponding points qi and qj is less than the critical
separation distance dcr In addition, with each node vi C
V, we associate a list of blue points pj. A blue point pj
will be in the list associated the node vi (representing
a green point qi), if and only if the Euclidean distance
between the points pj and qi is less than the sensing
radius rsen. The generalized independent set problem is
specified as follows:
Problem 3A: Generalized Independent Set Problem
(GIS)
INSTANCE: (i) a graph G = (V, E)
(ii) a set of elements A = {a,... , a} and
(iii) a subset Ai C A associated with each node vi C V.
We will refer to the subset Ai as list associated with vi
and will denote it by L(vi). We assume U v,vL(vi) = A.
(iv) an integer K.
QUESTION: Is there an independent set V' in the graph
G = (V, E) such that
(i) V' <K and
(ii) Uvicv,L(vi) = A

C. Feasibility issue in GSC and GIS Problems
In section II-B, we formulated the SenCov problem as
the GSC problem. In the optimization version of the GSC
problem, the goal is to find the smallest subset S' C S,
such that (i) Vsi C S, 3Sk c S', such that si C Sk and (ii)
if Si, Sj c S' then (Si, Sj) , INC. Since the optimization version of the SC problem is NP-complete [17], and
GSC reduces to SC when INC = 0, we can conclude
that the GSC is also NP-complete. This conclusion leads
us to look for approximation algorithms for GSC with
guaranteed performance bound, especially because it is
well known that such approximation algorithms with
guaranteed performance bound exists for SC [15]. It may
be noted however that although the problems SC and
GSC are very similar, they have a major difference. To
illustrate the point, we introduce the feasibility version
of the SC (SCF) and GSC (GSCF) problems. The SCF
and GSCF problems are special cases of the SC and
GSC problems respectively, when K= oc. In a similar
fashion, we can consider the feasibility version of the
GIS (GISF) and SenCov (SenCovF) problems as special
cases of the GIS and SenCov problems respectively,
when K= oc.
It may be noted that the SCF problem can be solved
easily by including in the set S' all the elements of
the set S and checking if it covers all the elements of
the set S. However, GSCF cannot be solved in such a
trivial fashion because of the incompatibility among the
elements of the set S. In fact, in the next section we show
that the GISF problem (and equivalently the GSCF and
SenCovF problems) is NP-complete.
The NP-completeness of GISF (together with equivalent GSCF and SenCovF) problem puts a brake on our
attempt to develop an approximation algorithm for the
GSC problem with a guaranteed performance bound. In
order to avoid the problem, we introduce a modified
version of the GIS problem (GISM) whose goal is to find
the smallest independent set V', whose [Uv,cv' L(vi) is
the largest. Informally, the goal of the GISM problem is
to find an independent set that covers the largest number
of blue points (points to be sensed) with smallest number
of green points (sensors). It may be noted that unlike the
GISF problem, feasibility is not an issue for the GISM
problem. Before we formally define the GISM problem,
we introduce the notion of cost of an independent set V'
as follows:
UvipEV L(vi)l + a) x VI,
C(V') = (I UviLvL(vi) LJwhere av is a number much smaller than 1/n. (note: V
is the set of nodes in the graph with IVI = n, V' is an
independent set. av is introduced to handle the case when
Uv,E]V L(vi) = Uv,E]V' L(vi) I)

523

Problem 3B: Generalized Independent Set Problem
(Modified) (GISM)
INSTANCE: (i) a graph G = (V, E), ( V n)
(ii) a set of elements A = {a, ..., a } and
(iii) a subset Ai C A associated with each node vi C V
We will refer to the subset Ai as list associated with vi
and will denote it by L(vi). We assume U v,vL(vi) = A.
In the optimization version of the GISM problem we
try to find an independent set V' whose cost C(V) is
the smallest and in the decision version of the problem,
we ask if there exists an independent set V' whose cost
C(V) is less than 1. Formally,
QUESTION: Is there an independent set V' in the graph
G = (V, E) whose cost is less than 1 (i.e., C(V') < 1)?
III. COMPUTATIONAL COMPLEXITY OF THE SENSOR

COVERAGE PROBLEM
In this section, we first show that the GISF problem
is NP-complete.
Theorem 1: The GISF problem is NP-complete.
Proof: We give a transformation from the 3Satisfiability (3-SAT) problem [17]. It may be noted
that an instance of the 3-SAT problem is made by a
set of variables U = {U, .... ug} and a set of clauses
C = {Cl,1.. ., ChI and want to find out if there is a truth
assignment to the variables in U such that all the clauses
in the set C are satisfied?
From an instance of the 3-SAT problem, we generate
an instance of the feasibility version of the GISF problem
and show that the GISF problem has a feasible solution,
if and only if the instance of the 3-SAT problem is satisfiable. The graph in the instance of GISF is constructed
in three phases.
Phase I: For each variable ui c U, we construct part of
the graph G by introducing two nodes Vi = {ui, u i} and
one edge Ei = { {ui, uv}i}, that is two nodes joined by a
single edge. Each node in the instance of GIS will have
a list of elements associated with it. In the instance we
are creating, the list of elements associated with both
the nodes ui and ui will contain a single element xi,
i.e., L(ui) = L(ui) = {fx}. It may be noted that any
independent set will contain at most one node from the

Phase III: For each clause cj c C, let the three literals
in cj be denoted by cj1, cj2, cj3. In this phase, we do
not introduce any nodes, but introduce three additional
edges, corresponding to each clause cj.
EJ' {{al[j] Cj}, a2[j], Cj2} {a3[j] Cj3}}
This concludes the construction of our instance of the
GIS, with graph G = (V, E), where
V = (ug 1Vi) U (U$ VJ) and
E = (Ug= E) U (Uh= E) U (UL iES)
where g and h corresponds to the number of variables
and clauses of the instance of the 3-SAT problem respectively. The instance of the graph G = (V, E) constructed
using the rules specified above will have 2g + 3h nodes
and g + 6h edges. Figure 3 shows the instance of the
GISF constructed from an instance of 3-SAT where U
{U1, U2, U3, U4} and C = {{ul, u3, U4}, {U1, 1U2, U4}}- It
may be noted that this construction can be completed in
polynomial time.

Fig. 3. Instance of the GISF constructed from an instance of 3-SAT
where U U{1i, 112, 113, 114} and C ={{ 1 13,11-4,11
, U2, X4}}

Claim 1: The instance of the 3-SAT problem has a
satisfying truth assignment if and only if the instance of
the GISF has a feasible solution.
Definition: The node cover of a graph G = (V, E) is
a subset V' C V, such that all edges in E have at least
one end point in the set V'.
As a step towards showing that the instance of the
3-SAT problem has a satisfying truth assignment if and
only if the instance of the GISF has a feasible solution,
we first establish that the instance of the 3-SAT problem
has a satisfying truth assignment if and only if the
instance of the GISF has a node cover of size at most
g + 2h.
Claim 2: The instance of the 3-SAT problem has a
satisfying truth assignment if and only if the instance
set {ui, ui}of the GISF has a node cover of size at most g + 2h.
Phase II: For each clause cj c C, we construct a part Moreover, this node cover must contain exactly one node
of the graph with three nodes and three edges.
from each Vi, 1 < i < g and exactly two nodes from each
V = {al[j], a2[j], a3[j}
VJ/ I < i < h.
Proof of Claim 2: The proof of this claim is given in
{{al [j] a2[j} {al [j] a3[j} {a2[j], a3[j}}
In addition, we assign L(al[j]) = L(a2[jl) = L(a3[ijl) [17] (pages 54-56) and is not repeated here for brevity.
Proof of Claim 1: In any graph G = (V, E), if V' is a
= {Yi}.
It may be noted that an independent set will contain at node cover, then V -V' must be an independent set. If
most one node from the set {aI[j], a2[jl, a3[ij]}the instance of the GISF has a node cover of size g + 2h

EJ,

524

that contains exactly one node from each Vi, 1 < i < g
and exactly two nodes from each V9', 1 < i < h, then
the instance of the GIS must also have an independent
set of size g + h that contains exactly one node from
each Vi, 1 < i < n and exactly one node from each
V', 1 < i < m. It can be easily verified that the union
of the lists associated with the nodes belonging to this
independent set will be equal to Uq 1X, U Uh lyi. This
independent set thus constitutes a feasible solution for
the GISF. Thus we can conclude that if the instance of
the 3-SAT problem has a satisfying truth assignment, the
instance of the GISF problem has a node cover of size
g+2h, which in turn implies that there is an independent
set of size g + h and the union of the lists associated with
the nodes in this independent set is equal to Ug 1Xi U
iYt.~
~~ ~~~~~~~~=
Conversely, if the instance of the 3-SAT problem
has no satisfying truth assignment, then the size of the
smallest node cover in the instance of the GISF problem
is greater than g + 2h. This implies that in the instance
of the GISF problem, size of the smallest independent
set is smaller than g + h. It may be observed that
the union of the list associated with the nodes of an
independent set of size smaller than g + h cannot be
equal to Ug=1X U Uh lyi. Thus, if the instance of the
3-SAT problem has no satisfying truth assignment, the
instance of the GISF problem has no feasible solution.
Theorem 2: The GISM problem is NP-complete.
Proof: The transformation is from the GISF problem.
The instance of the GISM problem constructed is exactly
the same as the instance of the GISF problem. It is not
difficult to verify that the instance of the GISF problem
will have an independent set V' in the graph G = (V, E)
such that U,,cv,L(vj) = A, if and only if the instance
of the GISM problem has an independent set V' whose
cost is less than 1. This proves that the GISM problem
is also NP-complete.
A. Hardness of approximation of the GISM problem
In this subsection we show that no polynomial time
E-approximation algorithm [17] can be developed for the
GISM problem unless P= NP.
Theorem 3: Unless P = NP, no E-approximation algorithm exists for the GISM problem with E < r (av is a
number much smaller than 1/n).
Proof: Suppose that there exists a polynomial time Eapproximation algorithm APP for the GISM problem
1 . This implies that for any instance I of the
with E <-nra
GISM problem, ratio between the approximate solution
for the instance I, APP(I), and the optimal solution,
OPT(I), is bounded by E.

Claim: If there exists a polynomial time e-approximation
algorithm APP for the GISM problem with e < 1,
then the GISF problem (proven to be NP-complete
earlier) can be solved in polynomial time.
The optimization version of the GISM problem finds
an independent set V' of minimum cost C(V'). The
approximation algorithm APP returns an independent
set V" with a cost value C(V"). If C(V") < 1, we know
that the instance of the GISF problem has an independent
set V" such that U,,Ev,,L(vi) = A. If C(VV") > 1,
we know that the instance of the GISF problem has no
independent set U such that Uv,EuL(vi) = A.
The reason for the last statement is the following. Suppose (if possible) C(V") returned by the Eapproximation algorithm APP (with e < n'a) is greater
than 1, but the instance of the GISF problem has an
independent set V' such that Uv,,v,L(vi) = A. If the
instance of the GISF problem has an independent set V'
such that Uv,Ev,L(vi) = A, then the optimal algorithm
OPT would have returned an independent set with cost
at most na, where n is the number of nodes in the
graph G = (V, E) and av is a number much smaller than
1/n. In this case, the ratio between the objective value
returned by the approximate algorithm APP (greater
than 1) and the objective value returned by the optimal
algorithm OPT (at most na), is not bounded by e as
e < r1a, contradicting the existence an e-approximation
algorithm.
IV. OPTIMAL SOLUTION FOR THE SENSOR
COVERAGE PROBLEM
From the discussion in section 2, it is clear that the
Sensor Coverage problem is equivalent to Generalized
Set Cover problem. The optimal solution for the Generalized Set Cover problem can be obtained by solving an
Integer Linear Program (ILP). The ILP formulation of
the GSC problem is given below.
From an instance of the GSC, first construct a n x m
matrix A whose entries are either 0 or 1. A(i, j) = 1 if
the element si is a member of the subset Sj, otherwise
A(i, j) = 0. We will use an indicator vector x =
{x1i ...* Xn }I to indicate if the subset Si is in the final
solution set S' or not. Accordingly, xi 1 iff Si C S'
and xi = 0 otherwise. The ILP formulation of GSC is
Minimize Zi=1 Xi, subject to the constraints
(i) Ax > 1, and
(ii) Vxi,xj,xi + xj < 1 if (Si, Sj) c INC.
The optimal solution to the GISM problem can be
found by solving the following integer linear programming problem. Corresponding to every node vi, we will
have one binary variable xi. The variable xi takes value

525

1, if vi is part of the independent set V' and 0 otherwise. Algorithm 1 heuristic generalized set cover
Corresponding to every element ai c A, we will have 1: initialize
Set-of-Free-Columns
..., S,k}
SI,
one binary variable yi. The variable yi takes value 1, if
0
Set-of-Blocked-Columns<=
ai c L(vi) and vi is part of the independent set V'. The
0
Set-of-Selected-Columns<=
variable yi is U otherwise. The ILP for the GISM is the 2: repeat
following:
3: Find the column with highest selection merit in Set-ofMinimize a En xi -Z
Y I yi (a is a number much
Free-Columns
smaller than 1/n), subject to the following constraints
4: Identify the set of incompatible columns for this column
5: Move the highest selection merit column to Set-of(i) >n7I X, >1
Selected-Columns
(ii) Vi, j, xi + xj < 1 if (vi, vj) c E.
6:
Move
the incompatible columns to Set-of-Blocked(iii) Vi, Yi < Zcvi xi where Vi {vi: ai c L(vi)}.
Columns
(iv) Vi, Xi, Yi =/
01.
7: if addition of this new column to Set-of-SelectedV. HEURISTICS FOR THE SENSOR COVERAGE
PROBLEM

8:

We showed that the Sensor Coverage Problem can
be viewed as a Generalized Set Cover Problem. There
exists a number of heuristic solutions for the Set Cover
problem. We will tailor one such algorithm [18] to suit
our needs to solve the sensor coverage problem stated in
the form of Generalized Set Cover problem. It may be recalled that the Generalized Set Cover (GSC) Problem is
specified by (i) a set of elements S {s. sm} (ii)
a collection of subsets Si C S, S = .SI, , Sn }, (iii)
a collection of tuples of incompatible subsets INC =
{(Silt Sj1l), (Si2, Sj2), ., (Sit, Sjt) } and the goal is to
find the smallest subset S' C S, such that
(i) Vsi C S, 3Sk C S', such that si C Sk and
(ii) if Si, Sj C S', then (Si,Sj) V INC.
From an instance of the GSC, we can construct an n x
m matrix A whose entries are either 0 or 1. A(i, j) = 1
if the element si is a member of the subset Sj, otherwise
A(i, j) = 0. We present two different greedy heuristics
for the solution of the GSC problem. Both the heuristics
select one column after another of matrix A with a goal
of covering the largest number of element of the set S.
It may be noted during execution of the algorithm
each column of matrix A (corresponding to each subset
SI, ... , Sr) can be in exactly one of the following three
states: selected, blocked, free. A column is classified as
selected if it is selected by the algorithm to be part
of the cover. A column is classified as blocked if it
is incompatible with a selected column. A column is
free if it is neither selected nor blocked. The benefit
associated with a column is measured by the number of
uncovered elements (at that time) that will be covered
by the selection of that column. The penalty of column
Ci is measured by the number of columns that will be
blocked, which were not blocked earlier by the columns
selected prior to selection of Ci. Both the heuristics use
selection-merit associated with a column for selecting the

9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

21:

Columns creates any redundant columns then
Move the redundant columns to Set-of-FreeColumns in decreasing order of their redundancy
merit
Move the incompatible columns corresponding to
the set of redundant columns from Set-of-BlockedColumns to Set-of-Free-Columns
end if
if all elements of the set S are covered then
Go to step 21
end if
if Set-of-Free-Columns is empty then
Go to step 21
end if
if Set-of-Free-Columns contains only columns with
zero merit then
Go to step 21
end if
until Set S is covered or
Set-of-Free-Columns 0 or
Set-of-Free-Columns contains only columns with
zero merit
print Set-of-Selected-Columns and the elements of the set
S covered by these columns

next column to be included in the cover. The selectionmerit of a column used by the first heuristic is the same
as its benefit value. The selection-merit of a column used
by the second heuristic the ratio of its benefit to penalty,
if penalty is greater than zero. If penalty is equal to zero
the selection-merit is taken to be equal to its benefit.

During each iteration, the algorithm chooses the free
column with the highest selection-merit to be included
in the cover. This column is moved from the set of free
columns to the set of selected columns. If in the set of
free columns, there exists columns that are incompatible
with this selected column, they are moved to the set
of blocked columns. A column in the set of selected
columns may become redundant, if addition of a new
column during one iteration renders its presence in the

526

set of selected columns unnecessary, i.e., all the elements
covered by this column is covered by some other selected
columns. A redundant column is removed from the set
of selected columns and is returned to the set of free
columns. In addition, some of the columns blocked due
to inclusion of this column in the set of selected columns,
may also be returned to the set of free columns at this
time (if they are not blocked by some other columns, still
part of the selected set). The redundancy-merit of a column is determined by the number of uncovered elements
that can potentially be covered by the movement of the
set of columns from the set of blocked columns to the set
of free columns. In case of multiple redundant columns,
the one with the highest redundancy merit is removed
first. Redundant columns are removed one after another
until no redundant columns are in the set of selected
columns.
VI. SIMULATIONS RESULTS AND DISCUSSION

We conducted extensive simulations to evaluate the
efficacy of the two heuristics proposed in the section VI
by comparing their performances with the optimal solution. For the simulation experiments, we generated two
sets of uniformly distributed random points on a plane.
In the absence of the information about the distribution
of these points in practice, we have assumed a random
distribution in our simulations. The first set represents
the points to be sensed (blue points) and the second
represents the potential location of the sensors (green
points). In addition, we generated the sensing radius of
the sensors rsen and the minimum separation distance
between the sensors dcr We are particularly interested
in exploring the impact of dcr in finding solution to the
GISM problem and its relation with rsen. If dcr 2rsen,
then we may have a situation where a part of the sensing
region cannot be covered by any sensors. This is depicted
in figure 4(b) where the area enclosed by the three points
where the circles meet cannot be covered by any sensors.
However, as shown in figure 4(a), this problem does not
exist if dcr < 3rsen. For this reason, in our simulation
experiments we have dcr < 3rsen.
*

Sensor node

.

"
C,a/
\i
I/

( /'I's\3
1

\

P-skn

a,

/ 3r1.1 \

R

i

)en
d rsen

(a) d,r <

v/3-rsen

(b) d,r

=

Fig. 4. Relationship of dcr with rsen (a) dcr
2rS en

2r,,en
=

Orsen (b) dcr

We conducted the simulation experiments by varying
the minimum separation distance parameter dcr from 0
through 50 while keeping the sensing radius parameter
(rsen) fixed at 30. The number of potential sensor locations (green points) were considered to be significantly
lower than that of the points to be sensed (blue points).
In our experiments we kept the number of blue points to
be thrice the number of green points. Five data sets (I1
through 15) were generated for each combination of (i)
the number of blue points (B), (ii) the number of green
points (G), (iii) rsen, and (iv) dcr The goal of the optimal
algorithm as well as the two heuristics were to find the
largest number of blue points that can be covered with
the smallest number of green points. It may be noted that
our optimal algorithm is guaranteed to find a solution
that covers all the blue points with the smallest number
of green points (subject to minimum separation distance
constraint), if such a solution exists. The results of our
simulation are presented in table I (next page). It may be
noted that some entries in the table have a number within
parentheses associated with it while many other entries
do not. The implication of an entry (say X) having no
number within parentheses associated with it is that X
green points (sensor locations) are sufficient to cover all
(150) blue points. If an entry (say Y) has a number (say
Z) within parentheses associated with it, it implies that
the largest number of blue points that can be covered is
Z (not 150) and it can be done using Y green points.
The evaluation of the heuristics show that both of
them produce near optimal solution most of the time.
Moreover, they produce such high quality solution in a
fraction of the time needed to find the optimal solution.
While some problem instances needed more than 24
hours of computing time to find the optimal solution on
a Pentium IV machine running CPLEX version 8.0, the
heuristics produced near optimal solutions in only a few
seconds. From their performance, we can conclude that
both heuristics are quite effective. Between heuristics 1
and 2, the second heuristic is somewhat more "intelligent" in the sense that its benefit function not only takes
into account the number of uncovered blue points being
covered by the selection of a green point, but also takes
into account the number of green points that are blocked
from future consideration due to selection of this green
point. However in practice, the seemingly "unintelligent"
heuristic 1 seem to be outperforming heuristic 2 in many
instances. Since this is somewhat counter-intuitive, we
try to explain the phenomenon below.
In the example shown in figure 5(a), we have dcr = 15
and rsen
30. If we use heuristic 1, whose selection metric is the number of uncovered blue points,
we have Selection-merit(A) = 5, Selection-merit(B) =3,

527

TABLE I
PERFORMANCE COMPARISON OF OPTIMAL AND HEURISTICS SOLUTIONS WITH B = 150, G = 50 AND r, en
OPTIMAL, HI = HEURISTIC 1, H2 = HEURISTIc 2.)
de

0
5
10
15
20

ropt
f

25~
30~

35
40
45
50

Data Set 1

6
6
6
6
6
6
6
6
6

6(145)

L4(138)

Hi
8
8
8
8
8
8
8

5(143)
5(143)
4(130)
4(128)

H2
8
8
8
7
8
8
8

IOpt]
7
7
7
7
7
7
7
7

Data Set 2

Hi
7
7
7
8
8

]

Opt

6
6
6
6
6
6
7(149)~
6(145) 8(149)~ 6
6
5(142)
7(148)
6
5(140)
6(145)

7(142)
4(131)
6(149)
4(99)
6(143)
5(134)
4(98) [5(139) 14(121)

...Node pair within minimum separation distance
* Points to be sensed

H2
7
7
7
10
9
9

5(140)
6(145)
4(113) L6(143)

FEPotential

sensor locations
Sensor locations not selected

Data Set 3

Hi
7
7
7
7
7
7

6(141)
6(130)
5(127)
3(105)
4(111)

to be

H2
7
7
8
8
7
7

Opt

6
6
6
6
6
6
9(144)~ 6
6
6(144)

6(143)
4(114)
4(112)

6(148)
6(146)
4(136)

Data Set 4

Hi
7
7
7
8
8
8

7

6(145)
5(129)
4(115)
4(115)

H2
7
7
8
7
9
8
10

7(143)
5(132)
5(130)
4(131)

30. (IN THE TABLE, OPT

Opt
7
7
7
7
7
7

7

7(148)
6(147)
5(139)
4(134)

Data Set 5

Hi

10
10
10
9
9

8(146)~

=

H2
10
10
8
10
8
9

6(144) 8(147)~
5(141)
6(140)
5(141)
5(128)
4(127)
4(120)
3(112) 4(125)

doing better than heuristic 1. These observations

are consistent with the simulation results.

VII. CONNECTED SENSOR NETWORK

(a)

~~(b)

(c)

Fig. 5. Comparison of Heuristics 1 and 2

Selection-merit(C)

=

1. So, heuristic 1 chooses the green

point A first, which is enough to cover all the blue
points as shown in 5(b). This is the optimal solution for
this problem instance. However if we use the heuristic

2, whose selection merit is the ratio of the number of
uncovered blue points to the number of green points
blocked, we have Selection-metric(A) = 5/2 = 2.5,
Selection-merit(B) = 3/1 = 3, Selection-metric(C) = 1/1
= 1. In this case, the algorithm will choose green point
B first, which only covers the blue points 2, 3 and 4,
and blocks green point A. Then the only green point
that the heuristic can choose is point C. This scenario
is depicted in figure 5(c). So, the final solution using
the heuristic 2 will comprise of green points B and C
covering only blue points 1, 2, 3 and 4. The reason why
heuristic 2 fails to find as good a solution as heuristic 1
in this case is the fact that the separation distance (dcr)
is much smaller than the sensing radius 'rsen.
It may be recalled that both the heuristics use the
selection-merit to identify the green point to be included
in the cover. In case of heuristic 1, the selection-metric
is equal to the benefit value and in case of heuristic 2,
it is equal to the benefit to penalty ratio. When rsen is
much larger than dc, the benefit associated with a green
point is much larger than the penalty and in such cases
the heuristic 1 seems to be doing better than heuristic 2.
On the other hand when d,r is much larger than 'rsen,
the penalty associated with a green point is much larger
than the benefit and in such cases the heuristic 2 seems

The previous section presented optimal and heuristic
solutions to find the minimum number of sensors required to cover a set of points in temperature sensitive
environments. In applications where these sensors are not
directly connected to the controller, they should form
a connected network so that the data sensed by any
sensor can be delivered to the controller (possibly by
multiple hops through other sensor nodes). We provide a
two-phase solution for such applications, where the first
phase optimizes on the coverage (using the heuristics of
the previous section) while the second phase determines
the minimum transmission range Tmtn, necessary so that
the selected sensors can form a connected communication network. We assume all sensors have same transmission range and two sensors can communicate if they
are within the transmission range of one another. Recall
that the minimum separation distance d,r between the
sensors depends on the power dissipation of the sensors
Pdiss. The power dissipation of a sensor comprises of
the power dissipation due to sensing and communication, of which the dissipation due to communication is
dominant. As a result, the minimum separation distance
d,r mainly depends on the the power dissipation due
to communication which determines the communication
range rcom. If the minimum transmission range Tmtn, is
such that Tmtn, < rcom, then the sensors selected in phase
1 already form a connected communication network. On
the other hand if Tmtn, > rcom, we need to select more
sensor locations to ensure connectivity while maintaining
the minimum separation distance. We have investigated
the problem of selecting minimum number of additional
sensor locations. We do not provide those results here
due to the space limitations. We provide the algorithm
to compute the minimum transmission range Tmtn, below.
Transmission Range Problem: Given a complete graph
G=(V, E) where V represents the set of sensors selected

528

in the first phase and non-negative edge weights w(e) for
e = (u, v), u, v c V representing the Euclidean distance
between sensor u and v, find a spanning tree T of G such
that maxeET w(e) is minimized. This minmax spanning
tree problem is also known as the bottleneck spanning
tree problem in the literature [19].

Algorithm 2 compute smallest transmission-range
1: Sort the edges of E such that w(el) < ... < w(em)
2: Set E' = ell}
3: for i = 2 to m do
4: if E' U {ei} is acyclic then
5:
E' =E'U{ei}
6:
end if
7: end for

8: return maxeEE/ w(e)

The algorithm computes the edges E' that form the
minmax spanning tree and returns the largest edge
weight in the tree. The transmission range of the sensors
set to the largest edge weight ensures that the resulting
communication network is connected. The algorithm
runs in O( E log( E )) time.
Theorem 2 Any spanning tree T*=(V, E') where
1 constructed using the above algoE'={e'1, e ... e' I}
rithm produces a minmax spanning tree (MMST).
Proof: The proof is by contradiction. Suppose that for
any spanning tree T of G other than T*, f (T) denotes
the smallest value of i such that e' is not in T. Assume
that T* is not a MMST and T is a MMST such that
f (T) is as large as possible. Suppose that f (T)=k. This
implies that e .,
es e. are in both T and T* but e/
is not in T. Clearly, adding the edge e/ to the edge set
E' of the tree T creates a unique cycle C, i.e., T + ek
contains C. Suppose that e' is an edge in C that is in
T but not in T*. Since e' is not a cut edge of T + e/,
T + e- e' is a connected graph with m -1 edges and
therefore another spanning tree of G. Clearly w(T')
max (w(T

-e'/), w (el)

The algorithm chose the edge e/ before it chose the
edge e'/, even though neither e/ nor e' would have
created a cycle with the edges e/, es,.. e,e,-/. This
implies w(e') > w(el). This observation, together with
the fact that w(T')=max(w(T -e), w(e/)), concludes
that w(T') < w(T). Thus T' is also a MMST. However,
f (T') > k = f (T), contradicting the choice of T as the
MMST with the largest f (T). Therefore T=T* and T*
is indeed a MMST.
VIII. CONCLUSION
In this paper, we have introduced a new version of
the sensor placement and coverage problem. We have

shown that both the optimization and the feasibility
versions of the problem are NP-complete. Moreover, an
e-approximation algorithm for the problem cannot be
developed unless P = NP. Our heuristics produce near
optimal solution for most of the problem instances in a
fraction of the time needed to find an optimal solution.
REFERENCES
[1] L. Schwiebert, S. K. Gupta, P. Siy, G. Auner, and et al,
"A biomedical smart sensor for the visually impaired," IEEE
Sensors, 2002.
[2] B. J. Sargent and D. A. Gough, "Design and validation of the
transparent oxygen sensor array," IEEE Trans. on Biomedical
Engineering, vol. 38, no. 5, 1991.
[3] Body sensor networks. [Online]. Available: http://bsn.media.
mit.edu
[4] L. Schwiebert, S. K. Gupta, and J. Weinmann, "Research
challenges in wireless networks of biomedical sensors," in
ACM/IEEE Conf on Mobile Computing and Networking, 2001.
[5] A. Hirata, G. Ushio, and T. Shiozawa, "Calculation of temperature rises in the human eye for exposure to em waves in the
ism frequency bands," IEICE Trans. on Communications, vol.
E83-B, no. 3, 2000.
[6] P. Bernardi, M. Cavagnaro, S. Pisa, and E. Piuzzi, "Sar distribution and temperature increase in an anatomical model ofthe
human eye exposed to the field radiated by the user antenna
in a wireless lan," IEEE Trans. on Microwave Theory and
Techniques, vol. 46, no. 12, 1998.
[7] Geodesic sensor networks, egi corporation. [Online]. Available:
http://www.egi.com
[8] Z. Abrams, A. Goel, and S. Plotkin, "Set k-cover algorithms
for energy efficient monitoring in wireless sensor networks," in
IPSN, 2004.
[9] S. Funke, A. Kesselman, F. Kuhn, and Z. Lotker, "Improved
approximation algorithms for connected sensor cover," in International Conf on AD-HOC Networks and Wireless, 2004.
[10] H. Gupta, S. R. Das, and Q. Gu, "Connected sensor cover: selforganization of sensor networks for efficient query execution,"
in MobiHoc, 2003.
[11] K. Kar and S. Banerjee, "Node placement for connected coverage in sensor networks," in WiOpt, 2003.
[12] X. Wang, G. Xing, Y. Zhang, C. Lu, R. Pless, and C. Gill,
"Integrated coverage and connectivity configuration in wireless
sensor networks," in SenSys, 2003.
[13] H. Zhang and J. Hou, "Maintaining sensing coverage and
connectivity in large sensor networks," UIUC, Tech. Rep.
UIUCDCS-R-2003-2351, 2003.
[14] R. J. Fowler, M. S. Paterson, and S. L. Tanimoto, "Optimal
packing and covering in the plance are np-complete," Information Processing Letters, vol. 12, no. 3, 1981.
[15] D. Hochbaum, Approximation Algorithms. PWS Publishing,
1997.
[16] A. Sen, N. Das, L. Zhou, B. Shen, S. Murthy, and P. Bhattacharya, "Coverage problem for sensors embedded in temperature sensitive environments," Arizona State University, Tech.
Rep. TR-06-015, 2006.
[17] M. Garey and D. Johnson, Computers and Intractability: A
Guide to the Theory of NP-Completeness. Freeman Press,
1979.
[18] E. Marchiori and A. Steenbeck, "An iterated heuristic algorithm
for the set covering problem," in Workshop on Algorithm
Engineering, 1998.
[19] J. Kleinberg and E. Tardos, Algorithm Design. Addison Wesley,
2005.

529

Milcom 2015 Track 2 - Networking Protocols and Performance

On the Smallest Pseudo Target Set Identification
Problem for Targeted Attack on Interdependent
Power-Communication Networks
Arun Das, Chenyang Zhou, Joydeep Banerjee, Arunabha Sen

Lloyd Greenwald

School of Computing, Informatics and Decision System Engineering
Arizona State University

Internet and Cybersecurity Research Department
LGS Innovations

Email: {arun.das, czhou24, joydeep.banerjee, asen}@asu.edu

Email: lgreenwald@LGSInnovations.com

Abstract—Recognizing the need for a deeper understanding
of the interdependence between critical infrastructures, such as
the power grid and the communication network, a number of
models have been proposed and analyzed in the last few years.
However, most of these proposed models are over simplified and
fail to capture complex interdependencies that may exist between
these critical infrastructures. The recently proposed Implicative
Interdependency Model is able to capture these complex interdependencies involving conjunctive and disjunctive relationships
to overcome most of these limitations. Due to the existing interdependencies between the power and communication networks,
a failure involving a small set of power and/or communication
network entities can trigger a cascading event, resulting in the
failure of a much larger set of entities through the cascading
failure process. This implies that an adversary with an intent
of destroying a specific set of entities E 0 (real targets), no longer
needs to make an effort to destroy E 0 directly, but instead identify
a set of smaller entities E 00 (pseudo targets), whose destruction
eventually leads to the destruction of the real target set E 0 due
to the cascading failure process. A clever adversary will thus try
to identify the smallest set of pseudo target entities E 00 , whose
destruction eventually destroys E 0 . We refer to this problem as
the Smallest Pseudo Target Set Identification Problem (SPTSIP).
We divide the problem into four classes, and show that it is
solvable in polynomial time for one class, and is NP-complete for
others. We provide an approximation algorithm for the second
class, and for the most general class, we provide an optimal
solution using ILP, and a heuristic solution. We evaluate the
efficacy of our heuristic using power and communication network
data of Maricopa County, Arizona. The experiments show that
our heuristic almost always produces near optimal results.

I. I NTRODUCTION
The last few years have seen a heightened awareness in
the research community that the critical infrastructures of the
nation do not operate in isolation. Instead, these infrastructures
are closely coupled together and form a complex ecosystem
of interdependent networks, where the well being of one
infrastructure depends heavily on the well being of another. A
case in point is the interdependent relationships between the
electric power grid and the communication network. Power
grid entities, such as the SCADA systems that control power
stations and sub-stations, are reliant on the communication
This research was supported in part by the NSF grant 1441214. The data
for the Maricopa county communication network used in this research was
provided by GeoTel communications (www.geo-tel.com).

978-1-5090-0073-9/15/$31.00 ©2015 IEEE

network to send and receive control signals. On the other
spectrum, communication network entities, such as the routers
and base stations are reliant on electric power. Understanding
the impact of cascading failures in the power grid, a not so
uncommon phenomena, becomes even more complex when the
coupling between the power grid and communication network
entities are considered. This coupling, or interdependence,
allows not only entities in the power network, such as generators and transmission lines, to trigger power failure, but also
communication network entities, such as routers and optical
fiber lines, can potentially trigger failures in the power grid.
Thus, it is imperative that the interdependencies in this complex network ecosystem be well understood, so that preventive
measures can be undertaken to avoid catastrophic failures in
Interdependent Power-Communication Networks (IPCN).
To address this need for a deeper understanding of interdependencies between multi-layered networks, in the last few
years, the research community has made significant efforts
[1-12], and accordingly proposed and analyzed a number of
models. However, most of these proposed models are over
simplified and fail to capture complex interdependencies that
may involve a combination of conjunctive and disjunctive relationships. Suppose the power network entities such as power
generators, transmission lines and substations are denoted by
the set A = {a1 , a2 , . . . , an } and the entities of the communication network, such as routers, fiber optic lines and base
stations are denoted by the set B = {b1 , b2 , . . . , bm }. Due to
the topological design of the power-communication networks,
it may so happen that an entity ai may be alive (or operational)
if (i) bj and bk are alive, or (ii) bl and bm and bn are alive,
or (iii) bp is alive. Graph based interdependency modeling,
such as in [1-7], [10] cannot capture such interdependencies
involving conjunctive and disjunctive terms. The recently
proposed Implicative Interdependency Model (IIM) [13] is a
Boolean logic based model that overcomes these limitations
and accommodates such complex interdependencies.
This interdependent relationship between the entities of the
IPCN implies that a failure involving a small set of entities can
trigger a cascading event that results in the failure of a much
larger set of entities. This creates a potential scenario where an
adversary with an intent to jeopardize a specific set of entities
E 0 , or real targets, now no longer needs to destroy E 0 directly.

1015

Milcom 2015 Track 2 - Networking Protocols and Performance
2

Instead, the adversary can take advantage of the cascading
failure process by identifying a smaller set of entities E 00 , or
pseudo targets, whose failure eventually leads to the failure
of E 0 due to the cascade. Thus, the objective of the adversary
is to identify the smallest set of pseudo targets E 00 whose
failure eventually causes E 0 to fail. In this paper we refer to
this problem as the Smallest Pseudo Target Set Identification
Problem (SPTSIP), and in the IIM setting, categorize the
problem in four classes. We show that one class of the problem
is solvable in polynomial time, whereas for others it is NPcomplete. For the second class of the problem we provide
an approximation algorithm, and for the most general form
of the problem we provide an optimal solution using ILP,
and a heuristic solution. Finally, we evaluate the efficacy of
our heuristic using power and communication network data
of Maricopa County, Arizona. The experiments show that our
heuristic almost always produces near optimal results.
II. I MPLICATIVE I NTERDEPENDENCY M ODEL (IIM)
The Implicative Interdependency Model (IIM) proposed in
[13], is an entity based model that allows representation of
complex dependency relations between entities of multi-layer
network systems. The dependent relationships between the
network entities are represented using Boolean Logic and are
termed as Implicative Interdependency Relations (IDRs). Table
I outlines a set of IDRs representing a sample IPCN where the
power network and communication network entities are represented by the sets A = {a1 , a2 , a3 , a4 } and B = {b1 , b2 , b3 }
respectively. The IDRs represent a set of Boolean conditions
that need to be satisfied for an entity to be operational. In
Table I, entity b1 is operational if either a2 is operational, or
both a1 and a3 is operational. It may be noted that although
in the IDRs of this example, A (B) type entities appear on
either the left hand side or the right hand side of an IDR, the
IIM does not require that A (B) type entities appear only on
one side of an IDR. In other words, an IDR can also be of the
form ai ← aq bj bk bl + ar bm bn + bp + as . The conjunction of
entities, such as ar bm bn , is also referred to as a minterm.
Power Network
a1 ← b1 b2
a2 ← b1 + b2
a3 ← b1 + b2 + b3
a4 ← b1 + b3

Comm. Network
b1 ← a1 a3 + a2
b2 ← a1 a2 a3
b3 ← a1 + a2 + a3
−−

TABLE I: A sample Interdependent Power-Communication Network
Entities
a1
a2
a3
a4
b1
b2
b3

0
0
1
0
0
0
0
1

1
0
1
0
0
0
1
1

Time
2
1
1
0
0
0
1
1

Steps (t)
3
4
1
1
1
1
0
1
0
1
1
1
1
1
1
1

5
1
1
1
1
1
1
1

6
1
1
1
1
1
1
1

TABLE II: Time Stepped Failure Propagation in a Multilayer Network. A value of 1 denotes entity failure.

The interdependencies expressed through IDRs govern the
failure cascade process in IIM, where the failure of a set of

entities can trigger further failures due to the interdependencies
shared between the entities. We illustrate this cascading failure
process with the help of an example. For the IPCN system of
Table I, Table II shows a time-stepped cascading failure of
entities triggered by the failure of {a2 , b3 } at time step 0.
The dependency (IDR) formulation in the IIM setting can
either be done by careful analysis of the underlying system
as was done in [11], or by consultation with subject matter
experts of these complex systems. In this paper we utilize
IIM to model the underlying IPCN system and proceed to
formulate and analyze the SPTSIP in this setting.
III. P ROBLEM F ORMULATION AND C OMPUTATIONAL
C OMPLEXITY A NALYSIS
In this section we formally state the Smallest Pseudo Target
Set Identification Problem (SPTSIP) in the IIM setting, and
analyze its complexity for different types of dependency
relations. We formulate the SPTSIP as follows:
The Smallest Pseudo Target Set Identification Problem
INSTANCE: Given:
(i) the set A and B representing the entities of the power and
communication networks respectively with n = |A|, m = |B|
(ii) a set of dependency relations or IDRs, between A and B
type entities
(iii) the set of real targets E 0 ⊆ A ∪ B
(iv) positive integer K
QUESTION: Is there a subset E 00 ⊆ A ∪ B of pseudo targets,
with |E 00 | ≤ K, whose failure at time step 0, triggers a cascade
of failures resulting in failure of the real target set E 0 by time
step p = n + m − 1?
We outline some assumptions for the SPTSIP: First, we
assume that an entity ei ∈ A ∪ B can fail by itself and not
due to its dependencies, only at time step 0. Any failures
after time step 0 occur due to the cascade effect of entities
that failed at time step 0. Second, we assume that dependent
entities immediately fail in the next time step, i.e. if ei ← ej ek ,
and ek fails at time step p − 1, then ei fails at p. Third, time
step p = n + m − 1 is a sure end of any failure cascade that
begins at time step 0 as there are at most n+m entities and we
assume that entities cannot become operational once they fail.
Finally, the pseudo target set E 00 does not have to be unique.
It may be noted that the SPTSIP and the Root Cause of
Failure (RCF) problem of [14] are considerably different.
In the RCF problem, given a failure set F, F ⊆ A ∪ B
the objective is to find the minimum number of entities
F 0 , F 0 ⊆ F such that when F 0 entities fail at time step 0, F
fails at time step p. It may be noted that for solving the RCF
problem it is sufficient to look at the entities in F and their
corresponding IDRs to find a solution F 0 . The set of entities
in (A ∪ B) \ F can completely be ignored for computing F 0 .
However, in the case of the SPTSIP, for the real target set
E 0 that must fail at time step p there is no requirement that
E 00 ⊆ E 0 and E 00 can be any subset of A∪B. This modification
to the problem considerably changes the techniques required
for tackling the SPTSIP.

1016

Milcom 2015 Track 2 - Networking Protocols and Performance
3

To analyze the complexity of the SPTSIP, we categorize the
type of IDRs encountered in IPCNs in terms of the number
of minterms they contain, and the size of each minterm. We
analyze the complexity of each of these categories as follows:

that xl is the first node in P 1, also, as xl ∈ P 2, xl has an
in-degree greater than 1. This implies that there is more than
one IDR in set S such that xl appears on the left hand side of
these IDRs. This is a contradiction as it violates the definition
of Case I type IDRs and hence the theorem is proved.

A. Case I: Problem Instance with One Minterm of Size One
For Case I the IDR’s are represented as: xi ← yj , where xi
and yj are elements of the set A (B) and B (A) respectively.
In the example ai ← bj , xi = ai , y1 = bj . As noted in [13], a
conjunctive implication of the form ai ← bj bk can be written
as two separate IDRs ai ← bj and ai ← bk . However, this
case is considered in Case II and not in Case I. This exclusion
implies that the entities that appear on the left hand side of an
IDR in Case I are unique. For Case I, Algorithm 1 presents a
polynomial time algorithm for the solution of the SPTSIP.
Algorithm 1: Case I Optimal Algorithm for SPTSIP

1
2

3

4
5
6
7
8
9
10

Data:
1. Set of network entities A ∪ B, with n = |A| and m = |B|
2. A set S of IDRs of the form y ← x, where x, y ∈ A ∪ B
3. A set of real targets E 0
Result: The smallest set of pseudo targets E 00 such that if E 00
fails at time step 0, the real target set E 0 fails by time
step p = n + m − 1
begin
Construct a directed graph G = (V, E), where V = A ∪ B.
For each IDR y ← x in S, where x, y ∈ A ∪ B, introduce
a directed edge (x, y) ∈ E;
For each node xi ∈ V , construct a transitive closure set
Cxi as follows: If there is a path from xi to some node
yi ∈ V in G, then include yi in Cxi . As
|A| + |B| = n + m, we have n + m transitive closure sets
Cxi , 1 ≤ i ≤ (n + m). Each xi is termed as the seed entity
for the transitive closure set Cxi ;
Remove all the transitive closure sets which are proper
subsets of some other transitive closure set;
E 00 ← ∅;
while E 0 6= ∅ do
For entity ej ∈ E 0 , find set Cxi such that ej ∈ Cxi ;
Include seed entity xi in E 00 ;
E 0 ← E 0 \ Cxi ;

Theorem 2. Algorithm 1 gives an optimal solution for the
SPTSIP in a multi-layer network for Case I type IDRs.
Proof. Theorem 1 proves that every pair of the transitive
closure sets created in Step 3 of Algorithm 1 are either disjoint
or is a proper subset of the other, in Step 4 of the algorithm
all transitive closure sets that are proper subsets of some
other transitive closure set are removed. This implies that
the remaining sets are all necessarily disjoint, and for every
ei ∈ E 0 , ei belongs to exactly one transitive closure set. This
necessitates that the seed entity xk of the transitive closure set
Cxk that ei belongs to, must be included in the solution. This
is done in the while loop of Step 6. To prove the optimality
claim we need to show that the number of seed entities chosen
by the algorithm is minimum. If we assume that the number
of seeds chosen is not minimum, then some Cxi chosen by
the algorithm must necessarily be a proper subset of another
closure. This contradicts Theorem 1, and hence Algorithm 1
always returns the optimal solution.
B. Case II: Problem Instance with One Minterm of Arbitrary
Size
For Case
Ql II the IDR’s
Qq are represented as:
xi ← k1 =1 yk1 k2 =1 xk2 (with xi 6= xk2 ∀xk2 , 1 ≤ k2 ≤
q), where xi , xk2 are elements of set A (B) and yk2 is an
element of set B (A). The size of the minterm is given as
l + q. In the example ar ← bu bv as . l + q = 3, xi = ar , y1 =
bu , y2 = bv and x1 = as .
Theorem 3. The SPTSIP for Case II is NP Complete

return E 00

Algorithm 1 Time Complexity: Since |A| + |B| = n + m.
Step 2 takes O(n + m + |S|) time. Step 3 can be executed in
O((n+m)3 ) time. Step 4 takes at most O((n+m)3 ) time. The
while loop in Step 6 takes at most O(n(n + m)). Therefore
the overall complexity of Algorithm 1 is O((n + m)3 ).
Theorem 1. For each pair of transitive closure sets Cxi and
Cxj produced in Step 3 of Algorithm 1, either Cxi ∩ Cxj = ∅
or Cxi ∩ Cxj = Cxi or Cxi ∩ Cxj = Cxj , where xi 6= xj .
Proof. We give a proof by contradiction, assume there exists
a pair of transitive closure sets Cxi and Cxj such that Cxi ∩
Cxj 6= ∅, Cxi ∩ Cxj 6= Cxi and Cxi ∩ Cxj 6= Cxj . Let xk ∈
Cxi ∩ Cxj , this implies that there exists a path P 1 from xi
to xk , as well as a path P 2 from xj to xk . Thus there exists
some xl such that xl ∈ P 1 and xl ∈ P 2. W.l.o.g. assume

Proof. We prove that the SPTSIP for Case II is NP-complete
by giving a transformation for the Set Cover (SC) problem
[15]. An instance of the set cover problem is specified by a
universal set S = {s1 , . . . , sn+m } and a set of subsets S 0 ,
S 0 = {S1 , . . . , Sq }, where Si ⊆ S, ∀i, 1 ≤ i ≤ q. In the set
cover problem one wants
S to know whether there exists a subset
of S 00 ⊆ S 0 such that Si ⊆S 00 Si = S and |S 00 | ≤ Q, for some
specified integer Q. From an instance of the SC problem we
create an instance of the SPTSIP in the followingQway: For
every si ∈ S, we create an IDR of the form si ← si ∈Sj Sj
∀Sj ∈ S 0 . We set the real target set E 0 = S and K = Q. It
can now easily be verified that the instance of the SC problem
has a set cover of size Q, iff in the created instance of SPTSIP
the failure of K entities at time step 0 triggers a cascade of
failures resulting in the failure of all entities in the set E 0 by
time step p = n + m − 1.
We now define the following:
Definition: Kill Set of a set of Entities P: The Kill Set of
a set of entities P, denoted by KillSet(P), is the set of all
entities in the multilayer network (including P) that fail by

1017

Milcom 2015 Track 2 - Networking Protocols and Performance
4

p = n + m − 1 time steps as a consequence of: (i) the failure
of P entities at time step 0, and (ii) the interdependency relationships (IDRs) shared between the entities of the network.
In Algorithm 2 we present an approximation algorithm for the
SPTSIP with Case II type IDRs.
Algorithm 2: Case II Approx. Algorithm for SPTSIP

1
2
3
4
5
6
7
8
9
10
11
12
13
14

Data:
1. Set of network entities A ∪ B, with
Q n = |A| and m = |B|
2. A set of IDRs of the form y ← qi=1 xi , where
xi , y ∈ A ∪ B, ∀1 ≤ i ≤ q
3. A set of real targets E 0 , with M = |E 0 |
Result: Set of entities E 00 ⊆ A ∪ B such that failure of E 00
entities in time step 0 results in failure of E 0 entities
by time step p = n + m − 1.
begin
U ← ∅;
DEPi ← ∅, Si ← ∅, ∀i = 1, . . . , M ;
KillSetj ← ∅, ∀j = 1, . . . , n + m;
foreach ei ∈ E 0 do
Q
foreach entity ej ∈ IDR ei ← qj=1 ej do
DEPi ← DEPi ∪ {ej };

19

return E 00

17
18

For Case
the following form:
Pl III an IDRPhas
q
xi ← k1 =1 yk1 + k2 =1 xk2 (with xi 6= xk2 ∀xk2 , 1 ≤ k2 ≤
q), where xi , xk2 are elements of set A (B) and yk2 is an
element of set B (A). The size of the minterm is given as
l + q. In the example ar ← bu + bv + as . l + q = 3, xi =
ar , y1 = bu , y2 = bv and x1 = as .
Theorem 5. The SPTSIP for Case III is NP Complete
Proof. We prove that the SPTSIP for Case III is NP-complete
by giving a transformation for the Vertex Cover (VC) problem
[15]. An instance of the vertex cover problem is specified by
an undirected graph G = (V, E) and an integer R. In the
vertex cover problem, one wants to know whether there is a
subset V 0 ⊆ V such that |V 0 | ≤ R, and for every edge e ∈ E,
at least one end vertex of e is in V 0 . From an instance of
the VC problem we create an instance of the SPTSIP in the
following way: From the graph G = (V, E) for each vertex
vi ∈ V that has adjacent nodes (say) vj , vk and vl , we create
an IDR vi ← vj + vk + vl . We set the real target set E 0 = V
and K = R. It can now be verified that the instance of the VC
problem has a vertex cover of size R, iff in the created instance
of SPTSIP the failure of K entities at time step 0 triggers a
cascade of failures resulting in the failure of all entities in the
set E 0 by time step p = |V | − 1.

foreach ei ∈ A ∪ B do
KillSeti ← KillSet(ei );
for d = 1 to M do
if KillSeti ∩ DEPd 6= ∅ then
Si ← Si ∪ {d};

20

16

C. Case III: Problem Instance with an Arbitrary Number of
Minterms of Size One

U ← U ∪ {i};
Si ← U ∪ {i};

E 00 ← ∅;
while U 6= ∅ do
Select Si , i = 1, ..., M that maximizes |Si ∩ U |;
E 00 ← E 00 ∪ {ei };
U ← U \ Si ;

15

Algorithm 2 Time Complexity: To construct the dependencies
in Steps 5-9 at most M IDRs will be traversed each with at
most n + m entities hence these steps take O(M (n + m))
time. In Steps 10-14 computing the kill set of each of the
n+m entities and comparing it to each of the M dependencies
of maximum size n + m requires O(M (n + m)3 ) time. And
finally, the greedy set cover in Steps 16-19 takes O(M log(n+
m)). Overall the complexity of Algorithm 2 is O(M (n+m)3 ).

Theorem 4. The approximation solution produced by Algorithm 2 for Case II type IDRs is at most O(ln(M )) times the
optimal, where M = |E 0 |
Proof. Algorithm 2 implements a a greedy approach for
solving a set cover problem. We set up the set cover problem
the following way: First, in Steps 5-9, for each entity ei ∈ E 0
we construct dependency DEPi as the set of entities out
of which at least one entity must fail for ei to fail, thus
“unsatisfying” the dependency. In Step 9 we also account
for dependency DEPi getting unsatisfied due to failure of ei
itself. The universe U contains the indexes of each of these M
dependencies. Next, in Steps 10-14, for each entity ei ∈ A∪B
we compute KillSet(ei ) and construct set Si that contains
the index of the dependencies in DEPj , j = 1, . . . , M that
has a non-empty intersection with KillSet(ei ). This implies
that with the failure of ei and the ensuing failure propagation,
DEPj gets unsatisfied. With the universe set of U and the
subsets Si , i = 1, . . . , M , the greedy technique for set cover is
used in Steps 16-19 that yields a known approximation factor
of O(ln(M )) times the optimal solution [16].

D. Case IV: Problem Instance with an Arbitrary Number of
Minterms of Arbitrary Size
This is the general case, where IDRs have arbitrary number
of minterms of arbitrary size.
Theorem 6. The SPTSIP for Case IV is NP Complete
Proof. As both Case II and Case III are special cases of Case
IV, the SPTSIP for Case IV is NP-Complete as well.
IV. A LGORITHMS FOR THE SPTSIP
In this section we propose an optimal solution for the
SPTSIP using Integer Linear Programming (ILP), and a polynomial time heuristic solution.
A. Optimal Solutions for the SPTSIP problem
We formulate an optimal solution for the SPTSIP with
an ILP that uses two variables xit and yjt . Where xit = 1,
when entity ai ∈ A is in a failed state at time step t, and 0
otherwise. And, yjt = 1, when entity bj ∈ B is in a failed
state at time step t, and 0 otherwise.

1018

Milcom 2015 Track 2 - Networking Protocols and Performance
5

The objective function can now be formulated as follows:
min

n
X

m
X

i=1

j=1

xi0 +

yj0

(1)

Where n = |A| and m = |B|. The constraints are as follows:
Failure Consistency Constraints: xit ≥ xi(t−1) , ∀t, 1 ≤ t ≤ p,
these constraints ensure that if an entity ai fails at time step t,
it continues to remain in a failed state for all subsequent time
steps. A similar constraint applies for yit variables [13].
Failure Propagation Constraints: These constraints govern the
failure cascade process caused by the dependencies shared
between the network entities. The correctness of these constraints is established in [13], we outline an overview of these
constraints here for consistency. For any Case IV type IDRs
of the form ai ← bj bk bl + bv bu + bq the subsequent steps are
followed to model the failure propagation:
Step 1: Transform the IDR to a disjunctive form of size one
minterms, i.e. ai ← c1 + c2 + bq .
Step 2: For each of the c type minterms create constraints
to model the failure cascade for individual c type minterms,
i.e. for c1 ← bj bk bl introduce c1t ≤ yj(t−1) + yk(t−1) +
yl(t−1) , ∀t, 1 ≤ t ≤ p.
Step 3: For each transformed IDR from Step 1, for example
ai ← c1 +c2 +bq , introduce a constraint of the form N ×xit ≤
c1(t−1) + c2(t−1) + bq , ∀t, 1 ≤ t ≤ p, where N is the number
of minterms in the transformed IDR, in this example N = 3.
Prior to the transformation of Step 1 if an IDR does not contain
any disjunctions (Case II), then Step 3 is skipped, or if it does
not contain any conjunctions (Case III), then Step 2 is skipped.
Real Target Set Failure Constraints: xip = 1, ∀ai ∈ E 0 , and
yip = 1, ∀bi ∈ E 0 , these constraints ensure that all entities of
the real target set E 0 are in a failed state at time step p.
Adhering to the above constraints, the objective in (1)
minimizes the total number of entities that need to fail at time
step 0 so that E 0 entities fail by time step p.
B. Heuristic Solution
We first outline the following definition:
Definition: Kill Impact of a set of Entities P: The Kill Impact
of a set of entities P, denoted by KillImpact(P), is defined
as the contribution of P entities in causing the failure of
entities in E 0 . It may be noted that any entity ei ∈ E 0 can
fail due to two reasons: (i) when ei itself fails at time step
0, or (ii) when at least one entity in all the minterms of ei ’s
IDR fail in some time step. KillImpact(P) captures these
two aspects by computing the impact of failure of P entities
on E 0 based on: (i) the number of entities that fail in E 0
at time step p when P entities fail at time step 0, and (ii)
the number of minterms in the IDR of each entity ei ∈ E 0
that get affected at time step p when P entities fail at time
step 0. For a given set of P entities, and the set of minterms
M Ti = {mt1 , mt2 , . . . , mt|M Ti | }, mtj ⊆ A ∪ B, for each
entity ei ∈ E 0 , to compute KillImpact(P) we first compute
impacti as the impact of failure of P on ei as follows:

If ei ∈ KillSet(P), impacti = 1, else if ei 6∈ KillSet(P):

S


 mtj ∩KillSet(P)6=∅ mtj 
, ∀mtj ∈ M Ti
impacti =
|M Ti |
We then compute KillImpact(P) as follows:
P|E 0 |
impacti
KillImpact(P) = i=1
|P|
In Algorithm 3, we present a heuristic technique to solve
the SPTSIP for the general case of the problem. The general
approach for Algorithm 3 is to greedily select a set of entities
that provide the maximum benefit towards reaching the objective of failing E 0 . In Steps 5-7, for each entity ej ∈ A ∪ B we
compute how frequently ej appears in all minterms in the set
of IDRs, we also compute KillImpact(ej ). Next, in Steps 814, for each entity ei ∈ E 0 , we examine each of the minterms
of ei ’s IDR and select the highest frequency entity of each
minterm to construct set ki and compute KillImpact(ki ). In
Step 15 we choose the most impactful set of entities from the
total KImpact sets constructed. Intuitively, this selection of a
higher kill impact set for inclusion implies more failures in the
target set. Also, since our objective is to minimize the size of
the entities selected, a set with the largest impact to size ratio
is preferred. Finally, the algorithm proceeds to update E 00 and
f ailed set of entities, and prunes the IDR set and minterm
set in Steps 16-19. This greedy selection process repeats until
E 0 ⊆ f ailed. The heuristic ensures that for every iteration of
the while loop in Step 3 the E 00 set increases in such a way
that at least one additional entity in E 0 fails than the previous
iteration, thus moving closer to the objective. Algorithm 3 runs
in polynomial time, specifically it runs in O(M (n+m)4 ) time,
where M = |E 0 |. In Section V, our experiments show that
Algorithm 3 almost always produces the optimal result.
V. E XPERIMENTAL R ESULTS
We now present experimental results for the SPTSIP and
compare the optimal solution computed using an ILP, with the
proposed heuristic algorithm. The experiments were conducted
on power and communication network data of Maricopa
County, Arizona. The power network data was obtained from
Platts (www.platts.com), and the communication network data
obtained from GeoTel (www.geo-tel.com). This data consisted
of 70 power plants, 470 transmission lines, 2, 690 cell towers,
7, 100 fiber-lit buildings and 42, 723 fiber links. We identified five non-intersecting geographical regions, and from the
consolidated power and communication network data of each
region, we set up interdependencies between the network
entities using the rules outlined in [13]. For continuity, we
briefly outline an overview of these rules here: For each
generator to be operational, either (i) the nearest cell tower
must be operational, or (ii) the nearest fiber-lit building and
the fiber link connecting the generator to the fiber-lit building
must be operational. For each fiber-lit building and cell tower
to be operational, at least one of the two nearest generators
and the connecting transmission lines must be operational. The
transmission lines and the fiber links have no dependencies.

1019

Milcom 2015 Track 2 - Networking Protocols and Performance

Algorithm 3: Case IV Heuristic for SPTSIP

3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

20

6
4

5
4
3

5

5

5

5

5

4

3

2

R1

R2

R3
Regions

R4

(a) Real Target Set Size: 5

0

foreach entity ei ∈ E and ei 6∈ f ailed do
Let idr in S be the IDR of entity ei ;
ki ← ∅;
foreach minterm M T in idr do
Select entity ej ∈ M T with largest
f requencyj from all entities in M T ;
ki ← ki ∪ ej ;

12
Optimal
Heuristic

10
8

7

7

4

7

7

7

7

7

6

6
4

4

2
0

R1

R2

R3
Regions

R4

R5

Optimal
Heuristic

10
8
6

6
4

6

6

6

6

6
5

4

5

4

2
0

R5

R1

R2

R3
Regions

R4

R5

(b) Real Target Set Size: 10
Number of Pseudo Targets

2

Optimal
Heuristic

8

0

Number of Pseudo Targets

1

Data:
1. Set of network entities A ∪ B, with n = |A| and m = |B|
2. A set S of IDRs of type Case IV (general case)
3. A set of real targets E 0
Result: A set of pseudo targets E 00 such that when E 00 fails at
time step 0, the real target set E 0 fails by time step
p=n+m−1
begin
E 00 ← ∅, f ailed ← ∅;
while E 0 6⊆ f ailed do
KImpact ← ∅;
foreach entity ej ∈ A ∪ B and ej 6∈ f ailed do
Compute f requencyj as the number of times ej
appears in a minterm for all IDRs in S;
KImpact ← KImpact ∪ (ej , KillImpact(ej ));

10

Number of Pseudo Targets

Number of Pseudo Targets

6

14
Optimal
Heuristic

12
10

9

8 7

9

8
7

9
8

9

8

7

6
4
2
0

R1

R2

R3
Regions

R4

R5

(c) Real Target Set Size: 15
(d) Real Target Set Size: 20
Fig. 1: Comparison of optimal and heuristic approaches for computing pseudo targets (E 00 ), for given real targets (E 0 ) of sizes 5, 10, 15
and 20, on five geographical regions of Maricopa County, Arizona.

KImpact ← KImpact ∪ (ki , KillImpact(ki ));

R EFERENCES

Select tuple (f ailSet, f ailV al) ∈ KImpact where
f ailV al ≥ val, ∀(set, val) ∈ KImpact;
E 00 ← E 00 ∪ f ailSet;
f ailed ← KillSet(E 00 );
Remove IDR of entity ek from S, ∀ek ∈ f ailed;
For each IDR in S remove all minterms that contain
entity ek , ∀ek ∈ f ailed;
return E 00

The optimal solutions were obtained by solving Integer
Linear Programs using the IBM CPLEX Optimizer 12.5. For
each of the five regions R1 through R5, real target sets of
entities of sizes 5, 10, 15 and 20 were chosen from the set of all
power and communication entities of that region. For each real
target set the optimal and heuristic solutions were computed,
and these results are presented in Fig. 1. Our experiments
showed that for the five regions considered, in the worst case
the heuristic solution differed from the optimal by a factor of
0.16, in the best case was equal to the optimal, and on an
average was within a factor of 0.02 of the optimal solution.
VI. C ONCLUSION
In this paper we presented the Smallest Pseudo Target Set
Identification Problem (SPTSIP) for targeted attack on IPCN.
We used the IIM to model interdependencies between the
two networks and classified the problem into four classes. We
showed that the problem is solvable in polynomial time for the
first class, whereas for others it is NP-complete. We provided
an approximation algorithm for the second class, and for the
general class we provided an optimal solution using ILP, and
a heuristic technique. Finally, we evaluated the efficacy of
our heuristic using power and communication network data
of Maricopa County, Arizona. Our experiments showed that
our heuristic almost always produced near optimal results.

[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
“Catastrophic cascade of failures in interdependent networks,” Nature,
vol. 464, no. 7291, pp. 1025–1028, 2010.
[2] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, “Networks formed
from interdependent networks,” Nature Physics, vol. 8, no. 1, 2011.
[3] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, “Cascade of
failures in coupled network systems with multiple support-dependence
relations,” Physical Review E, vol. 83, no. 3, p. 036116, 2011.
[4] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, “Modelling interdependent infrastructures using interacting
dynamical models,” International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63–79, 2008.
[5] P. Zhang, S. Peeta, and T. Friesz, “Dynamic game theoretic model of
multi-layer infrastructure networks,” Networks and Spatial Economics,
vol. 5, no. 2, pp. 147–178, 2005.
[6] J.-F. Castet and J. H. Saleh, “Interdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,” PloS one, vol. 8, no. 4, p. e60402, 2013.
[7] M. Parandehgheibi and E. Modiano, “Robustness of interdependent
networks: The case of communication networks and the power grid,”
in Global Communications Conference (GLOBECOM). IEEE, 2013.
[8] M. Parandehgheibi, E. Modiano, and D. Hay, “Mitigating cascading
failures in interdependent power grids and communication networks,”
in Smart Grid Communications (SmartGridComm), 2014 IEEE International Conference on. IEEE, 2014, pp. 242–247.
[9] C.-C. Liu, A. Stefanov, J. Hong, and P. Panciatici, “Intruders in the grid,”
Power and Energy Magazine, IEEE, vol. 10, no. 1, pp. 58–66, 2012.
[10] D. T. Nguyen, Y. Shen, and M. T. Thai, “Detecting critical nodes in
interdependent power networks for vulnerability assessment,” 2013.
[11] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
“Power grid vulnerability to geographically correlated failures-analysis
and control implications,” arXiv preprint arXiv:1206.1099, 2012.
[12] M. Korkali, J. G. Veneman, B. F. Tivnan, and P. D. Hines, “Reducing
cascading failure risk by increasing infrastructure network interdependency,” arXiv preprint arXiv:1410.6836, 2014.
[13] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, “Identification of k most vulnerable nodes in multi-layered network using a new
model of interdependency,” in Computer Communications Workshops
(INFOCOM WKSHPS). IEEE, 2014, pp. 831–836.
[14] A. Das, J. Banerjee, and A. Sen, “Root cause analysis of failures in
interdependent power-communication networks,” in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910–915.
[15] M. R. Garey and D. S. Johnson, “Computer and intractability,” A Guide
to the NP-Completeness. New York, NY: WH Freeman and Co., 1979.
[16] J. Kleinberg and É. Tardos, Algorithm design. Pearson Education, 2006.

1020

2015 Second European Network Intelligence Conference

Impairment Aware Dynamic Routing of Many-to-Many Flows in Elastic Optical
Networks
Damian Bulira
Department of Computer Systems and Networks
Wroclaw University of Technology
Wroclaw, Poland
Email: damian.bulira@pwr.edu.pl

visible m2m communication example are multimedia transmissions such as tele- and videoconferencing, distance learning and on-line gaming. There are also m2m applications
with demanding network requirements such as distributed
and high performance computing or data replication and load
balancing services, to name a few. Many-to-many communication architecture can be designed in multiple ways, in this
paper we investigate direct full mesh approach, as well as
replicated transmission using dedicated rendezvous replica
servers, being two-way transmission hubs. Furthermore,
we model replicated many-to-many communication using
anycast and unicast trafﬁc.
In this paper, we selected Elastic Optical Networks
(EONs) as an underlying network technology due to its
rapidly raising interest among scientists and optical devices
vendors as well as ﬂexibility for the core network interconnections. Routing algorithms presented in this work extend
Routing and Spectrum Allocation to Impairment Aware
Routing Modulation Level and Spectrum Allocation (IARMLSA), due to introduction of the notion of serial probing
on candidate paths and replica servers, usage of optical signal regenerators and distance-adaptive optical transmission.
The main contribution of this paper are i) algorithms of
dynamic IA m2m routing in EON, and ii) performance comparison of different dynamic m2m routing algorithms. To the
best of our knowledge, dynamic m2m routing algorithms,
especially focusing on different client-server assignment
strategies have not been yet investigated.
The remainder of this paper is organized as follows.
Section II presents related literature, section III describes
assumptions of many-to-many communication, while section IV describes the concept of Elastic Optical Networks. In
section V algorithms for m2m dynamic routing in EONs are
presented, whereas section VI presents key results. Finally,
the paper is concluded in section VII.

Abstract—Many-to-many (m2m) networking is a communication paradigm for provisioning numerous types of network
services. In this paradigm, a group of hosts exchange information between its members in a synchronous way. We focus on
two impairment aware dynamic routing approaches for m2m
communication: direct full mesh connectivity and replicated
transmission using rendezvous servers. Furthermore, in the
latter case several client-server assignment (CSA) schemes are
proposed. We provision m2m transmissions in Elastic Optical
Networks (EONs), being the next step in development of
optical communication. The main contribution of the paper is
performance comparison of several impairment aware routing
schemes using the metrics of m2m session blocking probability
and spectrum usage. In our extensive simulations, dynamic
server assignment using fallback replica selection always outperformed the static client-replica coupling.
Keywords-Many-to-many communication, Elastic Optical
Networks, dynamic routing, Routing and Spectrum Allocation.

I. I NTRODUCTION
Communication paradigms in computer networks are constantly evolving following the development of network applications and services. This adaptation enhances transmission
efﬁciency for growing bandwidth requirements and introduction of new technologies in all network layers. Evolution
of communication paradigms started in the beginning of the
Internet era, when unicast (one-to-one) transmission ﬁtted to
all communication services. The next step was introduction
of multicast (one-to-many) paradigm, that reduced data
transmission redundancy following the concept of communication using multicast trees. One of the current and heavily
used paradigms in the modern Internet is anycast, described
as one-to-one-of-many, where transmission is directed from
a single source host to one of the destination hosts selected
from the previously deﬁned node set. Anycast has been popularized with a dawn of Content Delivery Networks (CDNs)
that are currently a basis of modern Internet architecture.
Communication paradigm investigated in this paper,
namely many-to-many (m2m), is a well known approach
used in numerous services where group communication and
data synchronization are key underlying factors. The most
978-1-4673-7592-4/15 $31.00 © 2015 IEEE
DOI 10.1109/ENIC.2015.13

Arunabha Sen
ENGINE Center
Wroclaw University of Technology
Wroclaw, Poland
Email: asen@asu.edu

II. R ELATED

WORKS

Research ﬁndings related to many-to-many communication have appeared in numerous networking conferences and
journals. One of the ﬁrst papers on m2m communication, not
33

Figure 2. A schema of a single m2m demand from group gi in replicated
m2m communication with qi = 4.

Figure 1. A schema of a single m2m demand from group gi in meshed
m2m communication with qi = 4.

We deﬁne a m2m group to be a collection of hosts. In
considered scenario, group gi belonging to the set of groups
{g1 , g2 , . . . , gp }, each comprising of a set of hosts, is active
at any given point of time. We denote the hosts that belong
to group gi , 1 ≤ i ≤ p as {mi,1 , mi,2 , . . . , mi,qi }. Each host
mi,j , 1 ≤ i ≤ p, 1 ≤ j ≤ qi corresponds to a node v of the
underlying network topology graph G = (V, E), and streams
data at the rate of hi,j , 1 ≤ i ≤ p, 1 ≤ j ≤ qi . Each group
has a demand on the network which is expressed by (i) the
set of hosts belonging to that group, and (ii) the streaming
rates of the hosts. Thus, in this paper, demand can be referred
as an m2m group and vice versa. The total data
by
streamed
qi
hi,j . As
all hosts belonging to group gi , 1 ≤ i ≤ p is j=1
data streamed by a host mi,j , has to reach all members of its
own group (i.e., group gi , which has qi members), the total
data received by all receivers in gi from mi,j is (qi − 1)hi,j .
Thus the total 
data received by all receivers from all hosts
qi
(qi − 1)hi,j .
in group gi is j=1
In this paper, we present two different examples of manyto-many routing: direct unicast connections based on full
mesh connectivity (ﬁg. 1) and replicated communication
using rendezvous servers based on anycast and unicast
paradigms, shown in ﬁg. 2. In a full mesh approach.
Each node directly transmits data to all other nodes in the
demand with a deﬁned streaming rate. In the replicated
approach for each demand a single replica server si is
selected from {s1 , s2 , . . . , sr }. Hosts are transmitting data
to the rendezvous server, then aggregated data is forwarded
further to other participants of the m2m session. As it was
mentioned earlier, we use Elastic Optical Network as the
underlying networking topology, hence presented scenarios
are joint-network (multilayer) problems with many-to-many
connections in the the upper networking layer and EON as
a physical ﬂow provisioning technology.

strictly related to the computer networks, focuses mainly on
the early predictions of the development of teleconferences
[1]. In [2] the authors deﬁne m2m communication in the
context of multicast transmissions. They present various
examples of m2m services with multimedia conferencing,
synchronizing resources, distributed computing, shared document editing, distance learning or multiplayer gaming,
among others. The authors of [3] focus on m2m trafﬁc as
an example of HPC scheduling architecture. In [4] manyto-many trafﬁc grooming in WDM networks is investigated.
Further papers on m2m communication mention the other
application of this type of trafﬁc in the context of data
replication [5], dynamic load balancing [6] or virtual machines migration [7]. Our previous work on m2m networking
focuses on static ILP optimization [8], [9] and dynamic m2m
routing in WDM networks [10].
Overview of Elastic Optical Networks is presented in
[11]. The authors of [12] present a comprehensive survey on
routing and spectrum allocation in EONs. Impairmant aware
RSA methods are evaluated in [13]. Aspects of dynamic
routing of anycast and unicast trafﬁc in EONs are a subject
of [14]. Algorithms for dynamic routing of anycast trafﬁc
using replica servers are evaluated in [15].
III. M ANY- TO -M ANY COMMUNICATION
Many-to-many is a group communication paradigm,
where hosts being members of the particular transmission
(demand) are exchanging information between all the other
hosts in the group. Likewise during a videoconference, all
hosts send data to all other conference participants. In online (dynamic) routing, addressed in this work, demands
are not known in advance. Routing algorithms are fed with
incoming demands consisting of the set of hosts with deﬁned
streaming rate. This set of hosts initiate an m2m session, we
assume that allocation of the m2m session is successful only
if all internal ﬂows can be provisioned, otherwise the whole
session is rejected.

IV. E LASTIC O PTICAL N ETWORKS
EONs have been conceptually proposed in [11] as the
SLICE architecture. It was designed in order to provide efﬁcient and ﬂexible bandwidth provisioning in future optical

34

transport networks. EONs allow to allocate ﬂexibly-sized
optical bandwidth, using aggregation of contiguous optical
spectrum divided into narrow frequency slices. This allows
to form lightpaths tailored to the trafﬁc demand without
allocating big and not efﬁciently used chunks of spectrum.
In this paper, we investigate EON with bandwidth variable optical transponders (BV-T) implementing PolarizationDivision Multiplexing Orthogonal Frequency-Division Multiplexing (PDM-OFDM) technology with multiple modulation formats adaptively selected from BPSK, QPSK, and
{8, 16, 32, 64}-QAM. These modulation formats characterize 1, 2, . . . , 6 [bps/Hz] spectral efﬁciency with PDM
allowing to double these values. Using BV-Ts allows to
combine number of contiguous subcarriers, modulated with
the same format, what effects in serving requested bitrate. In
our simulation, we follow the ITU-T recommendation [16],
deﬁning ﬂexible frequency grid of 6.25 GHz (slice width),
furthermore, we include the guard band between allocated
subcarriers consisting of 2 contiguous frequency slices. We
rely on the transmission model estimating the transmission
reach of optical signal in a function of the selected modulation level and transported bitrate presented in [17]. The
model is based on OSNR-related metric and takes under consideration physical impairments of the optical transmission
in the ﬁber. In addition we assume that the transmission
reach is extended by regenerators, which are only used
if necessary and if the transmission reach of the lowestlevel modulation format, among available ones, is shorter
than the path length. Therefore, the optical transmission is
distance-adaptive. In our simulations the regenerators do not
perform the conversion of spectrum (wavelength change)
and modulation format.

be divided into two phases: client-server assignment (CSA)
and RMLSA in both directions - client-replica server (upload) and downstream propagation from server to clients. In
the remainder of this section we present proposed algorithms
in details, structuring the description into aforementioned
categories.
A. Full mesh connectivity
In the full mesh approach all hosts directly communicate
with all other hosts in the m2m group. This results in
qi (qi − 1) connections within a single demand, where qi
is a number of clients (hosts) taking part in the demand
gi . We propose RMLSA algorithm based on Impairment
Aware First Fit (IA-FF) wavelength assignment [18] with
precalculated candidate paths. The pseudocode of the algorithm is presented in listing Algorithm 1. Procedure
P laceM eshIAF F () (line 9) is to allocate single m2m
demand in the network. Next, ﬂows between all hosts in the
demand are processed (lines 10–11), then the priority is to
allocate ﬂows starting from the lowest available spectrum
slice (line 13) and then in the shortest path (line 14).
In our previous work [10], we showed that FF approach,
placing demands in lowest available wavelengths and then
looking for the shortest path between two nodes, is slightly
better than contradictory approach - Best Path (BP), where
ﬁrstly shortest path is assured followed by lowest wavelength
assignment. This is a result of lower level of spectrum
fragmentation in FF, as BP is prioritizing shortest path usage
without focusing on spectrum fragmentation. Furthermore,
we use range of precalculated candidate paths obtained by
k-shortest paths algorithm [19], where all ordered paths are
processed and the demand is rejected only if it can not be
allocated in any of them (line 19). The single node-node
ﬂow is allocated using allocF low() function (line 1), that
allocates n slices contiguous spectrum slices starting from
s slice in all links in requested path (lines 2-8).

V. A LGORITHMS
In this section, we present heuristic algorithms for dynamic routing of m2m ﬂows in Elastic Optical Networks.
We assume that that the routing and spectrum allocation is
centralized and path computation element (PCE) exists in the
network. We propose two communication approaches within
an m2m group: direct simultaneous communication between
hosts (full mesh connectivity) and transmission using rendezvous replica servers. In the former case, source and
destination nodes are clearly deﬁned, hence the algorithm
is only responsible for routing and spectrum allocation. As
it was mentioned in section IV we take under consideration
different modulation levels, therefore, the considered problem falls into Routing Modulation Level and Spectrum Allocation (RMLSA) category. The latter approach uses replica
servers as communication hubs. In our previous research
[10] we have shown through extensive simulations, that
using a single server per m2m group outperforms individual
host-replica assignment, hence in this paper we focus only
on transmission over single server for each m2m group.
Moreover, in a solution using rendezvous servers routing can

B. Replicated communication
Another approach investigated in this paper relies on
replicated communication using auxiliary replica servers.
Those additional nodes are aggregating, processing and
broadcasting data from the clients in order to decrease
required bandwidth allocated in the network. Introduction
of this type of transmission requires additional client-server
assignment phase before RSA stage. We propose three CSA
algorithms implementing fallback server selection. I.e. when
particular ﬂow cannot be allocated with the best replica,
next servers in the ordered list are investigated. The demand
is only rejected when it cannot be allocated to any of the
available replica servers. It is worth highlighting that for
each demand only one replica is provisioning communication for all the clients within the m2m group (demand). In
the RSA stage we propose, similarly to the meshed ﬂows,
IA-FF algorithm adapted to the m2m requirements.

35

are counted (lines 4–6). Finally, replicas are sorted in the
descending order of the number of outgoing links (line 7),
what prioritizes nodes with the highest degree.

Algorithm 1 Mesh/IA-FF
1: procedure ALLOC F LOW (path, s slice, n slices, dem)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:

for l=0 to path.links do
for s=s slice to s slice+n slices do
if l.s.is empty() then
l.s.allocate(dem)
else
release temp resources()
return false
 alloc. not possible

Algorithm 3 Assign/NodeDeg
1: procedure ASSIGN N ODE D EG (dem)
2:
for r=0 to replicas do
3:
cost[r] ← 0
4:
for l=0 to r.links do
5:
if r.l.src(r) then
6:
cost[r] ← cost[r] + 1
7:
cost.sort(desc)
8:
return cost

procedure P LACE M ESH IAFF(dem)
for src=0 to dem.clients do
for dst=0 to dem.clients do
if src = dst then
for s=0 to slices do
for p=0 to src.paths do
if src.p.dst(dst) then
if allocF low(src.p, s, ns, dem) then
break  go to next src-dst pair l:11
if NoAllocation() then
return false
 alloc. not possible - reject

The last CSA algorithm evaluated in this paper Assign/Spect, presented in listing Algorithm 4, prioritizes replicas with the highest amount of total available spectrum in
outgoing links. The rationale behind it is similar to the one
from the previously described algorithm. However, number
of available slices in the links connected to the replica
node depends on the current state of the network, that
makes the CSA algorithm more adaptive. In contrary to the
Assign/NodeDeg algorithm, Assign/Spect includes additional
f or loop traversing through all the slices in the links and
increasing the cost parameter in case that particular slice is
not assigned (lines 6–8).

1) Client-Server Assignment: First investigated algorithm
Assign/LTD chooses the servers with lowest total distance
(LTD) from the replica server to all clients within the
demand. This assures that the replica chosen among all
available servers has the shortest average distance to all the
clients. The procedure is described in listing Algorithm 2.
First, for each available replica the cost table is prepared
(line 3). Then shortest path distance from each client to the
replica is totaled (lines 4–7). At the end, the table is sorted
in the ascending order of totaled distances to each replica
(line 8).

Algorithm 4 Assign/Spect
1: procedure ASSIGN S PECT(dem)
2:
for r=0 to replicas do
3:
cost[r] ← 0
4:
for l=0 to r.links do
5:
if r.l.src(r) then
6:
for s=0 to l.slices do
7:
if r.l.s.is empty() then
8:
cost[r] ← cost[r] + 1
9:
cost.sort(desc)
10:
return cost

Algorithm 2 Assign/LTD
1: procedure ASSIGN LTD(dem)
2:
for r=0 to replicas do
3:
cost[r] ← 0
4:
for c=0 to dem.clients do
5:
for p=0 to c.paths do
6:
if c.p.dst(r) and p.is shortest() then
7:
cost[r] ← cost[r] + c.p.getHopCount()
8:
cost.sort(asc)
9:
return cost

2) RMLSA for replicated m2m ﬂows: The next stage,
after client-server assignment, in replicated m2m routing
in EON, is Routing and Spectrum Allocation. We propose
the algorithm Rep/IA-FF based on impairment aware ﬁrst
ﬁt spectrum allocation approach adapted to m2m routing.
The algorithm logic is similar to PlaceMeshIAFF, however
algorithm presented in listing Algorithm 5 is divided into
two phases: clients-replica ﬂow allocation (lines 2–9) and
replica-clients downstream ﬂow propagation (lines 10–17).
In clients-replica stage, each client in a demand is uploading
data to the replica server, then in the next phase, replica is
aggregating incoming ﬂows and propagating it back to all
the clients (own trafﬁc is not sent back to the uploading
client). In both phases Rep/IA-FF algorithm uses the same

The next algoritm Assign/NodeDeg, described in listing
Algorithm 3 as the selection criterion takes node degree of
the replica servers. As replication imposes star topology with
replica in the middle of the single demand, its degree has
the crucial role in successful ﬂow allocation. After initiation
of the cost table, all outgoing links from the replica server

36

allocFlow() function to allocate spectrum in the path between two network nodes.
Algorithm 5 Rep/IA-FF
1: procedure P LACE R EP IAFF(dem)
2:
for c=0 to dem.clients do  clients-replica trafﬁc
3:
for s=0 to slices do
4:
for p=0 to c.paths do
5:
if c.p.dst(replica(dem) then
6:
if allocF low(c.p, s, ns, dem) then
7:
break  go to replica-client alloc. l:10
8:
if NoAllocation() then
9:
return false
 alloc. not possible - reject
10:
for c=0 to dem.clients do  replica-clients trafﬁc
11:
for s=0 to slices do
12:
for p to c.paths do
13:
if c.p.src(replica(dem)) then
14:
if allocF low(c.p, s, ns, dem) then
15:
break
 go to next client l:10
16:
if NoAllocation() then
17:
return false
 alloc. not possible - reject

Figure 3.

Euro28 network topology with 7 replica servers.

VI. R ESULTS
The algorithms described in the previous section were
implemented using in-house developed simulation software.
The main goal of the experiments is to measure and compare
performance of different m2m routing strategies in EON.
The problem described in this paper is dynamic, what means
that the algorithm is fed with demands during its operation
phase, and needs to calculate routing decisions on-line
basing on the current state of the network. Demands being
the input of the routing algorithms are time-varying, therefore reserved network resources are released after demand
time-to-live is over. Number of concurrent demands in the
network is controlled by the average duration of the demand.
The experiments are performed using Euro28 network (ﬁg.
3) with 28 nodes and 82 directed links. Replica servers
location is given beforehand, as replica location problem
is not a subject of this paper. In the simulated network there
are 7 replica servers, placed in the selected network nodes.
Server location is chosen according to data center density
provided by Data Center Map [20]. In our experiments
we use a set of 30 candidate paths, precalculated using kshortest path algorithm for each pair of nodes in the network.
Considered EON consists of 2 THz of available spectrum in
a single ﬁber, divided into 320 slices of 6.25 Ghz width. The
minimum amount of slices allocated for the single ﬂow is
2, and a ﬂow must be ﬁtted into even number of spectrum
slices. As deﬁned before, ﬂows allocated in a ﬁber must be
separated with a guard band of 12.5 GHz.
The main performance criterion is m2m session blocking
probability (SBP), that is deﬁned as a ratio between the

Figure 4.

SBP for 80 Gbps of session bandwidth and 5 clients.

number of rejected m2m demands to the number of all
requested m2m ﬂows. It is essential to highlight, that an
m2m demand consist of numerous unicast ﬂows (i.e. in
mesh network in each m2m demand there are qi (qi − 1)
unicast ﬂows, where qi is a number of clients in the m2m
demand gi ). The demand is rejected if at least one of the
underlying network ﬂows can not be allocated. In this paper,
we introduce the notion of m2m session bandwidth, which
is total aggregated bandwidth of all clients’ streaming rate,
that take part in the m2m session (demand). For the sake
of simplicity, m2m session bandwidth is equally distributed
among members of the particular m2m session. To assure
quality of the results we repeat each simulation 10 times
and average the results. Furthermore, each scenario consists
of 1000 demands, where allocation result of the ﬁrst 100 is
not taken to the SBP calculation due to unsteady state of the
network.
Figure 4 presents m2m session blocking probability in
the function of number of simultaneous m2m sessions in

37

For the latter, we proposed several client-server assignment strategies. As a result of experimental simulations we
showed that replicated m2m communication outperforms
meshed scenario. Furthermore, we showed that from investigated CSA algorithms, LTD-IA together with First Fit RSA
algorithm is the best performer in the context of minimizing
m2m session blocking probability. LTD (in both variants,
with and without server fallback) consumes the least amount
of spectrum, due to selection of the server with the shortest
average paths to the clients in m2m demand. As the further
steps in dynamic m2m ﬂows provisioning in EON we
propose investigation of more sophisticated heuristics (such
as metaheuristic approaches) and introduction of multicast
routing. Furthermore, it might be beneﬁcial to deﬁne and
discuss new optimization metrics for m2m trafﬁc (such as
partial blocking), as session blocking probability (all or
none approach) introduces relatively high blocking ratio that
results in low total spectrum usage.

Figure 5. Spectrum usage for 80 Gbps of session bandwidth and 5 clients.

the network for the demands consisting of 5 clients and
total session bandwidth of 80 Gbps. We compared ﬁve m2m
routing approaches:
• FullMesh - an implementation of Mesh/IA-FF algorithm,
• LTD-FF - where Assign/LTD algorithm has been used
for CSA, however only single replica has been selected
(no server fallback is introduced) and Rep/IA-FF has
been used as RSA algorithm,
• LTD-IA-FF - similar to LTD-FF, but with server fallback,
• NodeDegree-IA-FF - Assign/NodeDeg used in CSA and
Rep/IA-FF in RSA stage,
• MostSpect-IA-FF - Assign/Spect used in CSA and
Rep/IA-FF in RSA stage.
From the results we can conclude that LTD-IA-FF outperforms all other algorithms in the SBP. In the investigated
scenario the ﬁrst session blocking events occured with 65
concurrent m2m sessions, while at the second algoritm LTDFF the ﬁrst blocking events occured at 55 m2m sessions.
This clearly shows that implementation of dynamic server
fallback enhances routing performance. MostSpect-IA-FF as
an improved version of NodeDegree-IA-FF performs slightly
better and FullMesh performs the worst from the investigated
algorithms.
We also provide comparison of aforementioned algorithms in total spectrum usage in the network (ﬁg. 5).
FullMesh uses the most spectrum due to meshed connectivity and lack of trafﬁc aggregation. Algorithms using
replication use less spectrum, with LTD algorithms utilizing
slightly less spectrum than the ones based on replica node
degree or availability of the slices in outgoing replica links.

ACKNOWLEDGMENT
This work was done when Arunabha Sen was at ENGINE
Center at Wroclaw University of Technology on his sabbatical leave from Arizona State University, Tempe, AZ, USA.
This work was supported by statutory funds of the Department of Systems and Computer Networks, Wroclaw
University of Technology and FP7 project ENGINE (Grant
Agreement No. 316097).
R EFERENCES
[1] C. H. Stevens, “Many-to many communication,” 1981.
[2] B. Quinn and K. Almeroth, “Rfc3170: Ip multicast applications: Challenges and solutions,” 2001.
[3] S. Banerjee, A. D. Chowdhury, K. Sinha, and S. K. Ghosh,
“Contention-free many-to-many communication scheduling
for high performance clusters,” in Distributed Computing and
Internet Technology. Springer, 2011, pp. 150–161.
[4] M. A. Saleh and A. E. Kamal, “Many-to-many trafﬁc grooming in wdm networks,” Journal of Optical Communications
and Networking, vol. 1, no. 5, pp. 376–391, 2009.
[5] N. Touheed, P. Selwood, P. K. Jimack, and M. Berzins, “A
comparison of some dynamic load-balancing algorithms for
a parallel adaptive ﬂow solver,” Parallel Computing, vol. 26,
no. 12, pp. 1535–1554, 2000.
[6] K. Sinha, A. D. Chowdhury, S. K. Ghosh, and S. Banerjee,
“Efﬁcient load balancing on a cluster for large scale online
video surveillance,” in Distributed Computing and Networking. Springer, 2009, pp. 450–455.
[7] T. C. Wilcox, “Dynamic load balancing of virtual machines
hosted on xen,” 2008.

VII. C ONCLUSION
In this paper we addressed a problem of impairment aware
dynamic (on-line) routing of m2m ﬂows in Elastic Optical
Networks. We deﬁned m2m ﬂow problem and proposed two
solutions based on full mesh and replicated connectivity.

[8] K. Walkowiak, D. Bulira, and D. Careglio, “Ilp modeling of
many-to-many transmissions in computer networks,” in Proc.
17th Polish Teletraf. Symp, 2012, pp. 123–128.

38

[9] ——, “Ilp modeling of many-to-many replicated multimedia
communication,” Journal of Telecommunications and Information Technology, 2013.
[10] D. Bulira and K. Walkowiak, “Dynamic routing of manyto-many trafﬁc in wdm networks,” in Transparent Optical
Networks (ICTON), 2014 16th International Conference on.
IEEE, 2014, pp. 1–4.
[11] M. Jinno, H. Takara, B. Kozicki, Y. Tsukishima, Y. Sone, and
S. Matsuoka, “Spectrum-efﬁcient and scalable elastic optical
path network: architecture, beneﬁts, and enabling technologies,” Communications Magazine, IEEE, vol. 47, no. 11, pp.
66–73, 2009.
[12] S. Talebi, F. Alam, I. Katib, M. Khamis, R. Salama, and G. N.
Rouskas, “Spectrum management techniques for elastic optical networks: A survey,” Optical Switching and Networking,
vol. 13, pp. 34–48, 2014.
[13] S. Yang and F. Kuipers, “Impairment-aware routing in translucent spectrum-sliced elastic optical path networks,” in Networks and Optical Communications (NOC), 2012 17th European Conference on. IEEE, 2012, pp. 1–6.
[14] M. Aibin and K. Walkowiak, “Dynamic routing of anycast
and unicast trafﬁc in elastic optical networks with various
modulation formatstrade-off between blocking probability
and network cost,” in High Performance Switching and Routing (HPSR), 2014 IEEE 15th International Conference on.
IEEE, 2014, pp. 64–69.
[15] K. Walkowiak, “Qos dynamic routing in content delivery networks,” in NETWORKING 2005. Networking Technologies,
Services, and Protocols; Performance of Computer and Communication Networks; Mobile and Wireless Communications
Systems. Springer, 2005, pp. 1120–1132.
[16] Recommendation G.694.1: Spectral grids for WDM applications: DWDM frequency grid, ITU-T, February 2012.
[17] C. T. Politi, V. Anagnostopoulos, C. Matrakidis, A. Stavdas,
A. Lord, V. López, and J. P. Fernández-Palacios, “Dynamic
operation of ﬂexi-grid ofdm-based networks,” in Optical
Fiber Communication Conference. Optical Society of America, 2012, pp. OTh3B–2.
[18] Y. Huang, J. P. Heritage, and B. Mukherjee, “Connection
provisioning with transmission impairment consideration in
optical wdm networks with high-speed channels,” Lightwave
Technology, Journal of, vol. 23, no. 3, pp. 982–993, 2005.
[19] J. Y. Yen, “Finding the k shortest loopless paths in a network,”
management Science, vol. 17, no. 11, pp. 712–716, 1971.
[20] (2015, Jun.) Data center map.
http://www.datacentermap.com/

[Online].

Available:

39

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

Design of a Delay-based Routing Protocol for
Multi-rate Multi-hop Mobile Ad Hoc Networks†
Sudheendra Murthy, Prasad Hegde and Arunabha Sen
Department of Computer Science and Engineering
Arizona State University, Arizona 85281
Email: {sudhi, prasad.hegde, asen}@asu.edu

Abstract—The temporal fluctuations in quality exhibited
by the wireless links act as a major challenge in the design
of efficient routing protocols in real-world Mobile Ad hoc
Networks (MANETs). None of the existing link metrics
fully account for the characteristics of data loss and delay
on the links of a MANET. This paper provides the design
of a novel delay-based link quality metric that uses realtime statistics from the wireless driver to take into account
wireless contention, congestion, channel loss and mobility.
The proposed delay metric is additive, does not introduce
much additional overhead and is reflective of the varying
wireless link quality. We design an efficient Link Delayaware Routing (LDAR) protocol based on the proposed
link metric. The proposed protocol has been implemented
on a MANET test bed consisting of 5 laptop nodes. The
evaluation of our protocol performed through extensive
experimentation on the 5-node multi-hop MANET testbed
and network simulator NS-2 demonstrate the superior
benefits of the new protocol.

I. I NTRODUCTION
In the last few years, there has been an immense effort
from the networking research community in building
reliable and robust mobile ad hoc networks (MANETs).
The self-organizing, self-forming and self-healing nature
of MANETs make it very attractive for use in military
and disaster recovery applications. However, providing
QoS guarantees for delay-sensitive applications such as
video, audio, etc. in MANETs is extremely challenging
due to the time-varying signal quality and capacity of the
shared wireless medium. The time-varying signal quality
of wireless links in a MANET is induced by many
factors such as the signal attenuation due to the physical
objects, multipath fading, the interference from other
radio sources in the medium, temperature and humidity
variations, etc. These factors introduce significant packet
losses at the higher layers and make it difficult to find
stable, high-throughput paths. With due consideration to
†

This material is based upon work supported by, or in part by, the
U.S. Army Research Laboratory and the U.S. Army Research Office
under contract/grant number W911NF-06-1-0354.

these factors, it is no surprise that the authors in [1] aptly
identify MANET as an elusive technology that is yet to
mature for many practical applications. The problem is
further exacerbated by the disparity that exists between
the many solution techniques proposed in the research
literature and the practicality of such techniques on real
MANETs. In this paper, we make an attempt at realizing
a practical 802.11-based MANET by linking statistics
from the wireless drivers and the routing protocol to
model a realistic link-quality metric that can be used for
computing efficient QoS-aware routes. To this end, we
have implemented and evaluated our link-quality metric
and the routing protocol in a 802.11a MANET testbed
consisting of 5 laptop nodes1 running Linux operating
system. The salient features of our work are as follows.
• The proposed link delay metric takes into account
the transmission, queuing, processing delays, different link data rates and 802.11 backoff in a MANET
by using real-time measurements from the radio
interface. The developed metric is an additive route
metric and can be used in shortest path routing
algorithms to efficiently compute end-to-end routes
in the network.
• The proposed metric does not introduce any significant network overhead. In case of existing link
traffic, the proposed metric passively monitors the
ongoing traffic to estimate the link quality.
• We provide the implementation design of the proposed metric on a commodity (Linux) TCP/IP stack.
The metric is combined with a proactive routing
protocol to compute high-throughput, low-delay
end-to-end routes.
• We provide experimental results of the proposed
protocol on a real MANET testbed with small
number of nodes and simulation results on network
simulator NS-2 for larger network. The results
demonstrate that the proposed metric often finds the
high-throughput paths in the network.
1

currently being extended to 20 nodes

978-1-4244-3435-0/09/$25.00 ©2009 IEEE

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

The rest of the paper is organized as follows. In
section II, we discuss the issues related to the link quality
metric selection and describe some existing metrics. We
describe the limitations of existing protocols and how the
proposed metric overcomes those limitations. In section
III, we provide the design and implementation details of
the proposed link quality metric and the routing protocol.
Section IV provides the experimental and simulation
results of the proposed protocol. Section V concludes
the paper.
II. R ELATED W ORK
The problem of finding high-throughput paths between
the MANET nodes has been the focus of some recent
research work. In particular, since multiple flows between multiple source-destination node pairs may exist
in the network at the same time, computing paths with
minimum interference is a major issue. The traditional
routing protocols, namely, AODV [2], DSR [3] and
OLSR [4] compute end-to-end routes that have minimum
hop-count. The use of the hop-count as the link metric
suffers from the fact that the minimum hop-count path
may not be the high throughput or low delay path in
the network. High quality paths can be computed in
MANETs by explicitly taking into account the quality of
the links. There are many research efforts addressing the
issue of efficient link quality metric design in MANETs.
The most relevant ones to this work are discussed next.
The expected transmission count (ETX) metric introduced in [5] has been widely adopted in many routing protocols. It uses a probing technique to estimate
the number of times a packet needs to be transmitted
(including retransmissions) on a link for a successful
transmission. Each node periodically broadcasts probe
packets. The number of probe packets that failed to
reach a node from its neighbor in a fixed time window
indicates the loss probability on the asymmetric link
from its neighbor. By including this information in the
subsequent probe packets, each node can determine the
loss probabilities with each of its neighbors in both the
forward and reverse directions, pf and pr respectively.
Then the bidirectional link loss probability p is given by
p = 1 − (1 − pf ) ∗ (1 − pr ). Based on the retransmission
policy of 802.11, the expected transmission count of
1
. The expected transa link is derived as ETX = 1−p
mission count of a path Ps,d from a source node s to
a destination node d is then the sum of the expected
transmission counts
 of the links forming the path. That
is, ETX(Ps,d ) = lij ∈Ps,d ETX(lij ).
There are several drawbacks in the design of ETX
metric. ETX employs packets of fixed size as probe packets. Thus, it makes the assumption that the probability

B

L in k R a te
36 M bps

L in k R a te
36 M bps

D

A
(S ender )
L in k R a te
6 M bps

C

(R eceiver )
L in k R a te
6 M bps

Fig. 1. Test setup of 4 nodes. Since loss rate at lowest data rate may
be equal on all the links, a routing protocol based on the ETX metric
may select A − C − D as the best path. However in this example,
A − B − D is the higher throughput path.

of packet loss on a link is independent of the packet
size. This is not necessarily true, since larger packets
are more susceptible to losses. In addition, ETX does
not pay attention to the different data rates available for
transmission on the links of the MANET since the probe
packets used in computing ETX are sent at a fixed rate
(usually, the lowest rate). This can lead to the selection
of low-throughput paths in the network as illustrated in
the network of Figure 1. In this network made up of
802.11a links (supports data rates: 6, 9, 12, 18, 24, 36,
48 and 54 Mbps), suppose that the links (A, B) and
(B, D) are operating at 36 Mbps and links (A, C) and
(C, D) are operating at 6 Mbps. Further, suppose that
the loss probabilities on all links are (approximately)
uniform. In this case, the probe packets used in the
measurement of ETX will experience (approximately)
equal loss on all the links of the network resulting in
almost identical values of ETX on all the links of the
network. An ETX based routing protocol may choose the
path A−C −D between nodes A and D, thus resulting in
reduced throughput. However, the best path in this case
would have been A−B −D. In fact, by opportunistically
placing the laptops in our MANET testbed, we were able
to recreate this scenario and observed the same behavior.
The recorded throughput results are provided in Section
IV. Another drawback of ETX is that in most wireless
device drivers, broadcast (probe) packets are given higher
priority by sending them into high priority queue in the
node and thus, data packets do not experience the same
amount of processing and queuing delays at a node.
The metric closest to ours is the Measured Transmission Time (M T T ) [6], in which the time difference
between the receipt of two successive packet acknowledgements is used as an estimate of the transmission
and the contention times of the packet. M T T is defined
as M T T = ACKi − ACKi−1 , where ACKi and
ACKi−1 are the timestamps of the current and previous

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

acknowledgements to a MAC-level packet respectively
gathered from the wireless driver. Although this metric
succeeds in significantly improving the accuracy of the
link delay estimates, it has some limitations. This can
highlighted by the fact that in modern Operating Systems
(OS) such as BSD, Linux and Windows, the 802.11
wireless device driver hands down a transmission packet
to the device in an asynchronous fashion, in which the
OS resumes its activity soon after sending the packet to
the wireless device for transmission. The device upon
receiving the acknowledgment (ACK) for the packet
either issues an interrupt to the OS or puts the ACK
in an asynchronous queue. Since handling interrupts
for every packet can result in increased system load,
contemporary OSes employ techniques of NAPI [7] or
polling. However, when NAPI or polling is used, there
is no guarantee for the OS to notice the receipt of the
ACK of every packet as soon as the ACK is received
by the device. In other words, there may be a time
lag between the actual receipt of the ACK for a packet
and the timestamp seen by the system in M T T . This
may introduce inaccuracies in the link delay estimates.
In addition, the M T T metric suggests something about
the quality of the link only when some data packets are
being sent out on the link. Our metric overcomes this
limitation by using ETX and existing data rates to derive
default link quality in the absence of data traffic on the
link. Thus, our metric takes lesser time to stabilize.
There are a few other important link quality metrics
such as the Expected Transmission Time [8], Efficient
and Accurate link quality monitor [9], Packet Drop
Probability [10], Modified ETX and Effective Number of
Transmissions [11] in the research literature. Although
some of these metrics offer improved accuracy than ETX,
they either achieve this at a higher network overhead or
still retain some limitations of ETX. The proposed metric
in this paper is far more effective since it uses real-time
statistics of the packets from the wireless driver. The
design of the proposed metric and protocol is explained
next.
III. D ESIGN AND I MPLEMENTATION OF THE L INK
D ELAY-AWARE ROUTING (LDAR) PROTOCOL
The rationale behind the LDAR model is based on the
observation that the link delay experienced by a packet
comprises of three types of delays, namely, processing
delay, queueing delay and transmission delay. In a
) is typically the
MANET, the processing delay (dprocess
i
delay incurred due to the processing of the packet in
) is the time spent
the node. The queuing delay (dqueue
i
by the packet in the interface queue. The transmission
delay (dtransmit
) of the packet depends on the behavior
i

of the 802.11 MAC protocol. The 802.11 MAC layer in
a MANET node provides the CSMA/CA media access
mechanism. A sender in an 802.11 MANET having a
packet ready for transmission waits for a fixed distributed
interframe space (dif s) interval if the medium is free. If
the medium becomes busy during the dif s interval, the
sender draws a random backoff number in a contention
window interval. After the medium becomes idle, the
sender waits for 1 dif s before starting to decrement its
backoff slot by slot. If the medium becomes busy during
the decrementing process, the process is suspended and
decrementing will resume with the remaining backoff
slots when the medium becomes free again. As soon the
backoff reaches zero, the packet is transmitted. Upon
successful reception of the packet, the receiver sends
an acknowledgement (ACK) after briefly waiting for a
short interframe space (sif s) interval. Collisions are
detected at the sender by the lack of ACKs. When
a collision is detected, the contention window size is
doubled and the process is repeated. After a fixed number
of retry attempts, the packet is dropped. Thus, based
on the 802.11 unicast retransmission mechanism, the
transmission delay of the i-th packet can be expressed
as follows.
dtransmit
= dif s + backof f (ri ) +
i
(ri + 1) × (sif s + ack + n/b) (1)

where ri is the number of retransmission attempts of the
i-th packet, n is the size of the packet, b is the data
rate of the packet. The values of dif s and sif s are
constants for a given protocol and specifically, for the
802.11a protocol considered in this paper, dif s = 28μs
and sif s = 9μs. ack is the transmission time for the
fixed sized acknowledgment packet sent at the lowest
rate2 . backof f (ri ) is the average backoff period for retry
attempt ri of the i-th packet3 given as follows.
25+ri − 1
× slot time
(2)
2
where slot time = 9μs for 802.11a protocol. Expressions similar to (1) and (2) were employed in [12] to
estimate the transmission time for use in 802.11 rate
control algorithm. Combining the processing, queueing
and transmission delays, we get the total delay di experienced by the i-th packet as follows.
backof f (ri ) =

di = dprocess
+ dqueue
+ dtransmit
i
i
i

(3)

Finally, the total delay experienced by the (i + 1)th packet Δi+1 on a wireless link is estimated as the
2
For a typical 38 byte acknowledgement packet sent at the lowest
6 Mbps rate in 802.11a, ack ≈ 51µs
3
The initial contention window size=32 for 802.11a protocol

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

exponential smoothing average of all the previous packet
delays as follows.
Δi+1 = β × di + (1 − β) × Δi

(4)

where 0 < β ≤ 1 is a smoothing parameter.
In order to estimate the total delay for the next packet,
we modified the Linux kernel and Madwifi driver for
Atheros-based wireless interface to compute the processing, queueing and transmission delays. The time taken
by the network and the data link layers of the TCP/IP
stack at the sender node to process the packet were
taken as the processing delay of the packet. This is
measured by keeping track of timestamp of the packet
when it enters the network layer and before it enters
the MAC interface queue4 . The queuing delay is the
time spent by the packet in the 802.11 MAC interface
queue. The block diagram of the LDAR implementation
is shown in Figure 2. In the LDAR architecture, the
Transport and Application Layers

Link Delay -Aware Routing (LDAR ) Protocol

Network Layer

P redic ted delay for the
nex t pac k et

IOC T L E xtension

Layer 2.5
Kernel M odule

Delay Estim ation M odule
R etrans m is s ion
C ount

B ac k off

S tatistics
M onitor

Trans m is s ion
R ate

R ate
C ontrol
M odule

U s er S pac e
K ernel S pac e

Q ueuing
D elay

P acket
Queue
M anager

802 .11 M AC
Layer
Legend
Standard layer
interfaces

802.11 Physical Layer

Fig. 2.

N ew proposed
interfaces

Block diagram of the proposed LDAR protocol

packet queue manager at the MAC layer maintains the
timestamp of the arrival and departure times of each
packet in the MAC layer queue. The rate control module
predicts the optimal data rate for the transmission of the
next packet. The choice of a specific rate depends on
various parameters such as the measure SINR of the
previous packets, rates used for the last few packets,
number of received ACKs, etc. We use the existing
SampleRate [12] rate control module provided with the
madwifi driver and extract the current data rate computed
4
In our implementation, the Linux IP layer code is modified to
keep track of the timestamps.

by SampleRate. The statistics monitor maintains the
retransmission count and backoff experienced by the
packets. The delay estimation module in the LDAR
architecture maintains the exponential smoothed average
delay of the previously seen packets. While calculating
the average time spent by the packet, the timestamps
are validated to ensure that stale data is discarded.
This is periodically sent to the proactive routing protocol (OLSR) using our IOCTL extension. The IOCTL
extension communicates information between the user
space and the kernel space by using copy f rom user
and copy to user system functions. The IOCTL-type
system communication model was preferred over other
interfaces (such as /proc) because of its efficiency and
less overhead.
The estimated delay values are sent up to LDAR
for dispersion throughout the network. LDAR uses a
modified Optimized Link State Routing (OLSR) protocol
with ETX extension5 . In the standard OLSR with ETX
extension, each node periodically broadcasts HELLO
messages that are used to maintain information about the
2-hop neighborhood around each node. The HELLO
packets are used as probe packets for the ETX computation. The 2-hop neighborhood information is used
in determining the multipoint relay (MPR) set of each
node. The Topology Control (TC) messages containing
ETX information about the links around each node are
efficiently dispersed in the network through the MPRs.
Upon receipt of the TC message, each node computes
in a centralized fashion the shortest paths to all other
nodes in the network and updates its routing tables. In the
LDAR design, the LDAR module periodically requests
the IOCTL extension module for the most recent link
delay estimates and disperses the link delay estimates to
the rest of the network using the modified TC message.
In addition, the shortest path routing in LDAR uses link
delay estimates as the link metric to compute routes to
other nodes in the network.
In equation 4, the value of smoothing parameter
β represents the tradeoff between responsiveness and
stability of the protocol. The optimal value of β may
further depend on the particular environment and the
mobility pattern of the nodes. To compute optimal value
of β for a given environment, as part of calibration
procedure, we generate ping traffic between arbitrarily
chosen nodes. The optimal value β is computed by
applying the iterative technique that minimizes squared
forecast error as described in [13].
In the absence of active data traffic on the links, LDAR
uses ETX values determined by the routing protocol
5

We modified OLSR implementation from www.olsr.org

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

in the delay computation. More specifically, we set
dprocess
= dqueue
= 0 and ri = ETX in equations (1)i
i
(3). The packet size n in equation (1) is taken to be
the packet size of the HELLO packet. The default rate
reported by the rate control module is taken to be the
.
data rate b in computing dtransmit
i
IV. E XPERIMENTAL R ESULTS

AND

A NALYSIS

We conducted two sets of experiments to measure
the effectiveness of the proposed link quality metric in
choosing superior paths between nodes in the MANET.
The first set of experiments were conducted on the
MANET testbed. In the testbed experiments, the static
scenario of Figure 1 was created with 4 nodes. The
nodes were placed in an indoor environment on a floor of
our department building. The department floor consists
of cubicles that make it a challenging environment for
testing wireless protocols. Typical distances between the
nodes were around 20 meters. In evaluating the routes
chosen by ETX-based routing protocol and LDAR, we
used the metrics - throughput, latency, jitter and packet
delivery ratio. Throughput between two nodes, which is
the maximum amount of traffic sustained on the end-toend path between the nodes was measured using iperf
tool. iperf sends a continuous stream of udp packets
between nodes for a specific amount of time (set to 1
minute) to measure the throughput. The experiment was
repeated 10 times for statistical significance of the results
and the average throughput values were computed.
Figure 3 shows the comparison in throughput between
routes chosen by ETX based routing algorithm and
LDAR. As indicated in the figure, the route chosen
by LDAR has much better throughput compared to
throughput of route chosen by ETX. The second metric
used in our experiments was end-to-end path latency,
which is defined as the average time required to transfer
data from source to the destination.
Figure 4 shows the comparison between latency between routes chosen by LDAR and ETX metric. The
route chosen by LDAR has much lesser latency compared to latency in ETX chosen route. The third metric
chosen in our experiments is jitter, which is defined as
the variation in the delay of received packets.
Figure 5 shows the comparison in jitter between routes
chosen by LDAR and ETX metric. It is clear that the
route chosen by LDAR has much lesser jitter compared
to jitter in ETX chosen route. The fourth metric selected
in our experiments is the packet delivery ratio. Packet
delivery ratio is defined as the ratio of number of packets
successfully received to the number of packets sent.
Figure 6 shows the comparison between packet delivery ratio between routes chosen by LDAR and ETX

metric. The route chosen by LDAR suffers slightly
compared to the route computed by ETX metric in this
aspect. This is due to the fact that the path used by
LDAR had high data rate resulting in slightly higher
packet losses.
A second set of experiments were conducted in a
MANET testbed consisting of 5 nodes to study the effect
of interfering traffic on the route selection. The static
network topology formed in this case is given in Figure
7. In this experiment, ping tool was used to generate
cross-traffic flows on the links (A, D) and (D, E). In the
different rounds of the experiment, the rate of the crosstraffic flow was varied. The results of the experiments
are given in Figures 8, 9 and 10. In the figures, Cross-1,
Cross-2 and Cross-3 refer to cross-traffic flows with rates
13 Kbps, 130 Kbps and 650 Kbps respectively. Even for
this setup, we can observe superior results for the LDAR
protocol compared to the ETX-based protocol. The
C

B

E

A
(S ender )

(R eceiver )
D

Fig. 7.

Test setup of 5 nodes

second set of experiments was conducted using network
simulator NS-2. In our experiment, we considered a grid
network consisting of 49 nodes arranged in a 7 × 7 grid.
The distance between neighboring nodes was chosen to
be 200m. The 802.11 MAC basic rate for control packets
(HELLO and T C messages of OLSR) and the data rate
were set at 1 Mbps and 2 Mbps respectively. Two-ray
ground propagation model was assumed for the physical
layer. The number of cross-traffic flows was varied from
0 to 15 in the grid network. For each CBR cross-traffic
flow, we selected a random sender and a random receiver
in the network and the rate for the CBR traffic was
randomly generated between 200 Kbps and 400 Kbps. In
each configuration, the throughput obtained on the routes
chosen by ETX based routing protocol and LDAR were
measured. The results of the experiment are shown in
figure 11. In our plots, the reduction in the number of
cross-traffic flows does not seem to affect the measured
throughput for some cases. This is counter-intuitive since
at first glance, it would seem that increasing the crosstraffic in the network should degrade the throughput
further. However, upon closer inspection, it revealed to

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings

15
10

ETX

5
0

2

ETX
10
5

LDAR

0

ETX vs LDAR packet delivery rao
100%

ETX

1.5
1
0.5
0

LDAR

packet delivery
probability

15

LDAR

20

ETX route jier vs LDAR route jier
2.5
Jier in ms

25

ETX route Latency vs LDAR route
latency
Latency in ms

Throughput in Mbps

Throughput in ETX route Vs LDAR
route

ETX

LDAR

80%
60%
40%
20%
0%

Fig. 3. Comparison of through- Fig. 4. Comparison of latency Fig. 5.
Comparison of jitter Fig. 6. Comparison of packet deput between routes chosen ETX between routes chosen ETX and between routes chosen ETX and livery ratio between routes chosen
and LDAR
LDAR
LDAR
ETX and LDAR

Fig. 8. Comparison of throughput between
routes chosen ETX and LDAR

Fig. 9.
Comparison of latency between
routes chosen ETX and LDAR

Throughput in ETX chosen route vs Throughput in LDAR
chosen route

Fig. 10. Comparison of jitter between routes
chosen ETX and LDAR

small network and on NS-2 for larger networks shows
the efficacy of the proposed techniques.

Throughput in Kb/sec

700
600

R EFERENCES

500
400
300
200
100
0
c ro s s -15

c ro s s -10

c ro s s -5

Number of cross flows in the network

c ro s s -0
E TX ro u te th ro u g h p u t
L D A R ro u te th ro u g h p u t

Fig. 11. Comparison of throughput between routes chosen ETX and
LDAR on 49-node grid network in NS-2

us that the additional random cross-traffic flows were
acting against the previously chosen random cross-traffic
flows. This is anticipated in the wireless network due to
the shared channel access nature.
V. C ONCLUSION

AND

F UTURE

WORK

In this paper, we proposed a new link quality metric and the routing protocol LDAR based on the new
metric that takes into account the transmission, queuing,
processing delays, different link data rates and 802.11
backoff by using real-time measurements to accurately
estimate the link quality in MANETs. The proposed
metric does not introduce any significant network overhead. We provided the implementation architecture of
the LDAR protocol that is based on commodity (Linux)
TCP/IP stack. The experimental results collected by
conducting experiments in a real MANET testbed for

[1] J. Burbank, P. Chimento, B. Haberman, and W. Kasch, “Key
Challenges of Military Tactical Networking and the Elusive
Promise of MANET Technology,” IEEE Communications Magazine, 2006.
[2] C. E. Perkins, E. M. Belding-Royer, and S. Das, “Ad hoc OnDemand Distance Vector (AODV) Routing,” ETF RFC 3561,
2003.
[3] D. B. Johnson and D. A. Maltz, “Dynamic Source Routing in
Ad Hoc Wireless Networks,” Mobile Computing, 1996.
[4] T. Clausen and P. Jacquet, “Optimized Link State Routing
Protocol (OLSR),” RFC 3626, 1996.
[5] D. Decouto, D. Aguayo, J. Bicket, and R. Morris, “A highthroughput path metric for multi-hop wireless networks,” Proceedings of MobiCom, 2003.
[6] R. Krishnan and T. cker Chiueh, “Link Characteristics Aware
Wireless Protocol Design,” Proceedings of Infocomm, 2008.
[7] A. Kuznetsov, J. H. Salim, and R. Olsson, “Introduction to
NAPI,” Linux NAPI Howto, 2002.
[8] R. Draves, J. Padhye, and B. Zill, “Routing in Multi-radio,
Multi-hop Wireless Mesh Networks,” proceedings of the 10th
ACM MobiCom, 2004.
[9] K. Kim and K. G. Shin, “Accurate Measurement of Link Quality
in Multi-hop Wireless Mesh Networks,” Proceedings of 12th
ACM Mobicom, 2006.
[10] W. Wei and A. Zakhor, “Path selection for multi-path streaming
in wireless ad-hoc networks,” in Proc. of International Conference on Image transmit, January 2006.
[11] C. Koksal and H. Balakrishnan, “Quality-aware routing metrics
for time-varying wireless mesh networks,” JSAC, 2006.
[12] J. Bicket, “Bit-rate selection in wireless networks,” MS Thesis,
MIT, 2005.
[13] R. D. Snyder, “Progressive Tuning of Simple Exponential
Smoothing Forecasts,” Journal of Operational Research Society,
1988.

